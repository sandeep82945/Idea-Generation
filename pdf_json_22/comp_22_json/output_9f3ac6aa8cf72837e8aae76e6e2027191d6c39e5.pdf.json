{
    "abstractText": "There is tremendous potential in using neural networks to optimize numerical methods. In this paper, we introduce and nalyze a framework for the neural optimization of discrete weak formulations, suitable for finite element methods. The main dea of the framework is to include a neural-network function acting as a control variable in the weak form. Finding the eural control that (quasi-) minimizes a suitable cost (or loss) functional, then yields a numerical approximation with desirable ttributes. In particular, the framework allows in a natural way the incorporation of known data of the exact solution, or the ncorporation of stabilization mechanisms (e.g., to remove spurious oscillations). The main result of our analysis pertains to the well-posedness and convergence of the associated constrained-optimization roblem. In particular, we prove under certain conditions, that the discrete weak forms are stable, and that quasi-minimizing eural controls exist, which converge quasi-optimally. We specialize the analysis results to Galerkin, least squares and minimalesidual formulations, where the neural-network dependence appears in the form of suitable weights. Elementary numerical xperiments support our findings and demonstrate the potential of the framework. 2022 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license http://creativecommons.org/licenses/by/4.0/). eywords: Optimal neural control; Artificial neural networks; Data-driven discretization; Weighted finite element methods; Quasi-minimization; uasi-optimal convergence",
    "authors": [
        {
            "affiliations": [],
            "name": "Ignacio Brevis"
        },
        {
            "affiliations": [],
            "name": "Ignacio Mugaa"
        },
        {
            "affiliations": [],
            "name": "Kristoffer G. van der Zeec"
        }
    ],
    "id": "SP:a7a3cd49e49b33351a977faf553de525376c5ceb",
    "references": [
        {
            "authors": [
                "C.F. Higham",
                "D.J. Higham"
            ],
            "title": "Deep learning: An introduction for applied mathematicians",
            "venue": "SIAM Rev. 61 (4) ",
            "year": 2019
        },
        {
            "authors": [
                "G.E. Karniadakis",
                "I.G. Kevrekidis",
                "L. Lu",
                "P. Perdikaris",
                "S. Wang",
                "L. Yang"
            ],
            "title": "Physics-informed machine learning",
            "venue": "Nat. Rev. Phys. 3 ",
            "year": 2021
        },
        {
            "authors": [
                "G.C.Y. Peng",
                "M. Alber",
                "A.B. Tepole",
                "W.R. Cannon",
                "S. De",
                "S. Dura-Bernal",
                "K. Garikipati",
                "G. Karniadakis",
                "W.W. Lytton",
                "P. Perdikaris",
                "L. Petzold",
                "E. Kuhl"
            ],
            "title": "Multiscale modeling meets machine learning: What can we learn? Arch",
            "venue": "Comput. Methods Eng. 28 (3) ",
            "year": 2021
        },
        {
            "authors": [
                "J.T. Oden",
                "J.N. Reddy"
            ],
            "title": "An Introduction to the Mathematical Theory of Finite Elements",
            "venue": "Dover, Mineola, New York, 2011, Unabridged republication of the edition published by John Wiley and Sons, New York",
            "year": 1976
        },
        {
            "authors": [
                "A. Ern",
                "J.-L. Guermond"
            ],
            "title": "Finite Elements II",
            "venue": "Galerkin Approximation, Elliptic and Mixed PDEs, in: Texts in Applied Mathematics, vol. 73, Springer Nature, Switzerland",
            "year": 2021
        },
        {
            "authors": [
                "M. Raissi",
                "P. Perdikaris",
                "G.E. Karniadakis"
            ],
            "title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "venue": "J. Comput. Phys. 378 ",
            "year": 2019
        },
        {
            "authors": [
                "J.L. Guermond"
            ],
            "title": "A finite element technique for solving first-order PDEs in L",
            "venue": "SIAM J. Numer. Anal. 42 (2) (2004) 714\u2013737. 32 I. Brevis, I. Muga and K.G. van der Zee Computer Methods in Applied Mechanics and Engineering 402 ",
            "year": 2022
        },
        {
            "authors": [
                "E. Burman",
                "A. Ern"
            ],
            "title": "Stabilized Galerkin approximation of convection-diffusion-reaction equations: discrete maximum principle and convergence",
            "venue": "Math. Comp. 74 ",
            "year": 2005
        },
        {
            "authors": [
                "V. John",
                "P. Knobloch"
            ],
            "title": "On spurious oscillations at layers diminishing (SOLD) methods for convection\u2013diffusion equations: Part I \u2013 A review",
            "venue": "Comput. Methods Appl. Mech. Engrg. 196 (17) ",
            "year": 2007
        },
        {
            "authors": [
                "J.A. Evans",
                "T.J. Hughes",
                "G. Sangalli"
            ],
            "title": "Enforcement of constraints and maximum principles in the variational multiscale method",
            "venue": "Comput. Methods Appl. Mech. Engrg. 199 (1) ",
            "year": 2009
        },
        {
            "authors": [
                "L. Demkowicz",
                "J. Gopalakrishnan",
                "I. Muga",
                "J. Zitelli"
            ],
            "title": "Wavenumber explicit analysis of a DPG method for the multidimensional Helmholtz equation",
            "venue": "Comput. Methods Appl. Mech. Engrg. 213\u2013216 ",
            "year": 2012
        },
        {
            "authors": [
                "D. Peterseim"
            ],
            "title": "Eliminating the pollution effect in Helmholtz problems by local subscale correction",
            "venue": "Math. Comp. 86 ",
            "year": 2017
        },
        {
            "authors": [
                "I. Brevis"
            ],
            "title": "I",
            "venue": "Muga, K.G. van der Zee, A machine-learning minimal-residual (ML-MRes) framework for goal-oriented finite element discretizations, Comput. Math. Appl. 95 ",
            "year": 2021
        },
        {
            "authors": [
                "L. Demkowicz",
                "J. Gopalakrishnan"
            ],
            "title": "Discontinuous Petrov\u2013Galerkin (DPG) method",
            "venue": "in: E. Stein, R. de Borst, T.J.R. Hughes (Eds.), Encyclopedia of Computational Mechanics, Second Edition, Wiley",
            "year": 2017
        },
        {
            "authors": [
                "J. Xu"
            ],
            "title": "Finite neuron method and convergence analysis",
            "venue": "Commun. Comput. Phys. 28 (5) ",
            "year": 2020
        },
        {
            "authors": [
                "J. Pousin"
            ],
            "title": "Least squares formulations for some elliptic second order problems",
            "venue": "feedforward neural network solutions and convergence results, J. Comput. Math. Data Sci. 2 ",
            "year": 2022
        },
        {
            "authors": [
                "J. M\u00fcller",
                "M. Zeinhofer"
            ],
            "title": "Error estimates for the deep Ritz method with boundary penalty",
            "venue": "in: B. Dong, Q. Li, L. Wang, Z.-Q.J. Xu (Eds.), Proceedings of Mathematical and Scientific Machine Learning, in: Proceedings of Machine Learning Research, vol. 190, PMLR",
            "year": 2022
        },
        {
            "authors": [
                "Y. Shin"
            ],
            "title": "Z",
            "venue": "Zhang, G.E. Karniadakis, Error estimates of residual minimization using neural networks for linear PDEs",
            "year": 2020
        },
        {
            "authors": [
                "S. Mishra",
                "R. Molinaro"
            ],
            "title": "Estimates on the generalization error of physics-informed neural networks for approximating PDEs",
            "venue": "IMA J. Numer. Anal. ",
            "year": 2022
        },
        {
            "authors": [
                "Z. Cai",
                "J. Chen",
                "M. Liu"
            ],
            "title": "Least-squares ReLU neural network (LSNN) method for linear advection-reaction equation",
            "venue": "J. Comput. Phys. 443 ",
            "year": 2021
        },
        {
            "authors": [
                "K. Kergrene",
                "S. Prudhomme",
                "L. Chamoin",
                "M. Laforest"
            ],
            "title": "A new goal-oriented formulation of the finite element method",
            "venue": "Comput. Methods Appl. Mech. Engrg. 327 ",
            "year": 2017
        },
        {
            "authors": [
                "A.N. Brooks",
                "T.J. Hughes"
            ],
            "title": "Streamline upwind/Petrov-Galerkin formulations for convection dominated flows with particular emphasis on the incompressible Navier-Stokes equations",
            "venue": "Comput. Methods Appl. Mech. Engrg. 32 (1) ",
            "year": 1982
        },
        {
            "authors": [
                "J. Barrett",
                "K. Morton"
            ],
            "title": "Approximate symmetrization and Petrov-Galerkin methods for diffusion-convection problems",
            "venue": "Comput. Methods Appl. Mech. Engrg. 45 (1) ",
            "year": 1984
        },
        {
            "authors": [
                "J.T. Oden"
            ],
            "title": "Optimal h-p finite element methods",
            "venue": "Comput. Methods Appl. Mech. Engrg. 112 ",
            "year": 1994
        },
        {
            "authors": [
                "D. Ray",
                "J.S. Hesthaven"
            ],
            "title": "An artificial neural network as a troubled-cell indicator",
            "venue": "J. Comput. Phys. 367 ",
            "year": 2018
        },
        {
            "authors": [
                "S. Mishra"
            ],
            "title": "A machine learning framework for data driven acceleration of computations of differential equations",
            "venue": "Math. Eng. 1 (1) ",
            "year": 2018
        },
        {
            "authors": [
                "Y. Bar-Sinai",
                "S. Hoyer",
                "J. Hickey",
                "M.P. Brenner"
            ],
            "title": "Learning data-driven discretizations for partial differential equations",
            "venue": "Proc. Natl. Acad. Sci. 116 (31) ",
            "year": 2019
        },
        {
            "authors": [
                "N. Discacciati",
                "J.S. Hesthaven",
                "D. Ray"
            ],
            "title": "Controlling oscillations in high-order discontinuous Galerkin schemes using artificial viscosity tuned by neural networks",
            "venue": "J. Comput. Phys. 409 ",
            "year": 2020
        },
        {
            "authors": [
                "Y. Wang",
                "Z. Shen",
                "Z. Long",
                "B. Dong"
            ],
            "title": "Learning to discretize: Solving 1D scalar conservation laws via deep reinforcement learning",
            "venue": "Commun. Comput. Phys. 28 (5) ",
            "year": 2020
        },
        {
            "authors": [
                "L. Schwander",
                "D. Ray",
                "J.S. Hesthaven"
            ],
            "title": "Controlling oscillations in spectral methods by local artificial viscosity governed by neural networks",
            "venue": "J. Comput. Phys. 431 ",
            "year": 2021
        },
        {
            "authors": [
                "T. Tassi",
                "A. Zingaro"
            ],
            "title": "L",
            "venue": "Dede\u2019, A machine learning approach to enhance the SUPG stabilization method for advection-dominated differential problems, Math. Eng. 5 (2) ",
            "year": 2023
        },
        {
            "authors": [
                "K.J. Fidkowski",
                "G. Chen"
            ],
            "title": "Metric-based",
            "venue": "goal-oriented mesh adaptation using machine learning, J. Comput. Phys. 426 ",
            "year": 2021
        },
        {
            "authors": [
                "J. Bohn",
                "M. Feischl"
            ],
            "title": "Recurrent neural networks as optimal mesh refinement strategies",
            "venue": "Comput. Math. Appl. 97 ",
            "year": 2021
        },
        {
            "authors": [
                "J. Sirignano",
                "K. Spiliopoulos"
            ],
            "title": "DGM: A deep learning algorithm for solving partial differential equations",
            "venue": "J. Comput. Phys. 375 ",
            "year": 2018
        },
        {
            "authors": [
                "J. Berg",
                "K. Nystr\u00f6m"
            ],
            "title": "A unified deep artificial neural network approach to partial differential equations in complex geometries",
            "venue": "Neurocomputing 317 ",
            "year": 2018
        },
        {
            "authors": [
                "M. Ainsworth",
                "J. Dong"
            ],
            "title": "Galerkin neural networks: A framework for approximating variational equations with error control",
            "venue": "SIAM J. Sci. Comput. 43 (4) ",
            "year": 2021
        },
        {
            "authors": [
                "M. Liu",
                "Z. Cai",
                "J. Chen"
            ],
            "title": "Adaptive two-layer ReLU neural network: I",
            "venue": "Best least-squares approximation, Comput. Math. Appl. 113 ",
            "year": 2022
        },
        {
            "authors": [
                "C. Uriarte",
                "D. Pardo",
                "\u00c1.J. Omella"
            ],
            "title": "A finite element based deep learning solver for parametric PDEs",
            "venue": "Comput. Methods Appl. Mech. Engrg. 391 (2022) 114562. 33 I. Brevis, I. Muga and K.G. van der Zee Computer Methods in Applied Mechanics and Engineering 402 ",
            "year": 2022
        },
        {
            "authors": [
                "J. Hesthaven",
                "S. Ubbiali"
            ],
            "title": "Non-intrusive reduced order modeling of nonlinear problems using neural networks",
            "venue": "J. Comput. Phys. 363 ",
            "year": 2018
        },
        {
            "authors": [
                "B. Khara",
                "A. Balu",
                "A. Joshi",
                "S. Sarkar",
                "C. Hegde",
                "A. Krishnamurthy"
            ],
            "title": "B",
            "venue": "Ganapathysubramanian, NeuFENet: Neural finite element solutions with theoretical bounds for parametric PDEs",
            "year": 2021
        },
        {
            "authors": [
                "G. Teichert",
                "A. Natarajan"
            ],
            "title": "A",
            "venue": "Van der Ven, K. Garikipati, Machine learning materials physics: Integrable deep neural networks enable scale bridging by learning free energy functions, Comput. Methods Appl. Mech. Engrg. 353 ",
            "year": 2019
        },
        {
            "authors": [
                "J. Berg",
                "K. Nystr\u00f6m"
            ],
            "title": "Neural networks as smooth priors for inverse problems for PDEs",
            "venue": "J. Comput. Math. Data Sci. 1 ",
            "year": 2021
        },
        {
            "authors": [
                "K. Xu",
                "E. Darve"
            ],
            "title": "Physics constrained learning for data-driven inverse modeling from sparse observations",
            "venue": "J. Comput. Phys. 453 ",
            "year": 2022
        },
        {
            "authors": [
                "H. You",
                "Y. Yu",
                "N. Trask",
                "M. Gulian"
            ],
            "title": "M",
            "venue": "D\u2019Elia, Data-driven learning of nonlocal physics from high-fidelity synthetic data, Comput. Methods Appl. Mech. Engrg. 374 ",
            "year": 2021
        },
        {
            "authors": [
                "K. Bhattacharya",
                "B. Hosseini",
                "N.B. Kovachki",
                "A.M. Stuart"
            ],
            "title": "Model reduction and neural networks for parametric PDEs",
            "venue": "SMAI J. Comput. Math. 7 ",
            "year": 2021
        },
        {
            "authors": [
                "L. Cao"
            ],
            "title": "T",
            "venue": "O\u2019Leary-Roseberry, P.K. Jha, J.T. Oden, O. Ghattas, Residual-based error correction for neural operator accelerated infinite-dimensional Bayesian inverse problems",
            "year": 2022
        },
        {
            "authors": [
                "S. Mishra",
                "R. Molinaro"
            ],
            "title": "Estimates on the generalization error of physics-informed neural networks for approximating a class of inverse problems for PDEs",
            "venue": "IMA J. Numer. Anal. 42 ",
            "year": 2022
        },
        {
            "authors": [
                "S. Berrone",
                "C. Canuto"
            ],
            "title": "M",
            "venue": "Pintore, Variational physics informed neural networks: the role of quadratures and test functions",
            "year": 2021
        },
        {
            "authors": [
                "J. Roth",
                "M. Schr\u00f6der",
                "T. Wick"
            ],
            "title": "Neural network guided adjoint computations in dual weighted residual error estimation",
            "venue": "SN Appl. Sci. 4 ",
            "year": 2022
        },
        {
            "authors": [
                "P. Minakowski",
                "T. Richter"
            ],
            "title": "A priori and a posteriori error estimates for the Deep Ritz method applied to the Laplace and Stokes problem",
            "venue": "J. Comput. Appl. Math. ",
            "year": 2022
        },
        {
            "authors": [
                "J.L. Lions"
            ],
            "title": "Optimal Control of Systems Governed By Partial Differential Equations",
            "venue": "Springer-Verlag, Berlin",
            "year": 1971
        },
        {
            "authors": [
                "M. Hinze",
                "R. Pinnau",
                "M. Ulbrich",
                "S. Ulbrich"
            ],
            "title": "Optimization with PDE Constraints",
            "venue": "Springer",
            "year": 2009
        },
        {
            "authors": [
                "F. Tr\u00f6ltzsch"
            ],
            "title": "Optimal Control of Partial Differential Equations: Theory",
            "venue": "Methods and Applications, in: Graduate Studies in Mathematics, vol. 112, American Mathematical Society, Providence",
            "year": 2010
        },
        {
            "authors": [
                "A. Borz\u00ec",
                "V. Schulz"
            ],
            "title": "Computational Optimization of Systems Governed By Partial Differential Equations",
            "venue": "in: Siam series on Computational Science and Engineering, Society for Industrial and Applied Mathematics",
            "year": 2012
        },
        {
            "authors": [
                "R. Rannacher",
                "B. Vexler"
            ],
            "title": "A priori error estimates for the finite element discretization of elliptic parameter identification problems with pointwise measurements",
            "venue": "SIAM J. Control Optim. 44 (5) ",
            "year": 2005
        },
        {
            "authors": [
                "P. Petersen",
                "M. Raslan",
                "F. Voigtlaender"
            ],
            "title": "Topological properties of the set of functions generated by neural networks of fixed size",
            "venue": "Found. Comput. Math. 21 (5) ",
            "year": 2021
        },
        {
            "authors": [
                "P. Bochev",
                "M. Gunzburger"
            ],
            "title": "Chapter 12 - least-squares methods for hyperbolic problems",
            "venue": "in: R. Abgrall, C.-W. Shu (Eds.), Handbook of Numerical Methods for Hyperbolic Problems, in: Handbook of Numerical Analysis, vol. 17, Elsevier",
            "year": 2016
        },
        {
            "authors": [
                "D. Yarotsky"
            ],
            "title": "Error bounds for approximations with deep ReLU networks",
            "venue": "Neural Netw. 94 ",
            "year": 2017
        },
        {
            "authors": [
                "I. G\u00fchring",
                "G. Kutyniok"
            ],
            "title": "P",
            "venue": "Petersen, Error bounds for approximations with deep ReLU neural networks in W s,p norms, Anal. Appl. 18 (05) ",
            "year": 2020
        },
        {
            "authors": [
                "S. Berrone",
                "C. Canuto"
            ],
            "title": "M",
            "venue": "Pintore, Solving PDEs by variational physics-informed neural networks: an a posteriori error analysis",
            "year": 2022
        },
        {
            "authors": [
                "D.A. Di Pietro",
                "A. Ern"
            ],
            "title": "Mathematical Aspects of Discontinuous Galerkin Methods",
            "venue": "in: Math\u00e9matiques et Applications, vol. 69, Springer, Berlin",
            "year": 2012
        },
        {
            "authors": [
                "L.A. Vese",
                "C.L. Guyader"
            ],
            "title": "Variational Methods in Image Processing",
            "venue": "in: Mathematical and Computational Imaging Sciences, Chapman and Hall/CRC, Boca Raton",
            "year": 2016
        },
        {
            "authors": [
                "H. Brezis"
            ],
            "title": "Functional Analysis",
            "venue": "Sobolev Spaces and Partial Differential Equations, in: Universitext, Springer, New York",
            "year": 2011
        },
        {
            "authors": [
                "P.G. Ciarlet"
            ],
            "title": "Linear and Nonlinear Functional Analysis with Applications",
            "venue": "SIAM, Philadelphia",
            "year": 2013
        }
    ],
    "sections": [
        {
            "text": "a i n a i\np n r e \u00a9 (\nK Q\na d e\nd\nh 0 o\nThere is tremendous potential in using neural networks to optimize numerical methods. In this paper, we introduce and nalyze a framework for the neural optimization of discrete weak formulations, suitable for finite element methods. The main dea of the framework is to include a neural-network function acting as a control variable in the weak form. Finding the eural control that (quasi-) minimizes a suitable cost (or loss) functional, then yields a numerical approximation with desirable ttributes. In particular, the framework allows in a natural way the incorporation of known data of the exact solution, or the ncorporation of stabilization mechanisms (e.g., to remove spurious oscillations).\nThe main result of our analysis pertains to the well-posedness and convergence of the associated constrained-optimization roblem. In particular, we prove under certain conditions, that the discrete weak forms are stable, and that quasi-minimizing eural controls exist, which converge quasi-optimally. We specialize the analysis results to Galerkin, least squares and minimalesidual formulations, where the neural-network dependence appears in the form of suitable weights. Elementary numerical xperiments support our findings and demonstrate the potential of the framework. 2022 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license http://creativecommons.org/licenses/by/4.0/).\neywords: Optimal neural control; Artificial neural networks; Data-driven discretization; Weighted finite element methods; Quasi-minimization; uasi-optimal convergence\n1. Introduction\nIn recent years there has been tremendous interest in the merging of neural networks and machine-learning lgorithms with traditional methods in scientific computing and computational science [1\u20134]. In this paper we emonstrate how neural networks can be utilized to optimize finite element methods. Let us first provide an lementary motivation as to why finite element methods would benefit from being optimized at all.\nIn one of its most familiar mathematical forms, the finite element method is a discretization technique for partial ifferential equations (PDEs) based on a weak formulation using discrete subspaces, i.e., the exact solution u \u2208 U\nis approximated by uh \u2208 Uh , which is the unique solution of the discrete problem:\nFind uh \u2208 Uh :\n\u2729 Dedicated to J. Tinsley Oden: An unprecedented homo universalis. \u2217 Corresponding author.\nE-mail address: kg.vanderzee@nottingham.ac.uk (K.G. van der Zee).\nttps://doi.org/10.1016/j.cma.2022.115716 045-7825/\u00a9 2022 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons. rg/licenses/by/4.0/).\nw o c f\nfi I i t o\n1\nT m\nf f\nn V i f c t\n1\nb(uh, vh) = f (vh) , \u2200vh \u2208 Vh , (1)\nhere Uh is a discrete subspace of the infinite-dimensional Hilbert or Banach space U (typically a Sobolev space n a domain \u2126 \u2282 Rd ), Vh is a subspace of a Hilbert or Banach space V with dimVh = dimUh , b : U\u00d7V \u2192 R is a ontinuous bilinear form, f : V \u2192 R a continuous linear form, and the exact solution u \u2208 U satisfies b(u, v) = f (v) or all v \u2208 V.1\nIt is well-known that the accuracy of uh can be improved by enlarging Uh (e.g., by refining the underlying nite element mesh).2 However, for a fixed value of h, the particular uh defined by (1) may be very unsatisfactory. n fact, there is no reason why a certain quantity of interest of uh is accurate at all,3 or why the approximation nherits certain qualitative features of the exact solution.4 Indeed, the discrete problem (1) is a rigid statement in he sense that it identifies a single element in Uh , irrespective of desired attributes, whereas there could be many ther elements in Uh that are far superior.\n.1. Neural optimization of discrete weak forms\nThe objective of this work is to propose and analyze a framework for the neural optimization of discrete weak formulations to significantly improve quantitative and qualitative attributes of discrete approximations. In particular, we consider Galerkin, least squares, and minimal-residual formulations.\nThe main idea of the framework is that it incorporates a neural-network function \u03be as a control variable in the discrete test space Vh(\u03be ). That is, the approximation uh = uh,\u03be now depends on \u03be and solves the discrete problem:\nFind uh = uh,\u03be \u2208 Uh : b(uh,\u03be , vh) = f (vh) , \u2200vh \u2208 Vh(\u03be ) . (2)\nhen, in order to obtain a desired approximation uh,\u03be\u0304 , we aim to find a neural-network function \u03be\u0304 that quasiinimizes a desired cost (or loss) functional5:\nJ (uh,\u03be\u0304 )\u2212\u2192 quasi-min . (3)\nThe notion of quasi-minimization is critical when aiming to minimize over a (non-closed) set of neural-network unctions (i.e., the set of functions implemented by neural networks of a fixed architecture); see Section 2.2 for urther details (in particular, Definitions 2.1 and 2.2).\nThe quasi-minimization problem (3) is essentially a nonstandard PDE-constrained optimization, with the onstandard part being the dependence of the state problem (2) on \u03be via the discrete test space Vh(\u03be ). Importantly, h(\u03be ) will be parameterized by \u03be in such a way so as to ensure stability of the discrete problem (2), as well as\nmply existence and convergence of corresponding quasi-minimizers of (3). Moreover, as will become clear in the ollowing sections, the basis functions in Vh(\u03be ) need not be computed explicitly, but equivalent formulations to (2) an be used, which instead incorporate \u03be by means of suitable weight functions. These formulations essentially lead o a PDE-constrained optimization with a nonlinear control-to-state map.\n.2. Potential of the methodology\nThere are two main benefits of having neural control of discrete weak forms:\n\u2022 Incorporation of data: Knowledge of quantities of the exact solution can be taken into account in a natural way by setting, for example,\nJ (uh,\u03be ) = 1 2 \u23d0\u23d0q(uh,\u03be ) \u2212 q\u0304\u23d0\u23d02 , 1 When Uh = Vh , this is a Galerkin method, otherwise it is a Petrov\u2013Galerkin method. 2 Indeed, a priori error analysis reveals that \u2225u \u2212 uh\u2225U \u2264 C infwh\u2208Uh \u2225u \u2212 wh\u2225U, provided b(\u00b7, \u00b7) satisfies a discrete inf\u2013sup condition on\nUh \u00d7 Vh ; see e.g., [5,6]. 3 E.g., the value uh (x0) for some point x0 \u2208 \u2126 is generally quite distinct from u(x0). 4 E.g., uh may exhibit spurious oscillations, while u is monotone. 5 We also allow for the inclusion of a regularization term in the cost functional; see Section 2.1.\ni\nm\nw\nwhere q : U \u2192 R is a functional measuring the quantity of interest and q\u0304 \u2208 R is known data.6 Minimizing such a J (\u00b7) ensures that the discrete solution uh to (2) is data-driven in the sense that uh becomes constrained by the data.7 We note that multiple quantities can be taken into account using, for example,\nJ (uh,\u03be ) = 1\nNdata Ndata\u2211 i=1 1 2 \u23d0\u23d0\u23d0qi (uh,\u03be ) \u2212 q\u0304i \u23d0\u23d0\u23d02 , or, more generally, using some operator Q : U \u2192 Z; see Section 2.\n\u2022 Incorporation of stabilization mechanisms: Qualitative attributes of the discrete solution can be enhanced by minimizing a suitably-chosen J (\u00b7). In this way, discrete solutions can be enforced to, e.g., satisfy an a priori known maximum principle, have monotone (or spurious oscillation free) behavior around discontinuities and layers, or have a certain discrete wave number (i.e., free from pollution). In the past decades, many different stabilized finite element methods have been proposed (and analyzed) that impose such attributes [8\u201313]. Within our framework, such a method is naturally obtained after (quasi-) minimization (i.e., method (2) with \u03be = \u03be\u0304 ). As an example, Guermond [8] advocates the L1-minimization of the residual; in other words, within our framework, one would choose:\nJ (uh,\u03be ) =  f \u2212 Buh,\u03beL1(\u2126) ,\nwhere f \u2212 Buh,\u03be is the strong form of the residual.\nThe idea of using neural networks to parameterize the test space was initially proposed in our earlier work [14], where it was restricted to minimal-residual formulations within a parametric PDE setting. The current work presents significantly more general settings and formulations as well as analyses of their well-posedness and convergence.\nWhile the above shows examples of J (\u00b7) corresponding to unsupervised learning (i.e., there is no need to know the exact solution u), when the original problem is parametric itself (e.g., a parametric PDE), supervised learning becomes meaningful. Indeed, in that case, the data may be the exact solution u\u03bbi for certain parameters \u03bbi ,\n= 1, . . . , Ndata. This then allows for the training of finite element discretizations with superior accuracy in quantities of interest even on very coarse meshes. We refer to our earlier work [14] for the methodology and illustrative examples in that case.\n1.3. Main contributions: Well-posedness, convergent quasi-minimizers, weighted conforming formulations\nLet us briefly outline the main contributions of this work. The first main contribution is the analysis of an abstract constrained-optimization problem associated to (3); see Section 2. In particular, we consider an abstract state problem equivalent to (2), but in the form of a mixed system with a \u03be -dependent bilinear form.8 We prove, under suitable conditions, that the state problem is well-posed (uniformly with respect to \u03be ); see Proposition 2.9. Furthermore, we present differentiability conditions (on the \u03be -dependence) that allow us to prove the existence of quasi-minimizers (within sets of neural-network functions, of some size n) to the associated constrained optimization (3), which converge quasi-optimally (upon n \u2192 \u221e); see Corollary 2.12 for details.\nWe note that our analysis is based on a fundamental result for the quasi-minimization of strongly-convex and differentiable functionals (see Theorem 2.A), which is of independent interest and applies, e.g., to the analysis of deep Ritz methods [17\u201319] and PINN methods [20\u201322].\nThe second main contribution of this work is the application of our framework to certain weak formulations used by conforming finite element methods; see Section 3. In these applications, the neural-network control variable \u03be will appear by means of suitable weights in the bilinear forms. In particular, we will analyze weighted least squares, weighted Galerkin, and weighted minimal-residual formulations.\nFor weighted least squares and weighted minimal-residual formulations, suitable conditions on the weights imply (via the abstract result of the first main contribution) stability of the discrete problem (uniformly in \u03be ). Furthermore,\n6 The data q\u0304 represents q(u), and it could be obtained through experiments, high-fidelity computation, or otherwise. 7 This is somewhat similar in spirit to physics-informed neural networks (PINN) [7], where however a single neural-network function inimizes a combination of the residual and data misfit. 8 The mixed system is motivated by residual-minimization theory [15,16]: Minimal residual formulations are equivalent to mixed systems, hich in turn are equivalent to Petrov\u2013Galerkin formulations.\nsuitable differentiability conditions on the weights imply existence of (quasi-optimally) convergent quasi-minimizers of the associated constrained minimization.\nOn the other hand, for weighted Galerkin, it turns out that stability is not immediate, and may require constraints on \u03be depending on the problem at hand.9 Therefore, neural control is far more convenient for least squares and minimal-residual formulations, the fundamental reason being the inherent stability that comes with their underlying minimization principle.\nWe support our findings with numerical experiments in Section 4. While our theoretical results directly apply to any linear operator, we choose the advection\u2013reaction PDE to illustrate various numerical aspects, viz., the incorporation of data (Section 4.1), the quasi-optimal convergence of quasi-minimizers (Section 4.2), and the incorporation of L1-type stabilization (Section 4.3).\n1.4. Related work\nThere are a number of works related to ours. Optimizing numerical methods: Traditionally, the incorporation of known data or other desired attributes in numerical PDE approximations is achieved via the method of Lagrange multipliers, see e.g., Evans, Hughes & Sangalli [11], Kergrene, Prudhomme, Chamoin & Laforest [23], and references therein. The classical idea of optimizing Petrov\u2013Galerkin methods by using special test spaces was originally motivated by the desire to obtain stable methods for nonsymmetric problems. It has a long history, with early work going back to, e.g., Brooks & Hughes [24], Barrett & Morton [25], and Oden [26], which have given rise to modern stabilized, minimal-residual and DPG methods.\nOptimizing numerical methods using neural networks: Much more recent is the use of neural networks for the optimization of numerical methods, i.e., for the learning of parameters that define a numerical method; see Ray & Hesthaven [27], Mishra [28] and others [29\u201333]. Interestingly, a learning methodology for optimizing the anisotropy of the finite element mesh has been proposed by Fidkowski & Chen [34], while a learning methodology for adaptive mesh refinement that ensures optimal adaptive convergence has been analyzed by Bohn & Feischl [35]. Within the context of optimizing finite-element formulations, a minimal-residual framework that is guaranteed to be stable was proposed in our previous work [14]. Our current work contributes to these developments by providing the analysis of a general framework for neural network optimization of finite element methods.\nNeural networks for PDEs: The use of neural networks for approximating directly the solution to PDEs has received wide-spread interest since the works by E & Yu [36], Sirignano & Spiliopoulos [37], Berg & Nystro\u0308m [38] and Raissi, Perdikaris & Karniadakis [7], amongst others. Recently, there have been a number of ideas that propose an adaptive construction of neural-network approximations; see Ainsworth & Dong [39], Liu, Cai & Chen [40] and Uriarte, Pardo & Omella [41]. Neural networks can also be used to obtain the coefficients of the basis expansion used by a standard (linear) approximation [42,43].\nNeural networks for inverse PDEs: In the context of inverse problems involving PDEs, the use of neural networks to represent unknown PDE coefficients (fields) and constitutive models has been explored by, e.g., Teichert, Natarajan, Van der Ven & Garikipati [44], Berg & Nystro\u0308m [45], Xu & Darve [46] and You et al. [47]. These works are similar to the current work in the sense that standard (finite element) methods are used to solve the PDE, while a neural network is embedded within the discrete formulation. We note that the analysis provided by our current work can be extended to those inverse problems. Other works involve the use of a neural network to approximate the parameter-to-solution map (so-called neural operators); see e.g., [41,48,49] and references therein. These approximations are particularly useful for large-scale problems for which model reduction is essential.\nError analysis for neural-network approximations: There are a number of works containing a priori error analysis for neural-network based PDE approximations. For those related to the deep Ritz method; see Xu [17, Section 5], Pousin [18, Section 3], and Mu\u0308ller & Zeinhofer [19]. For those related to physics-informed neural networks (PINN) and least squares methods; see Sirignano & Spiliopoulos [37, Section 7], Mishra & Molinaro [21,50], Pousin [18, Section 4], Cai, Chen & Liu [22], and Berrone, Canuto & Pintore [51]. Recently, a posteriori error analysis has also been studied, in particular goal-oriented analysis using the dual-weighted residual (DWR) methodology; see, e.g., Roth, Schro\u0308der & Wick [52], and Minakowski & Richter [53]. We note that in our current work, while\n9 In essence, the reason for instability relates to a discrete inf\u2013sup condition of a weighted bilinear form.\na o p a t\n2\na w\nw\nW n\nf\nwe have in mind the error analysis for neural-control approximations, the abstract analysis presented in Section 2 is essentially an extension of the above-mentioned a priori analysis to a certain class of problems involving a convex and differentiable cost functional.\n2. Abstract framework\nIn this section we present the analysis of the abstract state equation (in the form of a mixed system) and the ssociated optimization problem. We essentially follow the classical theory of optimal control (PDE-constrained ptimization) by Lions [54]; see also, [55\u201357]. Our resulting optimization problem bears similarity to that of arameter identification of PDE coefficients; see Rannacher & Vexler [58] and references therein for its error nalysis. While we present our abstract framework within Hilbert spaces (and using a quadratic cost), we note hat extensions to Banach spaces are feasible, but not within the scope of the current work.\n.1. Discrete state problem and associated cost functional\nLet X be a Hilbert space for the control variable. We shall think of a control variable \u03be \u2208 X as being a function in an infinite-dimensional function space X (for example, X = L2(\u2126 ) ).10 Let U and V be Hilbert spaces for trial nd test functions, respectively, Uh \u2282 U be a discrete (finite element) subspace, and V\u0302 \u2286 V.11 In all that follows, e think of h (hence Uh) as being fixed. Given \u03be \u2208 X and f \u2208 V\u2217 (the dual of V), we consider the discrete state problem given by:\u23a7\u23aa\u23a8\u23aa\u23a9 Find (r, uh) \u2208 V\u0302 \u00d7 Uh :\na(\u03be ; r, v) + b(uh, v) = f (v), \u2200v \u2208 V\u0302, (a) b(wh, r ) = 0, \u2200wh \u2208 Uh , (b)\n(4)\nwhere b(\u00b7, \u00b7) is a continuous bilinear form on U \u00d7 V, and for each \u03be \u2208 X, a(\u03be ; \u00b7, \u00b7) is a continuous bilinear form on V \u00d7 V. To explicitly indicate the dependence of r and uh on \u03be , we use the notation:\n(r\u03be , uh,\u03be ) = solution of (4)(a)\u2013(4)(b) for a given \u03be .\nIn Section 2.4, we demonstrate that (4)(a)\u2013(4)(b) is equivalent to (2) for a particular choice of Vh(\u03be ); see Proposition 2.10. The discrete problem in (4)(a)\u2013(4)(b) is essentially a general formulation, which for a specific choice of a(\u00b7 ; \u00b7, \u00b7) and V\u0302 reduces to a (weighted) Galerkin, least-squares or minimal residual method; see Section 3.\nNext, let Z be a Hilbert space, and let Q : U \u2192 Z be a linear continuous (observation) operator. Then, given an observation zo \u2208 Z and regularization parameter \u03b1 \u2265 0, we consider the cost (or loss) functional J : Uh \u00d7 X \u2192 R defined by:\nJ (wh, \u03be ):= J1(wh) + \u03b1 j2(\u03be ) , (5)\nwhere\nJ1(wh) := 1 2 Q(wh) \u2212 zo2Z , j2(\u03be ) :=\n1 2 \u2225\u03be\u22252X .\nThe associated reduced cost functional j : X \u2192 R is then given by:\nj(\u03be ):= j1(\u03be ) + \u03b1 j2(\u03be ) , (6)\nhere j1 : X \u2192 R is defined by:\nj1(\u03be ) := J1(uh,\u03be ) = 1 2 Q(uh,\u03be ) \u2212 zo2Z , hile ideally we would like to minimize j(\u00b7) over (the infinite-dimensional) X, we proceed by considering\neural-network approximations.\n10 In Section 2.2, we let \u03be be a neural network function. 11 Later on, when considering minimal residual formulations, V\u0302 will be a discrete (finite element) subspace of V, but for the other ormulations V\u0302 = V.\nm\nR\nC t\np\n2.2. Neural quasi-minimization\nTo accommodate neural optimization, we consider the subset Mn \u2282 X consisting of all functions implemented by neural networks of a fixed architecture parameterized by n.12 We shall simply refer to Mn as a set of neural-network functions, and we think of n as a measure of the size of the architecture (e.g., the total number of neurons, or total number of parameters).\nWhen aiming to minimize j(\u00b7), a significant complication is that the set Mn is not a linear subspace and it may not be closed (topologically) in X.13 Hence, even though j(\u00b7) may have an infimum on Mn , there may not be a\ninimizer in Mn . Therefore, one should not aim to completely minimize j(\u00b7), but instead use a relaxed notion of quasi-minimization as used by Shin, Zhang & Karniadakis [20]14 (for which the existence of an infimum implies the existence of a quasi-minimizer):\nDefinition 2.1 (Quasi-Minimizers and Quasi-Minimizing Sequences). Let j : X \u2192 R be a cost functional.\n(i) Let \u03b4n > 0 and Mn \u2282 X be a subset of X (not necessarily closed in X). A function \u03be\u0304n \u2208 Mn is said to be a quasi-minimizer of j(\u00b7) if the following holds true15:\nj(\u03be\u0304n) \u2264 inf \u03ben\u2208Mn\nj(\u03ben) + \u03b4n\n2 . (7)\n(ii) Consider a sequence of subsets (Mn)n\u2208N of X, with N being a strictly-increasing sequence of natural numbers. A sequence (\u03be\u0304n)n , with \u03be\u0304n \u2208 Mn , is said to be a quasi-minimizing sequence if (7) holds true for all n \u2208 N with \u03b4n > 0 such that:\n\u03b4n \u2192 0 as n \u2192 \u221e . \u25a1\nIn summary, the neural optimization problem that we consider is the following:\nDefinition 2.2 (The Quasi-Minimizing Control Problem). The following statements are equivalent. educed quasi-minimizing control problem: For j(\u00b7) given by (6), we aim to quasi-minimize j(\u00b7), i.e., given \u03b4n > 0,\u23a7\u23a8\u23a9 Find \u03be\u0304n \u2208 Mn :\nj(\u03be\u0304n) \u2264 inf \u03ben\u2208Mn\nj(\u03ben) + \u03b4n\n2 .\n(8)\nonstrained quasi-minimizing control problem: For J (\u00b7, \u00b7) given by (5), we aim to quasi-minimize J (uh, \u03be ) subject o (4)(a)\u2013(4)(b), i.e., given \u03b4n > 0,\u23a7\u23a8\u23a9 Find \u03be\u0304n \u2208 Mn :\nJ (uh,\u03be\u0304n , \u03ben) \u2264 inf\u03ben\u2208Mn J (uh,\u03ben , \u03b7n) +\n\u03b4n 2 .\n\u25a1 (9)\nExample 2.3 (Need for Quasi-Minimizers). Let us discuss a simple example illustrating the non-existence of minimizers, hence the need for quasi-minimizers.16\n12 In the terminology of Petersen, Raslan and Voigt [59], the set Mn consists of the realizations of all possible neural networks of some fixed architecture (and some given activation function). While a neural network is identified with the set of weight and bias parameters, its realization is the function implemented by the network.\n13 For example, [59, Theorem 3.1] shows that, under mild conditions on the architecture and activation function, Mn is not a closed subset of L2(\u2126 ) (or, more generally, L p(\u2126 ), with 0 < p < \u221e), unless, e.g., an upper bound is imposed on the weight\narameters [59, Proposition 3.7]. 14 Quasi-minimization can also be thought of as solving the minimization problem up to some optimization accuracy, cf. [19]. 15 Observe that if j(\u00b7) has an infimum on Mn , then immediately a quasi-minimizer exists (in Mn). This is true simply by the definition of the infimum. 16 This is essentially an example of a PINN problem, i.e., minimizing a strong residual and boundary condition in least-squares sense. It is not difficult to construct a similar example for a neural control problem.\ns\nf t a\nL\nN\nf\n2\nw t\nT\nLet x = (x1, x2) \u2208 \u2126 = (0, 1)2 \u2282 R2. Given z \u2208 (0, 1), let \u03c7[z,1] denote the characteristic function of the ubset [z, 1].17 Consider the following cost functional:\nj(\u03be ) = 1 2 \u222b 1 0 \u222b 1 0 ( \u2202\u03be \u2202x2 )2 dx1 dx2 + \u222b 1 0 ( \u03be (x1, 0) \u2212 \u03c7[z,1](x1) )2 dx1\nor \u03be \u2208 X = { \u03b7 \u2208 L2(\u2126 ) \u23d0\u23d0 \u2202\u03b7 \u2202x2 \u2208 L2(\u2126 ) }\n. Minimizing j(\u00b7) over X solves a first-order PDE (constant advection in he direction of the x2-axis) with discontinuous data given by \u03c7[z,1], which is a well-posed problem [60, Section 1 nd 6].\nLet Mn be the set of two-layer neural-network functions \u2126 \u21a6\u2192 R2 \u21a6\u2192 R using two neurons and the Rectified inear Unit ReLU(\u00b7) := max{0, \u00b7} activation function in the hidden layer, i.e.,\nMn = { \u03ben : \u2126 \u2192 R \u23d0\u23d0\u23d0 \u03ben(x) = 2\u2211 i=1 ai ReLU(wi \u00b7 x \u2212 bi ) , ai , bi \u2208 R, wi \u2208 R2 } .\note that an infimizing sequence of j(\u00b7) in Mn is given by:\n\u03bem(x) = \u23a7\u23aa\u23aa\u23a8\u23aa\u23aa\u23a9 0 0 \u2264 x1 < zm := (1 \u2212 1m )z , x1 \u2212 zm z \u2212 zm zm \u2264 x1 < z ,\n1 z \u2264 x1 \u2264 1 ,\nfor m = 1, 2, 3, . . ., but whose limit \u03bem \u2192 \u03be\u0304 in X as m \u2192 \u221e is a discontinuous function (with j(\u03be\u0304 ) = 0). Therefore the infimizer \u03be\u0304 does not exist in Mn \u2282 C(\u2126 ).\nOn the other hand, quasi-minimizers \u03be\u0304n do exist in Mn , in particular, \u03bem as defined above is a quasi-minimizer or m large enough.18 \u25a1\n.3. Analysis of reduced control problem\nWe first proceed with the analysis of the reduced control problem (8). Let the state operators Rh : X \u2192 V\u0302 and Sh : X \u2192 Uh be defined by:\nRh(\u03be ) := rh,\u03be , \u2200\u03be \u2208 X , (10a) Sh(\u03be ) := uh,\u03be , \u2200\u03be \u2208 X , (10b)\nhere rh,\u03be and uh,\u03be are the first and second component, respectively, of the solution to the mixed system (4). Then, he reduced cost j(\u00b7) given in (6) can be written as follows:\nj(\u03be ) = j1(\u03be ) + \u03b1 j2(\u03be ) = J1 ( Sh(\u03be ), \u03be ) + \u03b1 j2(\u03be )\n= 1 2 Q \u25e6 Sh(\u03be ) \u2212 zo2Z + \u03b12 \u2225\u03be\u22252X . (11) Our main result depends on the following fundamental theorem, which is of independent interest:\nheorem 2.A (Differentiable, Strongly-Convex Quasi-Minimization). Let j : X \u2192 R be a cost functional. Assume that j(\u00b7) is Ga\u0302teaux differentiable with derivative j \u2032 : X \u2192 X\u2217 being Lipschitz continuous, i.e., there is a constant L > 0 such that j \u2032(\u03be ) \u2212 j \u2032(\u03b7)X\u2217 \u2264 L\u03be \u2212 \u03b7X , \u2200\u03be, \u03b7 \u2208 X , Furthermore, assume that j(\u00b7) is strongly convex, i.e., there is a constant \u03b3 > 0 such that\u27e8\nj \u2032(\u03be ) \u2212 j \u2032(\u03b7) , \u03be \u2212 \u03b7 \u27e9 X\u2217,X \u2265 \u03b3 \u03be \u2212 \u03b72X , \u2200\u03be, \u03b7 \u2208 X . (12)\n17 That is, \u03c7[z,1](x1) = 1 if x1 \u2208 [z, 1] and = 0 otherwise. 18 Indeed, one can verify by direct calculation that m must be such that 1 (z \u2212 z ) \u2264 \u03b4n , i.e., m \u2265 2 z\u03b4\u22121.\n3 m 2 3 n\nb X\nA\nP\nC T\nP\nR e K a b\nR m s\nR n\nThen, the following hold true:\n(i) j(\u00b7) has a unique minimizer \u03be\u0304 \u2208 X, which satisfies:\nj \u2032(\u03be\u0304 ) = 0 in X\u2217 .\n(ii) For any subset Mn \u2282 X, j(\u00b7) has a quasi-minimizer \u03be\u0304n \u2208 Mn that satisfies (7). (iii) Any quasi-minimizer \u03be\u0304n in Mn satisfies the following quasi-optimal error estimate:\u03be\u0304 \u2212 \u03be\u0304nX \u2264 ( L\u03b3 inf\u03ben\u2208Mn \u03be\u0304 \u2212 \u03ben2X + \u03b4n\u03b3 )1/2 . \u25a1 (13)\nProof. See Appendix A.1. \u25a0\nWe now analyze when our j(\u00b7) satisfies the assumptions of Theorem 2.A.\nTheorem 2.B (Reduced Control Problem: Differentiability & Strong Convexity). Let \u03b1 > 0 and j(\u00b7) = j1(\u00b7)+\u03b1 j2(\u00b7) e as in (11). Let Q \u2208 L(U,Z). Assume Sh : X \u2192 Uh is differentiable, Sh(\u00b7) and S\u2032h(\u00b7) are uniformly bounded on , and S\u2032h(\u00b7) is Lipschitz continuous. Then:\n(i) j1, j2, j : X \u2192 R are Ga\u0302teaux differentiable with j \u20321, j \u20322, j \u2032 : X \u2192 X\u2217 Lipschitz continuous.\ndditionally, assume \u03b1 is sufficiently large. Then:\n(i i) j : X \u2192 R is strongly convex, i.e., there is a constant \u03b3 > 0 such that (12) holds true.19 \u25a1\nroof. See Appendix A.2. \u25a0\norollary 2.4 (Reduced Control Problem: (quasi-)Minimizers & Quasi-Optimality). Under the conditions of heorem 2.B, the statements (i), (ii) and (iii) of Theorem 2.A hold true. \u25a1\nroof. The results of Theorem 2.B are the assumptions of Theorem 2.A. \u25a0\nemark 2.5 (Quasi-Optimal Rates). The first part on the right-hand side of the quasi-optimality result (13) can be stimated in terms of n using results from neural-network approximation theory; see, e.g., Yarotsky [61], Gu\u0308hring, utyniok and Petersen [62], and references therein. Such a result may be useful in finding a proper balance of \u03b4n s n \u2192 \u221e. Alternatively, the choice of \u03b4n may be found through a proper a posteriori estimator, which seems to e an open problem.20 \u25a1\nemark 2.6 (Condition on \u03b1). The proof of Theorem 2.B reveals that the condition that \u03b1 is sufficiently large ay be weakened if j1 has additional structure (e.g., convexity). Indeed, convexity of j1 guarantees that j will be trongly convex, with strongly convexity constant equal to \u03b1 > 0. If the case, there is no need of Lipschitzness of j \u20321 in order to prove only \u03b1 > 0 will be enough. Furthermore, becomes:\n\u2225\u03be\u0304 \u2212 \u03be\u0304n\u2225X <\n( \u03b1 + L1\n\u03b1 inf \u03b7n\u2208Mn \u03be\u0304 \u2212 \u03b7n2X + \u03b4n\u03b1 )1/2 . \u25a1\nemark 2.7 (Physics-Informed Neural Networks (PINN)). Theorem 2.A can be applied to PINN [7] (for neuraletwork approximations to PDEs). Indeed, consider\nj(\u03be ) = 1 2  f \u2212 B\u03be2L, where B : X \u2192 L, and f \u2212 B\u03be is an abstract residual in some abstract Hilbert space L (which may include the PDE residual, initial condition and boundary conditions, as in [21], as well as a data residual, as in [50]). Note\n19 In particular, when \u03b1 > L1, where L1 is the Lipschitz constant of j \u20321(\u00b7), then \u03b3 = \u03b1 \u2212 L1. 20 There are some works on a posteriori error analysis for neural networks approximations; see, e.g., [63].\nw\nb 0\nP\nt\nthat \u03be is an approximation to the PDE solution, and not the underlying trainable parameters of a neural network. If B : X \u2192 L is a linear operator, then the assumptions of Theorem 2.A (Lipschitz continuity and strong convexity) hold true.21 \u25a1\nRemark 2.8 (Deep Ritz Method). Theorem 2.A can also be applied to the Deep Ritz method [36]. Indeed, consider\nj(\u03be ) = 1 2 b(\u03be, \u03be ) \u2212 f (\u03be ),\nhere b : X \u00d7 X \u2192 R is a continuous coercive and symmetric bilinear form, and f \u2208 X\u2217. For such a j(\u00b7), the assumptions of Theorem 2.A (Lipschitz continuity and strong convexity) hold true. \u25a1\n2.4. Analysis of constrained control problem\nWe now proceed with the analysis of the constrained control problem (9). We begin by providing conditions that guarantee the well-posedness of the state problem.\nProposition 2.9 (Stability of the State Problem). For each \u03be \u2208 X, let a(\u03be ; \u00b7, \u00b7) : V \u00d7 V \u2192 R and b : U \u00d7 V \u2192 R e continuous bilinear forms. For Uh \u2282 U and V\u0302 \u2286 V, consider the kernel subspace K\u0302 := {v \u2208 V\u0302 : b(wh, v) = , \u2200wh \u2208 Uh}. Then, the following statements hold true:\n(i) For each \u03be \u2208 X, problem (4) is well-posed (for any f \u2208 V\u2217) if and only if there exist constants \u03b1h \u2261 \u03b1h(\u03be ) > 0 and \u03b2h > 0 such that:22\ninf v1\u2208K\u0302 sup v2\u2208K\u0302\na(\u03be ; v1, v2) \u2225v1\u2225V\u2225v2\u2225V\n\u2265 \u03b1h ,{ v2 \u2208 K\u0302 : a(\u03be ; v1, v2) = 0, \u2200v1 \u2208 K\u0302 } = {0} , \u23ab\u23aa\u23ac\u23aa\u23ad (14a) inf\nwh\u2208Uh sup v\u2208V\u0302 b(wh, v) \u2225wh\u2225U\u2225v\u2225V \u2265 \u03b2h . (14b)\n(ii) If (14) is satisfied, then the following a priori bound holds true for the solution uh \u2208 Uh of problem (4):\n\u2225uh\u2225U \u2264 1 \u03b2h\n( 1 +\n\u2225a(\u03be ; \u00b7, \u00b7)\u2225L(V;V\u2217) \u03b1h\n) \u2225 f \u2225V\u2217 .\n(iii) Furthermore, if a(\u03be, \u00b7, \u00b7) is an equivalent inner-product on V, with associated norm \u2225 \u00b7 \u2225V,\u03be := \u221a\na(\u03be ; \u00b7, \u00b7), i.e., for some C1,\u03be , C2,\u03be > 0,\nC1,\u03be\u2225v\u2225V \u2264 \u2225v\u2225V,\u03be \u2264 C2,\u03be\u2225v\u2225V, \u2200v \u2208 V, (15)\nthen \u03b1h = (C1,\u03be )2 in (14a), and additionally, the following improved a priori bound holds true:\n\u2225uh\u2225U \u2264 C2,\u03be C1,\u03be 1 \u03b2h \u2225 f \u2225V\u2217 . \u25a1 (16)\nroof. See Appendix A.3. \u25a0\nTo establish the equivalence between the mixed system (4) and the Petrov\u2013Galerkin statement (2), let us define he operators A : X \u2192 L(V\u0302; V\u0302\u2217) and B \u2261 Bh \u2208 L(Uh; V\u0302\u2217) by:\nA(\u03be )v\u0302 := a(\u03be ; v\u0302, \u00b7) \u2208 V\u0302\u2217, \u2200\u03be \u2208 X, \u2200v\u0302 \u2208 V\u0302; (17a) Bwh := b(wh, \u00b7) \u2208 V\u0302\u2217, \u2200wh \u2208 Uh . (17b)\n21 To avoid confusion, let us stress that Lipschitz continuity and strong convexity are required with respect to the Hilbert space X, and not with respect to the trainable parameters defining a neural network function \u03ben \u2208 Mn \u2282 X. The same applies for Remark 2.8.\n22 Only when V\u0302 is infinite-dimensional, one needs the extra hypothesis in (14a)2. Whenever a(\u03be, \u00b7, \u00b7) is an equivalent inner product on V, then this condition is actually automatically satisfied. Indeed, zero is the only element in V which is orthogonal to itself.\nP b s\nw\nT\nP\nr a\nb t\nP\nC a 2\nt\nP T\nNote that the state Eqs. (4)(a)\u2013(4)(b) can then be written as follows:\nA(\u03be )r + Buh = f in V\u0302\u2217 , (18a) B\u2217r = 0 in (Uh)\u2217 . (18b)\nroposition 2.10 (Equivalent Petrov\u2013Galerkin Problem). For each \u03be \u2208 X, let a(\u03be ; \u00b7, \u00b7) : V \u00d7 V \u2192 R and : U\u00d7V \u2192 R be continuous bilinear forms. Given V\u0302 \u2286 V and a discrete trial space Uh \u2282 U, let the discrete test\npace be given by Vh(\u03be ) := { v \u2208 V\u0302 \u23d0\u23d0\u23d0 A(\u03be )\u2217v \u2208 BUh} , (19) here A(\u03be ) and B are defined in (17a) and (17b) (respectively). Assume the existence of \u03b1(\u03be ) > 0 such that\ninf v1\u2208V\u0302 sup v2\u2208V\u0302\na(\u03be ; v1, v2) \u2225v1\u2225V\u2225v2\u2225V \u2265 \u03b1(\u03be ) . (20)\nhen the state problem (18) is equivalent to the Petrov\u2013Galerkin problem (2). \u25a1\nroof. See Appendix A.4. \u25a0\nFinally, we now present (differentiability) conditions on \u03be \u21a6\u2192 A(\u03be ) that guarantee the (differentiability) equirements on \u03be \u21a6\u2192 Sh(\u03be ) in Theorem 2.B and Corollary 2.4. Once in place, existence of (quasi)-minimizers nd quasi-optimal convergence follow immediately for the constrained control problem.\nTo anticipate the connection between derivatives A\u2032 and S\u2032h (as well as R \u2032 h), 23 note that a formal differentiation\nof (18) (with r = Rh(\u03be ) and uh = Sh(\u03be )) with respect to \u03be in the direction \u03b7 \u2208 X yields:\nA(\u03be )R\u2032h(\u03be )\u03b7 + BS \u2032 h(\u03be )\u03b7 = \u2212A \u2032(\u03be )\u03b7 Rh(\u03be ) in V\u0302\u2217 ,\nB\u2217 R\u2032h(\u03be )\u03b7 = 0 in (Uh) \u2217 .\nOne may therefore expect that suitable conditions on A(\u00b7) will imply desired conditions on Sh(\u00b7) (and Rh(\u00b7)):\nProposition 2.11 (State Differentiability). Let Rh(\u00b7) and Sh(\u00b7) be the state operators as defined in (10), and let A(\u00b7) e as defined in (17a). Assume the conditions of Proposition 2.9, including the well-posedness conditions (14). Then, he following statements hold true:\n(i) If A(\u00b7) has a Ga\u0302teaux derivative at \u03be \u2208 X in the direction \u03b7 \u2208 X, then Rh(\u00b7) and Sh(\u00b7) have a Ga\u0302teaux derivative at \u03be in the direction \u03b7.\n(ii) If A(\u00b7) is Ga\u0302teaux-differentiable at \u03be , then so are Rh(\u00b7) and Sh(\u00b7). (iii) If A(\u00b7), A\u2032(\u00b7) and \u03b1\u22121h (\u00b7) are uniformly bounded on X, then R\u2032h(\u00b7) and S\u2032h(\u00b7) are also uniformly bounded on\nX. (iv) Additionally, if A\u2032(\u00b7) is Lipschitz continuous, then R\u2032h(\u00b7) and S \u2032 h(\u00b7) are Lipschitz continuous as well. \u25a1\nroof. See Appendix A.5. \u25a0\norollary 2.12 (Constrained Problem: (quasi-)Minimizers & Quasi-Optimality). Let J (wh, \u03be ) = J1(wh) + \u03b1 j2(\u03be ) s in (5) with Q \u2208 L(U;Z). Let the associated j(\u00b7) be as in (11). Under the conditions of Propositions 2.9 and .11, and assuming \u03b1 is sufficiently large, the statements (i), (ii) and (iii) of Theorem 2.A hold true.\nIn other words, the constrained control problem (9) has a quasi-minimizer in Mn that converges quasi-optimally o the unique minimizer in X. \u25a1\nroof. The results of Propositions 2.9 and 2.11, together with \u03b1 sufficiently large, are the assumptions of heorem 2.B, whose results are the assumptions of Theorem 2.A. \u25a0\n23 Recall that the Ga\u0302teaux derivative of, e.g., A at \u03be \u2208 X in the direction \u03b7 \u2208 X is given by A\u2032(\u03be )\u03b7 = limt\u21920 A(\u03be+t\u03b7)\u2212A(\u03be )t , provided the limit exists in L(V\u0302; V\u0302\u2217). If the map \u03b7 \u21a6\u2192 A\u2032(\u03be )\u03b7 is linear and continuous from X to L(V\u0302; V\u0302\u2217), then A is Ga\u0302teaux differentiable at \u03be \u2208 X.\na \u03be a d\nu 0 w\nE\nh\n3. Conforming weak formulations with suitable control\nIn this section, we study various weighted versions of conforming weak formulations, viz., least squares, Galerkin nd minimal-residual formulations, and we illustrate these with PDE examples. The aim is to propose suitable -dependent weighting within the weak forms, in order to be able to prove the assumptions of Propositions 2.9 nd 2.11. By Corollary 2.12, we can then conclude that the corresponding constrained neural-control problem has esired properties (existence of quasi-minimizers and quasi-optimal convergence).\nIn what follows, we often consider a (positive) weight function \u03c9(\u03be ) : \u2126 \u2192 R. E.g., we consider a mapping \u03c9 : L2(\u2126 ) \u2192 L\u221e(\u2126 ), which takes a control \u03be \u2208 X = L2(\u2126 ) and generates a function \u03c9(\u03be ), which is positive a.e. in \u2126 .24 Explicit examples of such mappings are discussed in Remarks 3.6 and 3.7. We shall use the notation \u03d6 (\u03be ) := 1/\u03c9(\u03be ) to indicate the (multiplicative) inverse of \u03c9(\u03be ).\n3.1. Weighted least squares formulations\nLet d \u2208 N and \u2126 \u2282 Rd be an open bounded Lipschitz domain. Let B : HB \u2192 L2(\u2126 ) be a linear differential operator in strong form, where U = HB denotes a general graph space:\nHB := { w \u2208 L2(\u2126 ) \u23d0\u23d0\u23d0 Bw \u2208 L2(\u2126 ) and suitable homogeneous boundary conditions}. Examples of HB are presented below. We assume that HB is a Hilbert space when endowed with the inner product(\nw1, w2 ) HB := ( w1, w2 ) L2(\u2126) + ( Bw1, Bw2 ) L2(\u2126) , \u2200w1, w2 \u2208 HB,\nand that B is boundedly invertible from HB onto V\u2217 := L2(\u2126 ) =: V = V\u0302. Let f \u2208 L2(\u2126 ), and let Uh \u2282 HB be a conforming discrete (finite element) space. Given a mapping \u03c9 : L2(\u2126 ) \u2192 L\u221e(\u2126 ), which generates a positive weight function \u03c9(\u03be ), we aim to find uh \u2261 Sh(\u03be ) \u2208 Uh , which is the solution of the weighted least squares problem:\nuh = arg min wh\u2208Uh 1 2 \u221a\u03c9(\u03be )( f \u2212 Bwh)2 L2(\u2126) .\nThe optimality condition of such a minimizer is given by the weighted least squares (LSQ) method:( \u03c9(\u03be )( f \u2212 Buh), Bwh ) L2(\u2126) = 0, \u2200wh \u2208 Uh . (21)\nIn particular, notice that we can directly identify the test space in (2) as Vh(\u03be ) = \u03c9(\u03be )BUh = { v \u2208 L2(\u2126 ) \u23d0\u23d0 v = \u03c9(\u03be )Bwh for some wh \u2208 Uh} . Example 3.1 (Weighted LSQ for Advection\u2013Reaction). Let \u03b2 \u2208 (L\u221e(\u2126 ))d be an advection field, and let c \u2208 L\u221e(\u2126 ) be a reaction coefficient. The inflow boundary is \u2202\u2126\u2212 := {x \u2208 \u2202\u2126 : \u03b2(x) \u00b7 n(x) < 0} , where n(x) corresponds to the\nnit outward normal. Next, define Bw := \u03b2 \u00b7\u2207w+cw and HB := { w \u2208 L2(\u2126 ) : \u03b2 \u00b7\u2207w+cw \u2208 L2(\u2126 ) and w|\u2202\u2126\u2212 =}\n. Therefore, finding u \u2208 HB such that Bu = f , corresponds to the strong form of the advection\u2013reaction PDE ith homogeneous inflow data, which is well-posed under suitable conditions on \u03b2 and c (see, e.g., [64]). The weighted LSQ method (21) translates into finding uh \u2208 Uh such that\u222b\n\u2126\n\u03c9(\u03be ) ( \u03b2 \u00b7 \u2207uh + cuh )( \u03b2 \u00b7 \u2207wh + cwh ) = \u222b \u2126 \u03c9(\u03be ) f ( \u03b2 \u00b7 \u2207wh + cwh ) , \u2200wh \u2208 Uh . \u25a1\nxample 3.2 (Weighted LSQ for the Strong Laplacian). Set HB := { w \u2208 H 10 (\u2126 ) : \u2206w \u2208 L 2(\u2126 ) } , where Bw := \u2212\u2206w. Finding u \u2208 UB such that Bu = f , corresponds to the strong form of the Poisson equation with omogeneous Dirichlet boundary data. The weighted LSQ problem (21) translates into finding uh \u2208 Uh such that\u222b\n\u2126 \u03c9(\u03be )\u2206uh \u2206wh = \u2212 \u222b \u2126 \u03c9(\u03be ) f \u2206wh , \u2200wh \u2208 Uh . \u25a1\n24 The rationale behind this mapping is that it generates a desired weight function \u03c9(\u03be ) in L\u221e(\u2126 ), while keeping an unconstrained Hilbert setting for the control variable \u03be \u2208 X = L2(\u2126 ).\np w c\nt\nf\ne\nT\nP s\nT\nP\nR P l w c\nR ( k\na f\nw\nRemark 3.3 (Weighted LSQ for Laplacian in Mixed Form). The above can be extended to least squares of mixed roblems, e.g., the Laplacian as a first-order system. Let V := L2(\u2126 ) \u00d7 ( L2(\u2126 ) )d and HB := H 10 (\u2126 ) \u00d7 H (div;\u2126 ), here B : HB \u2192 V is defined by B(u, q\u20d7 ) := (div q\u20d7, q\u20d7 \u2212 \u2207u). Given g \u2208 L2(\u2126 ), the problem B(u, q\u20d7 ) = (g, 0\u20d7 )\norresponds to the Poisson equation in mixed form with homogeneous Dirichlet boundary conditions. Given a pair of conforming discrete subspaces Uh \u2282 HB , a pair of controls \u03be = (\u03be1, \u03be2) \u2208 L2(\u2126 )2 =: X, and wo mappings \u03c91, \u03c92 : L2(\u2126 ) \u2192 L\u221e(\u2126 ), a possible weighted LSQ method is to find (uh, q\u20d7h) \u2208 Uh such that\u222b \u2126 \u03c91(\u03be1) div q\u20d7h div p\u20d7h + \u222b \u2126 \u03c92(\u03be2) (q\u20d7h \u2212 \u2207uh) \u00b7 ( p\u20d7h \u2212 \u2207wh) = \u222b \u2126 \u03c91(\u03be1) f div p\u20d7h , or all (wh, p\u20d7h) \u2208 Uh . \u25a1 To establish the connection with the general mixed system (4), we set r = \u03c9(\u03be )( f \u2212 Buh) so that (21) is quivalent to:( \u03d6 (\u03be ) r, v ) L2(\u2126) + ( Buh, v ) L2(\u2126) = ( f, v)L2(\u2126), \u2200v \u2208 V, (22a)(\nBwh, r )\nL2(\u2126) = 0, \u2200wh \u2208 Uh . (22b)\nhus, in this case, the continuous bilinear forms a(\u03be ; \u00b7, \u00b7) : V\u00d7V \u2192 R and b : HB \u00d7V \u2192 R in (4) are given by a(\u03be ; v1, v2) := ( \u03d6 (\u03be ) v1, v2 ) L2(\u2126), \u2200v1, v2 \u2208 V = L 2(\u2126 ), (23a)\nb(w, v) := (Bw, v)L2(\u2126), \u2200w \u2208 HB, \u2200v \u2208 V. (23b)\nroposition 3.4 (Weighted Least Squares). Let \u03d6 : L2(\u2126 ) \u2192 L\u221e(\u2126 ) be a differentiable mapping, such that for ome positive constants \u03d6min, \u03d6max, \u03d6 \u2032\u221e, and \u03d6L , the application \u03d6 (\u00b7) satisfies\n\u2022 \u03d6min \u2264 \u03d6 (\u03be ) \u2264 \u03d6max, for all \u03be \u2208 L2(\u2126 ); \u2022 \u2225\u03d6 \u2032(\u03be )\u2225L(L2(\u2126);L\u221e(\u2126)) \u2264 \u03d6 \u2032 \u221e\n, for all \u03be \u2208 L2(\u2126 ); \u2022 \u2225\u03d6 \u2032(\u03be1) \u2212 \u03d6 \u2032(\u03be2)\u2225L(L2(\u2126);L\u221e(\u2126)) \u2264 \u03d6L\u2225\u03be1 \u2212 \u03be2\u2225L2(\u2126), for all \u03be1, \u03be2 \u2208 L 2(\u2126 ).\nhen, the following statements hold true:\n(i) The bilinear forms in (23) satisfy the inf \u2212 sup conditions (14), and thus the weighted least squares problem (22) is well-posed.\n(ii) The state operator Sh(\u00b7) (= uh) of the problem (22) is uniformly bounded on X = L2(\u2126 ) and differentiable. (iii) The derivative S\u2032h(\u00b7) is uniformly bounded on X = L2(\u2126 ) and Lipschitz continuous. \u25a1\nroof. See Appendix A.6 \u25a0\nemark 3.5 (Neural Control of Weighted Least Squares). Proposition 3.4 guarantees that the conditions of ropositions 2.9 and 2.11 are satisfied, hence Corollary 2.12 applies to the neural optimization of the above weighted\neast squares formulation. In particular, this means that it can be applied to the PDEs in Examples 3.1 and 3.2 (and ith minor modifications also to the setting in Remark 3.3), provided the weight \u03d6 (\u03be ) satisfies the three nontrivial onditions in Proposition 3.4. The next two remarks discuss this in further detail.\nemark 3.6 (Suitable Weight Functions: Integral Operators). A general way to obtain a weight function \u03d6 (\u03be ) = 1/\u03c9(\u03be )) is when the mapping \u03d6 : L2(\u2126 ) \u2192 L\u221e(\u2126 ) is an integral operator. Indeed, given a kernel function : \u2126 \u00d7 \u2126 \u2192 R, and differentiable real functions f, g : R \u2192 R, we can define\n[\u03d6 (\u03be )](\u00b7) = f (\u222b\n\u2126\nk(\u00b7, y) g ( \u03be (y) ) dy ) , \u2200\u03be \u2208 L2(\u2126 ). (24)\nThere are several options to obtain a well-defined expression (24). For instance, we can ask f to be bounded nd k(x, \u00b7)g(\u03be (\u00b7)) \u2208 L1(\u2126 ), for all x \u2208 \u2126 . Moreover, if we want to obtain Ga\u0302teaux differentiability of \u03d6 , then the ollowing expression has to be well-defined for any \u03be, \u03b7 \u2208 L2(\u2126 ):[\n\u03d6 \u2032(\u03be ) ] \u03b7 = f \u2032 (\u222b \u2126 k(\u00b7, y) g ( \u03be (y) ) dy )\u222b \u2126 k(\u00b7, y) g\u2032(\u03be (y)) \u03b7(y) dy,\nhich additionally requires k(x, \u00b7) g\u2032 ( \u03be (\u00b7) ) \u2208 L2(\u2126 ), for each x \u2208 \u2126 .\nb\nw t\nf\nR w {\nI\nw\nb p\n3\ns\nLet us describe a fundamental example for \u03d6 (\u03be ) that satisfies the assumptions in Proposition 3.4. Consider a positive constant M > 0 and the sigmoid function \u03c3 (s) = 1/(1 + e\u2212s). Let \u03c7Br be the characteristic function of a\nall Br \u2282 \u2126 of radius r > 0 centered at the origin. Then define [\u03d6 (\u03be )](x) = 1 + M\u03c3 (\n1 |Br | \u222b \u2126 \u03c7Br (y \u2212 x) \u03be (y) dy ) , (25)\nhich has the form of expression (24) for f (s) = 1 + M\u03c3 (s), g(s) = s, and k(x, y) = 1 |Br | \u03c7Br (y \u2212 x). To verify he assumptions in Proposition 3.4, first observe that 1 \u2264 \u03d6 (\u03be ) \u2264 1 + M , for all \u03be \u2208 L2(\u2126 ). Second,[\u03d6 \u2032(\u03be )]\u03b7L\u221e(\u2126) \u2264 M4 |Br |\u2212 12 \u2225\u03b7\u2225L2(\u2126) , \u2200\u03be, \u03b7 \u2208 L2(\u2126 ),\nand thus, \u2225\u03d6 (\u03be )\u2032\u2225L(L2(\u2126);L\u221e(\u2126)) \u2264 M 4 |Br | \u2212 1 2 . And third, denoting the Lipschitz constant of \u03c3 \u2032(\u00b7) by L\u03c3 \u2032 > 0,25 we get \u03d6 \u2032(\u03be1)\u03b7 \u2212 \u03d6 \u2032(\u03be2)\u03b7L\u221e(\u2126) \u2264 M |Br |\u22121L\u03c3 \u2032\u2225\u03be1 \u2212 \u03be2\u2225L2(\u2126)\u2225\u03b7\u2225L2(\u2126) , or all \u03be1, \u03be2, \u03b7 \u2208 L2(\u2126 ), which implies that \u03d6 \u2032(\u00b7) is Lipschitz continuous. \u25a1\nemark 3.7 (Practical Weight Functions: Approximation of Integral Operator). For simplicity, let us consider (25) hen \u2126 \u2282 R. One can approximate the integral in (25) by (Gaussian) quadrature using points and weights\n(xq , wq )}, with xq \u2208 (\u22121, 1), for q = 1, 2, . . . , N . We then have [\u03d6 (\u03be )](x) = 1 + M\u03c3 (\n1 2r \u222b \u2126 \u03c7(\u2212r,r )(y \u2212 x) \u03be (y) dy )\n(26)\n\u2248 [\u03d6N (\u03be )](x) := 1 + M\u03c3 (\n1 2 N\u2211 q=1 wq \u03be (x + r xq ) ) . (27)\nn particular, for a single quadrature point (mid-point rule), we obtain the approximation\n[\u03d6 (\u03be )](x) \u2248 [\u03d61(\u03be )](x) = 1 + M\u03c3 (\u03be (x)) , (28)\nhich is just a composition of functions, (1 + M\u03c3 ) \u25e6 \u03be (\u00b7), hence attractive in practical implementations. We note however that \u03d61(\u00b7) does not satisfy the assumptions of Proposition 3.4,26 hence Corollary 2.12 cannot\ne applied. Nevertheless, numerical experiments in Section 4 indicate that the use of \u03d61(\u03be ) does not deteriorate erformance. Therefore, we expect the result of Corollary 2.12 to be valid for a larger class of mappings \u03d6 (\u00b7). \u25a1\n.2. Weighted Galerkin formulations\nConsider a Hilbert space U = V on \u2126 \u2282 Rd and a continuous bilinear form b : V \u00d7 V \u2192 R satisfying (for ome constant \u03b2 > 0) the following conditions\nsup v\u2208V b(w, v) \u2225v\u2225V\n\u2265 \u03b2\u2225w\u2225V , \u2200w \u2208 V , (29a){ v \u2208 V : b(w, v) = 0, \u2200w \u2208 V } = {0}. (29b)\nGiven f \u2208 V\u2217, Babus\u030cka\u2013Brezzi theory (see, e.g., [6]) ensures the existence of a unique u \u2208 V such that\nb(u, v) = f (v) , \u2200v \u2208 V . (30)\nNow, given a mapping \u03c9 : X \u2192 W (requirements on the space W are clarified below), a control \u03be \u2208 X, and a conforming discrete subspace Uh \u2282 V, consider the following weighted Galerkin discretization of (30):{\nFind uh \u2261 Sh(\u03be ) \u2208 Uh : b ( uh, \u03c9(\u03be )wh ) = f ( \u03c9(\u03be )wh ) , \u2200wh \u2208 Uh .\n(31)\n25 Indeed, we can use the uniform bound of \u03c3 \u2032\u2032(\u00b7). 26 Indeed, one can verify that \u03d6 \u20321(\u03be )(\u03b7) = ( 1+ M\u03c3 \u2032(\u03be ) ) \u03b7, which is in general not in L\u221e(\u2126 ) for \u03be, \u03b7 \u2208 L2(\u2126 ). Thus, the second assumption\nin Proposition 3.4 is violated.\nE f t\n\u03c9\no\ni\nR\nw i\nd\nNotice that one can directly connect (31) to the form stated in (2) by identifying Vh(\u03be ) = \u03c9(\u03be )Uh = { vh \u2208 V \u23d0\u23d0\u23d0 vh = \u03c9(\u03be )wh for some wh \u2208 Uh} . xample 3.8 (Weighted Galerkin for Laplacian). Let V = H 10 (\u2126 ) and f \u2208 V\u2217 = H\u22121(\u2126 ). Consider a weight\nunction \u03c9(\u03be ), where \u03c9 : L2(\u2126 ) \u2192 W 1,\u221e(\u2126 ) (hence W = W 1,\u221e(\u2126 )). Then, the weighted Galerkin formulation for he Poisson equation with homogeneous Dirichlet boundary conditions is to find uh \u2208 Uh \u2282 H 10 (\u2126 ) such that\u222b\n\u2126\n\u2207uh \u00b7 ( vh\u2207\u03c9(\u03be ) + \u03c9(\u03be )\u2207vh ) = \u27e8 f , \u03c9(\u03be )vh \u27e9 , \u2200vh \u2208 Uh . \u25a1 (32)\nThe above example illustrates that W should be such that the product \u03c9(\u03be ) vh is well-defined in V, whenever (\u03be ) \u2208 W and vh \u2208 Uh \u2282 V. In fact, we shall assume that W is such that, for any w \u2208 W, the multiplication perator Mw : V \u2192 V, defined by\nMwv := wv, \u2200v \u2208 V ,\ns a linear and continuous map.\nemark 3.9 (Multiplication in H 1). Let V = H 1(\u2126 ) and W = W 1,\u221e(\u2126 ). Then, it is easy to see that Mw : H 1(\u2126 ) \u2192 H 1(\u2126 ) is a linear and continuous map, for all w \u2208 W 1,\u221e(\u2126 ).27 \u25a1\nTo establish the equivalence with the mixed formulation (4) (and thereby fit the weighted Galerkin formulation ithin the abstract setting of Section 2), we furthermore let W \u2261 W(\u2126 ) consist of measurable functions on \u2126 , and\nntroduce a particular subset of interest: W+ := { w \u2208 W \u23d0\u23d0\u23d0 \u2203wmin > 0 for which wmin \u2264 w(x) \u2264 1wmin , \u2200x \u2208 \u2126} \u2282 W.\nNotice that 1w \u2208 W+ iff w \u2208 W+. We can then define the inverse of multiplication M \u22121 w := M 1w , which is justified by the fact that\nM\u22121w (Mwv) = v = Mw(M \u22121 w v) , \u2200v \u2208 V. (33)\nThe adjoint operators of Mw and M\u22121w will be denoted by M \u2217 w and M \u2212\u2217 w respectively. Using the relations (33), it is straightforward to see that the adjoint operators satisfy\nM\u2212\u2217w (M \u2217 w\u2113) = \u2113 = M \u2217 w(M \u2212\u2217 w \u2113) , \u2200\u2113 \u2208 V \u2217. (34)\nNext, we translate problem (31) into operator notation by means of the operator B \u2208 L(V;V\u2217) such that V \u220b w \u21a6\u2192 Bw := b(w, \u00b7) \u2208 V\u2217. Notice that such an operator is invertible thanks to conditions (29). Problem (31) translates into finding uh \u2261 Sh(\u03be ) \u2208 Uh such that\u27e8\nBuh, M\u22121\u03d6 (\u03be )vh \u27e9 = \u27e8 f, M\u22121\u03d6 (\u03be )vh \u27e9 , \u2200vh \u2208 Uh .\nHence, by means of the adjoint relation we get\u27e8 M\u2212\u2217\u03d6 (\u03be )( f \u2212 Buh), vh \u27e9 = 0 , \u2200vh \u2208 Uh . (35)\nSince B is invertible, so is B\u2217 : V \u2192 V\u2217 defined by V \u220b v \u21a6\u2192 b(\u00b7, v) \u2208 V\u2217. Therefore, there exists a unique r \u2208 V such that B\u2217r = M\u2212\u2217\u03d6 (\u03be )( f \u2212 Buh) in V\u2217. Thus, multiplying this last equation by M\u2217\u03d6 (\u03be ), using (34), (35), and the definition of r \u2208 V, we arrive to the mixed form{\u27e8\nB\u2217r, M\u03d6 (\u03be )v \u27e9 + b(uh, v) = f (v), \u2200v \u2208 V, (a)\nb(vh, r ) = 0, \u2200vh \u2208 Uh . (b) (36)\n27 Indeed, \u2225wv\u2225H1(\u2126) \u2264 C\u2225w\u2225W 1,\u221e(\u2126) \u2225v\u2225H1(\u2126). It is also true for Hilbert spaces V \u2282 L2(\u2126 ) containing at most first-order (weak) erivatives in L2(\u2126 ) (e.g., first-order graph spaces).\nT\nObserve that (36) has the structure of (4) for V\u0302 := V = U, and a(\u03be ; r, v) := \u27e8 B\u2217r, M\u03d6 (\u03be )v \u27e9 = b ( \u03d6 (\u03be )v, r ) .\nThe next proposition establishes the well-posedness of the weighted Galerkin formulation (31).\nProposition 3.10 (Weighted Galerkin Formulation). Let \u03c9 : X \u2192 W. Assume that for some positive function \u03b1h : X \u2192 R, the continuous bilinear form b : V \u00d7 V \u2192 R satisfies the discrete inf \u2212 sup condition\nsup vh\u2208Uh b(wh, \u03c9(\u03be )vh) \u2225vh\u2225V \u2265 \u03b1h(\u03be )\u2225wh\u2225V , \u2200wh \u2208 Uh , \u2200\u03be \u2208 X . (37)\nhen, the following statements hold true:\n(i) For any f \u2208 V\u2217 and \u03be \u2208 X, problem (31) is well-posed. (ii) If there exist uniform constants \u03b1 > 0 and \u03c9\u221e > 0 such that \u03b1h(\u03be ) \u2265 \u03b1 and \u2225\u03c9(\u03be )\u2225W \u2264 \u03c9\u221e for all \u03be \u2208 X,\nthen the solution uh \u2261 Sh(\u00b7) to problem (31) is uniformly bounded on X. (iii) Additionally, if \u03c9(\u00b7) is differentiable, then Sh(\u00b7) is also differentiable. Moreover, if \u03c9\u2032(\u00b7) is uniformly bounded\nand Lipschitz-continuous, then S\u2032h(\u00b7) is also uniformly bounded and Lipschitz-continuous. \u25a1\nProof. See Appendix A.7. \u25a0\nRemark 3.11 (Neural Control of Weighted Galerkin). Proposition 3.10 guarantees that the conditions of Propositions 2.9 and 2.11 are satisfied, hence Corollary 2.12 applies to the neural optimization of the above weighted Galerkin formulation. \u25a1\nRemark 3.12 (Inconvenient Condition for Weighted Galerkin). While for the weighted least squares method the conditions on the weight are explicit (recall Proposition 3.4), for weighted Galerkin the condition (37) is problem-dependent. This is even true when (37) is replaced by the stronger condition of coercivity:\nb ( vh, \u03c9(\u03be )vh ) \u2265 \u03b1h(\u03be )\u2225vh\u22252V , \u2200vh \u2208 Uh . (38)\nA more detailed analysis of when (37) or (38) is satisfied in general requires further study and is outside of the scope of this work. Indeed, for the examples in Remarks 3.13 and 3.14, satisfying (38) may require inconvenient constraints on \u03be . It is therefore much more convenient to have neural control of least squares formulations. When the continuous setting of the PDE at hand does not fit a least squares formulation (as in the examples in Remarks 3.13 and 3.14), instead of weighted Galerkin, we then recommend the use of (weighted) dual minimal-residual formulations; see Section 3.3. \u25a1\nRemark 3.13 (Weighted Galerkin for Laplacian: Nontrivial Stability). Let us illustrate the difficulty of satisfying coercivity (38) with an elementary example for the Laplacian. Recall Example 3.8, and consider a 1-D setting, taking \u2126 = (0, 1). Assume a Neumann boundary condition at x = 0 and a Dirichlet condition at x = 1. In that case\nV = H 10)(\u2126 ) := { v \u2208 H 1(\u2126 ) | v(1) = 0 } ,\nb(u, wv) = \u222b 1 0 u\u2032 ( v w\u2032 + w v\u2032 ) dx . (39)\nAssume Uh \u2282 V is a standard linear finite element space. Take the weight function as w(x) = cx for any c > 0. Note that w(x) > 0 for all x \u2208 (0, 1), hence this weight seems harmless. However, for any vh \u2208 Uh \u2282 V,\nb(vh, wvh) = c \u222b 1\n0 x (v\u2032h)\n2 dx + c \u222b 1\n0 vh v\n\u2032 h dx\n= c (\u222b 1\nx(v\u2032h) 2 dx \u2212 1 vh(0)2 ) (vh(1) = 0)\n0 2\nT\nf\no\nb { a\nSurprisingly, the right-hand side vanishes for, for example, the left-most half-hat function:\nvh(x) = { 1 \u2212 xh , x \u2208 (0, h), with (h \u2264 1) , 0 , otherwise .\nThis shows that coercivity (38) can not be satisfied in general without additional conditions on w.28\nOn the other hand, a condition on w can be found that ensures coercivity. We consider the general case for the Laplacian, starting from the bilinear form in (32). First notice that, for any w \u2208 W 1,\u221e(\u2126 ) such that w(x) \u2265 wmin for all x \u2208 \u2126 ,\nb(v, wv) \u2265 wmin\u2225\u2207v\u22252L2(\u2126) \u2212 \u2225\u2207w\u2225L\u221e(\u2126)\u2225v\u2225L2(\u2126)\u2225\u2207v\u2225L2(\u2126) \u2265 ( wmin \u2212 C\u2126\u2225\u2207w\u2225L\u221e(\u2126) ) \u2225\u2207v\u22252L2(\u2126) ,\nwhere a Poincare\u0301 inequality was used. Therefore, the constraint C\u2126\u2225\u2207w\u2225L2(\u2126) < wmin is sufficient to guarantee (38) (and thereby (37)). Unfortunately, since w = \u03c9(\u03be ), such a condition translates into a constraint on \u2207\u03be , which may be very inconvenient to impose in practice. \u25a1\nRemark 3.14 (Weighted Galerkin for Advection: Surprising Stability). While weighted Galerkin may destabilize the standard Galerkin method, as illustrated in Remark 3.13 for the Laplacian, for other PDEs, the addition of a weight may also stabilize an otherwise unstable method. In both situations, stability does require a nontrivial condition on the weight function.\nLet us illustrate the stabilizing effect of weighted Galerkin for advection in 1-D for simplicity, i.e., the differential equation u\u2032 = f in \u2126 = (0, 1), and u(0) = 0, and the following weak form:\nFind uh \u2208 Uh : b(uh, wvh) := \u2212 \u222b \u2126 uh (w vh)\u2032 = \u27e8 f , w vh \u27e9 \u2200vh \u2208 Uh ,\nwhere Uh \u2282 H 10)(0, 1) := { v \u2208 H 1(0, 1) : v(1) = 0 } and f is allowed to be rough, i.e., in [H 10)(0, 1)]\n\u2217.29 Note that standard Galerkin has w(x) = 1 for x \u2208 \u2126 , and fails to be coercive on H 10)(0, 1).\nFor weighted Galerkin, the left-hand side of (38) becomes in this case: b(vh, wvh) = \u2212 \u222b \u2126 w\u2032 v2h \u2212 \u222b \u2126 vh w v\u2032h = \u2212 \u222b \u2126 w\u2032 v2h + \u222b \u2126 vh (w vh)\u2032 + w(0) v2h(0)\nherefore,\nb(vh, wvh) = \u2212 1 2 \u222b \u2126 w\u2032 v2h + 1 2 w(0)v2h(0) ,\nwhich motivates to assume a (global) inverse inequality30:\n\u2225vh\u2225 2 L2(\u2126) \u2265 Cinvh 2 \u2225v\u2032h\u2225 2 L2(\u2126) \u2200vh \u2208 Uh ,\nand the following conditions on w:\nw\u2032(x) \u2264 \u2212 2C\u0302 h2 < 0 , \u2200x \u2208 \u2126 , and w(0) \u2265 0 , (40)\nor some C\u0302 > 0. Indeed, we then have coercivity (38) (and thereby (37)):\nb(vh, wvh) \u2265 C\u0302 h2 \u222b \u2126 v2h \u2265 CinvC\u0302 \u222b \u2126 (v\u2032h) 2 .\nThe conclusion of Remark 3.13 applies here as well: Since w = \u03c9(\u03be ), condition (40) translates into a constraint n derivatives of \u03be , which may be very inconvenient to impose in practice. \u25a1 28 We note that this counterexample to coercivity also works for strictly positive weights on [0, 1], in the case of, for example, quadratic finite elements: Let wmin > 0, and consider w(x) = wmin + cx , with c > 0 to be specified. Taking vh (x) = 12 (x \u2212 1) 2 yields\n(vh , wvh ) = wmin \u222b 1 0 (v \u2032 h ) 2 dx + c (\u222b 1 0 x (v \u2032 h ) 2 dx \u2212 12 vh (0) 2) = 1 24 (8wmin \u2212 c), which is zero for c = 1 8 wmin.\n29 A similar analysis also applies to the stronger setting having bilinear form b(uh , w vh ) = \u222b \u2126 u \u2032 h w vh , f \u2208 L 2(\u2126 ), and Uh \u2282 H10)(0, 1) :=\nw \u2208 H1(0, 1) : w(0) = 0 } . 30 This holds for example when Uh is a quasi-uniform FE space. If quasi-uniformity does not hold, one can extend the analysis by ssuming element-wise inverse inequalities.\nt i\nU\nT r\nE a f\nw t\n\u03c9 (\nE a t\nb\n3.3. Weighted dual minimal residual formulations\nIn this section, we consider weighted minimal residual (MinRes) formulations. These are particularly useful if he continuous setting of the PDE does not fit a weighted least squares formulation, that is, when the residual is not n L2(\u2126 ). As examples, we consider the Laplacian in H 10 (\u2126 ), and weak advection\u2013reaction with solution in L\n2(\u2126 ). Let Uh \u2282 U and Vh \u2282 V be discrete subspaces, and assume:\u23a7\u23aa\u23a8\u23aa\u23a9 dim(Vh) > dim(Uh), (a)\n\u2203 \u03b2h > 0 : inf wh\u2208Uh sup vh\u2208Vh b(wh, vh) \u2225wh\u2225U\u2225vh\u2225V \u2265 \u03b2h . (b) (41)\nFor each \u03be \u2208 X, we consider an equivalent (weighted) inner product (\u00b7, \u00b7)V,\u03be on V, i.e., such that its induced norm\nV \u220b v \u21a6\u2192 \u2225v\u2225V,\u03be := \u221a (v, v)V,\u03be\nsatisfies (15). The minimal-residual method that we consider is then: Given \u03be \u2208 X, find rh \u2208 Vh and uh \u2261 Sh(\u03be ) \u2208 h such that{(\nrh, vh ) V,\u03be + b(uh, vh) = f (vh) , \u2200vh \u2208 Vh , (a)\nb(wh, rh) = 0 , \u2200wh \u2208 Uh . (b) (42)\nhis has the structure of (4) for V\u0302 := Vh and a(\u03be ; r, v) := (r, v)V,\u03be . Because (\u00b7, \u00b7)V,\u03be and \u2225 \u00b7 \u2225Vh,\u03be depend on \u03be , we efer to the above as a weighted discrete-dual MinRes formulation.\nxample 3.15 (Weighted Minres for Weak Advection\u2013Reaction). Recall the advection\u2013reaction PDE \u03b2 \u00b7\u2207u+cu = f nd inflow boundary condition u|\u2202\u2126\u2212 = 0 of Example 3.1. Under suitable conditions on \u03b2 and c, this admits the ollowing well-posed weak formulation (see, e.g., [64,65]):\nFind u \u2208 U := L2(\u2126 ) : \u222b \u2126 ( \u2212u div(\u03b2 v) + c u v ) = \u27e8 f , v \u27e9 , \u2200v \u2208 V ,\nhere V := {v \u2208 L2(\u2126 ) : \u03b2 \u00b7 \u2207v \u2208 L2(\u2126 ) and v|\u2202\u2126+ = 0}, endowed with the norm \u2225\u03b2 \u00b7 \u2207(\u00b7)\u2225L2(\u2126), f \u2208 V\u2217, and he outflow boundary is defined by\n\u2202\u2126+ := { x \u2208 \u2202\u2126 : \u03b2(x) \u00b7 n(x) > 0 } .\nConsider now a discrete trial/test pairing Uh/Vh satisfying (41), and a mapping \u03c9 : L2(\u2126 ) \u2192 L\u221e(\u2126 ) such that (\u03be ) is a positive weight function for all \u03be \u2208 L2(\u2126 ). A weighted discrete-dual MinRes formulation is then to find\nrh, uh) \u2208 Vh \u00d7 Uh such that:\u222b \u2126 \u03c9(\u03be )(\u03b2 \u00b7 \u2207rh)(\u03b2 \u00b7 \u2207vh) + \u222b \u2126 ( \u2212uh div(\u03b2vh) + c u v ) = \u27e8 f , vh \u27e9 , \u2200vh \u2208 Vh ,\n\u2212 \u222b \u2126 wh div(\u03b2rh) = 0 , \u2200wh \u2208 Uh . \u25a1\nxample 3.16 (Weighted Minres for Poisson Equation). Consider f \u2208 H\u22121(\u2126 ), a mapping \u03c9 : L2(\u2126 ) \u2192 L\u221e(\u2126 ) s in Example 3.15, and discrete subspaces Uh \u2282 Vh \u2282 H 10 (\u2126 ). A weighted discrete-dual MinRes formulation for he Poisson equation with homogeneous Dirichlet boundary conditions is to find uh \u2208 Uh and rh \u2208 Vh such that\u222b\n\u2126 \u03c9(\u03be )\u2207rh \u00b7 \u2207vh + \u222b \u2126 \u2207uh \u00b7 \u2207vh = \u27e8 f , vh \u27e9 , \u2200vh \u2208 Vh ,\u222b\n\u2126\n\u2207wh \u00b7 \u2207rh = 0 , \u2200wh \u2208 Uh . \u25a1\nAs shown in [16, Theorem 4.1], the mixed formulation (42) is equivalent to minimizing the residual as measured y a discrete-dual norm:\nuh = arg min wh\u2208Uh\n( sup\nvh\u2208Vh | f (vh) \u2212 b(wh, vh)| \u2225vh\u2225Vh ,\u03be\n) . (43)\nP\nR P m 3 s a T\nR h\nT\nO\nH\nT d b\no\nA i i\nProposition 3.17 (Weighted MinRes). Let the continuous bilinear form b : U \u00d7 V \u2192 R and the pairing (Uh,Vh) satisfy (41). Consider a parameterized set of equivalent inner-products { (\u00b7, \u00b7)V,\u03be : \u03be \u2208 X } , whose induced norms \u2225 \u00b7 \u2225V,\u03be satisfy (15) for some equivalence constants C1,\u03be > 0 and C2,\u03be > 0. Let A : X \u2192 L(V;V\u2217) be defined by A(\u03be )v := (v, \u00b7)V,\u03be \u2208 V\u2217, for all \u03be \u2208 X and v \u2208 V. Then, the following statements hold true:\n(i) The mixed discrete formulation (42) is well-posed. (ii) If there exist uniform constants C\u03031 > 0 and C\u03032 > 0 such that C1,\u03be \u2265 C\u03031 and C2,\u03be \u2264 C\u03032 for all \u03be \u2208 X, then\nthe solution uh \u2261 Sh(\u00b7) to problems (42) and (43) is uniformly bounded on X. (iii) Additionally, if A(\u00b7) is differentiable, then Sh(\u00b7) is also differentiable. Moreover, if A\u2032(\u00b7) is uniformly bounded\nand Lipschitz-continuous, then also S\u2032h(\u00b7) is uniformly bounded and Lipschitz continuous. \u25a1\nroof. See Appendix A.8. \u25a0\nemark 3.18 (Neural Control of Weighted MinRes). Proposition 3.17 guarantees that the conditions of ropositions 2.9 and 2.11 are satisfied, hence Corollary 2.12 applies to the neural optimization of the above weighted inimal-residual formulation. In particular, this means that it can be applied to the PDEs in Examples 3.15 and .16, provided the weight \u03c9(\u03be ) is such that the induced norm \u2225 \u00b7 \u2225V,\u03be and operator A(\u03be ) (defined in Proposition 3.17) atisfy the stated nontrivial conditions. It turns out that these conditions hold true when \u03c9(\u00b7) satisfies the same three ssumptions as for weighted least squares; recall Proposition 3.4 (and the subsequent Remarks 3.5, 3.6 and 3.7). he next remark demonstrates this in further detail. \u25a1\nemark 3.19 (Weighted H 1(\u2126 ) Inner-Product). In this remark we show that the conditions in Proposition 3.17 old when (\u00b7, \u00b7)V,\u03be is a suitably-weighted H 1 inner-product.\nConsider a differentiable mapping \u03c9 : L2(\u2126 ) \u2192 L\u221e(\u2126 ), such that, for \u03c9max > \u03c9min > 0 and \u03c9\u2032\u221e, \u03c9L > 0,\n\u2022 \u03c9min \u2264 \u03c9(\u03be ) \u2264 \u03c9max, for all \u03be \u2208 L2(\u2126 ); \u2022 \u2225\u03c9\u2032(\u03be )\u2225L(L2(\u2126);L\u221e(\u2126)) \u2264 \u03c9 \u2032 \u221e\n, for all \u03be \u2208 L2(\u2126 ); \u2022 \u2225\u03c9\u2032(\u03be1) \u2212 \u03c9\u2032(\u03be2)\u2225L(L2(\u2126);L\u221e(\u2126)) \u2264 \u03c9L\u2225\u03be1 \u2212 \u03be2\u2225L2(\u2126), for all \u03be1, \u03be2 \u2208 L 2(\u2126 ).\nhe construction of such mappings was discussed in Remarks 3.6 and 3.7. Given \u03be \u2208 L2(\u2126 ), consider the weighted H 1(\u2126 ) inner-product\n(v1, v2)H1,\u03be := \u222b \u2126 \u03c9(\u03be )\u2207v1 \u00b7 \u2207v2 + \u222b \u2126 v1v2.\nbserve that\nmin{1, \u03c9min}\u2225v\u22252H1 \u2264 (v, v)H1,\u03be \u2264 max{1, \u03c9max}\u2225v\u2225 2 H1 , \u2200v \u2208 H 1(\u2126 ).\nence, statement (ii) of Proposition 3.17 is satisfied with C\u03031 = \u221a min{1, \u03c9min} and C\u03032 = \u221a\nmax{1, \u03c9max}. On the other hand, given \u03be \u2208 L2(\u2126 ), the operator A(\u03be ) is defined by the following action:\nA(\u03be )v = ( \u03c9(\u03be )\u2207v, \u2207(\u00b7) ) L2(\u2126) + (v, \u00b7)L2(\u2126) , \u2200v \u2208 H 1(\u2126 ).\nherefore, is easy to see that A(\u00b7) satisfies the statement (iii) of Proposition 3.17. Indeed, observe that A(\u00b7) is ifferentiable and [A\u2032(\u03be )\u03b7]v = ( [\u03c9\u2032(\u03be )\u03b7]\u2207v, \u2207(\u00b7) ) L2(\u2126) for any direction \u03b7 \u2208 L\n2(\u2126 ). Moreover, A\u2032(\u00b7) is uniformly ounded and Lipschitz-continuous, since \u03c9\u2032(\u00b7) is uniformly bounded and Lipschitz continuous.\nOf course, for any v1, v2 \u2208 H 1(\u2126 ), we could have chosen the following equivalent inner-products, for which ne can prove similar results:\n(v1, v2)H1,\u03be := ( \u2207v1, \u2207v2 ) L2(\u2126) + ( \u03c9(\u03be )v1, v2 ) L2(\u2126) ,\n(v1, v2)H1,\u03be := ( \u03c9(\u03be )\u2207v1, \u2207v2 ) L2(\u2126) + ( \u03c9(\u03be )v1, v2 ) L2(\u2126) .\nlso, for H 10 (\u2126 ), one can consider ( \u03c9(\u03be )\u2207v1, \u2207v2 ) L2(\u2126), as in Example 3.16. Finally, for the graph space V defined\nn Example 3.15, one can consider the weighted inner product ( \u03c9(\u03be ) \u03b2 \u00b7 \u2207v1 , \u03b2 \u00b7 \u2207v2 ) L2(\u2126) provided \u2225\u03b2 \u00b7 \u2207(\u00b7)\u2225L2(\u2126) s a norm on V. \u25a1\nb\n4\nv\n4\nS H\nf\nw\nW\nW f n t t\nF p ( T\n4. Numerical results\nIn this section, we consider numerical examples for the advection\u2013reaction PDE in 1-D and 2-D. We consider oth weighted least squares (Example 3.1) and weighted residual minimization (Example 3.15).31 We construct\nmappings \u03d6, \u03c9 : L2(\u2126 ) \u2192 L\u221e(\u2126 ) as explained in Remarks 3.6 and 3.7.\n.1. Quantities of interest (point values)\nThe examples in this section aim to incorporate knowledge of data, in particular, the imposition of the exact alue of the solution at some point in the domain.\n.1.1. Weighted least squares Let \u2126 = (0, 1) \u2282 R and c > 0. Consider the advection\u2013reaction problem{\nu\u2032 + c u = c in \u2126 , u(0) = 0 . (44)\nince the exact solution to (44) is u(x) = 1 \u2212 exp(\u2212cx), we observe that u(x) \u2192 1 when r \u2192 +\u221e, for all x > 0. ence, for c > 0 sufficiently large, the exact solution has a boundary layer in the neighborhood of x = 0. Let Uh \u2282 H 1(0(\u2126 ) = {w \u2208 H 1(\u2126 ) : w(0) = 0} be the lowest-order conforming subspace of continuous piecewise linear functions on the uniform mesh of N elements of size h = 1/N . We use the weighted least squares method rom (21), with practical weight function\n\u03c9 ( \u03be (x) ) := 1 +\nM 1 + exp(\u2212\u03be (x)) , M > 0, (45)\nhich fits (28) in Remark 3.7 with\n\u03c3 (\u00b7) = 1\n1 + exp(\u2212(\u00b7)) . (46)\nIt is well known that the standard least squares solution (i.e., the one with \u03c9(\u03be ) \u2261 1) will exhibit overshoots around the boundary layer. Aiming to remedy this situation, and assuming prior knowledge of the value that the exact solution takes at x = h, we choose a cost functional that measures the distance to the exact solution at the point value x = h. In fact, we consider\nj(\u03be ) := 1 2\n( u(h) \u2212 uh,\u03be (h) )2 + \u03b1\n2 \u2225\u03be\u22252L2 , \u03b1 \u2265 0.\nLet M8 be the set of neural network functions with one hidden layer, 8-neurons, and ReLU activation, i.e., M8 := { \u03b78(x) = 8\u2211 j=1 c j ReLU(W j x + b j ) \u23d0\u23d0\u23d0 c j , W j , b j \u2208 R} . (47)\ne then consider the neural optimization of j(\u00b7); see Definition 2.2. For our first experiment, we choose a finite element space Uh consisting of N = 16 elements of size h = 1/16.\ne set c = 160 and \u03b1 = 0. We compute least squares approximations (1-D version of formulation in Example 3.1) or several configurations of the weight function (45), varying the M constant. Fig. 1 (left) shows that the weight eeds to have enough room for variability (M = 100) in order to pull down the cost functional to zero (see also he associated weight functions at the right panel). Fig. 1 (middle) shows that our strategy is effective in reducing he overshoots of the finite element solution.\nFor the second experiment of this section, we fix M = 100 and we investigate variations of the \u03b1-parameter. ig. 2 (left) suggest that the L2-norm of \u03be has to be able to reach high values (case when \u03b1 = 0) in order to ull down to zero the cost functional. This is also related to allowing the weight to have more variability. Fig. 2 middle) shows the impact of \u03b1 reducing the overshoots of the finite element solution (the smaller \u03b1, the better). he associated weight functions are depicted in Fig. 2 (right).\n31 Weighted Galerkin is not considered in view of Remark 3.12.\nw w t w t i\nR a d t\no t p\n4\ni\nFor the third experiment, we investigate the use of the integral operator (26) for the mapping \u03c9 : L2(\u2126 ) \u2192 L\u221e(\u2126 ), with \u03c3 (\u00b7) given by (46). We use a sufficiently large number of quadrature points (i.e., (27) with N = 4),\nhen computing the integral in \u03c9(\u03be ). We vary the kernel width r , and are particularly interested in r \u2192 0, upon hich \u03c9(\u03be ) converges to the practical weight function (45) as used above. Fig. 3 shows a very minor effect of r on he results (left and middle of Fig. 3). There is only a minor deviation visible for the result of \u03c9(\u03be8) with r = 10\u22121, hich is attributed to the use of a tolerance in the optimizer. Furthermore, the results for the neural networks hemselves (right of Fig. 3) seem to converge upon r \u2192 0 (note that the large variations in \u03be are not at all noticed n the discrete solutions, because \u03c9(\u03be8) enters the method).\nFinally, we study the convergence upon varying the number of quadrature points, i.e., N = 1, 2, 3, and 4 in (27). ecall that N = 1 coincides with the practical weight function (45) as used above. We fix the kernel width t r = 10\u22123 (similar results (not shown) are obtained for other values of r ). Fig. 4 shows again a very minor ependence on N . The results for the discrete solutions (left) and for \u03c9(\u03be8) (middle) are all nearly the same, while here is very quick converge for \u03be8 itself as N \u2192 1.\nThese latter numerics show that practical weight functions perform equally well compared to the integral perators as covered by theory. This supports our conjecture stated at the end of Remark 3.7, that the results of the heory may apply to practical weight functions. (The remainder of the numerical experiments are performed with ractical weight functions.)\n.1.2. Weighted MinRes This experiment has exactly the same configuration of the previous experiment in Section 4.1.1, except that Sh(\u03be )\ns computed with the discrete-dual minimal residual methodology. First, the approximation (trial) space U \u2282 L2(\u2126 )\nh\nn\nC n\nc a o t s\nf f F\norresponds to the lowest-order space of piecewise constants functions over the mesh. Additionally, we make use of discrete test space Vh \u2282 H 10)(\u2126 ) := {v \u2208 H 1(\u2126 ) : v(1) = 0} consisting in conforming piecewise linear functions ver the refined uniform mesh of 2N = 32 elements. The weighted discrete-dual residual minimization formulation hat computes Sh(\u03be ) is as follows (1-D version of formulation in Example 3.15): Find rh \u2208 Vh and uh \u2261 Sh(\u03be ) \u2208 Uh uch that\u23a7\u23aa\u23aa\u23a8\u23aa\u23aa\u23a9 \u222b 1 0 \u03c9(\u03be )r \u2032hv \u2032 h \u2212 \u222b 1 0 uh(v\u2032h \u2212 c vh) = c \u222b 1 0 vh , \u2200vh \u2208 Vh , \u2212\n\u222b 1 0 wh(r \u2032h \u2212 c rh) = 0 , \u2200wh \u2208 Uh . (48)\nAs in the previous Section 4.1.1, the computation of Sh is carried out for several configurations of the weight unction \u03c9(\u03be ) (see (45)), varying its M constant. Fig. 5 (left) shows that larger values of M allow to pull down aster the cost functional in the training procedure (see also the associated weight functions at the right panel).\nig. 5 (middle) shows how the overshoots of the finite element solutions are controlled.\nThe second experiment investigates variations of the \u03b1-parameter. Fig. 6 (left) suggests that the smaller \u03b1, the better for faster minimization of j(\u00b7). Fig. 6 (middle) shows the impact of \u03b1 reducing the overshoots of the finite element solution; while Fig. 6 (right) exposes the associated weight functions.\n4.2. Convergence of artificial neural networks\nIn this section, we study the quasi-optimal convergence behavior expected from theory as neural networks become larger.\nLet \u2126 := (0, 1) \u2282 R and consider{ u\u2032 = f in \u2126 ,\nu(0) = 0 , (49)\nwith f (x) := \u03c0 sin(\u03c0x). Notice the exact solution to (49) is u(x) = 1 \u2212 cos(\u03c0x). Let H 1(0(\u2126 ) = {w \u2208 H\n1(\u2126 ) : w(0) = 0} and let Uh \u2282 H 1(0(\u2126 ) be the finite element subspace of continuous piecewise linear functions on a uniform mesh consisting of N elements of size h = 1/N . We consider the weighted\nl\nw\nC\n(\ni s\na (\n4\nc n\neast squares formulation (1-D version of Example 3.1):\u23a7\u23a8\u23a9 Find uh \u2261 Sh(\u03be ) \u2208 Uh :\u222b 1\n0 \u03c9(\u03be )\n( f \u2212 u\u2032h ) w\u2032h = 0 , \u2200wh \u2208 Uh ,\n(50)\nhere the weight function is similar as used in Section 4.1.1: \u03c9 ( \u03be (x) ) :=\n1 2 + 2 1 + exp(\u2212\u03be (x)) . (51)\nLet Mn be the set of neural network functions with one hidden layer, n-neurons, and ReLU activation, i.e., Mn := { \u03b7n(x) = n\u2211 j=1 c j ReLU(W j x + b j ) \u23d0\u23d0\u23d0 c j , W j , b j \u2208 R} .\nonsider the cost functional\nj(\u03be ) := 1 2 \u222b 1 0 \u03c9\u0304(x) ( f (x) \u2212 u\u2032h,\u03be (x) )2 dx , (52)\nwhere we choose \u03c9\u0304(x) as smooth function, i.e.,\n\u03c9\u0304(x) = 1 + sin(\u03c0x/2) ,\nto allow for optimal convergence behavior as n \u2192 \u221e. Indeed, since the minimization of the cost functional and the discrete problem (50) are both weighted least squares formulations of the same problem (49), we expect that \u03c9(\u03ben(\u00b7)) \u2192 \u03c9\u0304(\u00b7) as n \u2192 \u221e, which is confirmed in Fig. 7 (left). Additionally, solving for \u03ben we get (see Fig. 7 middle))\n\u03ben(x) \u2212\u2192 \u03be\u0304 (x) = \u2212 ln (\n2 sin(\u03c0x/2) + 1/2\n\u2212 1 ) , as n \u2192 +\u221e.\nTo initialize the minimization algorithm, we have chosen \u03be (0)n \u2208 Mn as the neural network function that (linearly) nterpolates \u03be\u0304 on a uniform mesh of n \u22121 subintervals of \u2126 (i.e., having n uniformly distributed nodal points). The pace Uh has been fixed to N = 16 uniform elements.\nThe error \u2225\u03be\u0304 \u2212\u03ben\u2225L2 is depicted in Fig. 7 (right), which confirms quasi-optimal convergence behavior; indeed the symptotic rate is O(n\u22121/2), which is expected for our single-hidden-layer ReLU neural network approximations continuous piecewise-linear polynomials).\n.3. L1-based controls\nWe now consider numerical experiments that incorporate a stabilization mechanism. We note that the employed ost functionals use an L1-type norm, and hence do not fit within the currently presented theory. However, our umerics show that desirable quasi-minimizers have been computed.\nF\n( t v\n4.3.1. Minimizing the total variation In this section we work exactly with the same problem of Section 4.1.1, but we introduce a modification in the cost functional. Instead of minimizing the distance to the exact solution of a particular point value (supervised training), we take an unsupervised approach by minimizing the total variation of uh (i.e., the L1-norm of u\u2032h). 32 Hence, we consider the cost functional:\nj(\u03be ) := u\u2032h,\u03beL1 + \u03b12 \u2225\u03be\u22252L2 , \u03b1 \u2265 0.\nor a fixed value of M = 100, Fig. 8 (left) shows the behavior of the cost functional for different values of \u03b1, indicating that this value has to be chosen small enough to speed up the minimization process. Fig. 8 (middle) shows the quality of overshoot reduction for several values of \u03b1; while Fig. 8 (right) exposes the associated weight functions.\n4.3.2. Minimizing the L1 residual (1D domain) This experiment is inspired by the example of Guermond [8, Section 4.6.2]. As usual \u2126 = (0, 1) \u2282 R. The idea is to interpret the following overconstrained problem:{ u\u2032 + u = 1 in \u2126 ,\nu(0) = u(1) = 0 , (53)\nas the limiting case of a vanishing viscosity regime (i.e., an equivalent problem having an extra \u2212\u03b5u\u2032\u2032 term that vanishes as \u03b5 \u2192 0+). Of course, the exact solution that we want to approach (u(x) = 1 \u2212 e\u2212x ) only satisfies one of the boundary conditions. However, any discrete solution in a H 10 (\u2126 )-conforming space must satisfy both constraints. In this case, it is well-known that the standard least squares solution to this problem does not deliver satisfactory results. To remedy this drawback, we propose a cost functional that mimics the L1 residual minimization as proposed in [8]. Thus, our (unsupervised) cost functional will be\nj(\u03be ) :=  1 \u2212 uh,\u03be \u2212 u\u2032h,\u03be  \nresidual\n L1 + \u03b1\n2 \u2225\u03be\u22252L2 , \u03b1 \u2265 0.\nWe consider the weighted least squares formulation for uh,\u03be , solved on a uniform mesh of N = 8 elements. For a fixed M = 1000 constant in the weight function (45), we compute the discrete solution for several values of the \u03b1-parameter. Large values of \u03b1 allow for small values of \u2225\u03be\u2225L2 , and thus the weight becomes almost constant close to the standard least squares approach). On the other hand, small values of \u03b1 allow for more variability of he weight (see Fig. 9, right), and thus, we observe that we can recover a discrete solution mimicking the vanishing iscosity case (see Fig. 9, left).\n32 It is well-known that minimizing the total variation translates into a reduction of overshoots; see, e.g., [66].\n4.3.3. Minimizing L1 residual (2D domain) This is the two-dimensional version of the previous example in Section 4.3.2. Let \u2126 = (0, 1)2 \u2282 R2. For an advection field \u03b2\u20d7 = (1, 0), we consider the over-constrained problem:{ \u03b2\u20d7 \u00b7 \u2207u + u = 1 in \u2126 ,\nu = 0 on {(x1, x2) \u2208 \u2202\u2126 : x1 = 0 or x1 = 1} . (54)\nWe approach (54) using a coarse (and over-constrained) finite element space of piecewise linear functions of the form\nUh \u2282 {w \u2208 H 10 (\u2126 ) : w(0, x2) = w(1, x2) = 0, \u2200x2 \u2208 [0, 1]}.\nWe use the weighted least squares method given in Example 3.1 using the weight (45) with M = 1000. On the other hand, the cost functional j(\u00b7) for this case is defined as\nj(\u03be ) := 1 \u2212 uh,\u03be \u2212 \u03b2 \u00b7 \u2207uh,\u03beL1 + \u03b12 \u2225\u03be\u22252L2 , \u03b1 \u2265 0.\nThe discrete neural network space where we minimize j(\u00b7) will be M8 (see (47)). Results for the \u03b1 = 0 case are depicted in Fig. 10. We observe a strong correlation with the results in [8, Figure 9].\n5. Concluding remarks\nThe objective of this work was to introduce and analyze the neural optimization of finite element methods. We proposed a notion of quasi-minimization to enable the consideration of neural network functions as control variables, and proved a general theorem on the existence and convergence of quasi-minimizers. We applied our theory to the optimization of least squares, Galerkin, and minimal residual finite element methods, where the neural network function entered as a suitable weight within the discrete weak forms.\nI o a w r l r\np d a a a\nu i a s\nD\nh\nD\nA\nw t a I\nThe notion of quasi-minimization is critical, since sets of neural-network functions are generally not closed. f instead, one aims to minimize over a (closed) linear space of classical approximations,33 the standard notion f minimization is adequate. We have been motivated to explore the use of neural networks as these generate new class of functions that have shown recent success in situations where classical approximations may not ork (e.g., high-dimensional problems [2]). Although there are many open questions (e.g., those related to obust and efficient optimization algorithms, hence training of the neural network), there is currently a growing iterature providing deeper mathematical understanding, proposing new algorithms and developing accessible elevant software.\nWhile this paper has explored the neural optimization of finite element methods from a mostly theoretical erspective, there remain many avenues that require further work to enable a practical methodology. For example, erivatives with respect to the trainable parameters would need to be computed to utilize a gradient-based algorithm, nd further work would be required to better understand the role of parameters used within the methodology (such s \u03b1 in (6), M in (45) and r in (25)), and the approximation of integral-operator weight functions by practical lternatives.\nFurthermore, the idea of a weighted Galerkin formulation, while straightforward, exposes itself to instability, nless the involved weight function is suitable constrained. Optimizing methods to control their stability is an nteresting avenue for further research. On the other hand, weighted least squares and weighted residual minimization re without such cumbersome constraints, and seem to be the only (conforming) weighted formulations for which tability is guaranteed.\neclaration of competing interest\nThe authors declare that they have no known competing financial interests or personal relationships that could ave appeared to influence the work reported in this paper.\nata availability\nData will be made available on request.\ncknowledgments\nThe authors are grateful to the reviewers for their helpful suggestions. KvdZ acknowledges helpful discussions ith Dante Kalise, Hamd Alsobhi, Anne Boschman and Andrew Stuart. This research has received funding from he European Union\u2019s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant greement No 777778 (MATHROCKS). IM acknowledges support from the project DI Investigacio\u0301n Innovadora nterdisciplinaria PUCV 2021 No 039.409/2021: Nanoio\u0301nica: Un enfoque interdisciplinario. The work by IB was\npartially supported by ANID FONDECYT/Postdoctorado No 3200827. The research by KvdZ was supported by the Engineering and Physical Sciences Research Council (EPSRC), UK under Grant EP/T005157/1 and EP/W010011/1.\nAppendix. Proofs\nA.1. Proof of Theorem 2.A\n(i) Strong convexity of j implies coercivity, i.e., j(\u03be ) \u2192 +\u221e when \u2225\u03be\u2225X \u2192 +\u221e. Moreover, j is continuous in the strong topology since it is differentiable. Additionally, we know that convexity plus continuity implies that j is weakly lower semicontinuous (see, e.g. [67, Corollary 3.9]). We thus satisfy all the hypothesis of the theorem of existence of minimizers for coercive and sequentially weakly lower semicontinuous functionals [68, Theorem 9.3-1]. Moreover, strong convexity ensures that such a (global) minimizer \u03be\u0304 \u2208 X is unique. Besides, global differentiability of j implies the first-order necessary optimality condition j \u2032 ( \u03be\u0304 ) = 0.\n33 That is, those spanned by a basis such as a finite element space, b-spline approximations, etc.\ne\nf\nA\n(ii) We know that j has a global lower bound. Thus, by the infimum property, for any \u03b4n > 0 there must exist \u03be\u0304n \u2208 Mn such that\nj(\u03be\u0304n) < inf \u03b7n\u2208Mn\nj(\u03b7n) + \u03b4n\n2 . (55)\n(iii) Let \u03be\u0304 \u2208 X be the global minimizer and let \u03be\u0304n \u2208 Mn satisfy (7). By characterization of strong convexity, we have for all t \u2208 (0, 1)\nj ( \u03be\u0304 ) \u2264 j ( t \u03be\u0304 + (1 \u2212 t)\u03be\u0304n ) \u2264 t j ( \u03be\u0304 ) + (1 \u2212 t) j ( \u03be\u0304n ) \u2212 \u03b3\n2 t(1 \u2212 t)\u2225\u03be\u0304 \u2212 \u03be\u0304n\u22252X .\nThus, for all t \u2208 (0, 1) and \u03b7n \u2208 Mn we get \u03b3\n2 t\u2225\u03be\u0304 \u2212 \u03be\u0304n\u22252X \u2264 j\n( \u03be\u0304n ) \u2212 j ( \u03be\u0304 ) < j ( \u03b7n ) \u2212 j ( \u03be\u0304 ) + \u03b4n\n2 . (56) On the other hand, using the facts that j \u2032 is L-Lipschitz and j \u2032 ( \u03be\u0304 )\n= 0, we deduce [68, cf. proof of Thm. 7.7-3, page 488]\nj ( \u03b7n ) \u2212 j ( \u03be\u0304 ) = \u222b 1 0 \u27e8 j \u2032 ( s\u03b7n + (1 \u2212 s)\u03be\u0304 ) , \u03b7n \u2212 \u03be\u0304 \u27e9 ds\n= \u222b 1 0 \u27e8 j \u2032 ( s\u03b7n + (1 \u2212 s)\u03be\u0304 ) \u2212 j \u2032 ( \u03be\u0304 ) , \u03b7n \u2212 \u03be\u0304 \u27e9 ds\n\u2264 L\u2225\u03b7n \u2212 \u03be\u0304\u22252X \u222b 1 0 s = L 2 \u2225\u03b7n \u2212 \u03be\u0304\u2225 2 X. (57)\nHence, combining (56) with (57), taking the limit when t \u2192 1 and the infimum over all \u03b7n \u2208 Mn , we get the stimate\n\u03b3 \u2225\u03be\u0304 \u2212 \u03be\u0304n\u2225 2 X < L inf\n\u03b7n\u2208Mn \u03be\u0304 \u2212 \u03b7n2X + \u03b4n, rom which (13) is deducted.\n.2. Proof of Theorem 2.B\nWe proceed to prove each one of the statements.\n(i) Since Z and X are a Hilbert spaces, the quadratic maps Z \u220b z \u21a6\u2192 12\u2225z\u2225 2 Z and X \u220b \u03be \u21a6\u2192 1 2\u2225\u03be\u2225 2 X are\ndifferentiable. On the other hand, Sh and Q are also differentiable (Q is linear), and thus j1 is differentiable by means of the chain rule (see, e.g. [56, Theorem 2.20]). Moreover,\nj \u20321(\u03b7)(\u00b7) = ( QSh(\u03b7) , QS\u2032h(\u03b7)(\u00b7) ) Z = ( S\u2032h(\u03b7) \u22c6 Q\u22c6 QSh(\u03b7) , \u00b7 ) X.\nThus, we conclude that j1 is Lipschitz since j \u20321(\u03b7) \u2212 j \u20321(\u03b6 )X\u2217 = S\u2032h(\u03b7)\u22c6 Q\u22c6 QSh(\u03b7) \u2212 S\u2032h(\u03b6 )\u22c6 Q\u22c6 QSh(\u03b6 )X \u2264 S\u2032h(\u03b7)\u22c6 Q\u22c6 Q(Sh(\u03b7) \u2212 Sh(\u03b6 ))X + (S\u2032h(\u03b7) \u2212 S\u2032h(\u03b6 ))\u22c6 Q\u22c6 QSh(\u03b6 )X\n\u2264 \u2225Q\u22252L(U;Z) ( M2S\u2032 + L S\u2032 MS ) \u2225\u03b7 \u2212 \u03b6\u2225X ,\nwhere we have used the mean value theorem together with\n\u2022 the boundedness of S\u2032h , with bounding constant MS\u2032 ; \u2022 the Lipschitzness of S\u2032h , with Lipschitz constant L S\u2032 ; \u2022 the boundedness of Sh , with bounding constant MS .\nFinally, by making L1 := \u2225Q\u22252L(U;Z) ( M2S\u2032 +L S\u2032 MS ) , it is straightforward to see that L1 +\u03b1 will be a Lipschitz\n\u2032 constant for j .\ne b\nI r\nw\ne\n(ii) Just observe that\u27e8 j \u2032(\u03b7) \u2212 j \u2032(\u03b6 ), \u03b7 \u2212 \u03b6 \u27e9 X\u2217,X = \u27e8 j \u20321(\u03b7) \u2212 j \u2032 1(\u03b6 ), \u03b7 \u2212 \u03b6 \u27e9 X\u2217,X + \u03b1\u2225\u03b7 \u2212 \u03b6\u2225 2 X\n\u2265 (\u2212L1 + \u03b1)\u2225\u03b7 \u2212 \u03b6\u22252X .\nThus, j is strongly convex whenever \u03b1 > 0 is sufficiently large.\nA.3. Proof of Proposition 2.9\nThe statements (i) and (ii) are classical from Babus\u030cka\u2013Brezzi theory (see, e.g., Ern & Guermond [6, Theorem 49.13]). To prove statement (iii), first observe that\nsup v2\u2208K\u0302\na(\u03be ; v1, v2) \u2225v1\u2225V\u2225v2\u2225V \u2265 a(\u03be ; v1, v1) \u2225v1\u2225V\u2225v1\u2225V \u2265 a(\u03be ; v1, v1) \u2225v1\u2225V,\u03be\u2225v1\u2225V,\u03be (C1,\u03be )2 = (C1,\u03be )2,\nwhich confirms \u03b1h = (C1,\u03be )2 in (14a). For the a priori bound, since a(\u03be ; \u00b7, \u00b7) is an equivalent inner-product on V\u0302, consider z\u0302 \u2208 V\u0302 such that\na(\u03be ; z\u0302, v\u0302) = b(uh, v\u0302), \u2200v\u0302 \u2208 V\u0302.\nHence,\nsup v\u0302\u2208V\u0302 b(uh, v\u0302) \u2225v\u0302\u2225V,\u03be = sup v\u0302\u2208V\u0302 a(\u03be ; z\u0302, v\u0302) \u2225v\u0302\u2225V,\u03be = a(\u03be ; z\u0302, z\u0302) \u2225z\u0302\u2225V,\u03be = b(uh, z\u0302) \u2225z\u0302\u2225V,\u03be . (58)\nMoreover,\na(\u03be, r\u0302 , z\u0302) = a(\u03be, z\u0302, r\u0302 ) = b(uh, r\u0302 ) = 0. (59)\nNext, observe that\n\u2225uh\u2225U \u2264 1 \u03b2h sup v\u0302\u2208V\u0302 b(uh, v\u0302) \u2225v\u0302\u2225V \u2264 C2,\u03be \u03b2h sup v\u0302\u2208V\u0302 b(uh, v\u0302) \u2225v\u0302\u2225V,\u03be\n(by (14) and (15))\n= C2,\u03be \u03b2h b(uh, z\u0302) \u2225z\u0302\u2225V,\u03be = C2,\u03be \u03b2h\n( f (z\u0302) \u2212 a(\u03be, r\u0302 , z\u0302) ) \u2225z\u0302\u2225V,\u03be\n(by (58) and (4))\n\u2264 C2,\u03be C1,\u03be 1 \u03b2h f (z\u0302) \u2225z\u0302\u2225V , (by (15) and (59))\nfrom which (16) can be easily deducted.\nA.4. Proof of Proposition 2.10\nAssumption (20) implies two important facts: A(\u03be )\u2217 is surjective, and the range of A(\u03be ) is closed (see, .g., [67, Theorem 2.21]). Therefore, by Banach closed range theorem, the range of A(\u03be ) must be characterized y ( ker A(\u03be )\u2217 )\u22a5. Assume that Eq. (2) is satisfied. Hence, f \u2212 Buh \u2208 Vh(\u03be )\u22a5. On another hand, observe that ker A(\u03be )\u2217 \u2282 Vh(\u03be ). n particular, f \u2212 Buh \u2208 (ker A(\u03be )\u2217)\u22a5, which means that f \u2212 Buh is in the range of A(\u03be ). Thus, there exists an \u2208 V\u0302 such that A(\u03be )r = f \u2212 Buh , which is Eq. (18a). Next, given any wh \u2208 Uh , there must be vh \u2208 V\u0302 such that\nA(\u03be )\u2217vh = Bwh (by surjectivity of A(\u03be )\u2217). Hence, vh \u2208 Vh(\u03be ) by definition of this last space (see (19)). Besides,\u27e8 r, Bwh \u27e9 = \u27e8 r, A(\u03be )\u2217vh \u27e9 = \u27e8 A(\u03be )r, vh \u27e9 = \u27e8 f \u2212 Buh, vh \u27e9 = 0,\nhich proves Eq. (18b). Conversely, let (r, uh) \u2208 V\u0302\u00d7Uh solve the state problem (4), or equivalently (18) in operator form. Testing with\nlements in vh \u2208 Vh(\u03be ), we get\n\u27e8 f, vh\u27e9 = \u27e8A(\u03be )r, vh\u27e9 + \u27e8Buh, vh\u27e9 (by (18a))\u27e8 \u2217 \u27e9\n= r, A(\u03be ) vh + \u27e8Buh, vh\u27e9 (using the adjoint property)\no\nN g\nf\nF o\nw t\nS\nT\nM\n= \u27e8r, Bwh\u27e9 + \u27e8Buh, vh\u27e9 (by definition of Vh(\u03be )) = \u27e8Buh, vh\u27e9 . (by (18b))\nThus, (2) is satisfied.\nA.5. Proof of Proposition 2.11\nLet us start proving statements (i), (ii), and (iii) at the same time. Recall the definition of the kernel space K\u0302 := ker B\u2217 \u2282 V\u0302. For any \u03be \u2208 X, consider the restricted operator A(\u03be ) \u23d0\u23d0 K\u0302 : K\u0302 \u2192 K\u0302 \u2217, as well as the restriction f \u23d0\u23d0 K\u0302 \u2208 K\u0302\n\u2217. Observe that the inf \u2212 sup condition (14) ensures that A(\u03be ) \u23d0\u23d0 K\u0302 is a boundedly invertible linear operator. Thus, given a direction \u03b7 \u2208 X and t \u2208 R, from the first equation\nf the mixed system (18) (restricted to K\u0302) we obtain that A(\u03be + t\u03b7) \u23d0\u23d0 K\u0302Rh(\u03be + t\u03b7) = f \u23d0\u23d0 K\u0302 (60a)\nA(\u03be ) \u23d0\u23d0 K\u0302Rh(\u03be ) = f \u23d0\u23d0 K\u0302 . (60b)\nIn particular, continuity of A(\u00b7) implies continuity of Rh(\u00b7). Moreover, using the inf \u2212 sup condition (14), it is clear that\n\u2225Rh(\u00b7)\u2225V \u2264 \u2225 f \u2225V\u0302\u2217 \u03b1h(\u00b7) . (61)\next, adding the term A(\u03be ) \u23d0\u23d0 K\u0302Rh(\u03be + t\u03b7) on both sides of Eq. (60a), rearranging it, and subtracting Eq. (60b) we\net Rh(\u03be + t\u03b7) \u2212 Rh(\u03be ) = [ A(\u03be ) \u23d0\u23d0 K\u0302 ]\u22121 (A(\u03be )\u23d0\u23d0K\u0302 \u2212 A(\u03be + t\u03b7)\u23d0\u23d0K\u0302) Rh(\u03be + t\u03b7),\nrom which, if A\u2032(\u03be )\u03b7 exists, it implies that Rh(\u00b7) has a Ga\u0302teaux derivative and R\u2032h(\u03be )\u03b7 = \u2212 [ A(\u03be ) \u23d0\u23d0 K\u0302 ]\u22121 A\u2032(\u03be )\u03b7\u23d0\u23d0\u23d0\nK\u0302 Rh(\u03be ). (62)\ninally, if A(\u00b7) is Ga\u0302teaux-differentiable at \u03be , then using the inf \u2212 sup condition (14), the boundedness of the linear perator A\u2032(\u03be ), and the estimate (61), we obtain\n\u2225R\u2032h(\u03be )\u03b7\u2225V \u2264 \u2225A\u2032(\u03be )\u03b7\u2225L(V\u0302;V\u0302\u2217)\u2225Rh(\u03be )\u2225V\n\u03b1h \u2264\n\u2225A\u2032(\u03be )\u2225\u2225 f \u2225V\u2217\n\u03b12h \u2225\u03b7\u2225X , (63)\nhich proves that Rh(\u00b7) is Ga\u0302teaux-differentiable at \u03be . Besides, if A\u2032(\u00b7) and \u03b1\u22121h (\u00b7) are uniformly bounded on X, hen R\u2032h(\u00b7) is uniformly bounded on X.\nNow is the turn of Sh . From the mixed system (18), we deduce\nBSh(\u03be + t\u03b7) = f \u2212 A(\u03be + t\u03b7)Rh(\u03be + t\u03b7) BSh(\u03be ) = f \u2212 A(\u03be )Rh(\u03be ).\nince B is boundedly invertible onto its closed range, we get Sh(\u03be + t\u03b7) \u2212 Sh(\u03be ) = B\u22121 ( [A(\u03be ) \u2212 A(\u03be + t\u03b7)]Rh(\u03be + t\u03b7) + A(\u03be )[Rh(\u03be ) \u2212 Rh(\u03be + t\u03b7)] ) .\nherefore, if A\u2032(\u03be )\u03b7 exists, then we already know that R\u2032h(\u03be )\u03b7 exists, and thus S\u2032h(\u03be )\u03b7 = B \u22121 ( \u2212[A\u2032(\u03be )\u03b7]Rh(\u03be ) \u2212 A(\u03be )R\u2032h(\u03be )\u03b7 ) . (64)\noreover, if A(\u00b7) is Ga\u0302teaux-differentiable, then using the inf \u2212 sup condition (14) and the estimate (63), we get\n\u2225S\u2032h(\u03be )\u03b7\u2225U \u2264 1 \u03b2h \u2225B[S\u2032h(\u03be )\u03b7]\u2225V\u0302\u2217\n\u2264 \u2225A\u2032(\u03be )\u2225\u2225Rh(\u03be )\u2225V + \u2225A(\u03be )\u2225L(V\u0302;V\u0302\u2217)\u2225R \u2032 h(\u03be )\u2225L(X;V\u0302) \u03b2h \u2225\u03b7\u2225X\n\u2264 \u2225A\u2032(\u03be )\u2225\u2225 f \u2225V\u2217 ( 1 + \u2225A(\u03be )\u2225L(V\u0302;V\u0302\u2217) )\n\u2225\u03b7\u2225X ,\n(65)\n\u03b1h\u03b2h \u03b1h\nH\nR\nH\nW\nc o t t\nA\nwhich proves that Sh(\u00b7) is Ga\u0302teaux-differentiable. Besides, it is clear from (65) that \u2225S\u2032h(\u00b7)\u2225L(X;U) will be uniformly bounded on X whenever \u2225A(\u00b7)\u2225L(V\u0302;V\u0302\u2217) and \u2225A\n\u2032(\u00b7)\u2225 are uniformly bounded on X, as well as \u03b1\u22121h (\u00b7). (iv) Let us prove Lipschitzness. Using (62), observe that for any \u03be1, \u03be2, \u03b7 \u2208 X we have\nA(\u03be2) \u23d0\u23d0 K\u0302 ( R\u2032h(\u03be1) \u2212 R \u2032 h(\u03be2) ) \u03b7 = [ A\u2032(\u03be2) \u2212 A\u2032(\u03be1) ] \u03b7 \u23d0\u23d0\u23d0 K\u0302 Rh(\u03be2) + [ A(\u03be2) \u2212 A(\u03be1) ]\u23d0\u23d0\u23d0 K\u0302 R\u2032h(\u03be1)\u03b7\n+ A\u2032(\u03be1)\u03b7 \u23d0\u23d0\u23d0 K\u0302 [ Rh(\u03be2) \u2212 Rh(\u03be1) ] .\nence,\n\u2225R\u2032h(\u03be1) \u2212 R \u2032 h(\u03be2)\u2225L(X;V\u0302) \u2264 \u2225Rh(\u03be2)\u2225V\n\u03b1h(\u03be2) \u2225A\u2032(\u03be1) \u2212 A\u2032(\u03be2)\u2225 (66a)\n+ \u2225R\u2032h(\u03be1)\u2225L(X;V\u0302)\n\u03b1h(\u03be2) \u2225A(\u03be1) \u2212 A(\u03be2)\u2225L(V\u0302;V\u0302\u2217) (66b)\n+ \u2225A\u2032(\u03be1)\u2225 \u03b1h(\u03be2) \u2225Rh(\u03be1) \u2212 Rh(\u03be2)\u2225V . (66c)\necall that under our hypothesis, \u03b1\u22121h (\u00b7), Rh(\u00b7), R \u2032 h(\u00b7), and A \u2032(\u00b7) are all uniformly bounded on X. Therefore, the first term on the right hand side (expression (66a)) is Lipschitz by the Lipschitz assumption on A\u2032(\u00b7); the second term (expression (66b)) is Lipschitz as a consequence of the mean value theorem on A(\u00b7) and the uniform boundedness of A\u2032(\u00b7); while the last term (expression (66c)) is Lipschitz by the mean value theorem on Rh(\u00b7) and the uniform boundedness of R\u2032h(\u00b7).\nFinally, to prove the Lipschitzness of S\u2032h(\u00b7), we use (64) to write B ( S\u2032h(\u03be1)\u03b7 \u2212 S \u2032 h(\u03be2)\u03b7 ) =[A\u2032(\u03be2)\u03b7] ( Rh(\u03be2) \u2212 Rh(\u03be1) ) + A(\u03be2) [ R\u2032h(\u03be2)\u03b7 \u2212 R \u2032 h(\u03be1)\u03b7 ]\n+ [ (A\u2032(\u03be2) \u2212 A\u2032(\u03be1))\u03b7 ] Rh(\u03be1) + [ A(\u03be2) \u2212 A(\u03be1) ] R\u2032h(\u03be1)\u03b7 .\nence,S\u2032h(\u03be1) \u2212 S\u2032h(\u03be2)L(X;U) \u2264 \u2225A\u2032(\u03be2)\u2225\u03b2h \u2225Rh(\u03be1) \u2212 Rh(\u03be2)\u2225V (67a) +\n\u2225A(\u03be2)\u2225L(V\u0302;V\u0302\u2217) \u03b2h \u2225R\u2032h(\u03be1) \u2212 R \u2032 h(\u03be2)\u2225L(X;V\u0302) (67b)\n+ \u2225Rh(\u03be1)\u2225V\n\u03b2h \u2225A\u2032(\u03be1) \u2212 A\u2032(\u03be2)\u2225 (67c)\n+ \u2225R\u2032h(\u03be1)\u2225L(X;V\u0302)\n\u03b2h \u2225A(\u03be1) \u2212 A(\u03be2)\u2225L(V\u0302;V\u0302\u2217) . (67d)\ne recall again that Rh(\u00b7), R\u2032h(\u00b7), A(\u00b7), and A \u2032(\u00b7) are all uniformly bounded on X. Therefore, the Lipschitzness of S\u2032h(\u00b7) is implied by the following facts: the Lipschitzness of the first term on right hand side (expression (67a)) is a onsequence of the mean value theorem applied to Rh(\u00b7) and the uniform boundedness of R\u2032h(\u00b7); the Lipschitzness f the second term (expression (67b)) is due to the previously proved Lipschitzness of R\u2032h(\u00b7); the Lipschitzness of he third term (expression (67c)) is implied by the assumed Lipschitzness of A\u2032(\u00b7); and the Lipschitzness of the last erm (expression (67d)) is consequence of the mean value theorem applied to A and the uniform boundedness of A\u2032(\u00b7).\n.6. Proof of Proposition 3.4\nLet us prove item by item.\n(i) Observe that in this case, the bilinear form a(\u03be, \u00b7, \u00b7) defines a weighted inner product in L2(\u2126 ), for which its induced norm \u2225v\u2225V,\u03be := \u221a (\u03d6 (\u03be )v, v)L2 satisfies\n\u221a \u03d6min \u2225v\u2225L2 \u2264 \u2225v\u2225V,\u03be \u2264 \u221a \u03d6max \u2225v\u2225L2 , \u2200v \u2208 V = L2(\u2126 ).\nHence, the first inf \u2212 sup condition in (14) is satisfied with \u03b1 = \u03d6 ; see Proposition 2.9(iii) and Footnote 22.\nh min\nA\nOn the other hand, we are under the assumption that the operator B : HB \u2192 V\u2217 is boundedly invertible. Hence, there must be a uniform constant \u03b2 > 0 such that\nsup v\u2208V b(wh, v) \u2225v\u2225V = \u2225Bwh\u2225V\u2217 \u2265 \u03b2\u2225wh\u2225HB , \u2200wh \u2208 Uh,\nwhich implies the second inf \u2212 sup condition in (14). (ii) Uniform boundedness of Sh(\u00b7) is a consequence of Proposition 2.9(iii). Indeed, in our particular case we get\n\u2225Sh(\u03be )\u2225HB \u2264 \u03d6max\n\u03d6min\n1 \u03b2 \u2225 f \u2225L2 , \u2200\u03be \u2208 L 2(\u2126 ).\nTo show differentiability of Sh(\u00b7), let us recall the operator A : X \u2192 L(V;V\u2217) defined in Section 2.4, which in this particular case takes the form\nA(\u03be )v := (\u03d6 (\u03be )v, \u00b7)L2 , \u2200v \u2208 L 2(\u2126 ).\nFurthermore, we have the uniform bound\n\u2225A(\u03be )\u2225 = sup v\u2208L2(\u2126) \u2225\u03d6 (\u03be )v\u2225L2 \u2225v\u2225L2 = \u2225\u03d6 (\u03be )\u2225L\u221e \u2264 \u03d6max . (68)\nSince \u03d6 (\u00b7) is differentiable, it is straightforward to check that A(\u00b7) is also differentiable, and given \u03be, \u03b7 \u2208 L2(\u2126 ), we have\n[A\u2032(\u03be )\u03b7]v = ( [\u03d6 \u2032(\u03be )\u03b7]v, \u00b7 ) L2 , \u2200v \u2208 L 2(\u2126 ).\nMoreover, we can verify\n\u2225A\u2032(\u03be )\u2225 = sup \u03b7\u2208L2(\u2126) \u2225\u03d6 \u2032(\u03be )\u03b7\u2225L\u221e \u2225\u03b7\u2225L2 = \u2225\u03d6 \u2032(\u03be )\u2225L(L2(\u2126);L\u221e(\u2126)) \u2264 \u03d6 \u2032 \u221e . (69)\nThus, the differentiability of Sh(\u00b7) is a consequence of Proposition 2.11(ii). (iii) Uniform boundedness of S\u2032h(\u00b7) is a consequence of Proposition 2.11(iii), using the fact that A(\u00b7), A\n\u2032(\u00b7), and \u03b1\u22121h \u2261 \u03d6 \u22121 min are all uniformly bounded (see the above expressions (68) and (69)). On the other hand, the Lipschitz-continuity of S\u2032h(\u00b7) relies on the Lipschitz-continuity of A \u2032(\u00b7) (by Proposition 2.11(iv)). The latter is true since\n\u2225A\u2032(\u03be1) \u2212 A\u2032(\u03be2)\u2225 = sup \u03b7\u2208L2(\u2126) \u2225\u03d6 \u2032(\u03be1)\u03b7 \u2212 \u03d6 \u2032(\u03be2)\u03b7\u2225L\u221e \u2225\u03b7\u2225L2 \u2264 \u03d6L\u2225\u03be1 \u2212 \u03be2\u2225L2 .\n.7. Proof of Proposition 3.10\n(i) This is a well-known result from Babus\u030cka\u2013Brezzi theory (see, e.g., [6]). (ii) Observe that uh \u2261 Sh(\u00b7) satisfies\n\u2225uh\u2225V \u2264 1\n\u03b1h(\u03be ) sup\nvh\u2208Uh\nb(uh, \u03c9(\u03be )vh) \u2225vh\u2225V = 1 \u03b1h(\u03be ) sup\nvh\u2208Uh\nf (\u03c9(\u03be )vh) \u2225vh\u2225V \u2264 \u03c9\u221e \u03b1 \u2225 f \u2225V\u2217 .\n(iii) Let \u03be, \u03b7 \u2208 X, t \u2208 R, and notice that for all vh \u2208 Uh we have b ( Sh(\u03be + t\u03b7) \u2212 Sh(\u03be ), \u03c9(\u03be + t\u03b7)vh ) = f (( \u03c9(\u03be + t\u03b7) \u2212 \u03c9(\u03be ) ) vh ) \u2212 b ( Sh(\u03be ), ( \u03c9(\u03be + t\u03b7) \u2212 \u03c9(\u03be ) ) vh ) .\nThus, the derivative of Sh at \u03be in the \u03b7 direction is the solution of b ( S\u2032h(\u03be )\u03b7, \u03c9(\u03be )vh ) = f ( \u03c9\u2032(\u03be )\u03b7 vh ) \u2212 b ( Sh(\u03be ), \u03c9\u2032(\u03be )\u03b7 vh ) , \u2200vh \u2208 Uh .\nMoreover,\n\u2225S\u2032h(\u03be )\u03b7\u2225V \u2264 \u2225 f \u2225V\u2217 + \u2225B\u2225L(V,V\u2217)\u2225Sh(\u03be )\u2225V\n\u03b1 \u2225\u03c9\u2032(\u03be )\u2225L(X,W)\u2225\u03b7\u2225L2(\u2126),\nwhich implies that Sh(\u00b7) is differentiable and S\u2032h(\u00b7) is uniformly bounded, whenever \u03c9(\u00b7) is differentiable with \u2032 uniformly bounded \u03c9 (\u00b7).\nA\nR\nOn the other hand, observe that for all \u03be1, \u03be2, \u03b7 \u2208 X, and all vh \u2208 Uh , we have b (\nS\u2032h(\u03be1)\u03b7 \u2212 S \u2032 h(\u03be2)\u03b7, \u03c9(\u03be1)vh ) = f (( \u03c9\u2032(\u03be1) \u2212 \u03c9\u2032(\u03be2) ) \u03b7vh ) + b ( Sh(\u03be2) \u2212 Sh(\u03be1), \u03c9\u2032(\u03be2)\u03b7vh\n) + b ( Sh(\u03be1), ( \u03c9\u2032(\u03be2) \u2212 \u03c9\u2032(\u03be1) ) \u03b7vh\n) + b ( S\u2032h(\u03be2)\u03b7, ( \u03c9(\u03be2) \u2212 \u03c9(\u03be1) ) vh ) .\nTherefore,\n\u2225S\u2032h(\u03be1) \u2212 S \u2032 h(\u03be2)\u2225L(X,V) \u2264 1 \u03b1 \u2225 f \u2225V\u2217 \u2225\u03c9\u2032(\u03be1) \u2212 \u03c9\u2032(\u03be2)\u2225L(X,W)\n+ 1 \u03b1 \u2225B\u2225L(V,V\u2217) \u2225Sh(\u03be2) \u2212 Sh(\u03be1)\u2225V \u2225\u03c9\u2032(\u03be2)\u2225L(X,W)\n+ 1 \u03b1 \u2225B\u2225L(V,V\u2217) \u2225Sh(\u03be1)\u2225V \u2225\u03c9\u2032(\u03be2) \u2212 \u03c9\u2032(\u03be1)\u2225L(X,W)\n+ 1 \u03b1 \u2225B\u2225L(V,V\u2217) \u2225S\u2032h(\u03be2)\u2225L(X,V) \u2225\u03c9(\u03be2) \u2212 \u03c9(\u03be1)\u2225W.\nThus, the Lipschitz-continuity of S\u2032h relies on: the Lipschitz-continuity of \u03c9 \u2032; the mean value theorem; and the uniform boundedness of Sh , S\u2032h , and \u03c9 \u2032.\n.8. Proof of Proposition 3.17\n(i) Making the identification V\u0302 \u2261 Vh and a(\u03be ; \u00b7, \u00b7) \u2261 (\u00b7, \u00b7)V,\u03be , we observe that the well-posedness of (42) is a direct consequence of Proposition 2.9, using the fact that (\u00b7, \u00b7)V,\u03be is an equivalent innerproduct, together with assumption (41)(b).\n(ii) Using the hypothesis of this statement and the estimate (16) in Proposition 2.9(iii), we get the uniform bound\n\u2225Sh(\u03be )\u2225U \u2264 1 \u03b2h C\u03032 C\u03031 \u2225 f \u2225V\u2217 , \u2200\u03be \u2208 X.\n(iii) Direct application of Proposition 2.11, noticing also that \u03b1\u22121h (\u03be ) \u2264 C\u0303 \u22122 1 and\n\u2225A(\u03be )\u2225L(V;V\u2217) = sup v1\u2208V \u2225(v1, \u00b7)V,\u03be\u2225V\u2217 \u2225v1\u2225V\n\u2264 sup v1\u2208V C\u030322 \u2225v1\u2225V,\u03be ( sup v2\u2208V |(v1, v2)V,\u03be | \u2225v2\u2225V,\u03be ) = C\u030322 .\neferences [1] C.F. Higham, D.J. Higham, Deep learning: An introduction for applied mathematicians, SIAM Rev. 61 (4) (2019) 860\u2013891. [2] W. E, Machine learning and computational mathematics, Commun. Comput. Phys. 28 (5) (2020) 1639\u20131670. [3] G.E. Karniadakis, I.G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang, L. Yang, Physics-informed machine learning, Nat. Rev. Phys. 3\n(2021) 422\u2013440. [4] G.C.Y. Peng, M. Alber, A.B. Tepole, W.R. Cannon, S. De, S. Dura-Bernal, K. Garikipati, G. Karniadakis, W.W. Lytton, P. Perdikaris,\nL. Petzold, E. Kuhl, Multiscale modeling meets machine learning: What can we learn? Arch. Comput. Methods Eng. 28 (3) (2021) 1017\u20131037. [5] J.T. Oden, J.N. Reddy, An Introduction to the Mathematical Theory of Finite Elements, Dover, Mineola, New York, 2011, Unabridged republication of the edition published by John Wiley and Sons, New York, 1976. [6] A. Ern, J.-L. Guermond, Finite Elements II. Galerkin Approximation, Elliptic and Mixed PDEs, in: Texts in Applied Mathematics, vol. 73, Springer Nature, Switzerland, 2021. [7] M. Raissi, P. Perdikaris, G.E. Karniadakis, Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations, J. Comput. Phys. 378 (2019) 686\u2013707.\np\n[8] J.L. Guermond, A finite element technique for solving first-order PDEs in L , SIAM J. Numer. Anal. 42 (2) (2004) 714\u2013737.\n[9] E. Burman, A. Ern, Stabilized Galerkin approximation of convection-diffusion-reaction equations: discrete maximum principle and convergence, Math. Comp. 74 (2005) 1637\u20131652. [10] V. John, P. Knobloch, On spurious oscillations at layers diminishing (SOLD) methods for convection\u2013diffusion equations: Part I \u2013 A review, Comput. Methods Appl. Mech. Engrg. 196 (17) (2007) 2197\u20132215. [11] J.A. Evans, T.J. Hughes, G. Sangalli, Enforcement of constraints and maximum principles in the variational multiscale method, Comput. Methods Appl. Mech. Engrg. 199 (1) (2009) 61\u201376. [12] L. Demkowicz, J. Gopalakrishnan, I. Muga, J. Zitelli, Wavenumber explicit analysis of a DPG method for the multidimensional Helmholtz equation, Comput. Methods Appl. Mech. Engrg. 213\u2013216 (2012) 126\u2013138. [13] D. Peterseim, Eliminating the pollution effect in Helmholtz problems by local subscale correction, Math. Comp. 86 (2017) 1005\u20131036. [14] I. Brevis, I. Muga, K.G. van der Zee, A machine-learning minimal-residual (ML-MRes) framework for goal-oriented finite element\ndiscretizations, Comput. Math. Appl. 95 (2021) 186\u2013199, Recent Advances in Least-Squares and Discontinuous Petrov\u2013Galerkin Finite Element Methods. [15] L. Demkowicz, J. Gopalakrishnan, Discontinuous Petrov\u2013Galerkin (DPG) method, in: E. Stein, R. de Borst, T.J.R. Hughes (Eds.), Encyclopedia of Computational Mechanics, Second Edition, Wiley, 2017, Part 2 Fundamentals. [16] I. Muga, K.G. van der Zee, Discretization of linear problems in Banach spaces: Residual minimization, nonlinear Petrov\u2013Galerkin, and monotone mixed methods, SIAM J. Numer. Anal. 58 (6) (2020) 3406\u20133426. [17] J. Xu, Finite neuron method and convergence analysis, Commun. Comput. Phys. 28 (5) (2020) 1707\u20131745. [18] J. Pousin, Least squares formulations for some elliptic second order problems, feedforward neural network solutions and convergence\nresults, J. Comput. Math. Data Sci. 2 (2022) 100023. [19] J. M\u00fcller, M. Zeinhofer, Error estimates for the deep Ritz method with boundary penalty, in: B. Dong, Q. Li, L. Wang, Z.-Q.J. Xu\n(Eds.), Proceedings of Mathematical and Scientific Machine Learning, in: Proceedings of Machine Learning Research, vol. 190, PMLR, 2022, pp. 215\u2013230. [20] Y. Shin, Z. Zhang, G.E. Karniadakis, Error estimates of residual minimization using neural networks for linear PDEs, 2020, arXiv:20 10.08019. [21] S. Mishra, R. Molinaro, Estimates on the generalization error of physics-informed neural networks for approximating PDEs, IMA J. Numer. Anal. (2022) 1\u201343, http://dx.doi.org/10.1093/imanum/drab093, in press. [22] Z. Cai, J. Chen, M. Liu, Least-squares ReLU neural network (LSNN) method for linear advection-reaction equation, J. Comput. Phys. 443 (2021) 110514. [23] K. Kergrene, S. Prudhomme, L. Chamoin, M. Laforest, A new goal-oriented formulation of the finite element method, Comput. Methods Appl. Mech. Engrg. 327 (2017) 256\u2013276, Advances in Computational Mechanics and Scientific Computation\u2014the Cutting Edge. [24] A.N. Brooks, T.J. Hughes, Streamline upwind/Petrov-Galerkin formulations for convection dominated flows with particular emphasis on the incompressible Navier-Stokes equations, Comput. Methods Appl. Mech. Engrg. 32 (1) (1982) 199\u2013259. [25] J. Barrett, K. Morton, Approximate symmetrization and Petrov-Galerkin methods for diffusion-convection problems, Comput. Methods Appl. Mech. Engrg. 45 (1) (1984) 97\u2013122. [26] J.T. Oden, Optimal h-p finite element methods, Comput. Methods Appl. Mech. Engrg. 112 (1994) 309\u2013331. [27] D. Ray, J.S. Hesthaven, An artificial neural network as a troubled-cell indicator, J. Comput. Phys. 367 (2018) 166\u2013191. [28] S. Mishra, A machine learning framework for data driven acceleration of computations of differential equations, Math. Eng. 1 (1)\n(2018) 118\u2013146. [29] Y. Bar-Sinai, S. Hoyer, J. Hickey, M.P. Brenner, Learning data-driven discretizations for partial differential equations, Proc. Natl. Acad.\nSci. 116 (31) (2019) 15344\u201315349. [30] N. Discacciati, J.S. Hesthaven, D. Ray, Controlling oscillations in high-order discontinuous Galerkin schemes using artificial viscosity\ntuned by neural networks, J. Comput. Phys. 409 (2020) 109304. [31] Y. Wang, Z. Shen, Z. Long, B. Dong, Learning to discretize: Solving 1D scalar conservation laws via deep reinforcement learning,\nCommun. Comput. Phys. 28 (5) (2020) 2158\u20132179. [32] L. Schwander, D. Ray, J.S. Hesthaven, Controlling oscillations in spectral methods by local artificial viscosity governed by neural\nnetworks, J. Comput. Phys. 431 (2021) 110144. [33] T. Tassi, A. Zingaro, L. Dede\u2019, A machine learning approach to enhance the SUPG stabilization method for advection-dominated\ndifferential problems, Math. Eng. 5 (2) (2023) 1\u201326. [34] K.J. Fidkowski, G. Chen, Metric-based, goal-oriented mesh adaptation using machine learning, J. Comput. Phys. 426 (2021) 109957. [35] J. Bohn, M. Feischl, Recurrent neural networks as optimal mesh refinement strategies, Comput. Math. Appl. 97 (2021) 61\u201376. [36] W. E., B. Yu, The Deep Ritz Method: A deep learning-based numerical algorithm for solving variational problems, Commun. Math.\nSci. 6 (1) (2018) 1\u201312. [37] J. Sirignano, K. Spiliopoulos, DGM: A deep learning algorithm for solving partial differential equations, J. Comput. Phys. 375 (2018)\n1339\u20131364. [38] J. Berg, K. Nystr\u00f6m, A unified deep artificial neural network approach to partial differential equations in complex geometries,\nNeurocomputing 317 (2018) 28\u201341. [39] M. Ainsworth, J. Dong, Galerkin neural networks: A framework for approximating variational equations with error control, SIAM J.\nSci. Comput. 43 (4) (2021) A2474\u2013A2501. [40] M. Liu, Z. Cai, J. Chen, Adaptive two-layer ReLU neural network: I. Best least-squares approximation, Comput. Math. Appl. 113\n(2022) 34\u201344. [41] C. Uriarte, D. Pardo, \u00c1.J. Omella, A finite element based deep learning solver for parametric PDEs, Comput. Methods Appl. Mech.\nEngrg. 391 (2022) 114562.\n[42] J. Hesthaven, S. Ubbiali, Non-intrusive reduced order modeling of nonlinear problems using neural networks, J. Comput. Phys. 363 (2018) 55\u201378. [43] B. Khara, A. Balu, A. Joshi, S. Sarkar, C. Hegde, A. Krishnamurthy, B. Ganapathysubramanian, NeuFENet: Neural finite element solutions with theoretical bounds for parametric PDEs, 2021, arXiv:2110.01601. [44] G. Teichert, A. Natarajan, A. Van der Ven, K. Garikipati, Machine learning materials physics: Integrable deep neural networks enable scale bridging by learning free energy functions, Comput. Methods Appl. Mech. Engrg. 353 (2019) 201\u2013216. [45] J. Berg, K. Nystr\u00f6m, Neural networks as smooth priors for inverse problems for PDEs, J. Comput. Math. Data Sci. 1 (2021) 100008. [46] K. Xu, E. Darve, Physics constrained learning for data-driven inverse modeling from sparse observations, J. Comput. Phys. 453 (2022)\n110938. [47] H. You, Y. Yu, N. Trask, M. Gulian, M. D\u2019Elia, Data-driven learning of nonlocal physics from high-fidelity synthetic data, Comput.\nMethods Appl. Mech. Engrg. 374 (2021) 113553. [48] K. Bhattacharya, B. Hosseini, N.B. Kovachki, A.M. Stuart, Model reduction and neural networks for parametric PDEs, SMAI J.\nComput. Math. 7 (2021) 121\u2013157. [49] L. Cao, T. O\u2019Leary-Roseberry, P.K. Jha, J.T. Oden, O. Ghattas, Residual-based error correction for neural operator accelerated\ninfinite-dimensional Bayesian inverse problems, 2022. [50] S. Mishra, R. Molinaro, Estimates on the generalization error of physics-informed neural networks for approximating a class of inverse\nproblems for PDEs, IMA J. Numer. Anal. 42 (2022) 981\u20131022. [51] S. Berrone, C. Canuto, M. Pintore, Variational physics informed neural networks: the role of quadratures and test functions, 2021,\nhttps://arxiv.org/abs/2109.02035. [52] J. Roth, M. Schr\u00f6der, T. Wick, Neural network guided adjoint computations in dual weighted residual error estimation, SN Appl. Sci.\n4 (2022) 62. [53] P. Minakowski, T. Richter, A priori and a posteriori error estimates for the Deep Ritz method applied to the Laplace and Stokes\nproblem, J. Comput. Appl. Math. (2022) 114845. [54] J.L. Lions, Optimal Control of Systems Governed By Partial Differential Equations, Springer-Verlag, Berlin, 1971. [55] M. Hinze, R. Pinnau, M. Ulbrich, S. Ulbrich, Optimization with PDE Constraints, Springer, 2009. [56] F. Tr\u00f6ltzsch, Optimal Control of Partial Differential Equations: Theory, Methods and Applications, in: Graduate Studies in Mathematics,\nvol. 112, American Mathematical Society, Providence, 2010. [57] A. Borz\u00ec, V. Schulz, Computational Optimization of Systems Governed By Partial Differential Equations, in: Siam series on\nComputational Science and Engineering, Society for Industrial and Applied Mathematics, 2012. [58] R. Rannacher, B. Vexler, A priori error estimates for the finite element discretization of elliptic parameter identification problems with\npointwise measurements, SIAM J. Control Optim. 44 (5) (2005) 1844\u20131863. [59] P. Petersen, M. Raslan, F. Voigtlaender, Topological properties of the set of functions generated by neural networks of fixed size,\nFound. Comput. Math. 21 (5) (2021) 375\u2014444. [60] P. Bochev, M. Gunzburger, Chapter 12 - least-squares methods for hyperbolic problems, in: R. Abgrall, C.-W. Shu (Eds.), Handbook\nof Numerical Methods for Hyperbolic Problems, in: Handbook of Numerical Analysis, vol. 17, Elsevier, 2016, pp. 289\u2013317. [61] D. Yarotsky, Error bounds for approximations with deep ReLU networks, Neural Netw. 94 (2017) 103\u2013114. [62] I. G\u00fchring, G. Kutyniok, P. Petersen, Error bounds for approximations with deep ReLU neural networks in W s,p norms, Anal. Appl.\n18 (05) (2020) 803\u2013859. [63] S. Berrone, C. Canuto, M. Pintore, Solving PDEs by variational physics-informed neural networks: an a posteriori error analysis, 2022,\nhttps://arxiv.org/abs/2205.00786. [64] D.A. Di Pietro, A. Ern, Mathematical Aspects of Discontinuous Galerkin Methods, in: Math\u00e9matiques et Applications, vol. 69, Springer,\nBerlin, 2012. [65] I. Muga, M.J.W. Tyler, K.G. van der Zee, The discrete-dual minimal-residual method (DDMRes) for weak advection-reaction problems\nin Banach spaces, Comput. Methods Appl. Math. 19 (3) (2019) 557\u2013579. [66] L.A. Vese, C.L. Guyader, Variational Methods in Image Processing, in: Mathematical and Computational Imaging Sciences, Chapman\nand Hall/CRC, Boca Raton, 2016. [67] H. Brezis, Functional Analysis, Sobolev Spaces and Partial Differential Equations, in: Universitext, Springer, New York, 2011. [68] P.G. Ciarlet, Linear and Nonlinear Functional Analysis with Applications, SIAM, Philadelphia, 2013."
        }
    ],
    "title": "Neural control of discrete weak formulations: Galerkin, least squares & minimal-residual methods with quasi-optimal weights",
    "year": 2022
}