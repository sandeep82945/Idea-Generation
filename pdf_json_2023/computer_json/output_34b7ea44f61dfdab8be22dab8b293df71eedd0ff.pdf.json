{
    "abstractText": "Explainable AI (XAI) techniques have become popular for multiple use-cases in the past few years. Here we consider its use in studying model predictions to gather additional training data. We argue that this is equivalent to Active Learning, where the query strategy involves a human-in-theloop. We provide a mathematical approximation for the role of the human, and present a general formalization of the end-to-end workflow. This enables us to rigorously compare this use with standard Active Learning algorithms, while allowing for extensions to the workflow. An added benefit is that their utility can be assessed via simulation instead of conducting expensive userstudies. We also present some initial promising results.",
    "authors": [
        {
            "affiliations": [],
            "name": "Emma Thuong Nguyen"
        },
        {
            "affiliations": [],
            "name": "Abhishek Ghose"
        }
    ],
    "id": "SP:159b459bb4b4902ecfee95a7becb344eca1cf94b",
    "references": [
        {
            "authors": [
                "T. Akiba",
                "S. Sano",
                "T. Yanase",
                "T. Ohta",
                "M. Koyama"
            ],
            "title": "Optuna: A next-generation hyperparameter optimization framework",
            "venue": "In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
            "year": 2019
        },
        {
            "authors": [
                "T.N. Cardoso",
                "R.M. Silva",
                "S. Canuto",
                "M.M. Moro",
                "M.A. Gon\u00e7alves"
            ],
            "title": "Ranked batch-mode active learning",
            "venue": "Information Sciences,",
            "year": 2017
        },
        {
            "authors": [
                "D. Cer",
                "Y. Yang",
                "S. yi Kong",
                "N. Hua",
                "N.L.U. Limtiaco",
                "R.S. John",
                "N. Constant",
                "M. Guajardo-C\u00e9spedes",
                "S. Yuan",
                "C. Tar",
                "Y. hsuan Sung",
                "B. Strope",
                "R. Kurzweil"
            ],
            "title": "Universal sentence encoder",
            "year": 2018
        },
        {
            "authors": [
                "T. Danka",
                "P. Horvath"
            ],
            "title": "modAL: A modular active learning framework for Python. 2018",
            "venue": "URL https: //github.com/cosmic-cortex/modAL. available on arXiv at https://arxiv.org/abs/1805",
            "year": 2018
        },
        {
            "authors": [
                "B. Ghai",
                "Q.V. Liao",
                "Y. Zhang",
                "R. Bellamy",
                "K. Mueller"
            ],
            "title": "Explainable active learning (xal): Toward ai explanations as interfaces for machine teachers",
            "venue": "Proc. ACM Hum.-Comput. Interact.,",
            "year": 2021
        },
        {
            "authors": [
                "S. Kim",
                "J. Yi",
                "E. Kim",
                "S. Yoon"
            ],
            "title": "Interpretation of NLP models through input marginalization",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "R. Liaw",
                "E. Liang",
                "R. Nishihara",
                "P. Moritz",
                "J.E. Gonzalez",
                "I. Stoica"
            ],
            "title": "Tune: A research platform for distributed model selection and training",
            "venue": "arXiv preprint arXiv:1807.05118,",
            "year": 2018
        },
        {
            "authors": [
                "Q. Liu",
                "Y. Zhu",
                "Z. Liu",
                "Y. Zhang",
                "S. Wu"
            ],
            "title": "Deep active learning for text classification with diverse interpretations",
            "venue": "In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, CIKM",
            "year": 2021
        },
        {
            "authors": [
                "M. Ribeiro",
                "S. Singh",
                "C. Guestrin"
            ],
            "title": "why should I trust you?\u201d: Explaining the predictions of any classifier",
            "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,",
            "year": 2016
        },
        {
            "authors": [
                "Diego",
                "California",
                "June"
            ],
            "title": "Association for Computational Linguistics",
            "venue": "doi: 10.18653/v1/N16-3020. URL https://aclanthology.org/N16-3020.",
            "year": 2016
        },
        {
            "authors": [
                "T. Scheffer",
                "C. Decomain",
                "S. Wrobel"
            ],
            "title": "Active hidden markov models for information extraction",
            "venue": "Advances in Intelligent Data Analysis,",
            "year": 2001
        },
        {
            "authors": [
                "R. Sennrich",
                "B. Haddow",
                "A. Birch"
            ],
            "title": "Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "venue": "Association",
            "year": 2016
        },
        {
            "authors": [
                "B. Settles"
            ],
            "title": "Active learning literature survey",
            "venue": "Computer Sciences Technical Report 1648,",
            "year": 2009
        },
        {
            "authors": [
                "B. Shahriari",
                "K. Swersky",
                "Z. Wang",
                "R.P. Adams",
                "N. de Freitas"
            ],
            "title": "Taking the human out of the loop: A review of bayesian optimization",
            "venue": "Proceedings of the IEEE,",
            "year": 2015
        },
        {
            "authors": [
                "D.Z. Slack",
                "S. Hilgard",
                "S. Singh",
                "H. Lakkaraju"
            ],
            "title": "Reliable post hoc explanations: Modeling uncertainty in explainability",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "R. Socher",
                "A. Perelygin",
                "J. Wu",
                "J. Chuang",
                "C.D. Manning",
                "A. Ng",
                "C. Potts"
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2013
        },
        {
            "authors": [
                "F. Zhdanov"
            ],
            "title": "Diverse mini-batch active learning",
            "venue": "ArXiv, abs/1901.05954,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Keeping in pace with the popularity of Machine Learning (ML), the past few years has seen a surge in the desire to understand how a model makes decisions. In certain domains, such as healthcare and law enforcement, such transparency is critical in acquiring the trust of its users. In others, it serves as a way to understand potential shortcomings of a system, e.g., if a system has overfit the data. This has led to accelerated research in the area of Explainable AI (XAI), which studies explaining of predictions from a given model, e.g. LIME (Ribeiro et al., 2016), SHAP (Lundberg & Lee, 2017), DeepLIFT (Shrikumar et al., 2017).\nHere we consider the latter use of XAI: that of improving an ML classifier. We specifically look at the use of explanations to identify data that we deem \u201cinteresting\u201d in some way, that is then used to further train our model. As an example, consider the workflow shown in Figure 1, describing the following sequence of events:\n*Equal contribution 1[24]7.ai, California, USA. Correspondence to: Emma Thuong Nguyen <emma.nguyen@247.ai>, Abhishek Ghose <abhishek.ghose@247.ai>.\nAI & HCI Workshop at the 40 th International Conference on Machine Learning (ICML), Honolulu, Hawaii, USA. 2023. Copyright 2023 by the author(s).\n1. An ML practitioner trains a supervised classifier on an initial labeled dataset - (Xorig, Yorig) - and deploys it (in Figure 1, this is shown by A). The model is referred to as Model v1.\n2. Users of this system interact with it, and in the process, incrementally generate (unlabeled) data, Xinc. Shown by B in Figure 1.\n3. The ML practitioner periodically inspects the system for correctness. She samples from Xinc and uses an explainer to review the model\u2019s decision process. Shown by C in Figure 1 - note that the model is required as an input to the explainer.\nSome explanations might indicate unintended behavior of the model. For example, both these reviews may be classified as positive, where the explainer has underlined the words that most influenced the classifier\u2019s decision:\n\u2022 I love the food here! \u2022 I gave them a 1-star rating - that\u2019s how much I\nlike the food here.\nOf course, the second review is sarcastic, and should be identified as negative.\n4. The ML practitioner decides to sample more such examples from Xinc (shown by D in Figure 1), and then has them labeled by human annotators (shown by E). This new dataset1 is denoted by (Xnew, Ynew), and is used to further train the model to obtain Model v2 (shown by F).\nThis process is repeated multiple times to generate improved versions of models. Figure 1 shows one such iteration. While this process seems intuitively appealing, we make rigorous the following aspects:\n1. First, we claim that this process essentially is Active Learning (Settles, 2009) that involves a human-in-theloop (Section 2).\n1This new dataset may be seen to contain only the newly identified instances if the model may be incrementally trained, or a combination of the original data and the new instances, if the model needs to be trained from scratch. We will adopt the latter convention here since its universal, i.e., not all models support incremental training.\nar X\niv :2\n30 6.\n13 93\n5v 2\n[ cs\n.A I]\n1 5\nJu l 2\n02 3\n2. Then, we mathematically formulate the workflow, which makes it convenient to (a) quantify its utility, and (b) compare with other AL techniques such as margin-based sampling (Scheffer et al., 2001) (Section 3).\nAn added benefit is such workflows may be evaluated via simulation bypassing the need for conducting timeconsuming or expensive user studies.\nOur primary interest is text classification, but much of the discussion here applies to other forms of data as well.\nRelated Work: Based on a thorough search of the relevant literature, we believe this is the first study that casts human-in-the-loop data selection based on explanations as an AL query strategy. Other intersections of AL and XAI have been studied however, e.g., Liu et al. (2021) uses certain properties of local explanations to determine instanceinformativeness for querying, Ghai et al. (2021) studies the benefits of annotating queried instances with explanations to obtain rich feedback from a human labeler."
        },
        {
            "heading": "2. XAI-based Data Selection is Active Learning",
            "text": "In many situations, while abundant unlabeled data is available, labeled data may be hard to procure, e.g., when manual annotation by experts is required. In such cases, one needs to explicitly account for the label acquisition cost. The Active Learning (AL) family of techniques solves for this\nproblem in the following way:\n1. An initial model is built by acquiring labels for a small batch of data. This batch may be randomly selected from the unlabeled pool of data.\n2. The model is then iteratively improved by strategically selecting data (from the unlabeled pool) to be annotated. This data is then used to further train the model. Such a strategy or query strategy2 picks instances that have the greatest influence on the model\u2019s accuracy. Some popular query strategies are entropy sampling and maximum margin sampling.\nInformally, the query strategy is a mechanism to identify maximally useful instances given (a) the current model, and (b) an unlabeled pool of data. Referring to Figure 1, we observe the following components effectively form a query strategy:\n\u2022 The explainer used to detect surprising patterns in model predictions (shown by C in Figure 1).\n\u2022 The process of using the explanations to solicit further instances from the unlabeled pool Xinc (D in Figure 1).\nSpecifically, this is the batch AL setting, where batches of data are iteratively identified, labeled and used to train the\n2So called because it is used to query instances from the unlabeled pool.\nmodel, e.g., BatchBALD (Kirsch et al., 2019). AL may be used in various other settings as well, such as stream-based - see Settles (2009) for an overview."
        },
        {
            "heading": "3. Mathematical Formulation",
            "text": "How do we compare this form of AL with standard AL techniques? Clearly, a challenge is that because there is a human-in-the-loop - the ML practitioner - this workflow needs to be tested with expensive or time consuming userstudies. In this section, we try to eliminate this roadblock by (1) providing a reasonable approximation for the task of the ML practitioner, and (2) offering a concise representation for the overall workflow. This makes it possible to efficiently simulate the workflow from Figure 1.\nWe introduce some notation first:\n1. We will denote the number of instances in the collection Xa by Na. We will also assume that our data resides in d dimensions, i.e., Xorig \u2208 RNorig\u00d7d, Xinc \u2208 RNinc\u00d7d and Xnew \u2208 RNnew\u00d7d.\n2. We will assume explanations are produced in d\u2032 dimensions. The case of d \u0338= d\u2032 is common for text explainers where the text input that a model sees maybe in form of n-grams or Byte Pair Encoding (BPE) (Sennrich et al., 2016) vectors, whereas the explanation might be in an \u201cinterpretable\u201d space such as presence/absence of words. For tabular data d = d\u2032. We\u2019ll use the \u201c\u2032\u201d superscript to denote data in the explanation space, e.g., X \u2032orig \u2208 RNorig\u00d7d \u2032 .\n3. Models Model v1 and Model v2 are denoted by the function f , parameterized by \u03a81 and \u03a82 respectively3. As examples, \u03a8 may be coefficients in Logistic Regression or weights in a Neural Network.\n4. The explainer is denoted by the function E(x; \u03b8,\u03a8), where x \u2208 Rd is an instance for which an explanation for its prediction by model \u03a8 is sought. The explainer itself has parameters \u03b8, such as the number of features to be used in explanations (Ribeiro et al., 2016).\n5. The explanation is a vector of weights q \u2208 Rd\u2032 that explains the input x in the explanation space, i.e., it applies to x\u2032 \u2208 Rd\u2032 . Intuitively, these weights indicate the importance of the corresponding feature.\nWhile this specific format for explanations is an assumption, it is common (Ribeiro et al., 2016; Lundberg & Lee, 2017; Kim et al., 2020; Slack et al., 2021) and allows our formulation to be broadly applicable.\n3As mentioned earlier, we discuss only one iteration of model improvement, but this discussion applies to the general case of learning \u03a8i+1, given \u03a8i.\n6. Finally, we account for two practical constraints in our setup:\n(a) BE , explanation budget: the number of instances whose explanations an ML practitioner might manually study.\n(b) BL, labeling budget: the number of instances that annotators can label within one iteration. This is equivalent to the batch size in AL.\nTypically, for real-world systems, BE < BL < Ninc. Some representative numbers are: BE is in the order of hundreds, BL is in the order of hundreds to thousands (depending on the labeling cost, e.g., skill required, number of annotators), and Ninc may be arbitrarily large, potentially running into millions of instances."
        },
        {
            "heading": "3.1. Task Formulation",
            "text": "Given the above notation, we now revisit the workflow from Figure 1:\n1. Step C in Figure 1: Explanations E(xi; \u03b8,\u03a81) are sought for instances xi \u2208 Xs, where Xs \u2286 Xinc is a set of instances randomly selected from Xinc, such that its size Ns does not exceed the explanation budget BE . For each instance xi \u2208 Rd, an explanation weight vector qi \u2208 Rd \u2032 is produced.\n2. Step D, representing unintended model behavior: Based on studying the explanations, the ML practitioner identifies instances in Xs that indicate model behavior that is either unintended or in some sense, surprising. An example is that different labels are predicted for a pair of instances that are either similar or produce similar explanations. Intuitively, this might mean the model requires more such instances to confidently tell them apart.\nRecall that the practitioner\u2019s goal is to select instances from Xinc similar to the ones that participate in such pairs in Xs. And, since she can select only up to BL instances, we want to favour instances in Xs that participate in a large number of such pairs.\nWe represent this in the following manner (note that all x appearing below belong to Xs):\n\u2022 Let matrix A \u2208 RNs\u00d7Ns represent similarity between instances - either in terms of the vectors themselves, or their explanations. We combine them in the following way:\nAij = (qi \u2299 x\u2032i) \u00b7 (qj \u2299 x\u2032j)T (1)\nThe \u201c\u2299\u201d symbol represents the element-wise product and the \u201c\u00b7\u201d symbol denotes the dot product.\n\u2022 Let B \u2208 RNs\u00d7Ns represent whether predicted labels are identical:\nBij = { 1 f(xi,\u03a81) \u0338= f(xj ,\u03a81) 0 otherwise (2)\nNote that pairs of instances are assigned a value of 1 when the predicted labels are not identical. \u2022 Finally, given the identity vector 1Ns \u2208 RNs\u00d71, we4 define C \u2208 RNs :\nC = (A\u2299B)1Ns (3)\nConsider the values for A\u2299B: (a) +ve values indicate pairs of instances with dif-\nferent predictions, i.e., Bij = 1, but similar explanations, i.e., Aij > 0 (preferred for retrieval).\n(b) -ve values indicate different predictions, i.e., Bij = 1, but also different explanations, i.e., Aij < 0 (not preferred). (c) 0 values either indicate same predictions, i.e., Bij = 0 or different explanations , i.e., Aij = 0 (not preferred).\nCi provides a row-wise sum for instances xi in A\u2299B, quantifying the extent to which they are preferred during retrieval. As an examples, consider for Ns = 3:\nA =  1 0.7 0.80.7 1 0.3 0.8 0.3 1  , B = 0 1 11 0 0 1 0 0  (4) A\u2299B =\n 0 0.7 0.80.7 0 0 0.8 0 0  (5) C = (A\u2299B)1Ns = 1.50.7 0.8\n (6) Here, as per B, x1 and x2 have the same predicted label, which is different from that of x0. As per A, x0 also has high explanation similarities to both x1 and x2. The desirability of x0 is reflected in its high value in C. Intuitively, C indicates preference weights for instances in Xs, to be used as queries for retrieval.\n3. Step D (continued), retrieving instances from Xinc: We now select BL instances from Xinc based on C. This is a human-in-the-loop activity, which we assume may be approximated with similarities via dot products. We detail this below.\n4The identity vector has a subscript that denotes its length. Also, we will assume, vectors are column vectors.\nWe compute the following score matrix S \u2208 RNinc\u00d7Ns :\nS = X \u2032inc(CITd\u2032 \u2299X \u2032s)T (7)\nNote that X \u2032inc and X \u2032 s are representations in the explanation space. CITd\u2032 \u2299X \u2032s multiplies each vector in Xs with the corresponding weight in C. Finally, S computes the similarity, i.e., dot-product, between vectors in Xinc and these weighted vectors from X \u2032s.\nTo continue with our example, let\u2019s consider the following X \u2032s with d \u2032 = 2:\nX \u2032s =  0.7 0.20.34 1.15 \u22120.1 3  , (8) Then,\nCITd\u2032 = 1.50.7 0.8  [11] = 1.5 1.50.7 0.7 0.8 0.8  (9) CITd\u2032 \u2299X \u2032s =  1.5\u00d7 0.7 1.5\u00d7 0.20.7\u00d7 0.34 0.7\u00d7 1.15 0.8\u00d7\u22120.1 0.8\u00d7 3\n , (10) S is a symmetric matrix, where Sij denotes the similarity between x\u2032i \u2208 X \u2032inc and x\u2032j \u2208 X \u2032s (or indirectly, xi and xj), accounting for the preferences encoded by C.\nTo obtain the overall retrieval desirability for an instance in X \u2032inc, we compute its row-wise sum. We define the retrieval weights W \u2208 RNinc as:\nW = S1Ns (11)\nWe select the top BL instances from Xinc based on W . We will refer to these instances as Xtop \u2208 RBL\u00d7d.\n4. Steps E and F: We obtain labels Ytop \u2208 RBL corresponding to Xtop via human annotation, and construct the following new dataset:\nXnew = [ Xorig Xtop ] , Xnew \u2208 R(Norig+BL)\u00d7d\nYnew = [ Yorig Ytop ] , Ynew \u2208 R(Norig+BL) (12)\n(Xnew, Ynew) is used to retrain f obtaining new parameters \u03a82.\nThese steps can be condensed into a single expression - given matrices A (this makes use of explanations) and B, the top-BL instances from Xinc based on these weights are picked:\nW = X \u2032inc(((A\u2299B)1Ns)ITd\u2032 \u2299X \u2032s)T1Ns (13)"
        },
        {
            "heading": "3.2. Objective",
            "text": "We want to minimize the generalization loss of f . In an AL setting the only labeled data available is (Xnew, Ynew) (Equation 12) and therefore, a validation set must be sampled from it. To keep the notation simple, we use Lv to denote generalization loss, i.e., loss on a validation set, and our objective as:\nmin \u03b8,\u03a8\nLv(Xorig, Yorig, Xinc, \u03b8,\u03a8) (14)\nNote here that we optimize for the explanation parameters \u03b8 as well to refine them to be helpful in the data selection process, i.e., construction of (Xnew, Ynew)."
        },
        {
            "heading": "3.3. Metrics",
            "text": "To measure competitiveness against standard AL approaches, we hold out a labeled test set (Xtest, Ytest) that is large enough to reflect the true distribution. Such a dataset is not available in real-world AL setups, and is used here to measure the true accuracy of a model. We report model accuracy scores on this dataset, at various iterations of data being sampled from Xinc."
        },
        {
            "heading": "3.4. Extensions",
            "text": "While we looked at at one form of unintended behavior here - different predictions but similar explanations - it is possible to define others. These may be defined based on (Xorig, Yorig) as well. Some examples are:\n1. For an instance xi \u2208 Xorig , the predicted label is incorrect, but the explanations for both predicted label and true label are similar. It is possible to specify this behavior only if the explainer can generate explanations for membership to any class, e.g., LIME, SHAP.\nIntuition: the model is misaligned with respect to its mapping from features to labels.\n2. For a pair of instances with different true labels, the explanations for membership to these labels are similar.\nIntuition: the model is unable to strongly discriminate between the two classes for these instances, and might need more similar instances to learn.\nThese criteria can be easily included in our formulation by appropriately defining matrices A and B."
        },
        {
            "heading": "3.5. Review of Assumptions",
            "text": "In our formulation we make two assumptions:\n\u2022 The format of explanations as a weight vector. As mentioned earlier, this is indeed a common format, and allows the formulation to be broadly applicable.\n\u2022 Approximating the human-in-the-loop process with retrieval based on dot-product based similarities. While this is probably reasonable, we require user studies to validate its adequacy.\nWe note that the above formulation is generic and applicable to different kinds of data, e.g., text, images, tabular, different explainers, as well as different models."
        },
        {
            "heading": "4. Experiments",
            "text": "We have begun empirical comparisons to standard AL techniques. While our goal is to cover a diverse set of data, models and explainers, we present initial results on the dataset SST-5 (Socher et al., 2013) using SHAP (Lundberg & Lee, 2017), specifically Partition SHAP, as the explainer and Support Vector Machine with linear kernel as our model (the scikit-learn (Buitinck et al., 2013) library is used). We use the F1-macro score to report accuracy; this metric is used since it accounts for class-wise accuracies even when there is class imbalance. The optimization in Equation 14 is solved using Bayesian Optimization (Shahriari et al., 2016), since they enable us to minimize non-differentiable functions. This is an important consideration since we are not guaranteed differentiability, e.g., when using a Decision Tree as our model. We specifically use the Ray Tune (Liaw et al., 2018) and Optuna (Akiba et al., 2019) libraries. For AL, we use the modAL library (Danka & Horvath, 2018).\nFor reasons of tractability, the joint optimization over \u03b8 and \u03a8 (Equation 14) is decomposed into the following nested optimization:\n1. The search space of the explanation parameters \u03b8 is explored by the Bayesian Optimizer. The SHAP parameters we varied are maximum number of predictions which model f makes for explaining one instance and maximum number of predictions in one model invocation.\n2. The model selection search space is explored by standard cross-validation with grid-search over hyperparameters: in this case, the regularization coefficient C.\nOther relevant details:\n1. The text representation used is Universal Sentence Encoding (USE) (Cer et al., 2018).\n2. Experiment settings:\n\u2022 Norig = 100, Ninc = 2900, Ntest = 2000.\n\u2022 Labeling budget (or batch size in AL), BL = 200, explanation budget BL = 200.\n3. Comparisons against:\n\u2022 AL query strategies: entropy-based sampling5, margin-based sampling6 (Scheffer et al., 2001). See Settles (2009) for an overview.\n\u2022 Baseline: we use a random strategy, which selects BL instances from Xinc uniformly at random with no replacement.\nWe visualize our results in Figure 2. \u201cAL\u201d or \u201cEXP\u201d in the legend denote whether a strategy comes from standard AL or is based on an explainer. We observe that the explanation based query strategy performs better than other standard AL techniques. It achieves higher scores right at the first few iterations and reaches a plateau in performance. We also note that the AL query strategies are not significantly better than random selection."
        },
        {
            "heading": "5. Conclusions and Future Work",
            "text": "In this short paper, we investigated a specific use of XAI: using it to select model training data. We showed that\n5This selects instances with high entropy values over prediction probabilities across classes.\n6Selects instances that have a small difference between the prediction probabilities of the two most confident classes.\nthis is equivalent to performing Active Learning, with a human-in-the-loop as part of the query strategy. Further, we mathematically approximated this workflow, so that it may be conveniently studied and empirically compared to other Active Learning techniques. We presented some initial results; these look promising, and we hope to continue the empirical analyses to definitively establish the utility of XAI in this setup.\nOur future work would focus on: (a) validating our approximation for the human-in-the-loop process via user-studies, (b) broadening the scope of this study by using different classifiers, text representations, datasets and AL techniques, especially those recently proposed, e.g., Zhdanov (2019), Cardoso et al. (2017), and (c) exploiting the differentiability of our formulation (Equation 13) to learn an AL strategy."
        }
    ],
    "title": "Are Good Explainers Secretly Human-in-the-Loop Active Learners?",
    "year": 2023
}