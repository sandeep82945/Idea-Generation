{
    "abstractText": "XLSR-53, a multilingual model of speech, builds a vector representation from audio, which allows for a range of computational treatments. The experiments reported here use this neural representation to estimate the degree of closeness between audio files, ultimately aiming to extract relevant linguistic properties. We use max-pooling to aggregate the neural representations from a \u2018snippet-lect\u2019 (the speech in a 5-second audio snippet) to a \u2018doculect\u2019 (the speech in a given resource), then to dialects and languages. We use data from corpora of 11 dialects belonging to 5 less-studied languages. Similarity measurements between the 11 corpora bring out greatest closeness between those that are known to be dialects of the same language. The findings suggest that (i) dialect/language can emerge among the various parameters characterizing audio files and (ii) estimates of overall phonetic/phonological closeness can be obtained for a little-resourced or fully unknown language. The findings help shed light on the type of information captured by neural representations of speech and how it can be extracted from these representations.",
    "authors": [
        {
            "affiliations": [],
            "name": "S\u00e9verine Guillaume"
        },
        {
            "affiliations": [],
            "name": "Guillaume Wisniewski"
        },
        {
            "affiliations": [],
            "name": "Alexis Michaud"
        }
    ],
    "id": "SP:90dcd77a7cfb1719846d80a262fc7c392338f998",
    "references": [
        {
            "authors": [
                "A. Conneau",
                "A. Baevski",
                "R. Collobert",
                "A. Mohamed",
                "M. Auli"
            ],
            "title": "Unsupervised cross-lingual representation learning for speech recognition",
            "venue": "Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association, Brno, Czechia, 30 August - 3 September 2021, H. Hermansky, H. Cernock\u00fd, L. Burget, L. Lamel, O. Scharenborg, and P. Motl\u00edcek, Eds. ISCA, 2021, pp. 2426\u20132430. [Online]. Available: https://doi.org/10.21437/Interspeech.2021-329",
            "year": 2021
        },
        {
            "authors": [
                "M. Moradshahi",
                "H. Palangi",
                "M.S. Lam",
                "P. Smolensky",
                "J. Gao"
            ],
            "title": "HUBERT untangles BERT to improve transfer across NLP tasks",
            "venue": "CoRR, vol. abs/1910.12647, 2019. [Online]. Available: http://arxiv.org/abs/1910.12647",
            "year": 1910
        },
        {
            "authors": [
                "M. Bartelds",
                "W. de Vries",
                "F. Sanal",
                "C. Richter",
                "M. Liberman",
                "M. Wieling"
            ],
            "title": "Neural representations for modeling variation in speech",
            "venue": "Journal of Phonetics, vol. 92, pp. 101\u2013137, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S.A. Fulop"
            ],
            "title": "The beginning of time-frequency analysis",
            "venue": "The Journal of the Acoustical Society of America, vol. 152, no. 5, pp. R9\u2013R10, 11 2022. [Online]. Available: https://doi.org/10.1121/10.0014987",
            "year": 2022
        },
        {
            "authors": [
                "G. Fant"
            ],
            "title": "Acoustic theory of speech production, with calculations based on X-ray studies of Russian articulations",
            "year": 1960
        },
        {
            "authors": [
                "N. San",
                "M. Bartelds",
                "M. Browne",
                "L. Clifford",
                "F. Gibson",
                "J. Mansfield",
                "D. Nash",
                "J. Simpson",
                "M. Turpin",
                "M. Vollmer",
                "S. Wilmoth",
                "D. Jurafsky"
            ],
            "title": "Leveraging pre-trained representations to improve access to untranscribed speech from endangered languages",
            "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2021, pp. 1094\u20131101.",
            "year": 2021
        },
        {
            "authors": [
                "S. Guillaume",
                "G. Wisniewski",
                "C. Macaire",
                "G. Jacques",
                "A. Michaud",
                "B. Galliot",
                "M. Coavoux",
                "S. Rossato",
                "M.-C. Nguy\u00ean",
                "M. Fily"
            ],
            "title": "Fine-tuning pre-trained models for Automatic Speech Recognition: experiments on a fieldwork corpus of Japhug (Trans-Himalayan family)",
            "venue": "Proceedings of the Fifth Workshop on the Use of Computational Methods in the Study of Endangered Languages. Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 170\u2013178. [Online]. Available: https://aclanthology.org/2022.computel-1.21",
            "year": 2022
        },
        {
            "authors": [
                "J. Good",
                "M. Cysouw"
            ],
            "title": "Languoid, doculect, and glossonym: formalizing the notion \u2018language",
            "venue": "Language Documentation & Conservation, vol. 7, pp. 331\u2013359, 2013. [Online]. Available: http://hdl.handle.net/10125/4606",
            "year": 2013
        },
        {
            "authors": [
                "G. Alain",
                "Y. Bengio"
            ],
            "title": "Understanding intermediate layers using linear classifier probes",
            "venue": "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24- 26, 2017, Workshop Track Proceedings. OpenReview.net, 2017. [Online]. Available: https://openreview.net/forum?id=HJ4-rAVtl",
            "year": 2017
        },
        {
            "authors": [
                "A. Tjandra",
                "D.G. Choudhury",
                "F. Zhang",
                "K. Singh",
                "A. Conneau",
                "A. Baevski",
                "A. Sela",
                "Y. Saraf",
                "M. Auli"
            ],
            "title": "Improved language identification through cross-lingual self-supervised learning",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore, 23-27 May 2022. IEEE, 2022, pp. 6877\u20136881. [Online]. Available: https://doi.org/10.1109/ICASSP43922.2022.9747667",
            "year": 2022
        },
        {
            "authors": [
                "Z. Fan",
                "M. Li",
                "S. Zhou",
                "B. Xu"
            ],
            "title": "Exploring wav2vec 2.0 on speaker verification and language identification",
            "venue": "Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association, Brno, Czechia, 30 August - 3 September 2021, H. Hermansky, H. Cernock\u00fd, L. Burget, L. Lamel, O. Scharenborg, and P. Motl\u00edcek, Eds. ISCA, 2021, pp. 1509\u20131513. [Online]. Available: https://doi.org/10.21437/Interspeech.2021-1280",
            "year": 2021
        },
        {
            "authors": [
                "J. Valk",
                "T. Alum\u00e4e"
            ],
            "title": "VoxLingua107: a dataset for spoken language recognition",
            "venue": "Proc. IEEE SLT Workshop, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "E.M. Bender"
            ],
            "title": "Linguistically na\u00efve!= language independent: Why NLP needs linguistic typology",
            "venue": "Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics: Virtuous, Vicious or Vacuous?, 2009, pp. 26\u201332.",
            "year": 2009
        },
        {
            "authors": [
                "A. Michaud",
                "O. Adams",
                "T. Cohn",
                "G. Neubig",
                "S. Guillaume"
            ],
            "title": "Integrating automatic transcription into the language documentation workflow: Experiments with Na data and the Persephone toolkit",
            "venue": "Language Documentation & Conservation, vol. 12, pp. 393\u2013429, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "D. van Esch",
                "B. Foley",
                "N. San"
            ],
            "title": "Future directions in technological support for language documentation",
            "venue": "Proceedings of the Workshop on Computational Methods for Endangered Languages, vol. 1, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "E. Prud\u2019hommeaux",
                "R. Jimerson",
                "R. Hatcher",
                "K. Michelson"
            ],
            "title": "Automatic speech recognition for supporting endangered language documentation",
            "venue": "Language documentation and conservation, vol. 15, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R. Cotterell",
                "J. Eisner"
            ],
            "title": "Probabilistic typology: Deep generative models of vowel inventories",
            "venue": "arXiv preprint arXiv:1705.01684, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "C. Chagnaud",
                "P. Garat",
                "P.-A. Davoine",
                "E. Carpitelli",
                "A. Vincent"
            ],
            "title": "Shinydialect: a cartographic tool for spatial interpolation of geolinguistic data",
            "venue": "Proceedings of the 1st ACM SIGSPATIAL workshop on Geospatial Humanities, 2017, pp. 23\u201330.",
            "year": 2017
        },
        {
            "authors": [
                "E.M. Ponti",
                "H. O\u2019horan",
                "Y. Berzak",
                "I. Vuli\u0107",
                "R. Reichart",
                "T. Poibeau",
                "E. Shutova",
                "A. Korhonen"
            ],
            "title": "Modeling language variation and universals: A survey on typological linguistics for natural language processing",
            "venue": "Computational Linguistics, vol. 45, no. 3, pp. 559\u2013601, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A. Suni",
                "M. Wlodarczak",
                "M. Vainio",
                "J. Simko"
            ],
            "title": "Comparative analysis of prosodic characteristics using wavenet embeddings",
            "venue": "20th Annual Conference of the International Speech Communication Association (INTERSPEECH 2019). ISCA, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M. De Seyssel",
                "G. Wisniewski",
                "E. Dupoux",
                "B. Ludusan"
            ],
            "title": "Investigating the usefulness of i-vectors for automatic language characterization",
            "venue": "Speech Prosody 2022-11th International Conference on Speech Prosody, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Bjerva",
                "I. Augenstein"
            ],
            "title": "Tracking typological traits of Uralic languages in distributed language representations",
            "venue": "arXiv preprint arXiv:1711.05468, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "F. Pedregosa",
                "G. Varoquaux",
                "A. Gramfort",
                "V. Michel",
                "B. Thirion",
                "O. Grisel",
                "M. Blondel",
                "P. Prettenhofer",
                "R. Weiss",
                "V. Dubourg",
                "J. Vanderplas",
                "A. Passos",
                "D. Cournapeau",
                "M. Brucher",
                "M. Perrot",
                "E. Duchesnay"
            ],
            "title": "Scikit-learn: Machine learning in Python",
            "venue": "Journal of Machine Learning Research, vol. 12, pp. 2825\u20132830, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "R. Schaeffer",
                "B. Miranda",
                "S. Koyejo"
            ],
            "title": "Are emergent abilities of Large Language Models a mirage?",
            "venue": "arXiv preprint arXiv:2304.15004,",
            "year": 2023
        },
        {
            "authors": [
                "B. Michailovsky",
                "M. Mazaudon",
                "A. Michaud",
                "S. Guillaume",
                "A. Fran\u00e7ois",
                "E. Adamou"
            ],
            "title": "Documenting and researching endangered languages: the Pangloss Collection",
            "venue": "Language Documentation & Conservation, vol. 8, pp. 119\u2013135, 2014. [Online]. Available: https://shs.hal.science/halshs-01003734",
            "year": 2014
        },
        {
            "authors": [
                "H. Hammarstr\u00f6m"
            ],
            "title": "Glottolog: A free, online, comprehensive bibliography of the world\u2019s languages",
            "venue": "3rd International Conference on Linguistic and Cultural Diversity in Cyberspace. UNESCO, 2015, pp. 183\u2013188.",
            "year": 2015
        },
        {
            "authors": [
                "G. Jacques",
                "A. Michaud"
            ],
            "title": "Approaching the historical phonology of three highly eroded Sino-Tibetan languages: Naxi, Na and Laze",
            "venue": "Diachronica, vol. 28, no. 4, pp. 468\u2013498, 2011. [Online]. Available: https://shs.hal.science/halshs-00537990",
            "year": 2011
        },
        {
            "authors": [
                "M. de Seyssel",
                "M. Lavechin",
                "Y. Adi",
                "E. Dupoux",
                "G. Wisniewski"
            ],
            "title": "Probing phoneme, language and speaker information in unsupervised speech representations",
            "venue": "Interspeech 2022 - 23rd INTERSPEECH Conference, Incheon, South Korea, Sep. 2022. [Online]. Available: https://hal.inria.fr/hal-03830470",
            "year": 2022
        },
        {
            "authors": [
                "V.D. Lai",
                "N.T. Ngo",
                "A.P.B. Veyseh",
                "H. Man",
                "F. Dernoncourt",
                "T. Bui",
                "T.H. Nguyen"
            ],
            "title": "ChatGPT beyond English: Towards a comprehensive evaluation of Large Language Models in multilingual learning",
            "venue": "arXiv preprint arXiv:2304.05613, 2023.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 5.\n18 60\n2v 1\n[ cs\n.C L\n] 2\n9 M\nay 2\n02 3\nsentation from audio, which allows for a range of computational treatments. The experiments reported here use this neural representation to estimate the degree of closeness between audio files, ultimately aiming to extract relevant linguistic properties. We use max-pooling to aggregate the neural representations from a \u2018snippet-lect\u2019 (the speech in a 5-second audio snippet) to a \u2018doculect\u2019 (the speech in a given resource), then to dialects and languages. We use data from corpora of 11 dialects belonging to 5 less-studied languages. Similarity measurements between the 11 corpora bring out greatest closeness between those that are known to be dialects of the same language. The findings suggest that (i) dialect/language can emerge among the various parameters characterizing audio files and (ii) estimates of overall phonetic/phonological closeness can be obtained for a little-resourced or fully unknown language. The findings help shed light on the type of information captured by neural representations of speech and how it can be extracted from these representations. Index Terms: pre-trained acoustic models, language documentation, under-resourced languages, similarity estimation\n1. Introduction\nThe present research aims to contribute to a recent strand of research: exploring how pre-trained multilingual speech representation models like XLSR-53 [1] or HuBERT [2] can be used to assist in the linguistic analysis of a language [3]. XLSR-53, a multilingual model of speech, builds a vector representation from an audio signal. The neural representation is different in structure from that of the audio recording. Whereas wav (PCM) audio consists in a vector of values in the range [-1:+1], at a bit-depth from 8 to 32 and a sampling rate on the order of 16,000 Hz, the XLSR-53 neural representation contains 1,024 components, at a rate of 47 frames per second. The size of the vector representation is on the same order of magnitude as that of the audio snippet, and the amount of information can be hypothesized to be roughly comparable. But the neural representation, unlike the audio format, comes in a vector form that is tractable to a range of automatic treatments building on the vast body of work in data mining and machine learning. The neural representation of speech holds potential for an epistemological turning-point comparable to the introduction of the spectrogram 8 decades ago [4, 5, 6].\nThe experiments reported here use the neural representation yielded by XLSR-53 (used off-the-shelf, without fine-tuning, unlike [7, 8]) as a means to characterize audio: estimating the degree of closeness between audio signals, and (ultimately) extracting relevant linguistic properties, teasing them apart from other types of information, e.g. technical characteristics\nof the recordings. We start out from 5-second audio snippets, and we pool neural representations (carrying out mean pooling, i.e. averaging across frames) to progress towards the level of the entire audio file, then the entire corpus (containing several audio files). We thereby gradually broaden the scope of the neural representation from a \u2018snippet-lect\u2019 (the speech present in an audio snippet1) to a \u2018doculect\u2019 (a linguistic variety as it is documented in a given resource [9]), then towards \u2018dialects\u2019 (other groupings could also be used: by sociolect, by speaking style/genre, etc.) and, beyond, entire languages.\nIn a set of exploratory experiments, we build neural representations of corpora of 11 dialects that belong to 5 under-resourced languages. We then use linguistic probes [10] (i.e. a multiclass classifier taking as input the frozen neural representation of an utterance and assigning it to a language, similarly to a language identification system) to assess the capacity of XLSR-53 to capture language information. Building on these first results, we propose to use our probe on languages not present in the training set and to use its decisions as a measure of similarity between two languages, following the intuition that if an audio segment of an unknown language is identified as being of language A, then the language in the audio segment is \u201cclose\u201d to A.\nRepresentations like XLSR-53 have already been used to develop language identification systems (e.g. [11, 12]), but their use in the context of under-resourced languages and linguistic fieldwork datasets raises many challenges. First, there is much less data available for training and testing these systems both in terms of number of hours of audio and number of speakers. For instance, VoxLingua [13], a dataset collected to train language identification models, contains 6, 628 hours of recordings in 107 languages, i.e. at least an order of magnitude more data per language than typical linguistic fieldwork corpora. Second, the languages considered in a language documentation context have not been used for (pre-)training speech representations and have linguistic characteristics that are potentially very different from the languages used for pre-training them (on consequences of narrow typological scope for Natural Language Processing research, see [14]). The ability of models such as XLSR-53 to correctly represent these languages remains an open question. We also aim to assess to what extent pre-trained models of speech can address these two challenges.\nSimilarity measurements between the 11 corpora bring out greatest closeness between those that are known to be dialects of the same language. Our findings suggest that dialect/language can emerge among the many parameters characterizing audio files as captured in XLSR-53 representations (which also include acoustic properties of the environment, technical characteristics\n1\u2018Snippet-lect\u2019 is coined on the analogy of \u2018doculect\u2019 [9], to refer to the characteristics of a 5-second audio snippet.\nof the recording equipment, speaker ID, speaker gender, age, social group, as well as style of speech: speaking rate, etc.), and that there is potential for arriving at useful estimates of phonetic/phonological closeness. The encouraging conclusion is that, even in the case of a little-resourced or fully unknown language, \u2018snippet-lects\u2019 and \u2018doculects\u2019 can be placed relative to other speech varieties in terms of their closeness.\nAn estimation of closeness between speech signals can have various applications. For computational language documentation [15, 16, 17, 8], there could be benefit in a tool for finding closest neighbours for a newly documented language (with a view to fine-tuning extant models for the newly documented variety, for instance), bypassing the need for explicit phoneme inventories, unlike in [18]. For dialectology, a discipline that traditionally relies on spatial models based on isogloss lines [19], neural representations of audio signals for cognate words allow for calculating a phonetic-phonological distance along a dialect continuum [3]; our work explores whether cross-dialect comparison of audio snippets containing different utterances also allows for significant generalizations. Last but not least, for the community of speech researchers, the task helps shed light on the type of information captured by neural representations of speech and how it can be extracted from these representations. This work is intended as a stepping-stone towards the mid-term goal of leveraging neural representations of speech to extract typological features from neural representations of speech signals: probing linguistic information in neural representations, to arrive at data-driven induction of typological knowledge [20]. Note that our work is speech-based, like [21, 22], and unlike text-based research predicting typological features (e.g. [23]).\nThis article is organized as follows. In Section 2 we introduce our system. In Section 3 we briefly review the languages used in our experiments. Finally, we report our main experimental findings in Section 4."
        },
        {
            "heading": "2. Probing Language Information in Neural Representations",
            "text": "Predicting the language of a spoken utterance can, formally, be seen as a multi-class classification task that aims at mapping an audio snippet represented by a feature vector to one of the language labels present in the training set. Our implementation of this principle is very simple: we use 5-second audio snippets and use, as feature vector, the representation of the audio signal built by XLSR-53, a cross-lingual speech representation that results from pre-training a single Transformer model from the raw waveform of speech in multiple languages [1]. XLSR-53 is a sequence-to-sequence model that transforms an audio file (a sequence of real numbers along the time dimension) into a sequence of vectors of dimension 1,024 sampled at 47 Hz (i.e. it outputs 47 vectors for each second of audio). We use max-pooling to aggregate these vectors and map each audio snippet to a single vector. In all our experiments, we use a logistic regression (as implemented in the sklearn library [24]) as the multi-class classifier with \u21132 regularization.\nImportantly, our language identification system uses the representations built by XLSR-53 without ever modifying them and is therefore akin to a linguistic probe [10]. We do not carry out fine-tuning of a pre-trained model. Language identification is a well-established task in the speech community and has been the focus of much research; our work does not aim at developing a state-of-the-art language identification model, but at showing that neural representations encode language\ninformation, and that this information can be useful for language documentation and analysis. Said differently, we do not aim to leverage \u201cemergent abilities\u201d of large language models [25], but to explore one of their latent abilities.\nOur experimental framework allows us to consider several questions of interest to linguists. We can use various sets of labels, e.g. language names, or any level of phylogenetic (diachronic) grouping, or again typological (synchronic) groupings. We can also vary the examples the classifier is trained on. Among the many possibilities, we consider three settings:\n\u2022 a dialect identification setting in which the classifier is trained\non recordings of N language varieties (dialects) and is then used (and evaluated) to recognize one of these;\n\u2022 a language identification setting which differs from the\nprevious setting only by the definition of the label to predict: the goal is now to identify languages, which constitute groups of dialects. Importantly, this classifier can be used to predict the language affiliation of a dialect that is not present in the train set, so that it can be used to predict, for instance, the language to which a hitherto unknown dialect belongs;\n\u2022 a similarity identification setting which differs from the first\nsetting only by the definition of the train set: in this setting, we use our model on utterances of a dialect that is not present in the train set. Since the classifier cannot predict the exact dialect (as its label is not available from within the train set), it seems intuitively likely to choose the label of a dialect with similar characteristics. Crucially, we believe that this setting will therefore allow to identify similarities between language varieties."
        },
        {
            "heading": "3. Information on Languages and Dialects",
            "text": "In all our experiments, we use datasets from the Pangloss Collection [26],2 an open archive of (mostly) endangered languages.Our experiments focus on 11 dialects that belong to five languages:\n\u2022 two dialects of Nepali: Achhami (Glottocode [27]: doty1234)\nand Dotyal (doty1234);\n\u2022 two dialects of Lyngam (lyng1241): Langkma and Nongtrei;\n\u2022 three varieties of Na-na\u0161u, a dialect of Shtokavian Serbo-\nCroatian (shto1241) spoken by Italian Croats;\n\u2022 two dialects of War (khas1268): Amwi (warj1242) and\nNongtalang (nong1246);\n\u2022 two dialects of Na (yong1270): Lataddi Na (lata1234) and\nYongning Na (yong1288).\nWe also consider two additional languages, Naxi (naxi1245) and Laze (laze1238), because of their closeness to Na [28].\nFor the sake of consistency in the experiments reported here, we use \u201cdialect\u201d as the lowest-level label, and \u201clanguage\u201d for the first higher level, as a convention. We are aware that the distance between \u201cdialects\u201d (and between \u201clanguages\u201d) varies significantly from one case to another. We do not assume that the distance between Achhami and Dotyal (dialects of Nepali) is (even approximately) the same as that between Langkma and Nongtrei (dialects of Lyngam), or between Lataddi Na and Yongning Na. The key assumption behind our use of terms is that language varieties referred to as \u201cdialects\u201d of the same language are close enough that it makes sense to assume that the degree of phonetic similarity between them can serve as a rule-of-thumb estimate for the distance that separates them,\n2Website: pangloss.cnrs.fr. A tool for bulk downloads and for tailoring reference corpora is available: OutilsPangloss.\nwithout requiring higher-level linguistic information (of the type used to train a language model).\nIn this preliminary study we have decided to focus on a small number of languages and to focus on qualitative analysis of our results, rather than running a large-scale experiment on dozens of languages. The languages are chosen according to the size of the available corpora and specific properties. We favored continuous speech (we left aside corpora consisting solely of word lists or materials elicited sentence by sentence).\nFor each of these languages we extracted 2 to 50 files of\nvariable length (from 33 seconds to 30 minutes).\n4. Experiments\nIn all our experiments, we evaluate the capacity of our classifier to predict the correct language information (either the label of a specific dialect or the name of a language) using the usual metrics for multi-class classification, namely, precision, recall and their combination in the F1 score.\nDialect Identification To test the ability of a classifier to recognize a dialect from the representations built by XLSR-53, we consider a classifier using the names of the 13 dialects or languages described in Section 3 as its label set. We try out two configurations. In the first one, all the utterances of a dialect are randomly divided into a test set (20% of utterances) and a training set (80%). In the second configuration, the training corpus is made up of 80% of the files of a dialect and the test corpus contains the remaining 20%. While the latter configuration is closer to the real conditions of use of our system (guaranteeing that the utterances of the test corpus come only from files that have not been seen at training), it is more difficult to control the size of the train and test sets, which makes the analysis less straightforward.\nThe results are reported in Table 1. They show that, in both configurations, a simple classifier is able to identify the correct dialect label for an utterance with high accuracy, showing that XLSR-53 representations encode language information. Similar observations have already been reported (see, e.g., [29]), but to the best of our knowledge, our work is the first evaluation of the capacity of XLSR-53 representations to identify under-documented language varieties whose characteristics are potentially very different from the languages seen at (pre)training [8]. Interestingly, the quality of predictions does not seem to be influenced by the amount of training data (a similar paradox is reported in the evaluation of another large language model in multilingual learning: ChatGPT [30]).\nThe recordings considered in the experiment we have just described were all collected in the context of linguistic fieldwork, and thus have some peculiarities that may distort the conclusions we have just drawn. In particular, most of the dialects we considered have recordings of a single speaker. Moreover, different dialects of the same language were often recorded by the same linguist, using the same recording setup (in particular, the same microphone). We therefore need to check whether our classifiers just learn to distinguish speakers (in many cases: one per dialect) or recording conditions. In order to rule out this possibility, we carried out a control experiment in which we tried to predict the file name (serving as proxy information for the speaker and the recording conditions). A logistic regression trained in the 80-20 condition described above achieved a macro F1 score of 0.45, showing that the decision of the classifier is largely based on linguistic information, not solely on information about the recording conditions.\nLanguage Identification In a second experiment, we test the ability of our classifier to identify languages (that is, groups\nof dialects). We consider, again, two conditions to train and evaluate our classifier. In a first condition, the train and test sets are randomly sampled from all the recordings we consider (with the usual 80%-20% split) without any condition being imposed on the files or languages. All dialects are therefore present in both the test and train sets. In a second condition, the test set is put together by selecting, for each language (group of dialects), all the recordings of a randomly chosen dialect. The test set is thus made up of 5 dialects that have not been seen at training.\nTable 2 reports results in the first condition. The classifier succeeds in identifying the correct language in the vast majority of cases, a very logical result since the same languages are present in the train and test sets and the experiments reported\nin the previous paragraph proved that it is possible to identify dialects with good accuracy. To verify that the classifier was able to extract linguistic information rather than merely memorizing arbitrary associations between dialects, we performed a control experiment in which we divided the 13 dialects into 5 arbitrary groups having the same size as the languages (dialect groups) considered in the previous experiment. A classifier considering these groups as labels achieves a macro F1 score of 0.85. While this score is high, it is notably lower than the score obtained by predicting linguistic families, showing that the classifier decisions are, to a significant extent, based on linguistic criteria.\nTable 3 shows the results for the second condition, in which we evaluate the capacity of a classifier to predict the language (dialect group) of a dialect that was not part of the train set. Scores vary greatly by language (group of dialects) and several factors make it difficult to interpret these results. First, removing a dialect completely from the train set can result in large variation in its size and the results of Table 3 are not necessarily comparable with those reported so far. Second, some confounders seem to cause particularly poor performance for certain groups of dialects. For example, recordings of Dotyal are mainly sung epic poetry, so it is not surprising that any generalization across the two dialects of Nepali is difficult. Gender seems to be another confounder: several corpora only contain recordings by speakers of the same gender, and a quick qualitative study seems to show that a model trained on a female speaker does not perform well on data by a male speaker (and conversely). Note, however, that our evaluation of the performance of the classifier puts it at a disadvantage since it is evaluated at the level of a 5-second snippet and not of an entire recording. It is not unlikely that the performance would be better if we predicted a single label for a whole recording (for example by taking the most frequent label among those of all snippets).\nSimilarity Identification Setting In our last experiment, we trained 12 classifiers, considering all dialects but one for training and looking at the distribution of predicted labels when\nthe classifier had to identify snippets of the held-out language. As explained in Section 2, the classifier cannot predict the correct label (since the target language is not present in the training corpus) but might, we believe, pitch on a language with similar characteristics. Results of this experiment are reported in Table 4. They allow us to draw several interesting conclusions.\nFirst, these results show that the classifier pitches consistently on one and the same label. In almost every case, the distribution of predicted labels is concentrated on a few labels. That means that the classifier typically identifies almost all snippets from an audio file as being in the same language. Second, in several cases (e.g. for dialects of Na, War or Na-na\u0161u), the classifier recognizes the unknown language as a dialect of the same group: for instance Yongning Na utterances are mainly labelled as Lataddi Na (the dialect of a nearby village). In addition to its interest for the automatic identification of dialect groups, this observation proves that XLSR-53 uncovers representations that somehow generalize over small dialectal variation.\nFurther experiments are needed to understand the two cases where the output of the classifier disagrees with the goldstandard clustering: the San Felice del Molise dialect of Na-na\u0161u, and the two dialects of Lyngam (Langkma and Nongtrei). (For Nepali, a plausible confounder was mentioned above: data type \u2013 genre \u2013, as the Dotyal corpus consists of sung epics.)"
        },
        {
            "heading": "5. Conclusions",
            "text": "Our exploratory experiments exploring the capacity of XLSR-53 to place audio signals in a language and dialect landscape confirm the interest of neural representations of speech as an exciting avenue of research. Further work is required to ensure that a dialect identification system bases its decisions on phenomena (detecting relevant phonetic-phonological structures), not on parameters such as recording conditions, speaker characteristics (gender, age...) and speech genre/style, which constitute confounders in a language identification task. In\nfuture work, we plan to reproduce the experiments on corpora of better-resourced languages, such as LibriVox or CommonVoice, for which it is easier to control recording conditions, speaker gender, and the amount of training data.\n6. References\n[1] A. Conneau, A. Baevski, R. Collobert, A. Mohamed, and M. Auli, \u201cUnsupervised cross-lingual representation learning for speech recognition,\u201d in Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association, Brno,\nCzechia, 30 August - 3 September 2021, H. Hermansky, H. Cernock\u00fd, L. Burget, L. Lamel, O. Scharenborg, and P. Motl\u00edcek, Eds. ISCA, 2021, pp. 2426\u20132430. [Online]. Available: https://doi.org/10.21437/Interspeech.2021-329\n[2] M. Moradshahi, H. Palangi, M. S. Lam, P. Smolensky, and J. Gao, \u201cHUBERT untangles BERT to improve transfer across NLP tasks,\u201d CoRR, vol. abs/1910.12647, 2019. [Online]. Available: http://arxiv.org/abs/1910.12647\n[3] M. Bartelds, W. de Vries, F. Sanal, C. Richter, M. Liberman, and M. Wieling, \u201cNeural representations for modeling variation in speech,\u201d Journal of Phonetics, vol. 92, pp. 101\u2013137, 2022.\n[4] R. K. Potter, G. A. Kopp, and H. C. Green, Visible speech. New York: D. Van Nostrand, 1947.\n[5] S. A. Fulop, \u201cThe beginning of time-frequency analysis,\u201d The Journal of the Acoustical Society of America, vol. 152, no. 5, pp. R9\u2013R10, 11 2022. [Online]. Available: https://doi.org/10.1121/10.0014987\n[6] G. Fant, Acoustic theory of speech production, with calculations based on X-ray studies of Russian articulations. The Hague & Paris: Mouton, 1960.\n[7] N. San, M. Bartelds, M. Browne, L. Clifford, F. Gibson, J. Mansfield, D. Nash, J. Simpson, M. Turpin, M. Vollmer, S. Wilmoth, and D. Jurafsky, \u201cLeveraging pre-trained representations to improve access to untranscribed speech from endangered languages,\u201d in 2021 IEEE Automatic Speech Recognition and Understanding\nWorkshop (ASRU), 2021, pp. 1094\u20131101.\n[8] S. Guillaume, G. Wisniewski, C. Macaire, G. Jacques, A. Michaud, B. Galliot, M. Coavoux, S. Rossato, M.-C. Nguy\u00ean, and M. Fily, \u201cFine-tuning pre-trained models for Automatic Speech Recognition: experiments on a fieldwork corpus of Japhug (Trans-Himalayan family),\u201d in Proceedings of the Fifth Workshop on the Use of Computational Methods in the Study of Endangered\nLanguages. Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 170\u2013178. [Online]. Available: https://aclanthology.org/2022.computel-1.21\n[9] J. Good and M. Cysouw, \u201cLanguoid, doculect, and glossonym: formalizing the notion \u2018language\u2019,\u201d Language Documentation & Conservation, vol. 7, pp. 331\u2013359, 2013. [Online]. Available: http://hdl.handle.net/10125/4606\n[10] G. Alain and Y. Bengio, \u201cUnderstanding intermediate layers using linear classifier probes,\u201d in 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-\n26, 2017, Workshop Track Proceedings. OpenReview.net, 2017. [Online]. Available: https://openreview.net/forum?id=HJ4-rAVtl\n[11] A. Tjandra, D. G. Choudhury, F. Zhang, K. Singh, A. Conneau, A. Baevski, A. Sela, Y. Saraf, and M. Auli, \u201cImproved language identification through cross-lingual self-supervised learning,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore, 23-27\nMay 2022. IEEE, 2022, pp. 6877\u20136881. [Online]. Available: https://doi.org/10.1109/ICASSP43922.2022.9747667\n[12] Z. Fan, M. Li, S. Zhou, and B. Xu, \u201cExploring wav2vec 2.0 on speaker verification and language identification,\u201d in Interspeech 2021, 22nd Annual Conference of the International\nSpeech Communication Association, Brno, Czechia, 30 August - 3 September 2021, H. Hermansky, H. Cernock\u00fd, L. Burget, L. Lamel, O. Scharenborg, and P. Motl\u00edcek, Eds. ISCA, 2021, pp. 1509\u20131513. [Online]. Available: https://doi.org/10.21437/Interspeech.2021-1280\n[13] J. Valk and T. Alum\u00e4e, \u201cVoxLingua107: a dataset for spoken language recognition,\u201d in Proc. IEEE SLT Workshop, 2021.\n[14] E. M. Bender, \u201cLinguistically na\u00efve!= language independent: Why NLP needs linguistic typology,\u201d in Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computa-\ntional Linguistics: Virtuous, Vicious or Vacuous?, 2009, pp. 26\u201332.\n[15] A. Michaud, O. Adams, T. Cohn, G. Neubig, and S. Guillaume, \u201cIntegrating automatic transcription into the language documentation workflow: Experiments with Na data and the Persephone toolkit,\u201d Language Documentation & Conservation, vol. 12, pp. 393\u2013429, 2018.\n[16] D. van Esch, B. Foley, and N. San, \u201cFuture directions in technological support for language documentation,\u201d in Proceedings of the Workshop on Computational Methods for Endangered Languages, vol. 1, 2019.\n[17] E. Prud\u2019hommeaux, R. Jimerson, R. Hatcher, and K. Michelson, \u201cAutomatic speech recognition for supporting endangered language documentation,\u201d Language documentation and conservation, vol. 15, 2021.\n[18] R. Cotterell and J. Eisner, \u201cProbabilistic typology: Deep generative models of vowel inventories,\u201d arXiv preprint arXiv:1705.01684, 2017.\n[19] C. Chagnaud, P. Garat, P.-A. Davoine, E. Carpitelli, and A. Vincent, \u201cShinydialect: a cartographic tool for spatial interpolation of geolinguistic data,\u201d in Proceedings of the 1st ACM SIGSPATIAL workshop on Geospatial Humanities, 2017, pp. 23\u201330.\n[20] E. M. Ponti, H. O\u2019horan, Y. Berzak, I. Vulic\u0301, R. Reichart, T. Poibeau, E. Shutova, and A. Korhonen, \u201cModeling language variation and universals: A survey on typological linguistics for natural language processing,\u201d Computational Linguistics, vol. 45, no. 3, pp. 559\u2013601, 2019.\n[21] A. Suni, M. Wlodarczak, M. Vainio, and J. Simko, \u201cComparative analysis of prosodic characteristics using wavenet embeddings,\u201d in 20th Annual Conference of the International Speech Communication Association (INTERSPEECH 2019). ISCA, 2019.\n[22] M. De Seyssel, G. Wisniewski, E. Dupoux, and B. Ludusan, \u201cInvestigating the usefulness of i-vectors for automatic language characterization,\u201d in Speech Prosody 2022-11th International Conference on Speech Prosody, 2022.\n[23] J. Bjerva and I. Augenstein, \u201cTracking typological traits of Uralic languages in distributed language representations,\u201d arXiv preprint arXiv:1711.05468, 2017.\n[24] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, \u201cScikit-learn: Machine learning in Python,\u201d Journal of Machine Learning Research, vol. 12, pp. 2825\u20132830, 2011.\n[25] R. Schaeffer, B. Miranda, and S. Koyejo, \u201cAre emergent abilities of Large Language Models a mirage?\u201d arXiv preprint arXiv:2304.15004, 2023.\n[26] B. Michailovsky, M. Mazaudon, A. Michaud, S. Guillaume, A. Fran\u00e7ois, and E. Adamou, \u201cDocumenting and researching endangered languages: the Pangloss Collection,\u201d Language Documentation & Conservation, vol. 8, pp. 119\u2013135, 2014. [Online]. Available: https://shs.hal.science/halshs-01003734\n[27] H. Hammarstr\u00f6m, \u201cGlottolog: A free, online, comprehensive bibliography of the world\u2019s languages,\u201d in 3rd International Conference on Linguistic and Cultural Diversity in Cyberspace. UNESCO, 2015, pp. 183\u2013188.\n[28] G. Jacques and A. Michaud, \u201cApproaching the historical phonology of three highly eroded Sino-Tibetan languages: Naxi, Na and Laze,\u201d Diachronica, vol. 28, no. 4, pp. 468\u2013498, 2011. [Online]. Available: https://shs.hal.science/halshs-00537990\n[29] M. de Seyssel, M. Lavechin, Y. Adi, E. Dupoux, and G. Wisniewski, \u201cProbing phoneme, language and speaker information in unsupervised speech representations,\u201d in Interspeech 2022 - 23rd INTERSPEECH Conference, Incheon, South Korea, Sep. 2022. [Online]. Available: https://hal.inria.fr/hal-03830470\n[30] V. D. Lai, N. T. Ngo, A. P. B. Veyseh, H. Man, F. Dernoncourt, T. Bui, and T. H. Nguyen, \u201cChatGPT beyond English: Towards a comprehensive evaluation of Large Language Models in multilingual learning,\u201d arXiv preprint arXiv:2304.05613, 2023."
        }
    ],
    "title": "From \u2018Snippet-lects\u2019 to Doculects and Dialects: Leveraging Neural Representations of Speech for Placing Audio Signals in a Language Landscape",
    "year": 2023
}