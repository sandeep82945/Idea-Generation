{
    "abstractText": "Fractional diffusion equations have been an effective tool for modeling anomalous diffusion in complicated systems. However, traditional numerical methods require expensive computation cost and storage resources because of the memory effect brought by the convolution integral of time fractional derivative. We propose a Bayesian Inversion with Neural Operator (BINO) to overcome the difficulty in traditional methods as follows. We employ a deep operator network to learn the solution operators for the fractional diffusion equations, allowing us to swiftly and precisely solve a forward problem for given inputs (including fractional order, diffusion coefficient, source terms, etc.). In addition, we integrate the deep operator network with a Bayesian inversion method for modelling a problem by subdiffusion process and solving inverse subdiffusion problems, which reduces the time costs (without suffering from overwhelm storage resources) significantly. A large number of numerical experiments demonstrate that the operator learning method proposed in this work can efficiently solve the forward problems and Bayesian inverse problems of the subdiffusion equation.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiong-bin Yan"
        },
        {
            "affiliations": [],
            "name": "Zhi-Qin John Xua"
        },
        {
            "affiliations": [],
            "name": "Zheng Maa"
        }
    ],
    "id": "SP:d1bc9206a76d137ad3c32de11ba347aac77b32ef",
    "references": [
        {
            "authors": [
                "J. Klafter",
                "I.M. Sokolov"
            ],
            "title": "Anomalous diffusion spreads its wings",
            "venue": "Physics World,",
            "year": 2005
        },
        {
            "authors": [
                "B. Berkowitz",
                "A. Cortis",
                "M. Dentz",
                "H. Scher"
            ],
            "title": "Modeling non-fickian transport in geological formations as a continuous time random walk",
            "venue": "Reviews of Geophysics,",
            "year": 2006
        },
        {
            "authors": [
                "M. Dentz",
                "A. Cortis",
                "H. Scher",
                "B. Berkowitz"
            ],
            "title": "Time behavior of solute transport in heterogeneous media: transition from anomalous to normal transport",
            "venue": "Advances in Water Resources,",
            "year": 2004
        },
        {
            "authors": [
                "R.R. Nigmatullin"
            ],
            "title": "The realization of the generalized transfer equation in a medium with fractal geometry. physica status solidi",
            "year": 1986
        },
        {
            "authors": [
                "S.C. Kou"
            ],
            "title": "Stochastic modeling in nanoscale biophysics: Subdiffusion within proteins",
            "venue": "The Annals of Applied Statistics,",
            "year": 2008
        },
        {
            "authors": [
                "K. Ritchie",
                "X.Y. Shan",
                "J. Kondo",
                "K. Iwasawa",
                "T. Fujiwara",
                "A. Kusumi"
            ],
            "title": "Detection of nonbrownian diffusion in the cell membrane in single molecule tracking",
            "venue": "Biophysical Journal,",
            "year": 2005
        },
        {
            "authors": [
                "B. Berkowitz",
                "J. Klafter",
                "R. Metzler",
                "H. Scher"
            ],
            "title": "Physical pictures of transport in heterogeneous media: Advection-dispersion, random-walk, and fractional derivative formulations",
            "venue": "Water Resources Research,",
            "year": 2002
        },
        {
            "authors": [
                "T.A.M. Langlands",
                "B.I. Henry"
            ],
            "title": "The accuracy and stability of an implicit solution method for the fractional diffusion equation",
            "venue": "Journal of Computational Physics,",
            "year": 2005
        },
        {
            "authors": [
                "X. Li",
                "C. Xu"
            ],
            "title": "A space-time spectral method for the time fractional diffusion equation",
            "venue": "SIAM Journal on Numerical Analysis,",
            "year": 2009
        },
        {
            "authors": [
                "Y. Lin",
                "X. Li",
                "C. Xu"
            ],
            "title": "Finite difference/spectral approximations for the fractional cable equation",
            "venue": "Mathematics of Computation,",
            "year": 2011
        },
        {
            "authors": [
                "S.B. Yuste",
                "L. Acedo"
            ],
            "title": "An explicit finite difference method and a new von neumann-type stability analysis for fractional diffusion equations",
            "venue": "SIAM Journal on Numerical Analysis,",
            "year": 2005
        },
        {
            "authors": [
                "F. Zeng",
                "C. Li",
                "F. Liu",
                "I. Turner"
            ],
            "title": "The use of finite difference/element approaches for solving the time-fractional subdiffusion equation",
            "venue": "SIAM Journal on Scientific Computing,",
            "year": 2013
        },
        {
            "authors": [
                "C.M. Chen",
                "F. Liu",
                "I. Turner",
                "V. Anh"
            ],
            "title": "A fourier method for the fractional diffusion equation describing sub-diffusion",
            "venue": "Journal of Computational Physics,",
            "year": 2007
        },
        {
            "authors": [
                "B. Jin",
                "B. Li",
                "Z. Zhou"
            ],
            "title": "Correction of high-order BDF convolution quadrature for fractional evolution equations",
            "venue": "SIAM Journal on Scientific Computing,",
            "year": 2017
        },
        {
            "authors": [
                "B. Jin",
                "R. Lazarov",
                "Z. Zhou"
            ],
            "title": "Numerical methods for time-fractional evolution equations with nonsmooth data: a concise overview",
            "venue": "Computer Methods in Applied Mechanics and Engineering,",
            "year": 2019
        },
        {
            "authors": [
                "M.W.M.G. Dissanayake",
                "N. Phan-Thien"
            ],
            "title": "Neural-network-based approximations for solving partial differential equations. communications",
            "venue": "Numerical Methods in Engineering,",
            "year": 1994
        },
        {
            "authors": [
                "M. Raissi",
                "P. Perdikaris",
                "G.E. Karniadakis"
            ],
            "title": "Physics-informed neural networks: a deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "venue": "Journal of Computational Physics,",
            "year": 2019
        },
        {
            "authors": [
                "Z. Liu",
                "W. Cai",
                "Z.Q.J. Xu"
            ],
            "title": "Multi-scale deep neural network (mscalednn) for solving poisson-boltzmann equation in complex domains",
            "venue": "Communications in Computational Physics,",
            "year": 1970
        },
        {
            "authors": [
                "W. E",
                "B. Yu"
            ],
            "title": "The deep Ritz method: a deep learning-based numerical algorithm for solving variational problems",
            "venue": "Communications in Mathematics and Statistics,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Liao",
                "P. Ming"
            ],
            "title": "Deep Nitsche method: deep Ritz method with essential boundary conditions",
            "venue": "Communications in Computational Physics,",
            "year": 2021
        },
        {
            "authors": [
                "Y. Zang",
                "G. Bao",
                "X. Ye",
                "H. Zhou"
            ],
            "title": "Weak adversarial networks for high-dimensional partial differential equations",
            "venue": "Journal of Computational Physics, 411:109409,",
            "year": 2020
        },
        {
            "authors": [
                "G. Bao",
                "X. Ye",
                "Y. Zang",
                "H. Zhou"
            ],
            "title": "Numerical solution of inverse problems by weak adversarial networks",
            "venue": "Inverse Problems, 36(11):115003,",
            "year": 2020
        },
        {
            "authors": [
                "G. Pang",
                "L. Lu",
                "G.E. Karniadakis"
            ],
            "title": "fPINNs: fractional physics-informed neural networks",
            "venue": "SIAM Journal on Scientific Computing,",
            "year": 2019
        },
        {
            "authors": [
                "L. Guo",
                "H. Wu",
                "X.C. Yu",
                "T. Zhou"
            ],
            "title": "Monte Carlo fPINNs: Deep learning method for forward and inverse problems involving high dimensional fractional partial differential equations",
            "venue": "Computer Methods in Applied Mechanics and Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Zhu",
                "N. Zabaras"
            ],
            "title": "Bayesian deep convolutional encoder-decoder networks for surrogate modeling and uncertainty quantification",
            "venue": "Journal of Computational Physics,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Afshar",
                "S. Bhatnagar",
                "S. Pan",
                "K. Duraisamy",
                "S. Kaushik"
            ],
            "title": "Prediction of aerodynamic flow fields using convolutional neural networks",
            "venue": "Computational Mechanics,",
            "year": 2019
        },
        {
            "authors": [
                "L. Lu",
                "P. Jin",
                "G.E. Karniadakis"
            ],
            "title": "DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators",
            "year": 1910
        },
        {
            "authors": [
                "P. Jin",
                "S. Meng",
                "L. Lu"
            ],
            "title": "MIONet: Learning multiple-input operators via tensor product",
            "venue": "arXiv preprint arXiv:2202.06137,",
            "year": 2022
        },
        {
            "authors": [
                "S. Wang",
                "H. Wang",
                "P. Perdikaris"
            ],
            "title": "Learning the solution operator of parametric partial differential equations with physics-informed deeponets",
            "venue": "Science Advances,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Mao",
                "L. Lu",
                "O. Marxen",
                "T.A. Zaki",
                "G.E. Karniadakis"
            ],
            "title": "DeepM&Mnet for hypersonics: predicting the coupled flow and finite-rate chemistry behind a normal shock using neural-network approximation of operators",
            "venue": "Journal of Computational Physics,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Li",
                "N.B. Kovachki",
                "K. Azizzadenesheli",
                "B. liu",
                "K. Bhattacharya",
                "A. Stuart",
                "A. Anandkumar"
            ],
            "title": "Fourier neural operator for parametric partial differential equations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Li",
                "N.B. Kovachki",
                "K. Azizzadenesheli",
                "B. Liu",
                "K. Bhattacharya",
                "A. Stuart",
                "A. Anandkumar"
            ],
            "title": "Neural operator: Graph kernel network for partial differential equations",
            "year": 2003
        },
        {
            "authors": [
                "Z. Li",
                "N.B. Kovachki",
                "K. Azizzadenesheli",
                "B. Liu",
                "K. Bhattacharya",
                "A. Stuart",
                "A. Anandkumar"
            ],
            "title": "Multipole graph neural operator for parametric partial differential equations",
            "venue": "In Proceedings of the 34th International Conference on Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "L. Zhang",
                "T. Luo",
                "Y. Zhang",
                "W. E",
                "Z.Q.J. Xu",
                "Z. Ma"
            ],
            "title": "MOD-Net: A machine learning approach via model-operator-data network for solving pdes",
            "venue": "Communications in Computational Physics,",
            "year": 2022
        },
        {
            "authors": [
                "K. Sakamoto",
                "M. Yamamoto"
            ],
            "title": "Initial value/boundary value problems for fractional diffusion-wave equations and applications to some inverse problems",
            "venue": "Journal of Mathematical Analysis and Applications,",
            "year": 2011
        },
        {
            "authors": [
                "I. Podlubny"
            ],
            "title": "Fractional differential equations, volume 198",
            "year": 1999
        },
        {
            "authors": [
                "Z.Z. Sun",
                "X. Wu"
            ],
            "title": "A fully discrete difference scheme for a diffusion-wave system",
            "venue": "Applied Numerical Mathematics,",
            "year": 2006
        },
        {
            "authors": [
                "Y.X. Zhang",
                "J. Jia",
                "L. Yan"
            ],
            "title": "Bayesian approach to a nonlinear inverse problem for a time-space fractional diffusion equation",
            "venue": "Inverse Problems, 34(12):125002,",
            "year": 2018
        },
        {
            "authors": [
                "X.B. Yan",
                "Y.X. Zhang",
                "T. Wei"
            ],
            "title": "Identify the fractional order and diffusion coefficient in a fractional diffusion wave equation",
            "venue": "Journal of Computational and Applied Mathematics,",
            "year": 2021
        },
        {
            "authors": [
                "S.L. Cotter",
                "G.O. Roberts",
                "A.M. Stuart",
                "D. White"
            ],
            "title": "MCMC methods for functions: Modifying old algorithms to make them faster",
            "venue": "Statistical Science,",
            "year": 2013
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n21 1.\n11 98\n1v 1\n[ m\nat h.\nN A\n] 2\n2 N\nFractional diffusion equations have been an effective tool for modeling anomalous diffusion in complicated systems. However, traditional numerical methods require expensive computation cost and storage resources because of the memory effect brought by the convolution integral of time fractional derivative. We propose a Bayesian Inversion with Neural Operator (BINO) to overcome the difficulty in traditional methods as follows. We employ a deep operator network to learn the solution operators for the fractional diffusion equations, allowing us to swiftly and precisely solve a forward problem for given inputs (including fractional order, diffusion coefficient, source terms, etc.). In addition, we integrate the deep operator network with a Bayesian inversion method for modelling a problem by subdiffusion process and solving inverse subdiffusion problems, which reduces the time costs (without suffering from overwhelm storage resources) significantly. A large number of numerical experiments demonstrate that the operator learning method proposed in this work can efficiently solve the forward problems and Bayesian inverse problems of the subdiffusion equation.\nKeywords: Anomalous diffusion, operator learning, Bayesian inverse problems."
        },
        {
            "heading": "1. Introduction",
            "text": "Reactions and diffusion phenomena, across a wide range of applications including animal coat patterns and nerve cell signals, are modeled extensively by standard reaction-diffusion equations, which complies with a basic assumption that the diffusion obeys the standard Brownian motion. The most distinctive characteristic of the standard Brownian motion is that the mean squared displacement \u3008x2(t)\u3009 of diffusing species follows a linear dependence in time, i.e., \u3008x2(t)\u3009 \u223c K1t.\n\u2217Corresponding author Email addresses: yanxb2015@163.com (Xiong-bin Yan), xuzhiqin@sjtu.edu.cn (Zhi-Qin John Xu),\nzhengma@sjtu.edu.cn (Zheng Ma)\nPreprint submitted to Elsevier November 23, 2022\nHowever, over the past few decades, it has been found that anomalous diffusion [1], in which diffusive motion cannot be modeled as the standard Brownian motion, has occurred in many phenomena. Unlike the standard diffusion, a hallmark of the anomalous diffusion is the non-linear power law growth of mean square displacement with time, i.e., \u3008x2(t)\u3009 \u223c K\u03b1t\u03b1, where \u03b1 > 1 is referred as superdiffusion and 0 < \u03b1 < 1 is referred as subdiffusion. To derive the fractional diffusion equation, at a mesoscopic level, one of the most efficient methods is continuous time random walks (CTRWs). In the CRTWs, subdiffusion arises when the waiting time distribution is heavytailed (e.g., \u03c8(t) \u223c t\u2212\u03b1\u22121 with \u03b1 \u2208 (0, 1)), where \u03c8(t) denotes a waiting time of the particle jumping between two successive steps. And the associated probability density function of the particle appearing at time t > 0 and spatial location x \u2208 Rd satisfies the subdiffusion equation. Recently, the subdiffusion model has been found in many applications in physics, finance, and biology, including solute transport in heterogeneous media [2, 3], thermal diffusion on fractal domains [4], protein transport within membranes [5, 6], flow in highly heterogeneous aquifers [7].\nIn this paper, let \u2126 = (0, 1)2, we consider the following subdiffusion problem:\n               \u2202\u03b10+u(x, t) = \u2207 \u00b7 (a(x)\u2207u(x, t)) + c(x)u(x, t) + f (x), x \u2208 \u2126, 0 < t \u2264 T, u(x, 0) = u0(x), x \u2208 \u2126, u(x, t) = 0, x \u2208 \u2202\u2126, 0 < t \u2264 T,\n(1.1)\nwhere 0 < \u03b1 < 1 and \u2202\u03b1 0+ u(x, t) is the Caputo left-sided fractional derivative defined by\n\u2202\u03b10+u(x, t) = 1\n\u0393(1 \u2212 \u03b1)\n\u222b t\n0\n(t \u2212 s)\u2212\u03b1\u2202u \u2202s (x, s)ds, t > 0,\na(x), c(x), f (x) and u0(x) represent the diffusion coefficient, reaction coefficient, source and initial value, respectively.\nIt is widely known that it is very challenging to find the time-fractional diffusion equation\u2019s analytical solution, thus, it is very important to design numerical methods for solving it. There are currently many numerical methods for discretizing the above fractional equation. Compared with solving standard diffusion equations, much effort is always used to construct appropriate methods to discretize the time-fractional derivative. Existing schemes include: the L1-type approximation [8\u201310], the Gru\u0308nwald-Letnikov approximation [11\u201313] and Convolution quadrature [14, 15], and so on. These methods of solving the problem (1.1) require using and storing the result at all previous time steps, which means that obtaining its numerical solution requires a lot of time and computational resources. Additionally, in order to solve the inverse problems for the time-fractional diffusion equation, it is necessary to repeatedly solve the forward problems, which brings more difficulties and computational complexity of the inverse problem solutions. At the same time, with numerous applications of machine learning in solving PDEs, it is increasingly possible to solve the above problems with machine learning approach. In this paper, we seek a machine learning-based\nmethod to deal with these difficulties of the subdiffusion problem (1.1).\nUsing the machine learning methods, particularly deep learning methods, to solve partial differential equations has grown rapidly. Here, we will discuss two major neural network-based approaches for solving partial differential equations. The first method is to use a deep neural network (DNN) to parameterize the differential equation solutions, so that the neural network\u2019s outputs satisfy the partial differential equations, and then use the strong or weak form of the differential equations as the loss function. Several examples are listed: Physics-informed neural network methods [16\u201318], variational methods [19, 20], weak adversarial network methods [21, 22]. Several works solve fractional partial differential equations by deep learning methods. For example, Pang et al. and Guo et al. in [23, 24] apply fPINN and Monte Carlo fPINN to solve the forward and inverse problems in fractional partial differential equations, respectively. However, the methods mentioned above suffer from retraining a new neural network for every new input parameter. The second approach is to parameterize the solution operator by a deep neural network, for example, deep convolutional encoder-decoder networks [25, 26], deep operator networks (DeepONet) [27\u201330], neural operator [31\u201333], and MOD-Net [34]. These operator networks only need to be trained once. To obtain a prediction solution for a new parameter, it only needs a forward pass of the network, which saves a lot of computational time. Therefore, learning operator can be an appropriate method for efficiently solving the complex subdiffusion problem (1.1).\nIn the paper, we propose a Bayesian Inversion with Neural Operator (BINO) approach for modeling subdiffusion problem and solving inverse subdiffusion equation (1.1). We use datadriven method to train a DeepONet for solving forward problems of the subdiffusion equation (1.1). The operator network only needs to be trained once, and then it can repeatedly solve the forward problems for different parameters fast. Then, we incorporate the deep operator learning into the Bayesian inversion approach for inverse problems of the subdiffusion model, which allows efficiently performing the Bayesian inversion. In summary, the main contributions of this paper are listed as follows:\n\u2022 We propose a BINO method for modelling a subdiffusion problem by identifying the frac-\ntional order of the equation (1.1).\n\u2022 We use BINO to recover the diffusion coefficient of subdiffusion equation (1.1), which can be\nused to rapidly and efficiently implement the Markov chain Monte Carlo (MCMC) sampling in Bayesian inference.\nThe rest of the paper is structured as follows: In section 2, we introduce the deep operator learning and use it to study the solution operator learning problems in subdiffusion (1.1). In section 3, we combine the operator learning method with the Bayesian inference method to solve several inverse problems in model (1.1). Finally, we conclude the paper in section 4."
        },
        {
            "heading": "2. Solving forward problems of subdiffusion by the deep operator networks.",
            "text": "Let a \u2208 C1(\u2126\u0304), a(x) > 0, c \u2208 C(\u2126\u0304), c(x) \u2264 0, x \u2208 \u2126 and u0, f \u2208 L2(\u2126). Then, by [35], the solution of problem (1.1) is unique and satisfies the following form\nu(x, t) =\n\u221e \u2211\nn=1\n(u0, \u03d5n)E\u03b1,1(\u2212\u03bbnt\u03b1)\u03d5n(x) + \u221e \u2211\nn=1\n1 \u2212 E\u03b1,1(\u2212\u03bbnt\u03b1) \u03bbn ( f , \u03d5n)\u03d5n(x), (2.1)\nwhere the notation (\u00b7, \u00b7) denotes the scalar product of L2(\u2126) space. The \u03bbn, \u03d5n, n = 1, \u00b7 \u00b7 \u00b7 ,\u221e in (2.1) are the eigenvalues and eigenfunctions of elliptic operator, which means \u2212\u2207 \u00b7 (a(x)\u2207\u03d5n(x)) \u2212 c(x)\u03d5n(x) = \u03bbn\u03d5n(x) and \u03d5n(x)|\u2202\u2126 = 0. The funciton E\u03b1,1(\u2212\u03bbnt\u03b1) in (2.1) denotes the Mittag-Leffler function [36]. By simplifying the formula (2.1), it can be seen that\nu(x, t) =\n\u222b\n\u2126\nk1(x, \u03be, t)u0(\u03be)d\u03be +\n\u222b\n\u2126\nk2(x, \u03be, t) f (\u03be)d\u03be, (2.2)\nwhere k1(x, \u03be, t) = \u2211\u221e n=1 E\u03b1,1(\u2212\u03bbnt\u03b1)\u03d5n(x)\u03d5n(\u03be), k2(x, \u03be, t) = \u2211\u221e n=1 1\u2212E\u03b1,1(\u2212\u03bbnt\u03b1)\n\u03bbn \u03d5n(x)\u03d5n(\u03be). From (2.2),\nwe can easily see that the solution to the problem (1.1) depends on the fractional order \u03b1, coefficients a(x), c(x) as well as the initial value u0(x) and source term f (x). Therefore, based on (2.2), we will use the deep operator learning method to learn the solution of subdiffusion problem (1.1).\nThe deep operator networks is proposed by Lu et al. in [27] and used to learn a operator between infinite-dimensional function spaces. A specific neural network structure is shown in Figure 1 (a). The outputs of the network are aggregated by element-wise product and summation of outputs of two sub-networks, named branch network and trunk network, respectively. Further, in order to learn a multiple inputs map, Jin et al. in [28] proposes a multiple inputs operator network (shown in Figure 1 (b)), to learn the solution operator with multiple inputs. The operator network encodes the input functions by several branch nets and encodes the coordinate of the domain by trunk net, and all branch and trunk nets have the same dimensional outputs, and they are aggregated by element-wise product and summation.\nIn the next section, we will apply deep operator networks to learn the solution operators of the subdiffusion problem (1.1). To sort out the experimental details clearly, we describe basic settings below.\nWe define a PDE solution operator G : v \u2208 V \u2192 u \u2208 U as\nG(v) = u,\nwhereV is a parameter space andU is a solution space. Now, one can represent the solution map by deep operator network GNN(v; \u03b8), the \u03b8 denotes all trainable parameters. Define\nl(v, \u03b8) = 1\nP\nP \u2211\ni=1\n\u2223 \u2223 \u2223GNN(v; \u03b8)(xi) \u2212 u(xi) \u2223 \u2223 \u2223 2 ,\nwhere u(xi) denotes the PDE solution evaluated at location xi. Then, the deep operator network can be trained by minimizing the following loss function:\nL(\u03b8) = 1 N\nN \u2211\nj=1\nl(v j, \u03b8)\n= 1\nNP\nN \u2211\nj=1\nP \u2211\ni=1\n\u2223 \u2223 \u2223GNN(v j; \u03b8)(xi) \u2212 u(v j)(xi) \u2223 \u2223 \u2223 2 .\nWe choose fully connected neural networks as the approximate network. Unless otherwise specified, we use four hidden layers with size 128 for all branch nets and trunk nets. In order to measure the accuracy of the approximate solutions with respect to the reference solutions, we calculate the relative l2 error between the approximate solutions u\u0302a and the reference solutions u\u0302r defined by\nrelative l2 error :=\n\u221a\n\u2211\ni\u22651 |u\u0302a(xi) \u2212 u\u0302r(xi)|2 \u2211\ni\u22651 |u\u0302r(xi)|2 .\nBased on the above introduction, we will study several kinds of operator learning problems in subdiffusion model (1.1)."
        },
        {
            "heading": "2.1. Task 1. Learning solution operator mapping (\u03b1, a) to solution u.",
            "text": "We begin with an example in learning the solution operator that maps fractional order \u03b1 and\ndiffusion coefficient a(x, y) to the solution u(x, y, t) in (2.2), that is, learning the following map\nG : (\u03b1, a)\u2192 u(x, y, t), (x, y, t) \u2208 \u2126\u0304 \u00d7 [0, T ], (2.3)\nwhere we denote the approximate deep operator network as G (\u03b1,a)\nNN . To obtain the training/testing\ndataset, we fix c(x, y) = \u2212xy\u22124, u0(x, y) = 6 sin(2\u03c0x) sin(3\u03c0y) and source f (x, y) = sin(3\u03c0x) sin(\u03c0y)+\n6 exp(x2 + y2). For the diffusion coefficient a(x, y), we randomly sample different functions from a mean-a0 GRF:\na \u223c G(a0, kl(x[1], x[2])),\nwhere the covariance kernel kl(x [1], x[2]) = exp(\u2212\u2016x[1]\u2212x[2]\u20162/2l2) is the radial-basis function (RBF) kernel with a length-scale parameter l > 0, and x[1] = (x1, y1), x [2] = (x2, y2). The length-scale l determines the smoothness of the sampled function, and a larger l leads to a smoother diffusion coefficient a. In this section, we take l = 0.3, a0 = 5. To get the training/testing dataset of \u03b1, we first divide [\u01eb, 1 \u2212 \u01eb] into 20 equal parts, and then randomly select a sample from these points, where we take the \u01eb = 0.001. For the corresponding numerical solution u which obtained by a L1-type finite difference scheme [37], the grid resolutions of time and space variables in the finite difference method are 51 and 101 \u00d7 101. We select 1000 training samples and 500 test samples to train and test the neural network. The training loss reaches a small value after training as shown in Figure 2. We also calculate the relative l2 error between the outputs of the deep operator network G (\u03b1,a)\nNN and the exact solutions on\nthe test dataset, and its average is 0.005912, which means that the trained deep operator network G (\u03b1,a)\nNN can accurately match the numerical solution corresponding to the diffusion coefficient on the\ntest dataset. To visually present the accuracy of outputs of G (\u03b1,a)\nNN , two different input samples a\nare shown in Figure 3 and Figure 5, and its corresponding outputs of the deep operator network as well as the exact solution (finite difference solution) on the test dataset are presented in Figure 4 and Figure 6, respectively. It again shows that the prediction of the deep operator network G (\u03b1,a)\nNN is\nwell agreement with the ground truth.\nNN\non 101 \u00d7 101 grid points corresponding to the fixed reaction coefficient c(x, y) = \u2212xy \u2212 4,\ninitial value u0(x, y) = 6 sin(2\u03c0x) sin(3\u03c0y), source f (x, y) = sin(3\u03c0x) sin(\u03c0y) + 6 exp(x 2 + y2), and the fractional order \u03b1 = 0.6.\nNN\non 101 \u00d7 101 grid points corresponding to the fixed reaction coefficient c(x, y) = \u2212xy \u2212 4,\ninitial value u0(x, y) = 6 sin(2\u03c0x) sin(3\u03c0y), source f (x, y) = sin(3\u03c0x) sin(\u03c0y) + 6 exp(x 2 + y2), and the fractional order \u03b1 = 0.1.\n2.2. Task 2. Learning solution operator mapping (a, f ) to solution u(x, y, t).\nIn this section, we will learn another solution operator that maps any diffusion coefficients\na(x, y) and sources f (x, y) to the solution (2.2), i.e.,\nG : (a, f )\u2192 u(x, y, t), (x, y, t) \u2208 \u2126\u0304 \u00d7 [0, T ], (2.4)\nwhere we denote the approximate deep operator network as G (a, f )\nNN . Similarly, in this part, to get\ntraining and testing dataset, we fixed \u03b1 = 0.5, c(x, y) = 0, u0(x, y) = sin(\u03c0x) sin(\u03c0y). The source f is generated according to f \u223c \u00b5where \u00b5 = N(0, (\u2212\u2206)\u2212s) with the homogeneous Dirichlet boundary conditions on the operator \u2212\u2206, N(0, (\u2212\u2206)\u2212s) denotes the Gaussian measure with mean function 0 and covariance operator (\u2212\u2206)\u2212s. In practice, the random function f given by the Karhunen-Loe\u0300ve expansion\nf =\n\u221e \u2211\nk=1\n\u221a \u03b3k\u03b6k\u03c6k,\nwhere the {\u03b3k, \u03c6k}\u221ek=1 denotes an orthonormal set of eigenvalues/eigenvectors for the operator (\u2212\u2206)\u2212s ordered so that \u03b31 \u2265 \u03b32 \u2265 \u00b7 \u00b7 \u00b7 . The {\u03b6k}\u221ek=1 to be an i.i.d. sequence with \u03b61 \u223c N(0, 1). The diffusion coefficient a, and the numerical solution u of the training/testing dataset are obtained with the same settings as the dataset in the previous section.\nWe take 1000 and 500 samples for aims of training and testing, respectively. Figure 7 shows that the training loss reaches a small value after training. To assess the accuracy of the approximate operator network G (a, f )\nNN in the subdiffusion problem (1.1), two different input samples (a, f ) of\nthe test dataset are shown in Figure 8 and Figure 10 and the predicted outputs of the operator network G (a, f )\nNN , the exact numerical solutions, and the errors at different time are shown in Figure\n9 and Figure 11. Moreover, we calculate the relative l2 error between the outputs of the deep operator network G (a, f )\nNN and the exact solution of the test dataset, and its average is 0.022617.\nThese numerical experiments illustrate that the trained deep operator network can be used as an accurate surrogate of solution operator of the subdiffusion problem (1.1)."
        },
        {
            "heading": "3. Deep Bayesian inversion in subdiffusion",
            "text": "In this section, we will study a few inverse problems that are ill-posed and difficult to solve in the subdiffusion problem (1.1). These difficulties stem from two sources. One is that inverse problems are ill-posed. The essence of inverse problems is to solve the inverse of the forward map. Because the inverse operator is unbounded, the noise in the measurement data is amplified when solving an inverse problem, making it difficult to obtain a stable solution. The complexity of the physical model poses the second challenge. To solve the inverse problems, the forward or adjoint problems of the physical model have to be solved repeatedly. At the same time, for the subdiffusion (1.1), the fractional derivative includes a convolution, which requires much time in\nNN\n.\nNN\non 101 \u00d7 101 grids points corresponding to the fixed fractional order \u03b1 = 0.5, reaction coefficient\nc(x, y) = 0, and initial value u0(x, y) = sin(\u03c0x) sin(\u03c0y).\nNN\n.\nNN\non 101\u00d7 101 grids points corresponding to the fixed fractional order \u03b1 = 0.5, reaction coefficient\nc(x, y) = 0, and initial value u0(x, y) = sin(\u03c0x) sin(\u03c0y).\ndiscretizing the integral when we simulating a forward problem. To summarize, in order to solve the inverse problems of the subdiffusion (1.1), it is necessary to combine effective regularization methods with fast numerical solution methods.\nIn this paper, we apply a Bayesian inversion method to solve inverse problems of the subdiffu-\nsion problem (1.1). The following provides a brief introduction to the Bayesian inversion.\nFirst, we need to define a forward map:\nF(m) = u(x, y, t), (x, y, t) \u2208 \u2126 \u00d7 [0, T ], (3.1)\nwhere m denotes the unknown parameter, u(x, y, t) represents the solution of the problem (1.1). Then the noisy observed data generated by\nd = g(m) + \u03b7, (3.2)\nhere the g = O \u25e6 F, O is an observation operator, and \u03b7 be the observed noise which is statistically independent with the input m and \u03b7 \u223c N(0,\u03a3), where \u03a3 = \u03c32I and\u03c3 denotes the standard deviation of the noise. In the Bayesian framework, the prior of unknown parameter m is described in terms of a probability density function p0(m) and the goal is to find the posterior probability measure pd(m). The probability density function of d given m can be denoted by\np(d|m) = p(d \u2212 g(m)),\nwhere p represents the probability density function of the noise \u03b7. Combining the Bayes formula, then the posterior probability density function of pd(m) is given via\npd(m) \u221d p(d \u2212 g(m))p0(m). (3.3)\nObtaining the posterior probability distribution is the first step in solving Bayesian inverse problems, and how to solve it becomes a complicated and crucial issue. There are many methods available today to solve the Bayesian posterior distribution, including the Kalman filter, Markov chain Monte Carlo, and others. However, these methods need to solve a large number of forward problems repeatedly. At the same time, traditional numerical methods such as the finite element method and the finite difference method are time-consuming, which makes solving the Bayesian inverse problems extremely difficult. Therefore, it is necessary to find a fast and effective numerical method to accelerate the Bayesian inversion. In this paper, we use the trained deep operator network as an approximation operator of the forward operator F to accelerate the Bayesian inversion. This allows us to solve the inverse problems in the subdiffusion problem (1.1) more efficiently."
        },
        {
            "heading": "3.1. Inverse the order of fractional derivative",
            "text": "The rate of particle diffusion is influenced by the fractional order \u03b1 in the time-fractional subdiffusion model (1.1). In many practical applications, however, we cannot know the exact value of the fractional order \u03b1 and need to infer it from the observed data. In the following, we will use the Bayesian inversion method to identify the fractional order \u03b1 based on the given measurement data u(x, y, T ). In Bayesian settings, we take m = \u03b1, d = (u(x1, y1, T ), u(x2, y2, T ), \u00b7 \u00b7 \u00b7 , u(xM, yM , T ))+\u03b7, where M is the total number of sensors. And we solve the Bayesian posterior distribution (3.3) by an iterative regularization ensemble Kalman method (IREKM), which has been widely used in the identification of fractional order in anomalous diffusion problems, such as [38, 39]. The idea of the IREKM is to estimate the Bayesian posterior distribution by generating an ensemble of the unknown, where each ensemble member is updated by iterative formula. Algorithm 1 summarizes the IREKM method\u2019s pseudocodes.\nWe fix c(x, y) = \u2212(xy+4), u0(x, y) = 6 sin(2\u03c0x) sin(3\u03c0y), and source f (x, y) = sin(3\u03c0x) sin(\u03c0y)+ 6 exp(x2 + y2) in the experiment. In Algorithm 1, we take the prior as p0(\u03b1) = U(\u03b1, \u03b1), where \u03b1 = 0.001, \u03b1 = 0.999. In this study, we use the trained deep operator network G (\u03b1,a)\nNN which is\nused to approximate the solution operator (2.3) to simulate forward problems that take a long time to solve using traditional numerical methods, where the diffusion coefficient is fixed. Although we know all the information of region \u2126\u0304 \u00d7 [0, T ] for the approximate solution uNN(x, y, t;\u03b1, a) (the outputs of the operator network G (\u03b1,a)\nNN ), in this inverse problem, we only need the terminal\noutputs of the approximate solution uNN(x, y, t;\u03b1, a), i.e., uNN(x, y, T ;\u03b1, a). Table 1 shows the inversion results of the fractional order \u03b1 when the standard deviation of the noise is 0.001 and 0.003, respectively. Furthermore, in Figure 12, we compare the finite difference solution u(x, y, t) evaluated at t = 1 with the exact \u03b1 and the approximate \u03b1 as an input, respectively. According to the numerical results, the deep Bayesian inversion method can identify the fractional order \u03b1 well, and the numerical solution u(x, y, 1) generated by the approximate \u03b1 is nearly identical to the numerical solution u(x, y, 1) generated by the exact \u03b1.\nAlgorithm 1 IREKM\nInput:\nInitial ensemble of inputs {\u03b1 j 0 }J j=1 from the prior p0(\u03b1). Let \u03bd \u2208 (0, 1) and \u03c4 > 1\u03bd . Measurements d and covariance of measurement errors \u03a3.\nFor n = 0, 1, \u00b7 \u00b7 \u00b7 Prediction:\nCompute G j n = G(\u03b1 j n) for j = {1, \u00b7 \u00b7 \u00b7 , J}. Compute G\u0304n = 1 J \u2211J j=1 G j n.\nDiscrepancy principle:\nLet \u03b4 = |d \u2212G(\u03b1\u2020)|\u03a3, where \u03b1\u2020 denotes the exact fractional order and | \u00b7 |\u03a3 = |\u03a3\u2212 1 2 \u00b7 |. If |d \u2212 G\u0304n|\u03a3 \u2264 \u03c4\u03b4, stop and output:\n\u03b1\u0304n = 1\nJ\nJ \u2211\nj=1\n\u03b1 jn.\nAnalysis:\nUpdate each ensemble member using the following formula\n\u03b1\u0303 j n+1 = \u03b1 jn +C \u03b1G n (C GG n + \u00b5n\u03a3) \u22121(d \u2212G jn), j = {1, 2, \u00b7 \u00b7 \u00b7 , J} \u03b1 j\nn+1 = min{0.999,max(0.001, \u03b1\u0303 j n+1 )},\nwhere\nCGGn = 1\nJ \u2212 1\nJ \u2211\nn=1\n(G jn \u2212 G\u0304n)(G jn \u2212 G\u0304n)T\nC\u03b1Gn = 1\nJ \u2212 1\nJ \u2211\nn=1\n(\u03b1 jn \u2212 \u03b1\u0304n)(G jn \u2212 G\u0304n)T .\nHere \u00b5n is chosen as follows: Let \u00b50 be an initial guess, and \u00b5 i+1 n = 2 i\u00b50. Choose \u00b5n = \u00b5 N n , where N is the first integer s.t.\n\u00b5Nn |\u03a3 1 2 (CGGn + \u00b5 N n \u03a3) \u22121(d \u2212 G\u0304n)| \u2265 \u03bd|\u03a3\u2212 1 2 (d \u2212 G\u0304n)|."
        },
        {
            "heading": "3.2. Inverse the diffusion coefficient from the terminal data",
            "text": "Next, we will consider the inverse diffusion coefficient problem using the known terminal data. Let m = a(x), d = (u(x1, y1, T ), u(x2, y2, T ), \u00b7 \u00b7 \u00b7 , u(xM, yM, T )) + \u03b7, M represents the number of measurement points. To solve the posterior probability distribution, we apply a function space Markov chain Monte Carlo (MCMC) approach in [40]. Algorithm 2 summarizes the pseudocodes of the MCMC method.\nAlgorithm 2 MCMC-pCN\nLet X is a Hilbert space, N(0,C) is a Gaussian measure. Define \u03c1(m, m\u0303) = min{1, exp(\u03a6(m)\u2212\u03a6(m\u0303))} where\u03a6(m) = 1 2 |d\u2212g(m)|2 \u03a3 . The sequences {m(0)}k\u22650 are generated as follows: 1. Set k = 0 and pick m0 \u2208 X; 2. Propose m\u0303k = \u221a\n(1 \u2212 \u03b22)mk + \u03b2\u03c5k, \u03c5k \u223c N(0,C); 3. Set mk+1 = m\u0303k with the probability \u03c1(mk, m\u0303k); 4. Set mk+1 = mk otherwise.\nIt is well known that implementing the MCMC method necessitates repeatedly solving many forward problems. Furthermore, due to the influence of the time-fractional derivative in the subdiffusion problem (1.1), traditional numerical methods require a long time to solve the forward problems, which poses significant challenges to the implementation of the MCMC method. Here, we seek to accelerate the implementation of the MCMC approach by employing the trained deep operator network G (\u03b1,a)\nNN as an approximate operator to the forward operator F.\nWe fix fractional order \u03b1 = 0.5, c(x, y) = \u2212(xy + 4), u0(x, y) = 6 sin(2\u03c0x) sin(3\u03c0y), and source f (x, y) = sin(3\u03c0x) sin(\u03c0y) + 6 exp(x2 + y2) in the experiment. In Algorithm 2, we take C = kl(x [1], x[2]), l = 0.3, \u03b2 = 0.005, where x[1] = (x1, y1), x [2] = (x2, y2). In addition, we run the MCMC sampling in Algorithm 2 for 10000 iterations, and the last 8000 realizations are used to compute the mean of the samples. The exact diffusion coefficient, exact measurement data, and different noises data are shown in Figure 13. The numerical inversion results under various noises\nproduced by combining the MCMC method with the finite difference method and the MCMC method with the deep operator learning method are shown in Figure 14 and Figure 15, respectively. We list the cost time of the MCMC sampling carried out by the deep operator learning approach and the finite difference method in Table 2. In comparison to the time needed by the finite difference method, the MCMC sampling with the operator network G (\u03b1,a)\nNN only takes 67 sec-\nonds, which is quite short. Even though accounting for the data generation and neural network training time which cost 24484 seconds, the time required for the MCMC sampling with the deep operator learning method is far less than the time 66303 seconds spent for the MCMC sampling\nwith the finite difference method.200\nTo solve the inverse problem more effectively, we retrain a new deep operator network that\nmaps any diffusion coefficients a(x, y) to solution u(x, y, T ), i.e.,\nG : a(x, y)\u2192 u(x, y, T ), (x, y) \u2208 \u2126\u0304, (3.4)\nwhere we denote the approximate deep operator network as Ga NN . Here, we use four hidden layers with size 256 for the branch net and trunk net. The optimizer and learning rate during neural network training is Adam and 1 \u00d7 10\u22124, respectively. Similarly, in order to generate the training dataset, we fix \u03b1 = 0.5, c(x, y) = \u2212(xy + 4), u0(x, y) = 6 sin(2\u03c0x) sin(3\u03c0y), and source f (x, y) = sin(3\u03c0x) sin(\u03c0y) + 6 exp(x2 + y2) in the experiment. In addition, we take 1000 samples for aims of training. The settings of the sampling diffusion coefficient a(x, y) and the numerical solutions u(x, y, T ) are identical to those in subsection 2.1.\nFigure 16 shows the numerical inversion results obtained by the MCMC method with the deep\noperator network Ga NN . In addition, in Table 3, we list the relative l2 errors of numerical inversion results generated by above methods. In comparison to the MCMC method with the deep operator network G (\u03b1,a)\nNN , the MCMC sampling with the deep operator network Ga NN produces more accurate\nnumerical inversion results. Moreover, the cost time of data generation and network training is 6881 seconds, and the inference time is 42 seconds, which is less than the time 66303 seconds required by the MCMC sampling with the finite difference method.\n, and the operator network Ga\nto simulate the forward problems.\nNN\n, MCMC+Ga\nNN\nindicate that MCMC sampling is carried\nout using the finite difference method, the operator network G (\u03b1,a) NN , and the operator network Ga NN to simulate the forward problems.\nNoise\nRelative l2 error Methods MCMC+FDM MCMC+G (\u03b1,a)\nNN MCMC+Ga NN\n\u03c3 = 0.001 0.006282 0.018877 0.011988 \u03c3 = 0.005 0.011683 0.022514 0.014754"
        },
        {
            "heading": "3.3. Inverse diffusion coefficient from interior measurements",
            "text": "In this part, we consider the inverse diffusion coefficient a(x, y) in the subdiffusion problem (1.1) based on the interior measurement data u(x, y, t), where (x, y) \u2208 \u03c9 = [0.25, 0.75]2, t \u2208 [0, T ] in (3.1). Similarly, in the experiment, we fix \u03b1 = 0.7, reaction coefficient c(x, y) = \u2212(xy+4), initial value u0(x, y) = 6 sin(2\u03c0x) sin(3\u03c0y), source f (x, y) = sin(3\u03c0x) sin(\u03c0y) + 6 exp(x 2 + y2). Instead of retraining a new deep operator network for this inverse problem, we use the trained deep operator network G (\u03b1,a)\nNN which is used to approximate the solution operator (2.3) to carry out the MCMC\nsampling. Figure 17 shows the exact diffusion coefficient. Figure 18 shows the numerical inversion results obtained by MCMC sampling with the operator network G\u03b1,a NN . According to the figure, using the MCMC sampling with the deep operator network G (\u03b1,a)\nNN can produce relatively accurate\nnumerical inversion results. This numerical experiment demonstrates that, once trained, the deep operator network can be used to solve the different inverse diffusion coefficient problems of the subdiffusion problem (1.1)."
        },
        {
            "heading": "4. Conclusion",
            "text": "A deep learning method for solving the forward problems and inverse problems in subdiffusion is presented. By using the deep operator network, we study two kinds of operator learning problems of the subdiffusion problem (1.1) through various numerical experiments. Furthermore, we apply a Bayesian inversion method to solve several inverse problems in the subdiffusion problem. To overcome the problem of time-consuming Bayesian inference with conventional numerical methods, we propose a BINO approach for modelling a subdiffusion problem and solving a inverse diffusion coefficient problem in the paper. Several numerical experiments show that the operator learning method we provide can effectively solve the forward problems and the Bayesian inverse problems in the subdiffusion model.\nAcknowledgments This work is sponsored by the National Key R&D Program of China Grant No. 2019YFA0709503 (Z. X.) and No. 2020YFA0712000 (Z. M.), the Shanghai Sailing Program (Z. X.), the Natural Science Foundation of Shanghai Grant No. 20ZR1429000 (Z. X.), the National Natural Science Foundation of China Grant No. 62002221 (Z. X.), the National Natural\nNN\nfrom the interior\nmeasurement data that was polluted by noise with 0-mean, and the standard deviation \u03c3 is 0.001 and 0.005, respectively.\nScience Foundation of China Grant No. 12101401 (Z. M.), the National Natural Science Foundation of China Grant No. 12031013 (Z. M.), Shanghai Municipal of Science and Technology Major Project No. 2021SHZDZX0102, and the HPC of School of Mathematical Sciences and the Student Innovation Center at Shanghai Jiao Tong University."
        }
    ],
    "title": "Bayesian Inversion with Neural Operator (BINO) for Modeling Subdiffusion: Forward and Inverse Problems",
    "year": 2022
}