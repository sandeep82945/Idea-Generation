{
    "abstractText": "Differentiable Architecture Search (DARTS) is a simple yet efficient Neural Architecture Search (NAS) method. During the search stage, DARTS trains a supernet by jointly optimizing architecture parameters and network parameters. During the evaluation stage, DARTS discretizes the supernet to derive the optimal architecture based on architecture parameters. However, recent research has shown that during the training process, the supernet tends to converge towards sharp minima rather than flat minima. This is evidenced by the higher sharpness of the loss landscape of the supernet, which ultimately leads to a performance gap between the supernet and the optimal architecture. In this paper, we propose Self-Distillation Differentiable Neural Architecture Search (SD-DARTS) to alleviate the discretization gap. We utilize self-distillation to distill knowledge from previous steps of the supernet to guide its training in the current step, effectively reducing the sharpness of the supernet\u2019s loss and bridging the performance gap between the supernet and the optimal architecture. Furthermore, we introduce the concept of voting teachers, where multiple previous supernets are selected as teachers, and their output probabilities are aggregated through voting to obtain the final teacher prediction. Experimental results on real datasets demonstrate the advantages of our novel self-distillation-based NAS method compared to state-of-the-art alternatives.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xunyu Zhu"
        },
        {
            "affiliations": [],
            "name": "Jian Lia"
        },
        {
            "affiliations": [],
            "name": "Yong Liu"
        },
        {
            "affiliations": [],
            "name": "Weiping Wang"
        }
    ],
    "id": "SP:b3b4d95b9c1672b4d623f052cdb30008cf72c209",
    "references": [
        {
            "authors": [
                "J. Deng",
                "W. Dong",
                "R. Socher",
                "L.-J. Li",
                "K. Li",
                "L. Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "in: IEEE conference on computer vision and pattern recognition (CVPR)",
            "year": 2009
        },
        {
            "authors": [
                "A. Yilmaz",
                "O. Javed",
                "M. Shah"
            ],
            "title": "Object tracking: A survey",
            "venue": "Acm computing surveys (CSUR) 38 (4) ",
            "year": 2006
        },
        {
            "authors": [
                "M. van Heel",
                "G. Harauz",
                "E.V. Orlova",
                "R. Schmidt",
                "M. Schatz"
            ],
            "title": "A new generation of the imagic image processing system, Journal of structural biology",
            "year": 1996
        },
        {
            "authors": [
                "H. Li",
                "D. Doermann",
                "O. Kia"
            ],
            "title": "Automatic text detection and tracking in digital video",
            "venue": "IEEE transactions on image processing 9 (1) ",
            "year": 2000
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for largescale image recognition",
            "venue": "in: Y. Bengio, Y. LeCun (Eds.), International Conference on Learning Representations (ICLR)",
            "year": 2015
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "in: Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)",
            "year": 2016
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "L. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems (NeurIPS) 30 ",
            "year": 2017
        },
        {
            "authors": [
                "J. Devlin",
                "M. Chang",
                "K. Lee",
                "K. Toutanova"
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "in: J. Burstein, C. Doran, T. Solorio (Eds.), Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)",
            "year": 2019
        },
        {
            "authors": [
                "T. Brown",
                "B. Mann",
                "N. Ryder",
                "M. Subbiah",
                "J.D. Kaplan",
                "P. Dhariwal",
                "A. Neelakantan",
                "P. Shyam",
                "G. Sastry"
            ],
            "title": "A",
            "venue": "Askell, et al., Language models are few-shot learners, Advances in neural information processing systems (NeurIPS) 33 ",
            "year": 2020
        },
        {
            "authors": [
                "B. Zoph",
                "Q.V. Le"
            ],
            "title": "Neural architecture search with reinforcement learning",
            "venue": "in: International Conference on Learning Representations (ICLR)",
            "year": 2017
        },
        {
            "authors": [
                "E. Real",
                "A. Aggarwal",
                "Y. Huang",
                "Q.V. Le"
            ],
            "title": "Regularized evolution for image classifier architecture search",
            "venue": "in: Proceedings of the aaai conference on artificial intelligence (AAAI), Vol. 33",
            "year": 2019
        },
        {
            "authors": [
                "K. Ostad-Ali-Askari",
                "M. Shayan"
            ],
            "title": "Subsurface drain spacing in the unsteady conditions by hydrus-3d and artificial neural networks",
            "venue": "Arabian Journal of Geosciences 14 ",
            "year": 2021
        },
        {
            "authors": [
                "B. Zoph",
                "V. Vasudevan",
                "J. Shlens",
                "Q.V. Le"
            ],
            "title": "Learning transferable architectures for scalable image recognition",
            "venue": "in: Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)",
            "year": 2018
        },
        {
            "authors": [
                "K. Ostad-Ali-Askari",
                "M. Shayannejad",
                "H. Ghorbanizadeh-Kharazi"
            ],
            "title": "Artificial neural network for modeling nitrate pollution of groundwater in marginal area of zayandeh-rood river",
            "venue": "isfahan, iran, KSCE Journal of Civil Engineering 21 ",
            "year": 2017
        },
        {
            "authors": [
                "H. Pham",
                "M. Guan",
                "B. Zoph",
                "Q. Le",
                "J. Dean"
            ],
            "title": "Efficient neural architecture search via parameters sharing",
            "venue": "in: International conference on machine learning (ICML)",
            "year": 2018
        },
        {
            "authors": [
                "Z. Guo",
                "X. Zhang",
                "H. Mu",
                "W. Heng",
                "Z. Liu",
                "Y. Wei",
                "J. Sun"
            ],
            "title": "Single path one-shot neural architecture search with uniform sampling",
            "venue": "in: A. Vedaldi, H. Bischof, T. Brox, J. Frahm (Eds.), European Conference on Computer Vision (ECCV), Vol. 12361",
            "year": 2020
        },
        {
            "authors": [
                "R. Luo",
                "F. Tian",
                "T. Qin",
                "E. Chen",
                "T.-Y. Liu"
            ],
            "title": "Neural architecture optimization",
            "venue": "Advances in neural information processing systems (NeurIPS) 31 ",
            "year": 2018
        },
        {
            "authors": [
                "Y. Xu",
                "L. Xie",
                "X. Zhang",
                "X. Chen",
                "G. Qi",
                "Q. Tian",
                "H. Xiong"
            ],
            "title": "PC-DARTS: partial channel connections for memory-efficient architecture search",
            "venue": "in: International Conference on Learning Representations (ICLR)",
            "year": 2020
        },
        {
            "authors": [
                "G. Li",
                "G. Qian",
                "I.C. Delgadillo",
                "M. Muller",
                "A. Thabet",
                "B. Ghanem"
            ],
            "title": "Sgas: Sequential greedy architecture search",
            "venue": "in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2020
        },
        {
            "authors": [
                "A. Zela",
                "T. Elsken",
                "T. Saikia",
                "Y. Marrakchi",
                "T. Brox",
                "F. Hutter"
            ],
            "title": "Understanding and robustifying differentiable architecture search",
            "venue": "in: International Conference on Learning Representations (ICLR)",
            "year": 2020
        },
        {
            "authors": [
                "X. Chen",
                "C.-J. Hsieh"
            ],
            "title": "Stabilizing differentiable architecture search via perturbation-based regularization",
            "venue": "in: International conference on machine learning (ICML)",
            "year": 2020
        },
        {
            "authors": [
                "X. Chen",
                "L. Xie",
                "J. Wu",
                "Q. Tian"
            ],
            "title": "Progressive DARTS: bridging the optimization gap for NAS in the wild",
            "venue": "Int. J. Comput. Vis. 129 (3) ",
            "year": 2021
        },
        {
            "authors": [
                "H. Liang",
                "S. Zhang",
                "J. Sun",
                "X. He",
                "W. Huang",
                "K. Zhuang",
                "Z. Li"
            ],
            "title": "DARTS+: improved differentiable architecture search with early stopping",
            "venue": "CoRR abs/1909.06035 ",
            "year": 2019
        },
        {
            "authors": [
                "R. Wang",
                "M. Cheng",
                "X. Chen",
                "X. Tang",
                "C. Hsieh"
            ],
            "title": "Rethinking architecture selection in differentiable NAS",
            "venue": "in: International Conference on Learning Representations (ICLR)",
            "year": 2021
        },
        {
            "authors": [
                "Y. Tian",
                "C. Liu",
                "L. Xie"
            ],
            "title": "Q",
            "venue": "Ye, et al., Discretization-aware architecture search, Pattern Recognition 120 ",
            "year": 2021
        },
        {
            "authors": [
                "K. Bi",
                "L. Xie",
                "X. Chen",
                "L. Wei",
                "Q. Tian"
            ],
            "title": "GOLD-NAS: gradual",
            "venue": "one-level, differentiable, CoRR abs/2007.03331 ",
            "year": 2020
        },
        {
            "authors": [
                "X. Chu",
                "X. Wang",
                "B. Zhang",
                "S. Lu",
                "X. Wei",
                "J. Yan"
            ],
            "title": "DARTS-: robustly stepping out of performance collapse without indicators",
            "venue": "in: International Conference on Learning Representations (ICLR)",
            "year": 2021
        },
        {
            "authors": [
                "Y. Lin",
                "C. Wang",
                "C. Chang",
                "H. Sun"
            ],
            "title": "An efficient framework for counting pedestrians crossing a line using low-cost devices: the benefits of distilling the knowledge in a neural network",
            "venue": "Multim. Tools Appl. 80 (3) ",
            "year": 2021
        },
        {
            "authors": [
                "L. Zhang",
                "J. Song",
                "A. Gao",
                "J. Chen",
                "C. Bao",
                "K. Ma"
            ],
            "title": "Be your own teacher: Improve the performance of convolutional neural networks via self distillation",
            "venue": "in: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)",
            "year": 2019
        },
        {
            "authors": [
                "S. Yun",
                "J. Park",
                "K. Lee",
                "J. Shin"
            ],
            "title": "Regularizing class-wise predictions via self-knowledge distillation",
            "venue": "in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)",
            "year": 2020
        },
        {
            "authors": [
                "A. Tarvainen",
                "H. Valpola"
            ],
            "title": "Mean teachers are better role models: Weightaveraged consistency targets improve semi-supervised deep learning results",
            "venue": "Advances in neural information processing systems (NeurIPS) 30 ",
            "year": 2017
        },
        {
            "authors": [
                "K. Li",
                "S. Wang",
                "L. Yu",
                "P.A. Heng"
            ],
            "title": "Dual-teacher++: Exploiting intradomain and inter-domain knowledge with reliable transfer for cardiac segmentation",
            "venue": "IEEE Transactions on Medical Imaging 40 (10) ",
            "year": 2020
        },
        {
            "authors": [
                "Z. Zhao",
                "F. Zhou",
                "K. Xu",
                "Z. Zeng",
                "C. Guan",
                "S.K. Zhou"
            ],
            "title": "Le-uda: Labelefficient unsupervised domain adaptation for medical image segmentation",
            "venue": "IEEE Transactions on Medical Imaging 42 (3) ",
            "year": 2022
        },
        {
            "authors": [
                "S. Li",
                "Z. Zhao",
                "K. Xu",
                "Z. Zeng",
                "C. Guan"
            ],
            "title": "Hierarchical consistency regularized mean teacher for semi-supervised 3d left atrium segmentation",
            "venue": "28 in: International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), IEEE",
            "year": 2021
        },
        {
            "authors": [
                "Z. Zhao",
                "A. Zhu",
                "Z. Zeng",
                "B. Veeravalli",
                "C. Guan"
            ],
            "title": "Act-net: Asymmetric co-teacher network for semi-supervised memory-efficient medical image segmentation",
            "venue": "in: IEEE International Conference on Image Processing (ICIP)",
            "year": 2022
        },
        {
            "authors": [
                "H.-R. Wei",
                "S. Huang",
                "R. Wang",
                "X. Dai",
                "J. Chen"
            ],
            "title": "Online distilling from checkpoints for neural machine translation",
            "venue": "in: Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)",
            "year": 2019
        },
        {
            "authors": [
                "P. Foret",
                "A. Kleiner",
                "H. Mobahi",
                "B. Neyshabur"
            ],
            "title": "Sharpness-aware minimization for efficiently improving generalization",
            "venue": "in: International Conference on Learning Representations (ICLR)",
            "year": 2021
        },
        {
            "authors": [
                "J. Kwon",
                "J. Kim",
                "H. Park",
                "I.K. Choi"
            ],
            "title": "Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks",
            "venue": "in: Proceedings of the 38th International Conference on Machine Learning (ICML), Vol. 139",
            "year": 2021
        },
        {
            "authors": [
                "Y. Zhao",
                "H. Zhang",
                "X. Hu"
            ],
            "title": "Penalizing gradient norm for efficiently improving generalization in deep learning",
            "venue": "in: International Conference on Machine Learning (ICML), Vol. 162",
            "year": 2022
        },
        {
            "authors": [
                "J. Du",
                "D. Zhou",
                "J. Feng",
                "V. Tan",
                "J.T. Zhou"
            ],
            "title": "Sharpness-aware training for free",
            "venue": "in: NeurIPS",
            "year": 2022
        },
        {
            "authors": [
                "H. Liu",
                "K. Simonyan",
                "Y. Yang"
            ],
            "title": "DARTS: Differentiable architecture search",
            "venue": "in: International Conference on Learning Representations (ICLR)",
            "year": 2019
        },
        {
            "authors": [
                "A. Krizhevsky"
            ],
            "title": "G",
            "venue": "Hinton, et al., Learning multiple layers of features from tiny images ",
            "year": 2009
        },
        {
            "authors": [
                "G. Huang",
                "Z. Liu"
            ],
            "title": "L",
            "venue": "Van Der Maaten, K. Q. Weinberger, Densely connected convolutional networks, in: Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)",
            "year": 2017
        },
        {
            "authors": [
                "C. Liu",
                "B. Zoph",
                "M. Neumann",
                "J. Shlens",
                "W. Hua",
                "L.-J. Li",
                "L. Fei-Fei",
                "A. Yuille",
                "J. Huang",
                "K. Murphy"
            ],
            "title": "Progressive neural architecture search",
            "venue": "in: Proceedings of the European conference on computer vision (ECCV)",
            "year": 2018
        },
        {
            "authors": [
                "S. Xie",
                "H. Zheng",
                "C. Liu",
                "L. Lin"
            ],
            "title": "SNAS: stochastic neural architecture search",
            "venue": "in: International Conference on Learning Representations (ICLR)",
            "year": 2019
        },
        {
            "authors": [
                "H. Cai",
                "L. Zhu",
                "S. Han"
            ],
            "title": "Proxylessnas: Direct neural architecture search on target task and hardware",
            "venue": "in: International Conference on Learning Representations (ICLR)",
            "year": 2019
        },
        {
            "authors": [
                "P. Ye",
                "B. Li",
                "Y. Li",
                "T. Chen",
                "J. Fan",
                "W. Ouyang"
            ],
            "title": "\u03b2-darts: Beta-decay regularization for differentiable architecture search",
            "venue": "in: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2022
        },
        {
            "authors": [
                "X. Dong",
                "Y. Yang"
            ],
            "title": "Searching for a robust neural architecture in four gpu hours",
            "venue": "in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2019
        },
        {
            "authors": [
                "L. Huang",
                "S. Sun",
                "J. Zeng",
                "W. Wang",
                "W. Pang",
                "K. Wang"
            ],
            "title": "U-DARTS: uniform-space differentiable architecture search",
            "venue": "Inf. Sci. 628 ",
            "year": 2023
        },
        {
            "authors": [
                "Y. Li",
                "S. Li",
                "Z. Yu"
            ],
            "title": "DARTS-PAP: differentiable neural architecture search by polarization of instance complexity weighted architecture parameters",
            "venue": "in: MultiMedia Modeling (MMM), Vol. 13834",
            "year": 2023
        },
        {
            "authors": [
                "C. Szegedy",
                "W. Liu",
                "Y. Jia",
                "P. Sermanet",
                "S. Reed",
                "D. Anguelov",
                "D. Erhan",
                "V. Vanhoucke",
                "A. Rabinovich"
            ],
            "title": "Going deeper with convolutions",
            "venue": "in: Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)",
            "year": 2015
        },
        {
            "authors": [
                "A.G. Howard",
                "M. Zhu",
                "B. Chen",
                "D. Kalenichenko",
                "W. Wang",
                "T. Weyand",
                "M. Andreetto",
                "H. Adam"
            ],
            "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
            "venue": "CoRR abs/1704.04861 ",
            "year": 2017
        },
        {
            "authors": [
                "X. Zhang",
                "X. Zhou",
                "M. Lin",
                "J. Sun"
            ],
            "title": "Shufflenet: An extremely efficient convolutional neural network for mobile devices",
            "venue": "in: Proceedings of the 30 IEEE conference on computer vision and pattern recognition (CVPR)",
            "year": 2018
        },
        {
            "authors": [
                "N. Ma",
                "X. Zhang",
                "H.-T. Zheng",
                "J. Sun"
            ],
            "title": "Shufflenet v2: Practical guidelines for efficient cnn architecture design",
            "venue": "in: Proceedings of the European conference on computer vision (ECCV)",
            "year": 2018
        },
        {
            "authors": [
                "Y. Liu",
                "X. Jia",
                "M. Tan",
                "R. Vemulapalli",
                "Y. Zhu",
                "B. Green",
                "X. Wang"
            ],
            "title": "Search to distill: Pearls are everywhere but not the eyes",
            "venue": "in: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR",
            "year": 2020
        },
        {
            "authors": [
                "X. Chu",
                "B. Zhang",
                "R. Xu"
            ],
            "title": "Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search",
            "venue": "in: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)",
            "year": 2021
        },
        {
            "authors": [
                "M. Tan",
                "B. Chen",
                "R. Pang",
                "V. Vasudevan",
                "M. Sandler",
                "A. Howard",
                "Q.V. Le"
            ],
            "title": "Mnasnet: Platform-aware neural architecture search for mobile",
            "venue": "in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2019
        },
        {
            "authors": [
                "O. Russakovsky",
                "J. Deng",
                "H. Su",
                "J. Krause",
                "S. Satheesh",
                "S. Ma",
                "Z. Huang",
                "A. Karpathy",
                "A. Khosla"
            ],
            "title": "M",
            "venue": "Bernstein, et al., Imagenet large scale visual recognition challenge, International journal of computer vision 115 (3) ",
            "year": 2015
        },
        {
            "authors": [
                "X. Dong",
                "Y. Yang"
            ],
            "title": "Nas-bench-201: Extending the scope of reproducible neural architecture search",
            "venue": "in: International Conference on Learning Representations (ICLR)",
            "year": 2020
        },
        {
            "authors": [
                "S. Hu",
                "S. Xie",
                "H. Zheng",
                "C. Liu",
                "J. Shi",
                "X. Liu",
                "D. Lin"
            ],
            "title": "Dsnas: Direct neural architecture search without parameter retraining",
            "venue": "in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2020
        },
        {
            "authors": [
                "M. Zhang",
                "S.W. Su",
                "S. Pan",
                "X. Chang",
                "E.M. Abbasnejad",
                "R. Haffari"
            ],
            "title": "idarts: Differentiable architecture search with stochastic implicit gradients",
            "venue": "in: Proceedings of the 38th International Conference on Machine Learning (ICML), Vol. 139",
            "year": 2021
        },
        {
            "authors": [
                "X. Chu",
                "X. Wang",
                "B. Zhang",
                "S. Lu",
                "X. Wei",
                "J. Yan"
            ],
            "title": "DARTS}-: Robustly stepping out of performance collapse without indicators",
            "venue": "in: International Conference on Learning Representations (ICLR)",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Differentiable Architecture Search (DARTS) is a simple yet efficient Neural Architecture Search (NAS) method. During the search stage, DARTS trains a supernet by jointly optimizing architecture parameters and network parameters. During the evaluation stage, DARTS discretizes the supernet to derive the optimal architecture based on architecture parameters. However, recent research has shown that during the training process, the supernet tends to converge towards sharp minima rather than flat minima. This is evidenced by the higher sharpness of the loss landscape of the supernet, which ultimately leads to a performance gap between the supernet and the optimal architecture. In this paper, we propose Self-Distillation Differentiable Neural Architecture Search (SD-DARTS) to alleviate the discretization gap. We utilize self-distillation to distill knowledge from previous steps of the supernet to guide its training in the current step, effectively reducing the sharpness of the supernet\u2019s loss and bridging the performance gap between the supernet and the optimal architecture. Furthermore, we introduce the concept of voting teachers, where multiple previous supernets are selected as teachers, and their output probabilities are aggregated through voting to obtain the final teacher prediction. Experimental results on real datasets demonstrate the advantages of our novel self-distillation-based NAS method compared to state-of-the-art alternatives.\nKeywords: neural architecture search, neural networks, flatness, knowledge\n\u2217Corresponding author Email addresses: zhuxunyu@iie.ac.cn (Xunyu Zhu), lijian9026@iie.ac.cn (Jian\nLi), liuyonggsai@ruc.edu.cn (Yong Liu), wangweiping@iie.ac.cn (Weiping Wang)\nPreprint submitted to Neural Networks September 4, 2023\nar X\niv :2\n30 2.\n05 62\n9v 2\n[ cs\n.C V\n] 1\nS ep\ndistillation, sharpness-aware minimization"
        },
        {
            "heading": "1. Introduction",
            "text": "Deep learning has gained significant popularity across various domains due to its ability to automate feature learning and achieve impressive performance. It has been successfully applied in areas such as image classification [1], object tracking [2], image processing [3], and text detection [4]. To enhance the efficiency of deep learning, several outstanding architectures have been developed by researchers and professors, including VGG [5], ResNet [6], Transformer [7], Bert [8], and GPT [9]. These architectures have significantly contributed to improving the performance of deep learning models.\nHowever, designing high-quality architectures manually can be a laborious and time-consuming process. The cost of manual architecture design is often high, requiring extensive expertise and trial-and-error experimentation. As a result, Neural Architecture Search (NAS) has gained popularity in academia and industry as it enables machines to automatically discover architectures with good performance. Initially, NAS approaches such as Reinforcement Learning [10] and Evolutionary Algorithms [11, 12] are introduced, which involve sampling and evaluating individual architectures by training them from scratch. However, these methods face challenges in dealing with the exponentially increasing search space, resulting in extensive computational resources being required to find the optimal architecture. NASNet [13, 14] introduces a cell-based approach, where individual cells are searched and then stacked together to form the final network. Another approach, ENAS [15], introduces the concept of a supernet, where a single network is trained with shared weights that serve as a proxy for evaluating the performance of individual subnetworks. These methods significantly reduce the computational cost associated with NAS. However, NAS is still considered a discrete optimization problem. In response, gradient-based methods such as SPOS [16] and NAO [17] are proposed. These methods relaxes the NAS problem as a continuous optimization problem and utilized gradient-based optimizers to explore the continuous search space. Among these methods, DARTS stands out as one of the most successful gradient-based approaches, offering high search efficiency.\nDifferentiable Architecture Search (DARTS) revolutionizes NAS by formulating it as a continuous optimization problem and leveraging gradient-based\noptimization techniques. DARTS achieves low search cost, requiring only 0.3 GPU days to discover high-quality architectures. To further improve efficiency, PC-DARTS [18] introduces partial channel connections and edge normalization to reduce memory consumption and computational overhead. SGAS [19] introduces sequential greedy architecture search to address the degenerate search-evaluation correlation problem. During the evaluation stage of DARTS, the supernet is discretized to determine the optimal architecture. However, despite consistently reducing the validation error of the supernet and the optimal architecture, a performance gap remains during evaluation. Several papers [20, 21] have discussed this issue and attributed the performance gap to the geometry of the supernet\u2019s loss landscape. SDARTS [21] and R-DARTS [20] identifiy the sharpness of the validation loss landscape in DARTS as the cause of the performance gap. SDARTS introduces the generation of weight perturbations to improve the supernet\u2019s training process, but this approach incured a significant computational cost. R-DARTS uses the dominant eigenvalue of the Hessian norm as an indicator, employing early stopping when the indicator reaches a threshold. However, R-DARTS prevents DARTS from exploring architectures with potentially better performance. Alleviating the discretization gap by flattening the supernet\u2019s loss landscape remains an open question that warrants further exploration.\nOur paper introduces a novel method called Self-Distillation DARTS (SDDARTS), which is illustrated in Fig. 1. SD-DARTS leverages knowledge distillation to guide the training of the supernet. Specifically, it transfers the knowledge learned by the supernet in the previous time step to guide the training of the current supernet. By enforcing DARTS to consider information from previous epochs during optimization, SD-DARTS reduces the sharpness of the supernet\u2019s loss landscape. In contrast to existing approaches, our method utilizes the information from previous epochs instead of generating weight perturbations. Additionally, we allow the supernet to be trained until convergence. These distinguishing factors highlight the novelty and value of our method in improving the performance of DARTS.\nAdditionally, we recognize that the information provided by a single teacher in SD-DARTS may not be sufficient. Ensemble learning suggests that the predictions of multiple models combined together are often more accurate than those of a single model, as the collective knowledge from multiple models is typically more comprehensive. Inspired by ensemble learning, we introduce the concept of \u201cvoting teachers\u201d to guide the training of the supernet. In practice, we aim to leverage the knowledge from multiple supernets in the\nprevious K time steps and aggregate their predictions through voting. By combining the predictions of these supernets, we generate the final teacher output probability. This integration of SD-DARTS with voting teachers enables us to explore and discover high-quality architectures more effectively.\nWe conduct a series of experiments to evaluate the effectiveness of our proposed method in improving the performance of DARTS. We utilize our method to search for high-quality architectures and assess their performance on various datasets. On the CIFAR-10 dataset, our architecture achieved a remarkable test error rate of 2.58%, surpassing the performance of DARTS and lots of its variants. The result highlights the superiority of our approach in producing architectures with improved performance on this dataset. Furthermore, we evaluated the performance of our architecture on the challenging ImageNet dataset and achieved a test error rate of 25.0%. These results reinforce the effectiveness of our novel self-distillation-based NAS method and demonstrate its advantages over state-of-the-art alternatives. Through these experiments on real datasets, we provide empirical evidence to support the efficacy and superiority of our proposed method in generating high-quality architectures with improved performance.\nOur contributions can be summarized as follows:\n\u2022 We propose SD-DARTS, a new DARTS method with better performance. It distills the knowledge from the previous steps of the supernet to guide the training of the supernet in the current step. It can flatten the supernet\u2019s loss landscape to bridge the performance gap between the supernet and the optimal subnet.\n\u2022 We propose voting teachers, a new method by using multiple teachers to guide the training of the supernet. The predictions of voting teachers are more accurate than a single teacher. We vote the predications of supernets in the previous K time steps to generate the final teacher output probability and use the teacher output probability to guide the training of the supernet itself.\n\u2022 Extensive experiments demonstrate that our method can effectively improve the performance of DARTS and achieves good results on mainstream datasets."
        },
        {
            "heading": "2. Related Work",
            "text": ""
        },
        {
            "heading": "2.1. Differentiable Architecture Search",
            "text": "Differentiable Architecture Search (DARTS) has gained popularity as a lowcost NAS method by formulating NAS as a continuous optimization problem. Several methods have been proposed to enhance the performance of DARTS. P-DARTS [22] gradually increases the depth of the supernet to address the discrepancy between architecture depths in search and evaluation scenarios. RDARTS [20] employes early stopping based on hessian eigenvalues to monitor changes in the optimization process. SDARTS [21] introduces perturbations on architecture parameters to control hessian eigenvalues and flatten the supernet\u2019s loss landscape. DARTS+ [23] incorporates early stopping when the number of skip connections in the normal cell exceeds a threshold. DARTSPT [24] introduces an alternative perturbation-based architecture selection method. DAAS [25] introduces an entropy-based loss term to guide the supernetwork towards the desired topology. Gold-nas [26] employes a variable resource constraint in one-level optimization to gradually prune out weaker operators. DARTS- [27] addes an auxiliary skip connection to ensure fair competition among all operations. Different from these works, we aim to alleviate the discretization gap of DARTS by focusing on flattening the supernet\u2019s loss landscape with less computational cost."
        },
        {
            "heading": "2.2. Self Distillation",
            "text": "Knowledge Distillation (KD) [28] is a technique that transfers the knowledge of a well-trained teacher model to a student model during training.\nTraditionally, a well-trained teacher model with higher capacity than the student model is used to guide the training process. However, obtaining a well-trained teacher model can be challenging. To address this issue, recent works [29, 30] have explored the use of self-distillation, where the student model itself is used as a teacher to distill its own knowledge.\nSelf-distillation has gained popularity in various domains. For instance, Mean Teacher [31] combines self-distillation with semi-supervised learning to improve performance. It minimizes the L2 distance between the teacher model\u2019s predictions and the student model\u2019s predictions during semisupervised learning, where the teacher model is the student model itself. Dual-Teacher++ [32] introduces a state-of-the-art semi-supervised domain adaptation framework using dual teacher models. It includes an inter-domain teacher model that explores cross-modality priors from the source domain and an intra-domain teacher model that investigates the knowledge within the unlabeled target domain. LE-UDA [33] introduces a self-ensembling consistency approach for knowledge transfer in unsupervised domain adaptation, combining it with a self-ensembling adversarial learning module to achieve better feature alignment. Li et al. [34] proposes a hierarchical consistency regularized mean teacher framework for 3D left atrium segmentation, where the student model is optimized using multi-scale deep supervision and hierarchical consistency regularization. ACT-NET [35] advances teacher-student learning with a co-teacher network, facilitating knowledge distillation from large models to small ones by alternating student and teacher roles. ODC [36] applies self-distillation to neural machine translation tasks, updating the teacher model when its performance surpasses the current teacher model on the validation data. In our paper, we aim to combine self-distillation with DARTS to enhance the performance of architecture search. By leveraging the knowledge distilled from the student model itself, we expect to improve the quality of the generated architectures in DARTS."
        },
        {
            "heading": "2.3. Sharpness Aware Minimisation",
            "text": "Sharpness-aware minimization (SAM) [37] is a regularization technique that aims to flatten a network\u2019s loss landscape. SAM achieves this by introducing adversarial noise to the network parameters during training, which encourages the loss to be more stable and less sensitive to small parameter perturbations. By promoting a flatter loss landscape, SAM can enhance the generalization ability of the model. ASAM [38] builds upon SAM by utilizing a generalization bound to establish a connection between the sharpness of the\nloss landscape and the generalization gap. By minimizing the sharpness of the loss landscape, ASAM aims to reduce overfitting and improve the model\u2019s ability to generalize to unseen data. Zhao et al. [39] proposes to penalize the gradient norm of the loss function during optimization to flatten the network\u2019s loss landscape. By regularizing the gradient norm, the optimization process becomes more stable and less prone to sharp fluctuations, leading to a flatter loss landscape. SAF [40] addresses the issue of sudden drops in loss that can occur in sharp local minima during the trajectory of weight updates. SAF aims to avoid these sudden drops by dynamically adjusting the learning rate based on the sharpness of the loss landscape. By preventing sharp drops in loss, SAF promotes a flatter optimization process and helps the model converge to better solutions. These approaches highlight the importance of considering the flatness of the loss landscape in training deep neural networks. By promoting a flatter and more stable loss landscape, these techniques aim to improve the generalization ability and convergence properties of the models."
        },
        {
            "heading": "3. Methodology",
            "text": ""
        },
        {
            "heading": "3.1. Preliminary",
            "text": "Differentiable Architecture Search (DARTS) [41] is a popular Neural Architecture Search (NAS) method known for its high search efficiency and low computational cost. Unlike searching for the entire network, DARTS focuses on searching for cells within the network. Each cell is represented as a directed acyclic graph (DAG) with multiple nodes, where each node represents a feature map. The cell has two input nodes, one output node, and several intermediate nodes. The information from the input nodes is propagated through the intermediate nodes to the output node using operations associated with directed edges. To determine the importance of different operations in the candidate operation space O, DARTS assigns weights to each operation using architecture parameters \u03b1(i,j). The weighted sum of operations is computed\nas o\u0304(i,j)(x) = \u2211\no\u2208O exp\n( \u03b1 (i,j) o ) \u2211\no\u2032\u2208O exp ( \u03b1 (i,j)\no\u2032 )o(x). The architecture parameters control the importance of each operation on the corresponding edge.\nThe search process of DARTS can be formulated as a bilevel optimization problem, where the goal is to minimize the validation loss Lval with respect to the architecture parameters \u03b1, while finding the optimal network parameters w\u2217(\u03b1) that minimize the training loss Ltrain with respect to w. This is expressed as:\nmin \u03b1 Lval(w\u2217(\u03b1), \u03b1) s.t. w\u2217(\u03b1) = arg min w Ltrain(w, \u03b1).\n(1)\nAfter optimizing this bilevel optimization problem, the supernet is discretized to derive the optimal architecture based on the architecture parameters. This is done by selecting the operation with the highest weight for each edge, i.e., o(i,j) = arg maxo\u2208O \u03b1 (i,j) o . The discretization step allows us to obtain the final architecture based on the learned architecture parameters, enabling the deployment of the optimized network architecture."
        },
        {
            "heading": "3.2. Motivation",
            "text": "Recent studies [20, 21] shows that there is a performance gap between the supernet and the optimal subnet, and an important factor that results in discretization gap of Differentiable Architecture Search (DARTS) is the sharpness of the supernet\u2019s loss landscape. It has been observed that when the supernet\u2019s loss landscape is flatter, DARTS is more likely to discover architectures with better performance through the discretization process. Conversely, when the supernet\u2019s loss landscape is sharp, the found architectures have worse performance. However, the existing methods proposed in these papers have limitations in effectively and efficiently addressing the problem of sharpness in the supernet\u2019s loss landscape. More research is needed to develop more effective and efficient techniques to alleviate this issue and further improve the performance of DARTS.\nSeveral recent papers [37, 38] have explored the idea of flattening the loss landscape by attaching adversarial noise to network parameters during the training stage. Building on this concept, SDARTS [21] introduces the attachment of adversarial noise to architecture parameters during the search stage to enhance the flatness of the supernet\u2019s loss landscape. However, this method incurs significant computational cost in generating the adversarial noise. Another approach introduced by SAF [40] suggests that minimizing the difference in loss between two consecutive iterations is equivalent to minimizing the sharpness of the loss landscape. Motivated by the concept of self-distillation, which utilizes the network\u2019s own information to guide its training, we employ self-distillation as a framework to guide the training of the supernet. This framework aims to enhance the flatness of the supernet\u2019s loss landscape."
        },
        {
            "heading": "3.3. Self-Distillation DARTS",
            "text": "In our paper, we propose the self-distillation DARTS (SD-DARTS) method, which utilizes the self-distillation framework to guide the training of the supernet in DARTS. SD-DARTS aims to improve the performance of the supernet by attaching self-distillation on the inner-level problem of DARTS\u2019s bi-level optimization. The formulation of SD-DARTS is as follows:\nmin \u03b1 Lval(w\u2217(\u03b1), \u03b1) + \u03bbH\n( fS(x), fT (x) ) s.t. w\u2217(\u03b1) = arg min\nw Ltrain (w, \u03b1) + \u03bbH\n( fS(x), fT (x) ) ,\n(2)\nIn Eq. (2), f represents the supernet of DARTS, fS and fT are the student and teacher models, respectively. The student model corresponds to the supernet at time step t, i.e., fS = ft(x), and the teacher model corresponds to the student model at the previous time step. The output probabilities of the student and teacher models are denoted as fS(x) and fT (x), respectively. The metric H is used to calculate the correlation between the output probabilities, and in our paper, we use the Kullback-Leibler (KL) divergence as the metric. The regularization coefficient \u03bb is used to balance the importance of the two loss terms. In SD-DARTS, the teacher at the present time step is the supernet at the previous time step, i.e.,\nfT (x) = ft\u22121(x).\nThus, the formulation in Equation (2) can be simplified as:\nmin \u03b1 Lval(w\u2217(\u03b1), \u03b1) + \u03bbH (ft(x), ft\u22121(x)) s.t. w\u2217(\u03b1) = arg min w Ltrain (w, \u03b1) + \u03bbH (ft(x), ft\u22121(x)) .\nThe detailed process of SD-DARTS is illustrated in Fig. 1. Here we will introduce SD-DARTS in detail.\nFirstly, we start by warming up the supernet f . Both the architecture parameters \u03b1 and the network parameters w are randomly initialized. The goal of this warm-up phase is to allow the supernet to learn valuable knowledge from the training data. It is important to note that only high-quality supernets can serve as qualified teachers to guide the training of the supernet itself. This is because low-quality teachers may provide misleading guidance and hinder the training process. Therefore, the warm-up phase plays a crucial role in preparing the supernet for effective self-distillation.\nAfter the warm-up phase, we introduce a self-distillation framework to guide the training of DARTS. At epoch t, we utilize the supernet at epoch t\u2212 1 as the teacher model, denoted as fT , where the previous supernet itself becomes the teacher. To obtain the teacher output probability, there are two approaches. The first approach involves storing the parameters of the teacher model in the GPU memory and generating the teacher output probability by running the teacher model when needed. However, this approach requires significant computational resources. To mitigate the computational cost, we propose an alternative approach. We store the output probability of the supernet at the previous time step, i.e., fT (x), directly in the memory and utilize it when needed. In practice, computer memory capacity is typically large, allowing for the allocation of a small portion of memory to store the student output probability. This approach effectively reduces the computational consumption associated with accessing the teacher output probability.\nNext, we calculate the correlation between the teacher output probability, fT (x), and the student output probability, ft(x), at epoch t, using the chosen correlation metric H. There are various correlation metrics available, such as Euclidean Distance (ED), Manhattan Distance (MD), Cosine Distance (CD), and Kullback-Leibler divergence (KL). Since our task is image classification, we utilize KL divergence as the correlation metric H in our paper. KL divergence is a simple metric used to quantify the differences between two probability distributions. In our case, we calculate the KL divergence as follows:\nH(fS(x), fT (x)) = \u2211 i fS(xi) log fS(xi) fT (xi) ,\nwhere f(xi) denotes the output probability for the i-th example. We use this correlation metric as a regularization term and incorporate it into the training of the supernet.\nThe overall process of SD-DARTS is summarized in Algorithm 1. Similar to DARTS, our method also uses alternative optimization strategy to optimize architecture parameters and network parameters. SGD and Adam are used to train network parameters and architecture parameters, respectively. The experiment settings in detail are shown in Sec. 4.1. After the training of the supernet, the optimal architecture is derived from the supernet based on architecture parameters.\nHere, we demonstrate how self-distillation can effectively flatten the loss\nAlgorithm 1 SD-DARTS\nInput: supernet f , correlation metric H, total epochs E, warm-up epochs \u03be. Initialize architecture parameters \u03b1 and network parameters w. Warm up the supernet f for \u03be epochs. for t \u2190 (\u03be + 1) to E do\nSelect the supernet ft\u22121 at the previous time step (epoch t\u2212 1) as the teacher. Update \u03b1 by optimizing min\n\u03b1 Lval(w\u2217(\u03b1), \u03b1) + \u03bbH (ft(x), ft\u22121(x));\nUpdate w by optimizing min w Ltrain (w, \u03b1) + \u03bbH (ft(x), ft\u22121(x));\nend for\nlandscape of an universal neural network f with its parameters denoted as \u03b1 by employing theoretical tools from SAM [37] and SAF [40]. SAM [37] is introduced to flatten the loss landscape by solving the following minmax problem:\nmin \u03b1 max \u2225\u03f5\u22252\u2264\u03c1\nL(f\u03b1+\u03f5).\nwhere L, \u03c1 and \u03f5 denote the loss function, allowed perturbation constraint and perturbation on parameters, respectively. Further, the sharpness loss R(f\u03b1) can be represented as L(f\u03b1+\u03f5\u0302)\u2212 L(f\u03b1), where \u03f5\u0302 denotes the optimal perturbation on parameters, and \u03f5\u0302 is calculated by \u03c1 \u2207\u03b1L(f\u03b1) \u22a4\n\u2225\u2207\u03b1L(f\u03b1)\u22252 [37]. We use a first-order Taylor expansion to decompose the sharpness loss as follows:\nR(f\u03b1) = L(f\u03b1+\u03f5\u0302)\u2212 L(f\u03b1) \u2248 L(f\u03b1) + \u03f5\u0302\u2207\u03b1L(f\u03b1)\u2212 L(f\u03b1)\n= \u03c1 \u2207\u03b1L(f\u03b1)\u22a4\n\u2225\u2207\u03b1L(f\u03b1)\u22252 \u2207\u03b1L(f\u03b1) = \u03c1\u2225\u2207\u03b1L(f\u03b1)\u22252, (3)\nEq. (3) shows that minimizing the sharpness loss is equivalent to minimizing the \u21132-norm of the gradient \u2207\u03b1L(f\u03b1).\nOn the other hand, motivated by SAF [40], the change of the network\u2019s validation loss in the two consecutive iterations can be represented as\nL(f\u03b1)\u2212 L(f\u03b1\u2212\u03bb\u2207\u03b1L(f\u03b1)) \u2248 \u03bb\u2225\u2207\u03b1L(f\u03b1)\u222522 \u2248 \u03bb\n\u03c12 R(f\u03b1)\n2. (4)\nwhere \u03bb denotes as the learning rate in the training of parameters. SAM [37] shows that the learning rate \u03bb in the training of parameters is typically\nsmaller than \u03c1. Based on Eq. (4), we find that the change of the loss is proportional to R(f\u03b1)\n2. Hence, minimizing the loss difference is equal to minimizing the sharpness of the network in this case. Our analysis can be easily generalized to DARTS. Fig. 2 also shows that dominant eigenvalue of the supernet\u2019s Hessian norm on SD-DARTS is smaller than standard DARTS, i.e., the loss landscape of SD-DARTS is flatter than DARTS.\nRemark 1. In our method, we utilize self-distillation (SD) as a framework to train the supernet. To assess the sharpness of the validation loss landscape, we compute the dominant eigenvalue of the supernet\u2019s Hessian norm. Fig. 2 demonstrates that the dominant eigenvalue of the supernet\u2019s Hessian norm in SD-DARTS is smaller than that in standard DARTS. This indicates that the loss landscape of SD-DARTS is flatter compared to DARTS. The flatter loss landscape in SD-DARTS suggests that our method is effective in reducing the sharpness and improving the stability of the supernet\u2019s training process."
        },
        {
            "heading": "3.4. Voting Teachers",
            "text": "In SD-DARTS, we utilize the supernet from the previous time step as the teacher to guide the training process. However, we recognize that the information provided by a single teacher may be limited. To overcome this limitation, we propose to utilize multiple teachers in order to obtain a more diverse and rich source of information. Each teacher contributes its own expertise and experience to guide the training of the supernet. To make predictions, we employ a voting scheme where the output probabilities of each teacher are combined to generate the final teacher prediction. This approach of utilizing multiple teachers allows us to leverage a broader range of knowledge and increase the diversity of guidance during the training process. By aggregating the insights from multiple teachers, we can potentially achieve better performance and more robust architectures in SD-DARTS.\nIn our proposed method, known as SD-DARTS, we introduce a concept called \u201cvoting teachers\u201d to guide the training of the supernet. Inspired by ensemble learning, which demonstrates that the performance of multiple models is often better than that of a single model, we leverage the idea of combining multiple teachers to enhance the training process. In the context of SD-DARTS, we consider a time window of size K, where we select multiple supernets from the previous K time steps as teachers. The output probabilities of these teacher supernets are then averaged to generate the final teacher\nAlgorithm 2 Voting Teachers\nInput: supernet f , correlation metric H, total epochs E, warm-up epochs \u03be, time window K. Initialize architecture parameters \u03b1 and network parameters w. Warm up the supernet f for \u03be epochs. for t \u2190 (\u03be + 1) to E do\nSelect the supernets at the previous K time steps (from epoch t\u2212K to epoch t\u2212 1) as the teachers.\nUpdate \u03b1 by optimizing min \u03b1 Lval(w\u2217(\u03b1), \u03b1) + \u03bbH\n( f(x), 1\nK K\u2211 i=1 ft\u2212i(x)\n) ;\nUpdate w by optimizing min w Ltrain (w, \u03b1) + \u03bbH\n( f(x), 1\nK K\u2211 i=1 ft\u2212i(x)\n) ;\nend for\noutput probability. Mathematically, the final teacher output probability can be represented as follows:\nfT (x) = 1\nK K\u2211 i=1 ft\u2212i(x),\nwhere ft\u2212i(x) denotes the output probability of the supernet at time step t\u2212 i. By combining the predictions from multiple teachers, we aim to benefit from their collective knowledge and insights, leading to improved guidance in the training process of the supernet. This approach allows us to leverage the strengths of different teacher supernets and potentially achieve better performance and more robust architectures in SD-DARTS.\nIn our proposed method, which combines voting teachers with SD-DARTS, we aim to leverage the knowledge and insights from multiple teacher supernets to guide the search for architectures. The objective function in this case is formulated as follows:\nmin \u03b1 Lval(w\u2217(\u03b1), \u03b1) + \u03bbH\n( f(x), 1\nK K\u2211 i=1 ft\u2212i(x)\n)\ns.t. w\u2217(\u03b1) = arg min w Ltrain (w, \u03b1) + \u03bbH\n( f(x), 1\nK K\u2211 i=1 ft\u2212i(x)\n) ,\nwhere t represents the present time step, \u03bb represents the regularization coefficient, K is the time window that controls the number of teachers,\nand 1 K K\u2211 i=1 ft\u2212i(x) denotes the final teacher output probability obtained by averaging the output probabilities of the multiple teacher supernets. The\nregularization term H ( f(x), 1\nK K\u2211 i=1 ft\u2212i(x)\n) encourages consistency between\nthe supernet\u2019s output probability and the averaged output probabilities of the voting teachers. To implement the voting teachers approach, we provide an algorithmic description in Algorithm 2. This algorithm outlines the steps for selecting the teacher supernets from the previous time steps, averaging their output probabilities, and using the averaged output probability as the final teacher output probability to guide the training of the supernet. By incorporating voting teachers into SD-DARTS, we aim to take advantage of the collective knowledge and insights from multiple teacher supernets, potentially leading to improved performance and more robust architectures during the search process.\nBy utilizing voting teachers in combination with SD-DARTS, we can leverage the collective knowledge of multiple teacher supernets to guide the search for architectures. This approach allows for more diverse and informed guidance during the training of the supernet, potentially leading to better performance and improved architectures. The additional memory required to store the output probabilities of the teacher supernets is relatively small compared to the overall memory requirements of the training process. This allows us to leverage the rich information from multiple teachers without significantly increasing the computational or memory overhead.\nRemark 2. 1. Ensemble learning has shown that combining the knowledge of multiple models can lead to improved performance compared to a single model. By selecting multiple supernets from the previous time steps as teachers, we can leverage the diverse knowledge and expertise of these models to guide the training of the supernet. The use of multiple teachers helps to capture a broader range of information and insights, leading to a more comprehensive and robust training process. It allows the supernet to benefit from the collective wisdom of multiple architectures, increasing the chances of finding high-quality architectures.\n2. Our method differs from Mean Teacher [31] in the approach of averaging. While Mean Teacher focuses on averaging the weights of the network, our method aims to average the output of the network. We have two reasons for choosing the voting teachers approach over Mean Teacher. Firstly,\nthe parameter scale of the network\u2019s output is significantly smaller than the weights, and computational and memory overhead will become bigger when we average weights instead of outputs. Because weights and outputs are very closely linked, by using the average of the output to guide our self-distillation training, our method still has an impact on the weights. It allows us to leverage the benefits of the output averaging while considering the relative importance of the weights. Secondly, the coupling between the architecture parameters and network parameters in our model, due to alternative optimization strategies, can lead to instability during the training process when using teachers generated by averaging the weights. The instability can hinder convergence and affect the overall performance of the model. By considering these factors, we have determined that the voting teachers approach is better suited to our specific objectives and model characteristics. The decision allows us to effectively leverage the advantages of averaging while mitigating potential instabilities in the training process."
        },
        {
            "heading": "4. Experiment",
            "text": "In the sections, we carry out extensive experiments to verify the effectiveness of our method. Our experiments are conducted on 3\u00d7 NVIDIA GeForce GTX 3090 GPUs. CIFAR-10 [42] and ImageNet [1] are two image classification datasets which our experiments are conducted on. We search for the optimal architecture by SD-DARTS on CIFAR-10 and then evaluate the architecture on CIFAR-10 and ImageNet."
        },
        {
            "heading": "4.1. Architecture Search",
            "text": "CIFAR-10 is the dataset that we search for the optimal architecture by SD-DARTS. There are 50K training images and 10K test images in CIFAR-10,\nand each image\u2019s spatial resolution is 32 \u00d7 32. These images are divided into 10 classes. During the search stage of SD-DARTS, training images are split into two subsets, i.e., the valid subset for optimizing architecture parameters and the training subset for optimizing network parameters. After SD-DARTS searches for the optimal architecture, training images in CIFAR-10 are used to train the optimal architecture and then evaluate the architecture\u2019s performance on test images.\nThe search space of our method is the same as DARTS, and the search space includes 8 candidate operations, skip connect, max pool 3\u00d73, avg pool 3\u00d7 3, sep conv 3\u00d7 3, sep conv 5\u00d7 5, dil conv 3\u00d7 3, dil conv 5\u00d7 5, zero. The supernet is stacked by 6 normal cells and 2 reduction cells, which the reduction cells are inserted in 1\n3 and 2 3 of the total depth of the supernet, respectively.\nThe stride of convolution in the reduction cell is 2; thus, the reduction cell can reduce the spatial resolution of feature maps. However, the stride of convolution in the normal cell is 1, i.e., the spatial size of the feature map is not reduced. Our method aims to search for one normal cell and one reduction cell to build the optimal architecture.\nThe train setting of our method is also the same as DARTS. The total epochs E of training the supernet are 50, and the batch size of the train is 64. The initial channel size of the supernet is 16. SGD is used to optimize network parameters wwith an initial learning rate 0.025, momentum 0.9 and weight decay 3\u00d710\u22124. Adam is used to optimize architecture parameters with an initial learning rate 3\u00d7 10\u22124, momentum (0.5, 0.999) and weight decay 10\u22123. We warm up the supernet for 25 epochs, i.e., the epoch for warm-up \u03be is 25. Then, time window K is set as 2, and it means that we will select supernets at the two previous time steps as teachers to guide the training of the supernet. Furthermore, we empirically set the regularization coefficient \u03bb as 1.0 to balance classification loss and correlation loss. Table 1 shows these hyperparameter settings of our method."
        },
        {
            "heading": "4.2. Architecture Evaluation on CIFAR-10",
            "text": "Fig. 4 shows cells searched by SD-DARTS, and we stack these cells to build the optimal architecture. In the subsection, we evaluate the performance of the optimal architecture on CIFAR-10. The training setting of our method is also the same as DARTS. We stack 20 cells to build a network with an initial channel size of 36, where two reduction cells are put into 1\n3 and 2 3 of\nthe total depth of the network. We train the network for 600 epochs with batch size 96. SGD is utilized to optimize the network with an initial learning rate of 0.025. Furthermore, we also use data augmentation to regularize the network\u2019s training, including cutout and auxiliary towers; the cutout length is 16, the weight of auxiliary towers is 0.4, and the probability of path dropout is 0.3.\nTable 2 shows the performance of the architecture searched by our method on CIFAR-10. The architecture searched by our method achieves a 2.58% mean test error on CIFAR-10, and it can achieve a 2.44% best test error when setting the appropriate random seed to train the architecture. The\nperformance of our method is beyond DARTS and its variants (e.g., DARTSPT, SGAS, GDAS) a lot. In other words, our method achieves state-of-the-art (SOTA) performance. The magnitude of our architecture\u2019s parameters is 3.3M, and it is equal to the architecture searched by DARTS and less than other variants.\nFurthermore, the search cost of our method is 0.37 GPU days, and it costs a few less GPU days than DARTS. In a word, with limited resources, our method can find the architecture with SOTA performance."
        },
        {
            "heading": "4.3. Architecture Evaluation on ImageNet",
            "text": "In the subsection, we utilize ILSVRC2012 [58] to evaluate the transferability of our method. ILSVRC2012 is a lightweight version of ImageNet with\n1,000 classes. It contains 1.28M train images and 50K valid images, and each image\u2019s spatial resolution is 224\u00d7224.\nFig. 4 shows the cells searched by our method on CIFAR-10. We stack the cells to build a network and evaluate the network\u2019s performance on ImageNet. The train setting of our method on ImageNet is the same as DARTS. We utilize 12 normal cells and 2 reduction cells to build the network, and the reduction cells are inserted into 1\n3 and 2 3 of the total depth of the network.\nThe initial channel size of the network is 48. We train the network for 250 epochs with batch size 1024 on 3\u00d7 NVIDIA GeForce GTX 3090 GPUs. We utilize SGD to optimize the network with an initial learning rate of 0.5, a momentum of 0.9, and a weight decay of 3\u00d7 10\u22125. Furthermore, we utilize an auxiliary loss tower to help train our network with an auxiliary weight of 0.4. We spend three GPU days training the network on 3\u00d7 NVIDIA GeForce GTX 3090 GPUs.\nTable 3 demonstrates the evaluation results of our network on the ImageNet dataset, where we achieve a test error rate of 25.0%. This performance indicates that our method\u2019s transferability is superior to DARTS and some of its variants. It is worth noting that certain DARTS variants exhibit lower test errors than our method. However, our architecture has fewer parameters compared to most of its variants. While there may be baseline methods with better results, it is important to consider that their goals might differ from ours. For instance, methods like SGAS [19] and P-DARTS [22] address weight sharing and the depth gap between search and evaluation scenarios, respectively. In contrast, our focus is on flattening the supernet\u2019s loss landscape. Furthermore, our method outperforms our concurrent works in terms of both performance and search cost efficiency. These results demonstrate the value and efficiency of our approach.\nWhile our experimental results on the ImageNet dataset may not be as strong as desired, we believe that they still provide valuable insights into the performance of our method. Prior works such as SDARTS [21] and SAM [37] have also demonstrated that the dominant eigenvalue of the Hessian norm is a reliable indicator for characterizing the sharpness of the loss landscape. The observation that the dominant eigenvalue of the supernet\u2019s Hessian norm in our method is smaller than in standard DARTS, as well as the analysis showing that self-distillation can flatten the loss landscape, provide strong evidence that our proposed algorithm effectively reduces the discretization gap. Combining these pieces of evidence, we can confidently assert that our method successfully mitigates the discretization gap and flattens the supernet\u2019s loss\nlandscape. Prior works such as R-DARTS [20] and SDARTS [21] have shown that a high level of sharpness in the supernet\u2019s loss landscape can have a detrimental effect on the performance of DARTS. These works have proposed techniques like early stopping and parameter perturbation to address this issue. However, early stopping may limit the exploration of architectures with better performance, and generating parameter perturbations can be time-consuming. In comparison, our method is more efficient as we only need to preserve the logits from previous training steps. This approach allows us to take advantage of the knowledge accumulated in earlier steps without the need for expensive parameter perturbations. By leveraging self-distillation and voting teachers, our method effectively flatten the supernet\u2019s loss landscape and achieves better performance in a more efficient manner compared to prior works such as SDARTS."
        },
        {
            "heading": "4.4. Results on NAS-Bench-201 Search Space",
            "text": "NAS-Bench-201 [59] is the most widely used NAS benchmark. NASBench-201 contains 4 internal nodes with 5 candidate operations. The search space includes 15,625 architectures, and the ground truth performance of each architecture on CIFAR-10, CIFAR-100 and ImageNet16-120 is provided. The searching settings of our method are the same as DARTS on on NASBench-201.\nTable 4 shows comparison results on NAS-Bench-201. We search on CIFAR-10 and use the found genotype to query the performance of various datasets. It is observed that our method achieves competitive results on CIFAR-10, CIFAR-100, and ImageNet16-120 when compared to other NAS\nmethods. Although the CIFAR-100-valid performance of our method is slightly lower than DARTS- [27], it outperforms other methods such as PCDARTS [18] and iDARTS [61] in terms of overall performance. These results indicate that SD-DARTS has the capability to discover architectures with competitive performance across different datasets. While it may not achieve the absolute state-of-the-art on every individual dataset, it offers a strong trade-off between search efficiency and performance. These results highlight the value of our method in finding high-quality architectures for a wide range of applications."
        },
        {
            "heading": "5. Ablation Study",
            "text": ""
        },
        {
            "heading": "5.1. The Effect of Warm-up",
            "text": "Because architecture and network parameters are randomly initialized, we need to warm up the supernet. After the warm-up, the supernet learns enough knowledge from train data to become a qualified teacher. In the subsection, we make an ablation study about the importance of warm-up.\nWe set time window K as 1 in the experiment, i.e., we utilize the supernet at the previous time step as the teacher to guide the training of the supernet. Our aim to choose a single teacher is to neglect the effect of multiple teachers. We run our method several times with different epochs for warm-up. Then, we evaluate the architecture\u2019s performance discovered by our experiments.\nFig. 5 shows the tendency of the accuracy of the architectures discovered by SD-DARTS in the different warm-ups. We find that the accuracy grows gradually when the warm-up epochs increase until 25. When the warm-up epochs are insufficient, the supernet cannot learn enough knowledge to become a qualified teacher. It will misguide the training of supernet and generate architecture with poor performance. When the warm-up epochs increase from 25 epochs, the accuracy decreases gradually. This is because the supernet is over-optimized as the training epochs increase, and the supernet plays a negative role when it is taken for a teacher. Thus, warm-up plays an essential role in our method, and we are supposed to warm up the supernet for proper epochs."
        },
        {
            "heading": "5.2. The Effect of Voting Teachers",
            "text": "In this subsection, the effect of voting teachers is analyzed by considering different time windows K, where K represents the number of voting teachers. The warm-up epochs are set to 25, and the optimal architecture is searched using several supernets from the previous K time steps as teachers to guide the self-distillation training of the supernet. The performance of these architecture is then evaluated. The purpose of this analysis is to understand the impact of the number of voting teachers on the performance of the search process. By varying the value of K, different amounts of knowledge and information from previous supernets are integrated to guide the training. This allows for a comprehensive exploration of the effect of using multiple teachers in the self-distillation process.\nFig. 6 illustrates the tendency of accuracy in different time windows when using the voting teachers method. It shows that the performance of the searched architectures varies with the size of the time window. Interestingly, the best performance is achieved when the time window is set to 2, indicating that utilizing the supernets from the previous two time steps as the teachers provides the most effective guidance for the self-distillation training of the supernet. As the size of the time window increases beyond 2, the performance of the architectures starts to deteriorate. This can be attributed to the fact that the information from supernets in earlier time steps becomes less relevant\nand correlated with the supernet at the present time step. Consequently, using supernets from these earlier time steps as voting teachers may introduce misleading information and hinder the effectiveness of the self-distillation training. This observation highlights the trade-off between the number of voting teachers and the correlation between the teachers and the student. Future research can focus on finding a balance between these factors to fully leverage the information from supernets at different time steps and optimize the guidance provided by the voting teachers in the self-distillation process. This would allow for a more effective and accurate search for optimal architectures."
        },
        {
            "heading": "6. Conclusion",
            "text": "In summary, the proposed SD-DARTS method, which combines selfdistillation and voting teachers, aims to improve the performance of DARTS by addressing the sharpness of the loss landscape. It achieves state-ofthe-art performance on CIFAR-10 and ImageNet datasets by reducing the\ndiscretization gap between the supernet and the optimal subnet. While SDDARTS demonstrates promising results, it also has limitations regarding the utilization of information from supernets at different time steps to guide the training of the supernet. In future research, it would be valuable to explore alternative approaches, such as using a decayed exponential average, to strike a better balance between the number of voting teachers and the correlation between the teachers and the student. This could potentially enhance the performance and effectiveness of SD-DARTS even further."
        },
        {
            "heading": "Acknowledgments",
            "text": "The work of Jian Li is supported partially by Natural Science Foundation of China (No. 62106257), China Postdoctoral Science Foundation (No. 2023T160680), and Excellent Talents Program of Institute of Information Engineering, CAS. The work of Yong Liu is supported partially by Natural Science Foundation of China (No. 62076234), Beijing Outstanding Young Scientist Program (No. BJJWZYJH012019100020098), the Unicom Innovation Ecological Cooperation Plan, and the CCF-Huawei Populus Grove Fund."
        }
    ],
    "title": "Improving Differentiable Architecture Search via Self-Distillation",
    "year": 2023
}