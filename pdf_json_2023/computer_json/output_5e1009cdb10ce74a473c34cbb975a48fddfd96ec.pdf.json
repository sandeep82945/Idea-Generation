{
    "abstractText": "Whole Slide Image (WSI) classification remains a challenge due to their extremely high resolution and the absence of fine-grained labels. Presently, WSI classification is usually regarded as a Multiple Instance Learning (MIL) problem when only slide-level labels are available. MIL methods involve a patch embedding module and a bag-level classification module, but they are prohibitively expensive to be trained in an end-to-end manner. Therefore, existing methods usually train them separately, or directly skip the training of the embedder. Such schemes hinder the patch embedder\u2019s access to slide-level semantic labels, resulting in inconsistency within the entire MIL pipeline. To overcome this issue, we propose a novel framework called Iteratively Coupled MIL (ICMIL), which bridges the loss back-propagation process from the baglevel classifier to the patch embedder. In ICMIL, we use category information in the bag-level classifier to guide the patch-level fine-tuning of the patch feature extractor. The refined embedder then generates better instance representations for achieving a more accurate bag-level classifier. By coupling the patch embedder and bag classifier at a low cost, our proposed framework enables information exchange between the two modules, benefiting the entire MIL classification model. We tested our framework on two datasets using three different backbones, and our experimental results demonstrate consistent performance improvements over state-of-the-art MIL methods. The code is available at: https://github.com/Dootmaan/ICMIL.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hongyi Wang"
        },
        {
            "affiliations": [],
            "name": "Luyang Luo"
        },
        {
            "affiliations": [],
            "name": "Fang Wang"
        },
        {
            "affiliations": [],
            "name": "Ruofeng Tong"
        },
        {
            "affiliations": [],
            "name": "Yen-Wei Chen"
        },
        {
            "affiliations": [],
            "name": "Hongjie Hu"
        },
        {
            "affiliations": [],
            "name": "Lanfen Lin"
        },
        {
            "affiliations": [],
            "name": "Hao Chen"
        }
    ],
    "id": "SP:63eb904f7f8049319bdb73db0e57ec887ca87f1e",
    "references": [
        {
            "authors": [
                "B.E. Bejnordi",
                "M. Veta",
                "P.J. Van Diest",
                "B. Van Ginneken",
                "N. Karssemeijer",
                "G. Litjens",
                "J.A. Van Der Laak",
                "M. Hermsen",
                "Q.F. Manson",
                "M Balkenhol"
            ],
            "title": "Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer",
            "venue": "Jama 318(22), 2199\u20132210",
            "year": 2017
        },
        {
            "authors": [
                "G. Campanella",
                "M.G. Hanna",
                "L. Geneslaw",
                "A. Miraflor",
                "V. Werneck Krauss Silva",
                "K.J. Busam",
                "E. Brogi",
                "V.E. Reuter",
                "D.S. Klimstra",
                "T.J. Fuchs"
            ],
            "title": "Clinicalgrade computational pathology using weakly supervised deep learning on whole slide images",
            "venue": "Nature medicine 25(8), 1301\u20131309",
            "year": 2019
        },
        {
            "authors": [
                "Q. Chen",
                "H. Xiao",
                "Y. Gu",
                "Z. Weng",
                "L. Wei",
                "B. Li",
                "B. Liao",
                "J. Li",
                "J. Lin",
                "M Hei"
            ],
            "title": "Deep learning for evaluation of microvascular invasion in hepatocellular carcinoma from tumor areas of histology images",
            "venue": "Hepatology International 16(3), 590\u2013602",
            "year": 2022
        },
        {
            "authors": [
                "R.J. Chen",
                "C. Chen",
                "Y. Li",
                "T.Y. Chen",
                "A.D. Trister",
                "R.G. Krishnan",
                "F. Mahmood"
            ],
            "title": "Scaling vision transformers to gigapixel images via hierarchical self-supervised learning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16144\u201316155",
            "year": 2022
        },
        {
            "authors": [
                "N. Cheng",
                "Y. Ren",
                "J. Zhou",
                "Y. Zhang",
                "D. Wang",
                "X. Zhang",
                "B. Chen",
                "F. Liu",
                "J. Lv",
                "Q Cao"
            ],
            "title": "Deep learning-based classification of hepatocellular nodular lesions on whole-slide histopathologic images",
            "venue": "Gastroenterology 162(7), 1948\u20131961",
            "year": 2022
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 770\u2013778",
            "year": 2016
        },
        {
            "authors": [
                "G. Hinton",
                "O. Vinyals",
                "J. Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531",
            "year": 2015
        },
        {
            "authors": [
                "L. Hou",
                "D. Samaras",
                "T.M. Kurc",
                "Y. Gao",
                "J.E. Davis",
                "J.H. Saltz"
            ],
            "title": "Patch-based convolutional neural network for whole slide tissue image classification",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2424\u20132433",
            "year": 2016
        },
        {
            "authors": [
                "M. Ilse",
                "J. Tomczak",
                "M. Welling"
            ],
            "title": "Attention-based deep multiple instance learning",
            "venue": "International conference on machine learning. pp. 2127\u20132136. PMLR",
            "year": 2018
        },
        {
            "authors": [
                "C. Jin",
                "Z. Guo",
                "Y. Lin",
                "L. Luo",
                "H. Chen"
            ],
            "title": "Label-efficient deep learning in medical image analysis: Challenges and future directions",
            "venue": "arXiv preprint arXiv:2303.12484",
            "year": 2023
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "International Conference on Learning Representations",
            "year": 2015
        },
        {
            "authors": [
                "B. Li",
                "Y. Li",
                "K.W. Eliceiri"
            ],
            "title": "Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 14318\u201314328",
            "year": 2021
        },
        {
            "authors": [
                "K. Liu",
                "W. Zhu",
                "Y. Shen",
                "S. Liu",
                "N. Razavian",
                "K.J. Geras",
                "C. Fernandez-Granda"
            ],
            "title": "Multiple instance learning via iterative self-paced supervised contrastive learning",
            "venue": "arXiv preprint arXiv:2210.09452",
            "year": 2022
        },
        {
            "authors": [
                "M. Lu",
                "Y. Pan",
                "D. Nie",
                "F. Liu",
                "F. Shi",
                "Y. Xia",
                "D. Shen"
            ],
            "title": "Smile: Sparse-attention based multiple instance contrastive learning for glioma sub-type classification using pathological images",
            "venue": "MICCAI Workshop on Computational Pathology. pp. 159\u2013 169. PMLR",
            "year": 2021
        },
        {
            "authors": [
                "M.Y. Lu",
                "T.Y. Chen",
                "D.F. Williamson",
                "M. Zhao",
                "M. Shady",
                "J. Lipkova",
                "F. Mahmood"
            ],
            "title": "Ai-based pathology predicts origins for cancers of unknown primary",
            "venue": "Nature 594(7861), 106\u2013110",
            "year": 2021
        },
        {
            "authors": [
                "M.Y. Lu",
                "D.F. Williamson",
                "T.Y. Chen",
                "R.J. Chen",
                "M. Barbieri",
                "F. Mahmood"
            ],
            "title": "Data-efficient and weakly supervised computational pathology on whole-slide images",
            "venue": "Nature biomedical engineering 5(6), 555\u2013570",
            "year": 2021
        },
        {
            "authors": [
                "Z. Luo",
                "D. Guillory",
                "B. Shi",
                "W. Ke",
                "F. Wan",
                "T. Darrell",
                "H. Xu"
            ],
            "title": "Weaklysupervised action localization with expectation-maximization multi-instance learning",
            "venue": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXIX 16. pp. 729\u2013745. Springer",
            "year": 2020
        },
        {
            "authors": [
                "O. Maron",
                "T. Lozano-P\u00e9rez"
            ],
            "title": "A framework for multiple-instance learning",
            "venue": "Advances in neural information processing systems 10",
            "year": 1997
        },
        {
            "authors": [
                "O. Russakovsky",
                "J. Deng",
                "H. Su",
                "J. Krause",
                "S. Satheesh",
                "S. Ma",
                "Z. Huang",
                "A. Karpathy",
                "A. Khosla",
                "M Bernstein"
            ],
            "title": "Imagenet large scale visual recognition challenge",
            "venue": "International journal of computer vision 115(3), 211\u2013252",
            "year": 2015
        },
        {
            "authors": [
                "Z. Shao",
                "H. Bian",
                "Y. Chen",
                "Y. Wang",
                "J. Zhang",
                "X Ji"
            ],
            "title": "Transmil: Transformer based correlated multiple instance learning for whole slide image classification",
            "venue": "Advances in neural information processing systems 34, 2136\u20132147",
            "year": 2021
        },
        {
            "authors": [
                "C.L. Srinidhi",
                "S.W. Kim",
                "F.D. Chen",
                "A.L. Martel"
            ],
            "title": "Self-supervised driven consistency training for annotation efficient histopathology image analysis",
            "venue": "Medical Image Analysis 75, 102256",
            "year": 2022
        },
        {
            "authors": [
                "Q. Wang",
                "G. Chechik",
                "C. Sun",
                "B. Shen"
            ],
            "title": "Instance-level label propagation with multi-instance learning",
            "venue": "Proceedings of the 26th International Joint Conference on Artificial Intelligence. pp. 2943\u20132949",
            "year": 2017
        },
        {
            "authors": [
                "X. Wang",
                "F. Tang",
                "H. Chen",
                "L. Luo",
                "Z. Tang",
                "A.R. Ran",
                "C.Y. Cheung",
                "P.A. Heng"
            ],
            "title": "Ud-mil: uncertainty-driven deep multiple instance learning for oct image classification",
            "venue": "IEEE journal of biomedical and health informatics 24(12), 3431\u2013 3442",
            "year": 2020
        },
        {
            "authors": [
                "C. Zhang",
                "Y. Song",
                "D. Zhang",
                "S. Liu",
                "M. Chen",
                "W. Cai"
            ],
            "title": "Whole slide image classification via iterative patch labelling",
            "venue": "2018 25th IEEE International Conference on Image Processing (ICIP). pp. 1408\u20131412. IEEE",
            "year": 2018
        },
        {
            "authors": [
                "H. Zhang",
                "Y. Meng",
                "Y. Zhao",
                "Y. Qiao",
                "X. Yang",
                "S.E. Coupland",
                "Y. Zheng"
            ],
            "title": "Dtfd-mil: Double-tier feature distillation multiple instance learning for histopathology whole slide image classification",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18802\u201318812",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Keywords: Multiple Instance Learning \u00b7 Whole Slide Image \u00b7 Deep Learning.\nar X\niv :2\n30 3.\n15 74\n9v 2\n[ cs"
        },
        {
            "heading": "1 Introduction",
            "text": "Whole slide scanning is increasingly used in disease diagnosis and pathological research to visualize tissue samples. Compared to traditional microscope-based observation, whole slide scanning converts glass slides into gigapixel digital images that can be conveniently stored and analyzed. However, the high resolution of WSIs also makes their automated classification challenging [15]. Patch-based classification is a common solution to this problem [8,24,3]. It predicts the slidelevel label by first predicting the labels of small, tiled patches in a WSI. This approach allows for the direct application of existing image classification models, but requires additional patch-level labeling. Unfortunately, patch-level labeling by histopathology experts is expensive and time-consuming. Therefore, many weakly-supervised [8,24] and semi-supervised [5,3] methods have been proposed to generate patch-level pseudo labels at a lower cost. However, the lack of reliable supervision directly hinders the performance of these methods, and serious class-imbalance problems could arise, as tumor patches may only account for a small portion of the entire WSI [12].\nIn contrast, MIL-based methods have become increasingly preferred due to their only demand for slide-level labels [18]. The typical pipeline of MIL methods is shown in Fig. 1, where WSIs are treated as bags, and tiled patches are considered as instances. The aim is to predict whether there are positive instances, such as tumor patches, in a bag, and if so, the bag is considered positive as well. In practice, a fixed ImageNet pre-trained feature extractor g(\u00b7) is usually used to convert the tiled patches in a WSI into feature maps due to limited GPU memory. These instance features are then aggregated by a(\u00b7) into a slide-level feature vector to be sent to the bag-level classifier f(\u00b7) for MIL training. Due to the high computational cost, end-to-end training of the feature extractor and bag classifier is prohibitive, especially for high-resolution WSIs. As a result, many methods focus solely on improving a(\u00b7) or f(\u00b7), leaving g(\u00b7) untrained on the WSI dataset (as shown in Fig.2(b)). However, the domain shift between WSI and natural images may lead to sub-optimal representations, so recently there have been methods proposed to fine-tune g(\u00b7) using self-supervised techniques [21,12,4] or weakly-supervised techniques [13,23,10] (as shown in Fig.2(c)). Nevertheless, since these two processes are still trained separately with different supervision signals, they lack joint optimization and may still leads to inconsistency within the entire MIL pipeline.\nTo address the challenges mentioned above, we propose a novel MIL framework called ICMIL, which can iteratively couple the patch feature embedding process with the bag-level classification process to enhance the effectiveness of MIL training (as illustrated in Fig. 2(d)). Unlike previous works that mainly focused on designing sophisticated instance aggregators a(\u00b7) [12,20,14] or bag classifiers f(\u00b7) [9,16,25], we aim to bridge the loss back-propagation process from f(\u00b7) to g(\u00b7) to improve g(\u00b7)\u2019s ability to perceive slide-level labels. Specifically, we propose to use the bag-level classifier f(\u00b7) to initialize an instance-level classifier f \u2032(\u00b7), enabling f(\u00b7) to use the category knowledge learned from bag-level features to determine each instance\u2019s category. In this regard, we further propose a teacher-student [7] approach to effectively generate pseudo labels and simultaneously fine-tune g(\u00b7). After fine-tuning, the domain shift problem is alleviated in g(\u00b7), leading to better patch representations. The new representations can be used to train a better bag-level classifier in return for the next round of iteration.\nIn summary, our contributions are: (1) We propose ICMIL which bridges the loss propagation from the bag classifier to the patch embedder by iteratively coupling them during training. This framework fine-tunes the patch embedder based on the bag-level classifier, and the refined embeddings, in turn, help train a more accurate bag-level classifier. (2) We propose a teacher-student approach to achieve effective and robust knowledge transfer from the bag-level classifier f(\u00b7) to the instance-level representation embedder g(\u00b7). (3) We conduct extensive experiments on two datasets using three different backbones and demonstrate the effectiveness of our proposed framework."
        },
        {
            "heading": "2 Methodology",
            "text": ""
        },
        {
            "heading": "2.1 Iterative Coupling of Embedder and Bag Classifier in ICMIL",
            "text": "The general idea of ICMIL is shown in Fig. 3, which is inspired by the ExpectationMaximization (EM) algorithm. EM has been used with MIL in some previous\nworks [17,13,22], but it was only treated as an assisting tool for aiding the training of either g(\u00b7) or f(\u00b7) in the traditional MIL pipelines. In contrast, we are the first to consider the optimization of the entire MIL pipeline as an EM alike problem, utilizing EM for coupling g(\u00b7) and f(\u00b7) together iteratively. To begin with, we first employ a traditional approach to train a bag-level classifier f(\u00b7) on a given dataset, with patch embeddings generated by a fixed ResNet50 [6] pre-trained on ImageNet [19] (step 1\u25cb in Fig.3). Subsequently, this f(\u00b7) is considered as the initialization of a hidden instance classifer f \u2032(\u00b7), generating pseudo-labels for each instance-level representation. This operation is feasible when the bag-level representations aggregated by a(\u00b7) are in the same hidden space as the instance representations, and most aggregation methods (e.g., max pooling, attention-based) satisfy this condition since they essentially make linear combinations of instance-level representations.\nNext, we freeze the weights of f(\u00b7) and fine-tune g(\u00b7) with the generated pseudo-labels (step 2\u25cb in Fig. 3), of which the detailed implementation is presented in Section 2.3. After this, g(\u00b7) is fine-tuned for the specific WSI dataset, which allows it to generate improved representations for each instance, thereby enhancing the performance of f(\u00b7). Moreover, with a better f(\u00b7), we can use the iterative coupling technique again, resulting in further performance gains and mitigation to the distribution inconsistencies between instance- and bag-level embeddings."
        },
        {
            "heading": "2.2 Instance Aggregation Method in ICMIL",
            "text": "Although most instance aggregators are compatible with ICMIL, they still have an impact on the efficiency and effectiveness of ICMIL. In addition to that a(\u00b7) has to project the bag representations to the same hidden space as the instance representations, it also should avoid being over-complicated. Otherwise, a(\u00b7) may lead to larger difference between the decision boundaries of bag-level classifer f(\u00b7) and instance-level classifier f \u2032(\u00b7), which may cause ICMIL taking more time to converge.\nTherefore, in our experiments, we choose to use the attention-based instance aggregation method [9] which has been widely used in many of the existing\nMIL frameworks [9,16,25]. For a bag that contains K instances, attention-based aggregation method firstly learns an attention score for each instance. Then, the aggregated bag-level representation H is defined as:\nH = K\u2211 k=1 akhk, (1)\nak = exp{\u03c9T (tanh(V1hk)\u2299 sigm(V2hk))}\u2211K j=1 exp{\u03c9T (tanh(V1hj)\u2299 sigm(V2hj))} , (2)\nwhere ak is the attention score for the k-th instance hk in the bag. Obviously, H and hk remains in the same hidden space, satisfying the prerequisite of ICMIL."
        },
        {
            "heading": "2.3 Label Propagation from Bag Classifier to Embedder",
            "text": "We propose a novel teacher-student model for accurate and robust label propagation from f(\u00b7) to g(\u00b7). The model\u2019s architecture is depicted in Fig. 4. In contrast to the conventional approach of generating all pseudo labels and retraining g(\u00b7) from scratch, our proposed method can simultaneously process the pseudo label generation and g(\u00b7) fine-tuning tasks, making it more flexible. Moreover, incorporating augmented inputs in the training process allows for the better utilization of supervision signals, resulting in a more robust g(\u00b7). We also introduce a learnable f \u2032(\u00b7) to self-adaptively modifying the instance-level decision boundary for more effective fine-tuning of the embedder.\nSpecifically, we freeze the weights of g(\u00b7) and f(\u00b7) and set them as the teacher. We then train a student patch embedding network, g\u2032(\u00b7), to learn category knowledge from the teacher. For a given patch input x, the teacher generates the corresponding pseudo label, while the student receives an augmented image x\u2032\nand attempts to generate a similar prediction to that of the teacher through a consistency loss Lc. This loss function is defined as:\nLc = C\u2211 c=1 [ f(x)clog ( f(x)c f \u2032(x\u2032)c )] , (3)\nwhere f(\u00b7) and f \u2032(\u00b7) are teacher classifer and student classifier respectively, f(\u00b7)c indicates the c-th channel of f(\u00b7), and C is the total number of channels.\nAdditionally, during training, a learnable instance-level classifier is used on the student to back-propagate the gradients to g\u2032(\u00b7). The initial weights of f \u2032(\u00b7) are the same as those of f(\u00b7), as the differences in the instance- and bag-level classification boundaries is expected to be minor. To make f \u2032(\u00b7) not so different from f(\u00b7) during training, a weight similarity loss, Lw, is further imposed to constrain it by drawing closer their each layer\u2019s outputs under the same input. By applying Lw, the patch embeddings from g\n\u2032(\u00b7) can still suit the bag-level classification task well, rather than being tailored solely for the instance-level classifier f \u2032(\u00b7). Lw is defined as:\nLw = L\u2211 l=1 C\u2211 c=1 [ f(x)lclog ( f(x)lc f \u2032(x)lc )] , (4)\nwhere f(\u00b7)lc indicates the c-th channel of l-th layer\u2019s output in f(\u00b7). The overall loss function for this step is Lc + \u03b1Lw, with \u03b1 set to 0.5 in our experiments."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Datasets",
            "text": "Our experiments utilized two datasets, with the first being the publicly available breast cancer dataset, Camelyon16 [1]. This dataset consists of a total of 399 WSIs, with 159 normal and 111 metastasis WSIs for the training set, and the remaining 129 for test. Although patch-level labels are officially provided in Camelyon16, they were not used in our experiments.\nThe second dataset is a private hepatocellular carcinoma (HCC) dataset collected from Sir Run Run Shaw Hospital, Hangzhou, China. This dataset comprises a total of 1140 valid tumor WSIs scanned at 40\u00d7 magnification, and the objective is to identify the severity of each case based on the Edmondson-Steiner (ES) grading. The ground truth labels are binary classes of low risk and high risk, which were provided by experienced pathologists."
        },
        {
            "heading": "3.2 Implementation Details",
            "text": "For Camelyon16, we tiled the WSIs into 256\u00d7256 patches on 20\u00d7 magnification using the official code of [25], while for the HCC dataset the patches are 384\u00d7384 on 40\u00d7 magnification following the pathologists\u2019 advice. For both datasets, we\nused an ImageNet pre-trained ResNet50 to initialize g(\u00b7). The instance embedding process was the same of [16], which means for each patch, it would be firstly embedded into a 1024-dimension vector, and then be projected to a 512- dimension hidden space for further bag-level training. For the training of bag classifier f(\u00b7), we used an initial learning rate of 2e-4 with Adam [11] optimizer for 200 epochs with batch size being 1. Camelyon16 results are reported on the official test split, while the HCC dataset used a 7:1:2 split for training, validation and test. For the training of patch embedder g(\u00b7), we used an initial learning rate of 1e-5 with Adam [11] optimizer with the batch size being 100. Three metrics were used for evaluation. Namely, area under curve (AUC), F1 score, and slide-level accuracy (Acc). Experiments were all conducted on a Nvidia Tesla M40 (12GB)."
        },
        {
            "heading": "3.3 Experimental Results",
            "text": "Ablation Study. The results of ablation studies are presented in Table 1. From Table 1(a), we can learn that as the number of ICMIL iteration increases, the performance will also go up until reaching a stable point. Since the number of\ninstances is very large in WSI datasets, we empirically recommend to choose to run ICMIL one iteration for fine-tuning g(\u00b7) to achieve the balance between performance gain and time consumption. From Table 1(b), it is shown that our teacher-student-based method outperforms the na\u0308\u0131ve \u201cpseudo label generation\u201d method for fine-tuning g(\u00b7), which demontrates the effectiveness of introducing the learnable instance-level classifier f \u2032(\u00b7).\nComparison with Other Methods. Experimental results are presented in Table 2. As shown, our ICMIL framework consistently improves the performance of three different MIL baselines (i.e., Max Pool, AB-MIL, and DTFDMIL), demonstrating the effectiveness of bridging the loss back-propagation from bag calssifier to embedder. It proves that a more suitable patch embedding can greatly enhance the overall MIL classification framework. When used with the state-of-the-art MIL method DTFD-MIL, ICMIL further increases its performance on Camelyon16 by 0.5% AUC, 2.1% F1, and 1.6% Acc.\nResults on the HCC dataset also proves the effectiveness of ICMIL, despite the minor difference on the relative performance of baseline methods. Mean Pooling performs better on this dataset due to the large area of tumor in the WSIs (about 60% patches are tumor patches), which mitigates the impact of average pooling on instances. Also, the performance differences among different vanilla MIL methods tends to be smaller on this dataset since risk grading is a harder task than Camelyon16. In this situation, the quality of instance representations plays a crucial role in generating more separable bag-level representations. As a result, after applying ICMIL on the MIL baselines, these methods all gain great performance boost on the HCC dataset.\nFurthermore, Fig. 5 displays the instance-level and bag-level representations of Camelyon16 dataset before and after applying ICMIL on AB-MIL backbone.\nThe results indicate that one iteration of g(\u00b7) fine-tuning in ICMIL significantly improves the instance-level representations, leading to a better aggregated baglevel representation naturally. Besides, the bag-level representations are also more closely aligned with the instance representations, proving that ICMIL can reduce the inconsistencies between g(\u00b7) and f(\u00b7) by coupling them together for training, resulting in a better separability."
        },
        {
            "heading": "4 Conclusion",
            "text": "In this work, we propose ICMIL, a novel framework that iteratively couples the feature extraction and bag classification stages to improve the accuracy of MIL models. ICMIL leverages the category knowledge in the bag classifier as pseudo supervision for embedder fine-tuning, bridging the loss propagation from classifier to embedder. We also design a two-stream model to efficiently facilitate such knowledge transfer in ICMIL. The fine-tuned patch embedder can provide more accurate instance embeddings, in return benefiting the bag classifier. The experimental results show that our method brings consistent improvement to existing MIL backbones.\nAcknowledgements This work was supported by the National Key Research and Development Project (No. 2022YFC2504605), National Natural Science Foundation of China (No. 62202403) and Hong Kong Innovation and Technology Fund (No. PRP/034/22FX). It was also supported in part by the Grant in Aid for Scientific Research from the Japanese Ministry for Education, Science, Culture and Sports (MEXT) under the Grant No. 20KK0234, 21H03470."
        }
    ],
    "title": "Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification",
    "year": 2023
}