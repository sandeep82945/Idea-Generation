{
    "abstractText": "Smart glasses for augmented reality are digital technology under investigation in the agricultural sector. The potential of augmented reality was underlined, in some scientific contributions, as a support tool for farmers\u2019 activities and for the decision\u2010making process. One of the most practical applications studied for augmented reality was in maintenance operations, where the use of smart glasses showed high capability. This work focuses on the evaluation of the performance and applicability of smart glasses with a video see\u2010through display system and testing the device\u2019s available functions in agricultural frameworks. In addition, an augmented assistance scenario describing the main steps involved in the functioning of the maintenance operation was developed for milking machine inspection. The audio\u2013video quality, battery life, detection capabilities of markers, and voice control interaction system were evaluated. The results showed the capabilities of smart glasses to reach augmented information from a long distance in a short time interval and to transmit audio and video with a high level of detail, allowing discrimination of small objects during remote assistance with reduced delay. The built maintenance scenario represents an example of augmented reality digital assistance application in the inspection and maintenance of the milking machine. The device performance and the proposed maintenance scenario underline the potential that augmented reality could have in the agricultural sector to assist and guide both farmers and technicians to timely problem solving. This solution fits into the agriculture 4.0 perspective, which is increasingly focused on digital transformation to improve farms\u2019 efficiency and sustainability.",
    "authors": [
        {
            "affiliations": [],
            "name": "Maria Caria"
        }
    ],
    "id": "SP:0ed17cbc12cb74e829c1241f4419865ed1aad523",
    "references": [
        {
            "authors": [
                "Banhazi",
                "T.M. et al. Precision Livestock Farming"
            ],
            "title": "An international review of scientific and commercial aspects",
            "venue": "Int. J. Agric. Biol. Eng. 5, 1\u201310",
            "year": 2012
        },
        {
            "authors": [
                "D.C. Rose",
                "Chlivers",
                "J. Agriculture 4.0"
            ],
            "title": "Broadening responsible innovation in an era of smart farming",
            "venue": "Front. Sustain. Food Syst. 87, 1\u20137. https:// doi. org/ 10. 3389/ fsufs. 2018. 00087",
            "year": 2018
        },
        {
            "authors": [
                "L. Klerkx",
                "E. Jakkub",
                "Labarthec",
                "P. A review of social science on digital agriculture",
                "smart farming",
                "agriculture 4.0"
            ],
            "title": "New contributions and a future research agenda",
            "venue": "NJAS-WAGEN J. Life Sci. 100315, 90\u201391. https:// doi. org/ 10. 1016/j. njas. 2019. 100315",
            "year": 2019
        },
        {
            "authors": [
                "S. Wolfert",
                "L. Ge",
                "C. Verdouw",
                "Bogaardt",
                "M.-J"
            ],
            "title": "Big data in smart farming\u2014A review",
            "venue": "Agric. Syst",
            "year": 2017
        },
        {
            "authors": [
                "V. Saiz-Rubio",
                "Rovira-M\u00e1s",
                "F. From smart farming towards agriculture 5.0"
            ],
            "title": "A review on crop data management",
            "venue": "Agronomy 10, 207. https:// doi. org/ 10. 3390/ agron omy10 020207",
            "year": 2020
        },
        {
            "authors": [
                "Bolfe",
                "\u00c9.L. et al. Precision",
                "digital agriculture"
            ],
            "title": "Adoption of technologies and perception of Brazilian farmers",
            "venue": "Agriculture 10, 653. https:// doi. org/ 10. 3390/ agric ultur e1012 0653 (2020). 11 Vol.:(0123456789) Scientific Reports |",
            "year": 2022
        },
        {
            "authors": [
                "E. Tullo",
                "A. Finzi",
                "M. Guarino"
            ],
            "title": "Environmental impact of livestock farming and Precision Livestock Farming as a mitigation strategy",
            "venue": "Sci. Total Environ",
            "year": 2019
        },
        {
            "authors": [
                "N.M. Trendov",
                "S. Varas",
                "M. Zeng"
            ],
            "title": "Digital Technologies in Agricultural and Rural Areas-Briefing Paper (Food and Agricultural Organization of United Nation (FAO), 2019)",
            "year": 2019
        },
        {
            "authors": [
                "L. Shang",
                "T. Heckelei",
                "M.K. Gerullis",
                "J. B\u00f6rner",
                "S. Rasch"
            ],
            "title": "Adoption and diffusion of digital farming technologies-integrating farm-level evidence and system interaction",
            "venue": "Agric. Syst",
            "year": 1016
        },
        {
            "authors": [
                "J. Huuskonen",
                "T. Oksanen"
            ],
            "title": "Soil sampling with drones and augmented reality in precision agriculture",
            "venue": "Comput. Electron. Agric",
            "year": 1016
        },
        {
            "authors": [
                "M. Caria",
                "G. Sara",
                "G. Todde",
                "M. Polese",
                "Pazzona",
                "A. Exploring smart glasses for augmented reality"
            ],
            "title": "A valuable and integrative tool in the precision livestock farming",
            "venue": "Animals 9, 903. https:// doi. org/ 10. 3390/ ani91 10903",
            "year": 2019
        },
        {
            "authors": [
                "M. Caria",
                "G. Todde",
                "G. Sara",
                "M. Piras",
                "A. Pazzona"
            ],
            "title": "Performance and usability of smartglasses for augmented reality in precision livestock farming",
            "venue": "operations. Appl. Sci",
            "year": 2020
        },
        {
            "authors": [
                "L. Mu\u00f1oz-Saavedra",
                "L. Mir\u00f3-Amarante",
                "M. Dom\u00ednguez-Morales"
            ],
            "title": "Augmented and virtual reality evolution and future tendency",
            "venue": "Appl. Sci",
            "year": 2020
        },
        {
            "authors": [
                "R. Palmarini",
                "J.A. Erkoyuncu",
                "R. Roy",
                "H. Torabmostaedi"
            ],
            "title": "A systematic review of augmented reality applications in maintenance",
            "venue": "Robot. Comput. Integr. Manuf",
            "year": 2018
        },
        {
            "authors": [
                "D.K. Baroroh",
                "C.H. Chu",
                "Wang",
                "L. Systematic literature review on augmented reality in smart manufacturing"
            ],
            "title": "Collaboration between human and computational intelligence",
            "venue": "J. Manuf. Syst. 61, 696\u2013711. https:// doi. org/ 10. 1016/j. jmsy. 2020. 10. 017",
            "year": 2020
        },
        {
            "authors": [
                "C.H. Chu",
                "C.H. Ko"
            ],
            "title": "An experimental study on augmented reality assisted manual assembly with occluded components",
            "venue": "J. Manuf. Syst",
            "year": 2021
        },
        {
            "authors": [
                "Z.H. Lai",
                "W. Tao",
                "M.C. Leu",
                "Z. Yin"
            ],
            "title": "Smart augmented reality instructional system for mechanical assembly towards workercentered intelligent manufacturing",
            "venue": "J. Manuf. Syst",
            "year": 2020
        },
        {
            "authors": [
                "N.M. Kumar",
                "N.K. Singh",
                "Peddiny",
                "V.K. Wearable smart glass"
            ],
            "title": "Features, applications, current progress and challenges",
            "venue": "Meta, Second International Conference on Green Computing and Internet of Things 10, 12",
            "year": 2018
        },
        {
            "authors": [
                "P. Milgram",
                "F. Kishino"
            ],
            "title": "A taxonomy of mixed reality visual displays",
            "venue": "IEICE Trans. Inf. Syst",
            "year": 1994
        },
        {
            "authors": [
                "F. Quint",
                "F. Loch",
                "P. Bertram"
            ],
            "title": "The challenge of introducing AR in industry-results of a participative process involving maintenance engineers",
            "venue": "Procedia Manuf",
            "year": 2017
        },
        {
            "authors": [
                "A. Syberfeldt",
                "O. Danielsson",
                "Guatavsson",
                "P. Augmented reality smart glasses in the smart factory"
            ],
            "title": "Product evaluation guidelines and review of available products",
            "venue": "IEEE Access 5, 9118\u20139130. https:// doi. org/ 10. 1109/ ACCESS. 2017. 27039 52",
            "year": 2017
        },
        {
            "authors": [
                "L.H. Lee",
                "Hui",
                "P. Interaction methods for smart glasses"
            ],
            "title": "A survey",
            "venue": "IEEE Access 6, 28712\u201328732. https:// doi. org/ 10. 1109/ ACCESS. 2018. 28310 81",
            "year": 2018
        },
        {
            "authors": [
                "A. Hamacher",
                "J. Hafeez",
                "R. Csizmazia",
                "T. Whangbo"
            ],
            "title": "Augmented reality user interface evaluation-performance measurement of hololens, moverio and mouse",
            "venue": "input. iJIM 13,",
            "year": 2019
        },
        {
            "authors": [
                "Siltanen",
                "S. Theory",
                "applications of marker-based augmented reality"
            ],
            "title": "Licentiate thesis",
            "venue": "VTT Science 3. https:// publi catio ns. vtt. fi/ pdf/ scien ce/ 2012/ S3. pdf",
            "year": 2012
        },
        {
            "authors": [
                "P. Daponte",
                "L. De Vito",
                "F. Picariello",
                "M. Riccio"
            ],
            "title": "State of the art and future developments of the Augmented Reality for measurement applications",
            "venue": "Measurement",
            "year": 2014
        },
        {
            "authors": [
                "M. Minteh",
                "T. Tchotang",
                "L. Meva\u2019a",
                "B. Kenmeugne"
            ],
            "title": "Assessment of the impact of preventive maintenance strategy on the reliability of a rice combine harvester in the Gambia",
            "venue": "Int. J. Agric. Eng",
            "year": 2019
        },
        {
            "authors": [
                "O.V. Myalo",
                "S.P. Prokopov",
                "V.V. Myalo",
                "G.V. Redreev",
                "Demchuk",
                "E.V. Feasibility",
                "efficiency of agricultural machinery maintenance. In IOP Conference Series"
            ],
            "title": "Earth and Environmental Science",
            "venue": "659, 012053. IOP Publishing",
            "year": 2021
        },
        {
            "authors": [
                "J.A. Schlueter"
            ],
            "title": "Remote maintenance assistance using real-time augmented reality authoring",
            "venue": "Graduate Theses and Dissertations",
            "year": 2018
        },
        {
            "authors": [
                "Aukstakalnis",
                "S. Understanding the human senses",
                "their relationship to output/input devices. In Practical Augmented Reality"
            ],
            "title": "A Guide to the Technologies, Applications, and Human Factors for AR and VR (ed",
            "venue": "Lewin, L.) 77\u2013101",
            "year": 2016
        },
        {
            "authors": [
                "W. Broll",
                "P. Grimm",
                "R. Herold",
                "D. Reiners",
                "C. Cruz-Neira"
            ],
            "title": "VR/AR output devices. In Virtual and Augmented Reality (VR/AR) (eds",
            "year": 2022
        },
        {
            "authors": [
                "O.J. Muensterer",
                "M. Lacher",
                "C. Zoeller",
                "M. Bronstein",
                "K\u00fcbler",
                "J. Google Glass in pediatric surgery"
            ],
            "title": "An exploratory study",
            "venue": "Int. J. Surg. 12, 281\u2013289. https:// doi. org/ 10. 1016/j. ijsu. 2014. 02. 003",
            "year": 2014
        },
        {
            "authors": [
                "D. Chatzopoulos",
                "C. Bermejo",
                "Z. Huang",
                "Hui",
                "P. Mobile augmented reality survey"
            ],
            "title": "From where we are to where we go",
            "venue": "IEEE Access 5, 6917\u20136950",
            "year": 2017
        },
        {
            "authors": [
                "A. Jones",
                "P. Sevcik",
                "R. Wetzel"
            ],
            "title": "Internet connection requirements for effective video conferencing to support work from home and eLearning",
            "venue": "NetForecast Report",
            "year": 2021
        },
        {
            "authors": [
                "S.A. Alduais",
                "K.F. Salama"
            ],
            "title": "Assessment of ambient-noise exposure among female nurses in surgical cardiac intensive care unit",
            "venue": "J. Multidiscip. Healthc",
            "year": 2019
        },
        {
            "authors": [
                "J. Depczynski",
                "R.C. Franklin",
                "K. Challinor",
                "W. Williams",
                "L.J. Fragar"
            ],
            "title": "Farm noise emissions during common agricultural activities",
            "venue": "J. Agric. Saf. Health",
            "year": 2013
        },
        {
            "authors": [
                "J. Upton",
                "J.F. Penry",
                "M.D. Rasmussen",
                "P.D. Thompson",
                "D.J. Reinemann"
            ],
            "title": "Effect of pulsation rest phase duration on teat end congestion",
            "venue": "J. Dairy Sci",
            "year": 2016
        },
        {
            "authors": [
                "E.J. O\u2019Callaghan"
            ],
            "title": "Effects of pulsation characteristics on machine yield, milking time and cluster stability",
            "venue": "Irish J. Agric. Food Res",
            "year": 1998
        }
    ],
    "sections": [
        {
            "text": "1 Vol.:(0123456789) Scientific Reports | (2022) 12:15729 | https://doi.org/10.1038/s41598-022-20154-2\nwww.nature.com/scientificreports"
        },
        {
            "heading": "Assessment of video see\u2011through",
            "text": "smart glasses for augmented reality to support technicians during milking machine maintenance Gabriele Sara*, Giuseppe Todde & Maria Caria\nSmart glasses for augmented reality are digital technology under investigation in the agricultural sector. The potential of augmented reality was underlined, in some scientific contributions, as a support tool for farmers\u2019 activities and for the decision\u2011making process. One of the most practical applications studied for augmented reality was in maintenance operations, where the use of smart glasses showed high capability. This work focuses on the evaluation of the performance and applicability of smart glasses with a video see\u2011through display system and testing the device\u2019s available functions in agricultural frameworks. In addition, an augmented assistance scenario describing the main steps involved in the functioning of the maintenance operation was developed for milking machine inspection. The audio\u2013video quality, battery life, detection capabilities of markers, and voice control interaction system were evaluated. The results showed the capabilities of smart glasses to reach augmented information from a long distance in a short time interval and to transmit audio and video with a high level of detail, allowing discrimination of small objects during remote assistance with reduced delay. The built maintenance scenario represents an example of augmented reality digital assistance application in the inspection and maintenance of the milking machine. The device performance and the proposed maintenance scenario underline the potential that augmented reality could have in the agricultural sector to assist and guide both farmers and technicians to timely problem solving. This solution fits into the agriculture 4.0 perspective, which is increasingly focused on digital transformation to improve farms\u2019 efficiency and sustainability.\nOver the last few years, interest in smart farming, precision agriculture, precision livestock farming, and agriculture 4.01\u20133 has greatly increased. All these terms have been related to different and even complex technologies and to information, digital and robotic technologies, which have been increasingly studied for use in the agricultural sector. The main objectives of introducing this type of technology are to improve farm production in terms of crop or livestock yield, streamline the input use (fertilizer, pesticides, etc.) and reduce waste during the whole production process, leading to more sustainable agriculture4\u20136. Hence, the spread of digital technologies could bring economic, social, and environmental benefits to the agricultural and livestock farming sector in relation to internet access, representing the most critical factor for digital technology diffusion7,8. Many new smart technologies have been studied and used to support and simplify farmers\u2019 work, such as autoguidance equipment, variable rate technology, and automatic milking systems. Moreover, farm size and education9 are influential factors with a large effect on digital farming technology adoption.\nSmart glasses (SG) for augmented reality (AR) are a digital technology under investigation in the agricultural sector, where potential applications are underlined as a support tool for farmers\u2019 activities and the decision-making process10\u201312. Furthermore, the popularity of AR technologies will grow in the coming years due to improvements in the evolution of cloud computing technologies that will strengthen the power of AR experiences13. Possible applications of AR have been found in the maintenance14 and manufacturing fields15, where AR showed high potential to enhance operator performance. AR-overlayed 3D information may replace\nOPEN\nDepartment of Agricultural Sciences, University of Sassari, Viale Italia 39A, 07100 Sassari, Italy. *email: gsara@ uniss.it\n2 Vol:.(1234567890) Scientific Reports | (2022) 12:15729 | https://doi.org/10.1038/s41598-022-20154-2\npaper-based or 2D digital instructions in the assembly/disassembly industrial sector as the use of AR technology was observed to require less time and workload for assembly operators16, reducing the error rate17. SG have been defined as a wearable computer, despite their lower computational power, and are designed to provide AR experiences through specific applications and visualization systems where virtual or digital contents are projected18. The central concept of AR systems concerns overlaying computer-generated contents over the real environment, hence augmenting the physical object with digital information19. SG technologies for AR can be considered an emerging technology, even more so in the agricultural sector, for which there are no customized solutions. In addition, AR content authoring is the main obstacle to AR spreading, since AR content creation \u201crequires high effort\u201d and cannot be done \u201cby shop-floor employees\u201d20. SG can be distinguished by features, principally for AR element visualization systems (optical see-through, video see-through, and retinal display), the available interaction method (specific button and touchpad placed on the device, external joypad, voice control), and the tracking module integrated into the AR device as well as other characteristics (operating system, processor, functionality, battery life, and framework design)21\u201323.\nThe tracking module is considered the central system of AR technology, allowing for the identification of the user, device, and environment positions and orientation, anchoring the virtual contents on the physical objects and rendering combined digital and real elements on the AR device display24. The tracking system can be supported by different sensors (accelerometer, magnetometer, gyroscope, video camera, RFID, GPS, etc.) in relation to which it is possible to distinguish the technique used by the AR device in vision-based or sensorbased methods. The former case concerns the use of markers (Template, DataMatrix, QR Code) or features of the environment, in contrast with the other cases that use inertial, magnetic, electromagnetic, or ultrasonic sensors to measure and pose the information14. Although sensor-based methods have been considered fast and robust, marker-based tracking modules are preferred when high accuracy during position estimation is needed, e.g., in the medical sector25. This remains true until the precision and accuracy of the GPS receiver of SG are improved for better AR content positioning, especially for outdoor applications10. Moreover, the use of QR code-based techniques have been studied in depth and applied to AR and SG case studies, representing one of the easiest methods to detect and overly augment information21,22,24.\nThe maintenance operations represent one of the main use cases of the AR technologies, where the use and application of AR devices to support operators have been widely studied. In the agricultural sector, the maintenance operations of machines have a high impact on farm management costs, both in terms of time and economical resources, which tend to increase with the machine\u2019s hours of use and with the increase in machine fleets26,27. Using the AR and SG technologies could reduce the economic impact of machine maintenance operations burdening the farmer. However, AR and SG devices have some issues and challenges to be addressed before their diffusion into the agricultural sector12,28. In fact, during maintenance operations the high level of audio and video quality transmission is a key factor for a profitable remote assistance. In addition, the complex pandemic situation caused by the SARS-CoV-2 virus poses new challenges to assist in the field. In this context, guided or remote assistance supported by the SG for AR could play a strategic role in enabling farm workers to solve problems that arise, especially with the machines, to assure their regular maintenance.\nThe main features of the SG (visualization system, interaction system, and tracking method) could influence user experience and farmer performance using the AR device while working. For this reason, it is important to evaluate and outline the differences among how different SG devices perform in an agricultural context. In fact, the agricultural environment is characterized by diverse operating situations, that may vary throughout the day. The operator performs on-farm activities in environments with different noise levels, lighting conditions (indoor, outdoor), and working space (e.g., milking parlor, barn, open field). Hence, considering the features that characterize SG, the visualization system represents one of the main discriminating characteristics, where, in previous studies, optical display systems were tested in an agricultural context11,12. This work focuses on the study and evaluation of the performances and applicability of an SG with a video see-through system, testing the available functions for agricultural frameworks. In fact, this type of visualization system, is considered to have a good visibility of bright and dark virtual contents in different backgrounds (bright and darks) and thus suitable for both indoor and outdoor use29,30. Moreover, considering the future employment of SG for AR in the maintenance of the agricultural machines to assist farmers in the field, an AR assistance scenario, which describes the main steps involved in the functioning of the maintenance operation through SG, was developed for milking machine inspection."
        },
        {
            "heading": "Materials and methods",
            "text": "In this study, a wearable device has been adopted to facilitate maintenance operations. In fact, to accomplish these activities, the operator needs to follow specific procedures that require the use of both hands (e.g., connecting sensors, machine components inspection, etc.), for this reason, handheld devices were not considered in this study. Specifically, the Vuzix\u00ae M400 Smart Glasses (M400), produced by the Vuzix Corporation (25 Hendrix Road West Henrietta, NY 14586, USA), were adopted. The M400 is an AR viewer designed for harsh environments (certified IP67), composed of three main components: smart viewer, power bank, and mounting frame. The optical system of the M400 is a video see-through display; on the side of the smart viewer, there is a navigation system that allows interaction with the user interface. There are two types of tangible navigation systems, the classical one, composed of three keys (up, down, enter) and then the touchpad. In addition, there is another navigation system controlled by the voice that allows interaction with the M400 with a preregistered voice command. The power bank is equipped with three LED indicating the status of the battery. In Table\u00a01, the technical specifications of the SG used are stated.\nTests were performed in the laboratory of the Agricultural Sciences Department of the University of Sassari. The trials involved evaluating the device\u2019s performance using the available functionalities and applications\n3 Vol.:(0123456789) Scientific Reports | (2022) 12:15729 | https://doi.org/10.1038/s41598-022-20154-2\nin controlled light conditions that replicate the milking facilities environment. Specifically, the tests evaluated based on the audio\u2013video quality transmitted and received from the M400, the battery life, the detection of AR markers (QR codes), and, finally, the voice control interaction system. Moreover, a milking machine maintenance framework was built through a specific application to support technicians and farmers during maintenance procedures. The performance tests were designed considering the expected end-use case and thus the maintenance procedures of the milking machine. Thus, getting information on scanning modes allows us to understand where to get the augmented information from, while evaluating the audio\u2013video quality and transmission delay allows understand if real-time assistance in the field is feasible.\nAll the procedures of this research were carried out in accordance with relevant guidelines and regulations. Informed consent was obtained from all the participating subjects. Moreover, based on previous research protocols the ATS Sardegna ethical committee reported that, when no health care investigators interact within the procedures of the study, the adopted protocol does not require an opinion from the ethics committee.\nQR codes tests. The scanner app was used to evaluate the performance of the M400 (time and distance) with respect to detecting markers that allow us to merge the AR contents onto the real environment. In this study, the QR code was used as a marker due to its capability to store a great amount of information into it, generating up to 40 versions in relation to the encoded information31. For the trials, specific farm data were encoded in two different QR code versions. In the first QR code (Fig.\u00a01a), the URL linked to the farm information was encoded. This QR code was a Version 3 (V3) composed of 29 \u00d7 29 modules (black or white squares in the QR code). In the second QR code (Fig.\u00a01b), the farm information was directly encoded in text format inside the code, resulting in a Version 11 (V11) composed of 61 \u00d7 61 modules.\nUsing both QR code versions (V3, V11), the minimum and maximum scanning distances were measured from the M400 camera. QR codes with increasing printed size, ranging from 15\u00a0mm per side to 200\u00a0mm per side, were adopted. To assess the scanning time, i.e., the required time to display the farm information on the SG, three QR codes of different printed sizes (35\u00a0mm, 40\u00a0mm, 75\u00a0mm per side) were tested. The scanning time represents the time frame from the activation of the scanner app to the visualization of the associated information. The codes\nItem Technical features\nProcessor 8 Core 2.52\u00a0GHz Qualcomm XR1\nFlash memory 64 GBytes\nOperating System (OS) Android 8.1\nDisplay Occluded OLED, 24-bit color, 640 \u00d7 360\nField of view 16.8 degrees (5 inches)\nSensors Gyroscope-accelerometer-magnetometer (3 axis)\nConnectivity GPS, Wi-Fi, Bluetooth, USB\nCamera 12.8 Megapixel, 30 fps\nBattery Lithium polymer 1000 mAh (power bank)\nBattery life 2\u201312\u00a0h\nController input Touchpad, 3 buttons, voice command\nFigure\u00a01. QR codes used during the trial: Version 3 (V3) with 29 modules (a); Version 11 (V11) with 61 modules (b).\n4 Vol:.(1234567890) Scientific Reports | (2022) 12:15729 | https://doi.org/10.1038/s41598-022-20154-2\nwere placed on the wall at the wearer\u2019s eye level and scanned at 0.4\u00a0m. The scanning tests were performed until the end of the battery and repeated for two charging cycles, collecting a total of 2254 records. This test allows us to investigate whether the size and type of QR code affects the time and distance of augmented information gathering. QR codes with high module density can be efficiently used to provide textual augmented information quickly. In fact, encoding a high number of bits (digits) inside the QR code requires a longer time frame but the information will be directly decoded by the SG\u2019s camera.\nAudio video quality transmission test. Two tests were performed to evaluate the audio\u2013video quality contents from the SG to a remote device (PC) using a voice over internet protocol (VoIP) remote call. The first test concerned lag time detection to assess the time needed to transmit audio videos between M400 and laptops. This trial was performed using two different applications: Vuzix\u00ae Remote Assist application (App1) and Brochesia\u00ae B View application (App2). Both apps are downloadable from the Vuzix App Store and allow to connect field workers, provided with a mobile device (smartphone, smart glasses), to a remote expert (web dashboard) for real-time audio/video cooperation. Moreover, to fully use both applications, subscription is required after the trial version. The lag time of the video and the audio transmitted from the M400 to the laptop were registered by synchronizing the clocks of the two operators and recording the emission and receiving time of a fixed signal for audio and a programmed position for video.\nThe second test concerns the evaluation of the observable level of detail of the transmitted image through SG. This trial will highlight whether, in a remote video call, the farmer wearing SG would be able to show to a remote operator specific components of a milking machine undergoing maintenance. It was performed using a standard Snellen chart, composed by eleven lines with decreasing size letters blocks. The printed chart was scanned with SG in a 0.5\u00a0m distance, and the receiving operator was reading the transmitted chart image on the laptop\u2019s 16-inch screen. The error rate was measured and related to the decreasing size of the characters32.\nDuring the trials, the SG and the laptop were connected through App1 and App2 using the University\u2019s Wi-Fi connection (20 Mbps upload/download on average). The tests were carried out with two different video quality resolutions (VGA, 640 \u00d7 480 pixels; HD, 1280 \u00d7 720 pixels) selectable in the SG application settings and with a Frames per second of 30 fps. For each resolution, the lag time was measured 20 times, whereas the Snellen chart test was performed by two operators. Last, during these trials, the status of the battery in a continuous audio\u2013video transmission was monitored.\nVoice control test. The voice commands were tested to evaluate the noise levels, reachable in different agricultural contexts, that might influence the M400 to recognize voice inputs. The available speech commands were tested at growing levels of noise starting from 65 to 85\u00a0dB and in a silent environment (40\u201350\u00a0dB) as a control. The commands tested were 39 voice controls encoded in the device, e.g., \u201cHello Vuzix\u201d, \u201cGo back\u201d, \u201cMove/scroll down\u201d, \u201cScroll left\u201d, \u201cGo up\u201d, \u201cOpen\u201d, \u201cSelect this\u201d, etc. The test was performed by four operators, and the sound pressure levels were monitored using a Trotec SL4 (Trotec GmbH) class 2 sound level meter. During the tests, the voice commands were considered unrecognized after three failed repetitions with an increasing tone of voice.\nAssistance scenario for the farmer. In this work, an assistance scenario was designed through an AR application developed by Brochesia\u00ae (BStep). This software application allows building workflows to guide the operator during maintenance activity with punctual augmented information using SG.\nThe assistance scenario was the measurement of pulsation characteristics related to the mechanical test from milking machines. These measurements commonly involve all the milking units of the milking machine. The workflow was developed according to the International Organization for Standardization (ISO) 6690:2007 and 3918:2007 standards33,34, considering the dry test. The assistance procedures were associated with a QR code placed on the milking machine and available through the scanning function of the SG. The workflow for the milking machine inspection process was tested on the milking system in the laboratory of the Agricultural Sciences Department of the University of Sassari (Italy).\nStatistical analysis. Descriptive statistics (arithmetic average, standard deviation, minimum and maximum values) were assessed and are reported for the QR code, the lag time and the voice control tests. Statistical analysis was carried out by performing a Kruskal\u2013Wallis rank sum test for the scanning time and audio\u2013video quality transmission data (non-normal data distribution) and a multiple comparison after the Kruskal\u2013Wallis test (P < 0.05). The analyses were performed with R Studio software (version: 4.0.5)."
        },
        {
            "heading": "Results and discussion",
            "text": "Performance and functionality tests. Markers, e.g., QR codes and bar codes, are important elements in AR systems because they represent a valuable tracking method to overlay digital contents on physical and real objects. The capability of the AR device to detect markers and the distance from which they are detectable are aspects that allow us to understand the accessibility of the digital information. For these reasons, the minimum and maximum scanning distances of different sized QR codes using the M400 were evaluated. Figure\u00a02 describes the results obtained from the scanning distance of the V3 (Fig.\u00a02a) and V11 (Fig.\u00a02b) QR codes in relation to their size. As expected, the minimum and maximum scanning distances were directly proportional to the QR code size for both code types. The minimum distance and code dimension ratio for the two types of codes was constant (6.5 on average), as the minimum scanning distance is dictated by the camera framing. A different situation was observed for the maximum scanning distance of the two codes. In fact, the maximum scanning distance was higher for the V3 than the V11 code, with a distance and code dimension ratio of 48.8 and 21.5,\n5 Vol.:(0123456789) Scientific Reports | (2022) 12:15729 | https://doi.org/10.1038/s41598-022-20154-2\nrespectively. Hence, using the M400 allowed detection of the augmented information from almost 2\u00a0m scanning a QR code of 40 \u00d7 40\u00a0mm size with 29 modules.\nThese results underline the importance of choosing an appropriate marker type and size to encode the required information. In farms, there are different contexts where specific information are needed to be detected (e.g., form long distance, in a small area, etc.). The scanning distance or the marker size may be influenced by the surrounding context. Furthermore, knowing the maximum scanning distance improves the labor organization. High ambient brightness (full sun) could affect the scanning of markers and thus the augmented information visualization. Although the agricultural context is characterized mainly by outdoor activities where brightness could be a marker-detection-related issue. Previous studies have shown the feasibility to obtain augmented information, by scanning a QR code through the SG camera, during outdoor activities and from the tractor cabin11. Moreover, high brightness would mainly affect AR content visualization on devices with the optical see-through systems rather than video see-through systems30,35.\nTable\u00a02 reports the results of the scanning time where significant differences were recorded among the three QR code sizes considered, which ranged on average between 1.90\u00a0s (75\u00a0mm) and 2.03\u00a0s (35\u00a0mm). However, these differences are very low (0.13\u00a0s) and do not appear to affect farmer performance.\nMoreover, the battery life was monitored during the QR code scanning tests and the videocall tests using both applications (App1, App2). The overall average of 3\u00a0h and 28\u00a0min was recorded for the scanning tests, whereas 3\u00a0h and 50\u00a0min and 4\u00a0h were recorded for the videocall test, respectively. These values could be a limit if considering the amount of worker hours per day (8\u00a0h). In any case, this device should be used for specific and punctual activities carried out on the farm.\nR\u00b2 = 0.9978\nR\u00b2 = 0.9995\n0\n2\n4\n6\n8\n10\n0 50 100 150 200 250\nSc an\nni ng\nd is\nta nc\ne (m\n)\nmin\nmax\na\nR\u00b2 = 0.9965\nR\u00b2 = 0.9961\n0\n1\n2\n3\n4\n5\n0 50 100 150 200 250\nSc an\nni ng\nd is\nta nc\ne (m\n)\nQR code size (mm)\nmin\nmax\nb\nFigure\u00a02. Simple linear regression between minimum and maximum scanning distance (m) and QR code size (mm). Version 3 QR code (V3) with 29 \u00d7 29 modules (a), Version 11 QR code (V11) with 61 \u00d7 61 modules (b).\n6 Vol:.(1234567890) Scientific Reports | (2022) 12:15729 | https://doi.org/10.1038/s41598-022-20154-2\nThe audio and video transmission lag time from the M400 to a laptop was measured to test the quality of a videocall and to evaluate the feasibility of video-remote assistance in real time. The comparison of the results obtained by the two applications (Table\u00a03) showed that the audio transmission was generally less than 0.5\u00a0s delay, and no significant difference was observed compared with the transmission quality (Tq). This delay seems to not influence the on-field assistance quality, since delays ranging from 0.4 to 0.15\u00a0s are considered acceptable values for high-quality real-time audio transmission36.\nConsidering the lag time for the video transmission, a 0.9\u00a0s delay on average was observed for both applications. Moreover, a significant difference of 0.7\u00a0s of delay between App1 and App2 was observed within the lower Tq. The results obtained were lower than 3\u00a0s on average observed by Muensterer et\u00a0al.32, thus the video conferencing quality of SG may be considered adequate to connect agricultural operators and technicians. Furthermore, the average delay observed in video transmission does not represent a limiting factor since packet loss is more influential on video quality transmission, which was not observed in our tests37.\nIn general, the audio\u2013video lag time was affected by the internet connection. From this perspective, it is important for the farm to have an adequate internet connection (20\u201330 Mbps) to support the data transfer and video streaming from the SG to a central or remote computer and minimize the delay during a remote videocall.\nFigure\u00a03 shows the results obtained from the visual acuity test using the standard Snellen chart. This test allows us to verify the level of content detail transmitted by the SG, which is important when farmers share information in remote assistance with an expert, technician, or another farm operator. The results underlined that a 13\u00a0mm character transmitted by the M400, from 0.5\u00a0m, on the laptop screen can be read by the receiving operator in every test. In the HD resolution, 9\u00a0mm characters were always recognized (100% of characters correctly recognized). On the other hand, 94% of the 9\u00a0mm characters were correctly detected at a lower resolution. In addition, it is possible to consider the 7\u00a0mm character the lowest printed size easily readable with both resolutions (HD and VGA) with 92% and 88% of the character recognized, respectively. With the 4\u00a0mm character size, the gap between the two resolutions increases drastically. In fact, 70% of characters were detected with the HD resolution, in contrast to 15% with the lower resolution. Hance, a farmer with the M400 could share his point of view with a high level of detail with an expert elsewhere, considering that the printed character size is recognizable depending on the videocall resolution.\nThe transmission results obtained in this study, both in terms of lag time and quality of the video sheering, were found to be appropriate to carry out maintenance activities in milking machine equipment. In fact, small details of the components (milking tubes, claws, liners, valves, etc.) were easily distinguishable from the remote technician.\nThe interactive system of the M400 includes three buttons, a touchpad, and a vocal interaction (voice commands/control). Considering the voice commands, several trials were carried out to evaluate the capability of the M400 system to respond to the voice action commands enabled at different levels of noise. Studying an\nApp Tq Audio (s) SD Video (s) SD\n1 VGA 0.55ab \u00b1 0.60 0.55b \u00b1 0.51\nHD 0.15b \u00b1 0.37 0.90ab \u00b1 0.64\n2 VGA 0.65a \u00b1 0.49 1.25a \u00b1 0.72\nHD 0.40ab \u00b1 0.50 0.95ab \u00b1 0.69\nFigure\u00a03. Percentage of correct characters of the Snellen chart read by an operator on the laptop\u2019s screen and transmitted by SG during a videocall. Two resolutions were used: VGA -\u25b2- (640 \u00d7 480 pixels) and HD -\u25cf- (1280 \u00d7 720 pixels). The distance between the smart glasses camera and the chart was 0.5\u00a0m.\n7 Vol.:(0123456789) Scientific Reports | (2022) 12:15729 | https://doi.org/10.1038/s41598-022-20154-2\nenvironment with soft noise (60\u201368\u00a0dB)38 was observed that all the speech commands tested were recognized by the M400 that made the specific action requested (Table\u00a04). A similar situation was observed with a noise level of 70\u00a0dB, where for all operators, 89% of the commands were recognized on average. At the 75\u00a0dB level of noise, an operator effect was observed. In fact, it was detected that three operators had a mean recognition rate of 79%, while the last operator had a recognition rate of just 5%. Therefore, 70\u201375\u00a0dB represents the border points of speech command detection for the Vuzix M400, considering that noises greater than or equal to 80\u00a0dB do not allow the use of the voice control function. This is an important function that, when implemented into the SG, allows the device to be managed completely hands-free. Regardless, in agriculture, the environmental noise can have different levels depending on the activities carried out.\nDespite Depczynski et\u00a0al.39 reported that only a few farm conditions stay below the threshold level of 70\u201375\u00a0dB observed for the M400, the noise levels in specific farm situations were measured. Through the sound level meter, the noise related to some agricultural and livestock activities (e.g., use of tractor or milking machine) was monitored to explore in which cases the voice control function of M400 could be effectively used. The results showed that M400 voice control could operate in a cabin tractor (73\u00a0kW power), in the milk room and in the milking parlor (Table\u00a05). In fact, the most frequent level of noise measured in the milk room and milking parlor was between 68.7 and 75.3\u00a0dB. Moreover, the levels over the threshold detected in these rooms (maximum levels) regarded different sources of sound, e.g., the entry of the animals in the milking parlor, the opening and closing of the gates, and the animals themselves. In contrast, the speech commands were difficult to use when the operator was inside the engine room because the minimum level of decibel recorded was 93.7, which is over the threshold measured for M400. Another agricultural working situation was monitored, involving tractor driving. Two situations were checked: a tractor with a cabin that was more isolated from external noise and a tractor without a cab. The minimum level of decibel recorded allowed us to interact with the M400 by voice, and was recorded in a no working condition or when the tractor engines were running but idling. When the tractors were set in motion, a level of noise over the acceptable threshold was found, as attested by the mode and maximum values recorded for both situations.\nMilking machine assistance scenario. To verify the proper operation of the milking machine, it is periodically necessary to check the system by certified operators. The ISO 6690:2007 standard specifies mechanical tests for the milking machine to verify installation compliance and components. In this work, a specific milking machine checking process was considered and made available in AR. In particular, the measurement process steps of pulsation characteristics were built on the Brochesia\u00ae portal in the workflow section (B Step). The workflow was developed as a sequence of step-by-step instructions that the operator must follow while wearing SG.\nPulsation is the cyclic opening and closing of the liner34. The pulsation cycle is composed of 4 phases: liner opening (phase a); liner open (phase b: milking); liner closing (phase c); liner close (phase d: rest). The pulsation characteristics affect milking performance and teat condition40,41. Hence, checking the pulsation characteristics as well as the milking system and components is needed at least once or twice a year. The certified operator to verify the system compliance must use specific measuring instruments and follow several procedures depending on the test to be performed, e.g., connect the sensors (flowmeter, pulsograph), disconnect some components of the milking machine (vacuum controller). Commonly, operators are supported by a paper guide or paper checklist to ensure that all test procedures are performed correctly. In this context, it appears that an augmented reality procedure supported by icons and images (visual information procedures) might be an important element in\nTable 4. Action commands correctly recognized (%) by the Vuzix M400 voice control function at different noise levels (dB) and for different operators.\nOperator 65\u00a0dB (%) 70\u00a0dB (%) 75\u00a0dB (%) 80\u00a0dB (%) 85\u00a0dB (%)\n1 100 89 92 3 0\n2 100 95 65 0 0\n3 97 86 81 0 0\n4 100 84 5 0 0\nAverage 99 89 61 1 0\nTable 5. Level of noise in decibel (dB) recorded in different livestock and agricultural working situations/ places with the sound level meter. *In all scenarios the machines were in operation.\nEngine room outside Engine room Milk room Milking parlor Milking parlor 2 Cabin tractor (73\u00a0kW)\nTractor without cab (41\u00a0kW)\nMin 80.8 93.7 67.3 68.6 68.7 58.5 73.6\nMax 85.1 98.2 82.2 89.8 90.5 90.7 91.3\nMode 82.0 96.8 68.7 75.3 74.9 78.8 86.3\nAverage 82.2 96.7 71.4 76.6 76.5 76.3 84.0\n8 Vol:.(1234567890) Scientific Reports | (2022) 12:15729 | https://doi.org/10.1038/s41598-022-20154-2\nlearning and performing this type of test. In addition, an SG-specific maintenance application for these mechanical tests is even more important, considering that during maintenance procedures, the operators need to make various movements and use their hands frequently.\nThe test for pulsation characteristics consists of several steps and is related to the previous tests carried out on the milking machine. Three steps are fundamental: the connection of the pulsograph to the machine, the recording of five complete pulsation cycles and the evaluation of the data recorded for each milking unit, i.e., length of the a, b, c, and d phases per pulsation cycle. In Fig.\u00a04, the workflow scheme of the pulsation characteristics process developed on the BStep application portal is reported, following the ISO 6690:2007 standard.\nThe workflow of the pulsation characteristics test was available from a 40 \u00d7 40\u00a0mm QR code with 29 modules placed on the milking machine (Fig.\u00a05). The QR code represented the marker that the operator needed to scan with the SG to obtain the augmented information. As observed in the previous tests, this type of QR code can be scanned quickly from a distance of 2\u00a0m using the M400. In addition, although different sources of noise were\nFigure\u00a04. Mechanical milking installations tests: workflow scheme built with the BStep application software describing the steps of the pulsation characteristics test.\n9 Vol.:(0123456789) Scientific Reports | (2022) 12:15729 | https://doi.org/10.1038/s41598-022-20154-2\nfound in the milking parlor (animal noises, pulsators, vacuum regulator valve, etc.), where the maintenance scenario was contextualized, the noise levels most frequently recorded did not affect the use of SG by voice commands (Table\u00a05). Therefore, the operator was able to proceed with the milking machine and components inspection while having his hands completely free.\nThe first augmented instruction provided to the operator was the question: Is this the first test performed on the milking machine? which implies if other instruments meter and configurations are in place or not. Interacting with the workflow by the touchpad of the M400, the operator can choose two different ways (Fig.\u00a04). Nevertheless, some preliminary steps before checking the pulsation were needed: in one case, e.g., stop the vacuum pump and set the system in simulated milking, mount vacuum regulator, disconnect the flowmeter (if previously used); in another case, turn on the vacuum pump (Fig.\u00a06a), wait fifteen minutes, set the system in simulated milking, etc., as shown in Fig.\u00a06c. Then, attaching the sensors of the pulsograph with the T-piece connection was requested and the right way was explained through the augmented instructions (Fig.\u00a06b). Afterward, recording a minimum of 5 pulsation cycles with the test equipment to obtain reliable data was indicated. The recommended values regarding the length of each phase (a, b, c, d) were visible on M400 to support the operator during the evaluation of\nFigure\u00a05. Operator with SG while accessing the maintenance augmented instructions through the QR code (40\u00a0mm), placed on the milking machine management control unit (green box).\nFigure\u00a06. Example of augmented information visualized on the smart glasses M400 during the mechanical milking machine test through the BStep app. (a) Indications for vacuum pump ignition, (b) indication for the sensor attachment (T connection); (c) setting the milking machine in simulated milking (moving the claw valve and closing the liner with the specific plug).\n10\nVol:.(1234567890)\nScientific Reports | (2022) 12:15729 | https://doi.org/10.1038/s41598-022-20154-2\nthe data recorded in all milking units. When anomalous pulsation curves were observed, the operator was able to view tips and suggestions to solve the problem and restore the pulsation system.\nOverall, the results obtained confirmed the suitability of the device tested for milking machine maintenance. The aspects characterizing the agricultural environment such as remote location, limited or difficult access routes, and reduced number of specialized maintenance workers per farm, are the main driving factors for using AR and SG as a support tool for specialized remote assistance. Additionally, the agricultural environment is remarkably heterogeneous and different from other workplace (i.e., industry sector) where different operating situations (field crop management, greenhouse, barn animals, grazing animals, etc.) and different machinery (tractors, implements, milking equipment, etc.) can coexist. The milking machine is one of the farm machines that require specific maintenance by skilled technicians. As indicated by ISO 6690:2007, several mechanical components (vacuum pump, milking units, pulsators, etc.) and operative parameters during milking must be checked periodically for an efficient milking operation42. Failures during milking of such equipment can cause economic losses due to a decreasing milk yield. Therefore, prompt and precise maintenance is necessary both in emergency situations and in scheduled checks."
        },
        {
            "heading": "Conclusions",
            "text": "In this work, the performance of specific smart glasses for augmented reality (Vuzix M400) was evaluated from an agricultural point of view. Thus, considering some features of the device (QR code scanner, AR image and video quality transmission) and the possibility of installing applications, a step-by-step maintenance scenario in augmented reality was developed to support technicians during the mechanical tests of the milking machines.\nThe tests underlined that the SG has a natural aptitude to be employed in the field of agricultural assistance. Specifically, the features of the SG adopted (AR precise content visualization, video transmission, voice control) could improve the operators\u2019 performance in the maintenance of on farms machines (tractors, milking machines, food processing equipment, etc.). In fact, the results showed the capability of SG to reach the AR information from a long distance (through a QR code) in a short time interval and to transmit audio and video with a high level of detail, allowing discrimination of small objects during remote assistance with reduced delay. Moreover, the tests allowed us to emphasize that this device can be managed completely hands-free with the voice control function but only in\u00a0situations with a maximum noise level of 70\u201375\u00a0dB, depending on the voice tone of the user.\nIn relation to this application context, a simulated maintenance scenario was built with an external service, representing an example of an AR remote assistance application in the agricultural context. The proposed maintenance scenario underlines the potential that AR could have in the agricultural sector to assist and support both farmers and technicians to solve problems while working on simultaneously visualizing different information on the SG. In relation to AR potentiality for the agricultural sector, more complex but not complicated, applications or assistance scenarios should be developed considering the farmers\u2019 needs, also thanks to the implementation of cloud computing where the AR instruction for the maintenance of agricultural machines can be stored.\nOverall, this study highlighted how video see-through SG can be used for agricultural machinery maintenance by providing digital guidance to the operator. Although the type of visualization system may be found in other handheld devices (smartphones and tablets), these devices do not leave the operator\u2019s hands free to simultaneously perform the tasks while receiving augmented instructions. Besides the highlighted attitudes of the SG tested, some critical aspects were found in the field of view (larger display size would help AR contents visualization), in the battery life (at least two extended power banks are suggested) and in the wearability (long time usage may result uncomfortable). Important opportunities will be provided by mixed reality (MR) devices (e.g., Microsoft Hololens2) equipped with Time of flight (ToF), eye and hand tracking sensors, spatial mapping and higher computational power, in order to accomplish complex tasks in the agricultural domain. However, MR devices, such as Hololens2 have higher costs, and outdoor activities may be compromised by direct sunlight. Anyway, future studies are needed to understand whether MR devices are suitable in agricultural use-case scenarios. Moreover, the intention to use of this kind of technology from the agricultural stakeholders should be assessed using proper evaluation models in future studies."
        },
        {
            "heading": "Data availability",
            "text": "The datasets generated during the current study are available from the corresponding author on reasonable request.\nReceived: 8 May 2022; Accepted: 9 September 2022"
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors thank Dr. Marco Polese for his valuable help during data collection and workflow development in augmented reality."
        },
        {
            "heading": "Author contributions",
            "text": "G.S.: Conceptualization, Methodology, Data curation, Formal analysis, Investigation, Writing\u2014original Draft, Writing\u2014Reviewing and Editing; G.T.: Conceptualization, Methodology, Formal analysis, Investigation,\n12\nVol:.(1234567890)\nScientific Reports | (2022) 12:15729 | https://doi.org/10.1038/s41598-022-20154-2\nWriting\u2014original Draft; M.C.: Conceptualization, Methodology, Formal analysis, Writing\u2014Reviewing and Editing, Supervision, Funding acquisition. All authors have read and agreed to the published version of the manuscript."
        },
        {
            "heading": "Funding",
            "text": "This work was funded by the ATLANTIDE project (Advanced Technologies for LANds management and Tools for Innovative Development of an EcoSustainable agriculture\u2014WP06), Progetti di Ricerca e Sviluppo della Regione Autonoma della Sardegna."
        },
        {
            "heading": "Competing interests",
            "text": "The authors declare no competing interests."
        },
        {
            "heading": "Additional information",
            "text": "Correspondence and requests for materials should be addressed to G.S.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or\nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n\u00a9 The Author(s) 2022"
        }
    ],
    "title": "Assessment of video see\u2010through smart glasses for augmented reality to support technicians during milking machine maintenance",
    "year": 2022
}