{
    "abstractText": "Nearest-Neighbor (NN) classification has been proven as a simple and effective approach for few-shot learning. The query data can be classified efficiently by finding the nearest support class based on features extracted by pretrained deep models. However, NN-based methods are sensitive to the data distribution and may produce false prediction if the samples in the support set happen to lie around the distribution boundary of different classes. To solve this issue, we present P3DC-Shot, an improved nearest-neighbor based few-shot classification method empowered by priordriven data calibration. Inspired by the distribution calibration technique which utilizes the distribution or statistics of the base classes to calibrate the data for few-shot tasks, we propose a novel discrete data calibration operation which is more suitable for NN-based few-shot classification. Specifically, we treat the prototypes representing each base class as priors and calibrate each support data based on its similarity to different base prototypes. Then, we perform NN classification using these discretely calibrated support data. Results from extensive experiments on various datasets show our efficient non-learning based method can outperform or at least comparable to SOTA methods which need additional learning steps.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shuangmei Wanga"
        },
        {
            "affiliations": [],
            "name": "Rui Maa"
        },
        {
            "affiliations": [],
            "name": "Tieru Wua"
        },
        {
            "affiliations": [],
            "name": "Yang Caoa"
        }
    ],
    "id": "SP:704701f818372ba01282aa0e7a46e9d3bdf06fe5",
    "references": [
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556 ",
            "year": 2014
        },
        {
            "authors": [
                "O. Russakovsky",
                "J. Deng",
                "H. Su",
                "J. Krause",
                "S. Satheesh",
                "S. Ma",
                "Z. Huang",
                "A. Karpathy",
                "A. Khosla"
            ],
            "title": "M",
            "venue": "Bernstein, et al., Imagenet large scale visual recognition challenge, Int. J. Comput. Vis. 115 ",
            "year": 2015
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "in: IEEE Conf. Comput. Vis. Pattern Recog.",
            "year": 2016
        },
        {
            "authors": [
                "R. Girshick"
            ],
            "title": "Fast r-cnn",
            "venue": "in: Int. Conf. Comput. Vis.",
            "year": 2015
        },
        {
            "authors": [
                "S. Ren",
                "K. He",
                "R. Girshick",
                "J. Sun"
            ],
            "title": "Faster r-cnn: Towards realtime object detection with region proposal networks",
            "venue": "Adv. Neural Inform. Process. Syst. 28 ",
            "year": 2015
        },
        {
            "authors": [
                "J. Redmon",
                "S. Divvala",
                "R. Girshick",
                "A. Farhadi"
            ],
            "title": "You only look once: Unified",
            "venue": "real-time object detection, in: IEEE Conf. Comput. Vis. Pattern Recog.",
            "year": 2016
        },
        {
            "authors": [
                "J. Long",
                "E. Shelhamer",
                "T. Darrell"
            ],
            "title": "Fully convolutional networks for semantic segmentation",
            "venue": "in: IEEE Conf. Comput. Vis. Pattern Recog.",
            "year": 2015
        },
        {
            "authors": [
                "K. He",
                "G. Gkioxari",
                "P. Doll\u00e1r",
                "R. Girshick"
            ],
            "title": "Mask r-cnn",
            "venue": "in: Int. Conf. Comput. Vis.",
            "year": 2017
        },
        {
            "authors": [
                "L.-C. Chen",
                "G. Papandreou",
                "I. Kokkinos",
                "K. Murphy",
                "A.L. Yuille"
            ],
            "title": "Deeplab: Semantic image segmentation with deep convolutional nets",
            "venue": "atrous convolution, and fully connected crfs, IEEE Trans. Pattern Anal. Mach. Intell. 40 ",
            "year": 2017
        },
        {
            "authors": [
                "T.-Y. Lin",
                "M. Maire",
                "S. Belongie",
                "J. Hays",
                "P. Perona",
                "D. Ramanan",
                "P. Doll\u00e1r",
                "C.L. Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "in: Eur. Conf. Comput. Vis.",
            "year": 2014
        },
        {
            "authors": [
                "M. Cordts",
                "M. Omran",
                "S. Ramos",
                "T. Rehfeld",
                "M. Enzweiler",
                "R. Benenson",
                "U. Franke",
                "S. Roth",
                "B. Schiele"
            ],
            "title": "The cityscapes dataset for semantic urban scene understanding",
            "venue": "in: IEEE Conf. Comput. Vis. Pattern Recog.",
            "year": 2016
        },
        {
            "authors": [
                "Y. Wang",
                "Q. Yao",
                "J.T. Kwok",
                "L.M. Ni"
            ],
            "title": "Generalizing from a few examples: A survey on few-shot learning",
            "venue": "ACM Comput Surv 53 ",
            "year": 2020
        },
        {
            "authors": [
                "J. Lu",
                "P. Gong",
                "J. Ye",
                "C. Zhang"
            ],
            "title": "Learning from very few samples: A survey",
            "venue": "arXiv preprint arXiv:2009.02653 ",
            "year": 2020
        },
        {
            "authors": [
                "G. Huang",
                "I. Laradji",
                "D. V\u00e1zquez",
                "S. Lacoste-Julien",
                "P. Rodriguez"
            ],
            "title": "A survey of self-supervised and few-shot object detection",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell. ",
            "year": 2022
        },
        {
            "authors": [
                "O. Vinyals",
                "C. Blundell",
                "T. Lillicrap"
            ],
            "title": "D",
            "venue": "Wierstra, et al., Matching networks for one shot learning, Adv. Neural Inform. Process. Syst. 29 ",
            "year": 2016
        },
        {
            "authors": [
                "J. Snell",
                "K. Swersky",
                "R. Zemel"
            ],
            "title": "Prototypical networks for fewshot learning",
            "venue": "Adv. Neural Inform. Process. Syst. 30 ",
            "year": 2017
        },
        {
            "authors": [
                "F. Sung",
                "Y. Yang",
                "L. Zhang",
                "T. Xiang",
                "P.H. Torr"
            ],
            "title": "T",
            "venue": "M. Hospedales, Learning to compare: Relation network for fewshot learning ",
            "year": 2018
        },
        {
            "authors": [
                "S. Ravi"
            ],
            "title": "H",
            "venue": "Larochelle, Optimization as a model for few-shot learning ",
            "year": 2016
        },
        {
            "authors": [
                "C. Finn",
                "P. Abbeel",
                "S. Levine"
            ],
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "venue": "in: Int. Conf. Mach. Learn., PMLR",
            "year": 2017
        },
        {
            "authors": [
                "M.A. Jamal",
                "G.-J. Qi"
            ],
            "title": "Task agnostic meta-learning for few-shot learning",
            "venue": "in: IEEE Conf. Comput. Vis. Pattern Recog.",
            "year": 2019
        },
        {
            "authors": [
                "A.A. Rusu",
                "D. Rao",
                "J. Sygnowski",
                "O. Vinyals",
                "R. Pascanu",
                "S. Osindero",
                "R. Hadsell"
            ],
            "title": "Meta-learning with latent embedding optimization",
            "venue": "in: Int. Conf. Learn. Represent.",
            "year": 2019
        },
        {
            "authors": [
                "K. Lee",
                "S. Maji",
                "A. Ravichandran",
                "S. Soatto"
            ],
            "title": "Meta-learning with differentiable convex optimization",
            "venue": "in: IEEE Conf. Comput. Vis. Pattern Recog.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Chen",
                "X. Wang",
                "Z. Liu",
                "H. Xu",
                "T. Darrell"
            ],
            "title": "A new metabaseline for few-shot learning",
            "venue": "arXiv preprint arXiv:2003.11539 ",
            "year": 2020
        },
        {
            "authors": [
                "T. Cao",
                "M.T. Law",
                "S. Fidler"
            ],
            "title": "A theoretical analysis of the number of shots in few-shot learning",
            "venue": "in: Int. Conf. Learn. Represent.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Wang",
                "W.-L. Chao",
                "K.Q. Weinberger"
            ],
            "title": "L",
            "venue": "van der Maaten, Simpleshot: Revisiting nearest-neighbor classification for fewshot learning, arXiv preprint arXiv:1911.04623 ",
            "year": 2019
        },
        {
            "authors": [
                "F. Aurenhammer"
            ],
            "title": "Voronoi diagrams\u2014a survey of a fundamental geometric data structure",
            "venue": "ACM Comput Surv 23 ",
            "year": 1991
        },
        {
            "authors": [
                "D.Z. Chen",
                "Z. Huang",
                "Y. Liu",
                "J. Xu"
            ],
            "title": "On clustering induced voronoi diagrams",
            "venue": "SIAM Journal on Computing 46 ",
            "year": 2017
        },
        {
            "authors": [
                "C. Ma",
                "Z. Huang",
                "M. Gao",
                "J. Xu"
            ],
            "title": "Few-shot learning as clusterinduced voronoi diagrams: A geometric approach",
            "venue": "in: Int. Conf. Learn. Represent.",
            "year": 2022
        },
        {
            "authors": [
                "P. Mangla",
                "N. Kumari",
                "A. Sinha",
                "M. Singh",
                "B. Krishnamurthy",
                "V.N. Balasubramanian"
            ],
            "title": "Charting the right manifold: Manifold mixup for few-shot learning",
            "venue": "in: WACV",
            "year": 2020
        },
        {
            "authors": [
                "Y. Tian",
                "Y. Wang",
                "D. Krishnan",
                "J.B. Tenenbaum",
                "P. Isola"
            ],
            "title": "Rethinking few-shot image classification: a good embedding is all you need",
            "venue": "in: Eur. Conf. Comput. Vis., Springer",
            "year": 2020
        },
        {
            "authors": [
                "W.-Y. Chen",
                "Y.-C. Liu",
                "Z. Kira",
                "Y.-C.F. Wang",
                "J.-B. Huang"
            ],
            "title": "A closer look at few-shot classification",
            "venue": "in: Int. Conf. Learn. Represent.",
            "year": 2019
        },
        {
            "authors": [
                "W. Ge",
                "Y. Yu"
            ],
            "title": "Borrowing treasures from the wealthy: Deep transfer learning through selective joint fine-tuning",
            "venue": "in: IEEE Conf. Comput. Vis. Pattern Recog.",
            "year": 2017
        },
        {
            "authors": [
                "O. Sbai",
                "C. Couprie",
                "M. Aubry"
            ],
            "title": "Impact of base dataset design on few-shot image classification",
            "venue": "Eur. Conf. Comput. Vis. ",
            "year": 2020
        },
        {
            "authors": [
                "L. Zhou",
                "P. Cui",
                "X. Jia",
                "S. Yang",
                "Q. Tian"
            ],
            "title": "Learning to select base classes for few-shot classification",
            "venue": "in: IEEE Conf. Comput. Vis. Pattern Recog.",
            "year": 2020
        },
        {
            "authors": [
                "W. Xue",
                "W. Wang"
            ],
            "title": "One-shot image classification by learning to restore prototypes",
            "venue": "in: AAAI, volume 34",
            "year": 2020
        },
        {
            "authors": [
                "J. Liu",
                "L. Song",
                "Y. Qin"
            ],
            "title": "Prototype rectification for few-shot learning",
            "venue": "in: Eur. Conf. Comput. Vis., Springer",
            "year": 2020
        },
        {
            "authors": [
                "Y. Guo",
                "R. Du",
                "X. Li",
                "J. Xie",
                "Z. Ma",
                "Y. Dong"
            ],
            "title": "Learning calibrated class centers for few-shot classification by pair-wise similarity",
            "venue": "IEEE Trans. Image Process. 31 ",
            "year": 2022
        },
        {
            "authors": [
                "S. Yang",
                "L. Liu",
                "M. Xu"
            ],
            "title": "Free lunch for few-shot learning: Distribution calibration",
            "venue": "in: Int. Conf. Learn. Represent.",
            "year": 2021
        },
        {
            "authors": [
                "M. Ren",
                "E. Triantafillou",
                "S. Ravi",
                "J. Snell",
                "K. Swersky",
                "J.B. Tenenbaum",
                "H. Larochelle",
                "R.S. Zemel"
            ],
            "title": "Meta-learning for semi-supervised few-shot classification",
            "venue": "in: Int. Conf. Learn. Represent.",
            "year": 2018
        },
        {
            "authors": [
                "C. Wah",
                "S. Branson",
                "P. Welinder",
                "P. Perona"
            ],
            "title": "S",
            "venue": "Belongie, The caltech-ucsd birds-200-2011 dataset ",
            "year": 2011
        },
        {
            "authors": [
                "T. Hospedales",
                "A. Antoniou",
                "P. Micaelli"
            ],
            "title": "A",
            "venue": "Storkey, Metalearning in neural networks: A survey 44 ",
            "year": 2021
        },
        {
            "authors": [
                "G. Koch",
                "R. Zemel"
            ],
            "title": "R",
            "venue": "Salakhutdinov, et al., Siamese neural networks for one-shot image recognition, in: ICML deep learning workshop",
            "year": 2015
        },
        {
            "authors": [
                "W. Xu",
                "Y. Xu",
                "H. Wang",
                "Z. Tu"
            ],
            "title": "Attentional constellation nets for few-shot learning",
            "venue": "in: Int. Conf. Learn. Represent.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Liu",
                "T. Zheng",
                "J. Song",
                "D. Cai",
                "X. He"
            ],
            "title": "Dmn4: Few-shot learning via discriminative mutual nearest neighbor neural network",
            "venue": "in: AAAI, volume 36",
            "year": 2022
        },
        {
            "authors": [
                "L. Torrey",
                "J. Shavlik"
            ],
            "title": "Transfer learning",
            "venue": "in: Handbook of research on machine learning applications and trends: algorithms, methods, and techniques, IGI global",
            "year": 2010
        },
        {
            "authors": [
                "C. Tan",
                "F. Sun",
                "T. Kong",
                "W. Zhang",
                "C. Yang",
                "C. Liu"
            ],
            "title": "A survey on deep transfer learning",
            "venue": "in: International conference on artificial neural networks, Springer",
            "year": 2018
        },
        {
            "authors": [
                "F. Zhuang",
                "Z. Qi",
                "K. Duan",
                "D. Xi",
                "Y. Zhu",
                "H. Zhu",
                "H. Xiong",
                "Q. He"
            ],
            "title": "A comprehensive survey on transfer learning",
            "venue": "Proceedings of the IEEE 109 ",
            "year": 2020
        },
        {
            "authors": [
                "G.S. Dhillon",
                "P. Chaudhari",
                "A. Ravichandran",
                "S. Soatto"
            ],
            "title": "A baseline for few-shot image classification",
            "venue": "in: Int. Conf. Learn. Represent.",
            "year": 2020
        },
        {
            "authors": [
                "W. Li",
                "L. Wang",
                "J. Xu",
                "J. Huo",
                "Y. Gao",
                "J. Luo"
            ],
            "title": "Revisiting local descriptor based image-to-class measure for few-shot learning",
            "venue": "in: IEEE Conf. Comput. Vis. Pattern Recog.",
            "year": 2019
        },
        {
            "authors": [
                "C. Ma",
                "Z. Huang",
                "M. Gao"
            ],
            "title": "J",
            "venue": "Xu, Few-shot learning as clusterinduced voronoi diagrams: A geometric approach ",
            "year": 2022
        },
        {
            "authors": [
                "F. Wu",
                "J.S. Smith",
                "W. Lu",
                "C. Pang",
                "B. Zhang"
            ],
            "title": "Attentive prototype few-shot learning with capsule network-based embedding",
            "venue": "in: Eur. Conf. Comput. Vis.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Ji",
                "X. Chai",
                "Y. Yu",
                "Z. Zhang"
            ],
            "title": "Reweighting and informationguidance networks for few-shot learning",
            "venue": "Neurocomputing 423 ",
            "year": 2021
        },
        {
            "authors": [
                "X. Wang",
                "J. Meng",
                "B. Wen",
                "F. Xue"
            ],
            "title": "Racp: A network with attention corrected prototype for few-shot speaker recognition using indefinite distance metric",
            "venue": "Neurocomputing 490 ",
            "year": 2022
        },
        {
            "authors": [
                "J. Xu",
                "X. Luo",
                "X. Pan",
                "W. Pei",
                "Y. Li",
                "Z. Xu"
            ],
            "title": "Alleviating the sample selection bias in few-shot learning by removing projection to the centroid",
            "venue": "in: Adv. Neural Inform. Process. Syst.",
            "year": 2022
        },
        {
            "authors": [
                "L. Zhou",
                "P. Cui",
                "S. Yang",
                "W. Zhu",
                "Q. Tian"
            ],
            "title": "Learning to learn image classifiers with visual analogy",
            "venue": "in: IEEE Conf. Comput. Vis. Pattern Recog.",
            "year": 2019
        },
        {
            "authors": [
                "J.W. Tukey"
            ],
            "title": "Exploratory data analysis, volume",
            "year": 1977
        },
        {
            "authors": [
                "B. Liu",
                "Y. Cao",
                "Y. Lin",
                "Q. Li",
                "Z. Zhang",
                "M. Long",
                "H. Hu"
            ],
            "title": "Negative margin matters: Understanding margin in few-shot classification",
            "venue": "in: Eur. Conf. Comput. Vis., Springer",
            "year": 2020
        },
        {
            "authors": [
                "J. Deng",
                "W. Dong",
                "R. Socher",
                "L.-J. Li",
                "K. Li",
                "L. Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "in: IEEE Conf. Comput. Vis. Pattern Recog., Ieee",
            "year": 2009
        },
        {
            "authors": [
                "S. Zagoruyko",
                "N. Komodakis"
            ],
            "title": "Wide residual networks",
            "venue": "arXiv preprint arXiv:1605.07146 ",
            "year": 2016
        },
        {
            "authors": [
                "L. van der Maaten",
                "G.E. Hinton"
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of Machine Learning Research",
            "year": 2008
        }
    ],
    "sections": [
        {
            "text": "Nearest-Neighbor (NN) classification has been proven as a simple and effective approach for few-shot learning. The query data can be classified efficiently by finding the nearest support class based on features extracted by pretrained deep models. However, NN-based methods are sensitive to the data distribution and may produce false prediction if the samples in the support set happen to lie around the distribution boundary of different classes. To solve this issue, we present P3DC-Shot, an improved nearest-neighbor based few-shot classification method empowered by priordriven data calibration. Inspired by the distribution calibration technique which utilizes the distribution or statistics of the base classes to calibrate the data for few-shot tasks, we propose a novel discrete data calibration operation which is more suitable for NN-based few-shot classification. Specifically, we treat the prototypes representing each base class as priors and calibrate each support data based on its similarity to different base prototypes. Then, we perform NN classification using these discretely calibrated support data. Results from extensive experiments on various datasets show our efficient non-learning based method can outperform or at least comparable to SOTA methods which need additional learning steps.\nKeywords: Few-Shot Learning, Image Classification, Prototype, Calibration"
        },
        {
            "heading": "1. Introduction",
            "text": "Deep learning has triggered significant breakthroughs in many computer vision tasks, such as image classification [1, 2, 3], object detection [4, 5, 6], and semantic segmentation [7, 8, 9] etc. One key factor for the success of deep learning is the emergence of large-scale datasets, e.g., ImageNet [2], MSCOCO [10], Cityscapes [11], just to name a few. However, it is difficult and expensive to collect and annotate sufficient data samples to train a deep model with numerous weights. The data limitation has become a main bottleneck for more broader application of deep leaning, especially for the tasks involving rarely seen samples. On the other hand, human can learn to recognize novel visual concepts from only a few samples. There is still a notable gap\n\u2022 This work is supported in part by the National Key Research and Development Program of China (Grant No. 2020YFA0714103) and the National Natural Science Foundation of China (Grant No. 61872162 and 62202199). \u2217Co-first authors. \u2217\u2217Corresponding authors.\nbetween human intelligence and the deep learning based artificial intelligence. Few-shot learning (FSL) aims to learn neural models for novel classes with only a few samples. Due to its ability for generalization, FSL has attracted extensive interests in recent years [12, 13, 14].\nFew-shot classification is the most widely studied FSL task which attempts to recognize new classes or classify data in an unseen query set. Usually, few-shot classification is formulated in a meta-learning framework [15, 16, 17, 18, 19, 20, 21, 22, 23]. In the meta-training stage, the N-way K-shot episodic training paradigm is often employed to learn generalizable classifiers or feature extractors for data of the base classes. Then, in the meta-testing stage, the meta-learned classifiers can quickly adapt to a few annotated but unseen data in a support set and attain the ability to classify the novel query data. Although meta-learning has shown the effectiveness for few-shot classification, it is unclear how to set the optimal class number (N) and per-class sample number (K) when learning the classifiers. Also, the learned classifier may not perform well when the sample number K used in meta-testing does not match\nPreprint submitted to Elsevier January 3, 2023\nar X\niv :2\n30 1.\n00 74\n0v 1\n[ cs\n.C V\n] 2\nJ an\n2 02\n3\nthe one used in the meta-training [24]. On the other hand, nearest-neighbor (NN) based classification has been proven as a simple and effective approach for FSL. Based on features obtained from the meta-learned feature extractor [15, 16] or the pretrained deep image models [25], the query data can be efficiently classified by finding the nearest support class. Specifically, the prediction is determined by measuring the similarity or distance between the query feature and the prototypes (i.e., average or centroid) of the support features. From the geometric view, NN-based classification can be solved using a Voronoi Diagram (VD) which is a partition of the space formed by the support features [26, 27]. Given a query feature, its class can be predicted by computing the closest Voronoi cell that corresponds to a certain support class. With proper VD construction and feature distance metrics, the state-ofthe-art performance can be achieved for few-shot classification [28]. However, due to the limited number of support samples, NN-based few-shot classification is sensitive to the distribution of the sampled data and may produce false prediction if the samples in the support set happen to lie around the distribution boundary of different classes (see Figure 1 left).\nTo solve above issues, various efforts have been paid to more effectively utilize the knowledge or priors from the base classes for few-shot classification. One natural way is to learn pretrained classifiers or image encoders with the abundant labeled samples of base classes and then adapt them the novel classes via transfer learning [29, 30, 31, 23]. Meanwhile, it has been shown that variations in selecting the base classes can lead to different performance on the novel classes [32, 33, 34] and how to select the base classes for better feature representation learning still needs more investigation. On the other hand, a series of works [35, 36, 37, 38] perform data calibration to the novel classes so that the results are less affected by the limited number of support samples. One representative is Distribution Calibration (DC) [38] which assumes the features of the data follow the Gaussian distribution and transfers the statistics from the similar base classes to the novel classes. Then, DC trains a simple logistic regression classifier to classify the query features using features sampled from the calibrated distributions of the novel classes. Although DC has achieved superior performance than previous meta-learning [19, 21, 22] or transfer-learning [29, 30, 31, 23] based methods, it relies on the strong assumption for Gaussian-like data distribution and it cannot be directly used for NN-based few-shot classification.\nIn this paper, we propose P3DC-Shot, an improved\nNN-based few-shot classification method that employs prior information from base classes to discretely calibrate or adjust the support samples so that the calibrated data is more representative for the underlying data distribution (Figure 1 right). Our main insight is even the novel classes have not been seen before, they still share similar features to some base classes, and the prior information from the base classes can serve as the context data for the novel classes. When only a few support samples are available for the novel classes, performing prior-driven calibration can alleviate the possible bias introduced by the few-shot support samples. With the calibrated support samples, the query data can be more accurately classified by a NN-based classifier.\nSpecifically, for the prior information, we compute the prototype, i.e., the average of features, for each base class. Then, we propose three different schemes for selecting the similar prototypes to calibrate the support data. Firstly, we propose the sample-level calibration which selects the top M most similar base prototypes for each support sample and then apply weighted averaging between each support sample and selected prototypes to obtain the calibrated support sample. Secondly, to utilize more context from the base classes, we propose the task-level calibration which combines the most similar base prototypes for each support sample into a union and performs the calibration for the support samples using each prototype in the union. In addition, we propose a unified calibration scheme that combines the two above schemes so that the calibration can exploit different levels of prior information from the base classes. To utilize the calibrated support samples for the NNbased classification, we further obtain the prototypes of\nthe support class using an attention-weighted averaging, while the attention weights are computed between the query sample and each calibrated support sample. Finally, the classification of a query sample is simply determined by finding its nearest support prototype measured by the cosine similarity.\nComparing to DC, our P3DC-Shot adopts the similar idea of transferring the information or statistics from the base classes to the novel classes. The key difference is our data calibration is performed on each individual support sample rather than the distribution parameters and we employ the NN-based classification instead of the learned classifier as in DC. Comparing to other NN-based few-shot classification methods such as SimpleShot [25], since our support data is calibrated, the NN classification is less affected by the sampling bias for the support data, e.g, the calibrated data is more likely to be close to the center of the corresponding novel class. We conduct extensive comparisons with recent state-of-the-art few-shot classificaiton methods on miniImageNet [2], tiredImageNet [39] and CUB [40] and the results demonstrate the superiority and generalizability of our P3DC-Shot. Ablation studies on different calibration schemes, i.e., different weights between the sample-level and task-level calibration also show the necessity of combining two schemes for better results.\nIn summary, our contributions are as follows: 1. We propose P3DC-Shot, a prior-driven dis-\ncrete data calibration strategy for nearest-neighbor based few-shot classification to enhance the model\u2019s robustness to the distribution of the support samples. 2. Without additional training and expensive computation, the proposed method can efficiently calibrate each support sample using information from the prototypes of the similar base classes. 3. We conduct extensive evaluations on three discrete calibration schemes on various datasets and the results show our efficient non-learning based method can outperform or at least comparable to SOTA few-shot classification methods."
        },
        {
            "heading": "2. Related Work",
            "text": "In this section, we first review the representative meta-learning and transfer learning based few-shot classification techniques. Then, we summarize the nearestneighbor and data calibration based approaches which are most relevant to our P3DC-Shot.\nMeta-learning based few-shot classification. Metalearning [41] has been widely adopted for few-shot classification. The core idea is to leverage the episodic\ntraining paradigm to learn generalizable classifiers or feature extractors using the data from the base classes in an optimization-based framework [18, 19, 20, 21, 22], as well as learn a distance function to measure the similarity between the support and query samples through metric-learning [42, 15, 17, 43, 44, 37]. For example, MAML [19] is one of the most representative optimization-based meta-learning method for fewshot classification and its goal is to learn good network initialization parameters so that the model can quickly adapt to new tasks with only a small amount of new training data from the novel classes. For metriclearning based methods such as the Matching Networks [15], Prototypical Networks [16] and Relation Networks [17], the network is trained to either learn an embedding function with a given distance function or learn both the embedding and the distance function in a meta-learning architecture. Unlike the optimization and metric-learning based methods which require sophisticated meta-learning steps, our method can directly utilize the features extracted by the pretrained models and perform the prior-driven calibration to obtain lessbiased support features for classification.\nTransfer learning based few-shot classification. Transfer learning [45, 46, 47] is a classic machine learning or deep learning technique that aims to improve the the learning of a new task through the transfer of knowledge from one or more related tasks that have already been learned. Pretraining a deep network on the base dataset and transferring knowledge to the novel classes via fine-tuning [31, 48, 30] has been shown as the strong baseline for the few-shot classification. To learn better feature representations which can lead to improved few-shot fine-tuning performance, Mangla et al. [29] propose S2M2, the Self-Supervised Manifold Mixup, to apply regularization over the feature manifold enriched via the self-supervised tasks. In addition to training new linear classifiers based on the pretrained weights learned from the base classes, Meta-Baseline [23] performs meta-learning to further optimize the pretrained weights for few-shot classification. On the other hand, it has been shown the results of the transfer learning based methods depend on different selections of the base classes for pretraining [32, 33], while how to select the base classes to achieve better performance is still challenging [34]. In comparison, our P3DC-shot does not need the additional cost for feature representation learning and can more effectively utilize the base classes in a NN-based classification framework.\nNearest neighbor based few-shot classification. NN-based classification has also been investigated for few-shot classification. The main idea is to compute the\nprototypes of the support samples, i.e., the mean or centroid of the support features, and classify the query sample using metrics such as L2 distance, cosine similarity or a learned distance function. In SimpleShot [25], it shows nearest neighbor classification with features simply normalized by L2 norm and measured by Euclidean distance can achieve competitive few-shot classification results. Instead of performing nearest neighbor classification on the image-level features, Li et al. [49] introduces a Deep Nearest Neighbor Neural Network which performs nearest neighbor search over the deep local descriptors and defines an image-to-class measure for few-shot classification. From a geometric view, Ma et al. [50] utilize the Cluster-induced Voronoi Diagram (CIVD) to incorporate cluster-to-point and cluster-tocluster relationships to the nearest neighbor based classification. Similar to above methods, our method is based on the nearest prototype classification, while we perform the prior-driven data calibration to obtain less-biased support data for the prototype computation. Meanwhile, computing the attentive or reweighted prototypes [51, 52, 53] that are guided by the base classes or query samples has also been investigated recently. We follow the similar idea and compute the attentionweighted prototypes for NN-based classification.\nData calibration for few-shot classification. Due to the limited number of samples, the prototypes or centroids computed from the few-shot support data may be biased and cannot represent the underlying data distribution. Simply performing NN-based classification on these biased prototypes will lead to inaccurate classification. Several methods have been proposed to calibrate or rectify the data to obtain better samples or prototypes of the support class [35, 36, 37, 54, 38]. Using the images in the base classes, RestoreNet [35] learns a class agnostic transformation on the feature of each image to move it closer to the class center in the feature space. To reduce the bias caused by the scarcity of the support data, Liu et al., [36] employ the pseudolabeling to add unlabelled samples with high prediction confidence into the support set for prototype rectification. In [37], Guo et al. propose a Pair-wise Similarity Module to generate calibrated class centers that are adapted to the query sample. Instead of calibrating individual support samples, Distribution Calibration (DC) [38] aims to calibrate the underlying distribution of the support classes by transferring the Gaussian statistics from the base classes. With sufficient new support data sampled from the calibrated distribution, an additional classifier is trained in [38] to classify the query sample. In contrast to these methods, we do not require additional training or assumption of the underlying dis-\ntribution. Instead, we directly use the prototypes of the base classes to calibrate each support sample individually and we adopt the NN-based classification which makes the whole pipeline discrete and efficient. One recent work that is similar to ours is Xu et al. [54] which proposes the Task Centroid Projection Removing (TCPR) module and transforms all support and query features in a given task to alleviate the sample selection bias problem. Comparing to [54], we only calibrate the support samples using the priors from the base classes and keep the query samples unchanged."
        },
        {
            "heading": "3. Method",
            "text": "To effectively utilize the prior knowledge from the base classes, we first propose two independent calibration strategies, i.e., sample-level calibration and tasklevel calibration, which exploit different levels of information from the base classes. Then, we combine the sample-level and task-level calibration together to obtain the final calibrated support samples which will be used for the nearest neighbor classification.\nFigure 2 shows an illustration of the P3DC-Shot pipeline. Given a pretrained feature extractor F and a set of prototypes of base classes, we perform the priordriven discrete calibration to the normalized features of the support data. Initially, the query sample in green is closer to the support sample in yellow. After the proposed calibration using the related base class prototypes, the query sample becomes closer to the calibrated support sample in blue. In the following, we provide technical details of the P3DC-Shot for few-shot classification."
        },
        {
            "heading": "3.1. Problem Statement",
            "text": "In this paper, we focus on the few-shot image classification which aims to classify the new image samples from the novel classes with just a few labeled image samples. Normally, the new data sample is called a query sample and the labelled samples are called support samples. With the aid of a set of base classes represented by their prototypes Pb = {pbi } nb i=1, our goal is to calibrate the support samples from novel-class so that they can be better matched with the query samples by a nearest neighbor classifier. Here, all data samples are represented by the features computed from a pretrained feature extractor F(\u00b7) : X \u2192 Rd, while X is the domain of the image space and d is the dimension of the feature space; pbi is the prototype of a base class, which is computed as the average feature of the samples within the\nclass; nb is the number of all base classes. For simplicity, we directly use xi to represent the feature F(xi) of an image xi.\nWe follow the conventional few-shot learning setting, i.e., build a series of N-way K-shot tasks where N is the number of novel classes and K is the number of support samples in each task. Formally, each task consists of a support set S = {(xi, yi)}N\u00d7Ki=1 and a query set Q = {qi}N\u00d7K+N\u00d7Qi=N\u00d7K+1 . Here, yi is the label of the corresponding sample, which is known for the support set and unknown for the query set; Q is the number of query sample for each novel class in the current task. Given a support feature xi, we perform our prior-driven calibration to obtain the calibrated support feature xci = C(xi), where C(\u00b7) : Rd \u2192 Rd conducts feature transformation based on the information from the base classes. Then, we predict the label of a query feature by performing nearest neighbor classification w.r.t the novel class prototypes computed from the calibrated support feature(s)."
        },
        {
            "heading": "3.2. Prior-Driven Discrete Data Calibration",
            "text": "Before we perform calibration to the support data, we first apply L2 normalization to the support and query features. It is shown in SimpleShot [25] that using L2-normalized feature with a NN-based classifier can lead to competitive results for few-shot classification. Hence, we obtain x\u0304i for a support feature xi by:\nx\u0304i = normalize(xi) = xi \u2016xi\u20162 . (1)\nSimilarly, the normalization of the query features are also computed: q\u0304i = normalize(qi). By working with the normalized features, we can obviate the absolute scales of the features and focus on the similarities and differences on their directions. Note that, the normalized features are used in the feature combination step\n(Eq. 7, 10 and 11) for obtaining the interpolation between the normalized features and in the NN-based classification step (Eq. 12) for performance improvement.\nNext, we propose the sample-level and task-level calibration, and their combination to utilize the priors from the base classes for obtaining the less-biased support features."
        },
        {
            "heading": "3.2.1. Sample-Level Calibration",
            "text": "According to previous works [55, 38] which also use the information from base classes for classifying the new classes, the base classes with higher similarities to the query classes are more important than other base classes. Hence, we first propose to perform calibration based on the top similar base classes for each support sample. Moreover, following DC [38], we apply the Tukeys\u2019s Ladder of Powers transformation [56] to the features of the support samples before the calibration:\nx\u0303i = {\nx\u03bbi if \u03bb , 0 log(xi) if \u03bb = 0\n(2)\nHere, \u03bb is a hyperparameter which controls the distribution of the transformed feature, with a smaller \u03bb can lead to a less skewed feature distribution. We set \u03bb = 0.5 and obtain the transformed support feature x\u0303i from the original feature xi.\nThen, we select the top M base classes with higher similarities to a transformed support feature x\u0303i:\n\u039bMi = {pbj | j \u2208 topM(Si)}, (3) where Si = {< x\u0303i, pbj > | j \u2208 {1, . . . nb}}. (4)\nHere, \u039bMi stores the M nearest base prototypes with respect to a transformed support feature vector x\u0303i; topM(\u00b7) is an operator that returns the index of top M elements from Si, the similarity set of x\u0303i, while the similarity between x\u0303i and a base prototype pbj is computed by the\ninner product < \u00b7, \u00b7 >. In DC [38], the distributions of the base and novel classes are assumed as Gaussian distribution and the statistics (mean and co-variance) of the base classes are used to calibrate the distribution of the novel classes. In contrast, we directly use the similar base prototypes to calibrate each support feature. Specifically, the calibration for x\u0303i driven by base prototypes pbj \u2208 \u039bMi is computed as:\nsi = x\u0303i + \u2211 j\u2208\u039bMi wi j pbj , (5)\nwhere the weights of the M nearest base classes prototypes in \u039bMi are obtained by applying Softmax to the similarities between x\u0303i and these prototypes:\nwi j = e<x\u0303i,p b j>\u2211\nk\u2208\u039bMi e <x\u0303i,pbk>\n, j \u2208 \u039bMi . (6)\nIt should be noted that, in Eq. 5, the support feature x\u0303i is a transformed feature, while the base prototypes are in the original feature space. This setting is the same as DC does for calibrating the distribution of the novel classes and it can be understood as follows: 1) the transformation can initially reduce the skewness of the fewshot-sampled support features; 2) the term wi j pbj can be regarded as the projection of x\u0303i w.r.t prototype pbj ; 3) x\u0303i is calibrated based on its projects to all of its similar base prototypes in \u039bMi .\nFinally, the sample-level calibration for a normalized support sample x\u0304i is defined as:\nx\u0304si = normalize((1 \u2212 \u03b1)x\u0304i + \u03b1s\u0304i), (7)\nwhere \u03b1 \u2208 [0, 1] is a parameter to linearly combine the normalized support feature x\u0304i and normalized baseprototypes-driven calibration s\u0304i = norm(si). As shown in Figure 2, x\u0304i and s\u0304i form a line in the normalized feature space and x\u0304si is the normalization of a in-between point on this line. In general, the sample-level calibration can rectify each support sample based on its own top M most similar base classes."
        },
        {
            "heading": "3.2.2. Task-Level Calibration",
            "text": "By performing the sample-level calibration, the bias induced by the few-shot support samples can be reduced to a certain degree. However, when the sampling bias is too large, e.g., the support sample is lying near the boundary of a class, the set of similar base classes \u039bMi obtained by Eq. 3 may also be biased. To alleviate such bias, we propose the task-level calibration which utilizes the base prototypes related to all support samples when\ncalibrating each individual support feature. Concretely, for a support set S = {(xi, yi)}N\u00d7Ki=1 w.r.t a task T , we collect the top M similar base prototypes for each support sample and form a union of related base prototypes for T :\n\u039bT = N\u00d7K\u22c3 i=1 \u039bMi . (8)\nThen, for a transformed support sample x\u0303i obtained by Eq. 2, the calibration using all of the task-related base prototypes is computed by:\nti = x\u0303i + \u2211 j\u2208\u039bT wi j pbj , (9)\nwhere wi j is calculated in the similar way as Eq. 6, but the similarities are computed using the prototypes from \u039bT instead of \u039bMi . By involving more prototypes to calibrate the support samples, the bias caused by only using nearby prototypes for a near-boundary support sample can be reduced.\nThen, we define the task-level calibration for a normalized support sample x\u0304i as:\nx\u0304ti = normalize((1 \u2212 \u03b2)x\u0304i + \u03b2t\u0304i), (10)\nwhere t\u0304i is the normalization of ti. Similar to the samplelevel calibration, x\u0304i and t\u0304i also form a line in the normalized feature space, while the calibration for each support sample is based on the union of all related base prototypes \u039bT ."
        },
        {
            "heading": "3.2.3. Unified Model",
            "text": "The sample-level and task-level calibration utilize different levels of information from the base classes to rectify the support samples in a discrete manner. To further attain the merits of both calibration schemes, we propose a unified model which linearly combines the sample-level and task-level calibration:\nxci = x\u0304 u i = normalize((1 \u2212 \u03b1 \u2212 \u03b2)x\u0304i + \u03b1s\u0304i + \u03b2t\u0304i). (11)\nHere, x\u0304ui which is also denoted as x c i , is the final calibration for a normalized support sample x\u0304i . Geometrically, xci can be understood as the normalization of an interpolated feature point xui locating in the triangle formulated by the three vertices x\u0304i, s\u0304i and t\u0304i, while 1 \u2212 \u03b1 \u2212 \u03b2, \u03b1 and \u03b2 are the barycentric coordinates of xui . Different \u03b1 and \u03b2 values can lead to different calibration effects. When \u03b2 = 0, the unified model degenerates to the samplelevel calibration, while when \u03b1 = 0, the model becomes to the task-level calibration. We quantitatively evaluate the effects of different \u03b1 and \u03b2 values in Section 4.4."
        },
        {
            "heading": "3.3. Nearest Prototype Classifier",
            "text": "With the calibrated support set Sc = {(xci , yi)}N\u00d7Ki=1 , we compute the prototypes {pn}Nn=1 for the novel classes and perform cosine similarity based nearest classification for a query feature q. To simplify the notation, we further represent Sc = {Scn}Nn=1, while Scn = {(xck, yk = n)}Kk=1 is the support set for a novel class CLS n.\nFor the 1-shot case, each calibrated support sample becomes one prototype and the class of the query feature is predicted by the nearest prototype classifier:\ny\u2217 = max pn cos(q\u0304, pn), (12)\nwhere pn = xcn is the calibrated prototype for novel class CLS n and q\u0304 is the normalization of query q.\nFor the multi-shot case, one way to obtain the prototype for a novel class is simply to compute the average of all support features for the given class as in Prototypical Networks [16]. However, merely using the unweighted average of the support features as prototype does not consider the importance of the support samples w.r.t the query. Therefore, we adopt the idea of attentive prototype which is proposed in recent works [51, 53] for query-guided prototype computation. In our implementation, we define the attention-weighted prototype as:\npqn = \u2211 xck\u2208Scn ak xck, (13)\nwhere ak = e<q,x c k>\u2211\nxcm\u2208Scn e <q,xcm>\n. (14)\nHere, xck and x c m are the calibrated support samples belonging to the CLS n\u2019s support set Scn and ak is the attention weight computed by applying Softmax to the similarities between query q and these calibrated support samples; pqn is the CLS n\u2019s prototype guided by query q. Similar to Eq. 12, the prediction for a query q is obtained by finding the novel class with the nearest prototype pqn."
        },
        {
            "heading": "4. Experiments",
            "text": "In this section, we perform quantitative comparisons between our P3DC-Shot and state-of-the-art few-shot classification methods on three representative datasets. We also conduct ablation studies on evaluating different hyperparameters and design choices for our methods. Our code is available at: https://github.com/breakaway7/P3DC-Shot."
        },
        {
            "heading": "4.1. Datasets",
            "text": "We evaluate our prior-driven data calibration strategies on three popular datasets for benchmarking few shot classificaiton: miniImageNet [2], tieredImageNet [39] and CUB [40]. miniImageNet and tieredImageNet contain a broad range of classes including various animals and objects, while CUB is a more fine-grained dataset that focuses on various species of birds.\nSpecifically, the miniImageNet [2] is derived from the ILSVRC-2012 [58] and it contains a subset of 100 classes, each of which consisting of 600 images. We follow the split used in [18] and obtain 64 base, 16 validation and 20 novel classes for miniImageNet. Comaring to miniImageNet, the tieredImageNet [39] is a larger subset of [58] which contains 608 classes and therefore more challenging. We follow [39] and split the tieredImageNet into 351, 97, and 160 classes for base, validation, and novel classes, respectively. For CUB [40], it is the short name for Caltech-UCSD Birds 200 dataset, which contains a total of 11,788 images covering 200 categories of different bird species. We split the CUB dataset into 100 base, 50 validation and 50 novel classes following [31]. Note that the set formed by the base classes can also be regarded as the train set and the novel classes correspond to the test set."
        },
        {
            "heading": "4.2. Implementation Details",
            "text": "For each image in the dataset, we represent it as a 640-dimensional feature vector which is extracted using the WideResNet [59] pretrained by the S2M2 [29] work. Our calibration pipeline can efficiently proceed in four steps: 1) find the M = 5 nearby base prototypes for each support sample xi; 2) compute the endpoint of the sample-level calibration for xi, i.e., si; 3) collect all nearby base prototypes for all support samples in the task and compute the endpoint of the task-level calibration for xi, i.e., ti; 4) combine the sample-level and task-level calibration and obtain the final calibrated support sample xci . The parameter \u03b1 and \u03b2 for weighting the sample-level and task-level calibration are selected based on the best results obtained on the validation set for each dataset. All experiments are conducted on a PC with a 2.70GHz CPU and 16G memory. No GPU is needed during the calibration. On average, for a 5- way 5-shot task, it takes 0.027 seconds to calibrate the support samples and 0.002 seconds for performing the nearest prototype classification."
        },
        {
            "heading": "4.3. Comparison and Evaluation",
            "text": "To evaluate the performance of our P3DC-Shot, we first conduct quantitative comparisons with some representative and state-of-the-art few-short classification\nmethods. Then, we compare with different data transformation or calibration schemes and provide qualitative visualization for showing the difference of our calibration results w.r.t existing works. In addition, we evaluate the generalizability of our method by performing classification tasks with different difficulties.\nQuantitative comparisons. As there are numerous efforts have been paid to the few-shot classification, we mainly compare our P3DC-Shot with representative and SOTA works which cover different types of fewshot learning schemes. The compared methods include the metric-learning based meta-learning [15, 16, 17], optimization-based meta-learning [19, 21, 22], transfer learning [31, 57, 29], nearest neighbor [25, 50] and calibration [35, 38, 37, 54] based methods. For certain methods such as [29, 28], we only compare with their basic versions and do not consider their model trained with data augmentation. Note that as not every method has conducted experiments on all three datasets, we\nmainly compare with their reported results. One exception is for [54], we compare with its results generated using its released code.\nFor our method, we report the results of our model with different hyperparameters \u03b1 and \u03b2. In particular, we consider the case when \u03b1 and \u03b2 are both zero, which makes our method a simple NN-based method with no data calibration and only shows the effect for using the query-guided prototype computation (Eq. 13). We also compare with the results of \u03b1 or \u03b2 is 1, or both of them are equal to 13 , which correspond to the cases that the endpoint of the sample-level or task-level calibration or the barycenter of the calibration triangle (Figure 2). In the end, we provide our best results with the \u03b1 or \u03b2 selected based on the validation set.\nFor each dataset, we evaluate on the 5-way 1-shot and 5-way 5-shot classification setting. For each setting, 2,000 testing tasks, each of which contains 5 \u00d7 K (K = 1 or 5) samples for the support set and 5 \u00d7 15\nsamples for the query set, are randomly generated from the test split of the corresponding dataset. Table 1 shows the quantitative comparison results on three datasets. It can be seen that our best results outperform most methods in the 5-way 1-shot setting and are comparable to the SOTA methods [28, 38] for the 5-way 5-shot setting. Note that although [37] achieves best results on the CUB dataset, it is inferior on miniImageNet and tieredImageNet. Moreover, since [37] follows a metric-based few-shot learning pipeline, it still requires to train the feature extractor and the metric module for each dataset. For [28], it performs generally well on all three datasets, but as an ensemble-based method, its computation time is much longer than our method, especially when the ensemble number is large. In contrast, our method does not require any training and only needs to perform an efficient calibration step for each testing task.\nAlso, from results of our method with different \u03b1 and \u03b2 values in Table 1, it can be found when \u03b1 and \u03b2 is zero, the query-guided prototype computation can lead to better performance than the simple NN-based SimpleShot [25]. When either the sample-level or task-level calibration is applied, i.e., \u03b1 or \u03b2 is not zero, the results are better than the non-calibrated version, showing the calibration can indeed reduce the bias for the support samples. Meanwhile, which calibration type is more suitable is depending on the underlying data distribution of the dataset. By selecting the \u03b1 and \u03b2 based on the validation set of each dataset, the results are further improved. In the ablation study, we perform more experiments and analysis of different \u03b1 and \u03b2 values.\nComparison with different data transformation or calibration schemes. To further verify the effectiveness\nof our prior-driven data calibration, we compare with several NN-based baseline methods which perform different data transformation or calibration schemes and the results are shown in Table 2. In this experiment, all methods are based on the pretrained WideResNet features. Also, only the 5-way 1-shot classification accuracy is measured so that the comparison is focused on feature transformation instead of the prototype computation schemes. The first baseline is NN, which is a naive inner product based nearest neighbor classifier. Then, L2N and CL2N represent L2 normalization and centered L2 normalization which have been shown as effective in SimpleShot [25]. In addition, another baseline that follows the data calibration scheme in DC [38] is compared. Comparing to the original DC, this baseline directly takes the calibrated and then normalized features and employs NN for classification instead of training new classifiers using the sampled data. From Table 2, it can be observed the data normalization or calibration can significantly improve the NN-based classi-\nfication. In addition, our data calibration achieves the best results comparing to other baselines. The main reason is the L2N and CL2N only perform transformation rather than calibration using the base priors, while the modified DC does not consider the attentive similarity between the support samples and the base classes when performing the calibration.\nVisualization of the calibration. To qualitatively verify the effectiveness of our calibration, we show the T-SNE [60] visualization of the calibration results for some example support samples in Figure 3. The results of calibrating the same sample using DC [38] are also compared. It can be seen from Figure 3 that our calibration can more effectively transform the support samples closer to the center of the underlying classes. For DC, the calibration may be minor or even be far away from the center. The reason is still due to it treats the nearby base classes with the same weights. In contrast, our calibration pays more attention to the similar base classes when determining the weights for combining the base prototypes (Eq. 5 and 9).\nGeneralizability test on different N in N-way classification. Following [35], we conduct a series of Nway 1-shot experiments on miniImageNet to test the generalizability of the proposed calibration for different classification tasks. Table 3 shows the results of the baseline methods [35], L2N and CL2N and ours. Note that with the N increases, there are more data samples in a test task and the classification becomes more difficult. It can be observed that our P3DC-Shot achieves consistent best results comparing to the baseline methods, verifying our method is generalizable to classification tasks with different difficulties."
        },
        {
            "heading": "4.4. Ablation Study",
            "text": "In this section, we perform ablation studies to verify the effectiveness of different modules and design choices of our method. First, we conduct experiments on different hyperparameter \u03b1 and \u03b2 to see how the sample-level and task-level calibration can affect the final results. Then, we perform the study on the effec-\ntiveness of using the query-guided attentive prototypes in the NN classification step.\nEffect on different hyperparameter \u03b1, \u03b2. Different \u03b1 and \u03b2 values correspond to different degrees of sample-level and task-level calibration applied to the input data. Geometrically, \u03b1, \u03b2 and 1 \u2212 \u03b1 \u2212 \u03b2 can also be understood as the coordinates of the calibration result w.r.t to the triangle formed by the three points x\u0304i, si, ti. To quantitatively reveal how these two hyperparameters can affect the results, we enumerate different \u03b1 and \u03b2 values on both the validation and test sets of different datasets. From the results in Figure 4, it can be found the accuracy near the origin of the figures are smaller, which means performing calibration can improve upon using the original features for classification, i.e., \u03b1 and \u03b2 is zero. Also, different datasets prefer different \u03b1 and \u03b2 combinations for achieving higher performance. For example, miniImageNet shows better results when \u03b1+\u03b2 is around 0.9 and CUB prefers a relatively smaller calibration, i.e., \u03b1 + \u03b2 is around 0.6. For tieredImageNet, better results are obtained around the topper left of the figure, showing the task-level calibration is more helpful than the sample-level. Overall, the trend on the test set is consistent with the validation set. From above experiments, it shows the sample-level and task-level calibration are consistently effective, while how to selecting the good \u03b1 and \u03b2 values are dataset dependent. Therefore, for our best results, we use the \u03b1 and \u03b2 selected based on the validation set and report their performance on the test set.\nEffect on using attentive prototypes in NN classification. To improve the conventional prototype based NN classificaiton, we propose to compute the queryguided attentive prototypes to represent the support class. To verify the effectiveness of this scheme, we perform ablation study for 5-way 5-shot tasks on different tasks using different prototype computation schemes. Specifically, we take the calibrated support features and compute the prototypes for the support classes by performing the conventional average operation or our query-guided attentive averaging (Eq. 13). The results\nin Table 4 show that the attentive prototypes can lead to better performance. Hence, we adopt the attentive prototypes in our NN-based classification."
        },
        {
            "heading": "5. Conclusion",
            "text": "In this paper, we propose a simple yet effective framework, named P3DC-Shot, for few-shot classification. Without any retraining and expensive computation, our prior-driven discrete data calibration method can efficiently calibrate the support samples based on priorinformation from the base classes to obtain the lessbiased support data for NN-based classification. Extensive experiments show that our method can outperform\nor at least comparable to SOTA methods which need additional learning steps or more computation. One limitation of our method is we rely on the whole validation set to select the good hyperparameters \u03b1 and \u03b2 to determine which degree of the sample-level and tasklevel calibration is more suitable for the given dataset. Investigating a more general scheme to combine the sample-level and task-level calibration is an interesting future work. Moreover, when exploring the combination schemes, we only focus on exploring the inner area of the calibration triangle. It is worthy to extend the parameter search to a larger area, i.e., by extrapolation of the calibration triangle, to find whether better results can be obtained."
        }
    ],
    "title": "P3DC-Shot: Prior-Driven Discrete Data Calibration for  Nearest-Neighbor Few-Shot Classification",
    "year": 2023
}