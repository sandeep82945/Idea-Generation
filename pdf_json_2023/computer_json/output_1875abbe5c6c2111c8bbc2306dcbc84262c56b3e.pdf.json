{
    "abstractText": "We propose Heuristic Blending (HUBL), a simple performance-improving technique for a broad class of offline RL algorithms based on value bootstrapping. HUBL modifies Bellman operators used in these algorithms, partially replacing the bootstrapped values with Monte-Carlo returns as heuristics. For trajectories with higher returns, HUBL relies more on heuristics and less on bootstrapping; otherwise, it leans more heavily on bootstrapping. We show that this idea can be easily implemented by relabeling the offline datasets with adjusted rewards and discount factors, making HUBL readily usable by many existing offline RL implementations. We theoretically prove that HUBL reduces offline RL\u2019s complexity and thus improves its finite-sample performance. Furthermore, we empirically demonstrate that HUBL consistently improves the policy quality of four state-of-the-art bootstrapping-based offline RL algorithms (ATAC, CQL, TD3+BC, and IQL), by 9% on average over 27 datasets of the D4RL and Meta-World benchmarks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sinong Geng"
        }
    ],
    "id": "SP:cf0e12473e7ed9fcba4f369ef489fb820d3cafc2",
    "references": [
        {
            "authors": [
                "Melda Alaluf",
                "Giulia Crippa",
                "Sinong Geng",
                "Zijian Jing",
                "Nikhil Krishnan",
                "Sanjeev Kulkarni",
                "Wyatt Navarro",
                "Ronnie Sircar",
                "Jonathan Tang"
            ],
            "title": "Reinforcement learning paycheck optimization for multivariate financial goals",
            "venue": "Risk & Decision Analysis,",
            "year": 2022
        },
        {
            "authors": [
                "Wissam Bejjani",
                "Rafael Papallas",
                "Matteo Leonetti",
                "Mehmet R Dogar"
            ],
            "title": "Planning with a receding horizon for manipulation in clutter using a learned value function",
            "venue": "IEEE-RAS 18th International Conference on Humanoid Robots (Humanoids),",
            "year": 2018
        },
        {
            "authors": [
                "Marc Bellemare",
                "Sriram Srinivasan",
                "Georg Ostrovski",
                "Tom Schaul",
                "David Saxton",
                "Remi Munos"
            ],
            "title": "Unifying count-based exploration and intrinsic motivation",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "David Blackwell"
            ],
            "title": "Discrete dynamic programming",
            "venue": "The Annals of Mathematical Statistics,",
            "year": 1962
        },
        {
            "authors": [
                "Ching-An Cheng",
                "Andrey Kolobov",
                "Adith Swaminathan"
            ],
            "title": "Heuristic-guided reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Ching-An Cheng",
                "Tengyang Xie",
                "Nan Jiang",
                "Alekh Agarwal"
            ],
            "title": "Adversarially trained actor critic for offline reinforcement learning",
            "venue": "arXiv preprint arXiv:2202.02446,",
            "year": 2022
        },
        {
            "authors": [
                "Gabriel Dulac-Arnold",
                "Nir Levine",
                "Daniel J Mankowitz",
                "Jerry Li",
                "Cosmin Paduraru",
                "Sven Gowal",
                "Todd Hester"
            ],
            "title": "Challenges of real-world reinforcement learning: definitions, benchmarks and analysis",
            "venue": "Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Justin Fu",
                "Katie Luo",
                "Sergey Levine"
            ],
            "title": "Learning robust rewards with adversarial inverse reinforcement learning",
            "venue": "arXiv preprint arXiv:1710.11248,",
            "year": 2017
        },
        {
            "authors": [
                "Justin Fu",
                "Aviral Kumar",
                "Ofir Nachum",
                "George Tucker",
                "Sergey Levine"
            ],
            "title": "D4rl: Datasets for deep data-driven reinforcement learning, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Scott Fujimoto",
                "Shixiang Shane Gu"
            ],
            "title": "A minimalist approach to offline reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Scott Fujimoto",
                "David Meger",
                "Doina Precup"
            ],
            "title": "Off-policy deep reinforcement learning without exploration",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "S Geng",
                "Z Kuang",
                "PL Peissig",
                "D Page",
                "L Maursetter",
                "KE Hansen"
            ],
            "title": "Parathyroid hormone independently predicts fracture, vascular events, and death in patients with stage 3 and 4 chronic kidney disease",
            "venue": "Osteoporosis International,",
            "year": 2019
        },
        {
            "authors": [
                "Sinong Geng",
                "Zhaobin Kuang",
                "Peggy Peissig",
                "David Page"
            ],
            "title": "Temporal poisson square root graphical models",
            "venue": "Proceedings of machine learning research,",
            "year": 2018
        },
        {
            "authors": [
                "Sinong Geng",
                "Houssam Nassif",
                "Carlos Manzanares",
                "Max Reppen",
                "Ronnie Sircar"
            ],
            "title": "Deep pqr: Solving inverse reinforcement learning using anchor actions",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Sinong Geng",
                "Houssam Nassif",
                "Carlos A Manzanares"
            ],
            "title": "A data-driven state aggregation approach for dynamic discrete choice models",
            "venue": "arXiv preprint arXiv:2304.04916,",
            "year": 2023
        },
        {
            "authors": [
                "Caglar Gulcehre",
                "Ziyu Wang",
                "Alexander Novikov",
                "Thomas Paine",
                "Sergio G\u00f3mez",
                "Konrad Zolna",
                "Rishabh Agarwal",
                "Josh S Merel",
                "Daniel J Mankowitz",
                "Cosmin Paduraru"
            ],
            "title": "Rl unplugged: A suite of benchmarks for offline reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "David Hoeller",
                "Farbod Farshidian",
                "Marco Hutter"
            ],
            "title": "Deep value model predictive control",
            "venue": "In Conference on Robot Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Hao Hu",
                "Yiqin Yang",
                "Qianchuan Zhao",
                "Chongjie Zhang"
            ],
            "title": "On the role of discount factor in offline reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Ehsan Imani",
                "Eric Graves",
                "Martha White"
            ],
            "title": "An off-policy policy gradient theorem using emphatic weightings",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Nan Jiang",
                "Alex Kulesza",
                "Satinder Singh",
                "Richard Lewis"
            ],
            "title": "The dependence of effective planning horizon on model accuracy",
            "venue": "In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Ray Jiang",
                "Tom Zahavy",
                "Zhongwen Xu",
                "Adam White",
                "Matteo Hessel",
                "Charles Blundell",
                "Hado Van Hasselt"
            ],
            "title": "Emphatic algorithms for deep reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Ying Jin",
                "Zhuoran Yang",
                "Zhaoran Wang"
            ],
            "title": "Is pessimism provably efficient for offline rl",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Ilya Kostrikov",
                "Ashvin Nair",
                "Sergey Levine"
            ],
            "title": "Offline reinforcement learning with implicit Q-learning",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Aviral Kumar",
                "Justin Fu",
                "Matthew Soh",
                "George Tucker",
                "Sergey Levine"
            ],
            "title": "Stabilizing off-policy q-learning via bootstrapping error reduction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Aviral Kumar",
                "Aurick Zhou",
                "George Tucker",
                "Sergey Levine"
            ],
            "title": "Conservative q-learning for offline reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Sascha Lange",
                "Thomas Gabel",
                "Martin Riedmiller"
            ],
            "title": "Batch reinforcement learning",
            "venue": "In Reinforcement learning,",
            "year": 2012
        },
        {
            "authors": [
                "Georg Ostrovski",
                "Marc G Bellemare",
                "A\u00e4ron Oord",
                "R\u00e9mi Munos"
            ],
            "title": "Count-based exploration with neural density models",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Marek Petrik",
                "Bruno Scherrer"
            ],
            "title": "Biasing approximate dynamic programming with a lower discount factor",
            "venue": "Advances in neural information processing systems,",
            "year": 2008
        },
        {
            "authors": [
                "Paria Rashidinejad",
                "Banghua Zhu",
                "Cong Ma",
                "Jiantao Jiao",
                "Stuart Russell"
            ],
            "title": "Bridging offline reinforcement learning and imitation learning: A tale of pessimism",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Harm Seijen",
                "Rich Sutton"
            ],
            "title": "True online td (lambda)",
            "venue": "In International Conference on Machine Learning,",
            "year": 2014
        },
        {
            "authors": [
                "Richard S Sutton",
                "Andrew G Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "Richard S Sutton",
                "A Rupam Mahmood",
                "Martha White"
            ],
            "title": "An emphatic approach to the problem of off-policy temporal-difference learning",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2016
        },
        {
            "authors": [
                "Harm Van Seijen",
                "Mehdi Fatemi",
                "Arash Tavakoli"
            ],
            "title": "Using a logarithmic mapping to enable lower discount factors in reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Martha White"
            ],
            "title": "Unifying task specification in reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Albert Wilcox",
                "Ashwin Balakrishna",
                "Jules Dedieu",
                "Wyame Benslimane",
                "Daniel Brown",
                "Ken Goldberg"
            ],
            "title": "Monte carlo augmented actor-critic for sparse reward deep reinforcement learning from suboptimal demonstrations",
            "venue": "arXiv preprint arXiv:2210.07432,",
            "year": 2022
        },
        {
            "authors": [
                "Robert Wright",
                "Steven Loscalzo",
                "Philip Dexter",
                "Lei Yu"
            ],
            "title": "Exploiting multi-step sample trajectories for approximate value iteration. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2013, Prague, Czech Republic",
            "venue": "Proceedings, Part I",
            "year": 2013
        },
        {
            "authors": [
                "Tianhe Yu",
                "Deirdre Quillen",
                "Zhanpeng He",
                "Ryan Julian",
                "Karol Hausman",
                "Chelsea Finn",
                "Sergey Levine"
            ],
            "title": "Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning",
            "venue": "In Conference on robot learning,",
            "year": 2020
        },
        {
            "authors": [
                "Andrea Zanette",
                "Ching-An Cheng",
                "Alekh Agarwal"
            ],
            "title": "Cautiously optimistic policy optimization and exploration with linear function approximation",
            "venue": "In Conference on Learning Theory,",
            "year": 2021
        },
        {
            "authors": [
                "Mingyuan Zhong",
                "Mikala Johnson",
                "Yuval Tassa",
                "Tom Erez",
                "Emanuel Todorov"
            ],
            "title": "Value function approximation and model predictive control",
            "year": 2013
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Offline reinforcement learning (RL) aims to learn decision-making strategies from static logged datasets [Fujimoto et al., 2019, Lange et al., 2012]. It has attracted increased interest in recent years, with the availability of large offline datasets being on the rise and online exploration required by alternative approaches such as online RL [Sutton and Barto, 2018] remaining expensive and risky in many real-world applications, such as robotics and healthcare.\nAmong offline RL algorithms, model-free approaches using dynamic programming with value bootstrapping have demonstrated particularly strong performance, achieving state-of-the-art (SoTA) results in offline RL benchmarks [Fu et al., 2020, Gulcehre et al., 2020]. The commonly used CQL [Kumar et al., 2020], TD3+BC [Fujimoto and Gu, 2021], IQL [Kostrikov et al., 2022], and ATAC [Cheng et al., 2022] belong to this category. They follow the actor-critic scheme and adopt the principle of pessimism in the face of uncertainty to optimize an agent via a performance lower bound that penalizes taking unfamiliar actions. Despite their strengths, existing model-free offline RL methods also have a major weakness: they do not perform consistently. An algorithm that does well on one dataset may struggle on another, sometimes even underperforming behavior cloning [DulacArnold et al., 2021, Kumar et al., 2019, Sutton and Barto, 2018]. These performance fluctuations stand in the way of applying even the strongest offline RL approaches to practical problems.\nIn this work, we propose Heuristic Blending (HUBL), an easy-to-implement technique to address offline RL\u2019s performance inconsistency. HUBL operates in combination with a bootstrapping-based offline RL algorithm by using heuristic value estimates to modify the rewards and discounts in the dataset that the base offline RL algorithm consumes. Effectively, this modification blends heuristic\nar X\niv :2\n30 6.\n00 32\n1v 1\n[ cs\n.L G\n] 1\nJ un\nvalues into dynamic programming to partially replace bootstrapping. Relying less on bootstrapping alleviates potential issues that bootstrapping causes and helps achieve more stable performance.\nHUBL can be easily implemented as a data relabeling procedure as summarized in Figure 1. Thus, it is readily applicable to many existing offline RL methods. Specifically, combining HUBL with an offline RL algorithm amounts to running this algorithm on a relabeled version of the original dataset with modified rewards r\u0303 and discounts \u03b3\u0303:\nr\u0303 = r + \u03b3\u03bbh, \u03b3\u0303 = \u03b3(1\u2212 \u03bb), where r is blended with a heuristic h, \u03b3\u0303 is the reduced discount, and \u03bb \u2208 [0, 1] is a blending factor representing the degree of trust towards the heuristic. We set h to be the Monte-Carlo returns of the trajectories in the dataset for offline RL. Such a heuristic is efficient and stable to compute, unlike bootstrapped Q-value estimates. The blending factor \u03bb in HUBL can be trajectory-dependent. Intuitively, we want \u03bb to be large (relying more on the heuristic) at trajectories where the behavior policy that collected the dataset performs well, and small (relying more on bootstrapped Q-values) otherwise. We provide three practical designs for \u03bb; they use only one hyperparameter, which, empirically, does not need active tuning.\nWe analyze HUBL\u2019s performance both theoretically and empirically. Theoretically, we provide a finite-sample\nperformance bound for offline RL with HUBL by framing it as solving a reshaped Markov decision process (MDP). To our knowledge, this is the first theoretical result for RL with heuristics in the offline setting. Our analysis shows that HUBL performs a bias-regret trade-off. On the one hand, solving the reshaped MDP with a smaller discount factor requires less bootstrapping and is relatively \u201ceasier\u201d, so the regret is smaller. On the other hand, HUBL induces bias due to reshaping the original MDP. Nonetheless, we demonstrate that the bias can be controlled by setting the \u03bb factor based on the above intuition, allowing HUBL to improve the performance of the base offline RL method.\nEmpirically, we run HUBL with the four aforementioned offline RL methods \u2013 CQL, TD3+BC, IQL, and ATAC \u2013 and show that enhancing these SoTA algorithms with HUBL can improve their performance by 9% on average across 27 datasets of D4RL [Fu et al., 2020] and Meta-World [Yu et al., 2020]. Notably, in some datasets where the base offline RL methd shows inconsistent performance, HUBL can achieve more than 50% relative performance improvement."
        },
        {
            "heading": "2 Related Work",
            "text": "Bootstrapping-based Offline RL A fundamental challenge of bootstrapping-based offline RL is the deadly triad [Sutton and Barto, 2018]: a negative interference between 1) off-policy learning from data with limited support, 2) value bootstrapping, and 3) the function approximator. Modern offline RL algorithms such as CQL [Kumar et al., 2020], TD3+BC [Fujimoto and Gu, 2021], IQL [Kostrikov et al., 2022], ATAC [Cheng et al., 2022], PEVI [Jin et al., 2021], and VI-LCB [Rashidinejad et al., 2021] employ pessimism to discourage the agent from taking actions unsupported by the data, which has proved to be an effective strategy to address the issue of limited support. However, they still suffer from a combination of errors in bootstrapping and function approximation. HUBL aims to address this with stable-to-compute Monte-Carlo return heuristics and thus is a complementary technique to pessimism.\nRL by blending multi-step returns The idea of blending multi-step Monte-Carlo returns into the bootstrapping operator to reduce the degree of bootstrapping and thereby increase its performance has a long history in RL. This technique has been widely used in temporal difference methods [Sutton and Barto, 2018], where the blending is achieved by modifying gradient updates [Jiang et al., 2021, Seijen and Sutton, 2014, Sutton et al., 2016] and reweighting observations [Imani et al., 2018]. It has also been applied to improve Q function estimation [Wright et al., 2013], online exploration [Bellemare\net al., 2016, Ostrovski et al., 2017] and the sensitivity to model misspecification [Zanette et al., 2021], and is especially effective for sparse-reward problems Wilcox et al. [2022]. In contrast to most of the aforementioned works, which focus on blending Monte-Carlo returns as part of an RL algorithm\u2019s online operation, HUBL is designed for the offline setting and acts as a simple data relabeling step. Recently, Wilcox et al. [2022] have also proposed the idea of data relabeling, but their design takes a max of multi-step returns and bootstrapped values and as a result tends to overestimate Q-functions. We observed this to be detrimental when data has a limited support.\nRL with heuristics More generally, HUBL relates to the framework of blending heuristics (which might be estimating quantities other than a policy\u2019s value) into bootstrapping [Bejjani et al., 2018, Cheng et al., 2021, Hoeller et al., 2020, Zhong et al., 2013]. However, the existing results focus only on the online case and do not conclusively show whether blending heuristics is valid in the offline case. For instance, the theoretical analysis in Cheng et al. [2021] breaks when applied to the offline setting as we will demonstrate in Section 5. The major difference is that online approaches rely heavily on collecting new data with the learned policy, which is impossible in the offline case. In this paper, we employ a novel analysis technique to demonstrate that blending heuristics is effective even in offline RL. To the best of our knowledge, ours is the first work to extend heuristic blending to the offline setting with both rigorous theoretical analysis and empirical results demonstrating performance improvement. One key insight is the adoption of a trajectory-dependent \u03bb blending factor, which is both a performance-improving design as well as a novel analysis technicality. Our trajectory-dependent blending is inspired by unifying task specification in online RL [White, 2017], where the discount factor is transition-dependent.\nDiscount regularization HUBL modifies the reward with a heuristic and reduces the discount. Discount regularization, on the other hand, is a complexity reduction technique that reduces only the discount. The idea of simplifying decision-making problems by reducing discount factors can be traced back to Blackwell optimality in the known MDP setting [Blackwell, 1962]. Most existing results on discount regularization [Jiang et al., 2015, Petrik and Scherrer, 2008] study the MDP setting or the online RL setting [Van Seijen et al., 2019]. Recently, Hu et al. [2022] has shown that discount regularization can also reduce the complexity of offline RL and serve as an extra source of pessimism. However, as we will show, simply reducing the discount without the compensation of blending heuristics in the offline setting can be excessively pessimistic and introduce large bias that hurts performance. In Section 6.1, we rigorously analyze this bias and empirically demonstrate the advantages of HUBL over discount regularization."
        },
        {
            "heading": "3 Background",
            "text": "In this section, we define the problem setup of offline RL (Section 3.1), review dynamic programming with value bootstrapping and briefly survey methods using heuristics (Section 3.2)."
        },
        {
            "heading": "3.1 Offline RL and Notation",
            "text": "We consider offline RL in a Markov decision process (MDP)M = (S,A,P, r, \u03b3), where S denotes the state space, A the action space, P the transition function, r : S \u00d7A \u2192 [0, 1] the reward function, and \u03b3 \u2208 [0, 1) the discount factor. A decision-making policy \u03c0 is a mapping from S to A, and its value function is defined as V \u03c0(s) = E[ \u2211\u221e t=0 \u03b3\ntr(st, at)|s0 = s, at \u223c \u03c0(\u00b7|st)]. In addition, we define Q\u03c0(s, a) := r(s, a) + \u03b3Es\u2032\u223cP|s,a[V \u03c0(s\u2032)] as its state-action value function (i.e., Q function). We use V \u2217 to denote the value function of the optimal policy \u03c0\u2217, which is our performance target. In addition, we introduce several definitions of distributions. First, we define the average state distribution of a policy \u03c0 starting from an initial state s0 as d\u03c0(s, a; s0) := \u2211\u221e t=0 d \u03c0 t (s, a; s0), where d\u03c0t (s, a; s0) is the state-action distribution at time t generated by running policy \u03c0 from an initial state s0. We assume that the MDP starts with a fixed initial state distribution d0. With slight abuse of notation, we define the average state distribution starting from d0 as d\u03c0(s, a) := d\u03c0(s, a; d0), and d\u03c0(s, a, s\u2032) := d\u03c0(s, a)P(s\u2032|s, a). The objective of offline RL is to learn a well-performing policy \u03c0\u0302 while using a pre-collected offline dataset D. The agent has no knowledge of the MDPM except information contained in D, and it cannot perform online interactions with environment to further collect data. We assume that the dataset D := {\u03c4} contains multiple trajectories collected by a behavior policy, where each \u03c4 = {(st, at, rt)}T\u03c4t=1 denotes a trajectory with length T\u03c4 . Suppose these trajectories contain N\ntransition tuples in total. With abuse of notation, we also writeD := {(s, a, s\u2032, r, \u03b3)}, where states s and action a follow a distribution \u00b5(s, a) induced by the behavior policy, s\u2032 is the state after each transition, r is the reward at s, a, and \u03b3 is the discount factor of the MDP. Note that the value of discount \u03b3 is the same in each tuple. We use \u2126 to denote the support of \u00b5(s, a). We do not make the full support assumption, in the sense that the dataset D may not contain a tuple for every s, a, s\u2032 transition in the MDP."
        },
        {
            "heading": "3.2 Dynamic Programming with Bootstrapping and Heuristics",
            "text": "Bootstrapping Many offline RL methods leverage dynamic programming with value bootstrapping. Given a policy \u03c0, we recall Q\u03c0 and V \u03c0 satisfy the Bellman equation:\nQ\u03c0(s, a) = r(s, a) + (bootstrapping)\ufe37 \ufe38\ufe38 \ufe37 \u03b3Es\u2032\u223cP(\u00b7|s,a)[V \u03c0(s\u2032)] . (1)\nThese bootstrapping-based methods compute Q\u03c0(s, a) using an approximated version of (1): given sampled tuples (s, a, s\u2032, \u03b3, r), such methods minimize the difference between the two sides of (1) with both Q\u03c0 and V \u03c0 replaced with function approximators 1. With limited offline data, learning the function approximator using bootstrapping can be challenging and yields inconsistent performance across different datasets [Dulac-Arnold et al., 2021, Kumar et al., 2019, Sutton and Barto, 2018].\nHeuristics A heuristic is a value function calculated using domain knowledge, Monte-Carlo averages, or pre-training. Heuristics are widely used in online RL, planning, and control to improve the performance of decision-making [Bejjani et al., 2018, Cheng et al., 2021, Hoeller et al., 2020, Zhong et al., 2013]. In this paper, we focus on the offline setting, and consider heuristics h that approximate the value function of the behavior policy. Such heuristics can be estimated fromD via Monte-Carlo methods."
        },
        {
            "heading": "4 Heuristic Blending (HUBL)",
            "text": "In this section we describe our main contribution \u2014 Heuristic Blending (HUBL), an algorithm that works in combination with bootstrapping-based offline RL methods and improves their performance.\n4.1 Motivation\nAlgorithm 3 HUBL + Offline RL 1: Input: DatasetD = {(s, a, s\u2032, r, \u03b3)} 2: Compute ht for each trajectory inD 3: Compute \u03bbt for each trajectory inD 4: Relabel r & \u03b3 by ht and \u03bbt as r\u0303 and \u03b3\u0303\nand create D\u0303 = {(s, a, s\u2032, r\u0303, \u03b3\u0303)} 5: \u03c0\u0302 \u2190 Offline RL on D\u0303 HUBL uses a heuristic computed as the Monte-Carlo return of the behavior policy in the training dataset D to improve offline RL. It reduces an offline RL algorithm\u2019s bootstrapping at trajectories where the behavior policy performs well, i.e., where the value of the behavior policy (i.e. the heuristic value) is high. With less amount of bootstrapping, it mitigates bootstrapping-induced issues on convergence stability and performance. In addition, since the extent of blending between the heuristic and bootstrapping is trajectory-dependent, HUBL introduce only limited performance bias to the base algorithm, and therefore can improve its performance overall."
        },
        {
            "heading": "4.2 Algorithm",
            "text": "As summarized in Algorithm 3, HUBL can be easily implemented: first relabel a base offline RL algorithm\u2019s training dataset D = {(s, a, s\u2032, r, \u03b3)} with modified rewards r\u0303 and discount factors \u03b3\u0303, creating a new dataset D\u0303 := {(s, a, s\u2032, r\u0303, \u03b3\u0303)}; next run the base algorithm on D\u0303. The data relabeling is done in three steps:\nStep 0: As a preparation step, we convert the data tuples in D back to trajectories like \u03c4 = {(st, at, rt)}T\u03c4t=1. For the next two steps, we work on data trajectories instead of data tuples to compute heuristics and blending factors.\n1Q-learning-based offline RL in Kostrikov et al. [2022] uses a dynamic programming equation similar to (1) but with V \u03c0(s) = argmaxa\u2208A Q \u03c0(s, a).\nStep 1: Computing heuristic ht We compute heuristics by Monte-Carlo returns. For each \u03c4 = {(st, at, rt)}T\u03c4t=1 \u2208 D, we calculate the heuristics as2 ht = \u2211T\u03c4 k=t \u03b3\nk\u2212trk and update the data trajectory as \u03c4 \u2190 {(st, at, rt, ht)}T\u03c4t=1. Step 2: Computing blending factor \u03bbt We append a scalar \u03bbt = \u03bb(\u03c4) \u2208 [0, 1] at each time point t of each trajectory as the blending factor, leading to \u03c4 \u2190 {(st, at, rt, ht, \u03bbt)}T\u03c4t=1. \u03bbt indicates the confidence in the heuristics on the trajectory. Intuitively, \u03bbt decides the contribution of heuristics over bootstrapped values in dynamic programming to update the Q-function. We desire \u03bbt to be closer to 1 when the heuristic value ht is higher (i.e., at states where the heuristic is closer to the optimal Q-value) to make offline RL rely more on the heuristic, and \u03bbt closer to zero when heuristic is lower to make offline RL to use more bootstrapping. We experiment with three different designs of \u03bb(\u03c4):\n\u2022 Constant: As a baseline, we consider \u03bb(\u03c4) = \u03b1 \u2208 [0, 1] for all s. We show that, despite forcing the same heuristic weight for every state, this formulation already provides performance improvements.\n\u2022 Sigmoid: As an alternative, we use the sigmoid function to construct a trajectory-dependent blending function \u03bb(\u03c4) = \u03b1\u03c3( \u2211T\u03c4 t=1 h(st)/T\u03c4 ), where \u03b1 \u2208 [0, 1] is a tunable constant and \u03c3 is the\nsigmoid function. Thus, \u03bb(\u03c4) varies with the performance of the behavior policy over data. \u2022 Rank: Similar to the Sigmoid labeling function, we provide a Rank labeling function \u03bb(\u03c4) = \u03b1 \u2211\n\u03c4 \u2032\u2208D 1h\u0304(\u03c4 \u2032)\u2264h\u0304(\u03c4)/n where n is the number of trajectories inD, and h\u0304(\u03c4) = 1 T \u2211 ht\u2208\u03c4 ht.\nStep 3: Relabeling r and \u03b3 Finally, we relabel the reward as r\u0303 and the discount factor as \u03b3\u0303 in each tuple ofD. To this end, we first convert the updated data trajectories {{(st, at, rt, ht, \u03bbt)}T\u03c4t=1} back into data tuples {(s, a, s\u2032, r, \u03b3, h\u2032, \u03bb\u2032)}, where h\u2032 and \u03bb\u2032 denote the next-step heuristic and blending factor. Then, for each data tuple, we compute\nr\u0303 = r + \u03b3\u03bb\u2032h\u2032 and \u03b3\u0303 = \u03b3(1\u2212 \u03bb\u2032), (2)\nto form a new dataset D\u0303 := {(s, a, s\u2032, r\u0303, \u03b3\u0303)}. Intuitively, one can interpret r\u0303 as injecting a heuristicdependent quantity \u03b3\u03bb\u2032h\u2032 into the original reward, and \u03b3\u0303 as reducing bootstrapping by shrinking the original discount factor by a factor of 1\u2212 \u03bb\u2032. We formally explain and justify the design of r\u0303 and \u03b3\u0303 in Section 5."
        },
        {
            "heading": "5 Understanding HUBL",
            "text": "In this section, we take a deeper look into how HUBL works. At a high level, our theoretical analysis explains that the modification made by HUBL introduces a trade-off between bias and regret (i.e., the variance of policy learning) into the base offline RL algorithm. Our analysis provides insights as to why reshaping of rewards and discounts per (2) gives a performance boost to state-of-the-art offline RL algorithms in the experiments in Section 6."
        },
        {
            "heading": "5.1 HUBL as MDP Reshaping",
            "text": "We analyze HUBL by viewing it as solving a reshaped MDP M\u0303 := (S,A,P, r\u0303, \u03b3\u0303) constructed by blending heuristics into the original MDP. To this end, with \u2126 denoting the support of the data distribution, we make a simplification by assuming that both the heuristic and blending factor are functions of in-\u2126 states (i.e., h(\u00b7) : \u2126\u2192 R and \u03bb(\u00b7) : \u2126\u2192 [0, 1]). For analysis, we them to out-of-\u2126 states as below:\nh(s) = { h(s) for s \u2208 \u2126 0 otherwise and \u03bb(s, s\u2032) = { \u03bb(s\u2032) for s, s\u2032 \u2208 \u2126 0 otherwise\n(3)\nWe note that this extension is only for the purpose of analysis, since HUBL never uses the values h(\u00b7) and \u03bb(\u00b7) outside \u2126; h on out-of-\u2126 can have any value and the following theorems still hold.\nWe define the reshaped MDP M\u0303 with redefined reward function and discount factor as r\u0303(s, a) := r(s, a) + \u03b3Es\u2032\u223cP(\u00b7|s,a) [ \u03bb(s, s\u2032)h(s\u2032) ] , and \u03b3\u0303(s, s\u2032) := \u03b3(1 \u2212 \u03bb(s, s\u2032)) respectively. Note that we\n2In our implementation, we also train a value function approximator to bootstrap at the end of a trajectory if the trajectory ends due to timeout.\nblend the original reward function with the expected heuristic and blending factor while adjusting the original discount factor correspondingly. The extent of blending is determined by the function \u03bb(\u00b7). Notice that this reshaped MDP has a transition-dependent3 discount factor \u03b3\u0303(s, s\u2032) which is compatible with generic unifying task specification of [White, 2017]. This novel definition of transition-dependent discount factor is a key analysis technique to show that blending heuristics is effective in the offline setting.\nReshaped Dynamic Programming When solving this reshaped MDP by dynamic programming, the Bellman equation changes from (1) accordingly into\nQ\u0303\u03c0(s, a) = r\u0303(s, a) + Es\u2032\u223cP(\u00b7|s,a)[\u03b3\u0303(s, s\u2032)V\u0303 \u03c0(s\u2032)]\n= r(s, a) + \u03b3 bootstrapping\ufe37 \ufe38\ufe38 \ufe37 Es\u2032\u223cP(\u00b7|s,a)[(1\u2212 \u03bb(s, s\u2032))V\u0303 \u03c0(s\u2032)] +\u03b3 heuristic\ufe37 \ufe38\ufe38 \ufe37 Es\u2032\u223cP(\u00b7|s,a)[\u03bb(s, s\u2032)h(s\u2032)] . (4)\nHere Q\u0303\u03c0 denotes the Q-function of policy \u03c0 in M\u0303, and V\u0303 \u03c0 denotes \u03c0\u2019s value function. Compared to the original Bellman equation (1), it can be seen that \u03bb(\u00b7) blends the heuristic with bootstrapping: the bigger \u03bb\u2019s values, the more bootstrapping is replaced by the heuristic. The effect of solving HUBL\u2019s reshaped MDP M\u0303 is twofold. On the one hand, M\u0303 is different from the original MDPM: the optimal policy for M\u0303 may not be optimal forM, so solving for M\u0303 could potentially lead to performance bias. One the other hand, M\u0303 has a smaller discount factor and thus is easier to solve thanM, as the agent needs to plan for a smaller horizon. Therefore, we can think of applying HUBL to offline RL problems as performing a bias-variance trade-off, which reduces the learning variance due to bootstrapping at the cost of the bias due to using a suboptimal heuristic. We will explain this more concretely next."
        },
        {
            "heading": "5.2 Bias-Regret Decomposition",
            "text": "The insight that HUBL reshapes the Bellman equation that the offline RL algorithm uses allows us to characterize HUBL\u2019s effects on policy learning. Namely, we use the modified Bellman equation from (4) to decompose the performance of the learned policy into bias and regret terms. Theorem 1. For any h : \u2126\u2192 R, \u03bb : \u2126\u2192 [0, 1], and policy \u03c0, with V \u2217 as the value function of the optimal policy, it holds that V \u2217(d0)\u2212 V \u03c0(d0) = Bias(\u03c0, h, \u03bb) + Regret(\u03c0, h, \u03bb), where\nBias(\u03c0, h, \u03bb) := \u03b3\n1\u2212 \u03b3 E(s,a,s\u2032)\u223cd\u03c0\u2217 [\u03bb(s\u2032)(V\u0303 \u03c0 \u2217 (s\u2032)\u2212 h(s\u2032))|s, s\u2032 \u2208 \u2126]\nRegret(\u03c0, h, \u03bb) := V\u0303 \u03c0 \u2217 (d0)\u2212 V\u0303 \u03c0(d0) +\n\u03b3\n1\u2212 \u03b3 E(s,a,s\u2032)\u223cd\u03c0 [\u03bb(s\u2032)(h(s\u2032)\u2212 V\u0303 \u03c0(s\u2032))|s, s\u2032 \u2208 \u2126].\nThe performance of \u03c0 depends on both bias and regret. The bias term describes the discrepancy caused by solving the reshaped MDP with \u03bb(\u00b7). When \u03bb(s\u2032) = 0, the bias becomes zero. The regret term describes the performance of the learned policy in the reshaped MDP. Intuitively, at states whose successors have bigger \u03bb(s\u2032) values, the reshaped MDP has a smaller discount factor and thus is easier to solve, which leads to smaller regret (i.e., smaller values for V\u0303 \u03c0 \u2217 (d0)\u2212 V\u0303 \u03c0\u0302(d0)). Therefore, solving a HUBL-reshaped MDP induces a bias term but may generate a smaller regret.\nRemark The critical difference \u2013 and novelty \u2013 of Theorem 1 compared to existing theoretical results for RL with heuristics in the offline setting is that both the bias and regret in Theorem 1 depend only on states in the data distribution support \u2126. This is crucial, because in the offline setting we have no access to observations beyond \u2126. In contrast, if in the preceding analysis we replace Theorem 1 by, for example, Lemma A.1 from Cheng et al. [2021] with a constant \u03bb, we will get a performance decomposition V \u2217(d0)\u2212 V \u03c0(d0) = (V \u2217(d0)\u2212 V\u0303 \u2217(d0)) + \u03b3\u03bb1\u2212\u03b3Es,a\u223cd\u03c0Es\u2032|s,a[h(s\n\u2032)\u2212 V\u0303 \u2217(s\u2032)] + (1 \u2212 \u03bb)(V\u0303 \u2217(d0) \u2212 V\u0303 \u03c0(d0)) + \u03bb1\u2212\u03b3 (V\u0303\n\u2217(d\u03c0) \u2212 V\u0303 \u03c0(d\u03c0)). The decomposition, however, suggests that the out-of-\u2126 values of \u03bb(s) or h(s) are important to the performance of HUBL."
        },
        {
            "heading": "5.3 Finite Sample Analysis for Bias and Regret",
            "text": "The finite-sample analysis of policy learning that we provide next illustrates more concretely how HUBL trades off bias and regret. Our analysis uses offline value iteration with lower confidence\n3A special case of trajectory-dependent discounts.\nbound (VI-LCB) [Rashidinejad et al., 2021] as the base offline RL method. Following its original convention, we make some technical simplifications to make the presentation cleaner. Specifically, we consider a tabular setting with \u03bb(s) = \u03b1 \u2208 [0, 1] for s \u2208 \u2126 as a constant value, which can be interpreted as the quality of the behavior policy averaged across states. Note that despite \u03bb(s) = \u03b1 is a constant on \u2126, the reshaped MDP is still defined by \u03bb(s, s\u2032) in (3) (i.e., \u03bb(s, s\u2032) = \u03b1 if s, s\u2032 \u2208 \u2126 and zero otherwise). We postpone the detailed procedure of VI-LCB with HUBL in Appendix C due to space limitation.\nTheorem 2 summarizes our finite-sample results. The proof of Theorem 2 can be found in Appendix B. Theorem 2. Under the setup described above, assume that the heuristic h(\u00b7) satisfies h(s) = V \u00b5(s) for any s \u2208 \u2126. Then the bias and the regret in Theorem 1 are bounded by\nBias(\u03c0\u0302, \u03bb) \u2264 \u03b3\u03bb 1\u2212 \u03b3E(s,a,s\u2032)\u223cd\u03c0 \u2217 [V \u2217(s\u2032)\u2212 V \u00b5(s\u2032)|s, s\u2032 \u2208 \u2126],\nED[Regret(\u03c0\u0302, \u03bb)] \u2272 min ( Vmax, \u221a V 2max(1\u2212 \u03b3)|S| N(1\u2212 \u03b3(1\u2212 \u03bb))4 (\u221a max s,a d\u03c0\u2217(s, a) \u00b5(s, a) + \u03b3\u03bb 1\u2212 \u03b3 \u221a max (s,a)\u2208\u2126 1 \u00b5(s, a) )) where Vmax denotes a constant upper bound for the value function.\nFor the bias bound, the assumption h(s) = V \u00b5(s) for any s \u2208 \u2126 is made for the ease of presentation. If it does not hold, an additive error term can be introduced in the bias bound to capture that. For the regret bound, maxs,a d\u03c0 \u2217 (s,a;d0)\n\u00b5(s,a) can be infinite, whereby the best regret bound is just Vmax. But when it is bounded as assumed by existing works [Rashidinejad et al., 2021], our results demonstrate how N , \u03b3 and \u03bb affect the regret bound.\nThe implications of Theorem 2 are threefold. First of all, it provides a finite-sample performance guarantee for HUBL with VI-LCB under the tabular setting. Compared with the performance bound of the original VI-LCB, min ( Vmax, \u221a V 2max|S| (1\u2212\u03b3)3N maxs,a d\u03c0\u2217 (s,a) \u00b5(s,a) ) , HUBL shrinks the discount factor\nby 1\u2212 \u03bb and thus potentially improves the performance while inducing bias. Second, Theorem 2 hints at the source of HUBL\u2019s bias and regret of HUBL. The bias is related to the the performance of the behavior (data-collection) policy, characterized by V \u2217(s) \u2212 V \u00b5(s). In the extreme case of data being collected by an expert policy, the bias induced by HUBL is 0. The regret is affected by d \u03c0\u2217 (s,a) \u00b5(s,a) , which describes the deviation of the optimal policy from the data distribution. Finally, Theorem 2 also provides guidance on how to construct a blending factor function \u03bb(\u00b7). To reduce bias, \u03bb(s) should be small at states where V \u2217(s) \u2212 V \u00b5(s) is small. To reduce regret, \u03bb(\u00b7) should be generally large but small at states where the learned policy is likely to deviate from the behavior policy. Therefore, an ideal \u03bb(\u00b7) should be large when the behavior policy is close to optimal but small when it deviates from the optimal policy. This is consistent with our design principle in Section 4.2."
        },
        {
            "heading": "6 Experiments",
            "text": "We study 27 benchmark datasets in D4RL and Meta-World. We show that HUBL is able to improve the performance of existing offline RL methods by 9% with easy modifications and simple hyperparameter tuning. Remarkably, in some datasets where the base offline RL shows inconsistent performance and especially underperforms, HUBL can achieve more than 50% performance improvement.\nHUBL variants and base offline RL methods We implement HUBL with four state-of-the-art offline RL algorithms as base methods: CQL [Kumar et al., 2020], TD3+BC [Fujimoto and Gu, 2021], IQL [Kostrikov et al., 2022], and ATAC [Cheng et al., 2022]. For each base method, we compare the performance of its original version to its performance with HUBL running three different blending strategies discussed in Section 4.2: Constant, Sigmoid and Rank. Thus, we experiment with 16 different methods in total. The implementation details of the base methods are in Appendix D.1.\nMetrics We use relative normalized score improvement, abbreviated as relative improvement, as a measure of HUBL\u2019s performance improvement. Specifically, for a given task and base method, we first compute the normalized score rbase achieved by the base method, and then the normalized score rHUBL of HUBL. The relative normalized score improvement is defined as rHUBL\u2212rbase|rbase| . We report the relative improvement of HUBL averaged over three seeds {0, 1, 10} in this section, with standard deviations, base method performance, behavior cloning performance and absolute normalized scores provided in Appendix D.2, D.5, and D.6.\nHyperparameter Tuning For each dataset, the hyperparameters of the base methods are tuned over six different configurations suggested by the original papers. HUBL has one extra hyperparameter, \u03b1, which is fixed for all the datasets but different for each base method. Specifically, \u03b1 is selected from {0.001, 0.01, 0.1} according to the relative improvement averaged over all the datasets. In practice, we notice that a single choice of \u03b1 around 0.1 is sufficient for good performance across most base offline RL methods and datasets as demonstrated by the sensitivity analysis in Appendix D.3."
        },
        {
            "heading": "6.1 D4RL Experiments",
            "text": "Results We study 9 benchmark datasets in D4RL. The relative improvement due to HUBL is reported in Figure 2. First, despite being simple and needing little hyperparameter tuning, HUBL improves the performance of base methods in most settings and only slightly hurts in some expert datasets where base offline RL methods are already performing very well with little space of improvement. Second, there are cases where HUBL achieve very significant relative improvement\u2014more than 50%. Such big improvement happens in the datasets where the base method shows inconsistent performance and underperforms other offline RL algorithms. HUBL solves this inconsistent performance issue by simply relabeling the data. Further, HUBL improves performance even on data with few expert trajec-\ntories like hopper-med-rep-v2, walker2d-med-rep-v2, and hopper-med-v2, because HUBL conducts more bootstrapping and relies less on the heuristic on suboptimal trajectories (see Section 4.2).\nAblation Studies Note that HUBL relabels both the reward and the discount factor, per (2). However, existing methods like Hu et al. [2022] suggest that a lower discount factor alone without blending heuristics into rewards can in general improve offline RL. Therefore, to demonstrate the necessity of modifying both the discount factor and rewards like (2), we consider ablation methods which only shrink the discount factor as \u03b3\u0303 without r\u0303. The achieved average relative improvement of these ablations is reported and compared with that of HUBL in Table 1. HUBL consistently outperforms these ablations, which justifies HUBL\u2019s coordinated modifications in both the reward and the discount factor. The advantage of HUBL is also consistent what our theoretical analysis predicts. The considered ablations do not modify the rewards and thus are equivalent to solving Q\u0303\u03c0(s, a) = r(s, a) + \u03b3Es\u2032\u223cP(\u00b7|s,a)[(1\u2212 \u03bb(s\u2032))V\u0303 \u03c0(s\u2032)]. Comparing it with (1), the solution will be consistently smaller than the true Q-function, inducing a pessimistic\nbias. Crucially, this bias is much more challenging to tackle than the bias induced by HUBL, because the former is inevitable even when the data is collected by an expert policy."
        },
        {
            "heading": "6.2 Meta-World Experiments",
            "text": "We also run considered methods on tasks from the Meta-World benchmark. We study 18 datasets collected following the procedure detailed in Appendix D.4. These problems are goal-oriented, which is in contrast to locomotion tasks in the previous D4RL experiments. The achieved relative improvement of HUBL in the Meta-World experiments are reported in Figure 3 and Figure 4. HUBL is again able to generally improve the performance of existing offline RL methods.\n6.3 Comparisons among blending strategies\nWe present results for three different blending factor designs for HUBL (constant, sigmoid, and rank), where the rank blending outperforms the other two. With 27 datasets and 4 base methods, we have 108 cases covered by Figure 2 3 and 4 We count the number of cases where a given blending strategy provides the best performance among\nthe three, and the results are reported in Table 2. We can see that Rank is favored on average."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this work, we propose HUBL, a method for improving the performance offline RL methods by blending heuristics with bootstrapping. HUBL is easy to implement and is generally applicable to various of offline RL methods. Empirically, we demonstrate the performance improvement of HUBL on 27 datasets in D4RL and Meta-World. We also provide a theoretic finite-sample performance bound for HUBL, which also sheds lights on HUBL\u2019s bias-regret trade-off and blending factor designs. To our knowledge, our theoretical analysis is the first formal proof to show the effectiveness of using heuristics in the offline setting.\nLimitations and Future Work In our current framework, HUBL calculate heuristics by Monte-Carlo returns. While this is a feasible and effective strategy in common practical scenarios where data are collected as trajectories, we remark that this strategy is not applicable to batch datasets with only very short trajectories or even no trajectories (i.e., data of just disconnected transition tuples). For such scenarios, HUBL requires other heuristic calculation strategies. We consider this direction as our future work. We also plan to apply HUBL to other tasks in healthcare and finance [Alaluf et al., 2022, Geng et al., 2019, 2018] where collecting data is especially expensive."
        },
        {
            "heading": "A Extended Results of Theorem 1",
            "text": "Below we prove the statement which is a restatemnt of theorem 1.\nTheorem 3. For any \u03bb : \u2126\u2192 [0, 1] and any h : \u2126\u2192 R, it holds V \u2217(s0)\u2212 V \u03c0\u0302(s0) = ( V\u0303 \u03c0 \u2217 (s0)\u2212 V\u0303 \u03c0\u0302(s0) ) (5)\n+ \u03b3\n1\u2212 \u03b3 E(s,a,s\u2032)\u223cd\u03c0\u2217 [\u03bb(s\u2032)(V\u0303 \u03c0 \u2217 (s\u2032)\u2212 h(s\u2032))|s, s\u2032 \u2208 \u2126]\n+ \u03b3\n1\u2212 \u03b3 E(s,a,s\u2032)\u223cd\u03c0 [\u03bb(s\u2032)(h(s\u2032)\u2212 V\u0303 \u03c0\u0302(s\u2032))|s, s\u2032 \u2208 \u2126]"
        },
        {
            "heading": "A.1 Technical Lemmas",
            "text": "Lemma 4. For any policy \u03c0,\nV \u03c0(s0)\u2212 V\u0303 \u03c0(s0) = \u03b3\n1\u2212 \u03b3 E(s,a,s\u2032)\u223cd\u03c0 [\u03bb(s\u2032)(V\u0303 \u03c0(s\u2032)\u2212 h(s\u2032))|s, s\u2032 \u2208 \u2126].\nProof. By the definition of \u03bb(s, s\u2032):\nV \u03c0(s0)\u2212 V\u0303 \u03c0(s0)\n= 1\n1\u2212 \u03b3 E(s,a,s\u2032)\u223cd\u03c0 [r(s, a) + \u03b3V\u0303 \u03c0(s\u2032)\u2212 V\u0303 \u03c0(s)]\n= 1\n1\u2212 \u03b3 E(s,a,s\u2032)\u223cd\u03c0 [r(s, a) + \u03b3V\u0303 \u03c0(s\u2032)\u2212 r(s, a)\u2212 \u03b3\u03bb(s, s\u2032)h(s\u2032) + \u03b3(1\u2212 \u03bb(s, s\u2032))V\u0303 \u03c0(s\u2032)]\n= \u03b3\n1\u2212 \u03b3 E(s,a,s\u2032)\u223cd\u03c0 [\u03bb(s, s\u2032)(V\u0303 \u03c0(s\u2032)\u2212 h(s\u2032))]\n= \u03b3\n1\u2212 \u03b3 E(s,a,s\u2032)\u223cd\u03c0 [\u03bb(s, s\u2032)(V\u0303 \u03c0(s\u2032)\u2212 h(s\u2032))|s, s\u2032 \u2208 \u2126]\n= \u03b3\n1\u2212 \u03b3 E(s,a,s\u2032)\u223cd\u03c0 [\u03bb(s\u2032)(V\u0303 \u03c0(s\u2032)\u2212 h(s\u2032))|s, s\u2032 \u2208 \u2126]"
        },
        {
            "heading": "A.2 Proof of Theorem 3",
            "text": "To prove the theorem, we decompose the regret into the following terms: V \u2217(s0)\u2212 V \u03c0\u0302(s0) = ( V \u2217(s0)\u2212 V\u0303 \u03c0 \u2217 (s0) ) + ( V\u0303 \u03c0 \u2217 (s0)\u2212 V\u0303 \u03c0\u0302(s0) ) + ( V\u0303 \u03c0\u0302(s0)\u2212 V \u03c0\u0302(s0) ) We apply theorem 4 to rewrite the first and the last terms as\nV \u2217(s0)\u2212 V\u0303 \u03c0 \u2217 (s0) =\n\u03b3\n1\u2212 \u03b3 E(s,a,s\u2032)\u223cd\u03c0\u2217 [\u03bb(s\u2032)(V\u0303 \u03c0 \u2217 (s\u2032)\u2212 h(s\u2032))|s, s\u2032 \u2208 \u2126]\nV\u0303 \u03c0\u0302(s0)\u2212 V \u03c0\u0302(s0) = \u03b3\n1\u2212 \u03b3 E(s,a,s\u2032)\u223cd\u03c0 [\u03bb(s\u2032)(h(s\u2032)\u2212 V\u0303 \u03c0\u0302(s\u2032))|s, s\u2032 \u2208 \u2126].\nCombining the two completes the proof."
        },
        {
            "heading": "B Extended Results for Theorem 2",
            "text": ""
        },
        {
            "heading": "B.1 Technical Lemmas",
            "text": "Lemma 5. Under the setup of Theorem 2, V\u0303 \u00b5(s) = V \u00b5(s).\nProof. Since \u2126 is the support of \u00b5, this can be shown by the following: for s \u2208 \u2126, V\u0303 \u00b5(s)\u2212 V \u00b5(s) = Ea\u223c\u00b5|sEs\u2032|s,a[r(s, a) + \u03b3\u03bb(s, s\u2032)h(s\u2032) + \u03b3(1\u2212 \u03bb(s, s\u2032))V\u0303 \u00b5(s\u2032)\u2212 r(s, a)\u2212 \u03b3V \u00b5(s\u2032)] = Ea\u223c\u00b5|sEs\u2032|s,a[r(s, a) + \u03b3\u03bb(s, s\u2032)h(s\u2032) + \u03b3(1\u2212 \u03bb(s, s\u2032))V\u0303 \u00b5(s\u2032)\u2212 r(s, a)\u2212 \u03b3V \u00b5(s\u2032)|s\u2032 \u2208 \u2126] = Ea\u223c\u00b5|sEs\u2032|s,a[r(s, a) + \u03b3\u03bb(s, s\u2032)V \u00b5(s\u2032) + \u03b3(1\u2212 \u03bb(s, s\u2032))V\u0303 \u00b5(s\u2032)\u2212 r(s, a)\u2212 \u03b3V \u00b5(s\u2032)|s\u2032 \u2208 \u2126] = \u03b3Ea\u223c\u00b5|sEs\u2032|s,a[(1\u2212 \u03bb(s, s\u2032))(V\u0303 \u00b5(s\u2032)\u2212 V \u00b5(s\u2032))|s\u2032 \u2208 \u2126].\nSince \u03b3 < 1, then by an argument of contraction, we have V\u0303 \u00b5(s)\u2212 V \u00b5(s) = 0 for s \u2208 \u2126.\nLemma 6. Assume h(s) \u2264 V \u2217(s), \u2200s \u2208 S. It holds that V\u0303 \u03c0\u2217(s) \u2264 V \u2217(s) for all s \u2208 S."
        },
        {
            "heading": "Proof.",
            "text": "V\u0303 \u03c0 \u2217 (s) = r(s, a) + \u03b3Es\u2032|s,a[\u03bb(s, s\u2032)h(s\u2032) + (1\u2212 \u03bb(s, s\u2032))V\u0303 \u03c0 \u2217 (s\u2032)]\n= V \u2217(s) + \u03b3Es\u2032|s,a[\u03bb(s, s\u2032)h(s\u2032) + (1\u2212 \u03bb(s, s\u2032))V\u0303 \u03c0 \u2217 (s\u2032)\u2212 V \u2217(s\u2032)] = V \u2217(s) + \u03b3Es\u2032|s,a[\u03bb(s, s\u2032)(h(s\u2032)\u2212 V \u2217(s\u2032)) + (1\u2212 \u03bb(s, s\u2032))(V\u0303 \u03c0 \u2217 (s\u2032)\u2212 V \u2217(s\u2032))] \u2264 V \u2217(s) + \u03b3Es\u2032|s,a[(1\u2212 \u03bb(s, s\u2032))(V\u0303 \u03c0 \u2217 (s\u2032)\u2212 V \u2217(s\u2032))]\nwhere in the inequality we used h(s) \u2264 V \u03c0\u2217(s). Then by a contraction argument, we can then show V\u0303 \u03c0 \u2217 (s)\u2212 V \u03c0\u2217(s) \u2264 0\nLemma 7 (Bias Upperbound). Under the assumptions of Theorem 2, the bias component can be bounded by:\nBias(\u03c0\u0302, \u03bb) \u2264 \u03bb\u03b3 1\u2212 \u03b3 E(s,a,s\u2032)\u223cd\u03c0\u2217 [V \u2217(s\u2032)\u2212 V \u00b5(s\u2032)|s, s\u2032 \u2208 \u2126].\nProof. Review the bias component\nBias(\u03c0\u0302, \u03bb) := \u03b3\n1\u2212 \u03b3 E(s,a,s\u2032)\u223cd\u03c0\u2217 [\u03bb(s\u2032)(V\u0303 \u03c0 \u2217 (s\u2032)\u2212 h(s\u2032))|s, s\u2032 \u2208 \u2126].\nUnder the assumptions of Theorem 2, we derive\nBias(\u03c0\u0302, \u03bb) = \u03bb\u03b3\n1\u2212 \u03b3 E(s,a,s\u2032)\u223cd\u03c0\u2217 [V\u0303 \u03c0\n\u2217 (s\u2032)\u2212 V \u2217(s\u2032) + V \u2217(s\u2032)\u2212 V \u00b5(s\u2032)|s, s\u2032 \u2208 \u2126].\nNext by Lemma 6 we have \u03bb\u03b31\u2212\u03b3E(s,a,s\u2032)\u223cd\u03c0\u2217 [V\u0303 \u03c0\u2217(s\u2032)\u2212 V \u2217(s\u2032)|s, s\u2032 \u2208 \u2126] \u2264 0, and thus\nBias(\u03c0\u0302, \u03bb) \u2264 \u03bb\u03b3 1\u2212 \u03b3 E(s,a,s\u2032)\u223cd\u03c0\u2217 [V \u2217(s\u2032)\u2212 V \u00b5(s\u2032)|s, s\u2032 \u2208 \u2126].\nLemma 8. The difference between V \u2217(s) and V\u0303 \u03c0 \u2217 (s) can be derived as\nV \u2217(s)\u2212 V\u0303 \u03c0 \u2217 (s) = E\u03c1\u03c0\u2217 (s)[ \u221e\u2211 t=1 [\u03bb(1\u2212 \u03bb)t\u22121\u03b3t(V \u2217(st)\u2212 h(st))]].\nProof. By the dynamic programming equation in both the original and shaped MDP.\nV \u2217(s)\u2212 V\u0303 \u03c0 \u2217 (s) =\u03b3(1\u2212 \u03bb)Ea\u223c\u03c0\u2217(\u00b7;s)[Es\u2032|s,a[V \u2217(s\u2032)\u2212 V\u0303 \u03c0 \u2217 (s\u2032)]]\n+ \u03b3\u03bbEa\u223c\u03c0\u2217(\u00b7;s)[Es\u2032|s,a[V \u2217(s\u2032)\u2212 h(s\u2032)]]. (6)\nThen, we use (6) recursively:\nV \u2217(s)\u2212 V\u0303 \u03c0 \u2217 (s) = \u03bbE\u03c1\u03c0\u2217 (s)[ \u221e\u2211 t=1 [(1\u2212 \u03bb)t\u22121\u03b3t(V \u2217(st)\u2212 h(st))]].\nLemma 9 (Regret Upperbound). The expected regret is bounded by\nED[Regret(\u03c0\u0302, \u03bb(\u00b7))] \u2272min ( Vmax, Vmax \u221a\u221a\u221a\u221a (1\u2212 \u03b3)|S|maxs,a d\u03c0\u2217 (s,a;d0)\u00b5(s,a) N(1\u2212 \u03b3(1\u2212 \u03bb))4 )\n+ \u03b3\u03bb\n1\u2212 \u03b3 min\n( Vmax, Vmax \u221a (1\u2212 \u03b3)|S| 1min(s,a)\u2208\u2126 \u00b5(s,a)\nN(1\u2212 \u03b3(1\u2212 \u03bb))4\n) .\nProof. Under the setups in Theorem 2, we have\nRegret(\u03c0\u0302, \u03bb(\u00b7)) = V\u0303 \u03c0 \u2217 (d0)\u2212 V\u0303 \u03c0\u0302(d0) +\n\u03b3\u03bb\n1\u2212 \u03b3 E(s,a,s\u2032)\u223cd\u03c0\u0302 [V \u00b5(s\u2032)\u2212 V\u0303 \u03c0\u0302(s\u2032)|s, s\u2032 \u2208 \u2126].\nFurther by Lemma 5, we can replace V \u00b5 by V\u0303 \u00b5:\nRegret(\u03c0\u0302, \u03bb(\u00b7)) = V\u0303 \u03c0 \u2217 (d0)\u2212 V\u0303 \u03c0\u0302(d0) +\n\u03b3\u03bb\n1\u2212 \u03b3 E(s,a,s\u2032)\u223cd\u03c0\u0302 [V\u0303 \u00b5(s\u2032)\u2212 V\u0303 \u03c0\u0302(s\u2032)|s, s\u2032 \u2208 \u2126]. (7)\nThen, by Lemma 10,\nED[V\u0303 \u03c0 \u2217 (d0)\u2212 V\u0303 \u03c0\u0302(d0)] \u2272 min ( Vmax, Vmax \u221a\u221a\u221a\u221a (1\u2212 \u03b3)|S|maxs,a d\u03c0\u2217 (s,a;d0)\u00b5(s,a) N(1\u2212 \u03b3(1\u2212 \u03bb))4 ) , (8)\nand\nED[V\u0303 \u00b5(d\u03c0\u0302)\u2212 V\u0303 \u03c0\u0302(d\u03c0\u0302)] \u2272 min ( Vmax, Vmax \u221a\u221a\u221a\u221a (1\u2212 \u03b3)|S|maxs,a d\u00b5(s,a;d\u03c0\u0302)\u00b5(s,a) N(1\u2212 \u03b3(1\u2212 \u03bb))4 ) .\nNote that, by Lemma 11, d\u03c0\u0302 stays in \u2126. Therefore, we get maxs,a d\u00b5(s,a;d\u03c0\u0302)\n\u00b5(s,a) =\nmaxs,a\u2208\u2126 d\u00b5(s,a;d\u03c0\u0302) \u00b5(s,a) \u2264 1 min(s,a)\u2208\u2126 \u00b5(s,a) , which leads to\nED[V\u0303 \u00b5(d\u03c0\u0302)\u2212 V\u0303 \u03c0\u0302(d\u03c0\u0302)] \u2272 min ( Vmax, Vmax \u221a (1\u2212 \u03b3)|S| 1min(s,a)\u2208\u2126 \u00b5(s,a)\nN(1\u2212 \u03b3(1\u2212 \u03bb))4\n) . (9)\nTake (8) and (9) into (7), we derive\nED[Regret(\u03c0\u0302, \u03bb(\u00b7))] \u2272min ( Vmax, Vmax \u221a\u221a\u221a\u221a (1\u2212 \u03b3)|S|maxs,a d\u03c0\u2217 (s,a;d0)\u00b5(s,a) N(1\u2212 \u03b3(1\u2212 \u03bb))4 )\n+ \u03b3\u03bb\n1\u2212 \u03b3 min\n( Vmax, Vmax \u221a (1\u2212 \u03b3)|S| 1min(s,a)\u2208\u2126 \u00b5(s,a)\nN(1\u2212 \u03b3(1\u2212 \u03bb))4\n) ."
        },
        {
            "heading": "B.2 Proof of Theorem 2",
            "text": "The proof follows by combining Lemma 7 and 9.\nC VI-LCB with HUBL\nWe use the offline value iteration with lower confidence bound (VI-LCB) [Rashidinejad et al., 2021] as the base algorithm to analyze concretely the effects of HUBL with finite samples for the tabular case.\nAlgorithm 4 HUBL with VI-LCB 1: Input: Batch datasetD and discount factor \u03b3. 2: Set T := logN1\u2212\u03b3 . 3: Randomly split D into T + 1 sets Dt = {(si, ai, ri, s\u2032i)} for t \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , T}, where D0\nconsists of N2 observations and other datasets have N 2T observations. 4: Set m0(s, a) := \u2211m\ni=1 1{(si, ai) = (s, a)} based on datasetD0. 5: For all a \u2208 A and s \u2208 S , initialize Q0(s, a) = 0, V0(s) = 0 and set \u03c00(s) = argmaxa m0(s, a).\n6: for t = 1, \u00b7 \u00b7 \u00b7 , T do 7: Initialize rt(s, a) = 0 and set P ts,a to be a random probability vector. 8: Set mt(s, a) := \u2211m i=1 1 {(si, ai) = (s, a)} based on datasetDt. 9: Compute penalty bt(s, a) for L = 2000 log(2(T + 1)|S||A|N)\nbt(s, a) := Vmax\n\u221a L\nmt(s, a) \u2228 1 .\n10: for (s, a) \u2208 (S,A) do 11: if mt(s, a) \u2265 1 then 12: Set P ts,a to be empirical transitions and rt(s, a) be empirical average of rewards. 13: end if 14: Set Qt(s, a)\u2190 rt(s, a)\u2212 bt(s, a) + \u03b3P ts,a \u2299 (I \u2212 \u039bs,:) \u00b7 Vt\u22121 + \u03b3P ts,a \u2299 \u039bs,: \u00b7 h. 15: end for 16: Compute V midt (s)\u2190 maxa Qt(s, a) and \u03c0midt (s) \u2208 argmaxa Qt(s, a). 17: for s \u2208 S do 18: if V midt (s) \u2264 Vt\u22121(s) then 19: Vt(s)\u2190 Vt\u22121(s) and \u03c0t(s)\u2190 \u03c0t\u22121(s). 20: else 21: Vt(s)\u2190 V midt (s) and \u03c0t(s)\u2190 \u03c0midt (s). 22: end if 23: end for 24: end for 25: Return \u03c0\u0302 := \u03c0T ."
        },
        {
            "heading": "C.1 Algorithm",
            "text": "We detail the procedure of HUBL when implemented with VI-LCB for the tabular setting. To start with, we introduce several definitions which will be used in the following algorithm and theoretical analysis. Without loss of generality, we assume that the sates take values in {1, 2, \u00b7 \u00b7 \u00b7 , |S|} and that the actions take values in {1, 2, \u00b7 \u00b7 \u00b7 , |A|}. Then, let h be a |S| \u00d7 1 vector which denotes the heuristic function. We assume each component satisfies hs = V \u00b5(s) for s \u2208 \u2126. Let \u039b be a |S| \u00d7 |S| matrix with \u039bs,s\u2032 = \u03bb if s, s\u2032 \u2208 \u2126 and \u039bs,s\u2032 = 0 if s or s\u2032 \u2208 \u2126. We use \u2299 to denote the component-wise multiplication, and \u039bs,: to denote a row of \u039b as a |S| \u00d7 1 vector. With slight abuse of notation, we use t to denote the index of iteration in this section, with T as the total number of iterations.\nTo conduct VI-LCB with HUBL, we follow Algorithm 3 in Rashidinejad et al. [2021] but with a modified updating rule. The procedure is summarized Algorithm 4. Compared with the original VI-LCB, we highlight the key modification in Line 14 with blue color. Specifically, at the tth iteration, based on (2), we modify updating rule of Algorithm 3 in Rashidinejad et al. [2021] into\nQt(s, a)\u2190 rt(s, a)\u2212 bt(s, a) + \u03b3P ts,a \u2299 (I \u2212 \u039bs,:) \u00b7 Vt\u22121 + \u03b3P ts,a \u2299 \u039bs,: \u00b7 h.\nNote that we introduce heuristics by \u03b3P ts,a \u2299 \u039bs,: \u00b7 h, while reducing the bootstrapping by \u03b3P ts,a \u2299 (I \u2212 \u039bs,:) \u00b7 Vt\u22121."
        },
        {
            "heading": "C.2 Regret Analysis",
            "text": "In this section, we study the regret under the reshaped MDP constructed by HUBL. Specifically, we bound the regret of Algorithm 4 in Lemma 10.\nLemma 10 (Regret of VI-LCB with HUBL). Let the assumptions in Section C.3 be satisfied. Then, for any initial distribution dinit in \u2126, the regret of Algorithm 4 is bounded by\nED[V\u0303 \u03c0 \u2217 (dinit)\u2212 V\u0303 \u03c0\u0302(dinit)] \u2272 min ( Vmax, Vmax \u221a\u221a\u221a\u221a (1\u2212 \u03b3)|S|maxs,a d\u03c0\u2217 (s,a;dinit)\u00b5(s,a) N(1\u2212 \u03b3(1\u2212 \u03bb))4 ) ."
        },
        {
            "heading": "C.3 Notations",
            "text": "We first provide some matrix notations for MDPs. We use P\u03c0 \u2208 R|S||A|\u00d7|S||A| to denote the transition matrix induced by policy \u03c0 whose (s, a)\u00d7 (s, a\u2032) element is equal to P (s\u2032|s, a)\u03c0(a\u2032|s\u2032), and d\u03c0 \u2208 R|S||A| to denote a state-action distribution induced by policy \u03c0 whose (s, a) element is equal to d(s)\u03c0(a|s). Similarly, we use \u039bQ \u2208 R|S||A|\u00d7|S||A| to denote a extended matrix version of \u039b.\nFurther, we focus on the policies which stay in the data distribution support \u2126. Such polices are formally defined as {\u03c0|d\u03c0(s, a; s0) = 0 for any (s, a) /\u2208 \u2126 and s0 \u2208 \u2126}. We also define a clean event:\nEMDP := { \u2200s, a, t, \u2223\u2223r(s, a)\u2212 rt(s, a) + \u03b3(Ps,a \u2212 P ts,a)\u2299 (I \u2212 \u039b) \u00b7 Vt\u22121\u2223\u2223 \u2264 bt(s, a)} ."
        },
        {
            "heading": "C.4 Technical Lemmas",
            "text": "With the aforementioned definitions and assumptions, we provide the following lemmas. Lemma 11. Let \u03c0 be a policy learned by Algorithm 4. Under the event Ecover := {\u2126 \u2286 D0} \u03c0 stays in \u2126 for any initial state in \u2126. The probability of Ecover is greater than\n1\u2212 |S|(1\u2212min s\u2208\u2126\n\u00b5(s)) N 2 .\nProof. Under the event Ecover, we first fix s \u2208 \u2126 and show that given s, the learned policy \u03c0 does not take any action a\u2032 /\u2208 \u2126. By Algorithm 4, for any (s, a\u2032) /\u2208 \u2126 and t = 1, 2, \u00b7 \u00b7 \u00b7 , T , we can derive\nQt(s, a \u2032) \u2264 \u2212Vmax \u221a 2000 log(2T + 1)|S||A|N + \u03b3Vmax.\nSince N and T are larger than 1, we can conclude that Qt(s, a\n\u2032) \u2264 0 for any a\u2032 such that (s, a\u2032) /\u2208 \u2126. Next we consider two cases:\nCase I: If there exists a such that (s, a) \u2208 \u2126 and that there exists t = 1, 2, \u00b7 \u00b7 \u00b7 , T such that Qt(s, a) > 0, we can conclude that QT (s, a) \u2265 Qt(s, a) > Q(s, a\u2032), which suggests that the learned policy \u03c0 never conducts action a\u2032 /\u2208 \u2126 given state s. Case II: If for any a such that (s, a) \u2208 \u2126 and any t = 1, 2, \u00b7 \u00b7 \u00b7 , T , we have\nQt(s, a) = 0,\naccording to Algorithm 4, \u03c0(s) = argmaxa m0(s, a). By the definition of Ecover, the policy \u03c0 stays in \u2126.\nNext, we consider the probability of Ecover. Given a state s \u2208 \u2126, we define the event Es := {m0(s, a) = 0 for all a such that (s, a) \u2208 \u2126}. The probability of Es can be derive as\nP(Es) \u2264 (1\u2212 \u00b5(s)) N 2 .\nBy union bound, we can derive that\nP(\u00acEcover) \u2264 |S|(1\u2212min s\u2208\u2126\n\u00b5(s)) N 2 .\nAs a result, \u03c0 stays in \u2126 with the probability greater than\n1\u2212 |S|(1\u2212min s\u2208\u2126\n\u00b5(s)) N 2 .\nLemma 12. Let \u03c0 be a policy that stays in \u2126. Then, with dinit as any initial distribution in \u2126, we can derive\ndinit(s)\n\u00b5(s, \u03c0(s)) \u2264\nd\u0303\u03c0(s,\u03c0(s);dinit) \u00b5(s,\u03c0(s)) 1\u2212 \u03b3(1\u2212 \u03bb) .\nProof. By definition, we have\nd\u0303\u03c0(s, \u03c0(s); dinit(s)) := 1\n1\u2212 \u03b3(1\u2212 \u03bb) \u221e\u2211 t=0 \u03b3t(1\u2212 \u03bb)tPt(St = s, \u03c0; dinit)\nTherefore, dinit(s) \u2264 11\u2212\u03b3(1\u2212\u03bb) d\u0303 \u03c0(s, \u03c0(s); dinit), which finishes the proof.\nLemma 13. Let v\u03c0k = d\u03c0init(\u03b3P\u03c0\u2299 (I\u2212\u039bQ))k. For a policy \u03c0 that stays in \u2126, the following equality holds: v\u03c0k = d \u03c0 init(\u03b3(1\u2212 \u03bb)P\u03c0)k.\nProof. Since \u03c0 stays in \u2126, P\u03c0 only accesses the entries in I \u2212 \u039bQ whose values equal to (1\u2212 \u03bb). This observation finishes the proof.\nLemma 14. Let \u03c0 be a policy that stays in \u2126. Under the event EMDP , for all t = 1, 2, \u00b7 \u00b7 \u00b7 , T ,\nV\u0303 (\u03c0)\u2212 V\u0303 (\u03c0t) \u2264 Vmax\u03b3t(1\u2212 \u03bb)t + 2 t\u2211\ni=1\nEv\u03c0t\u2212i [bi(s, a)].\nProof. The proof follows the Lemma 2 of Rashidinejad et al. [2021] combined with Lemma 13.\nLemma 15. For any policy \u03c0 that stays in \u2126, the following inequality is true:\nd\u0303\u03c0(s, a; dinit) \u2264 1\u2212 \u03b3\n1\u2212 \u03b3(1\u2212 \u03bb) d\u03c0(s, a; dinit),\nfor any (s, a) \u2208 \u2126.\nProof. By definition, we have\nd\u0303\u03c0(s, a; dinit) := 1\n1\u2212 \u03b3(1\u2212 \u03bb) \u221e\u2211 t=0 \u03b3t(1\u2212 \u03bb)tPt(St = s,At = a, ;\u03c0, dinit)\nd\u03c0(s, a; dinit) := 1\n1\u2212 \u03b3 \u221e\u2211 t=0 \u03b3tPt(St = s,At = a;\u03c0, dinit).\nTherefore,\nd\u0303\u03c0(s, a; dinit) \u2264 1\n1\u2212 \u03b3(1\u2212 \u03bb) \u221e\u2211 t=0 \u03b3tPt(St = s,At = a;\u03c0, dinit) = 1\u2212 \u03b3 1\u2212 \u03b3(1\u2212 \u03bb) d\u03c0(s, a; dinit)"
        },
        {
            "heading": "C.5 Proof of Lemma 10",
            "text": "By the event Ecover, we can decompose the regret into\nED[V\u0303 \u03c0 \u2217 (dinit)\u2212 V\u0303 \u03c0\u0302(dinit)]\n\u2264 ED[V\u0303 \u03c0 \u2217 (dinit)\u2212 V\u0303 \u03c0\u0302(dinit)1 {Ecover}] + ED[V\u0303 \u03c0 \u2217 (dinit)\u2212 V\u0303 \u03c0\u0302(dinit)1 {Eccover}]\n\u2264 ED[V\u0303 \u03c0 \u2217 (dinit)\u2212 V\u0303 \u03c0\u0302(dinit)1 {Ecover}] + Vmax|S| ( |S| \u2212 1 |S| )N 2 ,\n(10)\nwhere the second inequality is by Lemma 11. Next, we focus on ED[V\u0303 \u03c0 \u2217 (dinit) \u2212 V\u0303 \u03c0\u0302(dinit)1 {Ecover}]. Following the analysis in C.5 of Rashidinejad et al. [2021], we can derive\nED[(V\u0303 \u03c0 \u2217 (dinit)\u2212 V\u0303 \u03c0\u0302(dinit))1 {Ecover}]\n\u2264ED[(V\u0303 \u03c0 \u2217 (dinit)\u2212 V\u0303 \u03c0\u0302(d0))1 {Ecover}] \u2264ED[Es\u223cdinit [(V\u0303 \u03c0 \u2217 (dinit)\u2212 V\u0303 \u03c0\u0302(dinit))1 {\u2203t \u2264 T,mt(s, \u03c0\u2217(s)) = 0}1 {Ecover}]] := T1\n+ ED[Es\u223cdinit [(V\u0303 \u03c0 \u2217 (dinit)\u2212 V\u0303 \u03c0\u0302(dinit))\n1 {\u2200t \u2264 T,mt(s, \u03c0\u2217(s)) \u2265 1}1 {EMDP }1 {Ecover}]] := T2 + ED[Es\u223cdinit [(V\u0303 \u03c0 \u2217 (dinit)\u2212 V\u0303 \u03c0\u0302(dinit)) 1 {\u2200t \u2264 T,mt(s, \u03c0\u2217(s)) \u2265 1}1 {EcMDP }1 {Ecover}]] := T3.\nBy Lemma 11, under the event Ecover, \u03c0\u0302 stays in \u2126. In other words, Lemma 12, 13 and 14 are all applicable to \u03c0\u0302.\nNext, for the following analysis, we consider the case that \u03c0\u2217 stays in \u2126. We will discuss the case that \u2126 does not cover \u03c0\u2217 at the end of the proof.\nBy Lemma 12 and C.5.1 of Rashidinejad et al. [2021],\nT1 \u2264 8Vmax|S|T 2 maxs,a d\u0303\n\u03c0\u2217 (s,a;dinit) \u00b5(s,a)\n9(1\u2212 (1\u2212 \u03bb)\u03b3)N .\nBy Lemma 14 and C.5.2 of Rashidinejad et al. [2021],\nT2 \u2264 Vmax\u03b3T (1\u2212 \u03bb)T + 32 Vmax\n1\u2212 \u03b3(1\u2212 \u03bb)\n\u221a 2L|S|T maxs,a d\u0303 \u03c0\u2217 (s,a;dinit) \u00b5(s,a)\nN .\nBy C.5.2 of Rashidinejad et al. [2021],\nT3 \u2264 Vmax N .\nCombining T1, T2 T3, and (10), with T = logN/(1\u2212 \u03b3(1\u2212 \u03bb)),\nED[V\u0303 \u03c0 \u2217 (dinit)\u2212 V\u0303 \u03c0\u0302(dinit)] \u2272 min ( Vmax, Vmax \u221a\u221a\u221a\u221a |S|maxs,a d\u0303\u03c0\u2217 (s,a;dinit)\u00b5(s,a) N(1\u2212 \u03b3(1\u2212 \u03bb))3 ) .\nFinally, by Lemma 15, we finish the proof:\nED[V\u0303 \u03c0 \u2217 (dinit)\u2212 V\u0303 \u03c0\u0302(dinit)] \u2272 min ( Vmax, Vmax \u221a\u221a\u221a\u221a (1\u2212 \u03b3)|S|maxs,a d\u03c0\u2217 (s,a;dinit)\u00b5(s,a) N(1\u2212 \u03b3(1\u2212 \u03bb))4 ) . (11)\nNote that (11) is derived under the assumption that \u03c0\u2217 stays in \u2126. However, it also holds when \u03c0\u2217 is not covered by \u2126. In that case, maxs,a d\u03c0 \u2217 (s,a;dinit) \u00b5(s,a) =\u221e and ED[V\u0303 \u03c0\u2217(dinit)\u2212 V\u0303 \u03c0\u0302(dinit)] \u2264 Vmax."
        },
        {
            "heading": "D Extended Results for Experiments",
            "text": "We conduct HUBL with four offline RL base methods on 27 datasets in D4RL and Meta-World. By blending heuristics with bootstrapping, HUBL reduces the complexity of decision-making and provides smaller regret while generating limited bias. We demonstrate that HUBL is able to improve the performance of offline RL methods.\nD.1 Implementation Details\nWe implement base offline RL methods with code sources provided in Table 3.\nExperiments with ATAC, IQL, and TD3+BC are ran on Standard_F4S_V2 nodes of Azure, and experiments with CQL are ran on NC6S_V2 nodes of Azure. As suggested by the original implementation of the considered base offline RL methods, we use 3-layer fully connected neural networks for policy critics and value networks, where each hidden layer has 256 neurons and ReLU activation and the output layer is linear. The first-order optimization is implemented by ADAM [Kingma and Ba, 2014] with a minibatch size as 256. The learning rates are selected following the original implementation and are reported in Table 4.\nFor each dataset, the hyperparameters of base methods are tuned from six different configurations suggested by the original papers. Such configurations are summarized in Table 5.\nMeanwhile, HUBL has one extra hyperparameter, \u03b1, which is fixed for all the datasets but different for each base method. Specifically, \u03b1 is selected from {0.001, 0.01, 0.1} according to the relative improvement averaged over all the datasets in one task. Later in the robustness analysis, we show that the performance of HUBL is insensitive to the selection of \u03b1. For each configuration and each base method, we repeat experiments three times with seeds in {0, 1, 10}."
        },
        {
            "heading": "D.2 Base Performance and Standard Deviations for D4RL Datasets",
            "text": "We provide the performance of base offline RL methods and standarde deviations in Table 6."
        },
        {
            "heading": "D.3 Robustness of HUBL to \u03b1",
            "text": "In this section, we demonstrate the robustness of HUBL to \u03b1. Specifically, we provide the average relative improvements of HUBL on D4RL datasets in Table 7. Notice that most average relative improvements are positive, which shows that HUBL can improve the performance with different values.\nThe selected \u03b1\u2019s are reported in Table 8:"
        },
        {
            "heading": "D.4 Data Collection for Meta-World",
            "text": "We collect data for Meta-World tasks using normalized rewards with goal-oriented stopping. Specifically, given that the original rewards of Meta-World are in [0, 10], we shift them by \u221210 and divide by 10, so that the normalized rewards take values in [\u22121, 0]. Then, we use the hand scripted policy given in Meta-World with different Gaussian noise levels in {0.1, 0.5, 1} to collect 100 trajectory for each task. In this process, a trajectory ends if (i) it reaches the max length of a trajectory (150); (ii) it finishes the goal. Note that we follow the same rule when testing the performance of a learned policy."
        },
        {
            "heading": "D.5 Base Performance and Standard Deviations for Meta-World Datasets",
            "text": "We provide the performance of base offline RL methods and standard deviations in Table 9 and 10. We also consider behavior cloning (BC) as as a baseline, and also a representative for imitation learning and inverse reinforcement learning methods [Fu et al., 2017, Geng et al., 2020, 2023]. Since the behavior policy is the scripted policy with Gaussian noise, BC can effective recover the scripted policy and thus is especially competitive."
        },
        {
            "heading": "D.6 Extended Results for Absolute Improvement",
            "text": "We report the experiment results in absolute improvement of normalized score for D4RL and MetaWorld."
        }
    ],
    "title": "Improving Offline RL by Blending Heuristics",
    "year": 2023
}