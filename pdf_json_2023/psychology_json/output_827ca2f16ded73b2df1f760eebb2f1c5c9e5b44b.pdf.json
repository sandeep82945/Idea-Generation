{
    "abstractText": "Molecular dynamics (MD) simulation is a widely used technique to simulate molecular systems, most commonly at the all-atom resolution where equations of motion are integrated with timesteps on the order of femtoseconds (1fs = 10\u221215s). MD is often used to compute equilibrium properties, which requires sampling from an equilibrium distribution such as the Boltzmann distribution. However, many important processes, such as binding and folding, occur over timescales of milliseconds or beyond, and cannot be efficiently sampled with conventional MD. Furthermore, new MD simulations need to be performed for each molecular system studied. We present Timewarp, an enhanced sampling method which uses a normalising flow as a proposal distribution in a Markov chain Monte Carlo method targeting the Boltzmann distribution. The flow is trained offline on MD trajectories and learns to make large steps in time, simulating the molecular dynamics of 10\u221210 fs. Crucially, Timewarp is transferable between molecular systems: once trained, we show that it generalises to unseen small peptides (2-4 amino acids) at all-atom resolution, exploring their metastable states and providing wall-clock acceleration of sampling compared to standard MD. Our method constitutes an important step towards general, transferable algorithms for accelerating MD.",
    "authors": [
        {
            "affiliations": [],
            "name": "Leon Klein"
        },
        {
            "affiliations": [],
            "name": "Andrew Y. K. Foong"
        },
        {
            "affiliations": [],
            "name": "Tor Erlend Fjelde"
        },
        {
            "affiliations": [],
            "name": "Marc Brockschmidt"
        },
        {
            "affiliations": [],
            "name": "Sebastian Nowozin"
        },
        {
            "affiliations": [],
            "name": "Frank No\u00e9"
        }
    ],
    "id": "SP:a076f74db6c28454fbcc7e74d7f3604f75e5c7c0",
    "references": [
        {
            "authors": [
                "Frank No\u00e9",
                "Christof Sch\u00fctte",
                "Eric Vanden-Eijnden",
                "Lothar Reich",
                "Thomas R Weikl"
            ],
            "title": "Constructing the equilibrium ensemble of folding pathways from short off-equilibrium simulations",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 1901
        },
        {
            "authors": [
                "Kresten Lindorff-Larsen",
                "Stefano Piana",
                "Ron O Dror",
                "David E Shaw"
            ],
            "title": "How fast-folding proteins",
            "venue": "fold. Science,",
            "year": 2011
        },
        {
            "authors": [
                "Ignasi Buch",
                "Toni Giorgino",
                "Gianni De Fabritiis"
            ],
            "title": "Complete reconstruction of an enzymeinhibitor binding process by molecular dynamics simulations",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2011
        },
        {
            "authors": [
                "Nuria Plattner",
                "Stefan Doerr",
                "Gianni De Fabritiis",
                "Frank No\u00e9"
            ],
            "title": "Complete protein\u2013protein association kinetics in atomic detail revealed by molecular dynamics simulations and Markov modelling",
            "venue": "Nature chemistry,",
            "year": 2017
        },
        {
            "authors": [
                "Cecilia Clementi"
            ],
            "title": "Coarse-grained models of protein folding: toy models or predictive tools",
            "venue": "Current opinion in structural biology,",
            "year": 2008
        },
        {
            "authors": [
                "Sebastian Kmiecik",
                "Dominik Gront",
                "Michal Kolinski",
                "Lukasz Wieteska",
                "Aleksandra Elzbieta Dawid",
                "Andrzej Kolinski"
            ],
            "title": "Coarse-grained protein models and their applications",
            "venue": "Chemical reviews,",
            "year": 2016
        },
        {
            "authors": [
                "Alessandro Laio",
                "Michele Parrinello"
            ],
            "title": "Escaping free-energy minima",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2002
        },
        {
            "authors": [
                "Danilo Rezende",
                "Shakir Mohamed"
            ],
            "title": "Variational inference with normalizing flows",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "Frank No\u00e9",
                "Simon Olsson",
                "Jonas K\u00f6hler",
                "Hao Wu"
            ],
            "title": "Boltzmann generators \u2014 sampling equilibrium states of many-body systems with deep learning",
            "year": 2019
        },
        {
            "authors": [
                "Jonas K\u00f6hler",
                "Andreas Kr\u00e4mer",
                "Frank No\u00e9"
            ],
            "title": "Smooth normalizing flows",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jonas K\u00f6hler",
                "Michele Invernizzi",
                "Pim de Haan",
                "Frank No\u00e9"
            ],
            "title": "Rigid body flows for sampling molecular crystal structures",
            "venue": "In International Conference on Machine Learning, ICML 2023,",
            "year": 2023
        },
        {
            "authors": [
                "Manuel Dibak",
                "Leon Klein",
                "Andreas Kr\u00e4mer",
                "Frank No\u00e9"
            ],
            "title": "Temperature steerable flows and Boltzmann generators",
            "venue": "Phys. Rev. Res.,",
            "year": 2022
        },
        {
            "authors": [
                "Bowen Jing",
                "Gabriele Corso",
                "Jeffrey Chang",
                "Regina Barzilay",
                "Tommi S. Jaakkola"
            ],
            "title": "Torsional diffusion for molecular conformer generation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Leon Klein",
                "Andreas Kr\u00e4mer",
                "Frank Noe"
            ],
            "title": "Equivariant flow matching",
            "venue": "In Thirty-seventh Conference on Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Laurence Illing Midgley",
                "Vincent Stimper",
                "Javier Antoran",
                "Emile Mathieu",
                "Bernhard Sch\u00f6lkopf",
                "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
            ],
            "title": "SE(3) equivariant augmented coupling flows",
            "venue": "In Thirtyseventh Conference on Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Minkai Xu",
                "Lantao Yu",
                "Yang Song",
                "Chence Shi",
                "Stefano Ermon",
                "Jian Tang"
            ],
            "title": "Geodiff: A geometric diffusion model for molecular conformation generation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Mathias Schreiner",
                "Ole Winther",
                "Simon Olsson"
            ],
            "title": "Implicit transfer operator learning: Multiple time-resolution models for molecular dynamics",
            "venue": "In Thirty-seventh Conference on Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Xiang Fu",
                "Tian Xie",
                "Nathan J. Rebello",
                "Bradley Olsen",
                "Tommi S. Jaakkola"
            ],
            "title": "Simulate timeintegrated coarse-grained molecular dynamics with multi-scale graph networks",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Jan-Hendrik Prinz",
                "Hao Wu",
                "Marco Sarich",
                "Bettina Keller",
                "Martin Senne",
                "Martin Held",
                "John D Chodera",
                "Christof Sch\u00fctte",
                "Frank No\u00e9"
            ],
            "title": "Markov models of molecular kinetics: Generation and validation",
            "venue": "The Journal of chemical physics,",
            "year": 2011
        },
        {
            "authors": [
                "William C Swope",
                "Jed W Pitera",
                "Frank Suits"
            ],
            "title": "Describing protein folding kinetics by molecular dynamics simulations. 1. theory",
            "venue": "The Journal of Physical Chemistry B,",
            "year": 2004
        },
        {
            "authors": [
                "Brooke E Husic",
                "Vijay S Pande"
            ],
            "title": "Markov state models: From an art to a science",
            "venue": "Journal of the American Chemical Society,",
            "year": 2018
        },
        {
            "authors": [
                "Andreas Mardt",
                "Luca Pasquali",
                "Hao Wu",
                "Frank No\u00e9"
            ],
            "title": "VAMPnets for deep learning of molecular kinetics",
            "venue": "Nature communications,",
            "year": 2018
        },
        {
            "authors": [
                "Hao Wu",
                "Andreas Mardt",
                "Luca Pasquali",
                "Frank No\u00e9"
            ],
            "title": "Deep generative Markov state models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Jiaming Song",
                "Shengjia Zhao",
                "Stefano Ermon"
            ],
            "title": "A-NICE-MC: Adversarial training for MCMC",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Daniel Levy",
                "Matt D. Hoffman",
                "Jascha Sohl-Dickstein"
            ],
            "title": "Generalizing Hamiltonian Monte Carlo with neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Zengyi Li",
                "Yubei Chen",
                "Friedrich T. Sommer"
            ],
            "title": "A neural network MCMC sampler that maximizes proposal",
            "venue": "entropy. Entropy,",
            "year": 2021
        },
        {
            "authors": [
                "Michalis Titsias",
                "Petros Dellaportas"
            ],
            "title": "Gradient-based adaptive Markov chain Monte Carlo",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Robert H Swendsen",
                "Jian-Sheng Wang"
            ],
            "title": "Replica monte carlo simulation of spin-glasses",
            "venue": "Physical review letters,",
            "year": 1986
        },
        {
            "authors": [
                "David J. Earl",
                "Michael W. Deem"
            ],
            "title": "Parallel tempering: Theory, applications, and new perspectives",
            "venue": "Phys. Chem. Chem. Phys.,",
            "year": 2005
        },
        {
            "authors": [
                "Jerome P Nilmeier",
                "Gavin E Crooks",
                "David DL Minh",
                "John D Chodera"
            ],
            "title": "Nonequilibrium candidate monte carlo is an efficient tool for equilibrium simulation",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2011
        },
        {
            "authors": [
                "Christina Winkler",
                "Daniel Worrall",
                "Emiel Hoogeboom",
                "Max Welling"
            ],
            "title": "Learning likelihoods with conditional normalizing flows",
            "venue": "arXiv preprint arXiv:1912.00042,",
            "year": 2019
        },
        {
            "authors": [
                "Peter Eastman",
                "Jason Swails",
                "John D Chodera",
                "Robert T McGibbon",
                "Yutong Zhao",
                "Kyle A Beauchamp",
                "Lee-Ping Wang",
                "Andrew C Simmonett",
                "Matthew P Harrigan",
                "Chaya D Stern"
            ],
            "title": "Openmm 7: Rapid development of high performance algorithms for molecular dynamics",
            "venue": "PLoS computational biology,",
            "year": 2017
        },
        {
            "authors": [
                "C. Huang",
                "Laurent Dinh",
                "Aaron C. Courville"
            ],
            "title": "Augmented normalizing flows: Bridging the gap between generative flows and latent variable models",
            "year": 2002
        },
        {
            "authors": [
                "Jianfei Chen",
                "Cheng Lu",
                "Biqi Chenli",
                "Jun Zhu",
                "Tian Tian"
            ],
            "title": "Vflow: More expressive generative flows with variational data augmentation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Nicholas Metropolis",
                "Arianna W Rosenbluth",
                "Marshall N Rosenbluth",
                "Augusta H Teller",
                "Edward Teller"
            ],
            "title": "Equation of state calculations by fast computing machines",
            "venue": "The Journal of Chemical Physics,",
            "year": 1953
        },
        {
            "authors": [
                "Laurent Dinh",
                "Jascha Sohl-Dickstein",
                "Samy Bengio"
            ],
            "title": "Density estimation using real NVP",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Yao-Hung Hubert Tsai",
                "Shaojie Bai",
                "Makoto Yamada",
                "Louis-Philippe Morency",
                "Ruslan Salakhutdinov"
            ],
            "title": "Transformer dissection: An unified understanding for transformer\u2019s attention via the lens of kernel",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
            "year": 2019
        },
        {
            "authors": [
                "Jonas K\u00f6hler",
                "Leon Klein",
                "Frank No\u00e9"
            ],
            "title": "Equivariant flows: exact likelihood generative learning for symmetric densities",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Guillermo P\u00e9rez-Hern\u00e1ndez",
                "Fabian Paul",
                "Toni Giorgino",
                "Gianni De Fabritiis",
                "Frank No\u00e9"
            ],
            "title": "Identification of slow molecular order parameters for Markov model construction",
            "venue": "The Journal of chemical physics,",
            "year": 2013
        },
        {
            "authors": [
                "Radford M Neal"
            ],
            "title": "Probabilistic inference using Markov chain Monte Carlo methods",
            "venue": "Department of Computer Science,",
            "year": 1993
        },
        {
            "authors": [
                "G N Ramachandran",
                "C Ramakrishnan",
                "V Sasisekharan"
            ],
            "title": "Stereochemistry of polypeptide chain configurations",
            "venue": "Journal of Molecular Biology,",
            "year": 1963
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Molecular dynamics (MD) is a well-established technique for simulating physical systems at the atomic level. When performed accurately, it provides unrivalled insight into the detailed mechanics of molecular motion, without the need for wet lab experiments. MD simulations have been used to understand processes of central interest in molecular biophysics, such as protein folding [1, 2], protein-ligand binding [3], and protein-protein association [4]. Many crucial applications of MD boil down to efficiently sampling from the Boltzmann distribution, i.e., the equilibrium distribution of a\n\u2217Equal contribution. \u2020Work done while at Microsoft Research.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\nar X\niv :2\n30 2.\n01 17\n0v 2\n[ st\nat .M\nmolecular system at a temperature T . Let (xp(t), xv(t)) := x(t) \u2208 R6N be the state of the molecule at time t, consisting of the positions xp(t) \u2208 R3N and velocities xv(t) \u2208 R3N of the N atoms in Cartesian coordinates. The Boltzmann distribution is given by:\n\u00b5(xp, xv) \u221d exp ( \u2212 1kBT (U(x p) +K(xv)) ) , \u00b5(xp) = \u222b \u00b5(xp, xv) dxv. (1)\nwhere U(xp) is the potential energy, K(xv) is the kinetic energy, and kB is Boltzmann\u2019s constant. Many important quantities, such as the free energies of protein folding and protein-ligand binding, can be computed as expectations under \u00b5(xp). A popular MD method to sample from \u00b5(xp) is Langevin dynamics [5], which obeys the following stochastic differential equation (SDE):\nmidx v i = \u2212\u2207iUdt\u2212 \u03b3mixvi dt+ \u221a 2mi\u03b3kBTdBt. (2)\nHere i indexes the atoms, mi is the mass of atom i, U(xp) is the potential energy, \u03b3 is a friction parameter, and dBt is a standard Brownian motion process. Starting from an initial state x(0), simulating Equation (2), along with the relationship dxp = xvdt, yields values of x(t) that are distributed according to the Boltzmann distribution as t\u2192\u221e. Standard MD libraries discretise this SDE with a timestep \u2206t, which must be chosen to be \u223c 1fs = 10\u221215s for stability. Unfortunately, many biomolecules contain metastable states separated by energy barriers that can take milliseconds of MD simulation time (\u223c 1012 sequential integration steps) to cross, rendering this approach infeasible. To overcome this, prior work has produced an array of enhanced sampling methods, such as coarse graining [6, 7] and metadynamics [8]. However, these methods require domain knowledge specific to each molecular system to implement effectively.\nStandard MD simulations do not transfer information between molecular systems: for each system studied, a new simulation must be performed. This is a wasted opportunity: many molecular systems exhibit closely related dynamics, and simulating one system should yield information relevant to similar systems. In particular, proteins, being comprised of sequences of 20 kinds of amino acids, are prime candidates to study this kind of transferability. We propose Timewarp, a general, transferable enhanced sampling method which uses a normalising flow [9] as a proposal for a Markov chain Monte Carlo (MCMC) method targeting the Boltzmann distribution. Our main contributions are:\n1. We present the first ML algorithm working in general Cartesian coordinates that demonstrates transferability to small peptide systems unseen during training. 2. We demonstrate, for the first time, wall-clock acceleration of asymptotically unbiased MCMC sampling of the Boltzmann distribution of unseen peptide systems. 3. We define an MCMC algorithm targeting the Boltzmann distribution using a conditional normalising flow as a proposal distribution, with a Metropolis-Hastings (MH) correction step to ensure detailed balance (Section 3.4). 4. We design a permutation equivariant, transformer-based normalising flow. 5. We produce a novel training dataset of MD trajectories of thousands of small peptides. 6. We show that even when deployed without the MH correction (Section 3.5), Timewarp can\nbe used to explore metastable states of new peptides much faster than MD.\nThe code is available here: https://github.com/microsoft/timewarp. The datasets are available upon request3."
        },
        {
            "heading": "2 Related work",
            "text": "There has recently been a surge of interest in deep learning on molecules. Boltzmann generators [10, 11, 12] use flows to sample from the Boltzmann distribution asymptotically unbiased. There are two ways to generate samples: (i) Produce i.i.d. samples from the flow and use statistical reweighting to debias expectation values. (ii) Use the Boltzmann generator in an MCMC framework [13], as in Timewarp. Currently, Boltzmann generators lack the ability to generalize across multiple molecules, in contrast to Timewarp. The only exception is [14], who propose a diffusion model in torsion space and use the underlying ODE as a transferable Boltzmann generator. However, in contrast to Timewarp, they use internal coordinates and do not operate in the all atom system. Moreover, [15, 16] recently introduced a Boltzmann generators in Cartesian coordinates for molecules, potentially enabling transferable training. Recently, [17] proposed GeoDiff, a diffusion model that predicts molecular conformations from a molecular graph. Like Timewarp, GeoDiff works in Cartesian coordinates and generalises to unseen molecules. However, GeoDiff was not applied to proteins, but small molecules, and does not target the Boltzmann distribution. In contrast to Timewarp, [18] learn the transition probability for multiple time-resolutions, accurately capturing the dynamics. However, they do not show transferability between systems. Most similarly to Timewarp, in recent work, [19] trained a transferable ML model to simulate the time-coarsened dynamics of polymers. However, unlike Timewarp, their model acts on coarse grained representations. Additionally, it was not applied to proteins, and there is no MH step, which means that errors can accumulate in the simulation without being corrected.\nMarkov state models (MSMs) [20, 21, 22] work by running many short MD simulations, which are used to define a discrete state space, along with an estimated transition probability matrix. Similarly to Timewarp, MSMs estimate the transition probability between the state at a time t and the time t + \u03c4 , where \u03c4 \u226b \u2206t. Recent work has applied deep learning to MSMs, leading to VAMPnets [23] and deep generative MSMs [24], which replace the MSM data-processing pipeline with deep networks. In contrast to Timewarp, these models are not transferable and model the dynamics in a coarse-grained, discrete state space, rather than in the all-atom coordinate representation.\nThere has been much previous work on neural adaptive samplers [25, 26, 27], which use deep generative models as proposal distributions. A-NICE-MC [25] uses a volume-preserving flow trained using a likelihood-free adversarial method. Other methods use objective functions designed to encourage exploration. The entropy term in our objective function is inspired by [28]. In contrast to these methods, Timewarp focuses on generalising to new molecular systems without retraining.\nNumerous enhanced sampling methods exist to for MD, such as parallel tempering [29, 30] or proposing updates of collective variables along transition paths [8, 31]. Given Timewarp\u2019s ability to accelerate MD, it often offers the opportunity to be integrated with these techniques."
        },
        {
            "heading": "3 Method",
            "text": "Consider the distribution of x(t+\u03c4) induced by an MD simulation of Equation (2) for a time \u03c4 \u226b \u2206t, starting from x(t). We denote this conditional distribution by \u00b5(x(t + \u03c4)|x(t)). Timewarp uses a deep probabilistic model to approximate \u00b5(x(t+ \u03c4)|x(t)) (see Figure 1). Once trained, the model is used in an MCMC method to sample from the Boltzmann distribution."
        },
        {
            "heading": "3.1 Conditional normalising flows",
            "text": "We fit a conditional normalising flow, p\u03b8(x(t+ \u03c4)|x(t)), to \u00b5(x(t+ \u03c4)|x(t)), where \u03b8 are learnable parameters. Normalising flows are defined by a base distribution (usually a standard Gaussian), and a diffeomorphism f , i.e. a differentiable bijection with a differentiable inverse. Specifically, we set p\u03b8(x(t+ \u03c4)|x(t)) as the density of the distribution defined by the following generative process:\nzp, zv \u223c N (0, I), xp(t+ \u03c4), xv(t+ \u03c4) := f\u03b8(zp, zv;xp(t), xv(t)). (3) 3Please contact andrewfoong@microsoft.com for dataset access.\nHere zp \u2208 R3N and zv \u2208 R3N . For all settings of \u03b8 and x(t), f\u03b8( \u00b7 ;x(t)) is a diffeomorphism that takes the latent variables (zp, zv) \u2208 R6N to (xp(t+ \u03c4), xv(t+ \u03c4)) \u2208 R6N . The conditioning state x(t) parameterises a family of diffeomorphisms, defining a conditional normalising flow [32]. Note that there are no invertibility constraints on the mapping from x(t) to the output x(t+ \u03c4), only the map from z to x(t+ \u03c4) must be invertible. Using the change of variables formula, we can evaluate:\np\u03b8(x(t+ \u03c4)|x(t)) = N ( f\u22121\u03b8 (x(t+ \u03c4);x(t)); 0, I ) \u2223\u2223\u2223detJf\u22121\u03b8 (\u00b7;x(t))(x(t+ \u03c4))\u2223\u2223\u2223 , where f\u22121\u03b8 ( \u00b7 ;x(t)) : R6N \u2192 R6N is the inverse of the diffeomorphism f\u03b8( \u00b7 ;x(t)), and Jf\u22121\u03b8 (\u00b7;x(t))(x(t+ \u03c4)) denotes the Jacobian of f \u22121 \u03b8 ( \u00b7 ;x(t)) evaluated at x(t+ \u03c4)."
        },
        {
            "heading": "3.2 Dataset generation",
            "text": "We generate MD trajectories by integrating Equation (2) using the OpenMM library [33]. We simulate small proteins (peptides) in implicit water, i.e., without explicitly modelling the degrees of freedom of the water molecules. Specifically, we generate a dataset of trajectories D = {Ti}Pi=1, where P is the number of peptides. Each MD trajectory is temporally subsampled with a spacing of \u03c4 , so that Ti = (x(0), x(\u03c4), x(2\u03c4), . . .). During training, we randomly sample pairs x(t), x(t + \u03c4) from D. Each pair represents a sample from the conditional distribution \u00b5(x(t+ \u03c4)|x(t)). Additional details are provided in Appendix E. Since the flow is trained on trajectory data from multiple peptides, we can deploy it at test time to generalise to new peptides not seen in the training data."
        },
        {
            "heading": "3.3 Augmented normalising flows",
            "text": "We are typically primarily interested in the distribution of the positions xp, rather than the velocities xv. Thus, it is not necessary for xv(t), xv(t + \u03c4) to represent the actual velocities of the atoms in Equation (3). We hence simplify the learning problem by treating xv as non-physical auxiliary variables within the augmented normalising flow framework [34]. For each datapoint x(t) = xp(t), xv(t) in D, instead of obtaining xv(t) by recording the velocities in the MD trajectory, we discard the MD velocity and independently draw xv(t) \u223c N (0, I). The auxiliary variables xv(t) now contain no information about the future state xp(t + \u03c4), xv(t + \u03c4), since xv(t) and xv(t + \u03c4) are drawn independently. Hence we can simplify f\u03b8 to depend only on xp(t), with xp(t+ \u03c4), xv(t+ \u03c4) := f\u03b8(z\np, zv;xp(t)). We include auxiliary variables for two reasons: First, they increase the expressivity of the distribution for xp without a prohibitive increase in computational cost [34, 35]. Second, constructing a conditional flow that respects permutation equivariance is simplified with auxiliary variables \u2014 see Section 4.1."
        },
        {
            "heading": "3.4 Targeting the Boltzmann distribution with asymptotically unbiased MCMC",
            "text": "After training the flow p\u03b8(x(t+ \u03c4)|x(t)), we use it as a proposal distribution in an MCMC method to target the joint distribution of the positions xp and the auxiliary variables xv , which has density:\n\u00b5aug(x p, xv) \u221d exp ( \u2212U(x p) kBT ) N (xv; 0, I). (4)\nStarting from an initial state X0 = (X p 0 , X v 0 ) \u2208 R6N for state m = 0, we iterate: X\u0303m \u223c p\u03b8( \u00b7 |Xpm), Xm+1 := { X\u0303m with probability \u03b1(Xm, X\u0303m) Xm with probability 1\u2212 \u03b1(Xm, X\u0303m)\n(5)\nwhere \u03b1(Xm, X\u0303m) is the Metropolis-Hastings (MH) acceptance ratio [36] targeting Equation (4):\n\u03b1(X, X\u0303) = min ( 1, \u00b5aug(X\u0303)p\u03b8(X | X\u0303p) \u00b5aug(X)p\u03b8(X\u0303 | Xp) ) (6)\nThe flow used for p\u03b8 must allow for efficient sampling and exact likelihood evaluation, which is crucial for fast implementation of Equations (5) and (6). Additionally, after each MH step, we resample the auxiliary variables Xv using a Gibbs sampling update:\n(Xpm, X v m)\u2190 (Xpm, \u03f5), \u03f5 \u223c N (0, I). (7)\nIterating these updates yields a sample Xpm, X v m \u223c \u00b5aug as m \u2192 \u221e. To obtain a Boltzmanndistributed sample of the positions Xpm \u223c \u00b5, we simply discard the auxiliary variables Xvm. As sending m\u2192\u221e is infeasible, we simulate the chain until a fixed budget is reached. In practice, we find that acceptance rates for our models can be low, around 1%. However, we stress that even with a low acceptance rate, our models can lead to faster exploration if the changes proposed are large enough, as we demonstrate in Section 6. Furthermore, we introduce a batch sampling procedure which significantly speeds up sampling whilst maintaining detailed balance. This procedure samples a batch of proposals with a single forward pass, and accepts the first proposal that meets the MH acceptance criterion. Pseudocode for the MCMC algorithm is given in Algorithm 1 in Appendix C."
        },
        {
            "heading": "3.5 Fast but biased exploration of the state space without MH corrections",
            "text": "Instead of using the MH correction to guarantee asymptotically unbiased samples, we can opt to use Timewarp in a simple exploration algorithm. In this case, we neglect the MH correction and accept all proposals with energy changes below some cutoff. This allows much faster exploration of the state space, and in Section 6 we show that, although technically biased, this often leads to qualitatively accurate free energy estimates. It also succeeds in discovering all metastable states orders of magnitude faster than Algorithm 1 and standard MD, which could be used, e.g., to provide initialisation states for a subsequent MSM method. Pseudocode is given in Algorithm 2 in Appendix D."
        },
        {
            "heading": "4 Model architecture",
            "text": "We now describe the architecture of the flow f\u03b8(zp, zv;xp(t)), which is shown in Figure 2.\nRealNVP coupling flow Our architecture is based on RealNVP [37], which consists of a stack of coupling layers which affinely transform subsets of the dimensions of the latent variable based on the other dimensions. Specifically, we transform the position variables based on the auxiliary variables, and vice versa. In the \u2113th coupling layer of the flow, the following transformations are implemented:\nzp\u2113+1 = s p \u2113,\u03b8(z v \u2113 ;x p(t))\u2299 zp\u2113 + t p \u2113,\u03b8(z v \u2113 ;x p(t)), (8)\nzv\u2113+1 = s v \u2113,\u03b8(z p \u2113+1;x p(t))\u2299 zv\u2113 + tv\u2113,\u03b8(z p \u2113+1;x p(t)). (9)\nGoing forward, we suppress the coupling layer index \u2113. Here \u2299 is the element-wise product, and sp\u03b8 : R3N \u2192 R3N is our atom transformer, a neural network based on the transformer architecture [38] that takes the auxiliary latent variables zv and the conditioning state x(t) and outputs scaling factors for the position latent variables zp. The function tp\u03b8 : R3N \u2192 R3N is implemented as another atom transformer, which uses zv and x(t) to output a translation of the position latent variables zp. The affine transformations of the position variables (in Equation (8)) are interleaved with similar affine transformations for the auxiliary variables (in Equation (9)). Since the scale and translation factors for the positions depend only on the auxiliary variables, and vice versa, the Jacobian of the transformation is lower triangular, allowing for efficient computation of the density. The full flow f\u03b8 consists of Ncoupling stacked coupling layers, beginning from z \u223c N (0, I) and ending with a sample from p\u03b8(x(t+ \u03c4)|x(t)). This is depicted in Figure 2, Left. Note that there is a skip connection such that the output of the flow predicts the change x(t+ \u03c4)\u2212 x(t), rather than x(t+ \u03c4) directly.\nAtom transformer We now describe the atom transformer network. Let xpi (t), z p i , z v i , all elements of R3, denote respectively the position of atom i in the conditioning state, the position latent variable for atom i, and the auxiliary latent variable for atom i. To implement an atom transformer which takes zv as input (such as sp\u03b8(z v, xp(t)) and tp\u03b8(z v, xp(t)) in Equation (8)), we first concatenate the variables associated with atom i. This leads to a vector ai := [x p i (t), hi, z v i ]\nT \u2208 RH+6, where zpi has been excluded since sp\u03b8, t p \u03b8 are not allowed to depend on z\np. Here hi \u2208 RH is a learned embedding vector which depends only on the atom type. The vectors ai are fed into an MLP \u03d5in : RH+6 \u2192 RD, where D is the feature dimension of the transformer. The vectors \u03d5in(a1), . . . , \u03d5in(aN ) are then fed into Ntransformer stacked transformer layers. After the transformer layers, they are passed through another atom-wise MLP, \u03d5out : RD \u2192 R3. The final output is in R3N as required. This is depicted in Figure 2, Middle. When implementing sv\u03b8 and t v \u03b8 from Equation (9), a similar procedure is performed on the vector [xpi (t), hi, z p i ] T, but now including zpi and excluding z v i . There are two key differences between the atom transformer and the architecture in [38]. First, to maintain permutation equivariance,\nwe do not use a positional encoding. Second, instead of dot product attention, we use a simple kernel self-attention module, which we describe next.\nKernel self-attention We motivate the kernel self-attention module with the observation that physical forces acting on the atoms in a molecule are local: i.e., they act more strongly on nearby atoms. Intuitively, for values of \u03c4 that are not too large, the positions at time t + \u03c4 will be more influenced by atoms that are nearby at time t, compared to atoms that are far away. Thus, we define the attention weight wij for atom i attending to atom j as follows:\nwij = exp(\u2212\u2225xpi \u2212 x p j\u222522/\u21132)\u2211N\nj\u2032=1 exp(\u2212\u2225x p i \u2212 x p j\u2032\u222522/\u21132)\n, (10)\nwhere \u2113 is a lengthscale parameter. The outputs {rout,i}Ni=1, given the inputs {rin,i}Ni=1, are then:\nrout,i = \u2211N j=1 wijV \u00b7 rin,j , (11)\nwhere V \u2208 Rdout\u00d7din is a learnable matrix. This kernel self-attention is an instance of the RBF kernel attention investigated in [39]. Similarly to [38], we introduce a multihead version, where each head has a different lengthscale. This is illustrated in Figure 2, Right. We found that kernel self-attention was significantly faster to compute than dot product attention, and performed similarly."
        },
        {
            "heading": "4.1 Symmetries",
            "text": "The MD dynamics respects certain physical symmetries that would be advantageous to incorporate. We now describe how each of these symmetries is incorporated in Timewarp.\nPermutation equivariance Let \u03c3 be a permutation of the N atoms. Since the atoms have no intrinsic ordering, the only effect of a permutation of x(t) on the future state x(t+ \u03c4) is to permute the atoms similarly, i.e.,\n\u00b5(\u03c3x(t+ \u03c4)|\u03c3x(t)) = \u00b5(x(t+ \u03c4)|x(t)). (12)\nOur conditional flow satisfies permutation equivariance exactly. To show this, we use the following proposition proved in Appendix A.1, which is an extension of [40, 41] for conditional flows:\nProposition 4.1. Let \u03c3 be a symmetry action, and let f( \u00b7 ; \u00b7 ) be an equivariant map such that f(\u03c3z;\u03c3x) = \u03c3f(z;x) for all \u03c3, z, x. Further, let the base distribution p(z) satisfy p(\u03c3z) = p(z) for all \u03c3, z. Then the conditional flow defined by z \u223c p(z), x(t + \u03c4) := f(z;x(t)) satisfies p(\u03c3x(t+ \u03c4)|\u03c3x(t)) = p(x(t+ \u03c4)|x(t)).\nOur flow satisfies f\u03b8(\u03c3z;\u03c3x(t)) = \u03c3f\u03b8(z;x(t)) since the transformer is permutation equivariant, and permuting z and x(t) together permutes the inputs. Furthermore, the base distribution p(z) = N (0, I) is permutation invariant. Note that the presence of auxiliary variables allows us to easily construct a permutation equivariant coupling layer.\nTranslation and rotation equivariance Consider a transformation T = (R, a) that acts on xp:\nTxpi = Rx p i + a, 1 \u2264 i \u2264 N, (13)\nwhere R is a 3\u00d7 3 rotation matrix, and a \u2208 R3 is a translation vector. We would like the model to satisfy p\u03b8(Tx(t+\u03c4)|Tx(t)) = p\u03b8(x(t+\u03c4)|x(t)). We achieve translation equivariance by subtracting the average position of the atoms in the initial state (Appendix A.2). Rotation equivariance is not encoded in the architecture but is handled by data augmentation: each training pair (x(t), x(t+ \u03c4)) is acted upon by a random rotation matrix R to form (Rx(t), Rx(t+ \u03c4)) in each iteration."
        },
        {
            "heading": "5 Training objective",
            "text": "The model is trained in two stages: (i) likelihood training, the model is trained via maximum likelihood on pairs of states from the trajectories in the dataset. Let k index training pairs, such that {(x(k)(t), x(k)(t+ \u03c4))}Kk=1 represents all pairs of states at times \u03c4 apart in D. We optimise:\nLlik(\u03b8) := 1K \u2211K k=1 log p\u03b8(x (k)(t+ \u03c4)|x(k)(t)). (14)\n(ii) acceptance training, the model is fine-tuned to maximise the probability of MH acceptance. Let x(k)(t) be sampled uniformly fromD. Then, we use the model to sample x\u0303(k)\u03b8 (t+\u03c4) \u223c p\u03b8( \u00b7 |x(k)(t)) using Equation (3). We use this to optimise the acceptance probability in Equation (6) with respect to \u03b8. Let r\u03b8(X, X\u0303) denote the model-dependent term in the acceptance ratio in Equation (6):\nr\u03b8(X, X\u0303) := \u00b5aug(X\u0303)p\u03b8(X | X\u0303p) \u00b5aug(X)p\u03b8(X\u0303 | Xp) . (15)\nThe acceptance objective is then given by: Lacc(\u03b8) := 1K \u2211K k=1 log r\u03b8(x (k)(t), x\u0303 (k) \u03b8 (t+ \u03c4)). (16)\nTraining to maximise the acceptance probability can lead to the model proposing changes that are too small: if x\u0303(k)\u03b8 (t + \u03c4) = x\n(k)(t), then all proposals will be accepted. To mitigate this, during acceptance training, we use an objective which is a weighted average of Lacc(\u03b8), Llik(\u03b8) and a Monte Carlo estimate of the average differential entropy,\nLent(\u03b8) := \u2212 1K \u2211K k=1 log p\u03b8(x\u0303 (k) \u03b8 (t+ \u03c4)|x(k)(t)). (17)"
        },
        {
            "heading": "6 Experiments",
            "text": "We evaluate Timewarp on small peptide systems. To compare with MD, we focus on the slowest transitions between metastable states, as these are the most difficult to traverse. To find these, we use time-lagged independent component analysis (TICA) [42], a linear dimensionality reduction technique that maximises the autocorrelation of the transformed coordinates. The slowest components, TIC 0 and TIC 1, are of particular interest. To measure the speed-up achieved by Timewarp, we compute the effective sample size per second of wall-clock time (ESS/s) for the TICA components. The ESS/s is given by\nESS/s = Meff tsampling = M tsampling (1 + 2 \u2211\u221e \u03c4=1 \u03c1\u03c4 ) , (18)\nwhere M is the chain length, Meff is the effective number of samples, tsampling is the sampling wallclock time, and \u03c1\u03c4 is the autocorrelation for the lag time \u03c4 [43]. The speed-up factor is defined as the ESS/s achieved by Timewarp divided by the ESS/s achieved by MD. Additional experiments and results can be found in Appendix B. We train three flow models on three datasets: (i) AD, consisting of simulations of alanine dipeptide, (ii) 2AA, with peptides with 2 amino acids, and (iii) 4AA, with peptides with 4 amino acids. All datasets are created with MD simulations performed with the same parameters (see Appendix E). For 2AA and 4AA, we train on a randomly selected trainset of short trajectories (50ns = 108 steps), and evaluate on unseen test peptides. The relative frequencies of the amino acids in 2AA and 4AA are similar across the splits. For 4AA, the training set consists of about 1% of the total number of possible tetrapeptides (204), making the generalisation task significantly more difficult than for 2AA. For more details see Table 2 in Appendix E.\nAlanine dipeptide (AD) We first investigate alanine dipeptide, a small (22 atoms) single peptide molecule. We train Timewarp on AD as described in Section 5 and sample new states using Algorithm 1 for a chain length of 10 million, accepting roughly 2% of the proposals. In Figure 3a we visualise the samples using a Ramachandran plot [44], which shows the distribution of the backbone dihedral angles \u03c6 and \u03c8. Each mode in the plot represents a metastable state. We see that the Timewarp samples closely match MD, visiting all the metastable states with the correct relative weights. In Figure 3b we plot the free energy (i.e., the relative log probability) of the \u03c6 and \u03c8 angles, again showing close agreement. The roughness in the plot is due to some regions of state space having very few samples. In Figure 3c we show, for an initial state x(t), the conditional distribution of MD obtained by integrating Equation (2), \u00b5(x(t+ \u03c4)|x(t)), compared with the model p\u03b8(x(t+ \u03c4)|x(t)), demonstrating close agreement. Finally, Figure 3d shows the time-evolution of the \u03c6 angle for MD and Timewarp. Timewarp exhibits significantly more transitions between the metastable states than MD. As a result, the autocorrelation along the \u03c6 angle decays much faster in terms of wall-clock time, resulting in a \u2248 7\u00d7 speed-up in terms of ESS/s compared to MD (see Appendix B.4).\nDipeptides (2AA) Next, we demonstrate transferability on dipeptides in 2AA. After training on the train dipeptides, we deploy Timewarp with Algorithm 1 on the test dipeptides for a chain length of 20 million. Timewarp achieves acceptance probabilities between 0.03% and 2% and explores all metastable states (Appendix B.1). The results are shown for the dipeptides QW and HT in Figure 3ef, showing close agreement between Timewarp and long MD chains (1 \u00b5s = 2\u00d7 109 steps). For these dipeptides, Timewarp achieves ESS/s speed-up factors over MD of 5 and 33 respectively (Appendix B.4). In Figure 4, Left, we show the speed-up factors for Timewarp verses MD for each of the 100 test dipeptides. Timewarp provides a median speed-up factor of about five across these peptides. In addition, we generate samples with the Timewarp model without the MH correction as detailed in Section 3.5. We sample 100 parallel chains for only 10000 steps starting from the same\ninitial state for each test peptide. For each peptide we select only one of these chains that finds all meta-stable states for evaluations. As before, we compute the ESS/s to compare with MD, showing a median speedup factor of \u2248 600 (Figure 4c). Note that the actual speedup when using all the chains sampled in parallel will be much larger. Timewarp exploration leads to free energy estimates that are qualitatively similar to MD, but less accurate than Timewarp MCMC (Figure 3f).\nTetrapeptides (4AA) Finally, we study the more challenging 4AA dataset. After training on the trainset, we sample 20 million Markov chain states for each test tetrapeptide using Algorithm 1 and compare with long MD trajectories (1\u00b5s). In contrast to the simpler dipeptides, both Timewarp MCMC and the long MD trajectories miss some metastable states. However, Timewarp in exploration mode (Algorithm 2) can be used as a validation tool to quickly verify exploration of the whole state space. Figure 5a shows that metastable states unexplored by MD and Timewarp MCMC can be found by the Timewarp exploration algorithm. We carefully confirm the physical validity of these discovered states by running shorter MD trajectories in their vicinity (see Appendix B.5), to ensure that they are not simply artefacts invented by the model. As with 2AA, we again report the speedup factors for Timewarp relative to MD in Figure 4b,d. Although Timewarp MCMC fails to speed up sampling for most tetrapeptides, Timewarp exploration shows a median speedup factor of \u2248 50. For 8 test tetrapeptides, MD fails to explore all metastable states, whereas Timewarp succeeds \u2014 these are marked in green. For 10 tetrapeptides, Timewarp MCMC fails to find all metastable states found by MD \u2014 these are marked in grey. Figure 5b shows that when Timewarp MCMC discovers all metastable states, its free energy estimates match those of MD very well. However, it sometimes\nmisses metastable states leading to poor free energy estimates in those regions. Figure 5c shows that Timewarp MCMC also leads to a potential energy distribution that matches MD very closely. In contrast, Timewarp exploration discovers all metastable states (even ones that MD misses), but has less accurate free energy plots. It also has a potential energy distribution that is slightly too high relative to MD and Timewarp MCMC."
        },
        {
            "heading": "7 Limitations / Future work",
            "text": "The Timewarp MCMC algorithm generates low acceptance probabilities (< 1%) for most peptides (see Appendix B.1). However, this is not a limitation in itself. In general, a larger proposal timestep \u03c4 yields smaller acceptance rates as the prediction problem becomes more difficult. However, due to Algorithm 1, we can evaluate multiple samples in parallel at nearly no additional costs. As a result, a lower acceptance rate, when coupled with a larger timestep \u03c4 , is often a favorable trade-off. While we speed-up only roughly a third of the 4AA peptides when using the MH correction, beating MD in wall-clock time on unseen peptides in the all-atom representation is a challenging task which has not been demonstrated by ML methods before. Furthermore, one could consider targeting systems using a semi-empirical force field instead of a classical one. Given that Timewarp requires considerably fewer energy evaluations than MD simulations, one can anticipate a more substantial acceleration in this context.\nAlthough MD and Timewarp MCMC fail to find some metastable states that were found by Timewarp exploration, we refrained from running MD and Timewarp MCMC longer due to the high computational cost (Appendix F). Timewarp generates fewer samples compared to traditional MD simulations within the same timeframe. Consequently, this scarcity of samples becomes even more pronounced in transition states, which makes Timewarp difficult to apply to chemical reactions.\nTimewarp could be integrated with other enhanced sampling methods, like parallel tempering or transition path sampling. In the case of parallel tempering, the effective integration requires the training of the Timewarp model across multiple temperatures, which then allows to sample all the replicas with Timewarp instead of MD. We could also alternate Timewarp proposals with learned updates to collective variables, like dihedral angles. These combined steps would still allow unbiased sampling from the target distribution [31].\nMoreover, we only studied small peptide systems in this work. Scaling Timewarp to larger systems remains a topic for future research, and there are several promising avenues to consider. One approach is to explore different network architectures, potentially capturing all the symmetries inherent in the system. Another option is to study coarse-grained structures instead of all-atom representations, to reduce the dimensionality of the problem."
        },
        {
            "heading": "8 Conclusion",
            "text": "We presented Timewarp, a transferable enhanced sampling method which uses deep networks to propose large conformational changes when simulating molecular systems. We showed that Timewarp used with an MH correction can accelerate asymptotically unbiased sampling on many unseen dipeptides, allowing faster computation of equilibrium expectation values. Although this acceleration was only possible for a minority of the tetrapeptides we considered, we showed that Timewarp used without the MH correction explores the metastable states of both dipeptides and tetrapeptides much faster than standard MD, and we verify the metastable states discovered are physically meaningful. This provides a promising method to quickly validate if MD simulations have visited all metastable states. Although further work needs to be done to scale Timewarp to larger, more interesting biomolecules, this work clearly demonstrates the ability of deep learning algorithms to leverage transferability to accelerate the MD sampling problem."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank Bas Veeling, Claudio Zeni, Andrew Fowler, Lixin Sun, Chris Bishop, Rianne van den Berg, Hannes Schulz, Max Welling and the entire Microsoft AI4Science team for insightful discussions and computing help."
        },
        {
            "heading": "A Symmetries of the architecture",
            "text": "A.1 Proof of Proposition 4.1\nIn this appendix we provide more details on the equivariance of the Timewarp architecture. We first prove Proposition 4.1 from the main body:\nProof. Let X(t+ \u03c4)x(t) denote the random variable obtained by sampling Z \u223c p(z) and computing X(t+\u03c4) := f(Z;x(t)). Here we subscript X(t+\u03c4)x(t) by x(t) to emphasize that this is the random variable obtained when conditioning the flow on x(t). We first note that the equivariance condition on the densities p(\u03c3x(t+ \u03c4)|\u03c3x(t)) = p(x(t+ \u03c4)|x(t)) is equivalent to the following constraint on the random variables:\nX(t+ \u03c4)\u03c3x(t) d = \u03c3X(t+ \u03c4)x(t), (19)\nwhere d= denotes equality in distribution. To see this, let pX denote the density of the random variable X . Then, in terms of densities, Equation (19) is equivalent to stating that, for all x,\npX(t+\u03c4)\u03c3x(t)(x) = p\u03c3X(t+\u03c4)x(t)(x) (20)\n= pX(t+\u03c4)x(t)(\u03c3 \u22121x), (21)\nwhere in Equation (21) we used the change-of-variables formula, along with the fact that the group actions we consider (rotations, translations, permutations) have unit absolute Jacobian determinant. Redefining x\u2190 \u03c3x, we get that for all x,\npX(t+\u03c4)\u03c3x(t)(\u03c3x) = pX(t+\u03c4)x(t)(x). (22)\nRecalling the notation thatX(t+\u03c4)x(t) is interpreted as the random variable obtained by conditioning the flow on x(t), this can be written as\np(\u03c3x|\u03c3x(t)) = p(x|x(t)) (23) which is exactly the equivariance condition stated in terms of densities above. Having rephrased the equivariance condition in terms of random variables in Equation (19), the proof of Proposition 4.1 is straightforward.\nX(t+ \u03c4)\u03c3x(t) := f(Z, \u03c3x(t)) (24) d = f(\u03c3Z, \u03c3x(t)) (25) = \u03c3f(Z, x(t)) (26) := \u03c3X(t+ \u03c4)x(t), (27)\nwhere in Equation (25) we used the fact that the base distribution p(z) is \u03c3-invariant.\nA.2 Translation equivariance via canonicalisation\nWe now describe the canonicalisation technique used to make our models translation equivariant. Let q(xp(t + \u03c4), xv(t + \u03c4)|xp(t)) be an arbitrary conditional density model, which is not necessarily translation equivariant. We can make it translation equivariant in the following way. Let xp denote the average position of the atoms,\nxp := 1\nN N\u2211 i=1 xpi . (28)\nThen we define p(xp(t+ \u03c4), xv(t+ \u03c4)|xp(t)) := q(xp(t+ \u03c4)\u2212 xp(t), xv(t+ \u03c4)|xp(t)\u2212 xp(t)) (29)\nwhere the subtraction of xp(t) is broadcasted over all atoms. We now consider the effect of translating both xp(t) and xp(t+ \u03c4) by the same amount. Let a be a translation vector in R3. Then p(xp(t+ \u03c4) + a,xv(t+ \u03c4)|xp(t) + a) (30)\n= q(xp(t+ \u03c4) + a\u2212 (xp(t) + a), xv(t+ \u03c4)|xp(t) + a\u2212 (xp(t) + a)) (31) = q(xp(t+ \u03c4) + a\u2212 xp(t)\u2212 a, xv(t+ \u03c4)|xp(t) + a\u2212 xp(t)\u2212 a) (32) = q(xp(t+ \u03c4)\u2212 xp(t), xv(t+ \u03c4)|xp(t)\u2212 xp(t)) (33) = p(xp(t+ \u03c4), xv(t+ \u03c4)|xp(t)). (34)\nHence p is translation equivariant even if q is not.\nA.3 Chirality\nIn addition to the symmetries described in Section 4.1 the potential energy U(xp) of a molecular configuration is also invariant under mirroring. However, in the presence of chirality centers, a mirrored configuration is non-superposable to its original image [45]. An example of a chirality center in an amino acid is a Carbon atom connected to four different groups, e.g. a C\u03b1 atom. In nature most amino acids come in one form, namely L-amino acids. Hence, all our datasets consist of peptides containing only L-amino acids. In rare cases, as the model proposes large steps, one step might change one L-amino acid of a peptide to a D-amino acid in a way that the resulting configuration has a low energy and the step would be accepted. We prevent this by checking all chirality centers for changes at each step and reject samples where such a change occurs. This does not add any significant computational overhead."
        },
        {
            "heading": "B Additional results",
            "text": "In this section we show additional results like the conditional distribution as well as more peptide examples for experiments discussed in Section 6.\nB.1 2AA additional results\nMore examples from the 2AA test set are presented in Figures 7 and 8. We achieve the worst speed-up for the dipeptide GP ( Figure 7 last row) as it does not show any slow transitions. The distribution of the acceptance probabilities for the Timewarp MCMC algorithm is shown in Figure 6\nB.2 4AA additional results\nMore examples from the 4AA test set are presented in Figures 9 and 11. The distribution of the acceptance probabilities for the Timewarp MCMC algorithm is shown in Figure 6.\nB.3 Conditional distributions\nThe model was trained to generate samples from the conditional Boltzmann distribution \u00b5(x(t + \u03c4)|x(t)). Here we show some examples of the conditional distribution generated by the Timewarp model compared to the conditional distribution induced by MD. While we can generate 5, 000 samples from the conditional distribution of the model in parallel, we require 5, 000 distinct MD trajectories of simulation length \u03c4 to sample the conditional distribution with MD. Hence, generating samples from the conditional distribution is several orders of magnitude faster with the model. In Figures 10 to 12 we show example conditional distributions for alanine dipeptide and peptides from the 2AA and 4AA datasets. For all peptides the model learns a conditional distribution that is close to the conditional MD distribution. Moreover, the relative weights in the TICA projections as well as the bondlength distributions match very well. Only the energies of the model samples are higher,\nemphasising the importance of the Metropolis-Hastings correction to obtain unbiased samples from the Boltzmann distribution with the Timewarp model.\nB.4 Autocorrelations\nIn Section 6 we compute the speedup of the Timewarp model by comparing the effective sample sizes per second (Equation (18)) for the slowest transition with MD. As the ESS depends on the autocorrelation, it is also insightful to look at the autocorrelation decay in terms of wall-clock time. We show some example autocorrelations for the investigated peptides in Figures 7, 8 and 10. Note that the area under the autocorrleation curve is inversely proportional to the ESS.\nB.5 Exploration of new metastable states\nFor some tetrapetides in the test set even long MD trajectories (1\u00b5s) miss some metastable states, e.g. for LYVI and CSTA shown in Figure 12a. However, we can easily explore these with the Timewarp exploration algorithm (Algorithm 2). To confirm that these additional metastable states are in fact\nstable, we run several shorter MD trajectories (0.5 \u00d7 106fs), in the same way as in Appendix B.3, starting in a nearby metastable state already discovered with the long MD trajectory (Figure 12b). Once one of them hits the new, previously undiscovered state, we start new short MD trajectories (0.5\u00d7 106fs) from there as well (Figure 12c). These new short MD trajectories either sample within this previously undiscovered state, or transition to the other metastable states. This shows that this metastable state discovered by Timewarp exploration is indeed valid, and was simply undiscovered during the long MD trajectory. In addition, we compare in Figure 12 the conditional MD distributions with that of Timewarp, again showing close agreement."
        },
        {
            "heading": "C Batched sampling algorithm for Timewarp with MH corrections",
            "text": "Pseudocode for the algorithm described in Section 3.4 is given in Algorithm 1."
        },
        {
            "heading": "D Exploration of metastable states using Timewarp without MH corrections",
            "text": "We describe the exploration algorithm for Timewarp, which accepts all proposed states unless the energy is above a certain cutoff value. As there is no MH correction, the generated samples will not asymptotically follow the Boltzmann distribution, but the exploration of the state space is much faster than with MD or Algorithm 1. The pseudocode is shown in Algorithm 2:\nAlgorithm 1 Timewarp MH-corrected MCMC with batched proposals Require: Initial state X0 = (Xp0 , Xv0 ), chain length M , proposal batch size B. m\u2190 0 while m < M do\nSample X\u03031, . . . , X\u0303B \u223c p\u03b8( \u00b7 |Xpm) {Batch sample} for b = 1, . . . , B do \u03f5 \u223c N (0, I) {Resample auxiliary variables} Xb \u2190 (Xpm, \u03f5) Sample Ib \u223c Bernoulli(\u03b1(Xb, X\u0303b)) end for if S := {b : Ib = 1, 1 \u2264 b \u2264 B} =\u0338 \u2205 then a = min(S) {First accepted sample} (Xpm+1, . . . , X p m+a\u22121)\u2190 (Xpm, . . . , Xpm)\nXpm+a \u2190 X\u0303pa m\u2190 m+ a\nelse (Xpm+1, . . . , X p m+B)\u2190 (Xpm, . . . , Xpm)\nm\u2190 m+B end if\nend while output Xp0 , . . . X p M\nAlgorithm 2 Fast, biased exploration of the state space with Timewarp Require: Initial state Xp0 , number of steps M , maximum allowed energy increase \u2206Umax\nfor m = 0, . . . ,M do Sample X\u0303pm \u223c p\u03b8(\u00b7 | Xpm) {Sample from conditional flow} if U(X\u0303pm)\u2212 U(Xpm) < \u2206Umax then Xpm+1 \u2190 X\u0303pm\nelse Xpm+1 \u2190 Xpm {Reject if energy change is too high}\nend if end for\noutput Xp0 , . . . X p M\nNote that unlike Algorithm 1, there is no need for the auxiliary variables, since the conditional flow only depends on the positions, and no MH acceptance ratio is computed here. The potential energy U includes here also a large penalty if the ordering of a chirality center changes as described in Appendix A.3. As sampling proposals from Timewarp can be batched, we can generate B chains in parallel, all starting from the same initial state. This batched sampling procedure leads to even further speedups. For all exploration experiments we use a batch size of 100, and run M = 10000 exploration steps. The maximum allowed energy change cutoff is set at \u2206Umax = 300kJ/mol."
        },
        {
            "heading": "E Dataset details",
            "text": "We evaluate our model on three different datasets, AD, 2AA, and 4AA, as introduced in Section 6. All datasets are simulated in implicit solvent using the openMM library [33]. For all MD simulations we use the parameters shown in Table 1.\nWe present more dataset details, like simulation times and number of peptides, in Table 2."
        },
        {
            "heading": "F Hyperparameters",
            "text": "Depending on the dataset, different Timewarp model sizes were used, as shown in Table 3. For all datasets the Multihead kernel self-attention layer consists of 6 heads with lengthscales \u2113i = {0.1, 0.2, 0.5, 0.7, 1.0, 1.2}, given in nanometers.\nThe \u03d5in and \u03d5out MLPs use SiLUs as activation functions, while the Transformer MLPs use ReLUs. Note the transformer MLP refers to the atom-wise MLP shown in Figure 2, Middle inside the transformer block. The shapes of these MLPs vary for the different datasets as shown in Table 4.\nThe first linear layers in the kernel self-attention module always has the shape [128, 768] (in Section 4 denoted as V ), and the second (after concatenating the output of head head) has the shape [768, 128]. The transformer feature dimension D is for all datasets 128.\nAfter likelihood training, we fine-tune the model for the AD and 2AA dataset with a combination of all three losses discussed in Section 5. We did not perform fine tuning for the model trained on the 4AA dataset. We use a weighted sum of the losses with weights detailed in Table 5. We use the\nFusedLamb optimizer and the DeepSpeed library [46] for all experiments. The batch size as well as the training times are reported in Table 6. All simulations are started with a learning rate of 5\u00d7 10\u22124,\nthe learning rate is then consecutively decreased by a factor of 2 upon hitting training loss plateaus."
        },
        {
            "heading": "G Computing infrastructure",
            "text": "The training was performed on 4 NVIDIA A-100 GPUs for the 2AA and 4AA datasets and on a single NVIDIA A-100 GPU for the AD dataset. Inference with the model as well as all MD simulations were conducted on single NVIDIA V-100 GPUs for AD and 2AA, and on single NVIDIA A-100 GPUs for 4AA."
        }
    ],
    "title": "Timewarp: Transferable Acceleration of Molecular Dynamics by Learning Time-Coarsened Dynamics",
    "year": 2023
}