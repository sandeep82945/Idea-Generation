{
    "abstractText": "In recent works, utilizing a deep network trained on metatraining set serves as a strong baseline in few-shot learning. In this paper, we move forward to refine novel-class features by finetuning a trained deep network. Finetuning is designed to focus on reducing biases in novel-class feature distributions, which we define as two aspects: class-agnostic and class-specific biases. Class-agnostic bias is defined as the distribution shifting introduced by domain difference, which we propose Distribution Calibration Module(DCM) to reduce. DCM owes good property of eliminating domain difference and fast feature adaptation during optimization. Classspecific bias is defined as the biased estimation using a few samples in novel classes, which we propose Selected Sampling(SS) to reduce. Without inferring the actual class distribution, SS is designed by running sampling using proposal distributions around support-set samples. By powering finetuning with DCM and SS, we achieve state-of-the-art results on Meta-Dataset with consistent performance boosts over ten datasets from different domains. We believe our simple yet effective method demonstrates its possibility to be applied on practical few-shot applications.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ran Tao"
        },
        {
            "affiliations": [],
            "name": "Han Zhang"
        },
        {
            "affiliations": [],
            "name": "Yutong Zheng"
        },
        {
            "affiliations": [],
            "name": "Marios Savvides"
        }
    ],
    "id": "SP:248ade4f2e0b0308fbd94343c6a89b18820bba95",
    "references": [
        {
            "authors": [
                "L. Bertinetto",
                "J.F. Henriques",
                "P.H. Torr",
                "A. Vedaldi"
            ],
            "title": "Meta-learning with differentiable closed-form solvers",
            "venue": "arXiv preprint arXiv:1805.08136.",
            "year": 2018
        },
        {
            "authors": [
                "W.-Y. Chen",
                "Y.-C. Liu",
                "Z. Kira",
                "Y.-C.F. Wang",
                "J.-B. Huang"
            ],
            "title": "A Closer Look at Few-shot Classification",
            "venue": "arXiv:1904.04232.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Chen",
                "X. Wang",
                "Z. Liu",
                "H. Xu",
                "T. Darrell"
            ],
            "title": "A new meta-baseline for few-shot learning",
            "venue": "arXiv preprint arXiv:2003.04390.",
            "year": 2020
        },
        {
            "authors": [
                "G.S. Dhillon",
                "P. Chaudhari",
                "A. Ravichandran",
                "S. Soatto"
            ],
            "title": "A baseline for few-shot image classification",
            "venue": "arXiv preprint arXiv:1909.02729.",
            "year": 2019
        },
        {
            "authors": [
                "C. Doersch",
                "A. Gupta",
                "A. Zisserman"
            ],
            "title": "CrossTransformers: spatially-aware few-shot transfer",
            "venue": "arXiv preprint arXiv:2007.11498.",
            "year": 2020
        },
        {
            "authors": [
                "C. Finn",
                "P. Abbeel",
                "S. Levine"
            ],
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "venue": "Proceedings of the 34th International Conference on Machine Learning-Volume 70, 1126\u20131135. JMLR. org.",
            "year": 2017
        },
        {
            "authors": [
                "V. Garcia",
                "J. Bruna"
            ],
            "title": "Few-shot learning with graph neural networks",
            "venue": "arXiv preprint arXiv:1711.04043.",
            "year": 2017
        },
        {
            "authors": [
                "S. Gidaris",
                "A. Bursuc",
                "N. Komodakis",
                "P. P\u00e9rez",
                "M. Cord"
            ],
            "title": "Boosting Few-Shot Visual Learning with SelfSupervision",
            "venue": "CoRR, abs/1906.05186.",
            "year": 2019
        },
        {
            "authors": [
                "S. Gidaris",
                "N. Komodakis"
            ],
            "title": "Dynamic few-shot visual learning without forgetting",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 4367\u20134375.",
            "year": 2018
        },
        {
            "authors": [
                "S. Gidaris",
                "N. Komodakis"
            ],
            "title": "Generating Classification Weights with GNN Denoising Autoencoders for FewShot Learning",
            "venue": "arXiv preprint arXiv:1905.01102.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Guo",
                "N.-M. Cheung"
            ],
            "title": "Attentive Weights Generation for Few Shot Learning via Information Maximization",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 13499\u201313508.",
            "year": 2020
        },
        {
            "authors": [
                "B. Hariharan",
                "R. Girshick"
            ],
            "title": "Low-shot visual recognition by shrinking and hallucinating features",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision, 3018\u20133027.",
            "year": 2017
        },
        {
            "authors": [
                "R. Hou",
                "H. Chang",
                "M. Bingpeng",
                "S. Shan",
                "X. Chen"
            ],
            "title": "Cross Attention Network for Few-shot Classification",
            "venue": "Advances in Neural Information Processing Systems, 4005\u20134016.",
            "year": 2019
        },
        {
            "authors": [
                "J. Kim",
                "T. Kim",
                "S. Kim",
                "C.D. Yoo"
            ],
            "title": "EdgeLabeling Graph Neural Network for Few-shot Learning",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 11\u201320.",
            "year": 2019
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Advances in neural information processing systems, 25: 1097\u20131105.",
            "year": 2012
        },
        {
            "authors": [
                "K. Lee",
                "S. Maji",
                "A. Ravichandran",
                "S. Soatto"
            ],
            "title": "Meta-learning with differentiable convex optimization",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 10657\u201310665.",
            "year": 2019
        },
        {
            "authors": [
                "A. Li",
                "W. Huang",
                "X. Lan",
                "J. Feng",
                "Z. Li",
                "L. Wang"
            ],
            "title": "Boosting Few-Shot Learning With Adaptive Margin Loss",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 12576\u201312584.",
            "year": 2020
        },
        {
            "authors": [
                "H. Li",
                "D. Eigen",
                "S. Dodge",
                "M. Zeiler",
                "X. Wang"
            ],
            "title": "Finding Task-Relevant Features for Few-Shot Learning by Category Traversal",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1\u201310.",
            "year": 2019
        },
        {
            "authors": [
                "X. Li",
                "Q. Sun",
                "Y. Liu",
                "Q. Zhou",
                "S. Zheng",
                "T.S. Chua",
                "B. Schiele"
            ],
            "title": "Learning to self-train for semisupervised few-shot classification",
            "venue": "Advances in Neural Information Processing Systems, 10276\u201310286.",
            "year": 2019
        },
        {
            "authors": [
                "C. Liu",
                "C. Xu",
                "Y. Wang",
                "L. Zhang",
                "Y. Fu"
            ],
            "title": "An embarrassingly simple baseline to one-shot learning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 922\u2013923.",
            "year": 2020
        },
        {
            "authors": [
                "H. Qi",
                "M. Brown",
                "D.G. Lowe"
            ],
            "title": "Low-shot learning with imprinted weights",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 5822\u20135830.",
            "year": 2018
        },
        {
            "authors": [
                "M. Ren",
                "E. Triantafillou",
                "S. Ravi",
                "J. Snell",
                "K. Swersky",
                "J.B. Tenenbaum",
                "H. Larochelle",
                "R.S. Zemel"
            ],
            "title": "Meta-learning for semi-supervised few-shot classification",
            "venue": "arXiv preprint arXiv:1803.00676.",
            "year": 2018
        },
        {
            "authors": [
                "J. Requeima",
                "J. Gordon",
                "J. Bronskill",
                "S. Nowozin",
                "R.E. Turner"
            ],
            "title": "Fast and flexible multi-task classification using conditional neural adaptive processes",
            "venue": "arXiv preprint arXiv:1906.07697.",
            "year": 2019
        },
        {
            "authors": [
                "O. Russakovsky",
                "J. Deng",
                "H. Su",
                "J. Krause",
                "S. Satheesh",
                "S. Ma",
                "Z. Huang",
                "A. Karpathy",
                "A. Khosla",
                "M. Bernstein",
                "A.C. Berg",
                "L. Fei-Fei"
            ],
            "title": "ImageNet Large Scale Visual Recognition Challenge",
            "venue": "International Journal of Computer Vision (IJCV), 115(3): 211\u2013252.",
            "year": 2015
        },
        {
            "authors": [
                "A.A. Rusu",
                "D. Rao",
                "J. Sygnowski",
                "O. Vinyals",
                "R. Pascanu",
                "S. Osindero",
                "R. Hadsell"
            ],
            "title": "Metalearning with latent embedding optimization",
            "venue": "arXiv preprint arXiv:1807.05960.",
            "year": 2018
        },
        {
            "authors": [
                "T. Saikia",
                "T. Brox",
                "C. Schmid"
            ],
            "title": "Optimized generic feature learning for few-shot classification across domains",
            "venue": "arXiv preprint arXiv:2001.07926.",
            "year": 2020
        },
        {
            "authors": [
                "C. Simon",
                "P. Koniusz",
                "R. Nock",
                "M. Harandi"
            ],
            "title": "Adaptive Subspaces for Few-Shot Learning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4136\u20134145.",
            "year": 2020
        },
        {
            "authors": [
                "J. Snell",
                "K. Swersky",
                "R. Zemel"
            ],
            "title": "Prototypical networks for few-shot learning",
            "venue": "Advances in Neural Information Processing Systems, 4077\u20134087.",
            "year": 2017
        },
        {
            "authors": [
                "F. Sung",
                "Y. Yang",
                "L. Zhang",
                "T. Xiang",
                "P.H. Torr",
                "T.M. Hospedales"
            ],
            "title": "Learning to compare: Relation network for few-shot learning",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1199\u2013 1208.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Tian",
                "Y. Wang",
                "D. Krishnan",
                "J.B. Tenenbaum",
                "P. Isola"
            ],
            "title": "Rethinking few-shot image classification: a good embedding is all you need",
            "venue": "arXiv preprint arXiv:2003.11539",
            "year": 2020
        },
        {
            "authors": [
                "E. Triantafillou",
                "T. Zhu",
                "V. Dumoulin",
                "P. Lamblin",
                "U. Evci",
                "K. Xu",
                "R. Goroshin",
                "C. Gelada",
                "K. Swersky",
                "P.-A Manzagol"
            ],
            "title": "Meta-dataset: A dataset of datasets for learning",
            "year": 2019
        },
        {
            "authors": [
                "O. Vinyals",
                "C. Blundell",
                "T. Lillicrap",
                "D Wierstra"
            ],
            "title": "Matching networks for one shot learning",
            "venue": "In Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Y. Wang",
                "W.-L. Chao",
                "K.Q. Weinberger",
                "L. van der Maaten"
            ],
            "title": "Simpleshot: Revisiting nearest-neighbor classification for few-shot learning",
            "venue": "arXiv preprint arXiv:1911.04623",
            "year": 2019
        },
        {
            "authors": [
                "Y. Wang",
                "C. Xu",
                "C. Liu",
                "L. Zhang",
                "Y. Fu"
            ],
            "title": "Instance Credibility Inference for Few-Shot Learning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 12836\u201312845.",
            "year": 2020
        },
        {
            "authors": [
                "Y.-X. Wang",
                "R. Girshick",
                "M. Hebert",
                "B. Hariharan"
            ],
            "title": "Low-shot learning from imaginary data",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 7278\u20137286.",
            "year": 2018
        },
        {
            "authors": [
                "L. Yang",
                "L. Li",
                "Z. Zhang",
                "X. Zhou",
                "E. Zhou",
                "Y. Liu"
            ],
            "title": "Dpgn: Distribution propagation graph network for few-shot learning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 13390\u2013 13399.",
            "year": 2020
        },
        {
            "authors": [
                "S. Yang",
                "L. Liu",
                "M. Xu"
            ],
            "title": "Free lunch for few-shot learning: Distribution calibration",
            "venue": "arXiv preprint arXiv:2101.06395.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "In recent works (Chen et al. 2019; Tian et al. 2020; Chen et al. 2020; Dhillon et al. 2019), the importance of utilizing a good feature embedding in few-shot learning is well studied and addressed. A feature embedding is pre-trained as a classification task using meta-training set(base classes). Finetuning on the meta-test set(novel classes) (Tian et al. 2020; Yang, Liu, and Xu 2021; Dhillon et al. 2019) is shown to surpass most meta-learning methods. However, only finetuning a classifier on the meta-test set leaves the feature embedding unchanged. A pre-trained feature extractor is sufficient to have well-defined feature distributions on base classes, while this is not true for novel classes. Novel classes may come from a variety of domains different from base classes. Globally, initial feature distributions of novel classes could be affected mainly by the domain difference. Locally, features are not trained to cluster tightly within a class and separate well between classes, which intensifies the biased estimation of only a few samples. Those biases in novel-\nCopyright \u00a9 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nclass feature distributions address the importance of refining novel-class features.\nIn our work, we refine novel-class features by finetuning the feature extractor on the meta-test set using only a few samples. We focus on reducing biases in novel-class feature distributions by defining them into two aspects: classagnostic and class-specific biases. Class-agnostic bias refers to the feature distribution shifting caused by domain differences between novel and base classes. The unrefined features from novel classes could cluster in some primary direction due to the domain difference, which leads to the skewed feature distribution as shown in Fig. 1a. In other words, the feature distribution of novel classes is shifted by the domain difference when directly using the pre-trained feature extractor. Class-specific bias refers to the biased estimation using\nonly a few samples in one class. Biased estimation is always critical for few-shot learning. By only knowing a few samples, the estimation of feature distribution within one class is biased as shown in Fig. 1b. The bias between empirical estimation with its true value would be reduced with more samples involved. Running sampling under each class distribution is the most direct way to enlarge the support set. However, this is not applicable when each class distribution is unknown.\nIn our work, we propose the Distribution Calibration Module(DCM) to reduce class-agnostic bias. DCM is designed to eliminate domain difference by normalizing the overall feature distribution for novel classes and further reshaping the feature manifold for fast adaptation during finetuning. For class-specific bias, we propose Selected Sampling(SS) to augment more data for better estimation. More specifically, the Selected Sampling happens on a chain of proposal distributions centered with each data point from the support set. And the whole sampling process is guided by an acceptance criterion that only samples beneficial to the optimization will be selected. By eliminating the domain difference through distribution calibration, DCM boosts performance over ten datasets from different domains on MetaDataset evaluation, demonstrating the importance of reducing class-agnostic bias when dealing with domain issues. Meanwhile, we theoretically analyze how DCM is designed for fast feature adaptation and showcase its supreme faster convergence compared with direct finetuning in Fig. 4. Furthermore, based on DCM\u2019s competitive performance, reducing class-specific bias with Selected Sampling can further enhance the finetuning performance by a large margin over ten datasets. Without inferring the actual feature distribution, our selected sampling can effectively explore unknown feature space and augment features to reduce class-specific bias in estimation.\nOur contributions: 1) We address the understanding of biases in novel-class feature distributions when using a pre-trained feature extractor. By proposing to reduce classagnostic and class-specific biases through DCM and SS, we power finetuning with domain-agnostic consistent performance gain. 2) We propose an efficient Selected Sampling strategy to direct sample features for class-specific bias reduction. Without inferring the class distribution, the selected sampling effectively enlarges the support set with informative data on the feature space. 3) We evaluate our method by performing comprehensive experiments on Meta-Dataset. We achieve State-of-the-Art performance and a remarkably consistent performance improvement over ten datasets from different domains. We hope this work could contribute to the understanding of the feature space for classification-oriented tasks as well."
        },
        {
            "heading": "Related Work",
            "text": ""
        },
        {
            "heading": "Overview of the Few-Shot Problem",
            "text": "Few-shot learning has been quite an active research field in recent years. The branch of Meta-learning methods (Finn, Abbeel, and Levine 2017; Rusu et al. 2018; Vinyals et al. 2016; Snell, Swersky, and Zemel 2017; Sung et al. 2018;\nChen et al. 2020; Simon et al. 2020) on few-shot learning is designed to directly back-propagate the loss of the test set while the hypothesis for classification is proposed with the training set. Also methods are not adherent to metalearning only. There are: data argumentation with hallucinating more samples(Hariharan and Girshick 2017; Wang et al. 2018), optimization with ridge regression or support vector machine (Bertinetto et al. 2018; Lee et al. 2019), using graph neural networks (Garcia and Bruna 2017; Kim et al. 2019), self/semi-supervised learning (Ren et al. 2018; Gidaris et al. 2019; Li et al. 2019b; Wang et al. 2020), learning with semantic information (Li et al. 2020), class weight generalization (Gidaris and Komodakis 2018, 2019; Guo and Cheung 2020), modules working on attentive spatial features (Li et al. 2019a; Hou et al. 2019; Doersch, Gupta, and Zisserman 2020), knowledge distillation (Tian et al. 2020). The recent (Triantafillou et al. 2019) proposes a more realistic evaluation for few-shot learning where algorithms are evaluated over 10 datasets from different domains with a large-scale meta-training set spanned from ImageNet(Krizhevsky, Sutskever, and Hinton 2012). The evaluation on meta-Dataset not only requires algorithms to obtain a good performance on few-shot learning but also sets higher demands on generalization over different domains.\nDistribution calibration (Yang et al. 2020; Yang, Liu, and Xu 2021) raises attention in few-shot learning recently. In (Yang, Liu, and Xu 2021), to conduct sampling, the similarities between base and novel classes are used to transfer the distribution parameters from base classes to novel classes. The similarity measurement may limit its application for cross-domain problems. Unlike their method, we tackle the distribution calibration from coarse to refined bias reduction. The class-agnostic bias reduction is conducted to eliminate skewness shown in the overall feature distribution. Further, class-specific bias is reduced by sampling using proposal distribution centered with the support set. Our methods successfully run direct sampling on the feature space without inferring the actual feature distribution for each class. There have been some works on feature transformations in fewshot learning recently. (Wang et al. 2019) uses the mean estimation of base classes to normalize novel-class features, without considering the domain difference between novel and base classes. (Liu et al. 2020) proposes random pruning features after normalization for one-shot learning, and the random pruning is expected to find part of feature embedding that fits in the domain of novel classes. We directly solve the domain shifting issue in feature distribution, and as illustrated later, those class-sensitive features are further amplified during finetuning. Finetuning results in (Triantafillou et al. 2019) is to finetune the K-Nearest Neighbor trained model with the meta-training set(followed by innerloop finetuning on meta-test set); the experiments with our methods are more precisely defined as (inner-loop) finetuning, which is only to use the support set within one episode from meta-test set during evaluation. (Inner-loop) finetuning is an attempt to reach a good classification through only a handful of training samples. (Dhillon et al. 2019) proposes transductive finetuning which involves the query set also."
        },
        {
            "heading": "Method",
            "text": ""
        },
        {
            "heading": "Leverage a Feature Extractor to Few-Shot Problem",
            "text": "We first formalize the few-shot classification setting with notation. Let (x, y) denote an image with its ground-truth label. In few-shot learning, training and test sets are referred to as the support and query set respectively and are collectively called a C-way K-shot episode. We denote training(support) set as Ds = {(xi, yi)}Nsi=1 and test(query) set as Dq = {(xi, yi)} Nq i=1, where yi \u2208 C and |C| is the number of ways or classes and Ns equals to C \u00d7K. For supervised learning, a statistics \u03b8\u2217 = \u03b8\u2217(Ds) is learnt to classify Ds as measured by the cross-entropy loss:\n\u03b8\u2217(Ds) = arg\u03b8 min 1\nNs \u2211 (x,y)\u2208Ds \u2212 log p\u03b8(y|x) (1)\nWhere p\u03b8(\u00b7|x) is the probability distribution on C as predicted by the model in response to input x. More specifically:\np(y = k|x) = exp \u27e8wk, f\u03b8(x)\u27e9\u2211C j=1 exp \u27e8wj , f\u03b8(x)\u27e9\n(2)\n\u27e8\u00b7\u27e9 refers to dot-product between features with class prototypes. As widely used in (Snell, Swersky, and Zemel 2017; Qi, Brown, and Lowe 2018; Chen et al. 2020), the novel class prototype wc, c \u2208 C is the mean feature from the support set Ds:\nwc = 1\nNs \u2211 x\u2208Ds f\u03b8(x) (3)\nIn our work, f\u03b8(x) is first pre-trained with meta-training set using cross-entropy loss; and further in each testing episode, \u03b8\u2217 = \u03b8\u2217(Ds) is learned by finetuning f\u03b8(x) using Ds. Given a test datum x where (x, y) \u2208 Dq , y is predicted:\ny\u0302 = argmax c\np\u03b8\u2217(c|x) (4)\nWith this basic finetuning framework, we propose Distribution Calibration Module and Selected Sampling as introduced in the following section."
        },
        {
            "heading": "Class-Agnostic Bias Reduction by Distribution Calibration Module",
            "text": "We propose an easy-plug-in distribution calibration module(DCM) to reduce class-agnostic bias caused by domain difference.\nThe first step to reduce class-agnostic bias is to calibrate skewed feature distribution. A pre-trained feature extractor f\u03b8(x) could provide an initial feature space that general invariant features are learnt from a large-scale dataset. \u03b8\u2217 = \u03b8\u2217(Dbase) is sufficient to well classify those base classes which makes it inadequate to well distinguish novel classes. The overall feature distribution of novel classes may be skewed due to its domain property. And the feature distribution could be described statistically:\n\u00b5 = 1\nNs \u2211 xi\u2208Ds f\u03b8(xi), \u03c3 = 1 Ns \u2211 xi\u2208Ds (f\u03b8(xi)\u2212 \u00b5)2 (5)\nNote that \u00b5 and \u03c3 are class-agnostic parameters describing the feature distribution for all obtained novel classes. For feature distributions that are skewed in some directions, the \u00b5 and \u03c3 presented could be far from the normal distribution. We first apply to calibrate the distribution to approach zero centered mean and unit standard deviation: fi\u2212\u00b5\u03c3 . This distribution calibration by feature normalization helps correct the skewed directions brought by large domain differences between base and novel classes.\nMeanwhile, fast feature adaptation is enabled during finetuning. For a feature vector, there are locations encoding class-sensitive information and locations encoding common information. Values on class-sensitive locations are expected to vary between classes to distinguish them. Similar values are obtained from common locations among all samples, representing some domain information but contributing little to classification. By this normalization, those locations that encode class-sensitive features are relatively stood out compared with the locations encoding common information. We further element-wisely multiply a scale vector among the calibrated feature embedding:\nf\u0304\u03b8(xi) = f\u03b8(xi)\u2212 \u00b5\n\u03c3 \u2217 s (6)\n\u2217 is the element-wise multiplication. For simplicity, we use fi = f\u03b8(xi) for notation. The scale vector is learnable during fine-tuning. The element-wise multiplication allows each location on the scale vector could optimize independently, and thus the whole feature embedding could be reshaped on novel classes by this scale vector.\nAs s is element-wisely multiplied with fi, in the following discussion, we show only the partial derivative at a single location on the feature vector. In the 1-shot case with mean features as class prototypes, we have:\n\u2202Li \u2202s \u221d fi \u2212 \u00b5 \u03c3 [(p(yi|x)\u2212 1) fi \u2212 \u00b5 \u03c3 + C\u2211 j \u0338=yi p(j|x)fj \u2212 \u00b5 \u03c3 ]\n(7) After applying distribution calibration on fi and fj , the gradients of s for the class-sensitive locations have relatively larger values than the common ones. The difference between features is further addressed and enlarged correspondingly through gradient-based optimization. And the feature manifolds will fast adapt to the shape where distinguished parts are amplified."
        },
        {
            "heading": "Class-Specific Bias Reduction by Selected Sampling",
            "text": "Biased estimation in a class inevitably hinders the optimization of features. The gradient for feature f of (x, y) during finetuning is:\n\u2202Li \u2202f\n= (p(y|x)\u2212 1)wy + C\u2211\nj \u0338=y\np(j|x)wj (8)\nAs p(y|x) \u2264 1, the optimization of gradient descent focuses f moving close towards the direction of wy , its\nground-truth class prototypes. For a class c, mean feature from the support set is used as the class prototypes when computing the predicted probability (Snell, Swersky, and Zemel 2017; Qi, Brown, and Lowe 2018; Triantafillou et al. 2019): wc = 1Ns \u2211 x\u2208Ds f\u03b8(x). This is the empirical estimation of mean using the support set. We denote the true mean from the class distribution as mc. We further define the bias term \u03b4c between empirical estimation with its true value as:\n\u03b4c = wc \u2212mc (9) For few-shot learning, as the w is estimated from a small number of data, \u03b4c is indeed not neglectful. As defined in Eq. 9, wy can be replaced by \u03b4y +my . Then the gradient of feature f is:\n\u2202Li \u2202f\n= (p(y|x)\u2212 1)\u03b4y + (p(y|x)\u2212 1)my + C\u2211\nj \u0338=y\np(j|x)wj\n(10) The optimization of f towards its class prototype wy can be factorized into two parts: one part (p(y|x) \u2212 1)\u03b4y dominated by the bias and the other by the true mean my . Ideally, features are expected to tightly cluster around m for a refined feature distribution. However, (p(y|x)\u22121)\u03b4y in the gradient distracts the optimization of f by moving it close to the bias, which hinders its approaching to the true mean. And this inevitably impedes the optimization for few-shot learn-\ning. As shown in Fig. 2a, points in the support set could be strayed from a majority in the class distribution. The strayed points enlarge bias in estimation, and thus during optimization, the clustering of the feature distribution is distracted by the bias.\nAugmenting more data is an efficient way to reduce bias. If more features could be sampled and added to the sequence of computing the class prototype within one class, effects caused by the bias will be vastly reduced. However, the feature distribution is unknown for each class, which disables the direct sampling from that distribution.\nWithout inference of actual feature distribution, we propose the Selected Sampling, which guides the Monte-Carlo Sampling under a proposal distribution. By taking advantage of each known data in the support set and let these a few samples guide the direction of a Monte-Carlo Sampling, we directly augment features into the support set. For each known data point (xi, yi) the corresponding vector in the feature space is denoted as fi, a proposal distribution Q(f \u2032|f) = N (fi,\u03a3) is used to sample f \u2032\ni . p(y|f) is a deterministic variable as the predicted logits from the classifier given a feature f . The sampled points are queried by the criterion p(yi|f \u2032\ni ) > p(yi|fi) in determination of acceptance. If accepted, f \u2032\ni becomes the new starting feature point to run the next sampling step using proposal distribution N(f \u2032i , \u03c3 2); if rejected, the sampling process for (xi, yi)\nterminates. We illustrate the sampling process in Fig. 3. The proposal distribution ensures that samples are drawn from the vicinity around the known point during the process. N (fi,\u03a3) is a multivariate Gaussian distribution centered with fi. The covariance matrix \u03a3 is an identity matrix scaled with a hyper-parameter \u03c3, which allows each location on features to be sampled independently. However, the proposal distribution is only a random walk process which brings no further constraints on the sampled points. With a feature f = f\u03b8(x), the acceptance criterion is whether the sampled feature will have a more significant predicted probability of belonging to the ground-true class or not, which is p(yi|f \u2032 i ) > p(yi|fi):\nexp \u27e8wk, f \u2032 i \u27e9\u2211C j=1 exp \u27e8wj , f \u2032 i \u27e9 > exp \u27e8wk, fi\u27e9\u2211C j=1 exp \u27e8wj , fi\u27e9\n(11)\nThe numerator exp \u27e8wk, f \u2032\ni \u27e9 represents the distance between a feature with its class prototype, and the denominator\u2211C\nj=1 exp \u27e8wj , f \u2032\ni \u27e9 represents the overall distance between a feature with all class prototypes. This criterion indicates that a sampled point is accepted under the case either closer to its class prototype or further away from other classes in the high-dimensional feature space. Either way, the accepted point is ensured to provide helpful information that avoids the cons of random walk Sampling. This selected sampling on the feature space allows exploration of unknown feature space while still controlling the quality of sampling to optimize. As shown in Fig. 2b, by enlarging the support set with selected samples, the bias in mean estimation is reduced. And Selected Sampling is an ongoing process for each iteration that helps to enhance the feature distribution clustering.\nExperimental Validations In this section, we first conduct comprehensive ablation experiments to verify the effectiveness of both DCM and SS and analyze how our method boosts performance under shot analysis. Then we compare our results with the other latest\ntechniques. We follow the same setting and evaluation metrics in meta-Baseline (Chen et al. 2020). More specifically, when training the feature extractors, softmax cross-entropy loss is applied for learning. For finetuning on the (Meta-)test set, features and class prototypes are under normalization for the softmax loss. The temperature in the loss function is initialized to 10.\nWe evaluate our method on Meta-Dataset(Triantafillou et al. 2019), which is so far the most comprehensive benchmark for few-shot learning composed of multiple existing datasets in different domains. The cross-domain property of meta-Dataset raises the real challenges for few-shot learning. To strictly verify the feature adaptation ability under different domains, we follow the evaluation by using the Imagenet-only training set to pre-train the feature extractor."
        },
        {
            "heading": "Implementation Details",
            "text": ""
        },
        {
            "heading": "Pre-training the Backbone: Choice of the Network and Training Setting.",
            "text": "The ILSVRC-2012 (Russakovsky et al. 2015) in MetaDataset is splitted into 712 training, 158 validation and 130 test classes. We use the training set of 712 classes to train two feature extractors with backbones: ResNet18 and ResNet34. For ResNet18, we follow the same protocol in Meta-Baseline (Chen et al. 2020), which is: the images are randomly resized cropped to 128x128, horizontal flipped and normalized. For ResNet34, we follow the same structure modification in (Doersch, Gupta, and Zisserman 2020) which uses stride 1 and dilated convolution for the last residual block and the input image size is 224x224. The initial learning rate is set to 0.1 with 0.0001 weight decay and decreases by a factor of 0.1 every 30 epochs with total 90 epochs. Both models are trained using the SGD optimizer with batch size 256.\nSetting of Evaluation and Fine-tuning. The general evaluation on Meta-Dataset utilizes a flexible sampling of episodes (Triantafillou et al. 2019), which allows a maximum of 500 images in the support set in one episode. In the finetuning stage, the scale vector s is initialized with value 1. Data argumentation works as resizing and center cropping images to 128x128(ResNet18) and 224x224(ResNet34) followed by normalization. We follow the setting in (Dhillon et al. 2019) as described with learning rate of 0.00005, Adam optimizer and 25 total epochs. \u03c3 in the proposal distribution for sampling is set to 0.1. Finetuning experiments are conducted with whole batch update."
        },
        {
            "heading": "Ablation Studies",
            "text": "We first study the importance of applying DCM during finetuning with the typical class prototype(mean feature from support set) and then upon applying DCM we add SS to rectify class prototypes. All ablation results are in Table. 1.\nDCM enables competitive domain-agnostic fast feature adaptation for finetuning. There are two functionalities in DCM: feature normalization and the scale vector multiplication. We first independently evaluate the performance gain brought by feature normalization(FN) and then also verify the importance of the scale vector(S) in fast feature adaptation. Only finetuning the backbone cannot guarantee\nthe performance improvement over all datasets due to the different domain gaps, especially for CU-Birds and Fungi as shown in Table. 1. Among these datasets where finetuning the backbone is not practically working, the performances improve by 3.10% on CU-Birds and 3.39% on Fungi by simply adding feature normalization. And by adding the scale vector, the performance is further improved over 7 out of 10 datasets. These results indicate that DCM improves the generalization of finetuning over datasets from different domains. We further plot the training losses and accuracy during the finetuning iterations in Figure. 4. The plot shows that finetuning with DCM owes supreme convergence speed compared with direct finetuning. The advantage in convergence speed demonstrates the property of fast feature adaptation of DCM.\nSelected Sampling can consistently improve performance over all datasets. With adding selected sampling, performance on all datasets is improved from 0.34% to 1.75%. And for 6 out of 10 datasets, the performance is boosted by roughly 1%. Especially for ILSVRC, Birds, and MSCOCO, the performance gains brought by SS are the most significant compared with finetuning backbone or adding DCM with backbone. These datasets are diverse in\nobjects and cover significant variations within one class. For example, CU-Birds requires high demands on fine-grained classification. The performance gain strongly indicates that using Selected Sampling to rectify class prototypes works well with features in different domains.\nDCM with SS plays an essential role under extreme few shots. Meanwhile, we further evaluate how DCM+SS powers finetuning, especially under extreme few shots. As shown in Figure. 5, by fixing the number of shots in one episode, we provide the average performance gain by running 600 episodes for each dataset. Finetuning the backbone leads to a performance drop with only one or two shots per class, while DCM+SS can dramatically compensate for the performance loss. Meanwhile, by only increasing the number of shots(comparing FT-B performance on 2-shot and 3-shot), finetuning can be improved by a relatively small range. Adding DCM+SS leads to much more performance gain simply on 2-shot cases. DCM+SS essentially boosts performance on a few shots. In all, DCM+SS shows consistent significant performance increases over over extreme few shot cases.\nDCM with SS powers finetuning. From Table. 1, finetuning improves the performance around 7.31% to\n15.52% on several datasets, but performance drops on Birds and Fungi. DCM+SS with finetuning the backbone shows consistent performance boosts on all datasets from 1.92%(mscoco) to 29.7%(Traffic sign). We hope this results encourage further explorations on bias reduction in feature distribution."
        },
        {
            "heading": "Compare with the State of the Art Performance",
            "text": "We report our results under different backbone models and provide a comparison over other popular methods in Table. 2. Compared with directly using the pre-trained feature extractor for evaluation, finetuning backbone with DCM+SS boosts performance significantly over all datasets. The results are consistent among both backbones. This demonstrates the effectiveness of directly applying finetuning using\nthe support set. Comparing the performance on ResNet18 and ResNet34, we first observe that a larger backbone with a larger input image size gives a better quality of the feature extractor. Furthermore, our method shows the adaptation ability of the pre-trained feature extractor to new data domains can even be improved when the feature extractor itself is more powerful. By a simple testing-time finetuning, we achieve the State-of-the-Art performance over several datasets and closely competitive results for all datasets with ResNet18 and ResNet34. (Tian et al. 2020) only finetunes a classifier on the testing set, and with the same backbone ResNet18, our method surpasses its results with a large margin on most datasets. This addresses the importance of refining novel-class features for better generalization. For (Doersch, Gupta, and Zisserman 2020) which overpasses our results on four datasets, besides a pre-trained feature extractor, a comprehensive meta-training process using seven days to converge as reported in their work is also utilized. The ResNet34 feature extractor we use is only trained by supervised classification loss using the training set. Meanwhile, our method is computationally efficient as we barely involve any network structure change(only one DCM layer with a scale vector). The sampling is conducted in an efficient full batch style. (Dhillon et al. 2019) includes extra query set during transductive finetuning, which leads to a better result on DTD. While our finetuning is only using the support set. And we surpass the results on the other nine datasets and have a consistent performance gain on DTD."
        },
        {
            "heading": "Conclusion",
            "text": "We show in our experiments that without any meta-training process, the fast feature adaptation can also be achieved by better understanding biases in feature distribution for fewshot learning. We hope our work could provide insight into the importance of bias reduction in distribution when dealing with datasets from different domains for few-shot learning."
        }
    ],
    "title": "Powering Finetuning in Few-Shot Learning: Domain-Agnostic Bias Reduction with Selected Sampling",
    "year": 2022
}