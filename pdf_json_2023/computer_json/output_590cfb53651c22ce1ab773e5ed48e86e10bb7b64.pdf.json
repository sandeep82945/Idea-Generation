{
    "abstractText": "Deep learning-based methods have spearheaded the automatic analysis of echocardiographic images, taking advantage of the publication of multiple open access datasets annotated by experts (CAMUS being one of the largest public databases). However, these models are still considered unreliable by clinicians due to unresolved issues concerning i) the temporal consistency of their predictions, and ii) their ability to generalize across datasets. In this context, we propose a comprehensive comparison between the current best performing methods in medical/echocardiographic image segmentation, with a particular focus on temporal consistency and cross-dataset aspects. We introduce a new private dataset, named CARDINAL, of apical two-chamber and apical four-chamber sequences, with reference segmentation over the full cardiac cycle. We show that the proposed 3D nnU-Net outperforms alternative 2D and recurrent segmentation methods. We also report that the best models trained on CARDINAL, when tested on CAMUS without any fine-tuning, still manage to perform competitively with respect to prior methods. Overall, the experimental results suggest that with sufficient training data, 3D nnU-Net could become the first automated tool to finally meet the standards of an everyday clinical device.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hang Jung Ling"
        },
        {
            "affiliations": [],
            "name": "Nathan Painchaud"
        },
        {
            "affiliations": [],
            "name": "Pierre-Yves Courand"
        },
        {
            "affiliations": [],
            "name": "Pierre-Marc Jodoin"
        },
        {
            "affiliations": [],
            "name": "Damien Garcia"
        }
    ],
    "id": "SP:6972124cbc9288c66e1848ae07d4f1a7005eb0ed",
    "references": [
        {
            "authors": [
                "O. Bernard",
                "B. Heyde",
                "M. Alessandrini",
                "D. Barbosa",
                "S. Camarasu-Pop",
                "F. Cervenansky",
                "S. Valette",
                "O. Mirea",
                "E. Galli",
                "M.L. Geleijnse",
                "A. Papachristidis",
                "J.G. Bosch",
                "J. d\u2019Hooge"
            ],
            "title": "Challenge on Endocardial Three-dimensional Ultrasound Segmentation (CETUS)",
            "venue": "MICCAI CETUS. pp. 1\u20138",
            "year": 2014
        },
        {
            "authors": [
                "A. Degerli",
                "M. Zabihi",
                "S. Kiranyaz",
                "T. Hamid",
                "R. Mazhar",
                "R. Hamila",
                "M. Gabbouj"
            ],
            "title": "Early Detection of Myocardial Infarction in Low-Quality Echocardiography",
            "venue": "IEEE Access 9, 34442\u201334453",
            "year": 2021
        },
        {
            "authors": [
                "J. Hu",
                "E. Smistad",
                "I.M. Salte",
                "H. Dalen",
                "L. Lovstakken"
            ],
            "title": "Exploiting temporal information in echocardiography for improved image segmentation",
            "venue": "2022 IEEE International Ultrasonics Symposium (IUS)",
            "year": 2022
        },
        {
            "authors": [
                "F. Isensee",
                "P.F. Jaeger",
                "S.A.A. Kohl",
                "J. Petersen",
                "K.H. Maier-Hein"
            ],
            "title": "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation",
            "venue": "Nature Methods 18(2), 203\u2013211",
            "year": 2021
        },
        {
            "authors": [
                "S. Leclerc",
                "E. Smistad",
                "J. Pedrosa",
                "A. \u00d8stvik",
                "F. Cervenansky",
                "F. Espinosa",
                "T. Espeland",
                "E.A.R. Berg",
                "P.M. Jodoin",
                "T. Grenier",
                "C. Lartizien",
                "J. D\u2019hooge",
                "L. Lovstakken",
                "O. Bernard"
            ],
            "title": "Deep Learning for Segmentation Using an Open Large-Scale Dataset in 2D Echocardiography",
            "venue": "IEEE Transactions on Medical Imaging 38(9), 2198\u20132210",
            "year": 2019
        },
        {
            "authors": [
                "H.J. Ling",
                "D. Garcia",
                "O. Bernard"
            ],
            "title": "Reaching intra-observer variability in 2-D echocardiographic image segmentation with a simple U-Net architecture",
            "venue": "IEEE International Ultrasonics Symposium (IUS) (2022),",
            "year": 2022
        },
        {
            "authors": [
                "D. Ouyang",
                "B. He",
                "A. Ghorbani",
                "N. Yuan",
                "J. Ebinger",
                "C.P. Langlotz",
                "P.A. Heidenreich",
                "R.A. Harrington",
                "D.H. Liang",
                "E.A. Ashley",
                "J.Y. Zou"
            ],
            "title": "Video-based AI for beat-to-beat assessment of cardiac function",
            "venue": "Nature 580(7802), 252\u2013256",
            "year": 2020
        },
        {
            "authors": [
                "N. Painchaud",
                "N. Duchateau",
                "O. Bernard",
                "P.M. Jodoin"
            ],
            "title": "Echocardiography Segmentation With Enforced Temporal Consistency",
            "venue": "IEEE Transactions on Medical Imaging 41(10), 2867\u20132878",
            "year": 2022
        },
        {
            "authors": [
                "C. Sfakianakis",
                "G. Simantiris",
                "G. Tziritas"
            ],
            "title": "GUDU: Geometrically-constrained Ultrasound Data augmentation in U-Net for echocardiography semantic segmentation",
            "venue": "Biomedical Signal Processing and Control 82, 104557",
            "year": 2023
        },
        {
            "authors": [
                "E. Smistad",
                "I.M. Salte",
                "H. Dalen",
                "L. Lovstakken"
            ],
            "title": "Real-time temporal coherent left ventricle segmentation using convolutional LSTMs",
            "venue": "2021 IEEE International Ultrasonics Symposium (IUS)",
            "year": 2021
        },
        {
            "authors": [
                "H. Wei",
                "H. Cao",
                "Y. Cao",
                "Y. Zhou",
                "W. Xue",
                "D. Ni",
                "S. Li"
            ],
            "title": "Temporal-Consistent Segmentation of Echocardiography with Co-learning from Appearance and Shape",
            "venue": "MICCAI. pp. 623\u2013632",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Keywords: Ultrasound \u00b7 cardiac segmentation \u00b7 temporal segmentation \u00b7 deep learning \u00b7 CNN."
        },
        {
            "heading": "1 Introduction",
            "text": "Echocardiographic imaging has undergone major advances in recent years thanks to artificial intelligence, especially the deep learning (DL) paradigm. In particular, the automated extraction of clinical indices from the segmentation of cardiac structures has been the subject of intense research leading to major breakthroughs. A key component of these advances has been the publication\nar X\niv :2\n30 5.\n01 99\n7v 2\n[ ee\nss .I\nV ]\nof open access annotated datasets, including CETUS (45 patients, 3D images annotated at End-Diastole - ED and End-Systole - ES) [1], CAMUS (500 patients, 2D images annotated at ED and ES in apical two-chamber - A2C - and apical four-chamber - A4C - views) [5], EchoNet-Dynamic (10,036 patients, 2D sub-sampled images annotated at ED and ES in A4C view) [7], HMC-QU (109 patients, 2D sequences annotated in A4C view) [2] and TED (98 patients from the CAMUS dataset, 2D sequences annotated in A4C view) [8].\nThese datasets allowed effective and fair comparisons of methods, whether they are generic image segmentation models [5,6] or were specifically designed to process echocardiographic images [11,9]. Thus, the performance of current state-of-the-art (SOTA) methods on the CAMUS dataset confirmed the dominance of the DL-based methods, which finally achieved inter- and intra-observer variability for most of the geometric (Dice score, Hausdorff distance, mean absolute distance - MAD) and clinical metrics (ejection fraction - EF, volumes at ED/ES).\nAlthough these results are extremely promising and represent a crucial step towards the automation of echocardiographic image analysis, they are not sufficient to justify confidence in fully automated methods in a clinical context. Indeed, two crucial challenges on the path to the practical application of these algorithms remain understudied in the field: i) the frame-by-frame temporal consistency of the predictions, and ii) the generalization of the methods across datasets. Based on this observation, we propose the following contributions:\n1. We study the performance of two generic architectures based on common temporal data processing techniques on 2D echocardiography sequences, and compare them to current SOTAs in the same field; 2. We present a new private dataset called CARDINAL (240 patients, 2D sequences annotated in A4C and A2C views), and report the performance impact of training our methods exclusively on CARDINAL and testing on CAMUS."
        },
        {
            "heading": "2 Benchmarked Methods",
            "text": "CAMUS is currently the only dataset where an evaluation platform has been established to effectively compare the performance of segmentation methods5. We therefore relied on this dataset to select the methods we retained in this study."
        },
        {
            "heading": "2.1 2D DL Methods",
            "text": "The currently best performing method on the CAMUS dataset exploits the nnU-Net formalism [6]. This model is based on the U-Net architecture and implements several successful DL tricks, such as a patch-wise approach to preserve image resolution, data augmentation during both training and inference to enforce\n5 https://www.creatis.insa-lyon.fr/Challenge/camus/results.html\ngeneralization and automatic hyperparameter search of the U-Net architecture to increase accuracy. Note that the 2D version uses only one U-Net model [4].\nRecently, Sfakianakis et al. [9] developed a DL solution called GUDU based on three key aspects. First, they proposed to use data augmentations tailored to ultrasound acquisition, i.e. variation of the contrast between the myocardial tissue and the left ventricular (LV) cavity, random rotation from the origin of the sectorial shape to mimic different probe positioning, and perspective transformations to simulate probe twisting. Inspired by ensemble models, the authors also trained 5 U-Nets with different architectures and averaged their outputs during inference in order to compute the final prediction. Finally, a new loss function was proposed that takes into account the relative position of the cardiac structures with respect to each other. The authors demonstrated the usefulness and complementarity of each contribution in an ablation study."
        },
        {
            "heading": "2.2 2D+t DL Methods",
            "text": "Despite the success of 2D methods in producing accurate segmentations for individual echocardiographic frames, they often fail to maintain temporal consistency between frames [8]. Prior to the publication of HMC-QU and TED, there were no publicly available datasets to train and compare methods that incorporate the temporal dimension in 2D+time echocardiography. This explains why so few papers have focused on this topic.\nDue to the lack of 2D+time annotated datasets, Wei et al. [11] proposed a method to leverage the limited ED/ES annotated frames and propagate them to unannotated frames. This was achieved by training a 3D U-Net designed to predict both the deformation fields between each pair of consecutive frames and the segmentation masks at each frame of the sequence. The deformation fields are used to propagate the ED/ES reference annotations forward and backward in time through the sequence. The corresponding propagated masks are then used as targets for self-supervised segmentation of the entire sequence. This encourages the model to learn consistent temporal dynamics to find the best match between the predicted segmentation masks and the propagated annotations.\nMore recently, Smistad et al. [10] and Hu et al. [3] added convolutional longterm memory blocks to each layer of the encoder of a 2D U-Net (this type of model is hereafter referred to as U-Net LSTM). Thus, instead of processing a single frame, these methods take a series of frames as input and store the extracted features over time to produce the final segmentation of the entire sequence. Results show that such a strategy tends to reduce segmentation shifts from one frame to another.\nBy their very nature, echocardiographic sequences exhibit regular properties along the time axis. Therefore, it seems logical to consider 2D ultrasound sequences as complete volumes containing coherent 3D shapes and to extract 3D features using 3D convolutional layers to promote temporal consistency. Thus, in this paper, we propose to train a 3D nnU-Net to segment the complete cardiac sequences in a single run. We hypothesize that this model will inherently learn temporal consistency while maintaining a high level of segmentation accuracy."
        },
        {
            "heading": "3 Experimental Setup",
            "text": ""
        },
        {
            "heading": "3.1 CARDINAL Dataset",
            "text": "Acquisition Protocol: The proposed dataset consists of clinical examinations of 240 patients, acquired at the University Hospital of Lyon (Croix-Rousse Lyon Sud, France) under the regulation of the local ethics committee of the hospital. The complete dataset was acquired with GE ultrasound scanners. For each patient, 2D A4C and A2C view sequences were exported from the EchoPAC analysis software. Each exported sequence corresponds to a set of B-mode images expressed in polar coordinates. The same interpolation procedure as used for the CAMUS dataset was applied to express all sequences in Cartesian coordinates with a single grid resolution of 0.31 mm2. Each sequence in the CARDINAL dataset corresponds to a complete cardiac cycle defined as the interval between peaks of maximal LV cavity surface area.\nReference Annotations: To tackle the total number of frames to be annotated, an experienced observer first delineated the different contours using semiautomatic tools to ensure temporal consistency of the segmented shapes. Each corresponding output was then checked/corrected by two other experienced observers. To identify the ED/ES frames in the sequence, the ED frames correspond (by definition) to the beginning and end of each sequence, and the ES frame corresponds to the frame where the LV cavity surface is smallest."
        },
        {
            "heading": "3.2 Implemented DL Methods",
            "text": "For a fair comparison, we implemented the 2D nnU-Net, U-Net LSTM, and 3D nnU-Net described in section 2 using the same Python library called ASCENT 6. These models shared the following training hyperparameters: batch size of 2, SGD optimizer with a learning rate of 0.01 coupled with a polynomial decay scheduler, and 1000 training epochs. The 2D and 3D nnU-Net used a patch-wise approach to avoid resizing the images, thus preserving the native image resolution. To train the U-Net LSTM, the input images were resized to 256 \u00d7 256 and 24 consecutive frames were randomly selected and fed to the model to produce the corresponding segmentations. For inference, the sliding window approach with a Gaussian importance map was used. The prediction was given by the average of the softmax probabilities of all windows. To improve segmentation accuracy, the final prediction was obtained by averaging the predictions of the original and mirrored images along different axes. More implementation details for each model can be found in table 1."
        },
        {
            "heading": "4 Results",
            "text": "We evaluate the methods described in section 2 using three types of measures to get a complete picture of their performance in terms of segmentation accuracy\n6 https://github.com/creatis-myriad/ASCENT\n(table 2), extraction of clinical indices (table 3) and temporal consistency (table 4). In each of these tables, we group the methods according to the datasets on which they were trained and tested (CARDINAL is abbreviated as CL and CAMUS is abbreviated as CS ) to make it easier to observe the change in performance when generalizing to a new dataset."
        },
        {
            "heading": "4.1 Geometric and Clinical Accuracy",
            "text": "Table 2 shows the segmentation accuracy computed from the CARDINAL and CAMUS datasets for the 5 algorithms described in section 2. The values in bold correspond to the best scores for each metric for a given training/test dataset setup. From the results on the CARDINAL dataset (CL/CL case), we can see that the 3D nnU-Net has the best segmentation scores for all metrics, for both ED and ES. It is also interesting to note that the two temporal consistency methods (3D nnU-Net and U-Net LSTM) produce better results than the 2D nnU-Net method. This can be explained by the fact that the reference segmentation has regular properties along the temporal axis due to the annotation process. Methods that integrate the temporal dimension into their architecture are therefore more likely to produce segmentation results that are closer to the manual references.\nIt is worth mentioning that methods trained and tested on the same dataset (sections CL/CL and CS/CS in Table 2) get overall better results up to 1.7x for the Hausdorff and MAD metrics. One reason for such an improvement is the larger amount of annotated training images for CARDINAL (18,793 im-\nages from 190 training/validation patients, reference frames for the full cardiac cycle in A2C and A4C views) than for CAMUS (1,800 images from 450 training/validation patients, reference frames at ED and ES in A2C and A4C views).\nTable 3 reports the clinical metrics for the 5 methods. As in Table 2, the methods enforcing temporal consistency gets the best results on CARDINAL, especially for the ejection fraction for which temporal consistency is essential (mean correlation score of 0.917). Furthermore, the best models trained on CARDINAL or CAMUS produce similar results for volume estimation (average correlation of 0.978), revealing a limit reached by these approaches, certainly due to the resolution of the imaging systems."
        },
        {
            "heading": "4.2 Integration of Temporal Consistency",
            "text": "Table 4 allows a better investigation of the temporal performance of the methods by providing additional information on the number/percentage of frames considered temporally inconsistent w.r.t. their neighboring frames. As expected, the methods incorporating temporal persistence produced fewer temporal errors. Looking at the number of sequences with at least one temporally inconsistent frame, the 3D nnU-Net clearly outperforms U-Net LSTM, with only 4 inconsistent sequences over 100 compared to 98 sequences for U-Net LSTM. This result illustrates the greater ability of features computed from 3D convolutional layers to extract relevant spatio-temporal information. The few remaining temporal errors for the 3D nnU-Net are more an indication that the metrics we\nused are (overly) strict on the temporal smoothness. Indeed, the 3D nnU-Net temporal \u201dinconsistencies\u201d appear invisible to the expert eye. As a qualitative evaluation, fig. 1 illustrates in detail the temporal consistency of each of our own method on one patient from the CARDINAL test set. To complement this, we also provide in the supplementary material examples of temporally consistent and inconsistent segmentation results obtained by the 3D nnU-Net method for the CARDINAL and CAMUS datasets."
        },
        {
            "heading": "4.3 Generalization Across Datasets",
            "text": "The ability to generalize across datasets is crucially important to gauge the capacity of a method to properly analyze data affected by a distributional shift. To this end, the models trained on CARDINAL were also evaluated on the CAMUS test set without any fine-tuning. The results are reported in the \u201cCL/CS\u201d sections of tables 2 to 4. Among the methods evaluated, 3D nnU-Net is the undisputed best. It even produces competitive geometric and clinical scores compared with SOTA methods trained directly on CAMUS. Thanks to the integration of temporal consistency, the 3D nnU-Net trained on CARDINAL also produces one of the best correlation scores for the EF calculated on the CAMUS dataset, even when compared to SOTA methods trained directly on CAMUS. In view of these results, and considering that the annotation process between the two databases was not identical and was carried out by different experts (which inevitably introduces a bias during the learning phase), the generalization capacity of the 3D nnU-Net model seems remarkable."
        },
        {
            "heading": "5 Conclusion",
            "text": "We evaluated the ability of different methods to accurately segment echocardiographic images, with a focus on temporal consistency and cross-dataset generalization. To this end, we introduced a new private database called CARDINAL, with annotations from an expert on the full cardiac cycle for each sequence. The results show that 3D nnU-Net and U-Net LSTM produce the best geometric and clinical scores on the CARDINAL dataset due to the integration of temporal persistence. Regarding the temporal consistency metrics, 3D nnU-Net performed significantly better than U-Net LSTM with only four sequences (instead of 98) out of 100 having at least one image that was temporally inconsistent. As far as cross-dataset generalization is concerned, 3D nnU-Net is also the best performing method. When trained on CARDINAL and tested on CAMUS, it achieved comparable geometric and clinical scores to the best methods both trained and tested on CAMUS. All these results clearly show that 3D nnU-Net is a serious candidate to become the first automated tool to meet the requirements of routine clinical examinations."
        },
        {
            "heading": "Acknowledgment",
            "text": "This research was funded, in whole or in part, by l\u2019Agence Nationale de la Recherche (ANR), project ANR-22-CE45-0029-01. The authors gratefully acknowledge financial support of the MEGA doctoral school (ED 162), the NSERC Canada Graduate Scholarships-Doctoral Program, the FRQNT Doctoral Scholarships Program, and the LABEX PRIMES (ANR-11-LABX-0063) of Universite\u0301 de Lyon, within the program \u201cInvestissements d\u2019Avenir\u201d operated by the French\nANR. The authors also thank GENCI-IDRIS for providing access to HPC resources (Grant 2022-[AD010313603]). For the purpose of open access, the authors have applied a CC BY public copyright license to any Author Accepted Manuscript (AAM) version arising from this submission."
        }
    ],
    "title": "Extraction of volumetric indices from echocardiography: which deep learning solution for clinical use?",
    "year": 2023
}