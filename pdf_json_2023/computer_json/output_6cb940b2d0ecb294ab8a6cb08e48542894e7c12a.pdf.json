{
    "abstractText": "In deductive verification and software model checking, dealing with certain specification language constructs can be problematic when the back-end solver is not sufficiently powerful or lacks the required theories. One way to deal with this is to transform, for verification purposes, the program to an equivalent one not using the problematic constructs, and to reason about its correctness instead. In this paper, we propose instrumentation as a unifying verification paradigm that subsumes various existing ad-hoc approaches, has a clear formal correctness criterion, can be applied automatically, and can transfer back witnesses and counterexamples. We illustrate our approach on the automated verification of programs that involve quantification and aggregation operations over arrays, such as the maximum value or sum of the elements in a given segment of the array, which are known to be difficult to reason about automatically. We implement our approach in the MonoCera tool, which is tailored to the verification of programs with aggregation, and evaluate it on example programs, including SV-COMP programs.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jesper Amilon1(B"
        },
        {
            "affiliations": [],
            "name": "Zafer Esen2(B"
        },
        {
            "affiliations": [],
            "name": "Dilian Gurov1(B"
        },
        {
            "affiliations": [],
            "name": "Christian Lidstr\u00f6m1(B"
        },
        {
            "affiliations": [],
            "name": "Philipp R\u00fcmmer"
        },
        {
            "affiliations": [],
            "name": "3(B"
        }
    ],
    "id": "SP:dd8592c138bdc4ae3543de6ed39214a0f52b0c91",
    "references": [
        {
            "authors": [
                "M. Afzal",
                "S. Chakraborty",
                "A. Chauhan",
                "B. Chimdyalwar",
                "P. Darke",
                "A. Gupta",
                "S. Kumar",
                "C. Babu M",
                "D. Unadkat",
                "R. Venkatesh"
            ],
            "title": "VeriAbs: verification by abstraction and test generation (competition contribution)",
            "venue": "TACAS 2020. LNCS, vol. 12079, pp. 383\u2013387. Springer, Cham",
            "year": 2020
        },
        {
            "authors": [
                "W. Ahrendt",
                "B. Beckert",
                "R. Bubel",
                "R. H\u00e4hnle",
                "P.H. Schmitt",
                "Ulbrich",
                "M. (eds."
            ],
            "title": "Deductive Software Verification - The KeY Book - From Theory to Practice, Lecture Notes in Computer Science, vol",
            "venue": "10001. Springer",
            "year": 2016
        },
        {
            "authors": [
                "F. Alberti",
                "R. Bruttomesso",
                "S. Ghilardi",
                "S. Ranise",
                "N. Sharygina"
            ],
            "title": "Lazy abstraction with interpolants for arrays",
            "venue": "Bj\u00f8rner, N., Voronkov, A. (eds.) LPAR 2012. LNCS, vol. 7180, pp. 46\u201361. Springer, Heidelberg",
            "year": 2012
        },
        {
            "authors": [
                "J. Amilon",
                "Z. Esen",
                "D. Gurov",
                "C. Lidstr\u00f6m",
                "P. R\u00fcmmer"
            ],
            "title": "Automatic program instrumentation for automatic verification (extended technical report)",
            "venue": "CoRR abs/2306.00004",
            "year": 2023
        },
        {
            "authors": [
                "J. Amilon",
                "Z. Esen",
                "D. Gurov",
                "C. Lidstr\u00f6m",
                "P. R\u00fcmmer"
            ],
            "title": "Artifact for the CAV 2023 paper \u201cAutomatic Program Instrumentation for Automatic Verification",
            "year": 2023
        },
        {
            "authors": [
                "C. Barrett",
                "P. Fontaine",
                "C. Tinelli"
            ],
            "title": "The SMT-LIB Standard: Version 2.6",
            "venue": "Tech. rep., Department of Computer Science, The University of Iowa (2017),",
            "year": 2017
        },
        {
            "authors": [
                "D. Beyer"
            ],
            "title": "Progress on software verification: SV-COMP 2022",
            "venue": "TACAS 2022. LNCS, vol. 13244, pp. 375\u2013402. Springer, Cham",
            "year": 2022
        },
        {
            "authors": [
                "D. Beyer"
            ],
            "title": "SV-Benchmarks: Benchmark Set for Software Verification and Testing (SV-COMP 2022 and Test-Comp 2022), January 2022",
            "year": 2022
        },
        {
            "authors": [
                "D. Beyer",
                "M. Dangl",
                "P. Wendler"
            ],
            "title": "Boosting k -induction with continuously-refined invariants",
            "venue": "Kroening, D., P\u0103s\u0103reanu, C.S. (eds.) CAV 2015. LNCS, vol. 9206, pp. 622\u2013640. Springer, Cham",
            "year": 2015
        },
        {
            "authors": [
                "D. Beyer",
                "M.E. Keremoglu"
            ],
            "title": "CPAchecker: a tool for configurable software verification",
            "venue": "Gopalakrishnan, G., Qadeer, S. (eds.) CAV 2011. LNCS, vol. 6806, pp. 184\u2013190. Springer, Heidelberg",
            "year": 2011
        },
        {
            "authors": [
                "N. Bj\u00f8rner",
                "A. Gurfinkel",
                "K. McMillan",
                "A. Rybalchenko"
            ],
            "title": "Horn clause solvers for program verification",
            "venue": "Beklemishev, L.D., Blass, A., Dershowitz, N., Finkbeiner, B., Schulte, W. (eds.) Fields of Logic and Computation II. LNCS, vol. 9300, pp. 24\u201351. Springer, Cham",
            "year": 2015
        },
        {
            "authors": [
                "N. Bj\u00f8rner",
                "K. McMillan",
                "A. Rybalchenko"
            ],
            "title": "On solving universally quantified horn clauses",
            "venue": "Logozzo, F., F\u00e4hndrich, M. (eds.) SAS 2013. LNCS, vol. 7935, pp. 105\u2013 125. Springer, Heidelberg",
            "year": 2013
        },
        {
            "authors": [
                "E.M. Clarke",
                "T.A. Henzinger",
                "H. Veith",
                "Bloem",
                "R. (eds."
            ],
            "title": "Handbook of Model Checking",
            "venue": "Springer",
            "year": 2018
        },
        {
            "authors": [
                "P. Cuoq",
                "F. Kirchner",
                "N. Kosmatov",
                "V. Prevosto",
                "J. Signoles",
                "B. Yakobowski"
            ],
            "title": "Frama-C-A software analysis perspective",
            "venue": "Eleftherakis, G., Hinchey, M., Hol302 J. Amilon et al. combe, M. (eds.) SEFM 2012. LNCS, vol. 7504, pp. 233\u2013247. Springer, Heidelberg",
            "year": 2012
        },
        {
            "authors": [
                "P. Daca",
                "T.A. Henzinger",
                "A. Kupriyanov"
            ],
            "title": "Array folds logic",
            "venue": "Chaudhuri, S., Farzan, A. (eds.) CAV 2016. LNCS, vol. 9780, pp. 230\u2013248. Springer, Cham",
            "year": 2016
        },
        {
            "authors": [
                "E. De Angelis",
                "M. Proietti",
                "F. Fioravanti",
                "A. Pettorossi"
            ],
            "title": "Verifying catamorphismbased contracts using constrained Horn clauses",
            "venue": "Theory Pract. Log. Program. 22(4), 555\u2013572",
            "year": 2022
        },
        {
            "authors": [
                "A.F. Donaldson",
                "D. Kroening",
                "P. R\u00fcmmer"
            ],
            "title": "Automatic analysis of scratch-pad memory code for heterogeneous multicore processors",
            "venue": "Esparza, J., Majumdar, R. (eds.) TACAS 2010. LNCS, vol. 6015, pp. 280\u2013295. Springer, Heidelberg",
            "year": 2010
        },
        {
            "authors": [
                "G. Ernst"
            ],
            "title": "Korn - software verification with Horn clauses (competition contribution)",
            "venue": "Sankaranarayanan, S., Sharygina, N. (eds.) Tools and Algorithms for the Construction and Analysis of Systems - 29th International Conference, TACAS 2023, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2022, Paris, France, April 22\u201327, 2023, Proceedings, Part II. Lecture Notes in Computer Science, vol. 13994, pp. 559\u2013564. Springer",
            "year": 2023
        },
        {
            "authors": [
                "Z. Esen",
                "P. R\u00fcmmer"
            ],
            "title": "TriCera: Verifying C programs using the theory of heaps",
            "venue": "2022 Formal Methods in Computer Aided Design, FMCAD 2022, Trento, Italy, October 17 - October 21, 2022",
            "year": 2022
        },
        {
            "authors": [
                "G. Fedyukovich",
                "S. Prabhu",
                "K. Madhukar",
                "A. Gupta"
            ],
            "title": "Quantified invariants via syntax-guided synthesis",
            "venue": "Dillig, I., Tasiran, S. (eds.) CAV 2019. LNCS, vol. 11561, pp. 259\u2013277. Springer, Cham",
            "year": 2019
        },
        {
            "authors": [
                "J. Filli\u00e2tre",
                "L. Gondelman",
                "A. Paskevich"
            ],
            "title": "The spirit of ghost code",
            "venue": "Formal Methods Syst. Des. 48(3), 152\u2013174",
            "year": 2016
        },
        {
            "authors": [
                "C. Flanagan",
                "J.B. Saxe"
            ],
            "title": "Avoiding exponential explosion: generating compact verification conditions",
            "venue": "Hankin, C., Schmidt, D. (eds.) Proceedings of: Symposium on Principles of Programming Languages (POPL\u201901), pp. 193\u2013205. ACM",
            "year": 2001
        },
        {
            "authors": [
                "P. Garg",
                "C. L\u00f6ding",
                "P. Madhusudan",
                "D. Neider"
            ],
            "title": "Learning universally quantified invariants of linear data structures",
            "venue": "Sharygina, N., Veith, H. (eds.) CAV 2013. LNCS, vol. 8044, pp. 813\u2013829. Springer, Heidelberg",
            "year": 2013
        },
        {
            "authors": [
                "P. Georgiou",
                "B. Gleiss",
                "L. Kov\u00e1cs"
            ],
            "title": "Trace logic for inductive loop reasoning",
            "venue": "2020 Formal Methods in Computer Aided Design, FMCAD 2020, Haifa, Israel, September 21\u201324, 2020, pp. 255\u2013263. IEEE",
            "year": 2020
        },
        {
            "authors": [
                "A. Gurfinkel",
                "T. Kahsai",
                "A. Komuravelli",
                "J.A. Navas"
            ],
            "title": "The SeaHorn verification framework",
            "venue": "Kroening, D., P\u0103s\u0103reanu, C.S. (eds.) CAV 2015. LNCS, vol. 9206, pp. 343\u2013361. Springer, Cham",
            "year": 2015
        },
        {
            "authors": [
                "A. Gurfinkel",
                "S. Shoham",
                "Y. Vizel"
            ],
            "title": "Quantifiers on demand",
            "venue": "Lahiri, S.K., Wang, C. (eds.) ATVA 2018. LNCS, vol. 11138, pp. 248\u2013266. Springer, Cham",
            "year": 2018
        },
        {
            "authors": [
                "J. Harrison"
            ],
            "title": "Handbook of Practical Logic and Automated Reasoning",
            "venue": "Cambridge University Press",
            "year": 2009
        },
        {
            "authors": [
                "T.A. Henzinger",
                "T. Hottelier",
                "L. Kov\u00e1cs",
                "A. Rybalchenko"
            ],
            "title": "Aligators for arrays (tool paper)",
            "venue": "Ferm\u00fcller, C.G., Voronkov, A. (eds.) LPAR 2010. LNCS, vol. 6397, pp. 348\u2013356. Springer, Heidelberg",
            "year": 2010
        },
        {
            "authors": [
                "H. Hojjat",
                "P. R\u00fcmmer"
            ],
            "title": "The ELDARICA Horn solver",
            "venue": "FMCAD 2018. pp. 1\u20137",
            "year": 2018
        },
        {
            "authors": [
                "H.G.V.K.",
                "S. Shoham",
                "A. Gurfinkel"
            ],
            "title": "Solving constrained Horn clauses modulo algebraic data types and recursive functions",
            "venue": "Proc. ACM Program. Lang. 6(POPL), 1\u201329",
            "year": 2022
        },
        {
            "authors": [
                "T. Kahsai",
                "R. Kersten",
                "P. R\u00fcmmer",
                "M. Sch\u00e4f"
            ],
            "title": "Quantified heap invariants for object-oriented programs",
            "venue": "Eiter, T., Sands, D. (eds.) LPAR-21, 21st International Conference on Logic for Programming, Artificial Intelligence and Reasoning, Maun, Botswana, May 7\u201312, 2017. EPiC Series in Computing, vol. 46, pp. 368\u2013384. EasyChair",
            "year": 2017
        },
        {
            "authors": [
                "G.T. Leavens",
                "A.L. Baker",
                "C. Ruby"
            ],
            "title": "JML: A notation for detailed design",
            "venue": "Kilov, H., Rumpe, B., Simmonds, I. (eds.) Behavioral Specifications of Businesses and Systems, The Kluwer International Series in Engineering and Computer Science, vol. 523, pp. 175\u2013188. Springer",
            "year": 1999
        },
        {
            "authors": [
                "K.R.M. Leino"
            ],
            "title": "Dafny: an automatic program verifier for functional correctness",
            "venue": "Clarke, E.M., Voronkov, A. (eds.) LPAR 2010. LNCS (LNAI), vol. 6355, pp. 348\u2013370. Springer, Heidelberg",
            "year": 2010
        },
        {
            "authors": [
                "K.R.M. Leino",
                "R. Monahan"
            ],
            "title": "Reasoning about comprehensions with first-order SMT solvers",
            "venue": "Shin, S.Y., Ossowski, S. (eds.) Proceedings of the 2009 ACM Symposium on Applied Computing (SAC), Honolulu, Hawaii, USA, March 9\u201312, 2009, pp. 615\u2013622. ACM",
            "year": 2009
        },
        {
            "authors": [
                "Y. Matsushita",
                "T. Tsukada",
                "N. Kobayashi"
            ],
            "title": "RustHorn: CHC-based verification for Rust programs",
            "venue": "ACM Trans. Program. Lang. Syst. 43(4), 15:1\u201315:54",
            "year": 2021
        },
        {
            "authors": [
                "D. Monniaux",
                "L. Gonnord"
            ],
            "title": "Cell morphing: from array programs to array-free horn clauses",
            "venue": "Rival, X. (ed.) SAS 2016. LNCS, vol. 9837, pp. 361\u2013382. Springer, Heidelberg",
            "year": 2016
        },
        {
            "authors": [
                "F. Neven",
                "T. Schwentick",
                "V. Vianu"
            ],
            "title": "Finite state machines for strings over infinite alphabets",
            "venue": "ACM Trans. Comput. Log. 5(3), 403\u2013435",
            "year": 2004
        },
        {
            "authors": [
                "S. Priya",
                "X. Zhou",
                "Y. Su",
                "Y. Vizel",
                "Y. Bao",
                "A. Gurfinkel"
            ],
            "title": "Verifying verified code",
            "venue": "Hou, Z., Ganesh, V. (eds.) ATVA 2021. LNCS, vol. 12971, pp. 187\u2013202. Springer, Cham",
            "year": 2021
        },
        {
            "authors": [
                "J.C. Reynolds"
            ],
            "title": "Separation logic: A logic for shared mutable data structures",
            "venue": "17th IEEE Symposium on Logic in Computer Science (LICS 2002), 22\u201325 July 2002, Copenhagen, Denmark, Proceedings, pp. 55\u201374. IEEE Computer Society",
            "year": 2002
        },
        {
            "authors": [
                "L. Segoufin"
            ],
            "title": "Automata and logics for words and trees over an infinite alphabet",
            "venue": "\u00c9sik, Z. (ed.) CSL 2006. LNCS, vol. 4207, pp. 41\u201357. Springer, Heidelberg",
            "year": 2006
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Overview. Program specifications are often written in expressive, high-level languages: for instance, in temporal logic [14], in first-order logic with quantifiers [28], in separation logic [40], or in specification languages that provide extended quantifiers for computing the sum or maximum value of array elements [7,33]. Specifications commonly also use a rich set of theories; for instance, specifications could be written using full Peano arithmetic, as opposed to bitvectors or linear arithmetic used in the program. Rich specification languages make it possible to express intended program behaviour in a succinct form, and as a result reduce the likelihood of mistakes being introduced in specifications.\nThere is a gap, however, between the languages used in specifications and the input languages of automatic verification tools. Software model checkers, in particular, usually require specifications to be expressed using program assertions c\u00a9 The Author(s) 2023 C. Enea and A. Lal (Eds.): CAV 2023, LNCS 13966, pp. 281\u2013304, 2023. https://doi.org/10.1007/978-3-031-37709-9_14\nand Boolean program expressions, and do not directly support any of the more sophisticated language features mentioned. In fact, rich specification languages are challenging to handle in automatic verification, since satisfiability checks can become undecidable (i.e., it is no longer decidable whether assertion failures can occur on a program path), and techniques for inferring program invariants usually focus on simple specifications only.\nTo bridge this gap, it is common practice to encode high-level specifications in the low-level assertion languages understood by the tools. For instance, temporal properties can be translated to B\u00fcchi automata, and added to programs using ghost variables and assertions [14]; quantified properties can be replaced with non-determinism, ghost variables, or loops [13,37]; sets used to specify the absence of data-races can be represented using non-deterministically initialized variables [18]. By adding ghost variables and bespoke ghost code to programs [22], many specifications can be made effectively checkable.\nThe translation of specifications to assertions or ghost code is today largely designed, or even carried out, by hand. This is an error-prone process, and for complex specifications and programs it is very hard to ensure that the low-level encoding of a specification faithfully models the original high-level properties to be checked. Mistakes have been found even in industrial, very carefully developed specifications [39], and can result in assertions that are vacuously satisfied by any program. Naturally, the manual translation of specifications also tends to be an ad-hoc process that does not easily generalise to other specifications.\nThis paper proposes the first general framework to automate the translation of rich program specifications to simpler program assertions, using a process called instrumentation. Our approach models the semantics of specific complex operations using program-independent instrumentation operators, consisting of (manually designed) rewriting rules that define how the evaluation of the operator can be achieved using simpler program statements and ghost variables. The instrumentation approach is flexible enough to cover a wide range of different operators, including operators that are best handled by weaving their evaluation into the program to be analysed. While instrumentation operators are manually written, their application to programs can be performed in a fully automatic way by means of a search procedure. The soundness of an instrumentation operator is shown formally, once and for all, by providing an instrumentation invariant that ensures that the operator can never be used to show correctness of an incorrect program.\nAdditional instrumentation operator definitions, correctness proofs, and detailed evaluation results can be found in the accompanying extended report [4].\nMotivating Example. We illustrate our approach on the computation of triangular numbers sN = (N2 + N)/2, see left-hand side of Fig. 1. For reasons of presentation, the program has been normalised by representing the square N*N using an auxiliary variable NN. While mathematically simple, verifying the postcondition s == (NN+N)/2 in the program turns out to be challenging even for state-of-the-art model checkers, as such tools are usually thrown off course by\nthe non-linear term N*N. Computing the value of NN by adding a loop in line 16 is not sufficient for most tools either, since the program in any case requires a non-linear invariant 0 <= i <= N && 2*s == i*i + i to be derived for the loop in lines 4\u201312.\nThe insight needed to elegantly verify the program is that the value i*i can be tracked during the program execution using a ghost variable x_sq. For this, the program is instrumented to maintain the relationship x_sq == i*i: initially, i == x_sq == 0, and each time the value of i is modified, also the variable x_sq is updated accordingly. With the value x_sq == i*i available, both the loop invariant and the post-condition turn into formulas over linear arithmetic, and program verification becomes largely straightforward. The challenge, of course, is to discover this program transformation automatically, and to guarantee the soundness of the process. For the example, the transformed program is shown on the right-hand side of Fig. 1, and discussed in the next paragraphs.\nOur method splits the process of program instrumentation into two parts: (i) choosing an instrumentation operator, which is defined manually, designed to be program-independent, and induces a space of possible program transformations; and (ii) carrying out an automatic application strategy to find, among the possible program transformations, one that enables verification of a program.\nAn instrumentation operator for tracking squares is shown in Fig. 2, and consists of the declaration of two ghost variables (x_sq, x_shad) with initial value 0, respectively; four rules for rewriting program statements; and the instrumentation invariant witnessing correctness of the operator. The rewrite rules use formal variables x, y, which can represent arbitrary variables in the program (i, N, NN). An application of the operator to a program will declare the ghost variables in the form of global variables, and then rewrite some chosen set of program statements using the provided rules. Since the statements to be rewritten can\nbe chosen arbitrarily, and since moreover multiple rewrite rules might apply to some statements, rewriting can result in many different variants of a program. In the example, we rewrite the assignments C, D of the left-hand side program using rewrite rules (R2) and (R4), respectively, resulting in the instrumented and correct program on the right-hand side.\nInstrumentation operators are designed to be sound, which means that rewriting a wrong selection of program statements might lead to an instrumented program that cannot be verified, i.e., in which assertions might fail, but instrumentation can never turn an incorrect source program into a correct instrumented program. This opens up the possibility to systematically search for the right program instrumentation. We propose a counterexample-guided algorithm for this purpose, which starts from some arbitrarily chosen instrumentation, checks whether the instrumented program can be verified, and otherwise attempts to fix the instrumentation using a refinement loop. As soon as a verifiable instrumented program has been found, the search can stop and the correctness of the original program has been shown.\nThe concept of instrumentation invariants is essential for guaranteeing soundness of an operator. Instrumentation invariants are formulas that can (only) refer to the ghost variables introduced by an instrumentation operator, and are formulated in such a way that they hold in every reachable state of every instrumented program. To maintain their invariants, instrumentation operators use shadow variables that duplicate the values of program variables. In the operator in Fig. 2, the purpose of the shadow variable x_shad is to reproduce the value of the program variable whose square is tracked (i). The rewriting rules introduce guards to detect incorrect instrumentation (the assertions in (R2), (R3), (R4)), which are particular cases in which some update of a relevant variable\nwas missed and not correctly instrumented. The use of shadow variables and guards make instrumentation operators very flexible; in our example, note that instrumentation tracks the square of the value of i during the loop, but is also used later to simplify the expression N*N. This is possible because of the instrumentation invariant and because i == N holds after termination of the loop, which is verified through the assertion introduced in line 14.\nContributions and Outline. The operator shown in Fig. 2 is simple, and does not apply to all programs, but it can easily be generalised to other arithmetic operators and program statements. The framework presented in this paper provides the foundation for developing a (extendable) library of formally verified instrumentation operators. In the scope of this paper, we focus on two specification constructs that have been identified as particularly challenging in the literature: existential and universal quantifiers over arrays, and aggregation (or extended quantifiers), which includes computing the sum or maximum value of elements in an array. Our experiments on benchmarks taken from the SVCOMP [8] show that even relatively simple instrumentation operators can significantly extend the capabilities of a software model checker, and often make the automatic verification of otherwise hard specifications easy.\nThe contributions of the paper are: (i) a general framework for program instrumentation, which defines a space of program transformations that work by rewriting individual statements (Sect. 2); (ii) an application strategy search algorithm in this space, for a given program (Sect. 3); (iii) two instantiations of the framework\u2014one for instrumentation operators to handle specifications with quantifiers (Sect. 4.1), and one for extended quantifiers (Sect. 4.2); (iv) machinechecked proofs of the correctness of the instrumentation operators for quantifiers \u2200 and the extended quantifier \\max; (v) a new verification tool, MonoCera, that is tailored to the verification of programs with aggregation; and (vi) an evaluation of our method and tool on a set of examples, including such from SV-COMP [8] (Sect. 5)."
        },
        {
            "heading": "2 Instrumentation Framework",
            "text": "The next two sections formally introduce the instrumentation framework. Later, we instantiate the framework for quantification and aggregation over arrays. We split the instrumentation process into two parts:\n1. An instrumentation operator that defines how to rewrite program statements with the purpose of eliminating language constructs that are difficult to reason about automatically, but leaves the choice of which occurrences of these statements to rewrite to the second part (this section). 2. An application strategy for the instrumentation operator, which can be implemented using heuristics or systematic search, among others. The strategy is responsible for selecting the right (if any) program instrumentation from the many possible ones, Sect. 3 is dedicated to the second part.\nEven though instrumentation operators are non-deterministic, we shall guarantee their soundness: if the original program has a failing assertion, so will any instrumented program, regardless of the chosen application strategy; that is, instrumentation of an incorrect program will never yield a correct program.\nWe shall also guarantee a weak form of completeness, to the effect that if an assertion that has not been added to the program by the instrumentation fails in the instrumented program, then it will also fail in the original program. As a result, any counterexample (for such an assertion) produced when verifying the instrumented program can be transformed into a counterexample for the original program."
        },
        {
            "heading": "2.1 The Core Language",
            "text": "While our implementation works on programs represented as constrained Horn clauses [12], i.e., is language-agnostic, for readability purposes we present our approach in the setting of an imperative core programming language with datatypes for unbounded integers, Booleans, and arrays, and assert and assume statements. The language is deliberately kept simple, but is still close to standard C. The main exception is the semantics of arrays: they are defined here to be functional and therefore represent a value type. Arrays have integers as index type and are unbounded, and their signature and semantics are otherwise borrowed from the SMT-LIB theory of extensional arrays [6]:\n\u2013 Reading the value of an array a at index i: select(a, i); \u2013 Updating an array a at index i with a new value x: store(a, i, x).\nThe complete syntax of the core language is given in Table 1. Programs are written using a vocabulary X of typed program variables; the typing rules of the language are given in [4]. As syntactic sugar, we sometimes write a[i] instead of select(a, i), and a[i] = x instead of a = store(a, i, x).\nWe denote by D\u03c3 the domain of a program type \u03c3. The domain of an array type Array \u03c3 is the set of functions f : Z \u2192 D\u03c3.\nSemantics. We assume the Flanagan-Saxe extended execution model of programs with assume and assert statements (see, e.g., [23]), in which executing\nan assert statement with an argument that evaluates to false fails, i.e., terminates abnormally. An assume statement with an argument that evaluates to false has the same semantics as a non-terminating loop. Partial correctness properties of programs are expressed using Hoare triples {Pre} P {Post}, which state that an execution of P , starting in a state satisfying Pre, never fails, and may only terminate in states that satisfy Post . As usual, a program P is considered (partially) correct if the Hoare triple {true} P {true} holds.\nThe evaluation of program expressions is modelled using a function \u00b7 s that maps program expressions t of type \u03c3 to their value t s \u2208 D\u03c3 in the state s."
        },
        {
            "heading": "2.2 Instrumentation Operators",
            "text": "An instrumentation operator defines schemes to rewrite programs while preserving the meaning of the existing program assertions. Without loss of generality, we restrict program rewriting to assignment statements. Instrumentation can introduce ghost state by adding arbitrary fresh variables to the program. The main part of an instrumentation consists of rewrite rules, which are schematic rules r = t s, where the meta-variable r ranges over program variables, t is an expression that can contain further meta-variables, and s is a schematic program in which the meta-variables from r = t might occur. Any assignment that matches r = t can be rewritten to s.\nDefinition 1 (Instrumentation Operator). An instrumentation operator is a tuple \u03a9 = (G,R, I), where:\n(i) G = \u3008(x1, init1), . . . , (xk, initk)\u3009 is a tuple of pairs of ghost variables and their initial values; (ii) R is a set of rewrite rules r = t s, where s is a program operating on the ghost variables x1, . . . , xk (and containing meta-variables from r = t); (iii) I is a formula over the ghost variables x1, . . . , xk, called the instrumentation invariant.\nThe rewrite rules R and the invariant I must adhere to the following constraints:"
        },
        {
            "heading": "1. The instrumentation invariant I is satisfied by the initial ghost values, i.e.,",
            "text": "it holds in the state {x1 \u2192 init1, . . . , xk \u2192 initk}. 2. For all rewrites r = t s \u2208 R the following hold: (a) s terminates (normally or abnormally) for pre-states satisfying I, assum-\ning that all meta-variables are ordinary program variables. (b) s does not assign to variables other than r or the ghost vari-\nables x1, . . . , xk. (c) s preserves the instrumentation invariant: {I} s\u2032 {I}, where s\u2032 is s with\nevery assert(e) statement replaced by an assume(e) statement. (d) s preserves the semantics of the assignment r = t: the Hoare triple\n{I} z = t; s\u2032 {z = r}, where z is a fresh variable, holds.\nThe conditions imposed in the definition ensure that all instrumentations are correct, in the sense that they are sound and weakly complete, as we show below. In particular, the instrumentation invariant guarantees that the rewrites of program statements are semantics-preserving w.r.t. the original program, and thus, the execution of any assert statement of the original program has the same effect before and after instrumentation. Observe that the conditions can themselves be deductively verified to hold for each concrete instrumentation operator, and that this check is independent of the programs to be instrumented, so that an instrumentation operator can be proven correct once and for all.\nAn instrumentation operator \u03a9 does itself not define which occurrences of program statements are to be rewritten, but only how they are rewritten. Given a program P and the operator \u03a9, an instrumented program P \u2032 is derived by carrying out the following two steps: (i) variables x1, . . . , xk and the assignments x1 = init1; . . . ; xk = initk are added at the beginning of the program, and (ii) some of the assignments in P , to which a rewriting rule r = t s in \u03a9 is applicable, are replaced by s, substituting meta-variables with the actual terms occurring in the assignment. We denote by \u03a9(P ) the set of all instrumented programs P \u2032 that can be derived in this way. An example of an instrumentation operator and its application was shown Fig. 1 and Fig. 2."
        },
        {
            "heading": "2.3 Instrumentation Correctness",
            "text": "Verification of an instrumented program produces one of two possible results: a witness if verification is successful, or a counterexample otherwise. A witness consists of the inductive invariants needed to verify the program, and is presented in the context of the programming language: it is translated back from the back-end theory used by the verification tool, and is a formula over the program variables and the ghost variables added during instrumentation. A counterexample is an execution trace leading to a failing assertion.\nDefinition 2 (Soundness). An instrumentation operator \u03a9 is called sound if for every program P and instrumented program P \u2032 \u2208 \u03a9(P ), whenever there is an execution of P where some assert statement fails, then there also is an execution of P \u2032 where some assert statement fails.\nEquivalently, existence of a witness for an instrumented program entails existence of a witness for the original program, in the form of a set of inductive invariants solely over the program variables. Notably, because of the semanticspreserving nature of the rewrites under the instrumentation invariant, a witness for the original program can be derived from one for the instrumented program. One such back-translation is to add the instrumentation invariant as a conjunct to the original witness, and to existentially quantify over the ghost variables.\nExample. To illustrate the back-translation, we return to the instrumentation operator from Fig. 2 and the example program from Fig. 1. The witness produced by our verification tool in this case is the formula:\ni = x_shad \u2227 x_sq+ x_shad = 2s \u2227 N \u2265 i \u2227 N \u2265 1 \u2227 2s \u2265 i \u2227 i \u2265 0\nAfter conjoining the instrumentation invariant x_sq = x_shad2 and existentially quantifying over the involved ghost variables, we obtain an inductive invariant that is sufficient to verify the original program:\n\u2203xsq, xshad. (i = xshad \u2227 xsq + xshad = 2s \u2227 N \u2265 i \u2227 N \u2265 1 \u2227 2s \u2265 i \u2227 i \u2265 0 \u2227 xsq = x2shad)\nDefinition 3 (Weak Completeness). The operator \u03a9 is called weakly complete if for every program P and instrumented program P \u2032 \u2208 \u03a9(P ), whenever an assert statement that has not been added to the program by the instrumentation fails in the instrumented program P \u2032, then it also fails in the original program P .\nSimilarly to the back-translation of invariants, when verification fails, counterexamples for assertions of the original program, found during verification of the instrumented program, can be translated back to counterexamples for the original program. We thus obtain the following result.\nTheorem 1 (Soundness and weak completeness). Every instrumentation operator \u03a9 is sound and weakly complete.\nProof. Let \u03a9 = (G,R, I) be an instrumentation operator. Since I is a formula over ghost variables only, which holds initially and is preserved by all rewrites, I is an invariant of the fully instrumented program. This entails that rewrites of assignments are semantics-preserving. Furthermore, since instrumentation code only assigns to ghost variables or to r (i.e., the left-hand side of the original statement), program variables have the same valuation in the instrumented program as in the original one. Furthermore, since all rewrites are terminating under I, the instrumented program will terminate if and only if the original program does.\nIn the case when verification succeeds, and a witness is produced, weak completeness follows vacuously. A witness consists of the inductive invariants sufficient to verify the instrumented program. Thus, they are also sufficient to verify the assertions existing in the original program, since assertions are not rewritten and all program variables have the same valuation in the original and the instrumented programs. Since a witness for the instrumented program can be back-translated to a witness for the original program, any failing assertion in the original program must also fail after instrumentation, and \u03a9 is therefore sound.\nIn the case when verification fails, soundness follows vacuously, and if the failing assertion was added during instrumentation, also weak completeness follows. If the assertion existed in the original program, since such assertions are not rewritten, and since program variables have the same valuation in the instrumented program as in the original program, then any counterexample for the instrumented program is also a counterexample for the original program, when projected onto the program variables.\nInput: Program P ; statements S; instrumentation space R; oracle IsCorrect . Result: Instrumentation r \u2208 R with IsCorrect(Pr); Incorrect ; or Inconclusive.\n1 begin 2 Cand \u2190 R; 3 while Cand = \u2205 do 4 pick r \u2208 Cand ; 5 if IsCorrect(Pr) then 6 return r; 7 else 8 cex \u2190 counterexample path for Pr; 9 if failing assertion in cex also exists in P then\n/* cex is also a counterexample for P */ 10 return Incorrect ; 11 else\n/* instrumentation on cex may have been incorrect */\n12 C \u2032 \u2190 {p \u2208 C | insr(p) occurs on cex}; 13 Cand \u2190 Cand \\ {r\u2032 \u2208 Cand | r(s) = r\u2032(s) for all p \u2208 C \u2032}; 14 end 15 end 16 end 17 return Inconclusive; 18 end\nAlgorithm 1: Counterexample-guided instrumentation search"
        },
        {
            "heading": "3 Instrumentation Application Strategies",
            "text": "We will now define a counterexample-guided search procedure to discover applications of instrumentation operators that make it possible to verify a program.\nFor our algorithm, we assume that we are given an oracle IsCorrect that is able to check the correctness of programs after instrumentation. Such an oracle could be approximated, for instance, using a software model checker. The oracle is free to ignore the complex functions we are trying to eliminate by instrumentation; for instance, in Fig. 1, the oracle can over-approximate the term N*N by assuming that it can have any value. We further assume that C is the set of control points of a program P corresponding to the statements to which a given set of instrumentation operators can be applied. For each control point p \u2208 C, let Q(p) be the set of rewrite rules applicable to the statement at p, including also a distinguished value \u22a5 that expresses that p is not modified. For the program in Fig. 1, for instance, the choices could be defined by Q(A) = Q(B) = {(R1),\u22a5}, Q(C) = {(R2),\u22a5}, and Q(D) = {(R4),\u22a5}, referring to the rules in Fig. 2. Any function r : C \u2192 \u22c3p\u2208C Q(p) with r(p) \u2208 Q(p)\nwill then define one possible program instrumentation. We will denote the set of well-typed functions C \u2192 \u22c3p\u2208C Q(p) by R, and the program obtained by rewriting P according to r \u2208 R by Pr. We further denote the control point in Pr corresponding to some p \u2208 C in P by insr(p).\nAlgorithm 1 presents our algorithm to search for instrumentations that are sufficient to verify a program P . The algorithm maintains a set Cand \u2286 R of remaining ways to instrument P , and in each loop considers one of the remaining elements r \u2208 Cand (line 4). If the oracle manages to verify Pr in line 5, due to soundness of instrumentation the correctness of P has been shown (line 6); if Pr is incorrect, there has to be a counterexample ending with a failing assertion (line 8). There are two possible causes of assertion failures: if the failing assertion in Pr already existed in P , then due to the weak completeness of instrumentation also P has to be incorrect (line 10). Otherwise, the program instrumentation has to be refined, and for this from Cand we remove all instrumentations r\u2032 that agree with r regarding the instrumentation of the statements occurring in the counterexample (line 13).\nSince R is finite, and at least one element of Cand is eliminated in each iteration, the refinement loop terminates. The set Cand can be exponentially big, however, and therefore should be represented symbolically (using BDDs, or using an SMT solver managing the set of blocking constraints from line 13).\nWe can observe soundness and completeness of the algorithm w.r.t. the considered instrumentation operators (proof in [4]):\nLemma 1 (Correctness of Algorithm 1). If Algorithm 1 returns an instrumentation r \u2208 R, then Pr and P are correct. If Algorithm 1 returns Incorrect , then P is incorrect. If there is r \u2208 R such that Pr is correct, then Algorithm 1 will return r\u2032 such that Pr\u2032 is correct."
        },
        {
            "heading": "4 Instrumentation Operators for Arrays",
            "text": ""
        },
        {
            "heading": "4.1 Instrumentation Operators for Quantification over Arrays",
            "text": "To handle quantifiers in a programming setting, we extend the language defined in Table 1 by adding quantified expressions over arrays, as shown in Table 2. As seen, we also extend the language with a lambda expression over two variables. The rationale for this is that many quantified properties can be expressed as a binary predicate with the first argument corresponding to the value of an element and the second to the index. This allows us to express properties over both the value of an element and its index. For example, we can express that each element\nshould be equal to its index, as is done in the example program in Fig. 3. In the program, each element in the array is assigned the value corresponding to its index, after which it is asserted that this property indeed holds.\nUsing P(x0,i0) as shorthand for (\u03bb(x,i).P)(x0,i0), the new expressions can be defined formally as:\nforall(a, l, u, \u03bb(x,i).P) s = \u2200i \u2208 [l, u). P(a[i],i) s exists(a, l, u, \u03bb(x,i).P) s = \u2203i \u2208 [l, u). P(a[i],i) s\nNote that the types of x and a must be compatible and P be a Boolean-valued expression.\nTo handle programs such as the one in Fig. 3, we turn to the instrumentation framework outlined in Sect. 2.2, which we use here to define an instrumentation operator for universal quantification. The general idea is to instrument programs with a ghost variable, tracking if some predicate holds for all elements in an interval of the array, with shadow variables representing the tracked array, and the bounds of the interval. Naturally, an instrumentation operator for existential quantification can be defined in a similar fashion. For simplicity, we shall assume a normal form of programs, into which every program can be rewritten by introducing additional variables. In the normal form, store, select and forall can only occur in simple assignment statements. For example, stores are restricted to occur in statements of the form: a\u2019 = store(a, i, x).\nOver such normalised programs, and for a universally quantified expression forall(a, l, u, \u03bb(x,i)(P)), we define the instrumentation operator \u03a9\u2200,P = (G\u2200,P , R\u2200,P , I\u2200,P ) as shown in Fig. 4 over four ghost variables. The array over which quantification occurs is tracked by qu_ar and the variables qu_lo, qu_hi represent the bounds of the currently tracked interval. The result of the quantified expression is tracked by qu_P, whose value is true iff P holds for all elements in a in the interval [qu_lo, qu_hi). The rewrite rules for stores, selects and assignments of universally quantified expressions are then defined as follows. For stores, the first if-branch resets the tracking to the one element interval [i, i+ 1) when accessing elements far outside of the currently tracked interval, or if we are tracking the empty interval (as is the case at initialisation). If an access occurs immediately adjacent to the currently tracked interval\n(e.g., if i = qu_lo \u2212 1), then that element is added to the tracked interval, and the value of qu_P is updated to also account for the value of P at index i. If instead the access is within the tracked interval, then we either reset the interval (if qu_P is false) or keep the interval unchanged (if qu_P is true). Rewrites of selects are similar to stores, except tracking does not need to be reset when reading inside the tracked interval. For rewrites of quantified expressions, if the quantified interval is empty, b is assigned true. Otherwise, assertions check that the tracked interval matches the quantified interval before assigning t to qu_P. If qu_P is true, then it is sufficient that quantification occurs over a sub-interval of the tracked interval, and vice versa if qu_P is false.\nThe result of applying \u03a9\u2200,P to the program in Fig. 3 is shown in [4]. As exhibited by the experiments in Sect. 5, the resulting program is in many cases easier to verify by state-of-the-art verification tools. Note that the instrumentation operator defined is only one possibility among many. For example, one could track several ranges simultaneously over the array in question, or also track the index of some element in the array over which P holds, or make different choices on stores outside of the tracked interval.\nThe following lemma establishes correctness of the instrumentation operator. The proof can be found in [4].\nLemma 2 (Correctness of \u03a9\u2200,P). \u03a9\u2200,P is an instrumentation operator, i.e., it adheres to the constraints imposed in Definition 1."
        },
        {
            "heading": "4.2 Instrumentation Operators for Aggregation over Arrays",
            "text": "We now turn to the verification of safety properties with aggregation. As examples of aggregation, we consider in particular the operators \\sum and \\max, calculating the sum and maximum value of an array, respectively. Aggregation is supported in the form of extended quantifiers in the specification languages JML [33] and ACSL [7], and is frequently needed for the specification of functional correctness properties. Although commonly used, most verification tools do not support aggregation, so that properties involving aggregation have to be manually rewritten using standard quantifiers, pure recursive functions, or ghost code involving loops. This reduction step is error-prone, and represents an additional complication for automatic verification approaches, but can be handled elegantly using the instrumentation framework. For generality, we formalise aggregation over arrays with the help of monoid homomorphisms.\nDefinition 4 (Monoid). A monoid is a structure (M, \u25e6, e) consisting of a nonempty set M , a binary associative operation \u25e6 on M , and a neutral element e \u2208 M . A monoid is commutative if \u25e6 is commutative. A monoid is cancellative if x \u25e6 y = x \u25e6 z implies y = z, and y \u25e6 x = z \u25e6 x implies y = z, for all x, y, z \u2208 M .\nFor aggregation, we model finite intervals of arrays using the cancellative monoid (D\u2217, \u00b7, ) of finite sequences over some data domain D. The concatenation operator \u00b7 is non-commutative. Definition 5 (Monoid Homomorphism). A monoid homomorphism is a function h : M1 \u2192 M2 between monoids (M1, \u25e61, e1) and (M2, \u25e62, e2) with the properties h(x \u25e61 y) = h(x) \u25e62 h(y) and h(e1) = e2.\nOrdinary quantifiers can be modelled as homormorphisms D\u2217 \u2192 B, so that the instrumentation in this section strictly generalizes Sect. 4.1. A second classical example is the computation of the maximum (similarly, minimum) value in a sequence. For the domain of integers, the natural monoid to use is the algebra (Z\u2212\u221e,max,\u2212\u221e) of integers extended with \u2212\u221e,1 and the homomorphism hmax is generated by mapping singleton sequences \u3008n\u3009 to the value n. A 1 For machine integers, \u2212\u221e could be replaced with INT_MIN.\nthird example is the computation of the element sum of an integer sequence, corresponding to the monoid (Z ,+, 0) and the homomorphism hsum. Similarly, the number of occurrences of some element can be computed. The considered monoid in the last two cases of aggregation is even cancellative.\nProgramming Language with Aggregation. We extend our core programming language with expressions aggregateM,h(\u3008Expr\u3009,\u3008Expr\u3009,\u3008Expr\u3009), and use monoid homomorphisms to formalise them. Recall that we denote by D\u03c3 the domain of a program type \u03c3.\nDefinition 6. Let Array \u03c3 be an array type, \u03c3M a program type, M a commutative monoid that is a subset of D\u03c3M , and h : D\u2217\u03c3 \u2192 M a monoid homomorphism. Let furthermore ar be an expression of type Array \u03c3, and l and u integer expressions. Then, aggregateM,h(ar,l,u) is an expression of type \u03c3M , with semantics defined by:\naggregateM,h(ar,l,u) s = h(\u3008 ar s( l s), ar s( l s + 1), . . . , ar s( u s \u2212 1)\u3009)\nIntuitively, the expression aggregateM,h(ar,l,u) denotes the result of applying the homomorphism h to the slice ar [l .. u \u2212 1] of the array ar . As a convention, in case u < l we assume that the result of aggregate is h(\u3008\u3009). As with array accesses, we assume also that aggregate only occurs in normalised statements of the form t = aggregateM,h(ar,l,u).\nIn our examples, we use derived operations as found in ACSL: \\max as shorthand notation for aggregate(Z\u2212\u221e,max,\u2212\u221e),hmax\n2, and \\sum as short-hand notation for aggregate(Z,+,0),hsum .\nAn Instrumentation Operator for Maximum. For \\max, an operator \u03a9max = (Gmax , Rmax , Imax ) can be defined similarly to the operator \u03a9\u2200,P from Sect. 4.1, in that the maximum value in a particular interval of the array is tracked. One key difference is that an extra ghost variable ag_max_idx is added to track an array index where the maximum value of the array interval is stored, in order to not have to reset tracking on every store inside of the tracked interval. A complete definition is proposed in [4].\nAn Instrumentation Operator for Sum. Cancellative aggregation is aggregation based on a cancellative monoid. Cancellative aggregation makes it possible to track aggregate values faithfully even when storing inside of the tracked interval, unlike \\max and universal quantification. An example of a cancellative operator is the aggregate \\sum .\nThe instrumentation operator \u03a9sum = (Gsum , Rsum , Isum) is defined in Fig. 5. The instrumentation code tracks the sum of values in the interval, and 2 With a slight abuse of the framework, we assume that Z\u2212\u221e is represented by the pro-\ngram type Int, mapping \u2212\u221e to some fixed integer number. More elegant solutions are not difficult to devise, but add unnecessary complexity.\nwhen increasing the bounds of the tracked interval, the new values are simply added to the tracked sum. Since \\sum is cancellative, when storing inside of the tracked interval, the previous value at the index being written to is first subtracted from the sum, before adding the new value, ensuring that the correct aggregate value is computed. The following correctness result is proved in [4].\nLemma 3. (Correctness of \u03a9sum). \u03a9sum is an instrumentation operator, i.e., it adheres to the constraints imposed in Definition 1.\nDeductive Verification of Instrumentation Operators. As stated in Sect. 2.2, instrumentation operators may be verified independently of the programs to be instrumented. The operators described in this paper, i.e. square, universal quantification, maximum, and sum, have been verified in the\nverification tool Frama-C [15]. The verified instrumentations are adaptations for the C language semantics and execution model. More specifically, the adapted operators assume C native arrays, rather than functional ones."
        },
        {
            "heading": "5 Evaluation",
            "text": ""
        },
        {
            "heading": "5.1 Implementation",
            "text": "To evaluate our instrumentation framework, we have implemented the instrumentation operators for quantifiers and aggregation over arrays. The implementation is done over constrained Horn clauses (CHCs), by adding the rewrite rules defined in Sect. 4 to Eldarica [30], an open-source solver for CHCs. We also implemented the automatic application of the instrumentation operators, largely following Algorithm 1 but with a few minor changes due to the CHC setting. The CHC setting makes our implementation available to various CHC-based verification tools, for instance JayHorn (Java) [32], Korn (C) [19], RustHorn (Rust) [36], SeaHorn (C/LLVM) [26] and TriCera (C) [20].\nIn order to evaluate our approach at the level of C programs, we extended TriCera, an open-source assertion-based model checker that translates C programs into a set of CHCs and relies on Eldarica as back-end solver. TriCera is extended to parse quantifiers and aggregation operators in its input C programs and to encode them as part of the translation into CHCs. We call the resulting toolchain MonoCera. An artefact that includes MonoCera and the benchmarks is available online [5].\nTo handle complicated access patterns, for instance a program processing an array from the beginning and end at the same time, the implementation can apply multiple instrumentation operators simultaneously; the number of operators is incremented when Algorithm 1 returns Inconclusive."
        },
        {
            "heading": "5.2 Experiments and Comparisons",
            "text": "To assess our implementation, we assembled a test suite and carried out experiments comparing MonoCera with the state-of-the-art C model checkers CPAchecker 2.1.1 [11], SeaHorn 10.0.0 [26] and TriCera 0.2. It should be noted that deductive verification frameworks, such as Dafny and Frama-C, can handle, for example, the program in Fig. 3 if they are provided with a manually written loop invariant; however, since MonoCera relies on automatic techniques for invariant inference, we only benchmark against tools using similar automatic techniques. We also excluded VeriAbs [1], since its licence does not permit its use for scientific evaluation.\nThe tools were set up, as far as possible, with equivalent configurations; for instance, to use the SMT-LIB theory of arrays [6] in order to model C arrays, and a mathematical (as opposed to machine) semantics of integers. CPAchecker was configured to use k-induction [10], which was the only configuration that worked in our tests using mathematical integers. SeaHorn was run using the default settings. All tests were run on a Linux machine with AMD Opteron 2220 SE @ 2.8GHz and 6 GB RAM with a timeout of 300 s.\nTest Suite. The comparison includes a set of programs calculating properties related to the quantification and aggregation properties over arrays. The benchmarks and verification results are summarised in Table 3. The benchmark suite contains programs ranging between 16 to 117 LOC and is comprised of two parts: (i) 117 programs taken from the SV-COMP repository [9], and (ii) 26 programs crafted by the authors (min: 6, max: 8, sum: 9, forall: 3).\nTo construct the SV-COMP benchmark set for MonoCera we gathered all test files from the directories prefixed with array or loop, and singled out programs containing some assert statement that could be rewritten using a quantifier or an aggregation operator over a single array. For example, loops\nfor (int i = 0; i < N; i++) assert(a[i] <= 0);\ncan be rewritten using forall or max operators. We created a benchmark for each possible rewriting; for instance, in the case of max, by rewriting the loop into assert(\\max(a, 0, N) <= 0) . The original benchmarks were used for the evaluation of the other tools, none of which supported (extended) quantifiers.\nIn (ii), we crafted 9 programs that make use of aggregation or quantifiers, and derived further benchmarks by considering different array sizes (10, 100 and unbounded size); one combination (unbounded array inside a struct) had to be excluded, as it is not valid C. In order to evaluate other tools on our crafted benchmarks, we reversed the process described for the SV-COMP benchmarks and translated the operators into corresponding loop constructs.\nResults. In Table 3, we present the number of verified programs per instrumentation operator for each tool, as well as further statistics for MonoCera regarding verification times and instrumentation search space. The \u201cInst. space\u201d column indicates the size of the instrumentation search space (i.e., number of instrumentations producible by applying the non-deterministic instrumentation operator). \u201cInst. steps\u201d column indicates the number of attempted instrumentations, i.e., number of iterations in the while-loop in Algorithm 1. In our implementation, the check in Algorithm 1 line 5 can time out and cause the check to be repeated at a later time with a greater timeout, which can lead to more iterations than the size of the search space. In [4], we list results per benchmark for each tool.\nFor the SV-COMP benchmarks, CPAchecker managed to verify 1 program, while SeaHorn and TriCera could not verify any programs. MonoCera verified in total 42 programs from SV-COMP. Regarding the crafted benchmarks, several tools could verify the examples with array size 10. However, when the array size was 100 or unbounded, only MonoCera succeeded."
        },
        {
            "heading": "6 Related Work",
            "text": "It is common practice, in both model checking and deductive verification, to translate high-level specifications to low-level specifications prior to verification (e.g., [13,14,18,37]). Such translations often make use of ghost variables and ghost code, although relatively little systematic research has been done on the required properties of ghost code [22]. The addition of ghost variables to a program for tracking the value of complex expressions also has similarities with the concept of term abstraction in Horn solving [3]. To the best of our knowledge, we are presenting the first general framework for automatic program instrumentation.\nA lot of research in software model checking considered the handling of standard quantifiers \u2200,\u2203 over arrays. In the setting of constrained Horn clauses, properties with universal quantifiers can sometimes be reduced to quantifier-free reasoning over non-linear Horn clauses [13,37]. Our approach follows the same philosophy of applying an up-front program transformation, but in a more general setting. Various direct approaches to infer quantified array invariants have been proposed as well: e.g., by extending the IC3 algorithm [27], syntax-guided synthesis [21], learning [24], by solving recurrence equations [29], backward reachability [3], or superposition [25]. To the best of our knowledge, such methods have not been extended to aggregation.\nDeductive verification tools usually have rich support for quantified specifications, but rely on auxiliary assertions like loop invariants provided by the user, and on SMT solvers or automated theorem provers for quantifier reasoning. Although several deductive verification tools can parse extended quantifiers, few offer support for reasoning about them. Our work is closest to the method for handling comprehension operators in Spec# [35], which relies on code annotations provided by the user, but provides heuristics to automatically verify such annotations. The code instrumentation presented in this paper has similarity with the proof rules in Spec#; the main differences are that our method is based on an upfront program transformation, and that we aim at automatically finding required program invariants, as opposed to only verifying their correctness. The KeY tool provides proof rules similar to the ones in Spec# for some of the JML extended quantifiers [2]; those proof rules can be applied manually to verify human-written invariants. The Frama-C system [15] can parse ACSL extended quantifiers [7], but, to the best of our knowledge, none of the Frama-C plugins can automatically process such quantifiers. Other systems, e.g., Dafny [34], require users to manually define aggregation operators as recursive functions.\nIn the theory of algebraic data-types, several transformation-based approaches have been proposed to verify properties that involve recursive functions or catamorphisms [17,31]. Aggregation over arrays resembles the evaluation of recursive functions over data-types; a major difference is that data-types are more restricted with respect to accessing and updating data than arrays.\nArray folds logic (AFL) [16] is a decidable logic in which properties on arrays beyond standard quantification can be expressed: for instance, counting the number of elements with some property. Similar properties can be expressed using automata on data words [41], or in variants of monadic second-order logic [38]. Such languages can be seen as alternative formalisms to aggregation or extended quantifiers; they do not cover, however, all kinds of aggregation we are interested in. Array sums cannot be expressed in AFL or data automata, for instance."
        },
        {
            "heading": "7 Conclusion",
            "text": "We have presented a framework for automatic and provably correct program instrumentation, allowing the automatic verification of programs containing certain expressive language constructs, which are not directly supported by the existing automatic verification tools. Our experiments with a prototypical implementation, in the tool MonoCera, show that our method is able to automatically verify a significant number of benchmark programs involving quantification and aggregation over arrays that are beyond the scope of other tools.\nThere are still various other benchmarks that MonoCera (as well as other tools) cannot verify. We believe that many of those benchmarks are in reach of our method, because of the generality of our approach. Ghost code is known to be a powerful specification mechanism; similarly, in our setting, more powerful instrumentation operators can be easily formulated for specific kinds of programs. In future work, we therefore plan to develop a library of instrumentation operators for different language constructs (including arithmetic operators), non-linear arithmetic, other types of structures with regular access patterns such as binary heaps, and general linked-data structures.\nWe also plan to refine our method for showing incorrectness of programs more efficiently, as the approach is currently applicable mainly for verifying correctness (experiments in [4]). Another line of work is the establishment of stronger completeness results than the weak completeness result presented here, for specific programming language fragments.\nAcknowledgements. This work has been partially funded by the Swedish Vinnova FFI Programme under grant 2021-02519, the Swedish Research Council (VR) under grant 2018-04727, the Swedish Foundation for Strategic Research (SSF) under the project WebSec (Ref. RIT17-0011), and the Wallenberg project UPDATE. We are also grateful for the opportunity to discuss the research at the Dagstuhl Seminar 22451 on \u201cPrinciples of Contract Languages.\u201d"
        }
    ],
    "title": "Automatic Program Instrumentation for Automatic Verification",
    "year": 2023
}