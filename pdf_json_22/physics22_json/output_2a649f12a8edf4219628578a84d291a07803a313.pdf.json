{
    "abstractText": "The brain is a highly complex system. Most of such complexity stems from the intermingled connections between its parts, which give rise to rich dynamics and to the emergence of high-level cognitive functions. Disentangling the underlying network structure is crucial to understand the brain functioning under both healthy and pathological conditions. Yet, analyzing brain networks is challenging, in part because their structure represents only one possible realization of a generative stochastic process which is in general unknown. Having a formal way to cope with such intrinsic variability is therefore central for the characterization of brain network properties. Addressing this issue entails the development of appropriate tools mostly adapted from network science and statistics. Here, we focus on a particular class of maximum entropy models for networks, i.e. exponential random graph models (ERGMs), as a parsimonious approach to identify the local connection mechanisms behind observed global network structure. Efforts are reviewed on the quest for basic organizational properties of human brain networks, as well as on the identification of predictive biomarkers of neurological diseases such as stroke. We conclude with a discussion on how emerging results and tools from statistical graph modeling, associated with forthcoming improvements in experimental data acquisition, could lead to a finer probabilistic description of complex systems in network neuroscience.",
    "authors": [
        {
            "affiliations": [],
            "name": "Vito Dichio"
        },
        {
            "affiliations": [],
            "name": "Fabrizio De Vico Fallani"
        }
    ],
    "id": "SP:7e99e6266d78afeea4f3e35dba001309f1c250ae",
    "references": [
        {
            "authors": [
                "Herbert A Simon"
            ],
            "title": "The architecture of complexity. In Facets of systems science, pages 457\u2013476",
            "year": 1991
        },
        {
            "authors": [
                "Christopher W Lynn",
                "Danielle S Bassett"
            ],
            "title": "The physics of brain network structure, function and control",
            "venue": "Nature Reviews Physics,",
            "year": 2019
        },
        {
            "authors": [
                "Ed Bullmore",
                "Olaf Sporns"
            ],
            "title": "Complex brain networks: graph theoretical analysis of structural and functional systems",
            "venue": "Nature reviews neuroscience,",
            "year": 2009
        },
        {
            "authors": [
                "Danielle S Bassett",
                "Olaf Sporns"
            ],
            "title": "Network neuroscience",
            "venue": "Nature neuroscience,",
            "year": 2017
        },
        {
            "authors": [
                "Cornelis J Stam"
            ],
            "title": "Modern network science of neurological disorders",
            "venue": "Nature Reviews Neuroscience,",
            "year": 2014
        },
        {
            "authors": [
                "Danielle S Bassett",
                "Perry Zurn",
                "Joshua I Gold"
            ],
            "title": "On the nature and use of models in network neuroscience",
            "venue": "Nature Reviews Neuroscience,",
            "year": 2018
        },
        {
            "authors": [
                "Olaf Sporns",
                "Dante R Chialvo",
                "Marcus Kaiser",
                "Claus C Hilgetag"
            ],
            "title": "Complex networks: small-world and scale-free architectures",
            "venue": "Trends in Cognitive Sciences,",
            "year": 2004
        },
        {
            "authors": [
                "Richard F Betzel",
                "Andrea Avena-Koenigsberger",
                "Joaq\u00fa\u0131n Go\u00f1i",
                "Ye He",
                "Marcel A De Reus",
                "Alessandra Griffa",
                "Petra E V\u00e9rtes",
                "Bratislav M\u01d0sic",
                "Jean-Philippe Thiran",
                "Patric Hagmann"
            ],
            "title": "Generative models of the human connectome",
            "year": 2016
        },
        {
            "authors": [
                "Ed Bullmore",
                "Olaf Sporns"
            ],
            "title": "The economy of brain network organization",
            "venue": "Nature reviews neuroscience,",
            "year": 2012
        },
        {
            "authors": [
                "Fabrizio de Vico Fallani",
                "Jonas Richiardi",
                "Mario Chavez",
                "Sophie Achard"
            ],
            "title": "Graph analysis of functional brain networks: practical issues in translational neuroscience",
            "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences,",
            "year": 2013
        },
        {
            "authors": [
                "Sergei Maslov",
                "Kim Sneppen"
            ],
            "title": "Specificity and stability in topology of protein",
            "venue": "networks. Science,",
            "year": 2002
        },
        {
            "authors": [
                "Richard F Betzel",
                "John DMedaglia",
                "Danielle S Bassett"
            ],
            "title": "Diversity of meso-scale architecture in human and non-human connectomes",
            "venue": "Nature communications,",
            "year": 2018
        },
        {
            "authors": [
                "Joshua Faskowitz",
                "Xiaoran Yan",
                "Xi-Nian Zuo",
                "Olaf Sporns"
            ],
            "title": "Weighted stochastic block models of the human connectome across the life span",
            "venue": "Scientific reports,",
            "year": 2018
        },
        {
            "authors": [
                "Edwin T Jaynes"
            ],
            "title": "Information theory and statistical mechanics",
            "venue": "Physical review,",
            "year": 1957
        },
        {
            "authors": [
                "Giulio Cimini",
                "Tiziano Squartini",
                "Fabio Saracco",
                "Diego Garlaschelli",
                "Andrea Gabrielli",
                "Guido Caldarelli"
            ],
            "title": "The statistical physics of real-world networks",
            "venue": "Nature Reviews Physics,",
            "year": 2019
        },
        {
            "authors": [
                "David R Hunter",
                "Mark S Handcock"
            ],
            "title": "Inference in curved exponential family models for networks",
            "venue": "Journal of Computational and Graphical Statistics,",
            "year": 2006
        },
        {
            "authors": [
                "Stefano Boccaletti",
                "Vito Latora",
                "Yamir Moreno",
                "Martin Chavez",
                "D-U Hwang"
            ],
            "title": "Complex networks: Structure and dynamics",
            "venue": "Physics reports,",
            "year": 2006
        },
        {
            "authors": [
                "Olaf Sporns",
                "Richard F Betzel"
            ],
            "title": "Modular brain networks",
            "venue": "Annual review of psychology,",
            "year": 2016
        },
        {
            "authors": [
                "Danielle Smith Bassett",
                "ED Bullmore"
            ],
            "title": "Small-world brain networks",
            "venue": "The neuroscientist,",
            "year": 2006
        },
        {
            "authors": [
                "Danielle S Bassett",
                "Edward T Bullmore"
            ],
            "title": "Small-world brain networks revisited",
            "venue": "The Neuroscientist,",
            "year": 2017
        },
        {
            "authors": [
                "Gaolang Gong",
                "Yong He",
                "Luis Concha",
                "Catherine Lebel",
                "Donald W Gross",
                "Alan C Evans",
                "Christian Beaulieu"
            ],
            "title": "Mapping anatomical connectivity patterns of human cerebral cortex using in vivo diffusion tensor imaging tractography",
            "venue": "Cerebral cortex,",
            "year": 2009
        },
        {
            "authors": [
                "Sophie Achard",
                "Raymond Salvador",
                "Brandon Whitcher",
                "John Suckling",
                "ED Bullmore"
            ],
            "title": "A resilient, low-frequency, small-world human brain functional network with highly connected association cortical hubs",
            "venue": "Journal of Neuroscience,",
            "year": 2006
        },
        {
            "authors": [
                "Robert Rosenbaum",
                "Matthew A Smith",
                "Adam Kohn",
                "Jonathan E Rubin",
                "Brent Doiron"
            ],
            "title": "The spatial structure of correlated neuronal variability",
            "venue": "Nature neuroscience,",
            "year": 2017
        },
        {
            "authors": [
                "Alex Fornito",
                "Andrew Zalesky",
                "Michael Breakspear"
            ],
            "title": "The connectomics of brain disorders",
            "venue": "Nature Reviews Neuroscience,",
            "year": 2015
        },
        {
            "authors": [
                "Derek K Jones",
                "Thomas R Kn\u00f6sche",
                "Robert Turner"
            ],
            "title": "White matter integrity, fiber count, and other fallacies: the do\u2019s and don\u2019ts of diffusion",
            "venue": "mri. Neuroimage,",
            "year": 2013
        },
        {
            "authors": [
                "Onerva Korhonen",
                "Massimiliano Zanin",
                "David Papo"
            ],
            "title": "Principles and open questions in functional brain network reconstruction",
            "venue": "Human Brain Mapping,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Fornito",
                "Andrew Zalesky",
                "Edward Bullmore"
            ],
            "title": "Fundamentals of brain network analysis",
            "year": 2016
        },
        {
            "authors": [
                "Peter J Basser",
                "Sinisa Pajevic",
                "Carlo Pierpaoli",
                "Jeffrey Duda",
                "Akram Aldroubi"
            ],
            "title": "In vivo fiber tractography using dt-mri data",
            "venue": "Magnetic resonance in medicine,",
            "year": 2000
        },
        {
            "authors": [
                "Juliana Gonzalez-Astudillo",
                "Tiziana Cattai",
                "Giulia Bassignana",
                "Marie-Constance Corsi",
                "Fabrizio De Vico Fallani"
            ],
            "title": "Network-based brain\u2013computer interfaces: principles and applications",
            "venue": "Journal of Neural Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "J Craig Henry"
            ],
            "title": "Electroencephalography: basic principles, clinical applications, and related fields",
            "venue": "Neurology, 67(11):2092\u20132092,",
            "year": 2006
        },
        {
            "authors": [
                "Marcus E Raichle"
            ],
            "title": "Behind the scenes of functional brain imaging: a historical and physiological perspective",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 1998
        },
        {
            "authors": [
                "Raymond Salvador",
                "John Suckling",
                "Martin R Coleman",
                "John D Pickard",
                "David Menon",
                "ED Bullmore"
            ],
            "title": "Neurophysiological architecture of functional magnetic resonance images of human brain",
            "venue": "Cerebral cortex,",
            "year": 2005
        },
        {
            "authors": [
                "Bernadette CM Van Wijk",
                "Cornelis J Stam",
                "Andreas Daffertshofer"
            ],
            "title": "Comparing brain networks of different size and connectivity density using graph theory",
            "venue": "PloS one,",
            "year": 2010
        },
        {
            "authors": [
                "Fabrizio De Vico Fallani",
                "Vito Latora",
                "Mario Chavez"
            ],
            "title": "A topological criterion for filtering information in complex brain networks",
            "venue": "PLoS computational biology,",
            "year": 2017
        },
        {
            "authors": [
                "Kathleen A Garrison",
                "Dustin Scheinost",
                "Emily S Finn",
                "Xilin Shen",
                "R Todd Constable"
            ],
            "title": "The (in) stability of functional brain network measures across thresholds",
            "year": 2015
        },
        {
            "authors": [
                "Fabrizio De Vico Fallani",
                "Floriana Pichiorri",
                "Giovanni Morone",
                "Marco Molinari",
                "Fabio Babiloni",
                "Febo Cincotti",
                "Donatella Mattia"
            ],
            "title": "Multiscale topological properties of functional brain networks during motor imagery after stroke",
            "year": 2013
        },
        {
            "authors": [
                "M \u00c1ngeles Serrano",
                "Mari\u00e1n Bogun\u00e1",
                "Alessandro Vespignani"
            ],
            "title": "Extracting the multiscale backbone of complex weighted networks",
            "venue": "Proceedings of the national academy of sciences,",
            "year": 2009
        },
        {
            "authors": [
                "Bernard O Koopman"
            ],
            "title": "On distributions admitting a sufficient statistic",
            "venue": "Transactions of the American Mathematical society,",
            "year": 1936
        },
        {
            "authors": [
                "Edwin James George Pitman"
            ],
            "title": "Sufficient statistics and intrinsic accuracy",
            "venue": "In Mathematical Proceedings of the cambridge Philosophical society,",
            "year": 1936
        },
        {
            "authors": [
                "Ronald A Fisher"
            ],
            "title": "On the mathematical foundations of theoretical statistics",
            "venue": "Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character,",
            "year": 1922
        },
        {
            "authors": [
                "Ronald A Fisher"
            ],
            "title": "Theory of statistical estimation",
            "venue": "In Mathematical proceedings of the Cambridge philosophical society,",
            "year": 1925
        },
        {
            "authors": [
                "Ole Barndorff-Nielsen"
            ],
            "title": "Information and exponential families: in statistical theory",
            "year": 2014
        },
        {
            "authors": [
                "Lawrence D Brown"
            ],
            "title": "Fundamentals of statistical exponential families: with applications in statistical decision theory",
            "year": 1978
        },
        {
            "authors": [
                "Paul W Holland",
                "Samuel Leinhardt"
            ],
            "title": "An exponential family of probability distributions for directed graphs",
            "venue": "Journal of the american Statistical association,",
            "year": 1981
        },
        {
            "authors": [
                "David Strauss"
            ],
            "title": "On a general class of models for interaction",
            "venue": "SIAM review,",
            "year": 1986
        },
        {
            "authors": [
                "Julian Besag"
            ],
            "title": "Spatial Interaction and the Statistical Analysis of Lattice Systems",
            "venue": "Journal of the Royal Statistical Society. Series B (Methodological),",
            "year": 1974
        },
        {
            "authors": [
                "Stanley Wasserman",
                "Philippa Pattison"
            ],
            "title": "Logit models and logistic regressions for social networks: I. an introduction to markov graphs andp",
            "year": 1996
        },
        {
            "authors": [
                "Carolyn J Anderson",
                "Stanley Wasserman",
                "Bradley Crouch"
            ],
            "title": "A p* primer: Logit models for social networks",
            "venue": "Social networks,",
            "year": 1999
        },
        {
            "authors": [
                "Ginestra Bianconi"
            ],
            "title": "Entropy of network ensembles",
            "venue": "Physical Review E,",
            "year": 2009
        },
        {
            "authors": [
                "Filippo Radicchi",
                "Dmitri Krioukov",
                "Harrison Hartle",
                "Ginestra Bianconi"
            ],
            "title": "Classical information theory of networks",
            "venue": "Journal of Physics: Complexity,",
            "year": 2020
        },
        {
            "authors": [
                "Matthew J Silk",
                "Darren P Croft",
                "Richard J Delahay",
                "David J Hodgson",
                "Nicola Weber",
                "Mike Boots",
                "Robbie A McDonald"
            ],
            "title": "The application of statistical network models in disease research",
            "venue": "Methods in Ecology and Evolution,",
            "year": 2017
        },
        {
            "authors": [
                "Mark S Handcock"
            ],
            "title": "Statistical models for social networks: Inference and degeneracy",
            "year": 2003
        },
        {
            "authors": [
                "Garry Robins",
                "Pip Pattison",
                "Yuval Kalish",
                "Dean Lusher"
            ],
            "title": "An introduction to exponential random graph (p*) models for social networks",
            "venue": "Social networks,",
            "year": 2007
        },
        {
            "authors": [
                "Dean Lusher",
                "Johan Koskinen",
                "Garry Robins"
            ],
            "title": "Exponential random graph models for social networks: Theory, methods, and applications, volume 35",
            "year": 2013
        },
        {
            "authors": [
                "Mark S Handcock",
                "David R Hunter",
                "Carter T Butts",
                "Steven M Goodreau",
                "Martina Morris"
            ],
            "title": "statnet: Software tools for the representation, visualization, analysis and simulation of network data",
            "venue": "Journal of statistical software,",
            "year": 2008
        },
        {
            "authors": [
                "Steven M Goodreau",
                "Mark S Handcock",
                "David R Hunter",
                "Carter T Butts",
                "Martina Morris"
            ],
            "title": "A statnet tutorial",
            "venue": "Journal of statistical software,",
            "year": 2008
        },
        {
            "authors": [
                "David R Hunter",
                "Mark S Handcock",
                "Carter T Butts",
                "Steven M Goodreau",
                "Martina Morris"
            ],
            "title": "ergm: A package to fit, simulate and diagnose exponential-family models for networks",
            "venue": "Journal of statistical software,",
            "year": 2008
        },
        {
            "authors": [
                "H Chau Nguyen",
                "Riccardo Zecchina",
                "Johannes Berg"
            ],
            "title": "Inverse statistical problems: from the inverse ising problem to data science",
            "venue": "Advances in Physics,",
            "year": 2017
        },
        {
            "authors": [
                "Simona Cocco",
                "Christoph Feinauer",
                "Matteo Figliuzzi",
                "R\u00e9mi Monasson",
                "Martin Weigt"
            ],
            "title": "Inverse statistical physics of protein sequences: a key issues review",
            "venue": "Reports on Progress in Physics,",
            "year": 2018
        },
        {
            "authors": [
                "Charles J Geyer"
            ],
            "title": "Markov chain monte carlo maximum likelihood",
            "year": 1991
        },
        {
            "authors": [
                "Charles J Geyer",
                "Elizabeth A Thompson"
            ],
            "title": "Constrained monte carlo maximum likelihood for dependent data",
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological),",
            "year": 1992
        },
        {
            "authors": [
                "Bruce A Desmarais",
                "Skyler J Cranmer"
            ],
            "title": "Statistical inference for valued-edge networks: The generalized exponential random graph model",
            "venue": "PloS one,",
            "year": 2012
        },
        {
            "authors": [
                "David R. Hunter",
                "Carter T. Butts",
                "Chad Klumb",
                "Steven M. Goodreau",
                "Martina Morris"
            ],
            "title": "Statnet: Tools for the statistical modeling of network data",
            "year": 2022
        },
        {
            "authors": [
                "Alberto Caimo",
                "Nial Friel"
            ],
            "title": "Bayesian inference for exponential random graph models",
            "venue": "Social Networks,",
            "year": 2011
        },
        {
            "authors": [
                "James D Wilson",
                "Matthew J Denny",
                "Shankar Bhamidi",
                "Skyler J Cranmer",
                "Bruce A Desmarais"
            ],
            "title": "Stochastic weighted graphs: Flexible model specification and simulation",
            "venue": "Social Networks,",
            "year": 2017
        },
        {
            "authors": [
                "Michael Schweinberger",
                "Pamela Luna"
            ],
            "title": "Hergm: Hierarchical exponential-family random graph models",
            "venue": "Journal of Statistical Software,",
            "year": 2018
        },
        {
            "authors": [
                "Pavel N Krivitsky",
                "Martina Morris"
            ],
            "title": "Inference for social network models from egocentrically sampled data, with application to understanding persistent racial disparities in hiv prevalence in the us",
            "venue": "The annals of applied statistics,",
            "year": 2017
        },
        {
            "authors": [
                "Pavel N Krivitsky",
                "Mark S Handcock"
            ],
            "title": "A separable model for dynamic networks",
            "venue": "Journal of the Royal Statistical Society. Series B, Statistical Methodology,",
            "year": 2014
        },
        {
            "authors": [
                "Philip Leifeld",
                "Skyler J Cranmer",
                "Bruce A Desmarais"
            ],
            "title": "Temporal exponential random graph models with btergm: Estimation and bootstrap confidence intervals",
            "venue": "Journal of Statistical Software,",
            "year": 2018
        },
        {
            "authors": [
                "V Dichio"
            ],
            "title": "ergm minimal",
            "venue": "https://github.com/dichio/ergm_minimal,",
            "year": 2023
        },
        {
            "authors": [
                "Pavel N Krivitsky",
                "David R Hunter",
                "Martina Morris",
                "Chad Klumb"
            ],
            "title": "ergm 4: New features for analyzing exponential-family random graph models",
            "venue": "Journal of Statistical Software,",
            "year": 2023
        },
        {
            "authors": [
                "Juyong Park",
                "M.E.J. Newman"
            ],
            "title": "Statistical mechanics of networks",
            "venue": "Physical Review E,",
            "year": 2007
        },
        {
            "authors": [
                "R\u00e9ka Albert",
                "Albert-L\u00e1szl\u00f3 Barab\u00e1si"
            ],
            "title": "Statistical mechanics of complex networks",
            "venue": "Reviews of modern physics,",
            "year": 2002
        },
        {
            "authors": [
                "Paul Erdos",
                "Alfr\u00e9d R\u00e9nyi"
            ],
            "title": "On the evolution of random graphs",
            "venue": "Publ. Math. Inst. Hung. Acad. Sci,",
            "year": 1960
        },
        {
            "authors": [
                "R\u00e9ka Albert",
                "Albert-L\u00e1szl\u00f3 Barab\u00e1si"
            ],
            "title": "Statistical mechanics of complex networks",
            "venue": "Reviews of modern physics,",
            "year": 2002
        },
        {
            "authors": [
                "Tom AB Snijders"
            ],
            "title": "Markov chain monte carlo estimation of exponential random graph models",
            "venue": "Journal of Social Structure,",
            "year": 2002
        },
        {
            "authors": [
                "Mark S Handcock",
                "Garry Robins",
                "Tom Snijders",
                "Jim Moody",
                "Julian Besag"
            ],
            "title": "Assessing degeneracy in statistical models of social networks",
            "venue": "Technical report, Working paper,",
            "year": 2003
        },
        {
            "authors": [
                "Alessandro Rinaldo",
                "Stephen E Fienberg",
                "Yi Zhou"
            ],
            "title": "On the geometry of discrete exponential families with application to exponential random graph models",
            "venue": "Electronic Journal of Statistics,",
            "year": 2009
        },
        {
            "authors": [
                "Michael Schweinberger"
            ],
            "title": "Instability, sensitivity, and degeneracy of discrete exponential families",
            "venue": "Journal of the American Statistical Association,",
            "year": 2011
        },
        {
            "authors": [
                "Juyong Park",
                "Mark EJ Newman"
            ],
            "title": "Solution of the two-star model of a network. Physical Statistical models of complex brain networks: a maximum entropy approach",
            "venue": "Review E,",
            "year": 2004
        },
        {
            "authors": [
                "Juyong Park",
                "Mark EJ Newman"
            ],
            "title": "Solution for the properties of a clustered network",
            "venue": "Physical Review E,",
            "year": 2005
        },
        {
            "authors": [
                "Michael Schweinberger",
                "Pavel N Krivitsky",
                "Carter T Butts",
                "Jonathan R Stewart"
            ],
            "title": "Exponential-family models of random graphs: Inference in finite, super and infinite population scenarios",
            "venue": "Statistical Science,",
            "year": 2020
        },
        {
            "authors": [
                "Tom AB Snijders",
                "Philippa E Pattison",
                "Garry L Robins",
                "Mark S Handcock"
            ],
            "title": "New specifications for exponential random graph models",
            "venue": "Sociological methodology,",
            "year": 2006
        },
        {
            "authors": [
                "Sean L Simpson",
                "Satoru Hayasaka",
                "Paul J Laurienti"
            ],
            "title": "Exponential random graph modeling for complex brain networks",
            "venue": "PloS one,",
            "year": 2011
        },
        {
            "authors": [
                "Sean L Simpson",
                "Malaak N Moussa",
                "Paul J Laurienti"
            ],
            "title": "An exponential random graph modeling approach to creating group-based representative whole-brain connectivity",
            "venue": "networks. Neuroimage,",
            "year": 2012
        },
        {
            "authors": [
                "Michel RT Sinke",
                "Rick M Dijkhuizen",
                "Alberto Caimo",
                "Cornelis J Stam",
                "Willem M Otte"
            ],
            "title": "Bayesian exponential random graph modeling of whole-brain structural networks across lifespan",
            "year": 2016
        },
        {
            "authors": [
                "Catalina Obando",
                "Fabrizio De Vico Fallani"
            ],
            "title": "A statistical model for brain networks inferred from large-scale electrophysiological signals",
            "venue": "Journal of The Royal Society Interface,",
            "year": 2017
        },
        {
            "authors": [
                "Paul E Stillman",
                "James D Wilson",
                "Matthew J Denny",
                "Bruce A Desmarais",
                "Shankar Bhamidi",
                "Skyler J Cranmer",
                "Zhong-Lin Lu"
            ],
            "title": "Statistical modeling of the default mode brain network reveals a segregated highway structure",
            "venue": "Scientific reports,",
            "year": 2017
        },
        {
            "authors": [
                "John Dell\u2019Italia",
                "Micah A Johnson",
                "Paul M Vespa",
                "Martin M Monti"
            ],
            "title": "Network analysis in disorders of consciousness: four problems and one proposed solution (exponential random graph models)",
            "venue": "Frontiers in neurology,",
            "year": 2018
        },
        {
            "authors": [
                "Paul E Stillman",
                "James D Wilson",
                "Matthew J Denny",
                "Bruce A Desmarais",
                "Skyler J Cranmer",
                "Zhong-Lin Lu"
            ],
            "title": "A consistent organizational structure across multiple functional subnetworks of the human",
            "venue": "brain. NeuroImage,",
            "year": 2019
        },
        {
            "authors": [
                "BCL Lehmann",
                "RN Henson",
                "Linda Geerligs",
                "SR White"
            ],
            "title": "Characterising group-level brain connectivity: a framework using bayesian exponential random graph models",
            "year": 2021
        },
        {
            "authors": [
                "Catalina Obando",
                "Charlotte Rosso",
                "Joshua Siegel",
                "Maurizio Corbetta",
                "Fabrizio De Vico Fallani"
            ],
            "title": "Temporal exponential random graph models of longitudinal brain networks after stroke",
            "venue": "Journal of the Royal Society Interface,",
            "year": 2022
        },
        {
            "authors": [
                "David R Hunter",
                "Steven M Goodreau",
                "Mark S Handcock"
            ],
            "title": "Goodness of fit of social network models",
            "venue": "Journal of the american statistical association,",
            "year": 2008
        },
        {
            "authors": [
                "Jonathan D Power",
                "Bradley L Schlaggar",
                "Christina N Lessov-Schlaggar",
                "Steven E Petersen"
            ],
            "title": "Evidence for hubs in human functional brain",
            "venue": "networks. Neuron,",
            "year": 2013
        },
        {
            "authors": [
                "Jonathan D Power",
                "Alexander L Cohen",
                "Steven M Nelson",
                "Gagan S Wig",
                "Kelly Anne Barnes",
                "Jessica A Church",
                "Alecia C Vogel",
                "Timothy O Laumann",
                "Fran M Miezin",
                "Bradley L Schlaggar"
            ],
            "title": "Functional network organization of the human brain",
            "year": 2011
        },
        {
            "authors": [
                "Shi Gu",
                "Fabio Pasqualetti",
                "Matthew Cieslak",
                "Qawi K Telesford",
                "Alfred B Yu",
                "Ari E Kahn",
                "John D Medaglia",
                "Jean M Vettel",
                "Michael B Miller",
                "Scott T Grafton"
            ],
            "title": "Controllability of structural brain networks",
            "venue": "Nature communications,",
            "year": 2015
        },
        {
            "authors": [
                "Nikola T Markov",
                "M\u00e1ria Ercsey-Ravasz",
                "David C Van Essen",
                "Kenneth Knoblauch",
                "Zolt\u00e1n Toroczkai",
                "Henry Kennedy"
            ],
            "title": "Cortical high-density counterstream",
            "year": 2013
        },
        {
            "authors": [
                "Gustavo Deco",
                "Giulio Tononi",
                "Melanie Boly",
                "Morten L Kringelbach"
            ],
            "title": "Rethinking segregation and integration: contributions of whole-brain modelling",
            "venue": "Nature Reviews Neuroscience,",
            "year": 2015
        },
        {
            "authors": [
                "Ming Song",
                "Yong Liu",
                "Yuan Zhou",
                "Kun Wang",
                "Chunshui Yu",
                "Tianzi Jiang"
            ],
            "title": "Default network and intelligence difference",
            "venue": "IEEE Transactions on autonomous mental development,",
            "year": 2009
        },
        {
            "authors": [
                "Andrew Zalesky",
                "Alex Fornito",
                "Edward T Bullmore"
            ],
            "title": "Network-based statistic: identifying differences in brain",
            "venue": "networks. Neuroimage,",
            "year": 2010
        },
        {
            "authors": [
                "Robert J Barry",
                "Adam R Clarke",
                "Stuart J Johnstone",
                "Christopher A Magee",
                "Jacqueline A Rushby"
            ],
            "title": "Eeg differences between eyes-closed and eyes-open resting conditions",
            "venue": "Clinical neurophysiology,",
            "year": 2007
        },
        {
            "authors": [
                "Yu A Boytsova",
                "SG Danko"
            ],
            "title": "Eeg differences between resting states with eyes open and Statistical models of complex brain networks: a maximum entropy approach 33 closed in darkness",
            "venue": "Human physiology,",
            "year": 2010
        },
        {
            "authors": [
                "Gaolang Gong",
                "Pedro Rosa-Neto",
                "Felix Carbonell",
                "Zhang J Chen",
                "Yong He",
                "Alan C Evans"
            ],
            "title": "Age-and gender-related differences in the cortical anatomical network",
            "venue": "Journal of Neuroscience,",
            "year": 2009
        },
        {
            "authors": [
                "Patric Hagmann",
                "Olaf Sporns",
                "Neel Madan",
                "Leila Cammoun",
                "Rudolph Pienaar",
                "Van Jay Wedeen",
                "Reto Meuli",
                "J-P Thiran",
                "PE Grant"
            ],
            "title": "White matter maturation reshapes structural connectivity in the late developing human brain",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 1906
        },
        {
            "authors": [
                "Willem M Otte",
                "Eric van Diessen",
                "Subhadip Paul",
                "Rajiv Ramaswamy",
                "VP Subramanyam Rallabandi",
                "Cornelis J Stam",
                "Prasun K Roy"
            ],
            "title": "Aging alterations in whole-brain networks during adulthood mapped with the minimum spanning tree indices: the interplay of density, connectivity cost and life-time trajectory",
            "year": 2015
        },
        {
            "authors": [
                "F De Vico Fallani",
                "Vito Latora",
                "Laura Astolfi",
                "Febo Cincotti",
                "Donatella Mattia",
                "Maria Grazia Marciani",
                "Serenella Salinari",
                "Alfredo Colosimo",
                "Fabrizio Babiloni"
            ],
            "title": "Persistent patterns of interconnection in time-varying cortical networks estimated from high-resolution eeg recordings in humans during a simple motor act",
            "venue": "Journal of Physics A: Mathematical and Theoretical,",
            "year": 2008
        },
        {
            "authors": [
                "Miguel Valencia",
                "J Martinerie",
                "Samuel Dupont",
                "M Chavez"
            ],
            "title": "Dynamic small-world behavior in functional brain networks unveiled by an event-related networks approach",
            "venue": "Physical Review E,",
            "year": 2008
        },
        {
            "authors": [
                "Danielle S Bassett",
                "Nicholas F Wymbs",
                "Mason A Porter",
                "Peter J Mucha",
                "Jean M Carlson",
                "Scott T Grafton"
            ],
            "title": "Dynamic reconfiguration of human brain networks during learning",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2011
        },
        {
            "authors": [
                "Maria Giulia Preti",
                "Thomas AW Bolton",
                "Dimitri Van De Ville"
            ],
            "title": "The dynamic functional connectome: State-of-the-art and perspectives",
            "year": 2017
        },
        {
            "authors": [
                "Kai Wu",
                "Yasuyuki Taki",
                "Kazunori Sato",
                "Haochen Qi",
                "Ryuta Kawashima",
                "Hiroshi Fukuda"
            ],
            "title": "A longitudinal study of structural brain network changes with normal aging",
            "venue": "Frontiers in human neuroscience,",
            "year": 2013
        },
        {
            "authors": [
                "Sophie Dautricourt",
                "Robin de Flores",
                "Brigitte Landeau",
                "G\u00e9raldine Poisnel",
                "Matthieu Vanhoutte",
                "Nicolas Delcroix",
                "Francis Eustache",
                "Denis Vivien",
                "Vincent de la Sayette",
                "Ga\u00ebl Ch\u00e9telat"
            ],
            "title": "Longitudinal changes in hippocampal network connectivity in alzheimer\u2019s disease",
            "venue": "Annals of Neurology,",
            "year": 2021
        },
        {
            "authors": [
                "Joshua S Siegel",
                "Benjamin A Seitzman",
                "Lenny E Ramsey",
                "Mario Ortega",
                "Evan M Gordon",
                "Nico UF Dosenbach",
                "Steven E Petersen",
                "Gordon L Shulman",
                "Maurizio Corbetta"
            ],
            "title": "Reemergence of modular brain networks in stroke",
            "venue": "recovery. Cortex,",
            "year": 2018
        },
        {
            "authors": [
                "Naoki Masuda",
                "Renaud Lambiotte"
            ],
            "title": "A guide to temporal networks",
            "venue": "World Scientific,",
            "year": 2016
        },
        {
            "authors": [
                "Steve Hanneke",
                "Wenjie Fu",
                "Eric P Xing"
            ],
            "title": "Discrete temporal models of social networks",
            "venue": "Electronic journal of statistics,",
            "year": 2010
        },
        {
            "authors": [
                "Joshua Sarfaty Siegel",
                "Lenny E Ramsey",
                "Abraham Z Snyder",
                "Nicholas V Metcalf",
                "Ravi V Chacko",
                "Kilian Weinberger",
                "Antonello Baldassarre",
                "Carl D Hacker",
                "Gordon L Shulman",
                "Maurizio Corbetta"
            ],
            "title": "Disruptions of network connectivity predict impairment in multiple behavioral domains after stroke",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2016
        },
        {
            "authors": [
                "Christian Grefkes",
                "Gereon R Fink"
            ],
            "title": "Recovery from stroke: current concepts and future perspectives",
            "venue": "Neurological research and practice,",
            "year": 2020
        },
        {
            "authors": [
                "Carl G Hempel",
                "Paul Oppenheim"
            ],
            "title": "Studies in the logic of explanation",
            "venue": "Philosophy of science,",
            "year": 1948
        },
        {
            "authors": [
                "Joseph F Hanna"
            ],
            "title": "Explanation, prediction, description, and information",
            "venue": "theory. Synthese,",
            "year": 1969
        },
        {
            "authors": [
                "Galit Shmueli"
            ],
            "title": "To explain or to predict",
            "venue": "Statistical science,",
            "year": 2010
        },
        {
            "authors": [
                "Thomas M Cover"
            ],
            "title": "Elements of information theory",
            "year": 1999
        },
        {
            "authors": [
                "Elad Schneidman",
                "Michael J Berry",
                "Ronen Segev",
                "William Bialek"
            ],
            "title": "Weak pairwise correlations imply strongly correlated network states in a neural population",
            "year": 2006
        },
        {
            "authors": [
                "Thierry Mora",
                "Aleksandra M Walczak",
                "William Bialek",
                "Curtis G Callan Jr."
            ],
            "title": "Maximum entropy models for antibody diversity",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2010
        },
        {
            "authors": [
                "Sanjay G Reddy"
            ],
            "title": "What is an explanation? statistical physics and economics",
            "venue": "The European Physical Journal Special Topics,",
            "year": 2020
        },
        {
            "authors": [
                "Vito Dichio",
                "Hong-Li Zeng",
                "Erik Aurell"
            ],
            "title": "Statistical genetics in and out of quasi-linkage Statistical models of complex brain networks: a maximum entropy approach 34 equilibrium",
            "venue": "Reports on Progress in Physics,",
            "year": 2023
        },
        {
            "authors": [
                "Edwin T Jaynes"
            ],
            "title": "On the rationale of maximum-entropy methods",
            "venue": "Proceedings of the IEEE,",
            "year": 1982
        },
        {
            "authors": [
                "Steve Press\u00e9",
                "Kingshuk Ghosh",
                "Julian Lee",
                "Ken A Dill"
            ],
            "title": "Principles of maximum entropy and maximum caliber in statistical physics",
            "venue": "Reviews of Modern Physics,",
            "year": 2013
        },
        {
            "authors": [
                "Erik Aurell"
            ],
            "title": "The maximum entropy fallacy redux",
            "venue": "PLoS computational biology,",
            "year": 2016
        },
        {
            "authors": [
                "Gennaro Auletta",
                "Lamberto Rondoni",
                "Angelo Vulpiani"
            ],
            "title": "On the relevance of the maximum entropy principle in non-equilibrium statistical mechanics",
            "venue": "The European Physical Journal Special Topics,",
            "year": 2017
        },
        {
            "authors": [
                "James Ladyman",
                "James Lambert",
                "Karoline Wiesner"
            ],
            "title": "What is a complex system",
            "venue": "European Journal for Philosophy of Science,",
            "year": 2013
        },
        {
            "authors": [
                "Lars Onsager",
                "Stefan Machlup"
            ],
            "title": "Fluctuations and irreversible processes",
            "venue": "Physical Review,",
            "year": 1953
        },
        {
            "authors": [
                "C W Gardiner"
            ],
            "title": "Handbook of Stochastic Methods for Physics, Chemistry and the Natural Sciences, volume 13 of Springer Series in Synergetics",
            "venue": "Springer-Verlag, third edition,",
            "year": 2004
        },
        {
            "authors": [
                "Xiaona Fang",
                "Karsten Kruse",
                "Ting Lu",
                "Jin Wang"
            ],
            "title": "Nonequilibrium physics in biology",
            "venue": "Reviews of Modern Physics,",
            "year": 2019
        },
        {
            "authors": [
                "Per Block",
                "Johan Koskinen",
                "James Hollway",
                "Christian Steglich",
                "Christoph Stadtfeld"
            ],
            "title": "Change we can believe in: Comparing longitudinal network models on consistency, interpretability and predictive power",
            "venue": "Social Networks,",
            "year": 2018
        },
        {
            "authors": [
                "Edwin T Jaynes"
            ],
            "title": "Probability theory: The logic of science",
            "venue": "Cambridge university press,",
            "year": 2003
        },
        {
            "authors": [
                "Luca Peliti"
            ],
            "title": "Statistical mechanics in a nutshell",
            "year": 2011
        },
        {
            "authors": [
                "Ludwig Boltzmann"
            ],
            "title": "Weitere studien \u00fcber das w\u00e4rmegleichgewicht unter gasmolek\u00fclen",
            "venue": "In Kinetische Theorie II,",
            "year": 1970
        },
        {
            "authors": [
                "Christopher W Lynn",
                "Eli J Cornblath",
                "Lia Papadopoulos",
                "Maxwell A Bertolero",
                "Danielle S Bassett"
            ],
            "title": "Broken detailed balance and entropy production in the human brain",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2021
        },
        {
            "authors": [
                "Charley Presigny",
                "Fabrizio De Vico Fallani"
            ],
            "title": "Colloquium: Multiscale modeling of brain network organization",
            "venue": "Reviews of Modern Physics,",
            "year": 2022
        },
        {
            "authors": [
                "Bradley Efron",
                "Robert Tibshirani"
            ],
            "title": "Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy",
            "venue": "Statistical science,",
            "year": 1986
        },
        {
            "authors": [
                "David Gfeller",
                "Jean-C\u00e9dric Chappelier",
                "Paolo De Los Rios"
            ],
            "title": "Finding instabilities in the community structure of complex networks",
            "venue": "Physical Review E,",
            "year": 2005
        },
        {
            "authors": [
                "Brian Karrer",
                "Elizaveta Levina",
                "Mark EJ Newman"
            ],
            "title": "Robustness of community structure in networks",
            "venue": "Physical review E,",
            "year": 2008
        },
        {
            "authors": [
                "Pavel N Krivitsky",
                "Laura M Koehly",
                "Christopher Steven Marcum"
            ],
            "title": "Exponential-family random graph models for multi-layer",
            "venue": "networks. Psychometrika,",
            "year": 2020
        },
        {
            "authors": [
                "Tom AB Snijders",
                "Gerhard G Van de Bunt",
                "Christian EG Steglich"
            ],
            "title": "Introduction to stochastic actor-based models for network dynamics",
            "venue": "Social networks,",
            "year": 2010
        },
        {
            "authors": [
                "Jihui Lee",
                "Gen Li",
                "James D. Wilson"
            ],
            "title": "Varying-coefficient models for dynamic networks",
            "venue": "Computational Statistics & Data Analysis,",
            "year": 2020
        },
        {
            "authors": [
                "Paul W Holland",
                "Kathryn Blackmond Laskey",
                "Samuel Leinhardt"
            ],
            "title": "Stochastic blockmodels: First steps",
            "venue": "Social networks,",
            "year": 1983
        },
        {
            "authors": [
                "Brian Karrer",
                "Mark EJ Newman"
            ],
            "title": "Stochastic blockmodels and community structure in networks",
            "venue": "Physical review E,",
            "year": 2011
        },
        {
            "authors": [
                "Tiago P Peixoto"
            ],
            "title": "Hierarchical block structures and high-resolution model selection in large networks",
            "venue": "Physical Review X,",
            "year": 2014
        },
        {
            "authors": [
                "Tiago P Peixoto"
            ],
            "title": "Nonparametric weighted stochastic block models",
            "venue": "Physical Review E,",
            "year": 2018
        },
        {
            "authors": [
                "Tiago P Peixoto"
            ],
            "title": "Disentangling homophily, community structure, and triadic closure in networks",
            "venue": "Physical Review X,",
            "year": 2022
        },
        {
            "authors": [
                "Federico Battiston",
                "Giulia Cencetti",
                "Iacopo Iacopini",
                "Vito Latora",
                "Maxime Lucas",
                "Alice Patania",
                "Jean-Gabriel Young",
                "Giovanni Petri"
            ],
            "title": "Networks beyond pairwise interactions: structure and dynamics",
            "venue": "Physics Reports,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Yet, analyzing brain networks is challenging, in part because their structure represents only one possible realization of a generative stochastic process which is in general unknown. Having a formal way to cope with such intrinsic variability is therefore central for the characterization of brain network properties.\nAddressing this issue entails the development of appropriate tools mostly adapted from network science and statistics. Here, we focus on a particular class of maximum entropy models for networks, i.e. exponential random graph models (ERGMs), as a parsimonious approach to identify the local connection mechanisms behind observed global network structure. Efforts are reviewed on the quest for basic organizational properties of human brain networks, as well as on the identification of predictive biomarkers of neurological diseases such as stroke.\nWe conclude with a discussion on how emerging results and tools from statistical graph modeling, associated with forthcoming improvements in experimental data acquisition, could lead to a finer probabilistic description of complex systems in network neuroscience.\nKeywords: Statistical modeling, Complex systems, Exponential random graph model, Brain networks, Inference, Maximum entropy principle.\nContents"
        },
        {
            "heading": "1 Introduction 2",
            "text": ""
        },
        {
            "heading": "2 Brain networks in a nutshell 4",
            "text": "2.1 Building brain networks from experimental data . . . . . . . . . . . . 4 2.2 The rationale of a statistical approach . . . . . . . . . . . . . . . . . . 5"
        },
        {
            "heading": "3 Exponential random graph model (ERGM) 6",
            "text": "3.1 Model parameter estimation . . . . . . . . . . . . . . . . . . . . . . . . 7 3.2 Interpretation of ERGMs . . . . . . . . . . . . . . . . . . . . . . . . . 9 3.3 Graph statistics for ERGMs . . . . . . . . . . . . . . . . . . . . . . . . 10\nar X\niv :2\n20 9.\n05 82\n9v 4\n[ q-\nbi o.\nN C\n] 1\n1 A"
        },
        {
            "heading": "4 ERGMs in neuroscience 14",
            "text": "4.1 Minimal model of brain networks . . . . . . . . . . . . . . . . . . . . . 15 4.2 Identification of discriminant brain network features . . . . . . . . . . 19 4.3 Predicting states in temporal brain networks . . . . . . . . . . . . . . 21"
        },
        {
            "heading": "5 On the interpretation of ERGMs 24",
            "text": "5.1 ERGMs and the maximum entropy principle (MEP) . . . . . . . . . . 24 5.2 Description and prediction: the choice of graph statistics . . . . . . . . 26 5.3 Explanation: ERGMs and Stat.Mech. . . . . . . . . . . . . . . . . . . 26"
        },
        {
            "heading": "6 Conclusion and perspectives 27",
            "text": "Acknowledgements 28\nReferences 28"
        },
        {
            "heading": "1. Introduction",
            "text": "The human brain is a biological system of tremendous complexity. At different scales of neuronal organization, the paradigm of a system \u201cmade up of a large number of parts that interact in a nonsimple way\u201d [1] turns out to be an apt abstraction. Notably, it suits neurons interacting through synapses at the microscale as well as brain regions\u2019 activity coordinating at the macroscale and resulting in the rich spectrum of mind\u2019s functional states. The study of the brain as a complex system has flourished in the last 20 years, drawing analogies from the physics of disordered systems, graph theory, dynamical systems and fueled by the modern deluge of data upon nearly all scientific fields [2].\nBy explicitly representing the interactions between the system\u2019s components, networks, or graphs, constitute a natural and powerful way to inquire its organizing principles. The description of the brain under the lens of network science has lead to a number of fundamental results. Topological network properties, such as node centrality, modular organization and global efficiency, play a fundamental role in the emergence of basic physiological functions, as well as on the apperance of many brain diseases [3, 4, 5].\nBeyond purely descriptive analyses, important questions on the network structure include what is the underlying generative process and how local wiring rules result in the observed large-scale properties [6]. To this end, network models based on random edge rewiring or nodal preferential attachment rules have been initially investigated and allowed to assess nontrivial properties of brain networks, such as integration and segregation of information or the presence of few nodes with a high number of connections, i.e. the so-called hubs [7]. In addition to purely topological models, brain networks can be more finely modeled by explicitly taking into account the spatial position of the nodes so as to penalize the cost of having long distance connections and minimize the associated metabolic consumption [8, 9].\nWhile these models allow to identify putative mechanisms involved in generating the observed brain networks, they implicitly make the assumption that the connections have a physical meaning, i.e. they are tangible quantities. However, it is important to remind that almost all brain networks are currently inferred from experimental data through advanced tools from image and signal processing (Fig.1a) [10]. In particular,\nlinks are statistical estimates of possibly existing anatomical pathways or functional interactions between different brain areas and they are affected by some uncertainty (Fig.1b).\nStatistical modeling has been introduced in an effort to identify the probability distribution of all the possible network realizations associated with an observed one (Fig.1c). Approaches of this kind include generating a-posteriori network surrogates preserving some properties (e.g., node degree distribution [11]), or apriori reproducing, for example, the underlying modular structure of networks via stochastic block modeling [12, 13]. These approaches have effectively improved the characterization of brain networks with respect to random null-models and provided a statistical framework to estimate confidence intervals. Nevertheless, they remain quite limited in terms of the number of local network properties that can be simultaneously tested and modeled.\nThis is a crucial aspect as in general we don\u2019t know how many and which are the local connection properties that have generated the global observed structure. Exponential random graph models (ERGMs) represent an intriguing solution to this limitation, as demonstrated by the recent development in the field. Broadly speaking, ERGMs belong to the family of maximum entropy (Max.Ent.) models [14, 15]. Given an observed network G\u2217, the ERGM probability distribution is the one with the largest\ninformation entropy that statistically reproduces some key structural properties, or features, of G\u2217. The latter are associated with the model parameters, which can be inferred from the data.\nA fortunate alignment among availability of experimental brain data, development of network theory and of powerful computational tools - mainly borrowed from social sciences [16] - has allowed researchers to study the brain network structure and dynamics under the lens of the ERGMs. The goal of the present report is to track down these efforts and present them in a self-contained review with an unified accessible language. The ultimate ambition is to show their usefulness to a broad audience and encourage their exploitation to tackle current open questions in basic and clinical neuroscience.\nThe rest of the review is organized as follows. In (Sec.2) we introduce brain networks, briefly describe how to build them from experimental data and which are their main topological properties. In (Sec.3) we focus on a minimal version of the ERGM, we present both its theoretical and computational aspects, we linger on the interpretation of its parameters and introduce some widely used ERGM graph statistics. In (Sec.4) we review the existing literature of ERGM-based methods for neuroscience. We show how they have been used to characterize brain states, discriminate between different experimental conditions and model dynamic brain networks. In (Sec.5) the reader will find a discussion of the theoretical subtleties of the ERGMs with a focus on their applications to the brain. Finally, in (Sec.6) we conclude with a perspective on the future of the approach, and point out potentially useful alternatives."
        },
        {
            "heading": "2. Brain networks in a nutshell",
            "text": "The study of the brain from a network perspective is nowadays regarded as a vibrant interdisciplinary field of research, referred as network neuroscience [4]. Such efforts have initially focused on quantifying the organizational properties of the nervous system by means of different network metrics [17, 2]. For example, it has been shown that brain networks tend to exhibit a pronounced modular structure i.e. they are partitioned in densely inter-connected communities linked by sparse inter-community connections [18]. The high clustering together with the low average path length between pairs of nodes has suggested that neuronal interactions are organized in a small-world topology, optimizing the balance between segregation and integration of information [19, 20]. Moreover, there is mounting evidence on the presence of hubs [21, 22] and metabolic constraints of the brain\u2019s wiring, with spatially closer regions supporting stronger patterns of connections [8, 23]. These brain network properties tend to span different spatial and temporal scales, are in common to several species, correlate with individual behavior and can be significantly affected by neurological disorders [24, 25]. In the following, we will focus on brain networks derived from neuroimaging data resulting from the noninvasive recording of large-scale neural information in humans."
        },
        {
            "heading": "2.1. Building brain networks from experimental data",
            "text": "Neuroimaging broadly refers to a large set of techniques that allow to measure noninvasively the brain structure and function. The translation of these measurements into a network hinges on the definition of what nodes and edges are. In general, these\ndefinitions depend on experimental constraints as well as on the specific scientific question. Here, we will provide a gentle overview of the main approaches adopted so far in an effort to offer a minimal understanding of the nature of the networks that we will later explore. Should the readers be interested in more detailed technical descriptions, we refer them to some recent books and reviews [10, 26, 27, 28].\nA first type of brain networks aims at representing the structural wiring of the brain. Nodes are defined as different brain sites and edges represent their anatomical interconnections. To this purpose, diffusion tensor imaging (DTI) is a noninvasive technology that allows to infer the diffusion of water molecules through white matter tracts of the brain in the three-dimensional space [29]. The collected data are discretized into non-overlapping gray matter volumes (nodes) and the connection weights (edges) between two nodes are proportional to the anatomical properties of the fiber tracks (e.g. number, length).\nThe second type of brain networks aims instead at capturing the functional interactions between brain regions (nodes). These regions of interest (ROIs) typically correspond to cytoarchitectural landmarks known a-priori or more directly to sensors used for the measurements [30]. For functional networks, interactions (edges) are defined as statistical similarities between the dynamics taking place in different nodes. Many technologies exist to record nodal dynamics as signals with different temporal and spatial resolutions. Among others, electroencephalography (EEG) and functional magnetic resonance imaging (fMRI) allow for noninvasive recordings of electrical activity through sensors placed over the scalp [31] and blood oxygen levels from 3D images of the brain, respectively [32, 33]. Functional interactions are then inferred from the recorded signals using related measures such as Pearson or Spearman correlations, mutual information or Granger causality [10, 30]. Thus, the study of functional networks is primarily intended to reveal the brain organizational properties from an information processing point of view.\nAt this stage, both structural and functional brain networks are characterized by an adjacency matrix A whose generic entry aij is a scalar number, typically normalized between 0 and 1, representing the magnitude of the connection between nodes i and j. Thresholding procedures can be eventually adopted to filter out irrelevant links and obtain sparse unweighted networks. More details on why and how to filter information in complex brain networks can be found in [34, 35, 36]."
        },
        {
            "heading": "2.2. The rationale of a statistical approach",
            "text": "The rationale for statistical modeling is based on the assumption that the observed system is a realization of a stochastic process, and that by characterizing the properties of this process we can gain insight into the underlying mechanisms governing the system\u2019s behavior. More specifically, the probability of observing each possible realization depends on endogenous system constraints, which can be either postulated or inferred from the data.\nThis type of statistical uncertainty is different from that associated with exogenous experimental errors - here, the network construction process. Statistical uncertainty is due to the inherent variability of a system and is quantified using statistical measures. Uncertainty due to experimental errors arises from factors such as measurement noise and reflects the limitations of our measurement apparatus. In this review we deal with the former, discussion on the latter can be found elsewhere [10].\nIn network neuroscience, a common first approach to assess the statistical significance of a network\u2019s properties has been to compare it a-posteriori to a null model. The latter is a simplified version of the network that preserves some basic structural features, but removes other non-random ones [37]. By comparing a network to its null model, we can determine whether its observed properties are due to chance, or whether they reflect underlying mechanisms or constraints on the system. This comparison can be done using a variety of statistical tests, such as comparing the network\u2019s properties to those of a large ensemble of randomized networks, or comparing it to a null model that is specifically designed to test a particular hypothesis.\nSuch an approach however do not allow to test more specific hypotheses about the mechanisms that govern the network organization, e.g. the likelihood that a-priori defined structures are at the origin of the observed network. This kind of questions can be targeted by using instead an inferential approach. This involves (i) constructing a statistical model that describes the relationships between different variables in the system, and (ii) using this model to estimate the parameters that govern the behavior of the system. By comparing the predictions of the model to the observed data, we can determine whether the model accurately captures the underlying mechanisms that drive the system.\nFurthermore complex systems typically arise from multiple organizing principles across different scales. Statistical (inferential) frameworks that can simultaneously account for these effects are crucial to understanding the global properties of the system. In network neuroscience, where the focus is on understanding the brain\u2019s structure and function, this approach is particularly useful [24, 38, 39]. By using inferential methods, we can identify the combinations of relevant network properties that give rise to the observed large-scale structures and predict how these properties will change under different conditions or perturbations.\nRecent endeavors in exponential random graph models (ERGMs) have increasingly attracted the interest of the network science and neuroscience community due to their ability to match the aforementioned desiderata within a coherent and unifying framework. Hence, it is timely to discuss these emerging developments, and to seek to put them together into a common theoretical ground that can be used to tackle current open questions in modern neuroscience."
        },
        {
            "heading": "3. Exponential random graph model (ERGM)",
            "text": "The interest in the exponential class of probability distributions dates back to the dawn of modern statistics [40, 41] and was originally motivated by its properties with respect to sufficient statistics, as introduced by R.A. Fisher in the early \u201920s [42, 43]. A number of theoretical and computational properties makes them very appealing for the problem of statistical inference [44, 45] and justifies their ubiquity in all branches of scientific research. In graph theory, the exponential family, here referred to as exponential random graph models (ERGMs), came on stage in the early \u201980s [46, 47, 48] building on the seminal work of J. Besag [49] and further developing since then [50, 51]. Analogies with well-known methods of statistical mechanics have been recently explored in simple cases in conjunction with the outbreak of network science [52, 53, 15]. ERGMs have recently become popular in several scientific fields e.g. epidemiology [54], sociology [55, 56, 57] as well as neuroscience. As a consequence an increasing number of publicly available related softwares has appeared, such as the R language packages collected in the statnet suite [58, 59].\nOur reference implementation throughout this section is the ergm package [60]. Here, we give the theoretical minimum to familiarize the reader with ERGMs and their estimation methods. Notably, we will consider unweighted and undirected graphs as they represent the most common scenario in the current literature. Later in Sec.4 we will extend the discussion to more sophisticated cases, when appropriate.\nLet G be a set of all finite graphs of N nodes with no self-loops, containing at most one single edge between two nodes. Each graph G \u2208 G can be equivalently represented by a N \u00d7N adjacency matrix A containing Boolean values, i.e. aij = {1, 0}.\nAt the hearth of the ERGM approach there is the definition of an appropriate probability mass function P (G) over the ensemble G . The crucial idea of ERGMs is to encode the relevant information about the graph G in a vector x(G) \u2208 Rr of r statistics or metrics, where each element x\u03b1(G) measures a different network property of interest.\nSpecifically, the probability of observing a generic graph G reads as\nP (G|\u03b8) = e \u03b8\u00b7x(G)\u2211\nG\u0303\u2208G e \u03b8\u00b7x(G\u0303)\n, (1)\nwhere \u03b8 \u2208 Rr are the model parameters weighting the graph statistics and \u00b7 is the dot product. A formal analogy with the canonical Boltzmann distribution is readily recognized by defining the Hamiltonian\nH(G) = \u2212\u03b8 \u00b7 x(G) = \u2212 \u2211 \u03b1 \u03b8\u03b1x\u03b1(G) (2)\nHence, the denominator of Eq.1 is a normalizing constant, which turns out to play the role of partition function Z and we can rewrite P (G|\u03b8) = 1Z e\n\u2212H(G). Given a specific observed network G\u2217, one can infer the model parameters \u03b8\u2217 so that the expected value of each graph statistics over the ensemble G\n\u27e8x\u27e9 = \u2211 G\u2208G x(G)P (G|\u03b8\u2217) (3)\nstatistically matches the observed value \u2243 x(G\u2217). Note that (Eq.3) sets the number of model parameters from 2( N 2 ) - the number of possible edges, in the worst case - to a significantly smaller amount determined by the chosen graph statistics. By consequence, in general a tie-level matching between the networks generated with P (G|\u03b8\u2217) and the observed graph G\u2217 is neither expected nor desired."
        },
        {
            "heading": "3.1. Model parameter estimation",
            "text": "Apart from very simple cases, ERGM parameters are hard to obtain in a closed form. In general, the nature and the number of graph statistics make the denominator of Eq.1 impossible to compute analytically. The evaluation of the partition function Z is a very well-known problem in many situations, like for example when trying to infer parameters from Boltzmann-like distributions [61, 62].\nIn the last 30 years there has been a tremendous theoretical and computational effort in developing methods to address this issue and achieve efficient estimation algorithms. In the following, we present a brief introduction to the key-idea behind many of these methods [63, 64].\nGiven a graph G\u2217 and a set of statistics x(G), we can take a Bayesian perspective and argue that our knowledge on the parameters \u03b8 is better described by the posterior distribution\nP (\u03b8|G\u2217) = P (G \u2217,\u03b8)\nP (G\u2217) = P (G\u2217|\u03b8)P (\u03b8) P (G\u2217) . (4)\nIn the case where no prior information is available for \u03b8, we can take P (\u03b8) to be uniformly distributed. From Eq.4 we see that the posterior distribution P (\u03b8|G\u2217) is directly proportional to the distribution of the data G\u2217 given the parameters \u03b8. Accordingly, our best guess on the parameters value is given by the Maximum Likelihood Estimator (MLE)\n\u03b8\u2217 = argmax \u03b8 L (G\u2217|\u03b8) , (5)\nwhere L (G\u2217|\u03b8) = logP (G\u2217|\u03b8) is the log-likelihood function. The evaluation of (Eq.5) is hampered by the computation of the partition function Z inside P (G\u2217|\u03b8). As discussed above, this entails the need for non-exact numerical methods.\nRather than maximizing L (G\u2217|\u03b8) directly, the idea is to maximize a shifted loglikelihood L\u0304 (G\u2217|\u03b8) = L (G\u2217|\u03b8) \u2212 L (G\u2217|\u03b80), where \u03b80 is an arbitrarily parameter vector. \u2021\nA bit of algebra reveals that the new log-likelihood function can be written as\nL\u0304 (G\u2217|\u03b8) = (\u03b8 \u2212 \u03b80) \u00b7 x(G\u2217)\u2212 log \u27e8e(\u03b8\u2212\u03b80)\u00b7x(G)\u27e9\u03b80 (6)\nwhere the subscript \u27e8\u00b7\u27e9\u03b80 indicates the expectation value over the distribution P (G|\u03b80). Note that the new log-likelihood L\u0304 (G\u2217|\u03b8) is maximized by the same \u03b8\u2217 that maximizes L (G\u2217|\u03b8).\nIn practice, the evaluation of the partition functions Z is bypassed and we just need to calculate the expectation \u27e8\u00b7\u27e9\u03b80 , which can be efficiently performed with standard Markov-chain Monte-Carlo (MCMC) approximations. Eventually, the whole procedure is repeated until the convergence to some stable solution \u03b8\u2217 is reached (Tab. 1). In the following, we will refer to this method as MCMC-MLE.\n3.1.1. ERGMs in practice The increasing diffusion of ERGMs has been driven by a concurrent technical effort to develop efficient algorithms for carrying out the inference task. By far the most popular implementation available to date is the statnet suite of R-based software packages [58, 67]. To our knowledge, no other implementation has reached the same level of maturity and extending these techniques outside the R language is an open challenge for the next future. The reference implementation for the vast majority of existing packages is the ergm package [60]. A number of other packages have been built on it, both for static [68, 69, 70, 71] and temporal networks [72, 73].\nAn illustrative workflow based on the ergm package is implemented in [74], consisting of six fundamental steps. They include: i) creating a network object using the network package from the adjacency matrix and nodal/edge covariates (if any), ii) computing ERGM statistics for the experimental network (optional), iii) specifying\n\u2021 In practice \u03b80 should be close enough to \u03b8 to ensure convergence in a reasonable computational time. One popular choice is to take \u03b80 as the maximum pseudo-likelihood estimation (MPLE) of \u03b8 under the additional hypothesis that the edges are mutually independent [16, 65].\nand estimating the model, iv) assessing MCMC convergence (optional), v) evaluating the model fit using goodness of fit methods (GoF), and vi) simulating the model using estimated parameters (optional).\nThe computational time required for the model estimation might vary from a few seconds to several hours, depending on a number of factors including the type and number of graph statistics, the basic properties of the networks (e.g. size, sparsity) and several estimation settings that can be possibly adjusted. Because of the intrinsic characteristics of the current neuroimaging technology (Sec.2), brain networks of interest in this review have a relatively small number of nodes (at most hundreds). For such sizes, the ERGM estimation is in general doable. For further information regarding the computational aspects and a comprehensive overview of the latest technical enhancements of the ergm 4.0 package, we refer to [75, 66] and references therein."
        },
        {
            "heading": "3.2. Interpretation of ERGMs",
            "text": "A crucial, often overlooked, aspect in ERGM is the interpretation of the parameters \u03b8 in Eq.1. Let us address the question starting with a simple example [47, 76, 77]. Consider an ERGM based on a single graph metric x(G) corresponding to the number of edges in the network, i.e. x(G) = \u2211 i<j aij . We first consider the forward problem in which there is no observed graph but instead the parameter \u03b8 is given and the goal is to evaluate the expected value for x(G).\nIn such a simple case, the mass probability function can be computed exactly:\nP (G|\u03b8) = e \u03b8x(G)\u220f\ni<j \u22111 aij=0 e\u03b8aij =\ne\u03b8x(G)\n(1 + e\u03b8)( N 2 )\n. (7)\nNotably, by defining\np(\u03b8) = e\u03b8\n1 + e\u03b8 (8)\nwe can rewrite P (G|\u03b8) = px(G)(1 \u2212 p)( N 2 )\u2212x(G), which is the probability of a Bernoulli graph with N nodes and x(G) links [78]. The expected value for the number of edges in the graph is then simply \u27e8x\u27e9 = ( N 2 ) p.\nIn practice, the most common scenario is quite the opposite. One does not know the value of \u03b8 but rather has an observed graph G\u2217 with, say, M edges. In these situations, the goal is to solve the inverse problem by inferring \u03b8\u2217 from x(G\u2217) = M . Taking p\u2217 = M/ ( N 2 ) and inverting Eq.8:\n\u03b8\u2217(p\u2217) = log p\u2217\n1\u2212 p\u2217 (9) so that, consistently, \u27e8x\u27e9 = ( N 2 ) p = M . Notably, Eq.9 tells us that if we start from a Bernoulli graph with p\u2217 = 0.5, then \u03b8 = 0. This is the maximally random case since each dyad corresponds to the toss of a fair coin. But if we start from a denser (p\u2217 > 0.5) or sparser (p\u2217 < 0.5) graph, then \u03b8\u2217 > 0 or \u03b8\u2217 < 0, respectively.\nIn such a simple case we have therefore a complete understanding of both the forward and inverse problems. In real situations, however, the interpretation of the parameters becomes trickier since ERGMs may include several statistics, often exhibiting some degree of dependence. As a consequence, there might be interactions among the model parameters that are in general difficult to compute analytically.\nYet, inference is still possible via approximate numerical methods such as the MCMC-MLE. The interpretation of the inferred parameters can be given by quantifying their effect on the likelihood that an edge between two nodes exists or not in the network. For example, consider a single edge toggle between nodes i and j. Let us call P (Gaij=1,a\\ij |\u03b8\u2217) the probability of having an edge between the two nodes given the rest of the graph a\\ij , and P (Gaij=0,a\\ij |\u03b8\u2217) its complementary.\nFrom Eq.1 one easily finds that\nP (Gaij=1,a\\ij |\u03b8\u2217) P (Gaij=0,a\\ij |\u03b8\u2217)\n= exp ( \u03b8\u2217 \u00b7\u2206xij(G) ) , (10)\nwhere the \u2206xij(G) = x(Gaij=1,a\\ij ) \u2212 x(Gaij=0,a\\ij ) are the so-called change statistics. From Eq. 10 is clear that the probability of a link to exist depends on the magnitude and signs of both \u03b8\u2217 and \u2206xij(G).\nLet us consider a very simple case where the presence of a new link between i and j increases the value of a given graph statistic i.e. \u2206xij(G) > 0. If \u03b8\n\u2217 > 0, then this change statistic favors the probability of this edge to exist in the graph (aij = 1). Instead, if \u03b8\u2217 < 0 the same change statistic penalizes the existence of the connection, favouring instead (aij = 0). Overall, the magnitude of this effect is given by the term exp [\u03b8\u2217 \u2206xij(G)]. In other words, one can disentangle and quantify the relative influence of each graph statistic on the probability to observe, or not, an edge between two nodes i and j.\nNote that the above micro-level interpretation of the parameters is precisely at the core of the MCMC routines used to sample networks from ERGM probability distributions. Markov chains of graphs whose stationary distribution is P (G|\u03b8\u2217) are obtained by sequentially proposing changes on a graph, evaluating the probability of the new configuration by Eq.10 and accepting it according to the Metropolis-Hastings recipe. We refer to recent reviews in the field for a more detailed description of the MCMC methods used in ERGMs [16]."
        },
        {
            "heading": "3.3. Graph statistics for ERGMs",
            "text": "A broad spectrum of graph statistics, or metrics, x(G) have been formulated and included in Eq.1. Here, we present those that have been more frequently adopted in\nneuroscience. A first group of graph statistics includes the so-called dyadic independence terms, i.e. combinations of only single-dyad terms. The most intuitive metric is the edges term which measures the number of edges in the graph (Fig. 2a):\nxe(G) = \u2211 i<j aij . (11)\nA straightforward extension is the so-called edge covariate xdc(G) = \u2211 i<j aij\u03b6ij . (12)\nwhere \u03b6ij is an attribute associated defined for each dyad, such as the Euclidean distance in a spatial network.\nERGM terms based on nodal attributes also belong to this group. If \u03b7i is a categorical property of the nodes, then it is possible to define the so-called nodematch term as\nxnm(G) = \u2211 i<j \u03b4\u03b7i\u03b7jaij , (13)\nwhere \u03b4 is the Kronecker delta and xnm counts the number of edges whose nodes are labeled by the same categorical attribute. For instance, \u03b7i = R, L could indicate whether a node belongs to the right (R) or left (L) hemisphere.\nMore in general, real networks are characterized by the presence of complex connectivity structures leading to dependencies between dyads. Therefore, we now turn into presenting graph statistics that involve products of two or more dyadic variables aij . Following the seminal work of O. Frank and D. Strauss on Markov Graphs [47], we assume that only dyads that share a node can be dependent or, equivalently, that nonincident dyads are conditionally independent.\nUnder this general definition, several statistics may be introduced based on the count of local connection patterns. For instance, the presence of hubs, documented in many real-world networks [79, 77], can be measured in an ERGM by the k-stars term:\nx (k) st (G) =\n1\nk! \u2211 i0 \u00b7 \u00b7 \u00b7 \u2211 ik ai0i1 . . . ai0ik (14)\nwith k = 1, . . . , N \u2212 1. The global tendency of networks to form clusters, can be instead measured by the presence of triplets, or triads, of connected nodes. A k-triangles term can be then defined as:\nx (k) tr (G) =\n1\n\u03be(k) \u2211 i0 \u00b7 \u00b7 \u00b7 \u2211 i1+k ai0i1ai0i2ai1i2 . . . ai0i1+kai1i1+k (15)\nwhere the symmetry factor is \u03be(k) = 3! for k = 1 and \u03be(k) = 2k! for k = 2, . . . , N \u2212 2. The simplest case x\n(1) tr (G) = 1 6 \u2211 i0i1i2\nai0i1ai0i2ai1i2 counts the number of triangles in the graph.\n3.3.1. Degeneracy of simple ERGMs. Starting from the earliest numerical investigations, it has been observed that for some basic ERGM specifications, MCMC simulations hardly \u201dmix\u201d, meaning they do not reach a stationary distribution within a reasonable amount of time [48, 80]. The lack of mixing hampers the estimation task. This behavior is due to the geometry of ERGM stationary distributions. As it turns out, there exist choices of statistics that result in \u201ddegenerate\u201d models, meaning that assign a significant probability only to a few graph configurations, often with a very different structure, such as empty and fully connected graphs. [81, 82]. A general, thorough discussion of the causes and consequences of degeneracy in the context of ERGMs can be found in [83].\nFor a few simple ERGMs, such as the 2-star model (H = xe + x(2)st ) or the Strauss clustering model (H = xe + x(1)tr ), it has been demonstrated through analytic solutions that this phenomenon corresponds to a standard phase separation observed in statistical mechanics between a low- and high-density phase, representing almost empty and full networks, respectively [84, 85]. For such model specifications, there is no parameters combination that is able to reproduce intermediate densities of edges, two-stars and triangles as observed in real-world networks.\nThese examples demonstrate that degeneracy issues arise when the model includes unstable sufficient statistics\u00a7. Such models show excessive sensitivity in the sense that\n\u00a7 According to the definition in [83], x(G) is stable if there exist C > 0 and Mc > 0 such that Ux \u2261 maxG\u2208G x(G) \u2264 CM \u2200M > Mc, where M = (N 2 ) is the number of degrees of freedom in the network. Note that for instance Uxe = M implies that the edges term is stable while Ux(2)st \u223c NM\nsmall state changes can result in extremely large odds ratios eq.(10) [83]. The main conclusion of these theoretical investigations is that inference for simple ERGMs is simply not possible, ill-posed. To solve degeneracy issues, a possible solution is to add structure to the model specification by incorporating additional information such as i) edge-specific or node-specific covariates, ii) block structure, iii) temporal structure, iv) multilevel structure, or v) spatial structure. A recent comprehensive discussion of these cases can be found in [86].\nHowever, the most common approach to ERGM degeneracy issues in neuroscience applications has been so far to include the so-called \u201dcurved statistics\u201d in the model specification. In the next section, we will discuss this approach in detail.\n3.3.2. Curved Statistics. The inclusion of curved statistics into ERGM model specifications has been found to be effective in alleviating degenerate behaviors [83, 86]. These statistics are linear combinations of the complete distribution of degree or shared partner statistics, where the coefficients are geometrically weighted. Therefore, these statistics not only enumerate patterns but also include additional non-linear constraints on graph structures through the use of geometric coefficients. [16, 87].\nFor instance, the geometrically weighted degree (gwd) term reads:\nxgwd(G|\u03c4) = e\u03c4 N\u22121\u2211 k=1 { 1\u2212 ( 1\u2212 e\u2212\u03c4 )k} x (k) d (G) , (16)\nwhere \u03c4 > 0 is a parameter and x (k) d (G) is the number of nodes in the graph G whose degree is exactly equal to k (Fig.2b). Similarly, the geometrically weighted non-edgewise shared partner (gwnsp) statistic:\nxgwnsp(G|\u03c4) = e\u03c4 N\u22122\u2211 k=1 { 1\u2212 ( 1\u2212 e\u2212\u03c4 )k} x(k)nsp(G) , (17)\nwhere \u03c4 > 0 and x (k) nsp(G) is the number of non connected dyads having k neighbors in common, (Fig.2c). Finally, the geometrically weighted edgewise shared partner (gwesp) statistics:\nxgwesp(G|\u03c4) = e\u03c4 N\u22122\u2211 k=1 { 1\u2212 ( 1\u2212 e\u2212\u03c4 )k} x(k)esp(G) , (18)\nwhere \u03c4 > 0 and x (k) esp(G) is now the number of connected dyads that share exactly k neighbors (Fig.2d). To provide an interpretation of curved statistics and better understand the underlying modeling assumptions, let\u2019s examine the case of gwesp and proceed as in (Sec.3.2). Namely, let G be a graph and consider two connected nodes i, j with k shared partners. Let G\u2032 be the same graph as G but with an additional common neighbor between the nodes i, j, this implies\nx(k)esp(G \u2032) = x(k)esp(G)\u2212 1\nx(k+1)esp (G \u2032) = x(k+1)esp (G) + 1 ,\n(19)\nimplies that the 2-star term is unstable.\nwhere x (k) esp(G) is the number of connected dyads that share exactly k neighbors. For an ERGM including a gwesp term, we find\nP (G\u2032|\u03b8) P (G|\u03b8) \u221d exp\n[ \u03b8gwesp ( xgwesp(G \u2032|\u03c4)\u2212 xgwesp(G|\u03c4) )]\n\u221d exp [ \u03b8gwesp(1\u2212 e\u2212\u03c4 )k ] \u2261 \u03c1 .\n(20)\nWe start by noting that since \u03c4 > 0, we have 1\u2212 e\u2212\u03c4 \u2208 (0, 1). Moreover: (i) the scaling factor \u03b8gwesp > 0 implies \u03c1 > 1, hence preference for adding shared\npartners; \u03b8gwesp < 0 on the contrary implies a preference for deleting shared partners. This interpretation is akin to that of all standard ERGM parameters as discussed in (Sec.3.2).\n(ii) limk\u2192\u221e \u03c1 = 1 i.e. adding/removing shared partners to pairs of connected nodes with already many of them (k \u226b 1) has little effect on the ratio P (G\u2032)/P (G). This implies an effective cutoff for the order k of x (k) esp statistic that is relevant for\nthe system.\n(iii) The speed of the geometric decay is controlled by an external parameter \u03c4 , small values imply a rapid decay while high values imply a slow decay.\nHence, the gwesp statistic is a refined version of the simple sum of triangles. When the parameter \u03b8gwesp is positive, the general tendency for clustering will not push the system into a fully connected state. In fact, its implementation through the gwesp statistic implies a strong advantage for the addition of shared partners for poorly connected nodes, while almost no effect for nodes that already have many connections. This is indeed a more realistic behavior of a triadic closure, which in the case of brain networks it is expected to be constrained by the inner physical and functional resources [9].\nThe same reasoning applies to other curved statistics, such as gwd and gwnsp. These two statistics are alternatively used to test the hypothesis that shortest paths results from the presence of hubs or two-paths. The former would affect the node degree distribution, the latter the distribution of non-edgewise shared partners. Thus, via the gwd and gwnsp statistics, the presence of short paths in the network will result in positive \u03b8gwd and \u03b8gwnsp values, without the system ending up in a fully connected/empty state."
        },
        {
            "heading": "4. ERGMs in neuroscience",
            "text": "ERGMs have started to be exploited as statistical framework to study brain networks since 2010 [34]. The typical workflow of an ERGM analysis in neuroscience is summarized in Fig.3. In this section, we present an overview of the most recent results obtained in the last decade. A schematic summary of the existing literature can be found in Tab.2.\nWhile ERGMs are mathematical abstractions and can be applied to any network, most research has so far focused on functional brain connectivity derived from fMRI and EEG data. In the following, we structure the presentation of the current literature around three main scientific questions: i) what are the basic network mechanisms of healthy brain functioning (Sec.4.1), ii) how to statistically compare brain networks between different states (Sec.4.2), and iii) how to explicitly characterize the temporal network evolution following a pathological event (Sec.4.3)."
        },
        {
            "heading": "4.1. Minimal model of brain networks",
            "text": "At the core of many ERGM implementations for neuroscience there is the model first proposed by Simpson et al. in two consecutive works [88, 89], where the H-function reads as\n\u2212H(G|\u03b8) = \u03b81xe(G) + \u03b82xgwesp(G|\u03c4) + \u03b83xgwnsp(G|\u03bd) (21)\nThe edges term xe(G) characterizes the density of the network. This statistic is typically used to ensure that ERGMs also reproduce the actual number of connections in the observed network. The gwesp term xgwesp(G|\u03c4) is meant to capture the overall network clustering, a property that is crucial for the segregation of information in the brain [19, 20]. The gwnsp term xgwnsp(G|\u03c4) is related to the presence of two paths in the network and can be regarded as a measure of integration of information in the brain [98]. Hence, Eq.21 defines a parsimonious model of the fundamental properties of brain networks, i.e. connectedness, clustering, and global-efficiency.\nIn [88, 89] the authors applied this model to fMRI brain networks from 10 healthy subjects, fitted separately per each individual. The same model was also used in [95] to characterize fMRI brain networks from 200 healthy individuals. All these works agreed on assigning negative values to the parameters associated with edges and gwnsp statistics and positive values to gwesp statistics, while the decay parameter for the last two cases was fixed to \u03c4 = \u03bd = 0.75 [88]. The edges parameter \u03b81 < 0 confirms the tendency of the network to be sparse. The gwnsp parameter \u03b83 < 0 indicated that any tendency for global integration of information in the brain is unlikely to be supported by shortest paths with length equal to 2. The gwesp parameter \u03b82 > 0 showed that brain networks are statistically inclined to form clusters, which is a basic hallmark of brain segregation of information.\nIn subsequent studies, [92, 94] investigated whether and how the aforementioned\nwhole-brain statistical network properties were also present in specific subsystems associated with basic behavioral functions. Resting-state fMRI networks were then constructed and fitted separately in 21 healthy subjects and for 8 different subsystems, namely auditory, subcortical, dorsal attention, ventral attention, salience, cinguloopercular task control, fronto-parietal task control, default mode [99] (Fig.4).\nTo carry out this analysis, the authors introduced the so-called correlation generalized ERGM (cGERGM). This formulation of the model is still based on the ERGM in Eq.21, with a slightly different geometrical weighting strategy of the graph statistics [69, 94]. However, cGERGM differs from the standard approach in two substantial ways. First, cGERGM was designed to handle full weighted correlation matrices, instead of dealing with unweighted sparse networks. Secondly, additional geometric information on the nodes were modeled externally to the ERGM by using a beta regression of the group-averaged link weights on the graph statistics [92]. In\nparticular, two kinds of geometric information were included: a nodematch term (Eq.13) to measure the tendency of connected nodes to lie in the same hemisphere, and a edge covariate term (Eq.12) to measure the total Euclidean distance between all connected nodes. To emphasize the difference from the ERGM parameters, we shall call \u03b24, \u03b25 the ones controlling for these effects.\nAcross all sub-systems, and almost all subjects, authors found consistent results with parameter values statistically different from 0 especially for the larger subsystems, i.e. cingulo-opercular, salience, default mode, fronto-parietal. The sign of the \u03b82 > 0 and \u03b83 < 0 parameters associated respectively with gwesp and gwnsp statistics, were in line with those reported for the entire brain network. Instead, the edge statistics was associated with a positive parameter \u03b81 > 0. This result is consistent with the fact that brain subsystems are more densely connected internally than between each other [101, 18].\nThe estimation of the regression parameters associated with the spatial geometry of the brain structural networks turn out to be much more inconsistent across subsystems and subjects. The significantly positive hemispheric parameter (\u03b24 > 0) indicates that the nodes in the fronto-parietal and default-mode subnetwork are more likely to be connected within hemispheres. Instead, there is a general weak tendency of all subnetworks, but the auditory one, to penalize long distant connections (\u03b25 < 0, albeit significant for only between 20% and 62% of the individuals).\nThe overall emerging picture is that of a modular brain network reflecting a significant clustering behavior together with a strong intrahemispheric connectivity. Note that the negative gwnsp parameters do not necessarily imply the absence of\n2-paths in the networks. Instead, they indicate that short paths of length 2 are relatively underrepresented with respect to what could be expected when considering other statistics alone e.g., triangles.\nA related question is to what extent integration of information is statistically supported by short paths in the brain network. From a biological perspective, it would be more plausible that information flows through hubs and not via short paths, although both mechanisms are not mutually exclusive [102]. To address this question, [91] considered EEG resting-state networks in a group of 108 healthy subjects. In particular, they compared two different ERGMs, one including gwesp and gwnsp statistics, and the other including gwesp and gwd statistics, that is\n\u2212H(G|\u03b8) = \u03b82xgwesp(G|\u03c4) + \u03b83xgwd(G|\u03bd); (22)\nwhile holding fixed the number of edges in the network xe(G) \u2261 xe(G\u2217) and \u03c4 = \u03bd = 0.75.\nResults showed that the second ERGMwas able to better reproduce brain network properties not included in the model such as modularity, global- and local-efficiency. Notably, both \u03b82 and \u03b83 values were in average positive and significant confirming their statistical relevance for brain networks. Taken together, these findings support the hypothesis that while information segregation emerges from triangles, information integration could be actually mediated by hubs rather than shortest paths.\n4.1.1. Group-representative networks. Thus far, ERGMs have been used to characterize brain networks obtained from different individuals, separately. The resulting parameters distribution from many subjects can be then used to assess statistical properties at the group-level via standard tools such as hypothesis testing and regression analysis. Alternatively, assuming that brain networks from different individuals are stochastic realizations of the same system, they can be simultaneously modeled so as to obtain a unified group description. This is the rationale behind the construction of a group representative network (GRN) i.e. a network that summarizes the statistical properties of a given ensemble (Fig.5).\nIn network neuroscience, this question has been mainly addressed by aggregating the connectivity matrices of different individuals to obtain a group-average (mean-GRN ) or group-median (median-GRN ) representative network [22, 103]. Alternatively, methods to assess the statistical significance of the pooled connection values for each pair of nodes can be adopted [104]. Despite being computationally straightforward, the major limitation of these methods is that they treat edges independently and cannot directly inform on complex connection properties involving more than 2 nodes. An interesting possibility is therefore to incorporate the intersubject variability directly into the ERGM formulation.\nIn [89], the authors proposed to generate a GRN starting from the ERGMs in (Eq.21) fitted separately on different subjects. Then, a group representative network is simulated via a new ERGM with the H-function\n\u2212H(G|\u03b8) = \u00b51xe(G) + \u00b52xgwesp(G|\u03c4) + \u00b53xgwnsp(G|\u03bd) (23)\nwhere \u03c4 = \u03bd = 0.75 are fixed and the \u00b5 are the group-averaged values of the individual model parameters. This procedure better reproduced several topological properties of the actual brain networks, as compared to standard ERGMs fitted on the groupaveraged network. Note that this is a frequentist approach, the estimation of the\ngroup level parameters are point-like averages. We refer to this GRN construction scheme as a mean-ERGM.\nThe idea of summarizing individual estimates in a vector of hyper-parameters was pushed further in [95]. Accordingly, the model parameter vector for each individual is a realization of a normal probability distribution \u03b8 \u223c N (\u00b5,\u03a3\u03b8). The hyper-parameters (\u00b5,\u03a3\u03b8) are estimated by extending to a multilevel hierarchical setting a Bayesian formulation of the ERGM (BERGM), developed in [68]. A GRN can then by obtained by sampling a network from the posterior distribution of parameters. This method is named multi-BERGM."
        },
        {
            "heading": "4.2. Identification of discriminant brain network features",
            "text": "ERGMs can also be used to assess statistical differences between brain networks obtained under different experimental conditions or belonging to different groups of subjects.\nIn [91] the authors analyzed EEG networks with 56 nodes in 108 healthy subjects under two different conditions: eyes closed (EO) and eyes closed (EC) resting-states. To this end, they used the same ERGM defined in Eq.22. In particular, separate ERGMs were estimated from brain networks extracted from each individual, condition and frequency band theta (4 \u2212 7 Hz), alpha (8 \u2212 13 Hz), beta (14 \u2212 29 Hz), gamma (30 \u2212 40 Hz). The estimated parameter values were then compared between the two conditions. Results showed that \u03b82 values, associated with the gwesp statistics, were significantly higher in EO than EC, for both alpha and beta bands. This result is consistent with the local increase of EEG activity widely reported in literature for those bands [105, 106]. Notably, the difference in the beta band could not be detected by a standard network analysis.\nERGM-based analyses have been also adopted to characterize aging. To this end,\nin [95] the authors compared fMRI brain networks in a group of young (Y: 18 \u2212 33 ) and elderly (O: 74\u2212 89) human subjects [95]. Starting from the hierarchical Bayesian ERGM presented in (Sec.4.1.1), a further hierarchical layer is added to account for the population partitioned in two subgroups, yielding two different hyper-parameters \u03d5(Y ) = (\u00b5(Y ),\u03a3\u03b8) and \u03d5\n(O) = (\u00b5(O),\u03a3\u03b8) and same covariance structure is assumed for the two groups.\nThe results showed that the estimates for the parameter associated with the gwnsp statistics were significantly higher in the young group compared to the old one (Fig.6). These findings indicated that functional brain networks are prone to significant loss of integration of information across the lifespan, consistently with previous findings reporting a decreasing global-efficiency with age [22].\nOne might then wonder whether similar changes occur in structural brain networks, whose connectivity exhibits much slower changes [107]. Sinke et al. [90] studied DTI networks from 382 healthy subjects and considered 4 age-groups: 20\u221234, 35 \u2212 50, 51 \u2212 70, > 70 years (Fig. 7). Assuming a low variability between the individuals in the same group, authors constructed a mean-GRN for each group and used a Bayesian ERGM (BERGM) [68].\nThe H-function in this case was slightly different from Eq.21 i.e.\n\u2212H(G|\u03b8) = \u03b81xe(G) + \u03b82xgwesp(G|\u03c4) + \u03b83xgwnsp(G|\u03bd) + \u03b84xnm(G) , (24)\nwhere xnm(G) is a nodematch term (Eq.13) measuring in this case the number of connections within the same hemisphere.\nThe signs of the fitted model parameters (\u03b81 < 0, \u03b82 > 0, \u03b83 < 0) confirmed the general tendencies observed in functional brain networks (Sec.4.1). The parameter \u03b84 was instead negative for all groups, suggesting that the hemispheric nodematch is relatively low taking into account all the other included statistics. We stress that this does not necessary imply an absolute low intrahemispheric connectivity.\nMoreover, differently from previous studies [108, 109], only weak changes were found between the parameter values across age-groups. Future studies will elucidate whether this result is actually reflecting a more stable topology of structural brain networks or a result of the group-averaging of the original brain networks."
        },
        {
            "heading": "4.3. Predicting states in temporal brain networks",
            "text": "So far, ERGMs have not explicitly taken into account the fact that brain networks are intrinsically dynamic and their topology can change, fluctuate and evolve over multiple\ntime scales. Over short-time scales, brain networks rapidly reconfigure during motor or cognitive tasks to adapt to behavioral efforts or loads [110, 111, 112, 113]. Over longer time scales, brain networks can exhibit significant topological losses, such as in aging or in neurodegeneration [114, 115], as well as tentative restorative patterns in recovery after stroke or traumatic injuries [116].\nIn these cases, it is crucial to formulate statistical network models that explicitly include time, so as to capture time-varying connection properties that are predictive of the system behaviour [117, 118]. One possibility is to endow the ERGM with a time structure and estimate parameters from a time series, or sequence, of observed graphs G1, . . . , GT . One can start by assuming a first-order Markov time dependence, write the probability density function for the entire network sequence as\nP (Gt, . . . , Gt\u22121|\u03b8) = T\u220f\nt=2\nP (Gt|Gt\u22121,\u03b8) (25)\nand generalize Eq.1 in a straightforward fashion:\nP (Gt|Gt\u22121,\u03b8) = e \u03b8\u00b7x(Gt,Gt\u22121)\u2211\nG\u2217\u2208G e \u03b8\u00b7x(G\u2217,Gt\u22121) , (26)\nwhere the x statistics can be now defined over two consecutive graphs in the timeseries. We shall refer to this model as temporal ERGM (TERGM) [119].\nTERGMs have been recently adopted to model brain network reorganization after stroke Obando et al. [96]. The study was carried out on longitudinal fMRI restingstate networks measured in 49 patients at 2 weeks, 3 months and 1 year after their first ever unilateral stroke event [120, 116] (Fig.8a).\nThe model (Eq.26) was specified with the following temporal H function:\n\u2212H(Gt|Gt\u22121,\u03b8) = \u03b81xe(Gt) + \u03b82xstab(Gt, Gt\u22121) + + \u03b83xte(G t, Gt\u22121) + \u03b84xttr(G t, Gt\u22121)\n(27)\nBesides the standard edges term (Eq.11), the three other temporal statistics were defined as follows.\nThe stability term xstab simply counts the number of dyads that do not change in the time step t \u2212 1 \u2192 t and it has a fundamental role for the estimation since it sets the pace of allowed changes in the network sequence. The temporal-edge xte term counts the number of new inter-hemispheric edges appearing at time t, while the temporal-triangle xttr term counts the number of triangle closures over time within the lesioned hemispheres (Fig.8b).\nThese two last statistics are entrusted with the goal of mimicking the brain plasticity process i.e. the recovery of between-hemisphere integration and withinhemisphere segregation [116, 121]. The TERGM analysis resulted in positive estimates (\u03b83 > 0, \u03b84 > 0) and significantly higher in stroke patients compared to healthy controls (Fig.8c). This suggested that both xte and xttr are fundamental temporal processes of post-stroke brain reorganization. Notably, the same conclusions could not be reached when considering a static equivalent formulation of Eq.27 [96].\nMoreover, the same parameters estimated from the temporal networks in the subacute phase (from 2 weeks to 3 months) significantly predicted the future language\nscore (1 year) of the patients, suggesting their potential use as clinical biomarkers (Fig.8d).\nAn alternative approach was adopted in [93] to characterize the recovery from coma after traumatic brain injuries. Authors evaluated longitudinal fMRI brain networks obtained at 11, 18, 25 and 181 days after the trauma. In particular, they used separable temporal ERGMs (sTERGMs) [72], which model separately and indipendently the tendencies to form and dissolve instances of different graph statistics.\nThe network at the time-step t is therefore given by the following:\nGt = G+ \u2212 (Gt\u22121 \u2212G\u2212) (28)\nwhere Gt\u22121 it the graph time t \u2212 1, while G+ and G\u2212 are the so-called formation and dissolution graphs. More in detail, the formation graph is drawn from \u223c P+(Gt|Gt\u22121,\u03b8+), where P+ is the same as in Eq.26 but with the formation statistics x+ and parameters \u03b8+. In this formation process, ties can only be added to Gt\u22121. Analogously, the dissolution graph is obtained by removing edges from Gt\u22121 and the\ngraph is drawn from\u223c P\u2212(Gt|Gt\u22121,\u03b8\u2212), with dissolution statistics x\u2212 and parameters \u03b8\u2212. The goal of a sTERMG is to estimate both sets of parameters \u03b8+,\u03b8\u2212.\nThe approach in this study was to include in the model a huge number of potentially relevant statistics and let sTERMGs select the most relevant ones. Results showed indeed that only a subset of the estimated parameters turned out to be statistically significant. The recovery process was characterized by a tendency to preserve and strengthen connections within pairs of subsystems (e.g., default, frontoparietal, thalamus, visual) and between some of them (eg, default-visual, somatomotor-frontoparietal, ventral-visual). Similarly to the results found in [96], a significant propensity to close triangles over time also emerged from this study, thus confirming the importance of the local clustering connections in shaping the brain reorganization process from brain damage-induced coma."
        },
        {
            "heading": "5. On the interpretation of ERGMs",
            "text": "Up to now, we have not lingered on the foundation and subtleties of ERGMs, albeit they are crucial to understand and interpret the related results. Due to the number and variety of scientists and practitioners that have developed or used these models, there is sometimes confusion in the scientific literature of ERGMs, if not in the notions at least in the lexicon. This might lead to unsupported claims or reckless interpretations of the results. We therefore aim at clarifying in this section the philosophy of the method and its theoretical underpinnings.\nOur discussion lies on three fundamental concepts: i) description, ii) prediction, and iii) explanation. We briefly pin-down for clarity the corresponding definitions. Description means a characterization, or a summary, of the state of the system under investigation, based on the choice of suitable variables and observables to be measured. By definition, a description refers to the past or present state of the system. Prediction is the anticipation (forecast) of the future state of a system. Explanation aims at reaching a causal or mechanistic understanding of why a particular phenomenon or event has occurred, based on a theoretical framework or empirical evidence [122, 123, 124].\nWe argue that in general ERGMs as those described in (Eq.1) and their derivatives, are certainly descriptive, might be predictive, but they are not explicative. Note that a method can be tremendously efficient in predicting outcomes of an experiment while at the same time providing no explanation on the underlying physics. Analogously, a description of the phenomena under investigation is radically different from a scientific explanation [122]."
        },
        {
            "heading": "5.1. ERGMs and the maximum entropy principle (MEP)",
            "text": "First, let us explicit the origin of the ERGM formulation from basic information theoretic principles. ERGMs can be indeed seen as one of the many particular instances belonging to the broad class of Maximum Entropy (Max.Ent.) models [14, 15, 125]. Similar models have been applied to neural populations [126], antibody diversity [127], protein sequences [62], economics [128], population genetics [129] and many more. Let us then derive the ERGMs from the MEP.\u2225.\n\u2225 Note, however, that this is not the way the ERGMs were originally introduced, which instead was based on the Hammersley-Clifford theorem for Markov graphs [47], building on a previous work of J. Besag in the context of spatial models of lattice systems [49].\nConsider an observed graph G\u2217 \u2208 G and assume that all the relevant information about G\u2217 is encoded in a set of sufficient statistics x(G\u2217). According to the MEP, the most unbiased probability density function P (G) consistent with available knowledge is obtained by maximizing the Shannon information entropy S\nS[P ] = \u2212 \u2211 G\u2208G P (G) logP (G) (29)\nunder the constraints \u2211 G\u2208G\nP (G) = 1 , (30)\u2211 G\u2208G x(G)P (G) = x(G\u2217) : (31)\nthe first one is just a normalization, the others fix the expected values of the sufficient statistics over the ensamble G to the correspondent values computed for G\u2217.\nThe rationale at the basis of the Max.Ent. recipe is very pragmatic, as already stressed in [130], and dates back in similar forms to Bernoulli and Laplace. Let us consider the total number of ways in which the probability distribution of a system can be realized, given that all its possible states are equivalently likely. By a simple combinatorial theorem [130], it can be shown that the Max.Ent. distribution is the most likely and also that any other distribution becomes highly atypical as the number of degrees of freedom - in our case, the number of possible edges - becomes large.\nIn order to solve the constrained maximization problem defined by Eqs.29-31 we introduce a set of Lagrange multipliers \u03bb \u2208 Rr+1 and maximizing over P the expression:\n\u2212 \u2211 G\u2208G P (G) logP (G)+\u03bb0 ( \u2211 G\u2208G P (G)\u22121 ) + R\u2211 \u03b9=1 \u03bb\u03b9 ( \u2211 G\u2208G x\u03b9(G)P (G)\u2212x\u03b9(G\u2217) ) , (32)\nwhich gives P (G|\u03bb) = e\u03bb0\u22121+ \u2211r\n\u03b9=1 \u03bb\u03b9x\u03b9(G). By imposing (Eq.30-31), we find \u03bb0 = 1 \u2212 logZ and \u03bb\u03b9 = \u03b8\u03b9 for \u03b9 = 1, . . . , R, with the \u03b8 being defined as the choice of the corresponding multipliers which satisfy Eq.31. The result is precisely Eq.1.\nThe same argument can be used for a temporal series of graphs G = G1, . . . , GT . If the information is available in the form of graph statistics x(G\u2217) evaluated on the observed sequence G\u2217, the path entropy reads as\nS[P ] = \u2211\nG1\u2208G\n\u00b7 \u00b7 \u00b7 \u2211\nGT\u2208G\nP (G) logP (G) , (33)\nand the mass probability function becomes\nP (G|\u03b8) = e \u03b8\u00b7x(G)\u2211 G\u03031\u2208G \u00b7 \u00b7 \u00b7 \u2211 G\u0303T\u2208G e \u03b8\u00b7x(G\u0303) , (34)\nwhich is the more general case of the TERGMs presented before (Eq.25-26). This generalized variational principle is sometimes referred as maximum caliber (Max.Cal.) [131]."
        },
        {
            "heading": "5.2. Description and prediction: the choice of graph statistics",
            "text": "As discussed in (Sec.3.3.1), when formulating ERGMs it is crucial to avoid degeneracy issues which might occur especially with too simplistic model formulations. The possibility of including several potentially relevant effects (model terms) has two advantages. On the one hand, it helps alleviating degeneracy issues, on the other hand it allows to statistically test the joint effect of multiple network properties. Once the model is specified and as long as the estimation process converges properly, an ERGM is always descriptive, the estimated parameters convey information on the relevance of the associated statistics. Whether an ERGM is also predictive is a less trivial question that we address in the following.\nA delicate implicit assumption of ERGMs is that the selected statistics x(G\u2217) encode all the relevant information of the system, i.e. they are sufficient statistics. This assumption raised major criticism around Max.Ent. and related methods, see e.g. [132, 133, 129]. Indeed, what is deemed to be relevant entirely relies on the modeler\u2019s hypothesis and is consequently affected by some arbitrariness. This is particularly true in complex systems science where, almost by definition, we rarely have control on the hidden degrees of freedom [6, 134]. It might be that what we aim to measure, or what we are able to measure, simply is not a relevant feature of the system.\nIn the case of TERGMs, such as those in Eq.25-26, there are two more subtleties. First, the Markov hypothesis in Eq.25 is in general hard to justify a-priori [135, 133]. Second, when writing Eq.26 we implicitly assume that the selected statistics are relevant throughout the observed time window. This is true if we can assume the stationarity of the process [136], whose biological plausibility is however disputable [137]. Both these two assumptions are further complicated by the fact that in many real applications the measurements of networks are not equally spaced in time [138].\nTo address these issues, a-posteriori model-selection approaches like goodness of fit (GoF) or out-of-sample assessment can be adopted to rule out models that are markedly wrong. These methods evaluate the ability of the graphs simulated with a fitted ERGM to reproduce properties of the system other than those explicitly included. When this happens then the estimated ERGM is not only descriptive but also predictive. Thus, when using ERGMs with GOF assessments, it is not only important to use appropriate relevant graph statistics but also to have a precise hypothesis on the network features against which to test the predictive performance."
        },
        {
            "heading": "5.3. Explanation: ERGMs and Stat.Mech.",
            "text": "In the previous sections, we have insisted on the role of the modeler in shaping the mass probability distribution by selecting a set of graph statistics. This is consistent with the E.T. Jaynes\u2019 subjective interpretation of probability [139, 131], according to which probabilities are epistemic statements about physical systems. Put differently, they reflect our state of knowledge or, equivalently, of ignorance, about the world.\nSuch subjective probability is enough for purely inferential purposes. For example, even if Eq.1 is not the true probability density function of the system under investigation, it may nevertheless have good predictive performance (GoF) for a given set of test statistics. Indeed, the ERGM inferential scheme is agnostic about the data-generating process.\nThis property is somewhat unsatisfactory if the ultimate goal is to explain the physics underlying the observed data. In that case, probabilities should not reflect a\nsubjective state of knowledge but they should have an objective physical meaning. This is, for instance, the case of statistical mechanics where probabilities have an objective irreducible meaning, they do not depend on the observer. The role of the theory is then to explain how the macroscopic properties of bodies, mainly thermodynamic properties, statistically arise from their microscopic structure [140]. The formal analogy between the ERGM in (Eq.1) and the Gibbs-Boltzmann distribution for physical systems at equilibrium has instilled hopes for a similar microlevel objective interpretation of the ERGM probability density function. However, on a closer inspection we immediately realize that this equivalence is problematic.\nThese two exponential distributions have in fact very different theoretical underpinnings. Just as an example: at the very foundation of statistical mechanics, and hence of Gibbs-Boltzmann exponential distributions, there is the assumption that the system dynamics obeys a detailed balance [141]. In its simplest form, detailed balance means that for any two accessible states r, s the transition rate Wr\u2192s is equal to its inverse Ws\u2192r. This assumption becomes highly disputable when it comes to biological processes. Detailed balance is indeed violated at the molecular level and apparently so also for large-scale brain dynamics even during intrinsic spontaneous activity [142]. Therefore, in general, that between the ERGM and the Gibbs-Boltzmann distributions should not be considered more than a formal analogy, and their respective interpretations should not be confused."
        },
        {
            "heading": "6. Conclusion and perspectives",
            "text": "Statistical models are crucial to gain intuition on the connection rules and generative mechanisms of brain networks across multiple time and spatial scales [143]. Furthermore, they can be used to improve the detection of the underlying network properties in the presence of statistical noise in the data [144, 145, 146].\nHere, we have discussed a family of models - exponential random graph models (ERGMs) - which allows for simultaneously taking into account different local connection properties, disentangle their interactions and quantify their statistical influence on the global observed network. The number of studies using ERGMs to model brain connectivity networks has rapidly increased in the last decade. We have discussed how they have been used to characterize intrinsic brain connectivity, build group representative networks, discriminate different brain states, as well as model temporal patterns, with focused applications to brain diseases.\nIn addition, we have placed the ERGM framework in the broader context of maximum entropy models and discussed in detail the subtleties of network analyses based on the Max.Ent. recipe, with particular focus on the meaning of the inferred model parameters. We have discussed the descriptive and predictive power of ERGMs, and warned about its interpretation as an equilibrium theory of the brains\u2019 biological processes, for which a non-equilibrium picture is more appropriate.\nDespite the increasing number of works using or developing ERGMs to address neuroscience-related questions, the field is still in its infancy and we hope this review will encourage curious physicists, network scientists, and neuroscientists to pursue the research direction we have laid out here. In the years to come, we anticipate a boost in the field fueled by an increasing availability of high-quality experimental data, which is crucial to validate the hypothesized mechanisms. This includes the use of ERGMs to model brain diseases not only from a recovery perspective (e.g., stroke or coma), but also in terms of degeneration (e.g. Parkinson, Alzheimer\u2019s), as well as to evaluate the\nefficacy of treatments for psychiatric conditions (e.g. Schizofrenia) and developmental disorders (e.g. autism).\nOn a more methodological note, only a small fraction of the ERGM-like galaxy of statistical models has been employed so far in network neuroscience. Among the unexplored alternatives, we spot out the potential use of multilayer ERGMs [147] to capture interactions between different scales of neural organization [143]. As for temporal analyses, a promising family of models beyond the (T)ERGM paradigm is the related stochastic actor oriented model (SAOM), which simulates time-varying network changes through a microscale process defined at the level of the nodes, i.e. the actors [148]. Critically, both TERGMS and SAOM only take into account monotonic network changes increasing or decreasing over time. However, modeling brain reorganization might require capturing non-monotonic mechanisms, too. This is particularly true for intrinsic brain functioning and cognitive/motor tasks, which typically exhibit oscillatory time-varying dynamics [113]. In this direction, the development of models that integrate this dynamic component, such as the varyingcoefficient ERGM [149], appear particularly important to overcome such limitations. A distinct mention is for stochastic block models (SBMs), which allows flexible and scalable analysis of complex networks [150]. In addition to handling community structures, recent SBM extensions also include several types of structures, such as node degree sequences [151], hierarchical structures [152], edge weights [153], and triadic closures [154]. SBM applications to brain networks are still in their infancy (see [13] for a recent example), but they surely represent a future promising research avenue. Finally, a long-term and ambitious goal would be to enrich statistical models by including higher-order interactions, which might reveal previously unappreciated brain organization principles [155].\nWe believe that combining physics-inspired concepts, biological knowledge, and statistical methods is essential for unraveling the intricacies of the human brain\u2019s structure and dynamics. This synergy holds the potential to provide crucial insights into the complexity of the brain."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank the anonymous reviewers, whose comments helped us improving the initial version of the review. We thank Erik Aurell and Mario Chavez for many insightful discussions and for providing useful remarks on the manuscript. We are also thankful to Thibault Rolland and Remy Ben Messaoud for their contribution in the preparation of the figures. FDVF acknowledges support from the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation program (Grant Agreement No. 864729)"
        }
    ],
    "title": "Statistical models of complex brain networks: a maximum entropy approach",
    "year": 2023
}