{
    "abstractText": "Transformers are state-of-the-art networks for most sequence processing tasks. However, the self-attention mechanism often used in Transformers requires large time windows for each computation step and thus makes them less suitable for online signal processing compared to Recurrent Neural Networks (RNNs). In this paper, instead of the self-attention mechanism, we use a sliding window attention mechanism. We show that this mechanism is more efficient for continuous signals with finite-range dependencies between input and target, and that we can use it to process sequences element-by-element, this making it compatible with online processing. We test our model on a finger position regression dataset (NinaproDB8) with Surface Electromyographic (sEMG) signals measured on the forearm skin to estimate muscle activities. Our approach sets the new state-of-the-art in terms of accuracy on this dataset while requiring only very short time windows of 3.5 ms at each inference step. Moreover, we increase the sparsity of the network using Leaky-Integrate and Fire (LIF) units, a bio-inspired neuron model that activates sparsely in time solely when crossing a threshold. We thus reduce the number of synaptic operations up to a factor of\u00d75.3 without loss of accuracy. Our results hold great promises for accurate and fast online processing of sEMG signals for smooth prosthetic hand control and is a step towards Transformers and Spiking Neural Networks (SNNs) co-integration for energy efficient temporal signal processing.",
    "authors": [
        {
            "affiliations": [],
            "name": "N. Leroux"
        },
        {
            "affiliations": [],
            "name": "Peter Gr\u00fcnberg"
        }
    ],
    "id": "SP:75a1498875a95962abf096cc65811fbb0ebd9792",
    "references": [
        {
            "authors": [
                "Mingde Zheng",
                "Michael S. Crouch",
                "Michael S. Eggleston"
            ],
            "title": "Surface electromyography as a natural human\u2013machine interface: A review",
            "venue": "doi: 10.1109/JSEN.2022.3165988. Conference Name: IEEE Sensors Journal",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Tianyang Lin",
                "Yuxin Wang",
                "Xiangyang Liu",
                "Xipeng Qiu"
            ],
            "title": "A survey of transformers. 3:111\u2013132",
            "venue": "ISSN 2666-6510. doi: 10.1016/j.aiopen.2022.10.001. URL https: //www.sciencedirect.com/science/article/pii/S2666651022000146",
            "year": 2000
        },
        {
            "authors": [
                "Alessio Burrello",
                "Francesco Bianco Morghet",
                "Moritz Scherer",
                "Simone Benatti",
                "Luca Benini",
                "Enrico Macii",
                "Massimo Poncino",
                "Daniele Jahier Pagliari"
            ],
            "title": "Bioformers: Embedding transformers for ultra-low power sEMG-based gesture recognition",
            "venue": "doi: 10.23919/DATE54114.2022.9774639",
            "year": 2022
        },
        {
            "authors": [
                "Amirhossein Tavanaei",
                "Masoud Ghodrati",
                "Saeed Reza Kheradpisheh",
                "Timoth\u00e9e Masquelier",
                "Anthony Maida"
            ],
            "title": "Deep learning in spiking neural networks. 111:47\u201363",
            "venue": "ISSN 0893-6080. doi: 10.1016/j.neunet.2018.12.002. URL https://www.sciencedirect.com/science/ article/pii/S0893608018303332",
            "year": 2018
        },
        {
            "authors": [
                "Chen Liu",
                "Guillaume Bellec",
                "Bernhard Vogginger",
                "David Kappel",
                "Johannes Partzsch",
                "Felix Neum\u00e4rker",
                "Sebastian H\u00f6ppner",
                "Wolfgang Maass",
                "Steve B. Furber",
                "Robert Legenstein",
                "Christian G. Mayr"
            ],
            "title": "Memory-efficient deep learning on a SpiNNaker 2 prototype",
            "venue": "453X. URL https://www.frontiersin.org/articles/10.3389/fnins.2018.00840",
            "year": 2018
        },
        {
            "authors": [
                "Garrick Orchard",
                "E. Paxon Frady",
                "Daniel Ben Dayan Rubin",
                "Sophia Sanborn",
                "Sumit Bam Shrestha",
                "Friedrich T. Sommer",
                "Mike Davies"
            ],
            "title": "Efficient neuromorphic signal processing with loihi",
            "venue": "IEEE Workshop on Signal Processing Systems (SiPS),",
            "year": 2021
        },
        {
            "authors": [
                "Christian Pehle",
                "Sebastian Billaudelle",
                "Benjamin Cramer",
                "Jakob Kaiser",
                "Korbinian Schreiber",
                "Yannik Stradmann",
                "Johannes Weis",
                "Aron Leibfried",
                "Eric M\u00fcller",
                "Johannes Schemmel"
            ],
            "title": "The BrainScaleS-2 accelerated neuromorphic system with hybrid plasticity. 16:795876",
            "venue": "ISSN 1662-4548",
            "year": 2022
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E. Peters",
                "Arman Cohan"
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "URL http://arxiv.org/abs/2004.05150",
            "year": 2004
        },
        {
            "authors": [
                "Agamemnon Krasoulis",
                "Sethu Vijayakumar",
                "Kianoush Nazarpour"
            ],
            "title": "Effect of user practice on prosthetic finger control with an intuitive myoelectric decoder",
            "venue": "ISSN 1662-453X. URL https://www.frontiersin.org/articles/10.3389/fnins.2019.00891",
            "year": 2019
        },
        {
            "authors": [
                "Andr\u00e9s Jaramillo-Y\u00e1nez",
                "Marco E. Benalc\u00e1zar",
                "Elisa Mena-Maldonado"
            ],
            "title": "Real-time hand gesture recognition using surface electromyography and machine learning: A systematic literature review",
            "venue": "doi: 10.3390/s20092467. URL https://www.mdpi.com/1424-8220/20/9/2467",
            "year": 2009
        },
        {
            "authors": [
                "Bojan Milosevic",
                "Elisabetta Farella",
                "Simone Benatti"
            ],
            "title": "Exploring arm posture and temporal variability in myoelectric hand gesture recognition",
            "venue": "IEEE International Conference on Biomedical Robotics and Biomechatronics (Biorob),",
            "year": 2018
        },
        {
            "authors": [
                "Zahra Taghizadeh",
                "Saeid Rashidi",
                "Ahmad Shalbaf"
            ],
            "title": "Finger movements classification based on fractional fourier transform coefficients extracted from surface EMG signals. 68:102573",
            "venue": "ISSN 1746-8094. doi: 10.1016/j.bspc.2021.102573. URL https://www.sciencedirect. com/science/article/pii/S1746809421001701",
            "year": 2021
        },
        {
            "authors": [
                "Panagiotis Tsinganos",
                "Bruno Cornelis",
                "Jan Cornelis",
                "Bart Jansen",
                "Athanassios Skodras"
            ],
            "title": "Improved gesture recognition based on sEMG signals and TCN",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2019
        },
        {
            "authors": [
                "Marcello Zanghieri",
                "Simone Benatti",
                "Alessio Burrello",
                "Victor Kartsch",
                "Francesco Conti",
                "Luca Benini"
            ],
            "title": "Robust real-time embedded EMG recognition framework using temporal convolutional networks on a multicore IoT processor",
            "venue": "ISSN 1940-9990. doi: 10.1109/TBCAS.2019.2959160. Conference Name: IEEE Transactions on Biomedical Circuits and Systems",
            "year": 2019
        },
        {
            "authors": [
                "Marcello Zanghieri",
                "Simone Benatti",
                "Alessio Burrello",
                "Victor Javier Kartsch Morinigo",
                "Roberto Meattini",
                "Gianluca Palli",
                "Claudio Melchiorri",
                "Luca Benini"
            ],
            "title": "sEMG-based regression of hand kinematics with temporal convolutional networks on a low-power edge microcontroller",
            "venue": "IEEE International Conference on Omni-Layer Intelligent Systems (COINS),",
            "year": 2021
        },
        {
            "authors": [
                "Khairul Anam",
                "Cries Avian",
                "Dwiretno Istiyadi Swasono",
                "Aris Zainul Muttaqin",
                "Harun Ismail"
            ],
            "title": "Estimation of finger joint movement based on electromyography signal using long short-term memory",
            "venue": "In 2020 International Conference on Computer Engineering, Network, and Intelligent Multimedia (CENIM),",
            "year": 2020
        },
        {
            "authors": [
                "Philipp Koch",
                "Mark Dreier",
                "Anna Larsen",
                "Tim J. Parbs",
                "Marco Maass",
                "Huy Phan",
                "Alfred Mertins"
            ],
            "title": "Regression of hand movements from sEMG data with recurrent neural networks",
            "venue": "In 2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),",
            "year": 2020
        },
        {
            "authors": [
                "Zamroni Ilyas",
                "Khairul Anam",
                "Widjonarko",
                "Cries Avian",
                "Aris Zainul Muttaqin",
                "Mochammad Edoward Ramadhan"
            ],
            "title": "Evaluation of gated-recurrent unit for estimating finger-joint angle using surface electromyography signal",
            "venue": "In 2022 9th International Conference on Electrical Engineering, Computer Science and Informatics (EECSI),",
            "year": 2022
        },
        {
            "authors": [
                "Anand Kumar Mukhopadhyay",
                "Indrajit Chakrabarti",
                "Mrigank Sharad"
            ],
            "title": "Classification of hand movements by surface myoelectric signal using artificial-spiking neural network model",
            "venue": "IEEE SENSORS,",
            "year": 2018
        },
        {
            "authors": [
                "Man Yao",
                "Huanhuan Gao",
                "Guangshe Zhao",
                "Dingheng Wang",
                "Yihan Lin",
                "Zhaoxu Yang",
                "Guoqi Li"
            ],
            "title": "Temporal-wise attention spiking neural networks for event streams classification. pages 10221\u201310230",
            "venue": "URL https://openaccess.thecvf.com/content/ ICCV2021/html/Yao_Temporal-Wise_Attention_Spiking_Neural_Networks_for_ Event_Streams_Classification_ICCV_2021_paper.html",
            "year": 2021
        },
        {
            "authors": [
                "Alberto Sabater",
                "Luis Montesano",
                "Ana C. Murillo"
            ],
            "title": "Event transformer. a sparse-aware solution for efficient event data processing",
            "venue": "IEEE. ISBN",
            "year": 2022
        },
        {
            "authors": [
                "Stefano Pizzolato",
                "Luca Tagliapietra",
                "Matteo Cognolato",
                "Monica Reggiani",
                "Henning M\u00fcller",
                "Manfredo Atzori"
            ],
            "title": "Comparison of six electromyography acquisition setups on hand movement classification tasks. 12(10):e0186132",
            "venue": "ISSN 1932-6203. doi: 10.1371/journal.pone.0186132. URL https://dx.plos.org/10.1371/journal.pone.0186132",
            "year": 1932
        },
        {
            "authors": [
                "Emre O. Neftci",
                "Hesham Mostafa",
                "Friedemann Zenke"
            ],
            "title": "Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks. 36(6):51\u201363",
            "venue": "ISSN 1558-0792. doi: 10.1109/MSP.2019.2931595. Number: 6 Conference Name: IEEE Signal Processing Magazine",
            "year": 2019
        },
        {
            "authors": [
                "Yulong Yan",
                "Haoming Chu",
                "Yi Jin",
                "Yuxiang Huan",
                "Zhuo Zou",
                "Lirong Zheng"
            ],
            "title": "Backpropagation with sparsity regularization for spiking neural network learning",
            "venue": "ISSN 1662-453X. URL https://www.frontiersin.org/articles/10.3389/fnins.2022.760298",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Transformers are state-of-the-art networks for most sequence processing tasks. However, the self-attention mechanism often used in Transformers requires large time windows for each computation step and thus makes them less suitable for online signal processing compared to Recurrent Neural Networks (RNNs). In this paper, instead of the self-attention mechanism, we use a sliding window attention mechanism. We show that this mechanism is more efficient for continuous signals with finite-range dependencies between input and target, and that we can use it to process sequences element-by-element, this making it compatible with online processing. We test our model on a finger position regression dataset (NinaproDB8) with Surface Electromyographic (sEMG) signals measured on the forearm skin to estimate muscle activities. Our approach sets the new state-of-the-art in terms of accuracy on this dataset while requiring only very short time windows of 3.5 ms at each inference step. Moreover, we increase the sparsity of the network using Leaky-Integrate and Fire (LIF) units, a bio-inspired neuron model that activates sparsely in time solely when crossing a threshold. We thus reduce the number of synaptic operations up to a factor of\u00d75.3 without loss of accuracy. Our results hold great promises for accurate and fast online processing of sEMG signals for smooth prosthetic hand control and is a step towards Transformers and Spiking Neural Networks (SNNs) co-integration for energy efficient temporal signal processing."
        },
        {
            "heading": "1 Introduction",
            "text": "Surface Electromyography (sEMG) is a technique that senses currents running through muscular fibers\u2019 membrane to measure muscular activity [1]. As sEMG signals are triggered by electrical stimuli from the central nervous system, this method is gaining a strong interest as a mean for Human-Machine Interfacing [1]. Since sEMG measurements only require electrodes positioned on the forearm skin, this technique is very promising for future non-invasive wearable prosthetic hand control system [1].\nPreprint. Under review.\nar X\niv :2\n30 3.\n11 86\n0v 1\n[ cs\n.N E\nTransformers, which are the state-of-the-art networks for sequence processing [2, 3], can be very efficient to process sEMG signals [4]. However, the self-attention mechanism [2] used in conventional transformers requires to wait for large time windows, which induces a delay preventing fast online processing of continuous signals. Moreover, memory and computation of the self-attention mechanism scales quadratically with the sequence length.\nIn contrast, Recurrent Neural Networks (RNNs) integrate the concept of time into their operating model and are thus suited for continuous signals online processing. Spiking Neural Networks (SNNs) [5, 6] are a bio-inspired type of RNNs. They are very promising for low power applications because their neurons only transmit information when their membrane potential (an internal state of each neuron) reaches a threshold, and these events happen sparsely in time [6]. Many research focus on building new hardware that leverage the inherent temporal sparsity of SNNs [7, 8, 9, 10].\nIn this paper, we propose an online transformer that makes use of a linearized sliding window attention mechanism [11]. We adapt this attention mechanism for online processing of continuous signals by making it forward in time and serialized. Our online transformer thus performs inference for each token as they are generated. In order to leverage information from past inputs, we store information in the keys and the values of the attention mechanism, and we update this memory dynamically as the tokens are generated. The length of the sequences stored in the keys and the values is a hyper-parameter that we can tune to change the temporal depth of the information used in the attention mechanism, as well as the computational complexity and the memory usage.\nWe test our model on a finger position regression through sEMG signals using the Non-Invasive Adaptive Hand Prosthetics Database 8 (NinaProDB8) dataset. First, we show that our online transformer allows users to process sEMG signals with high accuracy using solely very short time windows of 3.5 ms, which permits a very fine granularity in time of prosthetic hand control. Secondly, we show that selecting the temporal depth of the attention improves the results of signal processing and makes our model outperform a self-attention-based transformer, as well as previous state-of-theart models. Finally, we show how our custom online attention mechanism allows us to SNNs inside the transformer architecture to increase the network sparsity, which in turn results in a reduction of the required number of synaptic operations by a factor of \u00d75.3 without loss of accuracy."
        },
        {
            "heading": "2 Related work",
            "text": "Deep Learning for Surface Electromyography processing Although sEMG signals and muscle activity are correlated, their relation is unknown and processing sEMG signals remains very challenging because of electrical noise (e.g., interference, ground noise, crosstalk between electrodes), inter-subject variability (e.g., different forearm circumferences, muscle characteristics), and intrasubject variability (e.g., variation of the electrodes position or the skin conductivity from one day to the next) [12].\nDeep learning methods can leverage large datasets to extract the more relevant features despite noise or variability [13]. They can thus outperform conventional machine learning techniques like Support Vector Machine (SVM) [14]. Moreover, deep networks can process raw sEMG signals whereas conventional networks require prior pre-processing like Principal Component Analysis [15], Linear Discriminant Analysis (LDA) [15], Fourier transforms [16], and others.\nDeep learning has already been applied to sEMG signals processing using Temporal Convolutions [17, 18, 19] and Recurrent Neural Networks (RNNs) [20, 21, 22, 23]. While the ability to compute on the edge with restricted memory capacity and low power consumption are essential to the deployment of autonomous wearable prosthetic hand control systems, most deep learning techniques are computationally intensive. Mukhopadhyay et al. [24] have shown that the inherent sparsity of SNNs can be leveraged to reduce drastically the computational intensity of sEMG signals processing. Burrello et al. [4] have shown that a transformer network can process sEMG signals with a limited memory usage and reduced number of Multiply-And-Accumulate (MAC) operations.\nTransformers Unlike RNNs, Transformers do not suffer from the vanishing gradient problem for learning features in time [3], they do not have inductive biases made from assumptions about the data structure, and they can be trained very fast on GPUs since they can process an entire temporal sequence in parallel. The workhorse of Transformers is the self-attention mechanism, an operation that allows all the elements of a sequence to be compared with each other\u2019s. For Natural Language\nProcessing (NLP), the strength of self-attention is that is allows one token to be compared with present, past, and future tokens [2]. However, depending on the application, conventional self-attention is not always the best choice. It has been shown that using local attention and sliding windows attention can lead to better results for long sequences in NLP [11] and in Machine Vision [25].\nTransformers with Spiking Neural Networks. SNNs, which mimic biological neural networks, are very promising for low power applications because their neurons only transmit information when their membrane potential (an internal state of each neuron) reaches a threshold, and these events happen sparsely in time [6]. Integrating SNNs in a transformer architecture is challenging and not intuitive. Just as RNNs, SNNs have a temporal dynamic. Thus, each element of a sequence must be fed to RNNs or SNNs sequentially. In contrast, since the self-attention mechanism compares all the different elements of a sequence in parallel, Transformers require to wait for the completion of a sequence before computing. For instance, the transformer used in [4] for sEMG classification used time windows of 150 ms. Naively stacking conventional self-attention layers and recurrent layers would then lead to undesirable delays due to the alteration between waiting time windows and processing sequences sequentially.\nYao et al. [26] have used a type of attention mechanism to select the importance of event frames, and then process the events with a SNN. Sabater et al. [27] have shown that a transformer can be used to process event-based vision sensor data more efficiently and accurately than convolutional neural networks. Zhou et al. [28] have used binarized self-attention to integrate sparsity in Transformers. Li et al. [29] have used a SNN as a pre-processing step for a transformer. It was also shown by Gehrig and Scaramuzza [30] that Long-Term Short-Term (LSTM) units can be integrated inside a transformer architecture, but in this work the attention mechanism was spatial and not temporal. Finally, Zhu et al. [31] have integrated spiking neurons inside a transformer architecture, but by using a custom attention mechanism that cannot be computed online.\nIn this paper, we introduce a transformer model that can perform attention in time online, and is compatible with spiking neurons at every layer of the architecture."
        },
        {
            "heading": "3 Methods",
            "text": ""
        },
        {
            "heading": "3.1 NinaproDB8: A Finger Position Regression Dataset",
            "text": "In this work, we used the Non-Invasive Adaptive Hand Prosthetics Database 8 (NinaProDB8) [12], a public sEMG database made as a benchmark for estimation of kinematic finger position. Many deep learning efforts applied to sEMG focus on simple functional movement classification [1, 4, 17, 18]. However, sequence-to-sequence regression of finger position can lead to a wider range of gestures and can be more easily coupled to sensory feedback from robotic hands for a closed-loop precise control [33].\nThe measurements of the database were made on 10 able-bodied subjects and two right trans-radial amputees. The sEMG signal, that is the input of our neural network (see Fig. 1 (a)), are recorded using 16 electrodes (Delsys Trigno IM Wireless EMG system) positioned around the right forearm of the participants. The finger positions were measured using a dataglove, the Cyberglove II, 18-Degrees of Freedom (DoF) model, that measures the finger-joint angles that correspond to the dots in Fig. 1 (b). The sEMG signals and the dataglove signals were up sampled to 2 kHz and post-synchronized. The details of the dataset can be found in [12].\nIn order to disregard the irrelevant degrees of freedom and focus directly on motions relevant for prosthetic hand control, it has been shown by Krasoulis et al. [12] that we can convert the 18-DoF recorded by the dataglove into 5-Degrees of Actuation (DoA) using a simple linear transformation. The matrix used for this linear transformation can be found in the supplementary materials of Krasoulis et al. [12]. We used the DoA as targets of our neural network.\nThree datasets were recorded for each participant: the first two datasets (acquisition 1 and 2) comprised 10 repetitions of each movement and the third dataset (acquisition 3) comprised only 2 repetitions. We used both acquisition 1 and 2 as training set and acquisition 3 as testing set. In Fig. 1 (a) and (c) we show the example of the testing set for subject 1 (target).\nTo facilitate the training of our neural network, we normalize each set of repetition by subtracting the sEMG signals by their mean and dividing by their standard deviation."
        },
        {
            "heading": "3.2 Online Inference with a Custom Attention Mechanism",
            "text": "In conventional transformers [2], the entire self-attention stage is calculated in parallel. The elements of the input sequence of a self-attention layer are called tokens, and the operation of self-attention is described as\nAttention(Q,K, V ) = softmax ( Q \u25e6KT\u221a\nd\n) \u25e6 V (1)\nwhere \u25e6 is the dot-product operator, Q \u2208 RN\u00d7d, K \u2208 RN\u00d7d, and V \u2208 RN\u00d7d are respectively called the queries, the keys, and the values and are three different projections of the same sequence of tokens:\nQ = WQx (2a) K = WKx (2b) V = WV x. (2c)\nThe attention dimension d is the size of each token projection and N is the sequence length. WQ \u2208 Rd\u00d7D, WK \u2208 Rd\u00d7D, and WV \u2208 Rd\u00d7D are learnable weights matrices with with D the embedding dimension, and x \u2208 RN\u00d7D the input of the attention mechanism. In the case of continuous signals (such as bio-medical signals), it is possible to split the input signal into finite time windows, and to wait for the end of each time window before carrying-out the inference (as in Burrello et al. [4]). However, this method induces delays due to waiting for the end of the time windows. Our online transformer uses a custom attention mechanism that can be computed online for each element of the sequence without delays.\nTo avoid waiting for future tokens, the tokens of time step t0 are not compared with future tokens of time steps t > t0. The information from previous tokens is stored in the keys and the values K \u2208 RM\u00d7d and V \u2208 RM\u00d7d. Unlike for self-attention, here the size of K and V does not depend on the full sequence length, but solely on M , which is the number past time steps we choose to store.\nK and V are initially zeroed. Then, the elements Ki \u2208 Rd and Vi \u2208 Rd are iteratively replaced token-wise using the projections of Eqs. 2b and 2c. At each time step, a single query Qt \u2208 Rd is also computed with Eq. 2a, and the attention is computed as\nAttentiont = softmax\n( Qt \u25e6K\u221a\nd\n) \u25e6 V (3)\nThe softmax is computed on the memory length dimension M . Since one different element of K and V is updated at each time step, and since the length of K and V is M , all their elements are updated with a frequency 1M . The above-mentioned procedure is summarized as the Algorithm 1.\nIn contrast to the quadratic dependence with respect to sequence length for conventional selfattention (O ( N2 ) ), the computational complexity of the sliding window attention mechanism is\nlinear (O (MN)).\nEq. 3 shows that our attention mechanism can be computed time step wise instead of waiting for the end of large time windows to compute the attention in parallel. Now, we will show how this attention mechanism fits in our full neural network."
        },
        {
            "heading": "3.3 Neural Network Architecture",
            "text": "Our neural network consists of three blocks as depicted in Fig. 2 (a): an embedding block that converts the raw EMG signal into a sequence of tokens, an encoder block that uses attention to find correlation between sequence elements, and a regression block that converts the output into five degrees of actuation.\nAlgorithm 1 Inference with Online Attention K = 0 . K \u2208 RM\u00d7d V = 0 . V \u2208 RM\u00d7d t = 0 i = 0 repeat\nQt \u2190WQxt . Qt \u2208 Rd Ki \u2190WKxt . Ki \u2208 Rd Vi \u2190WV xt . Vi \u2208 Rd\nAttentiont \u2190 softmax (\nQt\u25e6K\u221a d\n) \u25e6 V\nt\u2190 t+ 1 . New time step i\u2190 i+ 1 i\u2190 i (modM)\nuntil end of sequence Figure 3: Sliding window attention. Thesame K values are represented by the same colors.\nThe embedding is made of a temporal convolution layer. Convolutional layers have overlaps between input time windows, which means that unlike linear layers, they have an intrinsic order, and thus do not require positional embeddings [2]. The convolutional layer has C = 16 input channels matching the 16 electrodes, and D = 64 output channels. The network is tested with kernels of various sizes to vary the length of the input time window that matches one token. We chose to make an overlap of two time steps between time windows, which makes the convolutional layer stride be s = k \u2212 2, where k is the kernel size. With a padding p = 1, the number of tokens generated thus depends on the stride as\nN = bNsamples s c (4)\nwhere Nsamples is the number of processed samples of the input signal.\nThe encoder is described as\nf (x) = x+ MHA (LN (x)) z (f (x)) = f (x) + FNN (LN (f (x)))\n(5)\nwhere LN is a layer norm layer. MHA is the multi-head attention layer with h = 8 heads computed in parallel using Eq. 3, with an attention dimension d = 32. After that attention is computed, the h heads are concatenated and projected into dimension D = 64. FNN is a Feedforward Neural Network with one hidden layer of 128 GeLU units [34] and a dropout layer with probability 0.2. The linear projections of FNN are applied token-wise so that that they can be computed online. The entire encoder block can be repeated and stacked L times, but here we chose to keep L = 1. The backbone of our neural network is inspired from Burrello et al. [4].\nFinally, the regression block consists of a linear layer that projects each token from a dimension D = 64 to a dimension 5 (the number of degrees of actuation we perform the regression on), and an up sampling layer that duplicates the output of each token to generate as many samples as there are in the target signal (an example of target signal is shown in Fig. 1 (c)). The up sampling factor is equal to the stride that we use in the convolutional embedding layer (see Eq. 4)."
        },
        {
            "heading": "3.4 Increasing the network sparsity with binarization and spiking neurons",
            "text": "In order to reduce the number of required operations, we increase the network sparsity by using binarization and Leaky Integrate and Fire (LIF) units [5, 6] units. We test two sparse models. In the first one, we binarize the output of the convolutional embedding, we binarize the projections Q, K, and V , and we replace the FNN by a SNN with a first layer of 128 LIF units, and a second of D = 64 LIF units. The second sparse model is similar to the first one, but instead of binarizing Q, K, and V , we replace each projection of Eqs. 2a, 2b, and 2c by a single spiking layer of d = 32 LIF units, which adds an additional dynamic to the model.\nBinarization is done by applying a Heaviside function. The dynamics of the LIF units are defined by\nUt = \u03b1 (1\u2212 St\u22121)Ut\u22121 + (1\u2212 \u03b1) It\u22121 (6a) It = \u03b2It\u22121 + (1\u2212 \u03b2)Wxt (6b) St = H (Ut\u22121 \u2212\u0398) (6c)\nwhere t is the index of the tokens, U is the membrane potential, I is the synaptic current, S is the spike response, H is the Heaviside function, \u03b1 = 0.95, \u03b2 = 0.9, and \u0398 = 1. The outputs of the Q, K, and V projections are the spike responses St (see Eq. 6c). The outputs of the first layer of the SNN replacing the FNN are the spike responses St, and the outputs of the second layer are the membrane potentials Ut (see Eq. 6a).\nBecause the Heaviside function is not differentiable, during training the gradient of the different Heaviside functions (used for binarization and LIF units) are replaced by the SuperSpike surrogate gradient [5, 35]. To preserve the sparsity between the embedding and the encoder block, we remove the layer norm layer that precedes the embedding when the embedding is binarized. In addition, we remove the dropout layers in the two sparse models. The softmax of the attention mechanism is only computed on non-zero elements."
        },
        {
            "heading": "3.5 Training",
            "text": "To speed up training, the attention block is computed in parallel. Projections Q \u2208 RN\u00d7d, K \u2208 RN\u00d7d, and V \u2208 RN\u00d7d are computed for an entire time window with N tokens. The keys and values are then unfolded into sliding windows of size M and stride 1, similarly as for a convolution (see in Fig. 3 an example of sliding window attention). The product between queries and keys is thus computed as Q0K1\u2212M \u00b7 \u00b7 \u00b7 Q0K\u22122 Q0K\u22121 Q0K0 Q1K2\u2212M \u00b7 \u00b7 \u00b7 Q1K\u22121 Q1K0 Q1K1 Q2K3\u2212M \u00b7 \u00b7 \u00b7 Q2K0 Q2K1 Q2K2\n... . . . ...\n... ...\nQNKN\u2212M \u00b7 \u00b7 \u00b7 QN\u22121KN\u22123 QN\u22121KN\u22122 QN\u22121KN\u22121  . (7) Since the keys Ki<0 are forbidden values, we mask them by replacing them with \u2212\u221e as in Vaswani et al. [2], so that they are not computed in the softmax (see Eq. 3). The values Vi<0 are simply zeroed.\nTo improve training, we developed a simple data augmentation protocol: first, the training set signals are sliced into time windows of Nsamples = 2000 samples (which corresponds to 1 s since the sampling rate is 2 kHz). Then, each time window is duplicated 64 times. For data augmentation, the beginning of each of this duplicated time window is shifted with a random number sampled in a uniform distribution between 0 and 2000. Finally, the resulting time windows are shuffled to create the training dataset.\nWe trained each network for each subject for 10 epochs using the Adam optimizer [36], a learning rate of 10\u22123, batch sizes of 64, and since the metric we want to minimize is the mean average error (MAE) over the 5 degrees of freedom (DoA), we used the L1 loss function.\nFor the sparse models, we added a sparsity loss function term [37] to the global loss to increase the sparsity of the embedding, the queries, keys and values such that the total loss is:\nL (y, y\u0302) =\u2016 yi,j \u2212 y\u0302i,j \u20161 \u2212 1\n2 \u03bb (\u2016 x \u20162 + \u2016 Concat (Q,K, V ) \u20162) (8)\nwith y the network outputs, y\u0302 the targets, x the embeddings and \u03bb = 1.\nIn this study, we simply trained and tested datasets independently for each subject. To improve accuracy and repeatability in future studies, it is also possible to use transfer learning: the network can learn from multiple subjects before fine-tuning and testing on a new subject, as in [38]."
        },
        {
            "heading": "4 Results",
            "text": "In Fig. 1 (c) we show an example of the regression results for a sparse online transformer with a embedding convolution kernel size k = 7 and a memory length M = 150. We first investigate how k\nand M affect the final accuracy. For this study, we use the network without sparsity. Since s = k\u2212 2, we simultaneously change s and k, and thus the number of tokens N generated for a given time window (see Eq. 4). The memory length M defines how many past tokens are used in the attention mechanism. The time length of the signal used to store information in K and V is thus:\n\u03c4memory = M \u00d7 s\nSamplingRate . (9)\nIn Fig. 4 we plot the mean absolute error (MAE) over the different degrees of actuation for values of M swept between 10 and 150 with intervals of 20, and for five different values k = 7, 15, 20, 25, and 30 (which correspond to s = 5, 13, 18, 23 and 28). While sweeping M , we see for k = 15, 20, 25, and 30 that the MAE reaches a minimum and then increases. It shows that there is then an optimum value of memory length for each kernel size, and we see that this optimum value decreases with the kernel size and thus with the stride. Using Eq. 9, this result indicates that there is an optimum length of information \u03c4memory used in the attention mechanism, and that past that point increasing the stored information does not increase the accuracy. Then, we compare our different models using each time a kernel size k = 7 and a memory length is M = 150. As we see in Fig 4, these parameters lead to the best accuracy using the shortest time window for each token. For this study we also measure the 10\u00b0-accuracy and the 15\u00b0-accuracy, which are respectively the proportion of time samples that lead to mean average errors inferior to ten and fifteen degrees [21]. These additional metrics are important to measure the accuracy of the prediction within a margin of error. The different results are shown in\nTable 4. The mean and standard deviation of each metric are computed over the 12 subjects of the NinaproDB8 dataset.\nTo see the impact on our online transformer with custom sliding window attention mechanism, we compare it to a conventional Transformer with self-attention. For the three metrics, our online transformers outperform the transformer with conventional self-attention (see Table 4). This results further reveals the importance of selecting relevant information, and that for sEMG signal processing, it is likely more important to use local information from the past than global information from both past and future.\nOur two sparse models reach similar accuracy than our non-sparse online transformer (and thus also better accuracy than equivalent conventional transformer), and respectively lead to a reduction of the number of required Multiply-And-Accumulate operations (MAC ops) by factors of 3.8\u00d7 and 5.3\u00d7 compared to the non-sparse online transformer (the activation function operations are not included in these calculations). The method used to compute the number of required operations is described in the Appendix.\nMoreover, we see that our three online transformer models outperform LSTMs [21] by at least 0.88\u00b0 of MAE, outperform Temporal Convolutions [19] with at least 0.76\u00b0 of MAE (previous SoTA on NinaproDB8 dataset). To compare the inference speed of the different methods, we define the minimum time of computing as the length of the time windows used for each inference step, which for our online transformer is \u03c4min = kSamplingRate , with k the embedding convolution kernel size. Since k = 7 and the sampling rate is 2 kHz, our network can compute with a minimum latency of\n3.5 ms, which is shorter than any previous methods and in particular more than 30\u00d7 shorter than the Temporal Convolutional network [19] which was the previous SoTA for the Ninapro DB8 dataset."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this work, we developed an online transformer model that leverages sliding window attention to process tokens one at the time. We have shown that the locality of the sliding window makes it more efficient than self-attention. The proposed method makes sEMG signal processing with very short time windows (3.5 ms) possible, and sets the new state-of-the-art on the prosthetic hand control NinaproDB8 dataset. Using sliding window attention, our model also solves the problem of the integration of SNNs temporal dynamics in Transformers. We used a combination of binarization and SNNs to increase the network sparsity, thus reducing the number of required operation up to a factor 5.3\u00d7. In conclusion, this work is a step toward precise, smooth, and low-power Human-Machine Interfacing, and holds great promises for future neuromorphic transformer models."
        },
        {
            "heading": "A Appendix",
            "text": "To compute the number of synaptic MAC operations of our different models, we used the following set of equations, that represent the different operations (activation function excluded) of our models:\n#EmbeddingMACs = k \u00d7 C \u00d7D \u00d7 32 (10a) #QKVProjectionMACs = (1\u2212 EmbeddingSparsity)\u00d7 3\u00d7D \u00d7 d\u00d7 h\u00d7 32 (10b)\n#QKProductMACs =\u2016 QK \u20161 \u00d7M \u00d7 h\u00d7 32 (10c) #VProductMACs = (1\u2212VSparsity)\u00d7 d\u00d7M \u00d7 h\u00d7 32 (10d)\n#ConcatMACs = (1\u2212AttentionSparsity)\u00d7 d\u00d7 h\u00d7D \u00d7 32 (10e) #TotalAttentionMACs = #QKVProjectionMac + #QKProductMacs (10f)\n+ #VProductMacs + #ConcatMACs\n#FFL1MACs = D \u00d7H \u00d7 32 (10g) #FFL2MACs = (1\u2212 FFL1Sparsity)\u00d7H \u00d7D \u00d7 32 (10h)\n#RegressionMACs = D \u00d7 5\u00d7 32 (10i) #TotalMACs = #EmbeddingMacs + #TotalAttentionMACs (10j)\n+ #FFL1MACs + #FFL2MACs + #RegressionMACs"
        }
    ],
    "title": "Online Transformers with Spiking Neurons for Fast Prosthetic Hand Control",
    "year": 2023
}