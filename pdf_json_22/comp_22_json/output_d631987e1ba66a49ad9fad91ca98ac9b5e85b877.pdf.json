{
    "abstractText": "Unsupervised Source (data) Free domain adaptation (USFDA) aims to transfer knowledge from a well-trained source model to a related but unlabeled target domain. In such a scenario, all conventional adaptation methods that require source data fail. To combat this challenge, existing USFDAs turn to transfer knowledge by aligning the target feature to the latent distribution hidden in the source model. However, such information is naturally limited. Thus, the alignment in such a scenario is not only difficult but also insufficient, which degrades the target generalization performance. To relieve this dilemma in current USFDAs, we are motivated to explore a new perspective to boost their performance. For this purpose and gaining necessary insight, we look back upon the origin of the domain adaptation and first theoretically derive a new-brand target generalization error bound based on the model smoothness. Then, following the theoretical insight, a general and model-smoothness-guided Jacobian norm (JN) regularizer is designed and imposed on the target domain to mitigate this dilemma. Extensive experiments are conducted to validate its effectiveness. In its implementation, just with a few lines of codes added to the existing USFDAs, we achieve superior results on various benchmark datasets.",
    "authors": [
        {
            "affiliations": [],
            "name": "Weikai Li"
        },
        {
            "affiliations": [],
            "name": "Meng Cao"
        },
        {
            "affiliations": [],
            "name": "Songcan Chen"
        }
    ],
    "id": "SP:8c0ae743e4090a4fd295da79873473ecdd2b22db",
    "references": [
        {
            "authors": [
                "Y. Yao",
                "Y. Zhang",
                "X. Li",
                "Y. Ye"
            ],
            "title": "Heterogeneous domain adaptation via soft transfer network",
            "venue": "Proceedings of the 27th ACM international conference on multimedia, pp. 1578\u20131586, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "W.M. Kouw",
                "M. Loog"
            ],
            "title": "A review of domain adaptation without target labels",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "W. Li",
                "S. Chen"
            ],
            "title": "Unsupervised domain adaptation with progressive adaptation of subspaces",
            "venue": "arXiv preprint arXiv:2009.00520, 2020.",
            "year": 2009
        },
        {
            "authors": [
                "S. Li",
                "C.H. Liu",
                "B. Xie",
                "L. Su",
                "Z. Ding",
                "G. Huang"
            ],
            "title": "Joint adversarial domain adaptation",
            "venue": "Proceedings of the 27th ACM International Conference on Multimedia, pp. 729\u2013737, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Wang",
                "Y. Luo",
                "Z. Huang",
                "M. Baktashmotlagh"
            ],
            "title": "Prototypematching graph network for heterogeneous domain adaptation",
            "venue": "Proceedings of the 28th ACM International Conference on Multimedia, pp. 2104\u20132112, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Wang",
                "W. Wang",
                "B. Li",
                "X. Zhang",
                "L. Lan",
                "H. Tan",
                "T. Liang",
                "W. Yu",
                "Z. Luo"
            ],
            "title": "Interbn: Channel fusion for adversarial unsupervised domain adaptation",
            "venue": "Proceedings of the 29th ACM International Conference on Multimedia, pp. 3691\u20133700, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Long",
                "Z. Cao",
                "J. Wang",
                "M.I. Jordan"
            ],
            "title": "Conditional adversarial domain adaptation",
            "venue": "Advances in Neural Information Processing Systems, pp. 1640\u20131650, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "T. Jing",
                "H. Xia",
                "Z. Ding"
            ],
            "title": "Adaptively-accumulated knowledge transfer for partial domain adaptation",
            "venue": "Proceedings of the 28th ACM International Conference on Multimedia, pp. 1606\u20131614, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Cheng",
                "F. Wei",
                "J. Bao",
                "D. Chen",
                "F. Wen",
                "W. Zhang"
            ],
            "title": "Dual path learning for domain adaptation of semantic segmentation",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9082\u20139091, 2021. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8",
            "year": 2021
        },
        {
            "authors": [
                "H. Guo",
                "R. Pasunuru",
                "M. Bansal"
            ],
            "title": "Multi-source domain adaptation for text classification via distancenet-bandits",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, pp. 7830\u20137838, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Jiang",
                "Z. Ding",
                "Y. Fu"
            ],
            "title": "Deep low-rank sparse collective factorization for cross-domain recommendation",
            "venue": "Proceedings of the 25th ACM international conference on Multimedia, pp. 163\u2013171, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Luo",
                "Z. Huang",
                "Z. Wang",
                "Z. Zhang",
                "M. Baktashmotlagh"
            ],
            "title": "Adversarial bipartite graph learning for video domain adaptation",
            "venue": "Proceedings of the 28th ACM International Conference on Multimedia, pp. 19\u201327, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Ben-David",
                "J. Blitzer",
                "K. Crammer",
                "F. Pereira"
            ],
            "title": "Analysis of representations for domain adaptation",
            "venue": "Advances in neural information processing systems, vol. 19, p. 137, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "S. Ben-David",
                "J. Blitzer",
                "K. Crammer",
                "A. Kulesza",
                "F. Pereira",
                "J.W. Vaughan"
            ],
            "title": "A theory of learning from different domains",
            "venue": "Machine learning, vol. 79, no. 1-2, pp. 151\u2013175, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "Y. Tsuboi",
                "H. Kashima",
                "S. Hido",
                "S. Bickel",
                "M. Sugiyama"
            ],
            "title": "Direct density ratio estimation for large-scale covariate shift adaptation",
            "venue": "Journal of Information Processing, vol. 17, pp. 138\u2013155, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "M. Sugiyama",
                "T. Suzuki",
                "S. Nakajima",
                "H. Kashima",
                "P. von B\u00fcnau",
                "M. Kawanabe"
            ],
            "title": "Direct importance estimation for covariate shift adaptation",
            "venue": "Annals of the Institute of Statistical Mathematics, vol. 60, no. 4, pp. 699\u2013746, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "J. Huang",
                "A. Gretton",
                "K. Borgwardt",
                "B. Sch\u00f6lkopf",
                "A.J. Smola"
            ],
            "title": "Correcting sample selection bias by unlabeled data",
            "venue": "Advances in neural information processing systems, pp. 601\u2013608, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "S.J. Pan",
                "I.W. Tsang",
                "J.T. Kwok",
                "Q. Yang"
            ],
            "title": "Domain adaptation via transfer component analysis",
            "venue": "IEEE Transactions on Neural Networks, vol. 22, no. 2, pp. 199\u2013210, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "Y. Ganin",
                "V. Lempitsky"
            ],
            "title": "Unsupervised domain adaptation by backpropagation",
            "venue": "International conference on machine learning, pp. 1180\u20131189, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "W. Deng",
                "Y. Cui",
                "Z. Liu",
                "G. Kuang",
                "D. Hu",
                "M. Pietik\u00e4inen",
                "L. Liu"
            ],
            "title": "Informative class-conditioned feature alignment for unsupervised domain adaptation",
            "venue": "Proceedings of the 29th ACM International Conference on Multimedia, pp. 1303\u20131312, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Sankaranarayanan",
                "Y. Balaji",
                "C.D. Castillo",
                "R. Chellappa"
            ],
            "title": "Generate to adapt: Aligning domains using generative adversarial networks",
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J. Liang",
                "D. Hu",
                "J. Feng"
            ],
            "title": "Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation",
            "venue": "International Conference on Machine Learning, pp. 6028\u20136039, PMLR, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "R. Li",
                "Q. Jiao",
                "W. Cao",
                "H.-S. Wong",
                "S. Wu"
            ],
            "title": "Model adaptation: Unsupervised domain adaptation without source data",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9641\u20139650, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Ye",
                "J. Zhang",
                "J. Ouyang",
                "D. Yuan"
            ],
            "title": "Source data-free unsupervised domain adaptation for semantic segmentation",
            "venue": "Proceedings of the 29th ACM International Conference on Multimedia, pp. 2233\u20132242, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R. Siry",
                "L. H\u00e9madou",
                "L. Simon",
                "F. Jurie"
            ],
            "title": "On the inductive biases of deep domain adaptation",
            "venue": "CoRR, vol. abs/2109.07920, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C. Villani"
            ],
            "title": "Optimal transport: old and new",
            "year": 2009
        },
        {
            "authors": [
                "B. Fernando",
                "A. Habrard",
                "M. Sebban",
                "T. Tuytelaars"
            ],
            "title": "Unsupervised visual domain adaptation using subspace alignment",
            "venue": "Proceedings of the IEEE international conference on computer vision, pp. 2960\u20132967, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "Y. Chen",
                "Y. Pan",
                "Y. Wang",
                "T. Yao",
                "X. Tian",
                "T. Mei"
            ],
            "title": "Transferrable contrastive learning for visual domain adaptation",
            "venue": "Proceedings of the 29th ACM International Conference on Multimedia, pp. 3399\u20133408, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "W. Zellinger",
                "T. Grubinger",
                "E. Lughofer",
                "T. Natschl\u00e4ger",
                "S. Saminger-Platz"
            ],
            "title": "Central moment discrepancy (cmd) for domaininvariant representation learning",
            "venue": "arXiv preprint arXiv:1702.08811, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "M. Long",
                "H. Zhu",
                "J. Wang",
                "M.I. Jordan"
            ],
            "title": "Deep transfer learning with joint adaptation networks",
            "venue": "International conference on machine learning, pp. 2208\u20132217, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "K. Saito",
                "K. Watanabe",
                "Y. Ushiku",
                "T. Harada"
            ],
            "title": "Maximum classifier discrepancy for unsupervised domain adaptation",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3723\u20133732, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "N. Courty",
                "R. Flamary",
                "D. Tuia"
            ],
            "title": "Domain adaptation with regularized optimal transport",
            "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 274\u2013289, Springer, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "K. Liang",
                "J.Y. Zhang",
                "O. Koyejo",
                "B. Li"
            ],
            "title": "Does adversarial transferability indicate knowledge transferability",
            "venue": "arXiv preprint arXiv:2006.14512, 2020.",
            "year": 2006
        },
        {
            "authors": [
                "S. Yanga",
                "Y. Wanga",
                "J. van de Weijera",
                "L. Herranza",
                "S. Juic"
            ],
            "title": "Casting a bait for offline and online source-free domain adaptation",
            "venue": "arXiv preprint arXiv:2010.12427, 2021.",
            "year": 2010
        },
        {
            "authors": [
                "M. Ishii",
                "M. Sugiyama"
            ],
            "title": "Source-free domain adaptation via distributional alignment by matching batch normalization statistics",
            "venue": "arXiv preprint arXiv:2101.10842, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Yi",
                "L. Hou",
                "J. Sun",
                "L. Shang",
                "X. Jiang",
                "Q. Liu",
                "Z.-M. Ma"
            ],
            "title": "Improved ood generalization via adversarial training and pre-training",
            "venue": "arXiv preprint arXiv:2105.11144, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C. Shui",
                "Q. Chen",
                "J. Wen",
                "F. Zhou",
                "C. Gagn\u00e9",
                "B. Wang"
            ],
            "title": "Beyond hdivergence: Domain adaptation theory with jensen-shannon divergence",
            "venue": "arXiv preprint arXiv:2007.15567, 2020.",
            "year": 2007
        },
        {
            "authors": [
                "Z. Pan",
                "W. Yu",
                "B. Wang",
                "H. Xie",
                "V.S. Sheng",
                "J. Lei",
                "S. Kwong"
            ],
            "title": "Loss functions of generative adversarial networks (gans): opportunities and challenges",
            "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence, vol. 4, no. 4, pp. 500\u2013522, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "F. Nielsen",
                "K. Sun"
            ],
            "title": "Guaranteed deterministic bounds on the total variation distance between univariate mixtures",
            "venue": "2018 IEEE 28th International Workshop on Machine Learning for Signal Processing (MLSP), pp. 1\u20136, IEEE, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "R. M\u00fcller",
                "S. Kornblith",
                "G. Hinton"
            ],
            "title": "When does label smoothing help",
            "venue": "Advances in neural information processing systems, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M. Caron",
                "P. Bojanowski",
                "A. Joulin",
                "M. Douze"
            ],
            "title": "Deep clustering for unsupervised learning of visual features",
            "venue": "Proceedings of the European conference on computer vision (ECCV), pp. 132\u2013149, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "K. Saenko",
                "B. Kulis",
                "M. Fritz",
                "T. Darrell"
            ],
            "title": "Adapting visual category models to new domains",
            "venue": "European conference on computer vision, pp. 213\u2013226, Springer, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "H. Venkateswara",
                "J. Eusebio",
                "S. Chakraborty",
                "S. Panchanathan"
            ],
            "title": "Deep hashing network for unsupervised domain adaptation",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5018\u20135027, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "X. Peng",
                "Z. Huang",
                "X. Sun",
                "K. Saenko"
            ],
            "title": "Domain agnostic learning with disentangled representations",
            "venue": "International Conference on Machine Learning, pp. 5102\u20135112, PMLR, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M. Long",
                "Y. Cao",
                "J. Wang",
                "M. Jordan"
            ],
            "title": "Learning transferable features with deep adaptation networks",
            "venue": "International conference on machine learning, pp. 97\u2013105, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "X. Chen",
                "S. Wang",
                "M. Long",
                "J. Wang"
            ],
            "title": "Transferability vs. discriminability: Batch spectral penalization for adversarial domain adaptation",
            "venue": "International conference on machine learning, pp. 1081\u20131090, PMLR, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "C. Chen",
                "W. Xie",
                "W. Huang",
                "Y. Rong",
                "X. Ding",
                "Y. Huang",
                "T. Xu",
                "J. Huang"
            ],
            "title": "Progressive feature alignment for unsupervised domain adaptation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 627\u2013636, 2019.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Jacobian Norm, Unsupervised Domain Adaptation, Source-Free\nI. INTRODUCTION\nDEEP neural networks have achieved remarkable successin various multi-media applications, where sufficiently large-scale and well-labeled data are present. However, manually labeling sufficient data is often time-consuming and laborexhaustive, thus hard to meet the demand of rapid growth of the multi-media steaming or the content sharing applications [1]. To address it, unsupervised domain adaptation (UDA) is becoming an increasingly attractive research topic in the multimedia community [2]\u2013[6]. Specifically, by leveraging the discriminative knowledge from readily available and labeled source domains, UDA aims to establish a desired prediction model for the unlabeled target domain, which significantly relieves the burden of annotating magnanimous data. In the past few years, UDA has achieved several impressive results on various multi-media applications such as image recognition [7], [8], semantic segmentation [9], text classification [10], recommendation [11] and action recognition [12].\nThe authors are with College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics of (NUAA), Nanjing, 211106, China. E-mail: {leeweikai; alarsh; s.chen}@nuaa.edu.cn.\nCorresponding author is Songcan Chen. This paper was produced by the IEEE Publication Technology Group. They\nare in Piscataway, NJ. Manuscript received April 19, 2021; revised August 16, 2021.\nCurrently, a series of theoretical results have been presented to guide the solution for addressing UDA [13], [14], which illustrates that the target risk is bounded by the between-domain discrepancy. Motivated by these theoretical results, domain alignment comes to the dominant strategy for solving domain adaptation [2], whose goal is to alleviate the between-domain discrepancy, so that the learned source model can be naturally adapted to the target domain. Specifically, these methods can be categorized into the sample alignment or the feature alignment [2]. In particular, the sample alignment focuses on mitigating the domain shifts by the importance-weighting of samples [15]\u2013[17]. In contrast, the feature alignment attempts to learn a domain invariant feature or representation to alleviate the domain discrepancies by kernel matching [18], adversarial learning [19], prototype matching [5], optimal transport [20] and image reconstruction [21].\nIn practice, almost all existing UDA approaches require to access both raw source and target data while learning to adapt. Unfortunately, due to the cost of storage or the protection of private information, in reality, the source data is often not at hand. In light of this, a more realistic scenario, called Unsupervised Source-Free Domain Adaptation (USFDA), is considered, in which only source model can be accessed at adaptation [22]\u2013[24]. In such a scenario with no source data, the explicit sample alignment approaches completely fail. Instead, the existing USFDA approaches attempt to transfer knowledge by implicitly aligning the target features with the source by pseudo labeling [22] or image generation [23], which are derived from the source model. However, focusing on implicit alignment alone is often insufficient to address USFDA, since obtaining a desired alignment is usually difficult in the absence of source data, while such an implicit alignment cannot guarantee the success of the adaptation [25]. To relieve such dilemma, we are motivated to look back upon the origin of USFDA: how to guarantee the generalization performance of the learned model on target domain, without accessing to the original source samples? For this purpose, we aim to explore a new insight to relieve this dilemma in the existing USFDAs. Specifically, a novel target generalization error bound is derived, which incorporates a betweendomain discrepancy term and a new model-smoothness term. In contrast to the existing theoretical works, such a new term is the first time to be considered in adaptation. This term indicates that the model output should be smooth/consistent in the neighborhood of target sample, which instructs us to utilize such neighbor information for guiding the knowledge transfer. It should be noted that the existing USFDAs purely focus on mitigating the between-domain discrepancy, while0000\u20130000/00$00.00 \u00a9 2021 IEEE ar X iv :2 20 4. 03 46 7v 1\n[ cs\n.L G\n] 7\nA pr\n2 02\n2\nall of them neglect the neighbor information (i.e., model smoothness). Thus, motivated by the theoretical perspective, we focus on optimizing this term on the target domain to boost the adaptation ability of the existing USFDAs.\nDriven by this, a quite simple and model-smoothnessderived Jacobian Norm (JN) regularizer is designed as a plug-in unit to mitigate the dilemma and further boost the performance in the existing USFDAs and used to implicitly force the smoothness/consistency of the model output in the neighborhood of target sample. Consequently, it takes advantage of both the target data and the source model, as shown in Figure 1. Due to the convenience of the pseudo-based strategy, we adopt the prevailing pseudo-labelling approach as a baseline to potentially reduce the between-domain discrepancy. In its implementation, we freeze the pre-trained classifier and fine-tune the feature encoding module by minimizing the JN regularized objective to boost its performance. In the end, we conduct abundant experiments on several domain adaptation datasets with different sample sizes. The experimental results demonstrate significant superiority of our model in USFDA. The contributions of this paper are as follows:\n\u2022 We theoretically provide a new-brand target generalization bound based on the Total Variation distance [26] and model smoothness, which provides a novel insight for solving USFDA. \u2022 We develop a simple yet general JN regularizer to boost the performance of USFDA, which can be easily incorporated into any existing USFDA methods as a plug-in unit with a few lines of code increased. \u2022 We empirically find that JN regularizer can significantly improve the performance of the existing USFDA method, which achieves competitive results on multiple datasets.\nThe remainder of this paper is organized as follows. In Section II, we briefly overview unsupervised domain adaptation and unsupervised source-free domain adaptation. In Section III, we present the problem definition and derive a new-brand target generalization error bound. In Section IV, We develop the JN regularizer and the entire USFDA model to address USFDA. The experimental results and the post-hoc analysis are reported in Section V. In the end, we conclude the entire paper with future research directions in Section VI."
        },
        {
            "heading": "II. RELATED WORKS",
            "text": "In this section, we present the most related researches on UDA/USFDA and highlight the differences between these methods and ours."
        },
        {
            "heading": "A. Unsupervised Domain Adaptation",
            "text": "Recent practices on UDA usually attempt to minimize the domain discrepancy for knowledge transfer. Following this, multiple domain adaptation techniques have been developed, which can be summarized into the sample alignment [15]\u2013[17] and feature alignment [18], [19], [27]. In particular, the sample alignment methods focus on mitigating the between-domain divergence such as the A-distance [17], Maximum Mean Discrepancy (MMD) [16], or KL-divergence [15] through reweighting the individual samples. In contrast, the feature alignment methods generate the domain-invariant feature through kernel matching [18], adversarial learning [19], transferrable contrastive learning [28], prototype matching [5], optimal transport [20] and image reconstruction [21], to reduce the distribution differences across domains, such as MMD [18], central moment discrepancy [29], Joint MMD [30], A-distance [19] and maximum classifier discrepancy [31], Wasserstein distance [32], etc.\nCompared with the UDA methods which require access to both source and target data, our work does NOT require the source data while learning to adapt. This is more suitable in real-world applications."
        },
        {
            "heading": "B. Unsupervised Source-Free Domain Adaptation",
            "text": "Different from UDA, USFDA is a more practical scenario in which the source data is inaccessible at adaptation. Existing methods seek to implicitly align the target domain feature to the source domain by heuristically leveraging the information from the source model. The conventional USFDA methods include pseudo labelling (e.g., SHOT [33] and BAIT [34]), batch normalization (e.g., BN [35]) or data generation (e.g., MA [23]). Specifically, the pseudo labelling methods implicitly align representations from the target domains to the source model, the BN method minimizes the discrepancy between domains by BN statistics stored in the source model, and the image generation methods align the target domain to the annotated data generated by source model.\nThe existing USFDAs mainly focus on mitigating the between-domain divergence, which often neglect the neighbor information. Instead, the proposed JN regularization term focuses on optimizing the model smoothness to leverage the neighbor information from the target domain itself, which can be used as a plug-in unit to effectively boost performance of the existing USFDAs."
        },
        {
            "heading": "III. MODEL SMOOTHNESS FOR TARGET GENERALIZATION ERROR BOUND",
            "text": "The current theoretical works on domain adaptation are typically established on domain discrepancy, which encourages domain alignment in solving domain adaptation [13], [14]. However, in USFDA, due to the absence of source data, such an alignment is often difficult and insufficient. To boost the adaptation ability of USFDA, we now look back upon the origin of domain adaptation and attempt to induce a novel insights for addressing USFDA."
        },
        {
            "heading": "A. Preliminaries",
            "text": "In this paper, we focus on the USFDA task. We use X \u2282 Rd and Y \u2282 R to denote the feature and the label space, respectively. We assume that the feature space X of source and target domain has a compact support. Thus, there exists a constant D > 0, such that \u2200u,v \u2208 X , \u2016u\u2212 v\u2016 < D. In particular, we are given n labeled samples {xsi , ysi }ni=1 from the source domain Ds with the distribution P, where xsi \u2208 X and ysi \u2208 Y . We also have m unlabeled samples {xti}mi=1 from Dt with the distribution Q, where xti \u2208 X . In the USFDA setting, P 6= Q and the source data can be only accessed at the source model training procedure. In particular, we consider the K-way classification task. The goal of the USFDA is to learn a target function f : X \u2192 Y and predict the target label {yti}mi=1, where yti \u2208 Y , with only target data {xti}mi=1 and the source function fs : X \u2192 Y available in adaptation.\nIn addition, let L(f(x), y) be the continuous and differentiable loss function. Inspired by a current theoretical study [36], we assume that 0 \u2264 L(f(x), y) \u2264 M for constant M without loss of generality. Moreover, we denote the EP (f) = E{x,y}\u223cPL(f(x), y) as the expected risk of model f over distribution P. In order to find an alternative to mitigate the dilemma in USFDA. we also need the following definitions of Total Variation distance [26] and model smoothness:\nDefinition 1: Total Variation distance [26]: Given two distributions P and Q. The Total Variation distance TV(P,Q) between distributions P and Q is defined as:\nTV(P,Q) = 1\n2 \u222b X |dP(x)\u2212 dQ(x)|. (1)\nDefinition 2: Model Smoothness : A model f with parameter w is r-cover with -smoothness on distribution P, if\nEP [ sup \u2016\u03b4\u2016\u221e\u2264r |f(x+ \u03b4)\u2212 f(x)| ] \u2264 . (2)\nwhere \u2016 \u00b7 \u2016\u221e is the l\u221e-norm. Remark: Since the source data is unavailable, the current theoretical works based on H-divergence are often limited in the USFDA setting. Further, such a divergence is not consistent\nwith the current domain alignment work [37] and let alone the existing USFDA works. In contrast, TV distance is more probabilistically interpretable and consistent with the objective of the existing USFDAs. In particular, pseudo-labeling attempts to minimize the KL-divergence between the one-hot encoding and the model output of the target domain [22], BN method also minimizes the KL-divergence between domains by utilizing batch normalization (BN) statistics stored in the source model [35]. Moreover, the image generation methods align the target domain to the annotated data by GAN, which is proven to be an approximation of TV distance [38]. Note that TV distance can be upper bounded in terms of KL-divergence as TV (P,Q) \u2264 \u221a KL(P,Q)/2 log e [39]. Thus, we conduct the TV distance to analyze the new target generalization error bound."
        },
        {
            "heading": "B. Generalization Error Bound Via Model Smoothness",
            "text": "With the Total Variation distance in Definition 1 and model smoothness in Definition 2, we present our new-brand generalization error bound on target domain in Theorem 1.\nTheorem 1: Given two distributions P and Q, if a model f is 2r-cover with smoothness over distributions P and Q, with probability at least 1\u2212 \u03b8, we have: EQ (f) \u2264 EP (f) + 2 + 2MTV(P,Q)\n+M\n\u221a (2d) 2 2D r2 +1 log 2 + 2 log ( 1 \u03b8 ) m\n+M\n\u221a (2d) 2 2D r2 +1 log 2 + 2 log ( 1 \u03b8 ) n\n+M\n\u221a log(1/\u03b8)\n2m .\n(3)\nThe proof of Theorem 1 is given in the supplementary file. In contrast to the existing theoretical works, we utilize the TV-divergence TV(P,Q) to measure the domain discrepancy, since TV distance is more probabilistically interpretable and consistent with the objective of the existing USFDAs as mentioned. In particular, the model smoothness is firstly considered in adaptation, which provides a new perspective to address USFDA. To relieve the dilemma and boost the performance of the USFDA, we intuitively focus more on optimizing the new term to leverage the neighbor knowledge from the target domain."
        },
        {
            "heading": "IV. DOMAIN ADAPTATION WITHOUT ACCESS TO SOURCE DATA",
            "text": "In this section, following the idea of the theoretical result, we provide a novel learning framework to address USFDA. More precisely, for the model smoothness term, we present a JN regularizer on target domain as a plug-in unit to boost the existing USFDA, which implicitly forces the smoothness of model in the neighborhood of target sample. Subsequently, we adopt pseudo labelling strategy (i.e., SHOT [22]) as an attempt baseline to handle the domain discrepancy term, due to its simplicity.\nFigure 2 shows a pipeline of our approach. Specifically, we first generate the source model by source data. During its\ndevelopment, we keep the classifier frozen and utilize the feature encoding module as initialization for target domain. The feature encoding module is then fine-tuned by our proposed framework. In the following, we elaborate each step of our model."
        },
        {
            "heading": "A. Source Model Generation",
            "text": "To learn the source model for the subsequent target adaptation, referring to the baseline model [22], we adopt the crossentropy loss based on label smoothing, as it increases the discriminability of the learned source model [40] . We have the following objective Ls:\nLs = \u2212E(xs,ys)\u2208Ds K\u2211 k=1 qlsk log (\u03b4k (fs (xs))) , (4)\nwhere qlsk = (1\u2212\u03b1)qk+\u03b1/K is the smoothed label, qk is the one-of K encoding and \u03b1 is smoothing parameter empirically set to 0.1. \u03b4k (\u00b7) denotes the k-th element in the soft-max output of a K-dimensional vector."
        },
        {
            "heading": "B. Target Model Fine-tuning",
            "text": "To make the fixed classifier/model works well in the target domain, we aim to obtain a fine-tuned encoder that could implicitly mitigate the domain discrepancy and optimize the model smoothness on target domain. Therefore, the objective function of our framework is formulated as follows:\nLt = LM + LD, (5)\nwhere LM denotes the model smoothness objective, and LD represents the domain alignment objective. Note that the current USFDAs focus on heuristically modelling the LD, we\naims to formulate LM , which can act as a plug-in unit to any existing USFDAs. These two terms are detailed in the following subsections:\n1) Jacobian Norm for Model Smoothness: To formulate LM, according to the Definition 2, we are motivated to control the smoothness/consistence in the neighborhood of the target sample. Along this line, we relax Eq. 2 for simplicity, and obtain the following objective:\n1\n\u03c32 nt\u2211 i=1 E\u03b6 (f (xi + \u03b6)\u2212 f (xi))2 , (6)\nwhere \u03b6 \u223c N ( 0, \u03c32I ) . By first-order Taylor expansion, and let J(x) = \u2202f\u2202x \u2208 R K\u00d7D, we have\nf(x+ \u03b6) = f(x) + J(x)\u03b6 + o(\u03b6). (7)\nOmitting the high-order terms, Eq. 6 can be reformulated to the following JN regularizer (LJ ):\nLJ = 1\n\u03c32 nt\u2211 i=1 E\u03b6 (f (xi +\u03b6)\u2212 f (xi))2\n= 1\n\u03c32 nt\u2211 i=1 E\u03b6 \u2016J (xi) \u03b6\u20162\n= nt\u2211 i=1 tr [ J (xi) T J (xi) 1 \u03c32 E\u03b6 [ \u03b6\u03b6T ]] =\nnt\u2211 i=1 tr [ J (xi) T J (xi) 1 \u03c32 \u03c32I ] =\nnt\u2211 i=1 \u2016J (xi)\u20162F .\n(8)\nConsequently, we formulate LM to JN regularizer LJ where \u03bb is the balancing hyper-parameters.\nLM = \u03bbLJ = \u03bb nt\u2211 i=1 \u2016J (xi)\u20162F . (9)\n2) Nearest Centroid classifier for Obtaining Pseudo Label: To alleviate the harmful effects caused by the inaccurate network outputs, we further apply pseudo-labeling for each unlabeled data to better supervise the target data encoding training. Inspired by the DeepCluster [41], we first attain the centroid for each class in the target domain as follows:\nc (0) k =\n\u2211 xt\u2208Xt \u03b4 ( f\u0302 (k) t (x) ) g\u0302t(x)\u2211\nxt\u2208Xt \u03b4 ( f\u0302 (k) t (x) ) , (10) where f\u0302t denotes the previously learned target model and g\u0302t(x) is the target encoder. Then, we obtain the pseudo labels via the nearest centroid classifier\ny\u0302t = argmin k Df\n( g\u0302t (xt) , c (0) k ) , (11)\nwhere Df (\u00b7, \u00b7) measures the cosine distance. In the end, the target centroids is computed via the new pseudo labels:\nc (1) k =\n\u2211 xt\u2208Xt I (y\u0302t = k) g\u0302t(x)\u2211\nxt\u2208Xt I (y\u0302t = k) , (12)\nwhere I is the index function. The final pseudo label is obtained as followings:\ny\u0302t = argmin k Df\n( g\u0302t (xt) , c (1) k ) , (13)\n3) Pseudo Labeling for Implicit Alignment: To formulate the LD, referring to the baseline [22], we adopt both the information maximization loss LIM and self-supervised pseudo labelling loss LSSL toward implicit alignment. Their formulations are as follows, respectively:\nLIM = \u2212H\n( 1\nK K\u2211 i f (xi)\n) + 1\nK K\u2211 i H (f (xi)) , (14)\nLSSL = \u2212E(xt)\u2208Dt K\u2211 k=1 q\u0302k log (\u03b4k (f (xt))) , (15)\nwhere H(\u00b7) is the entropy function. q\u0302k is the one-of-K encoding of the target pseudo labels, which is generated by the nearest centroid classifier. For more details, please refer to [22]. In this way, we define LD as the composition of LIM and LSSL:\nLD = \u03b2LIM + \u03b3LSSL, (16)\nwhere \u03b2 and \u03b3 are the balancing hyper-parameters and we empirically set \u03b2 = 1 and \u03b3 = 1 in our following experiments.\nLast but not least importantly, we need to state that although our model is based on SHOT, for other USFDA methods, our framework can be easily applied by reformulating LD. Therefore, the JN regularizer is flexible enough to be embedded into any other existing USFDAs."
        },
        {
            "heading": "V. EXPERIMENTS",
            "text": "In this section, we present the experimental results on multiple domain adaptation benchmarks to demonstrate the effectiveness of our model."
        },
        {
            "heading": "A. Benchmark Datasets",
            "text": "To evaluate the performance of our model, we conduct abundant experiments over the most widely-used benchmark datasets [4] with different sample size including Office-31 (small-size), Office-Home (medium size) and VisDA-C (large size). Table I lists the statistics of these datasets. Office-31 ( [42] is a small-size benchmark, which contains 4652 images with 31 categories in three visual domains Amazon(A), DSLR(D), Webcam(W). Office-Home [43] is a medium-size benchmark, which contains 15588 images of 65 categories from 4 domains: Artistic images (Ar), Clipart images (Cl), Product images (Pr), and Real-world images (Rw). VisDA-C ( [44] is a large-scale benchmark, which contains 207000 images of 12 categories from synthesis and real domains. The source domain contains 152000 of synthetic images, while the target domain has 55000 real object images sampled from Microsoft COCO."
        },
        {
            "heading": "B. Comparison Methods",
            "text": "To verify the effectiveness of the our work, we compare it respectively with several UDA and USFDA SOTAs. The UDA methods include Deep Adaptation Network (DAN) [45], Domain Adversarial Neural Networks (DANN) [19] Conditional Adversarial Networks (CDAN) [7] and Batch Spectral Penalization (BSP) [46]. The USFDA methods contain Source HypOthesis Transfer (SHOT) [22], Model Adaptation (MA) [23], BAIT [34] and Batch Normalization (BN) [35]. Moreover, source model only (SO) denotes using the entire source model for target label prediction. JN only (JN(o)) represents using JN regularizer only to fine-tune the feature encoder. It should be noted that SHOT is the baseline of our model by setting \u03bb = 0. To evaluate their performance, we follow the widely used accuracy as a measurement. The results of comparison methods are directly obtained from the published papers, since we follow the same setting.\nC. Implementation Details\n1) Network architecture: Following the current UDA/USFDA works [19], [22], [30], we employ the pre-trained ResNet-50 or ResNet-101 [47] models as the backbone module. Specifically, we replace the original FC layer with a bottleneck layer (256 units) and a task-specific\nFC classifier layer. After that, a BN layer is put inside the bottleneck layer. Moreover, a weight normalization layer is utilized in the last FC layer. More specifically, for each task, referring to the existing works [7], [48], we employ the pre-trained ResNet-50 (Office-31 and Office-Home) or ResNet-101 (VisDA-C) [47] models as the backbone module.\n2) Parameter Settings: To fine-tune the adaptive model, we adopt the mini-batch SGD with momentum 0.9 and set the batch size as 64. For Office-31 and Office-Home, we empirically set the learning rate as 0.01 and \u03bb = 0.2. Since VisDA-C can easily converge, we utilize a smaller learning rate 0.001 and a bigger \u03bb = 0.8. For learning in the target domain, we update the pseudo-labels epoch by epoch. The whole network is trained by the back propagation, while the\nnewly added layers (e.g., task-specific FC classifier layer) are trained with learning rate 10 times of that of the pre-trained layers."
        },
        {
            "heading": "D. Experimental Results",
            "text": "The experimental results of Office-31, Office-Home, and VisDA-C are reported in Tables II, III, and IV, respectively. From these results, we can make several observations as follows.\nFirstly, by adding a simple JN regularization term, our model obtains the best mean accuracy on Office-31 and OfficeHome, and the best per-class accuracy on Visda-Home. Compared with the USFDA SOTAs [23], [34], [35], we achieve the best/second-best results on 5 out of 6 individual\ntasks at Office-31 dataset and the best/second best on all 12 tasks at Office-Home dataset, respectively. For largescale synthesis-to-real VisDA-C dataset, we achieve the best/second-best class accuracy among 9 out of 12 classes. These results obtained from a wide range of datasets with different sample sizes demonstrate that our model is capable of reducing the target risk while solving USFDA to great extent.\nSecondly, compared to the conventional UDA works, we also achieve the competitive results even with no direct access to the source domain data. Specifically, our model achieves better gains with 1.4%, 8.2% and 7.6% in performance than the UDA SOTAs on the Office-31, Office-Home and VisDA-C datasets, respectively. This implying the superior of our model even without source data.\nThirdly, compared to the baseline model (i.e, SHOT) which only achieves the second best results on two tasks at Office31 dataset and one task at Office-Home, the designed JN regularizer provides gains on almost all tasks (e.g., \u223c 4% on A\u2192D and W\u2192A tasks). Moreover, the performance of our model only degrades in class \u2019train\u2019 on VisDA-C, and the main reason may be that the background of this class is too complex."
        },
        {
            "heading": "E. Evaluation of Each Component",
            "text": "When solving the USFDA, our model involves two components: (1) Jacobian norm for model smoothness and (2)pseudo labeling for implicit alignment. To verify the performance of different components, we empirically select different components as shown in Table V. Specifically, SHOT only considers the implicit alignment, JN(o) only considers the model smoothness while our model contains both two terms.\nAs expected, compared to SHOT (i.e., LD only), the proposed JN regularizer provides a significant performance gain (i.e., 2% over Office-31, 3.7% over Office-Home and 5.8% over VisDA-C), which illustrates that implicit alignment is not sufficient to address USFDA and the proposed JN regularizer can effectively boost the performance of USFDA. Moreover,the results of JN alone (i.e., LJ only) also achieves comparable results on USFDA with the accuracy of 87.0%, 70.6% and 81.8% on the Office-31, Office-Home and VisDAC, respectively.\nThose results demonstrate that both two components are important for improving the accuracy in USFDA tasks, which is consistent with the derived theoretical result in the Theorem 1. Moreover, the results further reveal that implicit alignment and model smoothness can benefit each other in solving USFDA."
        },
        {
            "heading": "F. Time Complexity",
            "text": "We validate the time complexity of the proposed JN regularizer through the empirical analysis. Specifically, we compare the time cost of our model with SHOT on the largescale dataset, i.e., VisDA-C. The environment is Nvidia RTX 2080Ti with 11G memory. The results are given in Figure. 3. Specifically, for each batch, the proposed JN regularization term only introduces additional 0.06s cost on VisDA-C. As we can observe, despite its superiority in the performance gain, the time cost paid by the proposed JN regularization term is almost negligible."
        },
        {
            "heading": "VI. CONCLUSION",
            "text": "In this paper, we develop a JN regularizer as a plug-in unit to boost the performance of USFDA. With a few lines of codes, the proposed JN regularizer can significantly improve the performance of the existing USFDAs. It is worth noting that the JN regularization term does NOT need access to the source model and thus can be applied on more challenging black-box USFDA, which will be further studied in our future work."
        }
    ],
    "title": "Jacobian Norm for Unsupervised Source-Free Domain Adaptation",
    "year": 2022
}