{
    "abstractText": "Gene essentiality refers to the degree to which a gene is necessary for the survival and reproductive efficacy of a living organism. Although the essentiality of non-coding genes has been documented, there are still aspects of non-coding genes' essentiality that are unknown to us. For example, We do not know the contribution of sequence features and network spatial features to essentiality. As a consequence, in this work, we propose DeepHEN that could answer the above question. By buidling a new lncRNA-proteion-protein network and utilizing both representation learning and graph neural network, we successfully build our DeepHEN models that could predict the essentiality of lncRNA genes. Compared to other methods for predicting the essentiality of lncRNA genes, our DeepHEN model not only tells whether sequence features or network spatial features have a greater influence on essentiality but also addresses the overfitting issue of those methods caused by the low number of essential lncRNA genes, as evidenced by the results of enrichment analysis.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hanlin Zhang"
        },
        {
            "affiliations": [],
            "name": "Wenzheng Cheng"
        }
    ],
    "id": "SP:b746d412ea4e952aa006d88f1eb3fb8532af7c6c",
    "references": [
        {
            "authors": [
                "G Rancati",
                "J Moffat",
                "A Typas"
            ],
            "title": "Emerging and evolving concepts in gene essentiality",
            "venue": "Nat. Rev. Genet",
            "year": 2018
        },
        {
            "authors": [
                "CA Hutchison",
                "R-Y Chuang",
                "VN Noskov"
            ],
            "title": "Design and synthesis of a minimal bacterial genome",
            "venue": "Science 2016;",
            "year": 2016
        },
        {
            "authors": [
                "M Juhas",
                "L Eberl",
                "GM. Church"
            ],
            "title": "Essential genes as antimicrobial targets and cornerstones of synthetic biology",
            "venue": "Trends Biotechnol",
            "year": 2012
        },
        {
            "authors": [
                "R Marcotte",
                "KR Brown",
                "F Suarez"
            ],
            "title": "Essential Gene Profiles in Breast, Pancreatic, and Ovarian Cancer Cells",
            "venue": "Cancer Discov",
            "year": 2012
        },
        {
            "authors": [
                "M Sauvageau",
                "LA Goff",
                "S Lodato"
            ],
            "title": "Multiple knockout mouse models reveal lincRNAs are required for life and brain development",
            "venue": "eLife",
            "year": 2013
        },
        {
            "authors": [
                "Y Tang",
                "TR Meister",
                "M Walczak"
            ],
            "title": "A mutagenesis screen for essential plastid biogenesis genes in human malaria parasites",
            "venue": "PLOS Biol",
            "year": 2019
        },
        {
            "authors": [
                "DW Morgens",
                "RM Deans",
                "A Li"
            ],
            "title": "Systematic comparison of CRISPR/Cas9 and RNAi screens for essential genes",
            "venue": "Nat. Biotechnol",
            "year": 2016
        },
        {
            "authors": [
                "A Tsherniak",
                "F Vazquez",
                "PG Montgomery"
            ],
            "title": "Defining a Cancer Dependency Map",
            "venue": "Cell 2017;",
            "year": 2017
        },
        {
            "authors": [
                "JM Peters",
                "A Colavin",
                "H Shi"
            ],
            "title": "A Comprehensive, CRISPR-based Functional Analysis of Essential Genes in Bacteria",
            "venue": "Cell",
            "year": 2016
        },
        {
            "authors": [
                "S Nurk",
                "S Koren",
                "A Rhie"
            ],
            "title": "The complete sequence of a human genome",
            "year": 2022
        },
        {
            "authors": [
                "SJ Liu",
                "MA Horlbeck",
                "SW Cho"
            ],
            "title": "CRISPRi-based genome-scale identification of functional long noncoding RNA loci in human cells",
            "year": 2017
        },
        {
            "authors": [
                "S Zhu",
                "W Li",
                "J Liu"
            ],
            "title": "Genome-scale deletion screening of human long non-coding RNAs using a paired-guide RNA CRISPR\u2013Cas9",
            "venue": "library. Nat. Biotechnol",
            "year": 2016
        },
        {
            "authors": [
                "I Bartha",
                "J Di Iulio",
                "JC Venter"
            ],
            "title": "Human gene essentiality",
            "venue": "Nat. Rev. Genet",
            "year": 2018
        },
        {
            "authors": [
                "H Chen",
                "Z Zhang",
                "S Jiang"
            ],
            "title": "New insights on human essential genes based on integrated analysis and the construction of the HEGIAP web-based platform. Brief",
            "year": 2020
        },
        {
            "authors": [
                "X Zhang",
                "W Xiao",
                "W. Xiao"
            ],
            "title": "DeepHE: Accurately predicting human essential genes based on deep learning",
            "venue": "PLOS Comput. Biol",
            "year": 2020
        },
        {
            "authors": [
                "T Beder",
                "O Aromolaran",
                "J D\u00f6nitz"
            ],
            "title": "Identifying essential genes across eukaryotes by machine learning. NAR Genomics Bioinforma",
            "year": 2021
        },
        {
            "authors": [
                "P Zeng",
                "J Chen",
                "Y Meng"
            ],
            "title": "Defining Essentiality Score of Protein-Coding Genes and Long Noncoding RNAs",
            "venue": "Front. Genet",
            "year": 2018
        },
        {
            "authors": [
                "S Kuang",
                "Y Wei",
                "L. Wang"
            ],
            "title": "Expression-based prediction of human essential genes and candidate lncRNAs in cancer cells",
            "venue": "Bioinformatics",
            "year": 2021
        },
        {
            "authors": [
                "Xin X-H",
                "Zhang Y-Y",
                "Gao C-Q"
            ],
            "title": "SGII: Systematic Identification of Essential lncRNAs in Mouse and Human Genome With lncRNA-Protein-Protein Heterogeneous Interaction Network",
            "venue": "Front. Genet",
            "year": 2022
        },
        {
            "authors": [
                "Zhang Y-Y",
                "Liang D-M",
                "Du P-F"
            ],
            "title": "iEssLnc: quantitative estimation of lncRNA gene essentialities with meta-path-guided random walks on the lncRNA-protein interaction",
            "venue": "network. Brief. Bioinform",
            "year": 2023
        },
        {
            "authors": [
                "Z Hu",
                "Q Yu",
                "Y Guo"
            ],
            "title": "Drug Synergistic Combinations Predictions via Large-Scale PreTraining and Graph Structure Learning",
            "year": 2023
        },
        {
            "authors": [
                "Shen Z-A",
                "Luo T",
                "Zhou Y-K"
            ],
            "title": "NPI-GNN: Predicting ncRNA\u2013protein interactions with deep graph neural networks. Brief. Bioinform",
            "year": 2021
        },
        {
            "authors": [
                "Z Zhang",
                "M Xu",
                "A Jamasb"
            ],
            "title": "Protein Representation Learning by Geometric Structure Pretraining",
            "year": 2023
        },
        {
            "authors": [
                "J Chen",
                "Z Hu",
                "S Sun"
            ],
            "title": "Interpretable RNA Foundation Model from Unannotated Data for Highly Accurate RNA Structure and Function Predictions",
            "year": 2022
        },
        {
            "authors": [
                "P. Ng"
            ],
            "title": "dna2vec: Consistent vector representations of variable-length k-mers",
            "year": 2017
        },
        {
            "authors": [
                "R Oughtred",
                "J Rust",
                "C Chang"
            ],
            "title": "The BIOGRID database: A comprehensive biomedical resource of curated protein, genetic, and chemical interactions",
            "venue": "Protein Sci",
            "year": 2021
        },
        {
            "authors": [
                "Y Hao",
                "W Wu",
                "H Li"
            ],
            "title": "NPInter v3.0: an upgraded database of noncoding RNA-associated interactions",
            "venue": "Database 2016;",
            "year": 2016
        },
        {
            "authors": [
                "Zhang Y-Y",
                "Zhang W-Y",
                "Xin X-H"
            ],
            "title": "dbEssLnc: A manually curated database of human and mouse essential lncRNA genes",
            "venue": "Comput. Struct. Biotechnol. J",
            "year": 2022
        },
        {
            "authors": [
                "T Mikolov",
                "K Chen",
                "G Corrado"
            ],
            "title": "Efficient Estimation of Word Representations in Vector Space",
            "year": 2013
        },
        {
            "authors": [
                "A Vaswani",
                "N Shazeer",
                "N Parmar"
            ],
            "title": "Attention Is All You Need",
            "year": 2017
        },
        {
            "authors": [
                "Asgari E",
                "Mofrad MRK"
            ],
            "title": "Continuous Distributed Representation of Biological Sequences for Deep Proteomics and Genomics",
            "venue": "PLOS ONE 2015;",
            "year": 2015
        },
        {
            "authors": [
                "TN Kipf",
                "M. Welling"
            ],
            "title": "Variational Graph Auto-Encoders",
            "year": 2016
        },
        {
            "authors": [
                "Chang C-C",
                "Lin C-J"
            ],
            "title": "LIBSVM: A library for support vector machines",
            "venue": "ACM Trans. Intell. Syst. Technol",
            "year": 2011
        }
    ],
    "sections": [
        {
            "text": "Gene essentiality refers to the degree to which a gene is necessary for the survival and reproductive efficacy of a living organism. Although the essentiality of non-coding genes has been documented, there are still aspects of non-coding genes' essentiality that are unknown to us. For example, We do not know the contribution of sequence features and network spatial features to essentiality. As a consequence, in this work, we propose DeepHEN that could answer the above question. By buidling a new lncRNA-proteion-protein network and utilizing both representation learning and graph neural network, we successfully build our DeepHEN models that could predict the essentiality of lncRNA genes. Compared to other methods for predicting the essentiality of lncRNA genes, our DeepHEN model not only tells whether sequence features or network spatial features have a greater influence on essentiality but also addresses the overfitting issue of those methods caused by the low number of essential lncRNA genes, as evidenced by the results of enrichment analysis.\nKeywords: sample, graph neural network, representation learing, lncRNA-protein-protein network, essential non-coding genes"
        },
        {
            "heading": "INTORDUCTION",
            "text": "Gene essentiality refers to the degree to which a gene is necessary for the survival and reproductive success of a living system. Genes that are indispensable in fulfilling these functions are classified as essential genes[1]. The concept of gene essentiality is dynamic and influenced by the specific context in which it is assessed. It can vary across different genetic backgrounds and environmental conditions. Essential genes play a crucial role in defining the minimal genome of an organism[2] and represent potential targets for the development of anti-cancer or anti-infection drugs[3,4].\nExperimental approaches for determining gene essentiality encompass single or multiple-gene knockouts[5], mutagenesis screening[6], and ribonucleic acid (RNA) interference screening[7]. These methods have found widespread application in screening essential genes within single-cell organisms and cell lines derived from complex multicellular organisms[8]. However, due to technical constraints, the majority of reported essential genes are from single-cell organisms like bacteria[9].Thers is a limited documentation of the essentiality of non-coding genes.\nResearch show that the non-coding regions take up most of the human genome sequences[10]. Hence, there is a need to determine the essentiality of non-coding genes. There are advancements in CRISPR-Cas9 screening[11] techniques that enable the acquisition of genome-wide essentiality information for human cell lines[12]. However, as stated in a recent review, it remains challenging to determine the genome-wide essentiality of all non-coding regions in the human genome[13].\nExtensive research has been conducted on computational predictions of essential protein-coding genes. These predictions rely on the properties exhibited by essential protein-coding genes, such as\nhigher expression levels[14], greater conservation compared to non-essential genes[14]. Numerous computational models have been developed in this regard. DeepHE[15], for example, is a deep learning-based method that utilizes both sequence and network features to predict essential proteincoding genes in humans. Additionally, CLEARER[16] is a powerful machine learning-based method that leverages various types of genomic information to identify essential protein-coding genes across different organisms.\nHowever, for non-coding genes, due to the limitated number of essential lncRNA[13], it is hard to build computational models. When it comes to lncRNA(long non-coding RNA) genes, there exits two kinds of computational methods. One of them is those that originally target the estimation of coding gene essentialities, which have been extended to include lncRNA genes. For example, The GIC (Gene Importance Calculator) method[17] is a sequence-based scoring scheme for assessing gene essentiality, originally developed for protein-coding genes. The authors of GIC applied their scoring scheme directly to lncRNA genes. While the GIC method has shown success in predicting the essentiality of seven mouse lncRNA genes, it remains challenging to thoroughly evaluate its performance on a broader range of lncRNA genes and across different organisms. Besides, the XGEP[18] method integrated gene expression profiles from numerous cancer cell lines to develop a machine learning-based model for predicting essential protein-coding genes. Similarly to GIC, they also applied the XGEP model directly to lncRNA genes. While there are promising results and supporting evidence in the literature, quantitatively evaluating its performance on lncRNA genes remains challenging. Similar to GIC, the XGEP method also directly applied its model to lncRNA genes. While there are promising results and supporting evidence in the literature, it remains challenging to quantitatively evaluate the performance of the XGEP model on lncRNA genes. The another is those methods that target directly the estimation of non-coding gene essentialities. The SGII method[19], which prioritizes lncRNA genes based on their centralities within the lncRNAprotein-protein interaction network. Using a manually curated dataset of essential lncRNA genes, they observed that essential lncRNA genes tend to exhibit higher betweenness centrality (BC) and degree centrality (DC) compared to the average level of all lncRNA genes. This observation aligns with the characteristics of essential proteins in protein-protein interaction networks. However, a centrality-based scoring scheme is too simplistic to capture the intrinsic structural features of the lncRNA-protein interaction network. Therefore, Zhang et al. propose iEssLnc[20] model to solve the problem. iEssLnc is a deep learning model that can quantitative predict the essential lncRNA genes on the genome level. However, suffering from the low number of essential lncRNA genes, the data hungery model, iEssLnc, is a little over-fitting.\nLncRNA-protein interactions play a crucial role in determining the molecular functions (MF) of lncRNAs. Consequently, these interactions hold significant potential for predicting lncRNA essentiality. Meanwhile protein\u2013protein interaction is also important in indicating the MF of proteins. As a consequence, we build a lncRNA-protein-protein interaction network.\nWith the advancement of graph neural network(GNN), GNN has shown its power in representing graph structural information with low-dimensional representations. Hence, GNN has been used in many bioinformatics studies, such as , predicting drug-target interaction[21], predicting lncRNAprotein interaction[22].\nRepresentation learning, also known as feature learning or feature extraction, is a fundamental concept in machine learning and artificial intelligence. It refers to the process of automatically learning meaningful and informative representations or features from raw data. Recently,\nrepresentation learning demonstrate a greate power in bioinformatics, such as protein learning[23], RNA learning[24]. These power representations have greatly facilitated downstream tasks.\nIn this work, through taking the advantage of GNN and representation learning, we gain the meaningful representation of both sequence feature and network feature. The dbEssLnc database [73] comprises a comprehensive collection of essential lncRNA genes with literature evidence, encompassing over a hundred genes in both human and mouse. It stands as the largest repository of essential lncRNA genes to date. Based on the idea of self-supervised clustering and dbEssLnc database, we build a benchmarking dataset to train our supervised classification model. The result of enrichment analysis indicate that our model has a better performance over other methods. We also analysis the contribution of sequence features and network spatial features to essentiality."
        },
        {
            "heading": "MATERIALS AND METHODS",
            "text": ""
        },
        {
            "heading": "The workflow of DeepHEN model",
            "text": "The overview of DeepHEN is shown in Figure 1. In this research, we first constructed a lncRNAprotein-protein interaction (LPPI) network. We used graph embedding methods to capture network feature of lncRNA genes. We also utilized dna2vec[25] to capture sequence feature. Due to the limited amount essential lncRNAs, we conducted negative sample optimization depending on the idea of semi-supervised clustering. Then supervised machine-learning model was trained. Many essential lncRNAs were predicted by our DeepHEN model. The correctness of DeepHEN model was verified by enrichment analysis method."
        },
        {
            "heading": "Dataset Curation",
            "text": "The human protein-protein interaction data from BioGrid database version 4.4[26] was downloaded. The PPI network includes 32142 proteins and 664819 protein-protein interactions . The human lncRNA-protein interactions were obtained from the NPI database v4.0[27] to built lncRNA-protein interaction (LPI) network. By cross-referencing the names of proteins in both datasets, the lncRNAprotein interactions and protein-protein interactions were integrated, resulting in a heterogeneous network comprising two types of interactions. This network was named the LPPI network. In order to focus on lncRNAs, the proteins that do not interact with lncRNAs were removed. After processing, the LPPI network contains a total of 41530 lncRNAs, 3003 proteins, and 485087 interactions. To obtain the lncRNA gene sequence needed for our research, we finally acquired 29482 human lncRNAs.\nThe PPI network can be defined as P = (U, I), in which U is the set of protein nodes, and I defines edges that represent the protein-protein interactions. Besides, the LPPI network can be formulated as\na heterogeneous graph G = (V, E, T), in which each node v \u2208 V, and each interaction e \u2208 E, are\nassociated with their mapping functions \u03c6(v): V\u2192 TV, and \u03d5(e): E \u2192 TE respectively. The set TV \u2286 T and the set TE \u2286 T contain node types and edge types in G. In LPPI network, the set TV = {lncRNA, protein}, and the set TE = {lncRNA-protein, protein-protein}. Therefore, we have:\nET > 2TV + (1)\nWhere |.| is the cardinal operator in the set theory. This makes G a heterogenous graph. We gained lncRNA gene sequences from SGII study and 154 human essential lncRNA genes from the dbEssLnc database[28].\nSequence features learning\nThe traditional way to extract features from lncRNA gene sequence is to calculate features such as, Gene Importance Calculator (GIC) or sequence length that mainly depends on human knowledge. However, due to the lack of human knowledge, traditional way can not capture enough features from lncRNA gene sequence for downstream task. Therefore, we used dna2vec model to solve this problem. Like network embedding mentioned above, the dna2vec model can also learn continuous lowdimensional feature representations of k-mers that are expected to capture the similarity between kmers.\nThe first step of dna2vec is to separate genome into long non-overlapping DNA fragments. Next, dna2vec model converts long DNA fragments into overlapping variable-length k-mers that can be considered as corpus. Dna2vec model puts this corpus into the skip-gram of word2vec[29], gaining embeddings of k-mers, which maximizes the probability of retaining neighboring k-mers. In this research, we used the pretrained dna2vec results by hg38 dataset.\nUsing dna2vec can only get embeddings of k-mers, however, we need to extract feature from sequence. Therefore, we first convert human lncRNA sequences into overlapping 3-mers. For every lncRNA gene sequence, we have X = (x1, x2, x3\u2026\u2026xn) \u2208 \u211dn\u00d7d, where xi is the dna2vec pretrained result of i-th 3-mers, n is the number of non-overlapped 3-mers of lncRNA gene sequence, d is the dimension of dna2vec pretrained result and d is 100. Only summing the embeddings of every 3- mers of lncRNA gene sequence together according to the pretrained dna2vec result will omit positional information. Therefore, inspired by the great transformer[30], we utilize positional encoding in our sequence learning part. The positional matrix P is defined as follows:\n,2 2\n0.01 ( )\n10000 i j j d\ni P = sin\n(2)\n,2 +1 2\n0.01 ( )\n10000 i j j d\ni P = cos\n(3)\nWhere i is the i-th 3-mers of lncRNA gene sequence and 2j, 2j+1 are the 2j-th, 2j+1-th dimensions of the dna2vec pretrained results. The lncRNA gene sequence feature matrix Y is defined as follows:\nY = X + P (4)\nThen the lncRNA gene sequence feature is calculated by adding up each row of the matrix Y.\nSequence features are also extracted from proteins in our research. To learn protein sequence features, we utilize the method of lncRNA gene feature learning and employ ProtVec[31]. ProtVec generates three lists of shifted non-overlapping words for each protein sequence. These lists are then split into non-overlapping fixed-length 3-mers. The generated corpus is finally fed into the skip-gram of node2vec to learn the representation of protein 3-mers. In this research, we use the pretrained ProtVec results using the Swiss-Prot dataset.\nIn this research, each protein sequence is converted into overlapping 3-mers. Next, we sum the embeddings of every 3-mers of protein sequence together according to the pretrained ProtVec results. The dimension of the protein sequence feature is controlled to 100, which is consistent with the lncRNA gene sequence feature."
        },
        {
            "heading": "Network features learning",
            "text": "Topology features typically only extract one type of network topology feature and cannot capture the similarity between nodes in the network. To overcome this limitation, we used the Variational Graph Auto-Encoders (VGAE)[32] to learn embeddings of nodes in the LPPI network and obtain continuous low-dimensional feature representations of each node.\nWe first assigned features to nodes in the LPPI network. For nodes v \u2208 V and \u03c6(v) = protein, we used ProtVec to assign features as mentioned above. For nodes v \u2208 V and \u03c6(v) = lncRNA, we initialized the features of these nodes with a normal distribution. We wrote the resulting graph of feature assignments into the node feature matrix X and the adjacency matrix A, which serve as the input for subsequent modeling. Next, we used the VGAE model to learn a latent, interpretable representation of the input matrix data. VGAE consists of two parts: an inference model and a generation model.\nIn the inference model, VGAE learns to generate corresponding latent variables Z for each input variable by learning a normal distribution for each input variable. Z is represented as a Gaussian distribution, as shown below:\n2 = 1\n(Z X, A) = (z X, A) = (z , diag( )) N\ni i i ii q q \u03bc \u03c3| | | (5)\nWhere (Z X, A)q | refers to the encoder function that maps each input variable to its corresponding\nmean and variance vectors, denoted as \u03bci and \u03c3i respectively. N is the number of nodes in the LPPI network. And the hidden variable Z is sampled from a multivariate Gaussian distribution, where the mean (denoted as \u03bc) and variance (denoted as \u03c3) are derived from two 2-layer Graph Convolutional Network (GCN) models. These two GCN layers are defined as follows:\n~ ~\n0 1( , ) A ReLU(A XW )WGCN X A = (6)\nWhere X is the node feature matrix, W0 and W1 are weight matrices updated by GCN learning.\nThe ReLU activation function and symmetrically normalized adjacency matrix A ~ are defined as\nfollows:\n( ) ( ) R muL a 0E x ,x x=\n(7)\n1 1~ 2 2A = D AD \u2212 \u2212\n(8)\nWhere D is the degree matrix. We can then obtain the matrix of mean node vector representations, and similarly, the matrix of variance node vector representations. The GCN layer for calculating the mean vector and the GCN layer for calculating the variance vector share the same W0.\nIn the generative model section, the reconstructed graph is calculated by taking the inner product between latent variables, as shown below:\n1 1\n(A | Z) = ( | z , z ) N N\nij i ji= j= p p a \n(9)\n\u0442( =1| z ,z ) = \u03c3(z ,z )ij i j i jp a (10)\nWhere (A | Z)p is the decoder function which transformed the hidden variable Z sampled from the\nencoder function back to the adjacent matrix A\uff0cwhere aij are the elements of A, and \u03c3 is the\nlogistic sigmoid activate function, defined as follows:\n-\n1 \u03c3(x) =\n1+ xe (11)\nSimilar to VAE, the loss function of VGAE is defined as follows:\n(Z|X,A)[log (A | Z)]- KL[ (Z | X,A) || (Z)]qL = p q p\n(12)\nWhere KL[q()||p()] is the Kullback-Leibler divergence between q() and p(), and p(Z) is the Gaussian prior, defined as follows:\n1 1\n(Z) = (z ) = (z | 0, I) N N\ni ii i p p = =  (13)\nThe first term of the loss function L aims to minimize the difference between the reconstructed graph and the original graph. Meanwhile, the second term of the loss function L aims to make the learned distribution of latent variables similar to the gaussian prior. The KL divergence of Equation (12) restricts the posterior distribution to be close to the prior distribution we set beforehand, which acts as a regularization term to prohibit overfitting.\nOverall, the inference model encodes real samples as low-dimensional vector representations (latent variables) and learns the distribution of latent variables. The generation model samples the corresponding latent variables of real samples from the distribution of latent variables and generates data that closely matches the real samples. Gaussian noise is introduced in this process to ensure the model's generative capability, and the reparameterization trick is used to replace the sampling process and ensure that the model is trainable. Finally, when the model is trained, we obtain the feature embeddings of LPPI network nodes through the inference model."
        },
        {
            "heading": "Build benchmarking dataset",
            "text": "To construct a high-quality benchmark dataset, we extracted only 154 necessary lncRNA genes from the dbEssLnc database and marked the remaining lncRNA genes as unknown. To further filter out non-essential genes and annotate the importance of each gene, we connected feature vectors obtained from both the LPPI network and dna2vec to form a total feature vector for each lncRNA gene.\nFor each unknown lncRNA gene, we calculated the cosine similarity between its total feature vector and the total feature vectors of each necessary lncRNA gene. We then computed the average of all cosine similarity results to obtain the importance annotation. Based on their importance annotations, we ranked the unknown lncRNA genes and selected the lowest ranked ones to determine the most suitable non-essential lncRNA group. This method of selecting non-essential lncRNAs is known as the negative sample selection method.\nFinally, we selected the top 154 best non-essential lncRNA genes to construct the benchmark dataset, and we will describe in detail in the later discussion the validation method used to construct the benchmark dataset.\nLncRNA gene essentiality estimations\nIn the supervised machine-learning part of the DeepHEN model, we trained a Support Vector Machine (SVM) to predict the essentiality of lncRNA genes. Our SVM has both a binary classification mode and a quantitative scoring mode. The binary classification mode outputs a binary decision of whether a lncRNA gene is essential or non-essential. The quantitative scoring mode outputs an essentiality score, which is the decision function before applying the final cutoff. The score can be interpreted as the estimated likelihood of a lncRNA gene being essential, before applying the final cutoff.\nEssential lncRNA gene enrichment score\nWe calculated the essential lncRNA gene enrichment score using the Kolmogorov-Smirnov-like statistics. First, we sorted all lncRNA genes according to their quantitative essentiality score in descending order. For the k-th lncRNA gene in the sorted list, the statistic lk (k = 1, 2, \u2026, N) is defined as follows:\n1\n1 1 (1 )\nk k k\ne e\ni i N N N\u2212 = + \u2212 \u2212 \u2212k l l (14)\nWhere Ne is the number of essential lncRNA genes, N is the total number of lncRNA genes in the sorted list, and ik is equal to 1 if the k-th lncRNA gene in the list is essential, otherwise ik is equal to 0. The value of l0 is 0, since it represents the enrichment score for the empty set of genes. The enrichment score (ES) is defined as the maximal value of lk, which reflects the degree of enrichment of essential lncRNA genes in the top-ranked genes, as shown below:\n= 1 max k N ES   k l (15)\nA larger ES indicates a better performance of the DeepHEN model in predicting essential lncRNA genes. The ES ranges from 0 to 1, with 1 indicating that all essential lncRNA genes are ranked at the top of the list, and 0 indicating that the list contains no essential lncRNA genes."
        },
        {
            "heading": "Evaluation metrics",
            "text": "In this research, we used precision(pre), false positive rate (FPR), recall(sensitivity), accuracy and the Matthews correction coefficient (MCC) to evaluate the performance of our classifier. These metrics are defined as follows:\n= +\nPr TP\necision TP FP\n(16)\nFP\nFPR TN FP = +\n(17)\nTP\nSensitivity TP FN = +\n(18)\nTP TN\nAccuracy TP TN FP FN\n+ =\n+ + + (19)\n2\n1\u2212 = +\n* *pre recall F score\npre recall (20)\nWhere TP, TN, FP ,and FN are the number of true positives, true negatives, false positives, and false negatives respectively.\nIn addition, we used the area under the receiver operating characteristic curve (AUROC) to evaluate the overall performance of our classifier. An ROC curve plots the true positive rate (sensitivity) against the FPR for all possible classification thresholds. The AUROC assesses the classifier's ability to distinguish between positive and negative samples, with a higher score indicating better performance.\nWe also used the area under the precision-recall curve (AUPR) to evaluate the performance of our classifier. A PR curve plots the precision against the recall for all possible classification thresholds. The AUPR evaluates classifiers, with a higher AUPR indicates better performance in terms of the trade-off between precision and recall.\nTogether, these metrics provide a comprehensive and robust evaluation of the performance of our DeepHEN classifier in predicting essential lncRNA genes."
        },
        {
            "heading": "Parameter calibrations",
            "text": "The parameters used in our research were adjusted as follows. We used the PyG package in Python to implement VGAE. The hidden size of the GCN layer in VGAE was set to 128. The dimension of the latent variable was set to 64. The learning rate was set to 0.01, and the training epoch was set to 700. We implemented the SVM predictors using the LIBSVM[33] library in Python. The kernel type of our SVM was set to radial basis function. The gamma value was set to 0.07, and the C value was set to 0.0585."
        },
        {
            "heading": "Results and discussions",
            "text": "Prediction performance analysis and comparisons\nWe estimated the prediction performance of the DeepHEN model using 5-fold cross-validations. Our benchmarking dataset introduced information leaks to the training procedures of SGII, GIC, and XGPE methods, against which we compared the performance of our DeepHEN model. Despite this disadvantage, our DeepHEN model outperformed the other methods on all evaluation metrics. The XGEP techniques, which were trained using crucial protein-coding genes, demonstrated strong performance in identifying non-essential lncRNA genes with high precision and low false positive rates (FPR). However, their performance experienced a significant decline when recognizing essential lncRNA genes, with substantially lower values in terms of sensitivity, F1-score, and accuracy. This decline could be attributed to the limited knowledge available on essential lncRNA genes at the time the methods were developed, or to potential inherent variations in essentiality between coding and non-coding genes, a phenomenon that remains unexplored within the life sciences community. Figure 2 shows the performance of DeepHEN in predicting values, as well as comparisons to the SGII, GIC, and XGEP techniques.\nTo provide a comprehensive evaluation of the prediction performance of DeepHEN, we presented the ROC and PR curves in Figure 3 and computed the corresponding AUROC and AUPR values. The results of cross-validation, ROC curves, and PR curves demonstrate strong performance. However, our benchmarking dataset is small, and during its composition, we selected a portion of\nunannotated lncRNA genes as negative samples using cosine similarity. This selection process may introduce bias and produce misleading results in cross-validation. Therefore, it is important to verify the usefulness of DeepHEN and the validity of our negative sample selection approach.\nEnrichment analysis and Validation of non-essential lncRNA gene sample strategy\nOur goal is to classify essential human lncRNA, but there is currently no clear definition of human non-essential lncRNA. To compose our benchmarking dataset, we opted for a negative sample selection approach based on the idea of semi-supervised clustering. However, this approach lacks a clear biological basis, and thus its validity needs to be verified. Additionally, while our benchmarking dataset is balanced, in the real world, the number of non-essential lncRNA is likely to be higher than the number of essential lncRNA. Moreover, our benchmarking dataset is relatively small, which may lead to a high number of false positives when our DeepHEN model is applied at the genome level. Therefore, it is crucial to verify the validity of our negative sample selection approach and to test the ability of our DeepHEN model to accurately predict the essentiality of lncRNA at the genome level.\nAs mentioned earlier, the DeepHEN model outputs an essentiality score for every lncRNA gene. To evaluate the performance of DeepHEN in quantitative scoring mode, we conducted an enrichment analysis of all essential lncRNA genes. We calculated the enrichment score for each lncRNA gene using equation (15) at the genome level and compared it to the enrichment scores calculated for SGII and GIC methods. Figure 4 shows the enrichment curve for DeepHEN, SGII, and GIC methods. In addition to the enrichment curve, we also considered the essentiality score, so our DeepHEN models enable quantitative estimation of the essentiality of lncRNA genes in genomewide screenings.Our results demonstrate that the essentiality score of DeepHEN is higher than that of other methods, and that essential lncRNA genes tend to have higher ranks in the DeepHEN model. It is worth noting that in the enrichment analysis of the iEssLnc method, these essential noncoding genes tend to be predominantly distributed towards the front portion of the sort, which is too good to be ture. However, our DeepHEN models address the issue.\nTo further validate our negative sample strategy for non-essential lncRNA genes, we conducted additional experiments. As mentioned earlier, we sorted lncRNAs with unknown essentiality status in descending order of rank and selected the top-ranked lncRNAs as non-essential positive samples. We trained an SVM using this positive sample approach and evaluated its prediction performance, as shown in Table 1. We also conducted an enrichment analysis, as shown in Figure 4. Our results demonstrate that the prediction performance decreases with the positive sample approach, as evidenced by the lower enrichment score and less distinct concentration of essential genes at the genomic level. These findings indicate that the positive sample approach can make the trained predictor unreliable. Therefore, we can confidently conclude that our negative sample strategy for non-essential lncRNA genes is reasonable."
        },
        {
            "heading": "Sequence and Network Analysis",
            "text": "Although our DeepHEN model utilizes both sequence and network features of lncRNA genes, it is important to further explore the characteristics of these genes by studying the correlation between lncRNA essentiality and each feature type separately. To this end, we conducted two additional pipelines: Sequence_DeepHEN, which only uses sequence features, and Network_DeepHEN, which only uses network features. To ensure the accuracy of our analysis, we used the same SVM parameters for Sequence_DeepHEN and Network_DeepHEN as for DeepHEN, using the negative sample approach. We evaluated the prediction performance of Sequence_DeepHEN and Network_DeepHEN using both negative and positive sample approaches and plotted the\ncorresponding ROC and PR curves, as shown in Figure 3. We also conducted an enrichment analysis for both approaches using both negative and positive sample approaches, as shown in Figure 5, and present the prediction performance results in Table 1. As the result shows, both sequence feature and network feature contribute to the essentiality. To futher analysis, we find network feature contribute more to the essentialities of lncRNA genes. However, when only considering network feature, our model can not have an impressive proformance."
        }
    ],
    "title": "DeepHEN: quantitative prediction essential lncRNA genes and rethinking essentialities of lncRNA genes",
    "year": 2023
}