{
    "abstractText": "Understanding how proteins structurally interact is crucial to modern biology, with applications in drug discovery and protein design. Recent machine learning methods have formulated protein-small molecule docking as a generative problem with significant performance boosts over both traditional and deep learning baselines. In this work, we propose a similar approach for rigid protein-protein docking: DIFFDOCK-PP is a diffusion generative model that learns to translate and rotate unbound protein structures into their bound conformations. We achieve state-ofthe-art performance on DIPS with a median C-RMSD of 4.85, outperforming all considered baselines. Additionally, DIFFDOCK-PP is faster than all search-based methods and generates reliable confidence estimates for its predictions.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Mohamed Amine Ketata"
        },
        {
            "affiliations": [],
            "name": "Cedrik Laue"
        },
        {
            "affiliations": [],
            "name": "Ruslan Mammadov"
        },
        {
            "affiliations": [],
            "name": "Hannes St\u00e4rk"
        },
        {
            "affiliations": [],
            "name": "Menghua Wu"
        },
        {
            "affiliations": [],
            "name": "Gabriele Corso"
        },
        {
            "affiliations": [],
            "name": "C\u00e9line Marquet"
        },
        {
            "affiliations": [],
            "name": "Regina Barzilay"
        },
        {
            "affiliations": [],
            "name": "Tommi S. Jaakkola"
        }
    ],
    "id": "SP:d05064d3cc8ebc618943b0e29879da73c90f1c5b",
    "references": [
        {
            "authors": [
                "Sankar Basu",
                "Bj\u00f6rn Wallner"
            ],
            "title": "Dockq: a quality measure for protein-protein docking models",
            "venue": "PloS one,",
            "year": 2016
        },
        {
            "authors": [
                "Rong Chen",
                "Li Li",
                "Zhiping Weng"
            ],
            "title": "Zdock: an initial-stage protein-docking algorithm. Proteins: Structure, Function, and Bioinformatics",
            "year": 2003
        },
        {
            "authors": [
                "Gabriele Corso"
            ],
            "title": "Modeling molecular structures with intrinsic diffusion models",
            "venue": "arXiv preprint arXiv:2302.12255,",
            "year": 2023
        },
        {
            "authors": [
                "Gabriele Corso",
                "Hannes St\u00e4rk",
                "Bowen Jing",
                "Regina Barzilay",
                "Tommi Jaakkola"
            ],
            "title": "Diffdock: Diffusion steps, twists, and turns for molecular docking",
            "venue": "arXiv preprint arXiv:2210.01776,",
            "year": 2022
        },
        {
            "authors": [
                "Valentin De Bortoli",
                "Emile Mathieu",
                "Michael Hutchinson",
                "James Thornton",
                "Yee Whye Teh",
                "Arnaud Doucet"
            ],
            "title": "Riemannian score-based generative modeling",
            "venue": "arXiv preprint arXiv:2202.02763,",
            "year": 2022
        },
        {
            "authors": [
                "Sjoerd J De Vries",
                "Marc Van Dijk",
                "Alexandre MJJ Bonvin"
            ],
            "title": "The haddock web server for datadriven biomolecular docking",
            "venue": "Nature protocols,",
            "year": 2010
        },
        {
            "authors": [
                "Sjoerd J de Vries",
                "Christina EM Schindler",
                "Isaure Chauvot de Beauch\u00eane",
                "Martin Zacharias"
            ],
            "title": "A web interface for easy flexible protein-protein docking with attract",
            "venue": "Biophysical journal,",
            "year": 2015
        },
        {
            "authors": [
                "Israel T Desta",
                "Kathryn A Porter",
                "Bing Xia",
                "Dima Kozakov",
                "Sandor Vajda"
            ],
            "title": "Performance and its limits in rigid body protein-protein docking",
            "year": 2020
        },
        {
            "authors": [
                "Richard Evans",
                "Michael O\u2019Neill",
                "Alexander Pritzel",
                "Natasha Antropova",
                "Andrew Senior",
                "Tim Green",
                "Augustin \u017d\u0131\u0301dek",
                "Russ Bates",
                "Sam Blackwell",
                "Jason Yim"
            ],
            "title": "Protein complex prediction with alphafold-multimer",
            "year": 2021
        },
        {
            "authors": [
                "Octavian-Eugen Ganea",
                "Xinyuan Huang",
                "Charlotte Bunne",
                "Yatao Bian",
                "Regina Barzilay",
                "Tommi Jaakkola",
                "Andreas Krause"
            ],
            "title": "Independent se (3)-equivariant models for end-to-end rigid protein docking",
            "venue": "arXiv preprint arXiv:2111.07786,",
            "year": 2021
        },
        {
            "authors": [
                "Mario Geiger",
                "Tess Smidt",
                "M Alby",
                "Benjamin Kurt Miller",
                "Wouter Boomsma",
                "Bradley Dice",
                "Kostiantyn Lapchevskyi",
                "Maurice Weiler",
                "Micha\u0142 Tyszkiewicz",
                "Simon Batzner"
            ],
            "title": "Euclidean neural networks: e3nn",
            "venue": "Zenodo. https://doi. org/10.5281/zenodo,",
            "year": 2020
        },
        {
            "authors": [
                "Emiel Hoogeboom",
                "V\u0131ctor Garcia Satorras",
                "Cl\u00e9ment Vignac",
                "Max Welling"
            ],
            "title": "Equivariant diffusion for molecule generation in 3d",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Sheng-You Huang",
                "Xiaoqin Zou"
            ],
            "title": "An iterative knowledge-based scoring function for protein\u2013 protein recognition",
            "venue": "Proteins: Structure, Function, and Bioinformatics,",
            "year": 2008
        },
        {
            "authors": [
                "Sheng-You Huang",
                "Xiaoqin Zou"
            ],
            "title": "A knowledge-based scoring function for protein-rna interactions derived from a statistical mechanics-based iterative method",
            "venue": "Nucleic acids research,",
            "year": 2014
        },
        {
            "authors": [
                "John Ingraham",
                "Max Baranov",
                "Zak Costello",
                "Vincent Frappier",
                "Ahmed Ismail",
                "Shan Tie",
                "Wujie Wang",
                "Vincent Xue",
                "Fritz Obermeyer",
                "Andrew Beam"
            ],
            "title": "Illuminating protein space with a programmable generative model",
            "year": 2022
        },
        {
            "authors": [
                "Arian R Jamasb",
                "Ben Day",
                "C\u0103t\u0103lina Cangea",
                "Pietro Li\u00f2",
                "Tom L Blundell"
            ],
            "title": "Deep learning for protein\u2013protein interaction site prediction",
            "venue": "Proteomics Data Analysis,",
            "year": 2021
        },
        {
            "authors": [
                "Bowen Jing",
                "Gabriele Corso",
                "Jeffrey Chang",
                "Regina Barzilay",
                "Tommi Jaakkola"
            ],
            "title": "Torsional diffusion for molecular conformer generation",
            "venue": "arXiv preprint arXiv:2206.01729,",
            "year": 2022
        },
        {
            "authors": [
                "Wolfgang Kabsch"
            ],
            "title": "A solution for the best rotation to relate two sets of vectors. Acta Crystallographica Section A: Crystal Physics, Diffraction",
            "venue": "Theoretical and General Crystallography,",
            "year": 1976
        },
        {
            "authors": [
                "Dima Kozakov",
                "David R Hall",
                "Bing Xia",
                "Kathryn A Porter",
                "Dzmitry Padhorny",
                "Christine Yueh",
                "Dmitri Beglov",
                "Sandor Vajda"
            ],
            "title": "The cluspro web server for protein\u2013protein docking",
            "venue": "Nature protocols,",
            "year": 2017
        },
        {
            "authors": [
                "Marc F Lensink",
                "Ra\u00fal M\u00e9ndez",
                "Shoshana J Wodak"
            ],
            "title": "Docking and scoring protein complexes: Capri 3rd edition",
            "venue": "Proteins: Structure, Function, and Bioinformatics,",
            "year": 2007
        },
        {
            "authors": [
                "Zeming Lin",
                "Halil Akin",
                "Roshan Rao",
                "Brian Hie",
                "Zhongkai Zhu",
                "Wenting Lu",
                "Allan dos Santos Costa",
                "Maryam Fazel-Zarandi",
                "Tom Sercu",
                "Sal Candido"
            ],
            "title": "Language models of protein sequences at the scale of evolution enable accurate structure prediction",
            "venue": "BioRxiv,",
            "year": 2022
        },
        {
            "authors": [
                "Efrat Mashiach",
                "Dina Schneidman-Duhovny",
                "Aviyah Peri",
                "Yoli Shavit",
                "Ruth Nussinov",
                "Haim J Wolfson"
            ],
            "title": "An integrated suite of fast docking algorithms. Proteins: Structure",
            "venue": "Function, and Bioinformatics,",
            "year": 2010
        },
        {
            "authors": [
                "Matthew McPartlon",
                "Jinbo Xu"
            ],
            "title": "Deep learning for flexible and site-specific protein docking and design",
            "year": 2023
        },
        {
            "authors": [
                "Emanuele Rodol\u00e0",
                "Zorah L\u00e4hner",
                "Alexander M Bronstein",
                "Michael M Bronstein",
                "Justin Solomon"
            ],
            "title": "Functional maps representation on product manifolds",
            "venue": "In Computer Graphics Forum,",
            "year": 2019
        },
        {
            "authors": [
                "Christina EM Schindler",
                "Isaure Chauvot de Beauch\u00eane",
                "Sjoerd J de Vries",
                "Martin Zacharias"
            ],
            "title": "Protein-protein and peptide-protein docking and refinement using attract in capri",
            "venue": "Proteins: Structure, Function, and Bioinformatics,",
            "year": 2017
        },
        {
            "authors": [
                "Dina Schneidman-Duhovny",
                "Yuval Inbar",
                "Ruth Nussinov",
                "Haim J Wolfson"
            ],
            "title": "Patchdock and symmdock: servers for rigid and symmetric docking",
            "venue": "Nucleic acids research,",
            "year": 2005
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Generative modeling by estimating gradients of the data distribution",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "arXiv preprint arXiv:2011.13456,",
            "year": 2020
        },
        {
            "authors": [
                "Freyr Sverrisson",
                "Jean Feydy",
                "Joshua Southern",
                "Michael M. Bronstein",
                "Bruno Correia"
            ],
            "title": "Physicsinformed deep neural network for rigid-body protein docking",
            "venue": "In ICLR2022 Machine Learning for Drug Discovery,",
            "year": 2022
        },
        {
            "authors": [
                "Nathaniel Thomas",
                "Tess Smidt",
                "Steven Kearnes",
                "Lusann Yang",
                "Li Li",
                "Kai Kohlhoff",
                "Patrick Riley"
            ],
            "title": "Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds",
            "venue": "arXiv preprint arXiv:1802.08219,",
            "year": 2018
        },
        {
            "authors": [
                "Raphael Townshend",
                "Rishi Bedi",
                "Patricia Suriana",
                "Ron Dror"
            ],
            "title": "End-to-end learning on 3d protein structure for interface prediction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Brian L Trippe",
                "Jason Yim",
                "Doug Tischer",
                "Tamara Broderick",
                "David Baker",
                "Regina Barzilay",
                "Tommi Jaakkola"
            ],
            "title": "Diffusion probabilistic modeling of protein backbones in 3d for the motifscaffolding problem",
            "venue": "arXiv preprint arXiv:2206.04119,",
            "year": 2022
        },
        {
            "authors": [
                "Ilya A Vakser"
            ],
            "title": "Protein-protein docking: From interaction to interactome",
            "venue": "Biophysical journal,",
            "year": 2014
        },
        {
            "authors": [
                "Thom Vreven",
                "Iain H Moal",
                "Anna Vangone",
                "Brian G Pierce",
                "Panagiotis L Kastritis",
                "Mieczyslaw Torchala",
                "Raphael Chaleil",
                "Brian Jim\u00e9nez-Garc\u0131\u0301a",
                "Paul A Bates",
                "Juan Fernandez-Recio"
            ],
            "title": "Updates to the integrated protein\u2013protein interaction benchmarks: docking benchmark version 5 and affinity benchmark version 2",
            "venue": "Journal of molecular biology,",
            "year": 2015
        },
        {
            "authors": [
                "Yumeng Yan",
                "Di Zhang",
                "Pei Zhou",
                "Botong Li",
                "Sheng-You Huang"
            ],
            "title": "Hdock: a web server for protein\u2013protein and protein\u2013dna/rna docking based on a hybrid strategy",
            "venue": "Nucleic acids research,",
            "year": 2017
        },
        {
            "authors": [
                "Yumeng Yan",
                "Huanyu Tao",
                "Jiahua He",
                "Sheng-You Huang"
            ],
            "title": "The hdock server for integrated protein\u2013protein docking",
            "venue": "Nature protocols,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Understanding how proteins structurally interact is crucial to modern biology, with applications in drug discovery and protein design. Recent machine learning methods have formulated protein-small molecule docking as a generative problem with significant performance boosts over both traditional and deep learning baselines. In this work, we propose a similar approach for rigid protein-protein docking: DIFFDOCK-PP is a diffusion generative model that learns to translate and rotate unbound protein structures into their bound conformations. We achieve state-ofthe-art performance on DIPS with a median C-RMSD of 4.85, outperforming all considered baselines. Additionally, DIFFDOCK-PP is faster than all search-based methods and generates reliable confidence estimates for its predictions.1"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Proteins realize their myriad biological functions through interactions with biomolecules, such as other proteins, nucleic acids, or small molecules. The presence or absence of such interactions is dictated in part by the geometric and chemical complementarity of participating bodies. Thus, learning how individual proteins form complexes is crucial to understanding protein activity. In this work, we focus on rigid protein-protein docking: given two protein structures, the goal is to predict their resultant complex while maintaining internal bonds, angles, and torsion angles fixed.\nTraditional approaches for rigid protein-protein docking consist of a search algorithm followed by a scoring function (Chen et al., 2003; De Vries et al., 2010; Yan et al., 2020). After enumerating a vast search space of potential poses, these methods rely on heuristics or empirical methods to select the most plausible poses. Due to the exhaustive search required, these methods are often slow and computationally expensive. More recently, deep learning approaches have tackled protein-protein docking as a regression problem: given two structures, directly predict the final pose (Ganea et al., 2021; Jamasb et al., 2021). While fast, these models have yet to outperform search-based algorithms.\nInspired by recent breakthroughs in protein-small molecule docking (Corso et al., 2022), we instead propose that protein-protein docking be formulated as a generative problem: given two proteins, the goal is to estimate the distribution over all potential poses using a diffusion generative model. To obtain the final docked pose, we sample from this distribution multiple times and select the best one via a learned confidence model, as shown in Figure 1. We call our method DIFFDOCK-PP.\nEmpirically, DIFFDOCK-PP achieves a top-1 median complex root mean square deviation (CRMSD) of 4.85 on the Database of Interacting Protein Structures (DIPS), outperforming all considered baselines. Compared to popular search-based docking software, DIFFDOCK-PP is 5 to 60 times faster on GPU.\n*Equal contribution 1Our code is publicly available at https://github.com/ketatam/DiffDock-PP\nar X\niv :2\n30 4.\n03 88\n9v 1\n[ q-\nbi o.\nB M\n] 8\nA pr\n2 02"
        },
        {
            "heading": "2 BACKGROUND AND RELATED WORK",
            "text": "Protein-Protein Docking. The goal of protein-protein docking is to predict the (bound) structure of a protein-protein complex based on the individual proteins\u2019 (unbound) structures. Specifically, we focus on the task of rigid body protein-protein docking, which assumes that the proteins do not undergo any deformations during binding, restricting their relative degrees of freedom to a rotation and translation in 3D space. This assumption is often realistic (Vakser, 2014) and even leads to improved results for most interacting proteins (Desta et al., 2020).\nTo evaluate the quality of the predicted structures, a common approach is to compute the fraction of those lying within some threshold distance to the true bound structure (Vakser, 2014; Basu & Wallner, 2016; Lensink et al., 2007).\nSearch-based Docking Methods. Traditional methods for protein-protein docking usually rely on the physical properties of the complexes (Chen et al., 2003; De Vries et al., 2010; Yan et al., 2020). These methods typically 1) generate an initial population of plausible complex structures, 2) further pose proposals using optimization algorithms, and 3) refine the complexes with the highest score according to some scoring function. Template-based modeling (TBM), which predicts the structure of a target protein by aligning it to one or multiple template proteins with known structures, is also used as a subroutine by some search-based methods (Vakser, 2014). While some of these methods offer decent predictive performance, they are usually computationally expensive and impractical for large-scale molecular screening campaigns.\nDeep Learning-based Docking Methods. Deep learning approaches to protein-protein docking can be broadly partitioned into two categories: single-step and multi-step methods. Single-step methods directly predict the complex structure in a one-shot fashion. Notably, Ganea et al. (2021) proposed EQUIDOCK, a pairwise-independent SE(3)-equivariant graph matching network that directly predicts the relative rigid-body transformation of one of the interacting proteins. Furthermore, Sverrisson et al. (2022) incorporated different physical priors into an energy-based model for predicting protein complex 3D structure. In contrast, multi-step methods produce their final predictions by iteratively refining a set of proposed structures. For instance, ALPHAFOLD-MULTIMER (Evans et al., 2021) was designed to co-fold multiple protein structures, given their primary sequences and multiple sequence alignments (MSAs) to evolutionary-related proteins. In parallel with this work, McPartlon & Xu (2023) proposed DOCKGPT, a generative protein transformer for flexible and site-specific protein docking.\nOur method, DIFFDOCK-PP, naturally falls into the category of multi-step methods due to the multiple steps required to sample from the distribution induced by diffusion generative models. Compared to search-based methods, however, we sample orders of magnitude fewer poses during our refinement process.\nDiffusion Generative Models (DGMs). DGMs offer a powerful way to represent probability distributions beyond likelihood-based and implicit generative models, circumventing many of their issues. The main idea is to define a diffusion process transforming the data distribution in a tractable prior and learn the score function, which is the gradient of the log probability density function \u2207x log pt(x)2, of this evolving distribution. We can then use the learned score function to sample from the underlying probability distribution using well-established algorithms (Song et al., 2020). A plethora of DGMs have been developed for tasks in computational biology, including conformer generation (Jing et al., 2022), molecule generation (Hoogeboom et al., 2022), and protein design (Trippe et al., 2022). The foundation of our method is DIFFDOCK (Corso et al., 2022), a diffusion generative model over the product space of the ligand\u2019s degrees of freedom (translational, rotational, and torsional). We extend this approach to the protein docking task."
        },
        {
            "heading": "3 METHOD",
            "text": ""
        },
        {
            "heading": "3.1 BENEFITS OF GENERATIVE MODELING FOR RIGID PROTEIN DOCKING",
            "text": "Protein-protein docking is often evaluated on the basis of thresholding (Basu & Wallner, 2016; Lensink et al., 2007), e.g., a Ligand-RMSD < 5 A\u030a and an Interface-RMSD < 2 A\u030a are among several conditions to consider a given prediction to be of medium quality in Lensink et al. (2007).\nFollowing the arguments of Corso et al. (2022) and noting that directly optimizing such thresholdingbased objectives is not feasible because they are not differentiable, we argue that these objectives are better aligned with training a generative model to maximize the likelihood of the observed structures than with fitting a regression model as done in previous work. Concretely, since real-world data, as well as complex deep learning models, suffer from inherent multi-modal uncertainty, regressionbased methods trained to predict a single pose that, in expectation, minimizes some MSE-type loss (Ganea et al., 2021) would learn to predict a structure as the weighted mean among many viable alternatives, often not a plausible structure itself. In contrast, a generative model would aim to capture the distribution over these alternatives resulting in more plausible, accurate, and diverse structures.\nTo illustrate this phenomenon, we visualize some of the structures predicted by our model and compare them to those generated by the baselines, especially EQUIDOCK, which was trained using an MSE-type loss and whose one of the main limitations is the existence of steric clashes in its predicted structures (Ganea et al., 2021). Figure 2 illustrates such predictions. We observe that our model predicts structures with no steric clashes, which we hypothesize is in part due to the adopted generative approach to protein-protein docking."
        },
        {
            "heading": "3.2 METHOD OVERVIEW",
            "text": "As defined in Section 2, in rigid protein-protein docking, we aim to predict the complex structure of an interacting protein pair based on the individual structure of each protein.\nIn this work, we model the proteins on the residue level, representing each protein as a set of amino acid nodes. Each residue is, in turn, represented by its type and the position of its \u03b1-carbon atom. We denote X1 \u2208 R3n as the ligand consisting of n residues and X2 \u2208 R3m as the receptor with m residues. The ligand/receptor assignment can, in principle, be arbitrary; however, we define the ligand X1 as the protein with fewer residues. This means that with X\u22171 \u2208 R3n and X\u22172 \u2208 R3m denoting the ground truth complex, the receptor is kept fixed X2 = X\u22172 and the task is to predict the structure of the ligand with respect to the receptor.\nIt is important to note that - since we are considering rigid-body protein docking - we only need to consider poses that can be obtained by a rigid-body transformation (i.e., a rotation and a translation) of the initial pose X1. These poses lie on a 6-dimensional submanifoldM \u2282 R3n corresponding\n2Note that this is a very different concept from the scoring function of search-based docking methods.\nto the 6 degrees of freedom introduced by the rotation and translation in 3D space. We consider rigid-body protein-protein docking as the task of learning a probability distribution p(X1|X2) of ligand poses inM, conditioned on the receptor structure X2. Now we discuss how we can effectively deploy DGMs to learn this probability distribution. In order to avoid the inefficiencies arising from learning DGMs on arbitrary submanifolds (De Bortoli et al., 2022), we use the framework of intrinsic diffusion models Corso (2023): map the extrinsic submanifoldM to the intrinsic manifold defined by the product space of the rotation and translation group and define our DGM over this manifold."
        },
        {
            "heading": "3.3 DIFFUSION PROCESS",
            "text": "To formalize the discussion in the previous section, let us introduce the 3D translation group T(3) and the 3D rotation group SO(3) as well as their product space P = T(3) \u00d7 SO(3). With this, we can define a rigid-body transformation as the mapping A : P\u00d7 R3n \u2192 R3n with\nA((r, R),x)i = R (xi \u2212 x) + x+ r, (1)\nwhere xi \u2208 R3 corresponds to the position of the i-th residue and x represents the center of mass of the ligand protein. This equation simply describes a rotation around the center of mass followed by a translation.\nThe submanifold of ligand poses introduced informally in the previous section can now be described using this transformation asM = {A ((r, R) ,X1) | (r, R) \u2208 P}. For similar arguments to Corso et al. (2022), the map A (\u00b7,X1) : P \u2192 M is a bijection, which guarantees the existence of the inverse map. We can therefore develop a diffusion process over the product space P to generate a distribution on the manifold.\nGiven that P is a product manifold, we can define a forward diffusion process independently on each manifold (Rodola\u0300 et al., 2019) with the score as an element of the corresponding tangent space (De Bortoli et al., 2022). A score model can then be trained with denoising score matching (Song & Ermon, 2019). In both groups, we define the forward SDE as dx = \u221a d\u03c32 (t) /dt dw, where \u03c32 is \u03c32tr for T(3) and \u03c32rot for SO(3) and dw denotes the corresponding Brownian motion. The reader is referred to Corso et al. (2022) for a description of how we can sample from and compute the score of the diffusion kernel on each of these groups."
        },
        {
            "heading": "3.4 MODEL ARCHITECTURE",
            "text": "Both the score and confidence models are based on SE(3)-equivariant convolutional networks (Thomas et al., 2018; Geiger et al., 2020) adapted from DIFFDOCK\u2019s architecture (Corso et al., 2022) to mainly account for: i.) the symmetry of protein-protein pairs, and ii.) the rigidity assumption of the proteins. Below we summarise the main components of the architecture.\nInput representation. Protein structures are represented as heterogeneous geometric graphs with the amino acid residues as nodes. Node features comprise the residue\u2019s type, the positions of its \u03b1-carbon atoms, as well as language model embeddings trained on protein sequences from ESM2 (Lin et al., 2022). In order to construct the edges, we connect each node to its 20 nearest neighbors from the same protein (intra-edges), and we use a dynamic cutoff distance of (40 + 3 * \u03c3tr)A\u030a, where \u03c3tr is the current standard deviation of the diffusion translational noise to connect the nodes from different proteins (cross-edges). The intuition behind using a dynamic cutoff distance is to increase the chances that each node interacts with potentially relevant nodes from the other protein even when the proteins are still far apart (at early diffusion steps) while having a lower computational cost than using a fixed higher cutoff distance (especially at later diffusion steps).\nIntermediate layers. After an initial set of embedding layers to process the initial features, the diffusion time, and the edge lengths, we define a different set of convolutional layers for each edge type (intra-edges and cross-edges). However, in contrast to Corso et al. (2022), we use the same intra-edge layers for both proteins to account for symmetry.\nOutput layers. This is where the main difference between the score and confidence model lies. On the one hand, the score model applies a tensor-product convolution placed at the center of mass of the ligand to produce two SE(3)-equivariant 3-dimensional vectors as the translational and rotational scores (lying in the tangent space of the respective manifolds). On the other hand, the confidence model applies a fully connected layer on the mean-pooled scalar representations from the last convolution layer to produce the SE(3)-invariant confidence value."
        },
        {
            "heading": "3.5 TRAINING AND INFERENCE",
            "text": "The training and inference regimes follow very closely Corso et al. (2022). We reiterate the most important points here.\nDiffusion model. Even though the diffusion kernel and score matching objectives were defined on the product space P, we follow the extrinsic-to-intrinsic framework (Jing et al., 2022) and develop the training and inference procedures directly on ligand-receptor poses in 3D space. This allows the model to reason about physical interactions more easily and should lead to better generalization. Another interesting point to note is that each training example (X1,X2) is the only available sample from the conditional distribution p(\u00b7|X2). This is unlike the standard generative modeling setting, where many samples are drawn from the same data distribution. Therefore, during training, we iterate over distinct conditional distributions with only one sample. During inference, in order to avoid the problem of overdispersed distributions typically observed with generative models, we use low-temperature sampling (Ingraham et al., 2022), which allows the model to concentrate on modes with high likelihood.\nConfidence model. The confidence model is a simple classification network trained to predict whether the structures sampled from the score model are of \u201dgood\u201d quality, which we define as the structure having an L-RMSD below a certain threshold. To this end, we collect the training data for the confidence model by sampling from the (trained) diffusion model multiple times for each training complex and computing the L-RMSD for the sampled complexes. The labels are then generated by simply comparing the L-RMSD values to the threshold. In our experiments, we set the threshold to be 5A\u030a. The confidence model is then trained with cross-entropy loss.\nCombined Inference. During inference, we sample a set of candidate poses from the diffusion model. These samples are then ranked by the confidence model according to the predicted confidence value of whether each pose has an L-RSMD below 5A\u030a. The final prediction is the pose with the highest confidence score."
        },
        {
            "heading": "4 EXPERIMENTAL SETUP",
            "text": "We evaluate our model on the Database of Interacting Protein Structures (DIPS) (Townshend et al., 2019). DIPS consists of 42,826 binary protein complexes. We use the same protein-family-based dataset split proposed by Ganea et al. (2021).\nDIFFDOCK-PP has 1.62M parameters and was trained on the DIPS training set for 170 epochs. Every 10 epochs, we run reverse diffusion on the DIPS validation set to compute L-RMSD values. The best model obtained with this procedure is finally tested on the DIPS test set. During training and inference, the smaller protein is selected as a ligand, and we randomly rotate and translate the ligand in space before running our model.\nWe compare our method to search-based docking algorithms CLUSPRO (PIPER) (FFT-based) (Desta et al., 2020; Kozakov et al., 2017), ATTRACT (based on a coarse-grained force field) (Schindler et al., 2017; de Vries et al., 2015), PATCHDOCK (based on shape complementarity principles) (Mashiach et al., 2010; Schneidman-Duhovny et al., 2005), and HDOCK (makes use of template-based modeling and ab initio free docking) (Yan et al., 2020; 2017; Huang & Zou, 2014; 2008). For these baselines, we cannot control their training and testing data. This implies that some of them might have used a part of our test set to train or validate their models. Thus, the reported performance of these methods might be overestimating the true performance.\nFurthermore, we compare our method to deep learning baselines ALPHAFOLD-MULTIMER (Evans et al., 2021) and EQUIDOCK (Ganea et al., 2021). Details on how we evaluated these baselines can be found in Appendix A.\nTo ensure a fair comparison, we follow the evaluation scheme proposed by Ganea et al. (2021). All models were evaluated using complex root mean square deviation (C-RMSD) and interface root mean square deviation (I-RMSD). C-RMSD is determined by superimposing the ground truth and predicted complex structures via the Kabsch algorithm (Kabsch, 1976) and computing the RMSD between all C-\u03b1 coordinates. I-RMSD is determined by similarly aligning the interface residues of both complexes and computing the RMSD over interface C-\u03b1 coordinates (within 8A\u030a of the binding partner)."
        },
        {
            "heading": "5 RESULTS",
            "text": "Table 1 reports the performances of the different methods on the DIPS test set. DIFFDOCK-PP achieves a C-RMSD median of 4.85, in line with HDOCK and significantly outperforming all other baselines. This performance is further confirmed when looking at the percentage of predictions below a specific threshold with DIFFDOCK-PP achieving respectively 42% and 45% of C-RMSD and I-RMSD below 2 A\u030a.\nWhen limited to generating one sample for each complex, DIFFDOCK-PP still outperforms the majority of the baselines while having a significantly lower runtime than the search-based methods. Ensuring computational efficiency without a significant drop in performance is critical for computational screening applications like drug discovery and antibody design, where one needs to analyze a very large number of complexes.\nWe note that when we evaluate the model in such a way that we choose the complex with the smallest RMSD from the ones generated by the model, the performance exceeds that of all baselines by a large margin. As such, the performance of DIFFDOCK-PP can be significantly boosted by improving the used confidence model or designing more effective ranking methods. Additionally, this regime can be particularly beneficial for applications where it is desirable to have at least one high-quality recommendation among the predicted suggestions, e.g., if a practitioner is interested in discovering a single good structure based on prior knowledge from a set of proposals where the diversity of the proposals is important.\nFigure 3 shows, for all the considered methods, the fraction of predictions having a C-RMSD value below different thresholds ranging from 0 to 10. DIFFDOCK-PP outperforms most baselines for all threshold values, and outperforms HDOCK for thresholds higher than 5.\nTo evaluate how good our confidence model is, we plot in Figure 4-left the top-1 performance for different numbers of generated samples (this is done by picking the sample with the highest confi-\ndence score according to the confidence model) and compare it to the performance when we ignore the confidence score and instead pick the sample than minimizes the C-RMSD out of the generated samples. Performance here is defined as the fraction of samples having a C-RMSD below 5A\u030a.\nWhile the top-1 performance consistently increases with increasing number of generated samples, there still is a significant gap behind the performance in the perfect selection regime. This points out that DIFFDOCK-PP can achieve significantly better performance by improving the current confidence model or by developing a more involved selection algorithm.\nThe right plot of Figure 4 illustrates the selective accuracy of our model if we consider only complexes with confidence predictions above a certain threshold. We do this by ordering the final predicted complex structures by their assigned confidence prediction for all 100 complexes and removing the complexes with the lowest confidence score one by one. By removing predictions with lower confidence, we see an increase in the success rate of our prediction model.\nWe note that due to time constraints, it was not possible to properly evaluate our method on the Docking Becnhmark 5.5 (DB5.5) dataset Vreven et al. (2015) and leave it as an interesting future extension to this work."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We present DIFFDOCK-PP, a diffusion generative model for rigid protein-protein docking. Our approach is inspired by recent advancements in molecular docking (Corso et al., 2022), which tackles docking via a generative model over ligand poses. DIFFDOCK-PP outperforms existing deep learning models and performs competitively against search-based methods at a fraction of their computational cost. The effectiveness of our simple approach paves the way for further investigation into deep learning for modeling biomolecular interactions."
        },
        {
            "heading": "7 ACKNOWLEDGEMENTS",
            "text": "This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. 1745302. It is also supported by the Machine Learning for Pharmaceutical Discovery and Synthesis (MLPDS) consortium, the Abdul Latif Jameel Clinic for Machine Learning in Health, the DTRA Discovery of Medical Countermeasures Against New and Emerging (DOMANE) threats program, the DARPA Accelerated Molecular Discovery program and the Sanofi Computational Antibody Design grant. The authors thank Dr. Ricardo Acevedo Cabra and Prof. Dr. Massimo Fornasier for providing the opportunity to work on this project as part of the TUM Data Innovation Lab."
        },
        {
            "heading": "A BASELINES: EXPERIMENTAL DETAILS",
            "text": "In this section we give some details on how we evaluated the baselines to which we compared our method.\nFor HDOCK, we used the HDOCKlite package that can be downloaded from http://huanglab.phys.hust.edu.cn/software/hdocklite/.\nFor EQUIDOCK, we used the official implementation found in https://github.com/octavian-ganea/equidock public.\nFor ALPHAFOLD-MULTIMER, we first extracted FASTA sequences from the complexes using the residue names given in their respective PDB files. Each complex yielded one FASTA file with two chains. Then, we ran AlphaFold-Multimer v2.3.0 using the official implementation of the inference pipeline provided in https://github.com/deepmind/alphafold. This implementation utilizes the following datasets: BFD, MGnify, PDB70, PDB (structures in the mmCIF format), PDB seqres, UniRef30 (formerly UniClust30), UniProt, and UniRef90.\nFor ATTRACT, CLUSPRO and PATHDOCK, we used the results reported by Ganea et al. (2021)."
        }
    ],
    "title": "DIFFDOCK-PP: RIGID PROTEIN-PROTEIN DOCKING WITH DIFFUSION MODELS",
    "year": 2023
}