{
    "abstractText": "The UK offshore wind industry is rapidly growing to meet CO2 emission targets. However, the main drawback of the offshore environment is the increased cost of maintenance. Artificial Neural Networks (ANN) show great potential to reduce this cost. Long Short-Term Memory (LSTM) is a form of Recurrent Neural Network (RNN) that shows promising results in solving time series-based problems, making them ideally suited for wind turbine condition monitoring. A dedicated circuit for a LSTM-based ANN that uses memristors will allow for more power efficient and faster computation, whilst being able to overcome the von Neumann bottleneck.",
    "authors": [
        {
            "affiliations": [],
            "name": "Harry Burton"
        },
        {
            "affiliations": [],
            "name": "Jean-Sebastien Bouillard"
        }
    ],
    "id": "SP:b3eaa35f5aa98364ec26f8ac31433064f4d545e3",
    "references": [
        {
            "authors": [
                "L Bergstr\u00f6m",
                "L Kautsky",
                "T Malm",
                "R Rosenberg",
                "M Wahlberg",
                "N\u00c5 Capetillo"
            ],
            "title": "Effects of offshore wind farms on marine wildlife\u2014a generalized impact assessment",
            "venue": "Environmental Research Letters",
            "year": 2014
        },
        {
            "authors": [
                "P Mazidi",
                "Y Tohidi",
                "MA. Sanz-Bobi"
            ],
            "title": "Strategic maintenance scheduling of an offshore wind farm in a deregulated power system",
            "year": 2017
        },
        {
            "authors": [
                "J Helsen",
                "C Devriendt",
                "W Weijtjens",
                "P. Guillaume"
            ],
            "title": "Condition monitoring by means of scada analysis. In: Proceedings of European wind energy association international conference Paris",
            "year": 2015
        },
        {
            "authors": [
                "P Tchakoua",
                "R Wamkeue",
                "M Ouhrouche",
                "F Slaoui-Hasnaoui",
                "TA Tameghe",
                "G. Ekemb"
            ],
            "title": "Wind turbine condition monitoring: State-of-the-art review, new trends, and future challenges",
            "year": 2014
        },
        {
            "authors": [
                "K Chellapilla",
                "S Puri",
                "P. Simard"
            ],
            "title": "High performance convolutional neural networks for document processing",
            "venue": "Tenth international workshop on frontiers in handwriting recognition",
            "year": 2006
        },
        {
            "authors": [
                "P Chi",
                "S Li",
                "C Xu",
                "T Zhang",
                "J Zhao",
                "Y Liu"
            ],
            "title": "Prime: A novel processing-in-memory architecture for neural network computation in reram-based main memory",
            "venue": "ACM SIGARCH Computer Architecture News",
            "year": 2016
        },
        {
            "authors": [
                "M Hu",
                "JP Strachan",
                "Z Li",
                "EM Grafals",
                "N Davila",
                "C Graves"
            ],
            "title": "Dot-product engine for neuromorphic computing: Programming 1T1M crossbar to accelerate matrix-vector multiplication",
            "year": 2016
        },
        {
            "authors": [
                "A Shafiee",
                "A Nag",
                "N Muralimanohar",
                "R Balasubramonian",
                "JP Strachan",
                "M Hu"
            ],
            "title": "ISAAC: A convolutional neural network accelerator with in-situ analog arithmetic in crossbars",
            "venue": "ACM SIGARCH Computer Architecture News",
            "year": 2016
        },
        {
            "authors": [
                "T Gokmen",
                "Y. Vlasov"
            ],
            "title": "Acceleration of deep neural network training with resistive cross-point devices: Design considerations",
            "venue": "Frontiers in neuroscience",
            "year": 2016
        },
        {
            "authors": [
                "L. Chua"
            ],
            "title": "Memristor-the missing circuit element",
            "venue": "IEEE Transactions on circuit theory",
            "year": 1971
        },
        {
            "authors": [
                "O \u0160uch",
                "M Klimo",
                "N Kemp",
                "O. \u0160kvarek"
            ],
            "title": "Passive memristor synaptic circuits with multiple timing dependent plasticity mechanisms",
            "venue": "AEU- International Journal of Electronics and Communications",
            "year": 2018
        },
        {
            "authors": [
                "J Liu",
                "M. Brooke"
            ],
            "title": "A fully parallel learning neural network chip for realtime control. In: IJCNN\u201999",
            "venue": "International Joint Conference on Neural Networks. Proceedings (Cat. No. 99CH36339)",
            "year": 1999
        },
        {
            "authors": [
                "G. Snider"
            ],
            "title": "Molecular-junction-nanowire-crossbar-based neural network",
            "venue": "Google Patents;",
            "year": 2008
        },
        {
            "authors": [
                "Smagulova K",
                "James AP"
            ],
            "title": "A survey on LSTM memristive neural network architectures and applications",
            "venue": "The European Physical Journal Special Topics",
            "year": 2019
        },
        {
            "authors": [
                "S Hocheriter",
                "J. Schmidhuber"
            ],
            "title": "Long Short-Term Memory",
            "venue": "Neural Comp",
            "year": 1997
        },
        {
            "authors": [
                "FA Gers",
                "J Schmidhuber",
                "F. Cummins"
            ],
            "title": "Learning to forget: Continual prediction with LSTM",
            "venue": "Neural computation",
            "year": 2000
        },
        {
            "authors": [
                "ZC Lipton",
                "J Berkowitz",
                "C. Elkan"
            ],
            "title": "A critical review of recurrent neural networks for sequence learning",
            "venue": "arXiv preprint arXiv:150600019",
            "year": 2015
        },
        {
            "authors": [
                "I Sutskever",
                "O Vinyals",
                "QV. Le"
            ],
            "title": "Sequence to sequence learning with neural networks. Advances in neural information processing",
            "year": 2014
        },
        {
            "authors": [
                "H Kim",
                "T Kim",
                "J Kim",
                "JJ. Kim"
            ],
            "title": "Deep neural network optimized to resistive memory with nonlinear current-voltage characteristics",
            "venue": "ACM Journal on Emerging Technologies in Computing Systems (JETC)",
            "year": 2018
        },
        {
            "authors": [
                "X Zou",
                "S Xu",
                "X Chen",
                "L Yan",
                "Y. Han"
            ],
            "title": "Breaking the von Neumann bottleneck: architecture-level processing-in-memory technology",
            "venue": "Science China Information Sciences",
            "year": 2021
        },
        {
            "authors": [
                "S. Mittal"
            ],
            "title": "A survey of ReRAM-based architectures for processingin-memory and neural networks. Machine learning and knowledge extraction",
            "year": 2018
        },
        {
            "authors": [
                "Marco BG"
            ],
            "title": "Microelectronic neural systems: Analog VLSI for perception and cognition",
            "venue": "PhD thesis;",
            "year": 1998
        },
        {
            "authors": [
                "Sharifi MJ",
                "Banadaki YM"
            ],
            "title": "General spice models for memristor and application to circuit simulation of memristor-based synapses and memory cells",
            "venue": "Journal of Circuits, Systems, and Computers",
            "year": 2010
        },
        {
            "authors": [
                "Y Ou",
                "KE Tatsis",
                "VK Dertimanis",
                "MD Spiridonakos",
                "EN. Chatzi"
            ],
            "title": "Vibration-based monitoring of a small-scale wind turbine blade under varying climate conditions. Part I: An experimental benchmark",
            "venue": "Structural Control and Health Monitoring",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014LSTM, Long-Short Term Memory, ANN, Offshore wind, Memristor.\nI. INTRODUCTION\nOffshore wind farms in the United Kingdom are expected to produce a total of 40 GW by 2030 [1] and have ambitious goals of reaching 90 GW by 2050 [2]. Wind farms are often placed offshore due to the more stable and strong wind conditions. [3]. The offshore environment however has its downfalls, the main of which is the increased cost of maintenance for the turbines. This can cost around 25% of the total cost of the turbine. [4]. One of the ways in which this cost can be reduced is through the use of Condition Monitoring (CM). CM for a wind turbine is the process of observing the components of the turbine to identify any changes in the operation that would be indicative of a fault developing. Robust CM methods can lead to a reduction of costs in maintenance due to a reduction in down time [5] [6].\nRecently, the use of Artificial Neural Networks (ANN\u2019s) for CM has been gaining significant interest due to their ability to solve highly complex nonlinear problems [7]. The future expansion of ANN\u2019s depends on the expansion of network depth, which will require a vast amount of vector-matrix multiplications. Large and deep neural networks have been able to handle complex tasks using extensive amount of data with the advent of vector-matrix multiplication acceleration based on the use of graphics processing using (GPU\u2019s) [8]. Despite the advantage that GPU\u2019s offer of highly parallel computing that is suitable for ANN\u2019s, the high power consumption of a GPU is an obstacle that needs to be improved upon, [9]\u2013[12] which prohibits the use for offshore wind.\nThe memristor (or memory-resistor) was first proposed in 1971 [13] as a two-terminal device that exhibits non-volatile\nmemory whilst also being able to be scaled down to the nanometer range [14]. The use of a memristor as an in-memory compute unit for making smaller and more power efficient circuits that are dedicated hardware for ANN\u2019s [15] has recently been gaining significant interest [16]. Such systems are particularly required in remote sensing applications, where there is a need for high performance computing with low energy usage. In the case of wind turbine blades, there is a substantial cost benefit in not having to source energy from the main turbine, so a low-power computing engine, either battery or solar-powered would be advantageous."
        },
        {
            "heading": "II. BACKGROUND INFORMATION",
            "text": ""
        },
        {
            "heading": "A. Long Short-Term Memory (LSTM)",
            "text": "The recent rapid increase in volume of data from areas such as the Internet of Things (IoT) has led to the need for a more sophisticated form of near-edge smart memory and information processing. [17] One commonly used and efficient tool that is used for real-time contextualisation of information is the recurrent neural network (RNN). Unlike the classical feed-forward networks, a RNN has a feedback connection between nodes and layers that allow for the input of sequences and to predict the future of said sequence. However, the training of a simple RNN can become a difficult task due to this feedback connection. The algorithms used for the weight updates in RNN\u2019s are typically gradient descent based, which for these networks can give rise to an exploding or vanishing gradient during the training process. One way in which this issue has been overcome is through the use of gates to control this feedback loop, which has lead to the development of the Long Short-Term Memory (LSTM) cell.\nFigure 1a shows the architecture for a generic LSTM cell cj [18]. The heart of the memory block is a self-contained linear unit sc also referred to as the constant error carousel (CEC), which protects the LSTM cell from vanishing and exploding gradient problems that are typically associated with RNNs. The input and output gates here consist of activation functions, along with the corresponding weight matrices. The input gate\u2019s with weights netin and output yin is capable of blocking irrelevant data from entering the LSTM cell. Similarly, the output gate with its weights netout and output yout shapes the output of the cell yc. Finally, the forget gate\nhighlighted by the dashed line in Figure 1a, which allows the cell to forget or erase information [19]. It can therefore be concluded that the LSTM cell consists of an input layer, an output layer and a self contained hidden layer that is controlled by a forget gate. The mathematical description of an LSTM cell\u2019s output for the jth cell at time t can be given by\nycj (t) = youtj (t)h(scj (t)) (1)\nwhere scj (t) is an internal state of the cell, and y \u03c6 is an output for the forget gate, given by\nscj (t) = sscj (t\u2212 1) + yinj (t)g(netcj (t)) (2)\ny\u03c6j (t) = f\u03c6j (netcj (t)) (3)\nSince the first proposal of the LSTM cell there have been a variety of different configurations proposed, with other notations for the cell description [20] [21] the most frequent of which is shown in Figure 1b.\nB. Issues with Current Computing\nIn general, ANN have a wide range of applications, with their advancement depending heavily on networks becoming larger with more neurons and layers, and an ever-increasing number of matrix multiplications [22]. The use of graphics processing units (GPU\u2019s) has allowed for large and deep neural networks to handle more complex tasks with the use of vast amounts of data [8]. However, the GPU still follows the Von Neumann architecture of classical computers, with separate memory and compute units. This leads to a memory wall problem, or Von Neumann bottle neck, which is mainly due to three aspects [23]: (1) data movement between the processing unit and the memory has a non-negligible latency, (2) data movement in memory hierarchies is limited by bandwidth and (3) a high energy consumption [24]. Furthermore, even\nwith GPU acceleration, a CPU is still needed which can only fetch either data or execute new instruction at any given time, further limiting throughput. This leads to the Von Neumann bottleneck becoming more of a problem as processing speed and memory size increase. One recent way of overcoming this issue is through the use of emerging non-volatile memory (NVM) technologies, more specifically the memristor."
        },
        {
            "heading": "C. Memristor",
            "text": "The memory resistor, or memristor, was first proposed by Leon Chua in 1971 [13] to be the missing link between flux and charge. Recently, the use of memristors for synapse in artificial neural networks has been gaining significant attention [16] [25]. Analogue neural network chips are most commonly made from CMOS technology [15] [26], which have their limitations. A synapse in a neural network must be able to perform three tasks, namely store the weight associated with the synapse, apply this weight to a signal passing through the synapse, and update this weight. A single memristor is able to perform all three of these tasks.\nCompared to other implementation of synapses, the memristor has a significant increase in operating speeds. That is, a memristors state can be measured by flowing a current through the device with a low voltage such that the weight of the memristor does not change state. When operating like this, the memristor will appear as a simple resistor, resulting in the device having much higher operating frequencies. It is in this way that the memristor can act as a linear multiplier for a synapse in a neural network [27]. Figure 1d) shows how a memristor crossbar array will be implemented as the synapse for the final network, with the rows correlating to the input from the previous layer and the columns to outputs for this layer."
        },
        {
            "heading": "III. PRELIMINARY RESULTS",
            "text": "To be able to make the final memristor based neuromorphic circuit, the architecture for this specific task must first be determined. An initial computational model will be investigated to determine the optimum structure for the task of classifying and predicting faults in a wind turbines blade. This task can be broken down into two parts, the first being a classifier and the second being able to predict future evolution of faults. In order to investigate the appropriate network for these tasks, a suitable data set is required to train and test such networks. The computational networks in this paper were all made with pytorch, whilst the training of these networks used the Adam optimizer, and a Mean Squared Error loss (MSELoss) function."
        },
        {
            "heading": "A. The Data",
            "text": "The data used to train the networks was from Ou et al [28]. In these experiments, a 1.75 m long healthy (i.e. no faults present) wind turbine blade was secured in frame and placed in a humidity controlled environment. The blade was then vibrated and measurements were collected across the blade using a range of different strain gauges and accelerometer as shown in Figure 2. The blade was then subject to a few\ndifferent conditions to simulate some different fault cases. First, an increasing amount of mass was added to the blade at the loading position to simulate an increasing amount of icing on the blade. Finally, some cuts were introduced to the blade to simulate cracking. These cracks were all 4 mm deep 1.5 mm wide, with a varying length. The summary of these faults, along with their respective label, can be seen in Table I. This experiment was conducted using one sine wave as the driver for the vibration and four white noise cases for this driver. This data, whilst not exactly operational data, is open source and freely available to use, which allows for a controlled test with known faults present in the blade."
        },
        {
            "heading": "B. The Classifier",
            "text": "To classify faults present on the blade a fully connected network was used. This consisted of an input layer with 26 nodes (equal to the number of inputs from each time step in the data), two hidden layers with 50 nodes each, and an output layer with 12 nodes which corresponds to the number of fault ID\u2019s present in the data set. The maximum value for these output nodes was taken to be the classification that the network was predicting, e.g. if the 1st node was the maximum value, this would correspond to the network predicting that the fault ID was 0, if the 2nd node was the maximum value, this would correspond to the network predicting that the fault ID was 1 etc. The nature of these networks means that only a single time step can be used as an input, resulting in the training process being different from the final networks. For this experiment, only the four white noise cases of the data were used. Here, the data was split into time steps such that each point contained\nthe information from all the sensors at an individual time step. This data was then scaled using Equation 4, where x is the input data point, u is the mean of the sample, and s is the standard deviation. This scaling was done to reduce the effect that outliers in the data have on the training process.\nz = (x\u2212 u)/s (4)\nThis was then further split into a test set and a train set, containing 10% and 90% of the overall volume of the data respectively. This data was then used to train the classifier network. Here, batches of the data were fed into the network for a prediction to be made. The error between this prediction and the correct values were then calculated and back-propagated through the network. The results of which can be seen in Figure 3. For this network only a single epoch was needed to reach an accuracy of 100%, whilst it can be confidently concluded that the network did not over fit to the data since the Test line on the plot followed the same trend as the train set and did not deviate. It can therefore be concluded that a simple linear network such as this can be used to accurately classify the faults that are present in the data set used here."
        },
        {
            "heading": "C. The Predictor",
            "text": "The classifier network, whilst good at classifying faults, had no way of taking in a time series, or being able to make classifications/predictions on the future evolution of the blade, and so while being a good tool to implement for detecting faults in the blade, this network would have no use in the predictive maintenance of the blade by itself since no predictions of the future could be made. As such, the LSTM cell was added to the network to allow for a time series to be input to the network and the future of the blade to be predicted. As previously mentioned, the training process for this network differed from that of the simple classifying network since a time series can now be used as an input. To allow for this, some manipulation of the data was needed since in the data, only static faults were present. That is, a fault was introduced to the blade, then the vibrational analysis was done, and then the next experiment was performed. Initially, the icing fault was explored since one experiment will have had only a small impact on the overall health of the blade. Here, a section of\nthe healthy blade (case R) was used at the start, then a section of light icing (case A) was added with 86400s added to each time step to simulate a day later where some icing had started to form. This was then repeated with the slightly higher icing (case B) and again with the heaviest icing (case C). A further two samples of case C were added, then back down the levels of icing to a blade with no ice in order to create a set of data that consisted of a blade that undergoes a period of ice forming on the blade and then melting, the results of which can be seen by the black line in Figure 5. This was repeated for each of the white noise cases, and then split with one of the white noise cases being for testing the networks and the rest being for training. The data was also scaled using Equation 4.\nThe structure of the LSTM based network can be seen in Figure 1c. Here, an LSTM cell is used as the input for the network, there are then two hidden layers each with 50 nodes and an output layer with 12 nodes. Similar to the classifier, the output node with the highest value was taken to be the networks prediction for that time step.\nThe training data was then fed into the network as a whole set, and the network was made to make a prediction. The error was then calculated and back propagated through the network. This was then repeated for a much higher number of epochs than the previous example since here no batches were used. The results of this training can be seen in Figure 4. Here, it can be said that the network had started to learn the patterns in the data after fifty Epochs since the accuracy in both the test and train data sets were high. It can also be fairly confidently said that the network was not over fitting to the data since the validation curve was close to the train curve.\nThis LSTM based model was then evaluated using the test set of data that was not used for the training process, the results of which can be seen in Figure 5. Here, the true fault ID has been plotted vs time in black to be able to compare the predictions to. The network was given the first two thirds of the data to classify (plotted in blue) and was made to predict the future of the blade (plotted in red), and compare the the true values. This network appears to mostly be able to accurately classify the fault ID at each day, but appears to get the start of each day incorrect and then correct itself to predict the correct value. Similarly, the first day of the future prediction\nappears to get the initial value incorrect, then correctly predict the fault ID for the rest of the day. The rest of the future predictions however are not correct and the network appears to get stuck predicting a fault ID of 2, i.e. the moderate icing case. This result shows that while the network is able to make short term accurate predictions, more work is needed to discover the correct architecture for this task.\nSuch an architecture will then be replicated using a neuromorphic approach based on the memristor, more specifically through the use of memristors. This will consist of using memristor crossbar arrays to accelerate the vector matrix multiplications in the LSTM cell [17], and to fully recreate the linear layers of the rest of the network. Here, the biases in the linear layers, or the weight matrix for the LSTM cell, are mapped to the memristor states, the inputs for the LSTM cell and the linear layers correlate to the input voltages applied across the memristor crossbar array columns. For the calculation to take place, the voltage is applied and a current flows through the memrsitors, and get summed through Kirchhoff\u2019s law to the resulting output along each column in the crossbar array. The result of the multiplication is therefore a vector of currents through the columns in the array."
        },
        {
            "heading": "IV. CONCLUSION",
            "text": "The rapidly expanding offshore wind industry in the UK has a dire need for a more cost effective maintenance plan that utilises condition monitoring the enable the optimal scheduling of repairs and general maintenance in order to reduce costs. The use of a LSTM based ANN for this is showing promising results in classifying and predicting the future evolution of faults in the blade of wind turbines. These models however are computationally made on generic computers, meaning large and power inefficient circuits are needed. The development of a dedicated neuromorphic circuit would greatly reduce the size and power consumption of such a circuit, but using commonly used CMOS based components would still encounter the Von Neumann bottleneck. The use of a memristor in such a circuit would allow for in memory computing, overcoming such a bottleneck. This work highlights the promising results in the appropriate architecture for such a circuit, and gives a suitable application for memristors as synapses in an ANN\u2019s."
        }
    ],
    "title": "Memristor-based LSTM neuromorphic circuits for offshore wind turbine blade fault detection",
    "year": 2023
}