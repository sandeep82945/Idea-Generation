{
    "abstractText": "The estimation of the amount of uncertainty featured by predictive machine learning models has acquired a great momentum in recent years. Uncertainty estimation provides the user with augmented information about the model\u2019s confidence in its predicted outcome. Despite the inherent utility of this information for the trustworthiness of the user, there is a thin consensus around the different types of uncertainty that one can gauge in machine learning models and the suitability of different techniques that can be used to quantify the uncertainty of a specific model. This subject is mostly non existent within the traffic modeling domain, even though the measurement of the confidence associated to traffic forecasts can favor significantly their actionability in practical traffic management systems. This work aims to cover this lack of research by reviewing different techniques and metrics of uncertainty available in the literature, and by critically discussing how confidence levels computed for traffic forecasting models can be helpful for researchers and practitioners working in this research area. To shed light with empirical evidence, this critical discussion is further informed by experimental results produced by different uncertainty estimation techniques over real traffic data collected in Madrid (Spain), rendering a general overview of the benefits and caveats of every technique, how they can be compared to each other, and how the measured uncertainty decreases depending on the amount, quality and diversity of data used to produce the forecasts.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ibai La\u00f1aa"
        },
        {
            "affiliations": [],
            "name": "Ignacio (I\u00f1aki) Olabarrietaa"
        },
        {
            "affiliations": [],
            "name": "Javier Del Sera"
        }
    ],
    "id": "SP:6ae1b569af7130859f37f7d27b02b1ce9abf167b",
    "references": [
        {
            "authors": [
                "I. La\u00f1a",
                "J. Del Ser",
                "M. Velez",
                "E.I. Vlahogianni"
            ],
            "title": "Road traffic forecasting: Recent advances and new challenges",
            "venue": "IEEE Intelligent Transportation Systems Magazine, vol. 10, no. 2, pp. 93\u2013109, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M.S. Ahmed",
                "A.R. Cook"
            ],
            "title": "Analysis of freeway traffic time-series data by using Box-Jenkins techniques",
            "year": 1979
        },
        {
            "authors": [
                "M. Levin",
                "Y.-D. Tsao"
            ],
            "title": "On forecasting freeway occupancies and volumes (abridgment)",
            "venue": "Transportation Research Record, no. 773, 1980.",
            "year": 1980
        },
        {
            "authors": [
                "C. Moorthy",
                "B. Ratcliffe"
            ],
            "title": "Short term traffic forecasting using time series methods",
            "venue": "Transportation planning and technology, vol. 12, no. 1, pp. 45\u201356, 1988.",
            "year": 1988
        },
        {
            "authors": [
                "E.I. Vlahogianni",
                "M.G. Karlaftis",
                "J.C. Golias"
            ],
            "title": "Spatio-temporal short-term urban traffic volume forecasting using genetically optimized modular networks",
            "venue": "Computer-Aided Civil and Infrastructure Engineering, vol. 22, no. 5, pp. 317\u2013325, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "A.M. Nagy",
                "V. Simon"
            ],
            "title": "Survey on traffic prediction in smart cities",
            "venue": "Pervasive and Mobile Computing, vol. 50, pp. 148\u2013163, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "E.L. Manibardo",
                "I. La\u00f1a",
                "J. Del Ser"
            ],
            "title": "Transfer learning and online learning for traffic forecasting under different data availability conditions: Alternatives and pitfalls",
            "venue": "2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC). IEEE, 2020, pp. 1\u20136. 37",
            "year": 2020
        },
        {
            "authors": [
                "A. Ermagun",
                "D. Levinson"
            ],
            "title": "Spatiotemporal traffic forecasting: review and proposed directions",
            "venue": "Transport Reviews, vol. 38, no. 6, pp. 786\u2013814, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "W. Jiang",
                "J. Luo"
            ],
            "title": "Graph neural network for traffic forecasting: A survey",
            "venue": "Expert Systems with Applications, p. 117921, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "E.L. Manibardo",
                "I. La\u00f1a",
                "J.D. Ser"
            ],
            "title": "Deep learning for road traffic forecasting: Does it make a difference?",
            "venue": "IEEE Transactions on Intelligent Transportation Systems,",
            "year": 2021
        },
        {
            "authors": [
                "X. Yin",
                "G. Wu",
                "J. Wei",
                "Y. Shen",
                "H. Qi",
                "B. Yin"
            ],
            "title": "Deep learning on traffic prediction: Methods, analysis, and future directions",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 23, no. 6, pp. 4927\u20134943, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "I. La\u00f1a",
                "J.J. Sanchez-Medina",
                "E.I. Vlahogianni",
                "J. Del Ser"
            ],
            "title": "From data to actions in intelligent transportation systems: A prescription of functional requirements for model actionability",
            "venue": "Sensors, vol. 21, no. 4, p. 1121, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "E.I. Vlahogianni",
                "M.G. Karlaftis",
                "J.C. Golias"
            ],
            "title": "Short-term traffic forecasting: Where we are and where we\u2019re going",
            "venue": "Transportation Research Part C: Emerging Technologies, vol. 43, pp. 3\u201319, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "A.J. Majda",
                "N. Chen"
            ],
            "title": "Model error, information barriers, state estimation and prediction in complex multiscale systems",
            "venue": "Entropy, vol. 20, no. 9, p. 644, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "T.R. Anthes"
            ],
            "title": "Rieckh, \u201cEstimating observation and model error variances using multiple data sets.",
            "venue": "Atmospheric Measurement Techniques,",
            "year": 2018
        },
        {
            "authors": [
                "G. De Jong",
                "A. Daly",
                "M. Pieters",
                "S. Miller",
                "R. Plasmeijer",
                "F. Hofman"
            ],
            "title": "Uncertainty in traffic forecasts: literature review and new results for the netherlands",
            "venue": "Transportation, vol. 34, no. 4, pp. 375\u2013395, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "N.N. Taleb"
            ],
            "title": "Errors, robustness, and the fourth quadrant",
            "venue": "International Journal of Forecasting, vol. 25, no. 4, pp. 744\u2013759, 2009. 38",
            "year": 2009
        },
        {
            "authors": [
                "J. Hilden",
                "J.D.F. Habbema",
                "B. Bjerregaard"
            ],
            "title": "The measurement of performance in probabilistic diagnosis",
            "venue": "Methods of information in medicine, vol. 17, no. 04, pp. 227\u2013237, 1978.",
            "year": 1978
        },
        {
            "authors": [
                "H. Papadopoulos",
                "A. Gammerman",
                "V. Vovk"
            ],
            "title": "Confidence predictions for the diagnosis of acute abdominal pain",
            "venue": "IFIP International Conference on Artificial Intelligence Applications and Innovations. Springer, 2009, pp. 175\u2013184.",
            "year": 2009
        },
        {
            "authors": [
                "F. Yang",
                "H.-z. Wang",
                "H. Mi",
                "W.-w. Cai"
            ],
            "title": "Using random forest for reliable classification and cost-sensitive learning for medical diagnosis",
            "venue": "BMC bioinformatics, vol. 10, no. S1, p. S22, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "V.N. Balasubramanian",
                "R. Gouripeddi",
                "S. Panchanathan",
                "J. Vermillion",
                "A. Bhaskaran",
                "R. Siegel"
            ],
            "title": "Support vector machine based conformal predictors for risk of complications following a coronary drug eluting stent procedure",
            "venue": "2009 36th Annual Computers in Cardiology Conference (CinC). IEEE, 2009, pp. 5\u20138.",
            "year": 2009
        },
        {
            "authors": [
                "A. Lambrou",
                "H. Papadopoulos",
                "A. Gammerman"
            ],
            "title": "Reliable confidence measures for medical diagnosis with evolutionary algorithms",
            "venue": "IEEE Transactions on Information Technology in Biomedicine, vol. 15, no. 1, pp. 93\u201399, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "S. Zhou",
                "M. Mao",
                "J. Su"
            ],
            "title": "Short-term forecasting of wind power and non-parametric confidence interval estimation",
            "venue": "Zhongguo Dianji Gongcheng Xuebao(Proceedings of the Chinese Society of Electrical Engineering), vol. 31, no. 25. Chinese Society for Electrical Engineering, 2011, pp. 10\u201316.",
            "year": 2011
        },
        {
            "authors": [
                "V. Almeida",
                "J. Gama"
            ],
            "title": "Prediction intervals for electric load forecast: Evaluation for different profiles",
            "venue": "2015 18th International Conference on Intelligent System Application to Power Systems (ISAP). IEEE, 2015, pp. 1\u20136.",
            "year": 2015
        },
        {
            "authors": [
                "J. Neeven",
                "E. Smirnov"
            ],
            "title": "Conformal stacked weather forecasting",
            "venue": "Conformal and Probabilistic Prediction and Applications, 2018, pp. 220\u2013233. 39",
            "year": 2018
        },
        {
            "authors": [
                "S. Scher",
                "G. Messori"
            ],
            "title": "Predicting weather forecast uncertainty with machine learning",
            "venue": "Quarterly Journal of the Royal Meteorological Society, vol. 144, no. 717, pp. 2830\u20132841, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "H. Lee",
                "B.-T. Lee"
            ],
            "title": "Confidence-aware deep learning forecasting system for daily solar irradiance",
            "venue": "IET Renewable Power Generation, vol. 13, no. 10, pp. 1681\u20131689, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "C. Hubmann",
                "J. Schulz",
                "M. Becker",
                "D. Althoff",
                "C. Stiller"
            ],
            "title": "Automated driving in uncertain environments: Planning with interaction and uncertain maneuver prediction",
            "venue": "IEEE Transactions on Intelligent Vehicles, vol. 3, no. 1, pp. 5\u201317, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "S. Noh"
            ],
            "title": "Decision-making framework for autonomous driving at road intersections: Safeguarding against collision, overly conservative behavior, and violation vehicles",
            "venue": "IEEE Transactions on Industrial Electronics, vol. 66, no. 4, pp. 3275\u20133286, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "L. Zhang",
                "W. Ding",
                "J. Chen",
                "S. Shen"
            ],
            "title": "Efficient uncertainty-aware decision-making for automated driving using guided branching",
            "venue": "2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2020, pp. 3291\u20133297.",
            "year": 2020
        },
        {
            "authors": [
                "X. Huang",
                "S.G. McGill",
                "B.C. Williams",
                "L. Fletcher",
                "G. Rosman"
            ],
            "title": "Uncertainty-aware driver trajectory prediction at urban intersections",
            "venue": "2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019, pp. 9718\u20139724.",
            "year": 2019
        },
        {
            "authors": [
                "C. Choi",
                "A. Patil",
                "S. Malla"
            ],
            "title": "Drogon: A causal reasoning framework for future trajectory forecast",
            "venue": "arXiv preprint arXiv:1908.00024, 2019.",
            "year": 1908
        },
        {
            "authors": [
                "J. Li",
                "H. Ma",
                "W. Zhan",
                "M. Tomizuka"
            ],
            "title": "Coordination and trajectory prediction for vehicle interactions via bayesian generative modeling",
            "venue": "2019 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2019, pp. 2496\u20132503.",
            "year": 2019
        },
        {
            "authors": [
                "E.J. Casado Maga\u00f1a"
            ],
            "title": "Trajectory prediction uncertainty modelling for air traffic management",
            "venue": "Ph.D. dissertation, University of Glasgow, 2016. 40",
            "year": 2016
        },
        {
            "authors": [
                "P. Scala",
                "M.M. Mota",
                "J. Ma",
                "D. Delahaye"
            ],
            "title": "Tackling uncertainty for the development of efficient decision support system in air traffic management",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Chen",
                "L. Chen",
                "D. Sun"
            ],
            "title": "Air traffic flow management under uncertainty using chance-constrained optimization",
            "venue": "Transportation Research Part B: Methodological, vol. 102, pp. 124\u2013141, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "P. Parthasarathi",
                "D. Levinson"
            ],
            "title": "Post-construction evaluation of traffic forecast accuracy",
            "venue": "Transport Policy, vol. 17, no. 6, pp. 428\u2013443, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "M.S. Nicolaisen",
                "P.A. Driscoll"
            ],
            "title": "Ex-post evaluations of demand forecast accuracy: A literature review",
            "venue": "Transport Reviews, vol. 34, no. 4, pp. 540\u2013557, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "C. Yang",
                "A. Chen",
                "X. Xu",
                "S. Wong"
            ],
            "title": "Sensitivity-based uncertainty analysis of a combined travel demand model",
            "venue": "Transportation Research Part B: Methodological, vol. 57, pp. 225\u2013244, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "A. Matas",
                "J.-L. Raymond",
                "A. Ruiz"
            ],
            "title": "Traffic forecasts under uncertainty and capacity constraints",
            "venue": "Transportation, vol. 39, no. 1, pp. 1\u201317, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "M.B. Hugosson"
            ],
            "title": "Quantifying uncertainties in a national forecasting model",
            "venue": "Transportation Research Part A: Policy and Practice, vol. 39, no. 6, pp. 531\u2013547, 2005.",
            "year": 2005
        },
        {
            "authors": [
                "S. Rasouli",
                "H.J. Timmermans"
            ],
            "title": "Using ensembles of decision trees to predict transport mode choice decisions: Effects on predictive success and uncertainty estimates",
            "venue": "European Journal of Transport and Infrastructure Research, vol. 14, no. 4, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "M. Welde",
                "J. Odeck"
            ],
            "title": "Do planners get it right? the accuracy of travel demand forecasting in norway",
            "venue": "European Journal of Transport and Infrastructure Research, vol. 11, no. 1, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "I. La\u00f1a",
                "E. Villar-Rodriguez",
                "U. Etxegarai",
                "I. Oregi",
                "J. Del Ser"
            ],
            "title": "A question of trust: Statistical characterization of long-term traffic estimations for their improved actionability",
            "venue": "2019 IEEE Intelligent 41 Transportation Systems Conference (ITSC). IEEE, 2019, pp. 1922\u2013 1928.",
            "year": 2019
        },
        {
            "authors": [
                "R.C. Smith"
            ],
            "title": "Uncertainty quantification: theory, implementation, and applications",
            "venue": "Siam, 2013,",
            "year": 2013
        },
        {
            "authors": [
                "T.J. Sullivan"
            ],
            "title": "Introduction to uncertainty quantification",
            "year": 2015
        },
        {
            "authors": [
                "J. Guo",
                "B.M. Williams"
            ],
            "title": "Real-time short-term traffic speed level forecasting and uncertainty quantification using layered kalman filters",
            "venue": "Transportation Research Record, vol. 2175, no. 1, pp. 28\u201337, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "J. Guo",
                "W. Huang",
                "B.M. Williams"
            ],
            "title": "Adaptive kalman filter approach for stochastic short-term traffic flow rate prediction and uncertainty quantification",
            "venue": "Transportation Research Part C: Emerging Technologies, vol. 43, pp. 50\u201364, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "A. Barredo Arrieta",
                "N. D\u0131\u0301az-Rod\u0155\u0131guez",
                "J. Del Ser",
                "A. Bennetot",
                "S. Tabik",
                "A. Barbado",
                "S. Garcia",
                "S. Gil-Lopez",
                "D. Molina",
                "R. Benjamins",
                "R. Chatila",
                "F. Herrera"
            ],
            "title": "Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI",
            "venue": "Information Fusion, vol. 58, pp. 82\u2013115, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "K. Beven"
            ],
            "title": "Facets of uncertainty: epistemic uncertainty, nonstationarity, likelihood, hypothesis testing, and communication",
            "venue": "Hydrological Sciences Journal, vol. 61, no. 9, pp. 1652\u20131665, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "D.V. Lindley"
            ],
            "title": "The philosophy of statistics",
            "venue": "Journal of the Royal Statistical Society: Series D (The Statistician), vol. 49, no. 3, pp. 293\u2013337, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "A. Shapeev",
                "K. Gubaev",
                "E. Tsymbalov",
                "E. Podryabinkin"
            ],
            "title": "Active learning and uncertainty estimation",
            "venue": "Machine Learning Meets Quantum Physics, pp. 309\u2013329, 2020. 42",
            "year": 2020
        },
        {
            "authors": [
                "J. Zhu",
                "H. Wang",
                "B.K. Tsou",
                "M. Ma"
            ],
            "title": "Active learning with sampling by uncertainty and density for data annotations",
            "venue": "IEEE Transactions on audio, speech, and language processing, vol. 18, no. 6, pp. 1323\u20131331, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "Y. Yang",
                "M. Loog"
            ],
            "title": "Active learning using uncertainty information",
            "venue": "2016 23rd International Conference on Pattern Recognition (ICPR). IEEE, 2016, pp. 2646\u20132651.",
            "year": 2016
        },
        {
            "authors": [
                "V.-L. Nguyen",
                "M.H. Shaker",
                "E. H\u00fcllermeier"
            ],
            "title": "How to measure uncertainty in uncertainty sampling for active learning",
            "venue": "Machine Learning, vol. 111, no. 1, pp. 89\u2013122, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "E.L. Manibardo",
                "I. La\u00f1a",
                "E. Villar",
                "J. Del Ser"
            ],
            "title": "On the design of graph embeddings for the sensorless estimation of road traffic profiles",
            "venue": "arXiv preprint arXiv:2201.04968, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "L. Baier",
                "T. Schl\u00f6r",
                "J. Sch\u00f6ffer",
                "N. K\u00fchl"
            ],
            "title": "Detecting concept drift with neural network model uncertainty",
            "venue": "arXiv preprint arXiv:2107.01873, 2021.",
            "year": 1873
        },
        {
            "authors": [
                "S. Ghosh",
                "Q.V. Liao",
                "K.N. Ramamurthy",
                "J. Navratil",
                "P. Sattigeri",
                "K.R. Varshney",
                "Y. Zhang"
            ],
            "title": "Uncertainty quantification 360: A holistic toolkit for quantifying and communicating the uncertainty of ai",
            "venue": "2021.",
            "year": 2021
        },
        {
            "authors": [
                "E. H\u00fcllermeier",
                "W. Waegeman"
            ],
            "title": "Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods",
            "venue": "Machine Learning, vol. 110, no. 3, pp. 457\u2013506, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R. Senge",
                "S. B\u00f6sner",
                "K. Dembczy\u0144ski",
                "J. Haasenritter",
                "O. Hirsch",
                "N. Donner-Banzhoff",
                "E. H\u00fcllermeier"
            ],
            "title": "Reliable classification: Learning classifiers that distinguish aleatoric and epistemic uncertainty",
            "venue": "Information Sciences, vol. 255, pp. 16\u201329, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "A. Gammerman",
                "V. Vovk",
                "V. Vapnik"
            ],
            "title": "Learning by transduction",
            "venue": "arXiv preprint arXiv:1301.7375, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "V. Vovk",
                "A. Gammerman",
                "G. Shafer"
            ],
            "title": "Algorithmic learning in a random world",
            "venue": "Springer Science & Business Media,",
            "year": 2005
        },
        {
            "authors": [
                "V. Balasubramanian",
                "S.-S. Ho",
                "V. Vovk"
            ],
            "title": "Conformal prediction for reliable machine learning: theory, adaptations and applications",
            "year": 2014
        },
        {
            "authors": [
                "U. Johansson",
                "H. Bostr\u00f6m",
                "T. L\u00f6fstr\u00f6m",
                "H. Linusson"
            ],
            "title": "Regression conformal prediction with random forests",
            "venue": "Machine learning, vol. 97, no. 1-2, pp. 155\u2013176, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "T.G. Dietterich"
            ],
            "title": "Ensemble learning",
            "venue": "The handbook of brain theory and neural networks, vol. 2, no. 1, pp. 110\u2013125, 2002.",
            "year": 2002
        },
        {
            "authors": [
                "S. Gonz\u00e1lez",
                "S. Gar\u0107\u0131a",
                "J. Del Ser",
                "L. Rokach",
                "F. Herrera"
            ],
            "title": "A practical tutorial on bagging and boosting based ensembles for machine learning: Algorithms, software tools, performance study, practical perspectives and opportunities",
            "venue": "Information Fusion, vol. 64, pp. 205\u2013237, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "L. Mentch",
                "G. Hooker"
            ],
            "title": "Quantifying uncertainty in random forests via confidence intervals and hypothesis tests",
            "venue": "The Journal of Machine Learning Research, vol. 17, no. 1, pp. 841\u2013881, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "R. Koenker",
                "K.F. Hallock"
            ],
            "title": "Quantile regression",
            "venue": "Journal of economic perspectives, vol. 15, no. 4, pp. 143\u2013156, 2001.",
            "year": 2001
        },
        {
            "authors": [
                "O. Rahmati",
                "B. Choubin",
                "A. Fathabadi",
                "F. Coulon",
                "E. Soltani",
                "H. Shahabi",
                "E. Mollaefar",
                "J. Tiefenbacher",
                "S. Cipullo",
                "B.B. Ahmad"
            ],
            "title": "Predicting uncertainty of machine learning models for modelling nitrate pollution of groundwater using quantile regression and uneec methods",
            "venue": "Science of the Total Environment, vol. 688, pp. 855\u2013866, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "K. Vaysse",
                "P. Lagacherie"
            ],
            "title": "Using quantile regression forest to estimate uncertainty of digital soil mapping products",
            "venue": "Geoderma, vol. 291, pp. 55\u201364, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "H. Wang",
                "D.-Y. Yeung"
            ],
            "title": "Towards bayesian deep learning: A framework and some existing methods",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 28, no. 12, pp. 3395\u20133408, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "A.A. Abdullah",
                "M.M. Hassan",
                "Y.T. Mustafa"
            ],
            "title": "A review on bayesian deep learning in healthcare: Applications and challenges",
            "venue": "IEEE Access, 2022. 44",
            "year": 2022
        },
        {
            "authors": [
                "S. Hern\u00e1ndez",
                "J.L. L\u00f3pez"
            ],
            "title": "Uncertainty quantification for plant disease detection using bayesian deep learning",
            "venue": "Applied Soft Computing, vol. 96, p. 106597, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H. Lee",
                "B.-T. Lee"
            ],
            "title": "Bayesian deep learning-based confidence-aware solar irradiance forecasting system",
            "venue": "2018 International Conference on Information and Communication Technology Convergence (ICTC). IEEE, 2018, pp. 1233\u20131238.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Wu",
                "J.J. Yu"
            ],
            "title": "A bayesian learning network for traffic speed forecasting with uncertainty quantification",
            "venue": "2021 International Joint Conference on Neural Networks (IJCNN), 2021, pp. 1\u20137.",
            "year": 2021
        },
        {
            "authors": [
                "F. Zhou",
                "Q. Yang",
                "T. Zhong",
                "D. Chen",
                "N. Zhang"
            ],
            "title": "Variational graph neural networks for road traffic prediction in intelligent transportation systems",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 17, no. 4, pp. 2802\u20132812, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Amini",
                "W. Schwarting",
                "A. Soleimany",
                "D. Rus"
            ],
            "title": "Deep evidential regression",
            "venue": "Advances in Neural Information Processing Systems, vol. 33, pp. 14 927\u201314 937, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Gal",
                "Z. Ghahramani"
            ],
            "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
            "venue": "international Conference on Machine Learning. PMLR, 2016, pp. 1050\u20131059.",
            "year": 2016
        },
        {
            "authors": [
                "F. Rodrigues",
                "F.C. Pereira"
            ],
            "title": "Heteroscedastic gaussian processes for uncertainty modeling in large-scale crowdsourced traffic data",
            "venue": "Transportation research part C: emerging technologies, vol. 95, pp. 636\u2013651, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "K. Tran",
                "W. Neiswanger",
                "J. Yoon",
                "Q. Zhang",
                "E. Xing",
                "Z.W. Ulissi"
            ],
            "title": "Methods for comparing uncertainty quantifications for material property predictions",
            "venue": "Machine Learning: Science and Technology, vol. 1, no. 2, p. 025006, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "N.C. Petersen",
                "A. Parslov",
                "F. Rodrigues"
            ],
            "title": "Short-term bus travel time prediction for transfer synchronization with intelligent uncertainty handling",
            "venue": "arXiv preprint arXiv:2104.06819, 2021. 45",
            "year": 2021
        },
        {
            "authors": [
                "D.L. Shrestha",
                "D.P. Solomatine"
            ],
            "title": "Machine learning approaches for estimation of prediction interval for the model output",
            "venue": "Neural Networks, vol. 19, no. 2, pp. 225\u2013235, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "T. Gneiting",
                "F. Balabdaoui",
                "A.E. Raftery"
            ],
            "title": "Probabilistic forecasts, calibration and sharpness",
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 69, no. 2, pp. 243\u2013268, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "T. Gneiting",
                "A.E. Raftery"
            ],
            "title": "Strictly proper scoring rules, prediction, and estimation",
            "venue": "Journal of the American statistical Association, vol. 102, no. 477, pp. 359\u2013378, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "A. Carvalho"
            ],
            "title": "An overview of applications of proper scoring rules",
            "venue": "Decision Analysis, vol. 13, no. 4, pp. 223\u2013242, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "R.E. Schapire"
            ],
            "title": "Explaining adaboost",
            "venue": "Empirical inference. Springer, 2013, pp. 37\u201352.",
            "year": 2013
        },
        {
            "authors": [
                "I. La\u00f1a",
                "J. Del Ser",
                "A. Padr\u00f3",
                "M. V\u00e9lez",
                "C. Casanova-Mateo"
            ],
            "title": "The role of local urban traffic and meteorological conditions in air pollution: A data-based case study in madrid, spain",
            "venue": "Atmospheric Environment, vol. 145, pp. 424\u2013438, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "H.V. Gupta",
                "K.J. Beven",
                "T. Wagener"
            ],
            "title": "Model calibration and uncertainty estimation",
            "venue": "Encyclopedia of hydrological sciences, 2006. 46",
            "year": 2006
        }
    ],
    "sections": [
        {
            "text": "The estimation of the amount of uncertainty featured by predictive machine learning models has acquired a great momentum in recent years. Uncertainty estimation provides the user with augmented information about the model\u2019s confidence in its predicted outcome. Despite the inherent utility of this information for the trustworthiness of the user, there is a thin consensus around the different types of uncertainty that one can gauge in machine learning models and the suitability of different techniques that can be used to quantify the uncertainty of a specific model. This subject is mostly non existent within the traffic modeling domain, even though the measurement of the confidence associated to traffic forecasts can favor significantly their actionability in practical traffic management systems. This work aims to cover this lack of research by reviewing different techniques and metrics of uncertainty available in the literature, and by critically discussing how confidence levels computed for traffic forecasting models can be helpful for researchers and practitioners working in this research area. To shed light with empirical evidence, this critical discussion is further informed by experimental results produced by different uncertainty estimation techniques over real traffic data collected in Madrid (Spain), rendering a general overview of the benefits and caveats of every technique, how they can be compared to each other, and how the measured uncertainty decreases depending on the amount, quality and diversity of data used to produce the forecasts.\nKeywords: Uncertainty estimation, confidence, traffic forecasting.\n\u2217TECNALIA, P. Tecnologico Bizkaia, Ed. 700, 48160 Derio, Spain. Tl: +34 946 430 50. Fax: +34 901 760 009. E-mail: ibai.lana@tecnalia.com.\nPreprint submitted to Elsevier October 31, 2022\nar X\niv :2\n21 0.\n16 04\n9v 1\n[ cs\n.L G\n] 2\n8 O\nct 2"
        },
        {
            "heading": "1. Introduction",
            "text": "Road traffic forecasting has been a subject of intense academic research for decades [1]. This modeling task was originally approached based on regressive techniques for time series analysis [2, 3, 4], veering over time towards Machine Learning (ML) methods [5]. When compared to traditional regression techniques, ML allows for the detection and characterization of hidden relationships among data in exchange for a higher level of complexity. A glance at the recent literature contributed on ML-based traffic forecasting reveals that deep neural networks (also referred to as Deep Learning) conform nowadays the core of almost any traffic forecasting work [6, 7], yielding a torrent of works that are not only abundant, but increasing steadily every year[8, 9]. This sustained interest of the research community in this topic suggests that traffic forecasting still poses challenges for the method used to address this modeling task. However, several comprehensive studies expose that traffic forecasting has reached its modeling performance asymptote, wherein the addition of extra layers of complexity in the modeling proposal yields negligible performance gains [10, 11]. This prevailing performancedriven pursuit for new traffic forecasting models is far from being actionable from the perspective of Intelligent Transportation Systems [12]. As a result, most of the challenges of this research area identified years ago remain unsolved to date [1, 13].\nThis noted lack of actionability in current traffic forecasting solutions can be addressed by moving the research focus away from performance metrics towards augmenting their output with valuable information to support practical decision making processes of traffic managers. When the quality of estimated forecasts reaches enough quality in terms of model predictive error, it is of utmost practical interest to explore the individual level of confidence associated to each forecast. Model error is a quantitative measure of the performance of the model when producing forecasts for a certain set of input traffic values [14, 15]. However, both the input data and the modeling technique itself may be subject to different sources of uncertainty [16]. This means that even models with high performance metrics can yield predictions that are inaccurate beyond the threshold within which they are useful [17], which propagates to a confidence level of the model in its predicted outcomes. It is not possible to gauge the error of a given prediction until the actual value (ground truth) occurs. However, measuring the uncertainty under which the prediction is furnished \u2013 in other words, assigning a confidence level to the\nprediction \u2013 can be approached at inference time using different strategies. Thus, a manager or service consuming such forecasts would not only be informed about the estimated traffic value for a certain location in space and time, but also the confidence under which the predicted value can be trusted. Besides, from the practitioners point of view, knowing the uncertainty also provides information about when to stop trying to improve a model.\nIn this regard, many disciplines (especially those where decisions made based on the model\u2019s output may entail a risk for the human life) have dealt with the quantification of uncertainty in data-based modeling, so that the provision of augmented predictions with confidence estimates has already become a matter of intense research. Within the medical domain, for instance, the confidence of ML models is crucial in model-based clinical diagnosis [18], and it has been obtained for diverse purposes [19, 20, 21, 22]. Other fields for which the actionability of forecasts is highly relevant have also considered measuring the confidence of predictions, e.g. in the energy sector [23, 24] or in weather and climate modeling [25, 26, 27].\nThe ITS domain is not unfamiliar to the application of confidence estimation techniques, specially in cases where predictions issued by a model play an essential role in decision making processes. For instance, within the autonomous driving field, vehicles need to sense and anticipate the contextual circumstances in which they operate to determine their behavior in uncertain environment. Researchers working on vehicular perception are highly focused on dealing with uncertainty when estimating the next maneuver [28, 29, 30] or the trajectory of other vehicles and pedestrians [31, 32, 33]. It is also the case of air traffic management, a field comprising multiple subareas in which decisions must be made fast, and errors can entail important operational consequences and high economical costs. As a result, assessing the uncertainty of forecasts has been addressed in this area for diverse purposes, from aircraft trajectory prediction [34] to the demand estimation of airport facilities [35, 36] to help managers scale services. In relation to the latter example, and closer to the road traffic domain, a considerable body of literature related to forecasting uncertainty has focused on travel demand estimation. For instance, authors in [37, 38, 39] examine the statistical robustness of forecasts and propose different methods to detect them. In [40], infrastructure capacity constraints are considered to propose a new methodology that quantifies uncertainty, similar to what [41] does for an established and operating travel demand system in Sweden. Analogous research is performed for transport mode choice predictions in [42], with equivalent actionable results.\nAll these research contributions deal with the prediction of travel demand, which is intended to be provided to public authorities, infrastructure and public transportation managers and investors. By informing them about the uncertainty associated to the demand estimation, insights on how traffic and the demand of transportation services will operate in the long term are produced, which can be used to scale appropriately such infrastructures and services. When decision making is based on short-term traffic forecasts, the impact of a forecasting error has a shallower relevance. Anyhow, if these forecasts are meant to be used for any real-world purpose, measuring their uncertainty is of paramount importance for their trustworthiness and actionability. Following up the work in [43], the performance of travel demand forecasts was not properly considered in terms of the uncertainty it was subject to. Up to this point, all the mentioned works involve forecasting up to several minutes or hours. Long-term traffic estimations can also be studied for traffic demand analysis, goal for which measuring their uncertainty becomes an essential actionability driver [44].\nThis manuscript finds its motivation in the lack of an unified referential work consolidating the state of the art related to uncertainty estimation applied to short-term traffic forecasting problems. To cover this niche, we herein offer a short yet instructive summary about the different types of uncertainty that may arise in road traffic forecasting scenarios, an enumeration of the practical reasons why the confidence of traffic forecasting models is a key driver for their practicality, different uncertainty estimation approaches used nowadays to quantify such a confidence, and scores utilized to compare among such techniques. This multi-faceted analysis is complemented by an extensive experimental benchmark with real traffic and weather data collected over the city of Madrid (Spain). Specifically, the experimental setup and the results obtained therefrom permit to answer with empirical evidence three research questions (RQs) that touch the core of the overall study:\n\u2022 RQ1 (Techniques and comparison framework): How do different uncertainty quantification techniques perform when applied to a particular traffic forecasting model? Which are their main strengths and weaknesses for each problem? Which scores can be used to compare these techniques? What does each of them contribute to the improvement of the model\u2019s actionability?\n\u2022 RQ2 (Scenario under study): Which changes in the available data affect uncertainty? How can the confidence of a forecasting model be improved\nby changing/augmenting data at its input?\n\u2022 RQ3 (The relevance of calibration): What impact does the calibration process of some of the uncertainty estimation techniques have on their outcome? Why is it relevant for traffic forecasting?\nThe rest of this paper is structured as follows: first, Section 2 introduces the readership to the different sources of uncertainty that one can encounter in traffic forecasting scenarios. This section also enumerates different practical purposes for which the uncertainty of traffic forecasts must be measured, and provides an overview of uncertainty estimation techniques, along with scores that can be used to compare their effectiveness. Section 3 introduces the data and the experimental setup devised to answer the RQs formulated above. Section 4 presents and discusses the obtained results, structuring their analysis in terms of the RQ under target. Lastly, insightful concluding remarks are offered in Section 5, together with future research directions departing from this study."
        },
        {
            "heading": "2. Estimating the Confidence of Traffic Forecasting Models: Sources of Uncertainty, Practical Purposes, Techniques and Measures",
            "text": "As argued in the introduction, uncertainty quantification can improve significantly the worth of any forecasting model. Although it is an old area of study in statistics and probability, its application to machine learning techniques is relatively recent [45, 46]. It has been exposed before that decision makers in many application fields (e.g., medical diagnosis, industrial prognosis) demand the provision of quantitative metrics related to the trustworthiness and reliability of predictions issued by machine learning models. This is certainly not the case of traffic forecasting, a vibrant field with hundreds of publications on a yearly basis, in which confidence measurements are rarely provided. There are some notable exceptions that were surveyed in [16], which revealed that most works at the time reporting confidence intervals obtained them experimentally by repeating experiments and measuring the variance and standard deviation of forecasts. Some other works published thereafter took advantage of particular characteristics of Kalman filter modeling techniques to extract confidence intervals for the predictions [47, 48]. However, this is not a common practice: in the traffic forecasting literature, only performance scores are usually computed.\nInterestingly, machine learning techniques for predictive modeling have reached ever-growing levels of complexity over the years, rendering their modeled knowledge more opaque and difficult to understand for the user consuming their predictions [10]. As a result, different methods for explaining their inner structure in an user-interpretable manner are under active investigation lately [49]. As a supplement to such explainability methods, informing about the uncertainty associated to the predictions supports even further the trustworthiness of forecasting models, and favors decision making processes not only linked to the usefulness of the traffic forecasts themselves (e.g. traffic congestion management), but also in regards to the intelligent collection of traffic data.\nThis section delves into these ideas by introducing briefly to the different sources of uncertainty in traffic forecasting (Subsection 2.1), by discussing on the diverse purposes and inherent usefulness of uncertainty estimation in this context (Subsection 2.2), by shortly reviewing state-of-the-art uncertainty estimation techniques (Subsection 2.3), and by enumerating different quantitative metrics that can be used to evaluate the quality of estimated confidence intervals (Subsection 2.4). The overarching goal of this section is to set the knowledge basis towards the experimental part of this work."
        },
        {
            "heading": "2.1. Sources and Types of Uncertainty in Traffic Forecasting",
            "text": "The uncertainty of a model trained with supervised learning can have different kinds of sources. It is commonly accepted that this uncertainty of the output of the model (evaluations of the model with new data) can include a fraction of epistemic uncertainty, that accounts for all the uncertainty that the modeling process itself introduces, and aleatory uncertainty, that regards the irreducible uncertainty that is present in the data due to their inherent variability [50]. However, both the number of different uncertainty sources and their separability are open practical questions subject to debate among statisticians, frequently an outset for deeper philosophical discussion [51]. Epistemic uncertainty can (theoretically) be reduced, being it part of the way in which data are structured and represented. On the contrary, aleatory uncertainty cannot be decreased, as it is inherent to the variability and noisy nature of data. Discerning among both sources of uncertainty can be interesting precisely for reducing the share of epistemic uncertainty. Nonetheless, burrowing through the philosophical grounds of the very definition of uncertainty [52], it is unclear whether different sources of uncertainty can be\ndisentangled from each other, or even that the uncertainty associated to data can be isolated from other data features.\nPractical implications of segregating aleatory and epistemic uncertainty in traffic forecasting are clear: while epistemic uncertainty can be reduced to an extent depending on the modeling choice in use, measuring the amount of aleatory uncertainty can dictate whether the addition of new variables and/or the collection of new data helps reducing it effectively. Unfortunately, from a practical point of view such a segregation of uncertainty types might not be feasible [51]. Moreover, it might be even irrelevant as long as mechanisms are designed to reduce the global uncertainty that aggregates both sources. In fact, not all uncertainty estimation techniques discussed in this work provide separated uncertainty measurements, consideration that will be contemplated in their experimental comparison."
        },
        {
            "heading": "2.2. Uncertainty Estimation in Traffic Forecasting: What for?",
            "text": "The use of uncertainty estimation techniques in some of the aforementioned fields is straightforward: quantifying the confidence under which a model produces its output is critical for its actionability in many diverse applications. For this reason, research about uncertainty estimation methods is growing and becoming available for very diverse machine learning models. Short-term traffic forecasting has probably a less critical nature than, for instance, medical or finance sectors. However, measuring uncertainty of\ntraffic forecasts can also be profitable if we consider the purposes for which it can be performed. Such purposes are graphically illustrated in Figure 2 and explained in what follows:\n\u2022 Improved actionability : the unequivocal application of estimating uncertainty is to provide practitioners with a confidence interval of predictions, so that better decisions can be made [12]. Unlike customary regression scores, which are commonly obtained for a whole test dataset and apprise about the general (averaged) performance of the trained model, uncertainty metrics reach the individual prediction level, giving confidence information for each data point predicted by the model. This not only provides valuable information about the extent to which each individual prediction can be trusted, but also allows for a further study on how traffic behaves in different time intervals and seasons over the year.\n\u2022 Model selection and comparison: as explained before, a part of the total uncertainty of the output of a forecasting scheme can be attributed to the way the information is modeled. Uncertainty can be, therefore, another dimension in a model comparison framework, allowing researchers and ITS experts to choose models that perform with narrower confidence intervals. Models performing similarly in terms of regression error but introducing less epistemic uncertainty, can be considered more desirable in some contexts. Furthermore, examining the uncertainty associated to individual predictions can be also interesting to select models that reduce the uncertainty in critical segments of the traffic data series (e.g. in hours featuring high traffic intensity).\n\u2022 Feature selection: prior to the application of a training algorithm that fits the parameters of the model to represent a reality represented by data, raw traffic information is arranged into a dataset with a feature space X and an output space Y. Even before the training process starts, the way in which variables are selected and processed for constructing X and Y can affect their degree of uncertainty [51]. This uncertainty should be deemed as epistemic, as it falls under the way knowledge is represented and usually is subject to the experience of the expert at hand. From this conceptual point of view, it is interesting to highlight that the way in which input data are selected, shaped and fed to the model towards its training must be also considered a part of the model. Thus, analogously to the previous concept, uncertainty metrics can help, within the framework of a benchmark, to\ndefine which input features provide more trustworthy results. Likewise, metrics can be helpful to seek features that help reducing the uncertainty induced by the overall forecasting model.\n\u2022 Active learning: uncertainty estimation is also an essential component of algorithms that learn actively [53]. Active learning consists of constructing training datasets dynamically, where training samples are added progressively and aimed at reducing uncertainty and increasing diversity. Latest research in this vibrant field is highly reliant on uncertainty estimation [54, 55], and the way in which uncertainty is measured is a matter of study in itself [56]. In the context of road traffic forecasting, the installation of new traffic reading sensors (either provisional or fixed) can be decided upon the study of the distribution of uncertainty in space and time, especially when traffic measurements are scarce in locations of interest for traffic management. If forecasts for such locations exhibit a higher degree of uncertainty, new measurement campaigns can be commanded to collect more data that allow improving the confidence of the predictions issued by the model. This is especially relevant in practical circumstances where traffic profiles for a certain place are inferred from data collected in other nearby points of the road traffic network [57].\n\u2022 Trend change detection: a well-calibrated confidence interval for the output of the model should characterize, with the established confidence level,\nthe percentage of real traffic data that falls within the interval. This a priori notion can be used to detect whether, in a traffic data stream, an excessive number of real traffic data points fall outside the confidence interval of the model. If this occurs, it can be an evidence of a change in the underlying distribution of traffic data requiring, upon its detection, an update of the model\u2019s knowledge (via incremental training, selective forgetting or any other adaptation method alike). Despite its unquestionable practical benefits (especially in dynamic urban road networks prone to different non-stationarities affecting its traffic flows), this adaptation approach have not been studied until very recently [58].\nThere are deeper questions related to uncertainty that could concern practitioners, such as the risk of introducing dependence among random events that affect the system being modeled [51]. Nonetheless, the essential actionability aspects presented above can help ITS stakeholders design more confident models, and understand them better in order to put their output to practice."
        },
        {
            "heading": "2.3. A Brief Overview of Uncertainty Estimation Techniques",
            "text": "Quantifying the uncertainty present in the output of a model implies measuring the way in which it can vary. This can be expressed as the variance or standard deviation of each predicted value, or by reporting different statistics (e.g. confidence intervals or percentiles) of the estimated output distribution [16]. When dealing with machine learning models, once a dataset is built and the parameters of the chosen model are trained through a training algorithm, the outputs of such model will not vary given a fixed input. For a certain forecasting query, there is only one output given one input to the model. Unless probabilistic formulations of the model\u2019s parameters are formulated (as in Bayesian neural networks), most machine learning models are deterministic after their parameters have been trained. Thus, with an already trained model, estimating the variability of each forecast requires in general an uncertainty estimation technique.\nOn this basis, many techniques and methods have been developed by the community to characterize the output distribution of a machine learning model based on prior knowledge from training data. Once this output distribution is estimated, the usual approach consists of defining significance levels and obtain confidence intervals. Some machine learning models allow changing their default operation to obtain predictions based on percentiles. These\nmodel-specific approaches have been described in the literature as intrinsic [59]. Conversely, extrinsic uncertainty estimation techniques are not specific to a particular machine learning model, but allow obtaining a measurement of uncertainty once a model has been trained (i.e., a post-hoc estimation), normally through a calibration process.\nEven though some authors have tried to categorize the landscape of uncertainty measuring techniques [59], it appears to be difficult to find consensus beyond the differences between intrinsic and extrinsic techniques exposed above. For instance, some techniques allege to be able to discriminate between the epistemic and aleatory uncertainty present in the output of machine learning models [60, 61], frequently based on Bayesian formulations of their underlying training mechanisms. Other branch of techniques based on wrapper methods rely on a calibration process prior to the training phase of the algorithm, whereas other techniques hinge on the use of ensembles to approximate the output distribution. Lastly, different variants of deep neural networks have been proposed in the last years to estimate the output distribution, via the probabilistic definition of their trainable weights, evidential formulations of the loss function used to guide the training process, or the derivation of mechanisms to sample the sought distribution.\nBeyond the discussion on the suitability of one taxonomic criterion or another, in what follows we examine the most widely adopted options to estimate the uncertainty in machine learning models used for regression tasks. To this end we pause briefly at the specific techniques organized in the taxonomy shown in Figure 3. This short review connects tightly with the experiments designed to answer RQ1, where a comprehensive comparison benchmark over real traffic data will expose the strengths, weaknesses and the applicability of the methods reviewed below:\n\u2022 Conformal prediction: as defined in [62, 63], conformal predictors are confidence predictors. Their operation is based on the conception of nonconformity measures, which allow predicting regions of points instead of individual points (i.e., confidence intervals) by using the statistical knowledge that can be obtained through a calibration process from the training samples. This method has been prevalent in the machine learning community for years [64], being used in applications arising in many fields [65]. Conformal prediction is agnostic to the particularities of the underlying model, and can be used, in principle, with any machine learning algorithm and dataset configuration.\n\u2022 Ensemble methods : ensemble learning consists of the aggregation of knowledge obtained from different models learned from different views of the data. Combining the predictions issued by different methods learned from the same data has been proven to yield better performance than any contributing member to the ensemble, which is achieved by reducing the spread in the predictions made by such models [66, 67]. Besides the increased robustness favored by this smoothing process, ensembles represent an easy and straightforward approach to sample the distribution associated to the combined output; once trained, each base learner yields a different prediction for a given input, all of which are aggregated by averaging or other more sophisticated combination procedures (e.g. weighting based on out-of-bag performance estimates). Thus, confidence intervals can be obtained by characterizing statistically the distribution of all the different predictions produced by each learner in the ensemble for a given input [68].\n\u2022 Quantile regression: This family of methods was originally conceived for linear regression methods, for which, instead of using the median as an output, asymmetric errors were considered in order to have upper and lower boundaries of an interval defined by a quantile [69]. More modern contraptions have used this notion as a base to estimate uncertainty of other machine learning models [70, 71]. Quantiles have been introduced in loss functions of several learning algorithms, from gradient boosting regression to deep neural networks. These reformulation of the loss func-\ntions permit to predict not the median, but instead the extremes of an interval defined by the quantile sought. Their main difference with respect to previous methods in terms of prediction interval is the distribution of points inside the interval. Other methods generate predictions that may have an unknown distribution. Once they are characterized, the boundaries of the interval are obtained according to a significance level. This approach produces only boundary lines, and they are normally distributed. A characteristic prediction would be the median, but any other predicted points inside the interval would be placed according to their significance level. This becomes a relevant issue when comparing the output of these methods to other uncertainty estimation approaches, as the experimental discussions later held will clearly show.\n\u2022 Bayesian inference based algorithms : in general, these algorithms aim to characterize the posterior probability distribution of the model parameters based on the available data and the assumption of an a priori distribution of the parameters. Once this distribution is characterized after training, we have the possibility of establishing confidence levels. Particularly with deep Bayesian networks, the parameters of the model (weights) are driven by a priori probability distribution (often set to be Gaussian), making the so-called family of Bayesian neural networks a model choice particularly suited to estimate uncertainty in modeling problems [72]. Many authors in different fields have taken advantage of Bayesian formulations. For instance, a thorough review of different applications of Bayesian learning approaches in the health domain can be found in [73], whereas use cases have been reported for other diverse scenarios such as plant disease detection [74] or solar power forecasting [75], among others. More recently, some attempts at applying Deep Bayesian neural networks for the delivery of confidence-aware road traffic forecasts have been done [76, 77], but efforts in this regard to date have been significantly lower than in other application scenarios.\n\u2022 Model-specific methods : finally, in this last category we gather all techniques that are specific to a certain family of forecasting models. Many of them have been proposed for deep neural networks, leveraging the flexibility granted by the use of a loss function to guide the training process, or the existence of neural computation mechanisms that allow for an approximate representation of the sought output distribution. A representative\ntechnique of the former approach is evidential deep learning [78], which was proposed to overcome the high computational complexity of the training process of Bayesian neural networks and the dependence of the estimated uncertainty on the suitability of the assumed priors for the network weights. In doing so, evidential deep learning proposes a framework in which a prior distribution is placed on the statistics of a Gaussian output distribution (as opposed to Bayesian neural networks, where priors are imposed to the network weights), so that the addition of newly observed traffic samples provides more support for the neural network to learn the parameters of the evidential distribution. On the other hand, Montecarlo Dropout [79] resounded loudly in the community for its simplicity and scalability to integrate the model\u2019s output likelihood by randomly switching off neurons in a neural network at inference time. Lastly, an intrinsic approach included in IBM\u2019s UQ360 library [59] is heteroscedastic regression, which takes advantage of the the expected noise of the data in the model to capture both kinds of uncertainty. An alternative is homoscedastic regression, which assumes that noise is constant across data points."
        },
        {
            "heading": "2.4. Measuring and Comparing Uncertainty Estimations",
            "text": "The output of any of the methods presented above should be regarded as a measure of the amount of uncertainty associated to their predictions. This is not a trivial aspect: as argued in Subsection 2.1, boundaries between different types of uncertainty are not clear, and not all methods measure the model\u2019s confidence by following the same principles. For this reason, there is a series of metrics that, departing from a common ground, not only provide a notion of the amount of uncertainty, but also indicate whether the measurement of uncertainty itself is performed equally. As in any other regression task, in traffic forecasting the measurement of the uncertainty at the output of a model can be performed by establishing a significance level beforehand (usually denoted as \u03b1), and by defining confidence intervals that contain a fraction of the possible outputs that matches the defined \u03b1. Thus, uncertainty relates to the width of the interval for each estimated point. For a certain significance level \u03b1, a narrower interval represents a less uncertain output of the model for which it is estimated.\nBeyond these baseline principles, there is no consensus in the literature regarding these metrics, to the point of referring to the same metric with different names, or using metrics that cannot be measured for all uncertainty estimation methods. With a practical stance, we now summarize a list of\nuncertainty metrics that can be used to measure different aspects of the estimated uncertainty, their different usages and practical implications. The adopted nomenclature for the rest of the paper is the one used in [80]:\n\u2022 Interval width: this first metric refers to the amplitude between the lowest and highest points of the estimated interval around each predicted data point. The concept of efficiency or informational efficiency of a conformal predictor [63] is related to this width, while a common uncertainty quantification metric known as sharpness [81], which is computed based on the variance at each point, measures essentially the same. The name informational efficiency used by some sources in the related literature refers to the way in which the interval informs: a very narrow interval with high confidence is very informative, while a very wide interval that covers a great number of possible values gives less information with the same confidence, thus it is less efficient. This metric can be also found referred to as Mean Interval Length (MIL, as in [80, 82]) or Mean Prediction Interval Width (MPIW, [59, 83]). Given a significance level \u03b1, this metric expresses how wide must the interval be to include the fraction of values defined by \u03b1. As the width can be different for each traffic forecast, it is common to obtain an averaged value of MIL (or MPIW). Mathematically, this metric is defined as:\nMIL = 1\nT\nT\n\u2211 t=1 (ut \u2212 lt), (1)\nwhere ut and lt are the upper and lower boundaries of the confidence interval estimated by the technique at hand, and T is the number of instances in the test dataset.\n\u2022 Interval coverage: this second metric is the fraction of the possible outputs that is covered by the interval. This can be related to the so-called validity metric [63], and has been also referred to as Interval Coverage Percentage (ICP, [80, 82]) or Prediction Interval Coverage Probability (PICP, [59, 83]). This metric provides a notion of how valid the interval is, as its value should concur with the percentage of samples defined by the established significance level (1 \u2212 \u03b1). The metric is defined as per eq. 2:\nICP = 1\nT\nT\n\u2211 t=1\nI(lt \u2264 yt \u2264 ut), (2)\nwhere lt and ut are the lower and upper confidence interval boundaries as in Expression (1), yt denotes the ground truth value associated to the\nt-th predicted value y\u0302t in the test set, and I(\u22c5) denotes an auxiliary binary function taking value 1 if its argument is true (0 otherwise).\n\u2022 Interval width with relation to the forecasting error : RMIL [80] is a variant of MIL that relates the size of the estimated confidence interval to the error of the forecast. This allows for larger intervals in order to cover those cases with largest forecasting error (i.e., those traffic values that result to be more difficult to forecast precisely). As a result, RMIL permits to compare two forecasting models that deal with different input predictors in terms of uncertainty. The metric is defined as:\nRMIL = 1\nT\nT\n\u2211 t=1\n(ut \u2212 lt) \u2223yt \u2212 y\u0302t\u2223 , (3)\nwhere yt is the real value for the t-th test instance, and y\u0302t denotes the predicted value for that instance issued by the forecasting model. This metric can return very high (even infinite) values if the prediction is very close to the real values, reason for which it has not been considered for the experimentation of this work.\n\u2022 Calibration curves : The calibration of a forecasting model aims to achieve a statistical consistency between the distribution of the forecasts and the distribution of the real values [84]. The outputs of a well-calibrated forecasting model will follow a similar distribution to the real values of the ground truth the model is representing. This property is explored through calibration curves, i.e., a two-dimensional plot relating every predicted value to the real observation it approximates [59]: as such, the optimal calibration curve is the identity function, whereas the deviation of the curve with respect to the optimal calibration is denoted as calibration error. It is relevant to note that, although previous metrics can be comparable for any uncertainty estimation approach, the way in which output distributions are obtained may affect substantially to the measurement of the calibration curve and error.\n\u2022 Other metrics : Gneitig et al. [85] proposed a set of proper scoring rules that cover, among others, diverse probabilistic aspects of interval forecasts, including combined metrics of some of the aforementioned scores, e.g., calibration and sharpness. Metrics like Negative Log Likelihood, Interval Score or Check have been used in a variety of uncertainty estimation works\n[86], providing a different perspective to the analysis of the size of the intervals and their coverage of real samples. Considering that they provide similar insights as the above metrics, and there are some hindrances to apply them to all of the uncertainty estimation methods (for instance, some of them are only intended for quantile-based methods), we have prioritized the metrics introduced previously in our experiments for the sake of a clearer and more uniform analysis.\nThese metrics can be used to assess the impact of different datasets and configurations of the forecasting model, as well as to examine the convenience of each of the uncertainty quantification methods. The insights extracted from their analysis will be helpful answering the different research questions of the experimental study discussed in the next section."
        },
        {
            "heading": "3. Experimental Setup",
            "text": "In order to assess and compare the uncertainty estimation techniques and confidence metrics presented above, and to discuss on their implications for the actionability of traffic forecasts, an experimental setup has been defined, which comprises several scenarios with real-world traffic data and different features. To this end, the setup relies on the extensive collection of traffic flow readings made public by the Madrid City Council in its Open Data portal (https://datos.madrid.es/), which releases a 5-year historic record of 15-minute traffic flow observations collected in more than 3,800 locations over this city. With these data, together with meteorological and calendar information, different datasets X have been composed. Hence, an scenario is defined based on the datasets in use, the considered forecasting horizon (i.e., the gap between the query time and the time at which the target variable sought occurs), and the forecasting model used to model the relationship between its input variables and the traffic measure to be predicted. By examining the relative behavior of the confidence metrics in each one of such scenarios, we can 1) gauge how each dimension of the scenario affects uncertainty (in response to RQ2), and 2) compare uncertainty estimation methods in terms of the metrics in different contexts (providing informed insights for RQ1)."
        },
        {
            "heading": "3.1. Datasets",
            "text": "The data used for defining the scenarios introduced above are based on real urban traffic data collected in 10 different locations of the city of Madrid\n(Spain). The selection of inductive loops has been made according to the variability of the surrounding urban topology of every sensor, in order to analyze the potential relationship between the traffic profiles in those locations and the measured uncertainty of a modeling pipeline aimed to forecasts those traffic measures. The spatial distribution of the selected sensors, depicted in Figure 4, shows that some loops are placed in locations with heavy traffic conditions (i.e., references1 3697, 3910 and 5761), while others are placed in small roads of residential areas or main arterial roads traversing commercial districts.\nData collected for the whole year 2019 were built into datasets to train\n1These numerical references correspond to the labels assigned to the loops in the Open Data portal from which data were retrieved.\nand test different forecasting models. Such datasets include information of up to 5 steps (i.e., 1 hour and 15 minutes) of past traffic measurements before the traffic point to be predicted (t\u22124, t\u22123, t\u22122, t\u22121, and t0). This window of observations will be configured as the input to the models in 2 forms \u03c9 = {1,5}: either all past measurements (\u03c9 = 5) or just one (\u03c9 = 1). For each case, different forecasting horizons h = {1,2,4,8} are considered, producing predictions in instants t+1, t+2, t+4 and t+8, and providing traffic forecasts of up to two hours in the future. Datasets may include (or not) meteorological information (temperature, cloud cover, humidity and precipitation intensity) and calendar information regarding local and national holidays, academic and scholar calendar and day of the week. The availability of this data at the input to the models is denoted by two binary variables m = {0,1} (meteorology) and c = {0,1} (calendar). With these different types of input variables, datasets with all possible combinations are created following the scheme in Figure 5, leading to a collection of 320 datasets resulting from 10 loops \u00d7 2 window lengths \u2223\u03c9\u2223 \u00d7 \u2223m\u2223 (meteorological information available or not) \u00d7 \u2223c\u2223 (calendar information available or not) \u00d7 4 forecasting horizon values. Models will be tested and their uncertainty measured with different sets of these combinations, in order to extract insights and answer each of the research questions stated in the introduction.\nAdditionally, a standardization procedure was applied to all data to guarantee the correct operation of some of the models, particularly those that are sensitive to differences in the statistical support of their predictors. Lastly, datasets were split into train and test partitions, stratifying them across all months of the year available for training. For each month, 3 weeks were considered as train data, whereas 1 week was left for test data. This permits to include all kinds of traffic profiles (which are highly variable throughout the year) in the training dataset, and also allows for a proper train-calibrationtest split for the Conformal Prediction approach: the calibration set will consist of the 3rd week of each month, thus providing data from all along the year to calibrate the models. Techniques that do not include a calibration step do not use this part of the dataset as training data, so all the models receive the same training information."
        },
        {
            "heading": "3.2. Methodological Approach",
            "text": "Departing from the collection of 320 datasets defined in the previous section, a processing pipeline is established. The steps are schematically depicted in Figure 6.\nTo answer RQ1, several predictive methods that cover all the uncertainty estimation approaches described in Section 2.3 are considered. Prediction tests are conducted with each model and the dataset combination comprising all features (m = c = 1) and the forecasting horizon set to h = 1. The uncertainty of each (model, dataset) combination is then estimated with the available techniques. A relevant remark related to this first comparison among techniques is that not all techniques are suitable for all algorithms. For instance, measuring uncertainty through techniques closely linked to specific elements of deep neural networks (e.g., Monte Carlo dropout) are not available for tree-based shallow learning algorithms. Thus, the comparison of techniques will be influenced by the capacities and limitations of the underlying predictive models, as well as by the epistemic uncertainty they introduce. Table 1 shows the specific models that have been considered for each uncertainty estimation technique in our experimental benchmark.\nIn addition to the predictive performance of the considered forecasting models (which may differ among them), the comparison benchmark also accounts with the confidence metrics described in Section 2.4. By simultaneously reporting on both confidence and forecasting error, we will be able to compare them in model-agnostic dimensions, connecting clearly with the responses sought to answer RQ1. When it comes to RQ2, a single uncertainty estimation method and predictive model will be selected, based on the results obtained from the prior experimentation done in regards to RQ1. The rest of dataset combinations (different horizons and different sets of features) will be used to obtain forecasts, estimate uncertainty and assess their impact on the confidence levels. Lastly, tests with calibration-based and non-calibrated techniques will be conducted using different combinations of datasets, so that\nRQ3 can be replied in an empirically informed fashion."
        },
        {
            "heading": "4. Results and Discussion",
            "text": "We now discuss on the experimental results of the setup described in Figure 6. In doing so, we organize our examination of such results in terms of the research questions posed in the introduction. Models and uncertainty estimation techniques have been tested with different sensors placed in around the city in order to have different traffic profiles and also assess the impact of the traffic behavior on the modeling tools. Thus, traffic profiles of the 10 traffic sensors are shown on Figure 7. Each plot nested in this figure represents the average traffic (computed over 96 daily traffic traces) of every sensor in the ith 15-minute interval of a day, where the standard deviation is also provided as a shaded area to illustrate the intra-day variability of the traffic profiles at each sensor\u2019s location.\nAs revealed in this figure, sensors such as 3642, 6132 or 3500 have very low traffic profiles, with less than 300 vehicles/hour on average for the busiest hour. This implies more real traffic data close to 0 vehicles/hour and larger relative errors in the prediction. Large deviation areas like the one in sensor 4458 anticipate larger confidence intervals, while loops like 4192 with narrower deviations and wider dynamic range are expected to provide\nbetter performing and less uncertain models. As a sample of how intervals are produced, a single day of data collected by sensor 4458 is shown on Figure 8, in which the real data and the confidence intervals produced by the combination of conformal prediction (CP) and a Random Forest regressor (RFR) are presented. The interval is highly informative, as its width unveils when the most uncertain predictions are produced, and how uncertain they may be. A prediction at noon with a 90% confidence will be quite uncertain.\nOnce the inherent variability of traffic data has been shown, we proceed by discussing on the quantitative results produced for addressing every research question:"
        },
        {
            "heading": "4.1. RQ1: How do different uncertainty quantification techniques perform when applied to a particular traffic forecasting model?",
            "text": "The first research question regards the performance of the uncertainty estimation techniques considered in our benchmark. As indicated in Table 1, not all of such techniques operate in the same fashion, reason for which the metrics proposed in Section 2.4 are used as a standardized framework that allows comparing them under unified criteria.\nWe start our discussion around RQ1 by inspecting Figure 9, which shows the mean interval length, or the average width of the interval (MIL) obtained\nfor the datasets comprising meteorological and calendar features (m = c = 1), using each predictive model and uncertainty estimation approach. On top of each bar plot, the performance score R2 is shown with a red line. As expected, performance is generally lower for those sensors with lower traffic profiles, particularly for 3500, which scores the worst with R2 values not surpassing beyond 0.7. The lower performance of ADABoost based methods (namely, CP-ABR, E-ABR) is specially visible here, but can be also noticed in the rest of sensors. This predictive method suffers the burden of removing data or removing estimators for estimating uncertainty, and in general is not a suitable option if confidence is to be measured. On the other hand, deep learning approaches (including heteroscedastic regression) present generally wider intervals than conformal prediction approaches, with a noteworthy increase when using evidential formulations of deep learning models. Indeed, the confidence intervals provided by evidential Deep Learning are significantly wider than those elicited by other uncertainty estimation techniques, except for sensor 5761. This can be explained by the renowned sensitivity of evidential formulations with respect to its regularization coefficient \u03bb [78]. This value was left equal to a constant value (1.0) for all experiments so as to expose this known issue.\nOn the other hand, conformal prediction (CP) based methods perform\nvery similarly to each other across different traffic loops in terms of the MIL metric. This holds in all cases except for CP-ETR in two of the sensors (10124 and 3500). The removal of certain nodes that does not happen in the very similar Random Forest model produces extreme peaks in the estimation of the interval boundaries when real observations are very close or equal to 0. This explains these high values that are produced after averaging these peaks with the rest of the points of the boundary, which are essentially almost the same as for Random Forest. Conformal prediction, quantile regression or ensemble methods produce intervals of very similar widths for most of the cases, while Bayesian methods present an irregular behavior in terms of interval width and a subpar predictive performance reflected in generally lower R2 values.\nMeasuring the average interval width through the lens of the MIL metric shows one face of the uncertainty associated to a traffic forecast: the narrower the interval, the more trustworthy the prediction can be thought to be. However, too narrow intervals can leave too many real samples outside them. This reason motivates the adoption of ICP as the second confidence metric for the discussion of this first set of results.\nFigure 10 depicts the distributions of the interval coverage ICP of each method over the different sensors. The red line represents the statistical significance \u03b1 established for the confidence levels. In principle, the coverage of all methods should be close to this line, disregarding the input dataset, for the sake of a reliable behavior. It is noticeable that CP-based methods achieve a stable and close-to-\u03b1 distribution of the ICP metric: this implies that the interval width values discussed before steadily cover 90% of the real traffic samples, rendering them useful to know the model uncertainty. Quantile approaches have a bigger dispersion, specially for deep learning. This means that for intervals with similar width, the amount of real samples that are left out the interval is more dependent on the input data. Results for obtaining uncertainty estimates from the ensemble version of AdaBoost are clearly the worst, with confidence intervals similar in size to others, but covering much less real data (they are wider in some areas but much narrower in others, providing similar average widths, but worse coverage). This is linked to the way in which AdaBoost builds each estimator using information from the previous ones: since it is a sequential process in which every estimator specializes in difficult-to-forecast examples, weak learners composing the boosting ensemble are not by themselves reliable forecasting models [87]. This makes the estimation of uncertainty based on the individual predictions of such\nweak learners highly unreliable. Lastly, we again observe that deep learning based approaches (also HR and BNN) produce larger confidence intervals. The coverage analysis of these wider intervals reveals that, naturally, they cover more real samples. For evidential deep learning, very large intervals cover up to the entirety of the real data. This could appear as convenient, as the intervals cover more real samples, but it is far from informative and actionable, as the intervals do not comply with the established significance level: the desired output is the narrower interval that covers the established amount of samples. This can be appreciated in Figure 11, where intervals produced for two test days in loop 4458 for BNN, EvDL and CP-RFR are compared to each other. Predictive performances appear to be very similar: forecasts of BNN, in purple, show a higher deviation with respect to their ground truth at some points, yielding a slightly lower R2 score. The intervals\nof EvDL appear to be similar for a large part of the day, but they are overestimated in the night periods, leading to less useful confidence information for actionability purposes (e.g. adaptive street lighting in the concerned part of the road network). In the case of BNN, its confidence interval seems to be more similar to that produced by CP-RFR in this particular scenario, but it tends to be wider on the upper boundary, leaving space for more samples, and thereby increasing the value of the ICP metric above its target confidence level \u03b1.\nOn the other side, approaches featuring larger interval widths also present a higher dispersion in their measured ICP values, revealing their higher dependency on the input data. In general, CP-based approaches show the most stable behavior independently of data and underlying model, due to the calibration process whose importance will be analyzed in subsequent sections."
        },
        {
            "heading": "4.2. RQ2: Which changes in the available data affect uncertainty? How can the confidence of a forecasting model be improved by changing/augmenting data at its input?",
            "text": "After exploring a general performance view of all proposed methodological approaches in all available locations, the best performing model and uncertainty estimation technique is chosen in order to delve into the way in which the configuration of the input dataset X affects the uncertainty of forecasts issued by such models. We select the CP-RFR combination (namely, Conformal Prediction with Random Forest Regressor), so that both ICP and MIL metrics are computed over the different dataset configurations resulting from the reduction of the number of input features and the consideration of different prediction horizons h.\nTable 2 summarizes the metrics obtained in all these experiments. Several clear patterns arise in the light of these results. To begin with, an expected degradation of predictive performance occurs for all situations when the predictive horizon h is increased. However, removing features from the original complete dataset has, in most cases, a slim impact in what regards to the\nprediction error: forecasting with all features renders very similar predictive scores that considering only traffic and calendar (i.e., disregarding meteorological features). There are cases in which removing the meteorological features produces better results (sensors 3500 and 3910) for h = 4, from where one may conclude that these features do not contribute significantly to the model\u2019s performance in this circumstances. Analogously, when traffic and meteorological datasets are fed, the differences to the case when only traffic data is used for forecasting are negligible in most cases (with exceptions like the higher predictive horizons for sensor 10124). This minor contribution of weather features to the model\u2019s performance was identified in [88], as a consequence of an highly stable weather in this city, and the inherent relation of certain weather conditions with calendar features. This conclusion should motivate the community to go beyond predictive performance and also examine whether additional features contribute anyhow to the reduction of the uncertainty associated to the forecasts.\nSecondly, when the focus is set on the ICP metric, its steady behaviour throughout the whole set of scenarios is remarkable albeit expected, as the calibration stage of Conformal Prediction is precisely aimed at achieving this. The next section will revolve around this in connection to RQ3, analyzing the consequences of obtaining confidence intervals without any calibration. Furthermore, considering that the values of ICP guarantee that the demanded 90% of true samples are inside the intervals provided in all cases, the width of such intervals can be compared fairly to each other. In general, trends noted in the MIL metric are similar to those found in performance measurements: the more difficult to predict, the wider the interval. However, this measurement provides another perspective of the contribution of different variables in each case. For instance, increasing the prediction horizon from h = 1 to h = 8 results in a R2 degradation of around 0.2 for loops 4458 and 3500. For this similar performance degradation, the confidence interval is 2.27 times wider in h = 8 than in h = 1 for loop 4458, while it is only 1.3 times wider in the other sensor. Thus, performance and confidence intervals do not degrade equally, because they are not co-linear. This means that performance itself is not enough to estimate uncertainty. On the other hand, it is possible to observe some situations in which removing variables favors the confidence; in some of them like in sensor 6980 with forecasting horizon h = 2 the change is small, but the model is more confident without meteorological features: Traffic and calendar presents a narrower interval than all features, and only traffic is also narrower than traffic and meteorological features. This seems\nan interesting way of analyzing whether a set of features do contribute to the confidence of the model. This happens for loop 4458, where weather features have a positive impact in both cases. Moreover, there are cases in which removing a variable results in narrower intervals, such as loop 3697 with meteorological features and only traffic features. This improvement in uncertainty could be due to statistical variance of these particular data (MIL is computed by averaging over thousands of samples), but in any case it reinforces the intuition that these variables have a meager contribution to performance.\nWhen confidence estimation techniques are properly calibrated, the size of the confidence interval can be a valuable indicator of the way in which features contribute to the confidence levels of the output. Thus, besides obtaining an actionable piece of information with the confidence intervals that guarantee the inclusion of a certain amount of true samples, the analysis of these intervals allows for further insights than can be helpful when designing the dataset X for learning the forecasting model. In the particular datasets considered in this study, it is apparent that meteorological features contribute slightly in terms of predictive performance, but having the intervals not only confirms this point, but it even stresses it out for some sensors. For example, loop 5761 has equal results in terms of performance for all features and for traffic and calendar datasets, but the latter presents smaller confidence intervals. In conclusion: meteorological features are only contributing to the uncertainty of traffic forecasts."
        },
        {
            "heading": "4.3. RQ3: What impact does the calibration process of some of the uncertainty estimation techniques have on their outcome? Why is it relevant for traffic forecasting?",
            "text": "The last research question relates to the calibration of models and its impact on the quality of the estimated uncertainty. Certainly, the concept of calibration is not new [89], but is relatively overseen in a broad part of the literature focused on uncertainty estimation. The calibration process aims at guaranteeing the properties of the output interval. In the results shown in previous section it is possible to observe that regardless the dataset attributes, it the confidence estimation technique is calibrated, the output interval always covers a percentage of true samples very close to the significance level established by design. This means that the width of the intervals can be trusted (an important feature for the trustworthiness of the uncertainty estimation and the actionability of forecasts) and always have the\nsame meaning among the considered combinations of models and estimation techniques.\nIn order to assess the relevance of this calibration process, we note that it is an essential part of Conformal Prediction. Therefore, we perform a comparison of the aforementioned CP-RFR to a non-calibrated uncertainty estimation counterpart (E-RFR), as well as to other methods characterized by an unstable ICP behavior in Figure 10, namely, HR and EvDL. The outcomes of this comparison are presented in Table 3, considering the least favorable datasets (only traffic) as the effects of the lack of calibration on the estimated uncertainty become more noticeable with less variability in the input. As the calibration process consists precisely of statistically characterizing the behavior of such a variability, in low-variability scenarios out of distribution data instances may have more weight in the definition of the intervals. Uncertainty estimation techniques that do not consider this can end up providing 1) wider intervals to cover more real samples than the specified percentage (due to the fact that they consider for the intervals points that are outside the significance boundaries in the training data distribution); or 2) narrower intervals that do not meet the statistical significance specifications.\nThe first 4 columns of Table 3 correspond to the calibrated method: in them, it is possible to observe the persistence of the ICP value for all loops and all values of the forecasting horizon h. When considering the same forecasting model (RFR) but using the ensemble technique to estimate the intervals, very similar R2 scores are obtained (as expected, as they essentially resort to the same algorithm for producing the forecasts). However, in this case the whole set of results appears to be under-calibrated, as the coverage of the estimated intervals is lower than the expected one (85%), giving place to narrower intervals. Should these intervals and performance levels be considered to compare them to those elicited by CP-RFR, the comparison would declare ERFR as the best forecasting method. Nonetheless, it is leaving 5% of the real samples outside the coverage of its confidence intervals, thereby penalizing the trustworthiness of the estimated uncertainty. HR seems to work the other way around, by increasing the size of the interval up to covering almost all real cases (ICP close to 99%), and obtaining similar regression results. Lastly, EvDL was shown in the previous section that it failed to provide reliable confidence levels with the complete dataset and horizon h = 1: the reported interval width tended to be wider and ICP values were close to 1 in most cases. When only-traffic data were provided, this method performed erratically.\nApparently, wider intervals and ICP values close to 1 are consistent with h = 1, but while for some sensors this behavior is sustained, others present a notable decay in predictive performance (see the cases of sensors 4458, 6980, 3910, in comparison to other methods), and also in the size of the interval, reducing the coverage to very low levels (even to only 20% of samples in loop 3697). This demeanor is sharper for those sensors with larger dynamic ranges, suggesting the susceptibility of the method to deal with data in the tails of the distribution unless 1) its regularization parameter \u03bb is properly tuned; or 2) by performing a posterior calibration of the model\u2019s output.\nAdditional tests were conducted considering CP-RFR and levels of confidence equal to \u03b1 = 0.85 and \u03b1 = 0.99 in order to assess if such methods are just providing the intervals that are proper for other levels of confidence. Table 4 reports, for one of the sensors, a trend that prevails over the totality of loops analyzed in these experiments: in general, intervals obtained for \u03b1 = 0.9 that cover 85% of the real traffic samples are narrower than confidence intervals provided by CP-RFR for \u03b1 = 0.85. The same behavior occurs when \u03b1 = 0.99. One may arrive at the conclusion that intervals computed by these methods\nfor \u03b1 = 0.9 are better to estimate intervals with support equal to 0.85 and 0.99, as they provide narrower results; however, and unlike with CP, these results lack any statistically guarantees.\nTo further argue on the need for calibrated traffic forecasts, we inspect the relationship between different confidence levels \u03b1 and the observed traffic values for the forecasting horizon values and uncertainty estimation techniques reported in Table 4. Figure 12 shows the calibration curves (also referred to as reliability diagrams for each of these cases. A calibration plot examines whether the confidence interval estimated for a confidence level \u03b1 actually captures a fraction \u03b1 of the observed test values. The area between the curve obtained for different \u03b1 values and the ideal case (a perfectly calibrated model) is a numerical indicator of how miscalibrated the model can be regarded to be. Plots included in this figure reveal that both E-RFR and HR are not properly calibrated for high values of \u03b1: E-RFR is slightly overconfident in its estimated uncertainty (i.e., the computed intervals are too narrow for what they should be), whereas HR is found to be underconfident in this same region of \u03b1 values (namely, its estimated intervals cover are too wide). By contrast, the calibration process performed in conformal prediction allows computing accurate interval estimates for any value of \u03b1, as exposed by the closeness of its calibration curves to the ideal case and the notably smaller miscalibration area annotated inside the plots."
        },
        {
            "heading": "5. Takeaway Messages and Outlook",
            "text": "As in many other areas related to ITS, traffic forecasting has widely embraced the irruption of data-based modeling approaches relying on Machine Learning algorithms. Advances held over the years have achieved in a pursuit towards solutions capable of producing forecasts of ever-growing precision, exploiting efficiently relationships held within traffic data flows over space and time. Lately, performance-driven research studies are progressively steering towards the trustworthiness and actionability of traffic forecasts produced by such data-based models, in view of the narrow performance gaps attained by modern modeling choices.\nIn this context, this paper has aimed at bringing the attention of the community working in traffic forecasting to this matter. Among the manifold perspectives from which one can favor the trustworthiness of data-based models, we have emphasized on the need for quantifying the confidence of the data-based model associated to its predicted traffic values. Assessing the uncertainty propagated to the model\u2019s output allows a traffic manager to better design countermeasures against future congestion events in the road network, delineate better traffic light schedules, decide where to deploy new traffic sensors and collect data therefrom, or quantify whether forecasts can be predicted more confidently if the model is supplied with data supplied by new sources of information. In short, uncertainty quantification (also referred to as model\u2019s confidence) is a key for human decision making based\non the output of traffic forecasting models. Besides providing this rationale, this work provides an overview on the most representative uncertainty estimation methods, as well as quantitative measures to gauge the quality of the estimated confidence intervals. This first part of the manuscript pretends to be a soft entry point to newcomers interested in confidence-aware traffic forecasting, establishing the main motivational reasons for research in the area, essential information pointers, a summary of baseline techniques and a description of evaluation measures and protocols.\nTo further complement this material, a comprehensive set of experiments over real traffic data collected in the city of Madrid (Spain) has been designed, comprising different sets of features, uncertainty estimation techniques and Machine Learning based forecasting models. Results stemming from this setup have lead to several lessons learned about the role of confidence/uncertainty in the actionability of traffic forecasts. In the first place, a diversity of uncertainty estimation techniques has been compared to each other, evincing that there is not a single source of uncertainty, nor is there a unique way of approaching its calculation. Nevertheless, the metrics used to assess the validity and informational efficiency of the confidence intervals estimated by such techniques has uncovered that Conformal Prediction is more stable and reliable than approaches based on the ablation of models to statistically characterize their output (e.g. ensemble models or Monte Carlo dropout), but also better than models specifically suited for the purpose (e.g., Bayesian Neural Networks). Besides, Conformal Prediction yields a transparent and traceable way of obtaining confidence intervals, and the calibration stage that lies at the core of its procedure helps maintain the size of the intervals to the minimum that guarantees the coverage of unseen test samples. This feature, combined with its model-agnostic nature, renders Conformal Prediction as a very interesting option for estimating the uncertainty of traffic forecasts. On the other side, methods like evidential Deep Learning fail to produce reliable intervals due to its known susceptibility to the value of its parameters and the suitability of the evidential priors. Other methods yield different levels of quality in regards to their estimated confidence intervals, which depend on the characteristics present in the input data from which traffic forecasts are predicted.Our experiments have also confirmed that calibration is essential to reliably estimate the uncertainty of forecasts for a given confidence level.\nFurther along this line, we have verified that the particularities of the dataset at hand may affect the amount of uncertainty associated to the\nmodel\u2019s output. In the traffic dataset used for the experiments, the stability and inherent predictive potential of the traffic time series cause that other sources of data that could potentially help to deliver more precise forecasts have a low relevance and a low impact in uncertainty. In general, short-term traffic forecasting models can benefit marginally from additional sources of data, due to the acknowledged relevance of the time series variables that take part in the model. However, there are cases in which the impact becomes noticeable in the precision of traffic forecasts, whereas in other cases the addition of new data sources produces similar performance results, yet worst confidence intervals. These cases prove that the composition of the datasets used for traffic forecasting can also be driven by the examination of its consequences for the uncertainty of the model. Therefore, a principled assessment of the uncertainty of traffic forecasts throughout the modeling pipeline and the actionability that it grants should be of pivotal importance in future studies, considering it as an additional dimension in comparison benchmarks, and as a criterion to decide whether to include new sources of information (e.g. social media, traffic cameras) that may jeopardize the confidence of the model in its predictions.\nBeyond the questions addressed experimentally in this research work, other applications of uncertainty estimation that were presented in Section 2 have not been explored to date. We envision a rich agenda, plenty of uncharted research lines, related to confidence-aware traffic forecasting. Among them, we remark the high stability of properly calibrated intervals, which can be helpful for trend change detection over traffic data streams. Indeed, a continuously flowing stream of traffic data should fall within the estimated confidence interval, since it statistically represents the regular behavior of traffic data in the location of interest. If real samples eventually start falling out the confidence interval, this can be symptomatic of a contextual change of the traffic behavior (due to e.g., roadworks or any exceptional circumstance like an accident), so that a closer inspection can be enforced or an update of the traffic forecasting model be triggered upon its occurrence. Lastly, a promising application of uncertainty estimation techniques arises from its natural connection to the active learning research area: by measuring confidence intervals over different regions of the feature space at the input of the model, one can identify where most of its uncertainty is concentrated, contributing to the identification of suitable locations in the road network and/or periods in time where more traffic data should be collected. This information can be ultimately be used for designing new traffic measurement campaigns with\nprovisionally deployed sensors over the city, showcasing another purpose for which uncertainty estimations can effectively contribute to the actionability of traffic forecasts."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work has received funding support from the European Union\u2019s Horizon 2020 program (project URBANITE: Supporting the decision-making in urban transformation with the use of disruptive technologies, grant agreement 870338), as well as from the Basque Government (ELKARTEK program and the Consolidated Research Group MATHMODE, ref. IT1256-22)."
        }
    ],
    "title": "Measuring the Confidence of Traffic Forecasting Models: Techniques, Experimental Comparison and Guidelines towards Their Actionability",
    "year": 2022
}