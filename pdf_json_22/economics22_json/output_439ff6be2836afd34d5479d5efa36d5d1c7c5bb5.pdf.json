{
    "abstractText": "We suggest a simple practical method to combine the human and artificial intelligence to both learn best investment practices of fund managers, and provide recommendations to improve them. Our approach is based on a combination of Inverse Reinforcement Learning (IRL) and RL. First, the IRL component learns the intent of fund managers as suggested by their trading history, and recovers their implied reward function. At the second step, this reward function is used by a direct RL algorithm to optimize asset allocation decisions. We show that our method is able to improve over the performance of individual fund managers.",
    "authors": [
        {
            "affiliations": [],
            "name": "Igor Halperin"
        },
        {
            "affiliations": [],
            "name": "Jiayu Liu"
        },
        {
            "affiliations": [],
            "name": "Xiao Zhang"
        }
    ],
    "id": "SP:cedcefe4b8d7613605e13ebea8df66eadfa9a3bc",
    "references": [
        {
            "authors": [
                "D. Brown",
                "W. Goo",
                "P. Nagarajan",
                "S. Niekum"
            ],
            "title": "Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "M. Dixon",
                "I. Halperin"
            ],
            "title": "Goal-Based Wealth Management with Generative Reinforcement Learning",
            "venue": "RISK, July 2021,",
            "year": 2021
        },
        {
            "authors": [
                "M. Dixon",
                "I. Halperin",
                "P. Bilokon"
            ],
            "title": "Machine Learning in Finance: from Theory to Practice",
            "year": 2020
        },
        {
            "authors": [
                "R. Fox",
                "A. Pakman",
                "N. Tishby"
            ],
            "title": "Taming the Noise in Reinforcement Learning via Soft Updates",
            "venue": "In Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence,",
            "year": 2016
        },
        {
            "authors": [
                "E.J. Hannan"
            ],
            "title": "Multiple Time Series, volume 38",
            "year": 2009
        },
        {
            "authors": [
                "D.A. Pomerleau"
            ],
            "title": "Efficient Training of Artificial Neural Networks for Autonomous Navigation",
            "venue": "Neural computation,",
            "year": 1991
        },
        {
            "authors": [
                "S. Ross",
                "G. Gordon",
                "D. Bagnell"
            ],
            "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning",
            "venue": "In International conference on artificial intelligence and statistics,",
            "year": 2011
        },
        {
            "authors": [
                "F. Torabi",
                "G. Warnell",
                "P. Stone"
            ],
            "title": "Behavioral Cloning from Observation",
            "venue": "In International Joint Conference on Artificial Intelligence,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Portfolio management is a quintessential example of stochastic multi-period (dynamic) optimization, also known as stochastic optimal control. Reinforcement Learning (RL) provides data-driven methods for problems of optimal control, that are able to work with high dimensional data. Such applications are beyond the reach of classical methods of optimal control, which typically only work for lowdimensional data. Many of the most classical problems of quantitative finance such as portfolio optimization, wealth management and option pricing can be very naturally formulated and solved as RL problems, see e.g. (Dixon et al., 2020).\nA critically important input to any RL method is a reward function. The reward function provides a condensed formulation of the agent\u2019s intent, and respectively it informs the optimization process of RL. The latter amounts to finding policies that maximize the expected total reward obtained in a course of actions by the agent. One simple example of a reward function for RL of portfolio management is\n1AI Center of Excellence for Asset Management, Fidelity Investments. Emails: Igor Halperin, JiayuLiu, Xiao Zhang <igor.halperin@fmr.com, jiayu.liu@fmr.com, xiao.zhang2@fmr.com>.\nUnder Review\nprovided by the classical Markowitz mean-variance objective function applied in a multi-period setting ((Dixon et al., 2020)). However, there might exist other specifications of the reward function for portfolio management that might be more aligned with the actual practices of active portfolio managers.\nInverse Reinforcement Learning (IRL) is a sub-field of RL that addresses the inverse problem of inferring the reward function from a demonstrated behavior. The behavioral data are given in the form of sequences of state-action pairs performed either by a human or AI agent. IRL is popular in robotics and video games for cases where engineering a good reward function (one that promotes a desired behavior) may be a challenging task on its own. The problem of dynamic portfolio optimization arguably belongs in such class of problems. Indeed, while the \u2019best\u2019 objective function to be used for portfolio optimization is unknown, we can try to use IRL to learn it from the investment behavior of active portfolio managers (PMs).\nIn this paper, we propose a combined use of IRL and RL (in that sequence) to both learn the investment strategies of portfolio managers, and refine them, by suggesting tweaks that improve the overall portfolio performance. Our approach can be viewed as a way to develop a \u2018collective intelligence\u2019 for a group of fund managers with similar investment philosophies. By using trading histories of such funds, our algorithm learns the reward function from all of them jointly in the IRL step, and then uses this reward function to optimize the investment policy in the second, RL step.\nWe emphasize that our approach is not intented to replace active portfolio managers, but rather to assist them in their decision making. In particular, we do not re-optimize the stock selection made by PMs, but simply change allocations to stocks that were already selected. As will be explained in more details below, data limitations force us to work with a dimensionally reduced portfolio optimization problem. In this work, we choose to aggregate all stocks in a given portfolio by their industry sector. This provides a low-dimensional view of portfolios held by active managers. Respectively, the optimal policy is given in terms of allocations to industrial sectors, and not in terms of allocations to\nar X\niv :2\n20 1.\n01 87\n4v 1\n[ cs\n.L G\n] 6\nJ an\n2 02\n2\nindividual stocks. However, given any portfoflio-specific selection of stocks for a given sector, a recommended optimal sector-level allocation can be achieved simply by changing positions in stocks already selected.1\nOur paper presents a first practical example of a joined application of IRL and RL for asset allocation and portfolio management that leverages human intelligence and transfers it, in a digestible form, to an AI system (our RL agent). While we use specific algorithms for the IRL and RL steps (respectively, the T-REX and G-learner algorithms, see below), our framework is general and modular, and enables using of other IRL and RL algorithms, if needed. We show that a combined use of IRL and RL enables to both learn best investment practices of fund managers, and provide recommendations to improve them.\nThe paper is organized as follows. In Sect. 1.1 we overview previous related work. Sect. 2 outlines the theoretical framework of our approach, presenting the parametric T-REX algorithm for the IRL part, and the G-learner algorithm for the RL part. Sect. 3 shows the results of our analysis. The final Sect. 4 provides a summary and conclusions."
        },
        {
            "heading": "1.1. Related Work",
            "text": "Our approach combines approaches developed separately in the research community focused on RL and IRL problems. For a general review of RL and IRL along with their applications to finance, see (Dixon et al., 2020).\nIn its reliance on a combination of IRL and RL for optimization of a financial portfolio, this paper follows the approach of (Dixon & Halperin, 2021). The latter uses a continuous state-action version of G-learning, a probabilistic extension of Q-learning (Fox et al., 2016), for the RL step, and an IRL version of G-learning called GIRL for the IRL step. In this work, we retain the G-learner algorithm from (Dixon & Halperin, 2021) for the RL step, but replace the IRL step with a parametric version of a recent algorithm called TREX (Brown et al., 2019) which will be presented in more details in Sect. 2.1."
        },
        {
            "heading": "2. The method",
            "text": "In this section, we describe our proposed framework for asset allocation recommendations as a sequence that combines IRL and RL in a two-step procedure. The framework is designed to be generic enough to accommodate various IRL and RL methods. In this work, we focus on an extension of the approach of (Dixon & Halperin, 2021) that incorporates portfolio performance ranking information into\n1This assumes that stocks from all sectors are already present in the portfolio, and that changes of positions in individual stocks do not produce a considerable price impact.\nits policy optimization algorithm. This extension is inspired by a recently proposed IRL method called T-REX (Brown et al., 2019). Here we first provide a short overview of T-REX and G-learner methods, and then introduce our IRLRL framework. In its IRL step, our proposed parametric T-REX algorithm infers PMs\u2019 asset allocation decision rules from portfolio historical data, and provides a high quality reward function to evaluate the performance. The learned reward function is then utilized by G-learner in the RL step to learn the optimal policy for asset allocation recommendations. Policy optimization amounts to a recursive procedure involving only linear algebra operations, which can be computed within seconds."
        },
        {
            "heading": "2.1. The IRL step with parametric T-REX",
            "text": "IRL deals with recovering reward functions from the observed agents\u2019 behavior as a way of rationalizing their intent. The learned reward function can then be used for evaluation and optimization of action policies. The IRL approach reduces the labor of defining proper reward functions in RL for applications where their choice is not immediately obvious.\nMany traditional IRL methods try to learn the reward function from demonstrations under the assumption that the demonstrated history corresponds to an optimal or nearoptimal policy employed by an agent that produced the data, see e.g. (Pomerleau, 1991; Ross et al., 2011; Torabi et al., 2018). These IRL methods therefore simply try to mimic or justify the observed behavior by finding a proper reward function that explains this behavior. This implies that by following their approach, the best one can hope for IRL for investment decisions is that it would be able to imitate investment policies of asset managers.\nHowever, the assumption that the demonstrated behavior is already optimal might not be realistic, or even verifiable, for applications in quantitative finance. This is because, unlike robotics or video games, the real market environment cannot be reproduced on demand. Furthermore, relying on this assumption for inferring rewards may also have a side effect of transferring various psychological biases of asset managers into an inferred \u2018optimal\u2019 policy.\nInstead of simply trying to mimic the observed behavior of asset managers, it would be highly desirable to try to improve over their investment decisions, and potentially correct for some biases implicit in their decision-making. A principled way towards implementing such a program would be to infer the intent of asset managers from observing their trading decisions without the assumption they their strategies were fully or partially successful. With such an approach, demonstrated trajectories would be scored by how well they succeed in achieving a certain goal. This is clearly very different from seeking a reward function that would\nsimply rationalize the observed behavior of asset managers.\nSuch inference of agents\u2019 intent is made possible using a recently proposed IRL algorithm called T-REX, for Trajectorybased Reward Extrapolation (Brown et al., 2019). The TREX method finds the reward function that captures the intent of the agent without assuming a near-optimality of the demonstrated behavior. Once the reward function that captures agent\u2019s intent is found, it can be used to further improve the performance by optimizing this reward. This could be viewed as extrapolation of the reward beyond the demonstrated behavior, thus explaining the \u2018EX\u2019 part in the name T-REX.\nTo overcome the issue of potential sub-optimality of demonstrated behavior, T-REX relies on certain extra information that is not normally used in other IRL methods. More specifically, T-REX uses rankings of all demonstrated trajectories according to a certain final score. The final score is assigned to the whole trajectory rather than to a single time-step. The local reward function is then learned from the condition that it should promote the most desirable trajectories according to the score. In this work, we score different portfolio trajectories by their total realized returns, however, we could also use risk-adjusted metrics such as the Sharpe or Sortino ratios.\nLet O : {S,A} be a state-action space of an MDP environment, and r\u0302\u03b8(\u00b7) with parameters \u03b8 be a target reward function to be optimized in the IRL problem. Given M ranked observed sequences {om}Mm=1 (oi \u227a oj if i < j, where \u201c\u227a\u201d indicates the preferences, expressed e.g. via a ranking order, between pairwise sequences), T-REX conducts reward inference by solving the following optimization problem:\nmax \u03b8 \u2211 oi\u227aoj log e \u2211 {s,a}\u2208oj r\u0302\u03b8(s,a) e \u2211 {s,a}\u2208oi r\u0302\u03b8(s,a) + e \u2211 {s,a}\u2208oj r\u0302\u03b8(s,a)\n(1) This objective function is equivalent to the softmax normalized cross-entropy loss for a binary classifier, and can be easily trained using common machine learning libraries such as PyTorch or TensorFlow. As a result, the learned optimal reward function can preserve the ranking order between pairs of sequences.\nThe original T-REX algorithm is essentially a nonparametric model that encodes the reward function into a deep neural network (DNN). This might be a reasonable approach for robotics or video games where there is a plenty of data, while the reward function might be highly nonlinear and complex. Such setting is however ill-suited for applications to portfolio management where the amount of available data is typically quite small. Therefore, in this work we pursue a parametric version of T-REX, where the reward function is encoded into a function with a small\nnumber of parameters, which is then optimized using Eq.(1). A particular model of the reward function that will be presented in the next section has only 4 tunable parameters, which appears to be about the right order of model complexity given available data. The added benefit in comparison to the original DNN-based implementation of T-REX is that a parametric T-REX is much faster to train."
        },
        {
            "heading": "2.2. The RL step with G-learner",
            "text": "G-learner is an algorithm for the direct RL problem of portfolio optimizaiton that was proposed in (Dixon & Halperin, 2021). It relies on a continuous state-action version of Glearning, a probabilistic extension of Q-learning designed for noisy environments (Fox et al., 2016). G-learner solves a finite-horizon direct RL problem of portfolio optimization in a high-dimensional continuous state-action space using a sample-based approach that operates with available historical data of portfolio trading. The algorithm uses a closed-form state transition model to capture the stochastic dynamics in a noisy environment, and therefore belongs in the class of model-based RL approaches. In this work, we employ G-learner as a RL solver for the sequential portfolio optimization, while defining a new reward function to evaluate portfolio managers\u2019 performance.\nUnlike (Dixon & Halperin, 2021) that considered G-learning for a portfolio of individual stocks held by a retail investor, here we consider portfolios typically held by professional asset managers. Due to data limitations, it is unfeasible to estimate models that track the whole investment universe counting thousands of stocks, and maintain individual stocks holdings and their changes as, respectively, state and action variables. A viable alternative is is to aggregate individual stock holdings into buckets constructed according to a particular dimension reduction principle, and define the state and action variables directly in such dimensionally reduced space. In this paper, we choose to aggregate all stocks in the PM\u2019s portfolio into N = 11 sectors of the standard GICS industry classification, effectively mapping the portfolio into portfolio of sector exposures. The state vector xt \u2208 RN is respectively defined as a vector of dollar values of stock positions in each sector at time t. The action variable ut \u2208 RN is given by the vector of changes in these positions as a result of trading at time step t. Furthermore, let rt \u2208 RN represent the asset returns as a random variable with the mean r\u0304t and covariance matrix \u03a3r . As in our approach we identify assets with sector exposures, r\u0304t and \u03a3r will be interpreted as expected sector returns and sector return covariance, respectively.\nThe state transition model is defined as follows:\nxt+1 = At(xt + ut),At = diag(1 + rt) (2)\nIn this work we use a simplified form of the reward from\n(Dixon & Halperin, 2021), which we write as follows:\nRt(xt,ut|\u03b8) =\u2212 Et [( P\u0302t \u2212 Vt )2] \u2212 \u03bb \u00b7 ( 1Tut \u2212 Ct\n)2 \u2212 \u03c9 \u00b7 uTt ut (3) whereCt is a money flow to the fund at time step tminus the change of a cash position of the fund at the same time step, \u03bb and \u03c9 are parameters, and Vt and P\u0302t are, respectively, the values of asset manager\u2019s and reference portfolios, defined as follows:\nVt = (1 + rt) T (xt + ut) P\u0302t = \u03c1 \u00b7Bt + (1\u2212 \u03c1) \u00b7 \u03b7 \u00b7 1Txt (4)\nwhere \u03b7 and \u03c1 are additional parameters, and Bt is a benchmark portfolio, such as e.g. the SPX index portfolio, properly re-scaled to match the size of the PM\u2019s portfolio at the start of the investment period.\nThe reward function (3) consists of three terms, each encoding one of the portfolio managers\u2019 trading insights. In the first term, P\u0302t defines the target portfolio market value at time t. It is specified as a linear combination of a reference benchmark portfolio value Bt and the current portfolio growing with rate \u03b7 according to Eq.(4), where \u03c1 \u2208 [0, 1] is a parameter defining the relative weight between the two terms. Vt gives the portfolio value at time t+ \u2206t, after the trade ut is made at time t. The first term in (3) imposes a penalty for under-performance of the traded portfolio relative to its moving target. The second term enforces the constraint that the total amount of trades in the portfolio should match the inflow Ct to the portfolio at each time step, with \u03bb being a parameter penalizing violations of the equality constraint. The third term approximates transaction costs by a quadratic function with parameter \u03c9, thus serving as a L2 regularization. The vector \u03b8 of model parameters thus contains four reward parameters {\u03c1, \u03b7, \u03bb, \u03c9}.\nImportantly, the reward function (3) is a quadratic function of the state and action. Such a choice of the reward function implies that the value- and action-value functions are similarly quadratic functions of the state and action, with time-dependent coefficients. Respectively, with a quadratic reward functions such as (3), a G-learning algorithm should not engage neural networks or other function approximations, which would invariably bring their own parameters to be optimized. In contrast, with the quadratic reward (3), learning the optimal value and action-value functions amounts to learning the coefficients of quadratic functions, which amounts to a small number of linear algebra operations (Dixon et al., 2020). As a result, an adaptation of the general G-learning method for the quadratic reward (3) produces a very fast policy optimization algorithm called G-learner in (Dixon & Halperin, 2021).\nG-learner is applied to solve the proposed RL problem with our defined MDP model in Eq.(2) and reward function in Eq.(3). Algorithm 1 demonstrates the entire optimization solving process. It takes the model parameters as inputs, along with discount factor \u03b3. In addition, it uses a prior policy \u03c0(0) which usually encodes domain knowledge of real world problems. G-learner (and G-learning in general) control the deviation of the optimal policy \u03c0t from the prior \u03c0(0) by incorporating the KL divergence of \u03c0t and \u03c0(0) into a modified, regularized reward function, with a hyperparameter \u03b2 that controls the magnitude of the KL regularizer. When \u03b2 is large, the deviation can be arbitrarily large, while in the limit \u03b2 \u2192 0, \u03c0t is forced to be equal to \u03c0(0), so there is no learning in this limit. Furthermore, Ft(xt) is the value function, andGt(xt,ut) is the actionvalue function. The algorithm is initialized by computing the optimal terminal value and action-value functions F \u2217T andG\u2217T . They are computed by finding the optimal last-step action aT that achieves the highest one-step terminal reward, i.e. solves the equation \u2202Rt(xt,ut)\u2202ut |t=T = 0. Thereafter, the policy associated with earlier time steps can be derived recursively backward in time as shown in the for loop of Algorithm 1. For a detailed derivation of V alue Update and ActionV alue Update steps, we refer to Section 4 in (Dixon & Halperin, 2021).\nAlgorithm 1 G-learner policy optimization Input :\u03bb, \u03c9, \u03b7, \u03c1, \u03b2, \u03b3, {r\u0304t,xt,ut, Bt, Ct}Tt=0,\u03a3r,\u03c0(0) Output :\u03c0\u2217t = \u03c0(0) \u00b7 e\u03b2(G \u2217 t\u2212F \u2217 t ), t = 0, \u00b7 \u00b7 \u00b7 , T Initialize: F \u2217T ,G\u2217T while not converge do\nfor t \u2208 [T-1, -1, 0] do Ft \u2190 Value Update(Ft+1,Gt+1) Gt \u2190 ActionValue Update(Ft,Gt+1)\nend end return {F \u2217t ,G\u2217t ,\u03c0\u2217t }Tt=0\nOnce the optimal policy \u03c0\u2217t = \u03c0 \u2217 t (ut|xt) is learned, the recommended action at time t is given by the mode of the action policy for the given state xt."
        },
        {
            "heading": "2.3. A unified IRL-RL framework",
            "text": "Figure 1 illustrates the overall workflow of our proposed IRL-RL framework. We apply this framework to provide portfolio allocation recommendations to improve fund returns. It takes the observed M state-action sequences (om : {xt,ut}Tt=0) from multiple funds as input training data. As cumulative funds\u2019 returns can be used to measure the overall PM\u2019s performance over a given observation period, we use the realized total returns in the train dataset as a ranking criterion for the IRL step with the parametric T-REX. As an alternative, we could rely on risk-adjusted\nmetrics such as e.g. the Sharpe or Sortino ratios.\nWith the chosen ranking criterion, the T-REX IRL module finds the optimal parameters of the reward function defined in Eq.(3). The optimal reward function parameters are then passed to the RL module which optimizes the action policy. Both modules operate in the offline learning mode, and do not require an online integration with the trading environment.\nThe IRL module can also work individually to provide behavioral insights via analysis of inferred model parameters. It is designed to be generalizable to different reward functions, if needed. Ranking criteria need to be rationally determined to align with the logic of rewards and depict the intrinsic behavioral goal of demonstrations. The humanprovided ranking criterion based on the performance on different portfolio paths thus informs a pure AI system (our G-learner agent). As a result, our IRL/RL pipeline implements a simple human-machine interaction loop in the context of portfolio management and asset allocation.\nWe note that by using T-REX with the return-based scoring, our model already has a great starting point to hope for success. Indeed, as our reward function and ranking are aligned with PMs\u2019 own assessment of their success, this already defines a \u2018floor\u2019 for the model performance. This is because our approach is guaranteed to at least match individual funds\u2019 performance by taking the exact replica of their actions. Therefore, our method cannot do worse, at least in-sample, than PMs but it can do better - which is indeed the case as will be shown below.\nAnother comment due here is that while in this work we use the aggregation to the sectors as a particular scheme for dimension reduction, our combined IRL-RL framework could also be used with other ways of dimension reduction, for example we could aggregate all stocks by their exposure to a set of factors."
        },
        {
            "heading": "3. Experiments",
            "text": "This section describes our experiments. First, we explain data pre-processing steps. Once the state and action variables are computed, we sequentially apply the parametric T-REX and G-Learning algorithms according to Eq.(1) and Algorithm 1, respectively. We then show how portfolio managers\u2019 insights can be inferred using T-REX, and then demonstrate that G-learner is able to outperform active managers by finding optimal policies for rewards inferred at the first stage."
        },
        {
            "heading": "3.1. Data Preparation",
            "text": "In our experiments, two sets of mutual funds are chosen based on their benchmark indexes. To anonymize our data, we replace actual funds\u2019 names by single-letter names. The first set includes six funds ({Si}6i=1) that all have the S&P 500 index as a benchmark. The second set includes twelve funds that have the the Russell 3000 index as a benchmark. The second group is further divided into growth and value fund groups ({RGi}7i=1 and {RVi}5i=1). Each fund\u2019s trading trajectory covers the period from January 2017 to December 2019, and contains its monthly holdings and trades for eleven sectors (i.e., xt,ut) as well as monthly cashflows Ct. The first two years of data are used for training (so that we choose T=24 months), and the last year starting from January 2019 is used for testing. At the starting time step we assign each fund\u2019s total net asset value to its corresponding benchmark value (i.e., Bt=0) in order to align their size and use the actual benchmark return at each time step to calculate their values afterwards (i.e., t > 0). The resulting time series {xt,ut, Bt, Ct}Tt=0 are further normalized by dividing by their initial values at t = 0. In the training phase, the canonical ARMA model (Hannan, 2009) is applied to forecast expected sector returns rt, and the regression residue is then used to estimate the sector return covariance matrix \u03a3r . The prior policy \u03c0(0) is fitted to a multivariate Gaussian distribution with a constant mean and variance calculated from sector trades in the training set."
        },
        {
            "heading": "3.2. Parametric T-REX with portfolio return rankings",
            "text": "The pre-processed fund trading data is ranked based on funds\u2019 overall performance realized over the training period. T-REX is then trained to learn the four parameters {\u03c1, \u03b7, \u03bb, \u03c9} that enter Eq.(3). The training/test experiments are executed separately for all three fund sets (i.e. funds\nwith the S&P 500 benchmark, Russell 3000 Growth funds, and Russell 3000 Value funds).\nRecall that the loss function for T-REX is given by the crossentropy loss for a binary classifier, see Eq.(1). Figure 2 shows the classification accuracy in the training (91.1%) and test phases (83.2%) for the S&P 500 fund set. The plots in the two sub-figures show that the learned reward function preserves the fund ranking order. Correlation scores are used to measure their level of association, producing the values of 0.988 and 0.954 for the training and test sets, respectively. The excellent out-of-sample correlation scores gives a strong support to our chosen method of the reward function design.\nFund managers\u2019 investment strategies can be analyzed using the inferred reward parameters. Figure 3 shows the parameter convergence curve using the S&P 500 fund group. The values converge after about thirty iterations, which suggests adequacy of our chosen model of the reward to the actual trading data. The optimized benchmark weight \u03c1\u2217 = 0.951 captures the correlation between funds\u2019 performance and their benchmark index. Its high value implies that portfolio managers\u2019 trading strategy is to target an accurate benchmark tracking. The value \u03b7\u2217 = 1.247 implies funds are in the growing trend with prospective rate at 24.7% from fund managers\u2019 view. Since the sum of trades are close enough to the fund cashflows in our dataset, the introduced penalty parameter \u03bb converges to a very small value at 0.081. The estimated average trades\u2019 volatility \u03c9\u2217 is around 10%, which measures the level of consistency across trades in the S&P 500 fund group.\nThe experimental results of our proposed parametric T-REX are summarized in Table 1. Our model achieved high classification accuracy (acc) and correlation scores (cor ) on all three fund groups in both training and test phases. The optimal values of reward function parameters are listed in the table as well. Small values of \u03bb\u2217 verify the holding of equality constraint over the sum of trades in our dataset. Comparing across different fund groups, we can identify different trading strategies of portfolio managers. Funds in the S&P500 group are managed to track their benchmark very closely with correlation \u03c1\u2217 = 0.951. On the other hand,\ncorrelation \u03c1\u2217 reduces significantly to 0.584 and then to 0.186 for fund groups {RGi}7i=1 and {RVi}5i=1 which have the same benchmark index Russell 3000, while being segmented based on their investment approaches (i.e., growth vs. value). For the bucket denoted {Si}6i=1, growth and blend funds are combined. As shown in the table, funds in the segmented groups show very limited variations of their trading amounts compared to those combined in a single group. This can be seen as an evidence of fund managers\u2019 consistency in their investment decisions/policies. Growth funds in {RG}7i=1 with \u03b7\u2217 = 1.532 produce the highest expected growth rates as implied by their trading activities. This result is in line with the fact that growth investors select companies that offer strong earnings growth while value investors choose stocks that appear to be undervalued in the marketplace.\nThe four reward parameters can be used to group different fund managers or different trading strategies into distinct clusters in the space of reward parameters. While the present analysis in this paper suggests pronounced patterns based on the comparison across three fund groups, including more fund trading data from many portfolio managers might be able to bring further insights into fund managers\u2019 behavior.\nThis is left here for a future work."
        },
        {
            "heading": "3.3. G-learner for allocation recommendation",
            "text": "Once the reward parameters are learned for all groups of funds, they are passed to G-learner for the policy optimization step. The pre-processed fund trading data in year 2017 and 2018 is used as inputs for policy optimization (see Figure 1). Trading trajectories were truncated to twelve months\u2019 long in order to align with the length of the test period (year 2019). The optimal policy\u03c0\u2217t = \u03c0 \u2217 t (ut|xt) is learned using the train dataset, and then applied to the test set throughout the entire test period. Differently from the training phase, where we use expected sector returns in the reward function, in the test phase we use realized monthly returns to derive the next month\u2019s holdings xt+1 after trades ut was made for the current month with xt. At the start time of the test period, the holdings xt=0 coincide with actual historical holdings. We use this procedure along with the learned policy \u03c0\u2217t (ut|xt) onward from February 2019 to create counterfactual portfolio trajectories for the test period. We will refer to these RL-recommended trajectories as the AI Alter Ego\u2019s (AE) trajectories.\nFigures 4, 5, 6 show the outperformance of our RL policy from PM\u2019s trading strategies (i.e, AE \u2212 PM ) for all three fund groups for both the training and test sets for forward period of 12 months, plotted as functions of their time arguments. We note that both curves for the training and test sets follow similar paths for at least seven months before they started to diverge (the curves for Russell 3000 value group diverged significantly after the eighth months\u2019 trades and thus were truncated for a better visualization effect). In practice, we recommend to update the policy through a re-training once the training and test results start to diverge. In general, our results suggest that our G-learner is able to generalize (i.e. perform well out-of-sample) up to 6 months into the future.\nAnalysis at the individual fund level for all three groups is presented in Figures 7, 9, 11 for the training set, and Figures 8, 10, 12 for the test set. Our model outperformed most of PMs throughout the test period except funds RG5 and RV5. It is interesting to note that the test results outperformed the training ones for the S&P 500 fund group. This may be due to a potential market regime drift (which is favorable to us, in the present case). More generally, a detailed study would be desired to address the impact of potential market drift or a regime change on optimal asset allocation policies. Such topics are left here for a future research."
        },
        {
            "heading": "4. Conclusion and Future Work",
            "text": "In this work, we presented a first practical two-step procedure that combines the human and artificial intelligence\nfor optimization of asset allocation and investment portfo-\nlios. At the first step, we use inverse reinforcement learning (IRL), and more specifically the parametric T-REX algo-\nrithm, to infer the reward function that captures fund managers\u2019 intent of achieving higher returns. At the second step,\nwe use the direct reinforcement learning with the inferred reward function to improve the investment policy. Our approach is modular, and allows one to use different IRL or RL models, if needed, instead of the particular combination of the parametric T-REX and G-learner that we used in this work. It can also use other ranking criteria, e.g. sorting by the Sharpe ratio or the Sortino ratio, instead of ranking trajectories by their total return as was done above.\nUsing aggregation of equity positions at the sector level, we showed that our approach is able to outperform most of fund managers. Outputs of the model can be used by individual fund managers by keeping their stock selection and re-weighting their portfolios across industrial sectors according to the recommendation from G-learner. A practical implementation of our method should involve checking the feasibility of recommended allocations and controlling for potential market impact effects and/or transaction costs resulting from such rebalancing.\nFinally, we note that while in this work we choose a particular method of dimensional reduction that aggregates all stocks at the industry level, this is not the only available choice. An alternative approach could be considered, where all stocks are aggregated based on their factor exposure. This is left here for a future work."
        }
    ],
    "title": "Combining Reinforcement Learning and Inverse Reinforcement Learning for Asset Allocation Recommendations ",
    "year": 2022
}