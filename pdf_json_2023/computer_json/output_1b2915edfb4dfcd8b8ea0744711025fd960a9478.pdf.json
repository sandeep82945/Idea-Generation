{
    "abstractText": "Handwritten mathematical expression recognition (HMER) has attracted extensive attention recently. However, current methods cannot explicitly study the interactions between different symbols, which may fail when faced similar symbols. To alleviate this issue, we propose a simple but efficient method to enhance semantic interaction learning (SIL). Specifically, we firstly construct a semantic graph based on the statistical symbol co-occurrence probabilities. Then we design a semantic aware module (SAM), which projects the visual and classification feature into semantic space. The cosine distance between different projected vectors indicates the correlation between symbols. And jointly optimizing HMER and SIL can explicitly enhances the model\u2019s understanding of symbol relationships. In addition, SAM can be easily plugged into existing attention-based models for HMER and consistently bring improvement. Extensive experiments on public benchmark datasets demonstrate that our proposed module can effectively enhance the recognition performance. Our method achieves better recognition performance than prior arts on both CROHME and HME100K datasets.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhuang Liu"
        },
        {
            "affiliations": [],
            "name": "Ye Yuan"
        },
        {
            "affiliations": [],
            "name": "Zhilong Ji"
        },
        {
            "affiliations": [],
            "name": "Jinfeng Bai1[0000\u22120001\u22128940\u2212480X"
        }
    ],
    "id": "SP:15a5cc71b93b42e5e31901467c97f32279fc9c90",
    "references": [
        {
            "authors": [
                "F. Alvaro",
                "J.A. S\u00e1nchez",
                "J.M. Bened\u0301\u0131"
            ],
            "title": "Recognition of on-line handwritten mathematical expressions using 2d stochastic context-free grammars and hidden markov models",
            "venue": "Pattern Recognition Letters 35, 58\u201367",
            "year": 2014
        },
        {
            "authors": [
                "R.H. Anderson"
            ],
            "title": "Syntax-directed recognition of hand-printed two-dimensional mathematics",
            "venue": "Symposium on Interactive Systems for Experimental Applied Mathematics: Proceedings of the Association for Computing Machinery Inc. Symposium. pp. 436\u2013459",
            "year": 1967
        },
        {
            "authors": [
                "X. Bian",
                "B. Qin",
                "X. Xin",
                "J. Li",
                "X. Su",
                "Y. Wang"
            ],
            "title": "Handwritten mathematical expression recognition via attention aggregation based bi-directional mutual learning",
            "venue": "Proc. of the AAAI Conf. on Artificial Intelligence. pp. 113\u2013121",
            "year": 2022
        },
        {
            "authors": [
                "K.F. Chan",
                "D.Y. Yeung"
            ],
            "title": "Elastic structural matching for online handwritten alphanumeric character recognition",
            "venue": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No. 98EX170). vol. 2, pp. 1508\u20131511. IEEE",
            "year": 1998
        },
        {
            "authors": [
                "K.F. Chan",
                "D.Y. Yeung"
            ],
            "title": "Error detection, error correction and performance evaluation in on-line mathematical expression recognition",
            "venue": "Pattern Recognition 34(8), 1671\u20131684",
            "year": 2001
        },
        {
            "authors": [
                "J. Chung",
                "C. Gulcehre",
                "K. Cho",
                "Y. Bengio"
            ],
            "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
            "venue": "arXiv preprint arXiv:1412.3555",
            "year": 2014
        },
        {
            "authors": [
                "Y. Deng",
                "A. Kanervisto",
                "J. Ling",
                "A.M. Rush"
            ],
            "title": "Image-to-markup generation with coarse-to-fine attention",
            "venue": "International Conference on Machine Learning. pp. 980\u2013989. PMLR",
            "year": 2017
        },
        {
            "authors": [
                "S. Fang",
                "H. Xie",
                "Y. Wang",
                "Z. Mao",
                "Y. Zhang"
            ],
            "title": "Read like humans: Autonomous, bidirectional and iterative language modeling for scene text recognition",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7098\u20137107",
            "year": 2021
        },
        {
            "authors": [
                "L. Hu",
                "R. Zanibbi"
            ],
            "title": "Hmm-based recognition of online handwritten mathematical symbols using segmental k-means initialization and a modified pen-up/down feature",
            "venue": "2011 International Conference on Document Analysis and Recognition. pp. 457\u2013462. IEEE",
            "year": 2011
        },
        {
            "authors": [
                "G. Huang",
                "Z. Liu",
                "L. Van Der Maaten",
                "K.Q. Weinberger"
            ],
            "title": "Densely connected convolutional networks",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4700\u20134708",
            "year": 2017
        },
        {
            "authors": [
                "B. Keshari",
                "S. Watt"
            ],
            "title": "Hybrid mathematical symbol recognition using support vector machines",
            "venue": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007). vol. 2, pp. 859\u2013863. IEEE",
            "year": 2007
        },
        {
            "authors": [
                "A. Kosmala",
                "G. Rigoll",
                "S. Lavirotte",
                "L. Pottier"
            ],
            "title": "On-line handwritten formula recognition using hidden markov models and context dependent graph grammars",
            "venue": "Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR\u201999 (Cat. No. PR00318). pp. 107\u2013110. IEEE",
            "year": 1999
        },
        {
            "authors": [
                "S. Lavirotte",
                "L. Pottier"
            ],
            "title": "Mathematical formula recognition using graph grammar",
            "venue": "Document Recognition V. vol. 3305, pp. 44\u201352. International Society for Optics and Photonics",
            "year": 1998
        },
        {
            "authors": [
                "A.D. Le"
            ],
            "title": "Recognizing handwritten mathematical expressions via paired dual loss attention network and printed mathematical expressions",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. pp. 566\u2013567",
            "year": 2020
        },
        {
            "authors": [
                "A.D. Le"
            ],
            "title": "Recognizing handwritten mathematical expressions via paired dual loss attention network and printed mathematical expressions",
            "venue": "Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition Workshops. pp. 566\u2013567",
            "year": 2020
        },
        {
            "authors": [
                "A.D. Le",
                "B. Indurkhya",
                "M. Nakagawa"
            ],
            "title": "Pattern generation strategies for improving recognition of handwritten mathematical expressions",
            "venue": "Pattern Recognition Letters 128, 255\u2013262",
            "year": 2019
        },
        {
            "authors": [
                "B. Li",
                "Y. Yuan",
                "D. Liang",
                "X. Liu",
                "Z. Ji",
                "J. Bai",
                "W. Liu",
                "X. Bai"
            ],
            "title": "When counting meets hmer: Counting-aware network for handwritten mathematical expression recognition",
            "venue": "Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXVIII. pp. 197\u2013214. Springer",
            "year": 2022
        },
        {
            "authors": [
                "Z. Li",
                "L. Jin",
                "S. Lai",
                "Y. Zhu"
            ],
            "title": "Improving attention-based handwritten mathematical expression recognition with scale augmentation and drop attention",
            "venue": "arXiv preprint arXiv:2007.10092",
            "year": 2020
        },
        {
            "authors": [
                "H. Mouchere",
                "C. Viard-Gaudin",
                "R. Zanibbi",
                "U. Garain"
            ],
            "title": "Icfhr 2014 competition on recognition of on-line handwritten mathematical expressions (crohme 2014)",
            "venue": "Proc. of International Conference on Frontiers in Handwriting Recognition. pp. 791\u2013796",
            "year": 2014
        },
        {
            "authors": [
                "H. Mouch\u00e8re",
                "C. Viard-Gaudin",
                "R. Zanibbi",
                "U. Garain"
            ],
            "title": "Icfhr2016 crohme: Competition on recognition of online handwritten mathematical expressions",
            "venue": "Proc. 14 Z. Liu et al. of International Conference on Frontiers in Handwriting Recognition. pp. 607\u2013612",
            "year": 2016
        },
        {
            "authors": [
                "C.T. Nguyen",
                "H.T. Nguyen",
                "K. Morizumi",
                "M. Nakagawa"
            ],
            "title": "Temporal classification constraint for improving handwritten mathematical expression recognition",
            "venue": "International Conference on Document Analysis and Recognition. pp. 113\u2013125. Springer",
            "year": 2021
        },
        {
            "authors": [
                "Z. Qiao",
                "Y. Zhou",
                "D. Yang",
                "Y. Zhou",
                "W. Wang"
            ],
            "title": "Seed: Semantics enhanced encoder-decoder framework for scene text recognition",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 13528\u2013 13537",
            "year": 2020
        },
        {
            "authors": [
                "B. Shi",
                "X. Bai",
                "C. Yao"
            ],
            "title": "An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition",
            "venue": "IEEE transactions on pattern analysis and machine intelligence 39(11), 2298\u20132304",
            "year": 2016
        },
        {
            "authors": [
                "B. Shi",
                "M. Yang",
                "X. Wang",
                "P. Lyu",
                "C. Yao",
                "X. Bai"
            ],
            "title": "Aster: An attentional scene text recognizer with flexible rectification",
            "venue": "IEEE transactions on pattern analysis and machine intelligence 41(9), 2035\u20132048",
            "year": 2018
        },
        {
            "authors": [
                "I. Sutskever",
                "O. Vinyals",
                "Le"
            ],
            "title": "Q.V.: Sequence to sequence learning with neural networks",
            "year": 2014
        },
        {
            "authors": [
                "T.N. Truong",
                "C.T. Nguyen",
                "K.M. Phan",
                "M. Nakagawa"
            ],
            "title": "Improvement of endto-end offline handwritten mathematical expression recognition by weakly supervised learning",
            "venue": "2020 17th International Conference on Frontiers in Handwriting Recognition (ICFHR). pp. 181\u2013186. IEEE",
            "year": 2020
        },
        {
            "authors": [
                "T.N. Truong",
                "C.T. Nguyen",
                "K.M. Phan",
                "M. Nakagawa"
            ],
            "title": "Improvement of end-toend offline handwritten mathematical expression recognition by weakly supervised learning",
            "venue": "Proc. of International Conference on Frontiers in Handwriting Recognition. pp. 181\u2013186",
            "year": 2020
        },
        {
            "authors": [
                "B.Q. Vuong",
                "Y. He",
                "S.C. Hui"
            ],
            "title": "Towards a web-based progressive handwriting recognition environment for mathematical problem solving",
            "venue": "Expert Systems with Applications 37(1), 886\u2013893",
            "year": 2010
        },
        {
            "authors": [
                "J. Wang",
                "J. Du",
                "J. Zhang",
                "Z.R. Wang"
            ],
            "title": "Multi-modal attention network for handwritten mathematical expression recognition",
            "venue": "2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 1181\u20131186. IEEE",
            "year": 2019
        },
        {
            "authors": [
                "J. Wang",
                "J. Du",
                "J. Zhang",
                "Z.R. Wang"
            ],
            "title": "Multi-modal attention network for handwritten mathematical expression recognition",
            "venue": "Proc. of International Conference on Document Analysis and Recognition. pp. 1181\u20131186",
            "year": 2019
        },
        {
            "authors": [
                "H.J. Winkler"
            ],
            "title": "Hmm-based handwritten symbol recognition using on-line and offline features",
            "venue": "1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings. vol. 6, pp. 3438\u20133441. IEEE",
            "year": 1996
        },
        {
            "authors": [
                "J.W. Wu",
                "F. Yin",
                "Y. Zhang",
                "X.Y. Zhang",
                "C.L. Liu"
            ],
            "title": "Graph-to-graph: towards accurate and interpretable online handwritten mathematical expression recognition",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence. vol. 35, pp. 2925\u20132933",
            "year": 2021
        },
        {
            "authors": [
                "J.W. Wu",
                "F. Yin",
                "Y.M. Zhang",
                "X.Y. Zhang",
                "C.L. Liu"
            ],
            "title": "Image-to-markup generation via paired adversarial learning",
            "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases. pp. 18\u201334. Springer",
            "year": 2018
        },
        {
            "authors": [
                "J.W. Wu",
                "F. Yin",
                "Y.M. Zhang",
                "X.Y. Zhang",
                "C.L. Liu"
            ],
            "title": "Image-to-markup generation via paired adversarial learning",
            "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases. pp. 18\u201334",
            "year": 2018
        },
        {
            "authors": [
                "J.W. Wu",
                "F. Yin",
                "Y.M. Zhang",
                "X.Y. Zhang",
                "C.L. Liu"
            ],
            "title": "Handwritten mathematical expression recognition via paired adversarial learning",
            "venue": "International Journal of Computer Vision pp. 1\u201316",
            "year": 2020
        },
        {
            "authors": [
                "J.W. Wu",
                "F. Yin",
                "Y.M. Zhang",
                "X.Y. Zhang",
                "C.L. Liu"
            ],
            "title": "Handwritten mathematical expression recognition via paired adversarial learning",
            "venue": "International Journal of Computer Vision 128(10), 2386\u20132401",
            "year": 2020
        },
        {
            "authors": [
                "R. Yamamoto",
                "S. Sako",
                "T. Nishimoto",
                "S. Sagayama"
            ],
            "title": "On-line recognition of handwritten mathematical expressions based on stroke-based stochastic context-free grammar",
            "venue": "Tenth international workshop on frontiers in handwriting recognition. Suvisoft",
            "year": 2006
        },
        {
            "authors": [
                "D. Yu",
                "X. Li",
                "C. Zhang",
                "T. Liu",
                "J. Han",
                "J. Liu",
                "E. Ding"
            ],
            "title": "Towards accurate scene text recognition with semantic reasoning networks",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12113\u2013 12122",
            "year": 2020
        },
        {
            "authors": [
                "Y. Yuan",
                "X. Liu",
                "W. Dikubab",
                "H. Liu",
                "Z. Ji",
                "Z. Wu",
                "X. Bai"
            ],
            "title": "Syntax-aware network for handwritten mathematical expression recognition",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4553\u20134562",
            "year": 2022
        },
        {
            "authors": [
                "X. Yue",
                "Z. Kuang",
                "C. Lin",
                "H. Sun",
                "W. Zhang"
            ],
            "title": "Robustscanner: Dynamically enhancing positional clues for robust text recognition",
            "venue": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XIX. pp. 135\u2013151. Springer",
            "year": 2020
        },
        {
            "authors": [
                "M.D. Zeiler"
            ],
            "title": "Adadelta: an adaptive learning rate method",
            "venue": "arXiv preprint arXiv:1212.5701",
            "year": 2012
        },
        {
            "authors": [
                "J. Zhang",
                "J. Du",
                "L. Dai"
            ],
            "title": "Multi-scale attention with dense encoder for handwritten mathematical expression recognition",
            "venue": "2018 24th international conference on pattern recognition (ICPR). pp. 2245\u20132250. IEEE",
            "year": 2018
        },
        {
            "authors": [
                "J. Zhang",
                "J. Du",
                "L. Dai"
            ],
            "title": "Multi-scale attention with dense encoder for handwritten mathematical expression recognition",
            "venue": "Proc. of Intl. Conf. on Pattern Recognition. pp. 2245\u20132250",
            "year": 2018
        },
        {
            "authors": [
                "J. Zhang",
                "J. Du",
                "L. Dai"
            ],
            "title": "Track, attend, and parse (tap): An end-to-end framework for online handwritten mathematical expression recognition",
            "venue": "IEEE Transactions on Multimedia 21(1), 221\u2013233",
            "year": 2018
        },
        {
            "authors": [
                "J. Zhang",
                "J. Du",
                "L. Dai"
            ],
            "title": "Track, attend, and parse (tap): An end-to-end framework for online handwritten mathematical expression recognition",
            "venue": "IEEE Transactions on Multimedia 21(1), 221\u2013233",
            "year": 2018
        },
        {
            "authors": [
                "J. Zhang",
                "J. Du",
                "Y. Yang",
                "Y.Z. Song",
                "S. Wei",
                "L. Dai"
            ],
            "title": "A tree-structured decoder for image-to-markup generation",
            "venue": "International Conference on Machine Learning. pp. 11076\u201311085. PMLR",
            "year": 2020
        },
        {
            "authors": [
                "J. Zhang",
                "J. Du",
                "Y. Yang",
                "Y.Z. Song",
                "S. Wei",
                "L. Dai"
            ],
            "title": "A tree-structured decoder for image-to-markup generation",
            "venue": "Proc. of Intl. Conf. on Machine Learning. pp. 11076\u201311085",
            "year": 2020
        },
        {
            "authors": [
                "J. Zhang",
                "J. Du",
                "S. Zhang",
                "D. Liu",
                "Y. Hu",
                "J. Hu",
                "S. Wei",
                "L. Dai"
            ],
            "title": "Watch, attend and parse: An end-to-end neural network based approach to handwritten mathematical expression recognition",
            "venue": "Pattern Recognition 71, 196\u2013206",
            "year": 2017
        },
        {
            "authors": [
                "J. Zhang",
                "J. Du",
                "S. Zhang",
                "D. Liu",
                "Y. Hu",
                "J. Hu",
                "S. Wei",
                "L. Dai"
            ],
            "title": "Watch, attend and parse: An end-to-end neural network based approach to handwritten mathematical expression recognition",
            "venue": "Pattern Recognition 71, 196\u2013206",
            "year": 2017
        },
        {
            "authors": [
                "Z. Zhang",
                "T. He",
                "H. Zhang",
                "Z. Zhang",
                "J. Xie",
                "M. Li"
            ],
            "title": "Bag of freebies for training object detection neural networks",
            "venue": "arXiv preprint arXiv:1902.04103",
            "year": 2019
        },
        {
            "authors": [
                "W. Zhao",
                "L. Gao",
                "Z. Yan",
                "S. Peng",
                "L. Du",
                "Z. Zhang"
            ],
            "title": "Handwritten mathematical expression recognition with bidirectionally trained transformer",
            "venue": "International Conference on Document Analysis and Recognition. pp. 570\u2013584. Springer",
            "year": 2021
        },
        {
            "authors": [
                "W. Zhao",
                "L. Gao",
                "Z. Yan",
                "S. Peng",
                "L. Du",
                "Z. Zhang"
            ],
            "title": "Handwritten mathematical expression recognition with bidirectionally trained transformer",
            "venue": "Proc. of International Conference on Document Analysis and Recognition. pp. 570\u2013584",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Keywords: Handwritten Mathematical Expression Recognition, Semantic Graph, Co-occurrence Probabilities"
        },
        {
            "heading": "1 Introduction",
            "text": "Handwritten Mathematical Expression Recognition (HMER) is an important OCR task, which can be widely applied in question parsing and answer sheet correction. In recent years, with the rapid development of deep learning technology, scene text recognition approaches have achieved great progress [23,24,40,8]. However, due to the ambiguities brought by crabbed handwriting and the complicated structures of handwritten mathematical expressions, HMER is still a challenging task.\nar X\niv :2\n30 8.\n10 49\n3v 1\n[ cs\n.C V\n] 2\n\u03b1 \u03b2 a b\n\u03b1\n\u03b2\na\nb\n\u03b1 \u03b2 a b\n\u03b1\n\u03b2\na\nb\n\u2026 \u2026\nProjected Vectors\nCo-occurrence Probabilities\nCosine Distance Co-occurrence Matrix\nBuilt upon the recent progress in sequence-to-sequence learning and neural networks [25,6,10], some studies have addressed HMER with end-to-end trained encoder-decoder models and showed significant improvement in performance. Nevertheless, the encoder-decoder framework do not fully explore the correlation between different symbols in the mathematical expression, which may be struggling when facing similar handwritten symbols or crabbed handwritings.\nTo address above issues, we argue that an effective HMER model should be improved from the following two aspects: (1) capturing semantic dependencies among different symbols in the mathematical expression; (2) integrating more semantic information to locate the regions of interest.\nIn this paper, we propose an simple but efficient method to improve the robustness of the model, which incorporate the learning of semantic relations among different symbols into the end-to-end training (Fig. 1). Firstly, we built a semantic graph rely on statistical co-occurrence probabilities, which can explicitly exhibit the dependencies among different symbols. Secondly, we propose a semantic aware module, which takes the visual and classification features as input and maps them into the semantic space. The cosine distance between different projected vectors suggests the correlation of symbols. Optimizing the distance to close to the corresponding graph value make the network capture the relationships between different symbols. Therefore, the search for regions of interest and the learning of symbols semantic dependencies are enhanced, which further improved the performance of the model.\nThe major contributions of this paper are briefly summarized as follows:\n\u2013 To the best of our knowledge, we are the first to use co-occurrence to represent the relationship between symbols in mathematical expression and verify the effectiveness of enhancing semantic representation learning. \u2013 We propose a semantic aware method that jointly optimizes the symbol relations learning and HMER, which can consistently improve the performance of the model for HMER. \u2013 Our proposed semantic aware module can be easily plugged into attention based models for HMER and no extra computation during the inference stage.\nTo be specific about the performance, we adopt DWAP [48] as the baseline network. With the help of SAM, SAM-DWAP outperforms DWAP by 2.2%, 2.8% and 4.2% on CROHME 2014, 2016 and 2019, respectively. Moreover, with adopting the latest SOTA method CAN [17] as the baseline network, our method achieves new SOTA results (58.0% on CROHME 2014, 56.7% on CROHME 2016, 58.0% on CROHME 2019). This indicates that our method can be generalized to various existing encoder-decoder models for HMER and boost their performance."
        },
        {
            "heading": "2 Related Work",
            "text": "HMER is a fundamental OCR task, which has attracted research interests in the past several decades. In this section, we briefly introduce previous related works on HMER.\nTraditional methods on HMER could be mainly separated into two steps: a symbol segmentation/recognition step and a grammar guided structure analysis step. In the first step, several classic classification techniques were studied, such as HMM [31,12,9,1], Elastic Matching [4,28], Support Vector Machines [11], etc. In the second step, formal grammars were designed to model the 2D and syntactic structures of expression. Lavirotte et al. [13] proposed to use graph grammar to recognize mathematical expression. Chan et al. [5] incorporated correction mechanism into parser based on definite clause grammar (DCG). Yamamoto et al. [37] modeled handwritten mathematical expressions with a stochastic contextfree grammar and solved the recognition problem by using the CYK algorithm. In contrast to those traditional methods, our model incorporates grammatical structure and automatically learned encoder-decoder, therefore preventing from designing cumbersome rules.\nRecently, deep learning techniques rapidly boosted the performance of HEMR. The mainstream framework was encoder-decoder networks [7,42,48,44,51,29,26,21,14,44,16,32]. Deng et al. [7] firstly proposed an encoder-decoder framework to convert image to LATEX markup. A coarse-to-fine attention layer was used to reduce the attention complexity in their work. Zhang et al. [48] presented an encoder-decoder model, named WAP (Watch, Attend and Parse). In their model, the encoder is a FCN and a coverage vector is appended to the attention model. Wu et al. [33,35] focused on the pair-wise adversarial learning strategy to improve the recognition accuracy. To alleviate the challenge of lack of data, Le et al. [16] and Li et al. [18] employed distortion, decomposition and scale augmentation techniques, which\nachieved significant performance promotion. Le [14] proposed a dual loss attention model, which contains a new context match loss. Context matching loss is adapted to constrain the intra-class distance and enhance the discriminative power of model. Lately, Zhang et al. [46] devised a tree-based decoder to parse mathematical expression. At each step, a parent and child node pair was generated and the relation between parent node and child node reflects the structure type. Yuan et al. [39] firstly incorporate syntax information into the encoderdecoder, which achieved higer recognition accuracy while taking into account speed. Li et al.[17] design a weakly-supervised counting module and jointly optimizes HMER task and symbol counting task. With the help of integrated global information, it puts in a impressive performance."
        },
        {
            "heading": "3 Methodology",
            "text": "The overall framework of our approach is shown in Fig. 2. The pipeline includes several parts: densely connected convolutional network (DenseNet) [10] is applied as encoder to extract the features. The DenseNet takes a grayscale image X of size H\u00d7W \u00d71, where H and W are image height and image width, respectively, and returns a 2D feature map F \u2208 RH \u2032 \u00d7W \u2032 \u00d7684, where H/H \u2032 = W/W \u2032 = 16. The decoder uses the feature map and gradually predicts the LATEX markup. The Semantic Aware Module (SAM) comprises two branches with similar structure (visual branch and classification branch), which employ the visual and classification features, respectively. Visual and classification features are projected to semantic space to obtain projected visual and classification vectors, respectively. The cosine distance between projected vectors from different time steps indicates how related they are."
        },
        {
            "heading": "3.1 Semantic Graph",
            "text": "Capturing global context information has been proven to be an effective way to improve the robustness of recognition [38,22]. However, compared with words, the use of symbols in the mathematical expressions is relatively more casual. How to express the relationships among different symbols in the mathematical expressions is an open issue to be solved. Our intuition is that the magnitude of values in the co-occurrence graph reflects the relationship between different symbols, much like how different characters in text have different collocations. Making the distances close to the probabilities is aimed at enhancing the model\u2019s learning of the linguistic information in formulas.\nSemantic graph is defined as G = (S,E), where S = {s1, s2, ..., sN} represents the set of symbol nodes and E represents the edges, which suggest the dependence between any two symbols. The correlation matrix R = {ri,j}Ni,j=1 of graph G contains non-negative weights associated with each edge. The correlation matrix is a conditional probability matrix and the rij is set as P (si/sj), where P is calculated through training set. However, R is an asymmetric matrix,\nnamely rij \u0338= rji. In order to facilitate the calculation, we turn the asymmetric matrix into a symmetric matrix following:\nR \u2032 =\n1 2 (R+RT ). (1)"
        },
        {
            "heading": "3.2 Semantic Aware Module",
            "text": "In this section, we present the detail of the proposed semantic aware module (SAM). As shown in Fig. 3 (a), SAM contains two branches, namely visual feature branch and classification feature branch. Each branch comprises two \u201cLBR\u201d block followed by a linear layer. A \u201cLBR\u201d block is built by stacking Linear layer, Batch Normalization and ReLU activation. We apply SAM to project the visual vectors (vvis) and classification vector (vcls) to semantic space to get projected visual vectors (v \u2032\nvis) and projected classification vectors (v \u2032 cls):\nv \u2032\nvis = W vis 3 (\u03c3(\u03f5(W vis 2 (\u03c3(\u03f5(W vis 1 vvis + b vis 1 ))) + b vis 2 ))) + b vis 3 (2)\nv \u2032\ncls = W cls 3 (\u03c3(\u03f5(W cls 2 (\u03c3(\u03f5(W cls 1 vcls + b cls 1 ))) + b cls 2 ))) + b cls 3 (3)\nwhere \u03c3 is the ReLU activation and \u03f5 refers to Batch Normalization. W vis3 , W vis 2 , W vis1 , W cls 3 , W cls 2 and W cls 1 are learnable parameters.\nOur goal is to optimize the projected visual vectors (v \u2032\nvis) and projected\nclassification vectors (v \u2032 cls). Such that cos(v \u2032 i, v \u2032 j) is close to Rij for all i, j, where cos(v \u2032\ni, v \u2032 j) denotes the cosine similarity between v \u2032 i and v \u2032 j :\ncos(v \u2032 i, v \u2032 j) = v\n\u2032T i vj\n||v\u2032Ti || ||v \u2032T j ||\n(4)"
        },
        {
            "heading": "3.3 Decoder",
            "text": "Fig. 3 (b) shows the structure of decoder. The decoder mainly contains two Gated Recurrent Units (GRU) cells and an attention module. The first GRU takes the symbol embedding (E(yt\u22121)) and historical state (ht\u22121) predicted in the last step as input and output a new hidden state vector h \u2032\nt:\nh \u2032\nt = GRU(E(yt\u22121), ht\u22121) (5)\nThen the attention module calculates the attentional weights \u03b1t through its attention mechanism:\net = W\u03c9(tanh(W \u2032 hh \u2032 t +WfF +W\u03b1attt)) (6)\n\u03b1t = exp(et)/ \u2211 exp(et) (7)\nwhere W\u03c9, W \u2032\nh, Wf and W\u03b1 are trainable parameters. F represents the feature map and attt refers to coverage attention [48], which equals the sum of all past attention probabilities:\nattt = \u2211 i \u03b1i, i \u2208 [0, t\u2212 1] (8)\nThe \u03b1t and F are multiplied to obtain visual features vectors vvis:\nvvis = \u03b1t \u2297F (9)\nThe second GRU takes the vvis and h \u2032 t as input and returns the hidden state ht:\nht = GRU(vvis, h \u2032 t) (10)\nThen we aggregate E(yt\u22121), vvis and ht to obtain the classification feature vectors and symbol probabilities:\nvcls = WeE(yt\u22121) +Whht +Wvvvis (11)\npsymbol = softmax(Wsvcls) (12)\nwhere We, Wh, Wv and Ws are trainable parameters."
        },
        {
            "heading": "3.4 Loss Function",
            "text": "The overall function consists of three parts and is defined as follows:\nL = Lsymbol + Lvis + Lcls (13)\nwhere Lsymbol is cross entropy classification loss of the predicted probability psymbol with respect to its ground-truth. Lvis and Lcls are L2 regression loss defined as follows:\nLvis = n\u2211 i n\u2211 j (cos(vvis,i, vvis,j),\u2212Ri,j)2 (14) Lcls = n\u2211 i n\u2211 j (cos(vcls,i, vcls,j),\u2212Ri,j)2 (15)"
        },
        {
            "heading": "4 Experiments",
            "text": "We conduct experiments on three CROHME and HME100K benchmark datasets and compare the performance with previous state-of-the-art methods. In this section, we firstly specify the datasets, implementation details and evaluation protocol in Section 4.1, 4.2 and 4.3, respectively. Then, in Section 4.4 we evaluate our method on public datasets and compare it with other state-of-the-art methods. In Section 4.5, we exhibit the ablation studies and finally, in Section 4.6 we show few cases and discuss the effectiveness of our method."
        },
        {
            "heading": "4.1 Datasets",
            "text": "CROHME Dataset. CROHME dataset is from the competition on recognition of online handwritten mathematical expression, which is the most widely used public dataset. Images in CROHME dataset are synthesized from the handwritten stroke trajectory information in the InkML files. Therefore, the image background from CROHME dataset is clean (Fig. 4 (a)). The CROHME training set number is 8,836, while the test set contains 986, 1147 and 1199 images respectively due to different release years.\nHME100K Dataset. HME100K dataset is a real scene dataset and consequently, HME100K dataset are varied in color, blur, complicated background, twist (Fig. 4 (b-f)). HME100K dataset contains 74,502 images for training and 24,607 images for testing. The data size of HME100K dataset is ten times larger than CROHME dataset. The number of math symbols included in the HME100K dataset is 245, which is two times larger than that of CROHME dataset."
        },
        {
            "heading": "4.2 Implementation Details",
            "text": "The proposed methods is implemented in PyTorch. A single Nvidia Tesla V100 with 32GB RAM is used to conduct experiment. The batch size is set at 8. Both the hidden state sizes of the two GRUs and dimension of word embedding are set at 256. The Adadelta optimizer [41] is used during the training process, in which \u03c1 is set at 0.95 and \u03f5 is set at 10\u22126. The learning rate starts from 0 and monotonously increases to 1 at the end of the first epoch. After that the learning rate decays to 0 following the cosine schedules [50]. For CROHME dataset, the total training epoch is set to 240 and for HME100K dataset, the training epoch is set to 40."
        },
        {
            "heading": "4.3 Evaluation Protocol",
            "text": "Recognition Protocol. We employ expression recognition rate (ExpRate) to evaluate the performance of different approaches. The definition of ExpRate is the percentage of predicted mathematical expressions that exactly match the ground truth."
        },
        {
            "heading": "4.4 Comparison with State-of-the-Art",
            "text": "Results on the CROHME Datasets. Tab. 1 summaries the performance of our method and previous methods on the CROHME dataset. Since most of the previous work does not use data augmentation, we mainly discuss the results without data augmentation.\nAs shown in tab. 1, using DWAP [42] as the backbone, SAM-DWAP achieves competitive results to the last SOTA method CAN [17] on CROHME 2014 and CROHME 2016. On CROHME 2019 dataset, our method ourperforms CAN by 1.33 %.\nTo further verify our proposed SAM is compatible with other models and can consistently bring performance improvements. We integrate SAM into CAN to construct SAM-CAN. As shown in tab. 1, SAM-CAN achieves the best performance on all CROHME test set and outperforms CAN by 1.21 %, 0.61 % and 3.08 %, respectively. This result clearly demonstrates the effectiveness of our proposed module.\nResults on the HME100K Dataset. As shown in tab. 1 and 2, we compare our prosposed method with DWAP [42], DWAP-TD [46], BTTR [51], ABM [3], SAN [39] and CAN [17] on HME100K dataset. It is clear to notice that SAMDWAP and SAM-CAN achieves the best performance. Specifically, as shown in\ntable 2, SAM-DWAP and SAM-CAN outperform SAN by 0.1 % and 0.6 % on easy subset, respectively. However, as the difficulty of the test subset increases, the leading margin of our method increases to 2.5 % and 2.5 % on the hard subset. This further proves the effectiveness of the proposed SAM."
        },
        {
            "heading": "4.5 Ablation Study",
            "text": "In this subsection, we evaluate the effectiveness of visual feature branch and classification feature branch. SAM-DWAP and SAM-CAN are the default models. Vis-DWAP and Vis-CAN have a visual feature branch but not a classification feature branch. Cls-DWAP and Cls-CAN have a classification feature branch but not a visual feature branch. DWAP\u2020 and CAN \u2020 are our reproduced results. The results are summarized in tab. 3.\nImpact of Visual Feature Branch. Tab. 4.5 shows adopting visual feature branch to DWAP improves the recognition performance ExpRate by 1.2 % on\nCROHME 2014, 2.0 % on CROHME 2016 and 2.1 % on CROHME 2019. Inserting visual feature branch into CAN also can enhance the performance by 0.4 % on CROHME 2014, 1.0 % on CROHME 2016 and 1.7 % on CROHME 2019. Hence integrating visual feature branch can effectively improve the performance.\nImpact of Classification Feature Branch. Tab. 4.5 shows adopting classification feature branch to DWAP improves the recognition performance ExpRate by 1.0 % on CROHME 2014, 2.4 % on CROHME 2016 and 2.7 % on CROHME 2019. Inserting visual feature branch into CAN also can enhance the performance by 0.3 % on CROHME 2014, 1.2 % on CROHME 2016 and 0.9 % on CROHME 2019. Hence integrating classification feature branch can effectively improve the performance."
        },
        {
            "heading": "4.6 Case Study",
            "text": "In this section, we show two examples to illustrate the effect of using SAM. As shown in Fig. 5 (a), although CAN correctly focuses on the region of interest, it misidentifies the symbol \u201cB\u201d as symbol \u201c\u03b2\u201d and misidentifies the symbol \u201cb\u201d as symbol \u201c6\u201d. In contrast, the regions of interest of the SAM-CAN are similar to those of CAN, but SAM-CAN correctly predicts symbol \u201cB\u201d and \u201cb\u201d. The confidences of symbol \u201cB\u201d and \u201cb\u201d also increase from 10.1 % to 52.9 % and increase from 10.2% to 81.0%, respectively. This phenomenon indicates that adopting SAM can improve the robustness of recognition especially the recognition performance of similar symbols."
        },
        {
            "heading": "5 Conclusion",
            "text": "This paper has presented a simple and efficient method for handwritten mathematical expression recognition by incorporate semantic graph representation learning into end-to-end training. To our best knowledge, the proposed method is the first to learn the correlation between different symbols through symbol cooccurrence probabilities. Experiments on the CROHME dataset and HME100K dataset have validated the effectiveness and efficiency of our method."
        },
        {
            "heading": "6 Acknowledgement",
            "text": "This work was supported by National Key R&D Program of China, under Grant No. 2020AAA0104500 and National Science Fund for Distinguished Young Scholars of China (Grant No.62225603)."
        }
    ],
    "title": "Semantic Graph Representation Learning for Handwritten Mathematical Expression Recognition",
    "year": 2023
}