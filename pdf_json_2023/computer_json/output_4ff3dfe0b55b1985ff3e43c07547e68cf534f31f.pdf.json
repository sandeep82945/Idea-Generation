{
    "abstractText": "Extreme learning machines (ELMs) have recently attracted significant attention due to their fast training speeds and good prediction effect. However, ELMs ignore the inherent distribution of the original samples, and they are prone to overfitting, which fails at achieving good generalization performance. In this paper, based on expectile penalty and correntropy, an asymmetric C-loss function (called AC-loss) is proposed, which is non-convex, bounded, and relatively insensitive to noise. Further, a novel extreme learning machine called L1 norm robust regularized extreme learning machine with asymmetric C-loss (L1-ACELM) is presented to handle the overfitting problem. The proposed algorithm benefits from L1 norm and replaces the square loss function with the AC-loss function. The L1-ACELM can generate a more compact network with fewer hidden nodes and reduce the impact of noise. To evaluate the effectiveness of the proposed algorithm on noisy datasets, different levels of noise are added in numerical experiments. The results for different types of artificial and benchmark datasets demonstrate that L1-ACELM achieves better generalization performance compared to other state-of-the-art algorithms, especially when noise exists in the datasets.",
    "authors": [
        {
            "affiliations": [],
            "name": "Qing Wu"
        },
        {
            "affiliations": [],
            "name": "Fan Wang"
        },
        {
            "affiliations": [],
            "name": "Ke Li"
        }
    ],
    "id": "SP:e1006a1e6f4e5e39d69b1ac1b8d38d73f64a8ae0",
    "references": [
        {
            "authors": [
                "S. Ding",
                "C. Su",
                "J. Yu"
            ],
            "title": "An optimizing BP neural network algorithm based on genetic algorithm",
            "venue": "Artif. Intell. Rev",
            "year": 2011
        },
        {
            "authors": [
                "G.B. Huang",
                "Q.Y. Zhu",
                "C.K. Siew"
            ],
            "title": "Extreme learning machine: A new learning scheme of feedforward neural networks",
            "venue": "In Proceedings of the 2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No. 04CH37541), Budapest, Hungary,",
            "year": 2004
        },
        {
            "authors": [
                "G.B. Huang",
                "Q.Y. Zhu",
                "C.K. Siew"
            ],
            "title": "Extreme learning machine: Theory and applications",
            "venue": "Neurocomputing",
            "year": 2006
        },
        {
            "authors": [
                "B.L. Silva",
                "F.K. Inaba",
                "O.T. Evandro",
                "P.M. Ciarelli"
            ],
            "title": "Outlier robust extreme machine learning for multi-target regression",
            "venue": "Expert Syst. Appl",
            "year": 2020
        },
        {
            "authors": [
                "Y. Li",
                "Y. Wang",
                "Z. Chen",
                "R. Zou"
            ],
            "title": "Bayesian robust multi-extreme learning machine",
            "venue": "Knowl. -Based Syst",
            "year": 2020
        },
        {
            "authors": [
                "X. Liu",
                "Q. Ge",
                "X. Chen",
                "J. Li",
                "Y. Chen"
            ],
            "title": "Extreme learning machine for multivariate reservoir characterization",
            "venue": "J. Pet. Sci. Eng",
            "year": 2021
        },
        {
            "authors": [
                "O. Catoni"
            ],
            "title": "Challenging the empirical mean and empirical variance: A deviation study",
            "venue": "Annales de l\u2019IHP Probabilite\u0301s et Statistiques",
            "year": 2012
        },
        {
            "authors": [
                "W. Deng",
                "Q. Zheng",
                "L. Chen"
            ],
            "title": "Regularized extreme learning machine",
            "venue": "In Proceedings of the 2009 IEEE Symposium on Computational Intelligence and Data Mining, Nashville, TN, USA,",
            "year": 2009
        },
        {
            "authors": [
                "H.J. Rong",
                "Y.S. Ong",
                "A.H. Tan",
                "Z. Zhu"
            ],
            "title": "A fast pruned-extreme learning machine for classification problem",
            "venue": "Neurocomputing",
            "year": 2008
        },
        {
            "authors": [
                "Y. Miche",
                "A. Sorjamaa",
                "P. Bas",
                "O. Simula",
                "C. Jutten",
                "A. Lendasse"
            ],
            "title": "OP-ELM: Optimally pruned extreme learning machine",
            "venue": "IEEE Trans. Neural Netw",
            "year": 2009
        },
        {
            "authors": [
                "Q. Ye",
                "J. Yang",
                "F. Liu",
                "C. Zhao",
                "N. Ye",
                "T. Yin"
            ],
            "title": "L1-norm distance linear discriminant analysis based on an effective iterative algorithm",
            "venue": "IEEE Trans. Circuits Syst. Video Technol",
            "year": 2016
        },
        {
            "authors": [
                "C.N. Li",
                "Y.H. Shao",
                "N.Y. Deng"
            ],
            "title": "Robust L1-norm non-parallel proximal support vector machine",
            "venue": "Optimization",
            "year": 2016
        },
        {
            "authors": [
                "S. Balasundaram",
                "D. Gupta"
            ],
            "title": "1-Norm extreme learning machine for regression and multiclass classification using Newton method",
            "venue": "Neurocomputing",
            "year": 2014
        },
        {
            "authors": [
                "H. Dong",
                "L. Yang"
            ],
            "title": "Kernel-based regression via a novel robust loss function and iteratively reweighted least squares",
            "venue": "Knowl. Inf. Syst",
            "year": 2021
        },
        {
            "authors": [
                "H. Dong",
                "L. Yang"
            ],
            "title": "Training robust support vector regression machines for more general noise",
            "venue": "J. Intell. Fuzzy Syst",
            "year": 2020
        },
        {
            "authors": [
                "M. Farooq",
                "I. Steinwart"
            ],
            "title": "An SVM-like approach for expectile regression",
            "venue": "Comput. Stat. Data Anal",
            "year": 2017
        },
        {
            "authors": [
                "I. Razzak",
                "K. Zafar",
                "M. Imran",
                "G. Xu"
            ],
            "title": "Randomized nonlinear one-class support vector machines with bounded loss function to detect of outliers for large scale IoT data",
            "venue": "Future Gener. Comput. Syst",
            "year": 2020
        },
        {
            "authors": [
                "D. Gupta",
                "B.B. Hazarika",
                "M. Berlin"
            ],
            "title": "Robust regularized extreme learning machine with asymmetric Huber loss function",
            "venue": "Neural Comput. Appl",
            "year": 2020
        },
        {
            "authors": [
                "Z. Ren",
                "L. Yang"
            ],
            "title": "Correntropy-based robust extreme learning machine for classification",
            "venue": "Neurocomputing",
            "year": 2018
        },
        {
            "authors": [
                "Y. Ma",
                "Q. Zhang",
                "D. Li",
                "Y. Tian"
            ],
            "title": "LINEX support vector machine for large-scale classification",
            "venue": "IEEE Access",
            "year": 2019
        },
        {
            "authors": [
                "A. Singh",
                "R. Pokharel",
                "J. Principe"
            ],
            "title": "The C-loss function for pattern classification",
            "venue": "Pattern Recognit",
            "year": 2014
        },
        {
            "authors": [
                "R. Zhou",
                "X. Liu",
                "M. Yu",
                "K. Huang"
            ],
            "title": "Properties of risk measures of generalized entropy in portfolio selection",
            "venue": "Entropy 2017,",
            "year": 2017
        },
        {
            "authors": [
                "L.R. Ren",
                "Y.L. Gao",
                "J.X. Liu",
                "J. Shang",
                "C.H. Zheng"
            ],
            "title": "Correntropy induced loss based sparse robust graph regularized extreme learning machine for cancer classification",
            "venue": "BMC Bioinform",
            "year": 2020
        },
        {
            "authors": [
                "Y.P. Zhao",
                "J.F. Tan",
                "J.J. Wang",
                "Z. Yang"
            ],
            "title": "C-loss based extreme learning machine for estimating power of small-scale turbojet engine",
            "venue": "Aerosp. Sci. Technol",
            "year": 2019
        },
        {
            "authors": [
                "Y. He",
                "F. Wang",
                "Y. Li",
                "J. Qin",
                "B. Chen"
            ],
            "title": "Robust matrix completion via maximum correntropy criterion and half-quadratic optimization",
            "venue": "IEEE Trans. Signal Process",
            "year": 2019
        },
        {
            "authors": [
                "Z. Ren",
                "L. Yang"
            ],
            "title": "Robust extreme learning machines with different loss functions",
            "venue": "Neural Process. Lett",
            "year": 2019
        },
        {
            "authors": [
                "L. Chen",
                "H. Paul",
                "H. Qu",
                "J. Zhao",
                "X. Sun"
            ],
            "title": "Correntropy-based robust multilayer extreme learning machines",
            "venue": "Pattern Recognit. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "G. Huang",
                "G.B. Huang",
                "S. Song",
                "K. You"
            ],
            "title": "Trends in extreme learning machines: A review",
            "venue": "Neural Netw",
            "year": 2015
        },
        {
            "authors": [
                "C.L. Blake",
                "C.J. Merz"
            ],
            "title": "UCI Repository for Machine Learning Databases. Department of Information and Computer Sciences, University of California, Irvine",
            "venue": "Available online: http://www.ics.uci.edu/~{}mlearn/MLRepository.html (accessed on",
            "year": 1998
        },
        {
            "authors": [
                "J. Dem\u0161ar"
            ],
            "title": "Statistical comparisons of classifiers over multiple data sets",
            "venue": "J. Mach. Learn. Res. 2006,",
            "year": 2006
        }
    ],
    "sections": [
        {
            "text": "Citation: Wu, Q.; Wang, F.; An, Y.; Li,\nK. L1-Norm Robust Regularized Extreme Learning Machine with\nAsymmetric C-Loss for Regression.\nAxioms 2023, 12, 204. https://\ndoi.org/10.3390/axioms12020204\nAcademic Editor: Gustavo Olague\nReceived: 10 January 2023\nRevised: 13 February 2023\nAccepted: 14 February 2023\nPublished: 15 February 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: extreme learning machine; asymmetric least square loss; expectile; correntropy; robustness\nMSC: 65E99; 68T01; 68U01\n1. Introduction\nThe single hidden-layer feedforward neural network (SLFN) is one of the most important learning algorithms in data mining and machine learning fields. SLFN has only one hidden layer that connects the input and output layers. Generally, gradient-based algorithms are used to train SLFNs similar to back-propagation algorithms [1], which often leads to slow convergence, overfitting, and local minima. To overcome these problems, Huang et al. [2,3] proposed a widely used method based on the structure of SLFN called extreme learning machine (ELM). Compared to the traditional single hidden layer feedforward neural network, the input weights and thresholds of the hidden layer nodes in ELM are randomly generated, and there is no need for repeated adjustment via iterations. ELM identifies the output weight vector with the smallest norm by calculating the Moore-Penrose inverse. Therefore, the training speed of ELM is much higher than that of SLFN. Moreover, ELM also requires minimal training error and norm of the weights, which facilitates good generalization performance. Since ELM has a higher learning speed and better generalization performance, it has been successfully applied in many fields [4\u20136]. However, ELM still has several shortcomings. For example, ELM is based on empirical risk minimization (ERM) [7] which often leads to overfitting. To address this issue, many scholars have proposed various algorithms based on ELM to improve the generalization performance. In [8], Deng et al. introduced the weight factor \u03b3 into ELM for the first time and proposed the regularized extreme learning machine (RELM). By adjusting the weight factor \u03b3, the proportion of empirical risk and structural risk\nAxioms 2023, 12, 204. https://doi.org/10.3390/axioms12020204 https://www.mdpi.com/journal/axioms\nAxioms 2023, 12, 204 2 of 22\nin the actual prediction risk can be optimal, thereby avoiding model overfitting. However, RELM uses the L2 norm which is sensitive to outliers. To reduce the influence of outliers, Rong et al. proposed the pruned extreme learning machine (P-ELM) [9], which can remove irrelevant hidden nodes. P-ELM is only used for classification problems. To further address the regression problem, the optimally pruned extreme learning machine (OP-ELM) [10] was proposed. In OP-ELM, The L1 norm is used to remove irrelevant output nodes and select the corresponding hidden nodes, and then the weight of the corresponding hidden nodes is calculated using the least squares method. Given that the L1 norm is robust to outliers, it is used in various algorithms to improve the generalization performance [11,12]. Balasundaram et al. [13] proposed the L1 norm extreme learning machine, which produces sparse models such that decision functions can be determined using fewer hidden layer nodes. Generally speaking, RELM is composed of empirical risk and structural risk. Structural risk can effectively avoid overfitting, and structural risk is determined by loss function. Traditional RELMs use the squared loss function, which is symmetric and unbounded. The symmetry makes the model unable to take into account the distribution characteristics within the training samples, while unboundedness will cause the model to be sensitive to noise and outliers. In real life, the distribution of data is unbalanced, and noise is generally mixed in the process of data collection. Therefore, it is particularly important to choose an appropriate loss function to construct the model. Quantiles can reflect completely the distribution of random variables without missing any information Quantile regression can more accurately describe the distribution characteristics of random variables for comprehensive analysis. Therefore, quantile regression is more robust and has been successfully applied to statistical prediction [14,15]. Quantile loss can be thought of as a pinball penalty. Expectile loss is an asymmetric least squares loss, which is the square of the quantile loss function. It is often used in regression problems with imbalanced data [16]. However, the unboundedness of the expectile loss leads to a lack of robustness. From [17], the bounded loss function is less sensitive to noise and outliers than the unbounded loss function, whereas convex functions are usually unbounded. To further improve the robustness of ELM, researchers have proposed various non-convex loss functions to replace the convex loss functions [18\u201320]. Examples of common convex loss functions include square loss, hinge loss, and Huber loss, which allow for the determination of global optimal solutions and are easy to solve. However, the unboundedness of the convex loss function implies that it is not suited for handling outliers. Compared to convex loss functions, non-convex loss functions are more robust to outliers. Recently, Singh et al. [21] proposed a correntropy-based loss function called C-loss. Based on information theory and the kernel method, correntropy [22,23] is considered to be a generalized local similarity measure between two random variables. As a non-convex, bounded loss function, the C-loss function has been widely used in machine learning to improve robustness. In 2019, Zhao et al. [24] applied the C-loss function to ELM for the first time. They proposed the C-loss based ELM (CELM), and also experimentally demonstrated that the generalization performance was better compared to that of other algorithms. In real life, the distribution of datasets tends to be asymmetric, and the training samples are easily contaminated by noise. In order to better consider the distribution characteristics inside the data and improve the generalization ability of the algorithm, a non-convex robust loss function is proposed, called asymmetric C-loss (AC-loss). A robust extreme learning machine based on the asymmetric C-loss and L1-norm (called L1-ACELM) is then developed. The main contributions of this report are as follows:\n(1) Based on the expectile penalty and correntropy loss function, a new loss function (AC-loss) is developed. AC-loss retains some important properties of C-loss such as non-convexity and boundedness. AC-loss is asymmetric, and it can handle unbalanced noise. (2) A novel approach called the L1-norm robust regularized extreme learning machine with asymmetric C-loss (L1-ACELM) is proposed by applying the proposed AC-loss\nAxioms 2023, 12, 204 3 of 22\nfunction and the L1-norm in the objective function of ELM to enhance robustness to outliers.\n(3) The non-convexity of the AC-loss function makes it difficult for L1-ACELM to be solved. The half-quadratic optimization algorithm [25\u201327] is used to address these problems. Moreover, the convergence of the proposed algorithms is analyzed.\nThe remainder of this paper is structured as follows. Section 2 briefly reviews ELM, RELM, C-loss function, and the half-quadratic optimization algorithm. In Section 3, we propose the asymmetric C-loss function and the L1-ACELM model. Next, the half-quadratic optimization algorithm is used to solve L1-ACELM. In addition, we analyze the convergence of the algorithm. The experimental results for the artificial and benchmark datasets are presented in Section 4. Section 5 summarizes the main conclusions and further study.\n2. Related Work 2.1. Extreme Learning Machine (ELM)\nELM is a new single hidden layer feedforward neural network that is first proposed by Huang et al. [2]. Unlike traditional SLFN, the input weights and thresholds of the hidden layer in ELM are randomly generated and the output weights can be determined using the least square method. Hence, it is much faster than traditional SLFN. In addition, ELM has good generalization ability.\nGiven N arbitrary distinct samples {X, Y} = {xi, yi} N i = 1, xi = [xi1, xi2, . . . , xim] T \u2208 Rm and yi = [yi1, yi2, . . . , yin]\nT \u2208 Rn are the input samples and the corresponding output vectors, respectively. The output of a standard SLFN with L hidden nodes can be expressed as follows:\nf(xi) = L\n\u2211 j = 1\n\u03b2jh ( \u03b1j, bj, xi ) , i = 1, . . . , N (1)\nwhere \u03b1j = [ \u03b1j1, \u03b1j2, . . . , \u03b1jm ]T \u2208 Rm is the input weight vector that connects the input node to the j-th hidden layer node and bj \u2208 R is the bias of the j-th hidden node. \u03b2j = [ \u03b2 j1, \u03b2 j2, . . . , \u03b2 jn\n]T \u2208 Rn is the output weight vector that connects the j-th hidden layer node to the output node, and h ( \u03b1j, bj, xi ) is the output of the j-th hidden layer node with respect to the input xi. f(\u00b7) denotes the actual output vector of SLFN. For ELM, the input weight vector and the bias that connects the input node to the hidden layer node are randomly assigned instead of being updated. Therefore, it can be converted to a linear model:\nF = H\u03b2 (2)\nwhere\nH = h(x1)... h(xN)  =  h(\u03b11, b1, x1) . . . h(\u03b1L, bL, x1)... . . . ... h(\u03b11, b1, xN) . . . h(\u03b1L, bL, xN)  N\u00d7L , \u03b2 = \u03b2 T 1 ... \u03b2TL  L\u00d7n and F =  f(x1) T ... f(xN) T  N\u00d7n\nHere, H is the output matrix of the hidden layer. Thus, the output weight vector that connects the hidden layer node to the output node can be determined by solving the following equation:\nmin \u03b2 \u2016H\u03b2 \u2212 Y\u20162 (3)\nELM requires the approximation of the training samples with zero error. Therefore, Equation (3) can be written as:\nH\u03b2 = Y (4)\nThe output weight \u03b2 is the least squares solution of Equation (4), which can be obtained as follows:\n\u03b2 = H+Y (5)\nAxioms 2023, 12, 204 4 of 22\nwhere H+ is the Moore-Penrose generalized inverse of the matrix H. To avoid overfitting of the model, regularized ELM is proposed, which facilitates better generalization performance by minimizing the sum of the training error and the norm of the output weights [28]. RELM can be expressed as follows:\nmin \u03b2 \u2016H\u03b2 \u2212 Y\u201622 +\n\u03b3 2 \u2016\u03b2\u201622 (6)\nThe optimal solution to RELM is computed as follows:\n\u03b2 =\n{( HTH + \u03b3I )\u22121HTY i f N \u2265 L HT ( HHT + \u03b3I\n)\u22121Y i f N < L (7) where I is an identity matrix.\n2.2. Correntropy-Induced Loss (C-Loss)\nCorrentropy is a generalized similarity measure between two random variables in a small neighborhood defined by the kernel width \u03c3. For a regression problem, the choice of the loss function could ensure that the similarity between the actual output and the target value is maximized, which is equivalent to the maximization of correntropy. Thus, the C-loss function [21] is proposed by Singh et al., which is defined as:\nLC(yi, f (xi)) = 1 \u2212 exp { \u2212 (yi \u2212 f (xi)) 2\n2\u03c32\n} (8)\nAs a bounded non-convex loss function, the C-loss loss function is more robust to outliers than the traditional squared loss function.\n2.3. Half-Quadratic Optimization\nThe half-quadratic optimization algorithm based on the conjugate function theory [29] is usually used for convex optimization and non-convex optimization problems. This method transforms the original non-convex objective function into a half-quadratic objective function by introducing auxiliary variables. As such, the objective function cannot be solved directly, and a two-step alternating minimization method is required. The specific operations are as follows: given the original variables, the auxiliary variables are optimized. The variables are then optimized, and the original variables are determined.\nThe minimization problem is as follows:\nmin v \u03c6v(v) + F(v) (9)\nwhere v = [v1, v2, . . . , vN ] T \u2208 RN , \u03c6(\u00b7) is a potential loss function with \u03c6(v) = N \u2211\ni = 1 \u03c6(vi)\nand F(\u00b7) is a convex penalty function. Considering the half-quadratic optimization algorithm, we introduce an auxiliary variable p = [p1, p2, . . . , pN ] T \u2208 RN into \u03c6(\u00b7), which can then be expressed as:\n\u03c6(vi) = minpi {Q(vi, pi) + \u03d5(pi)} (10)\nwhere Q(vi, pi) is a half-quadratic function, which can be represented in the additive form QA(vi, pi) = 12 (\u221a cvi \u2212 pi/ \u221a c )2 or the multiplicative form QM(vi, pi) = 12 pivi2.\nSubstituting Equation (10) into Equation (9), we obtain the following optimization problem:\nmin v \u03c6v(v) + F(v) = minv,p {Q(v, p) + \u03d5(p) + F(v)} (11)\nAxioms 2023, 12, 204 5 of 22\nwhere pi is determined using a function g(\u00b7), which is the conjugate function of \u03c6(\u00b7). Alternatively, Equation (11) can then be optimized as follows:\npt+1 = g(v) (12)\nvt+1 = argmin v\n{ Q ( v, pt+1 ) + F(v) } (13)\nwhere t represents the t-th iteration.\n3. Main Contributions 3.1. Asymmetric C-Loss Function (AC-Loss)\nAs a measure of risk, the expectile is an extension of the quantile, which represents the distributional information of a random variable. The expectile loss is essentially a squared pinball loss, which can also be considered as an asymmetric squared loss. The asymmetric least square loss function can be expressed as:\nL\u03c4(yi, f (xi)) =\n{ \u03c4(yi \u2212 f (xi))2\n(1 \u2212 \u03c4)(yi \u2212 f (xi))2 i f yi \u2212 f (xi) \u2265 0 i f yi \u2212 f (xi) < 0\n(14)\nHowever, given that the asymmetric least square loss is an unbounded loss function, it is more sensitive to outliers. Therefore, we construct an asymmetric C-loss (AC-loss) function, based on the C-loss function and the expectile loss function, which is a nonconvex, asymmetric, and bounded function for dealing with outliers and noise. The AC-loss function is defined as follows:\nLalsC (yi, f (xi)) =  1 \u2212 exp { \u2212\u03c4(yi \u2212 f (xi))2 2\u03c32 } i f yi \u2212 f (xi) \u2265 0 1 \u2212 exp { \u2212(1 \u2212 \u03c4)(yi \u2212 f (xi))2\n2\u03c32\n} i f yi \u2212 f (xi) < 0\n(15)\nThe plot of the AC-loss function is shown in Figure 1.\nAxioms 2023, 12, x FOR PEER REVIEW 6 of 24\nAxioms 2023, 12, 204 6 of 22\n3.2. L1-ACELM\nTo improve the generalization performance of RELM, the proposed loss function is introduced to replace the squared loss function. To further enhance robustness to outliers, the L2 norm of structural risk in RELM is replaced with the L1 norm. Therefore, we propose a new robust ELM (called L1-ACELM):\nmin \u03b2\nJ(\u03b2) = N\n\u2211 i = 1 LalsC (yi \u2212 h(xi)\u03b2) + \u03b3\u2016\u03b2\u20161 (16)\nwhere \u03b3 > 0 is a regularized parameter. Since AC-loss is a non-convex loss function, it is difficult to directly optimize the objective function. The half-quadratic optimization algorithm is usually applied to optimize non-convex problems. Therefore, we chose the half-quadratic optimization algorithm to find the optimal solution of the objective function.\n3.3. Solving Method\nFor the function f (u) = exp(u), there exists a convex function g(v), which is expressed as follows:\ng(v) = \u2212v log(\u2212v) + v (17)\nwhere v < 0, and the conjugate function g\u2217(u) of the function g(v) is defined as:\ng\u2217(u) = sup v {uv + v log(\u2212v) \u2212 v} (18)\nwhere v = \u2212 exp(\u2212u) < 0 (19)\nBy substituting Equation (19) into Equation (18), we have\ng\u2217(u) = exp(\u2212u) (20)\nNow, let u =  \u03c4e2i 2\u03c32 i f ei \u2265 0\n(1 \u2212 \u03c4)e2i 2\u03c32 i f ei < 0\nand ei = yi \u2212 h(xi)\u03b2, then Equation (18) can\nbe expressed as:\ng\u2217(u) =  sup v { \u03c4e2i 2\u03c32 v + v log(\u2212v) \u2212 v } sup\nv\n{ (1 \u2212 \u03c4)e2i 2\u03c32 v + v log(\u2212v) \u2212 v } =  exp ( \u2212 \u03c4e 2 i 2\u03c32 ) i f ei \u2265 0 exp ( \u2212 (1 \u2212 \u03c4)e 2 i 2\u03c32 ) i f ei < 0 (21)\nwhere\nvi =  \u2212 exp ( \u2212 \u03c4e 2 i 2\u03c32 ) i f ei \u2265 0 \u2212 exp ( \u2212 (1 \u2212 \u03c4)e 2 i\n2\u03c32\n) i f ei < 0\n(22)\nBy combining Equations (21) and (16), we have\nmin \u03b2,v J(\u03b2, v) =  N \u2211 i = 1 ( 1 \u2212 sup vi { exp ( \u2212 \u03c4e 2 i 2\u03c32 ) vi + g(vi) }) + \u03b3\u2016\u03b2\u20161 i f ei \u2265 0 N \u2211\ni = 1\n( 1 \u2212 sup\nvi\n{ exp ( \u2212 (1 \u2212 \u03c4)e 2 i\n2\u03c32\n) vi + g(vi) }) + \u03b3\u2016\u03b2\u20161 i f ei < 0\ns.t. \u03b2h(xi) = yi \u2212 ei, i = 1, 2, . . . , N\n(23)\nAxioms 2023, 12, 204 7 of 22\nwhere v = [v1, v2, . . . , vN ] T . Equation (23) can be simplified as:\nmin \u03b2,v\nJ\u2032(\u03b2, v) =  sup v { N \u2211 i = 1 ( \u2212 \u03c4e 2 i 2\u03c32 vi \u2212 vi log(\u2212vi) + vi )} + \u03b3\u2016\u03b2\u20161 i f ei \u2265 0\nsup v\n{ N \u2211\ni = 1\n( \u2212 (1 \u2212 \u03c4)e 2 i 2\u03c32 vi \u2212 vi log(\u2212vi) + vi )} + \u03b3\u2016\u03b2\u20161 i f ei < 0\ns.t. h(xi)\u03b2 = yi \u2212 ei, i = 1, 2, . . . , N\n(24)\nThe optimal solution \u03b2 can be obtained by solving Equation (24) using the alternating optimization method. Firstly, given the original variables \u03b2t, we can obtain the optimal solution for the auxiliary variables vt+1. When \u03b2t is given, the minimization problem is given as follows:\nmin v J(v) =  N \u2211 i = 1 ( \u2212 \u03c4(yi \u2212 f (xi)) 2 2\u03c32 vi \u2212 vi log(\u2212vi) + vi ) i f ei \u2265 0 N \u2211\ni = 1\n( \u2212 (1 \u2212 \u03c4)(yi \u2212 f (xi)) 2 2\u03c32 vi \u2212 vi log(\u2212vi) + vi ) i f ei < 0 (25)\nAccording to the half-quadratic optimization algorithm, the auxiliary variables vt+1\ncan be obtained by solving Equation (24). Thus, we have:\nvt+1i =  \u2212 exp ( \u2212 \u03c4(yi \u2212 f t(xi)) 2 2\u03c32 ) i f ei \u2265 0 \u2212 exp ( \u2212 (1 \u2212 \u03c4)(yi \u2212 f t(xi)) 2\n2\u03c32\n) i f ei < 0 , i = 1, 2, . . . , N (26)\nSecondly, the auxiliary variables vt+1 are fixed and the optimal solution of the original variable \u03b2t+1 can be obtained by solving the following minimization problem:\nmin \u03b2t+1\nJ ( \u03b2t+1 ) =  N \u2211 i = 1 ( \u2212 \u03c4vi2\u03c32 e 2 i ) + \u03b3 \u2225\u2225\u03b2t+1\u2225\u22251 i f ei \u2265 0 N \u2211\ni = 1\n( \u2212 (1 \u2212 \u03c4)vi2\u03c32 e 2 i ) + \u03b3 \u2225\u2225\u03b2t+1\u2225\u22251 i f ei < 0 s.t. \u03b2t+1h(xi) = yi \u2212 ei, i = 1, 2, . . . , N\n(27)\nEquation (27) is equivalent to\nmin \u03b2t+1\nJ ( \u03b2t+1 ) =  N \u2211 i = 1 ( \u2212 \u03c4v t+1 i 2\u03c32 ( yi \u2212 h(xi)\u03b2t+1 )2) + \u03b3 \u2225\u2225\u03b2t+1\u2225\u22251 i f yi \u2265 h(xi)\u03b2t+1 N \u2211\ni = 1\n( \u2212 (1 \u2212 \u03c4)v t+1 i 2\u03c32 ( yi \u2212 h(xi)\u03b2t+1 )2) + \u03b3 \u2225\u2225\u03b2t+1\u2225\u22251 i f yi < h(xi)\u03b2t+1 (28) Since the L1 norm exists in the objective function, the proximal gradient descent\n(PGD) algorithm is applied to solve the optimization problem Equation (28). The objective function J ( \u03b2t+1 ) can be written as\nJ ( \u03b2t+1 ) = S ( \u03b2t+1 ) + \u03b3 \u2225\u2225\u2225\u03b2t+1\u2225\u2225\u2225 1 , (29)\nwhere\nS ( \u03b2t+1 ) =  N \u2211 i = 1 ( \u2212 \u03c4v t+1 i 2\u03c32 ( yi \u2212 h(xi)\u03b2t+1 )2) i f yi \u2265 h(xi)\u03b2t+1 N \u2211\ni = 1\n( \u2212 (1 \u2212 \u03c4)v t+1 i 2\u03c32 ( yi \u2212 h(xi)\u03b2t+1 )2) i f yi < h(xi)\u03b2t+1 (30)\nAxioms 2023, 12, 204 8 of 22\nS ( \u03b2t+1 ) is differentiable and its derivative is as follows:\n\u2207S ( \u03b2t+1 ) =  N \u2211 i = 1 ( \u03c4vt+1i \u03c32 hT(xi) ( yi \u2212 h(xi)\u03b2t+1 )) i f yi \u2265 h(xi)\u03b2t+1 N \u2211\ni = 1\n( (1 \u2212 \u03c4)vt+1i\n\u03c32 hT(xi) ( yi \u2212 h(xi)\u03b2t+1 )) i f yi < h(xi)\u03b2t+1\n(31)\nSince\u2207S ( \u03b2t+1 )\nsatisfies the L-Lipschitz continuity condition, there is a constant \u03b7 > 0 such that \u2225\u2225\u2225\u2207S(\u03b2) \u2212 \u2207S(\u03b2t+1)\u2225\u2225\u22252\n2 \u2264 \u03b7 \u2225\u2225\u2225\u03b2 \u2212 \u03b2t+1\u2225\u2225\u22252 2 , \u2200 ( \u03b2, \u03b2t+1 )\n(32)\nThe second-order Taylor expansion of the function S ( \u03b2t+1 ) can be expressed as\nS ( \u03b2; \u03b2t+1 ) \u2248 S ( \u03b2k+1 ) +\u2207S ( \u03b2k+1 )( \u03b2 \u2212 \u03b2k+1 ) + \u03b72 \u2225\u2225\u2225\u03b2 \u2212 \u03b2k+1\u2225\u2225\u2225 = \u03b72\n\u2225\u2225\u2225\u03b2 \u2212 (\u03b2k+1 \u2212 1\u03b7\u2207S(\u03b2k+1))\u2225\u2225\u222522 + \u03b4(\u03b2k+1) (33) where \u03b4 ( \u03b2t+1 ) is a constant that is independent of \u03b2t+1.\nIntroducing \u2225\u2225\u03b2t+1\u2225\u22251 into the objective function, the iterative equation of the proximal\ngradient descent can be expressed as\n\u03b2t+1 = argmin \u03b2t+1\n\u03b7\n2 \u2225\u2225\u2225\u2225\u03b2 \u2212 (\u03b2t+1 \u2212 1\u03b7\u2207S(\u03b2t+1) )\u2225\u2225\u2225\u22252\n2 + \u03b3\n\u2225\u2225\u2225\u03b2t+1\u2225\u2225\u2225 1\n(34)\nLet z = \u03b2t+1 \u2212 1\u03b7\u2207S ( \u03b2t+1 ) . Then, the closed-form solution of Equation (34) can be\nwritten as:\n\u03b2 t+1\ni =  zi \u2212 \u03b3/\u03b7 \u03b3/\u03b7 < zi\n0 |zi| \u2264 \u03b3/\u03b7 zi + \u03b3/\u03b7 zi < \u2212 \u03b3/\u03b7 , i = 1, 2, . . . , N (35)\nwhere \u03b2 t+1 i and zi represent the i-th component of \u03b2 t+1 and z, respectively. We develop a half-quadratic optimization to solve the proposed model, and the pseudo code is presented in Algorithm 1.\nAlgorithm 1. Half-quadratic optimization for L1-ACELM\nInput: The training dataset T = {(xi, yi)}Ni = 1, the number of hidden layer nodes L, the activation function h(x), the regularization parameter \u03b3, the maximum number of iterations tmax, window width \u03c3, a small number \u03c1 and the parameter \u03c4. Output: the output weight vector \u03b2. Step 1. Randomly generate input weight \u03b1i and hidden layer bias bi with L hidden nodes. Step 2. Calculate hidden output matrix H(x). Step 3. Compute \u03b2 by Equation (7). Step 4. Let \u03b20 = \u03b2 and \u03b21 = \u03b2, set t = 1. Step 5. While\n\u2223\u2223J(\u03b2t) \u2212 J(\u03b2t \u2212 1)\u2223\u2223 < \u03c1 or t < tmax do calculate vt+1i by Equation (26). update \u03b2t+1 using Equation (35). compute J ( \u03b2t+1 ) by Equation (29). update t: = t + 1. End while Step 6: Output result given by \u03b2 = \u03b2t \u2212 1.\nAxioms 2023, 12, 204 9 of 22\n3.4. Convergence Analysis Proposition 1. The sequence { J ( \u03b2t, vt ) , t = 1, 2, . . . , t } generated by Algorithm 1 is convergent.\nProof. Let \u03b2t and vt be the optimal solution to the objective function (23) after t iterations. In the half-quadratic optimization problem, the conjugate function g\u2217(\u00b7) satisfies {Q(\u03b2i, g\u2217(\u03b2i)) + \u03d5(\u03b2i)} \u2264 {Q(\u03b2i, g\u2217(vi)) + \u03d5(vi)}. When \u03b2t is fixed, we can obtain the optimal solution vt+1 of v at the (t + 1)-th iteration from Equation (26), then we have:\nJ ( \u03b2t, vt+1 ) \u2264 J ( \u03b2t, vt )\n(36)\nNext, when vt+1 is fixed, we can optimize (28) to obtain the solution \u03b2t+1 of \u03b2 at the (t + 1)-th iteration. Then we have:\nJ ( \u03b2t+1, vt+1 ) \u2264 J ( \u03b2t, vt+1 )\n(37)\nCombining Inequation (36) with Inequality (37), we have:\nJ ( \u03b2t+1, vt+1 ) \u2264 J ( \u03b2t, vt+1 ) \u2264 J ( \u03b2t, vt )\n(38)\nHence, the optimization problem J(\u03b2, v) is bounded, and the sequence{ J ( \u03b2t, vt ) , t = 1, 2, . . . , t } is convergent.\n4. Experiments 4.1. Experimental Setup\nTo evaluate the performance of the proposed L1-ACELM algorithm, we performed numerical simulations using two artificial datasets and ten standard benchmark datasets. To show the effectiveness of the L1-ACELM algorithm compared to traditional algorithms including extreme learning machine (ELM), regularized ELM (RELM), and C-loss based ELM (CELM), several experiments were performed. All experiments were implemented in Matlab2016a on a PC with an i5-7200U Intel(R) Core (TM) processor (2.70 GHz) 4 GB RAM. To evaluate the prediction performance of the L1-ACELM algorithm, the regression evaluation metrics are defined as follows:\n(1) The root mean square error (RMSE)\nRMSE = \u221a\u221a\u221a\u221a 1 N N\n\u2211 i = 1\n(yi \u2212 y\u0302i)2 (39)\n(2) Mean absolute error (MAE)\nMAE = 1 N\nN\n\u2211 i = 1 |yi \u2212 y\u0302i| (40)\n(3) The ratio of the sum squared error (SSE) to the sum squared deviation of the sample SST (SSE/SST) is given as:\nSSE/SST =\nN \u2211\ni = 1 (y\u0302i \u2212 yi)2\nN \u2211\ni = 1 (yi \u2212 yi)\n2 (41)\n(4) The ratio between the interpretable sum deviation SSR and SST (SSR/SST) is given by:\nAxioms 2023, 12, 204 10 of 22\nSSR/SST =\nN \u2211\ni = 1 (y\u0302i \u2212 yi)\n2\nN \u2211\ni = 1 (yi \u2212 yi)\n2 (42)\nwhere N is the number of samples. yi and y\u0302i denote the target values and the corresponding\npredicted values, respectively. yi can be calculated from yi = 1 N\nN \u2211\ni = 1 yi, which represents\nthe average value of y1, y2, . . . , yN . The sigmoid function is chosen as the activation function for ELM, RELM, CELM, and L1-ACELM, and can be expressed as:\nh(x) = 1 1 + exp ( \u2212aTi x + bi ) (43) Since the original algorithms and the proposed algorithm involve many parameters,\nto ensure the best performance, ten-fold cross-validation is used to determine the optimal parameters. In ELM and RELM, the number of hidden layer nodes L = 30 is fixed. For RELM, CELM, and L1-ACELM, the optimal value of the regularization parameter \u03b3 is selected from the set {2\u221250, 2\u221249, . . . , 249, 250}. For CELM and L1-ACELM, the window width \u03c3 is selected from the range {2\u22122, 2\u22121, 20, 21, 22}. For L1-ACELM, the parameter \u03c4 is obtained from the set {0.1, 0.2, . . . , 0.9}.\n4.2. Performance on Artificial Datasets\nTo verify the robustness of the proposed L1-ACELM, two artificial datasets were generated using six different types of noise, both of which consisted of 2000 data points. Table 1 shows the specific forms of two artificial datasets and different types of noise. \u03bbi \u223c N ( 0, s2 ) indicates that \u03bbi has a normal distribution with a mean of zero and variance of s2, \u03bbi \u223c U(a, b) means that \u03bbi has a uniform distribution in the interval [a, b], \u03bbi \u223c T(c) indicates that \u03bbi has a t-distribution with c degrees of freedom.\nType D: x \u2208 [\u22123, 3],\u03bbi \u223c U(0.5, 0.5) Type E: x \u2208 [\u22123, 3],\u03bbi \u223c T(5) Type F: x \u2208 [\u22123, 3],\u03bbi \u223c T(10) Self-defining function yi = ex 2 i sin c(0.3\u03c0xi) + \u03bbi\nFigure 2 shows different types of noise graphs, the graphs of the sinc function, and the graphs of the sinc function with different noises.\nAxioms 2023, 12, x FOR PEER REVIEW 12 of 24\n(a) (b)\n(c) (d)\n(e) (f)\nFigure 2. Graphs of the sinc function with different noises.\nFigure 3 shows different types of noise graphs, the graphs of the self-defining func-\ntion, and the graphs of the self-defining function with different noises.\nFigure 2. Cont.\nAxioms 2023, 12, 204 11 of 22\nAxioms 2023, 12, x FOR PEER REVIEW 12 of 24\n(a) (b)\n(c) (d)\n(e) (f)\nFigure 2. Graphs of the sinc function with different noises.\nFigure 3 shows different types of noise graphs, the graphs of the self-defining func-\ntion, and the graphs of the self-defining function with different noises.\nFigure 2. Graphs of the inc function with different nois s.\nFigure 3 shows different types of noise graphs, the graphs of the self-defining functio , and the graphs of the self-defining function with different noises.\nAxioms 2023, 12, x FOR PEER REVIEW 13 of 24\n(a) (b)\n(c) (d)\n(e) (f)\nFigure 3. Graphs of the self-defining function with different noises.\nIn our experiments, we randomly selected 1600 samples as the training dataset and\nthe remaining 400 samples as the testing dataset. To evaluate the effectiveness of the pro-\nposed algorithm, we compared its performance to that of ELM, RELM, and CELM. Table\n2 shows the optimal RMSE, MAE, SSE/SST, and SSR/SST of the four algorithms that were\nobtained based on the optimal parameters selected using the ten-fold cross-validation\nmethod. Table 2 also lists the optimal parameters for each algorithm. The regression fitting\nresults of ELM, RELM, CELM, and L1-ACELM on two artificial datasets with noise are\nshown in Figures 4 and 5.\nFigure 3. Cont.\nAxioms 2023, 12, 204 12 of 22\nAxioms 2023, 12, x FOR PEER REVIEW 13 of 24 (a) (b)\n(c) (d)\n(e) (f)\nFigure 3. Graphs of the self-defining function with different noises.\nIn our experiments, we randomly selected 1600 samples as the training dataset and\nthe remaining 400 samples as the testing dataset. To evaluate the effectiveness of the pro-\nposed algorithm, we compared its performance to that of ELM, RELM, and CELM. Table\n2 shows the optimal RMSE, MAE, SSE/SST, and SSR/SST of the four algorithms that were\nobtained based on the optimal parameters selected using the ten-fold cross-validation\nmethod. Table 2 also lists the optimal parameters for each algorithm. The regression fitting\nresults of ELM, RELM, CELM, and L1-ACELM on two artificial datasets with noise are\nshown in Figures 4 and 5.\nFigure 3. Graphs of the self-defining fu ction with different noises.\nIn our experiments, we randomly selected 1600 samples as the training dataset nd the remaining 400 samples as the testing dataset. To evalu te the eff ctiven ss of the proposed algorithm, we compared its performance to that of ELM, RELM, and CELM. Table 2 shows the optimal RMSE, MAE, S E/S T, and S R/SST of the four algorithms that were obtained based on the optimal p rameters s lect d using the ten-fold cr ss-validation method. Table 2 also lists the optimal parameters for each algorithm. The regression fitting results of ELM, RELM, CELM, and L1-ACELM on two artificial datasets with noise are shown in Figures 4 and 5. Axioms 2023, 12, x FOR PEER REVIEW 14 of 24\n(e) (f)\nFigure 4. Fitting results of the sinc function with different noises.\nAxioms 2023, 12, 204 13 of 22\nAxioms 2023, 12, x FOR PEER REVIEW 14 of 24 (a) (b)\n(c) (d)\n(e) (f)\nFigure 4. Fitting results of the sinc function with different noises. Figure 4. Fitting results of the sinc function with different noises.\nAxioms 2023, 12, x FOR PEER REVIEW 15 of 24\n(a) (b)\n(c) (d)\n(e) (f)\nFigure 5. Fitting results of the self-defining function with different noises.\nFigure 5. Fitting results of the self-defining function with different noises.\nAxioms 2023, 12, 204 14 of 22\nAxioms 2023, 12, 204 15 of 22\nFigures 4 and 5 demonstrate the fitting effect of the four algorithms on the two artificial datasets. Based on these figures, it is observed that the fitting curve of L1-ACELM is the closest to the real function curve compared to the other three algorithms. In Table 2, the best test results are shown in bold. The data in Table 2 demonstrate that L1-ACELM exhibits better performance in most cases when compared to the other three algorithms for the two artificial datasets with different noises. It is evident that L1-ACELM has smaller RMSE, MAE, and SSE/SST, and larger SSE/SSR. This indicates that L1-ACELM is more robust to noise. For example, for the sinc function, except for F noise, the performance of the proposed algorithm is superior to that of the other algorithms for different types of noise. Moreover, it is seen that L1-ACELM has better generalization performance in the case of unbalanced noise data. In conclusion, L1-ACELM is more stable in a noisy environment.\n4.3. Performance on Benchmark Datasets\nTo further test the robustness of L1-ACELM, experiments were performed on ten UCI datasets [30] with different levels of noise, including noise-free datasets, datasets with 5% noise, and datasets with 10% noise. Noise datasets were only added to the target output value of the training datasets. Among them, datasets with 5% noise indicate that the noisy data are 5% of the training dataset. The data in the noisy dataset are randomly taken from the set [0, d], where d is the average of the target output values of the training datasets. In the experiment, we randomly selected 80% of the data as the training dataset and the remaining 20% as the testing dataset for each benchmark dataset. The specific description is shown in Table 3.\nTo better reflect the performance of the proposed algorithm L1-ACELM, the RMSE, MAE, SSE/SST, and SSR/SST were compared with those of ELM, RELM, and CELM. The evaluation indicators and the ranking of each algorithm for different noise environments are listed in Tables 4\u20136, and the best test results are shown in bold. From Table 4 to Table 6, it is observed that the performance of each algorithm decreases as the noise level increases. However, compared to the other algorithms, the performance of L1-ACELM is still the best in most cases. From Table 4, it can be concluded that L1-ACELM performs best on nine datasets out of a total of ten datasets in term of the RMSE and SSR/SST values. Similarly, for the MAE and SSE/SST values, L1-ACELM exhibits the best performance on all the datasets. Table 5 shows that after adding 5% noise, the performance of each algorithm decreases, and according to the RMSE value, the proposed algorithm performed well on eight of the ten datasets. For the MAE, SSE/SST, and SSR/SST values, L1-ACELM performs better for nine datasets. Moreover, for the RMSE, MAE, and SSR/SST values, it exhibits superior performance in nine cases and for the SSE/SST values, it has better performance in all ten datasets.\nAxioms 2023, 12, 204 16 of 22\nAxioms 2023, 12, 204 17 of 22\nAxioms 2023, 12, 204 18 of 22\nTo further illustrate the difference between the proposed algorithm and traditional algorithms, we conducted statistical analysis on the experimental results. Friedman\u2019s test [31] is a well-known test for comparing the performance of various algorithms on datasets. Tables 7\u20139 list the average ranks of four algorithms on four performance measures under a noise-free environment and noisy environment.\nAxioms 2023, 12, 204 19 of 22\nwhich is distributed according to \u03c72F with k \u2212 1 degrees of freedom, where Rj is the average rank of the algorithms as listed in Tables 7\u20139. N = 10 and k = 4 are the number of datasets and the number of the algorithms, respectively. The Friedman statistic follows an F-distribution:\nFF = (N \u2212 1)\u03c72F\nN(k \u2212 1) \u2212 \u03c72F (45)\nwith k \u2212 1 and (k \u2212 1)(N \u2212 1) degrees of freedom. Table 10 shows the results of the Friedman test on the dataset without noise, with 5% noise, and with 10% noise. For \u03b1 = 0.05, the critical value of F\u03b1(3, 27) is 2.960. For the four algorithms, ELM, RELM, CELM, and L1-ACELM, FF > F\u03b1 is achieved by comparing the results from Table 10. Therefore, the assumption that all the algorithms perform the same is rejected. To further contrast the differences between paired algorithms, the Nemenyi test [32] is often used as a post hoc test.\nAxioms 2023, 12, 204 20 of 22\nThe critical difference can be expressed as:\nCD = q\u03b1\n\u221a k(k + 1)\n6N = 2.569\u00d7\n\u221a 4\u00d7 (4 + 1)\n6\u00d7 10 = 1.4832 (46)\nwhere the critical value of q0.05 is 2.569. Here, we can compare the average rank difference between the proposed algorithm and other algorithms using the CD value. If the average rank difference is greater than the CD value, this implies that the proposed algorithm is superior to the other algorithms. Otherwise, there is no difference between the two algorithms. Therefore, we can analyze the difference between the proposed algorithm and other algorithms in the following three cases:\n(1) Under noise-free environment. For the RMSE and SSR/SST index, the performance of L1-ACELM is better than that of ELM (4 \u2212 1.1 = 2.9 > 1.4832). For the MAE index, the performance of L1-ACELM is better than that of ELM (4 \u2212 1.0 = 3.0 > 1.4832) and RELM (2.6 \u2212 1.0 = 1.5 > 1.4832). There is no significant difference between L1-ACELM and CELM. (2) Under 5% noise environment. For the RMSE index, the performance of L1-ACELM is better than that of ELM (3.7 \u2212 1.0 = 2.7 > 1.4832), RELM (2.6 \u2212 1.0 = 1.6 > 1.4832), and CELM (2.5 \u2212 1.0 = 1.5 > 1.4832). For the MAE and SSE/SST index, the performance of L1-ACELM is better than that of ELM (3.7 \u2212 1.1 = 2.6 > 1.4832, 3.8 \u2212 1.1 = 2.7 > 1.4832) and RELM (2.7 \u2212 1.1 = 1.6 > 1.4832, 2.8 \u2212 1.1 = 1.7 > 1.4832). For the SSR/SST index, the performance of L1-ACELM is better than that of ELM (3.7 \u2212 1.15 = 2.55 > 1.4832) and CELM (2.65 \u2212 1.15 = 1.5 > 1.4832). (3) Under 10% noise environment. Similarly, for the RMSE, MAE, and SSE/SST index, the performance of L1-ACELM is better than that of ELM, RELM, and CELM. For the SSR/SST index, the performance of L1-ACELM is better than that of ELM and RELM.\n5. Conclusions\nIn this paper, a novel asymmetric, bounded, smooth non-convex loss function based on the expected loss and the correntropy loss is proposed, termed AC-loss. The AC-loss loss function and L1 norm are introduced into the regularized extreme learning machine, and an improved robust regularized extreme learning machine is proposed for regression. Owing to the non-convexity of the AC-loss function, it is difficult to solve L1-ACELM. As such, the half-quadratic optimization algorithm is applied to address the nonconvex optimization problem. To prove the effectiveness of L1-ACELM, experiments are conducted on artificial datasets and benchmark datasets with different types of noise, respectively. The results demonstrate the significant advantages of L1-ACELM in generalization performance and robustness, especially when the data distribution with noise and outliers are asymmetric. The PGD algorithm is used to solve the L1-ACELM in this paper. Since it is an iterative process, the training speed is reduced. In the future, we will research a faster method to solve this optimization problem.\nAuthor Contributions: Conceptualization, Q.W. and F.W.; methodology, Q.W.; software, F.W.; validation, F.W., Y.A. and K.L.; writing\u2014original draft preparation, F.W.; writing\u2014review and editing, Q.W.; visualization, Y.A.; funding acquisition, Q.W. All authors have read and agreed to the published version of the manuscript.\nFunding: This research was funded by the National Natural Science Foundation of China under Grant (51875457), the Key Research Project of Shaanxi Province (2022GY-050, 2022GY-028), the Natural Science Foundation of Shaanxi Province of China (2022JQ-636, 2021JQ-701, 2021JQ-714), and Shaanxi Youth Talent Lifting Plan of Shaanxi Association for Science and Technology (20220129).\nData Availability Statement: The data presented in the article are freely available and are listed at the reference address in the bibliography.\nConflicts of Interest: The authors declare no conflict of interest.\nAxioms 2023, 12, 204 21 of 22\nReferences 1. Ding, S.; Su, C.; Yu, J. An optimizing BP neural network algorithm based on genetic algorithm. Artif. Intell. Rev. 2011, 36, 153\u2013162. [CrossRef] 2. Huang, G.B.; Zhu, Q.Y.; Siew, C.K. Extreme learning machine: A new learning scheme of feedforward neural networks. In\nProceedings of the 2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No. 04CH37541), Budapest, Hungary, 25\u201329 July 2004; pp. 985\u2013990.\n3. Huang, G.B.; Zhu, Q.Y.; Siew, C.K. Extreme learning machine: Theory and applications. Neurocomputing 2006, 70, 489\u2013501. [CrossRef] 4. Silva, B.L.; Inaba, F.K.; Evandro, O.T.; Ciarelli, P.M. Outlier robust extreme machine learning for multi-target regression. Expert Syst. Appl. 2020, 140, 112877. [CrossRef] 5. Li, Y.; Wang, Y.; Chen, Z.; Zou, R. Bayesian robust multi-extreme learning machine. Knowl. -Based Syst. 2020, 210, 106468. [CrossRef] 6. Liu, X.; Ge, Q.; Chen, X.; Li, J.; Chen, Y. Extreme learning machine for multivariate reservoir characterization. J. Pet. Sci. Eng. 2021, 205, 108869. [CrossRef] 7. Catoni, O. Challenging the empirical mean and empirical variance: A deviation study. Annales de l\u2019IHP Probabilit\u00e9s et Statistiques 2012, 48, 1148\u20131185. [CrossRef] 8. Deng, W.; Zheng, Q.; Chen, L. Regularized extreme learning machine. In Proceedings of the 2009 IEEE Symposium on Computational Intelligence and Data Mining, Nashville, TN, USA, 30 March\u20132 April 2009; pp. 389\u2013395. 9. Rong, H.J.; Ong, Y.S.; Tan, A.H.; Zhu, Z. A fast pruned-extreme learning machine for classification problem. Neurocomputing 2008, 72, 359\u2013366. [CrossRef] 10. Miche, Y.; Sorjamaa, A.; Bas, P.; Simula, O.; Jutten, C.; Lendasse, A. OP-ELM: Optimally pruned extreme learning machine. IEEE Trans. Neural Netw. 2009, 21, 158\u2013162. [CrossRef] 11. Ye, Q.; Yang, J.; Liu, F.; Zhao, C.; Ye, N.; Yin, T. L1-norm distance linear discriminant analysis based on an effective iterative algorithm. IEEE Trans. Circuits Syst. Video Technol. 2016, 28, 114\u2013129. [CrossRef] 12. Li, C.N.; Shao, Y.H.; Deng, N.Y. Robust L1-norm non-parallel proximal support vector machine. Optimization 2016, 65, 169\u2013183. [CrossRef] 13. Balasundaram, S.; Gupta, D. 1-Norm extreme learning machine for regression and multiclass classification using Newton method. Neurocomputing 2014, 128, 4\u201314. [CrossRef] 14. Dong, H.; Yang, L. Kernel-based regression via a novel robust loss function and iteratively reweighted least squares. Knowl. Inf. Syst. 2021, 63, 1149\u20131172. [CrossRef] 15. Dong, H.; Yang, L. Training robust support vector regression machines for more general noise. J. Intell. Fuzzy Syst. 2020, 39, 2881\u20132892. [CrossRef] 16. Farooq, M.; Steinwart, I. An SVM-like approach for expectile regression. Comput. Stat. Data Anal. 2017, 109, 159\u2013181. [CrossRef] 17. Razzak, I.; Zafar, K.; Imran, M.; Xu, G. Randomized nonlinear one-class support vector machines with bounded loss function to detect of outliers for large scale IoT data. Future Gener. Comput. Syst. 2020, 112, 715\u2013723. [CrossRef] 18. Gupta, D.; Hazarika, B.B.; Berlin, M. Robust regularized extreme learning machine with asymmetric Huber loss function. Neural Comput. Appl. 2020, 32, 12971\u201312998. [CrossRef] 19. Ren, Z.; Yang, L. Correntropy-based robust extreme learning machine for classification. Neurocomputing 2018, 313, 74\u201384. [CrossRef] 20. Ma, Y.; Zhang, Q.; Li, D.; Tian, Y. LINEX support vector machine for large-scale classification. IEEE Access. 2019, 7, 70319\u201370331. [CrossRef] 21. Singh, A.; Pokharel, R.; Principe, J. The C-loss function for pattern classification. Pattern Recognit. 2014, 47, 441\u2013453. [CrossRef] 22. Zhou, R.; Liu, X.; Yu, M.; Huang, K. Properties of risk measures of generalized entropy in portfolio selection. Entropy 2017, 19, 657. [CrossRef] 23. Ren, L.R.; Gao, Y.L.; Liu, J.X.; Shang, J.; Zheng, C.H. Correntropy induced loss based sparse robust graph regularized extreme learning machine for cancer classification. BMC Bioinform. 2020, 21, 1\u201322. [CrossRef] [PubMed] 24. Zhao, Y.P.; Tan, J.F.; Wang, J.J.; Yang, Z. C-loss based extreme learning machine for estimating power of small-scale turbojet engine. Aerosp. Sci. Technol. 2019, 89, 407\u2013419. [CrossRef] 25. He, Y.; Wang, F.; Li, Y.; Qin, J.; Chen, B. Robust matrix completion via maximum correntropy criterion and half-quadratic optimization. IEEE Trans. Signal Process. 2019, 68, 181\u2013195. [CrossRef] 26. Ren, Z.; Yang, L. Robust extreme learning machines with different loss functions. Neural Process. Lett. 2019, 49, 1543\u20131565. [CrossRef] 27. Chen, L.; Paul, H.; Qu, H.; Zhao, J.; Sun, X. Correntropy-based robust multilayer extreme learning machines. Pattern Recognit. 2018, 84, 357\u2013370. 28. Huang, G.; Huang, G.B.; Song, S.; You, K. Trends in extreme learning machines: A review. Neural Netw. 2015, 61, 32\u201348. [CrossRef] 29. Robini, M.C.; Yang, F.; Zhu, Y. Inexact half-quadratic optimization for linear inverse problems. SIAM J. Imaging Sci. 2018, 11,\n1078\u20131133. [CrossRef]\nAxioms 2023, 12, 204 22 of 22\n30. Blake, C.L.; Merz, C.J.; UCI Repository for Machine Learning Databases. Department of Information and Computer Sciences, University of California, Irvine. 1998. Available online: http://www.ics.uci.edu/~{}mlearn/MLRepository.html (accessed on 15 June 2022). 31. Dem\u0161ar, J. Statistical comparisons of classifiers over multiple data sets. J. Mach. Learn. Res. 2006, 7, 1\u201330. 32. Benavoli, A.; Corani, G.; Mangili, F. Should we really use post-hoc tests based on mean-ranks? J. Mach. Learn. Res. 2016, 17,\n152\u2013161.\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content."
        }
    ],
    "title": "L1-Norm Robust Regularized Extreme Learning Machine with Asymmetric C-Loss for Regression",
    "year": 2023
}