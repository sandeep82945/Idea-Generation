{
    "abstractText": "Influence maximization is a prototypical problem enabling applications in various domains, and it has been extensively studied in the past decade. The classic influence maximization problem explores the strategies for deploying seed users before the start of the diffusion process such that the total influence can be maximized. In its adaptive version, seed nodes are allowed to be launched in an adaptive manner after observing certain diffusion results. In this paper, we provide a systematic study on the adaptive influence maximization problem, focusing on the algorithmic analysis of the scenarios when it is not adaptive submodular. We introduce the concept of regret ratio which characterizes the key trade-off in designing adaptive seeding strategies, based on which we present the approximation analysis for the well-known greedy policy. In addition, we provide analysis concerning improving the efficiencies and bounding the regret ratio. Finally, we propose several future research directions.",
    "authors": [
        {
            "affiliations": [],
            "name": "Guangmo (Amo"
        },
        {
            "affiliations": [],
            "name": "Ruiqi Wang"
        }
    ],
    "id": "SP:9d8a24d19785438c07a3e132bd4bee1d9dc464aa",
    "references": [
        {
            "authors": [
                "D. Kempe",
                "J. Kleinberg",
                "\u00c9. Tardos"
            ],
            "title": "Maximizing the spread of influence through a social network",
            "venue": "Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2003, pp. 137\u2013146.",
            "year": 2003
        },
        {
            "authors": [
                "Y. Li",
                "J. Fan",
                "Y. Wang",
                "K.-L. Tan"
            ],
            "title": "Influence maximization on social graphs: A survey",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 30, no. 10, pp. 1852\u20131872, 2018.",
            "year": 1852
        },
        {
            "authors": [
                "A. Guille",
                "H. Hacid",
                "C. Favre",
                "D.A. Zighed"
            ],
            "title": "Information diffusion in online social networks: A survey",
            "venue": "ACM Sigmod Record, vol. 42, no. 2, pp. 17\u201328, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "C. Aslay",
                "L.V. Lakshmanan",
                "W. Lu",
                "X. Xiao"
            ],
            "title": "Influence maximization in online social networks",
            "venue": "Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. ACM, 2018, pp. 775\u2013776.",
            "year": 2018
        },
        {
            "authors": [
                "D. Golovin",
                "A. Krause"
            ],
            "title": "Adaptive submodularity: Theory and applications in active learning and stochastic optimization",
            "venue": "Journal of Artificial Intelligence Research, vol. 42, pp. 427\u2013486, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "G. Tong",
                "W. Wu",
                "S. Tang",
                "D.-Z. Du"
            ],
            "title": "Adaptive influence maximization in dynamic social networks",
            "venue": "IEEE/ACM Transactions on Networking (TON), vol. 25, no. 1, pp. 112\u2013125, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S. Vaswani",
                "L.V. Lakshmanan"
            ],
            "title": "Adaptive influence maximization in social networks: Why commit when you can adapt?",
            "venue": "arXiv preprint arXiv:1604.08171,",
            "year": 2016
        },
        {
            "authors": [
                "Y. Chen",
                "A. Krause"
            ],
            "title": "Near-optimal batch mode active learning and adaptive submodular optimization.",
            "venue": "ICML (1),",
            "year": 2013
        },
        {
            "authors": [
                "L. Sun",
                "W. Huang",
                "P.S. Yu",
                "W. Chen"
            ],
            "title": "Multi-round influence maximization",
            "venue": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2018, pp. 2249\u20132258.",
            "year": 2018
        },
        {
            "authors": [
                "S. Lei",
                "S. Maniu",
                "L. Mo",
                "R. Cheng",
                "P. Senellart"
            ],
            "title": "Online influence maximization",
            "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2015, pp. 645\u2013654.",
            "year": 2015
        },
        {
            "authors": [
                "W. Chen",
                "Y. Wang",
                "Y. Yuan",
                "Q. Wang"
            ],
            "title": "Combinatorial multiarmed bandit and its extension to probabilistically triggered arms",
            "venue": "The Journal of Machine Learning Research, vol. 17, no. 1, pp. 1746\u20131778, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "S. Vaswani",
                "B. Kveton",
                "Z. Wen",
                "M. Ghavamzadeh",
                "L.V. Lakshmanan",
                "M. Schmidt"
            ],
            "title": "Model-independent online learning for influence maximization",
            "venue": "International Conference on Machine Learning, 2017, pp. 3530\u20133539.",
            "year": 2017
        },
        {
            "authors": [
                "Z. Wen",
                "B. Kveton",
                "M. Valko",
                "S. Vaswani"
            ],
            "title": "Online influence maximization under independent cascade model with semi-bandit feedback",
            "venue": "Advances in Neural Information Processing Systems, 2017, pp. 3022\u20133032.",
            "year": 2017
        },
        {
            "authors": [
                "L. Seeman",
                "Y. Singer"
            ],
            "title": "Adaptive seeding in social networks",
            "venue": "Foundations of Computer Science (FOCS), 2013 IEEE 54th Annual Symposium on. IEEE, 2013, pp. 459\u2013468.",
            "year": 2013
        },
        {
            "authors": [
                "G. Salha",
                "N. Tziortziotis",
                "M. Vazirgiannis"
            ],
            "title": "Adaptive submodular influence maximization with myopic feedback",
            "venue": "2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM). IEEE, 2018, pp. 455\u2013462.",
            "year": 2018
        },
        {
            "authors": [
                "K. Han",
                "K. Huang",
                "X. Xiao",
                "J. Tang",
                "A. Sun",
                "X. Tang"
            ],
            "title": "Efficient algorithms for adaptive influence maximization",
            "venue": "Proceedings of the VLDB Endowment, vol. 11, no. 9, pp. 1029\u20131040, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "D. Golovin",
                "A. Krause",
                "E. CH"
            ],
            "title": "Adaptive submodularity: Theory and applications in active learning and stochastic optimization",
            "venue": "arXiv preprint arXiv:1003.3967, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "E. Mossel",
                "S. Roch"
            ],
            "title": "On the submodularity of influence in social networks",
            "venue": "Proceedings of the thirty-ninth annual ACM symposium on Theory of computing. ACM, 2007, pp. 128\u2013134.",
            "year": 2007
        },
        {
            "authors": [
                "C. Borgs",
                "M. Brautbar",
                "J. Chayes",
                "B. Lucier"
            ],
            "title": "Maximizing social influence in nearly optimal time",
            "venue": "Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms. SIAM, 2014, pp. 946\u2013957.",
            "year": 2014
        },
        {
            "authors": [
                "Y. Tang",
                "Y. Shi",
                "X. Xiao"
            ],
            "title": "Influence maximization in near-linear time: A martingale approach",
            "venue": "Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data. ACM, 2015, pp. 1539\u20131554.",
            "year": 2015
        },
        {
            "authors": [
                "G. Tong",
                "W. Wu",
                "L. Guo",
                "D. Li",
                "C. Liu",
                "B. Liu",
                "D.-Z. Du"
            ],
            "title": "An efficient randomized algorithm for rumor blocking in online social networks",
            "venue": "IEEE Transactions on Network Science and Engineering, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "G. Tong",
                "D.-Z. Du"
            ],
            "title": "Beyond uniform reverse sampling: A hybrid sampling technique for misinformation prevention",
            "venue": "arXiv preprint arXiv:1901.05149, 2019.",
            "year": 1901
        },
        {
            "authors": [
                "Z. Wang",
                "Y. Yang",
                "J. Pei",
                "L. Chu",
                "E. Chen"
            ],
            "title": "Activity maximization by effective information diffusion in social networks",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 29, no. 11, pp. 2374\u20132387, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "A. Lancichinetti",
                "S. Fortunato",
                "F. Radicchi"
            ],
            "title": "Benchmark graphs for testing community detection algorithms",
            "venue": "Physical review E, vol. 78, no. 4, p. 046110, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "J. Leskovec",
                "A. Krevl"
            ],
            "title": "SNAP Datasets}:{Stanford} large network dataset collection",
            "venue": "2015.",
            "year": 2015
        },
        {
            "authors": [
                "G. Tong",
                "D.-Z. Du",
                "W. Wu"
            ],
            "title": "On misinformation containment in online social networks",
            "venue": "Advances in Neural Information Processing Systems, 2018, pp. 339\u2013349.",
            "year": 2018
        },
        {
            "authors": [
                "H.T. Nguyen",
                "M.T. Thai",
                "T.N. Dinh"
            ],
            "title": "Stop-and-stare: Optimal sampling algorithms for viral marketing in billion-scale networks",
            "venue": "Proceedings of the 2016 International Conference on Management of Data. ACM, 2016, pp. 695\u2013710.",
            "year": 2016
        },
        {
            "authors": [
                "K. Huang",
                "S. Wang",
                "G. Bevilacqua",
                "X. Xiao",
                "L.V. Lakshmanan"
            ],
            "title": "Revisiting the stop-and-stare algorithms for influence maximization",
            "venue": "Proceedings of the VLDB Endowment, vol. 10, no. 9, pp. 913\u2013 924, 2017.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "On Adaptive Influence Maximization under General Feedback Models Guangmo (Amo) Tong, Member, IEEE and Ruiqi Wang\nAbstract\u2014Influence maximization is a prototypical problem enabling applications in various domains, and it has been extensively studied in the past decade. The classic influence maximization problem explores the strategies for deploying seed users before the start of the diffusion process such that the total influence can be maximized. In its adaptive version, seed nodes are allowed to be launched in an adaptive manner after observing certain diffusion results. In this paper, we provide a systematic study on the adaptive influence maximization problem, focusing on the algorithmic analysis of the scenarios when it is not adaptive submodular. We introduce the concept of regret ratio which characterizes the key trade-off in designing adaptive seeding strategies, based on which we present the approximation analysis for the well-known greedy policy. In addition, we provide analysis concerning improving the efficiencies and bounding the regret ratio. Finally, we propose several future research directions.\nIndex Terms\u2014Adaptive Influence Maximization, General Feedback Model, Regret Ratio, Approximation Analysis\nF"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "INFORMATION diffusion is an essential function of today\u2019sonline social network. An information cascade is typically initiated by a seed set, and it then spreads from users to users stochastically. Influence maximization (IM), proposed by Kempe, Kleinberg and Tardos [1], investigates the approaches for selecting seed nodes such that the resulted influence can be maximized. This problem has become one of the core problems in social computing, and it has drawn tremendous attentions due to its potential for enabling applications in different domains [2], [3], [4].\nAs a natural variant as well as an important generalization of the IM problem, adaptive influence maximization (AIM) problem allows the seed nodes to be selected after observing certain diffusion results, and correspondingly it investigates adaptive seeding strategies for maximizing the influence. Compared to the non-adaptive seeding strategy, an adaptive one can better utilize the budget because it makes decisions adapted to the observations. In addition, an adaptive seeding strategy can take account of the dynamic features of a social network, such as the change of the network topology due to the frequent user join and leave. For example, while limiting the spread of rumor by launching a positive cascade, selecting positive seed nodes in an adaptive manner can react against the new actions of the rumor cascade. In viral marketing, one would prefer to observe the feedback from the customer and launch the campaign step by step. In this paper, we study the AIM problem focusing on algorithm design and approximation analysis.\nModel and Problem Formulation. This paper considers the prominent Independent Cascade (IC) model.1 Under the\n\u2022 G. Tong and R. Wang are with the Department of Computer and Information Sciences, University of Delaware, Newark, DE, USA, 19715. E-mail: {amotong, wangrq}@udel.edu\nManuscript received xx, xx, 2019; revised xx, xx. 1. The results in this paper apply to other submodular diffusion models, namely the Linear Threshold model.\nIC model, the diffusion process is defined by the propagation probability between the users, and there is one chance of activation between each pair of users. The seed users are the first who are active, and in each round the users activated in the last round attempt to activate their inactive neighbors. The cascade spreads round by round and it terminates when no node can be further activated. In the AIM problem, we may first select some seed nodes, observe the diffusion for a certain number of rounds, and then deploy more seed nodes according to the diffusion results. Consequently, an adaptive seeding strategy consists of two parts: feedback model and seeding policy. A feedback model specifies how many diffusion rounds we would wait for before selecting the next seed node, and a seeding policy indicates which node should be selected.2 The ultimate goal is to maximize the number of active nodes.\nGeneral Feedback Model. While designing feedback models, intuitively we wish for more observations and therefore should delay the seeding action as much as possible. Therefore, given a budget k, the optimal design is to consume one budget each time and always wait for the diffusion to terminate before selecting the next seed node, which is called Full Adoption feedback model [5]. However, this optimal feedback model is not feasible for the applications where the seed nodes are required to be deployed after a fixed number of rounds. For example, a company would prefer to post online advertisements every Monday, or that one would propagate a certain information cascade in a timely manner and therefore would like to utilize all the budget after every one diffusion round. For such purpose, we propose a generalized feedback model where one seed node is selected after every d diffusion rounds with d \u2208 Z+ \u222a {\u221e}, where d = \u221e denotes the case when we always wait for the diffusion to terminate before selecting the next seed node. When d = 1 and\n2. For simplicity, we assume only one seed node is selected each time, and our analysis applies to the batch mode as discussed later in Sec. 6.\nar X\niv :1\n90 2.\n00 19\n2v 3\n[ cs\n.S I]\n1 2\nA pr\n2 01\n9\nd = \u221e, it reduces to the Myopic feedback model [5] and Full Adoption feedback model [5], respectively.\nThe State-of-the-art. A series of literature has considered the AIM problem under different settings (e.g., [5], [6], [7], [8]) in which the main technique is adaptive submodular maximization invented by Golovin and Krause [5]. This optimization technique shows that when the considered the problem is adaptive submodular, the greedy policy yields a (1\u22121/e)approximation. The existing works have utilized this result to investigate the AIM problem for special feedback models under which the AIM problem is adaptive submodular. In particular, these feedback models satisfy the condition that the seeding decisions are always made after the current diffusion has terminated. However, the AIM problem is not adaptive submodular under general feedback models, and it remains unknown that how to bound the performance of the greedy policy for the general cases (i.e., d 6= \u221e), which is the primary motivation of our work.\nThis Paper. The main results of this paper are briefly summarized as follows. \u2022 We provide a systematic model for the AIM problem\nunder general feedback settings and formulate the considered problem as an optimization problem. Our model naturally fits the AIM problem and generalizes the existing ones. \u2022 We introduce the concept of regret ratio which describes the trade-off between waiting and seeding. This ratio is not only intuitively meaningful but also measures the performance bound of the greedy policy. \u2022 We analyze the greedy policy from the view of decision tree, and show that the greedy policy gives a (1 \u2212 e\u2212 1 \u03b1(Tg) )-approximation under the general feed-\nback model, where \u03b1(Tg) is the regret ratio associated with the greedy policy. \u2022 We show how to generalize the reverse sampling technique for the AIM problem to improve the efficiency of the greedy policy. \u2022 We propose several directions of future work taking account of more realistic applications. \u2022 We design simulations to experimentally evaluate the greedy policy under general feedback models and further examine the effect of the feedback models on the diffusion process.\nRoad Map. We survey the related work in Sec. 2. The preliminaries are provided in Sec. 3. The main analysis is given in Sec. 4. In Sec. 5, we present the experiments. The future work is discussed in Sec. 6. Sec. 7 concludes this paper. The missing proofs and additional experimental results are provided in the supplementary material."
        },
        {
            "heading": "2 RELATED WORK.",
            "text": "Due to space limitation, we focus on the AIM problem and will not survey the literature concerning the classic IM problem. The interested reader is referred to the recent surveys [2], [3], [4].\nThe AIM problem was first studied by Golovin and Krause [5] in the investigation on adaptive submodular maximization. In [5], the authors proposed two special feedback models: Full Adoption feedback model and Myopic feedback model. Before selecting the next seed node, one\nalways waits for the diffusion process to terminate under the Full Adoption feedback model while waits for one diffusion round under the Myopic feedback model. For the Full Adoption feedback model, it is shown in [5] that greedy policy has an approximation ratio of 1 \u2212 1/e by using the technique of adaptive submodularity. Combining the result in [6] that the Full Adoption feedback model is optimal, greedy policy plus the Full Adoption feedback model has the approximation ratio of 1 \u2212 1/e to any adaptive seeding strategy for the AIM problem under a budget constraint. Following the optimization framework given in [5], Sun et al. [9], Chen et al. [8] and Vaswani et al. [7] have studied the AIM problem under the feedback models which are variants of the Full Adoption feedback model. One common setting in these works is that we always wait until the diffusion terminates before making the next seeding decision, which is critical for the AIM problem to be adaptive submodular. As noted in [5] and [7], when we wait for a fixed number of rounds (e.g., Myopic feedback model) the AIM problem is unfortunately not adaptive submodular anymore, and to the best of our knowledge there is no analysis technique available for such general feedback models. We in this paper make an attempt to fill this gap by providing a metric for quantifying the approximation ratio when the AIM problem is not adaptive submodular.\nThere exist several other research lines concerning the AIM problem from perspectives different from ours in this paper. One research branch focuses on online influence maximization where the seeding process also consists several stages, but the primary consideration therein is to overcome the incomplete knowledge of the social network ( [10], [11], [12], [13]). In another issue, Seeman et al. [14] studied a twostage adaptive seeding process where the neighbors of the first-stage nodes are candidates for the second seeding stage, which is essentially different from the AIM problem considered in our paper. Later in [15], the authors considered the problem of modifying the IC model so that the AIM problem becomes adaptive submodular under the Myopic feedback model. Recently, Han et al. [16] investigated the issue of speeding up an adaptive seeding strategy."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "In this section, we introduce a collection of definitions some of which are extended from that in the work [5] of Golovin and Krause. We intend to spare more space in this section for explaining the definitions in order to make the analysis in Sec. 4 smooth. The references of the notations are given in Table 1.\nDefinition 1 (Convenient Set Notations). For an element v and a set S, we use v and v+S in replace of {v} and {v}\u222aS, respectively."
        },
        {
            "heading": "3.1 IC Model and Adaptive Seeding Process",
            "text": "Definition 2 (IC Model). A social network is represented by a directed graph G = (V,E). For each edge (u, v), we say u is an in-neighbor of v, and v is an out-neighbor of u. An instance of IC model is given by a directed graph\nSymbol Keyword Reference\nG = (V,E) and the probability pe \u2208 (0, 1] on each edge e \u2208 E.3\nDefinition 3 (Round). In one round, each node v activated in the last round attempts to activate each of v\u2019s inactive neighbor u, with the success probability of p(v,u). We assume the observations are made round by round.\nDefinition 4 (Adaptive Seeding Process). An adaptive seeding process alternates between the following two steps: \u2022 (seeding-step) Select and activate a certain set of seed\nnodes. \u2022 (observing-step) Observe the diffusion for a certain\nnumber of rounds."
        },
        {
            "heading": "3.2 Realizations and Status",
            "text": "The definitions in this section are used to describe an intermediate stage during a diffusion process.\nDefinition 5 (States of Edges). Following [1], we speak of each edge (u, v) as being live or dead to indicate that if u can activate v once u becomes active.\nDefinition 6 (Realization). A realization \u03c6 = (L(\u03c6), D(\u03c6)) \u2208 2E \u00d7 2E is an ordered two-tuple where L(\u03c6) \u2286 E,D(\u03c6) \u2286 E, and L(\u03c6) \u2229D(\u03c6) = \u2205, specifying the states of the edges that have been observed. In particular, e \u2208 L(\u03c6) (resp., e \u2208 D(\u03c6)) means e is a live (resp., dead) edge, and, the state of an edge e is unknown in \u03c6 if e /\u2208 L(\u03c6) \u222aD(\u03c6). We use \u03a6 to denote the set of all realizations. For each realization \u03c6, we define Pr[\u03c6] as Pr[\u03c6] := \u220f e\u2208L(\u03c6) pe \u220f e\u2208D(\u03c6)(1 \u2212 pe), which is the probability that \u03c6 can be realized (i.e., sampled).\n3. We assume the probability pe is strictly larger than 0 for otherwise we can remove it from the graph.\nDefinition 7 (t-live-path). For two nodes u, v \u2208 V and a realization \u03c6, a t-live path from u to v in \u03c6 is a path of at most t edges which are all in L(\u03c6). When there is no limit on the length of the path, we use the notation\u221e-live-path. Definition 8 (Sub-realization). For two realizations \u03c61 and \u03c62, we say \u03c61 is a sub-realization of \u03c62 if L(\u03c61) \u2286 L(\u03c62) and D(\u03c61) \u2286 D(\u03c62), and denote it as \u03c61 \u227a \u03c62. If \u03c61 \u227a \u03c62, we also say \u03c62 is a super-realization of \u03c61. Intuitively, \u03c62 is one possible outcome if we continue to observe the states of the edges after observing \u03c61. We use Pr[\u03c62|\u03c61] to denote the probability that \u03c62 can be realized conditioned on \u03c61, and therefore we have\nPr[\u03c62|\u03c61] = \u220f\ne\u2208L(\u03c62)\\L(\u03c61) pe\n\u220f\ne\u2208D(\u03c62)\\D(\u03c61) (1\u2212 pe).\nDefinition 9 (Full and Partial Realization). We say \u03c6 is a full realization if L(\u03c6) \u222a D(\u03c6) = E, which indicates that all the edges have been observed. We use \u03a8 to denote the set of all full realizations. Note that \u03a8 also represents the basic event space of the IC model. We use \u03c6\u2205 = (\u2205, \u2205) to denote the empty realization. A realization is a partial realization if it is not a full realization.\nWe use the following concepts to describe the observations during the seeding process.\nDefinition 10 (Status). A status U is a two-tuple U =( S\u0307(U), \u03c6\u0307(U) ) \u2208 2V \u00d7 \u03a6, where S\u0307(U) \u2286 V is the set of the current active nodes and realization \u03c6\u0307(U) \u2208 \u03a6 shows the state of the edges that have been observed. We use U\u2205 :=(\u2205, \u03c6\u2205) to denote the status where there is no active node and no edge has been observed.\nRemark 1. We use \u03c6 to denote a realization while use \u03c6\u0307() with an over-dot to denote a realization associated with an object. For example, \u03c6\u0307(U) is a realization associated with a status U . The similar rule of the use of over-dot applies to other notations in this paper.\nDefinition 11 (Final Status). We say a status U is final if there is no\u221e-live-path from any node in S\u0307(U) to V \\ S\u0307(U) in any full realization \u03c8 where \u03c6\u0307(U) \u227a \u03c8. Remark 2. A status is not final if and only if it is possible to have more active nodes in the future rounds even if we do not select any new seed node. When the diffusion process terminates, it reaches a final status.\nDefinition 12 (d-round Status). For a status U and d \u2208 Z+, we use U\u0307d(U) to denote the set of the possible statuses after d diffusion rounds following U . In addition, we use U\u0307\u221e(U) to denote the outcomes after the diffusion terminates following U ."
        },
        {
            "heading": "3.3 Seeding Process",
            "text": "The definitions in this section are used to describe an adaptive seeding process.\nDefinition 13 (Policy). A policy \u03c0 : 2V \u00d7 \u03a6 \u2192 2V maps a status U to a node-set S \u2286 V , which indicates that \u03c0 will select S as the seed set if the observed status is U .\nRemark 3. Note that a realization together with the current active nodes determines the rest of the diffusion process. Thus, a policy makes decisions according to the current status rather than the current realization.\nAn adaptive seeding process can be viewed as a decision tree. An illustration of the following definitions is shown in Fig. 1.\nDefinition 14 (Decision tree). A decision tree T of an adaptive seeding process is an arborescence, where each treeedge is associated with a status U which corresponds to an observing-step showing what have been observed, and each tree-node is a node-set S \u2286 V which corresponds to a\nseeding-step showing the seed nodes that are selected and activated.4\nDefinition 15 (Decision Tree Notations). For each treenode S, let U\u0307out(S) be the set of the tree-edges out of S, showing all possible different observations after selecting S.5 Since two edges in a tree cannot have the same realization, by abusing the notation, we also use U to denote the tree-edge of which the status is U . For a tree-edge U , we use S\u0307end(U) \u2286 V to denote the end-node of U . The treenodes can be grouped by levels. For a decision tree T and i \u2208 {1, 2, 3, ...}, we use STi \u2208 22 V\nto denote the set of the tree-nodes in the i-th level. For i \u2208 {2, 3, ...}, we useUTi \u2286 \u03a6 to denote the set of the statuses of the tree-edges from the tree-nodes in STi\u22121 to those in S T i , and define U T 1 as {U\u2205}. In addition, we use ST\u221e to denote the set of the nodes in the lowest level (i.e., the leaves) and use UT\u221e to denote the set of the tree-edges connecting to the leaves.\nRemark 4. When a decision tree represents an adaptive seeding process, level i shows all possible scenarios in the i-th seeding step. For each pair U and S\u0307end(U), it means that S\u0307end(U) is selected by \u03c0 as a seed set when U is observed. The edges out of a tree-node S indicate the possible observations after selecting S. For each sequence of statuses (U1, U2, ...) from the root to a leaf, we have \u03c6\u0307(Ui) \u227a \u03c6\u0307(Ui+1) and S\u0307end(Ui) \u2286 S\u0307end(Ui+1)."
        },
        {
            "heading": "3.4 Process Concatenation",
            "text": "The analysis of the policy requires to measure the effect of the union of two seeding processes. In the work [5] of Golovin and Krause, it was stated as: running one policy to completion and then running another policy as if from a fresh start, ignoring the information gathered during the running of the first policy. To make this concept mathematically tractable, we adopt the decision tree perspective and employ the following definitions.\nDefinition 16 (Realization Compatibility). For two realizations \u03c61, \u03c62 \u2208 \u03a6, we say they are compatible if L(\u03c61) \u2229 D(\u03c62) = \u2205 and D(\u03c61) \u2229 L(\u03c62) = \u2205. That is, there is no conflict observations. We denote this relationship by \u03c61 \u223c \u03c62. Definition 17 (Realization Concatenation). For a set of finite realizations {\u03c61, \u03c62, ..., \u03c6m} where\n( L(\u03c61) \u222a ... \u222a L(\u03c6m) ) \u2229 ( D(\u03c61) \u222a ... \u222aD(\u03c6m) ) = \u2205,\nwe define \u03c61 \u2295 ...\u2295 \u03c6m as a new realization with L(\u03c61 \u2295 ...\u2295 \u03c6m) = L(\u03c61) \u222a ... \u222a L(\u03c6m)\nand D(\u03c61 \u2295 ...\u2295 \u03c6m) = D(\u03c61) \u222a ... \u222aD(\u03c6m).\nDefinition 18 (Status Union). For two status U1 and U2 where \u03c6\u0307(U1) and \u03c6\u0307(U2) are compatible, we define U1 \u222a U2 as a new status with S\u0307(U1 \u222a U2) = S\u0307(U1) \u222a S\u0307(U2) and \u03c6\u0307(U1 \u222a U2) = \u03c6\u0307(U1)\u2295 \u03c6\u0307(U2).\n4. We reserve the term edge for the social network graph and the term tree-edge for the decision tree.\n5. Two observations are different if and only if there is at least one edge with different observed states.\nDefinition 19 (Decision Tree Conditioned on a Status). Given a decision tree T and a status U , we construct another decision tree by modifying T as follows. For each tree-edge U\u2217 in T such that \u03c6\u0307(U\u2217) is not compatible with \u03c6\u0307(U), we remove the tree-edge U\u2217 as well as the tree-node S\u0307end(U\u2217). For each tree-edge U\u2217 in T such that \u03c6\u0307(U\u2217) is compatible with \u03c6\u0307(U), we replace the status U\u2217 by U\u2217 \u222a U . We denote the resulted decision tree as T |U named as the decision tree of T conditioned on status U . An illustrative example is given in Fig. 2.\nRemark 5. One can see that if we remove one tree-edge U , we must also remove all of its following tree-edges due to Remark 4. In the tree T |U , each realization is a superrealization of \u03c6\u0307(U). Note that we only modify the status but do not change the tree-node. When T is an adaptive seeding process of a certain policy, the tree T |U shows the adaptive seeding process when the states of the edges in L(\u03c6\u0307(U)) \u222aD(\u03c6\u0307(U)) have been fixed and the nodes in S\u0307(U) are activated, but the policy does not have such information.\nDefinition 20 (Decision Tree Concatenation). Given two decision trees T1 and T2, we construct another decision tree by modifying T1, as follows. For each tree-edge U in T1 where S\u0307end(U) is a leaf, we replace S\u0307end(U) by the tree T2|U . We denote the new tree as T1 \u2295 T2. An example is shown in Fig. 3.\nRemark 6. In our context, a decision tree can be given by an adaptive seeding process, or its modification according to Defs. 19 and 20. The notations in Def. 15 also apply to a concatenation of two decision trees. For two seeding process, the concatenation of their decision trees provides a tractable representation of the process that first running the first process and then running the second one without considering the obtained observations.\nA decision tree naturally has the following property.\nProperty 1. For each tree-edge U1 where S\u0307end(U1) is not a leaf, we have\n\u2211\nU2\u2208U\u0307out(S\u0307end(U1)) Pr[\u03c6\u0307(U2)|\u03c6\u0307(U1)] = 1. (1)\nThis is because (a) Pr[\u03c6\u0307(U2)|\u03c6\u0307(U1)] is the probability that \u03c6\u0307(U2) happens conditioned on U1 and (b) U\u0307out(S\u0307end(U1)) is the set of all possible different observations after observing U1 and selecting S\u0307end(U1) as the seed set. Note that this is also true for a tree which a concatenation of other two trees."
        },
        {
            "heading": "3.5 Counting Active Nodes",
            "text": "Definition 21 (At(S, \u03c8)). For each S \u2208 2V and \u03c8 \u2208 \u03a8, we use At(S, \u03c8) to denote the number of the active nodes in \u03c8 after t diffusion rounds when S is set of the current active nodes. We use t =\u221e for the case when the diffusion terminates. For each S \u2286 V , V \u2217 \u2286 V , \u03c6 \u2208 \u03a6 and \u03c8 \u2208 \u03a8, we define that\n\u2206t(S, V \u2217, \u03c8) := |At(S \u222a V \u2217, \u03c8)| \u2212 |At(S, \u03c8)|, (2)\nas the marginal increase resulted by V \u2217, and we use\n\u2206ft(S, V \u2217, \u03c6) =\n\u2211\n\u03c6\u227a\u03c8,\u03c8\u2208\u03a8 Pr[\u03c8|\u03c6] \u00b7\u2206t(S, V \u2217, \u03c8) (3)\nto denote the expected marginal profit when the current status is (S, \u03c6).\nThe following lemma is a start point to calculating the influence.\nLemma 1 (Kempe et al. [1]). For a full-realization \u03c8, according to [1], v \u2208 At(S, \u03c8) iff there exists a t-live-path in \u03c8 from a node u \u2208 S to v ."
        },
        {
            "heading": "3.6 Discussions",
            "text": "Before proceeding with the analysis, we briefly discuss the issues regarding formulating the AIM problem. The existing works model the AIM following the seminal work [17] of Golovin and Krause where we select elements from a ground set and the elements may have random states. To utilize this framework, under the Full Adoption model, the random states of a node u are defined as the states of the edges reachable from u, in which way the selections are made among the nodes and meanwhile the randomness is also forced to be associated with the nodes. For other feedback models, we need to modify the definitions of the random states in order to apply the framework in [17]. However, such a method does not naturally fit the AIM problem because in the AIM problem the selections are made among nodes while the randomness comes from the edges which are independent of our selection of the nodes. Therefore, in this paper the randomness is directly defined\non the edges and we model the node selections and edge observations in a separate manner, which brings us higher flexibility in the analysis as shown later in Sec. 4, as well as in defining more general and natural AIM problems as later discussed in Sec. 6 ."
        },
        {
            "heading": "4 AIM PROBLEM AND GREEDY STRATEGY",
            "text": "This section gives the main analysis of this paper. We first formally formulate the considered problem and introduce the concept of regret ratio, and then show how to bound the performance of the greedy strategy. In addition, we discuss how to improve the efficiency with the reverse sampling technique. Finally, we provide an upper bound of the regret ratio."
        },
        {
            "heading": "4.1 Problem Statement",
            "text": "In this paper, we consider the AIM problem formulated as follows.\nProblem 1 ((k, d)-AIM Problem). Given a budget k \u2208 Z+ and an integer d \u2208 Z+, we consider the feedback model where one seed node is selected after every d rounds of diffusion until k seed nodes are selected. We aim to design a policy for selecting nodes under this pattern such that the influence can be maximized.\nRemark 7. When d \u2265 n\u22121, it is equivalent that we wait for the diffusion process to terminate before selecting the next seed node, and in this case, we denote it as the (k,\u221e)-AIM Problem which reduces to the problem studied in [5]. When d = 1, it reduces to the Myopic feedback model studied in [5] and [15].\nRemark 8. For Problem 1, as we always select one node in each seeding step, we restrict our attention to the policy \u03c0 where |\u03c0(S, \u03c6)| = 1 for each S \u2286 V and \u03c6 \u2208 \u03a6.\nThe adaptive seeding process in the (k, d)-AIM problem under a policy \u03c0 is described as follows:\nDefinition 22 ((\u03c0, k, d)-process).\n\u2022 Set (S, \u03c6) as (\u2205, \u03c6\u2205). Repeat the following process for k times. \u2013 (seeding-step) Select and activate the node \u03c0(S, \u03c6). \u2013 (observing-step) Observe the diffusion for d rounds.\nUpdate (S, \u03c6) by setting S as the set of the current active nodes and \u03c6 as the current realization.\n\u2022 Wait for the diffusion to terminate and output the number of active nodes.\nWe define that we can wait for one diffusion round even if there is no node can activate their neighbors, which conceptually allows us to wait for any number of rounds. The output of the above diffusion process is nondeterministic since the diffusion process is stochastic. We use F (\u03c0, k, d) to denote the expected number of the active nodes produced by the (\u03c0, k, d)-process. Problem 1 can be restated as follows.\nProblem 2. Given k \u2208 Z+ and d \u2208 Z+, find a policy \u03c0 such that F (\u03c0, k, d) is maximized.\nDefinition 23 (Greedy Policy \u03c0g). Given a status (S, \u03c6), the greedy policy \u03c0g always select the node that can maximize the marginal gain conditioned on (S, \u03c6), and therefore,\n\u03c0g(S, \u03c6) = arg max v \u2206f\u221e(S, v, \u03c6) (4)\nThe greedy strategy is the most popular strategy due to its excellent performance and simple implementation. It has been adopted by most of the research regarding the AIM problem and has been shown to have an approximation ratio of 1 \u2212 1/e for several special cases where this ratio is tight. In this paper, we are particularly interested in the performance bound of the greedy policy for Problem 1."
        },
        {
            "heading": "4.2 Regret Ratio",
            "text": "As aforementioned, Problem 1 is not adaptive submodular in general, which is essentially caused by not waiting for the diffusion to terminate before making seeding decisions. In this section, we provide a metric dealing with such scenarios.\nSuppose that the current status is U , the budget is one, and we aim at maximizing the number of active nodes after t rounds. Let us consider two ways to deploy this seed node. \u2022 Method 1: We select the seed node immediately\nbased on U . In this case, the best marginal profit we can achieve is maxv \u2206ft ( S\u0307(U), v, \u03c6\u0307(U) ) , by selecting arg maxv \u2206ft ( S\u0307(U), v, \u03c6\u0307(U) ) as the seed node. \u2022 Method 2: We wait for d < t rounds of diffusion and then select the seed node. After d rounds, for each possible status U\u2217 \u2208 U\u0307d(U), the best marginal profit would be max \u2206ft\u2212d ( S\u0307(U\u2217), v, \u03c6\u0307(U\u2217) ) . Thus, the total\nmarginal profit would be \u2211 U\u2217\u2208U\u0307d(U) Pr[\u03c6\u0307(U\u2217)|\u03c6\u0307(U)] \u00b7\nmax \u2206ft\u2212d ( S\u0307(U\u2217), v, \u03c6\u0307(U\u2217) ) .\nDefinition 24 (Regret Ratio). For a status U and two integers t, d \u2208 Z+ with d \u2264 t, we define the regret ratio \u03b1t,d(U) as\n\u03b1t,d(U) :=\u2211 U\u2217\u2208U\u0307d(U) Pr[\u03c6\u0307(U\u2217)|\u03c6\u0307(U)] \u00b7max \u2206ft\u2212d ( S\u0307(U\u2217), v, \u03c6\u0307(U\u2217) )\nmaxv \u2206ft ( S\u0307(U), v, \u03c6\u0307(U) ) ,\nwhich measures the ratio of the marginal profits resulted by those two methods. When there is no time constraint (i.e., t =\u221e), we denote it as\n\u03b1\u221e,d(U) :=\u2211 U\u2217\u2208U\u0307d(U) Pr[\u03c6\u0307(U\u2217)|\u03c6\u0307(U)] \u00b7max \u2206f\u221e ( S\u0307(U\u2217), v, \u03c6\u0307(U\u2217) )\nmaxv \u2206f\u221e ( S\u0307(U), v, \u03c6\u0307(U) ) .\nFurthermore, if we wait until the diffusion terminates before selecting the next seed node (i.e., d =\u221e), we have\n\u03b1\u221e,\u221e(U) :=\u2211 U\u2217\u2208U\u0307\u221e(U) Pr[\u03c6\u0307(U\u2217)|\u03c6\u0307(U)] \u00b7max \u2206f\u221e ( S\u0307(U\u2217), v, \u03c6\u0307(U\u2217) )\nmaxv \u2206f\u221e ( S\u0307(U), v, \u03c6\u0307(U) ) .\nFor a decision tree, we define that\n\u03b1(T ) := max U\u2208\u22c3UTi \u03b1\u221e,\u221e(U)\nwhich is the largest \u03b1\u221e,\u221e(U) among all the statuses in the tree.\nRemark 9. We can see that ifU is a final status, then the ratio \u03b1t,d(U) is always no larger than 1 as no more observation can be made, indicating that the first method is a dominant strategy. Because the root status is always final in each decision tree T , \u03b1(T ) is no less than 1. Furthermore, if U is a final status and t = \u221e, we have \u03b1\u221e,d(U) = 1 for each d.\nRemark 10. When t = \u221e, the ratio \u03b1\u221e,d(U) is always no less than 1, and therefore the second method is a dominant strategy. In this case, \u03b1\u221e,d(U) shows the penalty incurred by not waiting for the diffusion to terminate before determining the seed node. Furthermore, \u03b1\u221e,d(U) will not decrease with the increase of d. That is, the more rounds we wait for, the more profit we can gain. We will use \u03b1\u221e,\u221e to bound the approximation ratio of the greedy policy.\nRemark 11. When t < n \u2212 1 and U is not final, there is a trade-off determined by the number of the rounds d we would wait for. If we waited for more rounds, we would have more observations and had a better chance to explore high-quality seed nodes, but meanwhile we would lose more diffusion rounds."
        },
        {
            "heading": "4.3 Main Result",
            "text": "The main result of our paper is shown below.\nTheorem 1. For each policy \u03c0\u2217,\nF (\u03c0g, k, d) \u2265 (1\u2212 e\u22121/\u03b1(Tg)) \u00b7 F (\u03c0\u2217, k, d), where Tg is the decision tree of the (\u03c0g, k, d)-process\nRemark 12. The (k, d)-AIM problem has been studied for the case d = \u221e. An early discussion was given by Golovin and Krause in [17] where it was confirmed that the greedy policy provides a (1 \u2212 1/e)-approximation to the (k,\u221e)AIM problem based on the concept of adaptive submodularity. According to Remark 9, \u03b1(Tg) = 1 when d =\u221e, because all the status in Tg are now final status, and therefore we have 1\u2212 1/e again for this special case. 6"
        },
        {
            "heading": "4.4 Proof of Theorem 1",
            "text": "This section gives the main line of the proof where the details are deferred to the supplementary material. The main idea is to introduce another seeding process which is (a) equivalent to the seeding process we consider and (b) comparable to the greedy policy via the regret ratio.\nBefore delving into the proof, we first introduce the necessary preliminaries. For the purpose of analysis, let us consider the following L-(\u03c0, k, d)-process.\nDefinition 25 (L-(\u03c0, k, d)-process). Set (S, \u03c6) as (\u2205, \u03c6\u2205). \u2022 Step 1. Repeat the following process for k \u2212 1 times.\n\u2013 (seeding-step) Select and activate the node \u03c0(S, \u03c6). \u2013 (observing-step) Observe the diffusion for d rounds.\nUpdate (S, \u03c6) by setting S as the set of the current active nodes and \u03c6 as the current realization.\n\u2022 Step 2.\n6. In [6], we claimed that the greedy algorithm gives a (1 \u2212 1/e)approximation for Problem 1 when d < n \u2212 1. We now retract that claim.\nlevel 1\nd rounds\n\u2013 (seeding-step) Decide the seed node v\u2217 = \u03c0(S, \u03c6), but do not activate v\u2217. This is the k-th seeding step.\n\u2013 (observing-step) Wait for the diffusion to terminate. \u2022 Step 3.\n\u2013 (seeding-step) Activate the node v\u2217. This is the (k + 1)-th seeding step.\n\u2022 Wait for the diffusion to terminate and output the number of active nodes.\nThe L-(\u03c0, k, d)-process is identical to the (\u03c0, k, d)-process except that the last seed node is activated with a delay. However, the total profit remains the same. Let FL(\u03c0, k, d) be the expected number of the active nodes output by the L-(\u03c0, k, d)-process. We have the following result.\nLemma 2. F (\u03c0, k, d) = FL(\u03c0, k, d).\nProof. For a fixed full realization, the (\u03c0, k, d)-process and L-(\u03c0, k, d)-process always select the same seed nodes and the only difference is that the last seed in the L-(\u03c0, k, d)process may be seeded with a delay. However, with respect to the number of active nodes, it does not matter that when we make the seed node activated, as long as we allow the diffusion process to finally terminate. The idea of lazy seeding was also seen early in [18].\nDefinition 26 (L-(\u03c0, k, d)-tree). We denote the decision tree of the L-(\u03c0, k, d)-process as the L-(\u03c0, k, d)-tree. In an L(\u03c0, k, d)-process, there are totally k observing-steps and k+1 seeding-steps. Note that in the k-th seeding step no node is activated and therefore we label the tree-node by a special character . An illustration is given in Fig. 4.\nRemark 13. Different from the (\u03c0, k, d)-process, the leaves in a L-(\u03c0, k, d)-tree are now final status, which is the key to establishing the performance bound based on the regret ratio.\nDefinition 27 (Decision Tree Profit). For a decision tree T , we define that\nF (T ) := \u2211\nU\u2208UT\u221e\nPr[\u03c6\u0307(U)] \u2211\n\u03c8\u2208\u03a8 \u03c6\u0307(U)\u227a\u03c8\nPr[\u03c8|\u03c6\u0307(U)] \u00b7 |A\u221e(S\u0307(U) + S\u0307end(U), \u03c8)|\n= \u2211\nU\u2208UT\u221e\n\u2211\n\u03c8\u2208\u03a8 \u03c6\u0307(U)\u227a\u03c8\nPr[\u03c8] \u00b7 |A\u221e(S\u0307(U) + S\u0307end(U), \u03c8)| (5)\nas the profit of the decision tree7, which is the immediate sum of the profit among all possible outcomes.\nRemark 14. As one can see, if T is the decision tree of a certain adaptive seeding process, then F (T ) is the expected number of active users resulted by the process. Thus, we have F (\u03c0, k, d) = FL(\u03c0, k, d) = F (T ) where T is the L(\u03c0, k, d)-tree.\nNote that F (T1\u2295T2) is also well-defined for a concatenation of two trees, and we have the following result showing that a concatenation will not decrease the total profit.\nLemma 3 ( [5]). F (T1 \u2295 T2) \u2265 F (T2)\nProof. One can obtain a proof using the view of policy similar to that given in [5]. We provide a proof from the view of decision tree in the supplementary material.\nRemark 15. According to Remark 14, Eq. (5) gives an explicit formula of FL(\u03c0, k, d). However, this formula is only used for analysis and it is not feasible to compute Eq. (5) directly as there are exponential number of terms to sum. If d =\u221e, L-(\u03c0, k, d)-tree is the same as (\u03c0, k, d)-tree.\nNow we are ready to prove Theorem 1. By Lemma 2, it is sufficient to analyze the L-(\u03c0g, k, d)-process. In the rest of this section, we assume \u03c0\u2217, d and k are fixed, where \u03c0\u2217 can be an arbitrary policy. For each i \u2208 {1, ..., k}, we use T ig to denote the decision tree of the L-(\u03c0g, i, d)-process, and use T i\u2217 to denote the decision tree of the (\u03c0\u2217, i, d)-process. By Remark 14, we have F (T kg ) = F (\u03c0g, k, d) and F (T k \u2217 ) = F (\u03c0\u2217, k, d), and therefore to prove Theorem 1, it suffices to show that\nF (T kg ) \u2265 (1\u2212 e\u22121/\u03b1(Tg)) \u00b7 F (T k\u2217 ), (6)\nwhere \u03b1(Tg) is defined in Def. 24. For two integers i, j \u2208 {1, ..., k}, let us consider the decision tree T ig \u2295 T j\u2217 . We define that T 0g \u2295T j\u2217 :=T j\u2217 and T ig\u2295T 0\u2217 :=T ig . For conciseness, we denote T ig \u2295 T j\u2217 by Ti,j . Furthermore, we define that F (T0,0) := 0 but do not define the tree T0,0.8 By Lemma 3, we have\nF (Ti,k) \u2265 F (T0,k). (7)\nThe following is a key lemma for proving Theorem 1.\nLemma 4. For each i, l \u2208 {1, ..., k}, we have\nF (Ti\u22121,l)\u2212 F (Ti\u22121,l\u22121) \u2264 \u03b1(Tg) \u00b7 ( F (T0,i)\u2212 F (T0,i\u22121) ) .\nProof. The proof is based on subtle arrangements of the equations given by the decision tree so that we can analyze the marginal profit in a fine-grained manner. This is the main theoretical result of this paper, but for the sake of continuity we put this part in the supplementary material.\nWith Lemma 4, the rest of the proof follows the standard analysis of submodular maximization. Summing the inequality in Lemma 4 over l \u2208 {1, ..., k}, we have F (Ti\u22121,k)\u2212 F (Ti\u22121,0) \u2264 k \u00b7 \u03b1(Tg) \u00b7 ( F (T0,i)\u2212 F (T0,i\u22121) ) ,\n7. We slightly abuse the notation F by allowing it to have different definitions for different types of inputs.\n8. One can imagine T0,0 as an empty tree.\nand, due to Lemma 3 we have\nF (T0,k)\u2212 F (Ti\u22121,0) \u2264 k \u00b7 \u03b1(Tg) \u00b7 ( F (T0,i)\u2212 F (T0,i\u22121) ) .\nDefine that4i :=F (T0,k)\u2212F (Ti\u22121,0), and we therefore have 4i \u2264 k \u00b7\u03b1(Tg)\u00b7(4i\u22124i+1), implying4i+1 \u2264 (1\u2212 1\u03b1(Tg)\u00b7k )\u00b7 4i and consequently\n4k+1 \u2264 (1\u2212 1\n\u03b1(Tg) \u00b7 k )k \u00b7 41 \u2264 exp (\u2212\n1\n\u03b1(Tg) ) \u00b7 41.\nAs a result, we have F (T0,k) \u2212 F (Tk,0) \u2264 exp (\u2212 1\u03b1(Tg)\u00b7 ) \u00b7( F (T0,k) \u2212 F (T0,0) ) , and therefore, F (T kg ) = F (Tk,0) \u2265 (1 \u2212 e\u22121/\u03b1(Tg)) \u00b7 F (T0,k) = (1 \u2212 e\u22121/\u03b1(Tg)) \u00b7 F (T k\u2217 ), which completes the proof."
        },
        {
            "heading": "4.5 Generalized RR-set",
            "text": "In this section, we discuss one technique that can be used to improve the efficiency of the algorithms concerning the AIM problem.\nIn each seeding step, the greedy policy selects the node with the highest marginal profit conditioned on the current status, and this process demands to calculate \u2206f\u221e(S, v, \u03c6) which is a #P-hard problem [8]. One straightforward method is to utilize Monte Carlo simulation which is unfortunately not efficient as widely discussed. Alternatively, with the idea of reverse sampling [19], an efficient estimating approach is obtainable, and this technique has been extensively studied for the IM problem or its variants (e.g., [9], [16], [20], [21], [22], [23]). In particular, The previous work has shown that how to use this technique for the case when S = \u2205 and \u03c6 = \u03c6\u2205, by utilizing the so-called RR-set. In what follows, we will show that an analogous approach can be obtained to estimate \u2206ft(S, V \u2217, \u03c6) given in Eq. (3) for the general case. With the concept inherited from [20], we redefine the RR-set as follows.\nDefinition 28 (RR-set). Given S \u2286 V , \u03c6 \u2208 \u03a6 and t \u2208 Z+, an RR-set R \u2286 V is a set of nodes generated randomly as follows. \u2022 Step 1. For an edge e not in L(\u03c6)\u222aD(\u03c6), we sample its\nstate according to the probability pe. For an edge e in L(\u03c6) \u222aD(\u03c6) we keep its state as given by \u03c6. After this step, we obtain a full realization \u03c8\u2217. \u2022 Step 2. Select a node v which is not in S uniformly at random, and let Rv \u2286 V be the set of the nodes from which v is reachable via a t-live-path in \u03c8\u2217. If Rv\u2229S = \u2205, we returnR = Rv as the RR-set. Otherwise, we return R = \u2205.\nFollowing the standard analysis of reverse sampling [20], the following result can be readily derived.\nLemma 5. For each V \u2217 \u2286 V , we have (|V | \u2212 |S|) \u00b7 E[I(V \u2217 \u2229R)] = \u2206ft(S, V \u2217, \u03c6), (8)\nwhere\nI(V \u2217 \u2229R) := {\n1 if S \u2229 V \u2217 6= \u2205 0 else\nand R is a RR-set generated with the input S, \u03c6 and t. Proof. When a node v is selected in Step 2, E[I(V \u2217 \u2229 Rv)] is the marginal increase of the probability that v can be\nactivated. There are two cases depending on whether or not Rv \u2229 S is empty. If Rv \u2229 S 6= \u2205, it indicates that v can be activated by the current active nodes and therefore it cannot contribute to \u2206ft(S, V \u2217, \u03c6) for any V \u2217. Accordingly, Rv is set as \u2205 so that I(V \u2217 \u2229 Rv) = 0 for any V \u2217. If Rv \u2229 S = \u2205, Rv consists of the nodes that activate v within t rounds, and thus, E[I(V \u2217 \u2229Rv)] shows the marginal contribution of V \u2217 on node v. Since \u2206ft(S, V \u2217, \u03c6) is sum of the marginal increase of the activation probability over all the nodes not in S, calculating the mean of I(V \u2217 \u2229 R) immediately yields Eq. (8).\nRemark 16. Note that there is no need to determine the state of each edge in advance, and instead one can collect the reachable nodes Rv from v in step 2 in a reverse direction. Such an idea was invented by Borgs et al. [19].\nAccording to Lemma 5, one can estimate \u2206ft(S, V \u2217, \u03c6) by utilizing the sample mean of I(V \u2217 \u2229 R), and the estimation can be arbitrarily accurate provided sufficient samples are used. With the generalized RR-set, the greedy policy can be implemented in the similar manner as that in [20]. Precisely, the policy with such implementation is in fact a (1 \u2212 e\u22121/\u03b1(Tg) \u2212 )-approximation where is the error incurred by sampling and it can be arbitrarily small. We do not provide detailed analysis for the relationship between and the number of used RR-sets, as it is out of the scope of this paper, and the interested reader is referred to [9] and [16]."
        },
        {
            "heading": "4.6 An Upper Bound of \u03b1t,d(U)",
            "text": "Calculating the regret ratio \u03b1t,d(U) is computationally hard due to the #P-hardness in computing the function value of \u2206ft(S, V\n\u2217, \u03c6) and furthermore to the fact that it requires to consider an exponential number of statuses after d diffusion rounds. In this section, we provide an upper bound of \u03b1t,d(U). Throughout this part, we assume the function value can be obtained with an arbitrary high accuracy, and therefore the denominator maxv \u2206ft(S\u0307(U), v, \u03c6\u0307(U)) of \u03b1t,d(U) is obtainable. Let us denote the numerator as\nN(U) := \u2211 U\u2217\u2208U\u0307d(U) Pr[\u03c6\u0307(U\u2217)|\u03c6\u0307(U)] \u00b7max \u2206ft\u2212d(S\u0307(U\u2217), v, \u03c6\u0307(U\u2217)).\nIn addition, let us define a new status Ufinal where S\u0307(Ufinal) = S\u0307(U), L ( \u03c6\u0307(Ufinal) ) :=L(\u03c6\u0307(U)) and\nD(\u03c6\u0307(Ufinal)) :=\nD(\u03c6\u0307(U) \u222a {(u, v) : (u, v) /\u2208 L(\u03c6\u0307(U)) \u222aD(\u03c6\u0307(U)), u \u2208 S\u0307(U)}.\nNote that Ufinal is a super-realization of U , and Ufinal sets each undetermined edge out of an active node in S\u0307(U) as a dead edge. Informally, it is the worst outcome in U\u0307d(U) with respect to the number of total active nodes. The following lemma gives an upper bound of the numerator.\nLemma 6. For each status U , we have N(Ufinal) \u2265 N(U). Proof. See supplementary material.\nRemark 17. One can see that N(Ufinal) can be easily computed as it is a final status. We are particularly interested in the upper bound of \u03b1t,d(U) due to the interests in the lower bound of the approximation ratio in Theorem 1."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "The primary goal of our experiments is to evaluate the performance of the greedy algorithm with respect to Problem 1 by (a) comparing it with other baseline algorithms and (b) examining the effect of the feedback model on the influence pattern."
        },
        {
            "heading": "5.1 Experimental Settings.",
            "text": "Dataset. We employ seven datasets, from small to large, denoted as Power, Wiki, Higgs, Hepth, Hepph, DBLP and Youtube, where Power is a power-law graph generated by the LFT benchmark [24], Higgs is a Twitter dataset, and the rest of the datasets are borrowed from SNAP [25]. The details of the datasets are provided in the supplementary material. Due to space limitation, in the main paper we only show and discuss the results on Higgs, Hepph and DBLP, and the complete experimental results can be found in the supplementary material. The source code is maintained online [26].\nPropagation Probability. Higgs consists of a collection of activities between users, including re-tweeting action, replying action, and mentioning action. We follow the setting in [27] so that the propagation probability between users is proportional to the frequency of the actions between them. For the other datasets, we adopt either the uniform setting with pe = 0.1 or the well-known weighted cascade setting where p(u,v) = 1/deg(v) where deg(v) is the in-degree of v. The setting of the propagation probability primarily affects the scale of the influence, and we have similar observations under different settings.\nProblem Setting. We set the budget k as either 5 or 50, and select the parameter d from {0, 1, 2, 4, 8,\u221e} where d = 0 and d = \u221e denote the non-adaptive case and the Full Adoption feedback model, respectively. Because the diffusion process typically terminates within 8 rounds without new seed nodes, it is identical to the Full Adoption feedback model when d > 8. Thus, we do not test the case for d > 8.\nPolicies. Besides the greedy policy, we implemented two baseline algorithms, HighDegree and Random. HighDegree adaptively selects the node with the highest degree as the seed node, and Random selects the seed nodes randomly.\nSimulation Setting. Whenever the reverse sampling method in Sec. 4.5 is called, the number of used RR-sets is at least 100,000 which is sufficient for an accurate single node selection, as shown in [28], [29], [30]. For each set of dataset and policy, 500 simulations were performed and we report the average result."
        },
        {
            "heading": "5.2 Experiment I",
            "text": "In the first experiment we compare the seeding processes under the different policies. The result of this part on Higgs, Hepph and DBLP are shown in Figs. 5, 6 and 7, respectively. We have the following observations.\nEven though the greedy policy does not have a constant approximation ratio under general feedback models, it still dominates HighDegree which is an effective heuristic method. The superiority of Greedy becomes more significant with the increase in the network size. As we can see, on\nsmall graphs such as Higgs, HighDegree performs slightly worse than Greedy does, and Random even produces comparable results. However, on large graphs (e.g., Hepph and DBLP) Greedy outperforms the baseline methods by a significant gap. For example, as shown in Fig. 6, Random and HighDegree can hardly bring 2,000 active nodes on Hepph with k = 50, whereas Greedy achieves at least 4,000 active nodes under all cases.\nAnother observation is that HighDegree is occasionally better than Greedy if we restrict our attention to the first\nseveral rounds. For example on Higgs with k = 5, Figs. 5a, 5b and 5c, the influence resulted by HighDegree is higher than that of Greedy in the first two or three diffusion rounds. This is intuitive as HighDegree is a heuristic targeted on the influence right after the next round, and it also suggests that HighDegree is effective for maximizing the influence within few rounds."
        },
        {
            "heading": "5.3 Experiment II",
            "text": "In the second experiment, we compare the diffusion pattern under different feedback models. The diffusion pattern herein is characterized by the increase in the number of active nodes after each diffusion round. The result of this part is shown in Fig. 8.\nNon-adaptive vs Adaptive. The main question we are\ninterested in is how much the adaptivity can help in resulting in a higher influence. The first we can observe is that the adaptive setting it is not always significantly better the nonadaptive one, especially when the budget is small, which can be seen by comparing the cases with k = 5 and k = 50 on the same graph. For example, as shown in Fig. 8a, on Higgs with k = 5 the final influence is the same regardless of\nthe feedback models. Recall that one of the main advantages of an adaptive policy is that better selections can be made if the nodes that were optimal have been activated. Because the optimal seed nodes are relatively sparse when k is small, they are likely to remain inactive after other seed nodes are selected, resulting in that the adaptivity cannot provide better options for node selection and thus the final influence remains the same.\nComparing Adaptive Patterns. According to Fig. 8, the Full Adoption feedback model always produces the best result, coinciding with that it is theoretically optimal, but the difference between the patterns with different d is not significant. One plausible reason is that the diffusion process terminates very fast due to the setting of the propagation probability in our experiments. In such cases, the seeding decisions are always made when the diffusion process terminates, and thus a fixed d is equivalent to the Full Adoption feedback model. Such an observation suggests that d = 1 can be the best choice if one also considers a time constraint. As we can see, namely in Figs. 8d, 8e and 8f, different adaptive patterns have almost the same final influence but it reaches its maximum much faster when d = 1. Therefore, even though the Full Adoption feedback model is optimal, adopting a fixed small d is practically sufficient for certain datasets."
        },
        {
            "heading": "6 FUTURE WORK",
            "text": "In this section, we present the future work.\nFeedback Model with Further Generalizations. From the Full Adoption feedback model in [3] to the (k, d)feedback model proposed in this paper, we see that the manner in which we observe the diffusion results is generalized further and further. In a more general case, we can consider any feedback function which maps a status U to a subset E\u2217 \u2286 E of edge, indicating that the state of the edges in E\u2217 will be observed from status U . For example, under the Myopic feedback model,E\u2217 is the set of out-edges of the newly activated nodes. Furthermore, the observable edges can be nondeterministic. For example, under the Full Adoption feedback model, the edge we can observe in each step depends on the diffusion process which is stochastic. Therefore, the most general feedback model maps a status U to a distribution over the super-realizations of \u03c6\u0307(U). This general setting enables us to make observations independent of the diffusion process and it admits extra flexibility for modeling complex real applications. Several examples are shown below.\nExample 1 (Limited Observation). Considering the large scale of the social network, one often has a limit in observing the diffusion results. For example, we are given a subset E\u2217 of edges which are the only the edges we can observe. Combining the Full Adoption feedback model, in each observing step only the edges in E\u2217 reachable from the active nodes can be observed. Recall that the key issue in the AIM problem is to utilize the observations to make the next seeding decision. Due to the limited observation, there is an interesting trade-off between the quality of the observations and the quality of the seed nodes. On the one hand, we prefer the seed node which can result in more observations in E\u2217 but such a node may not necessarily be the influential\nnode. Conversely, the node with a high influence may be distant from E\u2217 and therefore brings no observation.\nExample 2 (Flexible Observation). Suppose that we aim at select k \u2208 Z+ seed nodes for a certain cascade in an IC social network. Due to privacy issues, the states of the edges can only be observed by probing, and we can probe at most p \u2208 Z+ edges. Under such a setting, we have a chance to decide the edges to observe, and an adaptive seeding process consists of probing-step and seeding-step. The problem asks for the co-design of the seeding strategy and the probing strategy such that the total influence can be maximized, which is another interesting future work.\nExample 3 (Observations Beyond Round-by-round). One typical setting in the AIM problem is that we always make observations round by round. However, this is not realistic in practice because one can hardly synchronize the diffusion process by round in real social networks namely Facebook and Twitter because the activations may take different periods. Instead, the seeding action can be made immediately once an important event has been observed. For example, in online advertising with several target users, a company would prefer to start advertising another one if the current target has been influenced. In another issue, for time-sensitive tasks, we can immediately deploy another seed node once we have a sufficient number of new active users. In such cases, the observations are not necessarily or even not allowed to be made round by round. It is promising to investigate how to model the AIM problem in such cases and design seeding policies accordingly.\nBatch Mode. The policy considered in this paper selects one node each time while it is possible to generalize it to a batch mode where more than one nodes are selected. Under such setting, the node selection in each seeding-step becomes an NP-hard problem in general but one can obtain a (1 \u2212 1/e)-approximation. For such cases, one can define the regret ratio for the batch model and the techniques in [8] and [9] are potentially applicable."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "In this paper, we study the AIM problem under the general feedback models applying to many real-world applications. We show that the performance of the greedy policy under the considered feedback models can be bounded by the regret ratio which quantifying the trade-off between waiting and seeding for the general case. The proposed analysis is the first applies to the AIM when it is not adaptive submodular. We design experiments to examine the performance of the greedy policy under general feedback models, as well as the effect of the feedback model on the final influence. In particular, the conducted experiments show that the adaptive settings are supreme in most cases. Finally, we discuss the future work."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "The authors would like to thank Wei Chen for his comments on the AIM problem."
        },
        {
            "heading": "ON ADAPTIVE INFLUENCE MAXIMIZATION UNDER GENERAL FEEDBACK MODELS (SUPPLEMENTARY MATERIAL)",
            "text": ""
        },
        {
            "heading": "8 MISSING PROOFS",
            "text": ""
        },
        {
            "heading": "8.1 Proof of Lemma 3",
            "text": "F (T1 \u2295 T2) {By Eq. (5)} = \u2211\nU\u2208UT1\u2295T2\u221e\n\u2211\n\u03c8\u2208\u03a8 \u03c6\u0307(U)\u227a\u03c8\nPr[\u03c8] \u00b7 |A\u221e(S\u0307(U) + S\u0307end(U), \u03c8)|\n{By Def. 20} = \u2211\nU1\u2208U T1\u221e\n\u2211\nU2\u2208U T2\u221e\n\u03c6\u0307(U1)\u223c\u03c6\u0307(U2)\n\u2211\n\u03c8\u2208\u03a8 \u03c6\u0307(U1\u222aU2)\u227a\u03c8\nPr[\u03c8] \u00b7 |A\u221e(S\u0307(U1 \u222a U2) + S\u0307end(U2), \u03c8)| {Step 3} = \u2211\nU2\u2208U T2\u221e\n\u2211\nU1\u2208U T1\u221e\n\u03c6\u0307(U1)\u223c\u03c6\u0307(U2)\n\u2211\n\u03c8\u2208\u03a8 \u03c6\u0307(U1\u222aU2)\u227a\u03c8\nPr[\u03c8] \u00b7 |A\u221e(S\u0307(U1 \u222a U2) + S\u0307end(U2), \u03c8)| {Step 4} \u2265 \u2211\nU2\u2208U T2\u221e\n\u2211\nU1\u2208U T1\u221e\n\u03c6\u0307(U1)\u223c\u03c6\u0307(U2)\n\u2211\n\u03c8\u2208\u03a8 \u03c6\u0307(U1\u222aU2)\u227a\u03c8\nPr[\u03c8] \u00b7 |A\u221e(S\u0307(U2) + S\u0307end(U2), \u03c8)| {Step 5} = \u2211\nU2\u2208U T2\u221e\n\u2211\n\u03c8\u2208\u03a8 \u03c6\u0307(U2)\u227a\u03c8\nPr[\u03c8] \u00b7 |A\u221e(S\u0307(U2) + S\u0307end(U2), \u03c8)| {By Eq. (5)} = F (T2)\nThe third step follows from the fact that the collection of {\u03c8 \u2208 \u03a8 : \u03c6\u0307(U1 \u222a U2) \u227a \u03c8} among all possible pairs U1 \u2208 UT1\u221e and U2 \u2208 UT2\u221e such that \u03c6\u0307(U1) \u223c \u03c6\u0307(U2) forms a partition of \u03a8. The fourth step follows from the fact that A\u221e(S, \u03c8) is monotone with respect to S. The fifth step follows from the fact that for each U2 \u2208 UT2\u221e we have\n\u22c3\nU1\u2208U T1\u221e\n\u03c6\u0307(U1)\u223c\u03c6\u0307(U2)\n{\u03c8 \u2208 \u03a8 : \u03c6\u0307(U1 \u222a U2) \u227a \u03c8} = {\u03c8 \u2208 \u03a8 : \u03c6\u0307(U2) \u227a \u03c8}."
        },
        {
            "heading": "8.2 Proof of Lemma 4",
            "text": "There will be two parts of analysis with long equations which are shown in Figs. 9 and 10. By Def. 27, we have\nF (T0,i\u22121) = \u2211\nU\u2208U Tig i\n\u2211\n\u03c6\u0307(U)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8] \u00b7 |A\u221e(S\u0307(U), \u03c8)|\nand\nF (T0,i) = \u2211\nU\u2208U Tig i\n\u2211\n\u03c6\u0307(U)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8] \u00b7 |A\u221e(S\u0307(U) + S\u0307end(U), \u03c8)|.\nTherefore, F (T0,i)\u2212 F (T0,i\u22121) is equal to \u2211\nU\u2208U Tig i\n\u2211\n\u03c6\u0307(U)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8] \u00b7\u2206\u221e(S\u0307(U), S\u0307(U), \u03c8)\n= \u2211\nU\u2208U Tig i\nPr[\u03c6\u0307(U)] \u2211\n\u03c6\u0307(U)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8|\u03c6\u0307(U)] \u00b7\u2206\u221e(S\u0307(U), S\u0307end(U), \u03c8)\n{By the greedy policy} = \u2211\nU\u2208U Tig i\nPr[\u03c6\u0307(U)] \u00b7max v\n( \u2211\n\u03c6\u0307(U)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8|\u03c6\u0307(U)] \u00b7\u2206\u221e(S\u0307(U), v, \u03c8) )\n(9)\nSimilarly, F (Ti\u22121,l)\u2212 F (Ti\u22121,l\u22121) is equal to \u2211\nU\u2208UTi\u22121,li\u22121+l\n\u2211\n\u03c6\u0307(U)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8] \u00b7\u2206\u221e(S\u0307(U), S\u0307end(U), \u03c8)\nGrouping the realizations in UTi\u22121,li\u22121+l according to that if they\nare the super-realizations of that in U T ig i , we have\n\u2211\nU\u2208U Ti\u22121,l i\u22121+l\n\u2211\n\u03c6\u0307(U)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8] \u00b7\u2206\u221e(S\u0307(U), S\u0307end(U), \u03c8) (10)\n= \u2211\nU1\u2208U Tig i\n\u2211\nU2\u2208U Ti\u22121,l i\u22121+l\n\u2211\n\u03c6\u0307(U2)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8] \u00b7\u2206\u221e(S\u0307(U2), S\u0307end(U2), \u03c8)\nNow to prove Lemma 4, it suffices to show that\nEq.(10) \u2264 \u03b1(Tg) \u00b7 Eq.(9).\nTo this end, let us further group the super-realizations of U \u2208 UT i g\ni by that if they are the super-realizations of that in\nU \u2208 UT i g\ni+1, and Eq. (10) can be further represented as shown in Fig. 9. Finally, we have the following lemma as the last ingredient.\nLemma 7. For each U2 \u2208 U T ig i+1, and U3 \u2208 U Ti\u22121,l i\u22121+l such that \u03c6\u0307(U2) \u227a \u03c6\u0307(U3), consider two functions over V ,\ng1(v) := \u2211\n\u03c6\u0307(U2)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8|\u03c6\u0307(U2)] \u00b7\u2206\u221e(S\u0307(U2), v, \u03c8)\nand\ng2(v) := \u2211\n\u03c6\u0307(U3)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8|\u03c6\u0307(U3)] \u00b7\u2206\u221e(S\u0307(U3), v, \u03c8).\nWe have g1(v) \u2265 g2(v).\nProof. See Sec. 8.3.\nSupposing Lemma 7 is true, we have the result in Fig .10, which completes the proof of Lemma 4."
        },
        {
            "heading": "8.3 Proof of Lemma 7",
            "text": "We prove the following general lemma of which Lemma 7 is a special case.\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. ?, NO. ?, ? 2019 15\nand, F (T0,i) = \u2211 U\u2208UT i g i \u2211 \u03c6(U)\u227a\u03c8 \u03c8\u2208\u03a8 Pr[\u03c8] \u00b7 |A\u221e(S\u0307(U) + S\u0307end(U), \u03c8)|. (17) Therefore, F (T0,i)\u2212 F (T0,i\u22121) is equal to\u2211 U\u2208UT i g i \u2211 \u03c6(U)\u227a\u03c8 \u03c8\u2208\u03a8 Pr[\u03c8] \u00b7\u2206\u221e(S\u0307(U), S\u0307(U), \u03c8) = \u2211 U\u2208UT i g i Pr[\u03c6\u0307(U)] \u2211 end \u03c6(U)\u227a\u03c8 \u03c8\u2208\u03a8 Pr[\u03c8|\u03c6\u0307(U)] \u00b7\u2206\u221e(S\u0307(U), S\u0307end(U), \u03c8) {By the greedy policy} = \u2211 U\u2208UT i g i Pr[\u03c6\u0307(U)] \u00b7max v ( \u2211 \u03c6(U)\u227a\u03c8 \u03c8\u2208\u03a8 Pr[\u03c8|\u03c6\u0307(U)] \u00b7\u2206\u221e(S\u0307(U), S\u0307end(U), \u03c8) ) (18) Similarly, F (Ti\u22121,l)\u2212 F (Ti\u22121,l\u22121) is equal to\u2211 U\u2208UTi\u22121,li\u22121+l \u2211 \u03c6(U)\u227a\u03c8 \u03c8\u2208\u03a8 Pr[\u03c8] \u00b7\u2206\u221e(S\u0307(U), S\u0307end(U), \u03c8) (19) Grouping the realizations in UTi\u22121,li\u22121+l by that if they are the super-realizations of that in U1 \u2208 U T ig i , we have \u2211 U\u2208UTi\u22121,l i\u22121+l \u2211 \u03c6(U)\u227a\u03c8 \u03c8\u2208\u03a8 Pr[\u03c8] \u00b7\u2206\u221e(S\u0307(U), S\u0307end(U), \u03c8) = \u2211 U1\u2208U Tig i \u2211 U2\u2208U Tig i+1 \u03c6\u0307(U1)\u227a\u03c6\u0307(U2) \u2211 \u03c6(U2)\u227a\u03c8 \u03c8\u2208\u03a8 Pr[\u03c8] \u00b7\u2206\u221e(S\u0307(U2), S\u0307end(U2), \u03c8) Now we further group the super-realizations of U \u2208 UT i g\ni by that if they are the super-realizations of\nthat in U \u2208 UT i g\ni+1, obtaining that\nF (Ti\u22121,l)\u2212 F (Ti\u22121,l\u22121) = \u2211\nU1\u2208U Tig i\n\u2211\nU2\u2208U Tig i+1\n\u03c6\u0307(U1)\u227a\u03c6\u0307(U2)\n\u2211\n\u03c6\u0307(U2)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8] \u00b7\u2206\u221e(S\u0307(U2), S\u0307end(U2), \u03c8)\n= \u2211\nU1\u2208U Tig i\n\u2211\nU2\u2208U Tig i+1\n\u03c6\u0307(U1)\u227a\u03c6\u0307(U2)\n\u2211\nU3\u2208U Ti\u22121,l i\u22121+l \u03c6\u0307(U2)\u227a\u03c6\u0307(U3)\n\u2211\n\u03c6\u0307(U3)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8] \u00b7\u2206\u221e(S\u0307(U3), S\u0307end(U3), \u03c8)\n= \u2211\nU1\u2208U Tig i\nPr[\u03c6\u0307(U1)] \u2211\nU2\u2208U Tig i+1\n\u03c6\u0307(U1)\u227a\u03c6\u0307(U2)\nPr[\u03c6\u0307(U2)|\u03c6\u0307(U1)] \u2211\nU3\u2208U Ti\u22121,l i\u22121+l \u03c6\u0307(U2)\u227a\u03c6\u0307(U3)\nPr[\u03c6\u0307(U3)|\u03c6\u0307(U2)] \u2211\n\u03c6\u0307(U3)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8|\u03c6\u0307(U3)] \u00b7\u2206\u221e(S\u0307(U3), S\u0307end(U3), \u03c8)\nLet consider the following lemma which will be proved later.\nLemma 6. For each U2 \u2208 U T ig i+1, and U3 \u2208 U Ti\u22121,l i\u22121+l such that \u03c6\u0307(U2) \u227a \u03c6\u0307(U3), consider two functions over V : g1(v) := \u2211\n\u03c6(U2)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8|\u03c6\u0307(U2)] \u00b7\u2206\u221e(S\u0307(U2), v, \u03c8) (20)\n15\nFig. 9: Analysis with Long Equations I.\nand g2(v) := \u2211\n\u03c6(U3)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8|\u03c6\u0307(U3)] \u00b7\u2206\u221e(S\u0307(U3), v, \u03c8). (21)\nWe have g1(v) \u2265 g2(v).\nSupposing Lemma 6 is true, we have,\nF (Ti\u22121,l)\u2212 F (Ti\u22121,l\u22121) {By Fig. 9} = \u2211\nU1\u2208U Tig i\nPr[\u03c6\u0307(U1)] \u2211\nU2\u2208U Tig i+1\n\u03c6\u0307(U1)\u227a\u03c6\u0307(U2)\nPr[\u03c6\u0307(U2)|\u03c6\u0307(U1)] \u2211\nU3\u2208U Ti\u22121,l i\u22121+l \u03c6\u0307(U2)\u227a\u03c6\u0307(U3)\nPr[\u03c6\u0307(U3)|\u03c6\u0307(U2)] \u2211\n\u03c6\u0307(U3)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8|\u03c6\u0307(U3)] \u00b7\u2206\u221e(S\u0307(U3), S\u0307end(U3), \u03c8)\n{By Lemma 7} \u2264 \u2211\nU1\u2208U Tig i\nPr[\u03c6\u0307(U1)] \u2211\nU2\u2208U Tig i+1\n\u03c6\u0307(U1)\u227a\u03c6\u0307(U2)\nPr[\u03c6\u0307(U2)|\u03c6\u0307(U1)] \u2211\nU3\u2208U Ti\u22121,l i\u22121+l \u03c6\u0307(U2)\u227a\u03c6\u0307(U3)\nPr[\u03c6\u0307(U3)|\u03c6\u0307(U2)] \u2211\n\u03c6\u0307(U2)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8|\u03c6\u0307(U2)] \u00b7\u2206\u221e(S\u0307(U2), S\u0307end(U3), \u03c8)\n{Since |S\u0307end(U3)| = 1} \u2264 \u2211\nU1\u2208U Tig i\nPr[\u03c6\u0307(U1)] \u2211\nU2\u2208U Tig i+1\n\u03c6\u0307(U1)\u227a\u03c6\u0307(U2)\nPr[\u03c6\u0307(U2)|\u03c6\u0307(U1)] \u2211\nU3\u2208U Ti\u22121,l i\u22121+l \u03c6\u0307(U2)\u227a\u03c6\u0307(U3)\nPr[\u03c6\u0307(U3)|\u03c6\u0307(U2)] \u00b7 (\nmax v\n\u2211\n\u03c6\u0307(U2)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8|\u03c6\u0307(U2)] \u00b7\u2206\u221e(S\u0307(U2), v, \u03c8) )\n{Since \u2211\nU3\u2208U Ti\u22121,l i\u22121+l \u03c6\u0307(U2)\u227a\u03c6\u0307(U3)\nPr[\u03c6\u0307(U3)|\u03c6\u0307(U2)] = 1}(Note that max v\n\u2211\n\u03c6\u0307(U2)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8|\u03c6\u0307(U2)] \u00b7\u2206(S\u0307(U2), v, \u03c8) is independent of U3)\n= \u2211\nU1\u2208U Tig i\nPr[\u03c6\u0307(U1)] \u2211\nU2\u2208U Tig i+1\n\u03c6\u0307(U1)\u227a\u03c6\u0307(U2)\nPr[\u03c6\u0307(U2)|\u03c6\u0307(U1)] \u00b7 (\nmax v\n\u2211\n\u03c6\u0307(U2)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8|\u03c6\u0307(U2)] \u00b7\u2206\u221e(S\u0307(U2), v, \u03c8) )\n{By the defitnion of \u03b1(Tg)} \u2264 \u2211\nU1\u2208U Tig i\nPr[\u03c6\u0307(U1)] \u00b7 \u03b1(Tg) \u00b7max v\n( \u2211\n\u03c6\u0307(U1)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8|\u03c6\u0307(U1)] \u00b7\u2206\u221e(S\u0307(U1), v, \u03c8) )\n{By Eq. (9)} = \u03b1(Tg) \u00b7 ( F (T0,i)\u2212 F (T0,i\u22121) )\nIt remains to prove Lemma 6.\nA.2 Proving Lemma 6\nLemma. For each U2 \u2208 U T ig i+1, and U3 \u2208 UTi\u22121,li\u22121+l such that \u03c6\u0307(U2) \u227a \u03c6\u0307(U3), consider two functions\ng1(v) := \u2211\n\u03c6(U2)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8|\u03c6\u0307(U2)] \u00b7\u2206\u221e(S\u0307(U2), v, \u03c8) (22)\nand g2(v) := \u2211\n\u03c6(U3)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8|\u03c6\u0307(U3)] \u00b7\u2206\u221e(S\u0307(U3), v, \u03c8). (23)\nWe have g1(v) \u2265 g2(v).\nSince \u03c6\u0307(U2) \u227a \u03c6\u0307(U3), we have (1) {\u03c8 \u2208 \u03a8 : \u03c6\u0307(U3) \u227a \u03c8} \u2286 {\u03c8 \u2208 \u03a8 : \u03c6\u0307(U2) \u227a \u03c8}, (2) L(\u03c6\u0307(U2)) \u2286 L(\u03c6\u0307(U3)) and (3) D(\u03c6\u0307(U2)) \u2286 D(\u03c6\u0307(U3)). Let us consider the edge set E2 :=E \\\n16\nFig. 10: Analysis with Long Equations II.\nLemma 8. For each pair of status U1 and U2, such that U1 is final, S\u0307(U1) \u2286 S\u0307(U2), and L ( \u03c6\u0307(U1) ) \u222a D ( \u03c6\u0307(U1) ) \u2286 L ( \u03c6\u0307(U2) ) \u222aD ( \u03c6\u0307(U2) ) . Let us consider two functions:\ng1(v) := \u2211\n\u03c6\u0307(U1)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8|\u03c6\u0307(U1)] \u00b7\u2206t(S\u0307(U1), v, \u03c8) (11)\nand\ng2(v) := \u2211\n\u03c6\u0307(U2)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8|\u03c6\u0307(U2)] \u00b7\u2206t(S\u0307(U2), v, \u03c8). (12)\nWe have g1(v) \u2265 g2(v) for each v \u2208 V and t \u2208 Z+. To prove Lemma 8, let us consider the edge set\nE1 :=E \\ ( L ( \u03c6\u0307(U1) ) \u222aD ( \u03c6\u0307(U1) ))\nwhich consists of the edges that are to be determined in each full-realization \u03c6\u0307(U1) \u227a \u03c8. Similarly, let us define\nE2 :=E \\ ( L ( \u03c6\u0307(U2) ) \u222aD ( \u03c6\u0307(U2) ))\nwith respect to U2. Since E2 \u2286 E1, the edge set E can be partitioned into three parts, {L(\u03c6\u0307(U1))\u222aD(\u03c6\u0307(U1))}, E1\\E2\nNode # Edge # Description\nPower 2,500 26,000 A synthetic lower-law network\nWiki 8,300 103,000 A who-votes-on-whom network from Wikipedia\nHiggs 10,000 22,482 A Twitter cascade regarding regarding the discovery of a new particle\nHepph 35,000 421,482 Arxiv High Energy Physics paper citation network\nHepth 28,000 353,482 Arxiv High Energy Physics Theory paper citation network\nDBLP 317,000 1,040,482 DBLP collaboration network\nYoutube 1,100,000 6,000,000 Youtube online social network\nTABLE 2: Datasets\nand E2. Consider two sets of realizations: \u03a62 = {\u03c6 \u2208 \u03a6 : L(\u03c6) \u222a D(\u03c6) = E2} and \u03a61|2 = {\u03c6 \u2208 \u03a6 : L(\u03c6) \u222a D(\u03c6) = E1 \\ E2}. With Def. 17, we have\n{\u03c8 \u2208 \u03a8 : \u03c6\u0307(U2) \u227a \u03c8} = {\u03c6\u0307(U2)\u2295 \u03c62 : \u03c62 \u2208 \u03a62}, and\n{\u03c8 \u2208 \u03a8 : \u03c6\u0307(U1) \u227a \u03c8} = {\u03c6\u0307(U1)\u2295 \u03c61 \u2295 \u03c62 : \u03c61 \u2208 \u03a61|2, \u03c62 \u2208 \u03a62}.\nWith these notations, we have\ng1(v) = \u2211\n\u03c6\u0307(U1)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8|\u03c6\u0307(U1)] \u00b7\u2206t(S\u0307(U1), v, \u03c8)\n= \u2211\n\u03c62\u2208\u03a82\n\u2211\n\u03c61\u2208\u03a61|2\nPr[\u03c6\u0307(U1)\u2295 \u03c61 \u2295 \u03c62|\u03c6\u0307(U1)]\n\u00b7\u2206t(S\u0307(U1), v, \u03c6\u0307(U1)\u2295 \u03c61 \u2295 \u03c62) = \u2211\n\u03c62\u2208\u03a82\nPr[\u03c62] \u2211\n\u03c61\u2208\u03a61|2\nPr[\u03c61]\n\u00b7\u2206t(S\u0307(U1), v, \u03c6\u0307(U1)\u2295 \u03c61 \u2295 \u03c62)\nand\ng2(v) = \u2211\n\u03c6\u0307(U2)\u227a\u03c8 \u03c8\u2208\u03a8\nPr[\u03c8|\u03c6\u0307(U2)] \u00b7\u2206t(S\u0307(U2), v, \u03c8)\n= \u2211\n\u03c62\u2208\u03a82\nPr[\u03c6\u0307(U2)\u2295 \u03c62|\u03c6\u0307(U2)] \u00b7\u2206t(S\u0307(U3), v, \u03c6\u0307(U2)\u2295 \u03c62)\n= \u2211\n\u03c62\u2208\u03a82\nPr[\u03c62] \u00b7\u2206t(S\u0307(U2), v, \u03c6\u0307(U2)\u2295 \u03c62)\nSince \u2211 \u03c61\u2208\u03a61|2 Pr[\u03c61] = 1, to prove g1(v) \u2265 g2(2), it\nsuffices to prove the following lemma.\nLemma 9. For each \u03c61 \u2208 \u03a61|2 and \u03c62 \u2208 \u03a62, we have \u2206t(S\u0307(U1), v, \u03c6\u0307(U1)\u2295 \u03c61\u2295 \u03c62) \u2265 \u2206t(S\u0307(U2), v, \u03c6\u0307(U2)\u2295 \u03c62) . Proof. For conciseness, let us define that\nV1 :=\n|At(S\u0307(U1) + v, \u03c6\u0307(U1)\u2295 \u03c61 \u2295 \u03c62) \\At(S\u0307(U1), \u03c6\u0307(U1)\u2295 \u03c61 \u2295 \u03c62)|\nand\nV2 := |At(S\u0307(U2) + v, \u03c6\u0307(U2)\u2295 \u03c62) \\At(S\u0307(U2), \u03c6\u0307(U2)\u2295 \u03c62)|,\nBy Def. 21, we have\n\u2206t(S\u0307(U1), v, \u03c6\u0307(U1)\u2295 \u03c61 \u2295 \u03c62) = |V1| and\n\u2206t(S\u0307(U2), v, \u03c6\u0307(U2)\u2295 \u03c62) = |V2|.\nThus, it is sufficient to prove that V2 is subset of V1. Suppose that u is in V2. It implies that u is in At(S\u0307(U2) + v, \u03c6\u0307(U2)\u2295 \u03c62) but not in At(S\u0307(U2), \u03c6\u0307(U2) \u2295 \u03c62). That is, in the full realization \u03c6\u0307(U2)\u2295 \u03c62, we have \u2022 (a) there exists a t-live-path from S(U2) + v to u, and \u2022 (b) there is no t-live-path from S(U2) to u. To prove u is in V1, we have to prove that (a) u is in At(S\u0307(U1) + v, \u03c6\u0307(U1) \u2295 \u03c61 \u2295 \u03c62) and (b) u is not in At(S\u0307(U1), \u03c6\u0307(U1)\u2295 \u03c61 \u2295 \u03c62).\nFirst, we prove that u is inAt(S\u0307(U1)+v, \u03c6\u0307(U1)\u2295\u03c61\u2295\u03c62). Since u is not in At(S\u0307(U2), \u03c6\u0307(U2) \u2295 \u03c62), we have u \u2208 V \\ S\u0307(U2). By (a), there is a t-live-path from v to u in \u03c6\u0307(U2) \u2295 \u03c62. Furthermore, by (b), this path cannot use any edge in L(\u03c6\u0307(U2)), and therefore this path only uses the edges in L(\u03c62). Thus, we have this live path in \u03c6\u0307(U1) \u2295 \u03c61 \u2295 \u03c62 as well, and therefore, u \u2208 At(S\u0307(U1) + v, \u03c6\u0307(U1)\u2295 \u03c61 \u2295 \u03c62).\nSecond, since U1 is a final status and u /\u2208 S\u0307(U1), we have u /\u2208 At(S\u0307(U1), \u03c6\u0307(U1)\u2295 \u03c61 \u2295 \u03c62)."
        },
        {
            "heading": "8.4 Proof of Lemma 6",
            "text": "Because\n\u2211 U\u2217\u2208U\u0307d(U) Pr[\u03c6\u0307(U\u2217)|\u03c6\u0307(U)] = 1, it suffices to prove\nthat for each U\u2217 \u2208 U\u0307d(U) we have \u2206ft\u2212d(S\u0307(Ufinal), v, \u03c6\u0307(Ufinal)) \u2265 \u2206ft\u2212d(S\u0307(U\u2217), v, \u03c6\u0307(U\u2217)),\nwhich is in fact a special case of Lemma 8 given in Sec. 8.3."
        },
        {
            "heading": "9 ADDITIONAL EXPERIMENTAL RESULTS",
            "text": "The details of the adopted datasets are given in Table. 2. Additional results from Experiment I and II are provided in Figs. 11, 12, 13 and 14.\nThe observations on other datasets are similar to those discussed in the main paper. One minor point is that when k = 5 HighDegree occasionally has the same performance as that of Greedy. In addition, the superiority of the Full Adoption feedback model is significant on certain graphs, e.g., Fig. 14d."
        }
    ],
    "title": "On Adaptive Influence Maximization under General Feedback Models",
    "year": 2019
}