{
    "abstractText": "Molecular property prediction is an important problem in drug discovery and materials science. As geometric structures have been demonstrated necessary for molecular property prediction, 3D information has been combined with various graph learning methods to boost prediction performance. However, obtaining the geometric structure of molecules is not feasible in many real-world applications due to the high computational cost. In this work, we propose a novel 3D pre-training framework (dubbed 3D PGT), which pretrains a model on 3D molecular graphs, and then fine-tunes it on molecular graphs without 3D structures. Based on fact that bond length, bond angle, and dihedral angle are three basic geometric descriptors corresponding to a complete molecular 3D conformer, we first develop a multi-task generative pre-train framework based on these three attributes. Next, to automatically fuse these three generative tasks, we design a surrogate metric using the total energy to search for weight distribution of the three pretext tasks since total energy corresponding to the quality of 3D conformer. Extensive experiments on 2D molecular graphs are conducted to demonstrate the accuracy, efficiency and generalization ability of the proposed 3D PGT compared to various pre-training baselines.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xu Wang"
        },
        {
            "affiliations": [],
            "name": "Huan Zhao"
        },
        {
            "affiliations": [],
            "name": "Wei-wei Tu"
        },
        {
            "affiliations": [],
            "name": "Quanming Yao"
        }
    ],
    "id": "SP:c99f39b82b4db7702c00e30f41e72322925aa7f5",
    "references": [
        {
            "authors": [
                "Simon Axelrod",
                "Rafael Gomez-Bombarelli"
            ],
            "title": "Molecular machine learning with conformer ensembles",
            "venue": "arXiv preprint arXiv:2012.08452",
            "year": 2020
        },
        {
            "authors": [
                "Simon Axelrod",
                "Rafael Gomez-Bombarelli"
            ],
            "title": "GEOM, energy-annotated molecular conformations for property prediction and molecular generation",
            "venue": "Scientific Data",
            "year": 2022
        },
        {
            "authors": [
                "Lowik Chanussot",
                "Abhishek Das",
                "Siddharth Goyal",
                "Weihua Lavril"
            ],
            "title": "Open catalyst 2020 (OC20) dataset and community challenges",
            "venue": "ACS Catalysis 11,",
            "year": 2021
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In ICML. PMLR,",
            "year": 2020
        },
        {
            "authors": [
                "Gabriele Corso",
                "Luca Cavalleri",
                "Dominique Beaini",
                "Pietro Li\u00f2",
                "Petar Veli\u010dkovi\u0107"
            ],
            "title": "Principal neighbourhood aggregation for graph nets",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "A Crum-Brown",
                "Thomas R Fraser"
            ],
            "title": "1865. The connection of chemical constitution and physiological action",
            "venue": "Trans R Soc Edinb",
            "year": 1969
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "year": 2018
        },
        {
            "authors": [
                "Felix A Faber",
                "Luke Hutchison",
                "Bing Huang"
            ],
            "title": "Machine learning prediction errors better than DFT accuracy",
            "year": 2017
        },
        {
            "authors": [
                "Chelsea Finn",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Model-agnostic metalearning for fast adaptation of deep networks",
            "venue": "In International conference on machine learning",
            "year": 2017
        },
        {
            "authors": [
                "Daan Frenkel",
                "Berend Smit"
            ],
            "title": "Understanding molecular simulation: from algorithms to applications",
            "year": 2001
        },
        {
            "authors": [
                "Johannes Gasteiger",
                "Florian Becker",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Gemnet: Universal directional graph neural networks for molecules",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Justin Gilmer",
                "Samuel S Schoenholz",
                "Patrick F Riley",
                "Oriol Vinyals",
                "George E Dahl"
            ],
            "title": "Neural message passing for quantum chemistry",
            "venue": "In International conference on machine learning",
            "year": 2017
        },
        {
            "authors": [
                "Corwin Hansch",
                "Toshio Fujita"
            ],
            "title": "p-\u03c3-\u03c0 Analysis. A Method for the Correlation of Biological Activity and Chemical Structure",
            "venue": "Journal of the American Chemical Society 86,",
            "year": 1964
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "In CVPR",
            "year": 2022
        },
        {
            "authors": [
                "Weihua Hu",
                "Matthias Fey",
                "Hongyu Ren"
            ],
            "title": "Ogb-lsc: A large-scale challenge for machine learning on graphs",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "Weihua Hu",
                "Bowen Liu",
                "Jure Gomes"
            ],
            "title": "Strategies for pre-training graph neural networks",
            "venue": "arXiv preprint arXiv:1905.12265",
            "year": 2019
        },
        {
            "authors": [
                "Ziniu Hu",
                "Yuxiao Dong",
                "Kuansan Wang",
                "Kai-Wei Chang",
                "Yizhou Sun"
            ],
            "title": "Gpt-gnn: Generative pre-training of graph neural networks",
            "venue": "In KDD",
            "year": 2020
        },
        {
            "authors": [
                "Md ShamimHussain",
                "Mohammed J Zaki",
                "Dharmashankar Subramanian"
            ],
            "title": "Global self-attention as a replacement for graph convolution",
            "venue": "In KDD",
            "year": 2022
        },
        {
            "authors": [
                "Eric Jang",
                "Shixiang Gu",
                "Ben Poole"
            ],
            "title": "Categorical reparameterization with gumbel-softmax",
            "venue": "arXiv preprint arXiv:1611.01144",
            "year": 2016
        },
        {
            "authors": [
                "Jinwoo Kim",
                "Tien Dat Nguyen",
                "Seonwoo Min"
            ],
            "title": "Pure transformers are powerful graph learners",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "TongIl Kim",
                "ChungIl Ri",
                "HakSung Yun"
            ],
            "title": "A novel Method for calculation of Molecular energies and charge Distributions by thermodynamic formalization",
            "venue": "Scientific Reports",
            "year": 2019
        },
        {
            "authors": [
                "Johannes Klicpera",
                "Janek Gro\u00df",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Directional message passing for molecular graphs",
            "venue": "arXiv preprint arXiv:2003.03123",
            "year": 2020
        },
        {
            "authors": [
                "Greg Landrum"
            ],
            "title": "Rdkit: Open-source cheminformatics software. 2016",
            "venue": "URL http://www. rdkit. org/, https://github. com/rdkit/rdkit 149,",
            "year": 2016
        },
        {
            "authors": [
                "Lihang Liu",
                "Donglong He",
                "Wu-Hua Fang",
                "Xiaomin"
            ],
            "title": "GEM-2: Next Generation Molecular Property Prediction Network by Modeling Full-range Many-body Interactions",
            "year": 2022
        },
        {
            "authors": [
                "Shengchao Liu",
                "Hanchen Wang",
                "Weiyang Liu"
            ],
            "title": "Pre-training Molecular Graph Representation with 3D Geometry",
            "venue": "In ICLR 2022 Workshop on Geometrical and Topological Representation Learning",
            "year": 2022
        },
        {
            "authors": [
                "Yi Liu",
                "Limei Wang",
                "Meng Liu",
                "Xuan Zhang",
                "Bora Oztekin",
                "Shuiwang Ji"
            ],
            "title": "Spherical message passing for 3d graph networks",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "Shengjie Luo",
                "Tianlang Chen",
                "Yixian Xu"
            ],
            "title": "One transformer can understand both 2d & 3d molecular data",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Dominic Masters",
                "Josef Dean",
                "Kerstin Klaser"
            ],
            "title": "GPS++: An Optimised Hybrid MPNN/Transformer for Molecular Property Prediction",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Maho Nakata",
                "Tomomi Shimazaki"
            ],
            "title": "PubChemQC project: a large-scale first-principles electronic structure database for data-driven chemistry",
            "venue": "Journal of chemical information and modeling 57,",
            "year": 2017
        },
        {
            "authors": [
                "Ajit Narayanan",
                "Edward C Keedwell",
                "Bj\u00f6rn Olsson"
            ],
            "title": "Artificial intelligence techniques for bioinformatics",
            "venue": "Applied bioinformatics",
            "year": 2002
        },
        {
            "authors": [
                "Wonpyo Park",
                "Woong-Gi Chang",
                "Donggeon Lee",
                "Juntae Kim"
            ],
            "title": "GRPE: Relative Positional Encoding for Graph Transformer. In ICLR2022 Machine Learning for Drug Discovery",
            "year": 2022
        },
        {
            "authors": [
                "Robert G Parr"
            ],
            "title": "Density functional theory",
            "venue": "Annual Review of Physical Chemistry 34,",
            "year": 1983
        },
        {
            "authors": [
                "Fabian Pedregosa"
            ],
            "title": "Hyperparameter optimizationwith approximate gradient",
            "venue": "In International conference on machine learning",
            "year": 2016
        },
        {
            "authors": [
                "Jiezhong Qiu",
                "Qibin Chen",
                "Yuxiao Dong",
                "Jie Zhang",
                "Tang"
            ],
            "title": "Gcc: Graph contrastive coding for graph neural network pre-training",
            "venue": "In KDD",
            "year": 2020
        },
        {
            "authors": [
                "Alec Radford",
                "Karthik Narasimhan",
                "Tim Salimans",
                "Ilya Sutskever"
            ],
            "title": "Improving language understanding by generative pre-training",
            "year": 2018
        },
        {
            "authors": [
                "Raghunathan Ramakrishnan",
                "Pavlo O Dral",
                "Matthias Rupp",
                "O Anatole Von Lilienfeld"
            ],
            "title": "Quantum chemistry structures and properties of 134 kilo molecules",
            "venue": "Scientific data",
            "year": 2014
        },
        {
            "authors": [
                "Ladislav Ramp\u00e1\u0161ek",
                "Mikhail Galkin",
                "Vijay Prakash Dwivedi",
                "Anh Tuan Luu",
                "Guy Wolf",
                "Dominique Beaini"
            ],
            "title": "Recipe for a General, Powerful, Scalable Graph Transformer",
            "year": 2022
        },
        {
            "authors": [
                "Yu Rong",
                "Yatao Bian",
                "Huang Junzhou Xu",
                "Tingyang"
            ],
            "title": "Self-supervised graph transformer on large-scale molecular data",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Kristof Sch\u00fctt",
                "Oliver Unke",
                "Michael Gastegger"
            ],
            "title": "Equivariant message passing for the prediction of tensorial properties and molecular spectra",
            "venue": "In International Conference on Machine Learning",
            "year": 2021
        },
        {
            "authors": [
                "Hannes St\u00e4rk",
                "Dominique Beaini",
                "Gabriele Corso",
                "Prudencio Tossou",
                "Christian Dallago",
                "Stephan G\u00fcnnemann",
                "Pietro Li\u00f2"
            ],
            "title": "2022. 3d infomax improves gnns for molecular property prediction",
            "venue": "In International Conference on Machine Learning",
            "year": 2050
        },
        {
            "authors": [
                "Jonathan M Stokes",
                "Kevin Yang",
                "Kyle Swanson"
            ],
            "title": "A deep learning approach to antibiotic discovery",
            "venue": "Cell 180,",
            "year": 2020
        },
        {
            "authors": [
                "Ruoxi Sun",
                "Hanjun Dai",
                "Adams Wei Yu"
            ],
            "title": "Does GNN Pretraining Help Molecular Representation",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yu Sun",
                "Xiang Xu"
            ],
            "title": "A graph neural network-based interpretable framework reveals a novel DNA fragility\u2013associated chromatin structural unit",
            "venue": "Genome Biology",
            "year": 2023
        },
        {
            "authors": [
                "Raphael Townshend",
                "Rishi Bedi",
                "Patricia Suriana",
                "Ron Dror"
            ],
            "title": "End-to-end learning on 3d protein structure for interface prediction",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Richard Tran",
                "Janice Lan"
            ],
            "title": "The Open Catalyst 2022 (OC22) Dataset and Challenges for Oxide Electrocatalysis",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Xu Wang",
                "Huan Zhao",
                "Lanning Wei",
                "Quanming Yao"
            ],
            "title": "Graph Property Prediction on Open Graph Benchmark: A Winning Solution by Graph Neural Architecture Search",
            "year": 2022
        },
        {
            "authors": [
                "Yusong Wang",
                "Liu Tie-Yan Li",
                "Shaoning"
            ],
            "title": "An ensemble of VisNet, Transformer-M, and pretraining models for molecular property prediction in OGB 2022",
            "year": 2022
        },
        {
            "authors": [
                "Yusong Wang",
                "Shaoning Li",
                "Xinheng He",
                "Tie-Yan Li",
                "Liu"
            ],
            "title": "ViSNet: a scalable and accurate geometric deep learning potential for molecular dynamics simulation",
            "year": 2022
        },
        {
            "authors": [
                "Zhanghao Wu",
                "Paras Jain",
                "Matthew Wright"
            ],
            "title": "Representing long-range context for graph neural networks with global attention",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Keyulu Xu",
                "Weihua Hu",
                "Jure Leskovec",
                "Stefanie Jegelka"
            ],
            "title": "How powerful are graph neural networks",
            "venue": "arXiv preprint arXiv:1810.00826",
            "year": 2018
        },
        {
            "authors": [
                "Minkai Xu",
                "Shitong Luo",
                "Yoshua Bengio",
                "Jian Peng",
                "Jian Tang"
            ],
            "title": "Learning neural generative dynamics for molecular conformation generation",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "Chengxuan Ying",
                "Tianle Cai",
                "Shengjie Luo",
                "Tie-Yan Liu"
            ],
            "title": "Do transformers really perform badly for graph representation",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Sheheryar Zaidi",
                "Michael Schaarschmidt",
                "James Martens"
            ],
            "title": "Pre-training via denoising for molecular property prediction",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Gengmo Zhou",
                "Zhifeng Gao",
                "Qiankun Ding",
                "Hang Zheng",
                "Hongteng Xu",
                "Zhewei Wei",
                "Linfeng Zhang",
                "Guolin Ke"
            ],
            "title": "Uni-Mol: A Universal 3D Molecular Representation Learning Framework",
            "venue": "In The Eleventh International Conference on Learning Representations",
            "year": 2023
        },
        {
            "authors": [
                "Jinhua Zhu",
                "Yingce Xia",
                "Lijun Wu",
                "Liu-Tie-Yan Xie",
                "Shufang"
            ],
            "title": "Unified 2d and 3d pre-training of molecular representations",
            "venue": "In KDD",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "CCS CONCEPTS \u2022 Computing methodologies\u2192 Neural networks; \u2022 Applied computing \u2192 Molecular structural biology.\nKEYWORDS Molecular property prediction, 3D pre-training, graph transformer ACM Reference Format: Xu Wang, Huan Zhao, Wei-wei Tu, and Quanming Yao. 2023. Automated 3D Pre-Training for Molecular Property Prediction. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n\u2217Huan is the corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA. \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0103-0/23/08. . . $15.00 https://doi.org/https://doi.org/10.1145/3580305.3599252\n(KDD \u201923), August 6\u201310, 2023, Long Beach, CA, USA. ACM, New York, NY, USA, 12 pages. https://doi.org/https://doi.org/10.1145/3580305.3599252"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Molecular property prediction is a foundational problem in drug discovery [45], materials science [3, 49] and bioinformatics [33, 50, 58]. Conventionally, in quantum chemistry, popular computational tools based on Density Functional Theory (DFT) [35] are used to compute a molecule\u2019s geometry and quantum properties, e.g. energy of atoms. Then recent years have witnessed the success of machine learning, especially graph neural networks (GNNs) [8, 12] and graph transformer based methods [18, 53, 56], in molecular property prediction, since a molecule can be naturally represented by a graph, where nodes and edges represent atoms and their chemical bonds, respectively. As shown in Figure 1, existing learning-based methods can be roughly categorized into two groups based on the usage of 3D geometry of molecules. Especially, as the molecular properties are mostly determined by their 3D structures [6, 13], methods [11, 22, 60] that directly model the 3D structures of molecules, e.g., 3D conformers, have been proposed to further improve the performance compared to 2D methods. However, due to the high computational expensive of DFT-based tools, it tends to be a challenging problem to obtain sufficient labels for these learning based methods.\nVery recently, inspired by the success of pre-training and finetuning paradigm in natural language processing [7, 38] and computer vision [4, 14], researchers started to apply self-supervised learning (SSL) to molecular property prediction [16, 17, 27, 37, 43, 57]. The key to pre-training lies in pretext tasks design to make better use of the rich unlabeled molecule data, no matter 2D or 3D graphs. Then, existing works can also be categorized into 2D and 3D pre-train methods, according to the type of pretext tasks. For example, in [16], several node-level and graph-level prediction tasks directly from 2D molecular graphs are designed to pre-train GNN models, which are then fine-tuned on downstream tasks. In terms of 3D pretraining methods, [57] conducts SSL tasks by position denoising of 3D molecular structures at equilibrium to learn the molecular force field, which can help the 3D model predict properties more accurately. All these works have demonstrated the effectiveness of pre-training methods for molecular prediction, especially designing pretext tasks on 3D molecular structures.\nAlthough 3Dmolecular structures are necessary to achieve strong performance in molecular property prediction [10], obtaining 3D\nar X\niv :2\n30 6.\n07 81\n2v 2\n[ q-\nbi o.\nQ M\n] 2\nJ ul\n2 02\nequilibrium structure, shown in Figure 1, still requires expensive DFT-based geometry optimization [15, 55]. Thus, even if we can obtain an accurate 3D GNN model by pre-training, prediction on downstream tasks can still be very slow, especially for large-scale real-world applications [15], since we need to compute 3D structure for each molecule before inference.\nIn this work, to address this problem, we propose a novel 3D pre-train approach on existing unlabeled 3D molecular datasets and then fine-tune and predict on the molecules only with 2D molecular graphs. In this way, the proposed method can enjoy the benefits of both two worlds, i.e., high accuracy from pre-training on 3D molecular graphs, and high efficiency from removing the need for computing 3D structures in the fine-tuning/prediction stage. To be specific, we first design multiple generative pre-training tasks based on three attributes, bond length, bond angle, and dihedral angle, which are three basic local geometric descriptors that can be combined into a complete molecular 3D conformer[31]. In this work, we build our 3Dmultiple pretext tasks on the assumption that these three tasks are necessary and sufficient for effective encoding of 3D structure in molecular property prediction, then it remains to be a critical problem that fuse these three tasks in the pre-training stage. Intuitively, different tasks have distinct importance given a pre-trained dataset, and improper integration of multiple tasks may lead to ineffective or negative transfer to downstream tasks [46]. A possible solution is to search for an automated fusion scheme for these three pretext tasks, while the difficulty is that no supervision signal from downstream tasks can be provided. To tackle this problem, we design a surrogate metric based on total energy of all atoms in a molecule in pre-training stage that can supervise the weight allocation of each pre-text tasks, considering that it corresponds to a lowest-energy 3D conformer for a molecule [2, 31]. By automatically searching for the pre-task and optimizing the total energy in the pre-training stage, we can potentially encode more accurate molecular geometry information on the 2D molecular graph, which\nwill promise generalization benefits for various molecular property prediction tasks. Besides, to guarantee the prediction performance, we choose GPS [40] as our backbone, which is a hybrid architecture of GNN and Transformer. Then the proposed method is dubbed 3D PGT (Pre-trained Graph Transformer).\nTo demonstrate the superiority of the proposed 3D PGT, we conduct extensive experiments on various scenarios. Here we highlight some results: a) On the well-known benchmark dataset, QM9 [39], 3D PGT not only improves the prediction performance compared to all baselines, but also enjoys high efficiency the same as methods directly operating on 2D molecular graphs; b) The performance gains in three kinds of downstream tasks demonstrate the strong generalization ability of the proposed 3D PGT; c) In a largescale application on HOMO-LUMO energy gap prediction in OGB benchmark [15] (3.37 million molecules for pre-training), where only 2D methods are feasible, the proposed 3D PGT achieves 10.6% relative MAE reduction compared to the same backbone without pre-training and outperforms all models from top solutions in the OGB leaderboard.\nTo summarize, the contributions of this work are as follows:\n\u2022 We propose a novel pre-training framework to encode 3D geometry with three important pretext tasks for molecular property prediction. To the best of our knowledge, we are the first to integrate these three tasks in a unifying manner for pre-training methods. \u2022 Further, based on important quantum chemistry knowledge, we design a surrogate metric by total energy, which serves as a supervision signal to search the adaptive weight of each pre-training task, and generalize the benefits to various downstream tasks. \u2022 Extensive experiments on various tasks demonstrate the superiority of the proposed 3D PGT in terms of accuracy, efficiency and generalization ability."
        },
        {
            "heading": "2 RELATEDWORK",
            "text": ""
        },
        {
            "heading": "2.1 Property Prediction on Molecular Graph",
            "text": "For property prediction on molecular graph, GNN has become an emerging research field which process it as a graph-level prediction task [47, 50]. After [8] demonstrated that ML models could be more accurate than hybrid DFT if explicitly electron correlated quantum data was available, [12] proposed a message passing framework for chemical prediction problems that are capable of learning their own features from molecular graphs directly, and was considered as the most general architectures for realizing GNNs. However, message passing based methods is weak in capturing long-range dependencies [40]. Complementary, the Graph Transformer based methods represented by Graphormer [56] which is directly build upon the standard Transformer realized the interaction between long-range nodes. Furthermore, EGT [18] uses global self-attention to update both node and edge representations, allowing unconstrained dynamic long-range interaction between nodes and improves the performance for the quantum-chemical regression task on the PCQM4Mv2 [15] dataset. GraphTrans [53] and GPS [40] regard GNN and Transformer as complementary, and explore the coexistence framework between them.\nAt the same time, one important branch of improvement is to effectively use the geometric information contained in 3D conformers [2, 48]. There is a series of work focus on using relative 3D information which can be derived based on absolute Cartesian coordinates, such as bond length and bond angle [22]. GemNet [11] further capture the information from dihedral angle to define all relative atom positions uniquely. Not limited in Cartesian coordinates, SphereNet [28] propose a generic framework of 3D graph network (3DGN), and design the spherical message passing to realize 3DGN in the spherical coordinate system. However, although 3D information has been proved to be valuable for the prediction of molecular properties, the generation of 3D conformer requires DFT-based geometric optimization which needs expensive calculation. This makes it infeasible to explicitly calculate the molecular structure for large-scale datasets."
        },
        {
            "heading": "2.2 Pre-training on Molecular Graph",
            "text": "The general pipeline of pre-training is to first pre-train the model on a large dataset through proxy tasks, and then conduct further parameter fine-tuning on specific downstream tasks based on corresponding supervision signals. The self-supervised pre-training on graph is usually designed to learn self-generated targets from the structure of the molecules, such as masked node reconstruction and context prediction [16]. There are also works use motif prediction as a pre-training task on 2D molecular graph to improve the performance of property prediction [41].\nConsidering the calculation cost of 3D geometry and its importance for molecular property prediction task, a series of work proposed to pre-train a graph encoder which encode implicit 3D information in its latent vectors by pre-training on publicly available 3D molecular structures. They first conduct pre-training on the dataset with 3D structure through designing proxy task, and then transfer the pre-trained model to the downstream task which only contained 2D topology for fine-tuning. Similar to the 3D models which contained 3D geometry as input, the designed pre-training task designed should also respect the symmetries of rotation and translation. For example, Liu et al. [27] and St\u00e4rk et al. [27] used mutual information between 2D and 3D views for molecular pretraining, therefore the GNN is still able to produce implicit 3D information that can used to inform property predictions. Zhu et al. [59] proposed a unified 2D and 3D pre-training which jointly leverages the 2D graph structure and the 3D geometry of the molecule. GEM [11] proposes a bond-angle graph and self-supervised tasks which use large-scale unlabelled molecules with coarse 3D spatial structures which can be calculated by cheminformatics tools such as RDKit. Inspired by the encoding method of Graphormer for 2D molecular graph, Transformer-M [29] further encodes 3D spatial distance into attention bias, which can take molecular data of 2D or 3D formats as input.\nDifferent from existing works, we design multiple generative pretext tasks based on 3D conformer, and fuse these tasks automatically by an interesting and useful supervision signal, i.e., the total energy of molecules."
        },
        {
            "heading": "3 METHOD",
            "text": ""
        },
        {
            "heading": "3.1 Problem Formulation",
            "text": "Let \ud835\udc3a2D = (\ud835\udc49 , \ud835\udc38) denote a 2D molecular graph with atom attributes x\ud835\udc63 for \ud835\udc63 \u2208 \ud835\udc49 and bond attributes e\ud835\udc62\ud835\udc63 for (\ud835\udc62, \ud835\udc63) \u2208 \ud835\udc38, 3D molecular graph refers to \ud835\udc3a3D = (\ud835\udc49 , \ud835\udc38, P) which contains 3D Cartesian coordinates P \u2208 R |\ud835\udc49 |\u00d73 for all atoms. The coordinates P contained in the 3D conformer is valuable information for the graph encoder \ud835\udc53\ud835\udf03 (\u00b7) in molecular property prediction, each conformer requires the calculation by DFT-based tools for several hours per CPU-core [44].\nAs generic pre-training pipelines, 3D pre-training with geometry for molecular representation has two stages: In the pre-training stage, the proxy task Lpre is designed to learn the self-generated targets from the publicly available 3D molecular graph \ud835\udc3a3D; then during the finetune stage, the pre-trained encoder \ud835\udc53\ud835\udf03 \u2217 (\u00b7) is finetuned according to specific supervision signals on downstream tasks, which only includes 2D molecular graphs."
        },
        {
            "heading": "3.2 Automated Fusion Framework for Multiple",
            "text": "3D Pre-training Tasks\nIn this work, we propose 3D PGT, an automated fusion framework that contains multiple generative 3D pre-training tasks to obtain an effective pre-trained 2D graph encoder \ud835\udc53\ud835\udf03 \u2217 (\u00b7). 3D PGT focuses on 1. Designing multiple generative pre-training tasks that can benefit downstream tasks where 3D information is not available; 2. Bridging the gap between 3D generative pre-training tasks and downstream property prediction, allowing a wider range of downstream tasks to benefit from pre-training.\nIn the following, we first present an overview of 3D PGT, and propose three generative pre-training tasks concerning 3D conformer generation. Then, to better fuse multiple pre-training tasks instead of purely average pre-training, we introduce an automated fusion framework with a surrogate metric to search the weight of each generative task during the pre-training stage. Finally, we summarize the variants for leveraging multiple conformers and searching the architecture of the backbone. Overview of 3D PGT. In general, 3D PGT focuses on designing the 3D pre-training framework to learn the robust transferable knowledge from \ud835\udc3a3D and then generalize to downstream tasks which consist of\ud835\udc3a2D. Different from existing contrastive learning based methods [27, 43], 3D PGT aims to encode geometric priors only through single-model pre-training. As shown in Figure 3, 3D PGT first splits the 3D conformer optimization into three generative tasks: bond length, bond angle and dihedral angel. By reconstructing these local descriptors that can fully describe the 3D conformer, the encoder could implicitly generate and encode 3D information in its latent vectors, which can better reflect certain molecular properties [59]. Considering the weight distribution problem of these three pre-training tasks, we design a pre-training surrogate metric to dynamically adjust the weights of each generative task, and conduct the pre-training of the model as a bi-level optimization problem:\nmin \ud835\udf061,\u00b7 \u00b7 \u00b7,\ud835\udf06\ud835\udc5b H (\ud835\udc53\ud835\udf03 \u2217 (\ud835\udc3a)), s.t.\ud835\udf03\u2217 = argmin \ud835\udf03 \ud835\udc5b\u2211\ufe01 \ud835\udc56=1 \ud835\udf06\ud835\udc56L\ud835\udc56 (\ud835\udc53\ud835\udf03 (\ud835\udc3a)), (1)\nwhere \ud835\udf06\ud835\udc56 is the lossweight for pre-training taskL\ud835\udc56 . As a pre-training surrogate metric, H is used as a medium to correlate conformer generation with downstream property prediction tasks, thus filling the gap between self-supervised pre-training tasks and downstream supervised tasks, so that the geometric priors can be better generalized to downstream tasks with different supervision signals. The overall framework of 3D PGT is shown in Figure 2."
        },
        {
            "heading": "3.2.1 3D generative pre-training tasks.",
            "text": "Generative pre-training task enables the model to understand the DFT based geometry optimization process of molecules from 2D topology to 3D geometry. Since molecular geometry is determined by the quantum mechanical behavior of the electrons, the generative task indirectly learns the prediction of quantum chemical properties by learning the generation of 3D conformers. In this work, we design multiple generative 3D pre-training tasks based on molecular structure generation, including the prediction of bond length, bond angle, and dihedral angles, which are complementary local geometric parameters that can elucidate a 3D conformer.\nAs the distance between the equilibrium nuclei of two bonded atoms, bond length is the basic configuration parameter for understanding molecular structure [24]. We first take bond length prediction as one of the generative pre-training tasks and define the loss function as follow:\nLlength(\ud835\udc38) = 1 |\ud835\udc38 | \u2211\ufe01 (\ud835\udc56, \ud835\udc57 )\u2208\ud835\udc38 (\ud835\udc53length(h (\ud835\udc3f) \ud835\udc56 ,h(\ud835\udc3f) \ud835\udc57 ) \u2212 \ud835\udc59\ud835\udc56 \ud835\udc57 )2, (2)\nwhere, h(\ud835\udc3f) \ud835\udc56 and h(\ud835\udc3f) \ud835\udc57\ndenotes the representation of atom \ud835\udc56 and \ud835\udc57 which are encoded from \ud835\udc53\ud835\udf03 (\u00b7), \ud835\udc53length(\u00b7) is a network that predicts the bond length, \ud835\udc59\ud835\udc56 \ud835\udc57 denotes the length of the bond (\ud835\udc56, \ud835\udc57 ) \u2208 \ud835\udc38. At the same time, the bond angle and dihedral angle which can be understood as the local spatial description of molecular geometry\nare the complementary information of bond length. These two prediction tasks could be designed as:\nLangle(\ud835\udc34) = 1 |\ud835\udc34| \u2211\ufe01 (\ud835\udc56, \ud835\udc57,\ud835\udc58)\u2208\ud835\udc34 (\ud835\udc53angle(h (\ud835\udc3f) \ud835\udc56 , h(\ud835\udc3f) \ud835\udc57 , h(\ud835\udc3f) \ud835\udc58 ) \u2212 \ud835\udefc \ud835\udc57,\ud835\udc56,\ud835\udc58 )2, (3)\nLdihedral(\ud835\udc37) = 1 |\ud835\udc37 | \u2211\ufe01 (\ud835\udc56, \ud835\udc57,\ud835\udc58,\ud835\udc5a)\u2208\ud835\udc37 (\ud835\udc53dihedral(h (\ud835\udc3f) \ud835\udc56 , h(\ud835\udc3f) \ud835\udc57 , h(\ud835\udc3f) \ud835\udc58 , h(\ud835\udc3f)\ud835\udc5a ) \u2212 \ud835\udf19\ud835\udc58,\ud835\udc56, \ud835\udc57,\ud835\udc5a)2, (4)\nwhere \ud835\udc34 denotes the set of bond angles and \ud835\udc37 denotes the set of dihedral angles. \ud835\udefc\ud835\udc56, \ud835\udc57,\ud835\udc58 and \ud835\udf19\ud835\udc58,\ud835\udc56, \ud835\udc57,\ud835\udc5a represent the bond angle formed by node \ud835\udc56, \ud835\udc57, \ud835\udc58 and the dihedral angle involving node \ud835\udc56, \ud835\udc57, \ud835\udc58,\ud835\udc5a. Their value ranges are [0, \ud835\udf0b] and [0, 2\ud835\udf0b]. \ud835\udc53angle and \ud835\udc53dihedral are two prediction networks\nHowever, with the number of neighbors |N |, the O(|N |2) and O(|N |3) computational complexity of bond angle calculation and dihedral angle calculation will cause a large amount of time and memory consumption during pre-training. Inspired by [42], we propose a task variant that directly predicts the sum of cosine values of bond angle and dihedral angle as follows:\nLangle(A) = 1 |A| \u2211\ufe01 \ud835\udc56\u2208V ( \u2211\ufe01 (\ud835\udc57,\ud835\udc58)\u2208A\ud835\udc56 \ud835\udc53angle(h (\ud835\udc3f) \ud835\udc56 , h(\ud835\udc3f) \ud835\udc57 , h(\ud835\udc3f) \ud835\udc58 ) \u2212 \u0398\ud835\udc56 )2, (5) Ldihedral(D) = 1 |D| \u2211\ufe01 (\ud835\udc56, \ud835\udc57 )\u2208E ( \u2211\ufe01 (\ud835\udc58,\ud835\udc5a)\u2208D\ud835\udc56 \ud835\udc57 \ud835\udc53dihedral(h (\ud835\udc3f) \ud835\udc56 , h(\ud835\udc3f) \ud835\udc57 , h(\ud835\udc3f) \ud835\udc58 , h(\ud835\udc3f)\ud835\udc5a ) \u2212 \u03a6(\ud835\udc56, \ud835\udc57 ))2, (6)\nwhere A\ud835\udc56 represents the angle set formed by node \ud835\udc56 , D\ud835\udc56 \ud835\udc57 represents the dihedral angle set with \ud835\udc52\ud835\udc56 \ud835\udc57 as the common rotation axis. The calculation details of \u0398\ud835\udc56 and \u03a6(\ud835\udc56, \ud835\udc57 ) which denotes the sum of cosine values of A\ud835\udc56 and D\ud835\udc56 \ud835\udc57 are described in Appendix B.1."
        },
        {
            "heading": "3.2.2 Fusion supervised by a surrogate Metric.",
            "text": "In order to integrate the multiple generative tasks we defined, we need a supervision signal to optimize the weights of each pretraining task. Given access to the downstream task labels, we can explicitly use the performance on the downstream task as an objective function to guide weight tuning. However, considering the variety of molecular property prediction tasks, searching \ud835\udf06\ud835\udc56 as a hyper-parameter can be highly expensive. Therefore, designing a generalized surrogate metric for searching these weights is necessary.\nThe total energy of a molecule is represented by the sum of the energy of each atomic domain and the energy of electrostatic interactions between the domains [21]. As mentioned above, the geometry optimization of DFT is to search the local minima on the potential energy surface [2], as shown in Figure 3. The basic functional form of potential energy inmolecularmechanics includes the bonded terms for interactions of atoms and the nonbonded terms that describe the long-range electrostatic and van der Waals forces, which means that the total energy of a molecule can reflect its geometric information. Considering that the total energy \ud835\udc38total as an attribute of 3D conformer can be acquired along with conformer generation, we design a graph-level prediction task based on the \ud835\udc38total, which can be expressed as:\nL\ud835\udc38total (\ud835\udc53\ud835\udf03 (\ud835\udc3a)) = (\ud835\udc53\ud835\udf03 (\ud835\udc3a) \u2212 \ud835\udc38total) 2 . (7)\nHere, we use L\ud835\udc38total as the surrogate metric to optimize \ud835\udf06\ud835\udc56 , and due to the association between 3D geometric information and molecular properties, L\ud835\udc38total (\ud835\udc3a) can be used to supervise the fusion of these generative pre-training tasks so that the geometric prior obtained by\npre-training is also generalized for downstream property prediction tasks.\nHowever, for the bi-level optimization problem in equation (1), computing the gradient ofL\ud835\udc38total to update \ud835\udf06\ud835\udc56 is very expensive [36]. Inspired by [9], we use mate-gradients to optimize this bi-level problem. By unrolling the training procedure, the meta-gradient \u2207meta{\ud835\udf06\ud835\udc56 } can be expressed as:\n\u2207meta{\ud835\udf06\ud835\udc56 } : = \u2207{\ud835\udf06\ud835\udc56 }L\ud835\udc38total (\ud835\udc53\ud835\udf03\ud835\udc47 (\ud835\udc3a)) (8) = \u2207\ud835\udc53\ud835\udf03\ud835\udc47 L\ud835\udc38total (\ud835\udc53\ud835\udf03\ud835\udc47 (\ud835\udc3a)) \u00b7 \u2207\ud835\udf03\ud835\udc47 \ud835\udc53\ud835\udf03\ud835\udc47 (\ud835\udc3a)\u2207{\ud835\udf06\ud835\udc56 }\ud835\udf03\ud835\udc47 , (9)\nwhere \ud835\udf03\ud835\udc47 denotes the encoder\u2019s parameters \ud835\udf03 at step \ud835\udc47 , which is depends on the task weights {\ud835\udf06\ud835\udc56 }. Thus, the gradient descent performed on {\ud835\udf06\ud835\udc56 } can be achieved bymeta-gradient\u2207meta{\ud835\udf06\ud835\udc56 } as {\ud835\udf06\ud835\udc56 }\u2212 \ud835\udf02\u2207meta{\ud835\udf06\ud835\udc56 } , where \ud835\udf02 is the learning rate for outer optimization. At the same time, inspired by DARTS [25], we only perform one step gradient descent on \ud835\udf03 and update {\ud835\udf06\ud835\udc56 } each iteration, which means {\ud835\udf06\ud835\udc56 } is dynamic during the pre-training stage and adjusts between [0, 1]. The calculation details of \u2207meta{\ud835\udf06\ud835\udc56 } can be referred to [9]."
        },
        {
            "heading": "3.3 Further Improvements",
            "text": ""
        },
        {
            "heading": "3.3.1 Pre-training by Multiple conformers.",
            "text": "As mentioned in Section 3.1, 3D molecular datasets usually calculate the conformer with the lowest energy. However, leveraging structural information of multiple conformers could bring additional benefits [27, 43]. Therefore, we consider introduce multiple conformers {P\ud835\udc50\n\ud835\udc56 }\ud835\udc50\u2208{1...\ud835\udc36 } of the same molecule graph \ud835\udc3a\ud835\udc56 in the\npre-training datasets, and repeat the lowest energy conformer if the conformer number of a molecule is fewer than \ud835\udc36 . For the 3D pre-training in our framework, we here simply consider it as a data augmentation and avoid the pipeline changes of pre-training."
        },
        {
            "heading": "3.3.2 Architecture search of Graph Transformer.",
            "text": "In addition to using the surrogate metricH to search the weight of each pre-training task, we also search the architecture of pretraining backbone. Based on the hybrid Graph Transformer structures which combined the local message passing module and global attention module, we build a search space that includes the above modules to search the backbones for different pre-training datasets. Specifically, in each layer of the graph encoder \ud835\udc53\ud835\udf03 (\u00b7), the search space of operations O\ud835\udc3a\ud835\udc47 includes 3 operations: local message passing, global attention and a hybrid operation through a Feed Forward Network (FFN). We then relax the discrete selection of operation by a weighted summation of all possible operations, and use a differentiable search algorithm based on Gumble-Softmax [19]. The details of the architecture search are described in Appendix B."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In this section, we conduct various experiments to demonstrate the effectiveness of the proposed 3D PGT. As explained in Introduction, we aim to address the scenario where only 2D molecular graphs are available for prediction, thus we can avoid the huge computational cost of generating 3D conformer for each molecule. Thus in the experiments, we pre-train using subsets of 3D molecular structure datasets, and then fine-tune the models in downstream tasks of 2D datasets or 3D datasets while ignoring the 3D structures.\nTable 1: Results for quantum properties prediction in QM9. The evaluation metric is MAE (Smaller is better). The compared methods include 2D models without pre-training, pre-training baselines, and the 3D model SMP. True 3D means to input the ground truth 3D structure, and RDkit means to input the structure calculated by RDkit. The best performance of each target is marked in bold.\nTarget 2D Model Pre-training Baselines 3D SMPPNA GPS GraphCL AttrMask GPT-GNN GraphMVP 3D Infomax \ud835\udc38Total 3D PGT RDKIT True 3D\n\ud835\udf07 0.4133 0.4087 0.3937 0.4626 0.3975 0.3489 0.3507 0.3927 0.3409 0.4344 0.0726 \ud835\udefc 0.3972 0.3884 0.3295 0.3570 0.3732 0.3227 0.3268 0.3514 0.3121 0.3020 0.1542 \ud835\udc3b\ud835\udc42\ud835\udc40\ud835\udc42 82.10 81.19 79.57 80.58 93.11 68.62 68.96 78.28 68.24 82.51 56.19 \ud835\udc3f\ud835\udc48\ud835\udc40\ud835\udc42 85.72 84.86 80.81 84.93 99.84 70.23 69.51 81.26 69.73 80.36 43.58 GAP 123.08 121.17 120.08 116.21 131.99 101.84 101.71 112.37 101.53 114.24 85.10 R2 22.14 21.78 21.84 29.23 29.21 17.03 17.39 21.86 16.89 22.63 1.510 ZPVE 15.08 12.29 12.39 25.91 11.17 7.958 7.960 18.28 7.924 5.180 2.690 \ud835\udc50\ud835\udc63 0.1670 0.1618 0.1422 0.1587 0.1795 0.1287 0.1306 0.1507 0.1217 0.1419 0.0498\nHere we briefly explain the organization of our experiments. Firstly, we evaluate the proposed 3D PGT on popular benchmark dataset,QM9 [39] , for quantum chemistry properties. Then broader downstream applications, including binary property classification, regression property, and drug-target affinity prediction, are designed to evaluate the generalization/transfer ability of the 3D PGT. Further ablation studies are conduct to show the importance of each design modules of our method. Finally, more interestingly, we apply 3D PGT to property prediction in a large-scale molecular dataset, PCQM4Mv2 [32], from the famous large-scale open graph benchmark (OGB) [15], where 3D structures are only given in training dataset, to justify the practical values of the problem we try to address in this work, as well as the technical contributions.\nConsidering the different settings of the designed experiments, we give the introduction of experimental setups, including datasets, evaluation metrics, and baselines, in the corresponding subsections. And more detailed information are given in Appendix A and ??. Implementation of 3D PGT.We choose GPS [40] as the Graph Transformer for pre-training due to its state-of-the-art performance on OGB-LSC [15]. As mentioned in Section 3.3.2, we further search the architecture by the surrogate metric L\ud835\udc38total , the details are described in Appendix ??. To implement 3D PGT and reproduce the results, the code is provided as an anonymous link 1. Datasets.We first explore the performance on eight quantum properties prediction of QM9 [39], which contains 134k molecules with a single conformer. Following the experimental setup of mainstream 3D pre-training work [43], we randomly select 50k molecules with geometric information from QM9 for pre-training. Then, we pick another 50k molecules from the rest and mask their 3D structures for model fine-tuning. We use the scaffold splits with an 8/1/1 ratio and report the Mean Absolute Error (MAE) of 8 quantum chemical properties in the test dataset. Baselines. As the SOTA performance on molecular tasks, we first select PNA [5] as the 2D model for comparison. It also includes GPS [40], the Graph Transformer that achieved SOTA on the OGB challenge [15] which is also the backbone of our 3D PGT. At the\n1https://github.com/LARS-research/3D-PGT\nFigure 4: The total inference time and performance ranking of different methods for \ud835\udc3b\ud835\udc42\ud835\udc40\ud835\udc42 prediction.\nsame time, we compare the most well-known pre-training methods, including AttrMask [16], GPT-GNN [17], GraphCL [1], GraphMVP [27] and 3D Infomax [43]. The backbone and hyperparameter settings of the above pre-training methods follow their original settings. We design a baseline that directly uses the total energy prediction as a pre-training task, so as to prove the significance of using total energy as a surrogate metric by comparison. We also chose the SMP [60] based on the 3D Graph Network framework as a 3D model for comparison. The configuration of SMP includes the real 3D structure calculated by DFT from the QM9 dataset; and the relatively rough 3D structure calculated based on RDKit [23]."
        },
        {
            "heading": "4.1 Quantum Chemistry Properties Prediction",
            "text": "Results. The compared results are shown in Table 1. It can be seen that 3D PGT achieves better or competitive performance compared to all baselines. First, we can find that our 3D pre-training framework brings an average 17.7% MAE reduction compared with the GPS backbone, while significantly outperforming the 2D pretraining methods. It is obvious that the improvement brought by the pre-training without geometry in GraphCL is limited, and the same limitation is also reflected in the pre-training which uses energy as\nthe only optimization objective. Besides, the latest two pre-training models on molecular 3D structures, i.e., GraphMVP and 3D Informax, achieve evident performance improvement compared others only pre-training on 2D molecular graphs, which verifies the usefulness of 3D geometry in molecular prediction. Then the further improvement of our 3D PGT compared GraphMVP and 3D Infomax demonstrates the effectiveness of our approach. It is clear that SMP achieves the best performance when given the 3D structures in the test dataset, while notably that our proposed 3D PGT beat SMP with coarse 3D information in 6 targets out of 8, which indicates the advantage of this 3D pre-training and fine-tuning paradigm.\nIn order to explicitly analyse the efficiency comparison of 2D, 3D models and pre-trained models, we statistics the total inference time and performance ranking of all baselines on the test set to analyze the efficiency between different methods, the result of HOMO prediction is shown in Figure 4. It can be seen that the 3D Pre-training based methods achieve a better trade-off between inference time and performance, especially 3D PGT. Although SMP using explicit ground truth 3D information has obvious advantages in predicting molecular properties, when the rough 3D information calculated by RDKIT is input, its performance is lower than that of 3D pre-training methods, which means that it is difficult for 3D models to balance computational efficiency and performance."
        },
        {
            "heading": "4.2 Generalizing to different downstream tasks",
            "text": "Datasets. In addition to predict the quantum chemical properties which is closely related to molecular geometry, we further generalize the downstream tasks to pharmacology, physical chemistry and biophysics, which only measured 2D structures of molecules. Following the experimental settings in [27], we randomly select 50k molecules in GEOM [2] with single conformer for pre-training, and finetune on 8 mainstream binary classification tasks and 6 classification tasks. These downstream datasets are all in the low-data regime and we describe their details in Appendix. Baselines. Following the baseline chosen by [27], in addition to the well-performing backbone GIN [54] and GPS [40], we also selected two 2D pre-training baselines, AttrMask [16] and ContextPred [16]; two predictive pre-training tasks DistancePred [59] and EnergyPred and the latest SOTA solution GraphMVP [27] and 3D Infomax [43].\nResults of MoleculeNet Benchmark.MoleculeNet is a popular benchmark for molecular property prediction. We first compare 3D PGT with other baselines on eight binary property prediction tasks. The performance is shown in Table 2, where the best results are marked in bold. It can be seen that 3D PGT outperforms other baselines on most of these downstream tasks, which means our method can generalize the benefits of our pre-training framework to a broader range of downstream tasks, even though most of these tasks have only a small number of samples. We observe that 3D PGT does not achieve the best performance on Tox21 and MUV tasks, which is mainly because the GPS itself as the backbone is easy to overfit on these datasets. However, it is certain that compared to GPS, 3D PGT can still achieve stable performance gains through pre-training. Results of regression and drug-target affinity tasks. To further confirm whether 3D pre-training can help in a wider range of downstream tasks, we organized additional experiments on 4 additional molecular property regression tasks and 2 Drug-Target Affinity (DTA) tasks. Different from the prediction of the molecular properties themselves, the DTA task is to predict the affinity score between the molecular drug and the target protein. As shown in Table 3, the results show that 3D PGT is still competitive in these 6 regression tasks, and it also proves that 3D Pre-training can help in DTA tasks."
        },
        {
            "heading": "4.3 Ablation studies",
            "text": "The effect of searched adaptive weight for multiple tasks. In Section 3.2, we propose a surrogate metric based on total energy to search the adaptive weight of each generative pre-training task. To verify its effectiveness, we design an ablation study by pre-training a variant of average pre-training. We compared the two methods of average training and the weight search, the results in Figure 5 demonstrate that the searched weights lead to greater benefits on various downstream tasks. And the results of the six regression tasks also reached the same conclusion, as shown in Figure 7. The influence of pre-training dataset sizes. From Figure 5, it can be seen that for most downstream tasks, the performance increases with the amount of pre-trained data, although there is a marginal effect. However, on Tox21 and Sider, when the pre-training dataset\nis further increased, the benefits of pre-training will decrease. One possible reason is that the backbone of GPS itself exhibits overfitting on the two data sets, and further injection of prior knowledge would not alleviate it [46]. Number of conformers for pre-training. For the multiple conformers of a single molecule, we verify whether useing these conformers can benefit the downstream task performance, as shown in Figure 6. The experiment shows that adding conformers for pre-training within a certain range can bring further pre-training benefits. However, consistent with the findings of previous work [1, 27, 43], we observe that there is a bottleneck in the improvement brought by adding more conformers. One accepted conclusion is that the top-5 conformers sampled according to the energy are sufficient to cover 80% of the equilibrium state, and further addition\nof conformers cannot supplement more abundant geometric prior for downstream tasks. As discussed in Section 4.3, we observe that the GPS backbone is prone to overfitting on these two few-shot datasets, and the results show that further introducing too many geometric priors may not lead to better gains in this case."
        },
        {
            "heading": "4.4 Pre-training on Large-scale Dataset",
            "text": "Datasets. As the graph-level track of OGB-LSC [15], PCQM4Mv2 is a quantum chemistry dataset under the PubChemQC project [32]. The dataset contains a total of 3.74 million molecules, of which the 3D geometric information of 3.37 million training samples is obtained by DFT optimization. We adhered to the default trainvalidation split provided by OGB, and the test set was reserved for the competition and leaderboard and not publicly disclosed. In order\nto approach the large-scale virtual screening scenario, the challenge [15] does not provide 3D conformers of the valid dataset and test dataset, and requires the property inference of 150k molecules to be completed within 4 hours using a single GPU, which means that it is not feasible to calculate the geometry of all test samples in the inference stage. Therefore, existing 3D GNN, e.g., SMP, can not work on this challenge due to the expensive computation cost of conformer generation. Baselines. Because the label of test datasets is officially hidden, we compared the results of validation with the top-tier method on the OGB leaderboard 2, which includes GRPE [34], TokenGT [20], EGT [18] , GPS [40], GEM-2 [26], Vis-Net [52] and TransformerM [29]. In addition, we also compared GPS++ [30], the winner solution at OGB LSC@ NeruIPS 2022 Challenge.Note that we only report the performance of single models from these solutions for fair comparisons, though it is clear that various engineering tricks,\n2https://ogb.stanford.edu/docs/lsc/leaderboards/#pcqm4mv2\nlike ensemble, can be applied to further improve the performance of all methods including our 3D PGT. Results. The results of validation MAE are shown in Table 4. First, compared with the existing GNNs and Graph Transformers which do not consider 3D geometry information, 3D PGT has been significantly improved by introducing generative pre-training tasks. For GPS, 3D PGT has achieved 10.6% relative MAE reduction by the designed pre-training framework. Our 3D PGT outperforms the champion solution GPS++ in terms of single-model performance, proving the advantage of our pre-training framework while using the same backbone."
        },
        {
            "heading": "5 CONCLUSION AND FUTUREWORK",
            "text": "In this work, we proposed 3D PGT, a 3D pre-training framework which focus on incorporating 3D information for benefiting the molecular property prediction when the 3D structures is unavailable. In 3D PGT, we designed multiple generative pre-training tasks which can bring geometric prior to finetune stage. To better integrate these pre-training tasks and make their benefits generalize, we designed a surrogate metric of pre-training to search the adaptive weight of each pre-task. The experiment we designed proves that the potential geometric prior is not only beneficial to quantum chemical property prediction, but also to the prediction in pharmacology, physical chemistry and biophysics, etc. Moreover, 3D PGT outperforms all baselines from top solutions for large-scale molecular prediction in OGB leaderboard.\nFor future work, it will be interesting to explore the effectiveness of our proposed framework on larger molecules, e.g. the development of catalysts for storing renewable energy to address global warming [3, 49], beyond the small molecules in this work."
        },
        {
            "heading": "ACKNOWLEDGMENT",
            "text": "Q. Yao is supported by NSF of China (No. 92270106)."
        },
        {
            "heading": "A DATASET DETAILS A.1 Pre-training Datasets with 3D Geometry",
            "text": "Following themainstream experimental setup, we use three datasets containing 3D information for pre-training, and the detailed parameters are shown in Table 6. The three datasets are:\n\u2022 QM9 [39] contains 134k molecules carrying 3D coordinates, each containing only one low-energy conformer. The largest atoms in QM9 are just nine heavy atoms, and each molecule is annotated with 12 quantum mechanical features. \u2022 GEOM-Drugs [2] contains 304kmolecules related to biology and pharmacology, and each molecule contains multiple 3D conformers, and Gibbs free energy and integrated energy are annotated as regression targets. The entire dataset contains 16 elements in total. \u2022 PCQM4Mv2 [15] contains 3.74m molecules with HOMOLUMO energy gap annotated as regression targets, it is a quantum chemistry dataset originally curated under the PubChemQCproject [32]. A total of 3.37m samples of low-energy conformers in the data set are annotated, while the remaining molecules that only contain 2D structures will be used as the validation set and the test set for the competition."
        },
        {
            "heading": "B FURTHER METHOD DETAILS",
            "text": "Due to space limitations, the baseline details, technical details, and additional relationship study results compared in the experiment can be seen here: https://github.com/LARS-research/3D-PGT/blob/ main/Supplementary_Appendix.pdf\nB.1 Computational Acceleration of Bond Angles and Dihedral Angles\nDue to the high computational complexity of bond angles and dihedral angles, we reduce the computational complexity of bond angle and dihedral angle to linear time complexity, and the search of bond angle and dihedral angle\u2019s node index is replaced by the traversal of the node set \ud835\udc49 and the edge set \ud835\udc38. Inspired by the Runtime Geometry Calculation (RGC) [51] which directly calculates the geometric information from the direction unit which only sums the vectors from the target node to its neighbors once, we design two loss functions based on the sum of cosine values of bond angle and dihedral angle. RGC calculates the sum of bond angle\u2019s cosine values as follows:\n\u00ae\ud835\udc62\ud835\udc56 \ud835\udc57 = \u00ae\ud835\udc5f\ud835\udc56 \ud835\udc57 \u2225\u00ae\ud835\udc5f\ud835\udc56 \ud835\udc57 \u2225 , \u00ae\ud835\udc62\ud835\udc56\ud835\udc58 = \u00ae\ud835\udc5f\ud835\udc56\ud835\udc58 \u2225\u00ae\ud835\udc5f\ud835\udc56\ud835\udc58 \u2225 , (10)\n\u0398\ud835\udc56 = \ud835\udc41\ud835\udc56\u2211\ufe01 \ud835\udc57=1 \ud835\udc41\ud835\udc56\u2211\ufe01 \ud835\udc58=1 cos\ud835\udefc \ud835\udc57\ud835\udc56\ud835\udc58 = \ud835\udc41\ud835\udc56\u2211\ufe01 \ud835\udc57=1 \ud835\udc41\ud835\udc56\u2211\ufe01 \ud835\udc58=1 \u2329 \u00ae\ud835\udc5f\ud835\udc56 \ud835\udc57 , \u00ae\ud835\udc5f\ud835\udc56\ud835\udc58 \u232a , (11)\nwhere \u00ae\ud835\udc5f\ud835\udc56 \ud835\udc57 is the vector from node \ud835\udc56 to its neighboring node \ud835\udc57 . Record \u00ae\ud835\udc62\ud835\udc56 \ud835\udc57 as the unit vector of \u00ae\ud835\udc5f\ud835\udc56 \ud835\udc57 and \u00ae\ud835\udc63\ud835\udc56 as the sum of all unit vectors from node \ud835\udc56 , \u00ae\ud835\udf14 \ud835\udc57\ud835\udc56 denotes the vector rejection Rej\u00ae\ud835\udc62\ud835\udc56 \ud835\udc57 (\u00ae\ud835\udc63\ud835\udc56 ), which represents the vector component of \u00ae\ud835\udc63\ud835\udc56 perpendicular to \u00ae\ud835\udc62\ud835\udc56 \ud835\udc57 . \u0398\ud835\udc56 denotes the sum of cosine values of \ud835\udefc \ud835\udc57\ud835\udc56\ud835\udc58 , which denotes the angle formed by node i and two of its neighboring nodes \ud835\udc56, \ud835\udc57 . For the sum of dihedral\nangle\u2019s cosine values, RGC is expressed as: \u00ae\ud835\udc63\ud835\udc56 = \ud835\udc41\ud835\udc56\u2211\ufe01 \ud835\udc57=1 \u00ae\ud835\udc62\ud835\udc56 \ud835\udc57 , \u00ae\ud835\udf14\ud835\udc56 \ud835\udc57 = Rej\u00ae\ud835\udc62\ud835\udc56 \ud835\udc57 (\u00ae\ud835\udc63\ud835\udc56 ) = \u00ae\ud835\udc63\ud835\udc56 \u2212 \u2329 \u00ae\ud835\udc63\ud835\udc56 , \u00ae\ud835\udc62\ud835\udc56 \ud835\udc57 \u232a \u00b7 \u00ae\ud835\udc62\ud835\udc56 \ud835\udc57 , (12)\n\u03a6(\ud835\udc56, \ud835\udc57 ) = \ud835\udc41\ud835\udc56\u2211\ufe01 \ud835\udc57=1 \ud835\udc41\ud835\udc56\u2211\ufe01 \ud835\udc58=1 cos\ud835\udf11\ud835\udc5a\ud835\udc56 \ud835\udc57\ud835\udc5b = \u2329 \u00ae\ud835\udf14\ud835\udc56 \ud835\udc57 , \u00ae\ud835\udf14 \ud835\udc57\ud835\udc56 \u232a , (13)\nwhere, the direction unit \u00ae\ud835\udc63\ud835\udc56 is the sum of all unit vectors from node \ud835\udc56 to its all neighboring nodes \ud835\udc57 , \u00ae\ud835\udf14 \ud835\udc57\ud835\udc56 denotes the vector rejection Rej\u00ae\ud835\udc62\ud835\udc56 \ud835\udc57 (\u00ae\ud835\udc63\ud835\udc56 ), which represents the vector component of \u00ae\ud835\udc63\ud835\udc56 perpendicular to \u00ae\ud835\udc62\ud835\udc56 \ud835\udc57 , termed as the vector rejection. Take \u0398 and \u03a6 as target ,we design two loss functions based on the sum of cosine values of bond angle and dihedral angle as shown in Section 3.2.\nB.2 Supplementary ablation study We further design an ablation study on PCQM4Mv2 to verify the impact of each module in 3D PGT. From the results in Table 5, we can summarize that: a) The backbone which is combined with local message passing and global attention module has more obvious performance advantages. b) The single generative pre-training task only focuses on the reconstruction of a local descriptor. Combining them can bring more significant pre-training benefits. c). The effectiveness of searched adaptive weights has been clearly verified."
        },
        {
            "heading": "C EXPERIMENTS DETAILS",
            "text": "We report the detailed hyperparameters setup for pre-training in Table 7. At the same time, the hyperparameter search space during finetune in all downstream tasks is shown in Table 8. In addition, in order to facilitate the implementation of various GNN variants, we use the popular GNN library: PYG (Pytorch Geometric) (version 2.0.1). For pre-training on different datasets, we set the max epoch number as Table 7 and stop pre-training when the validation loss does not improve for 10 consecutive epochs."
        }
    ],
    "title": "Automated 3D Pre-Training for Molecular Property Prediction",
    "year": 2023
}