{
    "abstractText": "Quantum machine learning is emerging as a strategy to solve real-world problems. As a quantum computing model, parameterized quantum circuits provide an approach for constructing quantum machine learning algorithms, which may either realize computational acceleration or achieve better algorithm performance than classical algorithms. Based on the parameterized quantum circuit, we propose a hybrid quantum-classical convolutional neural network (HQCCNN)model for image classification that comprises both quantum and classical components. ,e quantum convolutional layer is designed using a parameterized quantum circuit. It is used to perform linear unitary transformation on the quantum state to extract hidden information. In addition, the quantum pooling unit is used to perform pooling operations. After the evolution of the quantum system, wemeasure the quantum state and input the measurement results into a classical fully connected layer for further processing. We demonstrate its potential by applying HQCCNN to the MNISTdataset. Compared to a convolutional neural network in a similar architecture, the results reveal that HQCCNN has a faster training speed and higher testing set accuracy than a convolutional neural network.",
    "authors": [
        {
            "affiliations": [],
            "name": "Wei Li"
        },
        {
            "affiliations": [],
            "name": "Peng-Cheng Chu"
        },
        {
            "affiliations": [],
            "name": "Guang-Zhe Liu"
        },
        {
            "affiliations": [],
            "name": "Yan-Bing Tian"
        },
        {
            "affiliations": [],
            "name": "Tian-Hui Qiu"
        },
        {
            "affiliations": [],
            "name": "Shu-Mei Wang"
        }
    ],
    "id": "SP:242dc8509b80781bf33e534c42f4fe10181f04e8",
    "references": [
        {
            "authors": [
                "S.S. Li",
                "G.L. Long",
                "F.S. Bai",
                "S.L. Feng",
                "H.Z. Zheng"
            ],
            "title": "Quantum computing",
            "venue": "Proceedings of the National Academy of Sciences, vol. 98, no. 21, pp. 11847-11848, 2001.",
            "year": 2001
        },
        {
            "authors": [
                "E. Knill"
            ],
            "title": "Quantum computing",
            "venue": "Nature, vol. 463, no. 7280, pp. 441\u2013443, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "P. Xu",
                "Z. He",
                "T. Qiu",
                "H. Ma"
            ],
            "title": "Quantum image processing algorithm using edge extraction based on Kirsch operator",
            "venue": "Optics Express, vol. 28, no. 9, p. 12508, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Ma",
                "N. Li",
                "W. Zhang",
                "S. Wang",
                "H. Ma"
            ],
            "title": "Image encryption scheme based on alternate quantum walks and discrete cosine transform",
            "venue": "Optics Express, vol. 29, no. 18, p. 28338, 2021. Table 1: Complex binary classification task results. Classification task Model Accuracy HQCCNN CNN Even or odd With pooling 0.8759 0.8645 Without pooling 0.9240 0.9011 >4 or <4 With pooling 0.8931 0.8874 Without pooling 0.9252 0.9066 8 Quantum Engineering",
            "year": 2021
        },
        {
            "authors": [
                "H.S. Zhong",
                "H. Wang",
                "Y.H. Deng"
            ],
            "title": "Quantum computational advantage using photons",
            "venue": "Science, vol. 370, no. 6523, pp. 1460\u20131463, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "P. Rebentrost",
                "M. Mohseni",
                "S. Lloyd"
            ],
            "title": "Quantum support vector machine for big data classification",
            "venue": "Physical Review Letters, vol. 113, no. 13, Article ID 130503, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "S. Lu",
                "S.L. Braunstein"
            ],
            "title": "Quantum decision tree classifier",
            "venue": "Quantum Information Processing, vol. 13, no. 3, pp. 757\u2013770, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "N.R. Zhou",
                "X.X. Liu",
                "Y.L. Chen",
                "N.S. Du"
            ],
            "title": "Quantum K-Nearest-Neighbor image classification algorithm based on K-L transform",
            "venue": "International Journal of 5eoretical Physics, vol. 60, no. 3, pp. 1209\u20131224, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "T. Xin",
                "L. Che",
                "C. Xi"
            ],
            "title": "Experimental quantum principal component analysis via parametrized quantum circuits",
            "venue": "Physical Review Letters, vol. 126, no. 11, Article ID 110502, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Peruzzo",
                "J. McClean",
                "P. Shadbolt"
            ],
            "title": "A variational eigenvalue solver on a photonic quantum processor",
            "venue": "Nature Communications, vol. 5, no. 1, p. 4213, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "J.R. McClean",
                "J. Romero",
                "R. Babbush",
                "A. Aspuru-Guzik"
            ],
            "title": "e theory of variational hybrid quantum-classical algorithms",
            "venue": "New Journal of Physics, vol. 18, no. 2, Article ID 023023, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "M. Schuld",
                "A. Bocharov",
                "K.M. Svore",
                "N. Wiebe"
            ],
            "title": "Circuitcentric quantum classifiers",
            "venue": "Physical Review A, vol. 101, no. 3, Article ID 032308, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Zeng",
                "H. Wang",
                "J. He",
                "Q. Huang",
                "S. Chang"
            ],
            "title": "A multiclassification hybrid quantum neural network using an allqubit multi-observable measurement strategy",
            "venue": "Entropy, vol. 24, no. 3, p. 394, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "D. Wecker",
                "M.B. Hastings",
                "M. Troyer"
            ],
            "title": "Progress towards practical quantum variational algorithms",
            "venue": "Physical Review A, vol. 92, no. 4, Article ID 042303, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "T. Jones",
                "S. Endo",
                "S. McArdle",
                "X. Yuan",
                "S.C. Benjamin"
            ],
            "title": "Variational quantum algorithms for discovering Hamiltonian spectra",
            "venue": "Physical Review A, vol. 99, no. 6, Article ID 062304, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Xiao",
                "J. Wen",
                "S. Wei",
                "G. Long"
            ],
            "title": "Reconstructing unknown quantum states using variational layerwise method",
            "venue": "Frontiers of Physics, vol. 17, no. 5, Article ID 51501, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "G.G. Guerreschi",
                "A.Y. Matsuura"
            ],
            "title": "QAOA for Max-Cut requires hundreds of qubits for quantum speed-up",
            "venue": "Scientific Reports, vol. 9, no. 1, p. 6903, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "L. Zhou",
                "S.T. Wang",
                "S. Choi",
                "H. Pichler",
                "M.D. Lukin"
            ],
            "title": "Quantum approximate optimization algorithm: performance, mechanism, and implementation on near-term devices",
            "venue": "Physical Review X, vol. 10, no. 2, Article ID 021067, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S.E. Rasmussen",
                "N.J.S. Loft",
                "T. B\u00e6kkegaard",
                "M. Kues",
                "N.T. Zinner"
            ],
            "title": "Reducing the amount of single-qubit rotations in VQE and related algorithms",
            "venue": "Advanced Quantum Technologies, vol. 3, no. 12, Article ID 2000063, 2020.",
            "year": 2000
        },
        {
            "authors": [
                "A.V. Uvarov",
                "A.S. Kardashin",
                "J.D. Biamonte"
            ],
            "title": "Machine learning phase transitions with a quantum processor",
            "venue": "Physical Review A, vol. 102, no. 1, Article ID 012415, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Liu",
                "K.H. Lim",
                "K.L. Wood",
                "W. Huang",
                "C. Guo",
                "H.L. Huang"
            ],
            "title": "Hybrid quantum-classical convolutional neural networks",
            "venue": "Science China Physics, Mechanics & Astronomy, vol. 64, no. 9, Article ID 290311, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Schuld",
                "N. Killoran"
            ],
            "title": "Quantum machine learning in feature hilbert spaces",
            "venue": "Physical Review Letters, vol. 122, no. 4, Article ID 040504, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M. Jaderberg",
                "K. Simonyan",
                "A. Vedaldi",
                "A. Zisserman"
            ],
            "title": "Reading text in the wild with convolutional neural networks",
            "venue": "International Journal of Computer Vision, vol. 116, no. 1, pp. 1\u201320, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "N. Sharma",
                "V. Jain",
                "A. Mishra"
            ],
            "title": "An analysis of convolutional neural networks for image classification",
            "venue": "Procedia Computer Science, vol. 132, pp. 377\u2013384, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "P.M. Cheng",
                "H.S. Malhi"
            ],
            "title": "Transfer learning with convolutional neural networks for classification of abdominal ultrasound images",
            "venue": "Journal of Digital Imaging, vol. 30, no. 2, pp. 234\u2013243, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Q. Gao",
                "S. Lim",
                "X. Jia"
            ],
            "title": "Hyperspectral image classification using convolutional neural networks and multiple feature learning",
            "venue": "Remote Sensing, vol. 10, no. 2, p. 299, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "H.W. Wang",
                "Y.J. Xue",
                "Y.L. Ma",
                "N. Hua",
                "H.Y. Ma"
            ],
            "title": "Determination of quantum toric error correction code threshold using convolutional neural network decoders",
            "venue": "Chinese Physics B, vol. 31, no. 1, Article ID 010303, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Liu",
                "B. Tang",
                "Q. Chen",
                "X. Wang"
            ],
            "title": "Drug-drug interaction extraction via convolutional neural networks",
            "venue": "Computational and Mathematical Methods in Medicine, vol. 2016, Article ID 6918381, 8 pages, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "S. Wei",
                "Y. Chen",
                "Z. Zhou",
                "G. Long"
            ],
            "title": "A quantum convolutional neural network on NISQ devices",
            "venue": "AAPPS Bulletin, vol. 32, no. 1, p. 2, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Henderson",
                "S. Shakya",
                "S. Pradhan",
                "T. Cook"
            ],
            "title": "Quanvolutional neural networks: powering image recognition with quantum circuits",
            "venue": "Quantum Machine Intelligence, vol. 2, no. 1, p. 2, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "I. Cong",
                "S. Choi",
                "M.D. Lukin"
            ],
            "title": "Quantum convolutional neural networks",
            "venue": "Nature Physics, vol. 15, no. 12, pp. 1273\u20131278, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Li",
                "R.G. Zhou",
                "R. Xu",
                "J. Luo",
                "andW. Hu"
            ],
            "title": "A quantum deep convolutional neural network for image recognition",
            "venue": "Quantum Science and Technology, vol. 5, no. 4, Article ID 044003, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "R. Parthasarathy",
                "R.T. Bhowmik"
            ],
            "title": "Quantum optical convolutional neural network: a novel image recognition framework for quantum computing",
            "venue": "IEEE Access, vol. 9, pp. 103337\u2013103346, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Broughton",
                "G. Verdon",
                "T. McCourt"
            ],
            "title": "Tensorflow quantum: a software framework for quantum machine learning",
            "venue": "2020, https://arxiv.org/abs/2003.02989.",
            "year": 2020
        },
        {
            "authors": [
                "X.Z. Luo",
                "J.G. Liu",
                "P. Zhang",
                "L.Wang"
            ],
            "title": "Yao.jl: extensible, efficient framework for quantum algorithm design",
            "venue": "Quantum, vol. 4, p. 341, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: a method for stochastic optimization",
            "venue": "2014, https://arxiv.org/abs/1412.6980? context=cs#:%7E:text=We%20introduce%20Adam%2C% 20an%20algorithm,estimates%20of%20lower%2Dorder% 20moments.",
            "year": 2014
        },
        {
            "authors": [
                "W. Huggins",
                "P. Patil",
                "B. Mitchell",
                "K.B. Whaley",
                "E.M. Stoudenmire"
            ],
            "title": "Towards quantum machine learning with tensor networks",
            "venue": "Quantum Science and technology, vol. 4, no. 2, Article ID 024001, 2019. Quantum Engineering 9",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "Research Article",
            "text": "An Image Classification Algorithm Based on Hybrid Quantum"
        },
        {
            "heading": "Classical Convolutional Neural Network",
            "text": "Wei Li,1 Peng-Cheng Chu,2 Guang-Zhe Liu,2 Yan-Bing Tian,1 Tian-Hui Qiu ,2 and Shu-Mei Wang 2\n1School of Information and Control Engineering, Qingdao University of Technology, Qingdao, China 2School of Science, Qingdao University of Technology, Qingdao, China\nCorrespondence should be addressed to Tian-Hui Qiu; qiutianhui@qut.edu.cn and Shu-Mei Wang; wangshumei@qut.edu.cn\nReceived 21 April 2022; Revised 30 May 2022; Accepted 21 June 2022; Published 14 July 2022\nAcademic Editor: ShiJie Wei\nCopyright \u00a9 2022Wei Li et al. ,is is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\nQuantum machine learning is emerging as a strategy to solve real-world problems. As a quantum computing model, parameterized quantum circuits provide an approach for constructing quantum machine learning algorithms, which may either realize computational acceleration or achieve better algorithm performance than classical algorithms. Based on the parameterized quantum circuit, we propose a hybrid quantum-classical convolutional neural network (HQCCNN)model for image classification that comprises both quantum and classical components. ,e quantum convolutional layer is designed using a parameterized quantum circuit. It is used to perform linear unitary transformation on the quantum state to extract hidden information. In addition, the quantum pooling unit is used to perform pooling operations. After the evolution of the quantum system, wemeasure the quantum state and input the measurement results into a classical fully connected layer for further processing. We demonstrate its potential by applying HQCCNN to the MNISTdataset. Compared to a convolutional neural network in a similar architecture, the results reveal that HQCCNN has a faster training speed and higher testing set accuracy than a convolutional neural network."
        },
        {
            "heading": "1. Introduction",
            "text": "Quantum mechanics is a set of rules constructed by mathematical framework or physical theory. As a new computational framework for integrating the concepts of quantum mechanics, quantum computing [1,2] utilizes the properties of quantummechanics to process both quantum and classical information [3,4], which may cause an insurmountable gap between quantum and classical computers. As quantum technology is developing, noisy intermediate-scale quantum computers (NISQ) have begun to deal with some relatively complex computing tasks, and their computing power has surpassed classical computers in some tasks [5]. At this stage, quantum algorithms are dedicated to implementation on NISQ devices, which gives a new direction to some real classical computational problems.\nIn parallel to the recent advances in classical machine learning, interest in quantum machine learning has grown significantly in academia [6\u20139]. As a quantum computing\nmodel, parameterized quantum circuits (PQCs) have powerful parallel computing and expressive capabilities. Many researchers have constructed quantum machine learning algorithms using PQC. In 2014, Peruzzo et al. [10] proposed a variational quantum eigensolver (VQE) and realized it with a photon quantum processor and a classical computer. In 2016, McClean et al. [11] improved the variational quantum algorithm\u2019s optimization scheme. In 2020, Schuld et al. [12] used PQC to construct a circuit-centric quantum classifier. ,e simulation on the dataset showed that the quantum classifier had high performance. In 2022, Zeng et al. [13] proposed a hybrid quantum neural network model based on a ladder circuit structure and introduced a full measurement strategy. ,e model achieved stable and high accuracy in both binary and multiclassification tasks. Since PQC was first proposed, many quantum machine learning algorithms based on PQC have been proposed [14\u201320], and it has been proved that the quantum machine learning algorithm based on PQC has better expressive\nHindawi Quantum Engineering Volume 2022, Article ID 5701479, 9 pages https://doi.org/10.1155/2022/5701479\nperformance and computing ability than the classical algorithm [21,22].\nConvolutional neural networks (CNNs) have demonstrated their strong performance in deep learning tasks. Weight sharing and local receptive fields enable CNN to perform most tasks nowadays [23\u201328]. Furthermore, with the development of quantum information, quantum convolutional neural networks (QCNNs) have begun to interest researchers [29,30]. In 2019, inspired by classical CNN, Cong et al. [31] proposed a QCNN for phase classification and optimization of quantum error correction codes. ,e QCNN had a similar network architecture to CNN. In 2020, Li et al. [32] proposed a quantum deep convolutional neural network (QDCNN), which was a quantum version of classical convolution and achieved exponential computational acceleration. ,e model demonstrated the advantages of a quantum system over a classical system. In 2021, Parthasarathy et al. [33] proposed a quantum optical convolutional neural network that integrated quantum convolution and quantum optics. First, salient information was extracted using the linear unitary transformation of the quantum convolution part; subsequently, the quantum convolution results were input into the quantum optical neural network for further processing. ,e network exhibited good stability in terms of image recognition.\nCombining PQC with the classical neural network, we propose a hybrid quantum-classical model for image classification. ,e hybrid model not only utilizes the powerful performance of PQC but also inherits the characteristics of the classical neural network. ,e quantum convolutional layer is composed of quantum convolution kernels constructed by PQC. We transform the classic image into a quantum state and input it into the quantum convolutional layer.,e quantum convolutional layer performs the unitary transformation on the qubits corresponding to the convolution window. ,en, the quantum pooling layer composed of quantum pooling units performs pooling operations. ,e qubits are measured at specific locations to obtain the results of the evolution of the quantum system, and the results are input to a fully connected layer for further processing. In this study, we refer to this model as a hybrid quantum-classical convolutional neural network (HQCCNN); the simulation experiments show that HQCCNN exhibits strong learning ability and high image classification accuracy.\n,eorganization of the paper is as follows:CNNandPQC models are introduced in Section 2 briefly. ,e HQCCNN architectures and the algorithm execution process are described in Section 3. Section 4 describes simulation and analysis. Finally, the conclusion is drawn in Section 5."
        },
        {
            "heading": "2. Background",
            "text": "2.1. Convolutional Neural Network. A CNN consists of convolutional layers, pooling layers, and a fully connected layer that outputs the image\u2019s class. ,e convolution process is to convolve the convolution kernel with the image and then move the convolution kernel according to the step size (usually set to 1); the convolution kernel does the same operation.\n,e convolution operation can be defined as follows: suppose the input image A is the size of m \u00d7 n, the convolution kernel P is the size of w \u00d7 w, and the convolution operation can be completed through the equation\nf \ufffd \u03c3 Ai:i+w,j: j+wP . (1)\nHere, \u03c3 is the nonlinear function. In order to set the feature map\u2019s size to be the same as the input image, padding is usually used in the convolutional layer.\nAfter convolution, the feature map is input into the next convolutional layer or pooling layer for high-dimensional feature extraction or feature dimensionality reduction. ,e dimensionality reduction methods of the pooling layer include max pooling and average pooling. Finally, the image\u2019s class is obtained at the fully connected layer.,e architecture of CNN is illustrated in Figure 1.\n2.2. Parameterized Quantum Circuits. PQC is a quantum computing model, and it can be run on quantum computers. PQC usually consists of parametric unitary gates and nonparametric unitary gates. ,e parametric gate includes single-qubit rotation gate, and the three single-qubit rotation gates are defined as follows:\nRx(\u03b8) \ufffd\ncos \u03b8 2   \u2212i sin \u03b8 2  \n\u2212i sin \u03b8 2   cos \u03b8 2  \n\u23a1\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a3\n\u23a4\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a6 ,\nRy(\u03b8) \ufffd\ncos \u03b8 2   \u2212sin \u03b8 2  \nsin \u03b8 2   cos \u03b8 2  \n\u23a1\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a3\n\u23a4\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a6 ,\nRz(\u03b8) \ufffd e\n\u2212 i(\u03b8/2) 0\n0 ei(\u03b8/2) \u23a1\u23a2\u23a2\u23a2\u23a3 \u23a4\u23a5\u23a5\u23a5\u23a6.\n(2)\nAccording to the quantum circuit U(w), the parameterized circuit performs the unitary transformation of quantum state |x\u232a to produce the output quantum state |y\u232a, that is,\n|y\u232a \ufffd U(w)|x\u232a, (3)\nwhere w is the parameter in the circuit, such as the angle of the qubit rotation gate. PQC allows great freedom to design quantum algorithms, and it can realize different quantum algorithms by combining different quantum gates. ,e parameters of PQC are updated using the quantum-classical training method, as illustrated in Figure 2.\n,e quantum computer performs unitary transformation on the input quantum state according to the quantum circuits and then measures the output quantum state. ,e measurement results are input into a classical computer to\ncalculate the loss value. ,e quantum circuit parameters are optimized through multiple iterations of the classical computer until the expectation value is satisfied or the maximum number of iterations is reached."
        },
        {
            "heading": "3. HQCCNN",
            "text": "3.1. Preparing the Quantum State. When the number of qubits in a quantum system is limited and the quantum system is used to tackle classical problems, the dimensionality reduction processing of classical data is usually required first. ,is paper downsamples the image to m \u00d7 m size, scales the pixel value to [0, 1], and then flattens the image.\nWe get a 1 \u00d7 m2 vector x \ufffd [x1, x2 . . . xm2] after these operations and convert the vector to angle information \u03b1, that is,\n\u03b1 \ufffd \u03c0x, (4)\nwhere \u03b1 \ufffd [\u03b11, \u03b12 . . . \u03b1m2]. ,e angle information \u03b1 is taken as the rotation angle of the rotation gateRy, and the rotation gate Ry is applied to the initial quantum state for encoding, for a m2-input quantum system:\n\u03c6img  \u232a \ufffd \u2297 m2 i\ufffd1Ry \u03b1i( |0\u232a1 . . . |0\u232am2 . (5)\nHence, m \u00d7 m qubits are required to encode a downsampled image of size m \u00d7 m, and the quantum state |\u03c6img\u232a is obtained by encoding all pixels of the downsampled image.\n3.2. Quantum Convolutional Layer. After obtaining the quantum state |\u03c6img\u232a, the quantum convolution kernel u(\u03b8)\ndesigned by PQC is used to perform unitary transformation on |\u03c6img\u232a. ,e convolution kernel has 5 training parameters (\u03b8 \ufffd \u03b81, \u03b82, \u03b83, \u03b84, \u03b85), as shown in the circuit structure in the purple window in Figure 3.\n,e circuit in Figure 3 shows a convolutional layer circuit with an image size of 2 \u00d7 3. We use the convolution kernel to perform unitary transformation on the qubits corresponding to the convolution window. It should be noted that the quantum convolution window is consistent with the classical convolution window, but it corresponds to 4 qubits. ,e purpose of the convolution window repeatedly acting on the 4 qubits is to preserve the characteristics of the convolution as much as possible and extract the hidden information from the quantum state.\n3.3. Quantum Pooling Layer. In this study, the quantum pooling unitary gate V composed of 3 CNOTgates is used to reduce the dimension of the convolution results, as shown in the brown square in Figure 4.,e pooling window acts in the same position as the convolution window, and after pooling, the convolution result of a convolution window is mapped to a qubit. ,erefore, we only measure the specific qubit to obtain the expected value.\nClassical CNN introduces nonlinearity through the nonlinear functions. In a quantum system, we introduce nonlinearity through measurement. After the evolution of the quantum system to the quantum state, the final quantum state |\u03c6out\u232a is obtained. We perform Z-based measurement on the state |\u03c6out\u232a to obtain the expectation value, that is,\nE \ufffd\u2329\u03c6img U \u2020 (\u03b8)V\u2020 Z1, . . . , ZN( VU(\u03b8) \u03c6img  \u232a, (6)\nwhere (Z1, . . . , ZN) is a vector of Z operators acting on different qubits, V is the parameter-free unitary gate in the pooling layer, U(\u03b8) \ufffd u1(\u03b8)u2(\u03b8) . . . ul(\u03b8), l is the number of convolutions in a convolutional layer, where for an image of m \u00d7 m, l \ufffd (m \u2212 1)2, and the pooling unit also performs (m \u2212 1)2 times in a pooling layer. If we directly measure the output of the quantum convolutional layer, we get a quantum output E with dimension 1 \u00d7 m2; if we measure the output of the quantum pooling layer, we get a vector E with dimension 1 \u00d7 (m \u2212 1)2. E is a vector composed of Z expectation values of different qubits, which is not directly related to the image label, so the E should be input into the classical fully connected layer for further processing.\n3.4. Generating Hybrid Network. HQCCNN consists of a quantum convolutional layer, a quantum pooling layer, and a classical fully connected layer. As shown in Figure 5, the quantum convolutional layer consists of multiple convolution kernels, which complete the quantum convolution to obtain the feature map.,e convolution kernel in Figure 5 is the convolution structure introduced in Section 3.2, but the parameters of different convolution kernels are different. ,e convolution results are reduced by the quantum pooling layer, and the quantum pooling unit is the quantum pooling structure introduced in Section 3.3. ,en, measuring the specific qubits, the measurement results are input into the fully connected layer to obtain the image\u2019s class.\n3.5. Algorithm Execution Process. One of the reasons for the success of the classical neural network is that it has a developed backpropagation algorithm.We rely on the quantum computing framework TensorFlow Quantum [34] to complete the numerical simulation on a classical computer. TensorFlow Quantum implements a backpropagation algorithmthat canbeused inhybridquantum-classical structures. Weuse the adjoint differentiationmethod to obtain gradients of the quantum system,which is described in detail in [34,35]. ,rough the abovementioned definition of the HQCCNN, the image classification algorithm is shown in Algorithm 1."
        },
        {
            "heading": "4. Simulation and Analysis",
            "text": "In this section, we use the MNISTdataset for simulation and analysis of HQCCNN. First, for each class in the MNIST dataset, 200 training images and 50 testing images are\nrandomly selected. ,erefore, in this paper, a total of 2000 images in the training set and 500 images in the testing set are used. However, the size of each image in the MNIST dataset is 28 \u00d7 28, which is not realistic for the current quantum experimental simulation, so we downsample each image to the size of 4 \u00d7 4.\n,e experimental setup in this section is as follows: first, we test all binary classification subsets in the MNISTdataset, then analyze the HQCCNN\u2019s structural changes on classification performance, and finally compare them with CNNs of the same architecture. And, all experiment details are set as follows: we use a classical hidden layer when performing binary classification tasks, and two hidden layers are used when performing multiple classification tasks. ,e hidden layers use ReLu nonlinear function, and the classic optimizer Adam [36] is used to optimize the parameters, the batch size is set to 32, the learning rate is 0.01, and 100 epochs are set.\n4.1. Evaluation Indicators. To more intuitively evaluate the performance of HQCCNN, this study uses training set loss and testing set accuracy as evaluation indicators. Using the cross-entropy function as the loss function, the cross-entropy function is defined as follows:\nH \ufffd \u2212 1\nM \nM\ni\nyilog pi( , (7)\nwhere M is the size of the training set, yi is the one-hot vector of the i-th sample in the training set, and pi is the predicted probability vector by the model. Accuracy is defined as\nAcc \ufffd 1 N \nN i\ufffd1 argmaxpi \ufffd\ufffd yi( , (8)\nwhere N is the size of the testing set, yi is the true class of the i-th sample, and the argmax function outputs the maximum value\u2019s position in a vector.\n4.2. Binary Classification. In this section, we classify all binary subsets of MNISTandHQCCNN sets 1 convolutional layer, with 1 convolution kernel, and no pooling layer. ,e classical output layer uses a sigmoid nonlinear activation function. ,e binary classification results are shown in Figure 6.\nAmong all 45 binary classification tasks, HQCCNN shows high classification accuracy.,emaximum appears in the classification task of digits 4 and 5, reaching 99.80%. ,e minimum value of 91.51% appears in the classification task of digits 3 and 8. It shows that HQCCNN is perfectly\nqualified for the binary classification task. However, it can also be seen in the classification results that when other digits are classified with 5 or 8, the probability of lower accuracy is higher.,e reason for this difference may be that after image downsampling, some image features are lost, which increases the similarity between images.\nReference [37] proposes a quantum tensor network model. We compare some binary classification tasks with reference [37]. Reference [37] uses a model with 1008 parameters to test all binary classification tasks, while we use 150 parameters. ,e result is shown in Figure 7; it can be clearly seen from the figure that in the binary classification subsets \\{0, 9\\}, \\{1, 7\\}, \\{2, 9\\}, \\{4, 6\\}, \\{6, 7\\}, and \\{8, 9\\}, the accuracy of our model is better than in reference [37]."
        },
        {
            "heading": "4.3. 5e Influence of Convolution Kernel on HQCCNN.",
            "text": "Next, we discuss the influence of the convolution kernel\u2019s number on HQCCNN. 2 HQCCNNs with 1 quantum convolutional layer are set, and the number of convolution\nInput: classical image dataset 1 preparing quantum state, the training set |\u03c6\u232atrain and the testing set |\u03c6\u232atest are obtained. 2 constructing hybrid quantum-classical convolutional neural network 3 setting the values for batchsize, epoch, and learning rate 4 for each epoch do 5 for each batchsize do 6 quantum system perform unitary transformation U(\u03b8)V|\u03c6img\u232a, 7 measuring, inputting E into the classical fully connected layer for further processing, 8 calculating the loss value and gradient, updating the parameters based on the gradient and learning rate. 9 end for 10 validating testing set |\u03c6\u232atest 11 end for\nOutput: each image\u2019s class\nALGORITHM 1: HQCCNN classification algorithm.\nkernels is 2 and 4, respectively.,e fully connected layers are the same, and the output layer uses the softmax nonlinear activation function. We set a multiclassification task of digits 0\u20134. ,e results are plotted in Figure 8.\nAccording to Figure 8, the HQCCNN with 4 convolution kernels achieves a smaller training loss value, and at the beginning of the epoch, it achieves a higher classification accuracy than the HQCCNNwith 2 convolution kernels. However, with the increase in the epoch, the accuracy of the 2 architectures is almost the same, indicating that the HQCCNN with 2 convolution kernels already has high learning ability and image classification ability for the 5-class classification task.\n4.4. 5e Influence of Pooling Layer on HQCCNN. To explore the influence of adding a pooling layer on the performance of HQCCNN, we construct a HQCCNN with a pooling\nlayer. HQCCNN sets 1 convolutional layer with 2 convolution kernels. After adding a pooling layer, the convolution results are pooled and then input into the same classical fully connected layer.We set a classification task of digits 0\u20134.,e training set loss and testing set accuracy of HQCCNN are plotted in Figure 9.\nWhen adding a pooling layer, compared to HQCCNN without adding a pooling layer, the classification performance of HQCCNN is degraded. However, it still maintains high performance, indicating that the pooling unit used in this paper can keep the model with high learning ability and image classification ability.\n4.5. Comparative Experiment of HQCCNN and CNN. We explore the difference in classification performance between HQCCNN and CNN. We take the 5-class (0\u20134) task as an\nexample to illustrate the curve change in the numerical results of HQCCNN and CNN and set up other classification tasks to illustrate the performance differences. We set HQCCNN and CNN with the same network architecture, which means that the number of parameters between HQCCNN and CNN is the same. HQCCNN sets 1 convolutional layer with 2 convolution kernels and sets 2 architectures, one without pooling layer and another with a pooling layer. ,e corresponding CNN sets 1 convolutional layer with 2 convolution kernels, the convolutional layer with padding 1, the convolution kernel size is 2 \u00d7 2, with step size 1, and the activation function is ReLu. In the pooling layer, max pooling is used, and without padding, kernel size is 2 \u00d7 2, with step size 1. ,e classical fully connected layers are the same. ,e training set loss and testing set accuracy are plotted in Figure 10.\nNext, we set up two complex binary classification tasks. ,e numerical results of HQCCNN and CNN are shown in Table 1.\nAs can be seen from Figure 10, without pooling layer, HQCCNN and CNN have almost the same loss value. However, HQCCNN has higher testing set accuracy than CNN. After adding a pooling layer, HQCCNN has a faster convergence speed and higher testing set accuracy than CNN. From Table 1, for complex binary classification tasks, HQCCNN also shows strong competitiveness. Figure 10 and Table 1 also show that the hybrid quantum-classical model has better learning ability and classification performance than CNN."
        },
        {
            "heading": "5. Conclusion",
            "text": ",e development of quantum computing mode has made it possible to use quantum computing paradigm to solve classical problems. However, because of the limited number of available qubits in quantum computers at this stage, dealing with high-dimensional data is difficult. ,us, only the dimensionality of the data can be reduced, such as image downsampling. Although image downsampling is a compromise for current quantum devices, it makes it difficult to classify images because several images may become similar. Image downsampling makes the features disappear or become blur, thereby increasing uncertainty in the classification process. Amplitude encoding is a more efficient dataencoding approach, but it will result in intractable complexity. With researchers\u2019 efforts, the advent of large-scale quantum computers will facilitate the handling of highdimensional data, which will additionally enable them to handle more classical problems.\nIn conclusion, we propose a hybrid quantum-classical convolutional computing model that includes two parts: a quantum computing part and a classical computing part. ,e quantum computing part includes quantum convolutional layer and quantum pooling layer. ,e quantum convolutional layer has a parameterized quantum circuit design to achieve the linear unitary transformation of the quantum state. ,e potential of quantum computing can be harnessed using parameterized quantum circuits. Classical computing is used to map the expectation value of the quantum system further, making it related to the image label. In the experimental simulation, we classify all the binary subsets of MNIST dataset and achieve better performance. Furthermore, we discuss the impact of model architecture changes on HQCCNN and conduct comparative experiments using CNNs with the same architecture. ,e results show that compared to CNN,HQCCNNhas a faster training speed and higher testing set accuracy."
        },
        {
            "heading": "Data Availability",
            "text": "Data are available on request to the corresponding author."
        },
        {
            "heading": "Conflicts of Interest",
            "text": ",e authors declare that they have no conflicts of interest."
        },
        {
            "heading": "Acknowledgments",
            "text": ",is work was supported by the National Natural Science Foundation of China (Grant No. 61772295), Natural Science Foundation of Shandong Province, China(Grant Nos. ZR2021MF049 and ZR2019YQ01), and Project of Shandong Provincial Natural Science Foundation Joint Fund Application (ZR202108020011)."
        }
    ],
    "title": "An Image Classification Algorithm Based on Hybrid Quantum Classical Convolutional Neural Network",
    "year": 2022
}