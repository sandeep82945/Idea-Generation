{
    "abstractText": "We study robustness to test-time adversarial attacks in the regression setting with lp losses and arbitrary perturbation sets. We address the question of which function classes are PAC learnable in this setting. We show that classes of finite fat-shattering dimension are learnable in both realizable and agnostic settings. Moreover, for convex function classes, they are even properly learnable. In contrast, some non-convex function classes provably require improper learning algorithms. Our main technique is based on a construction of an adversarially robust sample compression scheme of a size determined by the fat-shattering dimension. Along the way, we introduce a novel agnostic sample compression scheme for real-valued functions, which may be of independent interest.",
    "authors": [
        {
            "affiliations": [],
            "name": "Idan Attias"
        },
        {
            "affiliations": [],
            "name": "Steve Hanneke"
        }
    ],
    "id": "SP:669388ee25016eb612f33a5ba4ccc04c6b0795fd",
    "references": [
        {
            "authors": [
                "Naman Agarwal",
                "Brian Bullins",
                "Elad Hazan",
                "Sham Kakade",
                "Karan Singh"
            ],
            "title": "Online control with adversarial disturbances",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Noga Alon",
                "Shai Ben-David",
                "Nicolo Cesa-Bianchi",
                "David Haussler"
            ],
            "title": "Scale-sensitive dimensions, uniform convergence, and learnability",
            "venue": "Journal of the ACM (JACM),",
            "year": 1997
        },
        {
            "authors": [
                "Oren Anava",
                "Elad Hazan",
                "Shie Mannor",
                "Ohad Shamir"
            ],
            "title": "Online learning for time series prediction",
            "venue": "In Conference on learning theory,",
            "year": 2013
        },
        {
            "authors": [
                "Dana Angluin"
            ],
            "title": "Queries and concept learning",
            "venue": "Machine learning,",
            "year": 1988
        },
        {
            "authors": [
                "Martin Anthony",
                "Peter L Bartlett"
            ],
            "title": "Function learning from interpolation",
            "venue": "Combinatorics, Probability and Computing,",
            "year": 2000
        },
        {
            "authors": [
                "Martin Anthony",
                "Peter L Bartlett"
            ],
            "title": "Neural network learning: Theoretical foundations, volume 9. cambridge university press",
            "year": 1999
        },
        {
            "authors": [
                "Hassan Ashtiani",
                "Vinayak Pathak",
                "Ruth Urner"
            ],
            "title": "Black-box certification and learning under adversarial perturbations",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Idan Attias",
                "Aryeh Kontorovich",
                "Yishay Mansour"
            ],
            "title": "Improved generalization bounds for robust learning. In Algorithmic Learning Theory, pages 162\u2013183",
            "venue": "PMLR, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Idan Attias",
                "Steve Hanneke",
                "Yishay Mansour"
            ],
            "title": "A characterization of semi-supervised adversarially-robust pac learnability",
            "venue": "arXiv preprint arXiv:2202.05420,",
            "year": 2022
        },
        {
            "authors": [
                "Pranjal Awasthi",
                "Natalie Frank",
                "Mehryar Mohri"
            ],
            "title": "Adversarial learning guarantees for linear hypotheses and neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Pranjal Awasthi",
                "Natalie Frank",
                "Anqi Mao",
                "Mehryar Mohri",
                "Yutao Zhong"
            ],
            "title": "Calibration and consistency of adversarial surrogate losses",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Pranjal Awasthi",
                "Natalie Frank",
                "Mehryar Mohri"
            ],
            "title": "On the existence of the adversarial bayes classifier",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Pranjal Awasthi",
                "Anqi Mao",
                "Mehryar Mohri",
                "Yutao Zhong"
            ],
            "title": "H-consistency bounds for surrogate loss minimizers",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Pranjal Awasthi",
                "Anqi Mao",
                "Mehryar Mohri",
                "Yutao Zhong"
            ],
            "title": "Multi-class h-consistency bounds",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Pranjal Awasthi",
                "Anqi Mao",
                "Mehryar Mohri",
                "Yutao Zhong"
            ],
            "title": "Theoretically grounded loss functions and algorithms for adversarial robustness",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "Peter L Bartlett",
                "Philip M Long"
            ],
            "title": "Prediction, learning, uniform convergence, and scale-sensitive dimensions",
            "venue": "Journal of Computer and System Sciences,",
            "year": 1998
        },
        {
            "authors": [
                "Robi Bhattacharjee",
                "Somesh Jha",
                "Kamalika Chaudhuri"
            ],
            "title": "Sample complexity of robust linear classification on separated data",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Battista Biggio",
                "Igino Corona",
                "Davide Maiorca",
                "Blaine Nelson",
                "Nedim \u0160rndi\u0107",
                "Pavel Laskov",
                "Giorgio Giacinto",
                "Fabio Roli"
            ],
            "title": "Evasion attacks against machine learning at test time",
            "venue": "In Joint European conference on machine learning and knowledge discovery in databases,",
            "year": 2013
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Yin Tat Lee",
                "Eric Price",
                "Ilya Razenshteyn"
            ],
            "title": "Adversarial examples from computational constraints",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Emmanuel Candes",
                "Benjamin Recht"
            ],
            "title": "Exact matrix completion via convex optimization",
            "venue": "Communications of the ACM,",
            "year": 2012
        },
        {
            "authors": [
                "Daniel Cullina",
                "Arjun Nitin Bhagoji",
                "Prateek Mittal"
            ],
            "title": "Pac-learning in the presence of adversaries",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Chen Dan",
                "Yuting Wei",
                "Pradeep Ravikumar"
            ],
            "title": "Sharp statistical guaratees for adversarially robust gaussian classification",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Amit Daniely",
                "Shai Shalev-Shwartz"
            ],
            "title": "Optimal learners for multiclass problems",
            "venue": "In Conference on Learning Theory,",
            "year": 2014
        },
        {
            "authors": [
                "Amit Daniely",
                "Sivan Sabato",
                "Shai Ben-David",
                "Shai Shalev-Shwartz"
            ],
            "title": "Multiclass learnability and the erm principle",
            "venue": "In Proceedings of the 24th Annual Conference on Learning Theory,",
            "year": 2011
        },
        {
            "authors": [
                "Ofir David",
                "Shay Moran",
                "Amir Yehudayoff"
            ],
            "title": "Supervised learning through the lens of compression",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Dimitrios Diochnos",
                "Saeed Mahloujifar",
                "Mohammad Mahmoody"
            ],
            "title": "Adversarial risk and robustness: General definitions and implications for the uniform distribution",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Richard M Dudley"
            ],
            "title": "A course on empirical processes",
            "venue": "In Ecole d\u2019e\u0301te\u0301 de Probabilite\u0301s de Saint-Flour XII-1982,",
            "year": 1984
        },
        {
            "authors": [
                "Nigel Duffy",
                "David Helmbold"
            ],
            "title": "Boosting methods for regression",
            "venue": "Machine Learning,",
            "year": 2002
        },
        {
            "authors": [
                "Uriel Feige",
                "Yishay Mansour",
                "Robert Schapire"
            ],
            "title": "Learning and inference in the presence of corrupted inputs",
            "venue": "In Conference on Learning Theory,",
            "year": 2015
        },
        {
            "authors": [
                "Sally Floyd",
                "Manfred Warmuth"
            ],
            "title": "Sample compression, learnability, and the vapnik-chervonenkis dimension",
            "venue": "Machine learning,",
            "year": 1995
        },
        {
            "authors": [
                "Yoav Freund",
                "Robert E Schapire"
            ],
            "title": "A decision-theoretic generalization of on-line learning and an application to boosting",
            "venue": "Journal of computer and system sciences,",
            "year": 1997
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "arXiv preprint arXiv:1412.6572,",
            "year": 2014
        },
        {
            "authors": [
                "Pascale Gourdeau",
                "Varun Kanade",
                "Marta Kwiatkowska",
                "James Worrell"
            ],
            "title": "On the hardness of robust classification",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Thore Graepel",
                "Ralf Herbrich",
                "John Shawe-Taylor"
            ],
            "title": "Pac-bayesian compression bounds on the prediction error of learning algorithms for classification",
            "venue": "Machine Learning,",
            "year": 2005
        },
        {
            "authors": [
                "Steve Hanneke"
            ],
            "title": "The optimal sample complexity of pac learning",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2016
        },
        {
            "authors": [
                "Elad Hazan",
                "Tengyu Ma"
            ],
            "title": "A non-generative framework and convex relaxations for unsupervised learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Elad Hazan",
                "Satyen Kale",
                "Shai Shalev-Shwartz"
            ],
            "title": "Near-optimal algorithms for online matrix prediction",
            "venue": "In Conference on Learning Theory,",
            "year": 2012
        },
        {
            "authors": [
                "Elad Hazan",
                "Roi Livni",
                "Yishay Mansour"
            ],
            "title": "Classification with low rank and missing data",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "Michael J Kearns",
                "Robert E Schapire"
            ],
            "title": "Efficient distribution-free learning of probabilistic concepts",
            "venue": "Journal of Computer and System Sciences,",
            "year": 1994
        },
        {
            "authors": [
                "Bal\u00e1zs K\u00e9gl"
            ],
            "title": "Robust regression by boosting the median",
            "venue": "In Learning Theory and Kernel Machines,",
            "year": 2003
        },
        {
            "authors": [
                "Justin Khim",
                "Po-Ling Loh"
            ],
            "title": "Adversarial risk bounds via function transformation",
            "venue": "arXiv preprint arXiv:1810.09519,",
            "year": 2018
        },
        {
            "authors": [
                "Pieter Kleer",
                "Hans Simon"
            ],
            "title": "Primal and dual combinatorial dimensions",
            "venue": "arXiv preprint arXiv:2108.10037,",
            "year": 2021
        },
        {
            "authors": [
                "Aryeh Kontorovich",
                "Idan Attias"
            ],
            "title": "Fat-shattering dimension of k-fold maxima",
            "venue": "arXiv preprint arXiv:2110.04763,",
            "year": 2021
        },
        {
            "authors": [
                "Nick Littlestone",
                "Manfred Warmuth"
            ],
            "title": "Relating data compression and learnability",
            "year": 1986
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "arXiv preprint arXiv:1706.06083,",
            "year": 2017
        },
        {
            "authors": [
                "Anqi Mao",
                "Mehryar Mohri",
                "Yutao Zhong"
            ],
            "title": "Cross-entropy loss functions: Theoretical analysis and applications",
            "venue": "arXiv preprint arXiv:2304.07288,",
            "year": 2023
        },
        {
            "authors": [
                "Shahar Mendelson"
            ],
            "title": "An optimal unrestricted learning procedure",
            "venue": "Journal of the ACM,",
            "year": 2019
        },
        {
            "authors": [
                "Omar Montasser",
                "Steve Hanneke",
                "Nathan Srebro"
            ],
            "title": "Vc classes are adversarially robustly learnable, but only improperly",
            "venue": "arXiv preprint arXiv:1902.04217,",
            "year": 2019
        },
        {
            "authors": [
                "Omar Montasser",
                "Surbhi Goel",
                "Ilias Diakonikolas",
                "Nathan Srebro"
            ],
            "title": "Efficiently learning adversarially robust halfspaces with noise",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Omar Montasser",
                "Steve Hanneke",
                "Nati Srebro"
            ],
            "title": "Reducing adversarially robust learning to nonrobust pac learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Omar Montasser",
                "Steve Hanneke",
                "Nathan Srebro"
            ],
            "title": "Adversarially robust learning with unknown perturbation sets",
            "venue": "In Conference on Learning Theory,",
            "year": 2021
        },
        {
            "authors": [
                "Omar Montasser",
                "Steve Hanneke",
                "Nathan Srebro"
            ],
            "title": "Transductive robust learning guarantees",
            "venue": "arXiv preprint arXiv:2110.10602,",
            "year": 2021
        },
        {
            "authors": [
                "David Pollard"
            ],
            "title": "Convergence of stochastic processes",
            "venue": "Springer Science & Business Media,",
            "year": 2012
        },
        {
            "authors": [
                "Mark Rudelson",
                "Roman Vershynin"
            ],
            "title": "Combinatorics of random processes and sections of convex bodies",
            "venue": "Annals of Mathematics,",
            "year": 2006
        },
        {
            "authors": [
                "Ludwig Schmidt",
                "Shibani Santurkar",
                "Dimitris Tsipras",
                "Kunal Talwar",
                "Aleksander Madry"
            ],
            "title": "Adversarially robust generalization requires more data",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Hans Ulrich Simon"
            ],
            "title": "Bounds on the number of examples needed for learning functions",
            "venue": "SIAM Journal on Computing,",
            "year": 1997
        },
        {
            "authors": [
                "Nathan Srebro",
                "Jason Rennie",
                "Tommi Jaakkola"
            ],
            "title": "Maximum-margin matrix factorization",
            "venue": "Advances in neural information processing systems,",
            "year": 2004
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wojciech Zaremba",
                "Ilya Sutskever",
                "Joan Bruna",
                "Dumitru Erhan",
                "Ian Goodfellow",
                "Rob Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "arXiv preprint arXiv:1312.6199,",
            "year": 2013
        },
        {
            "authors": [
                "Yue Xing",
                "Ruizhi Zhang",
                "Guang Cheng"
            ],
            "title": "Adversarially robust estimate and risk analysis in linear regression",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n20 6.\n12 97\n7v 2\n[ cs\n.L G\n] 7\nJ un\n2 02\n3\nWe study robustness to test-time adversarial attacks in the regression setting with \u2113p losses and\nthis setting. We show that classes of finite fat-shattering dimension are learnable in both realizable\nand agnostic settings. Moreover, for convex function classes, they are even properly learnable. In\ncontrast, some non-convex function classes provably require improper learning algorithms. Our main\ntechnique is based on a construction of an adversarially robust sample compression scheme of a size\ndetermined by the fat-shattering dimension. Along the way, we introduce a novel agnostic sample\ncompression scheme for real-valued functions, which may be of independent interest."
        },
        {
            "heading": "1 Introduction",
            "text": "Learning a predictor that is resilient to test-time adversarial attacks is a fundamental problem in contemporary machine learning. A long line of research has studied the vulnerability of deep learning-based models to small perturbations of their inputs (e.g., Szegedy et al. [60], Biggio et al. [18], Goodfellow et al. [32], Madry et al. [46]). From the theoretical standpoint, there has been a lot of effort to provide provable guarantees of such methods (e.g., Feige et al. [29], Schmidt et al. [57], Khim and Loh [42], Yin et al. [62], Cullina et al. [21], Attias et al. [8, 9], Montasser et al. [53, 50, 51, 52], Ashtiani et al. [7], Dan et al. [22], Awasthi et al. [10, 12, 11, 13, 14, 15], Bhattacharjee et al. [17], Xing et al. [61], Mao et al. [47]), which is the focus of this work.\nIn the robust PAC learning framework, the problem of learning binary function classes was studied by Montasser et al. [49]. They showed that uniform convergence does not hold in this setting, and as a result, robust empirical risk minimization is not sufficient to ensure learnability. Yet, they showed that VC classes are learnable, by considering an improper learning rule; the learning algorithm outputs a function that is not in the function class that we aim to learn.\nIn this work, we provide a theoretical understanding of the robustness of real-valued predictors in the PAC learning model, with arbitrary perturbation sets. The work of Attias et al. [8] considered this question for finite perturbation sets, they obtained sample complexity guarantees based on uniform convergence, which is no longer true for arbitrary perturbation sets. We address the fundamental question, which real-valued function classes are robustly learnable?\nFurthermore, we study the robust learnability of convex classes, a natural and commonly studied subcategory for regression. We address the question, are real-valued convex classes properly robustly learnable? On the one hand, some non-convex function classes provably require improper learning due to Montasser et al. [49]. On the other hand, Mendelson [48] showed that non-robust regression with the mean squared error is properly learnable.\nWe study the following learning models for real-valued functions. An adversarial attack is formalized by a perturbation function U : X \u2192 2X , where U(x) is the set of possible perturbations (attacks) on x. In practice, we usually consider U(x) to be the \u21131 ball centered at x. In this work, we have no restriction on U , besides x \u2208 U(x). Let D be an unknown distribution over X \u00d7 [0, 1] and let H \u2286 [0, 1]X be a concept class. In our first model, the robust error of concept h is defined as\nErr\u2113p(h;D) = E(x,y)\u223cD [\nsup z\u2208U(x)\n|h(z)\u2212 y|p ] , 1 \u2264 p <\u221e.\n\u2217Department of Computer Science, Ben-Gurion University; idanatti@post.bgu.ac.il. \u2020Department of Computer Science, Purdue University; steve.hanneke@gmail.com.\nThe learner gets an i.i.d. sample from D, and would like to output function h\u0302, such that with high probability,\nErr\u2113p(h\u0302;D) \u2264 inf h\u2208H Err\u2113p(h;D) + \u01eb. (1)\nThe sample complexity for learning H is the minimal i.i.d. sample from D such that there exists a learning algorithm with output as in Eq. (1). We refer to this model as Robust Regression with \u2113p robust losses. This is a robust formulation of the classic nonparametric regression setting.\nIn the second model, the robust error of concept h is defined as\nErr\u03b7(h;D) = E(x,y)\u223cD [ I { sup\nz\u2208U(x)\n|h(z)\u2212 y| \u2265 \u03b7 }] .\nWe refer to the loss function in this model as cutoff loss, where \u03b7 \u2265 0 is a predefined cutoff parameter. The learner gets an i.i.d. sample from D, and would like to output function h\u0302, such that with high probability,\nErr\u03b7+\u03b2(h\u0302;D) \u2264 inf h\u2208H Err\u03b7(h;D) + \u01eb,\nwhere \u03b2 > 0 is a predefined parameter. The sample complexity is defined similarly to the previous model. We refer to this model as Robust (\u03b7, \u03b2)-Regression. The non-robust formulation of this setting was studied by, e.g., Anthony and Bartlett [5], Simon [58]. See also Anthony et al. [6, section 21.4] and references therein.\nMain results and roadmap. Denote the \u03b3-fat-shattering dimension of H by fat(H, \u03b3), and the dual \u03b3-fat-shattering dimension by fat\u2217(H, \u03b3), which is the dimension of the dual class. The dimension of the dual class is finite as long as the \u03b3-fat-shattering of the primal class is finite (see Kleer and Simon [43], and Eq. (7)).\n\u2022 In Section 3 we provide a learning algorithm for robust regression with \u2113p losses, with sample complexity\nO\u0303 ( fat3(H, \u01eb/p) fat\u2217(H, \u01eb/p)\n\u01eb5\n) .\nMoreover, this algorithm is proper for convex function classes. We circumvent a negative result regarding non-convex function classes, for which proper learning is impossible, even for binary-valued functions [49].\n\u2022 In Section 4 we provide a learning algorithm with a substantial sample complexity improvement for the \u21131 loss,\nO\u0303 ( fat(H, \u01eb) fat\u2217(H, \u01eb)\n\u01eb2\n) .\n\u2022 In Section 5, we provide learning algorithms for the (\u03b7, \u03b2)-robust regression setting in the realizable and agnostic settings. Our sample complexity for the realizable case is\nO\u0303 ( fat(H, \u03b2) fat\u2217(H, \u03b2)\n\u01eb\n) ,\nand\nO\u0303 ( fat(H, \u03b2) fat\u2217(H, \u03b2)\n\u01eb2\n)\nfor the agnostic case.\nTechnical contributions and related work. The setting of agnostic adversarially robust regression with finite perturbation sets was studied by Attias et al. [8]. Subsequently, improved bounds appeared in Kontorovich and Attias [44]. Adversarially robust PAC learnability of binary-valued function classes with arbitrary perturbation sets was studied by Montasser et al. [49]. They showed that uniform convergence does not hold in this setting, which means that some classes provably require improper learning. Their main technique is constructing a sample compression scheme from a boosting-style algorithm, where the generalization follows from sample compression bounds.\nFirst, we explain our new technical ideas behind the algorithms for robust (\u03b7, \u03b2)-regression, and compare it to the ones of Montasser et al. [49] in the classification setting. We then explain why the approach for learning these models fails in the general robust regression setting and introduce the new ingredients behind the proofs for this setting.\nRobust (\u03b7, \u03b2)-regression. We construct an adversarially robust sample compression scheme of a size determined by the fat-shattering dimension of the function class. The following steps are different from the binary-valued case. First, we use a modified boosting algorithm for real-valued functions. In the nonrobust setting, Hanneke et al. [36] showed how to convert a boosting algorithm (originally introduced by K\u00e9gl [41]), into a sample compression scheme. In order to find weak learners (and prove their existence), we rely on generalization from approximate interpolation (see Anthony and Bartlett [5] and Anthony and Bartlett [5, section 21.4]). The idea is that any function f \u2208 F that approximately interpolates a sample S \u223c Dm, that is, |f(x) \u2212 y| \u2264 \u03b7 for (x, y) \u2208 S, also satisfies that P{(x, y) : |f(x)\u2212 y| \u2264 \u03b7 + \u03b2} > 1 \u2212 \u01eb with high probability, as long as O\u0303(fat(F , \u03b2)/\u01eb) \u2264 |S|. Crucially, this result relies on uniform convergence and does not apply to the robust loss function. Another difference is in the discretization step. In the classification setting, we inflate the data set to include all possible perturbations (potentially infinite set). We then define a function class H\u0302 by running a robust empirical minimizer on every subset of size VC(H) from the training set, where H is the class we want to learn. H\u0302 induces a finite partition on the inflated set into regions, such that any h \u2208 H\u0302 has a constant error in each region. This is no longer true in the real-valued case. Instead, we discretize the inflated set by taking a uniform cover using the supremum metric and controlling the errors that arise from the cover.\nRobust regression. We first explain which natural techniques fail. We cannot run boosting for the \u2113p loss as explained by Hanneke et al. [36]: \"Duffy and Helmbold [28, Remark 2.1] spell out a central technical challenge: no boosting algorithm can always force the base regressor to output a useful function by simply modifying the distribution over the sample. This is because unlike a binary classifier, which localizes errors on specific examples, a real-valued hypothesis can spread its error evenly over the entire sample and it will not be affected by reweighting\".\nAs a first attempt, we could try to learn with respect to the cutoff loss (with a fixed cutoff parameter), and conclude learnability in the general regression setting. However, the \u2113p loss can spread over different values for different points, which means that this approach fails. In another possible attempt, we could try to solve the realizable case first and try to reduce agnostic to realizable learning as in Montasser et al. [49] for binary-valued functions, as we prove the agnostic setting for robust (\u03b7, \u03b2)-regression. However, this attempt fails for the same reasons we mentioned above.\nTherefore, we introduce a novel technique for working with changing cutoffs. We establish generalization from approximate interpolation with different cutoff parameters, and thereby, we find a learner that approximates the loss of the target function on different points. Utilizing this idea, we provide a learning algorithm for \u2113p robust losses that constructs an ensemble and predict with the average. Further, we show that this algorithm is proper for convex function classes. In contrast, some non-convex function classes provably require improper learning [49]. Moreover, we show how to reduce the sample complexity substantially for the \u21131 robust loss with a different algorithm, by constructing an ensemble of weak learners and predicting with the median. Both algorithms can be represented as an agnostic sample compression scheme for the robust loss. This is a new result since constructing a sample compression scheme for real-valued functions is known only for the realizable setting [36]. We believe that this technique may be of independent interest."
        },
        {
            "heading": "2 Problem Setup and Preliminaries",
            "text": "Let H \u2286 [0, 1]X be a concept class. We implicitly assume that all concept classes are satisfying mild measure-theoretic conditions (see e.g., Dudley [27, section 10.3.1] and Pollard [54, appendix C]). Let D be a distribution over X \u00d7 Y, where Y = [0, 1]. Define a perturbation function U : X \u2192 2X that maps an input to an arbitrary set U(x) \u2286 X , such that x \u2208 U(x).\nWe consider the following loss functions. For 1 \u2264 p < \u221e, define the \u2113p robust loss function of h on (x, y) with respect to a perturbation function U ,\n\u2113p,U (h; (x, y)) = sup z\u2208U(x)\n|h(z)\u2212 y|p . (2)\nWe define also the \u03b7-ball robust loss function of h on (x, y) with respect to a perturbation function U ,\n\u2113\u03b7U(h; (x, y)) = I\n{ sup\nz\u2208U(x)\n|h(z)\u2212 y| \u2265 \u03b7 } . (3)\nThe non-robust version of this loss function is also known as \u03b7-ball or \u03b7-tube loss (see for example Anthony et al. [6, Section 21.4]).\nDefine the error of a function h on distribution D, with respect to the \u2113p robust loss,\nErr\u2113p(h;D) = E(x,y)\u223cD [\nsup z\u2208U(x)\n|h(z)\u2212 y|p ] ,\nand the error with respect to the \u03b7-ball robust loss\nErr\u03b7(h;D) = E(x,y)\u223cD [ I { sup\nz\u2208U(x)\n|h(z)\u2212 y| \u2265 \u03b7 }] .\nNote that in our model the learner is tested on the original label y while observing only the perturbed example z. There are formulations of robustness where the learner is compared to the value of the optimal function in the class on the perturbed example, i.e., if the optimal function in the class is h\u22c6, then the \u21131 robust loss would be supz\u2208U(x)|h(z)\u2212 h\u22c6(z)|. For more details and comparisons of the two models, see Gourdeau et al. [33], Diochnos et al. [26], Bubeck et al. [19].\nLearning models. We precisely define the models for robustly learning real-valued functions. Our first model is learning with the \u2113p robust losses (see Eq. (2)), we refer to this model as Robust Regression.\nDefinition 2.1 (Robust regression) For any \u01eb, \u03b4 \u2208 (0, 1), the sample complexity robust (\u01eb, \u03b4)-PAC learning a concept class H \u2286 [0, 1]X with \u2113p robust losses, denoted by M(\u01eb, \u03b4,H,U , \u2113p), is the smallest integer m such that the following holds: there exists a learning algorithm A : (X \u00d7 Y)m \u2192 [0, 1]X , such that for any distribution D over X \u00d7 [0, 1], for an i.i.d. random sample S \u223c Dm, with probability at least 1\u2212 \u03b4 over S, it holds that\nErr\u2113p(A(S);D) \u2264 inf h\u2208H Err\u2113p(h;D) + \u01eb.\nIf no such m exists, define M(\u01eb, \u03b4,H,U , \u2113p) = \u221e, and H is not robustly (\u01eb, \u03b4)-PAC learnable. We use the shorthandM =M(\u01eb, \u03b4,H,U , \u2113p) for notational simplicity.\nOur second model is learning with the \u03b7-ball robust loss (see Eq. (3)) in the realizable and agnostic settings, we refer to this model by Robust (\u03b7, \u03b2)-regression. We say that a distribution D is \u03b7-uniformly realizable with respect to H and U , if there exists h\u22c6 \u2208 H such that\nErr\u03b7(h \u22c6;D) = 0. (4)\nDefinition 2.2 (Robust (\u03b7, \u03b2)-regression) For any \u03b2, \u01eb, \u03b4 \u2208 (0, 1) and \u03b7 \u2208 [0, 1], the sample complexity of realizable robust (\u03b7, \u03b2, \u01eb, \u03b4)-PAC learning a concept class H \u2286 [0, 1]X , denoted byMRE(\u03b7, \u03b2, \u01eb, \u03b4,H,U), is the smallest integer m such that the following holds: there exists a learning algorithmA : (X \u00d7 Y)m \u2192 [0, 1]X , such that for any distribution D over X \u00d7 [0, 1] that is \u03b7-uniformly realizable w.r.t. H and U (see Eq. (4)), for an i.i.d. random sample S \u223c Dm, with probability at least 1\u2212 \u03b4 over S, it holds that\nErr\u03b7+\u03b2(A(S);D) \u2264 \u01eb.\nIf no such m exists, define MRE(\u03b7, \u01eb, \u03b4,H,U) =\u221e, and H is not robustly (\u03b7, \u03b2, \u01eb, \u03b4)-PAC learnable. The agnostic sample complexity, denoted by MAG(\u03b7, \u03b2, \u01eb, \u03b4,H,U), is defined similarly with the following difference. We require the learning algorithm to output a function, such that with probability at least 1\u2212 \u03b4,\nErr\u03b7+\u03b2(A(S);D) \u2264 inf h\u2208H Err\u03b7(h;D) + \u01eb.\nWe use the shorthand M\u03b7,\u03b2RE = MRE(\u03b7, \u03b2, \u01eb, \u03b4,H,U) and M\u03b7,\u03b2AG = MAG(\u03b7, \u03b2, \u01eb, \u03b4,H,U) for notational simplicity.\nWe do not define the setting of robust regression in the realizable setting since it coincides with the realizable setting of robust (\u03b7, \u03b2)-regression, by taking \u03b7 = 0, \u03b2 = \u01eb/2, and re-scaling \u01eb to \u01eb/2. Moreover, we could define the \u2113p variant of the \u03b7-ball loss in robust (\u03b7, \u03b2)-regression, however, results for our definition translate immediately by taking \u03b71/p.\nNote that there is a fundamental difference between the models. In the robust (\u03b7, \u03b2)-regression, we demand from the learning algorithm to find a function that is almost everywhere within \u03b7 + \u03b2 from the target function in class. That is, on 1\u2212 \u01eb mass of elements in the support of D, we find an approximation up to \u03b7+\u03b2. On the other hand, in the robust regression model, we aim to be close to the target function on average, and the error can possibly spread across all elements in the support.\nProper and improper learning algorithms. The learning algorithm is not limited to returning a function that is inside the concept class that we aim to learn. When learning a class H, whenever the learning algorithm returns a function inside the class, that is, A : (X \u00d7 Y)m \u2192 H, we say that the algorithm is proper and the class in properly learnable. Otherwise, we say that the algorithm is improper. Improper algorithms are extremely powerful and using them often circumvents computational issues and sample complexity barriers [59, 20, 3, 39, 35, 37, 38, 1, 24, 23, 4, 49].\nOracles. We make use of the following robust empirical risk minimizers. Let a set S = {(xi, yi)}mi=1. Define an \u03b7-robust empirical risk minimizer \u03b7-RERMH : (X \u00d7 Y)m \u00d7 [0, 1]m \u00d7 N\u2192 H,\n\u03b7-RERMH(S,\u03b7S , p) :=\nargmin h\u2208H\n1\nm\n\u2211\n(x,y)\u2208S\nI [ sup\nz\u2208U(x)\n|h(z)\u2212 y|p \u2265 \u03b7(x, y) ] ,\n(5)\nwhere \u03b7S = (\u03b7(x1, y1), . . . , \u03b7(xm, ym)). We refer to \u03b7(x, y) as a cutoff parameter. Note that \u03b7 is a function of (x, y) and not necessarily a constant.\nDefine a robust empirical risk minimizer for the \u2113p robust loss, \u2113p-RERMH : (X \u00d7 Y)m \u2192 H,\n\u2113p-RERMH(S) := argmin h\u2208H\n1\nm\n\u2211\n(x,y)\u2208S\nsup z\u2208U(x)\n|h(z)\u2212 y|p . (6)\nComplexity measures. Fat-shattering dimension. Let F \u2286 [0, 1]X and \u03b3 > 0. We say that S = {x1, . . . , xm} \u2286 X is \u03b3-shattered by F if there exists a witness r = (r1, . . . , rm) \u2208 [0, 1]m such that for each \u03c3 = (\u03c31, . . . , \u03c3m) \u2208 {\u22121, 1}m there is a function f\u03c3 \u2208 F such that\n\u2200i \u2208 [m] { f\u03c3(xi) \u2265 ri + \u03b3, if \u03c3i = 1 f\u03c3(xi) \u2264 ri \u2212 \u03b3, if \u03c3i = \u22121.\nThe fat-shattering dimension of F at scale \u03b3, denoted by fat(F , \u03b3), is the cardinality of the largest set of points in X that can be \u03b3-shattered by F . This parametrized variant of the Pseudo-dimension [2] was first proposed by Kearns and Schapire [40]. Its key role in learning theory lies in characterizing the PAC learnability of real-valued function classes [2, 16].\nDual fat-shattering dimension. Define the dual class F\u2217 \u2286 [0, 1]H of F as the set of all functions gw : F \u2192 [0, 1] defined by gw(f) = f(w). If we think of a function class as a matrix whose rows and columns are indexed by functions and points, respectively, then the dual class is given by the transpose of the matrix. The dual fat-shattering at scale \u03b3, is defined as the fat-shattering at scale \u03b3 of the dual-class and denoted by fat\u2217(F , \u03b3). We have the following bound due to Kleer and Simon [43, Corollary 3.8 and inequality 3.1],\nfat\u2217(F , \u03b3) . 1 \u03b3 2fat(F ,\u03b3/2)+1. (7)\nCovering numbers. We say that G \u2286 [0, 1]\u2126 is \u01eb-cover for F \u2286 [0, 1]\u2126 in d\u221e norm, if for any f \u2208 F the exists g \u2208 G such that for any x \u2208 \u2126, |f(x)\u2212 g(x)| \u2264 \u01eb. The \u01eb-covering number of F is the minimal cardinality of any \u01eb-cover, and denoted by N (\u01eb,F , d\u221e).\nSample compression scheme. We say that a pair of functions (\u03ba, \u03c1) is uniform \u03b1-approximate sample compression scheme of size k for a class H \u2286 [0, 1]X and the \u2113p loss, if for any m \u2208 N, h\u22c6 \u2208 H, and sample S = {(xi, yi)}mi=1, it holds for the compression function that \u03ba (S) \u2286 S, |\u03ba (S) | \u2264 k, and the reconstruction function \u03c1 (\u03ba (S)) = h\u0302 satisfies \u2200i \u2208 [m]\n\u2223\u2223h\u0302(xi)\u2212 yi \u2223\u2223p \u2264 \u2223\u2223h\u22c6(xi)\u2212 yi \u2223\u2223p + \u03b1. (8)\nSimilarly, we define an adversarially robust uniform \u03b1-approximate sample compression scheme if \u2200i \u2208 [m]\nsup zi\u2208U(xi)\n\u2223\u2223h\u0302(zi)\u2212 yi \u2223\u2223p \u2264 sup\nzi\u2208U(xi)\n\u2223\u2223h\u22c6(zi)\u2212 yi \u2223\u2223p + \u03b1. (9)\nNotation. We use the notation O\u0303(\u00b7) for omitting poly-logarithmic factors of (fat(H, \u03b3), fat\u2217(H, \u03b3), 1/\u01eb, 1/\u03b4, 1/\u03b7, 1/\u03b2). We denote [n] = {1, . . . , n}, and exp(\u00b7) = e(\u00b7). . and & denote inequalities up to a constant factor, and \u2248 denotes equality up to a constant factor. Vectors are written using bold symbols."
        },
        {
            "heading": "3 Robust Regression for \u2113p Losses",
            "text": "In this section, we provide an algorithm and prove its sample complexity for robust regression with \u2113p losses. Moreover, our learning algorithm is proper for convex function classes, arguably the most commonly studied subcategory of real-valued function classes for regression. This result circumvents a negative result from Montasser et al. [49]; there exist, non-convex function classes, where proper learning is impossible.\nTheorem 3.1 The sample complexity of Algorithm 1 for robust (\u01eb, \u03b4)-PAC learning a concept class H with the \u2113p robust loss is\nO\u0303 ( fat3(H, c\u01eb/p) fat\u2217(H, c\u01eb/p)\n\u01eb5 +\n1 \u01eb2 log 1 \u03b4\n) ,\nfor some numerical constant c \u2208 (0,\u221e). Recall that fat\u2217(F , \u01eb) . 1\u01eb2fat(F ,\u01eb/2)+1 by Eq. (7).\nRemark 3.2 The output of Algorithm 1 is a convex combination of the functions from the concept class, which is a proper predictor, assuming convexity of the function class.\nRemark 3.3 Similar to non-robust regression, our results generalize to loss functions with bounded codomain [0,M ]. The generalization bound should be multiplied by pMp and the scaling of the fatshattering dimension should be \u01eb/pMp.\nIn the following result, we establish generalization from approximate interpolation for changing cutoff parameters for different points. This generalizes a result by [5], where the cutoff parameter is fixed for all points. The proof is in Appendix A.\nTheorem 3.4 (Generalization from approximate interpolation with changing cutoffs) Let H \u2286 [0, 1]X with a finite fat-shattering dimension (at any scale). For any \u03b2, \u01eb, \u03b4 \u2208 (0, 1), any function \u03b7 : X \u00d7 Y \u2192 [0, 1], any distribution D over X \u00d7 Y, for a random sample S \u223c Dm, if\nm = O ( 1\n\u01eb\n( fat(H, \u03b2/8) log2 ( fat(H, \u03b2/8)\n\u03b2\u01eb\n) + log 1\n\u03b4\n)) ,\nthen with probability at least 1 \u2212 \u03b4 over S, for any h \u2208 H satisfying |h(x) \u2212 y| \u2264 \u03b7(x, y), \u2200(x, y) \u2208 S, it holds that P(x,y)\u223cD{(x, y) : |h(x) \u2212 y| \u2264 \u03b7(x, y) + \u03b2} \u2265 1\u2212 \u01eb.\nAlgorithm 1 Improper Robust Regressor with High-Vote Input: H \u2286 [0, 1]X , S = {(xi, yi)}mi=1. Parameters: \u01eb. Algorithms used: \u2113p-RERMH (Eq. (6)), \u03b7-RERMH (Eq. (5)), a variant of Multiplicative Weights (Algorithm 4).\n1. Compute h\u22c6 \u2190 \u2113p-RERMH(S). Denote \u03b7(x, y) = supz\u2208U(x) |h\u22c6(z)\u2212 y|, \u2200(x, y) \u2208 S.\n2. Inflate S to SU to include all perturbed points.\n3. Discretize S\u0304U \u2286 SU : (i) Construct a function class H\u0302, where each h\u0302 \u2208 H\u0302 defined by \u03b7-RERM optimizer on O\u0303 ( 1 \u01eb fat(H,O(\u01eb/p)) ) points from S. The input cutoff parameters to the optimizer are\n\u03b7(x, y), as computed in step 1. (ii) Each (z, y) \u2208 SU defines a function in the dual space, f(z,y) : H\u0302 \u2192 [0, 1] such that f(z,y)(h) =\u2223\u2223h(z)\u2212 y \u2223\u2223p. Define S\u0304U to be the minimal cover of SU under d\u221e norm at scale O(\u01eb)\n4. Compute a modified Multiplicative Weights on S\u0304U . Let { h\u03021, . . . , h\u0302T } be the returned set of clas-\nsifiers.\nOutput: h\u0302 = 1T \u2211T i=1 h\u0302i.\nWe construct an adversarially robust sample compression scheme of a size determined by the fatshattering dimension of the function class. Recall that uniform convergence does not necessarily hold. Instead, we derive generalization from sample compression bounds. Our proof crucially relies on Theorem 3.4.\nProof overview and algorithm outline. The complete proof is in Appendix B. We follow the steps in Algorithm 1.\n1. We start with computing a robust empirical risk minimizer (ERM) h\u22c6 on S for the \u2113p robust loss. This defines the target loss we are aiming for at any point in S. In other words, the robust loss of h\u22c6 on (x, y) defines a cutoff \u03b7(x, y) and our goal is to construct a predictor with a loss of \u03b7(x, y) + \u01eb for any (x, y) \u2208 S, which means that this predictor is an approximate robust ERM. In order to derive generalization, we cannot rely on uniform convergence. Instead, our predictor is based on a sample compression scheme from which we can generalize.\n2. Inflate the training set by including all possible perturbations. Whenever the same perturbation is mapped to more than one input, we assign the label of the input with the smallest index to prevent ambiguity. We denote this set by SU .\n3. Discretize the set SU as follows: (i) Construct a set of functions H\u0302, such that each function is the output of \u03b7-RERM for H (defined in Eq. (5)), performing on a subset S\u2032 \u2286 S of size d = O\u0303 ( 1 \u01eb fat(H,O(\u01eb/p)) ) .\nThis means that for any S\u2032 \u2286 S there exists h\u0302 \u2208 H\u0302 that is an approximate robust ERM on S\u2032, that is, h\u0302 is within \u03b7(x, y) + \u01eb for any (x, y) \u2208 S\u2032. The size of H\u0302 is bounded (m/d)d, where |S| = m. (ii) Define a discretization S\u0304U \u2286 SU by taking a uniform cover of the dual space. In the dual space, each (z, y) \u2208 SU defines a function f(z,y) : H\u0302 \u2192 [0, 1] such that f(z,y)(h) = \u2223\u2223h(z) \u2212 y \u2223\u2223p. We take a minimal O(\u01eb)-cover for SU with the supremum norm, which is of size N (O(\u01eb) , SU , d\u221e). We use covering numbers arguments [55] to upper bound the size of S\u0304U 4. Compute a variant of Multiplicative Weights (MW) update (Algorithm 4) on S\u0304U for T \u2248 log \u2223\u2223S\u0304U \u2223\u2223\nrounds as follows. From a corollary of Theorem 3.4 (see Corollary A.2), we know that for any distribution P on S\u0304U , upon receiving an i.i.d. sample S\u2032\u2032 from P of size d, with probability 2/3 over sampling S\u2032\u2032 from P , for any h \u2208 H with \u2200(z, y) \u2208 S\u2032\u2032 : |h(z)\u2212 y|p \u2264 \u03b7(z, y), it holds that P(z,y)\u223cP{(z, y) : |h(z)\u2212 y|p \u2264 \u03b7(z, y) + \u01eb} \u2265 1 \u2212 \u01eb, where \u03b7(z, y) is the \u03b7(x, y) for which z \u2208 U(x) as defined in step 2. We can conclude that for any distribution P on S\u0304U , there exists such a set of points S\u2032\u2032 \u2286 S\u0304U . Then, we can find a set S\u2032 of d points in S that S\u2032\u2032 originated from. Formally, S\u2032\u2032 \u2286 \u22c3(x,y)\u2208S\u2032\n\u22c3{(z, y) : z \u2208 U(x)}. We execute the optimizer h\u0302 \u2190 \u03b7-RERM on S\u2032 with the relevant cutoff parameters. h\u0302 has error of \u03b7(z, y) + \u01eb on a fraction of (1 \u2212 \u01eb) points with respect to the\ndistribution P . We start with P1 as the uniform distribution over S\u0304U and find h\u03021 respectively. We perform a multiplicative weights update on the distribution and find the next hypothesis w.r.t. the new distribution and so forth.\nFollowing the analysis of MW (or \u03b1-Boost) from Schapire and Freund [56, Section 6]), we know that for any point in S\u0304U , roughly (1\u2212 \u01eb) base learners are within \u01eb from the target cutoff. The rest \u01eb fraction can contribute an error of at most \u01eb since the loss is bounded by 1. We get that for any point in S\u0304U , the average loss of hypotheses in the ensemble is within 2\u01eb from the target cutoff. Crucially, we use strong base learners in the ensemble. By the covering argument, we get that for any point in SU , the average loss of the ensemble is within 4\u01eb,\n\u2200(z, y) \u2208 SU : 1\nT\nT\u2211\ni=1\n\u2223\u2223\u2223h\u0302i(z)\u2212 y \u2223\u2223\u2223 p \u2264 \u03b7(z, y) + 4\u01eb.\nWe are interested that the average prediction 1T \u2211T\ni=1 h\u0302i will be within the target cutoffs. For that reason, we use the convexity of the \u2113p loss to show that\n\u2223\u2223\u2223\u2223\u2223 1 T T\u2211\ni=1\nh\u0302i(z)\u2212 y \u2223\u2223\u2223\u2223\u2223 p \u2264 1 T T\u2211\ni=1\n\u2223\u2223\u2223h\u0302i(z)\u2212 y \u2223\u2223\u2223 p .\nTherefore, we conclude that\n\u2200(z, y) \u2208 SU : \u2223\u2223\u2223\u2223\u2223 1 T T\u2211\ni=1\nh\u0302i(z)\u2212 y \u2223\u2223\u2223\u2223\u2223 p \u2264 \u03b7(z, y) + 4\u01eb,\nwhich implies that we have an approximate robust ERM for S,\n\u2200(x, y) \u2208 S : sup z\u2208U(x) \u2223\u2223\u2223\u2223\u2223 1 T T\u2211\ni=1\nh\u0302i(z)\u2212 y \u2223\u2223\u2223\u2223\u2223 p \u2264 \u03b7(x, y) + 4\u01eb.\nThe proof follows by applying a sample compression generalization bound in the agnostic case, bounding the compression size, and rescaling \u01eb.\nFor convex classes, we have a proper learner. The output of the algorithm is a convex combination of functions from H which is also in the class."
        },
        {
            "heading": "4 Better Sample Complexity for the \u21131 Loss",
            "text": "In this section, we provide an algorithm with a substantial sample complexity improvement for the \u21131 robust loss. The key technical idea in this result is to note that, if we replace base learners with weak learners in the improper ensemble predictor, we can still get an accurate prediction by taking the median aggregation of the ensemble. Thus, we incorporate a variant of median boosting for real-valued functions [41, 36] in our algorithm. Each base learner requires fewer samples and as a result, we improve the sample complexity. On the contrary, in Algorithm 1 we obtained accurate predictions for a 1 \u2212 O(\u01eb) quantile of the predictors, and we output their average.\nTheorem 4.1 The sample complexity of Algorithm 2 for robust (\u01eb, \u03b4)-PAC learning a concept class H with the \u21131 robust loss is\nO\u0303 ( fat(H, c\u01eb) fat\u2217(H, c\u01eb)\n\u01eb2 +\n1 \u01eb2 log 1 \u03b4\n) ,\nfor some numerical constant c \u2208 (0,\u221e). Recall that fat\u2217(F , \u01eb) . 1\u01eb2fat(F ,\u01eb/2)+1 by Eq. (7). We shall define the notion of weak learners in the context of real-valued learners.\nDefinition 4.2 (Weak real-valued learner) Let \u03be \u2208 (0, 12 ], \u03b6 \u2208 [0, 1]. We say that f : X \u2192 [0, 1] is a (\u03b6, \u03be)-weak learner for the \u2113p loss, with respect to D and a target function h\u22c6 \u2208 H if\nP(x,y)\u223cD{(x, y) : |f(x)\u2212 y|p > |h\u22c6(x)\u2212 y|p + \u03b6} \u2264 1\n2 \u2212 \u03be.\nThis notion of a weak learner must be formulated carefully. For example, taking a learner guaranteeing absolute loss at most 12 \u2212 \u03be is known to not be strong enough for boosting to work. On the other hand, by making the requirement too strong (for example, AdaBoost.R in Freund and Schapire [31]), then the sample complexity of weak learning will be high that weak learners cannot be expected to exist for certain function classes. We can now present an overview of the proof and the algorithm.\nAlgorithm 2 Improper Robust Regressor Input: H \u2286 [0, 1]X , S = {(xi, yi)}mi=1. Parameters: \u01eb. Algorithms used: \u21131-RERMH (Eq. (6)), \u03b7-RERMH (Eq. (5)), a variant of median boosting: MedBoost (Algorithm 5), sparsification method (Algorithm 6).\n1. Compute h\u22c6 \u2190 \u21131-RERMH(S). Denote \u03b7(x, y) = supz\u2208U(x) |h\u22c6(z)\u2212 y|, \u2200(x, y) \u2208 S.\n2. Inflate S to SU to include all perturbed points.\n3. Discretize S\u0304U \u2286 SU : (i) Construct a function class H\u0302, where each h\u0302 \u2208 H\u0302 defined by \u03b7-RERM optimizer on O\u0303(fat(H,O(\u01eb))) points from S. The input cutoff parameters to the optimizer are \u03b7(x, y), as computed in step 1. (ii) Each (z, y) \u2208 SU defines a function in the dual space, f(z,y) : H\u0302 \u2192 [0, 1] such that f(z,y)(h) =\u2223\u2223h(z)\u2212 y \u2223\u2223. Define S\u0304U to be the minimal cover of SU under d\u221e norm at scale O(\u01eb).\n4. Compute modified MedBoost on S\u0304U , where H\u0302 consists of weak learners for any distribution over S\u0304U . Let F = { h\u03021, . . . , h\u0302T } be the returned set of classifiers.\n5. Sparsify the set F to a smaller set { h\u03021, . . . , h\u0302k } .\nOutput: h\u0302 = Median ( h\u03021, . . . , h\u0302k ) .\nProof overview and algorithm outline. The complete proof is in Appendix C. We explain the main differences from Algorithm 1 and where the sample complexity improvement comes from. In the discretization step, we replace the base learners in H\u0302 with weak learners. We construct an improper ensemble predictor via a median boosting algorithm, where the weak learners are chosen from H\u0302. Specifically, each function in H\u0302 is the output of \u03b7-RERM for H (defined in Eq. (5)), performing on a subset S\u2032 \u2286 S of size d = O\u0303(fat(H,O(\u01eb))) This is in contrast to Algorithm 1, where we use Multiplicative Weights update that operates with strong base learners. We can make accurate predictions by taking the median aggregation due to the \u21131 loss.\nAnother improvement arises from sparsifying the ensemble [36] to be independent of the sample size while keeping the median accurate almost with the same resolution. The sparsification step uses sampling and uniform convergence in the dual space (with respect to the non-robust loss).\nWe elaborate on the steps in Algorithm 2. Steps (1),(2), and (3) are similar to Algorithm 1, besides the construction of H\u0302 as we explained above. In step (4), we compute a modified version of the realvalued boosting algorithm MedBoost [41] on the discretized set S\u0304U , see Algorithm 5. Hanneke et al. [36] showed how to construct a sample compression scheme from MedBoost. From this step, we have that for any point in S\u0304U , the median of the losses of each hypothesis in the ensemble is within \u01eb of the target cutoff that was computed in step 1. By the covering argument, the median of the losses is within 3\u01eb for any point in (z, y) \u2208 SU ,\n\u2223\u2223\u2223Med ( h\u03021(z)\u2212 y, . . . , h\u0302T (z)\u2212 y )\u2223\u2223\u2223 \u2264 \u03b7(z, y) + 3\u01eb.\nThe median is translation invariant, so we have \u2223\u2223\u2223Med ( h\u03021(z), . . . , h\u0302T (z) ) \u2212 y \u2223\u2223\u2223 \u2264 \u03b7(z, y) + 3\u01eb.\nFinally, for any (x, y) \u2208 S,\nsup z\u2208U(x)\n\u2223\u2223\u2223Med ( h\u03021(z)\u2212 y, . . . , h\u0302T (z)\u2212 y )\u2223\u2223\u2223 \u2264 \u03b7(x, y) + 3\u01eb.\nTo further reduce the sample compression size, in step (5) we sparsify the ensemble to k = O\u0303(fat\u2217(H, c\u01eb)) functions,\nsup z\u2208U(x)\n\u2223\u2223\u2223Med ( h\u03021(z)\u2212 y, . . . , h\u0302k(z)\u2212 y )\u2223\u2223\u2223 \u2264 \u03b7(x, y) + 4\u01eb.\nThe proof follows by applying a sample compression generalization bound in the agnostic case, bounding the compression size, and rescaling \u01eb."
        },
        {
            "heading": "5 Robust (\u03b7, \u03b2)-Regression",
            "text": "In this section, we study robust (\u03b7, \u03b2)-regression in realizable and agnostic settings. We provide an algorithm for the realizable setting and show how to reduce agnostic to realizable learning. We conclude by deriving sample complexity guarantees for both settings.\nThis model is different than regression which guarantees a small expected error (with high probability). In the robust (\u03b7, \u03b2)-regression, we aim for a small pointwise absolute error almost everywhere on the support of the distribution. Results for this model do not follow from the standard regression model. We first present our result for the realizable case. The proof is in Appendix D.\nTheorem 5.1 Assuming the distribution D is \u03b7-uniformly realizable (see Eq. (4)) by a class H \u2286 [0, 1]X , the sample complexity of Algorithm 3 for robust (\u03b7, \u03b2, \u01eb, \u03b4)-PAC learning a concept class H is\nO\u0303 ( fat(H, c\u03b2) fat\u2217(H, c\u03b2)\n\u01eb +\n1 \u01eb log 1 \u03b4\n) ,\nfor some numerical constant c \u2208 (0,\u221e). Recall that fat\u2217(F , \u01eb) . 1\u01eb2fat(F ,\u01eb/2)+1 by Eq. (7).\nAlgorithm 3 Improper Robust (\u03b7, \u03b2)-Regressor\nInput: H \u2286 [0, 1]X , S = {(xi, yi)}mi=1. Parameters: \u03b7, \u03b2. Algorithms used: \u03b7-RERMH (Eq. (5)), a variant of median boosting: MedBoost (Algorithm 5), sparsification method (Algorithm 6).\n1. Inflate S to SU to include all perturbed points.\n2. Discretize S\u0304U \u2286 SU : (i) Construct a function class H\u0302, where each h\u0302 \u2208 H\u0302 defined by \u03b7-RERM optimizer on O\u0303(fat(H,O(\u03b2))) points from S. The input cutoff parameters to the optimizer are fixed \u03b7 for all points. (ii) Each (z, y) \u2208 SU defines a function in the dual space, f(z,y) : H\u0302 \u2192 [0, 1] such that f(z,y)(h) =\u2223\u2223h(z)\u2212 y \u2223\u2223. Define S\u0304U to be the minimal cover of SU under d\u221e norm at scale O(\u03b2).\n3. Compute modified MedBoost on S\u0304U , where H\u0302 consists of weak learners for any distribution over S\u0304U . Let F = { h\u03021, . . . , h\u0302T } be the returned set of classifiers.\n4. Sparsify the set F to a smaller set { h\u03021, . . . , h\u0302k } .\nOutput: h\u0302 = Median ( h\u03021, . . . , h\u0302k ) .\nWe explain the main differences from Algorithm 2. This model is different from robust regression with the \u21131 loss. Our goal is to find a predictor with a prediction within \u03b7 + \u03b2 of the true label almost everywhere the domain, assuming that the distribution is \u03b7-uniformly realizable by the function class (Eq. (4)).\nIn this model, the cutoff parameter is given to us as a parameter and is fixed for all points. This is different from Algorithms 1 and 2, where we computed the changing cutoffs with a robust ERM oracle. Moreover, the weak learners in H\u0302 are defined as the output of \u03b7-RERM performing on a subset S\u2032 \u2286 S of size d = O\u0303(fat(H,O(\u03b2))). Note that the scale of shattering depends on \u03b2 and not \u01eb. The resolution of discretization in the cover depends on \u03b2 as well.\nAgnostic setting We establish an upper bound on the sample complexity of the agnostic setting, by using a reduction to the realizable case. The main argument was originally suggested in [25] for the 0-1 loss and holds for the \u03b7-ball robust loss as well. The proof is in Appendix D.\nTheorem 5.2 The sample complexity for agnostic robust (\u03b7, \u03b2, \u01eb, \u03b4)-PAC learning a concept class H is\nO\u0303 ( fat(H, c\u03b2) fat\u2217(H, c\u03b2)\n\u01eb2 +\n1 \u01eb2 log 1 \u03b4\n) ,\nfor some numerical constant c \u2208 (0,\u221e). Recall that fat\u2217(F , \u01eb) . 1\u01eb2fat(F ,\u01eb/2)+1 by Eq. (7).\nRemark 5.3 An agnostic learner for robust (\u03b7, \u03b2)-regression does not apply to the robust regression setting. The reason is that the optimal function in H may have different scales of robustness on different points, which motivates our approach of using changing cutoffs for different points. In Appendix D.3 we show that by using a fixed cutoff for all points we can obtain an error of only \u221a OPTH + \u01eb."
        },
        {
            "heading": "6 Discussion",
            "text": "In this paper, we studied the robustness of real-valued functions to test time attacks. We showed that finite fat-shattering is sufficient for learnability. we proved sample complexity for learning with the general \u2113p losses and improved it for the \u21131 loss. We also studied a model of regression with a cutoff loss. We proved sample complexity in realizable and agnostic settings. We leave several interesting open questions for future research. (i) Improve the upper bound for learning with \u2113p robust losses (if possible) and show a lower bound. There might be a gap between sample complexities of different values of p. More specifically, what is the sample complexity for learning with \u21132 robust loss? (ii) We showed that the fat-shattering dimension is a sufficient condition. What is a necessary condition? In the binaryvalued case, we know that having a finite VC is not necessary. (iii) To what extent can we benefit from unlabeled samples for learning real-valued functions? This question was considered by Attias et al. [9] for binary function classes, where they showed that the labeled sample complexity can be arbitrarily smaller compared to the fully-supervised setting. (iv) In this work we focused on the statistical aspect of robustly learning real-valued functions. It would be interesting to explore the computational aspect as well."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank Meni Sadigurschi for helpful discussions regarding sample compression schemes of real-valued functions. We are also grateful to Uri Sherman for helpful discussions on the generalization of ERM in Stochastic Convex Optimization. This project has received funding from the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation program (grant agreement No. 882396), by the Israel Science Foundation (grants 993/17, 1602/19), Tel Aviv University Center for AI and Data Science (TAD), and the Yandex Initiative for Machine Learning at Tel Aviv University. I.A. is supported by the Vatat Scholarship from the Israeli Council for Higher Education and by Kreitman School of Advanced Graduate Studies."
        },
        {
            "heading": "A Auxiliary Results",
            "text": "Theorem A.1 (Generalization from approximate interpolation) [6, Theorems 21.13 and 21.14] Let H \u2286 [0, 1]X with a finite fat-shattering dimension (at any scale). For any \u03b2, \u01eb, \u03b4 \u2208 (0, 1), \u03b7 \u2208 [0, 1], any distribution D over X \u00d7 Y, for a random sample S \u223c Dm, if\nm(\u03b7, \u03b2, \u01eb, \u03b4) = O ( 1\n\u01eb\n( fat(H, \u03b2/8) log2 ( fat(H, \u03b2/8)\n\u03b2\u01eb\n) + log 1\n\u03b4\n)) ,\nthen with probability at least 1\u2212 \u03b4 over S, for any h \u2208 H satisfying |h(x)\u2212 y| \u2264 \u03b7, \u2200(x, y) \u2208 S, it holds that P(x,y)\u223cD{(x, y) : |h(x) \u2212 y| \u2264 \u03b7 + \u03b2} \u2265 1\u2212 \u01eb.\nProof Proof of Theorem 3.4 Let F \u2286 [0, 1]X and let H = {(x, y) 7\u2192 |f(x)\u2212 y| : f \u2208 F} .\nDefine the function classes\nF1 = {(x, y) 7\u2192 |h(x) \u2212 y| \u2212 \u03b7(x, y) : h \u2208 H} , and\nF2 = {(x, y) 7\u2192 max{f(x, y), 0} : f \u2208 F1} . We claim that fat(H, \u03b3) = fat(F1, \u03b3). Take a set S = {(x1, y1), . . . , (xm, ym)} that is \u03b3-shattered by H. There exists a witness r = (r1, . . . , rm) \u2208 [0, 1]m such that for each \u03c3 = (\u03c31, . . . , \u03c3m) \u2208 {\u22121, 1}m there is a function h\u03c3 \u2208 H such that\n\u2200i \u2208 [m] { h\u03c3((xi, yi)) \u2265 ri + \u03b3, if \u03c3i = 1 h\u03c3((xi, yi)) \u2264 ri \u2212 \u03b3, if \u03c3i = \u22121.\nThe set S is shattered by F1 by taking r\u0303 = (r1 + \u03b7(x1, y1), . . . , rm + \u03b7(xm, ym)). Similarly, any set that is shattered by F1 is also shattered by H.\nThe class F2 consists of choosing a function from F1 and computing its pointwise maximum with the constant function 0. In general, for two function classes G1,G2, we can define the maximum aggregation class max(G1,G2) = {x 7\u2192 max{g1(x), g2(x)} : gi \u2208 Gi}, and Kontorovich and Attias [44] showed that for any G1,G2\nfat(max(G1,G2), \u03b3) . (fat(G1, \u03b3) + fat(G2, \u03b3)) log2(fat(G1, \u03b3) + fat(G2, \u03b3)) . Taking G1 = F1 and G2 \u2261 0, we get\nfat(F2, \u03b3) . fat(F1, \u03b3) log2(fat(F1, \u03b3)) . For the particular case G2 \u2261 0, we can show a better bound of\nfat(F2, \u03b3) . fat(F1, \u03b3) . In words, it means that truncation cannot increase the shattering dimension. Indeed, take a set S = {(x1, y1), . . . , (xk, yk)} that is \u03b3-shattered by F2 = max(F1, 0), we show that this set is \u03b3-shattered by F1. There exists a witness r = (r1, . . . , rk) \u2208 [0, 1]m such that for each \u03c3 = (\u03c31, . . . , \u03c3k) \u2208 {\u22121, 1}k there is a function f\u03c3 \u2208 F1 such that\n\u2200i \u2208 [k] { max{f\u03c3((xi, yi)), 0} \u2265 ri + \u03b3, if \u03c3i = 1 max{f\u03c3((xi, yi)), 0} \u2264 ri \u2212 \u03b3, if \u03c3i = \u22121.\nFor max{f\u03c3((xi, yi)), 0} \u2264 ri \u2212 \u03b3, we simply have that f\u03c3((xi, yi)) \u2264 ri \u2212 \u03b3. Moreover, this implies that ri \u2265 \u03b3. As a result,\nmax{f\u03c3((xi, yi)), 0} \u2265 ri + \u03b3 \u2265 2\u03b3 > 0,\nwhich means that f\u03c3((xi, yi)) \u2265 ri + \u03b3. This shows that F1 \u03b3-shatters S as well. We can conclude the proof by applying Theorem A.1 to the class F2 and taking \u03b7(x, y) = 0.\nCorollary A.2 Let H \u2286 [0, 1]X with a finite fat-shattering dimension (at any scale). For any \u03b2, \u01eb, \u03b4 \u2208 (0, 1), 1 \u2264 p <\u221e, any function \u03b7 : X \u00d7Y \u2192 [0, 1], any distribution D over X \u00d7Y, for a random sample S \u223c Dm, if\nm(\u03b7, \u03b2, \u01eb, \u03b4) = O ( 1\n\u01eb\n( fat(H, \u03b2/8) log2 ( fat(H, \u03b2/8)\n\u03b2\u01eb\n) + log 1\n\u03b4\n)) ,\nthen with probability at least 1\u2212 \u03b4 over S, for any h \u2208 H satisfying |h(x)\u2212 y|p \u2264 \u03b7(x, y), \u2200(x, y) \u2208 S, it holds that P(x,y)\u223cD{(x, y) : |h(x) \u2212 y|p \u2264 \u03b7(x, y) + p\u03b2} \u2265 1\u2212 \u01eb.\nProof Following Theorem 3.4, we know that if m = O (\n1 \u01eb\n( fat(H, \u03b2/8) log2 ( fat(H,\u03b2/8)\n\u03b2\u01eb\n) + log 1\u03b4 )) , with\nhigh probability over S \u223c Dm, for any h \u2208 H satisfying |h(x)\u2212 y| \u2264 (\u03b7(x, y))1/p for all (x, y) \u2208 S, it holds that P(x,y)\u223cD { (x, y) : |h(x) \u2212 y| \u2264 (\u03b7(x, y))1/p + \u03b2 } \u2265 1\u2212 \u01eb.\nWe show that for any (x, y) with |h(x)\u2212 y| \u2264 (\u03b7(x, y))1/p + \u03b2, it holds that\n|h(x) \u2212 y|p (i) \u2264 ( (\u03b7(x, y)) 1/p + \u03b2 )p\n(ii)\n\u2264 \u03b7(x, y) + p\u03b2,\nand that will finish the proof. (i) Follows by just raising both sides to the power of p. (ii) Follows since the function x 7\u2192 |x\u2212 y|p is p-Lipschitz for (x\u2212 y) \u2208 [0, 1], and so\n\u2223\u2223\u2223 ( (\u03b7(x, y)) 1/p + \u03b2 )p \u2212 \u03b7(x, y) \u2223\u2223\u2223 = \u2223\u2223\u2223 ( (\u03b7(x, y)) 1/p + \u03b2 )p \u2212 ( (\u03b7(x, y)) 1/p )p\u2223\u2223\u2223\n\u2264 p \u2223\u2223\u2223(\u03b7(x, y))1/p + \u03b2 \u2212 (\u03b7(x, y))1/p \u2223\u2223\u2223 \u2264 p\u03b2.\nThe following is a bound on the covering numbers in d\u221e.\nLemma A.3 (Covering numbers for infinity metric) [55, Theorem 4.4] Let F \u2286 [0, 1]\u2126 be a class of functions and |\u2126| = n. Then for any 0 < a \u2264 1 and 0 < t < 1/2,\nlogN (t,F , d\u221e) \u2264 Cv log(n/vt) \u00b7 loga(2n/v) ,\nwhere v = fatcat(F), and C, c are universal constants.\nLemma A.4 (Fat-shattering of the loss class) Let H \u2282 Rm be a real valued function class on m points. Denote the \u2113p loss class of H by LpH, for 1 \u2264 p <\u221e. Assume LpH is bounded by M . For any H,\nfat(LpH, \u03b3) \u2264 O(log2(m)fat(H, \u03b3/pMp\u22121).\nProof For any X and any function class H \u2282 RX , define the difference class H\u2206 \u2282 RX\u00d7R as\nH\u2206 = {X \u00d7 R \u220b (x, y) 7\u2192 \u2206h(x, y) := h(x) \u2212 y;h \u2208 H} .\nIn words, H\u2206 consists of all functions \u2206h(x, y) = h(x)\u2212 y indexed by h \u2208 H. It is easy to see that for all \u03b3 > 0, we have fat\u03b3(H\u2206) \u2264 fat\u03b3(H). Indeed, if H\u2206 \u03b3-shatters some set {(x1, y1), . . . , (xk, yk)} \u2282 X \u00d7R with shift r \u2208 Rk, then H \u03b3-shatters the set {x1, . . . , xk} \u2282 X with shift r + (y1, . . . , yk).\nNext, we observe that taking the absolute value does not significantly increase the fat-shattering dimension. Indeed, for any real-valued function class F , define abs(F) := {|f |; f \u2208 F}. Observe that abs(F) \u2286 max((Fj)j\u2208[2]), where F1 = F and F2 = \u2212F =: {\u2212f ; f \u2208 F}. It follows from Attias et al. [8, Theorem 13] that\nfat\u03b3(abs(F)) < O(log2(m)(fat\u03b3(F) + fat\u03b3(\u2212F))) < O(log2(m)fat\u03b3(F)). (10)\nNext, define F as the L1 loss class of H:\nF = {X \u00d7 R \u220b (x, y) 7\u2192 |h(x)\u2212 y)|;h \u2208 H} .\nThen\nfat\u03b3(F) = fat\u03b3(abs(H\u2206)) \u2264 O(log2(m)fat\u03b3(H\u2206)) \u2264 O(log2(m)fat\u03b3(H));\nthis proves the claim for L1. To analyze the L2 case, consider F \u2282 [0,M ]X and define F\u25e6p := {fp; f \u2208 F}. We would like to bound fat\u03b3(F\u25e6p) in terms of fat\u03b3(F). Suppose that F\u25e6p \u03b3-shatters some set {x1, . . . , xk} with shift rp = (rp1 , . . . , r p k) \u2208 [0,M ]k (there is no loss of generality in assuming that the shift has the same range as the function class). Since for any y\u2032 the function y 7\u2192 |y \u2212 y\u2032|p is pMp\u22121 Lipschitz for (y \u2212 y\u2032) \u2208 [0,M ], we have\n|ap \u2212 bp| \u2264 pMp\u22121|a\u2212 b|, a, b \u2208 [0,M ],\nwe conclude thatF is able to \u03b3/(pMp\u22121)-shatter the same k points and thus fat\u03b3(F\u25e6p) \u2264 fat\u03b3/(pMp\u22121)(F). To extend this result to the case where F \u2282 [\u2212M,M ]X , we use Eq. (10). In particular, define F as the Lp loss class of H:\nF = {X \u00d7 R \u220b (x, y) 7\u2192 (h(x)\u2212 y)p;h \u2208 H} .\nThen\nfat\u03b3(F) = fat\u03b3((H\u2206)\u25e6p) = fat\u03b3((abs(H\u2206))\u25e6p) \u2264 fat\u03b3/(pMp\u22121)(abs(H\u2206)) \u2264 O(log2(m)fat\u03b3/(pMp\u22121)(H\u2206)) \u2264 O(log2(m)fat\u03b3/(pMp\u22121)(H)).\nThe following generalization for sample compression in the realizable case was proven by Littlestone and Warmuth [45], Floyd and Warmuth [30]. Their proof is for the 0-1 loss, but it applies similarly to bounded loss functions.\nLemma A.5 (Sample compression generalization bound) Let a sample compression scheme (\u03ba, \u03c1), and a loss function \u2113 : R\u00d7 R \u2192 [0, 1]. In the realizable case, for any \u03ba(S) . m, any \u03b4 \u2208 (0, 1), and any distribution D over X \u00d7 {0, 1}, for S \u223c Dm, with probability 1\u2212 \u03b4,\nErr(\u03c1(\u03ba(S));D) \u2264 E\u0302rr(\u03c1(\u03ba(S));S) +O ( |\u03ba(S)| log(m) + log 1\u03b4\nm\n) .\nThe following generalization for sample compression in the agnostic case was proven by Graepel et al. [34]. Their proof is for the 0-1 loss, but it applies similarly to bounded loss functions. We use it with the \u03b7-ball robust loss.\nLemma A.6 (Agnostic sample compression generalization bound) Let a sample compression scheme (\u03ba, \u03c1), and a loss function \u2113 : R \u00d7 R \u2192 [0, 1]. In the agnostic case, for any \u03ba(S) . m, any \u03b4 \u2208 (0, 1), and any distribution D over X \u00d7 {0, 1}, for S \u223c Dm, with probability 1\u2212 \u03b4,\nErr(\u03c1(\u03ba(S));D) \u2264 E\u0302rr(\u03c1(\u03ba(S));S) +O\n  \u221a( |\u03ba(S)| log(m) + log 1\u03b4 )\nm\n  ."
        },
        {
            "heading": "B Proofs for Section 3: Robust Regression for \u2113p Losses",
            "text": "Proof Proof of Theorem 3.1 Fix \u01eb, \u03b4 \u2208 (0, 1). Let H \u2286 [0, 1]X . Fix a distribution D over X \u00d7Y, and let S = {(xi, yi)}mi=1 be an i.i.d. sample from D. We elaborate on each one of the steps as described in Algorithm 1.\n1. Compute h\u22c6 \u2190 \u2113p-RERMH(S) in order to get the set of cutoffs \u03b7(x, y) = supz\u2208U(x) |h\u22c6(z) \u2212 y|p for (x, y) \u2208 S. Let \u03b7S = ((\u03b7(x1, y1)) , . . . , (\u03b7(xm, ym))). Our goal is to construct a predictor with an empirical robust loss of \u03b7(x, y) + \u01eb for any (x, y) \u2208 S, for the \u2113p loss, which means that our predictor is an approximate Robust ERM.\n2. Define the inflated training data set\nSU = \u22c3\ni\u2208[n]\n{ (z, yI(z)) : z \u2208 U(xi) } ,\nwhere I(z) = min{i \u2208 [m] : z \u2208 U(xi)}. For (z, y) \u2208 SU , let \u03b7(z, y) be the \u03b7(x, y) for which z \u2208 U(x) and yI(z) = y.\n3. Discretize SU to a finite set S\u0304U as following.\n(a) Define a set of functions, such that each function is defined by \u03b7-RERMH optimizer on d =\nO (\n1 \u01eb fat(H, \u01eb/8p) log\n2 (\np\u00b7fat(H,\u01eb/8p) \u01eb2\n)) points from S.\nH\u0302 = {\u03b7-RERMH(S\u2032,\u03b7S\u2032 , p) : S\u2032 \u2286 S, |S\u2032| = d} . Recall the definition of \u03b7-RERMH, see Eq. (5). The cardinality of this class is bounded as follows\n|H\u0302| \u2248 ( m\nd ) . (m d )d . (11)\n(b) A discretization S\u0304U \u2286 SU will be defined by covering of the dual class in d\u221e norm. Let Lp H\u0302\nbe the Lp loss class of H\u0302, namely, Lp H\u0302\n= { Z \u00d7 Y \u220b (z, y) 7\u2192 |h(z)\u2212 y|p : h \u2208 H\u0302 } . The dual\nclass of Lp H\u0302 , Lp H\u0302 \u2217 \u2286 [0, 1]H\u0302, is defined as the set of all functions f(z,y) : H\u0302 \u2192 [0, 1] such that f(z,y)(h) = \u2223\u2223h(z) \u2212 y \u2223\u2223p, for any (z, y) \u2208 SU . Formally, Lp\nH\u0302\n\u2217 = { f(z,y) : (z, y) \u2208 SU } , where\nf(z,y) = ( f(z,y)(h1), . . . , f(z,y)(h|H\u0302|) ) . We take S\u0304U \u2286 SU to be a minimal \u01eb-cover for SU in d\u221e,\nsup (z,y)\u2208SU inf (z\u0304,y\u0304)\u2208S\u0304U\n\u2225\u2225f(z,y) \u2212 f(z\u0304,y\u0304) \u2225\u2225 \u221e \u2264 \u01eb. (12)\nLet fat\u2217(H, \u01eb) be the dual \u01eb-fat-shattering of H. Using Lemma A.4, we can bound the dual fat-shattering of the LpH loss class by the fat-shattering of H,\nfat\u2217(LpH, \u01eb) \u2264 log2(m) fat\u2217(H, \u01eb/p) . (13)\nApplying a covering number argument from Lemma A.3 (taking a = 1) on the dual space, and upper bounding the dual fat-shattering of the Lp loss class as in Eq. (13), we have the following bound\n\u2223\u2223S\u0304U \u2223\u2223 = N (\u01eb, SU , d\u221e)\n. exp ( fat\u2217(LpH, c\u01eb) log ( |H\u0302|\n\u01eb \u00b7 fat\u2217(LpH, c\u01eb)\n) log ( |H\u0302|\nfat\u2217(LpH, c\u01eb)\n))\n. exp ( fat\u2217(H, c\u01eb/p) log ( |H\u0302|\n\u01eb \u00b7 fat\u2217(H, c\u01eb/p)\n) log ( |H\u0302|\nfat\u2217(H, c\u01eb/p)\n) log2(m) )\n. exp ( fat\u2217(H, c\u01eb/p) log2 ( |H\u0302|\n\u01eb \u00b7 fat\u2217(H, c\u01eb/p)\n) log2(m) ) ,\n(14)\nwhere c \u2208 (0,\u221e) is a numerical constant, derived from the covering argument in Lemma A.3.\n4. Compute the following variant of Multiplicative Wights (MW) algorithm on the discretized set S\u0304U for\nT \u2248 log \u2223\u2223S\u0304U \u2223\u2223. Let d = O ( 1 \u01eb fat(H, \u01eb/8p) log 2 ( p\u00b7fat(H,\u01eb/8p) \u01eb2 )) , and let \u03b7(z, y) be the \u03b7(x, y) for which z \u2208 U(x) as defined in step 2. From Corollary A.2, taking \u03b4 = 1/3, \u03b2 = \u01eb/p, we know that for any distribution P on S\u0304U , upon receiving an i.i.d. sample S\u2032\u2032 from P of size d, with probability 2/3 over sampling S\u2032\u2032 from P , for any h \u2208 H with \u2200(z, y) \u2208 S\u2032\u2032 : |h(z)\u2212 y|p \u2264 \u03b7(z, y), it holds that P(z,y)\u223cP{(z, y) : |h(z)\u2212 y|p \u2264 \u03b7(z, y) + \u01eb} \u2265 1 \u2212 \u01eb. We can conclude that for any distribution P on S\u0304U , there exists such a set of points S\u2032\u2032 \u2286 S\u0304U . Given that set, we can find the function with the aforementioned property in H\u0302. Let S\u2032 be the d points in S that the perturbed points S\u2032\u2032 originated from. That is, S\u2032\u2032 \u2286 \u22c3(x,y)\u2208S\u2032\n\u22c3{(z, y) : z \u2208 U(x)}. Take H\u0302 \u220b h\u0302 = \u03b7-RERMH(S\u2032,\u03b7S\u2032 , p), it holds that \u2200(z, y) \u2208 S\u2032\u2032 : \u2223\u2223\u2223h\u0302(z)\u2212 y \u2223\u2223\u2223 p \u2264 \u03b7(z, y), as a result we get\nP(z,y)\u223cP\n{ (z, y) : \u2223\u2223\u2223h\u0302(z)\u2212 y \u2223\u2223\u2223 p \u2264 \u03b7(z, y) + \u01eb } \u2265 1\u2212 \u01eb.\nAlgorithm 4 Modified Multiplicative Weights Input: H, S, S\u0304U . Parameters: \u01eb, T,\u03b7S = (\u03b7(x1, y1), . . . , \u03b7(xm, ym)) for (xi, yi) \u2208 S. Algorithms used: Robust ERM for the \u03b7-ball robust loss: \u03b7-RERMH (Eq. (5)).\nInitialize P1 = Uniform(S\u0304U ), d = O ( 1 \u01eb fat(H, \u01eb/8p) log 2 ( p\u00b7fat(H,\u01eb/8p) \u01eb2 )) , \u03b7(z, y) is the \u03b7(x, y) for which z \u2208 U(x). For t = 1, . . . , T :\n\u22b2 Compute a strong base learner w.r.t. distribution Pt by finding n points in S and executing \u03b7-RERMH on them. (a) Find d points S\u2032\u2032t \u2286 S\u0304U such that any h \u2208 H satisfying: \u2200(z, y) \u2208 S\u2032\u2032t : |h(z)\u2212 y|p \u2264 \u03b7(z, y), it holds that E(z,y)\u223cPt [ I { |h(z)\u2212 y|p \u2264 \u03b7(z, y) + \u01eb }] \u2265 1\u2212 \u01eb. (See the analysis for why this set exists).\n(b) Let S\u2032t be the d points in S that S \u2032\u2032 t originated from. Formally, S \u2032\u2032 t \u2286 \u22c3 (x,y)\u2208S\u2032t \u22c3{(z, y) : z \u2208 U(x)}.\n(c) Compute h\u0302t = \u03b7-RERMH(S\u2032t,\u03b7S\u2032t , p).\n\u22b2 Make a multiplicative weights update on Pt. (d) For each (z, y) \u2208 S\u0304U :\nPt+1(z, y) \u221d Pt(z, y)e\u2212\u03beI{|h\u0302t(z)\u2212y| p \u2264\u03b7(z,y)}\nOutput: classifiers h\u03021, . . . , h\u0302T and sets S\u20321, . . . , S \u2032 T .\nA uniformly 4\u01eb-approximate adversarially robust sample compression scheme for S. The output of the algorithm is a sequence of functions h\u03021, . . . , h\u0302T , and the corresponding sets that encode them S\u20321, . . . , S \u2032 T , where we predict with the average of the returned hypotheses, 1 T \u2211T i=1 h\u0302i(\u00b7). For\nT \u2248 log \u2223\u2223S\u0304U \u2223\u2223, we show that we have\n\u2200(z\u0304, y\u0304) \u2208 S\u0304U : 1\nT\nT\u2211\ni=1\n\u2223\u2223\u2223h\u0302i(z\u0304)\u2212 y\u0304 \u2223\u2223\u2223 p \u2264 \u03b7(z\u0304, y\u0304) + 2\u01eb. (15)\nFor any distribution Pt over S\u0304U , we have a base learner h\u0302, satisfying E(z\u0304,y\u0304)\u223cPt [ I {\u2223\u2223\u2223h\u0302(z\u0304)\u2212 y\u0304 \u2223\u2223\u2223 p\n\u2264 \u03b7(z\u0304, y\u0304) + \u01eb }] \u2265 1\u2212\u01eb, due to Theorem 3.4. Following standard analysis of MW / \u03b1-Boost (see Schapire and Freund\n[56, Section 6]), for any (z\u0304, y\u0304) \u2208 S\u0304U , 1 \u2212 \u01eb fraction of the base learners has an error within \u03b7(z\u0304, y\u0304) + \u01eb. The loss is bounded by 1, so the other \u01eb fraction can add an error of at most \u01eb. The overall average loss of the base learners is upper bounded by \u03b7(z\u0304, y\u0304) + 2\u01eb. Note that we can find these base learners in H\u0302, as defined in step 2(a) of the main algorithm. Crucially, we use strong base learners in order to ensure a low empirical loss of the average base learners.\nFrom the covering argument (Eq. (12)), we have\n\u2200(z, y) \u2208 SU : 1\nT\nT\u2211\ni=1\n\u2223\u2223\u2223h\u0302i(z)\u2212 y \u2223\u2223\u2223 p \u2264 \u03b7(z, y) + 4\u01eb. (16)\nIndeed, for any (z, y) \u2208 SU there exists (z\u0304, y\u0304) \u2208 S\u0304U , such that for any h \u2208 H, \u2223\u2223\u2223 \u2223\u2223h(z)\u2212 y \u2223\u2223p \u2212 \u2223\u2223h(z\u0304)\u2212 y\u0304 \u2223\u2223p \u2223\u2223\u2223 \u2264 \u01eb.\nSpecifically, it holds for { h\u03021, . . . , h\u0302T } \u2286 H and h\u22c6 \u2208 H. Using the triangle inequality we have\n1\nT\nT\u2211\ni=1\n\u2223\u2223\u2223h\u0302i(z)\u2212 y \u2223\u2223\u2223 p\n\u2264 1 T\nT\u2211\ni=1\n\u2223\u2223\u2223h\u0302i(z\u0304)\u2212 y\u0304 \u2223\u2223\u2223 p + \u01eb, (17)\nand\n\u03b7(z\u0304, y\u0304) = \u2223\u2223h\u22c6(z\u0304)\u2212 y\u0304 \u2223\u2223p \u2264 \u2223\u2223h\u22c6(z)\u2212 y \u2223\u2223p + \u01eb = \u03b7(z, y). (18)\nCombining Eqs. (17) and (18) we get Eq. (16). Finally, using the convexity of the \u2113p loss, we have \u2223\u2223\u2223\u2223\u2223 1 T T\u2211\ni=1\nh\u0302i(z)\u2212 y \u2223\u2223\u2223\u2223\u2223 p \u2264 1 T T\u2211\ni=1\n\u2223\u2223\u2223h\u0302i(z)\u2212 y \u2223\u2223\u2223 p . (19)\nFinally, from Eqs. (16) and (19) we conclude a uniformly 4\u01eb-approximate adversarially robust sample compression scheme for S,\n\u2200(z, y) \u2208 SU : \u2223\u2223\u2223\u2223\u2223 1 T T\u2211\ni=1\nh\u0302i(z)\u2212 y \u2223\u2223\u2223\u2223\u2223 p \u2264 \u03b7(z, y) + 4\u01eb, (20)\nwhich implies that\n\u2200(x, y) \u2208 S : sup z\u2208U(x) \u2223\u2223\u2223\u2223\u2223 1 T T\u2211\ni=1\nh\u0302i(x) \u2212 y \u2223\u2223\u2223\u2223\u2223 p \u2264 \u03b7(x, y) + 4\u01eb.\nWe summarize the compression size. We have have T = O ( log \u2223\u2223S\u0304U \u2223\u2223) predictors, where each one is\nrepresentable by d = O (\n1 \u01eb fat(H, \u01eb/8p) log\n2 (\np\u00b7fat(H,\u01eb/8p) \u01eb2\n)) points. By counting the number of predictors\nusing Eq. (14), we get\nlog (\u2223\u2223S\u0304U \u2223\u2223) . exp ( fat\u2217(H, c\u01eb/p/p) log2 ( |H\u0302|\n\u01eb \u00b7 fat\u2217(H, c\u01eb/p/p)\n) log2(m) )\n. fat\u2217(H, c\u01eb/p) log2 (\n|H\u0302| \u01eb \u00b7 fat\u2217(H, c\u01eb/p)\n) log2(m)\n. fat\u2217(H, c\u01eb/p) log2 (\n1 \u01eb \u00b7 fat\u2217(H, c\u01eb/p) (m d )d) log2(m)\n. fat\u2217(H, c\u01eb/p) ( log ( 1\n\u01eb \u00b7 fat\u2217(H, c\u01eb/p)\n) + d log (m d ))2 log2(m)\n. fat\u2217(H, c\u01eb/p) ( log2 ( 1\n\u01eb \u00b7 fat\u2217(H, c\u01eb/p)\n) + d log ( 1\n\u01eb \u00b7 fat\u2217(H, c\u01eb/p) ) log (m d ) + d2 log2 (m d )) log2(m)\n. fat\u2217(H, c\u01eb/p)d2 log2 (m d ) log2 ( 1 \u01eb \u00b7 fat\u2217(H, c\u01eb/p) ) log2(m) .\n(21)\nWe get a uniformly 4\u01eb-approximate adversarially robust sample compression scheme for S of size\nO ( fat\u2217(H, c\u01eb/p)d3 log2 (m d ) log2 ( 1 \u01eb \u00b7 fat\u2217(H, c\u01eb/p) ) log2(m) ) .\nBy plugging in d = O (\n1 \u01eb fat(H, \u01eb/8p) log\n2 (\np\u00b7fat(H,\u01eb/8p) \u01eb2\n)) , we have\nO (\n1 \u01eb3 fat\n3(H, \u01eb/8p) fat\u2217(H, c\u01eb/p) log6 (\np\u00b7fat(H,\u01eb/8p) \u01eb2\n) log2 ( m\n1 \u01eb fat(H,\u01eb/8p) log 2( p\u00b7fat(H,\u01eb/8p) \u01eb2 )\n) log2 ( 1\n\u01eb\u00b7fat\u2217(H,c\u01eb/p)\n) log2(m) ) .\nLet (\u03ba, \u03c1) be the compression scheme and |\u03ba(S)| the compression size. Let E\u0302rr\u2113p(h;S) be the empirical loss of h on S with the \u2113p robust loss. We can derive the error as follows,\nErr\u2113p(\u03c1(\u03ba(S));D) (i) . E\u0302rr\u2113p(\u03c1(\u03ba(S));S) +\n\u221a( |\u03ba(S)| log(m) + log 1\u03b4 )\nm\n(ii)\n. E\u0302rr\u2113p(h \u22c6;S) + 4\u01eb+\n\u221a( |\u03ba(S)| log(m) + log 1\u03b4 )\nm\n(iii)\n. Err\u2113p(h \u22c6;D) + 4\u01eb+\n\u221a( |\u03ba(S)| log(m) + log 1\u03b4 )\nm + \u221a log 1\u03b4 m\n. Err\u2113p(h \u22c6;D) + 4\u01eb+\n\u221a( |\u03ba(S)| log(m) + log 1\u03b4 )\nm ,\n(i) follows from a generalization of sample compression scheme in the agnostic case, see Lemma A.6, (ii) follows Eq. (20), (iii) follows from Hoeffding\u2019s inequality.\nTake m sufficiently large such that \u221a |\u03ba(S)| log(m) + log 1\u03b4\nm . \u01eb.\nRe-scale \u01eb = \u01eb/5 and plug in the compression size, we get sample complexity of size\nM = O ( 1\n\u01eb2\n( |\u03ba(S)| log 1\n\u01eb + log\n1\n\u03b4\n)) ,\nwhere |\u03ba(S)| is upper bounded as follows O (\n1 \u01eb3 fat\n3(H, \u01eb/8p) fat\u2217(H, c\u01eb/p) log6 (\np\u00b7fat(H,\u01eb/8p) \u01eb2\n) log2 ( m\n1 \u01eb fat(H,\u01eb/8p) log 2( p\u00b7fat(H,\u01eb/8p) \u01eb2 )\n) log2 ( 1\n\u01eb\u00b7fat\u2217(H,c\u01eb/p)\n) log2(m) ) .\nWe conclude the sample complexity\nM = O\u0303 ( 1\n\u01eb5 fat3(H, c\u01eb/p) fat\u2217(H, c\u01eb/p) + 1 \u01eb2 log 1 \u03b4\n) ,\nfor some numerical constant c \u2208 (0,\u221e)."
        },
        {
            "heading": "C Proofs for Section 4: Better Sample Complexity for the \u21131 Loss",
            "text": "Proof Proof of Theorem 4.1 Fix \u01eb, \u03b4 \u2208 (0, 1). Let H \u2286 [0, 1]X . Fix a distribution D over X \u00d7Y, and let S = {(xi, yi)}mi=1 be an i.i.d. sample from D. We elaborate on each one of the steps as described in Algorithm 2.\n1. Compute h\u22c6 \u2190 \u21131-RERMH(S) in order to get the set of cutoffs \u03b7(x, y) = supz\u2208U(x) |h\u22c6(z) \u2212 y| for (x, y) \u2208 S. Let \u03b7S = (\u03b7(x1, y1), . . . , \u03b7(xm, ym)). Our goal is to construct a predictor with an empirical robust loss of \u03b7(x, y)+ \u01eb for any (x, y) \u2208 S, which means that our predictor is an approximate Robust ERM.\n2. Define the inflated training data set\nSU = \u22c3\ni\u2208[n]\n{ (z, yI(z)) : z \u2208 U(xi) } ,\nwhere I(z) = min{i \u2208 [m] : z \u2208 U(xi)}. For (z, y) \u2208 SU , let \u03b7(z, y) be the \u03b7(x, y) for which z \u2208 U(x) and yI(z) = y.\n3. Discretize SU to a finite set S\u0304U as following.\n(a) Define a set of functions, such that each function is defined by \u03b7-RERMH optimizer on d =\nO ( fat(H, \u01eb/8) log2 ( fat(H,\u01eb/8)\n\u01eb2\n)) points from S.\nH\u0302 = {\u03b7-RERMH(S\u2032,\u03b7S\u2032 , 1) : S\u2032 \u2286 S, |S\u2032| = d} .\nRecall the definition of \u03b7-RERMH, see Eq. (5). In order to understand what this definition of H\u0302 serves for, see step 4 below. The cardinality of this class is bounded as follows\n|H\u0302| \u2248 ( m\nd ) . (m d )d . (22)\n(b) A discretization S\u0304U \u2286 SU will be defined by covering of the dual class in d\u221e norm. Let L1 H\u0302\nbe the L1 loss class of H\u0302, namely, L1 H\u0302\n= { Z \u00d7 Y \u220b (z, y) 7\u2192 |h(z)\u2212 y| : h \u2208 H\u0302 } . The dual\nclass of L1 H\u0302 , L1 H\u0302 \u2217 \u2286 [0, 1]H\u0302, is defined as the set of all functions f(z,y) : H\u0302 \u2192 [0, 1] such that f(z,y)(h) = \u2223\u2223h(z) \u2212 y \u2223\u2223, for any (z, y) \u2208 SU . Formally, L1\nH\u0302\n\u2217 = { f(z,y) : (z, y) \u2208 SU } , where\nf(z,y) = ( f(z,y)(h1), . . . , f(z,y)(h|H\u0302|) ) . We take S\u0304U \u2286 SU to be a minimal \u01eb-cover for SU in d\u221e,\nsup (z,y)\u2208SU inf (z\u0304,y\u0304)\u2208S\u0304U\n, \u2225\u2225f(z,y) \u2212 f(z\u0304,y\u0304) \u2225\u2225 \u221e \u2264 \u01eb. (23)\nLet fat\u2217(H, \u01eb) be the dual \u01eb-fat-shattering of H. Using Lemma A.4, we can bound the dual fat-shattering of the L1H loss class by the fat-shattering of H,\nfat\u2217 ( L1H, \u01eb ) \u2264 log2(m) fat\u2217(H, \u01eb) . (24)\nBy applying a covering number argument from Lemma A.3 (taking a = 1) on the dual space, and upper bounding the dual fat-shattering of the L1 loss class as in Eq. (24), we have the following bound\n\u2223\u2223S\u0304U \u2223\u2223 = N (\u01eb, SU , d\u221e)\n. exp ( fat\u2217 ( L1H, c\u01eb ) log ( |H\u0302|\n\u01eb \u00b7 fat\u2217(L1H, c\u01eb)\n) log ( |H\u0302|\nfat\u2217(L1H, c\u01eb)\n))\n. exp ( fat\u2217(H, c\u01eb) log ( |H\u0302|\n\u01eb \u00b7 fat\u2217(H, c\u01eb)\n) log ( |H\u0302|\nfat\u2217(H, c\u01eb)\n) log2(m) )\n. exp ( fat\u2217(H, c\u01eb) log2 ( |H\u0302|\n\u01eb \u00b7 fat\u2217(H, c\u01eb)\n) log2(m) ) ,\n(25)\nwhere c \u2208 (0,\u221e) is a numerical constant, derived from the covering argument in Lemma A.3.\n4. Compute a modified version of the real-valued boosting algorithm MedBoost [41, 36] on the discretized set S\u0304U . The output of the algorithm is a uniformly \u01eb-approximate sample compression scheme for the set S\u0304U , for \u2248 log (\u2223\u2223S\u0304U \u2223\u2223) boosting rounds. Moreover, the weak learners are chosen from the set H\u0302.\nOnce we have these weak learners, the guarantee of the algorithm follows from Hanneke et al. [36, Corollary 6]. We should explain why we have a weak learner for any distribution over S\u0304U . The existence of weak learners in H\u0302. Let d = O ( fat(H, \u01eb/8) log2 ( fat(H,\u01eb/8)\n\u01eb2\n)) and let \u03b7(z, y)\nbe the \u03b7(x, y) for which z \u2208 U(x) as defined in step 2. Taking \u03b4 = 1/3, we know that for any distribution P on S\u0304U , upon receiving an i.i.d. sample S\u2032\u2032 from P of size d, with probability 2/3 over sampling S\u2032\u2032 from P , for any h \u2208 H satisfying \u2200(z, y) \u2208 S\u2032\u2032 : |h(z)\u2212 y| \u2264 \u03b7(z, y), it holds that P(z,y)\u223cP((z, y) : |h(z)\u2212 y| > \u03b7(z, y) + \u01eb) \u2264 1/3. That is, such a function is a (\u01eb, 1/6)-weak learner for P and h\u22c6 (computed in step 1). We can conclude that for any distribution P on S\u0304U , there exists a set of points S\u2032\u2032 \u2286 S\u0304U of size d that defines a weak learner for P and h\u22c6.\nFurthermore, we can find these weak learners in H\u0302 as follows. Let S\u2032 be the d points in S that the perturbed points S\u2032\u2032 originated from. That is, S\u2032\u2032 \u2286 \u22c3(x,y)\u2208S\u2032 \u22c3{(z, y) : z \u2208 U(x)}. Therefore, we can conclude that h\u0302 = \u03b7-RERMH(S\u2032,\u03b7S\u2032) is a weak learner, and can be found in H\u0302. So, we can think of H\u0302 as a pool of weak learners for any possible distribution over the discretized set S\u0304U .\nAlgorithm 5 Modified MedBoost Input: H, S, S\u0304U . Parameters: \u01eb, T,\u03b7S = (\u03b7(x1, y1), . . . , \u03b7(xm, ym)) for (xi, yi) \u2208 S. Algorithms used: Robust ERM for the \u03b7-ball robust loss: \u03b7-RERMH (Eq. (5)).\nInitialize P1 = Uniform(S\u0304U ), d = O ( fat(H, \u01eb/8) log2 ( fat(H,\u01eb/8)\n\u01eb2\n)) , \u03b7(z, y) is the \u03b7(x, y) for which z \u2208\nU(x) as defined in step 2 of the main algorithm. For t = 1, . . . , T :\n\u22b2 Compute a weak base learner w.r.t. distribution Pt by finding d points in S and executing \u03b7-RERMH on them.\n(a) Find d points S\u2032\u2032t \u2286 S\u0304U such that any h \u2208 H satisfying: \u2200(z, y) \u2208 S\u2032\u2032t : |h(z)\u2212 y| \u2264 \u03b7(z, y), it holds that E(z,y)\u223cPt [ I { |h(z)\u2212 y| \u2265 \u03b7(z, y) + \u01eb }] \u2264 1/3. (See the analysis for why this set exists).\n(b) Let S\u2032t be the d points in S that S \u2032\u2032 t originated from. Formally, S \u2032\u2032 t \u2286 \u22c3 (x,y)\u2208S\u2032t \u22c3{(z, y) : z \u2208 U(x)}.\n(c) Compute h\u0302t = \u03b7-RERMH(S\u2032t,\u03b7S\u2032t , 1). From steps (a) and (b), it follows that h\u0302t is a (\u01eb, 1/6)-weak learner with respect to the distribution Pt over S\u0304U . \u22b2 Update the weight of the weak learner in the ensemble and make a multiplicative weights update on Pt. (d) For i = 1, . . . , n = \u2223\u2223S\u0304U \u2223\u2223:\ni. Set w\n(t) i = 1\u2212 2I [\u2223\u2223\u2223h\u0302t(zi)\u2212 yi \u2223\u2223\u2223 > \u03b7(zi, yi) + \u01eb ] .\nii. Set\n\u03b1t = 1\n2 log\n  (1\u2212 1/6)\u2211ni=1 Pt(zi, yi) I [ w (t) i = 1 ]\n(1 + 1/6) \u2211n i=1 Pt(zi, yi) I [ w (t) i = \u22121 ]\n  .\niii. \u2022 If \u03b1t =\u221e: return T copies of ht, (\u03b11 = 1, . . . , \u03b1T = 1), and S\u2032t. \u2022 Else:\nPt+1(zi, yi) = Pt(zi, yi) exp(\u2212\u03b1twti)\u2211n\nj=1 Pt(zj, yj) exp ( \u2212\u03b1twtj ) .\nOutput: Hypotheses h\u03021, . . . , h\u0302T , coefficients \u03b11, . . . , \u03b1T and sets S\u20321, . . . , S \u2032 T .\nA uniformly 3\u01eb-approximate adversarially robust sample compression scheme for S. The output of MedBoost is a uniformly \u01eb-approximate sample compression scheme for the set S\u0304U . We show that this is a uniformly 2\u01eb-approximate adversarially robust sample compression scheme for S, that is, a sample compression for S scheme with respect to the robust loss. For T \u2248 log \u2223\u2223S\u0304U \u2223\u2223 boosting rounds, it follows from Hanneke et al. [36, Corollary 6] that the output of the algorithm satisfy\n\u2200(z\u0304, y\u0304) \u2208 S\u0304U : \u2223\u2223\u2223Med ( h\u03021(z\u0304), . . . , h\u0302T (z\u0304);\u03b11, . . . , \u03b1T ) \u2212 y\u0304 \u2223\u2223\u2223 \u2264 \u03b7(z\u0304, y\u0304) + \u01eb, (26)\nMed ( h\u03021(z\u0304), . . . , h\u0302T (z\u0304);\u03b11, . . . , \u03b1T ) is the weighted median of h\u03021, . . . , h\u0302T with weights \u03b11, . . . , \u03b1T . From the covering argument (Eq. (23)), this implies that\n\u2200(z, y) \u2208 SU : \u2223\u2223\u2223Med ( h\u03021(z), . . . , h\u0302T (z);\u03b11, . . . , \u03b1T ) \u2212 y \u2223\u2223\u2223 \u2264 \u03b7(z, y) + 3\u01eb. (27)\nIndeed, for any (z, y) \u2208 SU there exists (z\u0304, y\u0304) \u2208 S\u0304U , such that for any h \u2208 H, \u2223\u2223\u2223 \u2223\u2223h(z)\u2212 y \u2223\u2223\u2212 \u2223\u2223h(z\u0304)\u2212 y\u0304 \u2223\u2223 \u2223\u2223\u2223 \u2264 \u01eb.\nSpecifically, it holds for { h\u03021, . . . , h\u0302T } \u2286 H and h\u22c6 \u2208 H.\nSo, \u2223\u2223\u2223Med ( h\u03021(z), . . . , h\u0302T (z);\u03b11, . . . , \u03b1T ) \u2212 y \u2223\u2223\u2223 (a)= \u2223\u2223\u2223Med ( h\u03021(z)\u2212 y, . . . , h\u0302T (z)\u2212 y;\u03b11, . . . , \u03b1T\n)\u2223\u2223\u2223 (b)\n\u2264 \u2223\u2223\u2223Med ( h\u03021(z\u0304)\u2212 y\u0304, . . . , h\u0302T (z\u0304)\u2212 y\u0304;\u03b11, . . . , \u03b1T )\u2223\u2223\u2223+ \u01eb (c) = \u2223\u2223\u2223Med ( h\u03021(z\u0304), . . . , h\u0302T (z\u0304);\u03b11, . . . , \u03b1T ) \u2212 y\u0304 \u2223\u2223\u2223+ \u01eb\n(d) \u2264 |h\u22c6(z\u0304)\u2212 y\u0304|+ 2\u01eb (e) \u2264 |h\u22c6(z)\u2212 y|+ 3\u01eb (f) = \u03b7(z, y) + 3\u01eb,\n(28)\n(a)+(c) follow since the median is translation invariant, (b)+(e) follow from the covering argument, (d) holds since the returned function by MedBoost is a uniformly \u01eb-approximate sample compression for S\u0304U , (f) follows from the definition in step 2 of the algorithm.\nFinally, from Eq. (28) we conclude a uniformly 3\u01eb-approximate adversarially robust sample compression scheme for S,\n\u2200(x, y) \u2208 S : sup z\u2208U(x)\n\u2223\u2223\u2223Med ( h\u03021(z), . . . , h\u0302T (z);\u03b11, . . . , \u03b1T ) \u2212 y \u2223\u2223\u2223 \u2264 \u03b7(x, y) + 3\u01eb. (29)\nWe summarize the compression size. We have have T = O ( log \u2223\u2223S\u0304U \u2223\u2223) predictors, where each one is\nrepresentable by d = O ( fat(H, \u01eb/8) log2 ( fat(H,\u01eb/8)\n\u01eb\n)) points. By counting the number of predictors\nusing Eq. (25), we get\nlog (\u2223\u2223S\u0304U \u2223\u2223) . log ( exp ( fat\u2217(H, c\u01eb) log2 ( |H\u0302|\n\u01eb \u00b7 fat\u2217(H, c\u01eb)\n) log2(m) ))\n. fat\u2217(H, c\u01eb) log2 (\n|H\u0302| \u01eb \u00b7 fat\u2217(H, c\u01eb)\n) log2(m)\n. fat\u2217(H, c\u01eb) log2 (\n1 \u01eb \u00b7 fat\u2217(H, c\u01eb) (m d )d) log2(m)\n. fat\u2217(H, c\u01eb) ( log ( 1\n\u01eb \u00b7 fat\u2217(H, c\u01eb)\n) + d log (m d ))2 log2(m)\n. fat\u2217(H, c\u01eb) ( log2 ( 1\n\u01eb \u00b7 fat\u2217(H, c\u01eb)\n) + d log ( 1\n\u01eb \u00b7 fat\u2217(H, c\u01eb) ) log (m d ) + d2 log2 (m d )) log2(m) .\nAll together we have a compression of size O ( d log (\u2223\u2223S\u0304U \u2223\u2223)), which is already sufficient for deriving generalization. We can reduce further the number of predictors to be independent of the sample size, thereby reducing the sample compression size and improving the sample complexity.\n5. We follow the sparsification method suggested by Hanneke et al. [36]. The idea is that by sampling functions from the ensemble, we can guarantee via a uniform convergence for the dual space, that with high probability it is sufficient to have roughly \u2248 fat\u2217(H, \u03b2) predictors. For \u03b11, . . . , \u03b1T \u2208 [0, 1] with \u2211T t=1 \u03b1t = 1, we denote the categorical distribution by Cat(\u03b11, . . . , \u03b1T ),\nwhich is a discrete distribution on the set [T ] with probability of \u03b1t on t \u2208 [T ]. The inputs to the algorithm are \u03c4(x, y) = \u03b7(x, y) + 4\u01eb and k = O ( fat\u2217(H, c\u01eb) log2 ( fat\u2217(H, c\u01eb) /\u01eb2 )) , where c \u2208 (0,\u221e) is a numerical constant.\nAlgorithm 6 Sparsify\nInput: Hypotheses h\u03021, . . . , h\u0302T , coefficients \u03b11, . . . , \u03b1T , S = {(xi, yi}mi=1. Parameter: \u01eb, k, \u03c4 = (\u03c4(x1, y1), . . . , \u03c4(xm, ym)).\n(a) Let \u03b1\u2032t = \u03b1t /\u2211T s=1 \u03b1s.\n(b) Repeat:\ni. Sample (J1, . . . , Jk) \u223c Cat(\u03b1\u20321, . . . , \u03b1\u2032T)k. ii. Let F = {hJ1 , . . . , hJk}. iii. Until \u2200(x, y) \u2208 S : \u2223\u2223\u2223 { f \u2208 F : supz\u2208U(x)|f(z)\u2212 y| > \u03c4(x, y)\n}\u2223\u2223\u2223 < k/2. Output: Hypotheses hJ1 , . . . , hJk .\nThe sparsification method returns with high probability function {f1, . . . , fk}, such that \u2200(x, y) \u2208 S : sup\nz\u2208U(x)\n\u2223\u2223Med ( f1(x), . . . , fk(x) ) \u2212 y \u2223\u2223 \u2264 \u03b7(x, y) + 4\u01eb. (30)\nWe get a uniformly 4\u01eb-approximate adversarially robust sample compression scheme for S, where we have O ( fat\u2217(H, c\u01eb) log2 ( fat\u2217(H, c\u01eb) /\u01eb2 )) functions, and each function is representable by O ( fat(H, \u01eb/8) log2 ( fat(H, \u01eb/8)/\u01eb2 )) points, therefore, the compression set size is\nfat(H, \u01eb/8) fat\u2217(H, c\u01eb) log2 ( fat(H, \u01eb/8)\n\u01eb2\n) log2 ( fat\u2217(H, c\u01eb)\n\u01eb2\n) .\nLet (\u03ba, \u03c1) be the compression scheme and |\u03ba(S)| the compression size. Let E\u0302rr\u21131(h;S) be the empirical loss of h on S with the \u21131 robust loss. We can derive the error as follows,\nErr\u21131(\u03c1(\u03ba(S));D) (i) . E\u0302rr\u21131(\u03c1(\u03ba(S));S) +\n\u221a( |\u03ba(S)| log(m) + log 1\u03b4 )\nm\n(ii)\n. E\u0302rr\u21131(h \u22c6;S) + 4\u01eb+\n\u221a( |\u03ba(S)| log(m) + log 1\u03b4 )\nm\n(iii)\n. Err\u21131(h \u22c6;D) + 4\u01eb+\n\u221a( |\u03ba(S)| log(m) + log 1\u03b4 )\nm + \u221a log 1\u03b4 m\n. Err\u21131(h \u22c6;D) + 4\u01eb+\n\u221a( |\u03ba(S)| log(m) + log 1\u03b4 )\nm ,\n(i) follows from a generalization of sample compression scheme in the agnostic case, see Lemma A.6, (ii) follows Eq. (30), (iii) follows from Hoeffding\u2019s inequality.\nTake m sufficiently large such that \u221a |\u03ba(S)| log(m) + log 1\u03b4\nm . \u01eb.\nRe-scale \u01eb = \u01eb/5 and plug in the compression size, we get sample complexity of size\nM = O ( 1\n\u01eb2\n( fat(H, c\u01eb) fat\u2217(H, c\u01eb) log2 ( fat(H, c\u01eb)\n\u01eb2\n) log2 ( fat\u2217(H, c\u01eb)\n\u01eb2\n) log 1\n\u01eb + log\n1\n\u03b4\n)) ,\nfor some numerical constant c \u2208 (0,\u221e)."
        },
        {
            "heading": "D Proofs for Section 5: Robust (\u03b7, \u03b2)-Regression",
            "text": "D.1 Realizable\nProof Proof of Theorem 5.1 Fix \u01eb, \u03b4, \u03b2 \u2208 (0, 1) and \u03b7 \u2208 [0, 1]. Let H \u2286 [0, 1]X . Fix a distribution D over X \u00d7 Y, and let S = {(xi, yi)}mi=1 be an i.i.d. sample from D.\nWe elaborate on each one of the steps as described in Algorithm 3.\n1. Define the inflated training data set\nSU = \u22c3\ni\u2208[n]\n{ (z, yI(z)) : z \u2208 U(xi) } ,\nwhere I(z) = min{i \u2208 [m] : z \u2208 U(xi)}.\n2. Discretize SU to a finite set S\u0304U as following.\n(a) Define a set of functions, such that each function is robustly accurate on\nd = O ( fat(H, \u03b2/8) log2 ( fat(H,\u03b2/8)\n\u03b2\n)) points in S, with respect to the \u03b7-ball robust loss,\nH\u0302 = {\u03b7-RERMH(S\u2032) : S\u2032 \u2286 S, |S\u2032| = d} .\nRecall the definition of \u03b7-RERMH, see Eq. (5). In order to understand what this definition of H\u0302 serves for, see step 3 below. The cardinality of this class is bounded as follows\n|H\u0302| \u2248 ( m\nd ) . (m d )d . (31)\n(b) A discretization S\u0304U \u2286 SU will be defined by covering of the dual class in d\u221e norm. Let L1 H\u0302\nbe the L1 loss class of H\u0302, namely, L1 H\u0302\n= { Z \u00d7 Y \u220b (z, y) 7\u2192 |h(z)\u2212 y| : h \u2208 H\u0302 } . The dual\nclass of L1 H\u0302 , L1 H\u0302 \u2217 \u2286 [0, 1]H\u0302, is defined as the set of all functions f(z,y) : H\u0302 \u2192 [0, 1] such that f(z,y)(h) = \u2223\u2223h(z) \u2212 y \u2223\u2223, for any (z, y) \u2208 SU . Formally, L1\nH\u0302\n\u2217 = { f(z,y) : (z, y) \u2208 SU } , where\nf(z,y) = ( f(z,y)(h1), . . . , f(z,y)(h|H\u0302|) ) . We take S\u0304U \u2286 SU to be a minimal \u03b2-cover for SU in d\u221e,\nsup (z,y)\u2208SU inf (z\u0304,y\u0304)\u2208S\u0304U\n, \u2225\u2225f(z,y) \u2212 f(z\u0304,y\u0304) \u2225\u2225 \u221e \u2264 \u03b2. (32)\nLet fat\u2217(H, \u03b2) be the dual \u03b2-fat-shattering of H. Using Lemma A.4, we can bound the dual fat-shattering of the L1H loss class by the fat-shattering of H,\nfat\u2217 ( L1H, \u03b2 ) \u2264 log2(m) fat\u2217(H, \u03b2) . (33)\nBy applying a covering number argument from Lemma A.3 (taking a = 1) on the dual space, and upper bounding the dual fat-shattering of the L1 loss class as in Eq. (33), we have the following bound\n\u2223\u2223S\u0304U \u2223\u2223 = N (\u03b2, SU , d\u221e)\n. exp ( fat\u2217 ( L1H, c\u03b2 ) log ( |H\u0302|\n\u03b2 \u00b7 fat\u2217(L1H, c\u03b2)\n) log ( |H\u0302|\nfat\u2217(L1H, c\u03b2)\n))\n. exp ( fat\u2217(H, c\u03b2) log ( |H\u0302|\n\u03b2 \u00b7 fat\u2217(H, c\u03b2)\n) log ( |H\u0302|\nfat\u2217(H, c\u03b2)\n) log2(m) )\n. exp ( fat\u2217(H, c\u03b2) log2 ( |H\u0302|\n\u03b2 \u00b7 fat\u2217(H, c\u03b2)\n) log2(m) ) ,\n(34)\nwhere c \u2208 (0,\u221e) is a numerical constant, derived from the covering argument in Lemma A.3.\n3. Compute a modified version of the real-valued boosting algorithm MedBoost (Algorithm 5) on the discretized set S\u0304U . The inputs to the algorithm are as follows. Set \u01eb = \u03b2, \u03b7S = (\u03b7, . . . , \u03b7), and T \u2248 log (\u2223\u2223S\u0304U \u2223\u2223) rounds of boosting.\nThe output of the algorithm is a uniform \u01eb-approximate sample compression scheme for the set S\u0304U . Moreover, the weak learners are chosen from the set H\u0302. Once we have these weak learners, the guarantee of the algorithm follows from Hanneke et al. [36, Corollary 6]. We should explain why we have a weak learner for any distribution over S\u0304U .\nThe existence of weak learners in H\u0302. From Theorem A.1, taking \u01eb = \u03b4 = 1/3, we know that for any distribution P on S\u0304U , upon receiving an i.i.d. sample S\u2032\u2032 from P of size O ( fat(H, \u03b2/8) log2 ( fat(H,\u03b2/8)\n\u03b2\n)) , with probability 2/3 over sampling S\u2032\u2032 from P , for any h \u2208 H with\n\u2200(z, y) \u2208 S\u2032\u2032 : |h(z)\u2212 y| \u2264 \u03b7, it holds that P(z,y)\u223cP{(z, y) : |h(z)\u2212 y| > \u03b7 + \u03b2} \u2264 1/3. That is, such a function is a (\u03b7 + \u03b2, 1/6)-weak learner for P (see Definition 4.2). We can conclude that for any distribution P on S\u0304U , there exists a set of points S\u2032\u2032 \u2286 S\u0304U of size O ( fat(H, \u03b2/8) log2 ( fat(H,\u03b2/8)\n\u03b2\n))\nthat defines a weak learner for P . Moreover, we can find these weak learners in H\u0302 as follows. Let S\u2032 be the O ( fat(H, \u03b2/8) log2 ( fat(H,\u03b2/8)\n\u03b2\n)) points in S that the perturbed points S\u2032\u2032 originated from. That is,\nS\u2032\u2032 \u2286 \u22c3(x,y)\u2208S\u2032 \u22c3{(z, y) : z \u2208 U(x)}. Therefore, we can conclude that h\u0302 = \u03b7-RERMH(S\u2032) is a weak learner, and can be found in H\u0302. So, we can think of H\u0302 as a pool of weak learners for any possible distribution over the discretized set S\u0304U .\nA uniformly 3\u03b2-approximate adversarially robust sample compression scheme for S. The output of MedBoost is a uniformly \u03b2-approximate sample compression scheme for the set S\u0304U . We show that this is a uniformly 2\u03b2-approximate adversarially robust sample compression scheme for S, that is, a sample compression for S scheme with respect to the robust loss. For T \u2248 log \u2223\u2223S\u0304U \u2223\u2223 boosting rounds, it follows from Hanneke et al. [36, Corollary 6] that the output of the algorithm satisfy\n\u2200(z\u0304, y\u0304) \u2208 S\u0304U : \u2223\u2223\u2223Med ( h\u03021(z\u0304), . . . , h\u0302T (z\u0304);\u03b11, . . . , \u03b1T ) \u2212 y\u0304 \u2223\u2223\u2223 \u2264 \u03b7(z\u0304, y\u0304) + \u03b2, (35)\nMed ( h\u03021(z\u0304), . . . , h\u0302T (z\u0304);\u03b11, . . . , \u03b1T ) is the weighted median of h\u03021, . . . , h\u0302T with weights \u03b11, . . . , \u03b1T . From the covering argument (Eq. (23)), this implies that\n\u2200(z, y) \u2208 SU : \u2223\u2223\u2223Med ( h\u03021(z), . . . , h\u0302T (z);\u03b11, . . . , \u03b1T ) \u2212 y \u2223\u2223\u2223 \u2264 \u03b7(z, y) + 3\u03b2. (36)\nIndeed, for any (z, y) \u2208 SU there exists (z\u0304, y\u0304) \u2208 S\u0304U , such that for any h \u2208 H, \u2223\u2223\u2223 \u2223\u2223h(z)\u2212 y \u2223\u2223\u2212 \u2223\u2223h(z\u0304)\u2212 y\u0304 \u2223\u2223 \u2223\u2223\u2223 \u2264 \u03b2.\nSpecifically, it holds for { h\u03021, . . . , h\u0302T } \u2286 H and h\u22c6 \u2208 H.\nSo, \u2223\u2223\u2223Med ( h\u03021(z), . . . , h\u0302T (z);\u03b11, . . . , \u03b1T ) \u2212 y \u2223\u2223\u2223 (a)= \u2223\u2223\u2223Med ( h\u03021(z)\u2212 y, . . . , h\u0302T (z)\u2212 y;\u03b11, . . . , \u03b1T\n)\u2223\u2223\u2223 (b)\n\u2264 \u2223\u2223\u2223Med ( h\u03021(z\u0304)\u2212 y\u0304, . . . , h\u0302T (z\u0304)\u2212 y\u0304;\u03b11, . . . , \u03b1T )\u2223\u2223\u2223+ \u03b2 (c) = \u2223\u2223\u2223Med ( h\u03021(z\u0304), . . . , h\u0302T (z\u0304);\u03b11, . . . , \u03b1T ) \u2212 y\u0304 \u2223\u2223\u2223+ \u03b2\n(d) \u2264 |h\u22c6(z\u0304)\u2212 y\u0304|+ 2\u03b2 (e) \u2264 |h\u22c6(z)\u2212 y|+ 3\u03b2 (f) = \u03b7(z, y) + 3\u03b2,\n(a)+(c) follow since the median is translation invariant, (b)+(e) follow from the covering argument, (d) holds since the returned function by MedBoost is a uniformly \u03b2-approximate sample compression for S\u0304U , (f) follows from the definition in step 2 of the algorithm.\nFinally, from Eq. (36) we conclude a uniformly 3\u03b2-approximate adversarially robust sample compression scheme for S,\n\u2200(x, y) \u2208 S : sup z\u2208U(x)\n\u2223\u2223\u2223Med ( h\u03021(z), . . . , h\u0302T (z);\u03b11, . . . , \u03b1T ) \u2212 y \u2223\u2223\u2223 \u2264 \u03b7(x, y) + 3\u03b2. (37)\nWe summarize the compression size. We have have T = O ( log \u2223\u2223S\u0304U \u2223\u2223) predictors, where each one is\nrepresentable by d = O ( fat(H, \u03b2/8) log2 ( fat(H,\u03b2/8)\n\u03b2\n)) points. By counting the number of predictors\nusing Eq. (25), we get\nlog (\u2223\u2223S\u0304U \u2223\u2223) . log ( exp ( fat\u2217(H, c\u03b2) log2 ( |H\u0302|\n\u03b2 \u00b7 fat\u2217(H, c\u03b2)\n) log2(m) ))\n. fat\u2217(H, c\u03b2) log2 (\n|H\u0302| \u03b2 \u00b7 fat\u2217(H, c\u03b2)\n) log2(m)\n. fat\u2217(H, c\u03b2) log2 (\n1 \u03b2 \u00b7 fat\u2217(H, c\u03b2) (m d )d) log2(m)\n. fat\u2217(H, c\u03b2) ( log ( 1\n\u03b2 \u00b7 fat\u2217(H, c\u03b2)\n) + d log (m d ))2 log2(m)\n. fat\u2217(H, c\u03b2) ( log2 ( 1\n\u03b2 \u00b7 fat\u2217(H, c\u03b2)\n) + d log ( 1\n\u03b2 \u00b7 fat\u2217(H, c\u03b2) ) log (m d ) + d2 log2 (m d )) log2(m) .\nAll together we have a compression of size O ( d log (\u2223\u2223S\u0304U \u2223\u2223)), which is already sufficient for deriving generalization. We can reduce further the number of predictors to be independent of the sample size, thereby reducing the sample compression size and improving the sample complexity.\n4. Compute the sparsification method (Algorithm 6). The idea is that by sampling functions from the ensemble, we can guarantee via a uniform convergence for the dual space, that with high probability it is sufficient to have roughly \u2248 fat\u2217(H, \u03b2) predictors. Applying Hanneke et al. [36, Theorem 10] with the parameters \u03c4 = \u03b7 + 4\u03b2 and k = O ( fat\u2217(H, c\u03b2) log2(fat\u2217(H, c\u03b2) /\u03b2) ) , where c \u2208 (0,\u221e) is\na numerical constant, the sparsification method returns with high probability function {f1, . . . , fk}, such that\n\u2200(x, y) \u2208 S : sup z\u2208U(x)\n\u2223\u2223Med ( f1(z), . . . , fk(z) ) \u2212 y \u2223\u2223 \u2264 \u03b7 + 4\u03b2.\nWe get a uniformly 4\u03b2-approximate adversarially robust sample compression scheme for S, where we have O ( fat\u2217(H, c\u03b2) log2 ( fat\u2217(H, c\u03b2) /\u03b22 )) functions, and each function is representable by O ( fat(H, \u03b2/8) log2 ( fat(H, \u03b2/8) /\u03b22 )) points, therefore, the compression set size is\nfat(H, \u03b2/8) fat\u2217(H, c\u03b2) log2 ( fat(H, \u03b2/8)\n\u03b22\n) log2 ( fat\u2217(H, c\u03b2)\n\u03b22\n) .\nLet (\u03ba, \u03c1) be the compression scheme and |\u03ba(S)| the compression size. Let E\u0302rr\u21131(h;S) be the empirical loss of h on S with the \u21131 robust loss. We can derive the error as follows,\nErr\u21131(\u03c1(\u03ba(S));D) (i) . E\u0302rr\u21131(\u03c1(\u03ba(S));S) +\n\u221a( |\u03ba(S)| log(m) + log 1\u03b4 )\nm\n(ii)\n. E\u0302rr\u21131(h \u22c6;S) + 4\u03b2 +\n\u221a( |\u03ba(S)| log(m) + log 1\u03b4 )\nm\n(iii)\n. Err\u21131(h \u22c6;D) + 4\u03b2 +\n\u221a( |\u03ba(S)| log(m) + log 1\u03b4 )\nm + \u221a log 1\u03b4 m\n. Err\u21131(h \u22c6;D) + 4\u03b2 +\n\u221a( |\u03ba(S)| log(m) + log 1\u03b4 )\nm ,\n(i) follows from a generalization of sample compression scheme in the agnostic case, see Lemma A.6, (ii) follows Eq. (30), (iii) follows from Hoeffding\u2019s inequality.\nTake m sufficiently large such that \u221a |\u03ba(S)| log(m) + log 1\u03b4\nm . \u03b2.\nRe-scale \u03b2 = \u03b2/5 and plug in the compression size, we get sample complexity of size\nM = O ( 1\n\u03b22\n( fat(H, c\u03b2) fat\u2217(H, c\u03b2) log2 ( fat(H, c\u03b2)\n\u03b22\n) log2 ( fat\u2217(H, c\u03b2)\n\u03b22\n) log 1\n\u03b2 + log\n1\n\u03b4\n)) ,\nfor some numerical constant c \u2208 (0,\u221e).\nD.2 Agnostic\nProof Proof of Theorem 5.2 The construction follows a reduction to the realizable case similar to [25], which is for the non-robust zero-one loss. Moreover, we use a margin-based analysis of MedBoost algorithm (see K\u00e9gl [41, Theorem 1]), and overcome some technical challenges.\nDenote \u039bRE = \u039bRE(\u03b7, 1/3, 1/3,H,U , \u2113\u03b7U), the sample complexity of Robust (\u03b7, \u03b2)-regression for a class H with respect to a perturbation function U , taking \u01eb = \u03b4 = 1/3\nUsing a robust ERM, find the maximal subset S\u2032 \u2286 S with zero empirical robust loss (for the \u03b7-ball loss), such that infh\u2208H E\u0302rr\u03b7(h, f ;S\u2032) = 0. Now, \u039bRE samples suffice for weak robust learning for any distribution D on S\u2032.\nCompute the MedBoost on S\u2032, with T \u2248 log(|S\u2032|) boosting rounds, where each weak robust learner is trained on \u2248 \u039bRE samples. The returned weighted median h\u0302 = Med ( h\u03021(z), . . . , h\u0302T (z);\u03b11, . . . , \u03b1T )\nsatisfies E\u0302rr\u03b7 ( h\u0302, f ;S\u2032 ) = 0, and each hypothesis h\u0302t \u2208 { h\u03021, . . . , h\u0302T } is representable as set of size O(\u039bRE).\nThis defines a compression scheme of size \u039bRET , and h\u0302i can be reconstructed from a compression set of points from S of size \u039bRET .\nRecall that S\u2032 \u2286 S is a maximal subset such that infh\u2208H E\u0302rr\u03b7(h, f ;S\u2032) = 0 which implies that inf h\u0302\u2208H E\u0302rr\u03b7(h, f ;S\n\u2032) \u2264 infh\u2208H E\u0302rr\u03b7(h, f ;S\u2032). Plugging it into an agnostic sample compression bound Lemma A.6, we have a sample complexity of O\u0303 ( \u039bRE \u01eb2 ) , which translates into O\u0303 ( fat(H,c\u03b7)fat\u2217(H,c\u03b7) \u01eb2 ) , for some numerical constant c \u2208 (0,\u221e).\nD.3 Naive approach with a fixed cutoff\nAn agnostic learner for robust (\u03b7, \u03b2)-regression does not apply to the robust regression setting. The reason is that the optimal function in H has different scales of robustness on different points. we show that by using a fixed cutoff for all points we can obtain an error of \u221a OPTH + \u01eb.\nTheorem D.1 For any H \u2286 [0, 1]X with finite \u03b3-fat-shattering for all \u03b3 > 0, any U : X \u2192 2X , and any \u01eb, \u03b4 \u2208 (0, 1), \u03b7 \u2208 [0, 1], for some numerical constant c \u2208 (0,\u221e), with probability 1 \u2212 \u03b4, Algorithm 7 outputs a function with error at most \u221a OPTH + \u01eb, for the \u21131,U(\u00b7) robust loss, and using a sample of size\nO\u0303 ( fat(H, c\u01eb) fat\u2217(H, c\u01eb)\n\u01eb2 +\n1 \u01eb2 log 1 \u03b4\n) .\nRecall that fat\u2217(F , \u01eb) . 1\u01eb2fat(F ,\u01eb/2)+1 by Eq. (7).\nAlgorithm 7\nInput: H \u2286 [0, 1]X , S = {(xi, yi)}mi=1, S\u0303 = {(xi, yi)} n i=1. Algorithms used: Agnostic learner for Robust (\u03b7, \u03b2)-regression (see Theorem 5.2): Agnostic-\u03b7-Regressor.\n1. Define a grid \u0398 = {\n1 m , 2 m , 4 m , 8 m , . . . , 1\n} .\n2. Define H\u0398 = {h\u03b8 = Agnostic-\u03b8-Regressor(S) : \u03b8 \u2208 \u0398}.\n3. Find an optimal function on the holdout set\nh\u0302\u03b8 = argmin h\u03b8\u2208H\u0398 1\u2223\u2223S\u0303 \u2223\u2223 \u2211\n(x,y)\u2208S\u0303\nI [ sup\nz\u2208U(x)\n|h\u03b8(z)\u2212 y| \u2265 \u03b8 ]\nOutput: h\u0302\u03b8.\nProof Proof of Theorem D.1 Let\nOPTH = inf h\u2208H E(x,y)\u223cD\n[ sup\nz\u2208U(x)\n|h(z)\u2212 y| ] ,\nwhich is obtained by h\u22c6 \u2208 H. By Markov Inequality we have\nP(x,y)\u223cD\n( sup\nz\u2208U(x)\n\u2223\u2223h\u22c6(z)\u2212 y \u2223\u2223 > \u03b7 ) \u2264 E(x,y)\u223cD\n[ supz\u2208U(x) \u2223\u2223h\u22c6(z)\u2212 y \u2223\u2223 ]\n\u03b7 .\nTaking \u03b7 = \u221a OPTH,\nP(x,y)\u223cD\n( sup\nz\u2208U(x)\n\u2223\u2223h\u22c6(z)\u2212 y \u2223\u2223 > \u221a OPTH ) \u2264 OPTH\u221a\nOPTH\n= \u221a OPTH.\nThis means that we can apply the algorithm for agnostic robust uniform \u03b7 regression with \u03b7 = \u221a OPTH,\nand obtain an error of \u221a OPTH + \u01eb. The problem is that OPTH is not known in advance. To overcome this issue, we can have a grid search on the scale of \u03b7, and then verify our choice using a holdout training set.\nWe define a grid,\u0398 = {\n1 m , 2 m , 4 m , 8 m , . . . , 1\n} , such that one of its elements satisfies \u221a OPTH < \u03b8\u0302 <\n2 \u221a OPTH. For each element in the grid, we compute the agnostic regressor for the \u03b7-robust loss. That is, we define H\u0398 = {h\u03b8 = Agnostic-\u03b8-Regressor(S) : \u03b8 \u2208 \u0398}. We choose the optimal function on a holdout labeled set S\u0303 of size \u2248 1\u01eb2 log 1\u03b4 ,\nh\u0302\u03b8 = argmin h\u03b8\u2208H\u0398 1\u2223\u2223S\u0303 \u2223\u2223 \u2211\n(x,y)\u2208S\u0303\nI [ sup\nz\u2208U(x)\n|h\u03b8(z)\u2212 y| \u2265 \u03b8 ] .\nWith high probability, the algorithm outputs a function with error at most \u221a OPTH + \u01eb for the \u21131\nrobust loss, using a sample of size\nO\u0303 ( fat(H, c\u01eb) fat\u2217(H, c\u01eb)\n\u01eb2\n) ."
        }
    ],
    "title": "Adversarially Robust PAC Learnability of Real-Valued Functions",
    "year": 2023
}