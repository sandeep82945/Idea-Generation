{
    "abstractText": "Ballistic missile defense systems require accurate target recognition technology. Effective feature extraction is crucial for this purpose. The deep convolutional neural network (CNN) has proven to be an effective method for recognizing high-resolution range profiles (HRRPs) of ballistic targets. It excels in perceiving local features and extracting robust features. However, the standard CNN's fully connected manner results in high computational complexity, which is unsuitable for deployment in real-time missile defense systems with stringent performance requirements. To address the issue of computational complexity in HRRP recognition based on the standard one-dimensional CNN (1DCNN), we propose a lightweight network called group-fusion 1DCNN with layer-wise auxiliary classifiers (GFAC-1DCNN). GFAC-1DCNN employs group convolution (G-Conv) instead of standard convolution to effectively reduce model complexity. Simply using G-Conv, however, may decrease model recognition accuracy due to the lack of information flow between feature maps generated by each G-Conv. To overcome this limitation, we introduce a linear fusion layer to combine the output features of G-Convs, thereby improving recognition accuracy. Additionally, besides the main classifier at the deepest layer, we construct layer-wise auxiliary classifiers for different hierarchical features. The results from all classifiers are then fused for comprehensive target recognition. Extensive experiments demonstrate that GFAC-1DCNN with such simple and effective techniques achieves higher overall testing accuracy than state-of-the-art ballistic target HRRP recognition models, while significantly reducing model complexity. It also exhibits a higher recall rate for warhead recognition compared to other methods. Based on these compelling results, we believe this work is valuable in reducing workload and enhancing missile interception rates in missile defense systems.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiaodan Wang"
        },
        {
            "affiliations": [],
            "name": "Rui Li"
        },
        {
            "affiliations": [],
            "name": "Lei Lei"
        }
    ],
    "id": "SP:181e721544d85b935de86f07de8b01130cbb5f50",
    "references": [
        {
            "authors": [
                "F. Sun",
                "H. Wang"
            ],
            "title": "Research on detection mission scheduling strategy for the LEO constellation to multiple targets",
            "venue": "J. Defense Model. Simul. Appl. Methodol. Technol. JDMS 18, 87\u2013104 (2021) International Journal of Computational Intelligence Systems",
            "year": 2023
        },
        {
            "authors": [
                "M. Xu",
                "X. Bu",
                "X. Chen",
                "Y. Song"
            ],
            "title": "Rotating missile self-infrared radiation interference compensation for dual-band infrared attitude measurement",
            "venue": "J. Appl. Phys. 128, 1\u20137",
            "year": 2020
        },
        {
            "authors": [
                "L. Lu",
                "S. Yan",
                "J. Zou",
                "W. Sheng"
            ],
            "title": "Performance analysis of infrared detection system based on near space platform",
            "venue": "Conference on Infrared, Millimeter-Wave, and Terahertz Technologies VI. Hangzhou, People\u2019s Republic of China",
            "year": 2019
        },
        {
            "authors": [
                "L. Lu",
                "W. Sheng",
                "W. Jiang",
                "F. Jiang"
            ],
            "title": "Estimating detection range of ballistic missile in infrared system based on near space platform",
            "venue": "Conference on Infrared, Millimeter-Wave, and Terahertz Technologies V. Beijing, People\u2019s Republic of China",
            "year": 2018
        },
        {
            "authors": [
                "J. Liu",
                "S. Huang",
                "W. Zhao",
                "C. Pang",
                "D. Huang"
            ],
            "title": "Analysis on demand of anti-missile operational ability of space-based infrared low-orbit satellites",
            "venue": "Syst. Eng. Electron. 40, 1777\u20131785",
            "year": 2018
        },
        {
            "authors": [
                "N. Zhu",
                "S. Xu",
                "C. Li",
                "J. Hu",
                "X. Fan",
                "W. Wu",
                "Z. Chen"
            ],
            "title": "An improved phase-derived range method based on high-order multiframe track-before-detect for warhead detection",
            "venue": "Remote Sens. 14, 1\u201329",
            "year": 2022
        },
        {
            "authors": [
                "Choi",
                "I.-O.H.",
                "Park",
                "S.-H.",
                "M. Kim",
                "Kang",
                "K.-B.",
                "Kim",
                "K.-T."
            ],
            "title": "Efficient discrimination of ballistic targets with micromotions",
            "venue": "IEEE Trans. Aerosp. Electron. Syst. 56, 1243\u20131261",
            "year": 2020
        },
        {
            "authors": [
                "I. Choi",
                "J. Jung",
                "K. Kim",
                "S. Park"
            ],
            "title": "Novel parameter estimation method for a ballistic warhead with micromotion",
            "venue": "J. Electromagn. Eng. Sci. 20, 262\u2013269",
            "year": 2020
        },
        {
            "authors": [
                "L. Niu",
                "Y. Xie",
                "C. Zhang",
                "D. Wu"
            ],
            "title": "Detection simulation of AEGIS combat system for ballistic missile in electronic warfare environment",
            "venue": "Syst. Eng. Electron. 41, 1195\u20131201",
            "year": 2019
        },
        {
            "authors": [
                "Y. Kim",
                "Choi",
                "Y.-J.",
                "Choi",
                "I.-S."
            ],
            "title": "Separation of dynamic RCS using Hough transform in multi-target environment",
            "venue": "J. Korean Inst. Inf. Technol. 17, 91\u201397",
            "year": 2019
        },
        {
            "authors": [
                "J. Chen",
                "S. Xu",
                "Z. Chen"
            ],
            "title": "Convolutional neural network for classifying space target of the same shape by using RCS time series",
            "venue": "IET Radar Sonar Navig. 12, 1268\u20131275",
            "year": 2018
        },
        {
            "authors": [
                "Hua",
                "H.-Q.",
                "Jiang",
                "Y.-S.",
                "He",
                "Y.-T."
            ],
            "title": "High-frequency method for scattering from coated targets with extremely electrically large size in terahertz band",
            "venue": "Electromagnetics 35, 321\u2013339",
            "year": 2015
        },
        {
            "authors": [
                "Q. Xiang",
                "X. Wang",
                "Y. Song",
                "R. Li",
                "J. Lai",
                "G. Zhang"
            ],
            "title": "Ballistic target recognition based on cost-sensitively pruned convolutional neural network",
            "venue": "J. Beijing Univ. Aeronaut. Astronaut. 47, 2387\u2013 2398",
            "year": 2021
        },
        {
            "authors": [
                "Q. Xiang",
                "X. Wang",
                "R. Li",
                "J. Lai",
                "G. Zhang"
            ],
            "title": "HRRP image recognition of midcourse ballistic targets based on DCNN",
            "venue": "Syst. Eng. Electron. 42, 2426\u20132433",
            "year": 2020
        },
        {
            "authors": [
                "Choi",
                "I.-O.",
                "J. Jung",
                "K. Kim",
                "Park",
                "S.-H."
            ],
            "title": "Effective discrimination between warhead and decoy in mid-course phase of ballistic missile",
            "venue": "J. Korean Inst. Electromagn. Eng. Sci. 31, 468\u2013477",
            "year": 2020
        },
        {
            "authors": [
                "A.R. Persico",
                "C.V. Ilioudis",
                "C. Clemente",
                "J.J. Soraghan"
            ],
            "title": "Novel classification algorithm for ballistic target based on HRRP frame",
            "venue": "IEEE Trans. Aerosp. Electron. Syst. 55, 3168\u20133189",
            "year": 2019
        },
        {
            "authors": [
                "Q. Xiang",
                "X. Wang",
                "Y. Song",
                "L. Lei",
                "R. Li",
                "J. Lai"
            ],
            "title": "One-dimensional convolutional neural networks for high-resolution range profile recognition via adaptively feature recalibrating and automatically channel pruning",
            "venue": "Int. J. Intell. Syst. 36, 332\u2013361",
            "year": 2021
        },
        {
            "authors": [
                "W. Liu",
                "J. Yuan",
                "G. Zhang",
                "Q. Shen"
            ],
            "title": "HRRP target recognition based on kernel joint discriminant analysis",
            "venue": "J. Syst. Eng. Electron. 30, 703\u2013708",
            "year": 2019
        },
        {
            "authors": [
                "J. Zhang",
                "J. Li",
                "Y. Li"
            ],
            "title": "TOPSIS missile target selection method supported by the posterior probability of target recognition",
            "venue": "Appl. Math. Nonlinear Sci. 7, 713\u2013720",
            "year": 2022
        },
        {
            "authors": [
                "T. Jasinski",
                "G. Brooker",
                "I. Antipov"
            ],
            "title": "W-band multi-aspect high resolution range profile radar target classification using support vector machines",
            "venue": "Sensors-Basel 21, 2385",
            "year": 2021
        },
        {
            "authors": [
                "S. Wang",
                "J. Li",
                "Y. Wang",
                "Y. Li"
            ],
            "title": "Radar HRRP target recognition based on gradient boosting decision tree",
            "venue": "9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI), pp. 1013\u20131017. Datong, People\u2019s Republic of China",
            "year": 2016
        },
        {
            "authors": [
                "J. Chen",
                "L. Liao",
                "W. Zhang",
                "L. Du"
            ],
            "title": "Mixture factor analysis with distance metric constraint for dimensionality reduction",
            "venue": "Pattern Recognit. 121, 108156",
            "year": 2022
        },
        {
            "authors": [
                "J. Liu",
                "M. Su",
                "Q. Xu",
                "N. Fang",
                "B.F. Wang"
            ],
            "title": "Multi-scale feature vector reconstruction for aircraft classification using high range resolution radar signatures",
            "venue": "J. Electromagn. Wave 35, 1843\u20131862",
            "year": 2021
        },
        {
            "authors": [
                "J.W. Wan",
                "B. Chen",
                "B. Xu",
                "H.W. Liu",
                "L. Jin"
            ],
            "title": "Convolutional neural networks for radar HRRP target recognition and rejection",
            "venue": "EURASIP J. Adv. Signal Process.",
            "year": 2019
        },
        {
            "authors": [
                "C. Du",
                "B. Chen",
                "B. Xu",
                "D. Guo",
                "H. Liu"
            ],
            "title": "Factorized discriminative conditional variational auto-encoder for radar HRRP target recognition",
            "venue": "Signal Process. 158, 176\u2013189",
            "year": 2019
        },
        {
            "authors": [
                "C. Guo",
                "H. Wang",
                "T. Jian",
                "C. Xu",
                "S. Sun"
            ],
            "title": "Method for denoising and reconstructing radar HRRP using modified sparse autoencoder",
            "venue": "Chin. J. Aeronaut. 33, 1026\u20131036",
            "year": 2020
        },
        {
            "authors": [
                "C. Li",
                "L. Du",
                "S. Deng",
                "Y. Sun",
                "H. Liu"
            ],
            "title": "Point-wise discriminative auto-encoder with application on robust radar automatic target recognition",
            "venue": "Signal Process. 169, 107385",
            "year": 2020
        },
        {
            "authors": [
                "L. Liao",
                "L. Du",
                "J. Chen"
            ],
            "title": "Class factorized complex variational auto-encoder for HRR radar target recognition",
            "venue": "Signal Process. 182, 107932",
            "year": 2021
        },
        {
            "authors": [
                "F.X. Zhao",
                "Y.X. Liu",
                "K. Huo"
            ],
            "title": "Radar target recognition based on stacked denoising sparse autoencoder",
            "venue": "J. Radars 6, 149\u2013156",
            "year": 2017
        },
        {
            "authors": [
                "X. Wang",
                "R. Li",
                "J. Wang",
                "L. Lei",
                "Y. Song"
            ],
            "title": "One-dimension hierarchical local receptive fields based extreme learning machine for radar target HRRP recognition",
            "venue": "Neurocomputing 418, 314\u2013325",
            "year": 2020
        },
        {
            "authors": [
                "B. Xu",
                "B. Chen",
                "J. Liu",
                "Q.",
                "P.H. Wang",
                "H.W. Liu"
            ],
            "title": "Radar HRRP target recognition by the bidirectional LSTM model",
            "venue": "J. Xidian Univ. 46, 29\u201334",
            "year": 2019
        },
        {
            "authors": [
                "J. Chen",
                "L. Du",
                "G. Guo",
                "L. Yin",
                "D. Wei"
            ],
            "title": "Target-attentional CNN for radar automatic target recognition with HRRP",
            "venue": "Signal Process. 196, 108497",
            "year": 2022
        },
        {
            "authors": [
                "L. Du",
                "L. Li",
                "Y. Guo",
                "Y. Wang",
                "K. Ren",
                "J. Chen"
            ],
            "title": "Two-stream deep fusion network based on VAE and CNN for synthetic aperture radar target recognition",
            "venue": "Remote Sens. 13, 4021",
            "year": 2021
        },
        {
            "authors": [
                "M. Pan",
                "A. Liu",
                "Y. Yu",
                "P. Wang",
                "J. Li",
                "Y. Liu",
                "S. Lv",
                "H. Zhu"
            ],
            "title": "Radar HRRP target recognition model based on a stacked CNNBi-RNN with attention mechanism",
            "venue": "IEEE Trans. Geosci. Remote Sens. 60, 1\u201314",
            "year": 2022
        },
        {
            "authors": [
                "Q. Xiang",
                "X.D. Wang",
                "J. Lai",
                "Y.F. Song",
                "R. Li",
                "L. Lei"
            ],
            "title": "Multiscale group-fusion convolutional neural network for high-resolution range profile target recognition",
            "venue": "IET Radar Sonar Navig. 16, 1997\u20132016",
            "year": 2022
        },
        {
            "authors": [
                "M.A. Budin"
            ],
            "title": "Stochastic approximation method",
            "venue": "IEEE Trans. Syst. Man Cybern. 3, 396\u2013402",
            "year": 1972
        },
        {
            "authors": [
                "S.R. Dubey",
                "S. Chakraborty",
                "S.K. Roy",
                "S. Mukherjee",
                "S.K. Singh",
                "B.B. Chaudhuri"
            ],
            "title": "diffGrad: an optimization method for convolutional neural networks",
            "venue": "IEEE Trans. Neural Netw. Learn. Syst. 31, 4500\u20134511",
            "year": 2020
        },
        {
            "authors": [
                "Q. Xiang",
                "X. Wang",
                "J. Lai",
                "L. Lei",
                "Y. Song",
                "J. He",
                "R. Li"
            ],
            "title": "Quadruplet depth-wise separable fusion convolution neural network for ballistic target recognition with limited samples",
            "venue": "Expert Syst. Appl. 235, 121182 (2024) International Journal of Computational Intelligence Systems",
            "year": 2023
        },
        {
            "authors": [
                "Q. Xiang",
                "X. Wang",
                "J. Lai",
                "Y. Song",
                "J. He",
                "L. Lei"
            ],
            "title": "Snapshot ensemble one-dimensional convolutional neural networks for ballistic target recognition",
            "venue": "2023 6th World Conference on Computing and Communication Technologies (WCCCT), pp. 187\u2013193. IEEE, Chengdu, China",
            "year": 2023
        },
        {
            "authors": [
                "B. Cortinas-Lorenzo",
                "F. Perez-Gonzalez"
            ],
            "title": "Adam and the ants: on the influence of the optimization algorithm on the detectability of DNN watermarks",
            "venue": "Entropy 22, 1379",
            "year": 2020
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Delving deep into rectifiers: surpassing human-level performance on imagenet classification",
            "venue": "2015 IEEE international conference on computer vision (ICCV), pp. 1026\u20131034. IEEE, Santiago, Chile",
            "year": 2015
        },
        {
            "authors": [
                "A. Paszke",
                "S. Gross",
                "F. Massa",
                "A. Lerer",
                "J. Bradbury",
                "G. Chanan",
                "T. Killeen",
                "Z. Lin",
                "N. Gimelshein",
                "L. Antiga",
                "A. Desmaison",
                "A. Kopf",
                "E. Yang",
                "Z. DeVito",
                "M. Raison",
                "A. Tejani",
                "S. Chilamkurthy",
                "B. Steiner",
                "L. Fang",
                "J. Bai",
                "S. Chintala"
            ],
            "title": "PyTorch: an imperative style, high-performance deep learning library",
            "venue": "Wallach, H., Larochelle, H., Beygelzimer, A., Buc, F.D.T.A., Fox, E., Garnett, R. (eds.) Advances in Neural Information Processing Systems 32, pp. 8026\u20138037. Curran Associates, Inc., Red Hook",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Vol.:(0123456789)\nKeywords Ballistic missile\u00a0\u00b7 High-resolution range profile\u00a0\u00b7 Convolutional neural network\u00a0\u00b7 Target recognition\u00a0\u00b7 Defense systems\u00a0\u00b7 Feature fusion\u00a0\u00b7 Auxiliary classifier"
        },
        {
            "heading": "1 Introduction",
            "text": "Efficient ballistic target recognition is a key issue for ballistic missile defense systems, especially the fast and effective recognition of missile warheads from numerous decoy targets is\na very challenging task. For ballistic target recognition, the infrared radiation [1\u20135], micro-Doppler [6\u20138] radar crosssection [9\u201312], and high-resolution range profile (HRRP) [13\u201316] of ballistic targets are all considered. Among them, HRRP represents the radial distribution of target scattering\n* Xiaodan Wang shiningvv@yeah.net\nQian Xiang qianxljp@126.com\nJie Lai 531418867@qq.com\nYafei Song yafei_song@163.com\nRui Li lazy136200@163.com\nLei Lei wendyandpaopao@163.com\n1 College of\u00a0Air and\u00a0Missile Defense, Air Force Engineering University, Xi\u2019an\u00a0710051, China\n2 Chinese People\u2019s Liberation Army 61932, Beijing\u00a0100089, China\n3 College of\u00a0Information and\u00a0Navigation, Air Force Engineering University, Xi\u2019an\u00a0710077, China\nInternational Journal of Computational Intelligence Systems (2023) 16:190\n1 3\n190 Page 2 of 19\ncenters on radar line of sight and benefits from its rich structural information of the target and the easiness of obtaining, which has become an effective feature for radar automatic target recognition (RATR) [13, 17].\nEfficient feature extraction is a critical step in improving ballistic target recognition. In recent years, machine learning techniques have been widely applied to HRRP-based radar automatic target recognition problems because they can perform efficient automatic feature extraction in a datadriven manner, among which representative methods include k-nearest neighbor [18], support vector machine [19, 20], decision tree [21], linear discriminant analysis [22, 23], etc. However, since these algorithms extract shallow features, they are showing deficiencies in robustness and generalization performance [24]. Compared with shallow machine learning techniques, deep learning (DL) can extract deep features from datasets, and the representative models, such as auto-encoders (AE) [25\u201329], extreme learning machines [30], long short-term memory (LSTM) [31], gate recurrent unit (GRU) [32], convolutional neural network (CNN) [33, 34], and their variants, have been widely applied in HRRP recognition research.\nAmong the aforementioned DL-based methods, onedimensional CNN (1DCNN) is a kind of HRRP target recognition method that can automatically extract deep features and has better local feature perception capability and robustness than other deep neural networks [13, 17], it has been widely applied to HRRP target recognition field. For example, Pan et\u00a0al. [34] take advantage of CNN and attention mechanisms to effectively extract envelope features and physical structural features from HRRP, achieving high robustness to the small translation of test samples and noises. Chen et\u00a0al. [32] take 1DCNN as the feature extractor and can excavate abundant local structural features of data and combine a bidirectional GRU (Bi-GRU) network to consider sequential relationships among different regional features in HRRP. Xiang et\u00a0al. [17] incorporate the global best leading artificial bee colony algorithm to automatically and heuristically search for the optimal channel number of the 1DCNN, avoiding the rule-of-thumb design of 1DCNN for HRRP recognition.\nHowever, the standard 1DCNN is a fully connected neural network in nature, i.e., the output feature maps of the previous layer are all performing convolution operations with each convolution kernel in the current convolution layer, which has the problem of high computational complexity [35] and is not conducive to the aim of the rapid recognition of ballistic targets. In this paper, we carried out a research on ballistic target recognition method based on lightweight 1DCNN, and a ballistic target recognition method based on group-fusion 1DCNN with layer-wise auxiliary classifiers (GFAC-1DCNN) is proposed. To address the problem of the high computational complexity of standard 1DCNN, as\nshown in Fig.\u00a02, this paper proposes a novel lightweight convolution, namely group-fusion convolution (GF-Conv), which first splits the standard 1D convolution filters as well as input feature maps into the largest number of groups equally, and each convolution filter only performs convolution operation with the input feature maps in the same group, avoiding the high computational complexity of using the fully connected structure. Simply replacing the standard onedimensional convolution with group convolution (G-Conv) will reduce the recognition effect accordingly, which is mainly caused by the lack of information flow between groups. Therefore, a linear fusion layer is constructed after G-Conv to realize the effective information between groups. Different convolution layers extract features at varying levels of abstraction, representing different levels of the target features. Thus we suggest using features at different levels for comprehensive target recognition. Besides the main classifier in the deepest layer, layer-wise auxiliary classifiers are built for different hierarchical features, and their results are finally fused for target recognition. In addition, to optimize the training of GFAC-1DCNN, the traditional mini-batch stochastic gradient descent (SGD) algorithm [36] is changed by equal-sized steps for all parameters, resulting in poor convergence. In this paper, we applied the diffGrad algorithm [37] for optimizing the training of GFAC-1DCNN leveraging the short-term gradient change to adjust the step size of each parameter in such a way that it should have a larger step size for faster gradient changing parameters and vice versa.\nCompared with other HRRP recognition methods based on deep learning, the proposed GFAC-1DCNN in this paper has the following advantages.\n1. Compared with the standard 1DCNN, the GFAC1DCNN not only greatly reduces the total number of model parameters, but also ensures that the model has a high recognition accuracy. It has a certain reference significance for reducing the workload of the missile defense system as well as improving the recognition speed. 2. The recognition performance of 1DCNN for ballistic targets is effectively improved by using layer-wise auxiliary classifiers to fuse the outputs of different convolution modules. This indicates that integrating intermediate layer features from different levels and output features can effectively enhance target recognition performance. 3. Compared with other deep neural networks, GFAC1DCNN has higher overall recognition accuracy, especially the highest recall rate of warhead recognition. The higher recall rate of warhead targets can reduce the missed detection rate of ballistic targets more, which has a certain reference significance for the missile defense system to improve the missile interception rate.\nInternational Journal of Computational Intelligence Systems (2023) 16:190\n1 3\nPage 3 of 19 190\nThe remainder of this paper is organized as follows. In Sect.\u00a02, we give a brief introduction to ballistic target HRRP recognition, and state-of-the-art (SOTA) models of ballistic target HRRP recognition. The proposed model, a.k.a, GFAC1DCNN for radar HRRP target recognition and the training optimizer based on diffGrad are introduced in Sect.\u00a03. The measured data of five ballistic targets, experimental settings, and detailed experimental results are elaborated in Sect.\u00a04, and we conclude Sect.\u00a05."
        },
        {
            "heading": "2 Related Works",
            "text": ""
        },
        {
            "heading": "2.1 Introduction to\u00a0Ballistic Target HRRP Recognition",
            "text": "HRRP is an effective 1D radar signature used for target recognition. HRRP captures the projection of target scattering centers onto the radar line of sight (RLOS), using wideband waveforms. It represents the range distribution of the complex returned signals from target scatterers. With wide bandwidths, modern radars can achieve high down-range resolution, resulting in fine-grained HRRP signatures that reveal micro-Doppler and structural characteristics of targets. The amplitude component of HRRP is most useful, as the phase is sensitive to aspect angles. HRRP combines high information content and noise robustness, enabling the classification and discrimination of targets based on unique range profile features extracted from their high-fidelity scattering responses. Therefore, HRRP has emerged as an efficient 1D basis for ballistic target HRRP recognition [38].\nHRRP signals are obtained by projecting the echo sum of target scattering points onto the RLOS using wideband radar signals. Its resolution is inversely proportional to the radar bandwidth \u0394r . Therefore, wideband radars can acquire HRRP signals with higher resolution [35]. In the RLOS direction, the target is equivalent to several range cells with width \u0394r . The echo of each range cell is the sum of echoes from all scatterers within that cell. The echo of the i th range cell is defined as\nwhere Ni is the number of target scatterers in the i th range cell, ai,k denotes the scattering intensity of the k th scatterer in the i th range cell, and i,k denotes the time for the radar wave to reach the k th scatterer in the ith range cell. f is the frequency of the incident wave, I(i) and Q(i) are the real and imaginary parts of the signal. j is the imaginary unit, the square root of \u2212 1.\n(1)x(i) = i(f ) = Ni\u2211 i=1 ai,ke j2 f i,k = I(i) + jQ(i),\nSince the phase information in HRRP is sensitive to target azimuth and elevation, while the amplitude information is relatively stable. The amplitude of HRRP is used as the basis for target recognition, which is defined as\nwhere vector X denotes the HRRP signal, and E denotes the dimension of the HRRP."
        },
        {
            "heading": "2.2 SOTA Models of\u00a0Ballistic Target HRRP Recognition",
            "text": "In recent years, CNN has become a predominant technique for ballistic target HRRP recognition. Several CNN-related innovations in elaborated architecture design, robust HRRP representation learning, and advanced training methodology have pushed the state-of-the-art in this field.\nElaborated Architecture Design Many sophisticated CNN architectures have been proposed for efficient and accurate ballistic target HRRP recognition. The stacked CNN-BiRNN [34] architecture with attention mechanism enables the model to exploit the envelope, local structure, and translation invariance characteristics of HRRP for robust recognition. By replacing regular convolutions with multiscale G-Convs and pointwise convolutions, MSGF-1DCNN [35] can reduce parameters while capturing target details at multiple scales. This improves feature learning and reduces complexity simultaneously. DSFCNN [38] utilizes depthwise and pointwise convolutions to reduce computational complexity compared to standard CNNs while improving recognition accuracy. TACNN [32] uses Bi-GRU to generate attention weights so that it can focus on HRRP target regions and filter out backgrounds to improve recognition performance. Introducing channel attention modules enables 1DCNN to recalibrate channel features for improved\n(2)\nX = [\ufffdx(1)\ufffd, \ufffdx(2)\ufffd, \ufffdx(3)\ufffd, ..., \ufffdx(E)\ufffd]T = [ \u221a I(1)2 + Q(1)2, \u221a I(2)2 + Q(2)2, ...., \u221a I(E)2 + Q(E)2]T ,\n1 3\nHRRP feature extraction [17]. Similar to MSGF-1DCNN, our proposed GFAC-1DCNN also employs G-Conv. However, the two approaches apply G-Conv in different ways to achieve complementary goals of efficiency and representation learning. GFAC-1DCNN focuses on optimizing efficiency by maximizing the number of groups, whereas MSGF-1DCNN focuses on enhancing feature extraction using multiple G-Conv scales.\nRobust HRRP Representation Learning Robust feature extraction from HRRP is another important area of focus. The two-stream fusion network architecture with separate VAE and CNN branches [33] extracts and fuses probabilistic and visual features from HRRP and SAR data, respectively, for enhanced SAR target recognition. The AE-based network architecture enables 1D ELM-LRF-AE [30] to learn hierarchical local receptive field representations of HRRP in an unsupervised manner, allowing more efficient and robust feature extraction. Converting HRRP to binary images enables DCNNs to extract visual patterns and spatial features from the images, enhancing local feature learning compared to using 1D HRRP directly [14]. In contrast to the above methods which aim to enhance feature extraction from HRRP, our proposed GFAC-1DCNN focuses on fusing features from different layers. Specifically, GFAC-1DCNN introduces a linear fusion layer to combine output features from group convolutions, enabling information aggregation across groups. Furthermore, layer-wise auxiliary classifiers are\nconstructed to perform decision-level fusion using predictions from multiple network layers. This hierarchical feature and decision fusion provides a more comprehensive target representation compared to solely improving feature learning at individual layers. In summary, while other models concentrate on boosting feature extraction, GFAC-1DCNN uniquely focuses on cross-layer feature aggregation and multi-level decision fusion, complementing existing HRRP recognition approaches. The hierarchical fusion design is the key differentiator of GFAC1DCNN compared to prior arts that augment single-layer feature learning.\nAdvanced Training Methodology Advanced CNN training methodologies have also emerged. The snapshot ensemble technique [39] allows 1DCNN to improve HRRP recognition performance without additional training costs. AdamW optimization enables faster convergence for the snapshot ensemble training process and taking models at different minima promotes diversity. The cost-sensitive pruning technique [13] based on the artificial bee colony algorithm can automatically find a compact 1DCNN architecture optimized for lower misclassification cost and complexity. The automated lottery ticket hypothesis-based neural architecture search technique can significantly reduce 1DCNN's complexity and promote overall recognition accuracy [17]. SFCD loss [38] optimizes sample fitting and between-class separability, enhancing 1DCNN recognition performance with\nInternational Journal of Computational Intelligence Systems (2023) 16:190\n1 3\nPage 5 of 19 190\nlimited training data. The cost-sensitive cross-entropy loss [13] focuses 1DCNN's training on minimizing the cost of misclassifying expensive categories. Unlike the advanced training methodologies mentioned previously, this paper applies the diffGrad algorithm [37] to optimize GFAC-1DCNN training.\nWhile the above-mentioned methods have advanced the SOTA in specific tasks, most still employ fully connected convolutional architectures, which lead to high complexity unsuitable for applications. Our proposed GFAC-1DCNN method introduces GF-Conv to reduce complexity, while improving accuracy compared to previous HRRP recognition models. The layer-wise auxiliary classifier design for hierarchical feature fusion further enhances recognition performance.\n3 Ballistic Target HRRP Recognition Based on\u00a0GFAC\u20111DCNN\nThe ballistic target recognition method based on GFAC1DCNN is shown in Fig.\u00a01, which includes the training phase and the testing phase. In the training phase, the onedimensional HRRP samples of ballistic targets are fed into the GFAC-1DCNN network, the output of the Softmax classifier with the corresponding labels is fed into the cross-entropy loss function, and the model parameters of the GFAC-1DCNN are updated using the mini-batch SGD algorithm [36]. In the testing phase, the HRRP samples are fed into the trained GFAC-1DCNN, and the prediction results of the samples are obtained by the Softmax classifier by maximizing the posterior probability."
        },
        {
            "heading": "3.1 Overall Structure of\u00a0GFAC\u20111DCNN",
            "text": "The overall structure of GFAC-1DCNN is shown in Fig.\u00a02. As can be seen from Fig.\u00a02, GFAC-1DCNN consists of the input layer, one-dimensional convolution (1D-Conv) layer, batch normalization (BN) layer, non-linear activation layer, global max-pooling layer, flatten layer, and fully connected layer. The 1D-Conv layer, BN layer, and\nnon-linear activation layer form a convolution module, while the alternately connected multiple convolution modules are mainly used for feature extraction. The global max-pooling layer, flatten layer, and fully connected layer form a classifier, and the classifier is connected after each convolution layer. As shown in Fig.\u00a02, the last classifier is the main classifier while other classifiers among the alternately connected convolution modules serve as auxiliary classifiers."
        },
        {
            "heading": "3.2 Convolution Module",
            "text": "In this paper, the specific settings and roles of the convolution module are as follows.\n1D-Conv Layer The 1D-Conv layer plays a pivotal role in the 1DCNN architecture as it utilizes fixed-size windows and 1D-Conv filters to extract features from input data. This layer incorporates a parameter-sharing mechanism, where each channel's features undergo convolution operations with the same 1D-Conv filters in a sliding manner. Such an approach effectively mitigates the significant increase in the number of parameters as the input data dimension expands. By stacking multiple 1D-Conv layers, local features are extracted progressively, allowing earlier layers to capture more finely grained details while later layers extract more global features. In our proposed method (as shown in Fig.\u00a0 2), we replace the standard 1D-Conv with a lightweight group-fusion 1D-Conv (GFConv) within the 1D-Conv layer. This modification aims to concurrently reduce model complexity and enhance feature extraction.\nBN Layer The BN layer is utilized after the 1D-Conv layer to normalize the features of each mini-batch sample. This normalization process contributes to faster convergence and improved training stability of the DCNN. The BN layer ensures that the output of the 1D-Conv layer is normalized to a distribution with a mean of 0 and a standard deviation of 1. Specifically, let the output of the 1D-Conv layer be denoted as x = [x(1), x(2), ..., x(d)] , where d represents the dimension of x . Each dimension of x is then normalized to\nInternational Journal of Computational Intelligence Systems (2023) 16:190\n1 3\n190 Page 6 of 19\nwhere (\u22c5) and Var(\u22c5) denote the expectation and standard deviation of each mini-batch sample respectively, is a very small number close to 0 to prevent the denominator from being 0. The normalized values are then scaled and shifted by introducing the hyper-parameters (k) and (k).\nNon-linear Activation Layer To enhance the model's non-linear fitting capability to features, neural networks commonly incorporate non-linear activation functions. In this study, we utilize the Mish function [17] as the chosen non-linear activation function. The formula for Mish is represented by Eq.\u00a0(5)."
        },
        {
            "heading": "3.3 Group\u2011Fusion 1D\u2011Conv Layer",
            "text": "Let the total number of convolution modules be L , the multichannel input of the l th ( l \u2208 {0, 2,\u2026 ,L \u2212 1} ) standard 1D-Conv layer is represented by the tensor X(l)\u2208\u211dD(l)\u00d7C(l) , each channel corresponds to a 1D feature map, D(l) and C(l) denote the length of a single 1D feature map and the channel number of the 1D feature maps, respectively. The output feature map of the 1D-Conv layer is denoted by the tensor X(l+1)\u2208\u211dD(l+1)\u00d7C(l+1) , where D(l+1) and C(l+1) denote the length and the channel number of the feature map after convolution operations as well. Let H(l) denote the kernel size of the l th standard 1D-Conv layer, and the weight tensor W(l) \u2208\u211dH(l)\u00d7C(l)\u00d7C(l+1) represents the convolution filters. The l th standard 1D-Conv convolves the multi-channel input feature maps X(l) with all the 1D-Conv filters W(l) to yield an output feature map X(l+1) with the channel number of C(l+1) . Specifically, each convolution filter is convolved with all the input feature maps, so that the 1D feature map generated by the k th ( k \u2208 {1, 2,\u2026 ,C(l+1)} ) 1D-Conv filter W(l)\nk and all\ninput feature maps X(l) is\nwhere the operator \u2217 denotes the convolution operation, b(l) \u2208\u211dC(l+1) , and b(l) k\ndenotes the bias parameter of the k th convolution filter. Considering that the input feature maps of each layer are padded with zeros, let the number of zeros padded on one side of each input feature map in the l th layer beP(l) , the length of the input 1D feature map becomesD(l) + 2P(l) , thus X(l)\u2208\u211d(D(l)+2P(l))\u00d7C(l) , then the convolution operation is performed with the convolution filter using a stride ofS(l) ,\n(3)x\u0302 (k) = x(k) \u2212 (x(k))\u221a Var(x(k)) + , k = {1, 2,\u2026 , d},\n(4)y(k) = (k)x\u0302(k) + (k).\n(5)Mish(x) = x \u22c5 tanh(ln(1 + ex)).\n(6)X(l+1)k = X (l) \u2217 W (l) k + b (l) k ,\nthe value of the k th output feature map at position i ( i \u2208 {1, 2, ...,D(l+1)} ) is\nwhere i\u2208{1, 2, ...,D(l+1)} , k\u2208{1, 2, ...,C(l+1)} . Therefore, the length of the output feature map is\nwhere \u230a\u22c5\u230b indicates rounding-down operation. As we can see from Eq.\u00a0(7) and Fig.\u00a03a, the standard 1D-Conv layer in which the convolutional filter is computed with all the input feature maps is similar to a fully connected model, so the standard 1D-Conv is very computationally intensive. To reduce the computational complexity of the standard 1D-Conv, we first use group convolution (G-Conv) to take the place of standard 1D-Conv. Both the input feature map X(l) and the weight tensor W(l) are divided into \u03a9(l) groups according to the spatial order, so \u03a9(l) needs to divide both the number of input channels C(l) and the number of output channels C(l+1) simultaneously. Let the input feature map, weight tensor and output feature map of the g th ( g\u2208{1, 2, ...,\u03a9(l)} ) group be X\u0303\n(g,l) , W\u0303 (g,l) and X\u0303 (g,l+1) respectively, then X\u0303 (g,l) \u2208 \u211dD\n(l)\u00d7(C(l)\u2215\u03a9(l)) , W\u0303 (g,l) \u2208 \u211dH (l)\u00d7(C(l)\u2215\u03a9(l))\u00d7(C(l+1)\u2215\u03a9(l)) , X\u0303 (g,l+1)\u2208\u211dD(l+1)\u00d7(C(l+1)\u2215\u03a9(l)) . Keeping the step size and number of padded zeros of each G-Conv the same as the standard 1D-Conv, the output feature map of the G-Conv is\nwhere (k,\u03a9(l)) denotes the remainder of k divided by \u03a9(l) , ceil(\u22c5) denotes rounding-up operation, and X\u0303\n(l+1) denotes all feature maps generated by G-Conv operation, then X\u0303 (l+1) \u2208\u211dD(l+1)\u00d7C(l+1) and D(l+1) can still be calculated via Eq.\u00a0(8). As shown in Fig.\u00a03, in the G-Conv, the convolution operation only occurs in each group, avoiding the fully connected structure, and the standard 1D-Conv is equivalent to the G-Conv when \u03a9 = 1. However, since the convolution operation occurs only within each group, the intragroup feature information exchange is relatively plentiful but the inter-group feature information exchange is quite scarce. Therefore, to promote the information exchange between groups, a simple linear fusion of the G-Conv\n(7)X(l+1)i,k = H(l)\u2211 j=1 C(l)\u2211 c=1 X (l) S(l)\u00d7(i\u22121)+1+j,c \u00d7W (l) j,c,k + b (l) k ,\n(8)D(l+1) = \u230aD (l) + 2P(l) \u2212 H(l)\nS(l) \u230b + 1,\n(9)\nX\u0303 (l+1) i,k = X\u0303 (ceil(k\u2215\u03a9(l)),l+1) i, (k,\u03a9(l))\n= Mish( H(l) \u2211\nj=1\nC(l)\u2215\u03a9(l) \u2211\nc=1 X\u0303 (ceil(k\u2215\u03a9(l)),l) S(l)\u00d7(i\u22121)+1+j,c\n\u00d7 W\u0303 (ceil(k\u2215\u03a9(l)),l) j,c, (k,\u03a9(l)) + b (l) k ),\nInternational Journal of Computational Intelligence Systems (2023) 16:190\n1 3\nPage 7 of 19 190\noutput features is performed by connecting pointwise convolution (PW-Conv) after G-Conv. The G-Conv with feature fusion by PW-Conv is termed group-fusion convolution (GF-Conv) hereinafter.\nUsing the tensor W\ufffd(l)\u2208\u211d1\u00d7C(l+1)\u00d7C(l+1) and b\ufffd(l)\u2208\u211dC(l+1) to denote the weight parameter and bias parameter of the PWConv filters respectively, with the number padded zeros P\ufffd(l) =0 and step size S\ufffd(l)=1, the feature fusion process using PW-Conv is\nwhere X\ufffd(l+1)\u2208\u211dD(l+1)\u00d7C(l+1) is the final output feature map of the GF-Conv layer. X\ufffd(l+1) is consistent with the output dimension of the standard 1D-Conv calculated by Eq.\u00a0(7), so the replacement of the standard 1D-Conv is feasible."
        },
        {
            "heading": "3.4 Complexity Analysis of\u00a0GF\u2011Conv",
            "text": "The number of parameters is an important metric for analyzing the model complexity, the more parameters there are, the more computation is required in both the training and testing phases. To compare the model complexity of the proposed GF-Conv with that of the standard 1D-Conv, we calculate the parameter number in the same layer of their model structure shown in Fig.\u00a02. For the standard CNN, the total parameter number of its l th layer is\nFor GF-1DCNN, the total parameters of the l th GF-Conv layer include two parts, i.e., the G-Conv parameters and the PW-Conv parameters. Thus, the total parameter number of the GF-Conv layer is\n(10)X\ufffd(l+1) i,k = C(l+1)\u2211 c=1 X\u0303 (l+1) i,c \u00d7W\ufffd (l) 1,c,k + b\ufffd (l) k ,\n(11)\u03a6std = H(l) \u00d7 C(l) \u00d7 C(l+1).\nFrom Eqs.\u00a0(11) and (12), we obtain\nwhere the convolution kernel size H(l) is generally taken as a positive odd number and the group number \u03a9(l) is generally taken as a positive integer. When \u03a9 = 1, the parameter number of GF-1DCNN is increased by 1/H(l) than that of standard 1DCNN, and when \u03a9(l) is increased, especially when the group number is larger, i.e., H(l) \u226a \u03a9(l) , then H(l)\u2215\u03a9(l) \u2248 0, the parameter number of standard 1DCNN is about H(l) times of GF-1DCNN. It can be seen that when other conditions remain constant, the larger the group number of GF1DCNN, the smaller the computational complexity of the model. Thus, the group number is maximized for the largest computation reduction."
        },
        {
            "heading": "3.5 Fusion of\u00a0Layer\u2011Wise Auxiliary Classifiers",
            "text": "Figure\u00a02 illustrates that each convolution module captures features at different depths, signifying varying levels of abstraction for the target. Based on this observation, we propose utilizing features from multiple levels for target recognition. In addition to the main classifier positioned at the deepest layer, layer-wise auxiliary classifiers are established to leverage hierarchical features, and their results are combined to achieve comprehensive recognition. The components of each classifier are outlined as follows.\nGlobal Max-Pooling Layer Global pooling can reduce each channel of a 1D feature map with length D(l) into the length of 1, decreasing the parameter demands of successive fully connected layers. There are generally two kinds of pooling operation, i.e., the max-pooling and the averagepooling. Here, we use global max-pooling, which extracts the maximum value of a feature map. From Eq.\u00a0(10), we get the output feature map of the GF-Conv X\ufffd(l+1)\u2208\u211dD(l+1)\u00d7C(l+1) , then we apply BN and non-linear activation to it, i.e.,\nwhere BN(\u22c5) denotes the batch normalization operation. Therefore, the global max-pooling operation can be described as Eq.\u00a0(15).\nwhere X\u2217(l+1)\u2208\u211d1\u00d7C(l+1) denotes the output of the global maxpooling layer.\nFlatten Layer and Softmax Classifier The main purpose of the flatten layer is to restructure the output of the global\n(12)\u03a6GF = H(l) \u00d7 C(l) \u03a9(l) \u00d7 C(l+1) \u03a9(l) \u00d7 \u03a9(l) + 1 \u00d7 C(l) \u00d7 C(l+1).\n(13) \u03a6std\n\u03a6GF =\nH(l)\nH(l)\u2215\u03a9(l) + 1 ,\n(14)X(l+1) = Mish(BN(X\ufffd(l+1))),\n(15)X \u2217(l+1) i,k = max\nj\u2208{1,2,...,D(l)}\n{ X (l+1)\nj,k\n} ,\nInternational Journal of Computational Intelligence Systems (2023) 16:190\n1 3\n190 Page 8 of 19\nmax-pooling layer X\u2217(l+1) into a one-dimensional vector u(l+1)\u2208\u211dC(l+1) , namely\nwhere u(l+1) contains the extracted features in the l th layer that are then passed on to the fully connected layer.\nThe fully connected layer acts as a Softmax classifier, which is essentially a single-layer neural network, whose number of neurons is equal to Q , the total number of target classes to be classified. It utilizes the Softmax function to activate and classify based on the maximization of posterior probabilities.\nLet the parameters of the Softmax classifier after the l th convolution module be \u0302 (l+1) = {W\u0302 (l+1) , b\u0302 (l+1) } , where W\u0302 (l+1) \u2208\u211dC(l+1)\u00d7Q , b\u0302 (l+1)\n\u2208\u211dQ . The output of the l th fully connected layer is\nwhere z(l+1) q\n= u(l+1)T \u00d7 W\u0302 (l+1)\nq + b\u0302\n(l+1) q , q \u2208 {1, 2,\u2026 ,Q}.\nTo fuse multiple classfiers\u2019 results, we first sum all outputs of fully connected layers in various depth, which is\nThen, the output of the q th neuron of the fused Softmax classifiers represents the probability of predicting the sample as the q th class, which is\nThe prediction process of the sample x(k) is to maximize the posterior probability, namely,\nwhere y\u0303 is the predicted label."
        },
        {
            "heading": "3.6 Model Training",
            "text": "The training of the network is the process of updating the network parameters using the training samples, which first requires the construction of a loss function indicating the difference between the output of the neural network and the ground-truth labels. Let train and test denote the training\n(16)u(l+1) = Flatten( \u2217(l+1)),\n(17)z(l+1) = u(l+1)T \u00d7 W\u0302 (l+1) + b\u0302 (l+1) ,\n(18)z\u2217 = L\u2211 i=1 z(i).\n(19)P(y = q) = ez \u2217 q\u2215 Q\u2211 j=1 e z\u2217 j .\n(20)y\u0303 = argmax q\u2208{1,2,\u2026,Q} P(y = q),\ndataset with Ntrain samples and the testing dataset with Ntest samples, and the tuple ( x , y)\u2208 train denotes the input features of a sample in the training dataset and its corresponding ground-truth label, respectively, where y\u2208{1, 2, ...,Q }, and Q denotes the total number of target classes, then the crossentropy loss function over train can be constructed as\nwhere denotes all trainable parameters and P(y = q|x; ) is the q th output of the Softmax classifier, indicating the probability that the prediction of the input feature x is the q th class.\nThe Eq.\u00a0(21) is called the empirical risk loss function, and the training of the neural network is essentially minimizing the empirical risk loss function, i.e.,\nFor a dataset with larger Ntrain , due to the limitation of computer memory and computing power, it is not possible to feed all the samples into the network at the same time to update the parameter, so the mini-batch SGD algorithm [36] is generally used to randomly select B samples at a time to update the network parameters, therefore, the whole training dataset needs to be randomly divided into T = ceil(Ntrain\u2215B) mini-batches, i.e., train = { train\nt }T t=1 . The cross-entropy loss function over the t th ( t \u2208 {1, 2,\u2026 , T} ) mini-batch dataset train\nt is\nFeeding T mini-batches of samples into GF-1DCNN to train with the SGD algorithm successively, for the t th iteration, the parameters are updated as\nwhere is the learning rate and gt is the gradient of the loss function w.r.t. the parameter t , that is\nIn Eq.\u00a0(24), the choice of the learning rate has a great impact on the training convergence. If is chosen too large, it may make the network training oscillate near or even far from the optimal, and if it is chosen too small, the network training speed may be too slow. To address this issue, the Adam\n(21)\nJ( ; train) = \u2212 1\nNtrain\n\u2211 (x,y)\u2208 train Q\u2211 q=1 1{y = q} P(y = q|x; ),\n(22) \u2217 = argmin\nJ ( ; train ) .\n(23)\nJ( ; train t\n) = \u2212 1\nB \u2211 (x,y)\u2208 traint Q\u2211 q=1 1{y = q} P(y = q|x; ).\n(24) t+1 = t \u2212 gt,\n(25)gt = J( t; train t )\nt .\n1 3\nInternational Journal of Computational Intelligence Systems (2023) 16:190\n1 3\n190 Page 10 of 19\noptimizer [40] combines the first-order and second-order moment estimates of the gradient to dynamically adjust the learning rate. For the t th iteration, the first-order moment estimate mt and the second-order moment estimate ut of the gradient gt are calculated as\nwhere 1 , 2\u2208[0,1] are the decay coefficients of mt and ut , respectively.\nThen, the deviations of mt and ut are corrected as\nFinally, the parameter update equation of the Adam optimizer is\nwhere is a small number close to 0 to prevent the denominator from being 0, and \u2299 indicates that the vector is multiplied by the corresponding element.\nThe Adam optimizer has a wide range of applications in the training process of neural networks. Based on the Adam optimizer, the diffGrad optimizer [37] introduces the short-term gradient changing into the parameter update equation, which is based on the idea that for larger gradient changing parameters, a larger learning rate should be used to accelerate the convergence speed; while for smaller gradient changing parameters, a smaller learning rate should be used to prevent training oscillations or even move away from the optimal. In diffGrad, the\n(26)mt = 1mt\u22121 + ( 1 \u2212 1 ) gt,\n(27)ut = 2ut\u22121 + ( 1 \u2212 2 ) g2 t ,\n(28)m\u0302t = mt\n1 \u2212 t 1\n,\n(29)u\u0302t = ut\n1 \u2212 t 2\n.\n(30) t+1 = t \u2212 \ud835\udefc\u221a \ufffdut + \ud835\udf00 \u2299 \ufffdmt,\nshort-term gradient change is characterized with the help of the AbsSig function, which is\nwhere \u0394gt = gt \u2212 gt\u22121. As shown in Fig.\u00a04, the AbsSig function compresses the input values into [0.5, 1.0] and is defined as\nUsing the AbsSig function to dynamically adjust the learning rate according to the short-term gradient changing, Eq.\u00a0(30) is updated as\nIn summary, the GFAC-1DCNN training optimization algorithm based on diffGrad is shown in Algorithm\u00a01, where Acc( T; test) denotes the testing accuracy over test with parameters T after T iterations."
        },
        {
            "heading": "4 Experiments and\u00a0Analysis",
            "text": ""
        },
        {
            "heading": "4.1 Dataset",
            "text": "To validate the proposed HRRP recognition method for ballistic targets based on GFAC-1DCNN, the physical optics method in electromagnetic scattering calculation is used to simulate five types of mid-ballistic targets such as warhead, high-imitated decoys, simple decoys, spherical decoys, and motherships, thus Q = 5, and the physical parameters of the five types of ballistic targets are shown in Fig.\u00a05. The hyperparameters in the simulation process are set as follows: the radar center frequency is 10 GHz, the polarization mode is horizontal, the azimuth angle range is 0\u00b0\u2013180\u00b0, and the simulation accuracy is 0.05\u00b0. For each type of target, 3601 HRRP samples with different azimuth angles were obtained, and the number of range cells for each HRRP sample was 256, thusE = 256 . For each class of targets, 80% of samples are randomly selected as the training dataset train , and the remaining samples are used as the testing dataset test."
        },
        {
            "heading": "4.2 Experimental Setup",
            "text": "The hyper-parameters of GFAC-1DCNN are set as follows: the number of convolution modules L = 4, the number of complementary padded zeros of all convolution kernels P(l) = 0 , the stride of convolution S(l) = 1 , and the size of all\n(31) t = AbsSig ( \u0394gt ) ,\n(32)AbsSig(x) = 1\n1 + e\u2212|x| .\n(33) t+1 = t \u2212 \ud835\udefc t\u221a \ufffdut + \ud835\udf00 \u2299 \ufffdmt.\n1 3\nconvolution kernels except PW-Conv H are chosen in the range { 3, 5, 7, 9 }. Let a 1D array C = [C(1),C(2),\u2026 ,C(L)] denote different channel settings of a 1D-Conv layer. To compare the generality of the proposed method under different channel settings, five channel settings are considered for experiments, i.e., C1 = [8, 16, 32, 64], C2 = [16, 32, 64, 128], C3 = [32, 64, 128, 256], C4 = [64, 128, 256, 512], C5 = [128, 256, 512, 1024], and C6 = [256, 512, 1024, 2048]. According to Eq.\u00a0(13), the group number G-Conv is set to the maximum under the five channel settings to minimize the computation complexity; thus, the group number in each layer under the five channel settings is [1, 8, 16, 32], [1, 16, 32, 64], [1, 32, 64, 128], [1, 64, 128, 256], and [1, 128, 256, 512], respectively. To perform ablation studies on GFAC1DCNN and verify the effectiveness of each component, we refer to GFAC-1DCNN without auxiliary classifiers as GF-1DCNN. The PW-Conv used for G-Conv feature fusion in GF-1DCNN is removed for comparison, we refer to the GF-1DCNN without PW-Conv as G-1DCNN for short hereinafter. In conclusion, the models with different components for ablation studies are shown in Table\u00a01.\nIn addition, during the model training, we set the training epochs T = 200, batch size B = 64, and the learning rate\n= 10\u22123 . In this study, Python 3.9 was utilized along with the open-source deep learning framework PyTorch 2.0.1 LTS [42] for constructing and training the deep learning models. Furthermore, network training was accelerated using CUDA 11.1.\nTo further compare GFAC-1DCNN with other DL-based state-of-the-art ballistic target HRRP recognition methods based on deep learning, we still use PyTorch to construct stacked denoising sparse autoencoder (sDSAE) [26], bidirectional LSTM (Bi-LSTM) [31], CNN1D-CA [17], MSGF1DCNN [35], and Bi-GRU [32] for comparison. Meanwhile, to analyze the recognition performance of each target over\ntest in more detail, the precision rate FP , the recall rate FR , and F1-measure FM are calculated for each HRRP recognition method, respectively; while the overall recognition performance is still evaluated using the overall testing accuracy over test . The FP , FR and FM are expressed by Eq.\u00a0(34), where FP represents the proportion of samples recognized as positive examples that are actually positive examples, FR represents the proportion of samples correctly recognized as positive examples to all samples that are actually positive,\nThe maximum value of each row is bold-faced\nNetwork type Kernel size 1D-Conv channel settings\nC 1\nC 2\nC 3\nC 4\nC 5\nC 6\n1DCNN H = 3 94.72 94.64 95.03 94.53 95.39 94.97 H = 5 94.86 95.56 95.58 95.97 95.75 95.64 H = 7 94.56 94.89 95.86 95.58 95.50 95.69 H = 9 94.81 95.28 95.22 95.69 95.83 95.72\nAC-1DCNN H = 3 92.86 94.72 95.69 95.58 95.50 95.64 H = 5 95.22 95.94 96.92 96.42 96.58 96.50 H = 7 95.86 96.75 96.97 96.81 96.69 96.64 H = 9 95.97 96.75 97.11 97.06 96.92 96.61\nG-1DCNN H = 3 72.39 78.72 83.42 87.25 89.64 90.33 H = 5 81.81 84.08 88.78 91.56 93.08 93.42 H = 7 84.92 88.81 91.11 92.89 93.92 94.28 H = 9 86.53 90.58 92.39 93.53 94.33 94.83\nGAC-1DCNN H = 3 73.47 77.78 80.89 85.58 88.33 90.33 H = 5 78.69 84.78 88.47 91.06 92.25 93.47 H = 7 82.92 89.08 91.72 92.97 93.83 94.67 H = 9 85.22 90.53 92.25 93.39 94.42 94.28\nGF-1DCNN H = 3 89.86 93.92 95.36 96.22 96.67 96.92 H = 5 91.75 95.33 96.19 96.47 97.19 97.19 H = 7 93.53 95.83 96.25 96.83 97.00 97.31 H = 9 94.33 95.56 96.47 96.92 97.22 97.17\nGFAC-1DCNN H = 3 89.31 93.97 95.33 95.83 96.39 96.47 H = 5 91.81 95.25 96.61 96.94 97.11 97.00 H = 7 93.17 96.00 96.83 97.00 97.11 97.31 H = 9 94.25 96.11 97.14 97.28 97.28 97.48\n1 3\nNetwork type Kernel size 1D-Conv channel settings\nC 1\nC 2\nC 3\nC 4\nC 5\nC 6\n1DCNN H = 3 8.77 33.67 131.85 521.73 2075.65 8280.07 H = 5 14.17 55.21 217.93 865.92 3452.17 13,785.61 H = 7 19.56 76.74 304.01 1210.12 4828.68 19,291.14 H = 9 24.95 98.28 390.09 1554.31 6205.19 24,796.68\nAC-1DCNN H = 3 9.07 34.24 132.98 523.99 2080.15 8289.04 H = 5 14.46 55.78 219.06 868.18 3456.66 13,794.58 H = 7 19.85 77.32 305.14 1212.37 4833.17 19,300.12 H = 9 25.24 98.85 391.22 1556.56 6209.68 24,805.65\nG-1DCNN H = 3 1.17 2.33 4.65 9.29 18.57 37.13 H = 5 1.41 2.81 5.61 11.21 22.41 44.81 H = 7 1.65 3.29 6.57 13.13 26.25 52.49 H = 9 1.89 3.77 7.53 15.05 30.09 60.17\nGAC-1DCNN H = 3 1.46 2.90 5.78 11.54 23.06 46.10 H = 5 1.70 3.38 6.74 13.46 26.90 53.78 H = 7 1.94 3.86 7.70 15.38 30.74 61.46 H = 9 2.18 4.34 8.66 17.30 34.58 69.14\nGF-1DCNN H = 3 6.61 24.09 91.69 357.45 1411.21 5607.69 H = 5 6.85 24.57 92.65 359.37 1415.05 5615.37 H = 7 7.09 25.05 93.61 361.29 1418.89 5623.05 H = 9 7.33 25.53 94.57 363.21 1422.73 5630.73\nGFAC-1DCNN H = 3 6.90 24.66 92.82 359.70 1415.70 5616.66 H = 5 7.14 25.14 93.78 361.62 1419.54 5624.34 H = 7 7.38 25.62 94.74 363.54 1423.38 5632.02 H = 9 7.62 26.10 95.70 365.46 1427.22 5639.70\nInternational Journal of Computational Intelligence Systems (2023) 16:190\n1 3\nPage 13 of 19 190\nand FM is a weighted summation average of FP and FR , which is a combination of precision and recall.\nwhere TP stands for True Positive which indicates the number of positive examples classified accurately, the term FP shows False-Positive value, i.e., the number of actual negative examples classified as positive, TN stands for True Negative which shows the number of negative examples classified accurately, and similarly, FN means a False-Negative value which is the number of actual positive examples classified as negative."
        },
        {
            "heading": "4.3 Ablation Study",
            "text": "To verify the impact of different hyper-parameters on the model performance as well as conduct ablation studies of feature fusion, 6 types of networks, i.e., 1DCNN, AC1DCNN G-1DCNN, GAC-1DCNN, GF-1DCNN, and GFAC-1DCNN were trained and tested with different kernel sizes and channel settings. To evaluate the recognition performance, Table\u00a02 shows the testing accuracy of those models with different kernel sizes and channel settings over test . What is more, to evaluate the computational complexity of different models, we count up the parameter number of different models and Table\u00a03 shows the results. From Tables\u00a02 and 3, the following conclusions can be drawn.\n1. The Impact of Kernel Size As shown in Table\u00a02, when the 6 models have the same 1D-Conv channel settings,\n(34) \u23a7\u23aa\u23a8\u23aa\u23a9 FP = TP\u2215(TP + FP), FR = TP\u2215(TP + FN), FM = 2 \u00d7 FP \u00d7 FR\u2215(FP + FR),\nincreasing the kernel size generally improves the recognition performance in most cases. Table\u00a03 clearly demonstrates that increasing the kernel size also leads to an increase in the number of parameters. This suggests that increasing the kernel size enhances recognition performance but also raises computational complexity. 2. The Impact of 1D-Conv Channel Settings As shown in Table\u00a02, increasing the channel number generally improves recognition performance for most models with the same kernel size. However, there may be a decrease in performance for some models when the channel number becomes too large. The optimal 1D-Conv channel settings for each kernel size are as follows: C5 for 1DCNN, C3 for AC-1DCNN, C6 for G-1DCNN, GAC1DCNN, GF-1DCNN, and GFAC-1DCNN. It is worth noting that increasing the channel number also results in an increase in the number of parameters, as shown in Table\u00a03. 3. The Impact of G-Conv (1DCNN vs G-1DCNN, AC1DCNN vs GAC-1DCNN) From Table\u00a02, it is evident that replacing the standard 1D-Conv with G-Conv results in a decrease in recognition performance, despite a significant decrease in complexity as shown in Table\u00a03. This is due to G-Conv breaking the fully connected manner of standard 1D-Conv, which leads to a hindered information flow between channel groups. 4. The Impact of GF-Conv (G-1DCNN vs GF-1DCNN, GAC-1DCNN vs GFAC-1DCNN, 1DCNN vs GF1DCNN, AC-1DCNN vs GFAC-1DCNN) Compared to G-Conv, GF-Conv improves recognition performance by utilizing PW-Conv to fuse grouped features from G-Conv. Although adding PW-Conv after G-Conv increases computational complexity, it is still lower than standard 1D-Conv. When the 1D-Conv channel settings are C1 , C2 , C3 and C4 , GF-Conv has lower testing recognition accuracy than standard 1D-Conv. However, when the channel settings are C5 and C6 , GF-Conv significantly improves recognition performance. We believe this phenomenon is due to standard 1D-Conv being over-parameterized, while GF-Conv is somewhat underparameterized, which can be seen in Table\u00a03. As the channel number increases, standard 1D-Conv becomes more over-parameterized, whereas GF-Conv becomes better parameterized. Consequently, the recognition performance of 1DCNN and AC-1DCNN decreases after C4 and C3 , respectively. In contrast, the recognition performance of GF-1DCNN and GFAC-1DCNN exhibits an approximately monotonically increasing trend from C1 to C6. 5. The Impact of Layer-Wise Auxiliary Classifiers (1DCNN vs AC-1DCNN, G-1DCNN vs GAC-1DCNN, GF-1DCNN vs GFAC-1DCNN) Table\u00a02 shows that, for certain kernel sizes and 1D-Conv channel settings, the recognition\nInternational Journal of Computational Intelligence Systems (2023) 16:190\n1 3\n190 Page 14 of 19\nperformance is improved with the inclusion of layerwise auxiliary classifiers. Specifically, the recognition of 1DCNN and GF-1DCNN is significantly enhanced with layer-wise auxiliary classifiers. However, there are still instances where the recognition performance is not improved, indicating the need for further in-depth research to enhance the effectiveness of layer-wise auxiliary classifiers in the future. Table\u00a03 demonstrates that the introduction of auxiliary classifiers only slightly increases the computational complexity, which is applicable to missile defense systems.\n6. The Impact of Combing GF-Conv and Layer-wise Auxiliary Classifiers (1DCNN vs GFAC-1DCNN) Based on the results presented in Table\u00a02, GFAC-1DCNN exhibits higher testing accuracy compared to 1DCNN when the 1D-Conv channel settings are adjusted to C5 and C6 . Additionally, Fig.\u00a06 depicts the testing accuracy curves of GFAC-1DCNN and 1DCNN. It is evident from Fig.\u00a06 that GFAC-1DCNN demonstrates a faster convergence rate, reaching a higher asymptote. Moreover, GFAC1DCNN has a smaller parameter number, further affirm-\nTable 4 Recognition results of different deep learning methods\nNetwork type Metrics Recognition results of each type of target (%) Overall testing accuracy (%)\nSpherical decoy Mothership Simple decoy High-imitated decoy Warhead\nsDSAE [26] F P 100.00 92.24 99.85 98.21 83.59 94.56 F R\n100.00 93.30 96.34 90.90 91.96 F M\n100.00 92.77 98.07 94.41 87.58 Bi-LSTM [31] F\nP 100.00 98.12 93.76 91.38 95.94 95.83\nF R 100.00 96.86 97.65 96.86 88.40 F M\n100.00 97.49 95.66 94.04 92.02 CNN1D-CA [17] F\nP 100.00 99.86 97.90 95.18 91.45 96.89\nF R 100.00 98.31 96.55 95.86 93.51 F M\n100.00 99.08 97.22 95.52 92.47 Bi-GRU [32] F\nP 100.00 97.99 94.67 94.29 93.78 96.14\nF R 100.00 97.29 98.34 93.86 91.37 F M\n100.00 97.64 96.47 94.07 92.56 1DCNN F\nP 100.00 99.56 95.21 92.51 92.76 95.97\nF R 100.00 96.86 96.40 96.11 90.85 F M\n100.00 98.19 95.81 94.27 91.80 MSGF-1DCNN [35] F\nP 100.00 99.56 99.01 96.04 91.68 97.11\nF R 100.00 97.57 96.95 94.31 96.52 F M\n100.00 98.56 97.97 95.17 94.03 AC-1DCNN F\nP 100.00 98.74 99.58 94.06 93.27 97.11\nF R 100.00 97.78 98.89 94.58 94.31 F M\n100.00 98.26 99.23 94.32 93.78 G-1DCNN F\nP 100.00 98.55 98.54 90.09 87.79 94.83\nF R 100.00 94.58 94.03 94.72 90.83 F M\n100.00 96.53 96.23 92.35 89.28 GAC-1DCNN F\nP 100.00 98.40 97.86 94.99 83.74 94.67\nF R 100.00 94.17 95.14 89.58 94.44 F M\n100.00 96.24 96.48 92.21 88.77 GF-1DCNN F\nP 100.00 99.15 99.44 94.11 93.91 97.31\nF R 100.00 97.64 99.31 95.42 94.17 F M\n100.00 98.39 99.37 94.76 94.04 GFAC-1DCNN F\nP 100.00 98.60 99.31 96.58 93.42 97.48\nF R 100.00 97.54 99.34 93.83 96.67 F M 100.00 98.12 99.38 95.29 95.02\n1 3\ning the effectiveness and advancement of our proposed method.\nFigure\u00a0 7 displays the receiver operating characteristic (ROC) curves and the corresponding area under curve (AUC) values of various HRRP recognition methods, with warheads as positive samples and other targets as negative samples, where FPR = TP\u2215(FN + TP) and TPR = TN\u2215(TN + FP) . Demonstrating exceptional performance specifically for warheads, GFAC-1DCNN achieves a remarkable AUC value of 0.9972. This high AUC value highlights the significant importance of GFAC-1DCNN's recognition capabilities in anti-missile missions.\nBased on the above experimental results, the proposed GF-Conv decreases computation complexity and enhances feature extraction by breaking away from the fully connected manner of standard 1D-Conv. Furthermore, the layerwise auxiliary classifier serves as a lightweight module to enhance recognition performance. It is noteworthy that our methods are particularly effective for over-parameterized 1DCNN models, especially those with larger channel numbers. The increase in recognition performance is more significant when the 1D-Conv channel settings are adjusted to C5 and C6."
        },
        {
            "heading": "4.4 Comparison with\u00a0Other Deep Learning Methods",
            "text": "Table\u00a04 presents the recognition results of deep neural network-based HRRP recognition methods. It is evident from\nthe table that each method yields varying recognition outcomes for different targets. Notably, all methods achieve 100% precision ( FP ) and recall ( FR ) for spherical decoys, which can be attributed to their distinct physical characteristics depicted in Fig.\u00a05. The distinguishability between spherical decoys and other targets is pronounced, thereby resulting in superior recognition performance for spherical decoys. However, some methods exhibit relatively low FR (< 95%) for warhead recognition, namely Bi-LSTM, sDSAE, BiGRU, CNN1D-CA, 1DCNN, AC-1DCNN, GAC-1DCNN, GF-1DCNN, and G-1DCNN, while GFAC-1DCNN demonstrates the highest FR for warhead recognition.\nFigure\u00a08 provides a detailed view of the recognition accuracy of different targets across the 11 compared models. Notably, GFAC-1DCNN exhibits excellent recognition performance for warheads and simple decoys, along with superior accuracy for all samples. However, the recognition accuracy for warheads lags behind that of other targets, demanding further research to enhance warhead recognition precision.\nIt is worth mentioning that GFAC-1DCNN exhibits a slightly lower FP for warhead recognition compared to BiLSTM, Bi-GRU, and GF-1DCNN, potentially leading to a higher false alarm rate in anti-missile systems. Nevertheless, the higher FR can enhance missile interception rates and reduce losses.\nTo further analyze the recognition performance of CNNbased methods in more detail, we plot the confusion matrix for each method using the test , as shown in Fig.\u00a09. From\n1 3\nInternational Journal of Computational Intelligence Systems (2023) 16:190\n1 3\nPage 17 of 19 190\nFig.\u00a09, it is evident that all CNN-based models, namely 1DCNN, AC-1DCNN, CNN1D-CA, G-1DCNN, GAC1DCNN, GF-1DCNN, MSGF-1DCNN, and GFAC-1DCNN, misclassify a significant proportion of warhead samples as high-imitated decoys. Similarly, all 8 DL methods misidentify a large number of high-imitated decoy samples as warheads. This observation aligns with the findings presented in Fig.\u00a05, demonstrating that the physical characteristics of warheads and high-imitated decoys are remarkably similar. Consequently, distinguishing between the two proves challenging, leading to poor recognition performance in this aspect. Furthermore, it is worth noting that 1DCNN, AC-1DCNN, CNN1D-CA, G-1DCNN, GAC-1DCNN, GF-1DCNN, and MSGF-1DCNN exhibit a higher rate of misclassification for mothership and simple decoy samples as warheads. However, this limitation has been effectively addressed by our proposed method, GFAC-1DCNN, showcasing its superiority."
        },
        {
            "heading": "5 Conclusions and\u00a0Future Scope",
            "text": "To address the issue of high computational complexity in standard 1DCNN for ballistic target HRRP recognition, this paper proposes a lightweight GFAC-1DCNN. The contributions are determined as follows.\n1. The proposed GFAC-1DCNN architecture introduces G-Conv to replace standard convolutions, which significantly reduces computational complexity and model size by maximizing the group number. This enables real-time deployment feasibility for missile defense systems. 2. The linear fusion layer after G-Conv promotes information flow between groups, overcoming the limitation of lost inter-group connections that occur with naive G-Conv. This allows the aggregation of multi-group features to enhance representation learning. 3. Layer-wise auxiliary classifiers are designed to leverage hierarchical features from different network depths. Fusing their outputs improves accuracy by combining multi-level information, complementing the representation learned by the deepest classifier.\nExperimental results demonstrate that GFAC-1DCNN advances the SOTA in higher overall testing accuracy and recall rate for warhead targets compared to other deep learning methods while maintaining lower computational complexity. This highlights the potential of GFC-1DCNN to reduce computational burden in ballistic missile defense systems and enhance interception rates, making it highly valuable in engineering applications.\nHowever, the selection of hyper-parameters in GFAC1DCNN is currently based on rule-of-thumb methods,\nwhich may not be suitable for real-life scenarios. In the future, we plan to incorporate neural architecture search methods to obtain proper hyper-parameter settings for various real-life scenarios. Additionally, it is important to note that auxiliary classifiers do not consistently improve recognition across all kernel sizes and 1D-Conv channel settings. Therefore, a further in-depth research is needed to fully understand and enhance the interpretation, versatility, and scalability of the proposed auxiliary classifiers.\nAcknowledgements The authors are grateful to the editor and the anonymous reviewers for their constructive comments.\nAuthor Contributions QX: conceptualization, methodology, writing\u2014 original draft, writing\u2014review and editing, software. XW: funding acquisition, methodology, project administration, supervision. YS: funding acquisition, project administration, supervision. LL: funding acquisition, software, supervision. RL: project administration, resources, software. JL: resources, software, validation.\nFunding This research was funded by the National Natural Science Foundation of China, grant numbers 61806219, 61876189, 61503407, 61703426, and 61273275; the Young Talent Fund of University Association for Science and Technology in Shaanxi, China, grant numbers 20190108 and 20220106; and the Innovation Talent Supporting Project of Shaanxi, China, grant number 2020KJXX-065.\nData Availability The datasets generated during and analyzed during the current study are available from the corresponding author on reasonable request.\nDeclarations\nConflict of Interest The authors declare no conflict of interest.\nEthical Approval Not applicable.\nConsent to Participate Not applicable.\nConsent for Publication Not applicable.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/."
        }
    ],
    "title": "Group\u2010Fusion One\u2010Dimensional Convolutional Neural Network for Ballistic Target High\u2010Resolution Range Profile Recognition with Layer\u2010Wise Auxiliary Classifiers",
    "year": 2023
}