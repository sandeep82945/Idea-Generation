{
    "abstractText": "populations Shengxian Wang,1, 2 Xiaojie Chen,1, \u2217 Zhilong Xiao,1, 3 Attila Szolnoki,4 and V\u0131\u0301tor V. Vasconcelos5, 6 1School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu 611731, China 2Faculty of Science and Engineering, University of Groningen, Groningen 9747 AG, The Netherlands 3School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou 510006, China 4Institute of Technical Physics and Materials Science, Centre for Energy Research, P.O. Box 49, Budapest H-1525, Hungary 5Computational Science Lab, Informatics Institute, University of Amsterdam, 1098XH Amsterdam, The Netherlands 6Institute for Advanced Study, University of Amsterdam, 1012 GC, Amsterdam, The Netherlands",
    "authors": [
        {
            "affiliations": [],
            "name": "Shengxian Wang"
        },
        {
            "affiliations": [],
            "name": "Xiaojie Chen"
        },
        {
            "affiliations": [],
            "name": "Zhilong Xiao"
        },
        {
            "affiliations": [],
            "name": "Attila Szolnoki"
        },
        {
            "affiliations": [],
            "name": "V\u0131\u0301tor V. Vasconcelos"
        }
    ],
    "id": "SP:d6af77971b4110a511669016f387873d93301781",
    "references": [
        {
            "authors": [
                "S Hauert",
                "S Mitri",
                "L Keller",
                "D. Floreano"
            ],
            "title": "Evolving Cooperation: From Biology to Engineering. The Horizons of Evolutionary Robotics",
            "year": 2010
        },
        {
            "authors": [
                "M Perc",
                "J Jordan J",
                "G Rand D",
                "Z Wang",
                "S Boccaletti",
                "A. Szolnoki"
            ],
            "title": "Statistical physics of human cooperation",
            "year": 2017
        },
        {
            "authors": [
                "J Hofbauer",
                "K. Sigmund"
            ],
            "title": "Evolutionary games and population dynamics",
            "year": 1998
        },
        {
            "authors": [
                "A Nowak M",
                "M. May R"
            ],
            "title": "Evolutionary games and spatial chaos",
            "venue": "Nature 359,",
            "year": 1992
        },
        {
            "authors": [
                "P Erd\u0151s",
                "A. R\u00e9nyi"
            ],
            "title": "On random graphs I",
            "venue": "Publ. Math. Debrecen",
            "year": 1959
        },
        {
            "authors": [
                "J Watts D",
                "H. Strogatz S"
            ],
            "title": "Collective dynamics of \u2018small-world",
            "venue": "networks. Nature",
            "year": 1998
        },
        {
            "authors": [
                "L Barab\u00e1si A",
                "R. Albert"
            ],
            "title": "Emergence of scaling in random networks",
            "venue": "Science 286,",
            "year": 1999
        },
        {
            "authors": [
                "C Santos F",
                "M. Pacheco J"
            ],
            "title": "Scale-free networks provide a unifying framework for the emergence of cooperation",
            "venue": "Phys. Rev. Lett",
            "year": 2005
        },
        {
            "authors": [
                "E Tarnita C",
                "H Ohtsuki",
                "T Antal",
                "F Fu",
                "A. Nowak M"
            ],
            "title": "Strategy selection in structured populations",
            "venue": "J. Theor. Biol",
            "year": 2009
        },
        {
            "authors": [
                "B Allen",
                "G Lippner",
                "T Chen Y",
                "B Fotouhi",
                "N Momeni",
                "T Yau S",
                "A. Nowak M"
            ],
            "title": "Evolutionary dynamics on any population",
            "year": 2017
        },
        {
            "authors": [
                "Q Su",
                "A McAvoy",
                "Y Mori",
                "B. Plotkin J"
            ],
            "title": "Evolution of prosocial behaviours in multilayer populations",
            "venue": "Nat. Hum. Behav",
            "year": 2022
        },
        {
            "authors": [
                "Q Su",
                "B Allen",
                "B. Plotkin J"
            ],
            "title": "Evolution of cooperation with asymmetric social interactions",
            "venue": "Proc. Natl. Acad. Sci. USA 119,",
            "year": 2022
        },
        {
            "authors": [
                "H Ohtsuki",
                "C Hauert",
                "E Lieberman",
                "A. Nowak M"
            ],
            "title": "A simple rule for the evolution of cooperation on graphs and social networks. Nature",
            "year": 2006
        },
        {
            "authors": [
                "H Ohtsuki",
                "A. Nowak M"
            ],
            "title": "The replicator equation on graphs",
            "venue": "J. Theor. Biol",
            "year": 2006
        },
        {
            "authors": [
                "A Nowak M",
                "E Tarnita C",
                "T. Antal"
            ],
            "title": "Evolutionary dynamics in structured populations",
            "venue": "Philos. Trans. R. Soc. B",
            "year": 2010
        },
        {
            "authors": [
                "L Zhou",
                "B Wu",
                "J Du",
                "L. Wang"
            ],
            "title": "Aspiration dynamics generate robust predictions in heterogeneous populations",
            "venue": "Nat. Comm",
            "year": 2021
        },
        {
            "authors": [
                "V Capraro",
                "M. Perc"
            ],
            "title": "Mathematical foundations of moral preferences",
            "venue": "J. R. Soc. Interface",
            "year": 2021
        },
        {
            "authors": [
                "V Capraro",
                "Y Halpern J",
                "M. Perc"
            ],
            "title": "From outcome-based to language-based preferences",
            "venue": "J. Econ. Lit. (in press)",
            "year": 2022
        },
        {
            "authors": [
                "J. Henrich"
            ],
            "title": "Cooperation, punishment, and the evolution of human institutions",
            "venue": "Science 312,",
            "year": 2006
        },
        {
            "authors": [
                "\u00d6 G\u00fcrerk",
                "B Irlenbusch",
                "B. Rockenbach"
            ],
            "title": "The competitive advantage of sanctioning institutions",
            "venue": "Science 312,",
            "year": 2006
        },
        {
            "authors": [
                "A Dreber",
                "G Rand D",
                "D Fudenberg",
                "A. Nowak M"
            ],
            "title": "Winners don\u2019t punish",
            "venue": "Nature 452,",
            "year": 2008
        },
        {
            "authors": [
                "G Rand D",
                "A Dreber",
                "T Ellingsen",
                "D Fudenberg",
                "A. Nowak M"
            ],
            "title": "Positive interactions promote public cooperation",
            "venue": "Science 325,",
            "year": 2009
        },
        {
            "authors": [
                "K Sigmund",
                "H De Silva",
                "A Traulsen",
                "C. Hauert"
            ],
            "title": "Social learning promotes institutions for governing the commons",
            "venue": "Nature 466,",
            "year": 2010
        },
        {
            "authors": [
                "A Han T",
                "M Pereira L",
                "T. Lenaerts"
            ],
            "title": "Avoiding or restricting defectors in public goods games",
            "venue": "J. R. Soc. Interface",
            "year": 2015
        },
        {
            "authors": [
                "P Mann R",
                "D. Helbing"
            ],
            "title": "Optimal incentives for collective intelligence",
            "venue": "Proc. Natl. Acad. Sci. USA",
            "year": 2017
        },
        {
            "authors": [
                "J Riehl",
                "P Ramazi",
                "M. Cao"
            ],
            "title": "Incentive-based control of asynchronous best-response dynamics on binary decision networks",
            "venue": "IEEE Trans. Control Netw. Syst",
            "year": 2018
        },
        {
            "authors": [
                "V Vasconcelos V",
                "A Dannenberg",
                "A. Levin S"
            ],
            "title": "Punishment institutions selected and sustained through voting and learning",
            "venue": "Nat. Sustain",
            "year": 2022
        },
        {
            "authors": [
                "T Sasaki",
                "\u00c5 Br\u00e4nnstr\u00f6m",
                "U Dieckmann",
                "K. Sigmund"
            ],
            "title": "The take-it-or-leave-it option allows small penalties to overcome social dilemmas",
            "venue": "Proc. Natl. Acad. Sci. USA",
            "year": 2012
        },
        {
            "authors": [
                "V Vasconcelos V",
                "C Santos F",
                "M. Pacheco J"
            ],
            "title": "A bottom-up institutional approach to cooperative governance of risky commons",
            "venue": "Nat. Clim. Change",
            "year": 2013
        },
        {
            "authors": [
                "X Chen",
                "T Sasaki",
                "\u00c5 Br\u00e4nnstr\u00f6m",
                "U. Dieckmann"
            ],
            "title": "First carrot, then stick: how the adaptive hybridization of incentives promotes cooperation",
            "venue": "J. R. Soc. Interface",
            "year": 2015
        },
        {
            "authors": [
                "S Wang",
                "X Chen",
                "A. Szolnoki"
            ],
            "title": "Exploring optimal institutional incentives for public cooperation",
            "venue": "Commun. Nonlinear Sci. Numer. Simulat",
            "year": 2019
        },
        {
            "authors": [
                "H Duong M",
                "A. Han T"
            ],
            "title": "Cost efficiency of institutional incentives for promoting cooperation in finite populations",
            "venue": "Proc. Roy. Soc. A 477,",
            "year": 2021
        },
        {
            "authors": [
                "G Szab\u00f3",
                "C. T\u0151ke"
            ],
            "title": "Evolutionary prisoner\u2019s dilemma game on a square lattice",
            "venue": "Phys. Rev. E",
            "year": 1998
        },
        {
            "authors": [
                "A Nowak M",
                "A Sasaki",
                "C Taylor",
                "D. Fudenberg"
            ],
            "title": "Emergence of cooperation and evolutionary stability in finite populations",
            "venue": "Nature 428,",
            "year": 2004
        },
        {
            "authors": [
                "C. Evans L"
            ],
            "title": "An introduction to mathematical optimal control theory",
            "year": 2005
        },
        {
            "authors": [
                "P. Geering H"
            ],
            "title": "Optimal control with engineering applications",
            "year": 2007
        },
        {
            "authors": [
                "S Lenhart",
                "T. Workman J"
            ],
            "title": "Optimal control applied to biological models",
            "year": 2007
        },
        {
            "authors": [
                "E. Ostrom"
            ],
            "title": "Governing the commons: the evolution of institutions for collective action",
            "year": 1990
        },
        {
            "authors": [
                "S. G\u00e4chter"
            ],
            "title": "Carrot or stick",
            "venue": "Nature 483,",
            "year": 2012
        },
        {
            "authors": [
                "S. Morita"
            ],
            "title": "Extended pair approximation of evolutionary game on complex networks",
            "venue": "Prog. Theor. Phys",
            "year": 2008
        },
        {
            "authors": [
                "E Overton C",
                "M Broom",
                "C Hadjichrysanthou",
                "J. Sharkey K"
            ],
            "title": "Methods for approximating stochastic evolutionary dynamics on graphs",
            "venue": "J. Theor. Biol",
            "year": 2019
        },
        {
            "authors": [
                "L Pinheiro F",
                "C Santos F",
                "M. Pacheco J"
            ],
            "title": "How selection pressure changes the nature of social dilemmas in structured populations",
            "venue": "New J. Phys",
            "year": 2012
        },
        {
            "authors": [
                "I Zisis",
                "S Di Guida",
                "A Han T",
                "G Kirchsteiger",
                "T. Lenaerts"
            ],
            "title": "Generosity motivated by acceptance- 18 evolutionary analysis of an anticipation",
            "venue": "game. Sci. Rep",
            "year": 2015
        },
        {
            "authors": [
                "A McAvoy",
                "A Rao",
                "C. Hauert"
            ],
            "title": "Intriguing effects of selection intensity on the evolution of prosocial behaviors",
            "venue": "PLoS Comput. Biol",
            "year": 2021
        },
        {
            "authors": [
                "A Han T",
                "L. Tran-Thanh"
            ],
            "title": "Cost-effective external interference for promoting the evolution of cooperation",
            "venue": "Sci. Rep",
            "year": 2018
        },
        {
            "authors": [
                "R Ibsen-Jensen",
                "K Chatteriee",
                "A. Nowak M"
            ],
            "title": "Computational complexity of ecological and evolutionary spatial dynamics",
            "venue": "Proc. Natl. Acad. Sci. USA",
            "year": 2015
        },
        {
            "authors": [
                "M Doebeli",
                "C. Hauert"
            ],
            "title": "Models of cooperation based on the prisoner\u2019s dilemma and the snowdrift game",
            "venue": "Ecol. Lett",
            "year": 2005
        },
        {
            "authors": [
                "B. Skyrms"
            ],
            "title": "The stag hunt and the evolution of social structure",
            "year": 2004
        },
        {
            "authors": [
                "M Perc",
                "J G\u00f3mez-Garde\u00f1es",
                "A Szolnoki",
                "M Flor\u0131\u0301a",
                "Y. Moreno"
            ],
            "title": "Evolutionary dynamics of group interactions on structured populations: A review",
            "venue": "J. R. Soc. Interface",
            "year": 2013
        },
        {
            "authors": [
                "A Li",
                "B Wu",
                "L. Wang"
            ],
            "title": "Cooperation with both synergistic and local interactions can be worse than each alone",
            "venue": "Sci. Rep",
            "year": 2014
        },
        {
            "authors": [
                "A Li",
                "M Broom",
                "J Du",
                "L. Wang"
            ],
            "title": "Evolutionary dynamics of general group interactions in structured populations",
            "venue": "Phys. Rev. E",
            "year": 2016
        },
        {
            "authors": [
                "J Grilli",
                "G Barab\u00e1s",
                "J Michalska-Smith M",
                "S. Allesina"
            ],
            "title": "Higher-order interactions stabilize dynamics in competitive network models",
            "venue": "Nature 548,",
            "year": 2017
        },
        {
            "authors": [
                "U Alvarez-Rodriguez",
                "F Battiston",
                "G Ferraz de Arruda",
                "Y Moreno",
                "M Perc",
                "V. Latora"
            ],
            "title": "Evolutionary dynamics of higher-order interactions in social networks",
            "venue": "Nat. Hum. Behav",
            "year": 2021
        },
        {
            "authors": [
                "A Han T",
                "S Lynch",
                "L Tran-Thanh",
                "C. Santos F"
            ],
            "title": "Fostering cooperation in structured populations through local and global interference strategies",
            "venue": "In Proc. of the 27th Int. Joint Conf. on Artificial Intelligence and the 23rd European Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "T Cimpeanu",
                "C Perret",
                "A. Han T"
            ],
            "title": "Cost-efficient interventions for promoting fairness in the ultimatum",
            "venue": "game. Knowl.-Based Syst",
            "year": 2021
        },
        {
            "authors": [
                "T Cimpeanu",
                "A Han T",
                "C. Santos F"
            ],
            "title": "Exogenous rewards for promoting cooperation in scale-free networks",
            "venue": "In Proc. of 2019 Conf. on Artificial Life,",
            "year": 2019
        },
        {
            "authors": [
                "V Capraro",
                "M Perc",
                "D. Vilone"
            ],
            "title": "Lying on networks: The role of structure and topology in promoting honesty",
            "venue": "Phys. Rev. E",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Optimization of institutional incentives for cooperation in structured\npopulations\nShengxian Wang,1, 2 Xiaojie Chen,1, \u2217 Zhilong\nXiao,1, 3 Attila Szolnoki,4 and V\u0131\u0301tor V. Vasconcelos5, 6\n1School of Mathematical Sciences, University of Electronic\nScience and Technology of China, Chengdu 611731, China\n2Faculty of Science and Engineering, University of Groningen, Groningen 9747 AG, The Netherlands\n3School of Computer Science and Engineering,\nSun Yat-sen University, Guangzhou 510006, China\n4Institute of Technical Physics and Materials Science,\nCentre for Energy Research, P.O. Box 49, Budapest H-1525, Hungary\n5Computational Science Lab, Informatics Institute,\nUniversity of Amsterdam, 1098XH Amsterdam, The Netherlands\n6Institute for Advanced Study, University of Amsterdam, 1012 GC, Amsterdam, The Netherlands\nThe application of incentives, such as reward and punishment, is a frequently applied way\nfor promoting cooperation among interacting individuals in structured populations. How-\never, how to properly use the incentives is still a challenging problem for incentive-providing\ninstitutions. In particular, since the implementation of incentive is costly, to explore the opti-\nmal incentive protocol, which ensures the desired collective goal at a minimal cost, is worthy\nof study. In this work, we consider the positive and negative incentives respectively for a\nstructured population of individuals whose conflicting interactions are characterized by a\nprisoner\u2019s dilemma game. We establish an index function for quantifying the cumulative\ncost during the process of incentive implementation, and theoretically derive the optimal\npositive and negative incentive protocols for cooperation on regular networks. We find that\nboth types of optimal incentive protocols are identical and time-invariant. Moreover, we\ncompare the optimal rewarding and punishing schemes concerning implementation cost and\nprovide a rigorous basis for the usage of incentives in the game-theoretical framework. We\nfurther perform computer simulations to support our theoretical results and explore their ro-\nar X\niv :2\n30 1.\n06 47\n2v 1\n[ cs\n.G T\n] 1\n6 Ja\nn 20\n23\n2 bustness for different types of population structures, including regular, random, small-world,\nand scale-free networks.\n\u2217Electronic address: xiaojiechen@uestc.edu.cn\n3"
        },
        {
            "heading": "1. Introduction",
            "text": "Cooperation is of vital importance in the contemporary era [1]. However, its evolution and emergence conflict with the immediate self-interest of interacting individuals [2]. Evolutionary game theory provides a common mathematical framework to depict how agents interact with each other in a dynamical process and to predict how cooperative action evolves from a population level [3]. As a representative paradigm, the prisoner\u2019s dilemma game has received considerable attention for studying the problem of cooperation in a population of interacting individuals [4].\nWhen individuals play the evolutionary prisoner\u2019s dilemma game in a large well-mixed population, in which all are equally likely to interact, cooperation cannot emerge. However, the realworld population structures are not well-mixed but relatively complicated, and the interactions among individuals are limited to a set of neighbors in a structured population [5]. Inspired by the fast progress of network science, several interaction topologies have been tested, including smallworld [6] and scale-free networks [7]. Features of these networks can influence the evolutionary dynamics of cooperation significantly [8\u201314]. Beside network topology, another crucial determinant of the evolution of cooperation is the strategy update rule, which determines the microscopic update procedure [15\u201318]. For instance, cooperation can be favored under the so-called deathbirth strategy update rule if the benefit-to-cost ratio in the prisoner\u2019s dilemma game exceeds the average degree of the specific interaction network [15\u201317]. In contrast, it cannot emerge under the alternative birth-death strategy update rule [15].\nOverall, situations abound when cooperation cannot emerge in structured populations when no additional regulation mechanisms or moral nudges are incorporated [15, 19, 20]. In these unfavorable environments, prosocial incentives can be used to sustain cooperation among unrelated and competing agents [21\u201329]. Specifically, cooperators can be rewarded for their positive acts or defectors are punished for their sweepingly negative impact. At the individual level, cooperation can be favored whenever the amount of incentive exceeds the payoff difference between cooperating and defecting, no matter whether the incentive is positive or negative. However, in a given unfavorable environment for the emergence of cooperation, it is still unclear how intensive positive or negative incentive is needed to drive the population toward the desired direction. Furthermore, applying incentives is always costly [30\u201334]. Previous related works assume that, when used, the incentive amount is fixed at a certain value [30, 32]. In those circumstances, the obtained incentive protocol is not necessarily the one with the minimal cumulative cost [33]. Therefore, finding the optimal time-varying incentive protocol is vital to ensure effective interventions that\n4 drive populations towards a productive and cooperative state at a minimal execution cost.\nOur work addresses how much incentive is needed for cooperation to emerge and explores the optimal incentive protocols in a game-theoretical framework, where time-varying institutional positive or negative incentives are provided for a structured population of individuals playing the prisoner\u2019s dilemma game. We establish an index function for quantifying the executing cumulative cost. We systematically survey all relevant strategy update rules, including death-birth (DB), birthdeath (BD), imitation (IM), and pairwise-comparison (PC) updating [15\u201317, 35]. Using optimal control theory, we obtain the dynamical incentive protocol under each strategy update rule leading to the minimal cumulative cost for the emergence of cooperation. Interestingly, we find that the optimal negative and positive incentive protocols are identical and time-invariant for each given strategy update rule. However, applying punishment can induce a lower cumulative cost than the usage of reward if the initial cooperation level is larger than the difference between the full cooperation state and the desired cooperation state. Otherwise, applying reward requires a lower cost. Beside analytical calculations, we perform computer simulations confirming that our results are valid in a broad class of population structures. 2. Results We start with a structured population of N individuals who interact in a regular network of degree k > 2. In this graph, vertices represent interacting agents and the edges determine who interacts with whom. Each individual i plays the prisoner\u2019s dilemma game with its neighbors. An agent can choose either to be a cooperator (C), which confers a benefit b to its opponent at a cost c to itself, or to be a defector (D), which is costless and does not distribute any benefits. After playing the game with one neighbor, if institutional positive incentives are in place, the agent is rewarded an amount \u00b5R of incentive for choosing C. When negative institutional incentives are implemented, it is fined by an amount \u00b5P for choosing D. The agent collects an accumulated payoff \u03c0i by interacting with all neighbors. We set the fitness, fi, of individual i, chiefly the reproductive rate, as 1\u2212 \u03c9 + \u03c9\u03c0i, where \u03c9 (0 \u2264 \u03c9 \u2264 1) measures the strength of selection [36]. In this work, we concentrate on the effects of weak selection, meaning that 0 < \u03c9 1. Four different fitness-dependent strategy update rules are considered separately, and the details are given in the Model and Methods section. In addition, in order to help readers intuitively understand the evolutionary process in networked prisoner\u2019s dilemma game with institutional reward or punishment, we present an illustration figure as shown in figure 1.\n5"
        },
        {
            "heading": "2.1. Theoretical predictions for optimal incentive protocols",
            "text": "In this subsection, we respectively explore the incentive protocols under four alternative strategy update rules, including DB, BD, IM, and PC updating. The details of these calculations are described in electronic supplementary material, and here we only summarize the pair approximation approach for our dynamical system with positive or negative incentive in the weak selection limit.\nUsing the pair approximation approach (see section 1 in electronic supplementary material), we get the dynamical equation of the fraction of cooperators under DB rule with positive or negative incentive as\ndpC dt = FDB(pC , \u00b5v, t) = \u03c9(k \u2212 2) k \u2212 1 [b+ k(\u00b5v \u2212 c)]pC(1\u2212 pC) + o(\u03c92) , (1)\nwhere pC is the fraction of cooperators in the whole population, and \u00b5v the amount of positive incentive that one cooperator receives from an incentive-providing institution or the amount of negative incentive imposed on a defector by a central institution (note that such incentive is applied for every interaction as explained in the Model and Methods section). Eq. (1) has two equilibria, one at pC = 0 and the other at pC = 1. If \u00b5v > c \u2212 bk , the first is unstable and the second stable, indicating that cooperators prevail over defectors (further details are presented in section 1 of electronic supplementary material). Notably, in the absence of incentives, i.e., \u00b5v = 0, we get back the previously identified b/c > k condition for the evolution of cooperation [15, 16].\nUsing the Hamilton-Jacobi-Bellman (HJB) equation [37\u201339], in the condition of \u00b5v > c\u2212 bk >\n0, we obtain analytically the optimal protocol \u00b5\u2217v = 2(ck\u2212b) k both for reward and punishment (see section 1 in electronic supplementary material for details). The optimal rewarding and optimal punishing protocols are both time-invariant and the optimal incentive levels for punishment and reward are identical, namely, \u00b5\u2217v = \u00b5 \u2217 R = \u00b5 \u2217 P . Accordingly, with the optimal rewarding or punishing protocol, the dynamical system described by Eq. (1) can be solved and its solution is pC = 1\n1+ 1\u2212p0 p0\ne\u2212\u03b2DBt , where \u03b2DB = \u03c9(k\u22122)(ck\u2212b) k\u22121 and p0 = pC(0) > 0 denotes the initial fraction\nof cooperators in the population.\nThe cumulative cost produced by the rewarding protocol \u00b5\u2217R for the dynamical system to reach\nthe expected terminal state pC(tf ) from the initial state p0 is\nJ\u2217R = \u222b tf 0 (kNpC\u00b5 \u2217 R) 2 2 dt = (kN\u00b5\u2217R) 2 2\u03b2DB [p0 + \u03b4 \u2212 1 + ln( 1\u2212 p0 \u03b4 )], (2)\n6 and, similarly, the cumulative cost produced by the punishing protocol \u00b5\u2217P becomes\nJ\u2217P = \u222b tf 0 (kNpD\u00b5 \u2217 P ) 2 2 dt = (kN\u00b5\u2217P ) 2 2\u03b2DB [p0 + \u03b4 \u2212 1 + ln( 1\u2212 \u03b4 p0 )]. (3)\nWe further find that J\u2217R > J \u2217 P if p0 > \u03b4 and J \u2217 R < J \u2217 P if p0 < \u03b4 (see section 1 in electronic supplementary material for details).\nFor BD updating, the dynamical equation is\ndpC dt = FBD(pC , \u00b5v, t) = \u03c9k(k \u2212 2) k \u2212 1 (\u00b5v \u2212 c)pC(1\u2212 pC) + o(\u03c92). (4)\nWe prove that when \u00b5v > c, the system can reach the stable full cooperation state. Naturally, in the absence of incentives we get back the results of Ohtsuki et al. [15, 16]. By solving the HJB equation, the optimal incentive protocol is \u00b5\u2217v = 2c both for reward and punishment. The solution of Eq. (4) is pC = 1 1+\n1\u2212p0 p0\ne\u2212\u03b2BDt , where \u03b2BD = \u03c9k(k\u22122)c k\u22121 (see section 2 in electronic supplementary\nmaterial). Consequently, the cumulative cost to reach the expected terminal state in case of the optimal rewarding protocol is\nJ\u2217R = (kN\u00b5\u2217R) 2\n2\u03b2BD [p0 + \u03b4 \u2212 1 + ln( 1\u2212 p0 \u03b4 )], (5)\nwhile for the optimal punishing protocol it is\nJ\u2217P = (kN\u00b5\u2217P ) 2\n2\u03b2BD [p0 + \u03b4 \u2212 1 + ln( 1\u2212 \u03b4 p0 )]. (6)\nIn case of IM updating, the dynamical equation becomes\ndpC dt = FIM(pC , \u00b5v, t) = \u03c9k2(k \u2212 2) (k + 1)2(k \u2212 1) [b+ (\u00b5v \u2212 c)(k + 2)]pC(1\u2212 pC) + o(\u03c92) , (7)\nwhich indicates that the system evolves to the full cooperation state if \u00b5v > c\u2212 bk+2 . The solution of HJB for the optimal incentive protocol gives \u00b5\u2217v = 2[c(k+2)\u2212b] k+2 both for reward and punishment (see section 3 in electronic supplementary material). The solution of Eq. (7) is pC = 1 1+\n1\u2212p0 p0\ne\u2212\u03b2IMt ,\nwhere \u03b2IM = \u03c9k2(k\u22122)[c(k+2)\u2212b]\n(k+1)2(k\u22121) . The cumulative cost requires for the optimal rewarding protocol is\nJ\u2217R = (kN\u00b5\u2217R) 2\n2\u03b2IM [p0 + \u03b4 \u2212 1 + ln( 1\u2212 p0 \u03b4 )], (8)\nwhile for the optimal punishing protocol it becomes\nJ\u2217P = (kN\u00b5\u2217P ) 2\n2\u03b2IM [p0 + \u03b4 \u2212 1 + ln( 1\u2212 \u03b4 p0 )]. (9)\n7 When PC updating is applied, the dynamical equation is given by\ndpC dt = FPC(pC , \u00b5v, t) = \u03c9k(k \u2212 2) 2(k \u2212 1) (\u00b5v \u2212 c)pC(1\u2212 pC) + o(\u03c92), (10)\nfrom which we find that when \u00b5v > c is satisfied, the stable full cooperation state can be reached (see section 4 in electronic supplementary material). For the optimal incentive protocol, we get \u00b5\u2217v = \u00b5 \u2217 R = \u00b5 \u2217 P = 2c. The solution of Eq. (10) is pC = 1\n1+ 1\u2212p0 p0\ne\u2212\u03b2PCt , where \u03b2PC = \u03c9k(k\u22122)c 2(k\u22121) .\nConsequently, the cumulative cost for the optimal rewarding protocol is\nJ\u2217R = (kN\u00b5\u2217R) 2\n2\u03b2PC [p0 + \u03b4 \u2212 1 + ln( 1\u2212 p0 \u03b4 )], (11)\nwhile it is\nJ\u2217P = (kN\u00b5\u2217P ) 2\n2\u03b2PC [p0 + \u03b4 \u2212 1 + ln( 1\u2212 \u03b4 p0 )], (12)\nfor the optimal punishing protocol.\nNote that for each update rule, the governing equation in the weak selection limit always has two equilibrium points, which are the full defection and full cooperation states, respectively. Accordingly, we can obtain the minimal amounts of incentive needed for the evolution of cooperation, as summarized in figure 2. Independently of the applied update rule, the optimal incentive protocols are time-invariant and equal both for reward and punishment. Hence, \u00b5\u2217v = \u00b5 \u2217 R = \u00b5 \u2217 P for each update rule. The optimal protocols are summarized in figure 2. We further present the cumulative cost for the optimal reward and punishment protocols for each update rule in figure 2. 2.2. Numerical calculations and computer simulations for optimal incentive protocols\nIn the following, we validate our analytical results for \u00b5\u2217v (v \u2208 {R,P}) by means of numerical calculations and computer simulations. Figure 3 illustrates the fraction of cooperators pC as a function of time t for the optimal \u00b5\u2217v and two other incentive schemes under four strategy update rule we considered. First, we note that our system always reaches the expected terminal state pC(tf ) = 0.99 when we launch the evolution from a random p0 = 0.5 state for each update rule. However, reaching this state requires significantly different costs for the incentive-providing institution, and this value is the lowest for \u00b5\u2217v, no matter whether we apply reward or punishment. Notably, the optimal incentive protocol does not produce the fastest relaxation.\nMoreover, we present the results of Monte Carlo simulations when positive or negative incentives are applied for each update rule (electronic supplementary material, figures S1-4). We can find that for different update rules the usage of optimal incentive protocol does not result in\n8 the fastest relaxation to the desired cooperation state, but it always leads to the smallest cumulative cost for the institution. We must stress that our findings are not limited to regular networks, but they remain valid for a broad range of interaction graphs from irregular random networks to small-world networks and scale-free networks. Our numerical calculations and simulations results coincide with the analytical predictions, and they illustrate that the cumulative cost is the lowest under the optimal incentive protocol. 2.3. Comparison between optimal reward and punishment protocols.\nThrough theoretical analysis presented in electronic supplementary material, we can conclude that the execution of optimal punishing scheme requires a lower cumulative cost, compared with the optimal rewarding scheme for each update rule, if the initial cooperation level is larger than the difference between the full and desired cooperation states. Otherwise, the usage of optimal rewarding scheme requires a lower cumulative cost. In order to verify such theoretical prediction and have an intuitive comparison, we show the cumulative cost values induced by the optimal punishing and rewarding protocols when the initial fraction p0 of cooperators is adjustable. By assuming the optimal protocols of incentives, the requested cumulative cost can be determined by numerically integrating or by using Monte Carlo simulations.\nOur numerical results are summarized in figure 4 for each update rule. From the top row of figure 4, we observe that when the initial cooperation level is larger than the difference between the full cooperation and desired terminal states (with a set tolerance for defection, \u03b4), the institution needs to spend less cumulative cost to reach the expected cooperation state by means of punishing. On the contrary, from the bottom row of figure 4, we see that when the initial cooperation level is less than the difference between the full cooperation and desired terminal states, the institution needs to spend less cumulative cost to reach the expected cooperation state by means of rewarding. However, in these different cases no matter whether the optimal rewarding or punishing protocol is applied, the corresponding cumulative cost value decreases as the initial cooperation level p0 increases. In addition, our simulations results presented in figure S5 of electronic supplementary material also support our theoretical analysis, which provide a rigorous basis for the usage of incentives under different initial conditions in the context of evolutionary prisoner\u2019s dilemmas game in structured populations."
        },
        {
            "heading": "3. Discussion",
            "text": "In human society, prosocial incentives are an essential means of avoiding the \u201ctragedy of the\n9 commons\u201d [21, 22, 40]. For incentive-providing institutions, however, the choice of incentives is mainly based on two aspects. One of them is knowing how much incentive is needed to promote the evolution of cooperation (potentially under different update rules) in structured populations. The other is whether the applied incentive scheme is an optimal time-dependent protocol requiring the minimal cost for the institution.\nIn order to investigate the above-mentioned tasks, we establish a game-theoretical framework. Namely, we consider the positive or negative incentive into the networked prisoner\u2019s dilemma game with four different strategy update rules, respectively. For a given update rule, we obtain the theoretical conditions of the minimal amounts of incentives needed for the evolution of cooperation. By establishing an index function for quantifying the executing cost, we derive the optimal positive and negative incentive protocols for each strategy update rule, respectively, by means of the approach of HJB equation. We find that these optimal incentive protocols are time-invariant for all the considered update rules. In addition, the optimal incentive protocols are identical both for negative and positive incentives. However, applying the punishing scheme requires a lower cumulative cost for the incentive-providing institution when the initial cooperation level is relatively high; otherwise, applying the rewarding scheme is cheaper. We further perform computer simulations, which confirm that our results are valid in different types of interaction topologies described by regular, random, small-world, and scale-free networks and thus demonstrate the general robustness of the findings.\nIn this work, we have quantified how much incentive is needed for the evolution of cooperation under each update rule we considered. However, when prosocial incentives are provided, the game structure may be changed. In particular, when the incentive amount \u00b5v > c, the prisoner\u2019s dilemma game will be transformed into the harmony game, where cooperators dominate defectors naturally [9]. Interestingly, we note that if \u00b5v > c\u2212b/k under DB updating and if \u00b5v > c\u2212b/(k+2) under IM updating, cooperation is favored. This implies that under DB and IM rule, the \u00b5v value can be smaller than the cost c, which guarantees that the game structure is not changed. For BD and PC updating, however, the incentive amount needed for the evolution of cooperation must completely outweigh the cost of cooperation. Hence the effectiveness of interventions might shed some light on which update type can best capture population behaviors.\nHere, we stress that the obtained optimal incentive protocols are time-invariant by solving the optimal control problems we formulated. More strikingly, the obtained optimal negative incentive protocol for each update rule is the same as the optimal positive incentive protocol. Thus, these\n10\noptimal incentive protocols are state-independent. Then the institution does not need to monitor the population state from time to time for optimal incentive implementation and can save monitoring costs since monitoring is generally costly. In addition, the optimal incentive level for DB and IM updating can guarantee that the dilemma faced by individuals is still a prisoner\u2019s dilemma. Accordingly, our work reflects that the incentive-based control protocols we obtained under DB and IM updating are simple and effective for promoting the evolution of cooperation in structured populations.\nIncentives can be used as controlling tools to regulate the decision-making behaviors of individuals [28]. However, there are significant preference differences in the usage of punishment and reward for the evolution of cooperation from the perspectives of individuals and incentiveproviding institutions [41]. It has been suggested that punishment is often not preferred since the usage of punishment leads to a low total income or a low average payoff in repeated games [23, 24]. In contrast, incentive-providing institutions prefer to use punishments more frequently. This is because punishment incurs a lower cost of implementing incentives for the promotion of cooperation [30, 41]. Here, we consider the top-down-like incentive mechanism under which cooperators can be rewarded or defectors can be punished directly by the external centralized institution which has existed and works stably. In this framework, we strictly compare the rewarding and punishing schemes concerning implementation cost and show how the best choice depends on the initial cooperation level, providing a rigorous basis for the usage of incentives in the context of the prisoner\u2019s dilemma game and where it might apply in human society.\nExtensions of our work are plentiful, especially in analytical terms. We would like to point out that we use the pair approximation approach to obtain our analytical results and this approach is used mainly for regular networks [15]. Indeed, this approach can be extended for other types of complex networks when the properties of these networks are additionally considered [42, 43]. Along this line, hence it is worth analytically investigating the low-cost incentive policy for the evolution of cooperation by fully considering spatial properties of the underlying population structures. Furthermore, our theoretical analysis could be extended by resorting to methods relying on calculating the coalescence times of random walks [11]. These methods might be particularly constructive when interventions at the topology level are at stake since they create a tighter link between the outcome and the network. In addition, we obtain our theoretical results in the limit of weak selection. Numerical analysis in other contexts has shown that due to relevant factors from the environment, the intensity of selection can change the game dynamics no matter the population\n11\nis well-mixed or structured, and plays a crucial role in the determination of cost-efficient institutional incentive [44\u201347]. Hence, a natural question arising here is whether our theoretical results are still valid when selection is not weak [48]. Indeed, analytical calculations for strong selection remain tractable for some structures with high symmetry. Thus, there is potential to identify the theoretical conditions of how much incentive is needed to promote cooperation and to explore the optimal incentive protocols for these population structures.\nIn addition, our work focuses on minimizing the incentive costs up to a set level of cooperation in a population, irrespectively of how long that takes. Further research is required for situations in which the rate at which the transition happens can be important and a time-varying protocol might be the solution. We consider our research in the framework of the prisoner\u2019s dilemma game, which is a paradigm for studying the evolution of cooperation. There are other prototypical twoperson dilemmas, e.g., snowdrift game [49] and stag-hunt game [50]. A promising extension of this work is to consider these mentioned games for future study as well as group interactions or higher-order interactions [51\u201355]. Furthermore, we design the cost-efficient incentive protocols by considering the global information (i.e., the fraction of cooperators in the whole population), but the local neighborhood properties on a structured network, such as how many cooperators are there in a neighbourhood, affect the final evolutionary outcomes [56\u201358]. Hence, it would be important to take into account these local information for optimal incentive protocols with minimal cost in the future work. Finally, other prosocial behaviours, such as honesty [59] or trust and trustworthiness [60], are also fundamental for cooperation, and hence it is a meaningful extension to study the optimization problems of incentives for promoting the evolution of these behaviors."
        },
        {
            "heading": "4. Model and Methods",
            "text": ""
        },
        {
            "heading": "4.1. Prisoner\u2019s dilemma game",
            "text": "We consider that a population of individuals are distributed on the nodes of an interaction graph. At each round, each individual plays the evolutionary prisoner\u2019s dilemma game with its neighbors and can choose to cooperate (C) or defect (D). We consider the payoff matrix for the game as\n C D C b\u2212 c \u2212c D b 0 , (13)\n12\nwhere b represents the benefit of cooperation and c (0 < c < b) represents the cost of cooperation. After engaging in the pairwise interactions with all the adjacent neighbors, each individual collects its payoff based on the payoff matrix. 4.2. Institutional incentives\nFurthermore, prosocial incentives provided by incentive-providing institutions can be used to reward cooperators or punish defectors after they play the game with their neighbors. Here, we consider both types of incentives, i.e., positive and negative incentives, respectively. If positive incentives are used, then a cooperator in the game is rewarded with a \u00b5R amount received from a central institution when interacting with a neighbor [24, 28]. If negative incentives are used, then a defector is fined by a \u00b5P amount for each interaction [23]. Consequently, when positive incentives are used, the modified payoff matrix becomes\n C D\nC b\u2212 c+ \u00b5R \u2212c+ \u00b5R D b 0 , (14) and when negative incentives are used, the modified payoff matrix becomes\n C D C b\u2212 c \u2212c D b\u2212 \u00b5P \u2212\u00b5P . (15) As a result, based on the above payoff matrices each individual collects its total payoff, which is derived from the pairwise interactions with neighbors and the incentive-providing institution. 4.3. Strategy update rules\nAccording to the evolutionary selection principle, players update their strategies from time to time, but the way how to do it may influence the evolutionary outcome significantly [9, 15]. In agreement with previous works [15, 16], we here consider four major strategy update rules, describing DB, BD, IM, and PC updating. Specifically, for DB updating, at each time step a random individual from the entire population is chosen to die; subsequently the neighbors compete for the empty site with probability proportional to their fitness. For BD updating, at each time step an individual is chosen for reproduction from the entire population with probability proportional to fitness; the offspring of this individual replaces a randomly selected neighbor. For IM updating, at each time step a random individual from the entire population is chosen to update its strategy; it will either stay with its own strategy or imitate one of the neighbors\u2019 strategies with probability\n13\nproportional to their fitness. For PC updating, at each time step a random individual is chosen to update its strategy, and it compares its own fitness with a randomly chosen neighbor. The focal individual either keeps its current strategy or adopts the neighbor\u2019s strategy with a probability that depends on the fitness difference. 4.4. Optimazing incentives\nSince providing incentives is costly for institutions, it has a paramount importance to find the optimal \u00b5\u2217v, which requires the minimal effort but is still capable of supporting cooperation effectively. To reach this goal, we first establish an index function for quantifying the executing cumulative cost, which is expressed as\nJv = \u222b tf t0 (kNpi\u00b5v) 2 2 dt, (16)\nwhere pi = pC if v = R which means that positive incentives are applied, otherwise pi = pD which means that negative incentives are applied. Here t0 is the initial time and is set to 0 in this work, and tf the terminal time for the system. Based on the above description, we then explore the optimal incentive protocol during the evolutionary period between 0 and tf by using optimal control theory [37\u201339]. It is a crucial assumption that tf is not fixed, but we monitor the evolution until the fraction of cooperators reaches the target level pC(tf ) at tf . Here, we suppose that pC(tf ) = 1\u2212 \u03b4 > p0, where p0 (p0 > 0) is the initial cooperation level and \u03b4 is the parameter determining the expected cooperation level at tf , satisfying 0 \u2264 \u03b4 < 1\u2212 p0.\nAccordingly, we formulate the optimal control problem for reward or punishment given as\nmin Jv = \u222b tf 0 (kNpi\u00b5v) 2 2 dt,\ns.t.\n dpC dt = Fj(pC , \u00b5v, t), j = DB, BD, IM, or PC, pC(0) = p0,\npC(tf ) = 1\u2212 \u03b4.\n(17)\nHere the cost function Jv characterizes the cumulative cost on average during the period [0, tf ] for the dynamical system to reach the terminal state 1\u2212\u03b4 from the initial state p0. Thus the quantity min Jv can work as the objective of calculating the optimal incentive protocol \u00b5\u2217v with the minimal executing cost. The details of solving the optimal control problems can be found in sections 1-4 of electronic supplementary material. 4.5. Monte Carlo simulations\n14\nDuring a full Monte Carlo step, on average each player has a chance to update its strategy. The applied four different update rules are specified above. Besides, we have tested alternative interaction topologies, including regular networks generated by using a two-dimensional square lattice of size N = L \u00d7 L [4] and scale-free networks obtained by using preferential-attachment model [7] starting from m0 = 6 where at every time step each new node is connected to m = 2 existing nodes fulfilling the standard power-law distribution. Alternatively, Erdo\u030bs-Re\u0301nyi random graph model [5] and small-world networks of Watts-Strogatz model with p = 0.1 rewiring parameter [6] are considered. These simulation results are summarized in figures S1-5 of electronic supplementary material.\nAuthor contributions X.C., S.W., V.V.V., and A.S. designed the research, S.W. and Z.X. performed the research, X.C., V.V.V., A.S., and S.W. wrote the manuscript, and all authors discussed the results and commented on and improved the manuscript.\nData Accessibility This article has no additional data.\nCompeting financial interests The authors declare no competing financial interests.\nEthics This article does not present research with ethical considerations.\nFunding This research was supported by the National Natural Science Foundation of China (Grant Nos. 61976048 and 62036002) and the Fundamental Research Funds of the Central Universities of China. S.W. acknowledges the support from China Scholarship Council (Grant No. 202006070122). A.S. was supported by the National Research, Development and Innovation Office (NKFIH) under Grant No. K142948. V.V.V. acknowledges funding from the Computational Science Lab - Informatics Institute of the University of Amsterdam.\n15"
        },
        {
            "heading": "1. DB Updating",
            "text": ""
        },
        {
            "heading": "1.1. Positive Incentive",
            "text": "Population structure is represented by a regular network of N nodes with degree k > 2. The vertices of network correspond to individuals and the edges represent who interacts with whom. Each individual plays the Prisoner\u2019s Dilemma game with its neighbors, who can either cooperate (C) or defect (D). Here, we introduce some notations. Let pi denote the proportion of individuals with strategy i, let pij denote the proportion of ij\u2013pairs, and finally let qi|j denote the conditional probability of finding an i\u2013individual given that the neighboring node is a j\u2013individual, where i, j \u2208 {C,D}. By using these notations, we have that pC + pD = 1, qC|i + qD|i = 1, pij = qi|jpj , and pCD = pDC .\nWe first consider the positive incentive into the networked Prisoner\u2019s Dilemma game with DB updating [15, 16]. According to DB updating, we randomly select a focal individual to die with probability pi (i \u2208 {C,D}), where i represents the strategy of the focal individual. Let kC and kD denote the numbers of cooperators and defectors among its k neighbors with kD + kC = k. If the\n25\nfocal individual adopts strategy D, then the fitness of a C\u2013neighbor is\nfC = 1\u2212 \u03c9 + \u03c9{(b\u2212 c+ \u00b5R)(k \u2212 1)qC|C + (\u00b5R \u2212 c)[(k \u2212 1)qD|C + 1]}, (S1)\nand the fitness of a D\u2013neighbor is\nfD = 1\u2212 \u03c9 + \u03c9[b(k \u2212 1)qC|D], (S2)\nwhere 0 \u2264 \u03c9 \u2264 1 measures the strength of selection.\nSince all the neighbors of the focal individual compete for the empty site with probability proportional to their fitness, the probability that a C\u2013neighbor replaces this empty site is given by\n\u0393 = kCfC\nkCfC + kDfD . (S3)\nBased on the above equations, pC increases by 1/N with probability\nP (\u2206pC = 1\nN ) = pD k\u2211 kC=0 ( k kC ) (qC|D) kC (qD|D) kD\u0393, (S4)\nand the number of CC-pairs increases by kC and hence pCC increases by kC/(kN/2) with probability\nP (\u2206pCC = 2kC kN ) = pD\n( k\nkC\n) (qC|D) kC (qD|D) kD\u0393. (S5)\nFurthermore, we consider another case, i.e., the randomly selected focal individual adopts strat-\negy C. In this case, the fitness of a C\u2013neighbor is\nfC = 1\u2212 \u03c9 + \u03c9{(b\u2212 c+ \u00b5R)[(k \u2212 1)qC|C + 1] + (\u00b5R \u2212 c)(k \u2212 1)qD|C}, (S6)\nand the fitness of a D\u2013neighbor is\nfD = 1\u2212 \u03c9 + \u03c9{b[(k \u2212 1)qC|D + 1] + 0 \u00b7 (k \u2212 1)qD|D}. (S7)\nThe probability that a D\u2013neighbor replaces the empty site is given by\nM = kDfD\nkCfC + kDfD . (S8)\nTherefore, pC decreases by 1/N with probability\nP (\u2206pC = \u2212 1\nN ) = pC k\u2211 kC=0 ( k kC ) (qC|D) kC (qD|D) kDM, (S9)\n26\nand the number of CC\u2013pairs decreases by kC and hence pCC decreases by kC/(kN/2) with probability\nP (\u2206pCC = \u2212 2kC kN ) = pC\n( k\nkC\n) (qC|D) kC (qD|D) kDM. (S10)\nWe suppose that one replacement event occurs in one unit of time, and the derivative of pC can be written as dpC dt = E(\u2206pC) \u2206t = 1 N P (\u2206pC = 1 N )\u2212 1 N P (\u2206pC = \u2212 1N )\n1 N\n= \u03c9 k \u2212 1 k\npCD[(\u00b5R \u2212 c) + (k \u2212 1)\u03b71](qC|C + qD|D) + o(\u03c92), (S11)\nin which \u03b71 = (b \u2212 c + \u00b5R)qC|C + (\u00b5R \u2212 c)qD|C \u2212 bqC|D. Accordingly, the derivative of pCC is given by\ndpCC dt = E(\u2206pCC) \u2206t =\n\u2211k kC=0 2kC kN P (\u2206pCC = 2kC kN )\u2212 \u2211k kC=0 2kC kN P (\u2206pCC = \u22122kCkN )\n1 N\n= 2pCD k\n[1 + (k \u2212 1)(qC|D \u2212 qC|C)] + o(\u03c9). (S12)\nDue to qC|C = pCCpC , we have\ndqC|C dt = d dt ( pCC pC ) = 2pCD kpC [1 + (k \u2212 1)(qC|D \u2212 qC|C)] + o(\u03c9). (S13)\nOther variables, such as pD and qD|C , can also be expressed by pC and qC|C through appropriate calculation, and then the dynamical system can be described by pC and qC|C . Rewriting the right-hand expressions of Eqs. (S11) and (S13) as functions of pC and qC|C yields the dynamical equation given by  dpCdt = \u03c9\u03a8RDB(pC , qC|C) + o(\u03c92),dqC|C dt = \u03a6RDB(pC , qC|C) + o(\u03c9), (S14) where  \u03a8RDB(pC , qC|C) = k\u22121k pCD[(\u2212c+ \u00b5R) + (k \u2212 1)\u03b71](qC|C + qD|D),\u03a6RDB(pC , qC|C) = 2pCDkpC [1 + (k \u2212 1)(qC|D \u2212 qC|C)]. (S15) Under weak selection (w 1), the velocity of qC|C can be large, and it may rapidly converge to the root defined by \u03a6RDB(pC , qC|C) = 0 as time t\u2192 +\u221e. Thus, we get\nqC|C = pC + 1\nk \u2212 1 (1\u2212 pC). (S16)\nAccordingly, the dynamical equation described by Eq. (S14) becomes\ndpC dt = \u03c9(k \u2212 2)[b+ k(\u00b5R \u2212 c)] k \u2212 1 pC(1\u2212 pC) + o(\u03c92), (S17)\n27\nwhich has two fixed points pC = 0 and pC = 1. We define the function FDB(pC , \u00b5R, t) as\nFDB(pC , \u00b5R, t) = \u03c9(k \u2212 2)[b+ k(\u00b5R \u2212 c)]\nk \u2212 1 pC(1\u2212 pC) + o(\u03c92). (S18)\nThis function is a continuously differentiable function, and the derivative of FDB(pC , \u00b5R, t) with respect to pC is dFDB dpC = \u03c9(k \u2212 2)[b+ k(\u00b5R \u2212 c)] k \u2212 1 (1\u2212 2pC) + o(\u03c92). (S19) For \u00b5R > c\u2212 bk , we have dFDB dpC |pC=1 = \u2212 \u03c9(k\u22122)[b+k(\u00b5R\u2212c)] k\u22121 < 0 and dFDB dpC |pC=0 = \u03c9(k\u22122)[b+k(\u00b5R\u2212c)] k\u22121 > 0. This means that the fixed point pC = 1 is stable and pC = 0 unstable, i.e., cooperators prevail over defectors.\nLastly, we then study the special case of \u00b5R = 0. In this case, we can see that for b/c > k, the fixed point pC = 1 is stable and pC = 0 unstable. Thus, we obtain the condition b/c > k for the evolution of cooperation as previously obtained in Refs. [15, 16]."
        },
        {
            "heading": "1.2. Negative Incentive",
            "text": "In this subsection, we then consider the negative incentive into the networked Prisoner\u2019s Dilemma game with DB updating, and the payoff matrix is given by Eq. (15) in the main text. According to DB updating, if the focal individual adopts strategy D, then the fitness of a C\u2013neighbor is\nfC = 1\u2212 \u03c9 + \u03c9{(b\u2212 c)(k \u2212 1)qC|C \u2212 c[(k \u2212 1)qD|C + 1]}, (S20)\nand the fitness of a D\u2013neighbor is\nfD = 1\u2212 \u03c9 + \u03c9{(b\u2212 \u00b5P )(k \u2212 1)qC|D \u2212 \u00b5P [(k \u2212 1)qD|D + 1]}. (S21)\nThe probability that a C\u2013neighbor replaces the empty site is given by the expression \u0393 in Eq. (S3). Therefore, pC increases by 1/N with probability\nP (\u2206pC = 1\nN ) = pD k\u2211 kC=0 ( k kC ) (qC|D) kC (qD|D) kD\u0393. (S22)\nAccordingly, the number of CC\u2013pairs increases by kC and hence pCC increases by kC/(kN/2) with probability\nP (\u2206pCC = 2kC kN ) = pD\n( k\nkC\n) (qC|D) kC (qD|D) kD\u0393. (S23)\nIn addition, we consider another case where the focal individual adopts strategyD. In this case,\nthe fitness of a C\u2013neighbor is\nfC = 1\u2212 \u03c9 + \u03c9{(b\u2212 c)[(k \u2212 1)qC|C + 1]\u2212 c(k \u2212 1)qD|C}, (S24)\n28\nand the fitness of a D\u2013neighbor is\nfD = 1\u2212 \u03c9 + \u03c9{(b\u2212 \u00b5P )[(k \u2212 1)qC|D + 1]\u2212 \u00b5P (k \u2212 1)qD|D}. (S25)\nThe probability that a D\u2013neighbor replaces the empty site can be also given by the expression M in Eq. (S8). Thus, pC decreases by 1/N with probability\nP (\u2206pC = \u2212 1\nN ) = pC k\u2211 kC=0 ( k kC ) (qC|D) kC (qD|D) kDM. (S26)\nAccordingly, the number of CC\u2013pairs decreases by kC and pCC decreases by kC/(kN/2) with probability\nP (\u2206pCC = \u2212 2kC kN ) = pC\n( k\nkC\n) (qC|D) kC (qD|D) kDM. (S27)\nBased on these calculations, we obtain the time derivative of pC given by\ndpC dt = E(\u2206pC) \u2206t =\n1 N P (\u2206pC = 1 N )\u2212 1 N P (\u2206pC = \u2212 1N )\n1 N\n= \u03c9(k \u2212 1)\nk pCD[\u00b5P \u2212 c+ (k \u2212 1)\u03b72](qC|C + qD|D) + o(\u03c92),\n(S28)\nin which \u03b72 = (b\u2212 c)qC|C \u2212 cqD|C + (\u00b5P \u2212 b)qC|D + \u00b5P qD|D. And the time derivative of pCC is given by\ndpCC dt = E(\u2206pCC) \u2206t =\n\u2211k kC=0 2kC kN P (\u2206pCC = 2kC kN )\u2212 \u2211k kC=0 2kC kN P (\u2206pCC = \u22122kCkN )\n1 N\n= 2pCD k\n[1 + (k \u2212 1)(qC|D \u2212 qC|C)] + o(\u03c9). (S29)\nFurthermore, we have\ndqC|C dt = 2pCD kpC [1 + (k \u2212 1)(qC|D \u2212 qC|C)] + o(\u03c9). (S30)\nHence, the dynamical equation can be described by dpCdt = \u03c9\u03a8PDB(pC , qC|C) + o(\u03c92),dqC|C dt = \u03a6PDB(pC , qC|C) + o(\u03c9), (S31) where  \u03a8PDB(pC , qC|C) = k\u22121k pCD[\u00b5P \u2212 c+ (k \u2212 1)\u03b72](qC|C + qD|D),\u03a6PDB(pC , qC|C) = 2pCDkpC [1 + (k \u2212 1)(qC|D \u2212 qC|C)].\n29\nUnder weak selection, the velocity of qC|C can be large, and it may rapidly converge to the root defined by \u03a6PDB(pC , qC|C) = 0 as time t\u2192 +\u221e. Thus, we get\nqC|C = pC + 1\nk \u2212 1 (1\u2212 pC). (S32)\nCorrespondingly, the dynamical equation described by Eq. (S31) becomes\ndpC dt = \u03c9(k \u2212 2)[b+ k(\u00b5P \u2212 c)] k \u2212 1 pC(1\u2212 pC) + o(\u03c92), (S33)\nwhich has two fixed points pC = 0 and pC = 1. We define the function FDB(pC , \u00b5P , t) as\nFDB(pC , \u00b5P , t) = \u03c9(k \u2212 2)[b+ k(\u00b5P \u2212 c)]\nk \u2212 1 pC(1\u2212 pC) + o(\u03c92), (S34)\nand the derivative of FDB(pC , \u00b5P , t) with respect to pC is\ndFDB dpC = \u03c9(k \u2212 2)[b+ k(\u00b5P \u2212 c)] k \u2212 1 (1\u2212 2pC) + o(\u03c92). (S35)\nHence, for \u00b5P > c \u2212 bk we have dFDB dpC |pC=1 = \u2212 \u03c9(k\u22122)[b+k(\u00b5P\u2212c)] k\u22121 < 0 and dFDB dpC |pC=0 = \u03c9(k\u22122)[b+k(\u00b5P\u2212c)] k\u22121 > 0, which means that the fixed point pC = 1 is stable and pC = 0 unstable, i.e., cooperators prevail over defectors. Particularly, when \u00b5P = 0, we can see that for b/c > k, the fixed point pC = 1 is stable and pC = 0 unstable. Thus, we obtain the condition b/c > k for the evolution of cooperation as obtained in Refs. [15, 16]."
        },
        {
            "heading": "1.3. Optimal Incentive Protocols",
            "text": "In subsections 1.1 and 1.2, we have theoretically derived the dynamical system with positive or negative incentive by means of the pair approximation method in the limit of weak selection, which is given by\ndpC dt = FDB(pC , \u00b5v, t) = \u03c9(k \u2212 2)[b+ k(\u00b5v \u2212 c)] k \u2212 1 pC(1\u2212 pC) + o(\u03c92). (S36)\nAs noted, this dynamical system has two equilibria which are pC = 0 and pC = 1. If \u00b5v > c\u2212 bk , the former is unstable and the latter is stable, which means that cooperation will be promoted in the long run. Since providing incentive is costly, our principal goal is to explore the optimal incentive protocol that is still able not only to promote cooperation, but also requires a minimal cost. To do that, we now solve the formulated optimal control problems for DB updating.\nFirst, we solve the optimal control problem for rewarding. The Hamiltonian function\nHDB(pC , \u00b5R, t) is defined as\nHDB(pC , \u00b5R, t) = (kNpC\u00b5R)\n2 2 + \u2202J\u2217R \u2202pC FDB(pC , \u00b5R, t), (S37)\n30\nwhere J\u2217R is the optimal cost function of pC and t for the optimal rewarding protocol, given as\nJ\u2217R = \u222b tf 0 (kNpC\u00b5 \u2217 R) 2 2 dt. (S38)\nBy solving \u2202HDB \u2202\u00b5R = 0, we know that the optimal rewarding protocol \u00b5\u2217R should satisfy\n\u00b5\u2217R = \u2212 \u03c9(k \u2212 2)(1\u2212 pC) N2k(k \u2212 1)pC \u2202J\u2217R \u2202pC . (S39)\nGenerally, we should solve the canonical equations of Eq. (S37) to obtain the optimal rewarding protocol [37\u201339]. Yet, the obtained dynamical systems are nonlinear which greatly increases the complexity of obtaining the exact expression of the optimal protocols by a direct calculation. Instead, to solve the optimal control problem we use the dynamic programming method, HJB equation for continuous-time systems [37\u201339]. This equation can be written as\n\u2212\u2202J \u2217 R\n\u2202t = HDB(pC , \u00b5\n\u2217 R, t). (S40)\nBy substituting Eq. (S39) into the above HJB equation, we have\n\u2212\u2202J \u2217 R\n\u2202t =\n(kNpC\u00b5 \u2217 R) 2 2 + \u03c9(k \u2212 2)[b+ k(\u00b5\u2217R \u2212 c)] k \u2212 1 pC(1\u2212 pC) \u2202J\u2217R \u2202pC . (S41)\nSince we assume that the terminal time tf is not fixed, the optimal cost function J\u2217R(pC , t) is independent of t. Consequently, we have\n\u2202J\u2217R \u2202t = 0. (S42)\nWe then yield \u2202J\u2217R \u2202pC = 0 or \u2202J\u2217R \u2202pC = 2N2(k \u2212 1)(b\u2212 ck)pC \u03c9(k \u2212 2)(1\u2212 pC) . (S43) As \u00b5R > 0 and pC \u2208 (0, 1), we have \u2202J\u2217R \u2202pC < 0. (S44) From Eq. (S44), we can see that this inequality is obviously satisfied for b/c \u2265 k. Instead, we consider the case, i.e., b/c < k, and hence only \u2202J \u2217 R\n\u2202pC = 2N 2(k\u22121)(b\u2212ck)pC \u03c9(k\u22122)(1\u2212pC) holds. By substituting this\nequation into Eq. (S39), we obtain the optimal rewarding level as\n\u00b5\u2217R = 2(ck \u2212 b)\nk . (S45)\nWith this \u00b5\u2217R the dynamical equation thus becomes\ndpC dt = \u03c9(k \u2212 2)(ck \u2212 b) k \u2212 1 pC(1\u2212 pC), (S46)\n31\nwhere the initial fraction of cooperators in the population is denoted by p0 = pC(0). The solution of this equation is\npC = 1\n1 + 1\u2212p0 p0 e\u2212\u03b2DBt\n, (S47)\nwhere \u03b2DB = \u03c9(k\u22122)(ck\u2212b)\nk\u22121 . It also means that the dynamical system needs infinitely long time to\nreach the full cooperation state from the initial p0 < 1. To avoid it, we suppose that the terminal state pC(tf ) is 1 \u2212 \u03b4, where \u03b4 is the parameter determining the cooperation level at the terminal time. Due to b/c < k, pC increases monotonically over time t, which leads to pC(tf ) > p0.\nFurthermore, the cumulative cost required by the optimal rewarding level \u00b5\u2217R for the dynamical\nsystem to reach the expected terminal state pC(tf ) becomes\nJ\u2217R = (kN\u00b5\u2217R) 2\n2\u03b2DB [p0 \u2212 1 + \u03b4 + ln( 1\u2212 p0 \u03b4 )]. (S48)\nThe optimal control problem for punishment can be solved similarly and for the \u00b5\u2217P optimal\nlevel we have\n\u00b5\u2217P = 2\nk (ck \u2212 b) (S49)\nand\npC = 1\n1 + 1\u2212p0 p0 e\u2212\u03b2DBt\n. (S50)\nHence, the cumulative cost produced by the optimal punishing protocol \u00b5\u2217P becomes\nJ\u2217P = (kN\u00b5\u2217P ) 2\n2\u03b2DB [p0 \u2212 1 + \u03b4 + ln( 1\u2212 \u03b4 p0 )]. (S51)\nFrom these results we can conclude that the optimal levels of negative and positive incentives are identical, i.e., \u00b5\u2217R = \u00b5 \u2217 P , but their cumulative costs could be different. For a proper comparison we can calculate their difference, which is\nJ\u2217R \u2212 J\u2217P = (kN\u00b5\u2217v) 2 2\u03b2DB ln[ p0(1\u2212 p0) \u03b4(1\u2212 \u03b4) ] , (S52)\nwhere \u00b5\u2217v = \u00b5 \u2217 R = \u00b5 \u2217 P . In this work we assume that p0 > 0, since we do not consider behavioral mutations or errors of strategy updating. In addition, p0 < pC(tf ) = 1\u2212\u03b4 and we have p0 +\u03b4 < 1. Thus when \u03b4 < p0, we have J\u2217R > J \u2217 P , which means that for DB updating the optimal punishment always requires lower cumulative cost than the usage of optimal reward. But when \u03b4 > p0, we have J\u2217R < J \u2217 P , which means that for DB updating the optimal reward always requires lower cumulative cost than the usage of optimal punishment. This observations are supported by numerical calculations and Monte Carlo simulations as plotted in figure 4 and figure S5, respectively.\n32"
        },
        {
            "heading": "2. BD Updating",
            "text": ""
        },
        {
            "heading": "2.1. Positive Incentive",
            "text": "According to BD update rule [15, 16], we randomly choose a focal individual for reproduction proportional to fitness who has kC cooperators and kD defectors among its k neighbors. If the focal individual adopts strategy C, then the fitness of the focal individual is given by\nfC = 1\u2212 \u03c9 + \u03c9[(b\u2212 c)kC \u2212 ckD]. (S53)\nSince the offspring of the selected individual replaces one of its neighbors randomly, the probability that pC increases by 1/N is\nP (\u2206pC = 1\nN ) = pC k\u2211 kC=0 ( k kC ) (qC|C) kC (qD|C) kD fC f\u0304 kD k , (S54)\nwhere f\u0304 represents the average fitness of the whole population. In this case, the number of CC\u2013 pairs increases by (k\u22121)qC|D+1 and pCC increases by [(k\u22121)qC|D+1]/(kN/2) with probability\nP (\u2206pCC = (k \u2212 1)qC|D + 1\nkN/2 ) = pC k\u2211 kC=0 ( k kC ) (qC|C) kC (qD|C) kD fC f\u0304 kD k . (S55)\nIn the alternative case, the randomly selected focal individual adopts strategy D. Here the\nfitness of the focal individual is given by\nfD = 1\u2212 \u03c9 + \u03c9(bkC + 0 \u00b7 kD),\nand therefore pC decreases by 1/N with probability\nP (\u2206pC = \u2212 1\nN ) = pD k\u2211 kC=0 ( k kC ) (qC|D) kC (qD|D) kD fD f\u0304 kC k . (S56)\nConsequently, the number of CC\u2013pairs decreases by (k \u2212 1)qC|C and pCC decreases by (k \u2212 1)qC|C/(kN/2) with probability\nP (\u2206pCC = \u2212 (k \u2212 1)qC|C kN/2 ) = pD k\u2211 kC=0 ( k kC ) (qC|D) kC (qD|D) kD fD f\u0304 kC k . (S57)\nHere, the average fitness of the whole population is thus denoted by\nf\u0304 = pC k\u2211 kC=0 ( k kC ) (qC|C) kC (qD|C) kDfC + pD k\u2211 kC=0 ( k kC ) (qC|D) kC (qD|D) kDfD\n= 1\u2212 \u03c9 + k\u03c9[(b\u2212 c+ \u00b5R)pCC + (\u00b5R \u2212 c)pCD + bpCD].\n(S58)\n33\nBased on these calculations, we respectively obtain the time derivatives of pC and pCC as\ndpC dt = E(\u2206pC) \u2206t =\n1 N P (\u2206pC = 1 N )\u2212 1 N P (\u2206pC = \u2212 1N )\n1 N\n= \u03c9pCD f\u0304 {(\u00b5R \u2212 c\u2212 b) + (k \u2212 1)[(b\u2212 c+ \u00b5R)qC|C + (\u00b5R \u2212 c)qD|C \u2212 bqC|D]}+ o(\u03c92)\n(S59)\nand\ndpCC dt = E(\u2206pCC) \u2206t =\n(k\u22121)qC|D+1 kN/2 P (\u2206pCC = (k\u22121)qC|D+1 kN/2 )\u2212 (k\u22121)qC|C kN/2 P (\u2206pCC = \u2212 (k\u22121)qC|C kN/2\n) 1 N\n= 2pCD k\n[(k \u2212 1)(qC|D \u2212 qC|C) + 1] + o(\u03c9). (S60)\nFurthermore, we have\ndqC|C dt = 2pCD kpC [(k \u2212 1)(qC|D \u2212 qC|C) + 1] + o(\u03c9). (S61)\nHence, the dynamical equation is described by dpCdt = \u03c9\u03a8RBD(pC , qC|C) + o(\u03c92),dqC|C dt = \u03a6RBD(pC , qC|C) + o(\u03c9), (S62) where \u03a8RBD(pC , qC|C) = pCDf\u0304 {(\u00b5R \u2212 c\u2212 b) + (k \u2212 1)[(b\u2212 c+ \u00b5R)qC|C + (\u00b5R \u2212 c)qD|C \u2212 bqC|D]},\u03a6RBD(pC , qC|C) = 2pCDkpC [(k \u2212 1)(qC|D \u2212 qC|C) + 1]. Under weak selection, the velocity of qC|C can be large, and it may rapidly converge to the root defined by \u03a6RBD(pC , qC|C) = 0 as time t\u2192 +\u221e. Thus, we get\nqC|C = pC + 1\nk \u2212 1 (1\u2212 pC). (S63)\nAccordingly, the dynamical equation described by Eq. (S62) becomes\ndpC dt = \u03c9k(k \u2212 2)(\u00b5R \u2212 c) k \u2212 1 pC(1\u2212 pC) + o(\u03c92), (S64)\nwhich has two fixed points pC = 0 and pC = 1. We define the function FBD(pC , \u00b5R, t) as\nFBD(pC , \u00b5R, t) = \u03c9k(k \u2212 2)(\u00b5R \u2212 c)\nk \u2212 1 pC(1\u2212 pC) + o(\u03c92), (S65)\nand the derivative of FBD(pC , \u00b5R, t) with respect to pC is\ndFBD dpC = \u03c9k(k \u2212 2)(\u00b5R \u2212 c) k \u2212 1 (1\u2212 2pC) + o(\u03c92). (S66)\n34\nFor \u00b5R > c, we have dFBDdpC |pC=1 = \u2212 \u03c9k(k\u22122)(\u00b5R\u2212c) k\u22121 < 0 and dFBD dpC |pC=0 = \u03c9k(k\u22122)(\u00b5R\u2212c) k\u22121 > 0. This implies that the fixed point pC = 1 is stable and pC = 0 unstable, i.e., cooperators prevail over defectors. Particularly, when \u00b5R = 0, we can see that the fixed point pC = 0 is always stable and pC = 1 unstable, which means that cooperation cannot emerge under BD update rule as obtained in Refs. [15, 16]."
        },
        {
            "heading": "2.2. Negative Incentive",
            "text": "In this subsection, we consider the negative incentive into the networked Prisoner\u2019s Dilemma game with BD updating. According to BD updating, a focal individual is randomly selected for reproduction who has kC cooperators and kD defectors among its k neighbors. Here, we first consider the focal individual adopts strategy C. Then, the fitness of the focal individual is given by\nfC = 1\u2212 \u03c9 + \u03c9[kC(b\u2212 c) + kD(\u2212c)], (S67)\nand therefore pC increases by 1/N with probability\nP (\u2206pC = 1\nN ) = pC k\u2211 kC=0 ( k kC ) (qC|C) kC (qD|C) kD fC f\u0304 kD k , (S68)\nwhere f\u0304 denotes the average fitness of the whole population. And the number of CC\u2013pairs increases by (k\u2212 1)qC|D + 1 and therefore pCC increases by [(k\u2212 1)qC|D + 1]/(kN/2) with probability\nP (\u2206pCC = (k \u2212 1)qC|D + 1\nkN/2 ) = pC k\u2211 kC=0 ( k kC ) (qC|C) kC (qD|C) kD fC f\u0304 kD k . (S69)\nIn addition, we consider another case, that is, the randomly selected focal individual adopts\nstrategy D. In this case, the fitness of the focal individual is given by\nfD = 1\u2212 \u03c9 + \u03c9[kC(b\u2212 \u00b5P ) + kD(\u2212\u00b5P )],\nand therefore pC decreases by 1/N with probability\nP (\u2206pC = \u2212 1\nN ) = pD k\u2211 kC=0 ( k kC ) (qC|D) kC (qD|D) kD fD f\u0304 kC k . (S70)\nAnd the number of CC\u2013pairs decreases by (k \u2212 1)qC|C and therefore pCC decreases by (k \u2212 1)qC|C/(kN/2) with probability\nP (\u2206pCC = \u2212 (k \u2212 1)qC|C kN/2 ) = pD k\u2211 kC=0 ( k kC ) (qC|D) kC (qD|D) kD fD f\u0304 kC k . (S71)\n35\nHere, the average fitness of whole population can be calculated by\nf\u0304 = pC k\u2211 kC=0 ( k kC ) (qC|C) kC (qD|C) kDfC + pD k\u2211 kC=0 ( k kC ) (qC|D) kC (qD|D) kDfD\n= 1\u2212 \u03c9 + \u03c9k[(b\u2212 c)pCC \u2212 cpCD + (b\u2212 \u00b5P )pCD \u2212 \u00b5PpDD].\n(S72)\nFrom these calculations, we respectively obtain the time derivatives of pC and pCC as\ndpC dt = E(\u2206pC) \u2206t =\n1 N P (\u2206pC = 1 N )\u2212 1 N P (\u2206pC = \u2212 1N )\n1 N\n= \u03c9pCD f\u0304 {\u00b5P \u2212 c\u2212 b+ (k \u2212 1)[(b\u2212 c)qC|C \u2212 cqD|C + (\u00b5P \u2212 b)qC|D + \u00b5P qD|D]}+ o(\u03c92),\n(S73)\nand\ndpCC dt = E(\u2206pCC) \u2206t =\n(k\u22121)qC|D+1 kN/2 P (\u2206pCC = (k\u22121)qC|D+1 kN/2 )\u2212 (k\u22121)qC|C kN/2 P (\u2206pCC = \u2212 (k\u22121)qC|C kN/2\n) 1 N\n= 2\nk pCD[(k \u2212 1)(qC|D \u2212 qC|C) + 1] + o(\u03c9).\n(S74)\nFurthermore, we have\ndqC|C dt = 2pCD kpC [(k \u2212 1)(qC|D \u2212 qC|C) + 1] + o(\u03c9). (S75)\nHence, the dynamical equation is described by dpCdt = \u03c9\u03a8PBD(pC , qC|C) + o(\u03c92),fracdqC|Cdt = \u03a6PBD(pC , qC|C) + o(\u03c9), (S76) where \u03a8PBD(pC , qC|C) = pCDf\u0304 {\u00b5P \u2212 c\u2212 b+ (k \u2212 1)[(b\u2212 c)qC|C \u2212 cqD|C + (\u00b5P \u2212 b)qC|D + \u00b5P qD|D]},\u03a6PBD(pC , qC|C) = 2pCDkpC [(k \u2212 1)(qC|D \u2212 qC|C) + 1]. Under weak selection, the velocity of qC|C can be large, and it may rapidly converge to the root defined by \u03a6PBD(pC , qC|C) = 0 as time t\u2192 +\u221e. Thus, we get\nqC|C = pC + 1\nk \u2212 1 (1\u2212 pC). (S77)\nAccordingly, the system described by Eq. (S76) becomes\ndpC dt = \u03c9k(k \u2212 2)(\u00b5P \u2212 c) k \u2212 1 pC(1\u2212 pC) + o(\u03c92), (S78)\n36\nwhich has two fixed points, pC = 0 and pC = 1. We define the function FBD(pC , \u00b5P , t) as\nFBD(pC , \u00b5P , t) = \u03c9k(k \u2212 2)(\u00b5P \u2212 c)\nk \u2212 1 pC(1\u2212 pC) + o(\u03c92), (S79)\nand the derivative of FBD(pC , \u00b5P , t) with respect to pC is\ndFBD dpC = \u03c9k(k \u2212 2)(\u00b5P \u2212 c) k \u2212 1 (1\u2212 2pC) + o(\u03c92). (S80)\nFor \u00b5P > c, we have dFBDdpC |pC=1 = \u2212 \u03c9k(k\u22122)(\u00b5P\u2212c) k\u22121 < 0 and dFBD dpC |pC=0 = \u03c9k(k\u22122)(\u00b5P\u2212c) k\u22121 > 0 which implies the fixed point pC = 1 is stable and pC = 0 is unstable, i.e., cooperators prevail over defectors. Particularly, when \u00b5P = 0, we can see that the fixed point pC = 0 is always stable and pC = 1 unstable, which means that cooperation can never emerge under BD update rule as obtained in Refs. [15, 16]."
        },
        {
            "heading": "2.3. Optimal Incentive Protocols",
            "text": "In subsections 2.1 and 2.2, we have theoretically obtained the dynamical equation with positive or negative incentive by means of the pair approximation approach in the limit of weak selection, which is given by\ndpC dt = FBD(pC , \u00b5v, t) = \u03c9k(k \u2212 2)(\u00b5v \u2212 c) k \u2212 1 pC(1\u2212 pC) + o(\u03c92) , (S81)\nhaving pC = 0 and pC = 1 fixed points. If \u00b5v > c, the former is unstable and the latter is stable, indicating that cooperation will be promoted in the long run. Furthermore, to explore the optimal rewarding and punishing protocols, we now employ the approach of HJB equation to solve the formulated optimal control problems for this BD updating.\nFirst we solve the optimal control problem for rewarding. We define the Hamiltonian function\nHBD(pC , \u00b5R, t) as\nHBD(pC , \u00b5R, t) = (kNpC\u00b5R)\n2 2 + \u2202J\u2217R \u2202pC FBD(pC , \u00b5R, t), (S82)\nwhere J\u2217R is the optimal cost function of pC and t for the optimal rewarding protocol given as\nJ\u2217R = \u222b tf 0 (kNpC\u00b5R) 2 2 dt. (S83)\nSolving \u2202HBD \u2202\u00b5R = 0, we know that the optimal rewarding protocol \u00b5\u2217R should satisfy\n\u00b5\u2217R = \u2212 \u03c9(k \u2212 2)(1\u2212 pC) k(k \u2212 1)N2pC \u2202J\u2217R \u2202pC . (S84)\n37\nThe corresponding HJB equation [37\u201339] for dynamical system with positive incentive can be written as\n\u2212\u2202J \u2217 R\n\u2202t = HBD(pC , \u00b5\n\u2217 R, t). (S85)\nSince we assume that the terminal time tf is not fixed, the optimal cost function J\u2217R(pC , t) is independent of t. Consequently, we have\n\u2202J\u2217R \u2202t = 0. (S86)\nWe then obtain \u2202J\u2217R \u2202pC = 0 or \u2202J\u2217R \u2202pC = \u2212 2N 2k(k \u2212 1)cpC \u03c9(k \u2212 2)(1\u2212 pC) . (S87) As \u00b5R > 0 and pC \u2208 (0, 1), we have \u2202J\u2217R \u2202pC < 0. (S88) Therefore only \u2202J \u2217 R\n\u2202pC = \u22122N 2k(k\u22121)cpC \u03c9(k\u22122)(1\u2212pC) holds. By substituting this equation into Eq. (S84), we obtain\nthe optimal rewarding protocol as\n\u00b5\u2217R = 2c. (S89)\nWith the optimal rewarding protocol \u00b5\u2217R, the dynamical equation thus becomes\ndpC dt = \u03c9k(k \u2212 2)c k \u2212 1 pC(1\u2212 pC), (S90)\nwhere the initial fraction of cooperators in the population is denoted by pC(0) = p0. Solving the above equation, we have\npC = 1\n1 + 1\u2212p0 p0 e\u2212\u03b2BDt\n, (S91)\nwhere \u03b2BD = \u03c9k(k\u22122)c k\u22121 . Hence, the cumulative cost produced by the optimal rewarding protocol is given by\nJ\u2217R = (kN\u00b5\u2217R) 2\n2\u03b2BD [p0 \u2212 1 + \u03b4 + ln( 1\u2212 p0 \u03b4 )]. (S92)\nIf we solve the optimal control problem for punishment, we obtain for the optimal protocol of\nnegative incentive\n\u00b5\u2217P = 2c, (S93)\nand\npC = 1\n1 + 1\u2212p0 p0 e\u2212\u03b2BDt\n. (S94)\n38\nAccordingly, the cumulative cost required by the optimal punishing protocol is\nJ\u2217P = (kN\u00b5\u2217P ) 2\n2\u03b2BD [p0 \u2212 1 + \u03b4 + ln( 1\u2212 \u03b4 p0 )]. (S95)\nTherefore the cumulative cost difference of optimal rewarding and punishing protocols is\nJ\u2217R \u2212 J\u2217P = (kN\u00b5\u2217v) 2 2\u03b2BD ln[ p0(1\u2212 p0) \u03b4(1\u2212 \u03b4) ], (S96)\nwhere \u00b5\u2217v = \u00b5 \u2217 R = \u00b5 \u2217 P . Similarly to the analysis of Eq. (S52) in subsection 1.3, we also find that J\u2217R > J \u2217 P when \u03b4 < p0 and J \u2217 R < J \u2217 P when \u03b4 > p0. This implies that for BD updating executing the optimal punishing protocol can induce a lower cumulative cost in comparison with the optimal rewarding one when \u03b4 < p0, and this conclusion is reversed when \u03b4 > p0, which has also been confirmed by numerical calculations and Monte Carlo simulations as presented in figure 4 and figure S5, respectively."
        },
        {
            "heading": "3. IM Updating",
            "text": ""
        },
        {
            "heading": "3.1. Positive Incentive",
            "text": "For IM updating [15, 16], a focal individual is randomly chosen to update its strategy who has kC cooperators and kD defectors among its k neighbors. If the focal individual adopts strategy D, then the fitness of a C\u2013neighbor is\nfC = 1\u2212 \u03c9 + \u03c9{(b\u2212 c+ \u00b5R)(k \u2212 1)qC|C + (\u00b5R \u2212 c)[(k \u2212 1)qD|C + 1]}, (S97)\nand the fitness of a D\u2013neighbor is\nfD = 1\u2212 \u03c9 + \u03c9{b(k \u2212 1)qC|D + 0 \u00b7 [(k \u2212 1)qD|D + 1]}. (S98)\nBesides, the fitness of the focal individual is\nf0 = 1\u2212 \u03c9 + \u03c9bkC . (S99)\nSince the focal individual can keep its own strategy or imitate a neighbor\u2019s strategy with probability proportional to the fitness, the probability that the focal individual adopts strategy C is given by\n\u0398 = kCfC\nkCfC + kDfD + f0 . (S100)\nTherefore, pC increases by 1/N with probability\nP (\u2206pC = 1\nN ) = pD k\u2211 kC=0 ( k kC ) (qC|D) kC (qD|D) kD\u0398. (S101)\n39\nConsequently, the number of CC\u2013pairs increases by kC and hence pCC increases by kC/(kN/2) with probability\nP (\u2206pCC = 2kC kN ) = pD\n( k\nkC\n) (qC|D) kC (qD|D) kD\u0398. (S102)\nIn the alternative case, the randomly selected focal individual adopts strategy C. Here the\nfitness of a C\u2013neighbor is\nfC = 1\u2212 \u03c9 + \u03c9{(b\u2212 c+ \u00b5R)[(k \u2212 1)qC|C + 1] + (\u00b5R \u2212 c)(k \u2212 1)qD|C}, (S103)\nand the fitness of a D\u2013neighbor is\nfD = 1\u2212 \u03c9 + \u03c9[(k \u2212 1)qC|D + 1]b. (S104)\nBesides, the fitness of the focal individual is\nf0 = 1\u2212 \u03c9 + \u03c9[(b\u2212 c+ \u00b5R)kC + (\u00b5R \u2212 c)kD]. (S105)\nThe probability that the focal individual adopts the strategy D is\n\u03a5 = kDfD\nkCfC + kDfD + f0 . (S106)\nThus, pC decreases by 1/N with probability\nP (\u2206pC = \u2212 1\nN ) = pC k\u2211 kC=0 ( k kC ) (qC|C) kC (qD|C) kD\u03a5. (S107)\nTherefore the number of CC\u2013pairs decreases by kC and hence pCC decreases by kC/(kN/2) with probability\nP (\u2206pCC = \u2212 2kC kN ) = pC\n( k\nkC\n) (qC|C) kC (qD|C) kD\u03a5. (S108)\nBased on these calculations, the time derivative of pC is given by\ndpC dt = E(\u2206pC) \u2206t =\n1 N P (\u2206pC = 1 N )\u2212 1 N P (\u2206pC = \u2212 1N )\n1 N\n= \u03c9kpCD (k + 1)2 [2(\u00b5R \u2212 c\u2212 b) + 2(k \u2212 1)\u03be1 + (k \u2212 1)(\u00b5R \u2212 c)(qC|C + qD|D) + (k + 1) 2qC|C + qD|D\u03be1] + o(\u03c9 2),\n(S109)\nwhere \u03be1 = (b\u2212 c+ \u00b5R)qC|C + (\u00b5R \u2212 c)qD|C \u2212 bqC|D. Accordingly, the time derivative of pCC is given by\ndpCC dt = E(\u2206pCC) \u2206t =\n\u2211k kC=0 2kC kN P (\u2206pCC = 2kC kN )\u2212 \u2211k kC=0 2kC kN P (\u2206pCC = \u22122kCkN )\n1 N\n= 2pCD k + 1\n[1 + (k \u2212 1)(qC|D \u2212 qC|C)] + o(\u03c9). (S110)\n40\nFurthermore, we have\ndqC|C dt = 2pCD (k + 1)pC [1 + (k \u2212 1)(qC|D \u2212 qC|C)] + o(\u03c9). (S111)\nHence, the dynamical equation is described by dpCdt = \u03c9\u03a8RIM(pC , qC|C) + o(\u03c92),dqC|C dt = \u03a6RIM(pC , qC|C) + o(\u03c9), (S112) where  \u03a8RIM(pC , qC|C) = kpCD (k+1)2 [2(\u00b5R \u2212 c\u2212 b) + 2(k \u2212 1)\u03be1 + (k \u2212 1)(\u00b5R \u2212 c)(qC|C +qD|D) + (k + 1) 2qC|C + qD|D\u03be1],\n\u03a6RIM(pC , qC|C) = 2pCD (k+1)pC [1 + (k \u2212 1)(qC|D \u2212 qC|C)].\nUnder weak selection, the velocity of qC|C can be large, and it may rapidly converge to the root defined by \u03a6RIM(pC , qC|C) = 0 as time t\u2192 +\u221e. Thus, we get\nqC|C = pC + 1\nk \u2212 1 (1\u2212 pC). (S113)\nAccordingly, the dynamical equation described by Eq. (S112) becomes\ndpC dt = \u03c9k2(k \u2212 2)[b+ (\u00b5R \u2212 c)(k + 2)] (k + 1)2(k \u2212 1) pC(1\u2212 pC) + o(\u03c92), (S114)\nwhich has two fixed points pC = 0 and pC = 1. We define the function FIM(pC , \u00b5R, t) as\nFIM(pC , \u00b5R, t) = \u03c9k2(k \u2212 2)[b+ (\u00b5R \u2212 c)(k + 2)]\n(k + 1)2(k \u2212 1) pC(1\u2212 pC) + o(\u03c92), (S115)\nand the derivative of FIM(pC , \u00b5R, t) with respect to pC is\ndFIM dpC = \u03c9k2(k \u2212 2)[b+ (\u00b5R \u2212 c)(k + 2)] (k + 1)2(k \u2212 1) (1\u2212 2pC) + o(\u03c92). (S116)\nFor \u00b5R > c \u2212 bk+2 , we have dFIM dpC |pC=1 = \u2212 \u03c9k2(k\u22122)[b+(\u00b5R\u2212c)(k+2)] (k+1)2(k\u22121) < 0 and dFIM dpC |pC=0 = \u03c9k2(k\u22122)[b+(\u00b5R\u2212c)(k+2)] (k+1)2(k\u22121) > 0 which implies that the fixed point pC = 1 is stable and pC = 0 is unstable, i.e., cooperators prevail over defectors. Particularly, when \u00b5R = 0, we can see that for b/c > k + 2, the fixed point pC = 1 is stable and pC = 0 unstable. Thus, we obtain the condition b/c > k + 2 for the evolution of cooperation under IM update rule as previously obtained in Refs. [15, 16].\n41"
        },
        {
            "heading": "3.2. Negative Incentive",
            "text": "In this subsection, we consider the negative incentive into the networked Prisoner\u2019s Dilemma game with IM updating. According to this rule, we randomly choose a focal individual to update its strategy who has kC cooperators and kD defectors among its k neighbors. If the focal individual adopts strategy D, then the fitness of a C\u2013neighbor is\nfC = 1\u2212 \u03c9 + \u03c9{(b\u2212 c)(k \u2212 1)qC|C \u2212 c[(k \u2212 1)qD|C + 1]}, (S117)\nand the fitness of a D\u2013neighbor is\nfD = 1\u2212 \u03c9 + \u03c9{(b\u2212 \u00b5P )(k \u2212 1)qC|D \u2212 \u00b5P [(k \u2212 1)qD|D + 1]}. (S118)\nBesides, the fitness of the focal individual is\nf0 = 1\u2212 \u03c9 + \u03c9[(b\u2212 \u00b5P )kC \u2212 \u00b5PkD]. (S119)\nThe probability that the focal individual adopts strategy C is given by the expression \u0398 in Eq. (S100). Therefore, pC increases by 1/N with probability\nP (\u2206pC = 1\nN ) = pD k\u2211 kC=0 ( k kC ) (qC|D) kC (qD|D) kD\u0398. (S120)\nFurthermore, the number of CC\u2013pairs increases by kC and hence pCC increases by kC/(kN/2) with probability\nP (\u2206pCC = 2kC kN ) = pD\n( k\nkC\n) (qC|D) kC (qD|D) kD\u0398. (S121)\nIn addition, we consider another case, that is, the randomly selected focal individual adopts\nstrategy C. In this case, the fitness of a C\u2013neighbor is\nfC = 1\u2212 \u03c9 + \u03c9{(b\u2212 c)[(k \u2212 1)qC|C + 1]\u2212 c(k \u2212 1)qD|C}, (S122)\nand the fitness of a D\u2013neighbor is\nfD = 1\u2212 \u03c9 + \u03c9{(b\u2212 \u00b5P )[(k \u2212 1)qC|D + 1]\u2212 \u00b5P (k \u2212 1)qD|D}. (S123)\nBesides, the fitness of the focal individual is\nf0 = 1\u2212 \u03c9 + \u03c9[(b\u2212 c)kC \u2212 ckD]. (S124)\n42\nThe probability that the focal individual adopts strategy D is given by the expression \u03a5 in Eq. (S106). Therefore, pC decreases by 1/N with probability\nP (\u2206pC = \u2212 1\nN ) = pC k\u2211 kC=0 ( k kC ) (qC|C) kC (qD|C) kD\u03a5. (S125)\nAnd the number of CC\u2013pairs decreases by kC and hence pCC decreases by kC/(kN/2) with probability\nP (\u2206pCC = \u2212 2kC kN ) = pC\n( k\nkC\n) (qC|C) kC (qD|C) kD\u03a5. (S126)\nBased on these calculations, we obtain the time derivative of pC given by\ndpC dt = E(\u2206pC) \u2206t =\n1 N P (\u2206pC = 1 N )\u2212 1 N P (\u2206pC = \u2212 1N )\n1 N\n= \u03c9kpCD (k + 1)2 {2[(\u00b5P \u2212 c\u2212 b) + (k \u2212 1)\u03be2] + (k \u2212 1)(\u00b5P \u2212 c)(qC|C + qD|D) + (k + 1)2(qC|C + qD|D)\u03be2}+ o(\u03c92),\n(S127)\nwhere \u03be2 = (b \u2212 c)qC|C \u2212 cqD|C + (\u00b5P \u2212 b)qC|D + \u00b5P qD|D. Accordingly, the time derivative of pCC is given by\ndpCC dt = E(\u2206pCC) \u2206t =\n\u2211k kC=0 2kC kN P (\u2206pCC = 2kC kN )\u2212 \u2211k kC=0 2kC kN P (\u2206pCC = \u22122kCkN )\n1 N\n= 2pCD k + 1\n[1 + (k \u2212 1)(qC|D \u2212 qC|C)] + o(\u03c9). (S128)\nFurthermore, we have\ndqC|C dt = 2pCD (k + 1)pC [1 + (k \u2212 1)(qC|D \u2212 qC|C)] + o(\u03c9). (S129)\nHence, the dynamical equation is described by dpCdt = \u03c9\u03a8PIM(pC , qC|C) + o(\u03c92),dqC|C dt = \u03a6PIM(pC , qC|C) + o(\u03c9), (S130)\nwhere \u03a8PIM(pC , qC|C) = kpCD (k+1)2 {2[(\u00b5P \u2212 c\u2212 b) + (k \u2212 1)\u03be2] + (k \u2212 1)(\u00b5P \u2212 c)(qC|C + qD|D) +(k + 1)2(qC|C + qD|D)\u03be2},\n\u03a6PIM(pC , qC|C) = 2pCD (k+1)pC [1 + (k \u2212 1)(qC|D \u2212 qC|C)].\n43\nUnder weak selection, the velocity of qC|C can be large, and it may rapidly converge to the root defined by \u03a6PIM(pC , qC|C) = 0 as time t\u2192 +\u221e. Thus, we get\nqC|C = pC + 1\nk \u2212 1 (1\u2212 pC). (S131)\nAccordingly, the dynamical equation described by Eq. (S130) thus becomes\ndpC dt = \u03c9k2(k \u2212 2)[b+ (\u00b5P \u2212 c)(k + 2)] (k + 1)2(k \u2212 1) pC(1\u2212 pC) + o(\u03c92), (S132)\nwhich has two fixed points pC = 0 and pC = 1. We define the function FIM(pC , \u00b5P , t) as\nFIM(pC , \u00b5P , t) = \u03c9k2(k \u2212 2)[b+ (\u00b5P \u2212 c)(k + 2)]\n(k + 1)2(k \u2212 1) pC(1\u2212 pC) + o(\u03c92), (S133)\nand the derivative of FIM(pC , \u00b5P , t) with respect to pC is\ndFIM dpC = \u03c9k2(k \u2212 2)[b+ (\u00b5P \u2212 c)(k + 2)] (k + 1)2(k \u2212 1) (1\u2212 2pC) + o(\u03c92). (S134)\nFor \u00b5P > c \u2212 bk+2 , we have dFIM dpC |pC=1 = \u2212 \u03c9k2(k\u22122)[b+(\u00b5P\u2212c)(k+2)] (k+1)2(k\u22121) < 0 and dFIM dpC |pC=0 = \u03c9k2(k\u22122)[b+(\u00b5P\u2212c)(k+2)] (k+1)2(k\u22121) > 0, which implies that the fixed point pC = 1 is stable and pC = 0 unstable, i.e., cooperators prevail over defectors. Particularly, when \u00b5P = 0, we can see that for b/c > k + 2, the fixed point pC = 1 is stable and pC = 0 unstable. Thus, we obtain the condition b/c > k + 2 for the evolution of cooperation under IM update rule as obtained in Refs. [15, 16]."
        },
        {
            "heading": "3.3. Optimal Incentive Protocols",
            "text": "In subsections 3.1 and 3.2, we have theoretically obtained the dynamical equation with positive or negative incentive by means of the pair approximation approach in the limit of weak selection, which is given by\ndpC dt = FIM(pC , \u00b5v, t) = \u03c9k2(k \u2212 2)[b+ (\u00b5v \u2212 c)(k + 2)] (k + 1)2(k \u2212 1) pC(1\u2212 pC) + o(\u03c92). (S135)\nThis dynamical equation has two equilibria which are pC = 0 and pC = 1. If \u00b5v > c \u2212 bk+2 , the former is unstable and the latter is stable, which means that cooperation will be promoted in the long run. Furthermore, to identify the optimal rewarding and punishing protocols, we now use the approach of HJB equation.\nThe Hamiltonian function for the control problem is\nHIM(pC , \u00b5R, t) = (kNpC\u00b5R)\n2 2 + \u2202J\u2217R \u2202pC FIM(pC , \u00b5R, t), (S136)\n44\nwhere J\u2217R is the optimal cost function of pC and t for the optimal rewarding protocol given as\nJ\u2217R = \u222b tf 0 (kNpC\u00b5 \u2217 R) 2 2 dt. (S137)\nSolving \u2202HIM \u2202\u00b5R = 0, we know that the optimal rewarding protocol \u00b5\u2217R should satisfy\n\u00b5\u2217R = \u2212 \u03c9(k \u2212 2)(k + 2)(1\u2212 pC) N2(k \u2212 1)(k + 1)2pC \u2202J\u2217R \u2202pC . (S138)\nThe HJB equation can be written as\n\u2212\u2202J \u2217 R\n\u2202t = HIM(pC , \u00b5\n\u2217 R, t). (S139)\nAs the terminal time tf is not fixed, the optimal cost function J\u2217R(pC , t) is independent of t. Consequently, we have \u2202J\u2217R \u2202t = 0. (S140) We then obtain\n\u2202J\u2217R \u2202pC = 0 or \u2202J\u2217R \u2202pC = 2N2pC [b\u2212 c(k + 2)](k + 1)2(k \u2212 1) \u03c9(k \u2212 2)(k + 2)2(1\u2212 pC) . (S141)\nAs \u00b5R > 0 and pC \u2208 (0, 1), we have \u2202J\u2217R \u2202pC < 0. (S142) From Eq. (S142), we can see that this inequality is obviously satisfied for b/c \u2265 k + 2. Instead, we consider the case, i.e., b/c < k + 2, and hence only \u2202J \u2217 R\n\u2202pC = 2N 2pC [b\u2212c(k+2)](k+1)2(k\u22121) \u03c9(k\u22122)(k+2)2(1\u2212pC) holds. By\nsubstituting this equation into Eq. (S138), we obtain the optimal rewarding protocol as\n\u00b5\u2217R = 2[c(k + 2)\u2212 b]\nk + 2 . (S143)\nWith the optimal rewarding protocol \u00b5\u2217R, the dynamical equation thus becomes\ndpC dt = \u03c9k2(k \u2212 2)[c(k + 2)\u2212 b] (k + 1)2(k \u2212 1) pC(1\u2212 pC), (S144)\nwhere the initial fraction of cooperators in the population is denoted by p0 = pC(0). To solve the above equation, we have\npC = 1\n1 + 1\u2212p0 p0 e\u2212\u03b2IMt\n, (S145)\nwhere \u03b2IM = \u03c9k2(k\u22122)[c(k+2)\u2212b)]\n(k+1)2(k\u22121) . Hence, the cumulative cost produced by the optimal rewarding\nprotocol is given by\nJ\u2217R = (kN\u00b5\u2217R) 2\n2\u03b2IM [p0 \u2212 1 + \u03b4 + ln( 1\u2212 p0 \u03b4 )]. (S146)\n45\nThen, we solve the optimal control problem for punishing described by Eq. (17) in the main text. After calculations, we respectively obtain the optimal punishing protocol \u00b5\u2217P and the corresponding solution of pC given by\n\u00b5\u2217P = 2[c(k + 2)\u2212 b]\nk + 2 , (S147)\nand\npC = 1\n1 + 1\u2212p0 p0 e\u2212\u03b2IMt\n. (S148)\nAccordingly, the cumulative cost produced by the optimal punishing protocol is given by\nJ\u2217P = (kN\u00b5\u2217P ) 2\n2\u03b2IM [p0 \u2212 1 + \u03b4 + ln( 1\u2212 \u03b4 p0 )]. (S149)\nConsequently, the cumulative cost difference between optimal rewarding and punishing protocols is given by\nJ\u2217R \u2212 J\u2217P = (kN\u00b5\u2217v) 2 2\u03b2IM ln[ p0(1\u2212 p0) \u03b4(1\u2212 \u03b4) ], (S150)\nwhere \u00b5\u2217v = \u00b5 \u2217 R = \u00b5 \u2217 P . Similarly to Eq. (S52), we also find that J \u2217 R > J \u2217 P when \u03b4 < p), but when \u03b4 > p0 we have J\u2217R < J \u2217 P . This implies that for IM updating the execution of the optimal punishing protocol requires lower cumulative cost in comparison with the optimal rewarding one for \u03b4 < p0 and this conclusion is reversed for \u03b4 > p0. These theoretical results can be confirmed by numerical calculations and Monte Carlo simulations as presented in figure 4 and figure S5, respectively."
        },
        {
            "heading": "4. PC Updating",
            "text": ""
        },
        {
            "heading": "4.1. Positive Incentive",
            "text": "For PC updating [16] we randomly choose a focal individual to update its strategy who has kC cooperators and kD defectors among its k neighbors. If the focal individual adopts strategy D, then the fitness of the focal individual is\nfD = 1\u2212 \u03c9 + \u03c9\u03c0D0 = 1\u2212 \u03c9 + \u03c9[bkC + 0 \u00b7 kD], (S151)\nand the fitness of a C\u2013neighbor is\nfC = 1\u2212 \u03c9 + \u03c9\u03c0DC = 1\u2212 \u03c9 + \u03c9{(b\u2212 c+ \u00b5R)(k \u2212 1)qC|C + (\u00b5R \u2212 c)[(k \u2212 1)qD|C + 1]}. (S152) where \u03c0D0 represents the payoff of the focal individual, and \u03c0 D C denotes the payoff of aC\u2013neighbor.\n46\nSince the focal individual either keeps its current strategy or adopts the strategy of a neighbor with a probability that depends on the payoff difference, i.e., \u03c0DC \u2212 \u03c0D0 , the probability that the focal individual adopts the strategy of a C\u2013neighbor for \u03c9 \u2192 0 is\n\u039b = 1\n1 + e\u2212\u03c9(\u03c0 D C\u2212\u03c0 D 0 )\n= 1\n2 + \u03c9 \u03c0DC \u2212 \u03c0D0 4 . (S153)\nSince fC \u2212 fD = \u03c9(\u03c0DC \u2212 \u03c0D0 ) for weak selection, we further have\n\u039b = 1\n1 + e\u2212\u03c9(\u03c0 D C\u2212\u03c0 D 0 )\n= 1\n1 + e\u2212(fC\u2212fD) =\n1 2 + fC \u2212 fD 4 . (S154)\nTherefore, pC increases by 1/N with probability\nP (\u2206pC = 1\nN ) = pD k\u2211 kC=0 ( k kC ) (qC|D) kC (qD|D) kD kC k \u039b. (S155)\nHence the number of CC\u2013pairs increases by (k\u2212 1)qC|D + 1 and pCC increases by [(k\u2212 1)qC|D + 1]/(kN/2) with probability\nP (\u2206pCC = (k \u2212 1)qC|D + 1\nkN/2 ) = pD k\u2211 kC=0 ( k kC ) (qC|D) kC (qD|D) kD kC k \u039b. (S156)\nIn addition, we consider another case where the randomly selected focal individual adopts\nstrategy C. In this case, the fitness of the focal individual is\nfC = 1\u2212 \u03c9 + \u03c9\u03c0C0 = 1\u2212 \u03c9 + \u03c9[(b\u2212 c+ \u00b5R)kC + (\u00b5R \u2212 c)kD], (S157)\nand the fitness of a D\u2013neighbor is\nfD = 1\u2212 \u03c9 + \u03c9\u03c0CD = 1\u2212 \u03c9 + \u03c9[(k \u2212 1)qC|D + 1]b, (S158)\nwhere \u03c0C0 represents the payoff of the focal individual, and \u03c0 C D denotes the payoff of a neighbor with strategy D. The probability that the focal individual adopts the strategy of a D\u2013neighbor for \u03c9 \u2192 0 is\n\u2126 = 1\n1 + e\u2212\u03c9(\u03c0 C D\u2212\u03c0 C 0 )\n= 1\n2 + \u03c9 \u03c0CD \u2212 \u03c0C0 4 = 1 2 + fD \u2212 fC 4 . (S159)\nTherefore, pC decreases by 1/N with probability\nP (\u2206pC = \u2212 1\nN ) = pC k\u2211 kC=0 ( k kC ) (qC|C) kC (qD|C) kD kD k \u2126. (S160)\n47\nTherefore the number of CC\u2013pairs decreases by (k \u2212 1)qC|C and hence pCC increases by (k \u2212 1)qC|C/(kN/2) with probability\nP (\u2206pCC = \u2212 (k \u2212 1)qC|C kN/2 ) = pC k\u2211 kC=0 ( k kC ) (qC|C) kC (qD|C) kD kD k \u2126. (S161)\nBased on these calculations, we respectively obtain the time derivatives of pC and pCC as\ndpC dt = E(\u2206pC) \u2206t =\n1 N P (\u2206pC = 1 N )\u2212 1 N P (\u2206pC = \u2212 1N )\n1 N\n= \u03c9pCD\n2 {(\u00b5R \u2212 c\u2212 b) + (k \u2212 1)[(b\u2212 c+ \u00b5R)qC|C + (\u00b5R \u2212 c)qD|C \u2212 bqC|D]}+ o(\u03c92).\n(S162)\nand\ndpCC dt = E(\u2206pCC) \u2206t =\n(k\u22121)qC|D+1 kN/2 P (\u2206pCC = (k\u22121)qC|D+1 kN/2 )\u2212 (k\u22121)qC|C kN/2 P (\u2206pCC = \u2212 (k\u22121)qC|C kN/2\n) 1 N\n= 1\nk pCD[1 + (k \u2212 1)(qC|D \u2212 qC|C)] + o(\u03c9).\n(S163)\nFurthermore, we have\ndqC|C dt = pCD kpC [1 + (k \u2212 1)(qC|D \u2212 qC|C)] + o(\u03c9). (S164)\nHence, the dynamical equation is described by dpCdt = \u03c9\u03a8RPC(pC , qC|C) + o(\u03c92),dqC|C dt = \u03a6RPC(pC , qC|C) + o(\u03c9), (S165) where \u03a8RPC(pC , qC|C) = pCD2 {(\u00b5R \u2212 c\u2212 b) + (k \u2212 1)[(b\u2212 c+ \u00b5R)qC|C + (\u00b5R \u2212 c)qD|C \u2212 bqC|D]}\u03a6RPC(pC , qC|C) = pCDkpC [1 + (k \u2212 1)(qC|D \u2212 qC|C)]. Under weak selection, the velocity of qC|C can be large, and it may rapidly converge to the root defined by \u03a6RPC(pC , qC|C) = 0 as time t\u2192 +\u221e. Thus, we get\nqC|C = pC + 1\nk \u2212 1 (1\u2212 pC). (S166)\nAccordingly, the dynamical equation described by Eq. (S165) becomes\ndpC dt = \u03c9k(k \u2212 2)(\u00b5R \u2212 c) 2(k \u2212 1) pC(1\u2212 pC) + o(\u03c92), (S167)\n48\nwhich has two fixed points pC = 0 and pC = 1. We define the function FPC(pC , \u00b5R, t) as\nFPC(pC , \u00b5R, t) = \u03c9k(k \u2212 2)(\u00b5R \u2212 c)\n2(k \u2212 1) pC(1\u2212 pC) + o(\u03c92), (S168)\nand the derivative of FPC(pC , \u00b5R, t) with respect to pC is\ndFPC dpC = \u03c9k(k \u2212 2)(\u00b5R \u2212 c) 2(k \u2212 1) (1\u2212 2pC) + o(\u03c92). (S169)\nFor \u00b5R > c, we have dFPCdpC |pC=1 = \u2212 \u03c9k(k\u22122)(\u00b5R\u2212c) 2(k\u22121) < 0 and dFPC dpC |pC=0 = \u03c9k(k\u22122)(\u00b5R\u2212c) 2(k\u22121) > 0 which implies that the fixed point pC = 1 is stable and pC = 0 is unstable, i.e., cooperators prevail over defectors. Particularly, when \u00b5R = 0, we can see that the fixed point pC = 0 is always stable and pC = 1 unstable, which means that cooperation can never emerge as observed in previous work [16]."
        },
        {
            "heading": "4.2. Negative Incentive",
            "text": "In this subsection, we consider how punishment works for PC updating. According to this rule, we randomly select a focal individual to update its strategy who has kC cooperators and kD defectors among its k neighbors. If the focal individual adopts strategy D, then the fitness of the focal individual is\nfD = 1\u2212 \u03c9 + \u03c9\u03c0D0 = 1\u2212 \u03c9 + \u03c9[(b\u2212 \u00b5P )kC \u2212 \u00b5PkD], (S170)\nand the fitness of a C\u2013neighbor is\nfC = 1\u2212 \u03c9 + \u03c9\u03c0DC = 1\u2212 \u03c9 + \u03c9{(b\u2212 c)(k \u2212 1)qC|C \u2212 c[(k \u2212 1)qD|C + 1]}, (S171)\nwhere \u03c0D0 represents the payoff of the focal individual, and \u03c0 D C denotes the payoff of aC\u2013neighbor.\nThe probability that the focal individual adopts the strategy of a C-neighbor is given by the\nexpression \u039b in Eq. (S154). Therefore, pC increases by 1/N with probability\nP (\u2206pC = 1\nN ) = pD k\u2211 kC=0 ( k kC ) (qC|D) kC (qD|D) kD kC k \u039b, (S172)\nand the number of CC\u2013pairs increases by (k \u2212 1)qC|D + 1 and hence pCC increases by [(k \u2212 1)qC|D + 1]/(kN/2) with probability\nP (\u2206pCC = (k \u2212 1)qC|D + 1\nkN/2 ) = pD k\u2211 kC=0 ( k kC ) (qC|D) kC (qD|D) kD kC k \u039b. (S173)\n49\nIn the alternative case, the randomly selected focal individual adopts strategy C. Here the\nfitness of the focal individual is\nfC = 1\u2212 \u03c9 + \u03c9\u03c0C0 = 1\u2212 \u03c9 + \u03c9[(b\u2212 c)kC \u2212 ckD], (S174)\nand the fitness of a D\u2013neighbor is\nfD = 1\u2212 \u03c9 + \u03c9\u03c0CD = 1\u2212 \u03c9 + \u03c9{(b\u2212 \u00b5P )[(k \u2212 1)qC|D + 1]\u2212 \u00b5P (k \u2212 1)qD|D}, (S175)\nwhere \u03c0C0 represents the payoff of the focal individual, and \u03c0 C D denotes the payoff of a neighbor with strategy D. The probability that the focal individual adopts the strategy of a D\u2013neighbor is given by the expression \u2126 in Eq. (S159). Therefore, pC decreases by 1/N with probability\nP (\u2206pC = \u2212 1\nN ) = pC k\u2211 kC=0 ( k kC ) (qC|C) kC (qD|C) kD kD k \u2126. (S176)\nAnd the number of CC\u2013pairs decreases by (k \u2212 1)qC|C and hence pCC increases by (k \u2212 1)qC|C/(kN/2) with probability\nP (\u2206pCC = \u2212 (k \u2212 1)qC|C kN/2 ) = pC k\u2211 kC=0 ( k kC ) (qC|C) kC (qD|C) kD kD k \u2126. (S177)\nBased on these calculations, we respectively obtain the time derivatives of pC and pCC as\ndpC dt = E(\u2206pC) \u2206t =\n1 N P (\u2206pC = 1 N )\u2212 1 N P (\u2206pC = \u2212 1N )\n1 N\n= \u03c9pCD\n2 {\u00b5P \u2212 c\u2212 b+ (k \u2212 1)[(b\u2212 c)qC|C \u2212 cqD|C + (\u00b5P \u2212 b)qC|D + \u00b5P qD|D]}+ o(\u03c92).\n(S178)\nand\ndpCC dt = E(\u2206pCC) \u2206t =\n(k\u22121)qC|D+1 kN/2 P (\u2206pCC = (k\u22121)qC|D+1 kN/2 )\u2212 (k\u22121)qC|C kN/2 P (\u2206pCC = \u2212 (k\u22121)qC|C kN/2\n) 1 N\n= pCD k\n[1 + (k \u2212 1)(qC|D \u2212 qC|C)] + o(\u03c9). (S179)\nFurthermore, we have\ndqC|C dt = pCD kpC [1 + (k \u2212 1)(qC|D \u2212 qC|C)] + o(\u03c9). (S180)\nHence, the dynamical equation is described by dpCdt = \u03c9\u03a8PPC(pC , qC|C) + o(\u03c92),dqC|C dt = \u03a6PPC(pC , qC|C) + o(\u03c9), (S181)\n50\nwhere \u03a8PPC(pC , qC|C) = pCD2 {\u00b5P \u2212 c\u2212 b+ (k \u2212 1)[(b\u2212 c)qC|C \u2212 cqD|C + (\u00b5P \u2212 b)qC|D + \u00b5P qD|D]},\u03a6PPC(pC , qC|C) = pCDkpC [1 + (k \u2212 1)(qC|D \u2212 qC|C)]. Under weak selection, the velocity of qC|C can be large, and it may rapidly converge to the root defined by \u03a6PPC(pC , qC|C) = 0 as time t\u2192 +\u221e. Thus, we get\nqC|C = pC + 1\nk \u2212 1 (1\u2212 pC). (S182)\nAccordingly, the dynamical equation described by Eq. (S181) thus becomes\ndpC dt = \u03c9k(k \u2212 2)(\u00b5P \u2212 c) 2(k \u2212 1) pC(1\u2212 pC) + o(\u03c92), (S183)\nwhich has two fixed points pC = 0 and pC = 1. We define the function FPC(pC , \u00b5P , t) as\nFPC(pC , \u00b5P , t) = \u03c9k(k \u2212 2)(\u00b5P \u2212 c)\n2(k \u2212 1) pC(1\u2212 pC) + o(\u03c92), (S184)\nand the derivative of FPC(pC , \u00b5P , t) with respect to pC is\ndFPC dpC = \u03c9k(k \u2212 2)(\u00b5P \u2212 c) 2(k \u2212 1) (1\u2212 2pC) + o(\u03c92). (S185)\nFor \u00b5P > c, we have dFPCdpC |pC=1 = \u2212 \u03c9k(k\u22122)(\u00b5P\u2212c) 2(k\u22121) < 0 and dFPC dpC |pC=0 = \u03c9k(k\u22122)(\u00b5P\u2212c) 2(k\u22121) > 0 which implies that the fixed point pC = 1 is stable and pC = 0 unstable, i.e., cooperators prevail over defectors. Particularly, when \u00b5P = 0, we can see that the fixed point pC = 0 is always stable and pC = 1 unstable, which means that cooperation can never emerge as obtained in Ref. [16]."
        },
        {
            "heading": "4.3. Optimal Incentive Protocols",
            "text": "By means of the pair approximation approach, in the weak selection limit we have the dynamical equation under PC update rule as\ndpC dt = FPC(pC , \u00b5v, t) = \u03c9k(k \u2212 2)(\u00b5v \u2212 c) 2(k \u2212 1) pC(1\u2212 pC) + o(\u03c92). (S186)\nThis dynamical equation has two equilibria which are pC = 0 and pC = 1. If \u00b5v > c, the former is unstable and the latter is stable, and hence cooperation will be promoted in the long run. Furthermore, to explore the optimal rewarding and punishing protocols, we now use the approach of HJB equation.\n51\nIn case of reward, we define the Hamiltonian function HPC(pC , \u00b5R, t) as\nHPC(pC , \u00b5R, t) = (kNpC\u00b5R)\n2 2 + \u2202J\u2217R \u2202pC FPC(pC , \u00b5R, t), (S187)\nwhere J\u2217R is the optimal cost function of pC and t for the optimal rewarding protocol given as\nJ\u2217R = \u222b tf 0 (kNpC\u00b5 \u2217 R) 2 2 dt. (S188)\nSolving \u2202HPC \u2202\u00b5R = 0, we know that the optimal rewarding protocol \u00b5\u2217R should satisfy\n\u00b5\u2217R = \u2212 \u03c9(k \u2212 2)(1\u2212 pC) 2N2k(k \u2212 1)pC \u2202J\u2217R \u2202pC . (S189)\nThe HJB equation can be written as\n\u2212\u2202J \u2217 R\n\u2202t = HPC(pC , \u00b5\n\u2217 R, t). (S190)\nAs the terminal time tf is not fixed, the optimal cost function J\u2217R(pC , t) is independent of t. Consequently, we have \u2202J\u2217R \u2202t = 0. (S191) We then yield \u2202J\u2217R \u2202pC = 0 or \u2202J\u2217R \u2202pC = \u2212 4N 2k(k \u2212 1)cpC \u03c9(k \u2212 2)(1\u2212 pC) . (S192) As \u00b5R > 0 and pC \u2208 (0, 1), we have \u2202J\u2217R \u2202pC < 0. (S193) Therefore only \u2202J \u2217 R\n\u2202pC = \u22124N 2k(k\u22121)cpC \u03c9(k\u22122)(1\u2212pC) holds. By substituting this equation into Eq. (S189), we\nobtain the optimal rewarding protocol \u00b5\u2217R as\n\u00b5\u2217R = 2c. (S194)\nWith the optimal rewarding protocol \u00b5\u2217R, the dynamical equation thus becomes\ndpC dt = \u03c9k(k \u2212 2)c 2(k \u2212 1) pC(1\u2212 pC), (S195)\nwhere the initial fraction of cooperators in the population is denoted by p0 = pC(0). By solving this equation, we have\npC = 1\n1 + 1\u2212p0 p0 e\u2212\u03b2PCt\n, (S196)\n52\nwhere \u03b2PC = \u03c9k(k\u22122)c\n2(k\u22121) . Hence, the cumulative cost produced by the optimal rewarding protocol is\ngiven by\nJ\u2217R = (kN\u00b5\u2217R) 2\n2\u03b2PC [p0 \u2212 1 + \u03b4 + ln( 1\u2212 p0 \u03b4 )]. (S197)\nFor punishment, we respectively obtain the optimal protocol \u00b5\u2217P and the corresponding solution\nof pC as\n\u00b5\u2217P = 2c (S198)\nand\npC = 1\n1 + 1\u2212p0 p0 e\u2212\u03b2PCt\n. (S199)\nAccordingly, the cumulative cost produced by the optimal punishing protocol is given by\nJ\u2217P = (kN\u00b5\u2217P ) 2\n2\u03b2PC [p0 \u2212 1 + \u03b4 + ln( 1\u2212 \u03b4 p0 )]. (S200)\nConsequently, the difference between the cumulative cost values is\nJ\u2217R \u2212 J\u2217P = (kN\u00b5\u2217v) 2 2\u03b2PC ln[ p0(1\u2212 p0) \u03b4(1\u2212 \u03b4) ], (S201)\nwhere \u00b5\u2217v = \u00b5 \u2217 R = \u00b5 \u2217 P . Similarly to Eq. (S52), we also find that J \u2217 R > J \u2217 P when \u03b4 < p0, but when \u03b4 > p0 we have J\u2217R < J \u2217 P . This implies that for PC updating the usage of optimal punishment requires less cost than the optimal rewarding protocol for \u03b4 < p0 and we have the opposite conclusion for \u03b4 > p0. These theoretical results can be confirmed by numerical calculations and Monte Carlo simulations as presented in figure 4 and figure S5, respectively.\n53\nSupplementary Figures\n54\n55\n56\n57"
        }
    ],
    "title": "Optimization of institutional incentives for cooperation in structured populations",
    "year": 2023
}