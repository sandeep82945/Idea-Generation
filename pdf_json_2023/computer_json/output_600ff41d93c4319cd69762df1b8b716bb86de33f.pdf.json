{
    "abstractText": "Sparse-view Computed Tomography (SVCT) reconstruction is an ill-posed inverse problem in imaging that aims to acquire high-quality CT images based on sparsely-sampled measurements. Recent works use Implicit Neural Representations (INRs) to build the coordinatebased mapping between sinograms and CT images. However, these methods have not considered the correlation between adjacent projection views, resulting in aliasing artifacts on SV sinograms. To address this issue, we propose a self-supervised SVCT reconstruction method \u2013 AntiAliasing Projection Representation Field (APRF), which can build the continuous representation between adjacent projection views via the spatial constraints. Specifically, APRF only needs SV sinograms for training, which first employs a line-segment sampling module to estimate the distribution of projection views in a local region, and then synthesizes the corresponding sinogram values using a center-based line integral module. After training APRF on a single SV sinogram itself, it can synthesize the corresponding denseview (DV) sinogram with consistent continuity. High-quality CT images can be obtained by applying re-projection techniques on the predicted DV sinograms. Extensive experiments on CT images demonstrate that APRF outperforms state-of-the-art methods, yielding more accurate details and fewer artifacts. Our code will be publicly available soon.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zixuan Chen"
        },
        {
            "affiliations": [],
            "name": "Lingxiao Yang"
        },
        {
            "affiliations": [],
            "name": "Xiaohua Xie"
        }
    ],
    "id": "SP:03c0e2bf7cbfe9064a5452d5af84eec76ff387b4",
    "references": [
        {
            "authors": [
                "A.C. Kak",
                "M. Slaney"
            ],
            "title": "Principles of computerized tomographic imaging",
            "year": 2001
        },
        {
            "authors": [
                "R. Gordon",
                "R. Bender",
                "G.T. Herman"
            ],
            "title": "Algebraic reconstruction techniques (art) for three-dimensional electron microscopy and x-ray photography",
            "venue": "Journal of theoretical Biology, vol. 29, no. 3, pp. 471\u2013 481, 1970.",
            "year": 1970
        },
        {
            "authors": [
                "D.R. Martin",
                "R.C. Semelka"
            ],
            "title": "Health effects of ionising radiation from diagnostic ct",
            "venue": "The Lancet, vol. 367, no. 9524, pp. 1712\u20131714, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "L. Shen",
                "J. Pauly",
                "L. Xing"
            ],
            "title": "Nerp: implicit neural representation learning with prior embedding for sparsely sampled image reconstruction",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Sun",
                "J. Liu",
                "M. Xie",
                "B. Wohlberg",
                "U.S. Kamilov"
            ],
            "title": "Coil: Coordinate-based internal learning for tomographic imaging",
            "venue": "IEEE Transactions on Computational Imaging, vol. 7, pp. 1400\u20131412, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Q. Wu",
                "R. Feng",
                "H. Wei",
                "J. Yu",
                "Y. Zhang"
            ],
            "title": "Self-supervised coordinate projection network for sparse-view computed tomography",
            "venue": "IEEE Transactions on Computational Imaging, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "S. Shakouri",
                "M.A. Bakhshali",
                "P. Layegh",
                "B. Kiani",
                "F. Masoumi",
                "S. Ataei Nakhaei",
                "S.M. Mostafavi"
            ],
            "title": "Covid19-ct-dataset: an openaccess chest ct image repository of 1000+ patients with confirmed covid- 19 diagnosis",
            "venue": "BMC Research Notes, vol. 14, no. 1, p. 178, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Huang",
                "Y. Zhang",
                "J. Ma",
                "D. Zeng",
                "Z. Bian",
                "S. Niu",
                "Q. Feng",
                "Z. Liang",
                "W. Chen"
            ],
            "title": "Iterative image reconstruction for sparse-view ct using normal-dose image induced total variation prior",
            "venue": "PloS one, vol. 8, no. 11, p. e79709, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "S. Niu",
                "Y. Gao",
                "Z. Bian",
                "J. Huang",
                "W. Chen",
                "G. Yu",
                "Z. Liang",
                "J. Ma"
            ],
            "title": "Sparse-view x-ray ct reconstruction via total generalized variation regularization",
            "venue": "Physics in Medicine & Biology, vol. 59, no. 12, p. 2997, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "K.H. Jin",
                "M.T. McCann",
                "E. Froustey",
                "M. Unser"
            ],
            "title": "Deep convolutional neural network for inverse problems in imaging",
            "venue": "IEEE Transactions on Image Processing, vol. 26, no. 9, pp. 4509\u20134522, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Han",
                "J.C. Ye"
            ],
            "title": "Framing u-net via deep convolutional framelets: Application to sparse-view ct",
            "venue": "IEEE transactions on medical imaging, vol. 37, no. 6, pp. 1418\u20131429, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "H. Zhang",
                "B. Liu",
                "H. Yu",
                "B. Dong"
            ],
            "title": "Metainv-net: Meta inversion network for sparse view ct image reconstruction",
            "venue": "IEEE Transactions on Medical Imaging, vol. 40, no. 2, pp. 621\u2013634, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "W. Wu",
                "D. Hu",
                "C. Niu",
                "H. Yu",
                "V. Vardhanabhuti",
                "G. Wang"
            ],
            "title": "Drone: Dual-domain residual-based optimization network for sparse-view ct reconstruction",
            "venue": "IEEE Transactions on Medical Imaging, vol. 40, no. 11, pp. 3002\u20133014, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Zhang",
                "X. Liang",
                "X. Dong",
                "Y. Xie",
                "G. Cao"
            ],
            "title": "A sparse-view ct reconstruction method based on combination of densenet and deconvolution",
            "venue": "IEEE transactions on medical imaging, vol. 37, no. 6, pp. 1407\u20131417, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "B. Zhou",
                "S.K. Zhou",
                "J.S. Duncan",
                "C. Liu"
            ],
            "title": "Limited view tomographic reconstruction using a cascaded residual dense spatialchannel attention network with projection data fidelity layer",
            "venue": "IEEE transactions on medical imaging, vol. 40, no. 7, pp. 1792\u20131804, 2021.",
            "year": 1804
        },
        {
            "authors": [
                "Y. Li",
                "K. Li",
                "C. Zhang",
                "J. Montoya",
                "G.-H. Chen"
            ],
            "title": "Learning to reconstruct computed tomography images directly from sinogram data under a variety of data acquisition conditions",
            "venue": "IEEE transactions on medical imaging, vol. 38, no. 10, pp. 2469\u20132481, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Xiang",
                "Y. Dong",
                "Y. Yang"
            ],
            "title": "Fista-net: Learning a fast iterative shrinkage thresholding network for inverse problems in imaging",
            "venue": "IEEE Transactions on Medical Imaging, vol. 40, no. 5, pp. 1329\u20131339, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "B. Mildenhall",
                "P.P. Srinivasan",
                "M. Tancik",
                "J.T. Barron",
                "R. Ramamoorthi",
                "R. Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "European conference on computer vision. Springer, 2020, pp. 405\u2013421.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Chen",
                "S. Liu",
                "X. Wang"
            ],
            "title": "Learning continuous image representation with local implicit image function",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 8628\u20138638.",
            "year": 2021
        },
        {
            "authors": [
                "M. Tancik",
                "P. Srinivasan",
                "B. Mildenhall",
                "S. Fridovich-Keil",
                "N. Raghavan",
                "U. Singhal",
                "R. Ramamoorthi",
                "J. Barron",
                "R. Ng"
            ],
            "title": "Fourier features let networks learn high frequency functions in low dimensional domains",
            "venue": "Advances in Neural Information Processing Systems, vol. 33, pp. 7537\u2013 7547, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "N. Heller",
                "N. Sathianathen",
                "A. Kalapara",
                "E. Walczak",
                "K. Moore",
                "H. Kaluzniak",
                "J. Rosenberg",
                "P. Blake",
                "Z. Rengel",
                "M. Oestreich"
            ],
            "title": "The kits19 challenge data: 300 kidney tumor cases with clinical context, ct semantic segmentations, and surgical outcomes",
            "venue": "arXiv preprint arXiv:1904.00445, 2019.",
            "year": 1904
        },
        {
            "authors": [
                "Z. Chen",
                "Y. Chen",
                "J. Liu",
                "X. Xu",
                "V. Goel",
                "Z. Wang",
                "H. Shi",
                "X. Wang"
            ],
            "title": "Videoinr: Learning video implicit neural representation for continuous 10 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2022 space-time super-resolution",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 2047\u20132057.",
            "year": 2022
        },
        {
            "authors": [
                "K. Schwarz",
                "Y. Liao",
                "M. Niemeyer",
                "A. Geiger"
            ],
            "title": "Graf: Generative radiance fields for 3d-aware image synthesis",
            "venue": "Advances in Neural Information Processing Systems, vol. 33, pp. 20 154\u201320 166, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Niemeyer",
                "A. Geiger"
            ],
            "title": "Giraffe: Representing scenes as compositional generative neural feature fields",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 11 453\u201311 464.",
            "year": 2021
        },
        {
            "authors": [
                "B. Poole",
                "A. Jain",
                "J.T. Barron",
                "B. Mildenhall"
            ],
            "title": "Dreamfusion: Text-to-3d using 2d diffusion",
            "venue": "The Eleventh International Conference on Learning Representations, 2023. [Online]. Available: https://openreview.net/forum?id=FjNys5c7VyY",
            "year": 2023
        },
        {
            "authors": [
                "Y.-J. Yuan",
                "Y.-T. Sun",
                "Y.-K. Lai",
                "Y. Ma",
                "R. Jia",
                "L. Gao"
            ],
            "title": "Nerfediting: geometry editing of neural radiance fields",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 18 353\u201318 364.",
            "year": 2022
        },
        {
            "authors": [
                "C. Bao",
                "Y. Zhang",
                "B. Yang",
                "T. Fan",
                "Z. Yang",
                "H. Bao",
                "G. Zhang",
                "Z. Cui"
            ],
            "title": "Sine: Semantic-driven image-based nerf editing with priorguided editing field",
            "venue": "Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR), 2023.",
            "year": 2023
        },
        {
            "authors": [
                "A. Corona-Figueroa",
                "J. Frawley",
                "S. Bond-Taylor",
                "S. Bethapudi",
                "H.P. Shum",
                "C.G. Willcocks"
            ],
            "title": "Mednerf: Medical neural radiance fields for reconstructing 3d-aware ct-projections from a single x-ray",
            "venue": "2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC). IEEE, 2022, pp. 3843\u20133848.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Chen",
                "J. Lai",
                "L. Yang",
                "X. Xie"
            ],
            "title": "Cunerf: Cube-based neural radiance field for zero-shot medical image arbitrary-scale super resolution",
            "venue": "arXiv preprint arXiv:2303.16242, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "O. Ronneberger",
                "P. Fischer",
                "T. Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer, 2015, pp. 234\u2013241.",
            "year": 2015
        },
        {
            "authors": [
                "G. Zang",
                "R. Idoughi",
                "R. Li",
                "P. Wonka",
                "W. Heidrich"
            ],
            "title": "Intratomo: self-supervised learning-based tomography via sinogram synthesis and prediction",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 1960\u20131970.",
            "year": 2021
        },
        {
            "authors": [
                "P. Kr\u00e4henb\u00fchl",
                "V. Koltun"
            ],
            "title": "Efficient inference in fully connected crfs with gaussian edge potentials",
            "venue": "Advances in neural information processing systems, vol. 24, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "J. Adler",
                "H. Kohr",
                "O. \u00d6ktem"
            ],
            "title": "Operator discretization library (odl)",
            "venue": "Zenodo, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "A. Vedaldi",
                "K. Lenc"
            ],
            "title": "Matconvnet: Convolutional neural networks for matlab",
            "venue": "Proceedings of the 23rd ACM international conference on Multimedia, 2015, pp. 689\u2013692.",
            "year": 2015
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "Z. Wang",
                "A.C. Bovik",
                "H.R. Sheikh",
                "E.P. Simoncelli"
            ],
            "title": "Image quality assessment: from error visibility to structural similarity",
            "venue": "IEEE transactions on image processing, vol. 13, no. 4, pp. 600\u2013612, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "R. Zhang",
                "P. Isola",
                "A.A. Efros",
                "E. Shechtman",
                "O. Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 586\u2013595.",
            "year": 2018
        },
        {
            "authors": [
                "S. van der Walt",
                "J.L. Sch\u00f6nberger",
                "J. Nunez-Iglesias",
                "F. Boulogne",
                "J.D. Warner",
                "N. Yager",
                "E. Gouillart",
                "T. Yu",
                "the scikit-image contributors"
            ],
            "title": "scikit-image: image processing in Python",
            "venue": "PeerJ, vol. 2, p. e453, 6 2014. [Online]. Available: https://doi.org/10.7717/peerj.453",
            "year": 2014
        },
        {
            "authors": [
                "L. Yen-Chen"
            ],
            "title": "Nerf-pytorch",
            "venue": "https://github.com/yenchenlin/ nerf-pytorch/, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Paszke",
                "S. Gross",
                "S. Chintala",
                "G. Chanan",
                "E. Yang",
                "Z. DeVito",
                "Z. Lin",
                "A. Desmaison",
                "L. Antiga",
                "A. Lerer"
            ],
            "title": "Automatic differentiation in pytorch",
            "venue": "Proceedings of Neural Information Processing Systems (NIPS), 2017.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014 Imaging Inverse Problem, Sparse-View Computed Tomography (SVCT), Implicit Neural Representation (INR), Self-Supervised Learning, Anti-Aliasing.\nI. INTRODUCTION\nCOMPUTED Tomography (CT) is a diagnostic imagingprocedure that uses a combination of X-rays to observe the internal structure of the scanned objects. It consists of two steps: i) emitting X-rays in a circle around the scanned subjects and storing the information about attenuation properties at each projection angle in the sinograms; ii) using re-projection techniques like [1], [2] to transform the sinograms into CT images. Acquiring high-quality CT images requires denselysampled projection views, which means that subjects must be\nManuscript received XXX XX, XXXX; revised XXX XX, XXXX. This work was supported in part by the National Natural Science Foundation of China under Grant 62072482.\n(Corresponding author: Xiaohua Xie.) All the authors are with the School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou 510006, China; and with the Guangdong Province Key Laboratory of Information Security Technology, Guangzhou 510006, China; and also with the Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, Guangzhou 510006, China. (e-mail: chenzx3@mail2.sysu.edu.cn; yanglx9@mail.sysu.edu.cn; stsljh@mail. sysu.edu.cn; xiexiaoh6@mail.sysu.edu.cn.)\nAPRF (Ours)NeRP SCOPEGT CoIL\nP ar\nal le\nlB\nea m\nS V\nC T\nF an\n-B ea\nm S\nV C\nT\n90 views\n90 views\nFig. 1. The visual examples of 2D SVCT reconstruction results between our APRF and the state-of-the-art INR-based methods: NeRP [4], CoIL [5] and SCOPE [6] on COVID-19 [7] dataset. Heatmaps at the bottom visualize the difference related to the GT images. Red text indicates the best performance.\nscanned for long periods of time without moving. Exposure to such prolonged ionizing radiation may increase the risk of cancer in subjects [3]. Consequently, Sparse-View Computed Tomography (SVCT) reconstruction, i.e., reconstructing CT images based on sparsely-sampled measurements, can significantly reduce the ionizing radiation from CT scans, which is of great concern in the field of public healthcare.\nSVCT reconstruction is a highly ill-posed inverse problem. Since directly using the conventional re-projection techniques [1] and [2] may produce severe artifacts, early studies [8], [9] combined [2] with the regularization terms for optimization. Subsequently, with the development of deep learning, many methods [10]\u2013[17] proposed different Convolutional Neural Networks (CNNs) to learn the mapping between SV sinograms and CT images. These CNN-based methods are fully-supervised, requiring considerable SV sinogram and CT image pairs for training. Recently, implicit neural representation (INR) techniques [18], [19] have yielded impressive performance in computer vision for numerous tasks. Many studies [4]\u2013[6], [20] utilize the INR techniques to handle the SVCT reconstruction challenges. One representative strategy is to build the coordinate-based field that directly represents the sinograms [5]. The other strategy is to build the coordinate-\nar X\niv :2\n30 7.\n05 27\n0v 1\n[ ee\nss .I\nV ]\n1 1\nJu l 2\n02 3\nbased representation field of CT density, and then synthesize the corresponding sinograms based on the differentiable Radon transformation [4], [6], [20]. However, these INR-based methods have not considered the spatial correlations between adjacent projections, producing blurry contents [4] and severe artifacts [5], [6] (see Figure 1).\nTo address the above issues, we propose an Anti-Aliasing Projection Representation Field (APRF), which aims to model the correlation between adjacent projection views from the SV sinograms. Unlike the above CNN-based methods, our APRF is a self-supervised SVCT reconstruction method, which is trained on the SV sinograms without the reference of CT images. Specifically, we propose a line-segment sampling module to randomly sample N projection angles within a spatial region. To synthesize corresponding sinogram values, we also propose a center-based line integral module, which employs a numerical integral on the sampled coordinates. After optimization, the internal regions between adjacent projection views can be indirectly modeled by the spatial constraints, and thus APRF can yield the dense-view (DV) sinogram with consistent continuity. By applying re-projection techniques like [1] on the predicted DV sinograms, our APRF can acquire highquality CT images (see Figure 1). Comprehensive experiments on COVID-19 [7] and KiTS19 [21] datasets demonstrate the superiority of our APRF, whose reconstruction quality surpasses state-of-the-art methods on parallel-, fan- and conebeam SVCT images.\nIn summary, the main contributions are: \u2022 We discover that existing INR-based methods are vul-\nnerable to aliasing errors in the projection domains, and propose an anti-aliasing SVCT reconstruction method. \u2022 We propose a line-segment sampling module and a center-based line integral module to alleviate aliasing errors via spatial constraints. \u2022 We conduct a series of experiments to demonstrate the effectiveness of our model."
        },
        {
            "heading": "II. RELATED WORKS",
            "text": "In this section, we first briefly review the Implicit Neural Representation (INR) techniques and their derived applications in computer vision and medical image reconstruction. We then introduce some impressive advances in sparse-view computed tomography (SVCT) reconstruction, including some very recent INR-based methods.\nA. Implicit Neural Representation\nModeling a continuous representation from discretely sampled data is a long-standing problem in image reconstruction. Recent studies propose implicit neural representation (INR) techniques to solve this ill-posed problem, which aim to build a coordinate-based representation field from the collected samples. Specifically, these INR-based methods use the Multi-Layer Perceptron (MLP) to encode the coordinates and learn the mapping between coordinates and training samples. After training, based on the image continuity priors, these coordinate-based representation fields can produce the i.i.d samples corresponding to the training set. INR techniques have yielded impressive advances in image reconstruction for numerous tasks: single image super-resolution [19], video super-resolution [22], novel view synthesis [18], generative modeling [23]\u2013[25], and editing [26], [27]. Recently, some researchers have tried to adapt INR techniques to the medical domain. Corona-Figueroa et al. [28] adapted [23] to synthesize novel projection views with training on multi-view projection images, while Chen et al. [29] proposed the cube-based modeling strategy to reform NeRF [18] for upsampling medical images at arbitrary scales."
        },
        {
            "heading": "B. Sparse-View Computed Tomography Reconstruction",
            "text": "Sparse-View Computed Tomography (SVCT) reconstruction aims to acquire CT images based on sparsely-sampled measurements, which can significantly reduce ionizing radiation.\nInitial studies proposed analytical reconstruction methods: the Filtered Back-Projection (FBP) [1], and the iterative methods: [2] to transform the sinograms into CT images. However, these methods have difficulty in obtaining high-quality CT images from the sparse-view (SV) sinograms and produce severe artifacts in their results. To eliminate the artifacts, [8], [9] combined [2] with the regularization terms for optimization. With the advent of Convolutional Neural Networks (CNNs), [10]\u2013[17] built the CNN-based models to address the challenges of SVCT reconstruction. [10]\u2013[13] reformed the U-Net [30] architecture to learn the mapping between the sparseand full-view CT pairs on a large dataset, while [14], [15] designed dense blocks to fuse hierarchical features. Recent works [4]\u2013[6], [20], [31] use INR techniques to construct the coordinate-based mapping between SV sinograms and CT images. These INR-based methods can be divided into two groups: i) [4], [20], [31] simulate the density field of CT images and synthesize SV sinograms via differentiable FBP [1] techniques for optimization; ii) [5], [6] build the coordinate-based representations of SV sinograms and apply FBP [1] on the synthesized DV sinograms to acquire CT images."
        },
        {
            "heading": "III. METHOD",
            "text": "In this section, we first analyze the reasons why existing INR-based methods suffer from aliasing errors in the projection domains. Subsequently, we propose Anti-Aliasing Projection Representation Field (APRF) to address this problem, which consists of two modules: line-segment sampling and center-based line integral. These two modules are introduced in the following subsections, and we also demonstrate why the aliasing errors can be eliminated by them. Finally, we report the details of our methods, including spatial normalization and the architecture of Multi-Layer Perceptron (MLP).\nThe overall architecture is depicted in Figure 2. As shown, in the training stage, APRF trains the models on a single SV sinogram itself. APRF employs the proposed line-segment sampling module (a) and center-based line integral module (b) to learn the mapping between the sparsely-sampled coordinates and the corresponding sparse-view (SV) sinogram pixels. In the test stage, given a dense coordinate set, APRF first synthesize the corresponding DV sinograms via (a) and (b), and then applies FBP [1] on the predicted results to reconstruct CT images."
        },
        {
            "heading": "A. Analysis on Aliasing Errors in the Projection Domains",
            "text": "Existing INR-based SVCT reconstruction methods aim to build the mapping between position coordinates and sinogram pixels. However, we discover that these methods have not considered building the correlation between adjacent projections, leading to aliasing errors in the projection domains. We provide a vivid example in Figure 3 (a) to explain our findings. As shown, these INR-based methods can only model the points corresponding to sparsely-sampled projection angles, while the internal regions between adjacent projection views have never been directly modeled during training. These unmodeled spaces may cause aliasing errors in building dense projection\nviews, yielding blurred results [4], [20] and severe artifacts [5], [6] (see Figure 1).\nTo alleviate aliasing errors, we argue that it is required to build the correlation between adjacent projection views. As shown in Figure 3 (b), sampling a line-segment region instead of a single point can build the distance-based correlation within the unmodeled spaces. After optimization, the model can build the continuous representation between adjacent projections, reducing the aliasing errors.\nTo achieve our motivation, we propose line-segment sampling module to sample projection angles within a linesegment region. We also propose a center-based line integral module to synthesize corresponding sinogram values. After optimization, the internal regions between adjacent SV projection views can be indirectly modeled by the spatial constraints, and thus APRF can yield the dense-view (DV) sinogram with consistent continuity."
        },
        {
            "heading": "B. Line-Segment Sampling",
            "text": "Implicit neural representation (INR) techniques aim to learn the continuous representation of projection views from sparseview (SV) sinograms. However, the above analysis demonstrates that existing INR-based SVCT methods suffer from aliasing errors in the projection domains, leading to blurry results and severe artifacts.\nTo address this issue, we propose a novel sampling strategy: line-segment sampling, which samples a line-segment region instead of a single point to build the correlation between adjacent projection views. Specifically, to predict the sinogram pixel at the location Ot, we construct a line-segment region \u2113(Ot, \u03c1) with the center of Ot, where \u03c1 is the length of that line segment. To estimate the distribution of \u2113(Ot, \u03c1), we first samples N points within that sampled region as a point set {xi}Ni=1. Each point is sampled by:\nxi \u223c U [\u2113(Ot, \u03c1)] , (1)\nwhere U denotes the uniform distribution. Then we feed the point set {xi}Ni=1 and the center Ot into a multi-layer perceptron (MLP) network F\u0398 to predict a set of density {\u03c3i}Ni=1 and intensity {ci}Ni=1 within the sampled region \u2113(Ot, \u03c1) by:\n\u03c3i, ci = F\u0398(\u03b3(Ot), \u03b3(xi)), (2)\nwhere \u03b3(\u00b7) denotes the positional encoding introduced in [18], which can be formulated as:\n\u03b3(y) = y \u03c9\u22121\u22c3 i=0 (sin(2iy), cos(2iy)), where \u03c9 \u2208 N, (3)\nwhere y is an arbitrary input vector, \u03c9 is set to 10 as default. Finally, since \u03c3i and ci denote the density and intensity of xi, we can build the continuous function \u03c3(Ot, \u00b7) and c(\u00b7) to estimate the distribution of the line-segment regions \u2113(Ot, \u03c1) via these discrete samples. Note the density function is related to (Ot, xi) while the intensity function is only related to xi, because we assume the density distribution of each linesegment region is independent, while the intensity of all points is fixed."
        },
        {
            "heading": "C. Center-based Line Integral",
            "text": "To obtain the sinogram pixel C(Ot, \u03c1) at Ot, an intuitive solution is to calculate the line integral as:\nC(Ot, \u03c1) = 1 \u03c1 \u2211\n\u03c3i \u222b \u03c1 0 \u03c3(Ot, \u03bd)c(Ot + \u03bd)d\u03bd, (4)\nwhere \u03bd is the distance between each sampling point xi to the starting point Ot \u2212 \u03c12 , and 1 \u03c1 \u2211 \u03c3i is the normalization term to ensure the integral output within a reasonable range. However, Eq. 4 neglects the image continuity priors within the sampling region \u2113(Ot, \u03c1), leading to sub-optimal results.\nInspired by [32] that assigns the nearby pixels with similar potentials, we assume the density of each point xi within the line-segment region \u2113(Ot, \u03c1) is attenuated from the center Ot to the ends. We employ the Lambert-Beer Law to estimate\nthe distribution of attenuation coefficients within the linesegment region \u2113(Ot, \u03c1), and thus the proposed center-based line integral can be formulated as:\nC(Ot, \u03c1) =\n\u222b \u03c1 2\n0\nc(Ot + \u03bd)(1\u2212 exp(\u2212\u03c3(Ot, \u03bd))) exp( \u222b \u03bd 0 \u03c3(Ot, \u03bd\u2032))d\u03bd\u2032 d\u03bd, (5)\nwhere \u03bd = \u2225Ot \u2212 xi\u2225 denotes the distance between each sampling point xi and the center Ot. Given N sampling points by the proposed line-segment sampling, we first sort these points by the distance between the center Ot and themselves. Subsequently, the above center-based line integral can be approximated via numerical quadrature rules:\nC\u0302(Ot, \u03c1) = N\u22121\u2211 i=1 1\u2212 exp(\u2212\u03c3(Ot, \u03bdi)(\u03bdi+1 \u2212 \u03bdi)) exp( \u2211i j=1 \u03c3(Ot, \u03bdj)(\u03bdj+1 \u2212 \u03bdj)) c(Ot+\u03bdi), (6) where C\u0302(Ot, \u03c1) denotes the approximated results of C(Ot, \u03c1).\nGiven the predicted result C\u0302(Ot, \u03c1) at Eq. 6 using N sorted sampling points, APRF can be optimized in the following MSE loss L:\nL = 1 len(B) \u2211 Ot\u2208B \u2225g.t.\u2212 C\u0302(Ot, \u03c1)\u222522, (7)\nwhere B denotes the SV coordinates in a batch, and g.t. is the corresponding SV sinogram pixels."
        },
        {
            "heading": "D. Spatial Normalization & Multi-Layer Perceptron",
            "text": "Spatial Normalization. To build the coordinate-based representations field for the given SV sinograms, we first normalize the coordinates from the projection domains to the field spaces within the range of (\u22121, 1) to adapt the positional encoding \u03b3(\u00b7) in Eq. 3. For example, given a L\u00d7H\u00d7W 3D sinogram, we convert the sinogram coordinate (l, h, w) into the corresponding field coordinate Ot by:\nOt = ( 2l \u2212 L L+ 2P , 2h\u2212H H + 2P , 2w \u2212W W + 2P , ) (8)\nwhere P set to 1 is a padding size to restrict Ot within the range of (\u22121, 1), L denotes the number of projection views,\nH and W indicate the height and width of detector plane. The Architecture of Multi-Layer Perceptron. We reform the multi-layer perceptron (MLP) architecture introduced in [18] to adapt the proposed line-segment sampling. As shown in Figure 4, we employ the positional encoding in Eq. 3 to enrich the feature of Ot and xi before feeding them into the MLP. Since \u03c3i is related to (Ot, xi) while ci is only corresponding to xi, we decouple the features of \u03c3i and ci. Specifically, we first feed \u03b3(xi) into the MLP to predict the intensity ci, and then we concatenate the features of intensity and the input center \u03b3(Ot) to predict the density \u03c3i. Because the proposed center-based line integral is differentiable, the MLP can be optimized by minimizing the MSE loss L in Eq. 7 between the predicted results and SV sinogram pixels."
        },
        {
            "heading": "IV. EXPERIMENTS",
            "text": "In this section, we conduct extensive experiments and indepth analysis to demonstrate the superiority of the proposed APRF for sparse-view computed tomography (SVCT) reconstruction. We utilize the same hyperparameters and model settings in all experiments for fair comparisons."
        },
        {
            "heading": "A. Experimental Details",
            "text": "Compared Methods. We conduct a comprehensive comparison over 6 state-of-the-art methods, including 1 conventional analytical reconstruction method: FBP [1]; 1 fully-supervised deep learning-based method: FBPConvNet [10]; and 4 recent self-supervised INR-based methods: CoIL [5], SCOPE [6], GRFF [20] and NeRP [4]. FBP is re-implemented by ODL [33], we follow the settings released on the official tutorials1 to deal with the re-projection of parallel, fan and cone Xray beam sinograms. For the fully-supervised methods: FBPConvNet2 [10], we use the MatConvNet toolbox [34] to train them on the training set extracted from COVID-19 [7] dataset. Specifically, for each X-ray beam (parallel, fan and cone Xray beams) and projection number (45, 60, and 90), we re-train a specific model for the corresponding CT reconstruction. The maximum iteration is set to 100 (epochs) and the learning rate is decreasing logarithmically from 1e\u22122 to 1e\u22123. We employ Adam [35] as the optimizer and batch size is set to 8. For the self-supervised INR-based methods, we directly train them on a single SV sinogram itself to reconstruct CT images. Since the output of CoIL [5], SCOPE [6] and our APRF is a DV\n1https://github.com/odlgroup/odl/tree/master/examples/tomo 2https://github.com/panakino/FBPConvNet\nsinogram, we apply the same re-projection process on the predicted DV sinograms to obtain the CT images. Note that the results of CoIL3 [5], SCOPE4 [6], GRFF5 [20] and NeRP6 [4] are obtained using their official implementations. Datasets. The experimental data are selected from 2 publicly available CT datasets: COVID-19 [7] and 2019 Kidney Tumor Segmentation Challenge (KiTS19) [21] datasets. Since the proposed APRF is the self-supervised method, i.e., only trained on the test CT image itself at test time without the demands of large training data, we do not require to build an extra dataset for training.\n1) COVID-19 [7] dataset, containing considerable 3D CT volumes from 1000+ subjects with confirmed COVID-19 infections, is a large-scale CT dataset for the clinical study of COVID-19 virus. We randomly select 600 2D slices from 100 subjects, each slice is extracted on axial view from the 3D CT volumes. Specifically, we select 500 slices from 50 subjects as the training set, 60 2D slices from 10 subjects as the validation set, and 40 2D slices from the rest 40 subjects as the test set. All the 2D CT slices have the same image dimension of 512\u00d7512. Note that the training and validation sets are only employed to train the fully-supervised methods: FBPConvNet [10], while FBP [1], CoIL [5], SCOPE [6], GRFF [20], NeRP [4] and the proposed APRF, are only trained on the test SV sinograms itself to reconstruct CT images.\n2) 2019 Kidney Tumor Segmentation Challenge (KiTS19) [21] dataset consists of arterial phase abdominal CT scans from 300 unique kidney cancer subjects who underwent partial or radical nephrectomy. Note that FBPConvNet [10] is only designed for 2D SVCT reconstruction, and thus we only prepare the test set to evaluate the performance of FBP [1] and other self-supervised INR-based methods. Specifically, we randomly select 40 3D CT volumes, each 3D CT volume is resized into 256\u00d7256\u00d780 image dimension. Dataset Simulation. For thoroughly evaluating the reconstruction quality of SVCT with various X-ray beams, we follow the strategies in [4], [5] to simulate parallel, fan and cone Xray beam sparse-view (SV) sinograms by projecting the raw CT images using Operator Discretization Library (ODL) [33]. Specifically, we follow the above-mentioned official settings to extract parallel- and fan-beam SV sinograms from 2D CT slices, and generate parallel- and cone-beam SV sinograms\n3https://github.com/wustl-cig/Cooridnate-based-Internal-Learning 4https://github.com/iwuqing/SCOPE 5https://github.com/tancik/fourier-feature-networks 6https://github.com/liyues/NeRP\nfrom 3D CT volumes. Each SV sinogram is generated at different projection views (45, 60, and 90). Following the selfsupervised settings, APRF is trained on a test SV sinogram itself, while the raw CT images are only used as ground truths for evaluation. Note that the parallel, fan and cone X-ray beam SVCT are considered as three independent reconstruction tasks, and thus we solely conduct the training and test processes of each X-ray beam. Evaluation Metrics. We employ three commonly-used objective image quality metrics: Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM) [36], and LPIPS [37] to evaluate the performance of the compared methods for 2D and 3D SVCT reconstruction. PSNR is based on pixel-to-pixel distance while SSIM utilizes the mean and variance of images to measure structural similarity. Unlike the above conventional metrics, LPIPS is an objective perceptual\nsimilarity metric based on deep learning. In our experiments, PSNR and SSIM are re-implemented by scikit-image [38] packages, while we use the default settings of LPIPS for evaluations.\nImplementation Details. Our method is implemented on top of [39], a Pytorch [40] re-implementation of NeRF [18]. Meanwhile, our experiments are also based on Pytorch [40] framework, and run on a single NVIDIA RTX A6000 GPU. The length \u03c1 of the line segment \u2113(Ot, \u03c1) is set to 2L+P , which means it depends on the number of projection views (i.e., long in SV sinograms but short in DV ones).\nFor training, we sample Ntrain = 65 points within a line segment \u2113(Ot, \u03c1) and then feed the sampling points into the multi-layer perceptron (MLP) F\u0398. We employ Adam [35] as the optimizer with a weight decay 1e\u22127 and a batch size 2048. The maximum iteration is 40000 for all CT images and the learning rate is annealed logarithmically from 3e\u22123 to 2e\u22125.\nFor test, we first generate DV sinograms by uniformly sampling 720 partitions within [0, \u03c0], and then apply FBP [1] re-implemented by ODL [33] to reconstruct the CT images.\nThe number of sampling points is set to Ntest = (Ntrain\u22121)\u03b7\n720 + 1, where \u03b7 is the number of projection views preseted in training. It means the number of test sampling points is 16sim8 times smaller than in training for the 45\u223c90-view sinograms, reducing considerable computational costs. Runtime & Parameters. The average training time of the proposed APRF for 2D SVCT reconstruction is about 5 mins while training for reconstructing 3D SVCT demands about 15 mins. Since the number of sampling points in the test stage is much smaller than in training, the inference time for a single 2D SV sinogram with 800\u00d7512 size is only about 1.4 secs, while synthesizing a 3D SV sinogram with 400\u00d7400\u00d780 size requires about 40 secs. It is worth noting that APRF is a lightweight model, which only contains 0.55M parameters."
        },
        {
            "heading": "B. Comparison with State-of-the-art Methods",
            "text": "In this subsection, we first report the quantitative and visual comparisons between the proposed APRF and the 6 above-mentioned state-of-the-art methods. Subsequently, we summarise and analyze the experimental results.\nQuantitative Comparison. As reported in Tables I and II, the proposed APRF favorably surpasses all the competitors for 2D and 3D SVCT reconstruction, yielding consistent preferable performance under the various number of projection views. The outperformance suggests APRF achieves better sample efficiency than state-of-the-art methods, obtaining higher reconstruction quality under the same number of projection views. Especially, compared to the exhibited methods, the performance of APRF has significant growth with the increasing number of projection views. Moreover, the experimental results also demonstrate that APRF can handle various X-ray beams, outperforming the other methods for parallel-, fan- and cone-beam SVCT reconstruction. Visual Comparison. We visualize the results of APRF and other competitors for 2D and 3D SVCT reconstruction in Figures 5 and 6, respectively. As shown, the difference maps of GRFF [20] and NeRP [4] contain conspicuous artifacts at object boundaries, suggesting GRFF [20] and NeRP [4] generate blurry contents in their results. In contrast, though CoIL [5] and SCOPE [6] preserve more details, they also produce severe artifacts in the inner regions of objects. Compared to the exhibited methods, the proposed APRF achieves better visual verisimilitude to the GT images, producing more accurate details and fewer artifacts."
        },
        {
            "heading": "C. Ablation Study & Parametric Sensitivity Analysis",
            "text": "In this subsection, we conduct comprehensive experiments to prove the correctness of the model design. We first carry out an ablation study to investigate the effectiveness of the proposed modules. Subsequently, we evaluate the APRF\u2019s\nparametric sensitivity under different parameter settings. For fair comparisons, each variant is trained with the same experimental settings, including the same positional encoding in Eq. 3, loss in Eq. 7, and the maximum iterations.\nAPRF\u2019s Ablation Variants. We conduct a thorough evaluation against ablation variants of the proposed APRF with each module: LSS, CLI, cnt. and dec., which indicate the linesegment sampling, center-based line integral, adding the center Ot into input vector, and decoupling the density and intensity features in MLP, respectively. The baseline model is directly feeding the points to synthesize the corresponding sinogram pixels. For fair comparisons, the MLP of each ablation variant has a similar parameter size (\u00b10.02M). As reported in Table III, our baseline model (row 1) can be conspicuously improved by the proposed LSS (row 2), outperforming the most stateof-the-art methods on 60 projection views. Employing the proposed CLI instead of the line integral introduced in Eq. 4 can also significantly enhance the reconstruction performance (row 3). Besides, since cnt. and dec. (rows 4 and 5) can further improve the reconstruction quality under similar computational costs, demonstrating the effectiveness of our modification against input vectors and MLP architecture.\nAPRF under Different Parameteric Settings. To analyze the parameteric sensitivity of the proposed APRF, we evaluate its performance under different parameteric settings: \u201c\u03c1= 1L+P \u201d, \u201c\u03c1 = 4L+P \u201d, and \u201cNtest = Ntrain\u201d, where \u201c\u03c1 = 1 L+P \u201d and \u201c\u03c1= 4L+P \u201d denote the different length of line-segment regions \u2113(Ot, \u03c1), while \u201cNtest = Ntrain\u201d indicates using the same number of sampling points as Ntrain in test stage. The default setting is introduced in Section IV-A, where \u03c1 = 2L+P and\nNtest = (Ntrain\u22121)\u03b7\n720 +1. Since the default length can only cover the whole coordinate-based representation fields, the 0.5\u00d7 length: \u201c\u03c1= 1L+P \u201d may leave some unmodeled spaces in the fields, while the 2\u00d7 length: \u201c\u03c1= 4L+P \u201d may produce blurry results. Besides, the default number of Ntest is 8\u223c16\u00d7 smaller than Ntrain for 45\u223c90 projection views. As reported in Table IV, the default version (row 1) achieves consistent preferable performance at various X-ray beams. As expected, \u201c\u03c1 = 1L+P \u201d and \u201c\u03c1 = 4 L+P \u201d (rows 2 and 3) are inferior to the default one, but they still achieve comparable performance to state-of-the-art methods on 60 projection views. Though \u201cNtest=Ntrain\u201d can slightly improve the default performance (row 4), it takes 8\u223c16 multiple of computational costs to deal with 45\u223c90 projection views, which is not cost-efficient."
        },
        {
            "heading": "D. Analysis",
            "text": "We compare our APRF with 6 state-of-the-art methods in Section IV-B. The performance on quantitative and visual comparisons confirms the correctness of our motivation and demonstrates the effectiveness of the proposed APRF. It is also worth noting that APRF can deal with different projection settings, including different types of X-ray beams and the various number of projection views, which means APRF acquires high-quality CT images under various situations. Moreover, we also thoroughly evaluate the performance of our APRF under different ablations and parametric settings in Section IV-C. The ablation results demonstrate the effectiveness of our model designs, while the consistent preferable performance under different parametric settings suggests our APRF is a parameter-insensitive method. Compared to stateof-the-art methods, APRF achieves superior performance and better robustness against various situations, which has broader application scenarios in practice."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "In this paper, we observed that existing Implicit Neural Representation-based methods suffer from aliasing issues in the projection domains, leading to blurry results and severe artifacts. Based on our findings, we propose a novel method \u2013 Anti-Aliasing Projection Representation Field (APRF), which aims to alleviate the aliasing errors in reconstructing CT images from sparse-view (SV) sinograms. Specifically, APRF first employs line-segment sampling to estimate the distribution of projections within a line segment. Then, APRF synthesize the corresponding sinogram values using centerbased line integral. After training APRF on a single SV sinogram itself, the internal regions between adjacent projection views can be modeled by spatial constraints. As a result, APRF can synthesize the corresponding dense-view sinograms with consistent continuity and yield high-quality CT images via the re-projection techniques. Comprehensive experiments on CT images demonstrate that APRF surpasses state-of-theart methods for sparse-view reconstruction, producing better visual verisimilitude and fewer artifacts.\nREFERENCES [1] A. C. Kak and M. Slaney, Principles of computerized tomographic\nimaging. SIAM, 2001.\n[2] R. Gordon, R. Bender, and G. T. Herman, \u201cAlgebraic reconstruction techniques (art) for three-dimensional electron microscopy and x-ray photography,\u201d Journal of theoretical Biology, vol. 29, no. 3, pp. 471\u2013 481, 1970. [3] D. R. Martin and R. C. Semelka, \u201cHealth effects of ionising radiation from diagnostic ct,\u201d The Lancet, vol. 367, no. 9524, pp. 1712\u20131714, 2006. [4] L. Shen, J. Pauly, and L. Xing, \u201cNerp: implicit neural representation learning with prior embedding for sparsely sampled image reconstruction,\u201d IEEE Transactions on Neural Networks and Learning Systems, 2022. [5] Y. Sun, J. Liu, M. Xie, B. Wohlberg, and U. S. Kamilov, \u201cCoil: Coordinate-based internal learning for tomographic imaging,\u201d IEEE Transactions on Computational Imaging, vol. 7, pp. 1400\u20131412, 2021. [6] Q. Wu, R. Feng, H. Wei, J. Yu, and Y. Zhang, \u201cSelf-supervised coordinate projection network for sparse-view computed tomography,\u201d IEEE Transactions on Computational Imaging, 2023. [7] S. Shakouri, M. A. Bakhshali, P. Layegh, B. Kiani, F. Masoumi, S. Ataei Nakhaei, and S. M. Mostafavi, \u201cCovid19-ct-dataset: an openaccess chest ct image repository of 1000+ patients with confirmed covid19 diagnosis,\u201d BMC Research Notes, vol. 14, no. 1, p. 178, 2021. [8] J. Huang, Y. Zhang, J. Ma, D. Zeng, Z. Bian, S. Niu, Q. Feng, Z. Liang, and W. Chen, \u201cIterative image reconstruction for sparse-view ct using normal-dose image induced total variation prior,\u201d PloS one, vol. 8, no. 11, p. e79709, 2013. [9] S. Niu, Y. Gao, Z. Bian, J. Huang, W. Chen, G. Yu, Z. Liang, and J. Ma, \u201cSparse-view x-ray ct reconstruction via total generalized variation regularization,\u201d Physics in Medicine & Biology, vol. 59, no. 12, p. 2997, 2014. [10] K. H. Jin, M. T. McCann, E. Froustey, and M. Unser, \u201cDeep convolutional neural network for inverse problems in imaging,\u201d IEEE Transactions on Image Processing, vol. 26, no. 9, pp. 4509\u20134522, 2017. [11] Y. Han and J. C. Ye, \u201cFraming u-net via deep convolutional framelets: Application to sparse-view ct,\u201d IEEE transactions on medical imaging, vol. 37, no. 6, pp. 1418\u20131429, 2018. [12] H. Zhang, B. Liu, H. Yu, and B. Dong, \u201cMetainv-net: Meta inversion network for sparse view ct image reconstruction,\u201d IEEE Transactions on Medical Imaging, vol. 40, no. 2, pp. 621\u2013634, 2020. [13] W. Wu, D. Hu, C. Niu, H. Yu, V. Vardhanabhuti, and G. Wang, \u201cDrone: Dual-domain residual-based optimization network for sparse-view ct reconstruction,\u201d IEEE Transactions on Medical Imaging, vol. 40, no. 11, pp. 3002\u20133014, 2021. [14] Z. Zhang, X. Liang, X. Dong, Y. Xie, and G. Cao, \u201cA sparse-view ct reconstruction method based on combination of densenet and deconvolution,\u201d IEEE transactions on medical imaging, vol. 37, no. 6, pp. 1407\u20131417, 2018. [15] B. Zhou, S. K. Zhou, J. S. Duncan, and C. Liu, \u201cLimited view tomographic reconstruction using a cascaded residual dense spatialchannel attention network with projection data fidelity layer,\u201d IEEE transactions on medical imaging, vol. 40, no. 7, pp. 1792\u20131804, 2021. [16] Y. Li, K. Li, C. Zhang, J. Montoya, and G.-H. Chen, \u201cLearning to reconstruct computed tomography images directly from sinogram data under a variety of data acquisition conditions,\u201d IEEE transactions on medical imaging, vol. 38, no. 10, pp. 2469\u20132481, 2019. [17] J. Xiang, Y. Dong, and Y. Yang, \u201cFista-net: Learning a fast iterative shrinkage thresholding network for inverse problems in imaging,\u201d IEEE Transactions on Medical Imaging, vol. 40, no. 5, pp. 1329\u20131339, 2021. [18] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, \u201cNerf: Representing scenes as neural radiance fields for view synthesis,\u201d in European conference on computer vision. Springer, 2020, pp. 405\u2013421. [19] Y. Chen, S. Liu, and X. Wang, \u201cLearning continuous image representation with local implicit image function,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 8628\u20138638. [20] M. Tancik, P. Srinivasan, B. Mildenhall, S. Fridovich-Keil, N. Raghavan, U. Singhal, R. Ramamoorthi, J. Barron, and R. Ng, \u201cFourier features let networks learn high frequency functions in low dimensional domains,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 7537\u2013 7547, 2020. [21] N. Heller, N. Sathianathen, A. Kalapara, E. Walczak, K. Moore, H. Kaluzniak, J. Rosenberg, P. Blake, Z. Rengel, M. Oestreich et al., \u201cThe kits19 challenge data: 300 kidney tumor cases with clinical context, ct semantic segmentations, and surgical outcomes,\u201d arXiv preprint arXiv:1904.00445, 2019. [22] Z. Chen, Y. Chen, J. Liu, X. Xu, V. Goel, Z. Wang, H. Shi, and X. Wang, \u201cVideoinr: Learning video implicit neural representation for continuous\nspace-time super-resolution,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 2047\u20132057. [23] K. Schwarz, Y. Liao, M. Niemeyer, and A. Geiger, \u201cGraf: Generative radiance fields for 3d-aware image synthesis,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 20 154\u201320 166, 2020. [24] M. Niemeyer and A. Geiger, \u201cGiraffe: Representing scenes as compositional generative neural feature fields,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 11 453\u201311 464. [25] B. Poole, A. Jain, J. T. Barron, and B. Mildenhall, \u201cDreamfusion: Text-to-3d using 2d diffusion,\u201d in The Eleventh International Conference on Learning Representations, 2023. [Online]. Available: https://openreview.net/forum?id=FjNys5c7VyY [26] Y.-J. Yuan, Y.-T. Sun, Y.-K. Lai, Y. Ma, R. Jia, and L. Gao, \u201cNerfediting: geometry editing of neural radiance fields,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 18 353\u201318 364. [27] C. Bao, Y. Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui, \u201cSine: Semantic-driven image-based nerf editing with priorguided editing field,\u201d in Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR), 2023. [28] A. Corona-Figueroa, J. Frawley, S. Bond-Taylor, S. Bethapudi, H. P. Shum, and C. G. Willcocks, \u201cMednerf: Medical neural radiance fields for reconstructing 3d-aware ct-projections from a single x-ray,\u201d in 2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC). IEEE, 2022, pp. 3843\u20133848. [29] Z. Chen, J. Lai, L. Yang, and X. Xie, \u201cCunerf: Cube-based neural radiance field for zero-shot medical image arbitrary-scale super resolution,\u201d arXiv preprint arXiv:2303.16242, 2023. [30] O. Ronneberger, P. Fischer, and T. Brox, \u201cU-net: Convolutional networks for biomedical image segmentation,\u201d in Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer, 2015, pp. 234\u2013241. [31] G. Zang, R. Idoughi, R. Li, P. Wonka, and W. Heidrich, \u201cIntratomo: self-supervised learning-based tomography via sinogram synthesis and prediction,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 1960\u20131970. [32] P. Kra\u0308henbu\u0308hl and V. Koltun, \u201cEfficient inference in fully connected crfs with gaussian edge potentials,\u201d Advances in neural information processing systems, vol. 24, 2011. [33] J. Adler, H. Kohr, and O. O\u0308ktem, \u201cOperator discretization library (odl),\u201d Zenodo, 2017. [34] A. Vedaldi and K. Lenc, \u201cMatconvnet: Convolutional neural networks for matlab,\u201d in Proceedings of the 23rd ACM international conference on Multimedia, 2015, pp. 689\u2013692. [35] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014. [36] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, \u201cImage quality assessment: from error visibility to structural similarity,\u201d IEEE transactions on image processing, vol. 13, no. 4, pp. 600\u2013612, 2004. [37] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, \u201cThe unreasonable effectiveness of deep features as a perceptual metric,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 586\u2013595. [38] S. van der Walt, J. L. Scho\u0308nberger, J. Nunez-Iglesias, F. Boulogne, J. D. Warner, N. Yager, E. Gouillart, T. Yu, and the scikit-image contributors, \u201cscikit-image: image processing in Python,\u201d PeerJ, vol. 2, p. e453, 6 2014. [Online]. Available: https://doi.org/10.7717/peerj.453 [39] L. Yen-Chen, \u201cNerf-pytorch,\u201d https://github.com/yenchenlin/ nerf-pytorch/, 2020. [40] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer, \u201cAutomatic differentiation in pytorch,\u201d in Proceedings of Neural Information Processing Systems (NIPS), 2017."
        }
    ],
    "title": "APRF: Anti-Aliasing Projection Representation Field for Inverse Problem in Imaging",
    "year": 2023
}