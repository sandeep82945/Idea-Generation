{
    "abstractText": "While generative modeling has been ubiquitous in natural language processing and computer vision, its application to image retrieval remains unexplored. In this paper, we recast image retrieval as a form of generative modeling by employing a sequence-to-sequence model, contributing to the current unified theme. Our framework, IRGen, is a unified model that enables end-to-end differentiable search, thus achieving superior performance thanks to direct optimization. While developing IRGen we tackle the key technical challenge of converting an image into quite a short sequence of semantic units in order to enable efficient and effective retrieval. Empirical experiments demonstrate that our model yields significant improvement over three commonly used benchmarks, for example, 20.2% higher than the best baseline method in precision@10 on In-shop dataset with comparable recall@10 score.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yidan Zhang"
        },
        {
            "affiliations": [],
            "name": "Ting Zhang"
        },
        {
            "affiliations": [],
            "name": "Dong Chen"
        },
        {
            "affiliations": [],
            "name": "Yujing Wang"
        },
        {
            "affiliations": [],
            "name": "Qi Chen"
        },
        {
            "affiliations": [],
            "name": "Xing Xie"
        },
        {
            "affiliations": [],
            "name": "Hao Sun"
        },
        {
            "affiliations": [],
            "name": "Weiwei Deng"
        },
        {
            "affiliations": [],
            "name": "Qi Zhang"
        },
        {
            "affiliations": [],
            "name": "Fan Yang"
        },
        {
            "affiliations": [],
            "name": "Mao Yang"
        },
        {
            "affiliations": [],
            "name": "Qingmin Liao"
        },
        {
            "affiliations": [],
            "name": "Baining Guo"
        }
    ],
    "id": "SP:33323e311f5248dfe76230acaa904b0350a73a0b",
    "references": [
        {
            "authors": [
                "Daniel Adiwardana",
                "Minh-Thang Luong",
                "David R So",
                "Jamie Hall",
                "Noah Fiedel",
                "Romal Thoppilan",
                "Zi Yang",
                "Apoorv Kulshreshtha",
                "Gaurav Nemade",
                "Yifeng Lu"
            ],
            "title": "Towards a human-like open-domain chatbot",
            "year": 2001
        },
        {
            "authors": [
                "Jean-Baptiste Alayrac",
                "Jeff Donahue",
                "Pauline Luc",
                "Antoine Miech",
                "Iain Barr",
                "Yana Hasson",
                "Karel Lenc",
                "Arthur Mensch",
                "Katie Millican",
                "Malcolm Reynolds"
            ],
            "title": "Flamingo: a visual language model for few-shot learning",
            "venue": "arXiv preprint arXiv:2204.14198,",
            "year": 2022
        },
        {
            "authors": [
                "Ahmad Alzu\u2019bi",
                "Abbes Amira",
                "Naeem Ramzan"
            ],
            "title": "Semantic content-based image retrieval: A comprehensive study",
            "venue": "Journal of Visual Communication and Image Representation,",
            "year": 2015
        },
        {
            "authors": [
                "Alexandr Andoni",
                "Piotr Indyk"
            ],
            "title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions",
            "venue": "Communications of the ACM,",
            "year": 2008
        },
        {
            "authors": [
                "Artem Babenko",
                "Victor Lempitsky"
            ],
            "title": "Additive quantization for extreme vector compression",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2014
        },
        {
            "authors": [
                "Artem Babenko",
                "Victor Lempitsky"
            ],
            "title": "The inverted multiindex",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2014
        },
        {
            "authors": [
                "Song Bai",
                "Peng Tang",
                "Philip HS Torr",
                "Longin Jan Latecki"
            ],
            "title": "Re-ranking via metric fusion for object retrieval and person re-identification",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Herbert Bay",
                "Tinne Tuytelaars",
                "Luc Van Gool"
            ],
            "title": "Surf: Speeded up robust features",
            "venue": "In European conference on computer vision,",
            "year": 2006
        },
        {
            "authors": [
                "Yoshua Bengio",
                "Nicholas L\u00e9onard",
                "Aaron Courville"
            ],
            "title": "Estimating or propagating gradients through stochastic neurons for conditional computation",
            "venue": "arXiv preprint arXiv:1308.3432,",
            "year": 2013
        },
        {
            "authors": [
                "Jon Louis Bentley"
            ],
            "title": "K-d trees for semidynamic point sets",
            "venue": "In Proceedings of the sixth annual symposium on Computational geometry,",
            "year": 1990
        },
        {
            "authors": [
                "Michele Bevilacqua",
                "Giuseppe Ottaviano",
                "Patrick Lewis",
                "Wen-tau Yih",
                "Sebastian Riedel",
                "Fabio Petroni"
            ],
            "title": "Autoregressive search engines: Generating substrings as document identifiers",
            "venue": "arXiv preprint arXiv:2204.10628,",
            "year": 2022
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Bingyi Cao",
                "Andre Araujo",
                "Jack Sim"
            ],
            "title": "Unifying deep local and global features for image search",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Yue Cao",
                "Mingsheng Long",
                "Jianmin Wang",
                "Shichen Liu"
            ],
            "title": "Collective deep quantization for efficient cross-modal retrieval",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Yang Cao",
                "Changhu Wang",
                "Liqing Zhang",
                "Lei Zhang"
            ],
            "title": "Edgel index for large-scale sketch-based image search",
            "year": 2011
        },
        {
            "authors": [
                "Mark Chen",
                "Alec Radford",
                "Rewon Child",
                "Jeffrey Wu",
                "Heewoo Jun",
                "David Luan",
                "Ilya Sutskever"
            ],
            "title": "Generative pretraining from pixels",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Qi Chen",
                "Bing Zhao",
                "Haidong Wang",
                "Mingqin Li",
                "Chuanjie Liu",
                "Zengzhong Li",
                "Mao Yang",
                "Jingdong Wang"
            ],
            "title": "Spann: Highly-efficient billion-scale approximate nearest neighborhood search",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jaemin Cho",
                "Jiasen Lu",
                "Dustin Schwenk",
                "Hannaneh Hajishirzi",
                "Aniruddha Kembhavi"
            ],
            "title": "X-lxmert: Paint, caption and answer questions with multi-modal transformers",
            "venue": "arXiv preprint arXiv:2009.11278,",
            "year": 2020
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "arXiv preprint arXiv:2204.02311,",
            "year": 2022
        },
        {
            "authors": [
                "Ondrej Chum",
                "James Philbin",
                "Andrew Zisserman"
            ],
            "title": "Near duplicate image detection: Min-hash and tf-idf weighting",
            "venue": "In Bmvc,",
            "year": 2008
        },
        {
            "authors": [
                "Yu-An Chung",
                "Wei-Ning Hsu",
                "Hao Tang",
                "James Glass"
            ],
            "title": "An unsupervised autoregressive model for speech representation learning",
            "venue": "arXiv preprint arXiv:1904.03240,",
            "year": 2019
        },
        {
            "authors": [
                "Nicola De Cao",
                "Wilker Aziz",
                "Ivan Titov"
            ],
            "title": "Highly parallel autoregressive entity linking with discriminative correction",
            "venue": "arXiv preprint arXiv:2109.03792,",
            "year": 2021
        },
        {
            "authors": [
                "Nicola De Cao",
                "Gautier Izacard",
                "Sebastian Riedel",
                "Fabio Petroni"
            ],
            "title": "Autoregressive entity retrieval",
            "venue": "arXiv preprint arXiv:2010.00904,",
            "year": 2020
        },
        {
            "authors": [
                "Nicola De Cao",
                "Ledell Wu",
                "Kashyap Popat",
                "Mikel Artetxe",
                "Naman Goyal",
                "Mikhail Plekhanov",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Sebastian Riedel",
                "Fabio Petroni"
            ],
            "title": "Multilingual autoregressive entity linking",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Heewoo Jun",
                "Christine Payne",
                "Jong Wook Kim",
                "Alec Radford",
                "Ilya Sutskever"
            ],
            "title": "Jukebox: A generative model for music",
            "venue": "arXiv preprint arXiv:2005.00341,",
            "year": 2020
        },
        {
            "authors": [
                "Ming Ding",
                "Wendi Zheng",
                "Wenyi Hong",
                "Jie Tang"
            ],
            "title": "Cogview2: Faster and better text-to-image generation via hierarchical transformers",
            "venue": "arXiv preprint arXiv:2204.14217,",
            "year": 2022
        },
        {
            "authors": [
                "Nan Du",
                "Yanping Huang",
                "Andrew M Dai",
                "Simon Tong",
                "Dmitry Lepikhin",
                "Yuanzhong Xu",
                "Maxim Krikun",
                "Yanqi Zhou",
                "Adams Wei Yu",
                "Orhan Firat"
            ],
            "title": "Glam: Efficient scaling of language models with mixture-of-experts",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Alaaeldin El-Nouby",
                "Natalia Neverova",
                "Ivan Laptev",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Training vision transformers for image retrieval",
            "venue": "arXiv preprint arXiv:2102.05644,",
            "year": 2021
        },
        {
            "authors": [
                "Venice Erin Liong",
                "Jiwen Lu",
                "Gang Wang",
                "Pierre Moulin",
                "Jie Zhou"
            ],
            "title": "Deep hashing for compact binary codes learning",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Oran Gafni",
                "Adam Polyak",
                "Oron Ashual",
                "Shelly Sheynin",
                "Devi Parikh",
                "Yaniv Taigman"
            ],
            "title": "Make-a-scene: Scenebased text-to-image generation with human priors",
            "venue": "arXiv preprint arXiv:2203.13131,",
            "year": 2022
        },
        {
            "authors": [
                "Weihao Gao",
                "Xiangjun Fan",
                "Jiankai Sun",
                "Kai Jia",
                "Wenzhi Xiao",
                "Chong Wang",
                "Xiaobing Liu"
            ],
            "title": "Deep retrieval: An end-to-end structure model for large-scale recommendations",
            "year": 2020
        },
        {
            "authors": [
                "Tiezheng Ge",
                "Kaiming He",
                "Qifa Ke",
                "Jian Sun"
            ],
            "title": "Optimized product quantization",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2013
        },
        {
            "authors": [
                "Yunchao Gong",
                "Svetlana Lazebnik",
                "Albert Gordo",
                "Florent Perronnin"
            ],
            "title": "Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2012
        },
        {
            "authors": [
                "Ishaan Gulrajani",
                "Kundan Kumar",
                "Faruk Ahmed",
                "Adrien Ali Taiga",
                "Francesco Visin",
                "David Vazquez",
                "Aaron Courville"
            ],
            "title": "Pixelvae: A latent variable model for natural images",
            "venue": "arXiv preprint arXiv:1611.05013,",
            "year": 2016
        },
        {
            "authors": [
                "Ruiqi Guo",
                "Philip Sun",
                "Erik Lindgren",
                "Quan Geng",
                "David Simcha",
                "Felix Chern",
                "Sanjiv Kumar"
            ],
            "title": "Accelerating large-scale inference with anisotropic vector quantization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Piotr Indyk",
                "Rajeev Motwani"
            ],
            "title": "Approximate nearest neighbors: towards removing the curse of dimensionality",
            "venue": "In Proceedings of the thirtieth annual ACM symposium on Theory of computing,",
            "year": 1998
        },
        {
            "authors": [
                "Suhas Jayaram Subramanya",
                "Fnu Devvrit",
                "Harsha Vardhan Simhadri",
                "Ravishankar Krishnawamy",
                "Rohan Kadekodi"
            ],
            "title": "Diskann: Fast accurate billion-point nearest neighbor search on a single node",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Herve Jegou",
                "Matthijs Douze",
                "Cordelia Schmid"
            ],
            "title": "Product quantization for nearest neighbor search",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2010
        },
        {
            "authors": [
                "Herv\u00e9 J\u00e9gou",
                "Matthijs Douze",
                "Cordelia Schmid",
                "Patrick P\u00e9rez"
            ],
            "title": "Aggregating local descriptors into a compact image representation",
            "venue": "In 2010 IEEE computer society conference on computer vision and pattern recognition,",
            "year": 2010
        },
        {
            "authors": [
                "Jeff Johnson",
                "Matthijs Douze",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Billionscale similarity search with gpus",
            "venue": "IEEE Transactions on Big Data,",
            "year": 2019
        },
        {
            "authors": [
                "HeeJae Jun",
                "Byungsoo Ko",
                "Youngjoon Kim",
                "Insik Kim",
                "Jongtack Kim"
            ],
            "title": "Combination of multiple global descriptors for image retrieval",
            "year": 1903
        },
        {
            "authors": [
                "Jonathan Krause",
                "Michael Stark",
                "Jia Deng",
                "Li Fei-Fei"
            ],
            "title": "3d object representations for fine-grained categorization",
            "venue": "In Proceedings of the IEEE international conference on computer vision workshops,",
            "year": 2013
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Communications of the ACM,",
            "year": 2017
        },
        {
            "authors": [
                "Miguel Lagunes-Fortiz",
                "Dima Damen",
                "Walterio Mayol- Cuevas"
            ],
            "title": "Centroids triplet network and temporallyconsistent embeddings for in-situ object recognition",
            "venue": "In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2020
        },
        {
            "authors": [
                "Doyup Lee",
                "Chiheon Kim",
                "Saehoon Kim",
                "Minsu Cho",
                "Wook-Shin Han"
            ],
            "title": "Autoregressive image generation using residual quantization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Seongwon Lee",
                "Hongje Seong",
                "Suhyeon Lee",
                "Euntai Kim"
            ],
            "title": "Correlation verification for image retrieval",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Michael S Lew",
                "Nicu Sebe",
                "Chabane Djeraba",
                "Ramesh Jain"
            ],
            "title": "Content-based multimedia information retrieval: State of the art and challenges",
            "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM),",
            "year": 2006
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi"
            ],
            "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
            "venue": "arXiv preprint arXiv:2301.12597,",
            "year": 2023
        },
        {
            "authors": [
                "Xirong Li",
                "Tiberio Uricchio",
                "Lamberto Ballan",
                "Marco Bertini",
                "Cees GM Snoek",
                "Alberto Del Bimbo"
            ],
            "title": "Socializing the semantic gap: A comparative survey on image tag assignment, refinement, and retrieval",
            "venue": "ACM Computing Surveys (CSUR),",
            "year": 2016
        },
        {
            "authors": [
                "Ying Liu",
                "Dengsheng Zhang",
                "Guojun Lu",
                "Wei-Ying Ma"
            ],
            "title": "A survey of content-based image retrieval with highlevel semantics",
            "venue": "Pattern recognition,",
            "year": 2007
        },
        {
            "authors": [
                "Zechun Liu",
                "Kwang-Ting Cheng",
                "Dong Huang",
                "Eric P Xing",
                "Zhiqiang Shen"
            ],
            "title": "Nonuniform-to-uniform quantization: Towards accurate quantization via generalized straight-through estimation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Ziwei Liu",
                "Ping Luo",
                "Shi Qiu",
                "Xiaogang Wang",
                "Xiaoou Tang"
            ],
            "title": "Deepfashion: Powering robust clothes recognition and retrieval with rich annotations",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101,",
            "year": 2017
        },
        {
            "authors": [
                "David G Lowe"
            ],
            "title": "Object recognition from local scaleinvariant features",
            "venue": "In Proceedings of the seventh IEEE international conference on computer vision,",
            "year": 1999
        },
        {
            "authors": [
                "Yu A Malkov",
                "Dmitry A Yashunin"
            ],
            "title": "Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Julieta Martinez",
                "Holger H Hoos",
                "James J Little"
            ],
            "title": "Stacked quantizers for compositional vector compression",
            "venue": "arXiv preprint arXiv:1411.2173,",
            "year": 2014
        },
        {
            "authors": [
                "Hyeonwoo Noh",
                "Andre Araujo",
                "Jack Sim",
                "Tobias Weyand",
                "Bohyung Han"
            ],
            "title": "Large-scale image retrieval with attentive deep local features",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Mohammad Norouzi",
                "David J Fleet"
            ],
            "title": "Cartesian k-means",
            "venue": "In Proceedings of the IEEE Conference on computer Vision and Pattern Recognition,",
            "year": 2013
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeff Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll L Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "arXiv preprint arXiv:2203.02155,",
            "year": 2022
        },
        {
            "authors": [
                "Mira Park",
                "Jesse S Jin",
                "Laurence S Wilson"
            ],
            "title": "Fast content-based image retrieval using quasi-gabor filter and reduction of image feature dimension",
            "venue": "In Proceedings fifth IEEE southwest symposium on image analysis and interpretation,",
            "year": 2002
        },
        {
            "authors": [
                "Zhaofan Qiu",
                "Ting Yao",
                "Tao Mei"
            ],
            "title": "Deep quantization: Encoding convolutional activations with deep generative model",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Karthik Narasimhan",
                "Tim Salimans",
                "Ilya Sutskever"
            ],
            "title": "Improving language understanding by generative pre-training",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical text-conditional image generation with clip latents",
            "venue": "arXiv preprint arXiv:2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Ali S Razavian",
                "Josephine Sullivan",
                "Stefan Carlsson",
                "Atsuto Maki"
            ],
            "title": "Visual instance retrieval with deep convolutional networks",
            "venue": "ITE Transactions on Media Technology and Applications,",
            "year": 2016
        },
        {
            "authors": [
                "Jie Ren",
                "Minjia Zhang",
                "Dong Li"
            ],
            "title": "Hm-ann: Efficient billion-point nearest neighbor search on heterogeneous memory",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jerome Revaud",
                "Jon Almaz\u00e1n",
                "Rafael S Rezende",
                "Cesar Roberto de Souza"
            ],
            "title": "Learning with average precision: Training image retrieval with a listwise loss",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Ali Sharif Razavian",
                "Hossein Azizpour",
                "Josephine Sullivan",
                "Stefan Carlsson"
            ],
            "title": "Cnn features off-the-shelf: an astounding baseline for recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition workshops,",
            "year": 2014
        },
        {
            "authors": [
                "Mohammad Shoeybi",
                "Mostofa Patwary",
                "Raul Puri",
                "Patrick LeGresley",
                "Jared Casper",
                "Bryan Catanzaro"
            ],
            "title": "Megatronlm: Training multi-billion parameter language models using model parallelism",
            "year": 1909
        },
        {
            "authors": [
                "Christian Siagian",
                "Laurent Itti"
            ],
            "title": "Rapid biologicallyinspired scene classification using features shared with visual attention",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2007
        },
        {
            "authors": [
                "Oriane Sim\u00e9oni",
                "Yannis Avrithis",
                "Ondrej Chum"
            ],
            "title": "Local features and visual words emerge in activations",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "Arnold WM Smeulders",
                "Marcel Worring",
                "Simone Santini",
                "Amarnath Gupta",
                "Ramesh Jain"
            ],
            "title": "Content-based image retrieval at the end of the early years",
            "venue": "IEEE Transactions on pattern analysis and machine intelligence,",
            "year": 2000
        },
        {
            "authors": [
                "Fuwen Tan",
                "Jiangbo Yuan",
                "Vicente Ordonez"
            ],
            "title": "Instancelevel image retrieval using reranking transformers",
            "venue": "In proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Yi Tay",
                "Vinh Q Tran",
                "Mostafa Dehghani",
                "Jianmo Ni",
                "Dara Bahri",
                "Harsh Mehta",
                "Zhen Qin",
                "Kai Hui",
                "Zhe Zhao",
                "Jai Gupta"
            ],
            "title": "Transformer memory as a differentiable search index",
            "venue": "arXiv preprint arXiv:2202.06991,",
            "year": 2022
        },
        {
            "authors": [
                "Marvin Teichmann",
                "Andre Araujo",
                "Menglong Zhu",
                "Jack Sim"
            ],
            "title": "Detect-to-retrieve: Efficient regional aggregation for image search",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Aaron Van Den Oord",
                "Oriol Vinyals"
            ],
            "title": "Neural discrete representation learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jingdong Wang",
                "Xian-Sheng Hua"
            ],
            "title": "Interactive image search by color map",
            "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),",
            "year": 2011
        },
        {
            "authors": [
                "Jianfeng Wang",
                "Jingdong Wang",
                "Jingkuan Song",
                "Xin-Shun Xu",
                "Heng Tao Shen",
                "Shipeng Li"
            ],
            "title": "Optimized cartesian k-means",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2014
        },
        {
            "authors": [
                "Jingdong Wang",
                "Ting Zhang"
            ],
            "title": "Composite quantization",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Wenhui Wang",
                "Hangbo Bao",
                "Li Dong",
                "Johan Bjorck",
                "Zhiliang Peng",
                "Qiang Liu",
                "Kriti Aggarwal",
                "Owais Khan Mohammed",
                "Saksham Singhal",
                "Subhojit Som"
            ],
            "title": "Image as a foreign language: Beit pretraining for all vision and visionlanguage tasks",
            "venue": "arXiv preprint arXiv:2208.10442,",
            "year": 2022
        },
        {
            "authors": [
                "Xiang-Yang Wang",
                "Bei-Bei Zhang",
                "Hong-Ying Yang"
            ],
            "title": "Content-based image retrieval by integrating color and texture features",
            "venue": "Multimedia tools and applications,",
            "year": 2014
        },
        {
            "authors": [
                "Yujing Wang",
                "Yingyan Hou",
                "Haonan Wang",
                "Ziming Miao",
                "Shibin Wu",
                "Hao Sun",
                "Qi Chen",
                "Yuqing Xia",
                "Chengmin Chi",
                "Guoshuai Zhao"
            ],
            "title": "A neural corpus indexer for document retrieval",
            "venue": "arXiv preprint arXiv:2206.02743,",
            "year": 2022
        },
        {
            "authors": [
                "Dirk Weissenborn",
                "Oscar T\u00e4ckstr\u00f6m",
                "Jakob Uszkoreit"
            ],
            "title": "Scaling autoregressive video models",
            "venue": "arXiv preprint arXiv:1906.02634,",
            "year": 2019
        },
        {
            "authors": [
                "Yandong Wen",
                "Kaipeng Zhang",
                "Zhifeng Li",
                "Yu Qiao"
            ],
            "title": "A discriminative feature learning approach for deep face recognition",
            "venue": "In European conference on computer vision,",
            "year": 2016
        },
        {
            "authors": [
                "Christian Wengert",
                "Matthijs Douze",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Bagof-colors for improved image search",
            "venue": "In Proceedings of the 19th ACM international conference on Multimedia,",
            "year": 2011
        },
        {
            "authors": [
                "Mikolaj Wieczorek",
                "Andrzej Michalowski",
                "Anna Wroblewska",
                "Jacek Dabrowski"
            ],
            "title": "A strong baseline for fashion retrieval with person re-identification models",
            "venue": "In International Conference on Neural Information Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Chenfei Wu",
                "Jian Liang",
                "Lei Ji",
                "Fan Yang",
                "Yuejian Fang",
                "Daxin Jiang",
                "Nan Duan"
            ],
            "title": "N\u00fcwa: Visual synthesis pretraining for neural visual world creation",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Yan Xia",
                "Kaiming He",
                "Fang Wen",
                "Jian Sun"
            ],
            "title": "Joint inverted indexing",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2013
        },
        {
            "authors": [
                "Tong Xiao",
                "Hongsheng Li",
                "Wanli Ouyang",
                "Xiaogang Wang"
            ],
            "title": "Learning deep feature representations with domain guided dropout for person re-identification",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Yuanzhong Xu",
                "HyoukJoong Lee",
                "Dehao Chen",
                "Blake Hechtman",
                "Yanping Huang",
                "Rahul Joshi",
                "Maxim Krikun",
                "Dmitry Lepikhin",
                "Andy Ly",
                "Marcello Maggioni"
            ],
            "title": "Gspmd: general and scalable parallelization for ml computation graphs",
            "venue": "arXiv preprint arXiv:2105.04663,",
            "year": 2021
        },
        {
            "authors": [
                "Min Yang",
                "Dongliang He",
                "Miao Fan",
                "Baorong Shi",
                "Xuetong Xue",
                "Fu Li",
                "Errui Ding",
                "Jizhou Huang"
            ],
            "title": "Dolg: Singlestage image retrieval with deep orthogonal fusion of local and global features",
            "venue": "In Proceedings of the IEEE/CVF International conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime Carbonell",
                "Russ R Salakhutdinov",
                "Quoc V Le"
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Jiahui Yu",
                "Xin Li",
                "Jing Yu Koh",
                "Han Zhang",
                "Ruoming Pang",
                "James Qin",
                "Alexander Ku",
                "Yuanzhong Xu",
                "Jason Baldridge",
                "Yonghui Wu"
            ],
            "title": "Vector-quantized image modeling with improved vqgan",
            "venue": "arXiv preprint arXiv:2110.04627,",
            "year": 2021
        },
        {
            "authors": [
                "Jiahui Yu",
                "Zirui Wang",
                "Vijay Vasudevan",
                "Legg Yeung",
                "Mojtaba Seyedhosseini",
                "Yonghui Wu"
            ],
            "title": "Coca: Contrastive captioners are image-text foundation models",
            "year": 1917
        },
        {
            "authors": [
                "Jiahui Yu",
                "Yuanzhong Xu",
                "Jing Yu Koh",
                "Thang Luong",
                "Gunjan Baid",
                "Zirui Wang",
                "Vijay Vasudevan",
                "Alexander Ku",
                "Yinfei Yang",
                "Burcu Karagol Ayan"
            ],
            "title": "Scaling autoregressive models for content-rich text-to-image generation",
            "venue": "arXiv preprint arXiv:2206.10789,",
            "year": 2022
        },
        {
            "authors": [
                "Ye Yuan",
                "Wuyang Chen",
                "Yang Yang",
                "Zhangyang Wang"
            ],
            "title": "In defense of the triplet loss again: Learning robust person re-identification with fast approximated triplet loss and label distillation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops,",
            "year": 2020
        },
        {
            "authors": [
                "Andrew Zhai",
                "Hao-Yu Wu"
            ],
            "title": "Classification is a strong baseline for deep metric learning",
            "venue": "arXiv preprint arXiv:1811.12649,",
            "year": 2018
        },
        {
            "authors": [
                "Lei Zhang",
                "Yong Rui"
            ],
            "title": "Image search\u2014from thousands to billions in 20 years",
            "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM),",
            "year": 2013
        },
        {
            "authors": [
                "Bolei Zhou",
                "Agata Lapedriza",
                "Aditya Khosla",
                "Aude Oliva",
                "Antonio Torralba"
            ],
            "title": "Places: A 10 million image database for scene recognition",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Kaiyang Zhou",
                "Yongxin Yang",
                "Andrea Cavallaro",
                "Tao Xiang"
            ],
            "title": "Omni-scale feature learning for person reidentification",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Wengang Zhou",
                "Houqiang Li",
                "Qi Tian"
            ],
            "title": "Recent advance in content-based image retrieval: A literature survey",
            "venue": "arXiv preprint arXiv:1706.06064,",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Generative modeling has made significant progress in a wide range of tasks including machine translation [83], conversational modeling [26, 12, 67, 66, 62, 1], image captioning [102], image classification [16], text-to-image synthesis [69, 68, 95, 28], and many more. Originating from language and then expanding to other modalities with specially designed tokenizers, such a universal modeling approach provides a promising direction for unifying different tasks into a versatile pretrained model, which has attracted widespread attention [102, 2, 88, 62, 51]. This paper aims to take the unified trend one step further and investigates generative modeling for an unexplored area, image retrieval.\nIn this paper, we treat image retrieval as a form of generative modeling and make use of standard Transformer architecture, as in GPT [12, 67, 66], to enable end-to-end differentiable search. Our model, IRGen, is a sequence-tosequence model that outputs corresponding nearest neighbors directly from a given query image. Specifically, the\n*Equal contribution. This work was done when Yidan Zhang was an intern at Microsoft Research, Beijing.\n225\nIRGen\nQuery QueryDatabase\nANN\nFeature Extraction\n(b)(a)\nFigure 1. Illustrating (a) conventional image search pipeline consisting of two disconnected stages: feature extraction and approximate nearest neighbor (ANN) search, and (b) our IRGen offering end-to-end retrieval thanks to generative modeling.\nmodel takes a query image as input and autoregressively predicts discrete visual tokens, which are considered as the identifier of an image. The predicted visual tokens are supposed to point to the query image\u2019s nearest neighbor.\nIRGen can be trained directly from the final retrieval target starting with raw images, which is essentially different from conventional image retrieval. Figure 1 illustrates the core difference. In practice, the most widely used retrieval systems consist of two stages: feature representation learning [30, 54, 49, 13, 76, 79, 81, 99] and Approximate Nearest Neighbor (ANN) search [6, 43, 37, 40, 71, 17]. Most image retrieval methods focus only on one individual stage while ignoring the fact that both stages are inherently and deeply connected in actual service. Thus, the practical system often requires careful per-task hyperparameter tuning to make the most out of the coordination of the feature extraction and ANN search. While recent progress [33, 23, 90, 80] have been made towards end-to-end search in the scenario of recommendation, entity retrieval and document retrieval, little has been done for image retrieval.\nThe problem of enabling efficient and effective image retrieval using generative modeling is highly challenging. Two fundamental concerns need to be addressed. First, autoregressive generative modeling is notable for its slow sampling process due to the inherently sequential nature,\nar X\niv :2\n30 3.\n10 12\n6v 3\n[ cs\n.C V\n] 2\n8 Ju\nn 20\n23\nthus the run-time cost for retrieval grows at least linearly with respect to the length of a sequence. Second, from the drastically shortened image identifier, it is particularly difficult to model the semantic relationship between the identifiers. As such, a semantic tokenizer specially designed for image retrieval is an immediate problem. We address both challenges and demonstrate the success of generative modeling for image retrieval.\nTo enable generative modeling for end-to-end image retrieval, our method first needs to represent the image as a sequence of tokens, namely the image identifier. While existing image tokenizers [82, 48] have rapidly developed in the past few years, image tokenizer for image retrieval remains an open problem. We observed that existing image tokenizers, normally designed for image generation task, are not suitable for image retrieval task, and thus lead to poor performance as analyzed in our experiments. We hence propose several key ingredients that (i) inject semantic information by applying image-level supervision rather than low-level pixel supervision, (ii) generate dependent tokens in a sequence by leveraging the recursive property of residual quantization, and (iii) ensure fast inference speed by tremendously reducing the length of the sequence via exploiting the global feature instead of spatial patch embeddings. Afterwards, we intentionally adopt the standard Transformer architecture so that it is easy to scale up the model using existing techniques and infrastructures.\nThe proposed IRGen model sets a new record across a wide range of image retrieval datasets thanks to its endto-end differentiable search ability, surpassing prior strong competitors by a large margin, even better than linear scan search in some cases. For example, compared with the best baseline method (probably with linear scan search), our model gets 20.2% improvement in precision@10 on In-shop Clothes dataset [55], 6.0% in precision@2 on CUB200 [84] and 2.4% in precision@2 on Cars196 [45]. To evaluate the scalability of our model, we further experiment on million-level datasets, ImageNet [25] and Places365 [107], and demonstrate superior performance.\nIt is our belief that generative models have the potential to revolutionize image retrieval. The applicability of generative modeling in image retrieval task opens up the potential opportunity to unify information retrieval of all modalities. At the technical level, IRGen naturally bridges the aforementioned feature representation learning and approximate search into an end-to-end differentiable model implicitly and effortlessly, allowing optimization directly from the retrieval target. Furthermore, the whole framework is conceptually simple, with all the components based on the standard Transformer, which is known to hold impressive scalability [29, 19, 74, 98]. To the best of our knowledge, we are the first to explore generative modeling for image retrieval, expanding the spectrum of generative modeling to a new\narea. Along this way, a fundamentally different retrieval scheme is arrived with verified impressive performance on retrieval benchmarks."
        },
        {
            "heading": "2. Related Work",
            "text": ""
        },
        {
            "heading": "2.1. Image Retrieval",
            "text": "Image retrieval has been a fundamental problem in the multimedia field as well as in industry. Here we focus on overview over recent research works. Extensive reviews can be found in existing surveys [78, 50, 53, 106, 3, 52, 109]. Representation learning. Traditionally, hand-crafted features are heuristically designed to describe the image content based on its color [93, 85], texture [63, 89] or shape [15]. Typical features include GIST [75], SIFT [57], SURF [8], VLAD [42] and so on. Recent years have witnessed the explosive research on deep learning based features trained over labeled images. Besides the evolvement of the network architecture designs [46, 77, 38, 83, 70, 73], numerous efforts [60, 104, 44, 94, 30] have been dedicated to various loss functions including classification loss [60, 97, 105, 108], triplet loss [104], contrastive loss [44, 30], center loss [94, 47, 92] and so on. The similarity between features can be calculated through some distance measure or evaluated through re-ranking techniques [7, 13, 72]. Approximate nearest neighbor search. Another different line of research focus on approximate nearest neighbor search to speed up the search process, with the sacrifice of search accuracy to some degree. One way is to enable fast distance computation through hashing and quantization techniques such as LSH [39, 4], min-Hash [20], ITQ [35], PQ [41], and many others [34, 61, 86, 87, 5, 59, 64, 14, 31, 110]. The other way is to reduce the number of distance comparison by retrieving a small number of candidates. Typical methods include partition-based indexing [10, 6, 96] that partitions the feature space into some non-overlapping clusters and graph-based indexing [40] that builds a neighborhood graph with edges connecting similar images. To improve the recall rate while ensuring fast search speed, hierarchical course-to-fine strategy [6, 58] has been the popular choice that the retrieved candidates are refined level by level. Additionally, a number of excellent works have introduced hybrid indexing [17, 40, 71] that improves search by leveraging the best of both indexing schemes while avoiding their limitations."
        },
        {
            "heading": "2.2. Deep Autoregressive Model",
            "text": "Deep autoregressive networks are generative sequential models that assume a product rule for factoring the joint likelihood and model each conditional distribution through a neural network. AR models have shown extremely powerful progress in generative tasks across multiple domains such as images [16, 18, 36, 103], texts [67, 100], au-\ndio [27, 21], and video [95, 91]. The particular key component involves linearizing data into a sequence of symbols with notable works such as VQ-VAE [82], RQ-VAE [48]. Recently, a number of works [80, 90, 23, 24, 22, 11] further explored the idea of using AR model to empower entity retrieval and document retrieval. Most related to our work are NCI [90] and DSI [80] targeting at document retrieval. Yet they simply apply hierarchical k-means over document embeddings obtained from a pretrained language model to get the document identifier. Differently, we propose to learn the identifier optimized directly from the semantic supervision, and demonstrate its efficacy in the image retrieval scenario. We believe such finding benefit document retrieval as well."
        },
        {
            "heading": "3. Method",
            "text": "Our model trains a sequence-to-sequence model to autoregressively predict image identifiers given a query image. A brief illustration of our pipeline is shown in Figure 2. For Top-K search, beam search can be naturally applied here to find the most likely identifiers given a set of possibilities. The image identifier may be randomly generated in which case the model bears the whole responsibility to learn the semantic relationship among database images. On the other hand, the image identifier may be heuristically designed with semantic prior, reducing optimization difficulty of the model. We will first describe how to obtain a semantic image identifier in detail and then introduce the end-to-end autoregressive model."
        },
        {
            "heading": "3.1. Semantic Image Tokenizer",
            "text": "The autoregressive model must address a sequence of discrete units. As Transformer becomes the ubiquitous architecture in computer vision, it has emerged many successful image tokenizers such as VQ-VAE [82, 69, 32, 101], RQ-VAE [48] and so on. Basically, these methods learn a variational auto-encoder with discrete latent variables, together with a learnable and indexable codebook over a collection of raw images. As a result, an image is represented as a sequence of accountable discrete codes indicating the\nentries in the codebook. A proper combination of entries can be decoded to a high-quality image through the decoder. Such tokenizer has been widely applied to image synthesis, and can be easily extended to audio and video synthesis if the auto-encoder is learned over audio or video data.\nDespite its success in generation, we argue that it is not amenable for the retrieval task. First, decoding the latent codes to reconstruct the raw image is necessary to support the ability to generate images for synthesis task, yet it is not required for retrieval. Besides, the sequence length has a huge effect on the inference speed of AR model, that is search efficiency in our case. It is thus especially critical to deal with a very short sequence of codes, whereas current sequence length of the codes is extremely long for retrieval (e.g., feature map of 8 \u00d7 8 with depth 4 of RQ-VAE leads to a length of 256). Furthermore, it needs to inject semantic information into the latent codes, while the image reconstruction loss is known to be a low-level objective that may force the latent representation to focus on imperceptible local details or even noise.\nBased on the above observations, we propose to explore the global feature outputted from the class token rather than the default spatial tokens. In this way, the sequence length can be significantly reduced (from 64 tokens to 1 token) and as a byproduct, the class token contains compact high-level semantic. Let fcls denote the d-dimensional feature vector outputted from the class token, which is taken as the image representation. We adopt residual quantization (RQ) or stacked composite quantization to approximate this feature. Suppose there are M codebooks with each containing L elements, Cm = {cm1, \u00b7 \u00b7 \u00b7 , cmL}, RQ recursively maps the embedding fcls to a sequentially ordered M codes, fcls \u2192 {l1, l2, \u00b7 \u00b7 \u00b7 , lM} \u2208 [L]M . Let r0 = fcls, we have\nlm = argminl\u2208[L]\u2225rm\u22121 \u2212 cml\u222522, (1) rm = rm\u22121 \u2212 cmlm , m = 1, 2, \u00b7 \u00b7 \u00b7 ,M. (2)\nSuch sequential generation of discrete codes naturally aligns with the sequential autoregressive generation, easing the optimization difficulty of modeling the relationship within identifiers.\nTo further inject semantic prior, we train the network under classification loss over both the original embedding as well as the reconstructed vector. In particular, we consider M levels of reconstruction f\u0302\u2264mcls = \u2211m i=1 cili ,m = 1, 2, \u00b7 \u00b7 \u00b7 ,M so that each prefix code also encodes semantic to a certain degree. Adding up the M levels of partial reconstruction error, the whole objective function is,\nL = Lcls(fcls) + \u03bb1 M\u2211\nm=1\nLcls(f\u0302\u2264mcls ) + \u03bb2 M\u2211\nm=1\n\u2225rm\u222522,\n(3)\nrm = fcls \u2212 sg[f\u0302\u2264mcls ], m = 1, 2, \u00b7 \u00b7 \u00b7 ,M, (4)\nwhere sg[\u00b7] is the stop gradient operator. During training, we adopt alternative optimization to update the codebook and the network. For computing the gradient of Lcls(f\u0302\u2264mcls ), we follow the straight-through estimator [9] as in [82] and approximate the gradient by copying the gradients at f\u0302\u2264mcls directly to fcls. After optimization, we hope that images with similar classes have close codes. In the experiments, we present comparison with other discrete identifiers including random codes and codes from hierarchical k-means algorithm or from RQ-VAE."
        },
        {
            "heading": "3.2. Encoder-Decoder for Autoregressive Retrieval",
            "text": "Once we have discovered a good discrete latent structure equipped with semantic prior, we train a powerful autoregressive sequence-to-sequence model over these discrete random variables without referring their visual content. Our encode-decoder structure decouples the input embedding from discrete codes generation. The model takes a query image as input to first get the query embedding and then yields the discrete codes based on the embedding. It is worth noting that the yielded discrete codes indicate the query\u2019s nearest neighbor images in the database. In that sense, we train the model over an image pair (x1, x2) where x2 is the nearest neighbor of x1, and our model aims to predict the identifiers of x2 given x1 as input.\nTo be specific, let the encoder be denoted as E based on ViT base and the decoder be D, a standard Transformer decoder composed of causal self-attention, crossattention and MLP. We leverage the spatial tokens outputted from the encoder as the embedding, e = E(x1), which is injected into the decoder through cross attention. We train the model with next-token prediction by maximizing the probability of the i-th token of the image identifier given the input embedding and previous token predictions, p(li|x1, l1, \u00b7 \u00b7 \u00b7 , li\u22121, \u03b8), where \u03b8 denotes the parameters of D and E, and l1, l2, \u00b7 \u00b7 \u00b7 , lM are the M tokens for x2 generated from the image tokenizer. By maximizing the probability of each token, we are actually maximizing the probability of generating the image identifier of an image,\np(l1, \u00b7 \u00b7 \u00b7 , lM |x1, \u03b8) = \u03a0Mm=1p(li|x1, l1, \u00b7 \u00b7 \u00b7 , lm\u22121, \u03b8). (5)\nWe apply softmax cross entropy loss on a vocabulary of M discrete image tokens.\nDuring inference, given a query image q, we first calculate the query embedding through the encoder E and then autoregressively predicts the discrete codes through the decoder D based on the query embedding. The image presented by the predicted discrete codes is regarded as the nearest neighbor of the query. The beam search decoding process can be used to retrieve top-K images. In order to ensure valid discrete codes, we constrain the beam\nsearch process traversing within a prefix tree containing valid codes."
        },
        {
            "heading": "3.3. Beam Search vs. ANN Search",
            "text": "In terms of the goal of efficiently finding the Top-K candidates, there are some similarities between beam search and ANN search that both aim to select Top-K promising candidates through traversing tree-like data structures. However they are quite different in the score calculation used to choose the current node. In ANN search, the score is generally calculated by the distance between the query feature and the node feature according to some distance measure. In contrast for beam search, the score or the probability is a function estimated via a differentiable neural network (typically an autoregressive model) conditioned on the query. As such, the whole retrieval pipeline naturally can be optimized in an end-to-end manner."
        },
        {
            "heading": "4. Experiments",
            "text": "We conduct comprehensive evaluations to demonstrate the performance of the proposed IRGen. We first evaluate our method on common image retrieval datasets and further present extensive ablation studies to verify the design of our framework on In-shop Clothes dataset. To show the scalability of our approach, we conduct experiments on two large-scale datasets, ImageNet [25] and Places365 [107]. In-shop Clothes retrieval dataset [55] is a large subset of DeepFashion with large pose and scale variations. This dataset consists of a training set containing 25,882 images with 3997 classes, a gallery set containing 12,612 images with 3985 classes and a query set containing 14,218 images with 3985 classes. The goal is to retrieve the same clothes from the gallery set given a fashion image from the query set. We use both the training set and the gallery set for training in our experiments. CUB200 [84] is a fine-grained dataset containing 11,788 images with 200 classes belong to birds. There are 5,994 images for training and 5,794 images for testing. Cars196 [45] is also a fine-grained dataset about cars. It contains 16,185 images with 196 car classes, which is split into 8,144 images for training and 8,041 images for testing. Implementation details. We adopt ViT-B for encoder and similar architecture for decoder (12 transformer decoder block with dimension 768). Intuitively, a warm initialization of encoder should largely stable the training process. We thus warm-start the model with encoder initialized by the pretrained CLIP model [65]. For training autoregressive model, we select similar image pairs (x1, x2). As current retrieval datasets are labeled with class information, we randomly sample an image x2 which shares the same class with x1 as the nearest neighbor. The hyperparameter for quantization is set to M = 4 and L = 256 for fast inference. We\ndiscuss other choices in the ablation study. More details can be found in the supplementary material. Baselines. We evaluate the performance comparing with following five competitive baselines: 1) ResNet-101 [38] trained from ImageNet dataset, denoted as Res101-Img, which is usually used as a feature extraction tool for many tasks; 2) CLIP [65] trained from 400M image-text pairs, whose features have exhibited powerful zero-shot capability; 3) CGD [44], a state-of-the-art method based on ResNet; 4) IRT [30], a Transformer-based model for image retrieval and the best model IRTR is adopted; 5) FT-CLIP, a baseline FineTuned from CLIP on the target dataset. For CGD and IRT, we reproduce them in order to ensure the same data process and the comparable model size (ResNet101 is adopted for CGD and DeiT-B is adopted for IRT) for fair comparison. We also include their best numbers from their original papers for context. Search process. The baseline models target at effective feature learning and after training, the features for database images are extracted from the learned model. The given query image during search is first passed through the model to get the query feature and then compared with the database features according to a distance metric. As conventional following [65, 44, 30], we use cosine distance for CLIP model and Euclidean distance for other baselines. We consider linear scan search, namely KNN, which is very time consuming and approximate nearest neighbor search, namely ANN,\nwhich is much efficient by contrast. Further for ANN, we consider (i) the popular Faiss IVF PQ [43] with the coarse clusters being set to 300, 100, 200 for In-shop, CUB200, Cars196 respectively, and we set the number of sub-spaces to 4, the number of centroids for each sub-space to 256 for all the datasets; (ii) the state-of-the-art memory-based algorithm ScaNN [37] with the default setting; and (iii) the state-of-the-art disk-based SPANN algorithm [17]."
        },
        {
            "heading": "4.1. Results",
            "text": "Table 1 shows the performance comparison in terms of precision@K that evaluates the percentage of similar images (sharing the same class as query) in the retrieved top K candidates. It can be clearly seen that our model achieves the best performance with significant gain, performing even better than models using linear scan search. For instance, our model gets 20.2% improvement in precision@10 on In-shop Clothes dataset, 6.0% in precision@2 on CUB200 dataset and 2.4% in precision@2 on Cars196 dataset. Additionally, we have following observations. 1) As expected, per-dataset finetuned models perform much better than off-the-shelf feature extractors such as CLIP and ImageNet pretrained ResNet-101. 2) Generally, equipped with ANN algorithm, models perform worse than the counterparts using linear scan search. However, it is possible for other way around, for example FT-CLIP with SPANN search on Cars196 dataset is slightly better than linear scan\nsearch. This suggests that end-to-end optimization is indeed of great importance. 3) It is worth noting that our model achieves consistently high precision number as K increases, while others get severe performance drop.\nWe further compare different models using the metric Recall@K in Table 2. The recall score is 1 if there exists one image out of the returned K candidates shares the same label as the query image, and is 0 otherwise. The average over the whole query set is Recall@K. Here we include the best result of CGD and IRT from their original papers for context (note that they adopt different data preprocesses, model sizes, and additional training techniques). 1) We can see that our model, IRGen, achieves the best Recall@1 amongst all the models. As for other recall scores, our model performs comparable and sometimes slightly worse. The reason might be that current objective loss used in AR makes the model highly optimized for Recall@1 while paying less attention to other scores. One potential solution is to integrate beam search process into training for joint optimization. 2) Besides, it is interesting to notice that different combinations of feature extractor and ANN algorithm have large variance over the three datasets, indicating the difficulty of coordination in practical scenarios. 3) Furthermore, despite high recall of baselines, they usually need an extra re-ranking stage to improve precision, while our model already attains high numbers for precision.\nWe also plot the precision-recall curve in Figure 3. Here recall stands for the conventional meaning, namely true positive rate. It can be easily seen that our approach, IRGen, gets remarkably impressive performance, maintaining high precision and high recall at the same time. Furthermore, we evaluate the metric mean reciprocal rank (MRR) measuring the inverse of the rank of the first relevant item. We compute MRR with respect to four different values 1, 2, 4, 8 and plot the curves in Figure 4. The baselines use SPANN for retrieval algorithm. We can see from the figure that our model again achieves the best number in terms of MRR, validating the effectiveness of our framework. Notably, the performance gap of each baseline to our model has a very large variance on three datasets."
        },
        {
            "heading": "4.2. Ablations",
            "text": "Random identifiers. A naive way for image identifier is to randomly assign discrete identifiers to images. We therefore experiment random identifiers with the same code length and the same range as our model for fair comparison. As expected, this counterpart gets lower performance as shown in Table 3. This is because the model with random identifiers is required to not only learn the interaction between query and the image identifiers but also spend capacity to learn the relationship within identifiers. In contrast, a semantic image identifier would ease the burden of the model\nin building connections within ids and help the model focus more on the input content and the output id. Hierachical k-means identifier. Another intuitive way to obtain semantic identifier is to apply hierarchical k-means (HKM) to pretrained features as in [90]. To show this, we run HKM over the feature of FT-CLIP as this feature exhibits strong performance. The results are evaluated with respect to different numbers of clusters (100, 200, 500). As presented in Table 3, we notice that overall HKM performs better than random, showing the importance of semantic identifiers. Nonetheless, our proposed semantic image identifiers further improve HKM with clear gain. RQ-VAE identifier. Here we compare with image tokenizer RQ-VAE [11] which is widely used in image synthesis, specially for text-to-image generation. We follow the standard default setting where the latent feature map is of 8 \u00d7 8 and the depth is 4, resulting in a sequence length of 256. The codebook is shared with size being 256. The final result significantly lags behind our model, less than 10 percent for performance. We argue that there are two main reasons. First, the sequence length is too long for the model\nto model the relationship within the identifiers. Second, the objective of RQ-VAE is to recover the pixel information, which makes the identifier sensitive to local pixel details. The sequence length. We further investigate the length of identifier in our image tokenizer. We experiment different lengths and report the results in Table 4. We can see that if the length of the identifier is too small (for example 2), the model gets inferior performance. As with the length gets longer to 4 or 6, the model gets better performance. At last the performance drops a little bit if the length is too long (8). We think 4-6 would be a good choice in most cases and we simply use 4 in all our experiments. Inference throughput. Apart from search accuracy, search efficiency is another critical criteria for retrieval. We use an NVIDIA V100-16G GPU to analyze the time cost of our AR model. We show the throughput for 100 queries in Figure 5 with the beam size set as 1, 10, 20, and 30 for comparison. We also present the time cost of adding each component during retrieval. The encoder is pretty fast and the autoregressive decoder is the major bottleneck and takes more time when beam size increases. Additional time has\nbeen consumed for checking the validity, since it is possible that the predicted identifier is not in the database. Overall the time cost is acceptable, e.g, it takes about 0.07s (0.19s) per query with beam size set as 10 (30). Note that our model is an end-to-end retrieval method without re-ranking, which however is usually required after ANN search to get higher precision in practical."
        },
        {
            "heading": "4.3. Scaling to Million-level Datasets",
            "text": "ImageNet. We further experiment our approach with ImageNet dataset [25] that contains 1,281,167 images for training and 50,000 validation images for testing, in which we randomly sample 5,000 images as queries to speed up the evaluation process. The experimental settings are the same as before except that we enlarge the layer of decoder to 24 to increase the capacity for AR modeling. We compare with the strong baselines including CLIP model pretrained from 400M image-text pairs, as well as FT-CLIP model finetuned based on CLIP model. The comparison is reported in Figure 6 and Table 5 in terms of precision@K and MAP@100. We can see that our model again achieves best results. The precision number remains constantly high as K increases, while baselines suffer noticable performance drop.\nPlaces365. We also apply our framework to another large scale dataset, Places365-Standard [107] containing about 1.8 million images from 365 scene categories, where there are at most 5000 images per category. The experimental settings are the same as in ImageNet. We show the comparison with CLIP and FT-CLIP in Figure 6 and Table 5. Again our model yields the best performance, demonstrating its efficacy in million-level datasets."
        },
        {
            "heading": "5. Conclusion",
            "text": "In this paper, we explore generative modeling to empower end-to-end image retrieval that directly maps the query image to its nearest neighbor. Equipped with the proposed semantic image tokenizer, we show that our model is capable of achieving impressive high precision while maintaining high recall at the same time. Extensive ablations and further evaluations on large scale datasets demonstrate the superiority of our approach. We believe such a way of generative modeling for image retrieval opens up new exciting research directions in this area, or even beyond. Limitations. Despite the significant performance, we are aware that our model has its limitations, which are also opportunities opening up for future research. First, although we have demonstrated its scalability to million-scale dataset, handling billion-scale dataset is not easy and may require a larger model with higher capacity. This will inevitably slow down the inference speed. Thus balancing the capacity and the speed is worth exploration for efficient and effective billion-scale search. Second, how to deal with fresh data is particularly critical for search scenario. We conduct a naive experiment that holds half gallery data of In-shop Clothes dataset from training and adds them during inference without updating the codebook and AR model. As shown in Table 6, our model suffers drastic precision drop as K increases while the recall remains consistently high. This is because inherently AR model memorizes the semantic structure within database in its parameters. Thus it is important to study fresh update for dynamic database. Third, training a large AR model requires massive amounts of energy, posing environmental problems. How to enable efficient training such as fast finetuning a pretrained model is a worthwhile question."
        },
        {
            "heading": "A. More Implementation Details",
            "text": "We adopt ViT-B for encoder and similar architecture for decoder (12 transformer decoder block with dimension 768). The input image is of resolution 224\u00d7224 and is partitioned to 14\u00d714 patches with each patch sized 16\u00d716. Intuitively, a warm initialization of encoder should largely stable the training process. We thus warm-start the model with encoder initialized by the pretrained CLIP model [65]. We randomly initialize the remaining fully connected layer and the decoder. The semantic image tokenizer is trained with a batch size of 128 on 8 V100 GPUs with 32G memory per card for 200 epochs. We adopt an AdamW optimizer [56] with betas as (0.9, 0.96) and weight decay as 0.05. We use cosine learning rate scheduling. Note that we set the initial learning rate as 5e \u2212 4 for the FC layers. The learning rate of the encoder is set as one percentage of the learning rate of FC layers. We train our models with 20 warming-up epochs and the initial learning rate is 5e \u2212 7. For training autoregressive model, we select similar image pairs (x1, x2). Since current retrieval datasets are usually labeled with class information, we randomly sample an image x2 which shares the same class with x1 as the nearest neighbor. For autoregressive model, we use batch size of 64 on 8 V100 GPUs with 32G memory per card for 200 epochs. The optimizer and the scheduler are same as the semantic image tokenizer mentioned above. The initial learning rate is 4e \u2212 5 for the decoder and the learning rate for encoder is always one percentage of that for decoder."
        },
        {
            "heading": "B. Qualitative Retrieval Results",
            "text": "Here we present several retrieval examples comparing our approach with baselines. The retrieval results on In-shop Clothes, Cars196, and ImageNet with different methods are shown in Figure 7, Figure 8, and Figure 9 respectively. The correct (incorrect) results are denoted with green (red) borders. By comparing the results in the figures, it can be proved that our proposed method perform favorably and is able to handle extremely hard examples."
        }
    ],
    "title": "IRGen: Generative Modeling for Image Retrieval",
    "year": 2023
}