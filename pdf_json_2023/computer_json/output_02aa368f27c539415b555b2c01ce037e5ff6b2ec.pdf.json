{
    "abstractText": "Existing methods on facial expression recognition (FER) are mainly trained in the setting when multi-class data is available. However, to detect the alien expressions that are absent during training, this type of methods cannot work. To address this problem, we develop a Hierarchical Spatial One Class Facial Expression Recognition Network (HS-OCFER) which can construct the decision boundary of a given expression class (called normal class) by training on only one-class data. Specifically, HS-OCFER consists of three novel components. First, hierarchical bottleneck modules are proposed to enrich the representation power of the model and extract detailed feature hierarchy from different levels. Second, multiscale spatial regularization with facial geometric information is employed to guide the feature extraction towards emotional facial representations and prevent the model from overfitting extraneous disturbing factors. Third, compact intra-class variation is adopted to separate the normal class from alien classes in the decision space. Extensive evaluations on 4 typical FER datasets from both laboratory and wild scenarios show that our method consistently outperforms state-of-theart One-Class Classification (OCC) approaches.",
    "authors": [
        {
            "affiliations": [],
            "name": "Bingjun Luo"
        },
        {
            "affiliations": [],
            "name": "Junjie Zhu"
        },
        {
            "affiliations": [],
            "name": "Tianyu Yang"
        },
        {
            "affiliations": [],
            "name": "Sicheng Zhao"
        },
        {
            "affiliations": [],
            "name": "Chao Hu"
        },
        {
            "affiliations": [],
            "name": "Xibin Zhao"
        },
        {
            "affiliations": [],
            "name": "Yue Gao"
        }
    ],
    "id": "SP:1e9539acbcce0c4ac4dac8908cb1fa21bcf3f234",
    "references": [
        {
            "authors": [
                "J. Cai",
                "Z. Meng",
                "A.S. Khan",
                "Z. Li",
                "J. O\u2019Reilly",
                "Y. Tong"
            ],
            "title": "Island loss for learning discriminative features in facial expression recognition",
            "venue": "In Proceedings of IEEE International Conference on Automatic Face and Gesture Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Chen",
                "Y. Tian",
                "G. Pang",
                "G. Carneiro"
            ],
            "title": "Deep One-Class Classification via Interpolated Gaussian Descriptor",
            "venue": "arXiv preprint arXiv:2101.10043.",
            "year": 2021
        },
        {
            "authors": [
                "J.F. Cohn",
                "A.J. Zlochower",
                "J.J. Lien",
                "T. Kanade"
            ],
            "title": "Feature-point tracking by optical flow discriminates subtle differences in facial expression",
            "venue": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition, 396\u2013401. IEEE.",
            "year": 1998
        },
        {
            "authors": [
                "C.A. Corneanu",
                "M.O. Sim\u00f3n",
                "J.F. Cohn",
                "S.E. Guerrero"
            ],
            "title": "Survey on rgb, 3d, thermal, and multimodal approaches for facial expression recognition: History, trends, and affect-related applications",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(8): 1548\u20131568.",
            "year": 2016
        },
        {
            "authors": [
                "S. Du",
                "Y. Tao",
                "A.M. Martinez"
            ],
            "title": "Compound facial expressions of emotion",
            "venue": "Proceedings of the National Academy of Sciences, 111(15): E1454\u2013E1462.",
            "year": 2014
        },
        {
            "authors": [
                "P. Ekman",
                "W.V. Friesen"
            ],
            "title": "Facial action coding system",
            "venue": "Environmental Psychology & Nonverbal Behavior.",
            "year": 1978
        },
        {
            "authors": [
                "Z. Fan",
                "T. Chen",
                "P. Wang",
                "Z. Wang"
            ],
            "title": "CADTransformer: Panoptic Symbol Spotting Transformer for CAD Drawings",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 10986\u201310996.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Fan",
                "L. Zhu",
                "H. Li",
                "X. Chen",
                "S. Zhu",
                "P. Tan"
            ],
            "title": "FloorPlanCAD: a large-scale CAD drawing dataset for panoptic symbol spotting",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 10128\u201310137.",
            "year": 2021
        },
        {
            "authors": [
                "N. Gopalan",
                "S. Bellamkonda",
                "V.S. Chaitanya"
            ],
            "title": "Facial expression recognition using geometric landmark points and convolutional neural networks",
            "venue": "2018 International Conference on Inventive Research in Computing Applications (ICIRCA), 1149\u20131153. IEEE.",
            "year": 2018
        },
        {
            "authors": [
                "W. Hu",
                "M. Wang",
                "Q. Qin",
                "J. Ma",
                "B. Liu"
            ],
            "title": "HRN: A holistic approach to one class learning",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2020
        },
        {
            "authors": [
                "S. Li",
                "W. Deng"
            ],
            "title": "Reliable crowdsourcing and deep locality-preserving learning for unconstrained facial expression recognition",
            "venue": "IEEE Transactions on Image Processing, 28(1): 356\u2013370.",
            "year": 2019
        },
        {
            "authors": [
                "S. Li",
                "W. Deng",
                "J. Du"
            ],
            "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2852\u20132861.",
            "year": 2017
        },
        {
            "authors": [
                "D. Lundqvist",
                "A. Flykt",
                "A. \u00d6hman"
            ],
            "title": "Karolinska directed emotional faces",
            "venue": "Cognition and Emotion.",
            "year": 1998
        },
        {
            "authors": [
                "C. Lv",
                "Z. Wu",
                "X. Wang",
                "M. Zhou"
            ],
            "title": "3D facial expression modeling based on facial landmarks in single image",
            "venue": "Neurocomputing, 355: 155\u2013167.",
            "year": 2019
        },
        {
            "authors": [
                "M. Lyons",
                "S. Akamatsu",
                "M. Kamachi",
                "J. Gyoba"
            ],
            "title": "Coding facial expressions with gabor wavelets",
            "venue": "Proceedings Third IEEE international conference on automatic face and gesture recognition, 200\u2013205. IEEE.",
            "year": 1998
        },
        {
            "authors": [
                "P. Oza",
                "V.M. Patel"
            ],
            "title": "One-class convolutional neural network",
            "venue": "IEEE Signal Processing Letters, 26(2): 277\u2013 281.",
            "year": 2018
        },
        {
            "authors": [
                "P. Oza",
                "V.M. Patel"
            ],
            "title": "Active authentication using an autoencoder regularized cnn-based one-class classifier",
            "venue": "Proceedings of IEEE International Conference on Automatic Face and Gesture Recognition, 1\u20138.",
            "year": 2019
        },
        {
            "authors": [
                "P. Perera",
                "R. Nallapati",
                "B. Xiang"
            ],
            "title": "Ocgan: Oneclass novelty detection using gans with constrained latent representations",
            "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2898\u20132906.",
            "year": 2019
        },
        {
            "authors": [
                "P. Perera",
                "P. Oza",
                "V.M. Patel"
            ],
            "title": "One-class classification: A survey",
            "venue": "arXiv preprint arXiv:2101.03064.",
            "year": 2021
        },
        {
            "authors": [
                "M.A. Pimentel",
                "D.A. Clifton",
                "L. Clifton",
                "L. Tarassenko"
            ],
            "title": "A review of novelty detection",
            "venue": "Signal Processing, 99: 215\u2013249.",
            "year": 2014
        },
        {
            "authors": [
                "D. Ruan",
                "Y. Yan",
                "S. Chen",
                "J.-H. Xue",
                "H. Wang"
            ],
            "title": "Deep disturbance-disentangled learning for facial expression recognition",
            "venue": "Proceedings of the 28th ACM International Conference on Multimedia, 2833\u20132841.",
            "year": 2020
        },
        {
            "authors": [
                "D. Ruan",
                "Y. Yan",
                "S. Lai",
                "Z. Chai",
                "C. Shen",
                "H. Wang"
            ],
            "title": "Feature decomposition and reconstruction learning for effective facial expression recognition",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7660\u20137669.",
            "year": 2021
        },
        {
            "authors": [
                "L. Ruff",
                "R. Vandermeulen",
                "N. Goernitz",
                "L. Deecke",
                "S.A. Siddiqui",
                "A. Binder",
                "E. M\u00fcller",
                "M. Kloft"
            ],
            "title": "Deep one-class classification",
            "venue": "International Conference on Machine Learning, 4393\u20134402.",
            "year": 2018
        },
        {
            "authors": [
                "M. Sabokrou",
                "M. Khalooei",
                "M. Fathy",
                "E. Adeli"
            ],
            "title": "Adversarially learned one-class classifier for novelty detection",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 3379\u20133388.",
            "year": 2018
        },
        {
            "authors": [
                "M. Salehi",
                "N. Sadjadi",
                "S. Baselizadeh",
                "M.H. Rohban",
                "H.R. Rabiee"
            ],
            "title": "Multiresolution knowledge distillation for anomaly detection",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 14902\u201314912.",
            "year": 2021
        },
        {
            "authors": [
                "B. Sch\u00f6lkopf",
                "J.C. Platt",
                "J. Shawe-Taylor",
                "A.J. Smola",
                "R.C. Williamson"
            ],
            "title": "Estimating the support of a high-dimensional distribution",
            "venue": "Technical Report MSR-T R99\u201387, Microsoft Research.",
            "year": 1999
        },
        {
            "authors": [
                "R.R. Selvaraju",
                "M. Cogswell",
                "A. Das",
                "R. Vedantam",
                "D. Parikh",
                "D. Batra"
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision, 618\u2013626.",
            "year": 2017
        },
        {
            "authors": [
                "C. Shan",
                "S. Gong",
                "P.W. McOwan"
            ],
            "title": "Facial expression recognition based on local binary patterns: A comprehensive study",
            "venue": "Image and vision Computing, 27(6): 803\u2013 816.",
            "year": 2009
        },
        {
            "authors": [
                "D.M. Tax",
                "R.P. Duin"
            ],
            "title": "Support vector data description",
            "venue": "Machine Learning, 54(1): 45\u201366.",
            "year": 2004
        },
        {
            "authors": [
                "K. Wang",
                "X. Peng",
                "J. Yang",
                "S. Lu",
                "Y. Qiao"
            ],
            "title": "Suppressing uncertainties for large-scale facial expression recognition",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 6897\u20136906.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Wen",
                "K. Zhang",
                "Z. Li",
                "Y. Qiao"
            ],
            "title": "A discriminative feature learning approach for deep face recognition",
            "venue": "Proceedings of the European Conference on Computer Vision, 499\u2013515.",
            "year": 2016
        },
        {
            "authors": [
                "H. Xi",
                "D. Aussel",
                "W. Liu",
                "S.T. Waller",
                "D. Rey"
            ],
            "title": "Single-leader multi-follower games for the regulation of two-sided mobility-as-a-service markets",
            "venue": "European Journal of Operational Research.",
            "year": 2022
        },
        {
            "authors": [
                "H. Xi",
                "L. He",
                "Y. Zhang",
                "Z. Wang"
            ],
            "title": "Differentiable road pricing for environment-oriented electric vehicle and gasoline vehicle users in the bi-objective transportation network",
            "venue": "Transportation Letters, 14(6): 660\u2013674.",
            "year": 2022
        },
        {
            "authors": [
                "M.Z. Zaheer",
                "J.-h. Lee",
                "M. Astrid",
                "S.-I. Lee"
            ],
            "title": "Old is gold: Redefining the adversarially learned one-class classifier training paradigm",
            "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 14183\u201314193.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Zeng",
                "Y. Fu",
                "G.I. Roisman",
                "Z. Wen",
                "Y. Hu",
                "T.S. Huang"
            ],
            "title": "One-class classification for spontaneous facial expression analysis",
            "venue": "Proceedings of IEEE International Conference on Automatic Face and Gesture Recognition, 281\u2013286. IEEE.",
            "year": 2006
        },
        {
            "authors": [
                "Z. Zeng",
                "M. Pantic",
                "G.I. Roisman",
                "T.S. Huang"
            ],
            "title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(1): 39\u201358.",
            "year": 2009
        },
        {
            "authors": [
                "L. Zhang",
                "D. Tjondronegoro"
            ],
            "title": "Facial expression recognition using facial movement features",
            "venue": "IEEE Transactions on Affective Computing, 2(4): 219\u2013229.",
            "year": 2011
        },
        {
            "authors": [
                "Z. Zhang",
                "P. Luo",
                "C.C. Loy",
                "X. Tang"
            ],
            "title": "From facial expression recognition to interpersonal relation prediction",
            "venue": "International Journal of Computer Vision, 126(5): 550\u2013569.",
            "year": 2018
        },
        {
            "authors": [
                "Z. Zhao",
                "Q. Liu",
                "S. Wang"
            ],
            "title": "Learning deep global multi-scale and local attention features for facial expression recognition in the wild",
            "venue": "IEEE Transactions on Image Processing, 30: 6544\u20136556.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "As more and more intelligent devices step into our daily life, ubiquitous computing environments are gradually coming into reality (Xi et al. 2022a). However, most existing Human-Computer Interaction (HCI) works are designed to understand and handle the explicit instructions of users, while ignoring their internal psychological and emotional states (Zeng et al. 2009). Such kinds of interactions lack emotional intelligence and present great challenges for building user-friendly HCI systems. This problem prompts researchers to turn their attention to a powerful emotional indicator, facial expression (FE), which makes it possible for HCI to uncover the subtleties of the users\u2019 affective behavior and deal with their emotional changes. With the help of facial expression recognition (FER), HCI systems can gain the ability to provide more warm and humanized service (Wen et al. 2016; Xi et al. 2022b).\n*Corresponding author. Copyright \u00a9 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nGenerally, the objective of FER is to extract and interpret subtle FE-related representation from the input image (Zhang and Tjondronegoro 2011). As shown in Fig. 1, FER is often modeled as a multi-class classification task with adequate multi-class training data in most of the recent works. However, in some real application scenarios with a lack of multi-class data, these FER methods do not always work. This situation is quite common in the open world due to the great costs and difficulties of the collection and annotation of FE data. For example, panic disorder is an anxiety disorder characterized by reoccurring unexpected intense fear. Psy-\nchologists hope to monitor the course of this disease by automatically detecting the patient\u2019s fearful expression during the attack. Since the attacks of panic disorder are unexpected and short in time, it is almost impossible to collect the fearful expression from the patient during the model training. Thus, the lack of fear data will directly lead ordinary multi-class FER methods to not be trained normally.\nThe above-mentioned real scenarios require an FER system that can be trained from the normal expression which is easy to observe, and then detect the alien expression which is hard to collect. This leads to a new task of One-Class Classification (OCC) for FER. OCC is a machine learning paradigm to detect the alien class that falls outside of the training data. Though showing good performance in many other vision tasks, existing OCC approaches do not seem to fit well in the FER task. The main reason is that current OCC methods tend to concentrate on the high-level semantic extraction of the input image and neglect the hierarchical spatial information in subtle facial representations, which greatly limits their representation power and leads to the underfitting problem in the complicated FER task.\nIn this paper, we study one class facial expression recognition. Specifically, we design a novel method, termed HSOCFER, to detect the alien expression with the help of hierarchical and spatial facial information. First, we construct the hierarchical bottleneck modules to enhance the representation ability of the auto-encoder backbone and extract rich latent features from various levels of the network. Comprised of low-level facial texture, middle-level muscle activation, and high-level semantic information, these features provide critical information for the FER task. Second, we propose to employ multi-scale spatial regularization by performing facial landmark detection on the extracted representation. This regularization term can be viewed as a constraint to guide the network towards emotional information in subtle facial representations. Third, we adopt the intraclass variation compacting in the decision space, which minimizes the volume of normal expression\u2019s hypersphere. To accommodate different tasks and features, we summarize the hybrid loss function and propose a decision-level fusion strategy for inference.\nIn summary, the contributions of this paper are threefold: (1) We construct a novel method HS-OCFER to detect alien expression that falls out of the training expression class. It is innovative to propose the deep OCC method on FER, as far as we know.\n(2) To balance the network\u2019s representation power appropriately between underfitting and overfitting, we propose hierarchical feature extraction with multi-scale spatial regularization and compacting intra-class variation. By jointly optimizing the three parts, the network is well-guided to extract more detailed FE-related features that are momentous for FER and construct a more precise decision boundary between the normal expression and alien expression.\n(3) We conduct extensive experiments on 4 representative FER datasets including the lab-controlled CFEE and KDEF, and in-the-wild ExpW and RAF-DB. The results demonstrate the superiority of the proposed HS-OCFER method compared to the state-of-the-art OCC approaches."
        },
        {
            "heading": "Related Work",
            "text": "Facial Expression Recognition. As a complicated vision task, the key to a well-performed FER method is superior representation learning ability with separable features (Corneanu et al. 2016). To solve this problem, conventional methods propose various algorithms to extract non-image representation of the facial image, including Gabor feature (Lyons et al. 1998), Local Binary Pattern (Shan, Gong, and McOwan 2009), and Optical flow (Cohn et al. 1998). Recently, more and more recognition methods are proposed with the development of deep neural networks and collection of large-scale datasets (Fan et al. 2021, 2022). The deep methods designed for FER includes DLP-CNN (Li, Deng, and Du 2017), IL-CNN (Cai et al. 2018), SCN (Wang et al. 2020), ADDL (Ruan et al. 2020), MA-Net (Zhao, Liu, and Wang 2021), and FDRL (Ruan et al. 2021). Deep methods have achieved great improvement in ordinary FER task due to their strong power of representation learning, especially with various environmental factors in the wild.\nOne Class Classification. OCC, also known as novelty detection, is an unsupervised learning task of detecting samples out of the distribution from training data. In OCC, the model is trained on the data of only one class (named normal class) and ought to detect the samples that lie out of the training samples (named alien class) during inference (Perera, Oza, and Patel 2021; Pimentel et al. 2014). Different from well-researched multi-class classification, it is much tougher for an OCC classifier to learn the distinction among different classes and extract more discriminative features for decision. There are generally two types of OCC methods: non-deep and deep methods. Non-deep methods focus on calculating the optimal margin of the training data and learning a data-enclosing region in the sample space (Scho\u0308lkopf et al. 1999; Tax and Duin 2004), which are proficient in handling structured data of a relatively small scale but may get stuck in the curse of dimensionality when coping with highdimensional images in the wild. Deep methods are proposed to overcome this curse in an end-to-end manner, including the discriminative (Oza and Patel 2019, 2018) and generative deep models (Zaheer et al. 2020; Sabokrou et al. 2018)."
        },
        {
            "heading": "Problem Definition",
            "text": "Our goal is to build a FER model that is trained with the expressions of only one class (called normal class) and can detect the expressions from alien classes during inference. Supposing c0 is the normal expression class that can be observed during training, and C = {c0} \u222a {ck}Kk=1 is the set of expression classes the model may encounter during inference, including K alien classes. Due to the fact that we do not know how many expression classes will occur, K is set as a scalar random variable. Let x and s denote the input expression image and spatial regularization data, with y as the corresponding label. Given training data {xn, sn, yn}Nn=1, we want to learn a feature extractor f(x;\u0398f ) to extract deep features z from the input data, where \u0398f is the weight parameter matrix of f . After that, an alien score function S(x, z;\u0398S) is needed to infer its probability score to be\nfrom the alien expression classes {ck}Kk=1, where \u0398S is the weight parameter matrix of S. As most existing research, our work focuses on the above feature extractor f and alien score function S (Hu et al. 2020; Perera, Nallapati, and Xiang 2019; Ruff et al. 2018). To overcome the influence of thresholding process and provide a calibration independent measurement for the given alien score, Area Under the Curve (AUC) of the Receiver Operating Characteristic is adopted as the evaluation metric."
        },
        {
            "heading": "Our Model",
            "text": "In this section, we present the proposed model named Hierarchical Spatial One Class Facial Expression Recognition (HS-OCFER)."
        },
        {
            "heading": "Model Overview",
            "text": "The main framework of our model is shown in Fig 2. Specifically, the proposed HS-OCFER consists of three main components. Firstly, we devise hierarchical bottleneck modules to extract the detailed feature hierarchy from the input image. By bridging the representation gap between the encoder and decoder, the modules can relieve the pressure of underfitting and achieve better feature extraction. Secondly, we employ multi-scale spatial regularization using FE-related information to avoid the potential overfitting problem and facilitate the robustness of feature extraction in the real world. Thirdly, we adopt compact intra-class variation on the hierarchical decisive feature to construct a compact decision space for OCC task. Finally, we conclude with a summarized loss function for optimization and propose a decision-level fusion inference algorithm to incorporate the evaluation of hierarchical feature compactness and image reconstruction quality. The whole framework is trained in an end-to-end manner."
        },
        {
            "heading": "Hierarchical Feature Extraction",
            "text": "As a typical model for unsupervised feature extraction, autoencoder (AE) is frequently adopted in recent OCC works (Ruff et al. 2018; Chen et al. 2021). Generally, AE consists of an encoder network, a decoder network, and a much smaller bottleneck layer in the middle. Benefiting from the typical image reconstruction task and encoder-bottleneckdecoder architecture, AE is capable of learning effective latent representation in various OCC problems.\nHowever, conventional AE model encounters a crucial challenge of underfitting when utilized in the one-class FER task, which accounts for its unpromising results in the experiment. In conventional AE, the latent feature is only extracted from the top layer of the encoder network, which emphasizes the high-level semantics while neglecting the lower-level visual representations, like low-level facial texture and middle-level muscle activation. Recent research has shown that these hierarchical representations are significantly involved with the formation of facial expressions and play an important role in FER (Du, Tao, and Martinez 2014; Zhao, Liu, and Wang 2021). Causing such valuable information to vanish through the network, the conventional bottleneck layer inappropriately limits the representation power\nof the whole model and leads to the underfitting problem for complicated representation learning tasks like FER.\nTo alleviate such an underfitting problem, we propose a series of hierarchical bottleneck modules to preserve multilevel hidden representations for better reconstruction and extract hierarchical latent features from different layers. As shown in Fig. 2, these modules are introduced as shortcut paths between the encoder and decoder in all hidden levels, which takes the feature map of the hidden layer in the encoder as its input and allows such intermediate information to flow to the decoder without further encoding. Specifically, the i-th (i = 1, \u00b7 \u00b7 \u00b7 , l) hierarchical feature extraction module firstly reduces the channels of the input feature map through convolutional and ReLU layers and derives its distilled abstract \u03d5i = ReLU(Conv1(xi)) where xi is the feature map of the i-th encoding layer. Afterward, there are two paths split for different purposes. For one path, further convolutional and max-pooling operations are conducted on the abstract to obtain the hierarchical latent feature zi = Pooling(Conv2(\u03d5i)) in the i-th level. For the other path, the abstract representation is detailed through another convolutional layer and turns into a recovered feature map x\u2032i = Conv3(\u03d5i) of the same size as xi. The recovered x \u2032 i is then fused with the output x\u0303i+1 of the (i+1)-th decoding layer using element-wise sum and fed into the i-th decoding layer. Similar to common AE, given the output image of the decoder x\u0303, the goal of hierarchical feature extraction is defined as:\nLrecons = \u2211\n(x,s,y)\n\u2225x\u2212 x\u0303\u222522 (1)"
        },
        {
            "heading": "Multi-scale Spatial Regularization",
            "text": "The extraction of hierarchical features strengthens the representation power of the model, but brings another vital problem, i.e., overfitting. With the enriched latent feature comes massive redundant information that is not related to the FER task (e.g. various backgrounds, illumination changes, arbitrary pose variations), while subtle facial expression changes can be easily neglected. Moreover, due to the highly varied environment in the real world, the influence of surroundings may cause a lot of disturbance in the feature extraction. Therefore, directly applying the hierarchical reconstruction feature with no spatial regularization will lead to sub-optimal results due to those extraneous factors.\nTo solve such an overfitting problem, we propose to learn robust multi-scale spatial regularization from the FE-related information, e.g., facial landmark coordinates, which encodes rich spatial locations for FER task (Lv et al. 2019; Gopalan, Bellamkonda, and Chaitanya 2018; Wang et al. 2020). Specifically, given the landmark coordinates of the input image, the spatial regularization term is performed by a subtask of facial landmark detection. As shown in informed research, different convolutional layers in the network are proficient in extracting the features on different scales (Selvaraju et al. 2017). For example, the lower layers tend to focus on small-scale textures, while the higher layers usually reflect large-scale structures. Therefore, we obtain the multi-scale spatial feature of the model by concatenating the feature hierarchies {z1, \u00b7 \u00b7 \u00b7 , zl} with the latent fea-\nture z0 in the conventional bottleneck layer. Then through a fully-connected layer, the detection result of the landmark coordinates can be derived as s\u0303. Following the protocol of numerical coordinate regression, given the landmark ground truth s, the goal of multi-scale spatial regularization can be formulated as:\nLreg = \u2211\n(x,s,y)\n\u2225s\u2212 s\u0303\u222522 (2)\nCompact Intra-class Variation After extracting robust hierarchical features with multi-scale spatial regularization, a key point for OCC problem is to construct a separable decision space in which the images of the normal expression can be easily distinguished from the alien ones. Hence, in such space, different images of the normal expression are expected to have a similar feature representation and a collection of the features must be placed in a compact hypersphere during training.\nIn adaptation to the one-class task, we construct a separable feature space with compact intra-class variation. Similar to the spatial regularization, we concatenate the hierarchical features from the bottleneck layers to form the decisive feature zd = (z0, z1, \u00b7 \u00b7 \u00b7 , zl). To limit the feature distribution of the normal expression, we assume that zd \u223c N (\u00b5,\u03c32 \u00b7I) where \u00b5 is a variable prototype center and \u03c32 \u00b7I is a constant covariance matrix for the given normal class. In order to perform the parameter estimation of \u00b5, we have to minimize the log-likelihood function of zd. From that, the Euclidean distance between the decisive feature zd and the prototype center \u00b5 is proved to be the minimization target of compact intra-class variation, i.e., the feature points of the normal expression class should be distributed in the neighborhood of\ntheir prototype \u00b5. Using the Maximum Likelihood Estimation method, the goal of compact intra-class variation is defined as:\nLcompact = \u2211\n(x,s,y)\n\u2225zd \u2212 \u00b5\u222522 (3)\nwhere \u00b5 is a trainable parameter."
        },
        {
            "heading": "Model Optimization and Inference",
            "text": "Comprising the above three tasks, the overall optimization target function of the proposed model can be briefly summarized as\nL = Lrecons + \u03bb \u00b7 Lreg + \u03b3 \u00b7 Lcompact (4)\nwhere Lrecons is the image reconstruction loss, Lreg is the spatial regularization loss, Lcompact is the compact variation loss, with \u03bb and \u03b3 as the weight parameters. By optimizing the target function, the proposed model can appropriately balance its representation power between underfitting and overfitting with the help of hierarchical reconstruction information and spatial regularization, and utilize the power to construct a competent decision space with compact intraclass variation for one-class FER task.\nDuring inference, we propose a decision-level fusion algorithm to calculate the alien scores of the test samples based on hierarchical feature compactness and image reconstruction quality. Since the features of the normal samples are compactly constricted by the compact variation loss during training, it is supposed that the decisive feature points of the normal expression tend to be located closer to the prototype center of training samples than the alien ones. In a similar way, the reconstructed images of the normal samples\nare believed to have lower errors than the alien ones. Therefore, given the input image x, its centripetal distance to the prestored feature prototype c and reconstruction error to the output image x\u0303 are both considered great candidates for the alien score function. For a more comprehensive evaluation of the test sample, we adopt the weighted fusion method to combine the decision information of the feature compactness and reconstruction quality and generate the final alien score as the model output."
        },
        {
            "heading": "Experiment Setup",
            "text": "In this section, we introduce the detailed experimental setup, including the datasets, task setting, baselines, and implementation details."
        },
        {
            "heading": "Datasets",
            "text": "In the experiment, we utilize four typical and representative FER datasets as our benchmark, including laboratorycontrolled CFEE (Du, Tao, and Martinez 2014) and KDEF (Lundqvist, Flykt, and O\u0308hman 1998), and in-the-wild ExpW (Zhang et al. 2018) and RAF-DB (Li and Deng 2019). The scale of FER datasets is generally smaller than many other computer vision tasks. This can be the difficulty of data collection and annotation caused by the subjectivity and subtleness of expressions. Furthermore, the distributions of FER datasets are also highly varied due to different environmental factors like lighting. The inadequate samples and diverse distribution cause great challenges for one-class FER."
        },
        {
            "heading": "Task Setting",
            "text": "All the aforementioned FER datasets have seven expression classes, from which we follow the previous OCC works (Perera, Nallapati, and Xiang 2019; Ruff et al. 2018; Hu et al. 2020; Chen et al. 2021) and create seven setups in the experiments respectively. For each of these four FER datasets, we use the training set of each expression class in turn as the normal class c0 to train the c0-th model and then test it on the full test set of all classes in which the rest of the expression classes except c0 are regarded as the alien classes. In order to alleviate the impact of dataset partitioning, we randomly split each dataset into 5-folds and report the average performance over 5 runs."
        },
        {
            "heading": "Baselines",
            "text": "To have comprehensive coverage of related works, we choose the following non-deep and deep OCC methods as the compared baselines. For non-deep methods, OCSVM (Scho\u0308lkopf et al. 1999) and SVDD (Tax and Duin 2004; Zeng et al. 2006) are chosen as non-deep baselines. With the usage of the kernel function, they can be extended into the corresponding kernel methods as OCSVM-k and SVDDk. For deep methods, the state-of-the-art deep OCC methods can be divided into two categories: discriminative methods and generative methods. For discriminative methods, we choose DSVDD (Ruff et al. 2018), HRN (Hu et al. 2020), and MKD (Salehi et al. 2021) for comparison. For generative methods, we choose OCGAN (Perera, Nallapati, and\nXiang 2019), OGNet (Zaheer et al. 2020), and IGD (Chen et al. 2021) as baselines.\nThe detailed properties of all the baselines and our method are summarized in Table 1. Specifically, Feature Hierarchy is the feature extraction structure from different levels, and Spatial Reg. denotes the regularization term of the spatial information to avoid overfitting."
        },
        {
            "heading": "Implementation Details",
            "text": "We build our model using PyTorch library. Before training, the location of facial regions and landmarks is loaded from the original dataset. The images of the facial regions are then cropped into the size of 224\u00d7 224 pixels and pre-processed using L1-norm global contrast normalization. During the training process, we choose Adam as the main optimizer and initialize its learning rate as 10\u22123, with an exponential decay factor of 0.99. The number of training epochs is set as 200 and the mini-batch size is set as 128. The fusion weight parameter \u03b1 is 0.01 for CFEE and KDEF and 0.005 for ExpW and RAF-DB. All experiments are performed on NVIDIA RTX 3070 GPU card. The code and supplementary materials are provided at https://github.com/KyleL99/HS-OCFER."
        },
        {
            "heading": "Results and Analysis",
            "text": "In this section, we present the statistics and analysis of the experiments. First, we evaluate the performance of the proposed method as compared to state-of-the-art OCC baselines. Second, we conduct an ablation study to analyze the profits of different components. Then we design experiments of different ratios to explore the impact of imbalanced datasets and verify the robustness of the proposed method. Finally, we visualize the attention maps to further explain the effectiveness of the proposed method. Owing to the space constraints of the main paper, the detailed results on KDEF and RAF-DB are presented in Supplementary Materials."
        },
        {
            "heading": "Comparison with State-of-the-arts",
            "text": "We present the results of all the comparison methods on CFEE and ExpW in Table 2 and Table 3 respectively. Based on the comparison results, it can be observed that:\n(1) Compared with other one-class classification methods, our method obtains superior performances in all the 4\ndatasets, with performance improvement of 6.55% (CFEE), 3.12% (ExpW), 2.87% (KDEF), and 4.38% (RAF-DB) as compared to the top baselines. The performance improvements benefit from the advantages of the proposed HSOCFER. First, hierarchical feature extraction and multiscale spatial regularization can enhance the representation power of the model and guide it to concentrate more on FErelated facial regions. Second, compact intra-class variation can improve the performance based on the more compact spatial distribution of the given class\u2019s feature in the decision space. Furthermore, the decision-level fusion algorithm can integrate the decisive information of both feature compactness and reconstruction quality into a more comprehensive evaluation during inference.\n(2) As for the non-deep baselines, the performance on two types of FER datasets shows a huge difference. In laboratory-controlled CFEE and KDEF, non-deep methods generally function well. However, in in-the-wild ExpW and RAF-DB, non-deep methods do not show competitive performance, which is believed to be caused by the relatively larger intra-class variation of the normal expression in the real world. As mentioned in Related Work, that is exactly the weakness of non-deep methods.\n(3) As for the deep baselines, there is one common phenomenon. The top deep baseline methods achieve better performance on in-the-wild ExpW and RAF-DB due to their relatively stronger representation ability and the larger scale of data. Specifically, overall IGD and MKD are the strongest\nbaselines, which work stably on all the four datasets and reach the second-best performance on most datasets. However, the prominent deficiency of them lies in that they do not employ FE-related spatial information to filter out the extraneous disturbance. The rest of deep baselines do not show a good result due to a lack of data distribution fitting and intra-class variation controlling.\n(4) Neither of the baselines can reach the best performance on 4 datasets consistently. As introduced in Datasets, there is a huge distinction between these two types of datasets which represent totally different collection conditions and environments. It is a great challenge for a method to achieve good robustness and performance on all datasets."
        },
        {
            "heading": "Ablation Study",
            "text": "The proposed HS-OCFER method contains three novel components: hierarchical feature extraction, multi-scale spatial regularization, and compact intra-class variation. We conduct an ablation study to further verify their effectiveness. Since the spatial regularization term is based on the multi-scale features extracted from the hierarchical modules, it cannot be separated from hierarchical feature extraction. Therefore, we testify all the six possible combinations of three components, as shown in Table 4. Besides, in order to verify the strength of the proposed decision-level fusion inference algorithm, we conduct additional experiments on different decision rules based on the well-trained HSOCFER network, as shown in Table 5.\nAs the results show, we have the following observations: (1) The basic model and that with only hierarchical feature extraction are the worst methods in all cases. (2) Simply adding compact intra-class variation performs better than the basic model. Besides, continuing to add hierarchical feature extraction and multi-scale spatial regularization can further improve the performance of the model. This demonstrates the necessity of taking these two components into consideration when minimizing the volume of a data-enclosing hypersphere. (3) The decision-level fusion algorithm performs better than using either of the reconstruction quality and feature compactness while using only reconstruction quality is less prominent. (4) All three components contribute to OCC for FER. The HS-OCFER method that jointly combines the three components and employs the decision-level fusion algorithm performs the best in all cases. These observations demonstrate the effectiveness of the proposed method."
        },
        {
            "heading": "Different Ratio",
            "text": "In all of the above experiments, we set the instance number ratio of the normal class and each alien class as 1 : 1 to follow the mainstream OCC works (Zaheer et al. 2020). But in real application scenarios with imbalanced data, the occurrence of alien expressions usually cannot be pre-determined. To explore the impact of different ratios and get a brief evaluation of the robustness under various ratio settings, we plot the performance comparison for different ratios from 1 : 10 to 10 : 1 in Fig. 3. It turns out that our model performs better than other baseline methods robustly."
        },
        {
            "heading": "Visualization",
            "text": "To demonstrate the interpretability of our model, the heat map generated by the Grad-Cam algorithm (Selvaraju et al. 2017) is used to visualize the significance of the spatial locations learned by the basic auto-encoder model and the proposed HS-OCFER method. By comparing the regions that are considered by different networks as being important for predicting a class, we attempt to see how this network is making good use of feature extraction. For a more intuitive demonstration of the remarkable FE-related regions, we highlight the regions of the representative Action Units (AU)\n(Ekman and Friesen 1978) of the corresponding expression class in each facial image. As shown in Figure 4, the proposed HS-OCFER method notices more attentive and discriminative regions that are related to FE, compared with the basic model. For example, AU12 (lip corner puller) is regarded as an exclusive action unit for happiness. In Figure 4, HS-OCFER focuses on the discriminative regions of this AU while the basic model focuses on other facial regions."
        },
        {
            "heading": "Conclusion",
            "text": "In this paper, by constructing a novel OCC method named HS-OCFER, we have shown that better one-class facial expression recognition can be achieved through joint learning with hierarchical feature extraction, multi-scale spatial regularization, and compact intra-class variation. Due to these three new components, the proposed HS-OCFER enhances its representation power of image reconstruction and extracts FE-related hierarchical visual information in different hidden layers. Comprehensive experiments demonstrate that the model consistently outperforms state-of-theart classifiers on multiple small-scale laboratory-controlled and large-scale in-the-wild datasets."
        },
        {
            "heading": "Acknowledgments",
            "text": "The work was supported by the National Natural Science Funds of China (No. 62076146, 62021002, 61977062, 62177046, U1801263, U20A6003)."
        }
    ],
    "title": "Learning Deep Hierarchical Features with Spatial Regularization for One-Class Facial Expression Recognition",
    "year": 2023
}