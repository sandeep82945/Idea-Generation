{
    "abstractText": "Clustering time-series data in healthcare is crucial for clinical phenotyping to understand patients\u2019 disease progression patterns and to design treatment guidelines tailored to homogeneous patient subgroups. While rich temporal dynamics enable the discovery of potential clusters beyond static correlations, two major challenges remain outstanding: i) discovery of predictive patterns from many potential temporal correlations in the multi-variate time-series data and ii) association of individual temporal patterns to the target label distribution that best characterizes the underlying clinical progression. To address such challenges, we develop a novel temporal clustering method, T-Phenotype, to discover phenotypes of predictive temporal patterns from labeled time-series data. We introduce an efficient representation learning approach in frequency domain that can encode variable-length, irregularlysampled time-series into a unified representation space, which is then applied to identify various temporal patterns that potentially contribute to the target label using a new notion of path-based similarity. Throughout the experiments on synthetic and real-world datasets, we show that T-Phenotype achieves the best phenotype discovery performance over all the evaluated baselines. We further demonstrate the utility of T-Phenotype by uncovering clinically meaningful patient subgroups characterized by unique temporal patterns.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuchao Qin"
        },
        {
            "affiliations": [],
            "name": "Mihaela van der Schaar"
        }
    ],
    "id": "SP:fcb516abbcbf4f53c3274545a58ce34fd8396f39",
    "references": [
        {
            "authors": [
                "H. Aguiar",
                "M. Santos",
                "P. Watkinson",
                "T. Zhu"
            ],
            "title": "Learning of cluster-based feature importance for electronic health record time-series",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "M.T. Bahadori",
                "D. Kale",
                "Y. Fan",
                "Y. Liu"
            ],
            "title": "Functional subspace clustering with application to time series",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "P.G. Bastos",
                "X. Sun",
                "D.P. Wagner",
                "A.W. Wu",
                "W.A. Knaus"
            ],
            "title": "Glasgow coma scale score in the evaluation of outcome in the intensive care unit: findings from the acute physiology and chronic health evaluation iii study",
            "venue": "Critical Care Medicine,",
            "year": 1993
        },
        {
            "authors": [
                "I.M. Baytas",
                "C. Xiao",
                "X. Zhang",
                "F. Wang",
                "A.K. Jain",
                "J. Zhou"
            ],
            "title": "Patient subtyping via time-aware lstm networks",
            "venue": "In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
            "year": 2017
        },
        {
            "authors": [
                "T. Ceritli",
                "A.P. Creagh",
                "D.A. Clifton"
            ],
            "title": "Mixture of inputoutput hidden markov models for heterogeneous disease progression modeling",
            "venue": "In Workshop on Healthcare AI and COVID-19,",
            "year": 2022
        },
        {
            "authors": [
                "I.Y. Chen",
                "R.G. Krishnan",
                "D. Sontag"
            ],
            "title": "Clustering interval-censored time-series for disease phenotyping",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "K. Cho",
                "B. Van Merri\u00ebnboer",
                "D. Bahdanau",
                "Y. Bengio"
            ],
            "title": "On the properties of neural machine translation: Encoder-decoder approaches",
            "venue": "arXiv preprint arXiv:1409.1259,",
            "year": 2014
        },
        {
            "authors": [
                "N. Coley",
                "S. Andrieu",
                "M. Jaros",
                "M. Weiner",
                "J. Cedarbaum",
                "B. Vellas"
            ],
            "title": "Suitability of the clinical dementia rating-sum of boxes as a single primary endpoint for alzheimer\u2019s disease",
            "venue": "trials. Alzheimer\u2019s & Dementia,",
            "year": 2011
        },
        {
            "authors": [
                "A. Giannoula",
                "A. Gutierrez-Sacrist\u00edan",
                "A. Bravo",
                "F. Sanz",
                "L.I. Furlong"
            ],
            "title": "Identifying temporal patterns in patient disease trajectories using dynamic time warping: A population-based study",
            "venue": "Scientific Reports,",
            "year": 2018
        },
        {
            "authors": [
                "L.E. Haas",
                "L. Van Dillen",
                "D. de Lange",
                "D. Van Dijk",
                "M. Hamaker"
            ],
            "title": "Outcome of very old patients admitted to the icu for sepsis: a systematic review",
            "venue": "European Geriatric Medicine,",
            "year": 2017
        },
        {
            "authors": [
                "A. Hayashi",
                "Y. Mizuhara",
                "N. Suematsu"
            ],
            "title": "Embedding time series data for classification. In Machine Learning and Data Mining in Pattern Recognition: 4th International Conference",
            "venue": "MLDM",
            "year": 2005
        },
        {
            "authors": [
                "J.C. Ho",
                "J. Ghosh",
                "J. Sun"
            ],
            "title": "Marble: High-throughput phenotyping from electronic health records via sparse nonnegative tensor factorization",
            "venue": "In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 2014
        },
        {
            "authors": [
                "G. Hripcsak",
                "D.J. Albers"
            ],
            "title": "Next-generation phenotyping of electronic health records",
            "venue": "Journal of the American Medical Informatics Association,",
            "year": 2013
        },
        {
            "authors": [
                "K.W. Kim",
                "S.Y. Woo",
                "S. Kim",
                "H. Jang",
                "Y. Kim",
                "S.H. Cho",
                "S.E. Kim",
                "S.J. Kim",
                "B.-S. Shin",
                "H.J. Kim"
            ],
            "title": "Disease progression modeling of alzheimer\u2019s disease according to education level",
            "venue": "Scientific Reports,",
            "year": 2020
        },
        {
            "authors": [
                "C. Lee",
                "M. van der Schaar"
            ],
            "title": "Temporal phenotyping using deep predictive clustering of disease progression",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "C. Lee",
                "J. Rashbass",
                "M. Van der Schaar"
            ],
            "title": "Outcomeoriented deep temporal phenotyping of disease progression",
            "venue": "IEEE Transactions on Biomedical Engineering,",
            "year": 2020
        },
        {
            "authors": [
                "C. Lee",
                "A. Light",
                "E.S. Saveliev",
                "M. van der Schaar",
                "V.J. Gnanapragasam"
            ],
            "title": "Developing machine learning algorithms for dynamic estimation of progression during active surveillance for prostate cancer",
            "venue": "npj Digital Medicine,",
            "year": 2022
        },
        {
            "authors": [
                "Q. Lei",
                "J. Yi",
                "R. Vaculin",
                "L. Wu",
                "I.S. Dhillon"
            ],
            "title": "Similarity preserving representation learning for time series clustering",
            "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Q. Ma",
                "J. Zheng",
                "S. Li",
                "G.W. Cottrell"
            ],
            "title": "Learning representations for time series clustering",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Y.L. Rao",
                "B. Ganaraja",
                "B. Murlimanju",
                "T. Joy",
                "A. Krishnamurthy",
                "A. Agrawal"
            ],
            "title": "Hippocampus and its involvement in alzheimer\u2019s disease: a review",
            "venue": "Biotech,",
            "year": 2022
        },
        {
            "authors": [
                "R.L. Richesson",
                "J. Sun",
                "J. Pathak",
                "A.N. Kho",
                "J.C. Denny"
            ],
            "title": "Clinical phenotyping in selected national networks: demonstrating the need for high-throughput, portable, and computational methods",
            "venue": "Artificial Intelligence in Medicine,",
            "year": 2016
        },
        {
            "authors": [
                "P.J. Rousseeuw"
            ],
            "title": "Silhouettes: a graphical aid to the interpretation and validation of cluster analysis",
            "venue": "Journal of Computational and Applied Mathematics,",
            "year": 1987
        },
        {
            "authors": [
                "P. Schulam",
                "F. Wigley",
                "S. Saria"
            ],
            "title": "Clustering longitudinal clinical marker trajectories from electronic health data: Applications to phenotyping and endotype discovery",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "D. Steinley"
            ],
            "title": "Properties of the hubert-arable adjusted rand index",
            "venue": "Psychological Methods,",
            "year": 2004
        },
        {
            "authors": [
                "N.X. Vinh",
                "J. Epps",
                "J. Bailey"
            ],
            "title": "Information theoretic measures for clusterings comparison: is a correction for chance necessary",
            "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,",
            "year": 2009
        },
        {
            "authors": [
                "J. Xie",
                "R. Girshick",
                "A. Farhadi"
            ],
            "title": "Unsupervised deep embedding for clustering analysis",
            "venue": "In International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Y. Yamazaki",
                "N. Zhao",
                "T.R. Caulfield",
                "C.-C. Liu",
                "G. Bu"
            ],
            "title": "Apolipoprotein e and alzheimer disease: pathobiology and targeting strategies",
            "venue": "Nature Reviews Neurology,",
            "year": 2019
        },
        {
            "authors": [
                "B. Yang",
                "X. Fu",
                "N.D. Sidiropoulos",
                "M. Hong"
            ],
            "title": "Towards k-means-friendly spaces: Simultaneous deep learning and clustering",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "X. Zhang",
                "J. Chou",
                "J. Liang",
                "C. Xiao",
                "Y. Zhao",
                "H. Sarv",
                "C. Henchcliffe",
                "F. Wang"
            ],
            "title": "Data-driven subtyping of parkinson\u2019s disease using longitudinal clinical records: A cohort study",
            "venue": "Scientific Reports,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Discovering predictive patterns of disease progression has been a long pursuit in healthcare. Clinicians have considered specific clinical (disease) status and the associated patterns as a phenotype to uncover the heterogeneity of diseases and to design therapeutic guidelines tailored to homogeneous subgroups (Hripcsak and Albers, 2013; Richesson et al., 2016). While rule-based phenotypes identified by domain experts have been widely used (Denny et al., 2013; Richesson et al., 2016), designing and validating such rules require tremendous effort. Unfortunately, disease progression can manifest through a broad spectrum of clinical factors, collected as a sequence of measurements in electronic health records (EHRs), that may vary greatly across individual patients. This makes it even more daunting for domain experts to transform such raw and complex clinical observations into clinically relevant and interpretable patterns.\nTemporal clustering has been recently used as a datadriven framework for phenotyping to partition patients with sequences of observations into homogeneous subgroups. To discover different temporal patterns, traditional notions of similarity focus on either adjusting similarity measures (Zhang et al., 2019; Baytas et al., 2017) or finding low-dimensional representations (Ho et al., 2014; Giannoula et al., 2018) for longitudinal observations. These approaches are purely unsupervised and discard valuable information about the disease status that is often available in the clinical data. More recently, predictive clustering methods (Lee and van der Schaar, 2020; Lee et al., 2020; 2022; Aguiar et al., 2022) have introduced a new notion of similarity such that each cluster shares similar disease status to provide a better prognostic value. Despite the effort to understand temporal dynamics in their mutual context, these clustering methods fail to capture the full picture of disease progression as reflected by covariate trajectories of prognostic characteristics, i.e., temporal patterns associated with specific disease status. Figure 1 illustrates a pictorial depiction of the notion of phenotypes behind different temporal clustering methods. ar X iv :2\n30 2.\n12 61\n9v 1\n[ cs\n.L G\n] 2\n4 Fe\nb 20\nContribution. In this paper, we propose a novel temporal clustering method to correctly uncover predictive temporal patterns descriptive of the underlying disease progression from the labeled time-series data. First, we formally define the notion of temporal phenotypes as predictive temporal patterns. Then, the association of individual temporal patterns with the target disease status is assessed by proposing a novel path-based similarity score. For effective evaluation of the path-based similarity, we introduce a representation learning approach based on the Laplace transform to convert variable-length, irregularly sampled time-series data into unified embeddings. Finally, based on the resulting path-based similarity graph, we formulate the task of temporal phenotyping as a temporal predictive clustering problem that can be efficiently solved by adopting the graph-constrained K-means clustering.\nWe validate our approach through experiments on synthetic and real-world time-series datasets. Our method discovers temporal phenotypes that provide superior prediction performance compared to state-of-the-art benchmarks, and we corroborate the interpretability of our discovered phenotypes with supporting medical and scientific literature."
        },
        {
            "heading": "2 TEMPORAL PHENOTYPING",
            "text": "Suppose disease progression manifests through a multivariate continuous-time trajectory x(t) \u2208 X defined on t \u2208 [0, 1], where X is the functional space of all possible patient trajectories.1 Each trajectory consists of\n1Trajectories defined within the interval R+ can be simply scaled to the unit interval [0, 1].\ndimx-dimensional time-varying covariates, i.e., x(t) = [x1(t), . . . , xdimx(t)]\n>, each of which can be described by a continuous-time function xi in L2[0,1] (i.e., L\n2-space under the interval [0, 1]).2 Thus, the considered trajectory space can be given as X = \u2297 dimx\nL2[0,1]. Each trajectory x is correlated with a target label vector y = [y1, . . . , ydimy ]\n> \u2208 Y that describes the clinical status of the underlying disease progression (e.g., clinical endpoints). Throughout the paper, we focus our description on the case where the outcome of interest y is categorical and represented by a one-hot vector, i.e., Y = {0, 1}dimy . Let p(x,y) be the joint distribution of the continuoustime trajectory and the label vector. To discover temporal patterns that are predictive of the clinical status of patients, we first define a vector-valued function g(x) = [p(y1|x), . . . , p(ydimy |x)]> which implies the categorical conditional distribution p(y|x). We assume the clinical status conditioned on a patient trajectory can be represented by one of the \u03b4-separable modes in g(x). These modes are \u03b4-separable such that they can be separated based on a proper distance metric dy with some threshold \u03b4 > 0. Here, we choose the Jensen\u2013Shannon (JS) divergence as our distance metric, i.e., dy(v,u) = 12KL(g(v)||m) + 1 2KL(g(u)||m), where KL is the Kullback-Leibler divergence, m = g(v)+g(u)2 .\n2In many practical scenarios, the continuous-time functions for time-varying covariates are bounded and fall into theL2-space which has a natural extension of Euclidean distance."
        },
        {
            "heading": "2.1 Phenotypes: Predictive Temporal Patterns",
            "text": "In this subsection, we introduce the formal definition of phenotypes as temporal patterns that are predictive of disease progression. To this goal, we start by describing how the temporal patterns in continuous-time trajectories can be discovered and how the specific disease progression can be associated with each individual pattern.\nTemporal Patterns. A temporal pattern characterizes some temporal dynamics that are shared by a subset of trajectories in X . Here, we introduce a novel definition to describe temporal patterns in the general form based on connectivity in trajectory space X . Given two trajectories x1,x2 \u2208 X , we define a translation from x1 to x2, denoted as \u0393(x1 \u2192 x2), as a continuous path \u0393 connecting the two trajectories in space X . Typically, \u0393(x1 \u2192 x2) can continuously morph the shape of x1 into that of x2. Then, we formally define a temporal pattern as a connected set \u03a6 \u2282 X such that all the trajectories in \u03a6 can be interconnected by translations within \u03a6. That is, there exists a series of translations from any trajectory to any other trajectory in \u03a6.\nPhenotypes. Considering multivariate continuous-time trajectories, a variety of temporal patterns may exist in X while only a few of them are relevant to the target label. In the meantime, the clinical status marked by the same target label may manifest in patient trajectories through different temporal characteristics. For instance, in lung transplant referral of cystic fibrosis patients, (i) low lung function score, (ii) rapid declining lung function score, and (iii) multiple exacerbations requiring intravenous antibiotics are identified as distinct predictive temporal patterns (Ramos et al., 2019) among various temporal dynamics.\nTo provide insights on disease progression, desirable phenotypes shall be defined based on distinct predictive temporal patterns. In line with such notion of phenotypes, we propose a new path-based similarity score that measures the variation of conditional label distribution (described by function g(x)) along a translation between two trajectories. Specifically, consider two continuous-time trajectories x1,x2 and a translation \u0393(x1 \u2192 x2), the score function evaluates the similarity between x1 and x2 via their impact on label y through path \u0393 as follows:\nd\u0393(x 1,x2) = max\nx\u2208\u0393(x1\u2192x2) i\u2208{1,2}\ndy(g(x), g(x i)). (1)\nSmall value of d\u0393(x1,x2) indicates that trajectories x1 and x2 share similar clinical status y and contain similar temporal patterns that are predictive of their associated label.\nFinally, we provide a formal definition of phenotype as a predictive temporal pattern associated with a distinct clinical status as follows:\nDefinition 1. (Phenotype) Let v be the centroid of a \u03b4separable mode in g(x). Then, there exists a unique phenotype, denoted as a tuple (v,\u03a6) with \u03a6 as a set of trajec-\ntories, that satisfies the following two properties:\n(Similar clinical status) max x\u2208\u03a6\ndy(g(x),v) \u2264 \u03b4\n2 ,\n(Similar predictive pattern) max x1,x2\u2208\u03a6\n\u0393\u2286\u03a6\nd\u0393(x 1,x2) \u2264 \u03b4,\nand any trajectory x \u2208 X \\ \u03a6 is either not connected to \u03a6 or has a different mode.\nIntuitively, the homogeneity of each phenotype (v,\u03a6) guarantees that the continuous-time trajectories exhibiting a similar temporal pattern will lead to a similar clinical status, which in turn provides a prognostic value on the underlying disease progression."
        },
        {
            "heading": "2.2 Predictive Temporal Clustering",
            "text": "In practice, the continuous-time trajectories of a patient are systematically collected in EHRs as discrete observations with irregular intervals during his/her regular follow-ups or stay at hospital. Hence, we focus this subsection on formulating the task of discovering phenotypes given discrete observations of trajectories as a novel clustering problem.\nSuppose we have a dataset D = {(ti,Xi,yi)}Ni=1 comprising discrete observations on the underlying continuous-time trajectories and target labels. Here, we denote discrete observations as time-series X = [x(t1),x(t2), . . . ,x(tT )] which contains sequential observations of a trajectory x at observation time stamps t = [t1, t2, . . . , tT ]\n> with 0 \u2264 t1 \u2264 . . . \u2264 tT \u2264 1. The label vector y \u2208 Y describes the clinical status sampled from the conditional distribution p(y|x). From this point forward, we will slightly abuse the notation and interchangeably write X to denote the discrete time-series and the associated time stamps.\nPath-Based Connectivity. Note that the property of a phenotype in Definition 1 requires all trajectories in that phenotype share a similar predictive pattern. Consider two time-series X1,X2 with underlying continuous-time trajectories x1,x2 from the same phenotype (v,\u03a6). There must exist a translation \u0393 from trajectory x1 to x2 such that the condition in d\u0393(x1,x2) \u2264 \u03b4 holds. Violating such a condition implies a significant difference between the two trajectories suggesting they are from different phenotypes. Therefore, we utilize the path-based connectivity test, i.e., \u2203\u0393(x1 \u2192 x2), d\u0393(x1,x2) \u2264 \u03b4, to assesses the phenotype similarity between two given trajectories X1 and X2. This enables discovery of predictive temporal patterns without access to the ground-truth phenotypes. Evaluation of the path-based connectivity on all possible pairs of time-series in datasetD generates a distance matrix S. Element-wise comparison of S and threshold \u03b4 yields a similarity graph G\u03b4 with edges between similar samples. We will discuss how we can approximately achieve the path-based connectivity test based on the discrete observations in the next section.\nTemporal Phenotyping. To discover phenotypes from dataset D, we assume that we have a proper approxima-\ntor f(X) of the conditional label distribution g(x) from discrete observations in X . Thus, similarity graph G\u03b4 can be constructed based on the path-based connectivity test with approximator f(X). Now, we formulate the task of temporal phenotyping as a predictive clustering problem (Lee and van der Schaar, 2020) to group time-series into different clusters on top of G\u03b4 . More specifically, the clusters (with distinct phenotypes) are discovered by solving the following constrained optimization problem:\nmin C \u2211 Ck\u2208C \u2211 X\u2208Ck dy(f(X),vk),\ns.t. \u2200X1,X2 \u2208 Ck, X1 G\u03b4\u2190\u2192X2,\n(2)\nwhere C = {C1, C2, . . . , CK} is a feasible set of K \u2208 N clusters each of which has a centroid vk as the average density f(X), Since threshold \u03b4 is usually unknown in advance, we set its value according to \u03b4 = 2 maxCk\u2208C,X\u2208Ck dy(f(X),vk) for consistency with Definition 1. Here, X1 G\u03b4\u2190\u2192 X2 implies that there exists a path over graph G\u03b4 such that X1 and X2 are interconnected. In (2), the objective function encourages the cluster centroids to be clearly distinguished in approximated label distribution f(X) while the constraint on similarity graph G\u03b4 ensures that samples in the same cluster are of similar phenotypes. Each discovered cluster Ck represents a unique phenotype with centroid vk describing the associated clinical status and allows us to explain the predictive temporal pattern in terms of the collection of time-series in Ck.\nUnfortunately, the optimization problem in (2) is highly non-trivial due to the following two challenges: First, it requires to learn a proper approximation of the conditional label distribution from irregularly-sampled discrete timeseries. Second, an efficient evaluation of the path-based connectivity test is required to construct similarity graph G\u03b4 given discrete time-series in D."
        },
        {
            "heading": "3 METHOD: T-PHENOTYPE",
            "text": "In this section, we propose a novel temporal clustering framework, T-Phenotype, that effectively discovers phenotypes from discrete time-series data. To estimate the conditional label distribution from discrete time-series, we introduce two networks, an encoder and a predictor. The encoder, fE , comprises dimx feature-wise Laplace encoders, each of which transforms a single feature dimension of discrete time-series X into a fixed-length latent embedding. The predictor, fP , takes embeddings from dimx Laplace encoders as the input z in the latent space and estimates the conditional label distribution. The proposed Laplace encoders, fL, allow us to establish (approximately) equivalence translation in the latent space and thereby to efficiently evaluate the path-based connectivity test between discrete time-series in dataset D. Then, given an approximate similarity graph G\u03b4 constructed from the result of pair-wise connectivity test, we propose a graphconstrained K-means algorithm to discover distinct phenotypes. The overview of steps involved in T-Phenotype is illustrated in Figure 2."
        },
        {
            "heading": "3.1 Time-Series Embedding via Laplace Encoder",
            "text": "Now, we introduce a novel time-series encoder which encodes each dimension of a given discrete time-series into a unified parametric function in the frequency domain as an approximation of the Laplace transform.\nLaplace Encoder. Let x(t) = [x(t1), . . . , x(tT )]> \u2208 RT be a time-series of discrete observations on a univariate trajectory x(t) at time stamps t = [t1, . . . , tT ]> in the unit interval. The Laplace encoder (parameterized by \u03b8L), fL : RT \u2192 Cn(d+1), encodes discrete time-series x(t) into a rational function on the complex plane with n \u2208 N poles of maximum degree of d \u2208 N as follows:\nFw(s) = n\u2211 m=1 d\u2211 l=1 cm,l (s\u2212 pm)l , cm,l, pm \u2208 C. (3)\nHere, w , fL(x(t)) = [p1, . . . , pn, c1,1, . . . , cn,d]> is the Laplace embedding comprising the poles and the corresponding coefficients. Note that the poles in (3) are distinct and are in a lexical order, i.e., pm \u2264 pm+1 for m = 1, . . . , n\u22121 where pm \u2264 pn if and only if Re(pm) < Re(pn) or Re(pm) = Re(pn) \u2227 Im(pm) \u2264 Im(pn) holds. Then, the time-domain function can be efficiently reconstructed via the inverse Laplace transform:\nx\u0302(t) = 1\n2\u03c0j lim T\u2192\u221e \u222b \u03c3+jT \u03c3\u2212jT estFw(s)ds, (4)\nwhere j2 = \u22121 and \u03c3 is some suitable complex number such that Re(\u03c3) > maxpm\u2208w Re(pm). With a sufficient number of poles, the Laplace embedding w becomes an equivalent description of the underlying trajectory x(t). That is, the orthonormal basis {e2\u03c0jmt,m \u2208 Z} of L2[0,1] is covered by the reconstruction x\u0302(t) when n\u2192\u221e. Given a dataset of N discrete univariate time-series, i.e., {xi(t)}Ni=1, we train the Laplace encoder utilizing the following loss function that consists of the time-series reconstruction error and the regularization term specifically designed to encourage unique Laplace embeddings:\nLlaplace(\u03b8L) = Lmse(\u03b8L) + \u03b1Lunique(\u03b8L) (5)\nwhere \u03b1 is a balancing coefficient. The former term, i.e., Lmse(\u03b8L) = 1N \u2211N i=1 \u2016xi(t) \u2212 x\u0302i(t)\u201622, is the reconstruction error from our Laplace embeddings, and the latter term, i.e., Lunique(\u03b8L) =\n1 N(N\u22121) \u2211 i6=j `unique(x\u0302\ni(t), x\u0302j(t)), encourages the uniqueness of the Laplace embedding. More specifically, `unique focuses on three aspects \u2013 (i) the obtained poles are distinct, (ii) the reconstructed trajectories are real-valued, and (iii) no two distinct Laplace embeddings generate the same trajectory. We further elaborate the uniqueness regularization in the Appendix.\nFrom Trajectory Space to Latent Space. Utilizing dimx feature-wise Laplace encoders as our encoder, fE , any discrete observations of a continuoustime trajectory x \u2208 X can be transformed into a fixedlength embedding z \u2208 Z in the latent space as a composition of dimx Laplace embeddings, i.e., z , [fL(x1(t)), . . . , fL(xdimx(t))]\n>. The following proposition builds a strong connection between the trajectory space X and the latent space Z: Proposition 1. Without loss of generality, consider univariate continuous-time trajectories x\u03021, x\u03022 \u2208 X and their corresponding latent embeddings z1, z2 \u2208 Z , respectively. Then, the distance between two trajectories can be bounded by \u2016x\u03021 \u2212 x\u03022\u20162\nL2 [0,1]\n\u2264 \u03c8\u2016z1 \u2212 z2\u201622, where\n\u03c8 > 0 is a constant and \u2016x(t)\u20162 L2\n[0,1]\n= \u222b 1\n0 x(t)x(t)dt.\nThe detailed proof can be found in the Appendix. Consider a subset of latent variables \u03a6z and the corresponding trajectory set \u03a6 of their time-domain representations. The upper bound in Proposition 1 implies that continuity of\n\u03a6z in the latent space leads to the continuity of \u03a6 in the trajectory space. This property allows efficient evaluation of the path-based connectivity test in the latent space as illustrated in the following subsection."
        },
        {
            "heading": "3.2 Efficient Evaluation of Path-based Similarity",
            "text": "Construction of similarity graph G\u03b4 involves iterative evaluation of the path-based similarity score d\u0393 in (1) for all possible pairs of time-series samples in D. This requires a substantial number of computations in both constructing translation \u0393 and calculating conditional g(x) on all available continuous-time trajectories x \u2208 \u0393. Instead, we efficiently approximate the similarity graph G\u03b4 via pathbased connectivity test in the latent space and estimate the conditional g(x) via neural networks.\nTranslation in Latent Space. Consider two trajectories x\u03021, x\u03022 \u2208 X with the corresponding latent embedding z1, z2 \u2208 Z . For any translation \u0393(x\u03021 \u2192 x\u03022) \u2286 X in trajectory space, we can always find a continuous path in the latent space, i.e., \u03b3(z1 \u2192 z2) \u2286 Z , such that the distance between its time-domain reconstruction and \u0393 is minimized. We consider \u03b3 to be an (approximately) equivalent translation of \u0393.3 This enables us to capitalize on the translation in the latent space without constructing intermediate trajectories along path \u0393, which significantly reduces computations in obtaining the path-based similarity in (1).\nPredictor. To estimate the function g(x), we utilize the time-series encoder fE , which consists of dimx Laplace encoders, and a predictor fP (an MLP parameterized by \u03b8P ) to construct the approximator as f(X) , fP \u25e6 fE(X) \u2248 g(x) where X is the discrete observation of trajectory x. The predictor fP is trained based on the cross-entropy loss:\nLpredictor(\u03b8P ) = \u2212 1\nN N\u2211 i=1 dimy\u2211 c=1 yic log fP (z i)c, (6)\nwhere z = fE(X) and subscript c indicates the c-th element in the output space. To maintain the property of the Laplace encoders, we only update the predictor via the signal from the label during training.\nConsider a trajectory translation \u0393(x\u03021 \u2192 x\u03022) and its equivalent translation \u03b3(z1 \u2192 z2) in latent space, the path-based similarity can be approximately calculated as\nd\u0393(x\u0302 1, x\u03022) \u2248 d\u03b3(z1, z2) = max\nz\u2208\u03b3,i=1,2 dy(fP (z), fP (z\ni)).\n(7) Hence, given two discrete time-series X1 and X2, the path-based connectivity test can be efficiently performed along translation \u03b3 in the latent space without assessing the corresponding translation in the trajectory space X . Approximate Similarity Graph. Consider a phenotype (v,\u03a6) where centroid v represents a specific clinical status\n3The equivalence is strict when all trajectories along translation \u0393 have rational Laplace transform as described in (3).\nand \u03a6 is the associated predictive temporal pattern. The encoder fE is trained to map time-series X sampled from trajectories in \u03a6 into a connected area \u03a6z in latent space Z via Laplace encoders. Given time-series X that is observed from trajectory x \u2208 \u03a6, Definition 1 implies that we have dy(f(X),v) \u2264 \u03b42 where f(X) = fP (z) and z = fE(X) \u2208 \u03a6z . Hence, for two embeddings z1, z2 \u2208 \u03a6z , there always exist a translation \u03b3(z1 \u2192 z2) \u2286 \u03a6z such that d\u03b3(z1, z2) \u2264 \u03b4 due to the connectivity of \u03a6z in the latent space. If two latent embeddings z1, z2 are located in the same convex subset of \u03a6z , linear path \u03b3\u0304(z1 \u2192 z2) = {z|(1\u2212 a)z1 + az2, a \u2208 [0, 1]} suffices the connectivity test. When z1 and z2 are in different convex subsets, the connectivity of \u03a6z guarantees that there exists a series of intermediate points zm1 , zm2 , . . . ,zml such that composite path \u03b3(z1 \u2192 z2) = \u03b3\u0304(z1 \u2192 zm1)\u222a. . .\u222a\u03b3\u0304(zml \u2192 z2) is inside \u03a6z and can be used for connectivity test. Therefore, in this work, we simplify the path-based connectivity test to the linear paths between latent variables as the similarity between two time-series can be inferred based on these linear paths. Overall, given two time-series Xi and Xj , we calculate the approximate distance d\u03b3\u0304(fE(Xi), fE(Xj)) via discrete points along path \u03b3\u0304, which is stored in element Sij of path-based distance matrix S. The approximate similarity graph G\u03b4 is then constructed with edges between samples Xi and Xj if and only if Si,j \u2264 \u03b4."
        },
        {
            "heading": "3.3 Predictive Clustering on Similarity Graph",
            "text": "Unfortunately, solving the clustering objective in (2) is a NP-hard combinatorial problem. Thus, we introduce a greedy approach to discover the temporal clusters from the path-based distance matrix S defined in the previous subsection.\nThe objective function in (2) has the following upper bound:\nJ , \u2211 Ck\u2208C \u2211 X\u2208Ck dy(f(X),vk),\n\u2264 \u2211 Ck\u2208C 1 |Ck| \u2211 Xi,Xj\u2208Ck dy(f(X i), f(Xj)),\n\u2264 \u2211 Ck\u2208C \u2211 Xi,Xj\u2208Ck d\u03b3\u0304(z i, zj),\n= \u2211 Ck\u2208C \u2211 Xi,Xj\u2208Ck Sij , J\u0304(S),\n(8)\nwhere zi = fE(Xi), latent translation \u03b3\u0304 is a linear path connecting two embeddings zi and zj . The first inequality comes from the convexity of the JS divergence, and the second inequality establishes from equation (7) and the fact that |Ck| \u2265 1. Local minimum of the upper bound J\u0304(S) can be achieved via a greedy K-partitioning algorithm based on pair-wise sample distances in matrix S.\nUtilizing the approximate solution in (8) as warm-start, we propose a graph-constrained K-means clustering approach to solve problem (2) via a greedy breadth-first search algorithm GK-means (details in Appendix). The overview of\nour predictive clustering method, T-Phenotype, is given in Algorithm 1. More details about the algorithm are provided in the Appendix.\nAlgorithm 1 T-Phenotype Input: dataset D, number of clusters K Output: C = {C1, C2, . . . , CK}\ncalculate distance matrix S based on (7) C \u2190 arg minC J\u0304(S) . warm-start \u03b4 \u2190 log(2) . upper bound of dJS while not converged do\nfor k = 1, 2, . . . ,K do update cluster seed ek via (9) end for \u03b4\u2032 \u2190 2 maxCk\u2208C,X\u2208Ck dy(f(X),vk) \u03b4 \u2190 min(\u03b4, \u03b4\u2032) . upper bound J \u2264 N\u03b4 create similarity graph G\u03b4 from Si,j \u2264 \u03b4 C \u2190 GK-means(J |e1, e2, . . . , eK ,G\u03b4)\nend while\nThe cluster seeds in Algorithm 1 are used to perform greedy cluster expansion over similarity graph G\u03b4. For the k-th cluster, the cluster seed ek = (vk,X(k)) can be given as\nvk = 1 |Ck| \u2211\nX\u2208Ck\nf(X), X(k) = arg min X\u2208Ck dy(f(X),vk),\n(9) where vk is the cluster centroid and X(k) is the representative time-series in cluster Ck with closest conditional to that of the centroid."
        },
        {
            "heading": "4 RELATED WORK",
            "text": "Different strands of clustering methods have been increasingly investigated for knowledge discovery from timeseries data with various similarity notions accustomed to specific application scenarios. One strand is unsupervised clustering methods that adopt the traditional notion of similarity into the time-series setting. To flexibly incorporate with variable-length irregularly-sampled time-series observations, the traditional methods applied K-means clustering by either finding fixed-length and low-dimensional representations using deep learning-based sequence-tosequence model (Ma et al., 2019; Zhang et al., 2019) or on modifying the similarity measure such as dynamic time warping (DTW) (Giannoula et al., 2018) and the associated graph Laplacian (Lei et al., 2019; Hayashi et al., 2005). Alternatively, Bahadori et al. (2015) focused on sample affinities to conduct spectral clustering, and Chen et al. (2022) proposed a deep generative model whose parametric space is then used for clustering. Further, advanced hidden Markov models (Ceritli et al., 2022) and Gaussian processes (Schulam et al., 2015) have also been utilized together with hierarchical graph models in disease subtype discovery. In general, these methods are limited by some model specifications such as the linear subspace assumptions and graphical models for the underlying data generation process.\nClusters identified through these methods are purely unsupervised \u2013 they do not account for patients\u2019 clinical outcomes that are often available in EHRs \u2013 which may lead to heterogeneous outcomes even for patients in the same cluster. To overcome this issue, another strand of clustering methods combine predictions on the future outcomes with clustering. Lee and van der Schaar (2020) proposed an actor-critic approach to divide time-series of patient trajectories into subgroups based on their associated clinical status. The discovered patient subgroups allow clinicians to investigate the temporal patterns related to the transition of disease stages. Aguiar et al. (2022) extended it to capture phenotype-related feature contributions by employing an attention mechanism. Given predicted clusters, visualizing the associated attention map provides additional interpretability about the underlying disease progression.\nUnfortunately, actionable information that can be inferred from the aforementioned temporal predictive clusters is still limited. These methods primarily focus on finding the discrete representations that can best describe the outcome labels rather without properly associating with temporal patterns that can be found among time-series samples. In this paper, we propose a novel temporal clustering method to correctly uncover predictive temporal patterns descriptive of the underlying disease progression from the labeled time-series data. Therefore, our method not only can provide clusters that have a prognostic value but also can offer interpretable information about the disease progression patterns."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In this section, we evaluate the clustering performance and the prognostic value of T-Phenotype with one synthetic dataset and two real-world datasets (detailed statistics are provided in the Appendix).\nSynthetic Dataset. We construct a synthetic dataset of N = 1200 samples with ground truth cluster labels. Each sample comprises discrete observations of a 2-dimensional trajectory x(t) and the target binary outcome. We design the two elements x1(t) and x2(t) to model trend and periodicity of a trajectory, respectively: we set x1(t) = \u03b9 \u00b7 sigmoid(a \u00b7 (t \u2212 b \u2212 \u03d5)) with sign \u03b9 \u2208 {\u22121, 1}, a = 10, b = 0.5, and \u03d5 \u223c exp( 310 ) and set x2(t) = sin(c \u00b7 (t\u2212\u03d5)) with c \u2208 {4, 6, 8} and \u03d5 identical\nto that of x1. The trajectory x = [x1, x2]> is irregularly observed over 20 time stamps in t \u2208 [0, 2] with a white noise N (0, 0.12) for each variable. We set c as the ground truth phenotype label representing different periodicity and set the target outcome label y as y = 0 when c = 6 and y = 1 otherwise.\nADNI Dataset. The Alzheimer\u2019s Disease Neuroimaging Initiative4 (ADNI) dataset includes records on the progression of Alzheimer\u2019s disease (AD) of N = 1346 patients with regular follow-ups every six months. Each patient is associated with various biomarkers, evaluation of MRI and PET images, and cognitive tests results. We set the target outcome at each time stamp as the three diagnostic groups \u2013 i.e., normal brain functioning (NL), mild cognitive impairment (MCI), and AD \u2013 which is used to indicate different stages of AD progression. We focus on three important temporal variables \u2013 i.e., the genetic biomarker of apolipoprotein (APOE) \u03b54 gene, the hippocampus evaluation from MRI, and the cognitive test result of CDRSB \u2013 to predict the AD progression.\nICU Dataset. The PhysioNet ICU5 (Goldberger et al., 2000) dataset contains temporal observations on 42 covariates of adult patients over the first 48 hours of ICU stay. We extract N = 1554 records of adult patients admitted to the medical or surgical ICU. Temporal covariates used in the experiments are age, gender, Glasgow Coma Scale (GCS), and partial pressure of arterial CO2 (PaCO2) with a time resolution of 1 hour, and we set patient mortality as the target binary outcome of interest.\nBaselines. We compare the performance of T-Phenotype with the following benchmarks ranging from traditional method to recently developed deep learning-based methods, where each clustering method reflects a different notion of temporal phenotypes: 1) K-means with warpingbased distance (KM-DTW); 2) deep temporal K-means with the encoder-predictor (E2P) structure introduced in (Lee and van der Schaar, 2020), i.e., KM-E2P(z) and KME2P(y); 3) K-means on top of our proposed Laplace encoder (KM-L); 4) sequence-to-sequence with K-means friendly representation space (SEQ2SEQ); and 5) the stateof-the-art temporal clustering approach AC-TPC (Lee and van der Schaar, 2020). Detailed description can be found\n4https://adni.loni.usc.edu 5https://physionet.org/content/challenge-2012/\nin Appendix. In addition, we consider the ablation study of T-Phenotype with joint optimization for the Laplace encoders and predictor fP and denote such model with T-Phenotype (J).\nThroughout the experiments, time stamps of discrete time-series are scaled into t \u2208 [0, 1]. For the synthetic and ADNI datasets, we use 64/16/20 train/validation/test splits in experiments. To get reliable clustering performance measurement on the ICU dataset, we use 48/12/40 train/validation/test splits for experiments. Hyperparameters of T-Phenotype and baselines are optimized through 3-fold cross-validation. For comparison of clustering performance, the number of clusters K for each dataset is shared by all methods. We select K as a hyperparameter of T-Phenotype, and the optimal cluster numbers are determined to be K = 3 (ground truth), K = 4 and K = 3 for the synthetic, ADNI and ICU dataset, respectively. Details can be found in the Appendix.\nPurity score, Rand index and normalized mutual information (NMI) are used to evaluate the clustering performance with ground truth phenotype labels. Best performance is highlighted in bold, and \u2021 indicates p-value< 0.01.\nThe area under the curve of receiving-operator characteristic (AUROC) and area under the curve of precision-recall (AUPRC) are used to assess the prognostic value of the discovered clusters on predicting target outcomes. Two composite metrics HROC and HPRC , calculated as harmonic means between predictive accuracy (AUROC or AUPRC) and a cluster consistency metric AUSIL, are used to measure the phenotype discovery performance. Please refer to the Appendix for details. Best performance is highlighted in bold, and \u2021 indicates p-value< 0.01.\nBenchmark. The clustering performance of TPhenotype is compared with six baselines, with all results reported using 5 random train/validation/test splits of the corresponding dataset. Benchmark results on synthetic dataset and two real-world datasets are provided in\nTable 2 and Table 3, respectively. Complete benchmark tables are available in the Appendix. On the synthetic dataset, T-Phenotype outperforms all baselines with significant gaps in considered clustering accuracy metrics. Similarly, T-Phenotype has the best (or very close to best) outcome prediction performance on both ADNI and ICU datasets and outperforms AC-TPC and most other baselines in phenotype discovery on the two datasets. The baseline of KM-E2P(y) directly discovers clusters over predicted outcome distributions and achieves the best prediction performance on the ADNI dataset, which is within expectation. However, its clustering performance, particularly HROC, is inferior to that of T-Phenotype due to the negligence of similarity in temporal patterns. On the ICU dataset, while T-Phenotype has close phenotype discovery performance HPRC to baseline SEQ2SEQ, the clusters discovered by our method provide greater prognostic values as reflected in the outcome prediction accuracy.\nPhenotypes of AD Progression. The CDRSB score measures the impairment on both cognitive abilities and brain function (Coley et al., 2011) and is widely used in AD progression assessment and staging (Kim et al., 2020; O\u2019Bryant et al., 2008). The temporal patterns in CDRSB trajectory vary in different disease stages and show stable prognostic power on patient outcomes (Delor et al., 2013). On the ADNI dataset, four phenotypes are discovered by T-Phenotype. We examine these phenotypes by plotting the CDRSB scores of Ntest = 270 test samples separately in corresponding clusters. As shown in Figure 3b, normal and high-risk patients with divergent cognitive test trajectories are correctly identified in phenotype 1 and 4 by T-Phenotype. In the meantime, for the predicted outcome of MCI, two subtypes of patients are clearly separated into two phenotypes (2 and 3) with different growth rates in CDRSB score. In comparison, AC-TPC fails to distinguish between these two subtypes as illustrated in Figure 3a, which impedes the prognostic value of clusters discovered by AC-TPC.\nPrognostic Value of T-Phenotype. We further demonstrate the prognostic value of T-Phenotype with the temporal phenotyping results obtained on a typical patient from the ADNI dataset. The studied patient had a positive biomarker of APOE \u03b54 gene which contributes to an increased risk of AD (Yamazaki et al., 2019). Consecutive observations of patient covariates at three time stamps are plotted in Figure 3c. Hippocampus volume (green triangle) and CDRSB score (blue dot) are displayed together with diagnosis obtained at the next follow-up (yellow bar). The temporal phenotype assignment via T-Phenotype is shown at the bottom. As a predictive factor of early-stage AD (Rao et al., 2022), fast decrease in hippocampus volume leads to the initial diagnosis of phenotype 2 (MCI) in Figure 3b by T-Phenotype despite a low CDRSB score from cognitive test. Then, with a clear trend of increase appearing in CDRSB trajectory, the studied patient is classified into phenotypes (2 \u2192 3 \u2192 4) that reflect the growing risk in developing AD. In contrast, as shown on the top of\nFigure 3c, AC-TPC simply assigns the same phenotype to the patient throughout the considered time period and is unable to provide comparable insights on AD progression from the patient trajectory."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we propose a novel phenotype discovery approach T-Phenotype to uncover predictive patterns from labeled time-series data. A representation learning method in frequency-domain is developed to efficiently embed the variable-length, irregularly sampled time-series into a unified latent space that provides insights on their temporal patterns. With our new notion of path-based phenotype similarity, a graph-constrained K-means approach is utilized to discover clusters representing distinct phenotypes. Throughout experiments on synthetic and real-world datasets, we show that T-Phenotype outperforms all baselines in phenotype discovery. The utility of T-Phenotype to discover clinically meaningful phenotypes is further demonstrated via comparison with the the state-of-theart temporal phenotyping method AC-TPC on real-world healthcare datasets."
        },
        {
            "heading": "7 LIMITATIONS",
            "text": "Our proposed method, T-Phenotype, leverages Laplace encoders as a general approach to capture temporal patterns from time-series data as distinct Laplace embeddings. However, there may exist some complex temporal patterns, e.g., interactions between patient covariates at two specific time points, that cannot be encoded in this manner. To address this issue, additional representations (e.g., representation via attention mechanism) from the input time-series can be introduced to augment the Laplace embedding, which we leave as a future work. In the meantime, the phenotype discovery performance of T-Phenotype is highly dependent on the quality of predictor fP . Unstable predictions from fP will directly lead to inaccuracies in phenotype assignment. Thus, effective regularization of\nthe predictor network would be another important future direction."
        },
        {
            "heading": "8 SOCIETAL IMPACT",
            "text": "Discovery of phenotypes from disease trajectories is a long pursuit in healthcare. In line with the target of precision medicine, the phenotype connects temporal patterns in patient trajectory and clinical outcomes is of great prognostic value since it allows clinicians to make more accurate diagnosis and issue the most appropriate treatment to their patients. By combining notions of similarity in both patient trajectories and clinical outcomes, our method, T-Phenotype, can effectively identify phenotypes of desired property. The discovered patient subgroups can be used to improve current clinical guidelines and help clinicians to better understand the disease progression of their patients. Nevertheless, the association between temporal patterns and clinical outcome in a phenotype cannot be interpreted as causal relationship without careful tests and examinations. Application of T-Phenotype without audits from human experts may lead to undesirable outcome of patients in certain edge cases."
        },
        {
            "heading": "Acknowledgements",
            "text": "Yuchao Qin was supported by the Cystic Fibrosis Trust. Changhee Lee was supported through the IITP grant funded by the Korea government (MSIT) (No. 2021-0- 01341, AI Graduate School Program, CAU). We thank all reviewers at AISTATS 2023 for their time in helping us to evaluate our work. Their insightful comments are greatly appreciated."
        },
        {
            "heading": "Appendix",
            "text": "The appendix is organized in the following structure.\nA Detailed discussion of the Laplace encoder. B Proof of Proposition 1 and relevant discussions. C The graph-constrained K-means algorithm D Experiment setup. E Hyperparameter Selection. F Complete benchmark results. G Additional analyses of results obtained on the two real-world datasets.\nA summary of major notations used in this paper is provided below."
        },
        {
            "heading": "NOMENCLATURE",
            "text": "x Continuous-time disease trajectory of a patient y Label vector indicating clinical status of a patient t A vector of time stamps z A vector of latent variables w A vector of Laplace embedding X Discrete-time observation to disease trajectory x(t) \u03a6 A connected set of patient trajectories which represents a temporal pattern g(x) Vector-valued function that describes the conditional distribution p(y|x) dy(\u00b7, \u00b7) Distance metric of two label distributions \u0393(x1 \u2192 x2) A translation from trajectory x1 to x2\n\u03b3(z1 \u2192 z2) A translation from latent representation z1 to z2\nd\u0393(x 1,x2) Path-based similarity score between trajectories x1 and x2\nd\u03b3(z 1, z2) Proxy of path-based similarity score d\u0393(x1,x2) in latent space\nS A distance matrix of path-based similarity score between samples in a dataset G\u03b4 A graph generated from matrix S with threshold \u03b4 K Number of clusters C A set of K clusters fL A Laplace encoder fE A composite encoder with feature-wise Laplace encoders fP A predictor for label distribution\nCode Availability. The source code of T-Phenotype can be found in the two GitHub repositories listed below:\n\u2022 The van der Schaar lab repo: https://github.com/vanderschaarlab/tphenotype \u2022 The author\u2019s personal repo: https://github.com/yvchao/tphenotype"
        },
        {
            "heading": "A ANALYSIS OF THE LAPLACE ENCODER",
            "text": "A.1 Implementation Details\nThe proposed Laplace encoder is implemented with a RNN-based neural network fL parameterized by \u03b8L. As shown in Figure A.1, given discrete time-series of a one-dimension trajectory x(t), the Laplace encoder first generates a summary of time-series x(t) via the RNN. With the summary as input, the MLP outputs a representation w \u2208 Cn(d+1).\nElements in w can be divided into two groups: poles and coefficients, which are further used to construct a function in the frequency domain, Fw(s), as defined in (3). Changing the order of poles (and associated coefficients) in w has no effect on Fw(s) since it is permutation-invariant to the poles in w. As discussed in the main manuscript and in the next paragraph, we impose a lexical order on poles in the MLP output w to make it a unique representation of Fw(s). The trajectory x(t) can be reconstructed as x\u0302(t) through the inverse Laplace transform (4) on Fw(s). Here, the reconstruction x\u0302(t) is a function and its value can be evaluated everywhere in t \u2208 [0, 1]. This allows us to compare input time-series with variable-length and irregularly-sampled observations in a unified latent space. For the sake of convenience, we denote with L\u22121(w) = x\u0302(t) = L\u22121[Fw(s)](t) the transform that maps embedding w to its time-domain reconstruction x\u0302(t).\nRobust Lexical Order of Poles. Due to the summation in (3), Fw(s) is permutation-equivariant with respect to the poles in w. Thus, we impose a lexical order (pm \u2264 pm+1 for m = 1, . . . , n \u2212 1) on the poles to obtain a unique Laplace embedding w as discussed in the manuscript. To guarantee this property, we transform the unordered representation (output of the MLP in Laplace encoder) into the final unique Laplace embedding w by sorting the poles (together with their associated coefficients) in a lexical order. To achieve a stable ordering that is robust to inevitable noise in w, we encourage any pair of two poles to be sufficiently different to avoid abrupt changes in their order. Hence, given two poles pm, pl, we say pm \u2264 pl if and only if (Re(pm) < Re(pl)) \u2227 (|Re(pm) \u2212 Re(pl)| > \u03b4pole) or (|Re(pm) \u2212 Re(pl)| \u2264 \u03b4pole) \u2227 (Im(pm) \u2264 Im(pl)), and pm > pl otherwise, where \u03b4pole \u2265 0 is a threshold that controls the robustness of the lexical order. The best threshold \u03b4pole is search as a hyperparameter in our experiment.\nRanges of Poles and Coefficients. Each pole pm in embedding w is located on the complex plane C. The real part Re(pm) indicates the increase or decay speed of the corresponding component (eRe(pm)t) in the time-domain reconstruction x\u0302(t) = L\u22121(w). Too large or small value of Re(pm) leads to unrealistic signals. In the meantime, The imaginary part Im(pm) represents the frequency of oscillations in the related component (cos(Im(pm)t) + j sin(Im(pm)t), j\n2 = \u22121) in reconstruction x\u0302(t). Very high-frequency oscillation in the input time-series x(t) are usually caused by random noise and should be discarded in reconstruction x\u0302(t). In our experiment, we limit the range of poles to the area of {p \u2223\u2223 |Re(p)| \u2264 rmax, |Im(p)| \u2264 freqmax}, where rmax limits that maximum increase or decrease speed of signals in reconstruction x\u0302, freqmax is the maximum allowed frequency such that high-frequency signals above freqmax are considered as a noise component in time-series x(t) and, thus, discarded when constructing Fw(s). In our experiments, we set rmax = 10 and freqmax = 20Hz. Similarly, the coefficient cm,l in embedding w is limited to a square area of {c\n\u2223\u2223 |Re(c)| \u2264 cmax, |Im(c)| \u2264 cmax}. We set cmax = 5 which is sufficient for normalized time-series (via min-max or normal scaling). The range of poles and coefficients in w can be adjusted accordingly based on needs in practical application scenarios. When fed into the predictor network fP , the poles and coefficients in embedding w are normalized by the corresponding maximum allowed values to facilitate the learning process.\nEmbedding of Static Features. In order to improve computation efficiency, when the d-th feature dimension xd of trajectory x is known to be constant over time, i.e., xd(t) \u2261 xd(0), instead of training a Laplace encoder, the static value xd(0) is directly used to represent xd(t), and the d-th component wd in latent variable z is replaced by xd(0).\nRegularization Terms. Apart from the lexical order imposed on the embedding w, we further introduce three regularization terms that encourage the Laplace encoder to provide a unique and consistent Laplace representation given an input time-series. These regularization terms are combined into the second term of Lunique in (5); we will describe each in turn.\nThe first regularizer, lsep, penalizes the case where two poles in embedding w are nearly identical \u2013 that is, pm and pl are considered as an identical pole when |pm \u2212 pl| \u2264 \u03b4pole \u2013 based on the following hinge loss:\nlsep(x\u0302(t)) = \u2211 m6=l max(0, \u03b4pole \u2212 |pm \u2212 pl|). (10)\nHere, pm and pl are two poles in the associated embedding w given the input time-series x(t), i.e., w = fL(x(t)), and the threshold \u03b4pole > 0 for robust pole sorting is reused here as a pole separation threshold.\nThe second regularizer, lreal, ensures that the reconstructed trajectory x\u0302(t) is real-valued on [0, 1] by suppressing the imaginary part of the reconstructed trajectory x\u0302(t) via the following loss:\nlreal(x\u0302(t)) = 1\nT \u2016Im(x\u0302(t))\u201622, (11)\nwhere t = [t1, . . . , tT ]> includes time stamps randomly sampled over tj \u2208 [0, 1] for j = 1, . . . , T . Specifically, tj = clamp( j T + 1 2T \u03b5,min = 0,max = 1), \u03b5 \u223c Normal(0, 1).\nThe last regularizer, ldistinct, encourages that no two distinct Laplace embeddings generate the same trajectory based on the following loss:\nldistinct(x\u0302 i(t), x\u0302j(t)) = \u2016wi \u2212wj\u201622e\u2212\u2016x\u0302 i(t)\u2212x\u0302j(t)\u201622 , (12)\nwhere the radial basis similarity function e\u2212\u2016x\u0302 i(t)\u2212x\u0302j(t)\u201622 is used to discover similar trajectories, wi and wj are embeddings of input time-series while x\u0302i(t) and x\u0302j(t) are their time-domain reconstructions. Since the input timeseries may be of different lengths and sampling intervals, we use the reconstructed trajectories for pair-wise comparison between time-series here.\nOverall, we construct Lunique(\u03b8L) as a combination of the three regularization terms introduced above:\nLunique(\u03b8L) = dimx\u2211 d=1  1 N \u2211 i lsep(x\u0302 i d(t)) + \u03b11 \u03b1 lreal(x\u0302 i d(t)) + \u03b12 \u03b1\n1 N(N \u2212 1) \u2211 i 6=j ldistinct(x\u0302 i d(t), x\u0302 j d(t))  , (13) where \u03b1 is the coefficient for Lunique(\u03b8L) in (5), \u03b11 and \u03b12 are balancing coefficients that trade-off different uniqueness properties in the Laplace encoder. In the experiment, due to the high computational complexity, the last term ldistinct is only evaluated on a subset of 10 randomly selected time-series in each training batch. In addition, since ldistinct relies on the reconstructed time-series x\u0302(t) which may be inaccurate in the beginning of training, we fix \u03b12 to 0.01 such that it majorly takes effect after the reconstruction error is small enough."
        },
        {
            "heading": "A.2 Quantitative Analysis",
            "text": "Comparison with Regular Auto-encoder. We provide a toy example to demonstrate the advantage of our proposed Laplace encoder over regular auto-encoders in time-series reconstruction. A Laplace encoder composed of a 1-layer GRU (Cho et al., 2014) and a 1-layer MLP with 10 hidden units in each layer is considered in the following discussion. Other parameters of the Laplace encoder are set as n = 4, d = 1, \u03b1 = 1.0, \u03b11 = 0.1, \u03b12 = 0.01, \u03b4pole = 1.0. A regular time-series auto-encoder is used for comparison. The auto-encoder has a 1-layer GRU network as the encoder. The decoder contains a 1-layer MLP on top of another 1-layer GRU. Each layer in the auto-encoder includes 10 hidden units. The auto-encoder maps the input time-series to a latent variable. Then, the latent variable is provided to the decoder network for reconstruction of the entire time-series.\nConsider a toy dataset with N = 1000 irregularly sampled time-series in t \u2208 [0, 1]. Each sample contains T = 15 observations from one of the following four types of trajectories:\n\u2022 Type 1: x(t) = cos(2\u03c0(t\u2212 \u03c6)). \u2022 Type 2: x(t) = cos(\u03c0(t\u2212 \u03c6)). \u2022 Type 3: x(t) = sin(\u03c0(t\u2212 \u03c6)). \u2022 Type 4: x(t) = sin(2\u03c0(t\u2212 \u03c6)).\nDelay term \u03c6 \u223c Exp( 12 ). Gaussian noise sampled from Normal(0, 0.03 2) is independently introduced to the observations at different time points. The mean squared error (MSE) in time-series reconstruction of the considered Laplace encoder and auto-encoder network is evaluated over 5 random splits of the toy dataset with the train/validation/test ratio of 64/16/20. Our proposed Laplace encoder achieves the best performance of MSE = 0.039\u00b1 0.008. The auto-encoder has a much higher reconstruction error of MSE = 0.108\u00b1 0.019. Comparison of typical reconstruction outcomes of the Laplace encoder and the auto-encoder is illustrated in Figure A.2.\nSensitivity Analysis. We further conduct a sensitivity analysis of the Laplace encoder fL under different hyperparameters on the toy dataset. The default hyperparameters are set as n = 4, d = 1, \u03b1 = 1.0, \u03b11 = 0.1, \u03b12 = 0.01, \u03b4pole = 1.0. To evaluate the impact of individual hyperparameter on the Laplace encoder, in each test, we only alter the value of one\nhyperparameter and keep other hyperparameters the same as default setting. The parameter sensitivity is measured via the reconstruction error (MSE), and the sensitivity test result is given in Figure A.3.\nIt can be found that our proposed Laplace encoder fL has relatively stable time-series reconstruction performance under different hyperparameters. As mentioned earlier, the regularizer lditinct may generate wrong gradients in the beginning of training due to the large reconstruction error. The increased MSE for larger \u03b12 in Figure A.3c is within expectation, and we choose to set \u03b12 to 0.01 such that it only takes effect when the reconstruction error is small enough.\nIn addition, the effect of pole separation threshold \u03b4pole on the Laplace embedding is illustrated in Figure A.4. When \u03b4pole = 0.0, the order of poles in Laplace embedding w can easily be affected by random noise in input time-series, which makes it difficult to ensure the uniqueness of w. In contrast, setting \u03b4pole = 1.0 effectively improves the representations learned by the Laplace encoder, and different components in the Laplace transform Fw(s) are clearly represented by distinct poles (marked with different colors).\nImpact of Sampling Rate in Input Data. The Nyquist Sampling Theorem states that a band-limited signal (maximum frequency of B) can be perfectly reconstructed from sequential observations with (average) sampling rate above 2B. It provides a lower bound on the number of time-series observations required for our proposed Laplace encoder to work. Thus, we assume that the sampling rate in real-world datasets is sufficiently large so that important temporal patterns can be correctly identified. To validate the above statement, we conduct a synthetic experiment on time-series\ndata generated by x(t) = sin(2\u03c0t+ \u03d5) where \u03d5 \u223c Exp( 12 ) with different sampling rates. Figure A.5 demonstrates that the reconstruction error of the Laplace encoder converges to zero when the sampling rate is sufficiently large."
        },
        {
            "heading": "B PROOF OF PROPOSITION 1",
            "text": "Proposition 1 states that, given two Laplace embeddings z1 and z2 in latent space Z , the distance between their corresponding time-domain trajectories x\u03021 and x\u03022 is upper-bounded by \u03c8\u2016z1 \u2212 z2\u201622 with some scalar \u03c8 > 0. The proof of Proposition 1 can be derived as the following:\nProof. Let us first consider the uni-variate case. Given two arbitrary Laplace embeddings w1,w2 \u2208 Cn(d+1), their timedomain reconstructions can be obtained via inverse Laplace transform, i.e., x\u0302i(t) = L\u22121(wi) , L\u22121[Fwi(s)](t), i = 1, 2. According to (3) and (4), we have\nx\u0302i(t) = n\u2211 m=1 d\u2211 l=1 cim,lt l\u22121 \u0393(l) ep i mt, t \u2265 0, (14)\nwhere \u0393(l) = (l \u2212 1)! is the Gamma function, wi = [pi1, pi2, . . . , ci1,1, . . . , cin,d]>, i = 1, 2.\nDifference in One Coefficient. Suppose w1 and w2 only differ at one coefficient cm,l, which leads to the result \u2016w1 \u2212w2\u201622 = |c1m,l \u2212 c2m,l|2. Then,\n\u2016x\u03021 \u2212 x\u03022\u20162L2 [0,1] = \u222b 1 0 |x\u03021(t)\u2212 x\u03022(t)|2dt,\n= \u222b 1 0 |c1m,l \u2212 c2m,l|2 \u2223\u2223\u2223\u2223 tl\u22121\u0393(l)epmt \u2223\u2223\u2223\u22232dt, \u2264 |c1m,l \u2212 c2m,l|2\u03c8cm,l = \u03c8cm,l\u2016w1 \u2212w2\u201622,\n(15)\nwhere \u03c8cm,l is some suitable constant.\nDifference in One Pole. Now, let us consider the case where w1 and w2 only differ at one pole pm which gives \u2016w1 \u2212w2\u201622 = |p1m \u2212 p2m|2. Without loss of generality, we assume p2m \u2212 p1m = r + j\u03b8, where r \u2264 0, j2 = \u22121. The following inequality can be established when t \u2208 [0, 1]:\n|1\u2212 e(p 2 m\u2212p 1 m)t|2 = |1\u2212 ert(cos(\u03b8t)\u2212 j sin(\u03b8t))|2,\n= (1\u2212 ert)2 + 2ert(1\u2212 cos(\u03b8t)),\n\u2264 (1\u2212 ert)2 + ert\u03b82t2, (via ert > 0 and 1\u2212 cos(x) \u2264 x 2\n2 )\n\u2264 r2t2 + ert\u03b82t2, (via r \u2264 0 and 0 \u2264 1\u2212 ert \u2264 (\u2212r)t) \u2264 (r2 + \u03b82)t2, = |p1m \u2212 p2m|2t2.\n(16)\nHence, we have\n\u2016x\u03021 \u2212 x\u03022\u20162L2 [0,1] = \u222b 1 0 |x\u03021(t)\u2212 x\u03022(t)|2dt,\n= \u222b 1 0 \u2223\u2223\u2223\u2223ci,ltl\u22121\u0393(l) \u2223\u2223\u2223\u22232|ep1mt|2|1\u2212 e(p2i\u2212p1i )t|2dt,\n\u2264 \u222b 1\n0\n\u2223\u2223\u2223\u2223ci,ltl\u22121\u0393(l) \u2223\u2223\u2223\u22232|ep1mt|2|p1m \u2212 p2m|2t2dt,\n\u2264 |p1m \u2212 p2m|2\u03c8pm = \u03c8pm\u2016w1 \u2212w2\u201622,\n(17)\nwhere \u03c8pm is some suitable constant.\nGeneral Cases. Now, we define an operator Si(w1,w2) that generates a new composite vector w\u0304i from w1 and w2. The first i elements of the composite vector w\u0304i are taken from w2 while the latter n(d + 1) \u2212 i elements of w\u0304i are obtained from w1. For instance, we have S0(w1,w2) = w1, S1(w1,w2) = [p21, p 1 2, p 1 3, . . . , c 1 1,1, . . . , c 1 n,d] >, S2(w 1,w2) = [p21, p 2 2, p 1 3, p 1 4 . . . , c 1 1,1, . . . , c 1 n,d] >, . . ., and Sn(d+1)(w1,w2) = w2. It is easy to see that Si(w1,w2) and Si+1(w1,w2) only differ at one pole or one coefficient, and \u2016Si(w1,w2)\u2212 Si+1(w1,w2)\u201622 = |p1i+1 \u2212 p2i+1|2 when 0 \u2264 i \u2264 n \u2212 1 and \u2016Si(w1,w2) \u2212 Si+1(w1,w2)\u201622 = |c1m,l \u2212 c2m,l|2 otherwise, where m = b i\u2212nd + 1c, l = i\u2212 n\u2212 (m\u2212 1)d+ 1. Each composite vector w\u0304i = Si(w1,w2) yields a time-domain trajectory L\u22121(w\u0304i) via inverse Laplace transform of Fw\u0304i(s).\nNote that L\u22121(w\u03040) = L\u22121(w1) = x\u03021 and L\u22121(w\u0304n(d+1)) = L\u22121(w2) = x\u03022. Based on the triangular inequality,\n\u2016x\u03021 \u2212 x\u03022\u20162L2 [0,1]\n= \u2016 n(d+1)\u22121\u2211\ni=0\nL\u22121(Si(w1,w2))\u2212 L\u22121(Si+1(w1,w2))\u20162L2 [0,1]\n\u2264 n(d+1)\u22121\u2211\ni=0\n\u2016L\u22121(Si(w1,w2))\u2212 L\u22121(Si+1(w1,w2))\u20162L2 [0,1] ,\n\u2264 n(d+1)\u22121\u2211\ni=0\n\u03c8\u2016Si(w1,w2)\u2212 Si+1(w1,w2)\u201622,\n= n\u2211 m=1 \u03c8|p1m \u2212 p2m|2 + n\u2211 m=1 d\u2211 l=1 \u03c8|c1m,l \u2212 c2m,l|2,\n= \u03c8\u2016w1 \u2212w2\u201622,\n(18)\nwhere we take \u03c8 = maxm,l(\u03c8pm, \u03c8 c m,l).\nFinally, for the multivariate case, let us consider two latent embeddings z1 and z2 as well as their associated time-domain reconstructions x\u03021 and x\u03022. We define the distance between trajectories x\u03021 and x\u03022 as\n\u2016x\u03021 \u2212 x\u03022\u20162L2 [0,1] , dimx\u2211 d=1 \u222b 1 0 |x\u03021d(t)\u2212 x\u03022d(t)|2dt, (19)\nwhere x\u0302md is the d-th dimension of trajectory, x\u0302 m = L\u22121(wmd ) = L\u22121[Fwmd (s)] for m = 1, 2, w m d is the d-th component of zm. According to (18), we have the following bound for each dimension d.\u222b 1 0 |x\u03021d(t)\u2212 x\u03022d(t)|2dt = \u2016x\u03021d \u2212 x\u03022d\u20162L2 [0,1] \u2264 \u03c8d\u2016w1d \u2212w2d\u201622, (20)\nwhere \u03c8d > 0 is some suitable scalar. Since \u2016z1 \u2212 z22\u201622 = \u2211dimx d=1 \u2016w1d \u2212 w2d\u201622, the distance between the two reconstructed trajectories x\u03021 and x\u03022 can be upper-bounded as follows with some suitable \u03c8 > 0.\n\u2016x\u03021 \u2212 x\u03022\u20162L2 [0,1] \u2264 dimx\u2211 d=1 \u03c8d\u2016w1d \u2212w2d\u201622 \u2264 \u03c8\u2016z1 \u2212 z2\u201622. (21)\nCorollary 1. Given a continuous set \u03a6z in latent space, the set \u03a6, which consists of reconstructed trajectories of z \u2208 \u03a6z , is also a continuous set in trajectory space X .\nProof. Consider a trajectory x\u0302 \u2208 \u03a6 and its corresponding latent embedding z \u2208 \u03a6z . For any \u03b5 > 0, due to the continuity of \u03a6z , there must exist another embedding z\u2032 \u2208 \u03a6z such that \u2016z \u2212 z\u2032\u201622 < \u03b4\u03b5, where \u03b4 > 0 is a scalar. Let us denote the time-domain reconstruction of z\u2032 as x\u0302\u2032 \u2208 \u03a6. According to Proposition 1, \u2016x\u0302\u2212 x\u0302\u2032\u20162\nL2 [0,1] \u2264 \u03c8\u2016z \u2212 z\u2032\u201622 holds for some \u03c8 > 0. Setting \u03b4 = 1\u03c8 leads to the inequality \u2016x\u0302\u2212 x\u0302 \u2032\u20162 L2\n[0,1] \u2264 \u03b5 which indicates the continuity of set \u03a6.\nEquivalent Translation in the Latent Space. Consider two trajectories x1,x2 \u2208 X with the corresponding latent embeddings z1 and z2 in the latent space. We construct a set Pz = {\u03b3\u0303(z1 \u2192 z2)} of all possible continuous path \u03b3\u0303 in the latent space that connects z1 and z2. Let gE : Z \u2192 X be a function that maps latent embedding z back to its time-domain reconstruction x\u0302 in the trajectory space. Then, given a translation \u0393(x1 \u2192 x2) in the trajectory space, we can define the (approximately) equivalent translation in the latent space as\n\u03b3(z1 \u2192 z2) , arg min \u03b3\u0303\u2208Pz min z\u2208\u03b3\u0303 max x\u2208\u0393 \u2016x\u2212 gE(z)\u20162L2 [0,1] , (22)\nwhere minz\u2208\u03b3\u0303 maxx\u2208\u0393 \u2016x\u2212 gE(z)\u20162L2 [0,1] measures the minimum distance between translation, i.e., \u0393, and the timedomain reconstruction of latent path \u03b3\u0303, i.e., \u0393\u0303 = {gE(z) | z \u2208 \u03b3\u0303}. In general, \u03b3 is the closet projection of \u0393 within the latent space Z , and the equivalence of trajectory translation is approximate. If every trajectory x \u2208 \u0393 has a rational Laplace transform with no more than n poles and maximum degree of d as described in (3), the equivalence becomes strict. Without loss of generality, let us consider the uni-variate case. Given a translation \u0393, we assume each x \u2208 \u0393 can be exactly described by the Laplace transform Fw(s) in (3), where w = fL(x(t)), t is a vector of some suitable sampling time stamps. For any two trajectories x, x\u2032 \u2208 \u0393 that satisfy |x(t)\u2212 x\u2032(t)| \u2264 \u03b4 almost everywhere in t \u2208 [0, 1], we have\n|Fw(s)\u2212 Fw\u2032(s)|2 = \u2223\u2223\u2223\u2223\u222b \u221e\n0\n(x(t)\u2212 x\u2032(t))e\u2212stdt \u2223\u2223\u2223\u22232 ,\n\u2264 \u222b \u221e\n0\n|x(t)\u2212 x\u2032(t)|2|e\u2212st|2dt,\n\u2264 \u03b42 \u222b \u221e\n0\n|e\u2212st|2dt,\n= \u03b42\n2Re(s) ,\n(23)\nholds for Re(s) > 0. When \u03b4 \u2192 0, we have x\u2032 \u2192 x and Fw\u2032 \u2192 Fw. Note that Fw \u2212 Fw\u2032 is rational and can be determined with a sufficient number of observations in its region of convergence, e.g., Re(s) > 0. The equivalence in Laplace transform, i.e., |Fw(s) \u2212 Fw(s)|2 \u2261 0, implies that w\u2032 = w.6 Thus, x\u2032 \u2192 x also leads to w\u2032 \u2192 w, which means that the collection of Laplace embeddings {w|w = fL(x(t)), x \u2208 \u0393} is in fact a continuous path \u03b3 in the latent space. Thereby, path \u03b3 is a latent translation that exactly yields the trajectory translation \u0393. Similar results can be easily extended to the multi-variate trajectory setting.\n6When Fw(s) and Fw\u2032(s) have less than n poles, w and w\u2032 may take value from multiple alternative embeddings. However, we can always select the combination such that w\u2032 = w.\nJustification for Latent Path-based Test. The path-based connectivity test d\u0393(x1,x2) is defined based on the oracle model g(x) of conditional distribution p(y|x). In our proposed method T-Phenotype, a predictor is built upon the Laplace embedding, i.e., f(X) = fP \u25e6 fE(X), to approximate the oracle conditional distribution such that f(X) \u2248 g(x) given time-series X sampled from x. Thus, we have d\u0393(x1,x2) \u2248 maxx\u2208\u0393,i=1,2 dy(f(x(t)), f(Xi)), where t is a vector of some suitable observation time stamps. Further, note that translation \u0393 in trajectory space can be approximated by \u0393\u0302 as time-domain reconstruction of latent translation \u03b3(z1 \u2192 z2) in Z , where zi = fE(Xi) for i = 1, 2. Then, we have\nmax x\u2208\u0393,i=1,2\ndy(f(x(t)), f(X i)) \u2248 max\nx\u0302\u2208\u0393\u0302,i=1,2 dy(f(x\u0302(t)), f(X i)) \u2248 max z\u2208\u03b3,i=1,2 dy(fP (z), fP (z i)), (24)\nwhich leads to the latent path-based test in (7).\nC GRAPH-CONSTRAINED K-MEANS ALGORITHM IN T-PHENOTYPE\nThe graph-constrained K-means iteration in Algorithm 1 is provided in Algorithm C.1. After each run via GKmeans, the objective function J in (2) is re-evaluated. The main algorithm of T-Phenotype stops after 5 iterations with no improvement in objective J under maximum of 1,000 iterations. Alternatively, T-Phenotype stops when the improvement is below certain tolerance tol = 10\u22127, i.e., |\u2206J | \u2264 tol.\nAlgorithm C.1 GK-means (Single K-means iteration over similarity graph G\u03b4) Input: J, e1, e2, . . . , eK ,G\u03b4 . J objective, ek cluster seed, G\u03b4: similarity graph Output: C = {C1, C2, . . . , CK}\n1: for k = 1, 2, . . . ,K do 2: vk,X(k) \u2190 ek 3: Ck \u2190 {X(k)} . Initialize cluster Ck with seed ek 4: end for 5: Dfree \u2190 {X|X 6\u2208 Ck, \u2200Ck \u2208 C} . Get the set of unclustered samples 6: while |Dfree| > 0 do 7: for X \u2208 Dfree do 8: C\u2217 \u2190 arg min\nCk\u2208C,X G\u03b4\u2190\u2192Ck\ndy(f(X),vk) . Find the best cluster assignment\n9: C\u2217 \u2190 C\u2217 \u222a {X} 10: Dfree \u2190 Dfree \\ {X} 11: end for 12: for k = 1, 2, . . . ,K do 13: vk \u2190 1|Ck| \u2211 X\u2208Ck f(X) . Update cluster centroid 14: end for 15: end while"
        },
        {
            "heading": "D EXPERIMENT SETUP",
            "text": ""
        },
        {
            "heading": "D.1 Datasets and Statistics",
            "text": "For the two real-world medical datasets, we want to capture recent temporal patterns and associated target outcomes. Thus, we utilize a sliding window of size 6 years and 24 hours to extract sub-sequences containing temporal predictive patterns among most recent observations for ADNI and ICU datasets, respectively. Statistics of major feature variables in the ADNI dataset and ICU dataset can be found in Table D.1 and Table D.2, respectively."
        },
        {
            "heading": "D.2 Baselines",
            "text": "We compare the performance of T-Phenotype with the following five benchmarks ranging from traditional method to state-of-the-art deep learning-based methods, where each clustering method reflects a different notion of temporal phenotypes:\nK-means with Warping-based Distance. The technique of dynamic time warping (DTW) provides one way to measure time-series similarity regardless of the observation interval. Time-series with similar temporal patterns usually\nleads to smaller DTW distances. We apply conventional K-means with the DTW-based similarity measure to discover clusters representing different temporal patterns. We denote this approach as KM-DTW.\nDeep TemporalK-means. Embedding (i.e., hidden representations) from RNNs can provide meaningful information to measure the similarity between time-series. With the encoder-predictor (E2P) structure introduced in (Lee and van der Schaar, 2020), we include the baseline of KM-E2P that performs clustering in a representation space via K-means. We denote the baseline as KM-E2P(z) when The representation space is formed by the latent embeddings from an encoder network. The discovered cluster will capture both similarities in input time-series and the output label prediction due to the E2P structure. When the representation space is selected to be the output (label prediction) of the predictor network, we refer to the method as KM-E2P(y). In this case, the discovered clusters are aligned to major modes in the label distribution and are not necessarily associated with certain temporal patterns in trajectory space.\nK-means with Laplace Encoder. Similar to the baseline of KM-DTW, the time-series embedding from Laplace encoder provides a unified representation of (potentially) irregularly sampled time-series. The Euclidean distance between Laplace embeddings can thus be used as a similarity measure for different patient trajectories. In practice, the longitudinal observations of patients are first converted to a latent space via the Laplace encoder. Then, K-means algorithm is performed over the latent representations to identify patient subgroups based on their similarity in temporal patterns.\nToward K-means Friendly Spaces using Sequence-to-sequence. Sequence-to-sequence (SEQ2SEQ) learning paradigm allows the learning of a representation space that is easier to perform clustering compared to the original time-series data. Such baseline reflects the recent trend of combining conventional clustering methods, e.g.,\nK-means, with dimension reduction using deep learning technique (Xie et al., 2016; Baytas et al., 2017). With different temporal patterns encoded in a low-dimension representation space, K-means clustering is applied to discover clusters that represent various temporal feature interactions in input time-series data. In the experiment, we use a modified version of DCN (Yang et al., 2017) as the SEQ2SEQ baseline.\nAC-TPC. AC-TPC (Lee and van der Schaar, 2020) is one of the state-of-the-art temporal clustering approach that discovers outcome-oriented clusters. AC-TPC learns a cluster assignment policy in the latent space based on an encoder network. The cluster assignment policy is trained with the actor-critic loss from reinforcement learning to find the optimal clusters that represent typical label distributions learned by a predictor network. Similar to KM-E2P(y), there is no guarantee on the association between temporal patterns and clusters discovered by AC-TPC."
        },
        {
            "heading": "D.3 Training Procedure of T-Phenotype",
            "text": "To fit the model of T-Phenotype on a dataset, the Laplace encoder for each trajectory dimension is firstly pre-trained based on (5) calculated at each time step. Then, we fit the predictor fL with observed patient outcomes y. Finally, the temporal clusters are discovered via graph-constrained K-means algorithm C.1 based on the output from the predictor. Latent embeddings from the Laplace encoder has a clear mathematical meaning. Thus, we freeze the pre-trained Laplace encoder to be isolated from gradients due to outcome predictions. Joint optimization of the encoder and predictor may lead to slower convergence and lower performance as shown in Table 2 and Table 3 with \u201cT-Phenotype (J)\u201d as the ablation study."
        },
        {
            "heading": "D.4 Performance Metrics",
            "text": "Prediction Performance. Area under the curve of receiving-operator characteristic (AUROC) and area under the curve of precision-recall (AUPRC) are used to assess the prognostic value of the discovered clusters on predicting the target label y. For non-binary (category larger than 2) labels, these scores are calculated individually for each category and averaged over the entire categories.\nClustering Performance. For synthetic data, we evaluate the clustering performance in terms of the purity score (Lee and van der Schaar, 2020), adjusted Rand index (RAND) (Steinley, 2004), and normalized mutual information (NMI) (Vinh et al., 2009) as the ground-truth cluster label is available. For the real-world dataset, there is no ground-truth of cluster label. In such a case, the Silhouette coefficient (Rousseeuw, 1987) is commonly used as a measure of cluster consistency by assessing the homogeneity within each cluster and heterogeneity across different clusters. More specifically, the traditional Silhouette index assumes convex clusters and uses the average intra-cluster distance (a) and inter-cluster distance (b) to evaluate the consistency between cluster assignment and pattern distribution as s = |b\u2212a|max(a,b) . Averaging s over all samples gives the Silhouette index S.\nIn this paper, the clusters are identified via predictive temporal patterns and are not necessarily in convex shapes. To better reflect our new notion of clusters, we instead use an m-nearest neighbor version of Silhouette index, i.e., Sm. Specifically, suppose there are K clusters C = {C1, C2, . . . , CK}. Given a time-series X in cluster Ck, we only consider its m nearest samples in the corresponding cluster when calculating intra- and inter-cluster distances am and bm as given below:\nam = 1 |Nm(X, Ck)| \u2211\nX\u2032\u2208Nm(X,Ck)\n\u2016X \u2212X \u2032\u201622, bm = min i 6=k\n1 |Nm(X, Ci)| \u2211\nX\u2032\u2208Nm(X,Ci)\n\u2016X \u2212X \u2032\u201622, (25)\nwhere Nm(X, Ck) indicates the set of m nearest neighbors of X in cluster Ck. Then, the clustering consistency in our variant Silhouette index is calculated as sm = |b\nm\u2212am| max(am,bm) . The average score S\nm of all samples is used to measure the overall clustering consistency. Note that when m \u2265 maxCk\u2208C |Ck|, the variant Sm is identical to the original Silhouette index, i.e., Sm = S.\nFocusing on m closest samples allows us to effectively evaluate pattern consistency in non-convex and irregularly shaped clusters. Nonetheless, when multiple temporal patterns are put into the same cluster, Sm may still generate a high score due to the focus on local similarity. To address this issue, we use another connectivity-based metric Pm to evaluate the purity of a cluster in terms of temporal patterns. Consider a cluster Ck, a connectivity graph over time-series in Ck can be derived via m-nearest neighbor discovery. We use the count pk of connected subgraphs to estimate the number of temporal pattern included in cluster Ck and calculate the temporal pattern purity via Pm = 1K \u2211 Ck\u2208C 1 pk\n. It is clear that Pm = 1 when m is sufficiently large and each cluster only contains a single temporal pattern, and Pm = 1K \u2211 Ck\u2208C 1 |Ck| when m = 0.\nTo get an overall assessment of cluster consistency, we normalize Sm into [0, 1] and calculate the summary metric AUSIL as the area under the curve of Sm verses Pm for m = 1, 2, . . . ,M,M \u2208 N. For the evaluation of phenotype discovery, we combine the prediction accuracy (AUROC and AUPRC) and cluster consistency (AUSIL) into two composite metrics HROC and HPRC. Similar to the F1-score in classification, these composite metrics are defined respectively as\nHROC , 2 AUROC \u00b7AUSIL AUROC + AUSIL , HPRC , 2 AUPRC \u00b7AUSIL AUPRC + AUSIL . (26)"
        },
        {
            "heading": "E HYPERPARAMETER SELECTION",
            "text": "In the experiment, T-Phenotype, KM-E2P(y), KM-E2P(z) are implemented with PyTorch and are trained with learning rate of 0.1 in 50 epochs. AdamW optimizer is used to tune the network parameters. The K-means clustering in KM-E2P(y), KM-E2P(z) and KM-DTW is performed with K-means++ initialization based on implementation in PyClustering.7 The baselines of AC-TPC and SEQ2SEQ are implemented in TensorFlow. They are trained with Adam optimizer with training epochs set to 200 due to different learning rates in their implementation.\nWe perform hyperparameter selection on each dataset via 3-fold cross-validation. For T-Phenotype, the best hyperparameters of the Laplace encoders are searched to minimize the average reconstruction error over all temporal dimensions. For each real-world dataset, the best number of clusters K is searched via maximizing the composite metric HPRC of T-Phenotype. The selected best cluster number K is used for all baselines on the same dataset. For baselines of KM-E2P(y) and KM-E2P(z), the hyperparameters for each dataset are search to maximize HPRC (or purity score on the synthetic dataset) given the selected cluster number K. The hyperparameters of AC-TPC and SEQ2SEQ are set to be the same with the original implementation in (Lee and van der Schaar, 2020) (dropout layers are disabled to ensure reproducibility). The hyperparameter space considered in our experiment is discussed as follows."
        },
        {
            "heading": "E.1 Hyperparameter Selection of T-Phenotype",
            "text": "Laplace Encoder. In the experiment, each Laplace encoder fL in T-Phenotype contains a 1-layer GRU and a 1-layer MLP with 10 hidden units in each layer. Given a time-series input, each Laplace encoder generates an embedding with n = 4 poles and maximum degree of d = 1. As mentioned earlier, coefficient \u03b12 for regularization term ldistinct is set to 0.01 throughout the experiment. The rest hyperparameters are searched in the parameter space as follows.\n\u2022 Coefficient for pole separation loss lsep: \u03b1 \u2208 {1.0, 10.0}. \u2022 Coefficient loss lreal: \u03b11 \u2208 {0.1, 1.0}. \u2022 Threshold for pole sorting and the separation loss: \u03b4pole \u2208 {1.0, 2.0}.\nTo address the complex temporal patterns in the ICU dataset, the maximum degree of poles d is also added to the search space, and the range of d \u2208 {1, 2} is considered. The best hyperparameter for Laplace encoder on the three datasets are given as follows.\n\u2022 Synthetic dataset: \u03b1 = 1.0, \u03b11 = 0.1, \u03b4pole = 1.0.\n\u2022 ADNI dataset: \u03b1 = 1.0, \u03b11 = 0.1, \u03b4pole = 2.0.\n\u2022 ICU dataset: \u03b1 = 1.0, \u03b11 = 0.1, \u03b4pole = 2.0, d = 2.\nPredictor. The predictor fP is composed of a 3-layer MLP with 10 hidden units in each layer.\nCluster Number K. The best number of K for each dataset is selected based on the optimal Laplace encoder and predictor structures selected above. We use the ground truth cluster number K = 3 for the synthetic dataset. For the two real-world datasets, the cluster number is searched among K \u2208 {2, 3, 4, 5} to maximize the composite clustering performance HPRC. The optimal cluster number selection result is given below.\n\u2022 Synthetic dataset: K = 3 (we directly use the ground truth).\n\u2022 ADNI dataset: K = 4.\n\u2022 ICU dataset: K = 3.\n7https://pyclustering.github.io/"
        },
        {
            "heading": "E.2 Hyperparameter Selection of Baselines",
            "text": "KM-E2P(y). The KM-E2P(y) model includes a 1-layer GRU network to extract temporal features from input timeseries. A 2-layer MLP is stacked on top of the GRU network to form an encoder. Given the encoder output, another 2-layer MLP is used to predict the categorical label y. All layers in the GRU and MLP share the same number h of hidden units. Hyperparameters of h \u2208 \u00d7{5, 10, 20} is searched in each dataset basedd on the corresponding K determined above. By maximizing the composite metric HPRC or purity score, the hyperparameter selection result is obtained as follows.\n\u2022 Synthetic dataset: h = 20.\n\u2022 ADNI dataset: h = 20.\n\u2022 ICU dataset: h = 20.\nKM-E2P(z). Similar to KM-E2P(y), the KM-E2P(z) model is composed of a encoder with 2-layer MLP on top of a 1-layer GRU network to extract temporal features from input time-series. The encoder outputs a r-dimension latent vector, which is then used by a 2-layer MLP-based predictor for label prediction. All layers in the GRU and MLP share the same number h of hidden units. Given the best cluster numbers of K found by T-Phenotype, on each dataset, the optimal combination of h and r are search in the space of (h, r) \u2208 {10, 20} \u00d7 {5, 10, 20} to maximize the composite metric HPRC or purity score when ground truth cluster label is available. The hyperparameter selection result is given as follows.\n\u2022 Synthetic dataset: h = 10, r = 10.\n\u2022 ADNI dataset: h = 10, r = 20.\n\u2022 ICU dataset: h = 20, r = 10.\nKM-L. The baseline KM-L simply shares hyperparameters with T-Phenotype for its Laplace encoders on each dataset."
        },
        {
            "heading": "F COMPLETE BENCHMARK RESULT",
            "text": "The complete benchmark result on synthetic dataset is shown in Table F.1. T-Phenotype has significantly better clustering performance (purity score, adjusted Rand index, normalized mutual information) over all baselines on the synthetic data. In the meantime, the advantage of T-Phenotype over other baselines (except for AC-TPC) is clearly demonstrated via the proposed phenotype discovery performance metrics of HROC and HPRC. An extra baseline of KM-Laplacian (K-means on graph Laplacian calculated via dynamic time warping) is included in Table F.1 for reference. We note that this method has two major drawbacks: 1) there is not a stable and consistent representation space for cluster assignment for new samples; and 2) the distance matrix computation complexity in dynamic time warping could be extremely high, which makes this baseline infeasible for the two real-world datasets.\nTable F.1: Complete Benchmark Result on the Synthetic Dataset.\nMETHOD AUROC AUPRC PURITY RAND NMI HROC HPRC\nKM-E2P(y) 0.973\u00b10.014 0.962\u00b10.019 0.663\u00b10.019 0.477\u00b10.033 0.569\u00b10.045 0.846\u00b10.012 0.842\u00b10.010 KM-E2P(z) 0.963\u00b10.012 0.948\u00b10.011 0.677\u00b10.029 0.418\u00b10.024 0.485\u00b10.047 0.879\u00b10.011 0.873\u00b10.009 KM-DTW 0.722\u00b10.033 0.649\u00b10.028 0.469\u00b10.017 0.068\u00b10.021 0.077\u00b10.022 0.787\u00b10.020 0.742\u00b10.019 KM-Laplacian 0.736\u00b10.024 0.663\u00b10.017 0.490\u00b10.021 0.086\u00b10.011 0.094\u00b10.010 0.797\u00b10.016 0.752\u00b10.013 KM-L 0.646\u00b10.030 0.593\u00b10.027 0.687\u00b10.033 0.395\u00b10.058 0.447\u00b10.059 0.735\u00b10.020 0.700\u00b10.017 SEQ2SEQ 0.507\u00b10.028 0.505\u00b10.014 0.378\u00b10.008 -0.003\u00b10.003 0.005\u00b10.003 0.630\u00b10.022 0.628\u00b10.011 AC-TPC 0.966\u00b10.012 0.952\u00b10.017 0.659\u00b10.020 0.487\u00b10.035 0.596\u00b10.043 0.931\u00b10.011 0.925\u00b10.014\nT-Phenotype (J) 0.967\u00b10.020 0.954\u00b10.025 0.655\u00b10.021 0.440\u00b10.051 0.543\u00b10.064 0.845\u00b10.064 0.840\u00b10.064 T-Phenotype 0.975\u00b10.013 0.960\u00b10.024 0.965\u00b10.018\u2021 0.902\u00b10.048 \u2021 0.875\u00b10.050\u2021 0.927\u00b10.010 0.920\u00b10.014\nBest performance is highlighted in bold. Symbol \u2021 indicates p-value < 0.01\nThe complete benchmark result on two real-world datasets is provided in Table F.2. T-Phenotype in general has the best (or second best) phenotype discovery performance (HROC and HPRC) while achieving high accuracy in outcome prediction (AUROC and AUPRC), which demonstrates the prognostic value of the phenotypes discovered by T-Phenotype.\nTable F.2: Complete Benchmark Result on Two Real-world Datasets.\nMETHOD AUROC AUPRC AUSIL HROC HPRC\nA D\nN I KM-E2P(y) 0.893\u00b10.005 0.728\u00b10.017 0.677\u00b10.019 0.770\u00b10.013 0.701\u00b10.012 KM-E2P(z) 0.884\u00b10.012 0.711\u00b10.020 0.672\u00b10.028 0.763\u00b10.018 0.690\u00b10.013 KM-DTW 0.743\u00b10.013 0.522\u00b10.020 0.762\u00b10.049 0.752\u00b10.027 0.618\u00b10.021 KM-L 0.697\u00b10.029 0.465\u00b10.019 0.820\u00b10.022\u2021 0.753\u00b10.019 0.593\u00b10.018 SEQ2SEQ 0.775\u00b10.023 0.550\u00b10.030 0.772\u00b10.014 0.773\u00b10.012 0.642\u00b10.022 AC-TPC 0.861\u00b10.012 0.665\u00b10.020 0.726\u00b10.020 0.788\u00b10.014 0.694\u00b10.013 T-Phenotype (J) 0.867\u00b10.020 0.679\u00b10.040 0.690\u00b10.007 0.768\u00b10.011 0.684\u00b10.021 T-Phenotype 0.891\u00b10.005 0.716\u00b10.015 0.711\u00b10.023 0.791\u00b10.013 0.713\u00b10.009\u2021\nIC U KM-E2P(y) 0.697\u00b10.014 0.593\u00b10.012 0.668\u00b10.046 0.682\u00b10.029 0.628\u00b10.025 KM-E2P(z) 0.677\u00b10.030 0.579\u00b10.018 0.698\u00b10.042 0.686\u00b10.031 0.633\u00b10.024 KM-DTW 0.539\u00b10.030 0.515\u00b10.011 0.786\u00b10.072 0.636\u00b10.023 0.621\u00b10.021 KM-L 0.577\u00b10.019 0.532\u00b10.009 0.834\u00b10.024 0.682\u00b10.009 0.649\u00b10.004 SEQ2SEQ 0.592\u00b10.024 0.539\u00b10.012 0.830\u00b10.016 0.690\u00b10.011 0.653\u00b10.004 AC-TPC 0.660\u00b10.008 0.573\u00b10.003 0.735\u00b10.024 0.695\u00b10.014 0.644\u00b10.011 T-Phenotype (J) 0.697\u00b10.025 0.595\u00b10.017 0.691\u00b10.091 0.691\u00b10.056 0.636\u00b10.048 T-Phenotype 0.681\u00b10.017 0.585\u00b10.015 0.726\u00b10.015 0.703\u00b10.007 0.648\u00b10.008\nBest performance is highlighted in bold. Symbol \u2021 indicates p-value < 0.01"
        },
        {
            "heading": "G FURTHER ANALYSIS ON PHENOTYPE DISCOVERY",
            "text": "Comparison of Cluster Assignments on ADNI Dataset. On the ADNI dataset, typical phenotypes from KME2P(y), SEQ2SEQ, AC-TPC and T-Phenotype are compared in Figure G.1. Due to the model design, KM-E2P(y) only focuses on the predicted outcome distribution when discovering phenotypes (as shown in Figure G.1a). Compared to T-Phenotype, KM-E2P(y) wrongly splits normal patients with the same temporal pattern (stable CDRSB trajectory) into two clusters under K = 4. Additionally, KM-E2P(y) fails to discover the two subtypes of patients with high-risk of MCI as illustrated in phenotype 2 and 3 in Figure G.1d. While the SEQ2SEQ method is able to capture temporal patterns exhibit in patient trajectories, it is incapable to properly associate these temporal patterns with patient outcomes. For instance, SEQ2SEQ wrongly splits high-risk patients with increasing CDRSB scores over time into two different subgroups with similar outcome distributions.\nAs discussed in the main manuscript, AC-TPC aims at discovering the minimum number of clusters that can sufficiently represent the outcome distribution. Thus, it only identifies three phenotypes under K = 4 and combines the two subtypes (Phenotype 2 and 3 in Figure G.1d) of MCI patients into the same cluster. In comparison, T-Phenotype discovers phenotypes based on both predicted outcome and the associated predictive temporal patterns. The two subgroups of patients with expected diagnosis of MCI are correctly identified by T-Phenotype, which demonstrates the prognostic value of our method over the considered baselines.\nPhenotypes on ICU Mortality. On the ICU dataset, T-Phenotype is applied to identify phenotypes based on the patient\u2019s age, gender, GCS score and the fraction of PaCO2. Three major phenotypes are discovered by T-Phenotype, and the GCS trajectories of test samples in each subgroup are illustrated in Figure G.2. Based on the stability of their GCS trajectory, patients in each phenotype are plotted separately in two subfigures. The GCS score is predictive of patient mortality after ICU discharge (Leitgeb et al., 2013) and shows good discrimination accuracy on high- and low-risk patients admitted to ICU (Bastos et al., 1993). The predicted mortality rates in phenotypes 1, 2 and 3 are 15.3%, 3.2% and 32.4%, respectively. The GCS levels of patients in the three subgroups manifest a clear association to their corresponding mortality risks. For instance, many patients of phenotype 3 had lower GCS score (below 10) than the two other subgroups. In contrast, while having higher GCS levels, many patients in Phenotype 1 and 2 had an increase pattern (as shown in Figure G.2b) in their recent GCS measurements, which potentially contributes to their decreased risks of death. In the meantime, age is reported to be another risk factor for ICU mortality (Blot et al., 2009; Haas et al., 2017). With the average patient age of 63.0 (IQR: 53.0 \u2013 76.0), 43.0 (IQR: 29.8 \u2013 55.3) and 70.6 (IQR: 62.0 \u2013 82.0) in the three identified subgroups, phenotype 1 and 2 are clearly separated.8\n8Interquartile range (IQR) is the range defined by 25% and 75% quantiles of a variable.\n(a) Four phenotypes from KM-E2P(y).\n(b) Four phenotypes from SEQ2SEQ.\n(c) Three phenotypes from AC-TPC.\n(d) Four phenotypes from T-Phenotype.\nFigure G.1: Comparison of Phenotypes Discovered on the ADNI Dataset.\n(a) Patients with stable (std < 1) GCS trajectories.\n(b) Patients with less stable (std \u2265 1) GCS trajectories.\nFigure G.2: Three Phenotypes Discovered by T-Phenotype on ICU Dataset. The GCS trajectory of patients with different phenotypes are illustrated in the considered time period. All trajectories start at t = 0 and are smoothed with a rolling window of size 5."
        }
    ],
    "title": "T-PHENOTYPE: DISCOVERING PHENOTYPES OF PREDICTIVE TEMPORAL PATTERNS IN DISEASE PROGRESSION",
    "year": 2023
}