{
    "abstractText": "Recently, vision Transformers (ViTs) have been actively applied to fine-grained visual recognition (FGVR). ViT can effectively model the interdependencies between patchdivided object regions through an inherent self-attention mechanism. In addition, patch selection is used with ViT to remove redundant patch information and highlight the most discriminative object patches. However, existing ViTbased FGVR models are limited to single-scale processing, and their fixed receptive fields hinder representational richness and exacerbate vulnerability to scale variability. Therefore, we propose multi-scale patch selection (MSPS) to improve the multi-scale capabilities of existing ViT-based models. Specifically, MSPS selects salient patches of different scales at different stages of a multi-scale vision Transformer (MS-ViT). In addition, we introduce class token transfer (CTT) and multi-scale cross-attention (MSCA) to model cross-scale interactions between selected multi-scale patches and fully reflect them in model decisions. Compared to previous single-scale patch selection (SSPS), our proposed MSPS encourages richer object representations based on feature hierarchy and consistently improves performance from small-sized to large-sized objects. As a result, we propose M2Former, which outperforms CNN-/ViTbased models on several widely used FGVR benchmarks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiyong Moon"
        },
        {
            "affiliations": [],
            "name": "Junseok Lee"
        },
        {
            "affiliations": [],
            "name": "Yunju Lee"
        },
        {
            "affiliations": [],
            "name": "Seongsik Park"
        }
    ],
    "id": "SP:ab6c7106baed6d6fad1af1a30a23c32dc8c30360",
    "references": [
        {
            "authors": [
                "Anelia Angelova",
                "Shenghuo Zhu",
                "Yuanqing Lin"
            ],
            "title": "Image segmentation for large-scale subcategory flower recognition",
            "venue": "IEEE Workshop on Applications of Computer Vision (WACV),",
            "year": 2013
        },
        {
            "authors": [
                "Ardhendu Behera",
                "Zachary Wharton",
                "Pradeep RPG Hewage",
                "Asish Bera"
            ],
            "title": "Context-aware attentional pooling (cap) for fine-grained visual classification",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Thomas Berg",
                "Peter N Belhumeur"
            ],
            "title": "Poof: Part-based onevs.-one features for fine-grained categorization, face verification, and attribute estimation",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2013
        },
        {
            "authors": [
                "Dongliang Chang",
                "Yifeng Ding",
                "Jiyang Xie",
                "Ayan Kumar Bhunia",
                "Xiaoxu Li",
                "Zhanyu Ma",
                "Ming Wu",
                "Jun Guo",
                "Yi-Zhe Song"
            ],
            "title": "The devil is in the channels: Mutual-channel loss for fine-grained image classification",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Dongliang Chang",
                "Kaiyue Pang",
                "Yixiao Zheng",
                "Zhanyu Ma",
                "Yi-Zhe Song",
                "Jun Guo"
            ],
            "title": "Your\u201d flamingo\u201d is my",
            "venue": "bird\u201d: fine-grained,",
            "year": 2021
        },
        {
            "authors": [
                "Liang-Chieh Chen",
                "George Papandreou",
                "Iasonas Kokkinos",
                "Kevin Murphy",
                "Alan L Yuille"
            ],
            "title": "Semantic image segmentation with deep convolutional nets and fully connected crfs",
            "venue": "arXiv preprint arXiv:1412.7062,",
            "year": 2014
        },
        {
            "authors": [
                "Yue Chen",
                "Yalong Bai",
                "Wei Zhang",
                "Tao Mei"
            ],
            "title": "Destruction and construction learning for fine-grained image recognition",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In 2009 IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Ruoyi Du",
                "Dongliang Chang",
                "Ayan Kumar Bhunia",
                "Jiyang Xie",
                "Zhanyu Ma",
                "Yi-Zhe Song",
                "Jun Guo"
            ],
            "title": "Fine-grained visual classification via progressive multi-granularity training of jigsaw patches",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Ruoyi Du",
                "Jiyang Xie",
                "Zhanyu Ma",
                "Dongliang Chang",
                "Yi- Zhe Song",
                "Jun Guo"
            ],
            "title": "Progressive learning of categoryconsistent multi-granularity features for fine-grained visual classification",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Abhimanyu Dubey",
                "Otkrist Gupta",
                "Pei Guo",
                "Ramesh Raskar",
                "Ryan Farrell",
                "Nikhil Naik"
            ],
            "title": "Pairwise confusion for finegrained visual classification",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Abhimanyu Dubey",
                "Otkrist Gupta",
                "Ramesh Raskar",
                "Nikhil Naik"
            ],
            "title": "Maximum-entropy fine grained classification",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Haoqi Fan",
                "Bo Xiong",
                "Karttikeya Mangalam",
                "Yanghao Li",
                "Zhicheng Yan",
                "Jitendra Malik",
                "Christoph Feichtenhofer"
            ],
            "title": "Multiscale vision transformers",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Jianlong Fu",
                "Heliang Zheng",
                "Tao Mei"
            ],
            "title": "Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Shang-Hua Gao",
                "Ming-Ming Cheng",
                "Kai Zhao",
                "Xin-Yu Zhang",
                "Ming-Hsuan Yang",
                "Philip Torr"
            ],
            "title": "Res2net: A new multi-scale backbone architecture",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Weifeng Ge",
                "Xiangru Lin",
                "Yizhou Yu"
            ],
            "title": "Weakly supervised complementary parts models for fine-grained image classification from the bottom up",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Pei Guo",
                "Ryan Farrell"
            ],
            "title": "Aligned to the object, not to the image: A unified pose-aligned representation for finegrained recognition",
            "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV),",
            "year": 2019
        },
        {
            "authors": [
                "Ali Hassani",
                "Steven Walton",
                "Jiachen Li",
                "Shen Li",
                "Humphrey Shi"
            ],
            "title": "Neighborhood attention transformer",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Ju He",
                "Jie-Neng Chen",
                "Shuai Liu",
                "Adam Kortylewski",
                "Cheng Yang",
                "Yutong Bai",
                "Changhu Wang"
            ],
            "title": "Transfg: A transformer architecture for fine-grained recognition",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Spatial pyramid pooling in deep convolutional networks for visual recognition",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 1904
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Jie Hu",
                "Li Shen",
                "Samuel Albanie",
                "Gang Sun",
                "Andrea Vedaldi"
            ],
            "title": "Gather-excite: Exploiting feature context in convolutional neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Jie Hu",
                "Li Shen",
                "Gang Sun"
            ],
            "title": "Squeeze-and-excitation networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Yunqing Hu",
                "Xuan Jin",
                "Yin Zhang",
                "Haiwen Hong",
                "Jingfeng Zhang",
                "Yuan He",
                "Hui Xue"
            ],
            "title": "Rams-trans: Recurrent attention multi-scale transformer for fine-grained image recognition",
            "venue": "In Proceedings of the 29th ACM International Conference on Multimedia,",
            "year": 2021
        },
        {
            "authors": [
                "Shaoli Huang",
                "Zhe Xu",
                "Dacheng Tao",
                "Ya Zhang"
            ],
            "title": "Partstacked cnn for fine-grained visual categorization",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Zixuan Huang",
                "Yin Li"
            ],
            "title": "Interpretable and accurate finegrained recognition via region grouping",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Ruyi Ji",
                "Longyin Wen",
                "Libo Zhang",
                "Dawei Du",
                "Yanjun Wu",
                "Chen Zhao",
                "Xianglong Liu",
                "Feiyue Huang"
            ],
            "title": "Attention convolutional binary neural tree for fine-grained visual categorization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Xiao Ke",
                "Yuhang Cai",
                "Baitao Chen",
                "Hao Liu",
                "Wenzhong Guo"
            ],
            "title": "Granularity-aware distillation and structure modeling region proposal network for fine-grained image classification",
            "venue": "Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Krause",
                "Michael Stark",
                "Jia Deng",
                "Li Fei-Fei"
            ],
            "title": "3d object representations for fine-grained categorization",
            "venue": "In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), Sydney, Australia,",
            "year": 2013
        },
        {
            "authors": [
                "Hanting Li",
                "Mingzhe Sui",
                "Feng Zhao",
                "Zhengjun Zha",
                "Feng Wu"
            ],
            "title": "Mvt: mask vision transformer for facial expression recognition in the wild",
            "venue": "arXiv preprint arXiv:2106.04520,",
            "year": 2021
        },
        {
            "authors": [
                "Yanghao Li",
                "Chao-Yuan Wu",
                "Haoqi Fan",
                "Karttikeya Mangalam",
                "Bo Xiong",
                "Jitendra Malik",
                "Christoph Feichtenhofer"
            ],
            "title": "Mvitv2: Improved multiscale vision transformers for classification and detection",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Piotr Doll\u00e1r",
                "Ross Girshick",
                "Kaiming He",
                "Bharath Hariharan",
                "Serge Belongie"
            ],
            "title": "Feature pyramid networks for object detection",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In Computer Vision\u2013ECCV 2014: 13th European Conference,",
            "year": 2014
        },
        {
            "authors": [
                "Chuanbin Liu",
                "Hongtao Xie",
                "Zheng-Jun Zha",
                "Lingfeng Ma",
                "Lingyun Yu",
                "Yongdong Zhang"
            ],
            "title": "Filtration and distillation: Enhancing region attention for fine-grained visual categorization",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Yun Liu",
                "Ming-Ming Cheng",
                "Xiaowei Hu",
                "Kai Wang",
                "Xiang Bai"
            ],
            "title": "Richer convolutional features for edge detection",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Ze Liu",
                "Han Hu",
                "Yutong Lin",
                "Zhuliang Yao",
                "Zhenda Xie",
                "Yixuan Wei",
                "Jia Ning",
                "Yue Cao",
                "Zheng Zhang",
                "Li Dong"
            ],
            "title": "Swin transformer v2: Scaling up capacity and resolution",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Long",
                "Evan Shelhamer",
                "Trevor Darrell"
            ],
            "title": "Fully convolutional networks for semantic segmentation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Sgdr: Stochastic gradient descent with warm restarts",
            "venue": "arXiv preprint arXiv:1608.03983,",
            "year": 2016
        },
        {
            "authors": [
                "Wei Luo",
                "Xitong Yang",
                "Xianjie Mo",
                "Yuheng Lu",
                "Larry S Davis",
                "Jun Li",
                "Jian Yang",
                "Ser-Nam Lim"
            ],
            "title": "Cross-x learning for fine-grained visual categorization",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Fuyan Ma",
                "Bin Sun",
                "Shutao Li"
            ],
            "title": "Facial expression recognition with visual transformers and attentional selective fusion",
            "venue": "IEEE Transactions on Affective Computing,",
            "year": 2021
        },
        {
            "authors": [
                "Subhransu Maji",
                "Esa Rahtu",
                "Juho Kannala",
                "Matthew Blaschko",
                "Andrea Vedaldi"
            ],
            "title": "Fine-grained visual classification of aircraft",
            "venue": "arXiv preprint arXiv:1306.5151,",
            "year": 2013
        },
        {
            "authors": [
                "Shunan Mao",
                "Yaowei Wang",
                "Xiaoyu Wang",
                "Shiliang Zhang"
            ],
            "title": "Multi-proxy feature learning for robust fine-grained visual recognition",
            "venue": "Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Rafael M\u00fcller",
                "Simon Kornblith",
                "Geoffrey E Hinton"
            ],
            "title": "When does label smoothing help",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Maria-Elena Nilsback",
                "Andrew Zisserman"
            ],
            "title": "Automated flower classification over a large number of classes",
            "venue": "Sixth Indian Conference on Computer Vision, Graphics & Image Processing,",
            "year": 2008
        },
        {
            "authors": [
                "Yuxin Peng",
                "Xiangteng He",
                "Junjie Zhao"
            ],
            "title": "Object-part attention model for fine-grained image classification",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2017
        },
        {
            "authors": [
                "Joseph Redmon",
                "Ali Farhadi"
            ],
            "title": "Yolov3: An incremental improvement",
            "venue": "arXiv preprint arXiv:1804.02767,",
            "year": 2018
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun"
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "Unet: Convolutional networks for biomedical image segmentation",
            "venue": "18th International Conference,",
            "year": 2015
        },
        {
            "authors": [
                "Sofia Serrano",
                "Noah A Smith"
            ],
            "title": "Is attention interpretable",
            "venue": "arXiv preprint arXiv:1906.03731,",
            "year": 1906
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "Guolei Sun",
                "Hisham Cholakkal",
                "Salman Khan",
                "Fahad Khan",
                "Ling Shao"
            ],
            "title": "Fine-grained recognition: Accounting for subtle differences between similar classes",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Hao Tang",
                "Chengcheng Yuan",
                "Zechao Li",
                "Jinhui Tang"
            ],
            "title": "Learning attention-guided pyramidal features for few-shot fine-grained recognition",
            "venue": "Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Andrea Vedaldi",
                "Matthijs Douze",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Fixing the train-test resolution discrepancy",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Zhengzhong Tu",
                "Hossein Talebi",
                "Han Zhang",
                "Feng Yang",
                "Peyman Milanfar",
                "Alan Bovik",
                "Yinxiao Li"
            ],
            "title": "Maxvit: Multi-axis vision transformer",
            "venue": "In European conference on computer vision,",
            "year": 2022
        },
        {
            "authors": [
                "Grant Van Horn",
                "Steve Branson",
                "Ryan Farrell",
                "Scott Haber",
                "Jessie Barry",
                "Panos Ipeirotis",
                "Pietro Perona",
                "Serge Belongie"
            ],
            "title": "Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Catherine Wah",
                "Steve Branson",
                "Peter Welinder",
                "Pietro Perona",
                "Serge Belongie"
            ],
            "title": "The caltech-ucsd birds-200-2011 dataset",
            "year": 2011
        },
        {
            "authors": [
                "Haonan Wang",
                "Peng Cao",
                "Jiaqi Wang",
                "Osmar R Zaiane"
            ],
            "title": "Uctransnet: rethinking the skip connections in u-net from a channel-wise perspective with transformer",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Jun Wang",
                "Xiaohan Yu",
                "Yongsheng Gao"
            ],
            "title": "Feature fusion vision transformer for fine-grained visual categorization",
            "venue": "arXiv preprint arXiv:2107.02341,",
            "year": 2021
        },
        {
            "authors": [
                "Qilong Wang",
                "Banggu Wu",
                "Pengfei Zhu",
                "Peihua Li",
                "Wangmeng Zuo",
                "Qinghua Hu"
            ],
            "title": "Eca-net: Efficient channel attention for deep convolutional neural networks",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Wenhai Wang",
                "Enze Xie",
                "Xiang Li",
                "Deng-Ping Fan",
                "Kaitao Song",
                "Ding Liang",
                "Tong Lu",
                "Ping Luo",
                "Ling Shao"
            ],
            "title": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Zhihui Wang",
                "Shijie Wang",
                "Shuhui Yang",
                "Haojie Li",
                "Jianjun Li",
                "Zezhou Li"
            ],
            "title": "Weakly supervised fine-grained image classification via guassian mixture model oriented discriminative learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Peter Welinder",
                "Steve Branson",
                "Takeshi Mita",
                "Catherine Wah",
                "Florian Schroff",
                "Serge Belongie",
                "Pietro Perona"
            ],
            "title": "Caltech-ucsd birds",
            "year": 2010
        },
        {
            "authors": [
                "Yandong Wen",
                "Kaipeng Zhang",
                "Zhifeng Li",
                "Yu Qiao"
            ],
            "title": "A discriminative feature learning approach for deep face recognition",
            "venue": "In Computer Vision\u2013ECCV 2016: 14th European Conference,",
            "year": 2016
        },
        {
            "authors": [
                "Sanghyun Woo",
                "Jongchan Park",
                "Joon-Young Lee",
                "In So Kweon"
            ],
            "title": "Cbam: Convolutional block attention module",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Haiping Wu",
                "Bin Xiao",
                "Noel Codella",
                "Mengchen Liu",
                "Xiyang Dai",
                "Lu Yuan",
                "Lei Zhang"
            ],
            "title": "Cvt: Introducing convolutions to vision transformers",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Tianjun Xiao",
                "Yichong Xu",
                "Kuiyuan Yang",
                "Jiaxing Zhang",
                "Yuxin Peng",
                "Zheng Zhang"
            ],
            "title": "The application of twolevel attention models in deep convolutional neural network for fine-grained image classification",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Lingxi Xie",
                "Qi Tian",
                "Richang Hong",
                "Shuicheng Yan",
                "Bo Zhang"
            ],
            "title": "Hierarchical part matching for fine-grained visual categorization",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2013
        },
        {
            "authors": [
                "Saining Xie",
                "Zhuowen Tu"
            ],
            "title": "Holistically-nested edge detection",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Fanglei Xue",
                "Qiangchang Wang",
                "Guodong Guo"
            ],
            "title": "Transfer: Learning relation-aware facial expression representations with transformers",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Fanglei Xue",
                "Qiangchang Wang",
                "Zichang Tan",
                "Zhongsong Ma",
                "Guodong Guo"
            ],
            "title": "Vision transformer with attentive pooling for robust facial expression recognition",
            "venue": "IEEE Transactions on Affective Computing,",
            "year": 2022
        },
        {
            "authors": [
                "Xuhui Yang",
                "Yaowei Wang",
                "Ke Chen",
                "Yong Xu",
                "Yonghong Tian"
            ],
            "title": "Fine-grained object classification via selfsupervised pose alignment",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Ze Yang",
                "Tiange Luo",
                "Dong Wang",
                "Zhiqiang Hu",
                "Jun Gao",
                "Liwei Wang"
            ],
            "title": "Learning to navigate for fine-grained classification",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Fisher Yu",
                "Vladlen Koltun"
            ],
            "title": "Multi-scale context aggregation by dilated convolutions",
            "venue": "arXiv preprint arXiv:1511.07122,",
            "year": 2015
        },
        {
            "authors": [
                "Fisher Yu",
                "Vladlen Koltun",
                "Thomas Funkhouser"
            ],
            "title": "Dilated residual networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Xiaohan Yu",
                "Jun Wang",
                "Yang Zhao",
                "Yongsheng Gao"
            ],
            "title": "Mix-vit: Mixing attentive vision transformer for ultrafine-grained visual categorization",
            "venue": "Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Hang Zhang",
                "Kristin Dana",
                "Jianping Shi",
                "Zhongyue Zhang",
                "Xiaogang Wang",
                "Ambrish Tyagi",
                "Amit Agrawal"
            ],
            "title": "Context encoding for semantic segmentation",
            "venue": "In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Lianbo Zhang",
                "Shaoli Huang",
                "Wei Liu"
            ],
            "title": "Intra-class part swapping for fine-grained image classification",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Lianbo Zhang",
                "Shaoli Huang",
                "Wei Liu",
                "Dacheng Tao"
            ],
            "title": "Learning a mixture of granularity-specific experts for finegrained categorization",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Ning Zhang",
                "Jeff Donahue",
                "Ross Girshick",
                "Trevor Darrell"
            ],
            "title": "Part-based r-cnns for fine-grained category detection",
            "venue": "In Computer Vision\u2013ECCV 2014: 13th European Conference,",
            "year": 2014
        },
        {
            "authors": [
                "Shifeng Zhang",
                "Longyin Wen",
                "Xiao Bian",
                "Zhen Lei",
                "Stan Z Li"
            ],
            "title": "Single-shot refinement neural network for object detection",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Qijie Zhao",
                "Tao Sheng",
                "Yongtao Wang",
                "Zhi Tang",
                "Ying Chen",
                "Ling Cai",
                "Haibin Ling"
            ],
            "title": "M2det: A single-shot object detector based on multi-level feature pyramid network",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Heliang Zheng",
                "Jianlong Fu",
                "Tao Mei",
                "Jiebo Luo"
            ],
            "title": "Learning multi-attention convolutional neural network for finegrained image recognition",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Heliang Zheng",
                "Jianlong Fu",
                "Zheng-Jun Zha",
                "Jiebo Luo"
            ],
            "title": "Learning deep bilinear transformation for fine-grained image representation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Heliang Zheng",
                "Jianlong Fu",
                "Zheng-Jun Zha",
                "Jiebo Luo"
            ],
            "title": "Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Haowei Zhu",
                "Wenjing Ke",
                "Dong Li",
                "Ji Liu",
                "Lu Tian",
                "Yi Shan"
            ],
            "title": "Dual cross-attention learning for fine-grained visual categorization and object re-identification",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Lei Zhu",
                "Xinjiang Wang",
                "Zhanghan Ke",
                "Wayne Zhang",
                "Rynson WH Lau"
            ],
            "title": "Biformer: Vision transformer with bi-level routing attention",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Peiqin Zhuang",
                "Yali Wang",
                "Yu Qiao"
            ],
            "title": "Learning attentive pairwise interaction for fine-grained classification",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Despite recent rapid advances, fine-grained visual recognition (FGVR) is still one of the non-trivial tasks in computer vision community. Unlike conventional recognition tasks, FGVR aims to predict subordinate categories of a given object, e.g., subcategories of birds [59, 65], flowers [1, 46] and cars [30, 43]. It is a highly challenging task due to inherently subtle inter-class differences caused by similar subordinate categories and large intra-class varia-\n*corresponding author\ntions caused by object pose, scale, or deformation.\nThe most common solution for FGVR is to decompose the target object into multiple local parts [3, 28, 70, 74, 82, 87]. Due to subtle differences between fine-grained categories mostly resides in the unique properties of object parts [26], decomposed local parts provide more discriminative clues of the target object. For example, a given bird object can be decomposed into its beak, wing, and head parts. At this time, Glaucous Winged Gull and California Gull can be distinguished by comparing their corresponding object parts. Early approaches of these part-based methods finds discriminative local parts using manual part annotations [3, 26, 70, 82]. However, curating manual annotations for all possible object parts is labor-intensive and carries the risk of human-error [10]. Therefore, research fo-\nar X\niv :2\n30 8.\n02 16\n1v 1\n[ cs\n.C V\n] 4\nA ug\n2 02\ncus has consequently shifted to a weakly-supervised manner [17, 28, 35, 74, 85, 87]. They use additional tricks such as attention mechanisms [28,54,85] or region proposal networks (RPN) [17, 29, 35] to estimate local parts with only category-level labels. However, the part proposal process greatly increases the overall computational cost. Additionally, they tend not to deeply consider the interactions between estimated local parts that are essential for accurate recognition [20].\nRecently, vision Transformers (ViTs) [9] are being actively applied to FGVR [20, 25, 31, 42, 61, 72, 73, 78, 88]. Relying exclusively on the Transformer [58] architecture, ViT has shown competitive image classification performance at large-scale. Similar to token sequences in NLP, ViT embeds the input images into fixed-size image patches, and the patches pass through multiple Transformer encoder blocks. ViT\u2019s patch-by-patch processing is highly suitable for FGVR because each image patch can be considered as a local part. This means that the cumbersome part proposal process is no longer necessary. Additionally, the selfattention mechanism [58] inherent in each encoder block facilitates modeling of global interactions between patchdivided local parts. ViT-based FGVR models use patch selection to further boost the performance [20, 25, 61, 88]. Because ViT deals with all patch-divided image regions equally, many irrelevant patches may lead to inaccurate recognition. Similar to part proposals, patch selection selects the most salient patches from a set of generated image patches based on the computed importance ranking, i.e., accumulated attention weights [25,51]. As a result, redundant patch information is filtered out and only selected salient patches are considered for final decision.\nHowever, the existing ViT-based FGVR methods suffer from their single-scale limitations. ViT uses fixed-size image patches throughout the entire network, which ensures that the receptive field remains the same across all layers, and it prevents the ViT from obtaining multi-scale feature representations [14, 38, 63]. On the other hand, convolutional neural networks (CNNs) are suitable for multiscale feature representations thanks to their staged architecture, where feature resolution decreases as layer depth increases [21, 33, 39, 49, 76]. In early stages, spatial details of an object are encoded on high-resolution feature maps, and as the stages deepens, the receptive field expands with decreasing feature resolution, and higher-order semantic patterns are encoded into low-resolution feature maps. Multi-scale features are important for most vision tasks, especially pixel-level dense predictions tasks, e.g., object detection [48, 83, 84], and segmentation [6, 50, 79]. In the same context, single-scale processing can cause two failure cases in FGVR, which lead to suboptimal recognition performance. (i) First, it is vulnerable to scale changes of finegrained objects [33, 38, 48]. Fixed patch size may be insuf-\nficient to capture very subtle features of small-scale objects due to too coarse patches, and conversely, discriminative features may be over-decomposed for large-scale objects due to too finely split patches. (ii) Second, single-scale processing limits representational richness for objects [14, 84]. Compared to CNN that explores rich feature hierarchies from multi-scale features, ViT considers only monotonic single-scale features due to its fixed receptive field.\nIn this paper, we improve existing ViT-based FGVR methods by enhancing multi-scale capabilities. One simple solution is to use the recent multi-scale vision Transformers (MS-ViT) [14, 19, 32, 37, 38, 56, 63, 68, 89]. In fact, we can achieve satisfactory results simply by using MS-ViT. However, we further boost the performance by adapting patch selection to MS-ViT. Specifically, we propose a multi-scale patch selection (MSPS) that extends the previous singlescale patch selection (SSPS) [20, 25, 61, 88] to multi-scale. MSPS select salient patches of different scales from different stages of the MS-ViT backbone. As shown in Fig. 1, multi-scale salient patches selected through MSPS include both large-scale patches that capture object semantics and small-scale patches that capture fine-grained details. Compared to single-scale patches in SSPS, feature hierarchies in multi-scale patches provide richer representations of objects, which leads to better recognition performance. In addition, the flexibility of multi-scale patches is useful for handling extremely large/small objects through multiple receptive fields.\nHowever, we argue that patch selection alone cannot fully explain the object, and consideration is required of how to model interactions between selected patches and effectively reflect them in the final decision. It is more complicated than the case considering only single-scale patches. Therefore, we introduce class token transfer (CTT) and multi-scale cross-attention (MSCA) to effectively deal with selected multi-scale patches. First, CTT aggregates the multi-scale patch information by transferring the global CLS token to each stage. Each stage-specific patch information is shared through transferred global CLS tokens, which generate richer network-level representations. In addition, we propose MSCA to model direct interactions between selected multi-scale patches. In the MSCA block, crossscale interactions in both spatial and channel dimensions are computed for selected patches of all stages. Finally, our multi-scale vision Transformer with multi-scale patch selection (M2Former) obtains improved FGVR performance over other ViT-based SSPS models, as well as CNN-based models.\nOur main contributions can be summarized as follows:\n\u2022 We propose MSPS that further boosts the multi-scale capabilities of MS-ViT. Compared to SSPS, MSPS generates richer representations of fine-grained objects with feature hierarchies, and obtains flexibility\nfor scale changes with multiple receptive fields.\n\u2022 We propose CTT that effectively shares the selected multi-scale patch information. Stage-specific patch information is shared through transferred global CLS tokens to generate enhanced network-level representations.\n\u2022 We design multi-scale cross-attention (MSCA) block to capture the direct interactions of selected multiscale patches. In the MSCA block, the spatial/channel-wise cross-scale interdependencies can be captured.\n\u2022 Extensive experimental results on widely used FGVR benchmarks show the superiority of our M2Former over conventional methods. In short, our M2Former achieves an accuracy of 92.4% on Caltech-UCSD Birds (CUB) [65], and 91.1% on NABirds [57]."
        },
        {
            "heading": "2. Related Work",
            "text": ""
        },
        {
            "heading": "2.1. Part-based FGVR",
            "text": "A number of methods have been proposed to classify subordinate object categories [3\u20135, 10, 15, 17, 26, 35, 44, 66, 70, 82, 85, 86, 90]. Among them, part-based methods decompose target objects into multiple local parts to capture more discriminative features [3, 15, 26, 35, 70, 82, 85]. Encoded local representations can be used either in conjunction with image-level representations or by themselves for recognition. This entails the use of a detection branch to generate discriminative part proposals from the input image either before or in parallel with the classification layer. Early works leverage manual part annotations for a fullysupervised train of detection branches [3, 26, 70, 82]. However, curating large-scale part annotations is labor-intensive and highly susceptible to human error. Thus, recent partbased methods localize informative regions in a weaklysupervised way using only category-level labels [2, 7, 15, 17,27\u201329,35,53,54,85]. TASN [87] introduces trilinear attention sampling to generate detail-preserved views based on feature channels. P2P-Net [74] and DF-GMM [64] use a Feature Pyramid Network (FPN) [33] to generate local part proposals from convolutional feature maps. RA-CNN [15] iteratively zoom in local discriminative regions and reinforce multi-scale feature learning with inter-scale ranking loss. InSP [80] extracts potential part regions from the attention maps of two different images and applies content swapping to learn fine-grained local structures. Those partbased methods are good at locating discriminative parts, but they are ambiguous at capturing interactions between estimated local parts. Additionally, explicitly generating region proposals incurs a large computational burden, and performance can degrade significantly if the generated proposals\nare inaccurate. Thus, we focus on ViT-based FGVR methods that can avoid cumbersome region proposals."
        },
        {
            "heading": "2.2. ViT-based FGVR",
            "text": "Transformer [58] is originally introduced in the natural language processing (NLP) community, and recently shown great potential in the field of computer vision. Specifically, vision Transformer (ViT) [9] has recorded remarkable performance in image classification on large-scale datasets relying entirely on a pure Transformer encoder architecture. ViT splits images into non-overlapping patches and then applies multiple Transformer encoder blocks, consisting of multi-head self-attention module (MSA) and feed-forward networks (FFN). Recently, ViTs are being actively applied to FGVR [20, 25, 31, 42, 61, 72, 73, 78, 88]. ViT is highly suitable for FGVR in that (i) patch-by-patch processing can effectively replace part proposal generation, and (ii) it is easy to model global interactions between patch-divided local parts through the inherent self-attention mechanism. In most cases, patch selection, which selects discriminative patches from a initial patch sequence, is used with ViTbased models. TransFG [20] selects the most discriminative patches based on the attention score at the last encoder block to consider only the important image regions. Similarly, DCAL [88] conducts patch selection based on attention scores to highlight the interaction of high-response image regions. FFVT [61] extends this selection process to all layers of ViT to utilize layer-wise patch information. RAMS-Trans [25] enhances local representations by recurrently zooming-in salient object regions obtained by patch selection. For the same purpose, we use patch selection to strengthen the decisiveness of the network. However, we extend the existing patch selection from single-scale to multi-scale to improve representational richness and obtain flexibility for scale changes."
        },
        {
            "heading": "2.3. Multi-Scale Processing",
            "text": "Multi-scale features are important for most vision tasks, e.g., object detection [33, 48, 49, 83, 84], semantic segmentation [6, 50, 79], edge detection [36, 71], and image classification [16, 21, 77], because visual patterns occur at multiscales in natural scenes. CNNs naturally learn coarse-tofine multi-scale features through a stack of convolutional operators [22, 52], so most research has been proposed to enhance the multi-scale capabilities of CNNs [21, 33, 39, 49, 76]. FPN [33] introduces a feature pyramid to extract features with different scales from a single image, and fuses them in a top-down way to generate a semantically strong multi-scale feature representation. SPPNet [21] proposes spatial pyramid pooling to improve backbone networks to model multi-scale features of arbitrary size. Faster R-CNN [48] proposes region proposal networks that generate object bounding boxes of different scales. FCN [39]\nproposes a fully convolutional architecture that generates dense prediction maps for semantic segmentation from hierarchical representations of CNNs.\nOn the other hand, ViT suffers from being unsuitable to handle multi-scale features due to fixed scale image patches. After the initial patch embedding layer, ViT maintains image patches of the same size, and these single-scale feature maps are not suitable for many vision tasks, especially those requiring pixel-level dense prediction. To alleviate this issue, model architectures that adapt multi-scale feature hierarchies to ViT have recently been proposed. CvT [68] adapts convolutional operations to patch embedding layers and attention projection layers to enable feature hierarchies with improved efficiency. MViT [14] introduces attention pooling that controls feature resolution by adjusting the pooling stride of queries to implement multi-scale feature hierarchies. PVT [63] proposes a progressive shrinking strategy that conducts patch embedding with different patch sizes at the beginning of each stage. SwinT [38] generates multi-resolution feature maps by merging adjacent local patches using a patch merging layer. We also focus on feature hierarchies of fine-grained objects using multi-scale vision Transformers. However, we propose additional methods to further boost the multi-scale capability, i.e., MSPS, CTT, and MSCA."
        },
        {
            "heading": "3. Our Method",
            "text": "The overall framework of our method is presented in Fig 2. First, we use multi-scale vision Transformers (MSViT) as our backbone network (Section 3.1). After that, multi-scale patch selection (MSPS) is equipped on different stages of MS-ViT to extract multi-scale salient patches (Section 3.2). Class token transfer (CTT) aggregates multiscale patch information by transferring the global CLS token to each stage. Multi-scale cross-attention (MSCA) blocks are used to model spatial-/channel-wise interactions of selected multi-scale patches (Section 3.4). Finally, we use additional training strategies for better optimization (Section 3.5). More details are described as follows."
        },
        {
            "heading": "3.1. Multi-Scale Vision Transformer",
            "text": "To enhance the multi-scale capability, we use MS-ViT as our backbone network, specifically the recent Multiscale Vision Transformer (MViT) [14,32]. MViT constructs fourstage pyramid structure for low-level to high-level visual modeling instead of single-scale processing. To produce a hierarchical representation, MViT introduces Pooling Attention (PA), which pools query tensors to control the downsampling factor. We refer the interested reader to the original work [14, 32] for details.\nLet X0 \u2208 Rh0\u00d7w0\u00d7c0 denote the input image where h0, w0, and c0 refer to the height, width, and the number of channels, respectively. X0 first goes through a\npatch embedding layer to produce initial feature maps with a patch size of 4 \u00d7 4. As the stage deepens, the resolution of the feature maps decreases and the channel dimension increases proportionally. As a result, at each stage i \u2208 {1, 2, 3, 4}, we can extract the feature maps Xi \u2208 {X1, X2, X3, X4} with resolutions hi \u00d7 wi \u2208 {h04 \u00d7 w0 4 , h0 8 \u00d7 w0 8 , h0 16 \u00d7 w0 16 , h0 32 \u00d7 w0 32 } and channel dimensions ci \u2208 {96, 192, 384, 768}. We can also flatten Xi into 1D patch sequence as Xi \u2208 Rli\u00d7ci , where li = hi \u00d7 wi. In fact, after patch embedding, we attach a trainable class token (CLS token) to patch sequence, and all patches X\u0303i \u2208 Rl\u0303i\u00d7ci are fed into consecutive encoder blocks, where l\u0303i = li + 1. After the last block, the CLS token is detached from the patch sequence and used for class prediction through a linear classifier."
        },
        {
            "heading": "3.2. Multi-Scale Patch Selection",
            "text": "Single-scale patch selection (SSPS) has limited representations due to its fixed receptive field. Therefore, We propose multi-scale patch selection (MSPS) that extends SSPS to multi-scale. With multiple receptive fields, our proposed MSPS encourages rich representations of objects from deep semantic information to fine-grained details when compared to SSPS. We design MSPS based on the MViT backbone. Specifically, we select salient patches from the intermediate feature maps produced at each stage of MViT.\nGiven patch sequence X\u0303i \u2208 Rl\u0303i\u00d7ci , we start by detaching the CLS token and reshaping it into 2D feature maps to Xi \u2208 Rhi\u00d7wi\u00d7ci . And then, we group r \u00d7 r neighboring patches, reshaping Xi into X\u0302i \u2208 Rr 2\u00d7hiwi r2\n\u00d7ci . This means hiwi r2 neighboring patch groups are generated. Afterwards, we apply a per-group average to merge patch groups, producing X\u0302i \u2208 Rl\u0302i\u00d7ci , where l\u0302i = hiwir2 . We set r = 2 to merge patches within a 2 \u00d7 2 local region. This merging process removes the redundancies of neighboring patches, which forces MSPS to search for salient patches in wider areas of the image.\nNow, we produce a score map Si \u2208 Rl\u0302i using a predefined scoring function S (\u00b7). Then, patches with top-k scores are selected from X\u0302i,\nPi = MSPS ( X\u0302i;Si, ki ) , (1)\nwhere Pi \u2208 Rki\u00d7ci . We set k differently for each stage to consider hierarchical representations. Since the highresolution feature maps of the lower stage capture the detailed shape of the object with a small patch size, we set k to be large so that enough patches are selected to sufficiently represent the details of the object. On the other hand, low-resolution feature maps of the higher stage capture the semantic information of objects with a large patch size, so small k is sufficient to represent the overall semantics.\nFor patch selection, we have to decide how to define the scoring function S. Attention roll-out [51] has been mainly used as a scoring function for SSPS [20, 88]. Attention roll-out aggregates the attention weights of the Transformer blocks through successive matrix multiplications, and the patch selection module selects the most salient patches based on the aggregated attention weights. However, since we use MS-ViT as the backbone, we cannot use attention roll-out because the size of attention weights is different for each stage, even each block. Instead, we propose a simple scoring function based on mean activation, where the score for the j-th patch of X\u0302i is calculated by:\nSji = S(X\u0302i) = 1\nci ci\u2211 c=1 X\u0302ji (c) , (2)\nwhere c is the channel index c \u2208 {1, 2, . . . , ci}. Mean activation measures how strongly the channels in each patch are activated on average. After computing the score map, our MSPS conducts patch selection based on it. This is implemented through top-k and gather operations. We extract ki patch indices with the highest scores from the Si through the top-k operation, and patches corresponding to the patch indices Ii are selected from X\u0302i,\nIi = topkIndex(Si; ki), Pi = gather(X\u0302i; Ii), (3)\nwhere Ii \u2208 Nki , and Pi \u2208 Rki\u00d7ci ."
        },
        {
            "heading": "3.3. Class Token Transfer",
            "text": "Through MSPS, we can extract salient patches from each stage, P = {P1, P2, P3, P4}. In Section 3.2, the CLS token is detached from the patch sequence before MSPS at each stage. The simplest way to reflect the selected multiscale patches in the model decisions is to concatenate the detached CLS token CLSi with the Pi again and feed it into a few additional ViT blocks, consisting of multi-head selfattention (MSA) and feed-forward networks (FFN):\nP\u0303i = concat(Pi,CLSi), O\u0303i = FFN(MSA(P\u0303i)), (4)\nwhere P\u0303i, O\u0303i \u2208 Rk\u0303i\u00d7ci , and k\u0303i = ki + 1. Finally, predictions for each stage are computed by extracting the CLSi from O\u0303i and connecting the linear classifier. It should be noted that the CLSi is shared by all stages: the set of CLSi is derived from the global CLS token and it is detached with different dimensions at each stage. This means that the stage-specific multi-scale information is shared to some extent through CLSi. However, the current sharing method may cause inconsistency between stage features because the detached CLSi does not equally utilize the representational\n(a) CCA\npower of the network. For example, CLS1 is detached right after stage-1 and it will always lag behind CLS4, which shares the same root as CLS1 but utilizes representations of all stages.\nTo this end, we introduce a class token transfer (CTT) strategy that aggregates multi-scale information more effectively. The core idea is to use the CLS token transferred from the global CLS token CLSg rather than using the detached CLSi at each stage. It should be noted that CLSg is equal to CLS4, so CLSg \u2208 Rc4 . We transfer the CLSg according to the dimension of each stage through a projection layer consisting of two linear layers along with Batch Normalization (BN) and ReLU activation:\nCLSi = W 1 i\n( ReLU ( BN ( W 0i CLSg ))) , (5)\nwhere W 0i \u2208 R2ci\u00d7c4 , W 1i \u2208 Rci\u00d72ci are the weight matrices, and CLSi is the transferred CLSg in stage i \u2208 {1, 2, 3}. Now (4) is reformulated as:\nP\u0303i = concat(Pi,CLSi), O\u0303i = FFN(MSA(P\u0303i)). (6)\nCompared to conventional approaches, CTT guarantees consistency between stage features as it uses CLS tokens utilized with the same representational power. Each stage encodes stage-specific patch information into a globally updated CLS token. CTT is similar to the top-down pathway [33, 84]: it combines high-level representations of objects with multi-scale representations of lower layers to generate richer network-level representations."
        },
        {
            "heading": "3.4. Multi-Scale Cross-Attention",
            "text": "Although CTT can aggregate multi-scale patch information from all stages, it cannot model direct interactions between multi-scale patches, which indicates how interrelated they are. Therefore, we propose multi-scale crossattention (MSCA) to model the interactions between multiscale patches.\nMSCA takes P\u0303 = {P\u03031, P\u03032, P\u03033, P\u03034} as input and models the interactions between selected multi-scale salient patches. Specifically, MSCA consists of channel crossattention (CCA) and spatial cross-attention (SCA), so (6) is reformulated as:\nO\u0303 = MSCA(P\u0303) = SCA(CCA(P\u0303)), (7)\nwhere O\u0303 = {O\u03031, O\u03032, O\u03033, O\u03034}."
        },
        {
            "heading": "3.4.1 Channel Cross-Attention",
            "text": "Exploring feature channels has been very important in many vision tasks because feature channels encode visual patterns that are strongly related to foreground objects [4, 7, 47, 69, 87]. Many studies have been proposed to enhance the representational power of a network by explicitly modeling the interdependencies between the feature channels [23, 24, 60, 62, 67]. In the same vein, we propose CCA to further enhance the representational richness of multi-scale patches by explicitly modeling their crossscale channel interactions.\nWe illustrate CCA in Fig. 3 (a). First, we apply global average pooling (GAP) to P\u0303i to obtain a global channel descriptor Di \u2208 Rci for each stage. The c-th element of Di is calculated by:\nDci = 1\nk\u0303i k\u0303i\u2211 j=1 P\u0303 ci (j), (8)\nwhere j is the patch index j \u2208 {1, 2, . . . , k\u0303i}. From the stage-specific channel descriptors, we compute the channel attention score as follows:\nD = concat (D1, D2, D3, D4) \u2208 Rc, C = sigmoid ( W c,1ReLU ( BN ( W c,0D ))) \u2208 Rc, (9)\nwhere c = \u2211 ci, W c,0 \u2208 R c 2\u00d7c, and W c,1 \u2208 Rc\u00d7 c2 . We then split C back into Ci \u2208 Rci and recalibrate the channels of P\u0303i as follows:\nY\u0303i = P\u0303i \u2297 Ci + P\u0303i, (10)\nwhere \u2297 indicates element-wise multiplication. In (9), we compute the channel attention score by aggregating the channel descriptors of all multi-scale patches. It captures channel-dependencies in a cross-scale way and reflects them back to each stage-specific channel information."
        },
        {
            "heading": "3.4.2 Spatial Cross-Attention",
            "text": "In addition to channel-wise interactions, we can compute the spatial-wise interdependencies of selected multi-scale patches. To this end, we propose SCA, which is a multiscale extension of MSA [9, 58].\nWe illustrate SCA in Fig. 3 (b). First, We compute query, key, value tensors Qi, Ki, Vi for every Y\u0303i,\nQi = Y\u0303iW Q i , Ki = Y\u0303iW K i , Vi = Y\u0303iW V i , (11)\nwhere WQi , W K i , W V i \u2208 Rci\u00d7d, and Qi, Ki, Vi \u2208 Rk\u0303i\u00d7d. After that, we concatenate the Ki and Vi of all stages to generate global key and value tensors K, V,\nK = concat (K1,K2,K3,K4) \u2208 Rk\u0303\u00d7d, V = concat (V1, V2, V3, V4) \u2208 Rk\u0303\u00d7d, (12)\nwhere k\u0303 = \u2211\nk\u0303i. Now, we can compute self-attention for Qi, K, V, and single linear layer is used to restore the dimension,\nA\u0303i = Softmax ( QiK T / \u221a d ) V,\nO\u0303i = A\u0303iW s i ,\n(13)\nwhere A\u0303i \u2208 Rk\u0303i\u00d7d, W si \u2208 Rd\u00d7ci , and O\u0303i \u2208 Rk\u0303i\u00d7ci . SCA is also implemented in a multi-head manner [58]. For global key and value, SCA captures how strongly multiscale patches interact spatially with each other. Specifically, SCA models how large-scale semantic patches decompose into more fine-grained views, and conversely, how smallscale fine-grained patches can be identified in more global views."
        },
        {
            "heading": "3.5. Training",
            "text": "After the MSCA block, we can extract CLSi from O\u0303i and compute the class prediction yi using a linear classifier. In addition, we can compute ycon by concatenating all CLSi tokens. For model training, we compare every y = {y1, y2, y3, y4, ycon} for the ground-truth label y\u0302,\nL = \u2211 y\u2208y n\u2211 t=1 \u2212y\u0302t log yt, (14)\nwhere n is the total number of classes, and t denotes the element index of the label. To improve model generalization and encourage diversity of representations from specific stages, we employ soft supervision using label smoothing [45, 74]. We modify the one-hot vector y\u0302 as follows:\ny\u0302\u03b1[t] = { \u03b1 t = t\u0302 1\u2212\u03b1 n t \u0338= t\u0302 , (15)\nwhere t\u0302 denotes index of the ground-truth class, and \u03b1 denotes a smoothing factor \u03b1 \u2208 [0, 1]. \u03b1 controls the magnitude of the ground-truth class. As a result, the different predictions are supervised with different labels during training. We set \u03b1 to increase in equal intervals by 0.1 from 0.6 to 1, so y1 has the smallest \u03b1 = 0.6.\nFor inference, we conduct a final prediction considering all of y, yall = \u2211 y\u2208y y, (16)\nwhere the maximum entry in the yall corresponds to the class prediction."
        },
        {
            "heading": "4. Experiments",
            "text": "In this section, we evaluate our proposed M2Former on widely used fine-grained benchmarks. In addition, we conduct ablation studies and further analyses to validate the effectiveness of our method. More details are described as follows."
        },
        {
            "heading": "4.1. Datasets",
            "text": "We use two FGVR datasets to evaluate the proposed method: Caltech-UCSD Birds (CUB) [65], and NABirds [57]. The CUB dataset consists of 11,788 images and 200 bird species. All images are split into 5,994 for training and 5,794 for testing. The NABirds is a larger dataset, consisting of 48,562 images and 555 classes. All images are split into 23,929 for training and 24,633 for testing. Most of our experiments are conducted on CUB."
        },
        {
            "heading": "4.2. Implementation Details",
            "text": "We use MViTv2-B [32] pre-trained on ImageNet21K [8] as our backbone network. We add MSPS to every stage of MViTv2-B. After patch selection, selected patches pass through one MSCA block. We empirically set k = {k1, k2, k3, k4}, the number of patches selected for each stage, to {6, 18, 54, 162}. Our training recipe follows the same as recent work [20]. The model is trained for a total of 10,000 iterations, and an SGD optimizer with momentum of 0.9 and weight decay of 0 is used. The batch size is set to 16, and the initial learning rate is set to 0.03. The learning rate has a cosine decay schedule [40]. For augmentations, raw images are first resized into 600\u00d7 600 followed by cropping into 448 \u00d7 448. We use random cropping for training, and center cropping for testing. Random horizontal flipping is adapted to training images. We implement the whole model with the PyTorch framework on three NVIDIA A5000 GPUs."
        },
        {
            "heading": "4.3. Main Results",
            "text": "We compare our M2Former with state-of-the-art FGVR methods including ViT-based and CNN-based models on each dataset."
        },
        {
            "heading": "4.3.1 Results on CUB",
            "text": "First, the evaluation results on CUB are presented in Table 1. As shown in Table 1, our M2Former obtained a top-1 accuracy of 92.4%, which significantly outperforms CNN-based methods. Especially, M2Former improves recent P2P-Net [74] by 2.2% higher. In addition, M2Former achieved higher accuracy compared to ViTbased models using SSPS. Specifically, M2Former outperforms TransFG [20], RAMS-Trans [25], FFVT [61], and DCAL [88] by 0.7%, 1.1%, 0.8%, and 0.4% higher respectively. These results indicate that our proposed MSPS encourages enhanced representations compared to SSPS."
        },
        {
            "heading": "4.3.2 Results on NABirds",
            "text": "The evaluation results on NABirds are presented in Table 2. Our proposed M2Former obtains the top-1 accuracy of 91.1% on NABirds. Compared to CNN-based models,\nour M2Former shows significantly improved performance. For example, M2Former improves PMGv2 [11] by 2.7% higher. In addition, our method outperforms other ViTbased models. For example, M2Former improves ViT [9] by 1.2% higher and TransFG [20] by 0.3% higher."
        },
        {
            "heading": "4.4. Ablation Study",
            "text": "We analyzed each component of the proposed M2Former through the ablation study. All experiments are conducted on CUB dataset."
        },
        {
            "heading": "4.4.1 Ablation on Proposed Modules",
            "text": "We investigate the influence of the proposed modules (i.e., MSPS, CTT, CCA, SCA) included in the M2Former architecture. The results are presented in Table 3. The pure MViTv2-B baseline obtained top-1 accuracy of 91.6% on CUB (Table 3 (a)). When MSPS was added, we did not find any noticeable improvement (Table 3 (b)). This indicates\nthat effectively incorporating selected patches into model decisions is more important than patch selection itself. For this purpose, CTT allows multi-scale patch information to be shared across the entire network through transferred global CLS tokens. Indeed, with CTT, we improve the baseline by 0.5% higher (Table 3 (c)). In addition, CCA and SCA capture more direct interactions between multi-scale patches, but using CCA or SCA alone was not effective for improvement (Table 3 (d) and (e)). On the other hand, when using both CCA and SCA (full MSCA block) we obtain the highest top-1 accuracy of 92.4%, which improves baseline by 0.8% higher (Table 3 (f)). This means that spatial-/channel-level interactions of multi-scale patches are strongly correlated and need to be considered simultaneously."
        },
        {
            "heading": "4.4.2 Number of MSCA Blocks",
            "text": "Table 4 shows the results of the ablation experiment on the number of our MSCA blocks. The results show that using a single MSCA block performs best and increasing the number of MSCA blocks no longer yields a meaningful improvement. This means that a single MSCA block is sufficient to model the interactions of multi-scale patches. Moreover, using one MSCA block is efficient as it only introduces small extra parameters."
        },
        {
            "heading": "4.4.3 Number of Selected Patches",
            "text": "Table 5 shows the influences from the number of selected patches. We define it as k = {k1, k2, k3, k4}, where ki is the number of patches selected through patch selection at stage i. ki is set differently for each stage. As shown in Table 5, the larger the overall number of selected patches, the better the performance in general. However, after it grows to some extent, increasing selected patches is insignificant and caused unnecessary computations. We empirically found the optimal k = {6, 8, 54, 162}."
        },
        {
            "heading": "4.4.4 Different Backbone Networks",
            "text": "In Table 6, we analyze whether our method provides consistent effects on different backbone networks. We use CvT21 [68] and SwinT-B [38] as backbone architectures and compare them to their baseline. Both are initialized with\npre-trained weights from ImageNet21K [8]. The proposed modules are added to all three stages for CvT-21 and all four stages for SwinT-B. We follow the training recipe from recent work [20]. It should be noted that CTT could not be applied to the SwinT-B backbone as it does not use a CLS token. In addition, SwinT-B uses an input resolution of 384 \u00d7 384: raw images are first resized into 510 \u00d7 510 followed by cropping into 384 \u00d7 384. As shown in Table 6, Cvt-21 and SwinT-B baselines obtained top-1 accuracies of 89.3% and 90.6%, respectively. When we added the proposed modules, we could obtain top-1 accuracies of 90.0% and 91.3%, which are both 0.7% higher than their pure counterparts. This suggests that selecting multi-scale salient patches and modeling their interactions is important for fine-grained recognition, and our method is beneficial for that purpose."
        },
        {
            "heading": "4.5. Further Analysis",
            "text": "In this section, we conduct additional experiments to further validate the effectiveness of our proposed methods. All experiments are conducted on CUB dataset."
        },
        {
            "heading": "4.5.1 Contributions from CTT",
            "text": "In Table 7, we examine the contributions from CTT. First, we start by not using the CLS token. In this setup, the patch sequence selected at each stage does not contain any CLS tokens. Instead, selected patches that pass through the MSCA block are projected as feature vectors with global average pooling (GAP), and the features are used for final prediction through a linear layer. This is noted as the \u2018global\npool\u2019 in Table 7. As a result, the global pool obtains a top-1 accuracy of 91.5%, which lags behind the MViTv2-B baseline (91.6% in Table 3). We conjecture that this low accuracy is due to the lack of a shared intermediary (i.e., CLS token) to aggregate the information of the selected multiscale patches, even when using the MSCA block.\nThen, we now initialize the CLS token of the backbone, and the patch sequence selected in each stage is concatenated with the CLS token of the same dimension that was detached before patch selection (as in Section 3.3 (4)). This is simply re-attaching the CLS token that was detached before MSPS, so it is noted \u2018simple attach\u2019 in Table 7. As a result, the simple attach obtains an accuracy of 91.9%, which improve the global pool by 0.4% higher. We argue that this improvement comes from that simple attach can share information between multi-scale patches through the CLS tokens. Since the global CLS token is detached with different channel dimensions at each stage, stage-specific patch information can be shared through the global CLS token to generate richer representations.\nAs discussed in Section 3.3, we propose CTT to enhance multi-scale feature sharing. In this setup, each stage uses a \u2019transferred\u2019 global CLS token rather than a simply \u2019detached\u2019 one. This can be implemented using a single projection layer to match its dimensions (noted \u2018CTT w/ 1- MLP\u2019 in Table 7). As a result, it obtains an accuracy of 92.1%, which improve the simple attach by 0.2% higher. Compared to using a detached CLS token at an intermediate stage, transferring the globally updated CLS token is more effective in that it can inject local information of each stage to the object\u2019s deep semantic information, utilizing the same representational power of the network. Finally, CTT performs best when using 2-layer MLP with non-linearity (noted \u2018CTT w/ 2-MLP\u2019 in Table 7)."
        },
        {
            "heading": "4.5.2 Contributions from MSPS",
            "text": "In the earlier section, we pointed out that SSPS is suboptimal because it is difficult to deal with scale changes. We now show that our proposed MSPS can consistently improve performance on different object scales.\nFirst, we need to classify a given set of objects accord-\ning to their scale. Following COCO [34], we categorize all objects into large, medium, and small-sized objects according to their bounding box size. We compute the size of the bounding box using the given box coordinates and sort them in ascending order. Then, we compute the quartiles for sorted bounding box sizes. Finally, we classify an object as a small-sized (obs) object if its bounding box size is less than the first quartile, as a large-sized object (obl) if its bounding box size is greater than the third quartile, and as a medium-sized (obm) object otherwise. Specifically, 25% of objects are small, 50% are medium, and 25% are large. Example images for each category are shown in Fig. 4.\nWe train five model variants with different MSPS stages: M2Formernone, M2Former4, M2Former3,4, M2Former2,3,4, M2Formerfull. It should be noted that M2Formernone is exactly the same as the MViTv2-B baseline. For comparison, we trained TransFG [20], which conducts SSPS at the last encoder block.\nThe results are presented in Table 8. First, TransFG obtained a total top-1 accuracy of 91.3%, which obtained accuracies of 91.7% for obl, 91.7% for obm, and 90.1% for obs. The performance of obs seems to lag behind obl and obm. M2Formernone obtains a total top-1 accuracy of 91.6%, which outperforms TransFG by 0.3% higher. This means that we can achieve satisfactory results simply by using MS-ViT. Additionally, the improvement was prominent in obl and obs (0.6% and 0.4% higher, respectively), indicating that multi-scale features are important in mitigating scale variability.\nWhen we add MSPS to stage-4 (M2Former4), we obtain a top-1 accuracy of 91.5%, which is 0.2% higher than the baseline. M2Former4 is almost identical to SSPS as it selects only single-scale patches in a single stage. However, M2Former4 obtained a lower total top-1 accuracy than M2Formernone, and it generally results in lower accuracy at all scales. This is also in contrast to previous findings [20] where performance was improved when patch selection was conducted at the last block.\nHowever, when adding the MSPS in stage-3 (M2Former3,4), we obtained a top-1 accuracy of 91.9%, which outperforms the SSPS baseline by 0.6% higher.\nThe improvements were found at all object scales, but were noticeable at obl (1.2% higher compared to the SSPS baseline). This means that salient patches from stage-3\n(along with selected patches from stage-4) provide the definitive cue for large objects.\nWe can see this improvement even when adding MSPS in stage-2 (M2Former2,3,4). Especially, adding MSPS in stage-2 leads to improvements for obm and obs. Compared to the SSPS baseline, it is 0.7% higher for obm, 1.3% higher for obs, and 1.0% higher for total accuracy. This indicates that the finer-grained object features extracted at stage-2 enhance representations for small/medium-sized objects.\nFinally, when adding MSPS to stage-1 (M2Formerfull), we obtained the highest total top-1 accuracy of 92.4%, with a slight further improvement in obm. In summary, MSPS from high to low stages models a feature hierarchy from deep semantic features to subtle fine-grained features, which consistently improves recognition accuracy for large to small-sized objects. As a result, MSPS encourages networks to generate richer representations of fine-grained objects and to be more flexible to scale changes."
        },
        {
            "heading": "4.6. Visualization",
            "text": "To further investigate the proposed method, We present the visualization results in Fig. 5 and Fig. 6.\nFig. 5 shows the selected patches at each stage by conducting MSPS for several images sampled from CUB. In each subfigure, the first column is the original image, and selected patches from stage-4 to stage-1 are marked with red rectangles. At higher stages, large-sized patches that capture several parts of the object are selected. Sufficiently large patches are appropriate for modeling the intra-image structure and overall semantics of a given object. On the\nother hand, at the lower stage, smaller patches are chosen to model subtle details. Especially, the smallest size patches selected in stage-1 capture the coarsest features such as object edges. As a result, MSPS enhances object representations at different levels for each stage.\nFig. 6 shows the cross-attention maps of MSCA for selected patches. For visualization, we sample some patches selected from stage-4. And then, we extract cross-attention maps between sampled patches and patches selected at different stages from SCA. In Fig. 6, the first column shows the sampled stage-4 patch as a red rectangle. The second to fourth columns show the attention maps between sampled patches and selected patches from stage-3 to stage-1 as orange rectangles. The brightness of a color indicates the strength of attention: brighter means stronger interactions. In addition to modeling channel interactions in CCA, SCA models spatial interactions between selected multiscale patches. As a result, each selected patch can be correlated with other patches that exist in different locations at different scales."
        },
        {
            "heading": "5. Conclusion",
            "text": "In this paper, we propose a multi-scale patch selection (MSPS) for fine-grained visual recognition (FGVR). MSPS selects salient patches in multi-scale at each stage of a multi-scale vision Transformer (MS-ViT) based on mean activation. In addition, we introduce class token transfer (CTT) and multi-scale cross-attention (MSCA) to effectively deal with selected multi-scale patch information. CTT transfers the globally updated CLS token to each stage so that stage-specific patch information can be shared throughout the entire network. MSCA directly models the spatial-/channel-wise correlation between selected multi-scale patches. Compared to single-scale patch selection (SSPS), MSPS provides richer representations for fine-grained objects and flexibility for scale changes. As a result, our proposed M2Former obtains accuracies of 92.4%, and 91.1% on CUB and NABirds, respectively, which outperform CNN-based models and ViT-based SSPS models. Our ablation experiments and further analyses validate the effectiveness of our proposed methods."
        }
    ],
    "title": "M2Former: Multi-Scale Patch Selection for Fine-Grained Visual Recognition",
    "year": 2023
}