{
    "abstractText": "Psychomotor retardation in depression has been associated with speech timing changes from dyadic clinical interviews. In this work, we investigate speech timing features from freeliving dyadic interactions. Apart from the possibility of continuous monitoring to complement clinical visits, a study in free-living conditions would also allow inferring sociability features such as dyadic interaction frequency implicated in depression. We adapted a speaker count estimator as a dyadic interaction detector with a specificity of 89.5% and a sensitivity of 86.1% in the DIHARD dataset. Using the detector, we obtained speech timing features from the detected dyadic interactions in multi-day audio recordings of 32 participants comprised of 13 healthy individuals, 11 individuals with depression, and 8 individuals with psychotic disorders. The dyadic interaction frequency increased with depression severity in participants with no or mild depression, indicating a potential diagnostic marker of depression onset. However, the dyadic interaction frequency decreased with increasing depression severity for participants with moderate or severe depression. In terms of speech timing features, the response time had a significant positive correlation with depression severity. Our work shows the potential of dyadic interaction analysis from audio recordings of free-living to obtain markers of depression severity.",
    "authors": [
        {
            "affiliations": [],
            "name": "Bishal Lamichhane"
        },
        {
            "affiliations": [],
            "name": "Nidal"
        },
        {
            "affiliations": [],
            "name": "Ankit B. Patel"
        },
        {
            "affiliations": [],
            "name": "Ashutosh Sabharwal"
        }
    ],
    "id": "SP:b7e5343705f1fbc95b9e183a9581f0eae2c38df8",
    "references": [
        {
            "authors": [
                "M. Basta",
                "K. Micheli",
                "P. Simos",
                "I. Zaganas",
                "S. Panagiotakis",
                "K. Koutra",
                "C. Krasanaki",
                "C. Lionis",
                "A. Vgontzas"
            ],
            "title": "Frequency and risk factors associated with depression in elderly visiting primary health care (PHC) settings: Findings from the Cretan aging cohort",
            "venue": "Journal of Affective Disorders Reports, vol. 4, p. 100109, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R.L. Spitzer",
                "K. Kroenke",
                "J.B. Williams"
            ],
            "title": "Validation and utility of a self-report version of PRIME-MD: The PHQ primary care study.",
            "venue": "JAMA: Journal of the American Medical Association,",
            "year": 1999
        },
        {
            "authors": [
                "L.S. Radloff"
            ],
            "title": "The CES-D scale: A self-report depression scale for research in the general population",
            "venue": "Applied psychological measurement, vol. 1, no. 3, pp. 385\u2013401, 1977.",
            "year": 1977
        },
        {
            "authors": [
                "S. Saeb",
                "M. Zhang",
                "C.J. Karr",
                "S.M. Schueller",
                "M.E. Corden",
                "K.P. Kording",
                "D.C. Mohr"
            ],
            "title": "Mobile phone sensor correlates of depressive symptom severity in daily-life behavior: an exploratory study",
            "venue": "Journal of medical Internet research, vol. 17, no. 7, p. e175, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "J. Cao",
                "A.L. Truong",
                "S. Banu",
                "A.A. Shah",
                "A. Sabharwal",
                "N. Moukaddam"
            ],
            "title": "Tracking and predicting depressive symptoms of adolescents using smartphone-based self-reports, parental evaluations, and passive phone sensor data: development and usability study",
            "venue": "JMIR mental health, vol. 7, no. 1, p. e14045, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "K. Yoshida",
                "Y. Shimizu",
                "J. Yoshimoto",
                "M. Takamura",
                "G. Okada",
                "Y. Okamoto",
                "S. Yamawaki",
                "K. Doya"
            ],
            "title": "Prediction of clinical depression scores and detection of changes in whole-brain using resting-state functional MRI data with partial least squares regression",
            "venue": "PloS one, vol. 12, no. 7, p. e0179638, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "M. De Choudhury",
                "M. Gamon",
                "S. Counts",
                "E. Horvitz"
            ],
            "title": "Predicting depression via social media",
            "venue": "Seventh international AAAI conference on weblogs and social media, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "N. Cummins",
                "S. Scherer",
                "J. Krajewski",
                "S. Schnieder",
                "J. Epps",
                "T.F. Quatieri"
            ],
            "title": "A review of depression and suicide risk assessment using speech analysis",
            "venue": "Speech communication, vol. 71, pp. 10\u2013 49, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "A. Afshan",
                "J. Guo",
                "S.J. Park",
                "V. Ravi",
                "J. Flint",
                "A. Alwan"
            ],
            "title": "Effectiveness of voice quality features in detecting depression",
            "venue": "Interspeech 2018, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M. Yamamoto",
                "A. Takamiya",
                "K. Sawada",
                "M. Yoshimura",
                "M. Kitazawa",
                "K.-c. Liang",
                "T. Fujita",
                "M. Mimura",
                "T. Kishimoto"
            ],
            "title": "Using speech recognition technology to investigate the association between timing-related speech features and depression severity",
            "venue": "PloS one, vol. 15, no. 9, p. e0238726, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Cannizzaro",
                "B. Harel",
                "N. Reilly",
                "P. Chappell",
                "P.J. Snyder"
            ],
            "title": "Voice acoustical measurement of the severity of major depression",
            "venue": "Brain and cognition, vol. 56, no. 1, pp. 30\u201335, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "C. Xu",
                "S. Li",
                "G. Liu",
                "Y. Zhang",
                "E. Miluzzo",
                "Y.-F. Chen",
                "J. Li",
                "B. Firner"
            ],
            "title": "Crowd++ unsupervised speaker count with smartphones",
            "venue": "Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing, 2013, pp. 43\u201352.",
            "year": 2013
        },
        {
            "authors": [
                "W. Wang",
                "F. Seraj",
                "N. Meratnia",
                "P.J. Havinga"
            ],
            "title": "Speaker counting model based on transfer learning from SincNet bottleneck layer",
            "venue": "2020 IEEE International Conference on Pervasive Computing and Communications (PerCom). IEEE, 2020, pp. 1\u20138.",
            "year": 2020
        },
        {
            "authors": [
                "B. Lamichhane",
                "N. Moukaddam",
                "A.B. Patel",
                "A. Sabharwal"
            ],
            "title": "Econet: Estimating everyday conversational network from freeliving audio for mental health applications",
            "venue": "IEEE Pervasive Computing, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "T. Elmer",
                "C. Stadtfeld"
            ],
            "title": "Depressive symptoms are associated with social isolation in face-to-face interaction networks",
            "venue": "Scientific reports, vol. 10, no. 1, p. 1444, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H. Bredin",
                "R. Yin",
                "J.M. Coria",
                "G. Gelly",
                "P. Korshunov",
                "M. Lavechin",
                "D. Fustes",
                "H. Titeux",
                "W. Bouaziz",
                "M.-P. Gill"
            ],
            "title": "Pyannote. audio: neural building blocks for speaker diarization",
            "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7124\u20137128.",
            "year": 2020
        },
        {
            "authors": [
                "D. Snyder",
                "D. Garcia-Romero",
                "G. Sell",
                "D. Povey",
                "S. Khudanpur"
            ],
            "title": "X-Vectors: Robust DNN embeddings for speaker recognition",
            "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 5329\u20135333.",
            "year": 2018
        },
        {
            "authors": [
                "N. Ryant",
                "P. Singh",
                "V. Krishnamohan",
                "R. Varma",
                "K. Church",
                "C. Cieri",
                "J. Du",
                "S. Ganapathy",
                "M. Liberman"
            ],
            "title": "The third dihard diarization challenge",
            "venue": "arXiv preprint arXiv:2012.01477, 2020.",
            "year": 2012
        },
        {
            "authors": [
                "A. Nilsonne"
            ],
            "title": "Speech characteristics as indicators of depressive illness",
            "venue": "Acta Psychiatrica Scandinavica, vol. 77, no. 3, pp. 253\u2013 263, 1988.",
            "year": 1988
        },
        {
            "authors": [
                "M. Alpert",
                "E.R. Pouget",
                "R.R. Silva"
            ],
            "title": "Reflections of depression in acoustic measures of the patient\u2019s speech",
            "venue": "Journal of affective disorders, vol. 66, no. 1, pp. 59\u201369, 2001.",
            "year": 2001
        },
        {
            "authors": [
                "Y. Yang",
                "C. Fairbairn",
                "J.F. Cohn"
            ],
            "title": "Detecting depression severity from vocal prosody",
            "venue": "IEEE transactions on affective computing, vol. 4, no. 2, pp. 142\u2013150, 2012.",
            "year": 2012
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Major depressive disorder (MDD), or depression, is one of the most common mental health disorders [1] and remains underdiagnosed [2, 3]. Currently, questionnaires such as PHQ [4] and CES-D [5] are used as a diagnostic tool for depression. These questionnaires rely on subjective symptom reporting from the individuals and could lead to biased or incorrect assessments. Several recent works have thus investigated objective markers of depression based on mobility [6], communication logs [7], brain imaging [8], social media posts [9], etc.\nSpeech is one of the commonly investigated modalities to identify objective markers of depression. Psychomotor retardation in depression reportedly alters acoustic and timing features (turn-taking behaviors) in one\u2019s speech which has been explored as potential markers of depression severity [10]. For example, the authors in [11] found voice quality features to improve depression severity prediction. Similarly, the authors in [12] analyzed dyadic clinical interviews and found the pause time (time between utterances of the interviewee) and the response time (time between interviewer and interviewee\u2019s utterance) was longer for the depressed participants compared to healthy participants.\nCurrently, speech timing features as depression severity markers are obtained from clinical interviews [12, 13]. If such features could be extracted from free-living conversations, frequent monitoring would be possible to complement the assessments from the clinical visits. As the speech timing-based depression severity markers have been obtained from dyadic interactions (interviews) in existing studies, we aim to identify dyadic interactions in the free-living audio and study speech timing features in such interactions.\nIdentifying dyadic interactions in free-living audio could be challenging due to background noise, intra-speaker speech variabilities, the spontaneity of conversations, etc. No existing studies have investigated the dyadic interaction detection task. But several studies have addressed audio-based speaker counting [14, 15] which is closely related. A dyadic interaction detection, after all, is a binarized version of speaker counting (if the number of speakers is two or not). Speaker counting pipelines\u2019 ability to detect dyadic interactions, however, is also yet to be quantified. Similarly, though the findings of altered speech iteration and varying prosody are already documented from controlled studies, these have not been studied well in free-living with participants representing a wide depression severity range. Additionally, though isolation as a symptom of worsening depression is understood, it is not known whether it affects all sorts of interactions such as dyadic.\nOur work is the first demonstration of the feasibility of dyadic interaction analysis from multi-day free-living audio for mental health applications; we make the following two main contributions. First, we develop a dyadic interaction detector based on a speaker counting pipeline and demonstrate the feasibility of dyadic interaction detection in diverse audio recording settings. We adapted ECoNet [16], a speech processing pipeline to assess everyday conversational networks from free-living audio, for dyadic interaction detection. ECoNet deals with the challenges encountered in free-living audio by, for example, using a robust voice activity detection model (VAD) and identifying spuriously detected speakers due to background noise and intra-speaker speech variations. ECoNet-based dyadic detector outperformed a baseline model of dyadic interaction detection using VAD outputs, indicating the significance of speaker inference for the detection task.\nOur second contribution is dyadic interaction analysis in multi-day audio recordings of free-living from a diverse participant group, including a clinical population. We used a wristworn wearable to obtain continuous audio recordings, for up to a week, from 32 participants which included healthy, depressed, and individuals with psychotic disorders. From the dataset obtained, we analyzed the relation of the dyadic interaction frequency and speech timing features with depression severity. Our results are in line with the dyadic interac-\nar X\niv :2\n20 9.\n03 90\n1v 1\n[ cs\n.S D\n] 8\nS ep\n2 02\n2\ntion hypothesis reported in previous work [17] for a comparable participant group; the frequency of dyadic interactions increased with increasing depression severity. Thus, we provide an audio-based sensing alternative to RFID-based sensing proposed in [17] which required all individuals in a dyadic pair to have worn an RFID tag. The audio-based sensing only requires the target individual to wear the sensor. We additionally found that the association between dyadic interaction frequency and depression severity is reversed for the moderate and severely depressed participant groups which were not represented in the earlier study [17]. In terms of speech timing features, we found that response time has a significant positive correlation with depression severity, similar to the observations from controlled studies."
        },
        {
            "heading": "2. Dyadic Interaction Detection",
            "text": ""
        },
        {
            "heading": "2.1. ECoNet-based Detector",
            "text": "We developed a dyadic interaction detector based on the everyday conversational network estimator (ECoNet) proposed in [16]. ECoNet was adapted for the dyadic interaction detection by binarizing the inferred conversational network size as 2 (dyadic) or not 2 (non-dyadic). The architecture of ECoNet is given in Figure 1. It consists of a SincNet-LSTM-based VAD [18] to identify speech segments, xvectors [19] as speaker embeddings, clustering for unsupervised speaker identification/count, and a random forest-based machine learning model for spurious speaker detection. The ECoNet pipeline has the clustering threshold as a hyperparameter which was tuned for the dyadic interaction detection task in the DIHARD development set (Section 2.3)."
        },
        {
            "heading": "2.2. Baseline Model Using VAD Features",
            "text": "A dyadic interaction could also be possibly detected from the properties of the identified speech segments with VAD. An audio recording with a higher number of speakers could have higher variability in speech segment duration, characterizing the inter-speaker variability of typical utterance lengths. Similarly, there might be differences in inter-segment duration due to the different number of constituent speakers. Accordingly, we trained a Random Forest (RF)-based dyadic interaction detector using the following four features: mean and standard deviation of speech segment length; mean and standard deviation of intersegment duration. The number of estimators in RF was set to 51; a higher number of estimators did not result in improved cross-validation performance in the DIHARD development set (Section 2.3) in an increment of 10 estimators starting with 11 estimators."
        },
        {
            "heading": "2.3. Evaluation of Dyadic Interaction Detectors",
            "text": "We trained and evaluated the dyadic interaction detectors on the DIHARD development (DIHARD-DEV) and evaluation (DIHARD-EVAL) dataset [20] respectively. DIHARD dataset consists of audio sequences from diverse settings such as restaurant conversations, courtroom recordings, conversations in clinics, meetings, etc. Each audio sequence is nominally 10 minutes long, though there are some shorter sequences also, and has speaker diarization annotations, i.e., the speech segments have corresponding speaker labels.\nWe define a sequence in the DIHARD dataset as dyadic if most interactions (>90%) are attributed to exactly two speakers. This allows to include some sequences that have been in-\ncorrectly annotated with new speakers assigned to a few speech segments of existing speakers or have annotations with insignificant speakers (e.g., background, non-involved speakers) as dyadic. Further, we only include sequences longer than 5 minutes since we aim to extract speech timing features from longer conversations only, as employed in previous studies in controlled settings [12].\nWe trained ECoNet by identifying the clustering threshold resulting in the highest dyadic interaction detection accuracy in the DIHARD-DEV dataset. Then, the trained model is evaluated in the independent DIHARD-EVAL dataset. The results obtained is shown in Table 1. A high accuracy of 86.1% for dyadic interaction detection in the DIHARD-EVAL dataset is obtained. The spurious speaker detection module helped improve the detection accuracy. Naturally, the accuracy is higher in the DIHARD-DEV dataset where the clustering threshold was tuned. The baseline RF-based model was also trained on the DIHARD-DEV dataset and evaluated in the DIHARDEVAL dataset. The accuracy obtained with the baseline model was lower compared to the ECoNet-based detector, indicating the importance of speaker inference for dyadic interaction detection."
        },
        {
            "heading": "3. Free-living Audio Dataset Analysis",
            "text": "We used the ECoNet-based dyadic interaction detector to analyze the multi-day free-living audio from a diverse participant group. Particularly, we inferred where dyadic interaction happens and obtained speech timing features in the detected dyadic interactions using the speaker diarization output from ECoNet. Then, we assessed the relation of dyadic interaction frequency and speech timing features with depression severity."
        },
        {
            "heading": "3.1. Data Collection",
            "text": "We conducted a study with 32 participants comprising 13 healthy individuals, 11 individuals with major depressive disorders, and 8 individuals with psychotic disorders, as depicted in Figure 2. For each participant, we obtained continuous audio recordings during the day, for up to 7 days, using a wrist-worn audio recorder. The audio dataset thus obtained is referred to as the free-living audio dataset. We assessed the depression severity of the participant using the 9-item Patient Health Questionnaire (PHQ-9) [4]. The study was approved by the Institutional Review Board (IRB) at Rice University, Harris Health System, and Baylor College of Medicine."
        },
        {
            "heading": "3.2. Dyadic Ratio",
            "text": "We evaluated each 10-minute non-overlapping sliding window of the audio in the free-living audio dataset as being a dyadic\ninteraction or not. The number of dyadic interaction windows compared to the total number of windows for a participant is defined as the dyadic ratio, representing the dyadic interaction frequency. The relation of the dyadic ratio with the participants\u2019 PHQ-9 scores is shown in Figure 3. The dyadic ratio increased with increasing PHQ-9 scores (Pearson\u2019s correlation coefficient: 0.56, p-value: 0.046) but reduced as the PHQ-9 scores are higher (correlation coefficient: -0.29, p-value:0.31)."
        },
        {
            "heading": "3.3. Speech Timing Features",
            "text": "Among the dyadic interaction windows detected, some interactions are speech-rich (containing a high percentage of speech segments) while others could have less speech content overall, indicating less active interactions. To extract speech timing features, it would be desirable to consider speech-rich interactions which would match the scenarios in a clinical interview setting. Thus, we extracted speech timing features from the top-10 windows with the highest speech percentages from each participant\u2019s dyadic interaction windows, averaging the features from different windows. However, the target speaker, i.e., the participant, among the two speakers in each dyadic interaction, has to be identified first.\nFor all speakers in the dyadic interactions from a participant\u2019s audio recordings, we computed their average speaker embeddings from corresponding speech segments. Then, we\nobtained the cosine distances between the embeddings. Figure 4 shows an example of such distances from a participant\u2019s audio recordings. A checkerboard-like pattern is seen in the interspeaker distance map. This pattern would arise when a common speaker is present across interactions. The common speaker would have a small distance (high similarity) between their embeddings, appearing as a dark square in the example heatmap. We assume the common speaker to be the target speaker, i.e., the participant.\nFor the identified target speaker, we assessed the pause time, i.e., the average time between the speaker\u2019s utterances and response time, i.e., the average time between the other speaker\u2019s and the target speaker\u2019s utterances. The average speech timing features compared to the participant\u2019s PHQ-9 scores are shown in Figure 5. The response time correlated significantly with the PHQ-9 score (Pearson\u2019s correlation coefficient of 0.39). Of the 9 items in PHQ-9, the group difference of response time between participants with a score of 0 for a particular item and those with a score > 0 was highest for the 1st item Little interest or pleasure in doing things? (t-stats: 2.17, p-value: 0.039), followed by the 9th item Thoughts that you would be better off dead, or thoughts of hurting yourself in some way? (t-stats: 2.10 , p-value: 0.045), and the 2nd item Feeling down, depressed, or hopeless? (t-stats: 1.89 , p-value: 0.068). Higher response time could hence be driven by the general lack of interest in activities when in a depressed state. The group difference for the 8th item Moving or speaking so slowly that other people could have noticed? Or so fidgety or restless that you have been moving a\nlot more than usual?, chosen for analysis because the item directly implicates speech behaviors, was not significant (t-stats: 1.36, p-value: 0.184). The group with non-zero scores on the 8th item, though, had a higher average response time (1.28 seconds) than participants with a score of 0 (1.11 seconds).\nWe also analyzed the differences in PHQ-9 scores and average response time between the three groups represented in the free-living audio dataset. The depression and psychosis group had significantly higher PHQ-9 scores (18.1\u00b16.5 and 13.9\u00b19.9 respectively) than the healthy group (2.9\u00b12.6). The psychosis group had a higher response time (1.29\u00b10.23 seconds) than the depression group (1.21\u00b10.28) but the difference was not significant."
        },
        {
            "heading": "4. Discussion",
            "text": "Identification and characterization of dyadic interactions from free-living audio could provide depression severity markers. Dyadic interaction frequency represents a sociability behavior. Similarly, the speech timing features from dyadic interactions could model the psychomotor retardation associated with depression. We evaluated a dyadic interaction detector based on a speaker counting architecture in this work. The detector had a high accuracy in the DIHARD dataset and was used to iden-\ntify instances of dyadic interaction in a free-living audio dataset of multi-day audio recordings from 32 participants with diverse mental health conditions. We identified the target speaker in each dyadic interaction and obtained their speech timing features.\nWe obtained a positive association between dyadic ratio and depression severity in participants with no or mild depression, i.e., for participants with PHQ-9 scores < 10 (Figure 3). This result is in line with the dyadic interaction hypothesis proposed in [17]. Individuals with higher depression severity tend to have more dyadic interactions, e.g., to ruminate about their worries/concerns. The number of unique dyadic partners for a depressed individual could still be less, reflecting a lower total social network size, and needs to be investigated in future work. The study of [17] consisted of healthy participants with only\u224815-20% of them with some depression; participants with higher depression severity were not represented. We found that the association between the dyadic ratio and depression severity is rather negative for the moderately or severely depressed participants (PHQ-9 scores \u2265 10). This association could reflect the tendency of highly depressed individuals to avoid any form of interaction. Our results, though preliminary, indicate that the dyadic ratio could be useful only as a pre-diagnostic/monitoring feature for healthy individuals prone to depression. Further, non-linear functions would be required to model depression severity from features such as dyadic ratio.\nIn terms of the speech timing features, we found that the response time obtained from the dyadic interactions in freeliving is positively correlated with depression severity (Figure 5). This observation is in line with previous studies that have reported increasing response time with higher depression severity [12, 21, 22, 23]. Though pause time has also been reported to correlate positively with depression severity in earlier studies [12], we did not obtain similar observations in our analysis. The dyadic interactions of free-living could be eliciting different speech timings compared to clinical interviews, leading to this discrepancy, and need further investigation.\nAll three participant groups had good adherence to the study. The average per-participant audio data duration was 49.16, 53.27, and 53.75 hours in the healthy, depression, and psychosis groups, respectively. The psychosis group had a slightly lower average number of recording days (4.87 days) than the healthy and depression group (5.53 days, and 6.09 days respectively) but the difference was not significant.\nOur study shows how audio from free-living could be leveraged to obtain dyadic interaction-based markers of depression severity. These markers could complement inferences obtained from controlled conditions such as clinical interviews and allow more continuous monitoring. However, audio recording in freeliving also raises questions about wider acceptability due to privacy concerns. Our analysis does not parse any spoken content or attempt to identify speakers\u2019 true identities which could help retain some privacy. Future work should further investigate the trade-offs between privacy/acceptability and benefits in health monitoring inherent in speech technologies. Continuous audio monitoring-based health applications could gain wider acceptance by improving the technology to preserve user privacy by design and providing compelling (health) applications."
        },
        {
            "heading": "5. Acknowledgements",
            "text": "BL was partially supported by the Ken Kennedy Institute 2021/22 Energy HPC Conference Graduate Fellowship."
        },
        {
            "heading": "6. References",
            "text": "[1] \u201cDepression.\u201d [Online]. Available: https://www.who.int/\nnews-room/fact-sheets/detail/depression\n[2] K. Argyropoulos, C. Bartsokas, A. Argyropoulou, P. Gourzis, and E. Jelastopulu, \u201cDepressive symptoms in late life in urban and semi-urban areas of South-West Greece: An undetected disorder?\u201d Indian journal of psychiatry, vol. 57, no. 3, p. 295, 2015.\n[3] M. Basta, K. Micheli, P. Simos, I. Zaganas, S. Panagiotakis, K. Koutra, C. Krasanaki, C. Lionis, and A. Vgontzas, \u201cFrequency and risk factors associated with depression in elderly visiting primary health care (PHC) settings: Findings from the Cretan aging cohort,\u201d Journal of Affective Disorders Reports, vol. 4, p. 100109, 2021.\n[4] R. L. Spitzer, K. Kroenke, and J. B. Williams, \u201cValidation and utility of a self-report version of PRIME-MD: The PHQ primary care study.\u201d JAMA: Journal of the American Medical Association, 1999.\n[5] L. S. Radloff, \u201cThe CES-D scale: A self-report depression scale for research in the general population,\u201d Applied psychological measurement, vol. 1, no. 3, pp. 385\u2013401, 1977.\n[6] S. Saeb, M. Zhang, C. J. Karr, S. M. Schueller, M. E. Corden, K. P. Kording, and D. C. Mohr, \u201cMobile phone sensor correlates of depressive symptom severity in daily-life behavior: an exploratory study,\u201d Journal of medical Internet research, vol. 17, no. 7, p. e175, 2015.\n[7] J. Cao, A. L. Truong, S. Banu, A. A. Shah, A. Sabharwal, N. Moukaddam et al., \u201cTracking and predicting depressive symptoms of adolescents using smartphone-based self-reports, parental evaluations, and passive phone sensor data: development and usability study,\u201d JMIR mental health, vol. 7, no. 1, p. e14045, 2020.\n[8] K. Yoshida, Y. Shimizu, J. Yoshimoto, M. Takamura, G. Okada, Y. Okamoto, S. Yamawaki, and K. Doya, \u201cPrediction of clinical depression scores and detection of changes in whole-brain using resting-state functional MRI data with partial least squares regression,\u201d PloS one, vol. 12, no. 7, p. e0179638, 2017.\n[9] M. De Choudhury, M. Gamon, S. Counts, and E. Horvitz, \u201cPredicting depression via social media,\u201d in Seventh international AAAI conference on weblogs and social media, 2013.\n[10] N. Cummins, S. Scherer, J. Krajewski, S. Schnieder, J. Epps, and T. F. Quatieri, \u201cA review of depression and suicide risk assessment using speech analysis,\u201d Speech communication, vol. 71, pp. 10\u2013 49, 2015.\n[11] A. Afshan, J. Guo, S. J. Park, V. Ravi, J. Flint, and A. Alwan, \u201cEffectiveness of voice quality features in detecting depression,\u201d Interspeech 2018, 2018.\n[12] M. Yamamoto, A. Takamiya, K. Sawada, M. Yoshimura, M. Kitazawa, K.-c. Liang, T. Fujita, M. Mimura, and T. Kishimoto, \u201cUsing speech recognition technology to investigate the association between timing-related speech features and depression severity,\u201d PloS one, vol. 15, no. 9, p. e0238726, 2020.\n[13] M. Cannizzaro, B. Harel, N. Reilly, P. Chappell, and P. J. Snyder, \u201cVoice acoustical measurement of the severity of major depression,\u201d Brain and cognition, vol. 56, no. 1, pp. 30\u201335, 2004.\n[14] C. Xu, S. Li, G. Liu, Y. Zhang, E. Miluzzo, Y.-F. Chen, J. Li, and B. Firner, \u201cCrowd++ unsupervised speaker count with smartphones,\u201d in Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing, 2013, pp. 43\u201352.\n[15] W. Wang, F. Seraj, N. Meratnia, and P. J. Havinga, \u201cSpeaker counting model based on transfer learning from SincNet bottleneck layer,\u201d in 2020 IEEE International Conference on Pervasive Computing and Communications (PerCom). IEEE, 2020, pp. 1\u20138.\n[16] B. Lamichhane, N. Moukaddam, A. B. Patel, and A. Sabharwal, \u201cEconet: Estimating everyday conversational network from freeliving audio for mental health applications,\u201d IEEE Pervasive Computing, 2022.\n[17] T. Elmer and C. Stadtfeld, \u201cDepressive symptoms are associated with social isolation in face-to-face interaction networks,\u201d Scientific reports, vol. 10, no. 1, p. 1444, 2020.\n[18] H. Bredin, R. Yin, J. M. Coria, G. Gelly, P. Korshunov, M. Lavechin, D. Fustes, H. Titeux, W. Bouaziz, and M.-P. Gill, \u201cPyannote. audio: neural building blocks for speaker diarization,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7124\u20137128.\n[19] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur, \u201cX-Vectors: Robust DNN embeddings for speaker recognition,\u201d in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 5329\u20135333.\n[20] N. Ryant, P. Singh, V. Krishnamohan, R. Varma, K. Church, C. Cieri, J. Du, S. Ganapathy, and M. Liberman, \u201cThe third dihard diarization challenge,\u201d arXiv preprint arXiv:2012.01477, 2020.\n[21] A. Nilsonne, \u201cSpeech characteristics as indicators of depressive illness,\u201d Acta Psychiatrica Scandinavica, vol. 77, no. 3, pp. 253\u2013 263, 1988.\n[22] M. Alpert, E. R. Pouget, and R. R. Silva, \u201cReflections of depression in acoustic measures of the patient\u2019s speech,\u201d Journal of affective disorders, vol. 66, no. 1, pp. 59\u201369, 2001.\n[23] Y. Yang, C. Fairbairn, and J. F. Cohn, \u201cDetecting depression severity from vocal prosody,\u201d IEEE transactions on affective computing, vol. 4, no. 2, pp. 142\u2013150, 2012."
        }
    ],
    "title": "Dyadic Interaction Assessment from Free-living Audio for Depression Severity Assessment",
    "year": 2022
}