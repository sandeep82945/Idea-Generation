{
    "abstractText": "Tree ensembles are powerful models that are widely used. However, they are susceptible to adversarial examples, which are examples that purposely constructed to elicit a misprediction from the model. This can degrade performance and erode a user\u2019s trust in the model. Typically, approaches try to alleviate this problem by verifying how robust a learned ensemble is or robustifying the learning process. We take an alternative approach and attempt to detect adversarial examples in a post-deployment setting. We present a novel method for this task that works by analyzing an unseen example\u2019s output configuration, which is the set of predictions made by an ensemble\u2019s constituent trees. Our approach works with any additive tree ensemble and does not require training a separate model. We evaluate our approach on three different tree ensemble learners. We empirically show that our method is currently the best adversarial detection method for tree ensembles.",
    "authors": [
        {
            "affiliations": [],
            "name": "Laurens Devos"
        },
        {
            "affiliations": [],
            "name": "Wannes Meert"
        }
    ],
    "id": "SP:f60087c3b4946286093a95888c36b6b810df1a54",
    "references": [
        {
            "authors": [
                "Maksym Andriushchenko",
                "Matthias Hein"
            ],
            "title": "Provably robust boosted decision stumps and trees against adversarial attacks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Battista Biggio",
                "Igino Corona",
                "Davide Maiorca",
                "Blaine Nelson",
                "Nedim \u0160rndi\u0107",
                "Pavel Laskov",
                "Giorgio Giacinto",
                "Fabio Roli"
            ],
            "title": "Evasion attacks against machine learning at test time",
            "venue": "In Machine Learning and Knowledge Discovery in Databases,",
            "year": 2013
        },
        {
            "authors": [
                "Leo Breiman",
                "Adele Cutler"
            ],
            "title": "Random forests manual",
            "venue": "https://www.stat.berkeley.edu/ ~breiman/RandomForests,",
            "year": 2002
        },
        {
            "authors": [
                "Markus M Breunig",
                "Hans-Peter Kriegel",
                "Raymond T Ng",
                "J\u00f6rg Sander"
            ],
            "title": "Lof: identifying densitybased local outliers",
            "venue": "In Proceedings of the 2000 ACM SIGMOD international conference on Management of data,",
            "year": 2000
        },
        {
            "authors": [
                "Hongge Chen",
                "Huan Zhang",
                "Duane Boning",
                "Cho-Jui Hsieh"
            ],
            "title": "Robust decision trees against adversarial examples",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Hongge Chen",
                "Huan Zhang",
                "Si Si",
                "Yang Li",
                "Duane Boning",
                "Cho-Jui Hsieh"
            ],
            "title": "Robustness verification of tree-based models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Tianqi Chen",
                "Carlos Guestrin"
            ],
            "title": "Xgboost: A scalable tree boosting system",
            "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201916,",
            "year": 2016
        },
        {
            "authors": [
                "Corinna Cortes",
                "Giulia DeSalvo",
                "Mehryar Mohri"
            ],
            "title": "Learning with rejection",
            "venue": "In Proceedings of The 27th International Conference on Algorithmic Learning Theory (ALT",
            "year": 2016
        },
        {
            "authors": [
                "Laurens Devos",
                "Wannes Meert",
                "Jesse Davis"
            ],
            "title": "Versatile verification of tree ensembles",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Dimitrios Diochnos",
                "Saeed Mahloujifar",
                "Mohammad Mahmoody"
            ],
            "title": "Adversarial risk and robustness: General definitions and implications for the uniform distribution",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Gil Einziger",
                "Maayan Goldstein",
                "Yaniv Sa\u2019ar",
                "Itai Segall"
            ],
            "title": "Verifying robustness of gradient boosted models",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Alhussein Fawzi",
                "Hamza Fawzi",
                "Omar Fawzi"
            ],
            "title": "Adversarial vulnerability for any classifier",
            "venue": "In Proceedings of the 32nd International Conference on Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Reuben Feinman",
                "Ryan R Curtin",
                "Saurabh Shintre",
                "Andrew B Gardner"
            ],
            "title": "Detecting adversarial samples from artifacts",
            "venue": "arXiv preprint arXiv:1703.00410,",
            "year": 2017
        },
        {
            "authors": [
                "Zhitao Gong",
                "Wenlu Wang",
                "Wei-Shinn Ku"
            ],
            "title": "Adversarial and clean data are not twins",
            "venue": "arXiv preprint arXiv:1704.04960,",
            "year": 2017
        },
        {
            "authors": [
                "Ian J. Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "In International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "Kathrin Grosse",
                "Praveen Manoharan",
                "Nicolas Papernot",
                "Michael Backes",
                "Patrick McDaniel"
            ],
            "title": "On the (statistical) detection of adversarial examples",
            "venue": "arXiv preprint arXiv:1702.06280,",
            "year": 2017
        },
        {
            "authors": [
                "Kathrin Grosse",
                "David Pfaff",
                "Michael Thomas Smith",
                "Michael Backes"
            ],
            "title": "The limitations of model uncertainty in adversarial settings",
            "venue": "In 4th workshop on Bayesian Deep Learning (NeurIPS",
            "year": 2018
        },
        {
            "authors": [
                "Kilian Hendrickx",
                "Lorenzo Perini",
                "Dries Van der Plas",
                "Wannes Meert",
                "Jesse Davis"
            ],
            "title": "Machine learning with a reject option: A survey",
            "year": 2021
        },
        {
            "authors": [
                "Alex Kantchelian",
                "J Doug Tygar",
                "Anthony Joseph"
            ],
            "title": "Evasion and hardening of tree ensemble classifiers",
            "venue": "In International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Ziv Katzir",
                "Yuval Elovici"
            ],
            "title": "Detecting adversarial perturbations through spatial behavior in activation spaces",
            "venue": "In 2019 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2019
        },
        {
            "authors": [
                "Guolin Ke",
                "Qi Meng",
                "Thomas Finley",
                "Taifeng Wang",
                "Wei Chen",
                "Weidong Ma",
                "Qiwei Ye",
                "TieYan Liu"
            ],
            "title": "LightGBM: A highly efficient gradient boosting decision tree",
            "venue": "In Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Kimin Lee",
                "Kibok Lee",
                "Honglak Lee",
                "Jinwoo Shin"
            ],
            "title": "A simple unified framework for detecting out-of-distribution samples and adversarial attacks",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Fei Tony Liu",
                "Kai Ming Ting",
                "Zhi-Hua Zhou"
            ],
            "title": "Isolation forest",
            "venue": "In 2008 8th IEEE international conference on data mining,",
            "year": 2008
        },
        {
            "authors": [
                "Xuanqing Liu",
                "Yao Li",
                "Chongruo Wu",
                "Cho-Jui Hsieh"
            ],
            "title": "Adv-BNN: Improved adversarial defense through robust bayesian neural network",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Jan Hendrik Metzen",
                "Tim Genewein",
                "Volker Fischer",
                "Bastian Bischoff"
            ],
            "title": "On detecting adversarial perturbations",
            "venue": "In Proceedings of 5th International Conference on Learning Representations (ICLR),",
            "year": 2017
        },
        {
            "authors": [
                "R Kelley Pace",
                "Ronald Barry"
            ],
            "title": "Sparse spatial autoregressions",
            "venue": "Statistics & Probability Letters,",
            "year": 1997
        },
        {
            "authors": [
                "F. Pedregosa",
                "G. Varoquaux",
                "A. Gramfort",
                "V. Michel",
                "B. Thirion",
                "O. Grisel",
                "M. Blondel",
                "P. Prettenhofer",
                "R. Weiss",
                "V. Dubourg",
                "J. Vanderplas",
                "A. Passos",
                "D. Cournapeau",
                "M. Brucher",
                "M. Perrot",
                "E. Duchesnay"
            ],
            "title": "Scikit-learn: Machine learning in Python",
            "venue": "Journal of Machine Learning Research,",
            "year": 2011
        },
        {
            "authors": [
                "Konstantinos Pliakos",
                "Celine Vens"
            ],
            "title": "Feature induction and network mining with clustering tree ensembles",
            "venue": "In Proceedings of the International Workshop on New Frontiers in Mining Complex Patterns,",
            "year": 2016
        },
        {
            "authors": [
                "Francesco Ranzato",
                "Marco Zanella"
            ],
            "title": "Abstract interpretation of decision tree ensemble classifiers",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Francesco Ranzato",
                "Marco Zanella"
            ],
            "title": "Genetic adversarial training of decision trees",
            "venue": "In Proceedings of the Genetic and Evolutionary Computation Conference,",
            "year": 2021
        },
        {
            "authors": [
                "Kevin Roth",
                "Yannic Kilcher",
                "Thomas Hofmann"
            ],
            "title": "The odds are odd: A statistical test for detecting adversarial examples",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Jan-Philipp Schulze",
                "Philip Sperl",
                "Konstantin B\u00f6ttinger"
            ],
            "title": "DA3G: Detecting adversarial attacks by analysing gradients",
            "venue": "In European Symposium on Research in Computer Security,",
            "year": 2021
        },
        {
            "authors": [
                "Tao Shi",
                "Steve Horvath"
            ],
            "title": "Unsupervised learning with random forest predictors",
            "venue": "Journal of Computational and Graphical Statistics,",
            "year": 2006
        },
        {
            "authors": [
                "Philip Sperl",
                "Ching-Yu Kao",
                "Peng Chen",
                "Xiao Lei",
                "Konstantin B\u00f6ttinger"
            ],
            "title": "DLA: dense-layeranalysis for adversarial example detection",
            "venue": "IEEE European Symposium on Security and Privacy (EuroS&P),",
            "year": 2020
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wojciech Zaremba",
                "Ilya Sutskever",
                "Joan Bruna",
                "Dumitru Erhan",
                "Ian Goodfellow",
                "Rob Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2014
        },
        {
            "authors": [
                "Sarah Tan",
                "Matvey Soloviev",
                "Giles Hooker",
                "Martin T. Wells"
            ],
            "title": "Tree space prototypes: Another look at making tree ensembles interpretable",
            "venue": "In Proceedings of the 2020 ACM-IMS on Foundations of Data Science Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Jinyu Tian",
                "Jiantao Zhou",
                "Yuanman Li",
                "Jia Duan"
            ],
            "title": "Detecting adversarial examples from sensitivity inconsistency of spatial-transform domain",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "John T\u00f6rnblom",
                "Simin Nadjm-Tehrani"
            ],
            "title": "Formal verification of input-output mappings of tree ensembles",
            "venue": "Science of Computer Programming,",
            "year": 2020
        },
        {
            "authors": [
                "Celine Vens",
                "Fabrizio Costa"
            ],
            "title": "Random forest based feature induction",
            "venue": "In Proceedings of 11th IEEE International Conference on Data Mining,",
            "year": 2011
        },
        {
            "authors": [
                "Dani\u00ebl Vos",
                "Sicco Verwer"
            ],
            "title": "Efficient training of robust decision trees against adversarial examples",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Yihan Wang",
                "Huan Zhang",
                "Hongge Chen",
                "Duane Boning",
                "Cho-Jui Hsieh"
            ],
            "title": "On Lp-norm robustness of ensemble decision stumps and trees",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Geoffrey I Webb",
                "Kai Ming Ting"
            ],
            "title": "On the application of roc analysis to predict classification performance under varying class distributions",
            "venue": "Machine learning,",
            "year": 2005
        },
        {
            "authors": [
                "Puyudi Yang",
                "Jianbo Chen",
                "Cho-Jui Hsieh",
                "Jane-Ling Wang",
                "Michael Jordan"
            ],
            "title": "ML-LOO: Detecting adversarial examples with feature attribution",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Yao-Yuan Yang",
                "Cyrus Rashtchian",
                "Yizhen Wang",
                "Kamalika Chaudhuri"
            ],
            "title": "Robustness for nonparametric classification: A generic attack and defense",
            "venue": "In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Chong Zhang",
                "Huan Zhang",
                "Cho-Jui Hsieh"
            ],
            "title": "An efficient adversarial attack for tree ensembles",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Shigeng Zhang",
                "Shuxin Chen",
                "Xuan Liu",
                "Chengyao Hua",
                "Weiping Wang",
                "Kai Chen",
                "Jian Zhang",
                "Jianxin Wang"
            ],
            "title": "Detecting adversarial samples for deep learning models: A comparative study",
            "venue": "IEEE Transactions on Network Science and Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Chen",
                "Guestrin",
                "LightGBM Ke"
            ],
            "title": "2017], where the number of leaves are often limited to less than 256 (e.g. maximum depth 8). Hence it is possible to assign each leaf in a tree a short binary code word (e.g. an 8-bit unsigned integer). An output configuration is then an array of M short binary codes. This makes it possible to exploit instruction-level parallelism using SIMD to compute the Hamming distance",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Tree ensembles such as (gradient) boosted trees and random forests are a popular class of models. However, like many other model families, such as neural networks Szegedy et al. [2014], Goodfellow et al. [2015], Biggio et al. [2013], they are susceptible to adversarial attacks Kantchelian et al. [2016], Einziger et al. [2019], Chen et al. [2019b], Zhang et al. [2020], Wang et al. [2020], Devos et al. [2021]. While there are many types of attacks, this paper explores the setting where the model is deployed and operating in the wild, and exposed to adversarial examples. Intuitively in this context an adversarial example is one that is purposely constructed to elicit a misprediction from the model Diochnos et al. [2018]. Such examples are undesirable because they degrade a model\u2019s performance and erode a user\u2019s trust in the model. For tree ensembles, the literature attempts to deal with this in one of two ways. First, verification techniques attempt to ascertain how robust a learned ensemble is to adversarial examples Chen et al. [2019b], Ranzato and Zanella [2020], Devos et al. [2021]. This is often done by empirically determining how much an example would have to be perturbed (according to some norm) for its predicted label to change. Second, the problem can be addressed at training time by trying to learn a more robust model by adding adversarial examples to the training set Kantchelian et al. [2016], pruning the training data Yang et al. [2020b], changing aspects of the learner such as the splitting criteria Chen et al. [2019a], Andriushchenko and Hein [2019], Vos and Verwer [2021], or interleaving learning and verification Ranzato and Zanella [2021].\nThis paper explores an alternative approach to mitigating the effect of adversarial examples in a post deployment setting. Given a current example for which a prediction is required, we attempt to ascertain if this current example is adversarial or not. If the example is identified as being adversarial,\nPreprint. Under review.\nar X\niv :2\n20 6.\n13 08\n3v 1\n[ cs\n.L G\n] 2\n7 Ju\nthen the deployed model could refrain from making a prediction similar to a learning with rejection setting Cortes et al. [2016]. While this question has been extensively explored for neural networks, this is not the case for tree ensembles. Unfortunately, most existing methods for neural networks are not applicable to tree ensembles because they use properties unique to neural networks Zhang et al. [2022]. For example, some modify the model Grosse et al. [2017], Gong et al. [2017], Metzen et al. [2017], learn other models (e.g., nearest neighbors) on top of the network\u2019s intermediate representations Feinman et al. [2017], Lee et al. [2018], Katzir and Elovici [2019], Sperl et al. [2020], or learn other models on top of the gradients Schulze et al. [2021]. Moreover, nearly all methods focus on detecting adversarial examples only in the context of image classification.\nTree ensembles are powerful because they combine the predictions made by many trees. Hence, the prediction procedure involves sorting the given example to a leaf node in each tree. The ordered set of the reached leaf nodes is an output configuration of the ensemble and fully determines the ensemble\u2019s resulting prediction. However, there are many more possible output configurations than there are examples in the data used to train the model. For example, the California housing dataset Pace and Barry [1997] only has eight features, but training an XGBoost ensemble containing 6, 7, or 8 trees each of at most depth 5 yields 62 248, 173 826, and 385 214 output configurations respectively.1 These numbers (far) exceed the 20,600 examples in the dataset. The situation will be worse for the larger ensembles sizes that are used in practice. Our hypothesis is that\nadversarial examples exploit unusual output configurations, that is, ones that are very different to those observed in the data used to train the model.\nThat is, small, but carefully selected perturbations can yield an example that is quite similar to another example observed during training, but yields an output configuration that is far away from those covered by the data used to train the model.\nBased on this intuition, we present a novel method to detect adversarial examples based on assessing whether an example encountered post deployment has an unusual output configuration. When an example is encountered post deployment, our approach encodes it by its output configuration and then measures the distance between the encoded example and its nearest (encoded) neighbor in a reference set. If this distance is sufficiently high, the example is flagged as being an adversarial one and the model can abstain from making a prediction. Our approach has several benefits. First, it is general: it works with any additive tree ensemble. Second, it is integrated: it does not require training a separate model to identify adversarial examples, one simply has to set a threshold on the distance. Finally, it is surprisingly fast as the considered distance metric can be efficiently computed by exploiting instruction level parallelism (SIMD).\nEmpirically, we evaluate and compare our approach on three ensemble methods: gradient boosted trees (XGBoost Chen and Guestrin [2016]), random forests Breiman [2001], and GROOT Vos and Verwer [2021], which is a recent approach for training robust tree ensembles. We empirically show that our method outperforms multiple competing approaches for detecting adversarial examples post deployment for all three considered tree ensembles. Moreover, it can detect adversarial examples with a comparable computational effort."
        },
        {
            "heading": "2 Preliminaries",
            "text": "Given an input space X and an output space Y , we detect adversarial examples in deployed models learned from a dataset D \u2286 X \u00d7 Y mapping X to Y ."
        },
        {
            "heading": "2.1 Additive tree ensembles",
            "text": "This paper proposes a method that works with additive ensembles of decision trees (e.g., those available in XGBoost Chen and Guestrin [2016], LightGBM Ke et al. [2017] and Scikit-learn Pedregosa et al. [2011]). This encompasses both random forests and (gradient) boosted decision trees.\nA binary tree is a recursive data structure consisting of nodes. It has one root node, which is the only node that is not a descendent of any other node. Every node is either a leaf node containing an output value, or an internal node storing a test (e.g., is Xi less than 5?) that indicates whether to go left or\n1Computed using Veritas Devos et al. [2021].\nright, and references to left and right sub-trees. A decision tree T is evaluated on an example x \u2208 X by starting at the root and executing the tests until a leaf node is reached.\nAn additive ensemble of decision trees is a sum of trees. The prediction T (x) for an example x \u2208 X involves summing up the predicted leaf values of each decision tree in the ensemble: \u2211 m Tm(x), m = 1, . . . ,M , with M the number of trees in the ensemble."
        },
        {
            "heading": "2.2 Output configurations",
            "text": "The output configuration (OC) of an example x is the ordered set of leaf nodes (l1, . . . , lM ) that are reached when evaluating the trees in the ensemble. An output configuration corresponds to a feasible combination of root-to-leaf paths where there is one such path for each tree in the ensemble. We define a mapping OC : x 7\u2192 (l1, . . . , lM ) mapping an example x to its output configuration. We call the discrete space of all feasible output configurations O = {OC(x) | x \u2208 X} the OC-space of an ensemble. This OC-space O corresponds to equivalence classes in T\u00f6rnblom and Nadjm-Tehrani [2020]. Note thatO is not just the Cartesian product of all leaves. This is because some combinations of leaves are invalid, e.g., in Figure 1, (1, 2) and (1, 3) are invalid configurations because Xi cannot be both less than 3 and greater than 4 at the same time."
        },
        {
            "heading": "2.3 Adversarial examples",
            "text": "We use the same definition of adversarial examples as Kantchelian et al. [2016], Chen et al. [2019b], Devos et al. [2021] and others: x\u0303 is an adversarial example of x \u2208 X when three conditions hold: (1) \u2016x\u0303 \u2212 x\u2016 is small according to some norm, (2) T (x) returns the correct label for x, and (3) T (x\u0303) 6= T (x). This is similar to the prediction-change setting of Diochnos et al. [2018], which only requires conditions (1) and (3)."
        },
        {
            "heading": "3 Detecting adversarial examples",
            "text": "We assume a post-deployment setting where a tree ensemble T is operating in the wild. Our task can then be defined as follows:\nGiven: a deployed tree ensemble T and an example x \u2208 X for which a prediction is required Do: assign a score to x indicating whether x is an adversarial example\nOur algorithm is based on the fact that for sufficiently large models, the vast majority of the model\u2019s possible outputs configurations will not be observed in the data used to train the model. Given this insight, our hypotheses are that\nH1. Adversarial examples arise because certain minimal changes to the original example can produce large changes in the output configuration, causing the model\u2019s prediction to change.\nH2. Adversarial examples produce highly unusual output configurations, exploiting the fact that most such configurations were never observed in the data used to train the model.\nDecision tree learners employ heuristics to select a split criterion in each internal node that helps distinguish among the different classes. Consequently, most leaf nodes tend to be (strongly) predictive of one class. In an ensemble, correctly classified positive examples will tend to have output configurations consisting largely of leaves that predict the positive class whereas the converse is true for negative examples. Adversarial examples contain carefully selected perturbations to a small number\nof feature values that result in more leaves of the opposing class appearing in an output configuration, yielding an unusual output configuration with an incorrect output label. This suggests that measuring how similar a newly encountered example\u2019s output configuration is to those that appear in the training set will be an effective way to detect adversarial examples.\nFirst, we discuss how to measure the distance between output configurations. Second, we introduce our OC-score metric which computes the distance between a newly encountered example and the reference set. A higher score indicates that the example is more likely adversarial. Finally, we show how to efficiently compute the OC-score using instruction-level parallelism."
        },
        {
            "heading": "3.1 Distances between output configurations",
            "text": "Following the intuition that adversarial examples exploit unusual output configurations, our method measures the abnormality of an output configuration by comparing it to the typical output configurations traversed by examples in a reference set. Two output configurations O = {l1, . . . , lM} and O\u2032 = {l\u20321, . . . , l\u2032M} can be compared using the Hamming distance:\nham(O,O\u2032) = M\u2211 m=1 I(lm 6= l\u2032m). (1)\nThe Hamming distance counts the number of leaves that differ between the two output configurations. It measures the distance between two examples in OC-space rather than the input space X . This is isomorphic to the proximity metric in Random Forests Breiman and Cutler [2002].\n3.2 The OC-score metric: distance to the closest reference set example\nOur approach requires a learned ensemble T and a subset DR of the data D used to train the model. It constructs a reference set R = {OC(x) | x \u2208 DR} by encoding the examples in DR into the OC-space by finding each one\u2019s output configuration. In practice, R is a matrix of small integers (e.g. uint8) that identify the leaves (e.g. the black identifiers in Figure 1). The ith row in R contains the identifiers of the leaves in the output configuration of the ith example in DR In the experiments, we take R to be the output configurations of the correctly classified training examples.\nGiven a newly encountered example x \u2208 X , the ensemble is used to obtain its predicted label y\u0302 = T (x) and output configuration OC(x). Then it receives a OC-score by computing the OC-space distance to the closest example in a reference dataset R:\nOC-score(x) = min O\u2032\u2208R[y=y\u0302]\nham(OC(x), O\u2032), (2)\nwhere R[y = y\u0302] is the subset of examples in R with label y\u0302. Higher OC-scores correspond to a higher chance of being an adversarial example. To operationalize this, a threshold can be set on the OC-scores to flag potential adversarial examples: when the threshold is exceeded the model should abstain from making a prediction."
        },
        {
            "heading": "3.3 Fast distance computations via SIMD",
            "text": "Finding an example\u2019s nearest neighbors in the OC-space can be done very efficiently by exploiting the fact that ensembles tend to produce trees with a relatively small number of leaf nodes. Hence it is possible to assign each leaf in a tree a short binary code word that can be represented by a small integer and exploit instruction-level parallelism using SIMD to compute the Hamming distance.\nTo compute OC-score(x), we need to compute the Hamming distance to each example in the reference set, that is, we need to slide the vector OC(x) over the rows of R. The trees used in the experiments have no more than 256 leaves. Hence, when using the 256-bit AVX2 registers, we can compute the Hamming distance of 32 reference set examples in parallel. This massively speeds-up the computation even when R is large. The SIMD pseudo-code is given in the supplement."
        },
        {
            "heading": "4 Related Work",
            "text": "Beyond the approaches mentioned in the introduction for detecting adversarial examples in neural networks, there are methods that look at the behavior of the decision boundary in an example\u2019s\nneighborhood Fawzi et al. [2018], Roth et al. [2019], Tian et al. [2022]. Unfortunately, these methods do not work well with tree ensembles because the use of binary axis-parallel splits make them step functions, which makes it difficult to extract information from the neighborhood. Also, relevant to this paper is the work investigating the relation between model uncertainty and adversarial examples Liu et al. [2019], Grosse et al. [2018].\nThe random forest manual Breiman and Cutler [2002] discusses defining distances between training examples in an analogous manner to OC-score. Typically, (variations on) this distance has been used for tasks such as clustering Shi and Horvath [2006] or making tree ensembles more interpretable Tan et al. [2020]. To our knowledge, it has not been used for detecting adversarial examples. Alternatively, some works propose different ways to use the path an example follows in a tree ensemble to reencode them, typically with the idea of using this encoding as a new feature space for training a model Vens and Costa [2011], Pliakos and Vens [2016].\nEach example\u2019s OC-score can be viewed as a model\u2019s secondary output with the predicted class being its primary output. This fits into the larger task of machine learning with a reject option Cortes et al. [2016]. Rejection aims to identify test examples for which the model was not properly trained. For such examples, the model\u2019s predictions have an elevated risk of being incorrect, and hence may not be trustworthy. An example can be rejected due to ambiguity (i.e., how well the decision boundary is defined in a region) or novelty (i.e., how anomalous an example is wrt the observed training data) Hendrickx et al. [2021]. The OC-score metric goes beyond measuring ambiguity in an ensemble (i.e., the model\u2019s confidence in a prediction). Therefore, it can detect adversarial examples even if they fall in a region of the input space where the model\u2019s decision boundary appears to be well defined given the training data."
        },
        {
            "heading": "5 Experimental Evaluation",
            "text": "Our experimental evaluation addresses three questions:2\nQ1. Can our approach more accurately detect adversarial examples than its competitors? Q2. What is each approach\u2019s prediction time cost associated with detecting adversarial examples? Q3. How does the size of the reference set affect the performance of our OC-score metric?\nWe compare our OC-score to four approaches:\nAmbiguity (ambig) This approach uses the intuition that because adversarial examples are somehow different than the training ones, the model will be uncertain about an adversarial example\u2019s predicted label Grosse et al. [2018]. This entails deciding whether an example lies near a model\u2019s decision boundary. This can be done by ranking examples according to the uncertainty of the classifier:\nambig(x) = 1\u2212 |2 pT (x)\u2212 1|, (3)\nwhere pT is the probability of the positive class as predicted by the ensemble T for an example x.\nLocal outlier factor (lof ) Breunig et al. [2000] Another intuition to detect adversarial examples is to employ an anomaly detector under the assumption that adversarial examples are drawn from a different distribution than non-adversarial ones. lof is a state-of-the-art unsupervised anomaly detection method that assigns a score to each example denoting how anomalous it is. This approach entails learning a lof model which is applied to each example.\nIsolation forests (iforest) An isolation forest Liu et al. [2008] is state-of-the-art anomaly detector. It learns a tree ensemble that separates anomalous from normal data points by splitting on a randomly selected attribute using a randomly chosen split value between the minimum and maximum value of the attribute. Outliers tend to be split off earlier in the trees, so the depth of an example in the tree is indicative of how normal an example is. Again, this requires learning a separate model at training time.\nML-LOO This is an approach for detecting adversarial examples from the neural network literature Yang et al. [2020a]. Unlike most other approaches, it is model agnostic as it looks at statistics of the\n2The supplement addresses a fourth question: How does the proportion of adversarial examples in the test set affect performance of our OC-score metric?\nfeatures. It uses the accumulated feature attributions to rank examples: mlloo(x) = stdk { pT (x)\u2212 pT (x(k)) } , (4)\nwhere pT is the probability prediction of ensemble T , and x(k) is x with the kth attribute set to 0. The observation in Yang et al. [2020a] is that variation in the feature attributions is larger for adversarial examples."
        },
        {
            "heading": "5.1 Experimental methodology",
            "text": "We mimic the post-deployment setting using 5-fold cross validation. In each fold, a model is trained on clean training data (4 folds). Then, in the remaining fold some of the correctly classified examples are perturbed and turned into adversarial examples. This fold augmented by the perturbed examples is used as the unseen examples. The task is then, given the model and a reference set R, to detect which of the examples in the unseen set are adversarial using the OC-score.\nWe test our approach on the eight benchmark datasets listed in Table 1. All datasets are min-max normalized to make perturbations of the same size to different attributes comparable. To demonstrate our approach\u2019s generality, we consider three types of additive tree ensembles: (1) XGBoost boosted trees Chen and Guestrin [2016], (2) Scikit-learn random forests Pedregosa et al. [2011], and (3) GROOT robustified random forests Vos and Verwer [2021], which modifies the criteria for selecting the split condition when learning a tree to make them more robust against adversarial examples. Due to space constraints, we only show plots for 4 out of 8 datasets. The results for the remaining datasets are along the same lines and are provided in the supplement.\nExperimental settings For a given dataset, each learner has the same number of trees in the ensemble and each tree is restricted to be of the same maximum depth. Details for each dataset are given in Table 1. We use the scikit-learn Pedregosa et al. [2011] implementation for lof and iforest and use the default scikit-learn hyper-parameters. The supplement reports the average accuracies of the learned models on each dataset and the attack model of the GROOT ensembles. All experiments ran on an Intel E3-1225 with 32GB of memory. Multi-threading was enabled for all methods.\nGenerating adversarial examples We generate adversarial examples using Veritas Devos et al. [2021] with the l\u221e norm. Per fold in the benchmarks, we generate three different sets of adversarial examples. Each set is based on 500 randomly selected, correctly classified test set examples. The first set are the closest adversarial examples. For each of these adversarial examples, it is guaranteed that no other adversarial example exists that is closer to the original example. This set of examples corresponds to the ones generated by the Kantchelian et al.\u2019s MILP approach Kantchelian et al. [2016]. The second set of adversarial examples allows perturbations of size 2 \u00b7 \u03b4med, where \u03b4med is the median adversarial perturbation observed in the set of closest adversarial examples. The third set has a larger perturbations of size 5 \u00b7 \u03b4med. We refer to these three sets as closest adv., adv. x2, and adv. x5. Each set of adversarial examples thus has its own properties. The closest adversarial examples tend to barely cross a model\u2019s decision boundary whereas for the adversarial examples in adv. x5 the models could return extremely confident outputs. The supplement contains illustrative adversarial examples for the image datasets mnist2v4 and fmnist2v4. For each set of 500 adversarial examples, we construct the final evaluation set by adding 2500 randomly selected normal previously\nunseen (i.e., not used to train the model) test set examples, apart from phoneme and spambase where we select 1080 and 920 normal examples, respectively.3"
        },
        {
            "heading": "5.2 Results Q1: detecting adversarial examples",
            "text": "The task is to distinguish the 500 adversarial examples from the normal test set examples. We measure detection performance in two different ways: by evaluating ranking and coverage versus detection rate tradeoff.\nRanking. Each method assigns every test example a score, which can be used to rank examples from most to least likely that they are adversarial. The area under the ROC (AUC ROC) curve measures the quality of a ranking with respect to the classification task of separating adversarial from normal examples. In this case, it captures an approach\u2019s ability to distinguish adversarial examples from non-adversarial ones. Figure 2 shows the AUC ROC as a function of the magnitude of the adversarial perturbation for all methods, datasets, and ensemble learners.\nOur OC-score consistently performs the best overall. Its performance is stable across the three considered types of adversarial examples for all three ensemble learning techniques.\nAmbiguity is effective at detecting the closest adversarial examples on all datasets and ensemble types. This is unsurprising as the closest adversarial examples tend to be constructed by perturbing the attribute values just enough to cause the generated adversarial example to fall just on the other side of a model\u2019s decision boundary. Hence, by definition, the model is uncertain for these adversarial examples. Ambiguity\u2019s ability to detect adversarial examples declines when they exhibit larger perturbations. However, an exception is ijcnn1 and mnist2v4 for Random Forests and GROOT, where ambiguity detects most adversarial examples. For phoneme, the AUC values consistently drop below 0.5 for larger perturbation sizes. This arises because the model makes exceedingly confident incorrect\n3Because these two datasets have fewer examples.\npredictions for these adversarial examples, which causes normal examples to be ranked as more adversarial than actual adversarial examples. To illustrate: the mean ambiguity score is 0.017 for normal examples and drops from 0.68 for close adversarial examples to 0.008 for adv.x5 ones.\nThe anomaly detectors (iforest and lof ) tend to perform poorly on most settings. The exceptions are the adv. x5 examples on phoneme, covtype, and ijcnn1 because these start to become out of sample (i.e., very far away from the training data). These are also the datasets with the smallest number of attributes. These methods also do better on GROOT where the modified split criteria produces more robust trees. Hence, the adversarial examples are farther away from the normal ones.\nML-LOO has highly variable performance and is consistently worse than OC-score. It tends to work best on image data. However, for random forests and GROOT, it performs extremely poorly on several other datasets (e.g., ijcnn1, covtype). For the XGBoost ensembles, ML-LOO is generally effective at detecting the closest adversarial examples, but its performance degrades for the more distant ones.\nCoverage versus detection rate tradeoff. We now evaluate the detection approaches in an operational context where a model will first assess if an unseen example is adversarial or not. The model will only make predictions in cases where an examples is not flagged as being adversarial. In practice, this requires thresholding the scores produced by each method to arrive at a hard decision as to whether an example is adversarial. This induces a tradeoff between a method\u2019s (a) coverage, which is the fraction of normal test examples that are correctly identified as being normal and hence a prediction is made, and (b) detection rate, or the percent of correctly identified adversarial examples. Figure 3 shows each approach\u2019s detection rate as a function of coverage. The results average over all three types of adversarial examples. In nearly all cases, OC-score results in a higher detection for each level of coverage than all its competitors. Figure 3 only shows results for 4 out of 8 datasets, the results for the remaining datasets are in the supplement."
        },
        {
            "heading": "5.3 Q2: Prediction time cost",
            "text": "The run time cost has two parts: the setup time and the evaluation time. The setup time is the one-time cost incurred during training by some of the methods. This is negligible for most methods, apart from lof and is discussed in detail in the supplement.\nThe evaluation time measures the overhead of computing a score indicating how likely it is that a test example is adversarial. Table 2 reports the average per example evaluation time for each method on every dataset. Regardless of the method, the scores can typically be computed well under 1 millisecond, with some cases taking slightly longer. Unsurprisingly, ambiguity is almost always the fastest as this is a simple mathematical computation. iforest is also fast because it only requires executing a tree ensemble. Computing the OC-score is also relatively efficient due to exploiting SIMD. It scales with number of trees and the size of the reference set, yielding higher evaluation times for large datasets and ensemble sizes (i.e., covtype and higgs). However, the results in Subsection 5.4 indicate that it is possible to decrease the size of the reference set without degrading performance. ML-LOO\u2019s costs comes from computing an importance for each feature. Hence, it yields longer times for datasets with many features (mnist2v4, fmnist2v4, webspam). lof is almost always the (second) slowest because computing the local density for each example is expensive."
        },
        {
            "heading": "5.4 Q3: Effect of the reference set\u2019s size",
            "text": "Finally, we explore the effect of the size of the reference set by varying the proportion of correctly classified training examples in the reference set. Figure 4 shows the results for this experiment on four datasets using XGBoost ensembles. The plot on the left shows ROC AUC values for detecting adversarial vs. non-adversarial test examples. On all datasets, these values are relatively stable. There is a small decline in performance for the smallest reference set proportion, where the number of examples in the reference set ranges from 342 (spambase, RF) to 41 700 (covtype, XGB).\nThe plot on the right shows the average time in milliseconds to compute the OC-score per example as a function of the size of the reference set. The three smallest datasets show modest increases in the evaluation time for increasing reference set sizes. Higgs, which is by far the largest dataset, shows a steep increase as the size of the reference set increases which arises due to cache effects (i.e., misses) for the larger reference set sizes. However, these experiments suggest that using a small reference set would drastically improve the run time on this dataset without degrading performance. Note that changing the reference set does not require relearning the underlying ensemble used to make predictions, which is still learned using the full training set."
        },
        {
            "heading": "6 Conclusions and Discussion",
            "text": "This paper explored how to detect adversarial examples post deployment for additive tree ensembles by detecting unusual output configurations. Our approach works with any additive tree ensemble and does not require training a separate model. If a newly encountered example\u2019s output configuration differs substantial from those in the training set, then it is more likely to be an adversarial example. Empirically, our proposed OC-score metric resulted in superior detection performance than existing approaches for three different tree ensemble learners on multiple benchmark datasets. One limitation of our work is that operationalizing it requires setting a threshold on the OC-score which may be difficult in practice. Moreover, while our approach achieves good performance it is unclear\nwhat constitutes acceptable detection performance for a deployed system because this is use-case dependent. While our approach makes it more difficult to perform adversarial attacks, it may also inspire novel strategies to construct adversarial examples."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was supported by iBOF/21/075, Research Foundation-Flanders (EOS No. 30992574, 1SB1320N to LD) and the Flemish Government under the \u201cOnderzoeksprogramma Artifici\u00eble Intelligentie (AI) Vlaanderen\u201d program."
        },
        {
            "heading": "B Predictive performance of used tree ensembles",
            "text": "At the basis of each experiment in this paper are tree ensembles learned from data: we generate adversarial examples for these ensembles, and the OC-score uses the OC-space defined by the trees. We mimic the post-deployment setting, which means that we consider the tree ensembles given, and are not concerned with the circumstances in which it was learned, i.e., the point of this paper is not to train the most accurate models, nor to deal with adversarial attacks during the training phase. However, for completeness sake, we do include the test set accuracies and empirical robustness values achieved by the XGBoost, Random Forest and GROOT ensemble models to show that we are using competitive models.\nTable 3 shows the average accuracy over five folds for each ensemble type on each dataset. XGBoost generally performs the best. Random forests and GROOT ensembles tend to produce more robust models than XGBoost. They exhibit better empirical robustness scores as shown by the \u03b4med values in Table 4 and the adversarial examples generated for these ensembles tend to be further away from the original distribution. lof and iforest, which are particularly well suited at detecting if a test example is out-of-sample, are better at detecting adversarial examples for these ensembles than ones for XGBoost."
        },
        {
            "heading": "C AUC plots for all datasets",
            "text": "The main paper only shows the ROC AUC values for the first 4 datasets in Figure 2. Figure 5 shows ROC AUC plots for all datasets.\nThe ROC AUC values show how effective each method is at distinguishing adversarial examples from normal ones for a post-deployed model. Figure 5 show the performance in function of the magnitude of the adversarial perturbation. OC-score\u2019s performance is stable across the three considered types of adversarial examples for all three ensemble types."
        },
        {
            "heading": "D Detection rate plots for all datasets",
            "text": "Figure 3 in the main paper only shows the coverage versus detection rate results for the first four datasets. Figure 6 shows the coverage versus detection rate results for XGBoost, Random Forests and GROOT ensembles for all datasets. Our OC-score metric is consistently the best or equally as good as a competing method. It is also the only method that consistently performs well on all datasets and for all ensemble types."
        },
        {
            "heading": "E Setup time",
            "text": "Table 5 gives the setup times for each detection method. These are the one-time costs incurred during training. The setup time for OC-score is simply the time taken to encode the reference set R.4 The setup time for iforest and lof is the time needed to construct the models. lof has by far the largest setup time because it constructs an index of the training examples. The setup times of ambiguity and ML-LOO are 0, because they do not use any auxiliary structures that need to be initialized.\nF Time complexity of OC-score metric\nThe time complexity of the OC-score for a single example at prediction time is O(M \u00b7 |DR|). M is the number of trees and thus the size of an output configuration. |DR| is the size of the reference set. Figure 7 shows that, indeed, changing the size of the reference set has a linear effect on the evaluation time, except for the larger datasets where cache misses can have a considerable effect on the run time. Note that this effect is CPU dependent and can be less pronounced on other CPUs."
        },
        {
            "heading": "G Effect of the reference set\u2019s size",
            "text": "Figure 4 in the main paper shows the effect of varying the size of the reference set only for a selection of four datasets for XGBoost ensembles. Figure 7 shows the results for all datasets and all ensemble types. We observe similar results for all datasets and all ensemble types. The ROC AUC values (left subplots) for detecting adversarial vs. non-adversarial test examples are relatively stable, showing only a small decline in performance for the smallest reference set consisting of only 10% of the correctly classified training set examples. The time savings (right subplots) are also consistent across datasets and ensemble types.\nH Distribution of OC-space hamming distance for adversarial and random perturbations\nOne assumption that this paper makes is that adversarial perturbations are magnified in OC-space, and that they can be picked up in OC-space using the Hamming distance. To show that this is the case, we randomly select 100 unseen correctly classified test set examples, and perturb them in two ways. The first perturbation is adversarial and flips the predicted label of the example. The second perturbation is random and has the same (small) l\u221e and same l0 norm (i.e., same number of affected features) as the adversarial perturbation. We repeat this 5 times, once for each fold, and average the results.\nFigure 8 plots the Hamming distances between the original examples and the adversarially and randomly perturbed example respectively, for the three adversarial sets and the each ensemble type. We see that the adversarial perturbations have a much larger effect on the Hamming distance in the OC-space than the random perturbations overall, even though the magnitudes of the perturbations are the same. This is not surprising, as in order to flip the label, the adversarial perturbation has to be carefully crafted in such a way that different leaves are activated. This illustrates that adversarial\n4Note that tree learning entails sorting each training example to a leaf node, unless bagging is employed.\n19\nperturbations are magnified in OC-space, and that a simple metric like the Hamming distance can be used to detect them.\nThe plots in Figure 8 also show that the larger the perturbations, the larger the relative distances in OC-space become. Although this effect exists for both the random and adversarial perturbations, it is more pronounced for the latter. We also see that the smaller the number of attributes in the data, the weaker the difference is (e.g. phoneme). With fewer attributes, a random permutation is more likely to change an attribute that is used in many trees, which in turn has an effect on the predicted leaves."
        },
        {
            "heading": "I Prevalence of adversarial examples",
            "text": "While ROC analysis is generally invariant to varying class proportions, there is some debate if this is always the case Webb and Ting [2005]. Thus, for completeness, we explore the effect of the prevalence of adversarial examples in the test set on the detection performance of the OC-score metric. We reuse the same adversarial examples generated for the experiments in the main paper. To obtain the desired ratios, we randomly selected the following numbers of normal and adversarial examples respectively: (500, 500), (1000, 500), (1500, 500), (2000, 500), (2500, 500), (2400, 400), (2100, 300), (2400, 300), (1800, 200), (2500, 250). This is repeated five times for each fold, each time with a different model, different reference set, and different adversarial examples.\nFigure 9 shows the AUC ROC values for using OC-score to detect adversarial vs. non-adversarial test examples as function of the ratio of normal to adversarial examples in the test set. Results are shown for all datasets. Regardless of the ratio, the detection performance is relatively stable on all datasets. This is true for all three considered ensemble learners."
        },
        {
            "heading": "J Examples of adversarial examples for each ensemble type",
            "text": "Figures 10 and 11 show four adversarial examples, two of each class, for mnist and fmnist (pullover vs. coat). For each example and each ensemble type, we show the original example, the closest adversarial example (close adv.), adv. x2 and adv. x5. The predicted probability is shown below the image for each ensemble type.\nA first observation is that the predicted probabilities of the closest adversarial examples are indeed close to 0.5, which explains why ambiguity tends to pick them up.\nA second observation is that XGBoost is extremely confident in its predictions, and the perturbation sizes required to trick it are the smallest overall. For the 4s and all the fmnist examples, the affected pixels are hard to discern. Moreover, XGBoost is very confident in its wrong prediction for adv. x5 examples, even though the perturbations are tiny.\nA third observation is GROOT ensembles are more difficult to trick. Veritas often finds the same example for adv. x2 and adv. x5 because the space of adversarial examples is much smaller, and it thus tends to arrive at the same result even though maximum perturbation sizes are different."
        },
        {
            "heading": "K Error bars on coverage vs. detection rate and timings",
            "text": "We mentioned in the main paper that we did not include error ranges for the coverage versus detection rate plots for clarity. For completeness, we include these plots in Figure 12. Similarly, we also include the evaluation timings table with the standard deviations in Table 6 (Table 2 in the main paper).\n(XGB)"
        },
        {
            "heading": "L Total compute time for adversarial example generation",
            "text": "Table 7 shows the compute time required to compute the three sets of adversarial examples for each dataset and each model type. The total time taken to compute all adversarial examples is just below 46 hours.\nThe total training time for the ensembles is negligible with a total training time of less than 1 hour for all models. Training a GROOT model takes on average about 10 times longer than training a scikit-learn Pedregosa et al. [2011] Random Forests."
        }
    ],
    "title": "Adversarial Example Detection in Deployed Tree Ensembles",
    "year": 2022
}