{
    "abstractText": "Motivated by the emergence of decentralized machine learning ecosystems, we study the delegation of data collection. Taking the field of contract theory as our starting point, we design optimal and near-optimal contracts that deal with two fundamental machine learning challenges: lack of certainty in the assessment of model quality and lack of knowledge regarding the optimal performance of any model. We show that lack of certainty can be dealt with via simple linear contracts that achieve 1\u2212 1/e fraction of the first-best utility, even if the principal has a small test set. Furthermore, we give sufficient conditions on the size of the principal\u2019s test set that achieves a vanishing additive approximation to the optimal utility. To address the lack of a priori knowledge regarding the optimal performance, we give a convex program that can adaptively and efficiently compute the optimal contract.",
    "authors": [
        {
            "affiliations": [],
            "name": "Nivasini Ananthakrishnan"
        },
        {
            "affiliations": [],
            "name": "Stephen Bates"
        },
        {
            "affiliations": [],
            "name": "Michael I. Jordan"
        },
        {
            "affiliations": [],
            "name": "Nika Haghtalab"
        }
    ],
    "id": "SP:9e3026221e32a4496b7d1d5c49963b77d4bea608",
    "references": [
        {
            "authors": [
                "Daron Acemoglu",
                "Ali Makhdoumi",
                "Azarakhsh Malekian",
                "Asu Ozdaglar"
            ],
            "title": "Too much data: Prices and inefficiencies in data markets",
            "venue": "American Economic Journal: Microeconomics,",
            "year": 2022
        },
        {
            "authors": [
                "Anish Agarwal",
                "Munther Dahleh",
                "Tuhin Sarkar"
            ],
            "title": "A marketplace for data: An algorithmic solution",
            "venue": "In Proceedings of the 2019 ACM Conference on Economics and Computation,",
            "year": 2019
        },
        {
            "authors": [
                "Anish Agarwal",
                "Munther Dahleh",
                "Thibaut Horel",
                "Maryann Rui"
            ],
            "title": "Towards data auctions with externalities",
            "venue": "arXiv preprint arXiv:2003.08345,",
            "year": 2020
        },
        {
            "authors": [
                "Tal Alon",
                "Paul D\u00fctting",
                "Inbal Talgam-Cohen"
            ],
            "title": "Contracts with private cost per unit-of-effort",
            "venue": "In Proceedings of the 22nd ACM Conference on Economics and Computation,",
            "year": 2021
        },
        {
            "authors": [
                "Tal Alon",
                "Paul D\u00fctting",
                "Yingkai Li",
                "Inbal Talgam-Cohen"
            ],
            "title": "Bayesian analysis of linear contracts",
            "venue": "arXiv preprint arXiv:2211.06850,",
            "year": 2022
        },
        {
            "authors": [
                "Stephen Bates",
                "Michael I. Jordan",
                "Michael Sklar",
                "Jake A. Soloff"
            ],
            "title": "Principal-agent hypothesis testing",
            "venue": "arXiv preprint arXiv:2205.06812,",
            "year": 2022
        },
        {
            "authors": [
                "Dirk Bergemann",
                "Alessandro Bonatti"
            ],
            "title": "Markets for information: An introduction",
            "venue": "Annual Review of Economics,",
            "year": 2019
        },
        {
            "authors": [
                "Yang Cai",
                "Constantinos Daskalakis",
                "Christos Papadimitriou"
            ],
            "title": "Optimum statistical estimation with strategic data sources",
            "venue": "In Conference on Learning Theory,",
            "year": 2015
        },
        {
            "authors": [
                "Gabriel Carroll"
            ],
            "title": "Robustness and linear contracts",
            "venue": "American Economic Review,",
            "year": 2015
        },
        {
            "authors": [
                "Hector Chade",
                "Jeroen Swinkels"
            ],
            "title": "Disentangling moral hazard and adverse selection",
            "venue": "Technical report, Working Paper, Arizona State",
            "year": 2019
        },
        {
            "authors": [
                "Junjie Chen",
                "Minming Li",
                "Haifeng Xu"
            ],
            "title": "Selling data to a machine learner: Pricing via costly signaling",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Lingjiao Chen",
                "Paraschos Koutris",
                "Arun Kumar"
            ],
            "title": "Towards model-based pricing for machine learning in a data marketplace",
            "venue": "In Proceedings of the 2019 International Conference on Management of Data,",
            "year": 2019
        },
        {
            "authors": [
                "Alessandro Chiesa",
                "Tom Gur"
            ],
            "title": "Proofs of proximity for distribution testing",
            "venue": "Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik,",
            "year": 2018
        },
        {
            "authors": [
                "Paul D\u00fctting",
                "Tim Roughgarden",
                "Inbal Talgam-Cohen"
            ],
            "title": "Simple versus optimal contracts",
            "venue": "In Proceedings of the 2019 ACM Conference on Economics and Computation,",
            "year": 2019
        },
        {
            "authors": [
                "Paul D\u00fctting",
                "Tim Roughgarden",
                "Inbal-Talgam Cohen"
            ],
            "title": "The complexity of contracts",
            "venue": "In Proceedings of the 2020 ACM-SIAM Symposium on Discrete Algorithms,",
            "year": 2020
        },
        {
            "authors": [
                "Peter Frazier",
                "David Kempe",
                "Jon Kleinberg",
                "Robert Kleinberg"
            ],
            "title": "Incentivizing exploration",
            "venue": "In Proceedings of the Fifteenth ACM Conference on Economics and Computation,",
            "year": 2014
        },
        {
            "authors": [
                "Shafi Goldwasser",
                "Guy N Rothblum",
                "Jonathan Shafer",
                "Amir Yehudayoff"
            ],
            "title": "Interactive proofs for verifying machine learning",
            "venue": "In 12th Innovations in Theoretical Computer Science Conference (ITCS 2021). Schloss Dagstuhl-Leibniz-Zentrum fu\u0308r Informatik,",
            "year": 2021
        },
        {
            "authors": [
                "Guru Guruganesh",
                "Jon Schneider",
                "Joshua R Wang"
            ],
            "title": "Contracts under moral hazard and adverse selection",
            "venue": "In Proceedings of the 22nd ACM Conference on Economics and Computation,",
            "year": 2021
        },
        {
            "authors": [
                "Chien-Ju Ho",
                "Aleksandrs Slivkins",
                "Jennifer Wortman Vaughan"
            ],
            "title": "Adaptive contract design for crowdsourcing markets: Bandit algorithms for repeated principal-agent problems",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2016
        },
        {
            "authors": [
                "Hengrui Jia",
                "Mohammad Yaghini",
                "Christopher A Choquette-Choo",
                "Natalie Dullerud",
                "Anvith Thudi",
                "Varun Chandrasekaran",
                "Nicolas Papernot"
            ],
            "title": "Proof-of-learning: Definitions and practice",
            "venue": "IEEE Symposium on Security and Privacy (SP),",
            "year": 2021
        },
        {
            "authors": [
                "Jon Kleinberg",
                "Robert Kleinberg"
            ],
            "title": "Delegated search approximates efficient search",
            "venue": "In Proceedings of the 2018 ACM Conference on Economics and Computation,",
            "year": 2018
        },
        {
            "authors": [
                "Jean-Jacques Laffont",
                "David Martimort"
            ],
            "title": "The Theory of Incentives",
            "year": 2009
        },
        {
            "authors": [
                "Zhiyuan Liu",
                "Huazheng Wang",
                "Fan Shen",
                "Kai Liu",
                "Lijun Chen"
            ],
            "title": "Incentivized exploration for multi-armed bandits under reward drift",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Esther Rolf",
                "Theodora T Worledge",
                "Benjamin Recht",
                "Michael Jordan"
            ],
            "title": "Representation matters: Assessing the importance of subgroup allocations in training data",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Chris Ying",
                "Aaron Klein",
                "Eric Christiansen",
                "Esteban Real",
                "Kevin Murphy",
                "Frank Hutter"
            ],
            "title": "Nasbench-101: Towards reproducible neural architecture search",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The design of machine learning pipelines is increasingly a cooperative, distributed endeavor, in which the expertise needed for the design of various components of an overall pipeline is spread across many stakeholders. Such expertise pertains in part to classical design choices such as how much and what kind of data to use for training, how much test data to use for verification, how to train a model, and how to tune hyper-parameters, but, more broadly, expertise may reflect experience, access to certain resources, or knowledge of local conditions. To the extent that there is a central designer, their role may in large part be that of setting requirements, developing coordination mechanisms, and providing incentives.\nOverall, we are seeing a flourishing new industry at the intersection of ML and operations which makes use of specialization and decentralization to achieve high performance and operational efficiency. Such an ML ecosystem creates a need for new design tools and insights that are not focused merely on how the designer could perform a task in this pipeline, but rather how she should delegate it to agents who are willing and capable of performing the task on her behalf. How should the designer interact with this ecosystem? How should she evaluate and compensate other agents for their work? How does the outcome of the delegated pipeline compare with the outcome if the designer were to perform the task by herself? In this work, we initiate the study of delegating machine learning pipelines through the lens of contract theory and take a step towards answering these questions.\nContract theory provides a principal-agent perspective, where the principal\u2014who is the designer interested in the outcome of the learning pipeline\u2014can create a contractual arrangement\u2014a menu of services and compensations\u2014with an agent. At the heart of the issue is creating contracts that incentivize the agents, who may be more knowledgeable and skilled than the principal, to take the appropriate actions. The uncertain and data-centric nature of machine learning tasks brings to\nar X\niv :2\n30 9.\n01 83\n7v 1\n[ cs\n.L G\n] 4\nS ep\n2 02\n3\nlight interesting sources of knowledge asymmetry between the principal and the agent and requires extensions of classical contract theory.\nConsider a scenario where a firm delegates a predictive task to an ML service provider. In this context, the service provider may offer the firm either a dataset for learning or a pre-trained predictive model based on that dataset. To ensure aligned incentives, the firm needs to assess the dataset or predictive model and design the payment structure for the service provider accordingly. Since the accuracy of the model is crucial to the firm as it directly influences revenue, a natural evaluation approach involves directly measuring the accuracy of the model that the service provider produces for the firm. Several challenges arise during this evaluation process. Firstly, the firm generally only has limited data in the form of historical data or data acquired shortly after deploying the model for evaluation. So there is inherent noise in the evaluation. Secondly, the firm lacks knowledge about the baseline accuracy that is realistically achievable. This makes it harder for the firm to reward the service provider in a way that yields accuracy close to the optimal accuracy. These challenges are due to two sources of uncertainty and asymmetry that we study in this work:\n\u2022 Hidden actions (aka Moral Hazard): Contracts must compensate the agent for his effort towards creating a good outcome for the principal. These contracts therefore must depend on observable and verifiable outcome quality, such as the true accuracy of a classifier. This is particularly challenging in machine learning pipelines, where the accuracy of the learned model is unknown a priori and random. The principal may be able to invest in resources, such as large test sets, that reduce this uncertainty at a cost and better evaluate the agent\u2019s effort. An important consideration here is determining whether the principal must accurately verify the outcome or instead incentivize the agent in the first place to ensure a high-quality outcome.\n\u2022 Hidden state (aka Adverse Selection): Effective contracts use the knowledge of the best achievable outcome to incentivize the agents to work towards such outcomes. This is challenging in machine learning pipelines where the true error of the best model is unknown. Furthermore, generic methods that estimate the optimal accuracy tend to use almost as many resources as are needed to learn a model of that accuracy. Here again, we must ask whether contracts exist that appropriately incentivize the agent to perform his best while knowing very little about the optimal possible accuracy."
        },
        {
            "heading": "1.1 Our results",
            "text": "We consider performance-based contracts where the agent is compensated as a function of the accuracy of the learned model. We use assumptions on the mapping between the agent\u2019s actions to expected observed performance to design the contract. These assumptions are inspired by statistical error characterization (see Section 2 for more details). The principal\u2019s utility is the accuracy of the learned classifier minus the monetary transfer she makes to the agent.\nWe address the challenges of hidden actions (i.e., unknown solution quality) and hidden state(i.e., unknown quality of the best classifier) in delegated learning. For hidden actions, our main theorem (Proposition 2) shows that a simple linear mechanism gets 1\u22121/e fraction of the optimal contract and can work with test sets of size as small as 1. Furthermore, this approximation is tight (Theorem 4). To go beyond 1\u2212 1/e, in Section 4 we explore settings where the test sets have significantly more samples. We show in Theorem 2 that when the test set has about \u03f5\u22122 samples (independently of the model size), there is a contract that achieves an \u03f5-additive approximation to the optimal utility.\nFocusing on the hidden state challenge, we consider a setting where the optimal accuracy can be one of N values and design a contract that is individually rational (i.e., has nonnegative utility to the agent) and incentive-compatible (i.e., incentivizes the agent to work towards the optimal\naccuracy). Our main result here casts this problem as a convex optimization problem (see Opt) with O(n2) constraints.\nIn Sections 4.2 a more realistic form of information asymmetry between the principal and agent where the agent gradually learns about the hidden state as she collects more samples. Numerically, we show that we can achieve approximate optimality against these learning agents."
        },
        {
            "heading": "1.2 Related work",
            "text": "There is a rich literature on contract theory in economics (see, e.g., Laffont and Martimort, 2009; Bolton and Dewatripont, 2004). More recently, there has been work on algorithmic and statistical aspects of contract theory (Carroll, 2015; D\u00fctting et al., 2019; D\u00fctting et al., 2020; Bates et al., 2022; Alon et al., 2022) which include results on approximation by simple contracts. These results hold for either finite actions or outcomes, and thus are not directly applicable to our setting, which involves infinite actions and a continuous space of outcomes. Working in such spaces requires utilizing the structure of our problem, and specifically exploiting fundamental results on statistical minimax rates.\nThe pricing of data has been considered for various purposes and considerations (Bergemann and Bonatti, 2019; Acemoglu et al., 2022; Cai et al., 2015; Ho et al., 2016) including in learning problems (Agarwal et al., 2019; Chen et al., 2022). The latter study the pricing of previously collected data to incentivize the seller and buyer to be forthright about the valuation and quality of their data, respectively. We are interested instead in pricing for the purpose of incentivizing the data collecting agent to exert effort to collect data. Some of these papers also consider incentivizing high-quality data labelling by relying on multiple labellers who can be compared. We study delegation of learning in the setting of a single agent.\nAn adversarial perspective on the delegation problem has been considered for machine learning from the lens of interactive proofs. In this line of work (Goldwasser et al., 2021; Chiesa and Gur, 2018), the principal wants to fully verify the effort of an agent who may be an adversary that is interested in getting his effort verified. While they deal with similar challenges, such as not knowing the optimal achievable error, they do not consider incentivizing the agent (via contracts and compensations) to improve the outcome.\nConcurrent work by Saig et al. (2023) studies a similar setting of incentivizing data collection for classification. They characterize the optimal contract for a given test set size, under the hidden action challenge, as a threshold contract when the agent has two choices for actions. They provide conditions which make the threshold contract optimal even for additional actions. They study empirically the effect of the hidden state challenge. We provide results for an arbitrary number of actions and propose a contract that is based on a single test sample that is optimal relative to what is achievable without the hidden action and hidden state challenges. We show that this contract is robust to hidden state challenges in many cases and describe other approaches of dealing with hidden state challenges outside of these cases."
        },
        {
            "heading": "2 Model",
            "text": "We have a task distribution D representing the joint distribution over the domain and label set. The principal aims to learn a classifier h that achieves high accuracy on D, denoted by 1\u2212 LD(h). To accomplish this, the principal delegates the task to an agent who selects the number of samples to collect and trains a classifier. We prioritize the collection of samples as the primary effort, considering it more significant than classifier training. The principal\u2019s primary objective is to incentivize high-quality data collection, leading to the development of an accurate classifier. To evaluate the performance of the model obtained through delegation, the principal possesses an\nindependent test set consisting of independently and identically distributed (i.i.d.) points drawn from the distribution D. The principal utilizes this test set to evaluate the learned classifier\u2019s accuracy.\nAs in many other delegation settings, the principal faces the hidden state and hidden action challenges when delegating learning. While the principal desires to construct a contract based on the true accuracy of the learned model, 1\u2212 LD(h), they can only obtain a noisy estimate of this value using test data. Our focus is on scenarios where the size of the test dataset is not excessively large. If the test dataset is too large, it becomes more beneficial for the principal to learn a model using their own test data rather than delegating the data collection process. Even when the estimate of the learned model\u2019s accuracy has negligible noise, the principal still faces the hidden state challenge, i.e., the principal does not know how to value the accuracy since she does not know the optimal error achievable. We use 1\u2212 \u03b8 to indicate the optimal accuracy achievable on D. Assigning a low payment for the model\u2019s accuracy when the optimal error, \u03b8, is high would result in negative agent utility, discouraging agent participation. Conversely, assigning a high payment when the optimal error is low might incentivize the agent to collect a smaller dataset than is optimal for the principal.\nThe delegation process begins with the principal publishing a contract which is a mapping from test accuracy to payment for the agent. Seeing the contract, the agent collects data and provides a classifier to the principal. The principal then executes the contract by evaluating the classifier on her own test set. The principal pays the agent the amount specified by the contract for the measured test accuracy. We assume that the principal can commit to a test set in advance and that this test set is not accessible to the agent until the contract is executed after the agent\u2019s data collection.\nUtilities. Upon receiving a classifier with accuracy a for the task distribution D and paying the agent t, the principal gets utility a\u2212 \u03b2t for some constant \u03b2 > 0. The agent exerts effort \u03b1 per sample it collects. So the utility for the agent receiving payment t by collecting n samples is t\u2212 \u03b1n.\nOutcome as a function of the agent\u2019s action. We assume that when the agent collects n samples,1 the classifier\u2019s observed accuracy on the principal\u2019s test set drawn from D is drawn from a distribution with mean 1\u2212 \u03b8 \u2212 dnp and variance that is determined by the size of the test set. The constant d depends on the complexity of the training algorithm and the constant p describes the rate of decay of the excess error. These rates are motivated in part by minimax statistical rates and scaling laws.\nEven though minimax rates are typically upper bounds, we treat them as exact rates in the main body and defer the discussion on the implications of treating them as upper bounds to Appendix B.1.\nRemark 1 (VC dimension bound). An algorithm that PAC-learns a function class H with VC dimension d using n i.i.d. samples drawn from D and returns a classifier h satisfying LD(h) \u2264 \u03b8(D,H)+C \u221a d/n, where \u03b8(D,H) = minh\u2208H LD(h). This is minimax-optimal as there is a distribution\nD such that LD(h) \u2265 \u03b8(D,H) + C \u221a d/n.\nRemark 2 (Linear regression model). In a d-dimensional linear model with covariates xi \u223c N (0,\u03a3) and outcomes yi = \u03b2txi + \u03f5i, where \u03f5i \u223c N (0, \u03c32) for i \u2208 [n], the Ordinary Least Squares (OLS) estimator \u03b2\u0302 satisfies the property E[(xt\u03b2\u0302 \u2212 yi)2] = \u03c32 (1 +O(d/n)).\nFirst-best contracts. As a benchmark for the best performance we can hope to achieve, we first consider the problem in an idealized setting without the hidden state and hidden action challenges. This is when the principal knows the optimal error \u03b8 and the mapping between the agent\u2019s action n and the test accuracy of the resulting model is deterministic (i.e., is exactly 1\u2212 \u03b8\u2212 dnp ). The optimal contract in this idealized setting is called the first-best contract. The next proposition provides a closed form for this contract.\n1We will consider agent\u2019s action as continuous and the true sample size is a rounding of the action.\nProposition 1 (First-best contract). For any set of problem parameters \u03b8 \u2208 [0, 1), d, p, \u03b1, \u03b2 > 0, the first-best contract offers payment \u03b1n\u2217 when the test accuracy is at least 1 \u2212 \u03b8 \u2212 d/n\u2217p, where n\u2217 = (pd/\u03b1\u03b2)1/(p+1).\nOne way to interpret the first-best contract is that it asks the agent to collect n\u2217 samples and compensates the agent exactly for n\u2217 samples. Without hidden state or hidden action, the first-best contract yields zero utility to the agent. In this idealized scenario, the principal\u2019s utility due to the first-best contract is called the first-best utility and serves as a benchmark for comparison in our analysis of delegation. While first-best utility is used as a benchmark, the first-best contract itself may not be optimal due to existing randomness in test accuracy (hidden action). Additionally, each optimal error value \u03b8 leads to a different first-best contract, which is not implementable when the principal doesn\u2019t know the \u03b8 parameter exactly (hidden state). When dealing only with hidden action but known \u03b8, the principal\u2019s goal is to set up a contract specified for \u03b8 that deals with the randomness in the test accuracy to recover some fraction of the first-best utility. However, when both actions and states are unknown (uncertainty in \u03b8 and test accuracy) the contract must ensure good principal utility for a range of possible states \u03b8.\nLinear contracts. As opposed to first-best contracts that can be quite complex, linear contracts are simple contracts that compensate an agent by a linear function of the test accuracy. That is, a c-linear contract for parameter c \u2208 R+ assigns payment Tc(a) = c\u00d7 a when the test accuracy is a.\nLinear contracts must have non-negative parameter c, since the principal cannot make negative payments to the agent."
        },
        {
            "heading": "3 Optimality of linear contracts",
            "text": "In this section, we aim to find near-optimal contracts in the realistic scenario with hidden state and hidden actions, recognizing that the first-best contract may not be optimal. Our main result is that a linear contract compensating the agent based on the test (and not true) accuracy is approximately optimal across all possible contracts for the principal. Moreover, the slope of the linear contract has an explicit value that is the same across a wide range of \u03b8 making it possible to deal with both hidden state and hidden state challenges.\nA crucial advantage of our linear contract is that it works with any unbiased estimator of the accuracy of the learned model. Therefore, even a test set of size one suffices to enact this contract. We state our main results in this section and defer their formal proofs to Appendix A.\nConsider the hidden action (but known state) challenge where the principal knows \u03b8 but not the random mapping from the agent\u2019s action n to test accuracy. This mapping has a mean 1\u2212 \u03b8 \u2212 dnp ) and a variance dependent on the test set size. The variance is finite but possibly arbitrarily large. This setting includes the delegation problem where the principal has as little as just a single sample x \u223c D in her test set. Furthermore, we assume that beyond knowing the mean of the distribution of the test accuracy 1\u2212 \u03b8 \u2212 dnp , the distribution can be arbitrary and unknown to the principal.\nOur main result is that we can design an approximately optimal linear contract. Furthermore, under a wide range of problem parameters \u03b8, the principal does not even need to know the optimal error to construct this contract. This allows us to deal with both hidden state and hidden action challenges. Our results in fact show a stronger comparison, that linear contracts approximate not just the optimal utility but also the first-best utility. This is quite a strong guarantee as there is often no contract that can achieve the first-best utility in presence of the hidden action challenge.\nBefore we state our main theorem, we start with the following proposition which deals only with the hidden action challenge while assuming that optimal error \u03b8 is known to the principal. +Our\nmain result in Theorem 1 follows from this proposition and shows that the linear contract in this proposition is also a good choice in more general settings.\nProposition 2 (Linear contracts are approximately optimal when optimal error is known). For any set of problem parameters \u03b8 \u2208 [0, 1), d, p, \u03b1, \u03b2 > 0, if the principal knows \u03b8 (but not the distribution of the test error) she can construct a linear contract that brings an expected utility that is at least 1\u2212 1/e times the first-best utility. Furthermore this contract only requires a single test sample.\nThe linear contract c\u2217 that achieves this approximately optimal utility is the following:\nc\u2217 = max\n( 1\n\u03b2(p+ 1) p+1 p\n, \u03b1d\n1 p p \u00b7 ( p+ 1 1\u2212 \u03b8 ) p+1 p ) .\nAt a finer level, this linear contract approximates the first-best utility by a factor of\n1\u2212 1\n(p+ 1) p+1 p\n\u2265 1\u2212 1 e .\nLet us first note that previous work (Alon et al., 2022; D\u00fctting et al., 2019) has provided constant approximation guarantees but is limited to settings where the agent\u2019s action set is a finite set or where the ratio of the maximum and minimum reward for the principal is bounded by H. In the former case, an approximation ratio of 1/2 is obtained and in the latter the ratio is 1/2 log(H). Neither of these conditions hold in our settings, as the action is the number of samples collected and is unbounded and the reward can take any value in (0, 1). Instead, we use the structure of first-best contract (Proposition 1), the linearity of contracts, and the structure of the utility functions to obtain this 1\u2212 1/e approximation guarantees.\nProof sketch of Proposition 2. The full details are deferred to the appendices; here we provide some intuition and a proof sketch. Underlying the proof is the linearity of expectation and the fact that the agent is expectation-maximizing. Under a linear contract c, the expectation-maximizing agent aims to maximize E[c \u00b7 a(n) \u2212 \u03b1n] = c \u00b7 E[a(n)] \u2212 \u03b1n, where a(n) is the test-set accuracy of a model trained on n samples drawn from an unknown distribution with mean 1 \u2212 \u03b8 \u2212 d/np. The only distribution-dependent quantity in this maximizing objective is the expected accuracy E[a(n)] = 1\u2212 \u03b8\u2212 d/np. So the agent\u2019s action and hence the principal\u2019s contract design only depends on the expectation of the test accuracy and not on the exact distribution of the test accuracy. Next we sketch a proof for the approximation result and use the structured way the expected accuracy depends on the number of samples drawn.\nNote that c\u2217 is the maximum of two terms. Let us denote these terms by c1, c2. Given a linear contract with parameter c, the agent\u2019s best response is to choose n so as to maximize u(n; c) = c (1\u2212 \u03b8 \u2212 d/np) \u2212 \u03b1n. The maximizing value is n(c) = (cdp/\u03b1) 1 p+1 . By setting c large enough, we have u(n(c), c) \u2265 0 where c2 is the threshold above which this holds. So the value of c2 is set to ensure the agent gets non-negative utility from participating.\nWhen c1 \u2265 c2, c1 satisfies the participation constraint. By computing the principal\u2019s utility from the linear contract c1 using the expression for the agent\u2019s best response, we see that it is 1\u2212 \u03b2c1 times the first-best utility. Moreover, we have 1\u2212 \u03b2c1 = 1\u2212 1/(p+ 1) p+1 p . It turns out the same upper bound holds for the approximation ratio of the linear contract c2 to the first-best utility when c2 \u2265 c1. This upper bound is decreasing in p and the limit as p \u2192 0 is 1\u2212 1/e.\nImportantly, by inspecting the contract in Proposition 2, we see that in many cases it does not depend on problem-specific parameters like the optimum error. This makes c\u2217 deployable in practice.\nThe optimal-error-parameter-agnostic linear contract is appropriate when the cost per sample collection is small enough and when the optimal error is low enough. As a result, when \u03b1 is small, we can relax the assumption that the principal knows the exact optimum error \u03b8 to that the principal knows that \u03b8 lies in a certain range. Moreover, even under this relaxation, linear contracts are still approximately optimal. This is stated as the following theorem.\nTheorem 1 (Main result). For any d, p, \u03b2 > 0, consider the linear contract c\u0304 = 1 \u03b2(p+1) p+1 p . For any\n\u03b8 \u2208 [0, 1), suppose the optimum error \u03b8 is any value in [0, \u03b8) and that 0 < \u03b1 \u2264 p \u03b2d1/p\n( 1\u2212\u03b8\n(p+1)2\n) p+1 p .\nThen, c\u0304 has utility at least (1\u2212 1/e) times the first-best utility.\nNote that c\u0304 is constructed based on p (error decay rate) and \u03b2 (how the principal values accuracy relative to payment). The principal knows these quantities. In contrast, the optimal contract requires additional knowledge, such as \u03b8 (optimum error) and \u03b1 (agent\u2019s cost per sample). The theorem demonstrates a simple contract that requires less knowledge but remains approximately optimal in utility."
        },
        {
            "heading": "4 Extensions",
            "text": "Medium test set regime. In our analysis, we have examined the impact of the hidden action challenge when dealing with a small test set size. The significance of the hidden action challenge diminishes as the test set size increases, as the principal can obtain highly accurate estimates of the model\u2019s accuracy. However, when the test set becomes too large, delegation loses its value since the principal can independently learn an accurate model without delegation. Is there a regime in which the test set size is large enough for hidden action to not be significant while also being small enough for the principal to benefit from delegating data collection? In this section, we demonstrate the existence of such a regime, referred to as the \u201cmedium test set regime.\u201d Later, we outline how we can capitalize on the larger size of the test set to achieve stronger results.\nThe sample complexity for learning an \u03f5-optimal model is \u0398(d/\u03f52). In particular, this bound is linear in the training algorithm\u2019s complexity which can be problematic when using highly complex training algorithms. We say that the medium test set regime exists, if the sample complexity for hidden action is significantly smaller than \u0398(d/\u03f52), where \u03f5 captures the significance level of hidden action which we will make precise in the following definition.\nDefinition 1 (Insignificance of hidden action at level \u03f5). In a finite test set setting with hidden action, for any optimal error parameter \u03b8, let OPT denote the optimal expected utility of contracting. We say that hidden action is insignificant at level \u03f5, for any \u03f5 > 0, if the expected utility of the first-best contract based on \u03b8 in this setting is at least OPT\u2212 \u03f5.\nWe next state a theorem giving the sample complexity of the principal\u2019s test set to achieve insignificance of the hidden action. The sample complexity stated in the theorem is logarithmic in d while learning would have required a number of samples linear in d. This demonstrates the existence of a medium test set regime where it is possible to employ delegation without considering hidden action.\nTheorem 2 (Sample complexity for insignificant hidden action). For any \u03f5 > 0, if the principal has a test set of size O ( 1 \u03f52 log d\u03f5 ) , then hidden action is insignificant at the level \u03f5."
        },
        {
            "heading": "4.1 Optimal contracts for hidden state",
            "text": "By ignoring hidden action in the medium test set regime, we can hope to design contracts with stronger guarantees. Previously we were able to design contracts with high utility when the optimal error lies in a particular range given in Theorem 1. By ignoring hidden action, we can design contracts with utility guarantees for when the optimal error lies in any arbitrary set. When the principal holds a finitely supported prior belief over the optimal error value, we show how to compute the optimal contract by setting up a convex optimization problem. We also describe some qualitative properties of the optimal contract in this setting.\nWhen we ignore the hidden action challenge, we can assume that the observed accuracy is deterministic in the agent\u2019s action. That is, when the agent collects n samples, the observed accuracy is a(n, \u03b8) = 1\u2212 \u03b8\u2212 d/np. We assume that the principal holds a prior belief on the optimal error but does not know the exact value. The agent knows more about the optimal error since he collects data that informs him more about the optimal error. We assume that the agent knows the exact optimal error. We start by analyzing the optimal contract in this setting. Later in Section 4.2, we discuss how to design contracts in the more realistic setting of the agent learning the optimal error instead of knowing this value exactly. And we show that the utility guarantees by making the perfectly aware agent assumption still hold approximately in the more realistic case with a learning agent.\nLet us analyze the optimization problem for computing the optimal contract. Let the finite support of the prior over optimal error be {\u03b81, . . . , \u03b8N}. The principal puts forth a contract of accuracy-payment pairs {(ai, ti) : i \u2208 [N ]} with the pair i intended for when the optimal error is \u03b8i.2 Let us denote the expected accuracy from collecting ni when optimal error is \u03b8i by ai = a(ni, \u03b8i). Here ni is the number of samples the agent would collect to achieve accuracy ai when optimal error is \u03b8i. The principal optimizes over (ni, ti)i\u2208[N ]. The constraints of the optimization problem for the principal\u2019s contract design for hidden state are one of two types. The first type of constraint is the participation constraint, which ensures that the agent is adequately compensated for his effort when he chooses the contract intended for the optimal error. For each i \u2208 [N ], the participation constraint (PCi) can be expressed as \u03b1ni \u2264 ti, where \u03b1 represents the compensation rate.\nThe second type of constraint is the incentive compatibility constraint to ensure that the agent chooses the option intended in the contract for the optimal error. For any i, j \u2208 [N ], the corresponding incentive compatibility constraint is that when the optimal error is \u03b8i, the utility of choosing (aj , tj) is worse for the agent than choosing (ai, ti). The number of samples the agent would choose to achieve aj accuracy under optimal error \u03b8i is nij such that aj = a(nij , \u03b8i).3 The constraint (ICij) is tj \u2212 \u03b1nij \u2264 ti \u2212 \u03b1ni. Due to the structure of a(n, \u03b8), the IC constraints are convex. The principal\u2019s expected utility which it maximizes is \u2211N i=1 \u03bd(\u03b8i)(ai \u2212 \u03b2ti). So the contract design problem is the following optimization problem:\nmin (ni,ti)Ni=1 N\u2211 i=1 \u03bd(\u03b8i)(ai \u2212 \u03b2ti)\ns.t. \u03b1ni \u2264 ti, i \u2208 [N ] tj \u2212 \u03b1nij \u2264 ti \u2212 \u03b1ni, i, j \u2208 [N ] ni, ti \u2265 0, i \u2208 [N ].\n(Opt)\n2This is implied by the revelation principle that states that, with hidden state, any delegation mechanism is equivalent to an incentive compatible mechanism where all agents inform their private information to a planner who then recommends actions.\n3Note that all accuracies cannot be achieved for all optimal errors. If no such nij exists, an incentive compatibility constraint is not needed.\nQualitative insights on the optimal contract. We derive the following insights (see\nFigure 4.1) when there are two values for the optimal error, \u03b81 < \u03b82, in the Appendix B.3. These properties also hold more generally for finitely supported beliefs and have been studied for classical contract design for many other delegation problems Laffont and Martimort (2009).\n\u2022 Decreased utility . The principal gets lower utility than the first-best utility and this utility decreases\nas \u2206\u03b8 = \u03b82 \u2212 \u03b81 increases. \u2022 Information rent. In the first-best contract, the agent gets no more payment than to compensate\nhis effort. That is, t = \u03b1n. Under hidden state, for problems with lower optimal error, the agent gets positive utility. This information rent is to incentivize the agent to not pretend the problem is harder and exert lower effort to achieve an accuracy that requires more effort if the problem was harder.\n\u2022 Downward distortion. The first-best contract calls for the agent to collect a particular number of samples regardless of the optimal error. Under hidden state, when the problem is harder, agents are asked to collect fewer samples compared to the first-best contract. When the problem is the easiest in the support, the agent is asked to collect the same number of samples as in the first contract."
        },
        {
            "heading": "4.2 Designing contracts against state-learning agents",
            "text": "In Section 4.1 and particularly Opt, we assumed perfect knowledge of the hidden state (\u03b8) by the agent. However, in reality, the agent does not know the optimal error beforehand. Instead, as the agent executes the contract, he learns more about the optimal error and adapts his actions accordingly. To design a contract for such a state-learning agent, the principal would need to predict the agent\u2019s response to the contract. However, this is challenging for arbitrary contracts since the principal would require knowledge of the agent\u2019s exact learning strategy, which is often unreasonable. Therefore, we focus on analyzing simple contracts for which we can easily derive the agent\u2019s response. We demonstrate numerically that the utility achieved with these simple contracts is close to the utility we previously derived for state-aware agents, which we refer to as \u201cstate-aware utility.\u201d This provides evidence that qualitative insights we derived about the state-aware utility in Section 2 and Section 3 are applicable in the more realistic case of a state-learning agent. We focus on the case\nwhere the optimal error can take one of two possible values, \u03b81 < \u03b82, but these design principles also extend to more possible values of the optimal error. The simple contract we consider, which we call the state-learning contract, is the best of two simple contracts: optimal pooling and separating contracts.\nSeparating contract. Separating contracts allow the agent to perfectly infer the hidden state while executing the contract. These are incentive-compatible contracts that ask agents to collect n1, n2 samples under optimal errors \u03b81 < \u03b82 respectively. Additionally, n1, n2 are such that the agent can successfully infer the optimal error after collecting min(n1, n2) samples. The agent\u2019s response to this contract would be to first collect min(n1, n2) samples and decide whether to collect more depending on the inferred optimal error. The agent\u2019s successful inference of the optimal error makes computing optimal separating contracts similar to the contract design problem against a state-aware agent, which was solving the optimization problem Opt. The new optimization problem that yields optimal separating contracts has the same objective and constraints as Opt with the added constraint that n1, n2 > n0 for some n0 that we will describe soon. The additional constraint ensures that the agent knows the optimal error (with high probability) after collecting min(n1, n2) samples.\nTo determine the value of n0, we rely on assumptions about the agent\u2019s learning strategy. We assume that the agent can distinguish between \u03b81, \u03b82 with high probability using k/(\u2206\u03b8)2 samples. Here \u2206\u03b8 = \u03b82 \u2212 \u03b81 and k is a constant reflecting the degree of assumption made about the agent\u2019s efficiency. A lower value of k is a stronger assumption, assuming a more efficient agent. This assumption on the agent\u2019s learning strategy is more reasonable compared to assuming precise knowledge of the agent\u2019s learning strategy.\nWhen n0 is small enough that the added constraint n1, n2 > n0 is not active, the state-learning agent is behaving exactly as the state-aware agent, so our results from Section 4.1 apply. On the other hand, if n0 is large (which happens when \u2206\u03b8 is small) the additional constraint becomes too restrictive and the utility becomes low. In this case, another approach works well.\nPooling contract. In pooling contracts, the agent has no incentive to learn the optimal error. The pooling contract asks the agent to achieve one accuracy level a\u0304 regardless of the optimal error. The payment for this accuracy is set to ensure the agent can get nonnegative utility regardless of the optimal error. It is again straightforward to understand the agent\u2019s response to this contract. Suppose a\u0304 can be achieved by collecting n\u03041 < n\u03042 samples under optimal errors \u03b81 < \u03b82 respectively. To execute this contract, the agent starts collecting n\u03041 and sees if it achieves a\u0304 accuracy. If it does not, he collects n\u03042 \u2212 n\u03041 more samples since this action is guaranteed to yield nonnegative utility. Furthermore, collecting fewer or no additional samples results in less than a\u0304 expected accuracy and hence zero payment even though the agent exerted effort.\nA pooling contract does not let agents differentiate actions for different optimal errors and would be sub-optimal for this reason. However, when the difference in both problems is not significant i.e., \u2206\u03b8 is low, the benefit to the principal for distinguishing the agents is low. In summary, the separating contract has good utility when \u2206\u03b8 is large and the pooling contract has good utility when \u2206\u03b8 is small. By deploying the contract of the two with the higher utility, we can hope to have good utility for all values of \u2206\u03b8 \u2208 [0, 0.5].\nNumerical results. We compute the utility difference between the state-aware contract and the state-learning contract, varying problem parameters \u2206\u03b8 = \u03b82 \u2212 \u03b81 and k. We highlight a few observations (see Fig 2), that reflect the intuition we used to design the approach for state-learning contracts. \u2022 Figure 2a shows that the state-learning contract is pooling when \u2206\u03b8 is less than some threshold\nand is separating otherwise. For small and large values of \u2206\u03b8, the state-learning contract has utility close to the state-aware utility.\n\u2022 Figure 2b shows that when it is more difficult to distinguish between \u03b81, \u03b82, the pooling contract is better than the separating contract for more values of \u2206\u03b8. \u2022 Figure 2c shows that the worst-case sub-optimality over all \u2206\u03b8 values of the state-learning contract compared to the state-aware utility increases as k increases. When the agent can test more efficiently, the state-learning contract has greater utility for the principal."
        },
        {
            "heading": "5 Conclusions",
            "text": "Different parts of the learning pipeline are increasingly being delegated to autoML services and firms. Focusing on the delegation of data collection in such settings, we developed a principal-agent model capturing the practical challenges of hidden action and hidden state arising due to information asymmetry. We hope our work inspires future research exploring and addressing other incentivetheoretic obstacles that arise in this domain and in the delegation of other parts of the learning pipeline.\nWe identified the practicality of linear contracts under many problem parameters. We also obtained insights that hold for many other delegation settings, including the decreased utility and information rent the principal faces due to not knowing the hidden state. We also addressed a more realistic form of hidden state information asymmetry where the agent gradually learns the hidden state while executing the contract.\nOur results rely on a specific structure for the dependence of error rates on number of samples used for training. These rates are exact for some learning tasks (see Remark 2) but are generally upper bounds. From the principal\u2019s perspective, the contracts we designed continue to have good accuracy guarantees even if the error rates are just upper bounds and not exact. The agent\u2019s perspective is more complicated. Since the agent can learn more about the true error curve during learning and not rely on the upper bound, analyzing how the error curve learning occurs is needed to allow us to design truly incentive-compatible contracts in this more general setting. However, learning the error curve shape is learning from a much broader class and is likely to be more challenging. How this challenge impacts the utility of contracts would be an interesting direction for future work."
        },
        {
            "heading": "Acknowledgements",
            "text": "Funded in part by the European Union (ERC Synergy program). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council."
        },
        {
            "heading": "A Omitted proofs",
            "text": "A.1 Proof of Proposition 2\nProof. The linear contract c\u2217 that achieves this approximately optimal utility is the following:\nc\u2217 = max\n( 1\n\u03b2(p+ 1) p+1 p\n, \u03b1d\n1 p p \u00b7 ( p+ 1 1\u2212 \u03b8 ) p+1 p ) We first show that this contract satisfies the participation constraint for the agent. When the linear contract is c times the accuracy, the agent the number of samples n to maximize the agent\u2019s\nutility c ( 1\u2212 \u03b8 \u2212 dnp ) \u2212 \u03b1n. The number of samples the agent chooses as a function of c is ( cdp \u03b1 ) 1 p+1 . The contract c satisfies the participation constraint if the utility from choosing this number of samples is non-negative. This utility is:\nc ( 1\u2212 \u03b8 \u2212 d ( \u03b1\ncdp\n) p p+1 ) \u2212 \u03b1 ( cdp\n\u03b1\n) 1 p+1\n= c(1\u2212 \u03b8)\u2212 c 1 p+1\n( \u03b1d 1 p\np\n) p p+1\n\u2212 c 1 p+1 ( \u03b1d 1 p p p+1 ) p 1 p+1\n= c(1\u2212 \u03b8)\u2212 c 1 p+1 \u00b7 p+ 1 1\u2212 \u03b8 \u00b7\n( \u03b1d 1 p\np\n) p p+1\n.\nThis utility is non-negative when\nc \u2265 \u03b1d 1 p\np\n( p+ 1\n1\u2212 \u03b8\n) p+1 p\n.\nBy the definition of c\u2217, it is greater than the above quantity and so satisfies the participation constraint.\nWhen the principal chooses a linear contract c, it achieves a utility\n(1\u2212 \u03b2c) 1\u2212 \u03b8 \u2212(\u03b1d 1p pc ) p p+1  . We can provide an upper bound on the optimum utility using the optimum utility of the principal when there is no noise in the observed accuracy. In this case, the principal gets the agent to collect(\ndp \u03b1\u03b2\n) 1 p+1 and pays the agent \u03b1 times this amount. So the optimum utility is at most\n1\u2212 \u03b8 \u2212 (p+ 1)\n( \u03b1\u03b2d 1 p\np\n) p p+1\n.\nTo show that c\u2217 achieves the approximation guaranteed in the theorem, we consider two cases. The first case is when c\u2217 = 1\n\u03b2(p+1) p+1 p\n. In this case the utility of c\u2217 is\n( 1\u2212 1\n(p+ 1) p+1 p\n)1\u2212 \u03b8 \u2212 (p+ 1)(\u03b1\u03b2d 1p p ) p p+1 \nThe other case is when c\u2217 = \u03b1d 1 p\np ( p+1 1\u2212\u03b8 ) p+1 p . In this case,\n1\n\u03b2(p+ 1) p+1 p\n\u2264 \u03b1d 1 p\np\n( p+ 1\n1\u2212 \u03b8\n) p+1 p\n=\u21d2 1 p+ 1 \u2264 p+ 1 1\u2212 \u03b8 \u00b7\n( \u03b1\u03b2d 1 p\np\n) p p+1\n.\nThe ratio of the utility of c\u2217 to the optimum utility in this case is at least( 1\u2212 \u03b1\u03b2d 1 p\np ( p+1 1\u2212\u03b8 ) p+1 p ) p(1\u2212\u03b8) p+1\n1\u2212 \u03b8 \u2212 (p+ 1) ( \u03b1\u03b2d 1 p\np\n) p p+1\nFor t = p+11\u2212\u03b8 \u00b7 ( \u03b1\u03b2d 1 p p ) p p+1 ,\n=\np p+1\n( 1\u2212 t p+1 p ) 1\u2212 t .\nThis quantity is increasing in t. From the condition on c\u2217, we have 1p+1 \u2264 t. So we can obtain a lower bound on the above quantity by setting t = 1p+1 .\n\u2265 1\u2212 1\n(p+ 1) p+1 p\nA.2 Proof of Theorem 2\nProof. Recall that the first-best contract has a threshold form. The contract offers payment t\u2217 when the test error is less than or equal to \u2113\u2217 and offers payment zero otherwise. Let us denote the sample complexity to get expected loss at most \u2113 by n(\u2113). That is,\nn(\u2113) =\n( d\n\u2113\u2212 \u03b8\n)1/p .\nThe optimal contract offers t\u2217 = \u03b1n(\u2113\u2217) where \u03b1 is the cost per sample for the agent. Let us denote n(\u2113\u2217) by n\u2217. And n\u2217 = (pd/\u03b1\u03b2)1/(p+1).\nWe will show that the best response for the agent against this contract is never to collect samples less than n(\u2113+ \u03f5) when the test set has size O ( 1 \u03f52 log d\u03f5 ) . We show this by showing that the agent\u2019s utility in choosing n(\u2113+\u2206) is less than the agent\u2019s utility in collecting n(\u2113\u2212\u2206) for all \u2206 > \u03f5. The number of samples the agent would collect to get expected error l\u2217 +\u2206 is such that:\n\u03b8 + d\nnp1 = \u03b8 +\nd\nn\u2217p +\u2206\nn1 = n\u2217dp\n(d+\u2206)p .\nSimilarly, the number of samples needed to get expected error l\u2217 \u2212\u2206 is\nn2 = n\u2217dp\n(d\u2212\u2206)p .\nFor any action of the agent, the probability that the observed loss is \u03f5 or more away from the expected loss is less than 2 exp(\u22122m\u03f52). This is by applying Hoeffding\u2019s inequality on the observed loss random variable which is bounded between 0 and 1. As a result, for \u2206 > \u03f5, the expected payment when collecting n1 and n2 samples is \u2264 2t\u2217 exp(\u22122m\u03f52) and \u2265 t\u2217(1 \u2212 2 exp(\u22122m\u03f52)) respectively. The agent\u2019s utility due to n1 is less than the utility due to n2 when\n\u03b1n\u2217 ( 1\u2212 4 exp(\u22122m\u03f52) ) \u2265 \u03b1n\u2217dp\n( 1\n(d\u2212 2\u2206n\u22171/p)p \u2212 1 (d+ 2\u2206n\u22171/p)p\n)\nLet us denote \u03ba = dp (\n1 (d\u22122\u2206n\u22171/p)p \u2212 1 (d+2\u2206n\u22171/p)p\n) . So this occues when\nm \u2265 1 2\u03f52 log 4 1\u2212 \u03ba .\nNote that 11\u2212\u03ba is polynomial in both d and 1 \u03f5 ."
        },
        {
            "heading": "B Miscellaneous results and discussions",
            "text": "B.1 Treating error curves as upper bounds\nFor most of our results we have made use of the structured form of error curves reflecting how expected error of a learned model is assumed to vary with the number of samples used for training. This structure is inspired by statistical minimax bounds and are upper bounds rather the true error curves. We designed contracts assuming the bounds to be actual error curves. Here we discuss what we can say about these contracts without assuming the bounds to be exact error curves.\nFrom the principal\u2019s perspective, these contracts result in accuracy that is just as good as that of learned models. However, the principal would end up paying the agent more than it could have if the principal knew the exact error curve. We can view the shape of the true error curve as another piece of information the principal is unaware of in addition to the optimal error. This hidden information results in more information rent but does not impact the accuracy of the model obtained from delegation.\nThe agent\u2019s perspective of what changes is more complicated. Our contracts assumed that the agent responded assuming that the upper bound was the true error curve. It may be reasonable that before starting the delegation process, the agent believes the upper bounds to be the true curves having no other frame of reference. However after starting to collect data, it is possible that the agent will learn more about the form of the true error curve and respond differently. This is similar to how the agent can learn the optimal error while executing the contract. Analyzing how this error curve learning occurs will allow us to design truly incentive-compatible contracts. However, learning the error curve shape is learning from a much broader class and is likely to be more challenging.\nB.2 Variable label quality model\nThe setting above models the scenario where the agent does not have the option to choose the quality of the data is collects. However, the agent might be able to control the quality of the data as a\nfunction of the cost per sample. We study a model of quality of data where the quality corresponds to the quality of the labels of the data. The quality parameter q = 1\u2212 2\u03b7 captures the likelihood of the labels being correct. Here, \u03b7 \u2208 (0, 1/2) is the probability of the label being incorrect. In this setting, the expected accuracy on the principal\u2019s test set from the agent collecting n samples at quality level q when the optimal error is \u03b8 is 1\u2212 \u03b8\u2212 1qnp for some p > 0. We assume that the cost of collecting a single sample at quality level q for the agent is given by \u03b1(q) a function increasing in q and convex. So the cost for the agent of collecting n samples at quality level q is C(n, q) = \u03b1(q)n.\nIn this section, we provide results for \u03b1(q) = qb + \u03b10 for b > 0. We can think of \u03b1 as the cost of collecting an unlabelled sample and qb as the cost of labelling an unlabelled point. The main message of this section is that even though the quality of labels is an action that the agent chooses, effectively, this choice is not information that is private from the principal. It turns out that whatever the contract is, the utility-maximizing agent executes the contract by choosing a single quality value q\u2217. The principal can also compute q\u2217 so the quality is not a reflection of information asymmetry. Therefore, this regime is essentially the same as the one studied in the previous section.\nTheorem 3 (Constant quality level). For any problem parameters d, p, \u03b10 > 0, b > 1, when \u03b1(q) = qb +\u03b10, for any expected accuracy a the agent wishes to achieve, the agent chooses a constant q\u2217 that only depends on \u03b10, b as the quality level.\nProof. If the agent aims to achieve an expected accuracy of at least s, then the agent chooses the number of samples and quality level by solving the following optimization problem:\nmin q,n\n( qb + \u03b10 ) n\ns.t. \u03b8 + 1\nqnp \u2264 1\u2212 a\n0 \u2264 q \u2264 1.\nThe solution of this optimization problem can be calculated as follows:\nL = ( qb + \u03b1 ) n+ \u03bb1 ( \u03b8 \u2212 l + 1\nqnp\n) + (\u03bb2 \u2212 \u03bb3)q\n\u2207nL = qb + \u03b1\u2212 p\u03bb1 bnp+1 \u2207qL = bqb\u22121n\u2212 \u03bb1 q2np + \u03bb2 \u2212 \u03bb3.\nIf \u03bb\u22172 = \u03bb\u22173 = 0, we obtain\n=\u21d2 \u03bb\u22171 = bq\u2217 b+1np+1 =\u21d2 q\u2217 = ( \u03b1\nb\u2212 1\n) 1 b\n,\nand n\u2217 is obtained by solving\n\u03b8 + 1\nq\u2217n\u2217p = l.\nIf (\u03b1/(b\u2212 1))1/b is not in [0, 1], then \u03bb\u22172 or \u03bb\u22173 is non-zero and q\u2217 is either 0 or 1.\nB.3 Closed-form solution for the two optimal error, hidden state problem\nHere we solve the state-aware optimization problem Opt when there are two optimal errors, \u03b8 \u2264 \u03b8, with a prior probability of \u03bd for \u03b8. Let L(n) = d/np. We solve the following optimization problem and show that the solution is the optimal solution we are looking for. Note that this problem omits the incentive-compatibility constraint for the problem \u03b8 and the participation constraint for the easy problem.\nmin n,n,t,t\n\u03bd(L(n) + \u03b2t) + (1\u2212 \u03bd)(L(n) + \u03b2t)\ns.t. t\u2212 \u03b1n (1 + \u2206np)1/p \u2212 t+ \u03b1n \u2264 0\n\u03b1n\u2212 t \u2264 0. First note that this is a convex optimization problem, where the objective is convex by the convexity of the loss and the PC constraint is a linear constraint. All that is left is to check that the IC constraint is convex. This is the sum of linear terms and the term \u2212\u03b1d\n1/pn (d+\u2206np)2 . The second\nderivative of this term is 3\u03b1d 1/p\u2206\n2np(d+\u2206np)4 . Since the second derivative is positive, the IC constraint is\nconvex. Consider the Lagrangian\nL(n, n, t, t;\u03bb1, \u03bb2) = \u03bd(L(n)+\u03b2t)+(1\u2212\u03bd)(L(n)+\u03b2t)+\u03bb1 ( t\u2212 \u03b1d 1/pn\n(d+\u2206np)1/p \u2212 t+ \u03b1n\n) +\u03bb2 ( \u03b1n\u2212 t ) ,\nwhich has the following gradients:\n\u2207nL = \u03bdL\u2032(n) + \u03b1\u03bb1 (G1) \u2207tL = \u03bd\u03b2 \u2212 \u03bb1 (G2) \u2207tL = (1\u2212 \u03bd)\u03b2 \u2212 \u03bb2 + \u03bb1 (G3)\n\u2207nL = (1\u2212 \u03bd)L\u2032(n) + \u03b1\u03bb2 \u2212 \u03bb1\n( \u03b1d1/p\n(d+\u2206np)(p+1)/p\n) (G4)\nTo choose values n\u2217, n\u2217, t\u2217, t\u2217, \u03bb\u22171, \u03bb\u22172 that satisfy the KKT conditions, first we set the gradients to zero:\nL\u2032(n\u2217) = \u2212\u03b1\u03b2 (From (G1), (G2)) \u03bb\u22171 = \u03bd\u03b2 (From (G2) \u03bb\u22172 = \u03b2 (From (G3) and value of \u03bb \u2217 1)\n(1\u2212 \u03bd)L\u2032(n\u2217) + \u03b1\u03b2 \u2212 \u03bd\u03b1\u03b2 (1 + \u2206np)(p+1)/p = 0 (From (G4) and values of \u03bb\u22171, \u03bb \u2217 2)\n=\u21d2 L\u2032(n\u2217) = \u2212 \u03b1\u03b2 1\u2212 \u03bd\n( 1\u2212 \u03bdd 1/p\n(d+\u2206n\u2217p)(p+1)/p\n)\nBy complementary slackness,\n\u03b1n\u2217 = t \u2217\nt\u2217 = \u03b1 ( n\u2217 \u2212 d 1/pn\u2217\n(d+ n\u2217p)1/p + n\u2217\n) .\nThe contract described by (n\u2217, n\u2217, t\u2217, t\u2217) satisfies the properties of the second-best contract in the classical contract theory setting. We list these properties here:\nP1 No output distortion for the easy problem: n\u2217 is the solution of L\u2032(n\u2217) = \u2212\u03b1\u03b2 which is also the value of nfb. So for the easy problem, the agent gathers the same number of samples as in the full information case.\nP2 Downward distortion for the hard problem:\nL\u2032(n\u2217) = \u2212 \u03b1\u03b2 1\u2212 \u03bd\n( 1\u2212 \u03bdd 1/p\n(d+\u2206n\u2217p)(p+1)/p ) < \u2212\u03b1\u03b2 = L\u2032(nfb).\nSo n\u2217 < nfb. For the hard problem, the agent gathers fewer samples than in the full information case.\nP3 When the problem is easy, the agent gets positive information rent:\nt\u2217 \u2212 \u03b1n\u2217 = n\u2217 \u2212 d 1/pn\u2217\n(d+\u2206n\u2217p)(p+1)/p\n> 0.\nWe now check that the contract that is the solution to the above optimization problem also satisfies the omitted constraints. First we start with the participation constraint for the easy problem. By the positive information rent property (P3) we know that \u03b1n\u2217 < t\u2217. Next consider the incentive-compatibility constraint for the hard problem. We only need to check when \u2206n\u2217p < d. Otherwise, the IC constraint automatically holds. The difference in agent\u2019s utility between choosing the \u2113\u2217 = \u03b8 + d/n\u2217p option and the \u2113\u2217 = \u03b8 + d/n\u2217p option is:\n\u2212 \u03b1n\u2217 + t+ \u03b1d 1/pn\u2217\n(d\u2212\u2206n\u2217p)p \u2212 t\u2217\n= \u2212 \u03b1d 1/pn\u2217\n(d+\u2206n\u2217p)1/p + \u03b1n\u2217 +\n\u03b1d1/pn\u2217\n(d\u2212\u2206n\u2217p)1/p \u2212 \u03b1n\u2217 (Using values of t\u2217, t\u2217)\n= \u03b1\n( d1/pn\u2217\n(d\u2212\u2206n\u2217p)1/p \u2212 n\u2217 + d\n1/pn\u2217\n(d+\u2206n\u2217p)1/p \u2212 n\u2217\n)\n\u2265 \u03b1n\u2217 (\n1\n(1\u2212\u2206n\u2217p)1/p +\n1 (1 + \u2206n\u2217p)1/p \u2212 2 )\n(Since n\u2217 < n\u2217)\nNote that the function d 1/p (d\u2212x)q + d1/p (d+x)q is increasing in the interval [0, 1) for every q. The derivative of that function is q ( d1/p\n(d\u2212x)q+1 \u2212 d1/p (d+x)q+1\n) . This is nonnegative and lies in [0, 1).\n\u2265 0 (Since we assume 0 < \u2206np\u2217 < 1).\nThis solution finds the optimal contract under hidden state.\nB.3.1 Separating contracts\nTo be able to compute any separating contract, it suffices to solve the above optimization problem with the additional constraint n, n \u2265 n0 for some n0 \u2265 0. The new optimizers n(n0), n(n0), t(n0).t(n0) are as follows:\nn(n0) = max(n \u2217, n0) n(n0) = max(n \u2217, n0)\nt(n0) = \u03b1n(n0)\nt(n0) = \u03b1 ( n(n0)\u2212 d1/pn(n0)\n(d+ n(n0))1/p + n(n0)\n) .\nB.3.2 Optimal pooling contract\nThe optimal pooling contract optimizes over n. t = \u03b1n. n is chosen such that\n\u03b8 + d\nnp = \u03b8 +\nd\nnp\n=\u21d2 n = d 1/pn\n(d+\u2206np)1/p .\nThus the optimization problem is choosing n to be the minima of\n\u03bd d(\nd1/pn (d+\u2206np)1/p )p + (1\u2212 \u03bd) dnp + \u03b1\u03b2n. B.4 Tightness of linear contracts approximation\nOur main result (Theorem 1) gave a linear contract that provably approximates the optimal contract up to a constant factor. This approximation factor stated in Proposition 2 is also tight as stated in the following theorem, which shows that there is a problem instance for which no linear contract can do better than a given approximation factor. The problem instance for which the approximation ratio is tight is one that has deterministic test error distribution, which arises when the size of the test set tends to infinity.\nTheorem 4 (Tightness of approximation bound). For every \u03b8 \u2208 [0, 1), p, d > 0, there are problem parameters \u03b1, \u03b2 > 0 such that for the problem instance with these parameters, all linear contracts have at most 1\u2212 1\n(p+1) p+1 p\ntimes the optimal utility.\nProof. We will show that there exist \u03b1, \u03b2 such that the contract 1 \u03b2(p+1) p+1 p is the optimal contract chosen by the principal. This contract will also satisfy the participation constraint for our chosen values of \u03b1, \u03b2. Recall that the participation constraint is\n1\n\u03b2(p+ 1) p+1 p\n\u2265 \u03b1d 1 p p \u00b7 ( p+ 1 1\u2212 \u03b8 ) p+1 p\n\u2261 1\u2212 \u03b8 \u2265\n( \u03b1\u03b2d 1 p\np\n) p p+1\n(p+ 1)2.\nThe principal chooses the contract that sets the derivative of the above quantity to zero as long as that contract satisfies the participation constraint. If setting 1\n\u03b2(p+1) p+1 p\nyields a zero derivative\nand it satisfies the participation constraint, then it is the optimal linear contract. The derivative relative to c is\n(1\u2212 \u03b2c)\n( \u03b1d 1 p\np\n) p p+1\n\u00b7 p p+ 1 \u00b7 1\nc 2p+1 p+1\n\u2212 \u03b2 1\u2212 \u03b8 \u2212(\u03b1d 1p p ) p p+1  . Setting c = 1\n\u03b2(p+1) p+1 p\n, the derivative is\n( 1\u2212 1\n(p+ 1) p+1 p\n) p\np+ 1\n( \u03b1d 1 p\np\n) p p+1\n\u03b2 2p+1 p+1 (p+ 1) 2p+1 p\n\u2212 \u03b2 1\u2212 \u03b8 \u2212 (p+ 1)(\u03b1\u03b2d 1p p ) p p+1  . We can choose \u03b1, \u03b2 to set this derivative to zero by choosing \u03b1, \u03b2 satisfying:\n1\u2212 \u03b8 =\n( \u03b1\u03b2d 1 p\np\n) p p+1\n(p+ 1) ( 1 + ( 1\u2212 1\n(p+ 1) p+1 p\n) p(p+ 1) 1 p ) .\nNote that for every p > 0, ( 1 + ( 1\u2212 1\n(p+1) p+1 p\n) p(p+ 1) 1 p ) > p+ 1. So,\n\u2265\n( \u03b1\u03b2d 1 p\np\n) p p+1\n(p+ 1)2.\nThis shows that there are problem parameters that make c\u2217 = 1 \u03b2(p+1) p+1 p the optimal linear contract. In the proof of Proposition 2, we showed that this linear contract achieves at least 1 \u2212 1 (p+1) p+1 p times the optimum utility. When the problem involves a deterministic mapping between the number of samples and the observed accuracy, this ratio is exact."
        }
    ],
    "title": "Delegating Data Collection in Decentralized Machine Learning",
    "year": 2023
}