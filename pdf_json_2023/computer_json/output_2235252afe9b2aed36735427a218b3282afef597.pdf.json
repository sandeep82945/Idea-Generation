{
    "abstractText": "We present a new data structure for maintaining dynamic permutations, which we call a forest of splay trees (FST). The FST allows one to efficiently maintain the cycle structure of a permutation \u03c0 when the allowed updates are transpositions. The structure stores one conceptual splay tree for each cycle of \u03c0, using the position within the cycle as the key. Updating \u03c0 to \u03c4 \u00b7 \u03c0, for a transposition \u03c4 , takes O(log n) amortized time, where n is the size of \u03c0. The FST computes any \u03c0(i), \u03c0\u22121(i), \u03c0(i) and \u03c0\u2212k(i), in O(log n) amortized time. Further, it supports cycle-specific queries such as determining whether two elements belong to the same cycle, flip a segment of a cycle, and others, again within O(log n) amortized time. 2012 ACM Subject Classification Theory of computation \u2192 Design and analysis of algorithms \u2192 Data structures design and analysis",
    "authors": [
        {
            "affiliations": [],
            "name": "Zsuzsanna Lipt\u00e1k"
        },
        {
            "affiliations": [],
            "name": "Francesco Masillo"
        },
        {
            "affiliations": [],
            "name": "Gonzalo Navarro"
        }
    ],
    "id": "SP:2df1e2970fc2657ff7b4c82b057a0088c3c0b975",
    "references": [
        {
            "authors": [
                "J. Barbay",
                "F. Claude",
                "T. Gagie",
                "G. Navarro",
                "Y. Nekrich"
            ],
            "title": "Efficient fully-compressed sequence",
            "venue": "representations. Algorithmica,",
            "year": 2014
        },
        {
            "authors": [
                "J. Barbay",
                "G. Navarro"
            ],
            "title": "On compressing permutations and adaptive sorting",
            "venue": "Theoretical Computer Science,",
            "year": 2013
        },
        {
            "authors": [
                "Mikl\u00f3s B\u00f3na"
            ],
            "title": "Combinatorics of Permutations, Second Edition",
            "venue": "Discrete mathematics and its applications. CRC Press,",
            "year": 2012
        },
        {
            "authors": [
                "M. Burrows",
                "D.J. Wheeler"
            ],
            "title": "A block-sorting lossless data compression algorithm",
            "venue": "Technical report,",
            "year": 1994
        },
        {
            "authors": [
                "P. Ferragina",
                "G. Manzini"
            ],
            "title": "Indexing compressed text",
            "venue": "Journal of the ACM,",
            "year": 2005
        },
        {
            "authors": [
                "Guillaume Fertin",
                "Anthony Labarre",
                "Irena Rusu",
                "Eric Tannier",
                "St\u00e9phane Vialette"
            ],
            "title": "Combinatorics of Genome Rearrangements. Computational molecular biology",
            "year": 2009
        },
        {
            "authors": [
                "J. Fischer",
                "V. M\u00e4kinen",
                "G. Navarro"
            ],
            "title": "Faster entropy-bounded compressed suffix trees",
            "venue": "Theoretical Computer Science,",
            "year": 2009
        },
        {
            "authors": [
                "T. Gagie",
                "G. Navarro",
                "N. Prezza"
            ],
            "title": "Fully-functional suffix trees and optimal text searching in BWT-runs bounded space",
            "venue": "Journal of the ACM,",
            "year": 2020
        },
        {
            "authors": [
                "Sara Giuliani",
                "Zsuzsanna Lipt\u00e1k",
                "Francesco Masillo",
                "Romeo Rizzi"
            ],
            "title": "When a dollar makes a BWT",
            "venue": "Theor. Comput. Sci.,",
            "year": 2021
        },
        {
            "authors": [
                "Roberto Grossi",
                "Jeffrey Scott Vitter"
            ],
            "title": "Compressed suffix arrays and suffix trees with applications to text indexing and string matching",
            "venue": "SIAM J. Comput.,",
            "year": 2005
        },
        {
            "authors": [
                "D. Gusfield"
            ],
            "title": "Algorithms on Strings, Trees and Sequences: Computer Science and Computational Biology",
            "year": 1997
        },
        {
            "authors": [
                "Sebastian Kreft",
                "Gonzalo Navarro"
            ],
            "title": "On compressing and indexing repetitive sequences",
            "venue": "Theor. Comput. Sci.,",
            "year": 2013
        },
        {
            "authors": [
                "Veli M\u00e4kinen",
                "Gonzalo Navarro"
            ],
            "title": "Succinct suffix arrays based on run-length encoding",
            "venue": "Nord. J. Comput.,",
            "year": 2005
        },
        {
            "authors": [
                "Udi Manber",
                "Eugene W. Myers"
            ],
            "title": "Suffix arrays: A new method for on-line string searches",
            "venue": "SIAM J. Comput.,",
            "year": 1993
        },
        {
            "authors": [
                "J.I. Munro",
                "R. Raman",
                "V. Raman",
                "S.S. Rao"
            ],
            "title": "Succinct representations of permutations and functions",
            "venue": "Theoretical Computer Science,",
            "year": 2012
        },
        {
            "authors": [
                "J. Ian Munro",
                "Yakov Nekrich"
            ],
            "title": "Compressed data structures for dynamic sequences",
            "venue": "In Proc. of the 23rd Annual European Symposium (ESA2015),",
            "year": 2015
        },
        {
            "authors": [
                "Gonzalo Navarro"
            ],
            "title": "Compact Data Structures - A Practical Approach",
            "year": 2016
        },
        {
            "authors": [
                "Jo\u00e3o Setubal",
                "Jo\u00e3o Meidanis"
            ],
            "title": "Introduction to Computational Molecular Biology",
            "venue": "PWS Publishing Company,",
            "year": 1997
        },
        {
            "authors": [
                "Daniel Dominic Sleator",
                "Robert Endre Tarjan"
            ],
            "title": "Self-adjusting binary search trees",
            "venue": "J. ACM,",
            "year": 1985
        }
    ],
    "sections": [
        {
            "text": "2012 ACM Subject Classification Theory of computation \u2192 Design and analysis of algorithms \u2192 Data structures design and analysis\nKeywords and phrases permutations, cycle structure, binary search trees, splay trees, dynamic data structures\nar X\niv :2\n30 6.\n04 47\n0v 1\n[ cs\n.D S]\n7 J\nun 2\n02 3\n1 Introduction\nPermutations play a central role in many applications in mathematics and computer science, spanning from combinatorics and group theory to algorithms for random generation and computational biology (most notably, genome rearrangements). Permutations are also of fundamental importance in text indexing: a text index is a dedicated data structure that allows fast execution of pattern matching queries and other text processing tasks. All current compressed and non-compressed text indexes, such as suffix arrays (SAs) [15], compressed suffix arrays (CSAs) [11], compressed suffix trees (CSTs) [9, 8], the FM-index [6], the RLFMindex [14], the r-index [9], and the LZ-index [13], to name a few, have at their core some permutation (see, e.g., [18, Ch. 11 & Section 13.3]). The best known of these, the SA, is itself a permutation of n; other permutations used by text indexes include those known as \u03a8, LF (a.k.a. standard permutation), or \u03d5. Another prominent example where permutations are central is genome rearrangements in computational biology [19, 7], which consists of sorting a permutation: Given a set of legal operations and a permutation \u03c0, find a shortest sequence of operations that transforms \u03c0 into the identity.\nPermutations, in most situations, are simply stored as an array of integers, allowing constant-time access to each entry \u03c0(i) of the permutation \u03c0. Sometimes one needs to take the inverse or powers of the permutation, which can be done efficiently with little additional space [16, 3, 2]. Other queries of interest, such as determining if two elements belong to the same cycle, may also be supported efficiently. There is not much support, instead, to let \u03c0 change between queries. In this case one needs a dynamic data structure that can be efficiently updated when some operation is applied to the permutation to convert it into another permutation \u03c0\u2032.\nA simple workaround is to regard the permutation as a sequence and use a representation for dynamic sequences [17]. Such a representation can support some simple queries on \u03c0, but in some applications (e.g., [10]), the cycle structure of the permutation needs to be kept track of along updates, including the number and cardinality of the cycles, or deciding whether two elements are in the same cycle.\nIn this paper we fill this gap, presenting a data structure tailored for maintaining dynamic permutations called FST (forest of splay trees), with special focus on their cycle structure. The FST supports arbitrary transpositions and flips in the cycles, while answering the described queries and some others, all in O(log n) amortized time. Indeed, the FST is the only structure to date able to solve both those queries and updates within o(n) time."
        },
        {
            "heading": "1.1 Related work",
            "text": "Given a permutation \u03c0 on n, O(1/\u03f5)-time support for accessing any \u03c0(i), its inverse \u03c0\u22121(j), and in general positive and negative powers of \u03c0 (i.e., \u03c0k(i) or \u03c0\u2212k(j) for any integer k > 0), can be obtained with a representation using (1 + \u03f5)n log n bits of space, for any \u03f5 > 0 [16].1 This is, in principle, an array storing the consecutive values not of \u03c0, but of its sequence of cycles seen as another permutation \u03c1. For example, if the cycle decomposition of \u03c0 is (1, 3)(2, 6, 5)(4), then \u03c1 = 1, 3, 2, 6, 5, 4. They provide O(1)-time access to \u03c1(i) and O(1/\u03f5)-time access to \u03c1\u22121(j). They also store a bitvector of length n marking with 1s the places where cycles end in \u03c1, with support to find, in constant time, the 1s that precede and follow any given position. In our example, this bitvector is 0, 1, 0, 0, 1, 1. This structure\n1 By default, our logarithms are in base 2.\nallows them to compute any positive or negative power of \u03c0 (including \u03c0 and \u03c0\u22121) in time O(1/\u03f5). Other representations aim to store some classes of permutations in compressed form while supporting the same functionality [3, 2].\nNone of those constructions support updates to \u03c0, however. A simple way to support updates and some queries is to regard the one-line representation of \u03c0 as a string P of length n over the alphabet {1, . . . , n}, where each symbol appears exactly once. A sequence representation of P that supports accessing any P [i] and finding the first occurrence of any value j in P (called selectj(P, 1) in the literature) simulates the queries \u03c0(i) = P [i] and \u03c0\u22121(j) = selectj(P, 1). Dynamic sequence representations [17] applied on P then support both operations in time O( log nlog log n ) and use n log n + o(n lg n) bits. Within the same time, they support insertions and deletions of symbols in P , which can be used to reflect the updates we wish to perform on \u03c0. An insertion is an operation that, given as input a sequence S of length n, a position i, and a character c, gives as output a new sequence S\u2032 where S\u2032[1, i\u22121] = S[1, i\u22121], S\u2032[i] = c, and S\u2032[i+1, n+1] = S[i, n]. A deletion is an operation that, given as input a sequence S of length n and a position i, returns as output a new sequence S\u2032 where S\u2032[1, i \u2212 1] = S[1, i \u2212 1] and S\u2032[i, n \u2212 1] = S[i + 1, n]. Other more sophisticated queries are supported only naively, however, for example arbitrary powers \u03c0k and \u03c0\u2212k take time O(k \u00b7 log nlog log n ). In general, such a representation does not efficiently support important queries and update operations regarding the cycle structure of \u03c0."
        },
        {
            "heading": "1.2 Our contribution",
            "text": "We introduce a new data structure, called forest of splay trees (FST), which is to the best of our knowledge the most efficient one to maintain dynamic permutations when the focus of interest is the permutation\u2019s cycle structure. The FST supports the following operations, which significantly enhance the data structure presented by Giuliani et al. [10].\nAccess: compute \u03c0(i) and \u03c0\u22121(j) for any i and j. Powers: compute \u03c0k(i) and \u03c0\u2212k(j) for any i, j, and integer k. Number of cycles: give the number of cycles in \u03c0. Cycle size: give the number of elements in the cycle of i, for any i. Same cycle: determine if i and j are in the same cycle, for any i and j. Distance: return d such that \u03c0d(i) = j, if such a d exists, \u221e otherwise, for any i and j. Transpose: exchange the values i and j in the one-line representation of \u03c0 (\u201ctranspose i and j\u201d), or exchange the values at positions i and j (\u201ctranspose \u03c0(i) and \u03c0(j)\u201d). Flip: reverse a segment of the cycle to which both values i and j belong.\nThe following theorem summarizes our contribution.\n\u25b6 Theorem 1. Let \u03c0 be a permutation on n. The FST is built from \u03c0 in O(n) time and uses 3n log n +O(n) bits of space. Once built, it supports each of the queries and updates above in O(log n) amortized time.\nThe original data structure, which we now generalize with the FST, supports only a restricted class of transpositions on permutations with specific properties. It already proved useful on an application using the Burrows-Wheeler Transform [5], where it allowed replacing an O(n2) algorithm with an O(n log n) time one [10].\nThe rest of the paper is organized as follows: In Section 2, we give the necessary definitions. In Section 3, we present the FST data structure and the operations. In Section 4, we give a comparison of the FST to existing data structures for permutations. We close with an outlook in Section 5.\n2 Basics"
        },
        {
            "heading": "2.1 Permutations",
            "text": "Given a positive integer n, a permutation of n is a bijection from the set {1, 2, . . . , n} to itself. Two common ways to represent a permutation \u03c0 are the two-line notation ( 1 2 ... n \u03c0(1) \u03c0(2) ... \u03c0(n)\n) and the one-line notation, \u03c0(1)\u03c0(2) \u00b7 \u00b7 \u00b7\u03c0(n), where the top row is omitted. The set of permutations of n forms a group together with the composition (or product) of functions, and is called the symmetric group, denoted Sn. We write \u03c4 \u00b7 \u03c0 for the permutation resulting from applying \u03c4 after \u03c0, that is, (\u03c4 \u00b7 \u03c0)(i) = \u03c4(\u03c0(i)). A cycle of \u03c0 is a minimal subset C of {1, 2, . . . , n} such that \u03c0(C) = C. A permutation consisting of only one cycle is said to be cyclic, a cycle of length 1 is a fixpoint, and a cycle of length 2 is called a transposition. Every permutation can be uniquely decomposed into disjoint cycles (cycle decomposition), and can thus be represented as a composition of its cycles. For example, the cycle representation of the permutation \u03c0 = ( 1 2 3 4 5 63 6 1 4 2 5 ) is (1, 3)(2, 6, 5)(4): of the three cycles, (1, 3) is a transposition, (2, 6, 5) is a cycle of length 3, and (4) is a fixpoint. A cycle can be written starting from any one of its elements, for example (2, 6, 5) = (6, 5, 2) = (5, 2, 6). Further, permutations in general do not commute, but disjoint cycles do. Thus, the cycle representation is unique only up to order of the cycles and up to cycle rotations, for example the above permutation could also be written as \u03c0 = (3, 1)(4)(6, 5, 2). It is common to drop fixpoints from the cycle representation, thus an n-permutation \u03c4 with n\u2212 2 fixpoints and one transposition (i, j) is written simply as \u03c4 = (i, j).\nFor more on permutations, see the book by B\u00f3na [4], or any textbook on discrete mathematics, such as that of Aigner [1].\nLet \u03c0 be a permutation of n and \u03c4 a transposition. It is a well-known fact that the number of cycles of \u03c0\u2032 = \u03c4 \u00b7 \u03c0 increases by 1 if the two elements are in the same cycle, and decreases by 1 if they are in different cycles. In particular, the exact form of the resulting permutation is given by the following lemma. For convenience, the transposition is given in the form (\u03c0(x), \u03c0(y)), for some x and y. Note that an element is always in the same cycle as its image, so x and y are in the same cycle if and only if \u03c0(x) and \u03c0(y) are in the same cycle.\n\u25b6 Lemma 2 ([10], Lemma 3). Let \u03c0 = C1 \u00b7 \u00b7 \u00b7Ck be the cycle decomposition of the permutation \u03c0, x \u0338= y, and \u03c0\u2032 = (\u03c0(x), \u03c0(y)) \u00b7 \u03c0. 1. (Split case) If x and y are in the same cycle Ci, then this cycle is split into two.\nIn particular, let Ci = (c1, c2, . . . , cj , . . . , cm), with cm = x and cj = y. Then \u03c0\u2032 = (c1, c2, . . . , cj\u22121, y) (cj+1, . . . , cm\u22121, x) \u220f \u2113 \u0338=i C\u2113. 2. (Join case) If x and y are in different cycles Ci and Cj , then these two cycles are merged. In particular, let Ci = (c1, c2, . . . , cm), with cm = x, and Cj = (c\u20321, c\u20322, . . . , c\u2032r), with c\u2032r = y, then \u03c0\u2032 = (c1, . . . , cm\u22121, x, c\u20321, . . . , c\u2032r\u22121, y) \u220f \u2113 \u0338=i,j C\u2113.\n\u25b6 Example 3. Let \u03c0 = ( 1 2 3 4 5 6 7 8 98 2 5 3 1 7 9 4 6 ) = (1, 8, 4, 3, 5)(2)(6, 7, 9), then (\u03c0(1), \u03c0(4)) \u00b7 \u03c0 = ( 1 2 3 4 5 6 7 8 93 2 5 8 1 7 9 4 6 ) = (1, 3, 5)(2)(4, 8)(6, 7, 9) [Split case] (\u03c0(3), \u03c0(6)) \u00b7 \u03c0 = ( 1 2 3 4 5 6 7 8 93 2 7 8 1 5 9 4 6 ) = (1, 3, 7, 9, 6, 5)(2)(4, 8) [Join case]"
        },
        {
            "heading": "2.2 Splay Trees",
            "text": "Splay trees [20] are binary search trees that allow joining and splitting of trees in addition to the usual operations (such as access to, or insertion and deletion of, items). They are not necessarily balanced, but they undergo a self-adjusting procedure after each operation that guarantees that operations take amortized logarithmic time in the total number of items. In\nparticular, after each operation involving an element x, this element x is moved to the root of its tree via a structural rearrangement called splaying. Splaying consists of a series of edge rotations applied repeatedly on x until it becomes the root of the tree. The rotations can be of one of three types, referred to as zig, zig-zig, and zig-zag, depending on the relative position of x w.r.t. its parent and grandparent. For more details, see the Appendix and the original paper.\nConcretely, splay trees support the following operations:\naccess(i, t): return a pointer to item i if it is in tree t, otherwise return NIL insert(i, t): insert item i into t (assuming it is not present) delete(i, t): delete item i from t (assuming it is present) join(t1, t2): construct tree t containing all items in t1 and t2 (assuming that all items in t1 are strictly smaller than those in t2) split(i, t): return two new trees t1 and t2, where t1 contains all items in t less than or equal to i, and t2 contains all items in t greater than i\nThe next theorem from the original paper gives the complexity of the operations.\n\u25b6 Theorem 4 (Balance Theorem with Updates [20, Thm. 6]). A sequence of m arbitrary operations on a collection of initially empty splay trees takes O(m + \u2211m j=1 log nj) time, where nj is the number of items in the tree or trees involved in operation j.\n3 Forest of splay trees (FST)\nOur data structure, FST, stores a forest of splay trees partitioning the set {1, . . . , n}. The FST allows constant-time access to each node in addition to the usual operations on splay trees. The permutation \u03c0 is represented by one splay tree for each of its cycles.\nThe elements are keyed by their position in their cycle, that is, the splay tree is a binary search tree with respect to the positions in the cycle (and not w.r.t. the elements themselves); see Figure 1 for an example. In particular, an in-order traversal of one of the splay trees yields the corresponding cycle. The fact that the linearization of a cycle is not unique (i.e., that all rotations represent the same cycle) will be important later.\nThe concrete FST data structure consists of a counter cycles and a 3\u00d7 n matrix M ; for reasons of presentation we give a variant of the matrix with 4 rows and show later how to reduce this to only 3 rows. The counter cycles contains the number of cycles of \u03c0, while, for 1 \u2264 i \u2264 n, M1i is the parent of i (NIL if i is the root of its tree), M2i is the left child and M3i the right child of i (NIL if no left, resp. right, child is present), and M4i is the size of the subtree rooted in i. See Figure 1 again for an example.\nWe can reduce the matrix to only 3 rows by applying a standard trick on binary trees. We substitute the first three rows with only two rows containing the left child and right sibling information. If the right sibling does not exist, then the entry points to the parent. Thus, one can access the right child by accessing the left child and then its right sibling (2 steps); and the parent by accessing the right sibling: if there is none, then we get the parent immediately, otherwise we access the right sibling of the node pointed to. In both cases, we have replaced one step by one or two steps, thus, navigation time remains within the same bounds. The space required by the FST is then 3n\u2308log n\u2309 = 3n log n +O(n) bits."
        },
        {
            "heading": "3.1 Construction and splay tree operations",
            "text": "Given an input permutation \u03c0, we compute the cycle decomposition of \u03c0 and assign to c the number of cycles. We then build a splay tree for each cycle. Instead of building it\n\u03c0 = (\n1 2 3 4 5 6 7 8 9 10 11 1 3 6 4 2 8 11 5 10 7 9\n) \u03c0 = ( 1 1)( 1 2, 2 3, 3 6, 4 8, 5 5)( 1 4)( 1 7, 2 11, 3 9, 4 10)\nby inserting all the cycle elements in an initially empty splay tree, which would lead to O(n log n) construction time, we apply a standard linear-time construction that builds a perfectly balanced binary search tree for each cycle. We now show that the potential function of the splay trees created with that shape is also O(n), which allows us combine this O(n) construction time with all the other amortized operation times.\n\u25b6 Lemma 5. The potential function of a perfectly balanced splay tree with r nodes is 2r +O(log2 r) \u2286 O(r).\nProof. Let d be the depth of the deepest leaves in a perfectly balanced binary tree, and call \u2113 = d \u2212 d\u2032 + 1 the level of any node of depth d\u2032. It is easy to see that there are at most 1 + r/2\u2113 subtrees of level \u2113. Those subtrees have at most 2\u2113\u2212 1 nodes. The potential function used in the analysis of splay trees [20] is \u03d5 = \u2211 v log s(v), where s(v) is the number of nodes in the subtree rooted at v and the sum ranges over all the nodes of the splay tree. Separating this sum by levels \u2113 and using the bound s(v) < 2\u2113 if v is of level \u2113, we get\n\u03d5 < log r\u2211 \u2113=1 ( 1 + r2\u2113 ) log 2\u2113 = 2r +O(log2 r).\n\u25c0\nSince all the splay trees together add up to n nodes, the potential function \u03d5 of the forest is O(n) after building them all from the permutation. Further, since each splay tree contains at most n nodes at any given time, we can derive the following corollary.\n\u25b6 Corollary 6. A sequence of m arbitrary splay tree operations (access, insertion, deletion, split, or join) on an FST of an n-permutation takes O(n + m log n) time.\nProof. An edge rotation can be implemented on the FST with 6 updates in the matrix M (update cells M1x, M2x, M1y, M3y, M1M2y , M2M1x or M3M1x for a right rotation; it is similar for a left rotation). Regarding the join and split operations, a removal or addition of an edge implies changing 2 cells in M . The cycles counter has to be incremented for each split operation and decremented for each join operation, adding constant time in each case. Furthermore, after each edge rotation and join and split operations, the subtree sizes of the involved nodes need to be updated. This is done in the standard way for binary trees that are annotated with subtree sizes. We illustrate an edge rotation in Figure 2. Altogether, the splay tree operations access, join, and split can be implemented in constant time on the FST.\nIt follows from Lemma 5 that the original splay trees can be constructed in O(n) time and makes us start with the potential function at \u03d5 = O(n). Combined with Theorem 4 and the fact that each cycle has at most n nodes, we get that the total time is O(n + m log n), where O(n) owes to the initial construction of the splay trees. \u25c0"
        },
        {
            "heading": "3.2 Operations supported by the FST",
            "text": "We now describe in detail the operations that can be performed on the FST data structure. In the following, recall that (1) in the course of each splay tree operation involving element i, i must be splayed, (2) in the FST, every splay tree corresponds to a cycle, keyed by position in the cycle according to some of its rotations, and (3) we have direct access to each element. Due to this direct access, we will refer to the splay tree operation access(i, t) simply as access(i) from now on."
        },
        {
            "heading": "3.2.1 Cycle rotation",
            "text": "We will need a technical operation, namely for a cycle C and an element i in C, rotate C such that i becomes the last element in the splay tree t that represents C. We call this operation rotate(C, i) and implement it as follows.\nIf i is the largest element of t, then there is nothing to do. Otherwise, let us write C = (A, i, B), where A is the sequence of elements that come in C before i, and B is that of those that come after i in C. With split(i, t), we turn t into two trees: t1, with i in the root, A in the left subtree and no right child; and t2 representing B. Now we perform join(t2, t1): this involves first splaying the largest element of B, say Bmax, in t2, and then attaching t1 (with root i) as the right child of Bmax. The trick is that since both (A, i, B) and (B, A, i) represent the same cycle C, the elements of t1 are regarded as smaller than those of t2 in\n\u03c0 = (\n1 2 3 4 5 6 7 8 9 10 11 1 3 6 4 2 8 11 5 10 7 9\n) \u03c0 = ( 1 1)( 1 2, 2 3, 3 6, 4 8, 5 5)( 1 4)( 1 7, 2 11, 3 9, 4 10)\nthe split-step but as larger in the join-step, so that we can correctly apply join(t2, t1). See Figure 3 for an illustration.\nThe operation consists of one splay operation\u2014this is equivalent to access(t, i) but skipping the search phase, since we have direct access to i\u2014, one split, and one join operation. Therefore, the total amortized time is O(log n)."
        },
        {
            "heading": "3.2.2 Return \u03c0(i) and \u03c0\u22121(j)",
            "text": "Since \u03c0 is stored in form of its cycles, \u03c0(i) is the element following i in its cycle. This is the next larger element in the cycle, viewed cyclically; in other words, if i is the largest node in its splay tree, then we have to return the smallest node; otherwise we have to return the successor node.\nTo do this, we first splay i, moving it to the root of its tree. If it has no right child, then it is the largest node and we descend left from the root as long as there is a left child, thus returning the smallest node. Otherwise, from the root i we move to the right child and then descend to the left as long as possible; this gives the successor of i in the tree. In both cases, we splay the node corresponding to \u03c0(i) after we find it. See Figure 4 for an example.\nIn terms of splay tree primitives, our operation is equivalent to access(i) (search for i in its tree, then splay i), followed by access(\u03c0(i)) (search for \u03c0(i) in the splay tree, then splay \u03c0(i)), except that we are skipping the first part of the access(i) operation due to our direct access to node i. Note that both searches are for the key (i.e., the position of the element in\nits cycle), and not for the element itself. The total amortized time is then O(log n). Finding \u03c0\u22121(j) is analogous, except that now we need the predecessor in the cycle rather than the successor. We splay node j, go left once, then keep going to the right, and finally splay the node that we find at the end of the operation. Again, we have a boundary case: if i the leftmost element of the cycle, then we go straight to the rightmost element in the tree. The cost is O(log n) amortized time, equivalent to access(j) followed by access(\u03c0\u22121(j))."
        },
        {
            "heading": "3.2.3 Return the number of cycles",
            "text": "This operation takes constant time since we just return the contents of counter cycles."
        },
        {
            "heading": "3.2.4 Same-cycle query",
            "text": "Given i \u0338= j, samecycle(i, j) returns TRUE if and only if i and j are in the same cycle. We do this by first splaying i (after direct access to it), thus moving it to the root of its tree, and then splaying j (after direct access to it), thus moving it to the root of its tree. Now we check whether i has a parent, and return TRUE if the answer is yes, since this means that j has now replaced i as root of their common tree. Since this is equivalent to access(i) followed by access(j) (in both cases skipping the search phase), the amortized running time is O(log n)."
        },
        {
            "heading": "3.2.5 Return \u03c0k(i) and \u03c0\u2212k(i)",
            "text": "Note that if i is in position j in its cycle C, then \u03c0k(i) is in position (j + k) mod |C|. In other words, we need to find the k\u2032th successor of i in C, where k\u2032 = k mod |C|. After splaying i, the k\u2032th successor is in the right subtree if j + k\u2032 \u2264 |C|, and in the left subtree otherwise.\nSo we first splay i, then check if k\u2032 \u2264 size(right(i)). If so, then we have to return the k\u2032-th smallest element in the right subtree, otherwise we need to return the (k\u2032\u2212size(right(i)))-th smallest element in the whole tree. This can be done using a standard function on binary search trees: the function min(x, \u2113) returns the \u2113th smallest element in the subtree rooted in x, defined recursively as follows:\nmin(x, \u2113) =  return x if \u2113 = size(left(x))+1 min(left(x), \u2113) if \u2113 < size(left(x))+1 min(right(x), \u2113\u2212 (size(left(x))+1)) if \u2113 > size(left(x))+1\nWe return min(right(i),k\u2032) if k\u2032 \u2264 size(right(i)), and min(i,k\u2032\u2212 size(right(i)) otherwise. After finding \u03c0k(i), we splay it. As the series of operations corresponds to access(i) (without the search phase) and access(\u03c0k(i)), the total amortized running time is O(log n).\nReturning \u03c0\u2212k(i) is analogous, where we set k\u2032 = \u2212k mod |C|, thus again we obtain O(log n) amortized time."
        },
        {
            "heading": "3.2.6 Distance between two elements i, j",
            "text": "The distance between two elements i, j given by \u03c0 is dist\u03c0(i, j) = min{d \u2265 0 : \u03c0d(i) = j}; in particular, dist\u03c0(i, j) =\u221e if no such d exists. Clearly, dist\u03c0(i, j) is finite if and only if i and j are in the same cycle. Note that dist\u03c0 is not symmetric.\nTo compute dist\u03c0(i, j), we first execute a query samecycle(i, j) and return\u221e if the answer is FALSE. Otherwise, let C be the cycle containing i and j. We move j to the end of C with\nrotate(C, j). Then we splay i and return size(right(i))). To see that this is correct, notice that splaying does not change the relative positions of the elements of the cycle.\nAll the involved operations take O(log n) amortized time (samecycle, rotate, splay), so this is also the cost of this operation."
        },
        {
            "heading": "3.2.7 Size of the cycle of an element i",
            "text": "This can be computed by accessing i, splaying it so that it becomes the root of its splay tree, and then returning its subtree size. The operation takes O(log n) amortized time.\n3.2.8 Update: transpositions (\u03c0(i), \u03c0(j)) and (i, j)\nThe application of the transposition (\u03c0(i), \u03c0(j)) results in a new permutation \u03c0\u2032 = (\u03c0(i), \u03c0(j))\u00b7 \u03c0. Lemma 2 gives the exact form of \u03c0\u2032, depending on whether i and j are in the same cycle.\nSo first we need to do a samecycle(i, j) check. If the answer is TRUE, let C be the cycle containing both i and j. We move i to the end of C with rotate(C, i). Now we have C = (A, j, B, i), and by Lemma 2, C will be split into C1 = (A, j) and C2 = (B, i), which can be implemented as the splay tree operation split(j, t), where t is the splay tree of C.\nOtherwise, let C = (A, i, B) be the cycle containing i and C \u2032 = (D, j, E) the cycle containing j. We perform two cycle rotations, rotate(C, i) and rotate(C \u2032, j), moving i to the end of C and j to the end of C \u2032. Let the two trees be t1 and t2. The last step is to merge these two trees with join(t1, t2), which results in i, the largest element of t1, being splayed and t2 being attached as i\u2019s right child. The merged cycle represents (B, A, i, E, D, j), in agreement with Lemma 2. See Figure 5 for an illustration.\nNote that in both cases, the cycles counter has to be updated: incremented by one if samecycle(i, j) is TRUE, since a split-operation is performed; and decremented by one otherwise, since a join-operation is performed.\nFor the analysis, we have applied a same cycle query, followed by splitting a cycle or merging two cycles. Splitting a cycle consists of one cycle rotation and one split-operation. Merging two cycles, of two cycle rotations and one join-operation. Altogether we have, in both cases, O(log n) amortized time.\nFor i, j \u2208 {1, 2, . . . , n}, i \u0338= j, the transposition (i, j) is equivalent to (\u03c0(\u03c0\u22121(i)), \u03c0(\u03c0\u22121(j)))\u25e6 \u03c0. So we can access \u03c0\u22121(i) and \u03c0\u22121(j) in O(log n) time, and then perform the transposition as in Section 3.2.8. Overall, this takes O(log n) amortized time."
        },
        {
            "heading": "3.2.9 Update: Flips",
            "text": "Our final operation, that is strongly related to the cycle structure of the permutations, is to reverse part of a cycle. Given a cycle C = (i1, i2, . . . , i\u2113) of \u03c0, the operation flip(ir, it), with r < t, converts the cycle into (i1, . . . , ir\u22121, it, it\u22121, . . . , ir, it+1, . . . , i\u2113). That is, the direction of the cycle segment between ir and it is reversed. It might also be that r > t, which yields (it, it\u22121, . . . , i1, i\u2113, . . . , ir, it+1, . . . , ir\u22121).\nNote that this operation is distinct from what is called a \u201creversal\u201d in the area of genome rearrangements [12], because reversals act on the one-line representation of the permutation.\nReflecting this operation in our current structure requires O(n) time, because potentially large parts of a splay tree need to be reversed. Instead, we extend our FST data structure so that the subtree-size component becomes signed. If the subtree-size field of a node v is \u2212s, with s > 0, this means that the actual subtree size is s and that its subtree should be reversed, that is, its nodes should be read right-to-left. We will de-amortize the reversal work along future visits to the subtree, as explained soon. The extra space required is just n bits for the signs, so it stays within 3n log n +O(n) bits.\nIn order to apply the described flip, we first perform rotate(C, it+1) to make sure that ir\u22121 is behind it in the tree. We now splay it+1 and then ir\u22121. After this, ir\u22121 is the root of the tree, it+1 is its right child, and the left child v of it+1 is the subtree with all the elements from ir to it. We then toggle the sign of the subtree-size field of v and finish.\nThis takes O(log n) amortized time because it builds on a constant number of other operations we have already analyzed. We must, however, adapt all the other operations to handle negative subtree-size fields.\nThe general solution is that every time we access a tree node, if its subtree-size field is negative, we toggle it, exchange the left and right children, and toggle their subtree-size fields, before proceeding with any other action. Precisely, we define the primitive fix(x) as follows: (i) if size(x) < 0, then (ii) toggle size(x)\u2190 \u2212size(x), size(left(x))\u2190 \u2212size(left(x)), size(right(x)) \u2190 \u2212size(right(x)), and (iii) swap left(x) with right(x). See Figure 6 for an example of fix(x). We then alter the splay and tree traversal operations as follows:\nBefore performing a rotation on node x during a splay, we fix the grandparent of x, then its parent, and then x. The order is important because fixing a node may change the signs of its children. The other subtrees involved in the rotations can be left unfixed. Then we perform the zig, zig-zig, or zig-zag to move x upwards, as it corresponds. When we descend in the tree from a node x (e.g., in the function min(x, \u2113)), we perform fix(x) before processing it.\nNote that fix takes constant time and does not change the potential function \u03d5, so no time complexities change due to our adjustments. All the structural changes to the splay tree are performed over traditional (i.e., fixed) nodes, so no algorithm needs further changes.\n4 Comparison with other data structures\nWe now compare the running times of different operations of the FST, to four baselines: (1) one array for the one-line notation, (2) two arrays for the one-lines of the permutation and its inverse, (3) the dynamic sequence representation of Munro and Nekrich [17], and (4) the static structure of Munro et al. [16].\nThe FST takes 3n log n+O(n) bits of space for the matrix M and the counter cycles. The one-line notation is an integer array, taking n\u2308log n\u2309 = n log n +O(n) bits. The permutation and its inverse in one-line notation require 2n\u2308log n\u2309 = 2n log n +O(n) bits. The dynamic sequence representation takes n log n + o(n log n) bits. Finally, we will use the variant of the structure of Munro et al. that uses (1 + \u03f5)n log n bits, for any constant \u03f5 > 0.\nIn the array for the one-line notation (1), the update by transposition (\u03c0(i), \u03c0(j)) and returning \u03c0(i) take constant time. To enable also the update by (i, j) and returning \u03c0\u22121(i) in constant time, we need also the inverse permutation stored in another array (2); otherwise we need O(c) time, on a cycle of length c, to find the inverses of i and j. For both structures, returning the number of cycles after an update operation is not constant, because at each update we lose the information about cycles. Even if we used a counter for storing the number of cycles, as in the forest of splay trees, we would need O(n) time to update it. The main problem is how to answer the same-cycle query during each update of the permutation, which takes O(c) time. Also, computing the distance between two elements i, j takes O(c) time. A flip can also be applied in O(c) time by following the cycle.\nThe dynamic sequence representation (3) provides access to both \u03c0(i) and \u03c0\u22121(j) within less space, in O( log nlog log n ) time. It can also implement both transpositions by means of inserting and deleting two pairs of symbols in the sequence, within the same time complexity. The first pair would delete i (or \u03c0(i)) and insert j (or \u03c0(j)) in its position, while the second couple works symmetrically. Just as the simpler preceding structures, however, we cannot answer queries related to cycles in less than O(c) steps, each taking O( log nlog log n ) time.\nMunro et al. [16] present a representation specialized in answering powers of permutations (4). From its description in Section 1.1, it follows that they support \u03c0(i), \u03c0\u22121(j), \u03c0k(k), and \u03c0\u2212k(j), all in time O(1/\u03f5). It is not hard to see that this structure can also determine if i and j are in the same cycle, by checking whether the 1s preceding and following \u03c1\u22121(i) and \u03c1\u22121(j) are the same; their distance in the cycle and the size of their cycle are also trivially found. The number of cycles (or 1s in the bitvector) can be stored to answer that query in constant time. Overall, they support all the queries in time O(1/\u03f5). The problem is that this structure is static, so any update requires reconstructing the whole structure in O(n) time.\nTable 1 summarizes the space and running times. The FST is the only dynamic structure that efficiently handles queries about the cycle structure of the underlying permutation.\n5 Conclusion\nWe have introduced a new dynamic data structure to represent permutations, the forest of splay trees (FST), which is unique in supporting various operations related to the cycle structure of the permutation, while permitting to perform arbitrary transpositions on it and flips in the cycles. Concretely, for a permutation on n, the FST is built in O(n) time, and then supports a number of queries and updates in O(log n) amortized time each. No structure we know of supports both kinds of queries/updates in o(n) time.\nA future direction to extend the FST is to incorporate other queries and updates, motivated by applications. Another interesting direction is to extend the scope of FSTs from permutations to general functions in [1, n], in the lines of the representation of Munro et\nal. [16]. They combine cycles with ordinal trees in order to support the operations fk(i) and f\u2212k({j}) in optimal time, but again, do not support updates on f .\nReferences 1 Martin Aigner. Discrete Mathematics. American Mathematical Society, 2007. 2 J. Barbay, F. Claude, T. Gagie, G. Navarro, and Y. Nekrich. Efficient fully-compressed\nsequence representations. Algorithmica, 69(1):232\u2013268, 2014. 3 J. Barbay and G. Navarro. On compressing permutations and adaptive sorting. Theoretical\nComputer Science, 513:109\u2013123, 2013. 4 Mikl\u00f3s B\u00f3na. Combinatorics of Permutations, Second Edition. Discrete mathematics and its\napplications. CRC Press, 2012. Sec. ed. 5 M. Burrows and D. J. Wheeler. A block-sorting lossless data compression algorithm. Technical\nreport, 1994. 6 P. Ferragina and G. Manzini. Indexing compressed text. Journal of the ACM, 52:552\u2013581,\n2005. 7 Guillaume Fertin, Anthony Labarre, Irena Rusu, Eric Tannier, and St\u00e9phane Vialette. Com-\nbinatorics of Genome Rearrangements. Computational molecular biology. MIT Press, 2009. 8 J. Fischer, V. M\u00e4kinen, and G. Navarro. Faster entropy-bounded compressed suffix trees.\nTheoretical Computer Science, 410(51):5354\u20135364, 2009. 9 T. Gagie, G. Navarro, and N. Prezza. Fully-functional suffix trees and optimal text searching\nin BWT-runs bounded space. Journal of the ACM, 67(1):article 2, 2020. 10 Sara Giuliani, Zsuzsanna Lipt\u00e1k, Francesco Masillo, and Romeo Rizzi. When a dollar makes a BWT. Theor. Comput. Sci., 857:123\u2013146, 2021. 11 Roberto Grossi and Jeffrey Scott Vitter. Compressed suffix arrays and suffix trees with applications to text indexing and string matching. SIAM J. Comput., 35(2):378\u2013407, 2005. 12 D. Gusfield. Algorithms on Strings, Trees and Sequences: Computer Science and Computational Biology. Cambridge University Press, 1997. 13 Sebastian Kreft and Gonzalo Navarro. On compressing and indexing repetitive sequences. Theor. Comput. Sci., 483:115\u2013133, 2013. 14 Veli M\u00e4kinen and Gonzalo Navarro. Succinct suffix arrays based on run-length encoding. Nord. J. Comput., 12(1):40\u201366, 2005. 15 Udi Manber and Eugene W. Myers. Suffix arrays: A new method for on-line string searches.\nSIAM J. Comput., 22(5):935\u2013948, 1993."
        },
        {
            "heading": "16 J. I. Munro, R. Raman, V. Raman, and S. S. Rao. Succinct representations of permutations",
            "text": "and functions. Theoretical Computer Science, 438:74\u201388, 2012.\n17 J. Ian Munro and Yakov Nekrich. Compressed data structures for dynamic sequences. In Proc. of the 23rd Annual European Symposium (ESA2015), Patras, Greece, September 14-16, 2015, volume 9294 of Lecture Notes in Computer Science, pages 891\u2013902. Springer, 2015.\n18 Gonzalo Navarro. Compact Data Structures - A Practical Approach. Cambridge University Press, 2016.\n19 Jo\u00e3o Setubal and Jo\u00e3o Meidanis. Introduction to Computational Molecular Biology. PWS Publishing Company, 1997.\n20 Daniel Dominic Sleator and Robert Endre Tarjan. Self-adjusting binary search trees. J. ACM, 32(3):652\u2013686, 1985.\nAPPENDIX\nA Details on splay trees\nThese chains of rotations have three different names based on the relative position of a node x w.r.t. its parent and grandparent. If the parent of x is the root, then only one rotation is required to move x to the root (zig). If both x and the parent of x are right children of their parent, or if both are left children of their parent, then two rotations are performed in sequence: first between the parent of x and the grandparent of x, then between x and its parent (zig-zig, see Figure 7). The last possibility is that x is a right child and its parent is a left child of the grandparent of x, or that x is a left child and its parent is a right child. Two rotations with different direction are concatenated: this time we first perform a rotation between x and its parent, then between x and its former grandparent (zig-zag, see Figure 8).\nWe now explain two other operations on splay trees, namely split and join.\nSplit operation on splay trees\nGiven a splay tree t and an element x we can split t in two different trees, i.e. t1 and t2 s.t. every element in t1 is less than or equal to x and every element in t2 is greater than x. This can be done by splaying x and removing the right edge from x to its right child. This operation takes amortized O(log n) time, because the splay operation takes amortized O(log n) time, while removing the edge takes O(1) time.\nJoin operation on splay trees\nGiven two splay trees t1 and t2, we can combine them into a single tree. We have to splay the rightmost element of t1 (max(t1)) (if we assume that every element of t1 is smaller than the minimum of t2) and then create an edge between max(t1) and the root of t2. This operation also takes amortized O(log n) time (one splay operation and one edge creation).\nB Comparison to FST in Giuliani et al.\nIn [10], the splay-tree based data structure was used exclusively for transpositions, and other possible operations were not discussed. Moreover, only one specific type of transposition was used: that of two contiguous elements: \u03c0\u2032 = (\u03c0(i), \u03c0(i + 1)) \u00b7 \u03c0. More precisely, the authors always applied the operation \u03c0\u2032 = (1, \u03c0(i + 1)) \u00b7 \u03c0, because in the particular application, in the given permutation \u03c0i, 1 was always in position i. Another peculiarity of the application is that right after the ith transposition is performed, value i + 1 is always the rightmost element of its splay tree.\nThe specialized version of the FST in [10] furthermore did not have the information about subtree sizes that is key to multiple operations supported in our current version."
        }
    ],
    "title": "Maintaining the cycle structure of dynamic permutations",
    "year": 2023
}