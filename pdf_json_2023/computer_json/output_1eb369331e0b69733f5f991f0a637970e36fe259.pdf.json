{
    "abstractText": "The recently introduced Genetic Column Generation (GenCol) algorithm has been numerically observed to efficiently and accurately compute high-dimensional optimal transport plans for general multi-marginal problems, but theoretical results on the algorithm have hitherto been lacking. The algorithm solves the OT linear program on a dynamically updated low-dimensional submanifold consisting of sparse plans. The submanifold dimension exceeds the sparse support of optimal plans only by a fixed factor \u03b2. Here we prove that for \u03b2 \u2265 2 and in the two-marginal case, GenCol always converges to an exact solution, for arbitrary costs and marginals. The proof relies on the concept of c-cyclical monotonicity. As an offshoot, GenCol rigorously reduces the data complexity of numerically solving twomarginal OT problems from O(l) to O(l) without any loss in accuracy, where l is the number of discretization points for a single marginal. At the end of the paper we also present some insights into the convergence behavior in the multi-marginal case.",
    "authors": [
        {
            "affiliations": [],
            "name": "Gero Friesecke"
        },
        {
            "affiliations": [],
            "name": "Maximilian Penka"
        }
    ],
    "id": "SP:9d64fbec4e0c43b870d846379f05d1e65058841e",
    "references": [
        {
            "authors": [
                "Jason M. Altschuler",
                "Enric"
            ],
            "title": "Boix-Adsera, Hardness results for multimarginal optimal transport problems",
            "venue": "Discrete Optim",
            "year": 2021
        },
        {
            "authors": [
                "Karthekeyan Chandrasekaran",
                "L\u00e1szl\u00f3 A V\u00e9gh",
                "Santosh S Vempala"
            ],
            "title": "The cutting plane method is polynomial for perfect matchings",
            "venue": "Mathematics of Operations Research",
            "year": 2016
        },
        {
            "authors": [
                "Li Chen",
                "Rasmus Kyng",
                "Yang P. Liu",
                "Richard Peng",
                "Maximilian Probst Gutenberg",
                "Sushant Sachdeva"
            ],
            "title": "Maximum flow and minimum-cost flow 12 in almost-linear",
            "venue": "IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS),",
            "year": 2022
        },
        {
            "authors": [
                "Joel Franklin",
                "Jens Lorenz"
            ],
            "title": "On the scaling of multidimensional matrices, Linear Algebra and its applications",
            "year": 1989
        },
        {
            "authors": [
                "Gero Friesecke",
                "Maximilian Penka"
            ],
            "title": "The GenCol algorithm for highdimensional optimal transport: general formulation and application to barycenters and Wasserstein splines",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Marco E L\u00fcbbecke",
                "Jacques Desrosiers"
            ],
            "title": "Selected topics in column generation, Operations research",
            "year": 2005
        },
        {
            "authors": [
                "Filippo Santambrogio"
            ],
            "title": "Optimal transport for applied mathematicians, Birkh\u00e4user",
            "year": 2015
        },
        {
            "authors": [
                "Bernhard Schmitzer"
            ],
            "title": "Stabilized sparse scaling algorithms for entropy regularized transport problems",
            "venue": "SIAM J. Sci. Comput",
            "year": 2019
        },
        {
            "authors": [
                "Christoph Str\u00f6ssner",
                "Daniel Kressner"
            ],
            "title": "Low-rank tensor approximations for solving multimarginal optimal transport problems",
            "venue": "SIAM J. Imaging Sci",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 3.\n07 13\n7v 2\n[ m\nat h.\nN A\n] 1\n3 Se"
        },
        {
            "heading": "1 Introduction",
            "text": "Large-scale discrete optimal transport problems are difficult to solve numerically because the full problem has a huge number of possible configurations. At the same time it is guaranteed that a rather sparse solution exists, a particularly well known manifestation in continuous OT being Brenier\u2019s theorem. This effect is especially important in the multi-marginal case, but occurs already in the classical two-marginal case when the support size of both marginals is large.\nIn recent years, computational strategies for optimal transport were driven by the idea of approximating the problem by adding an entropy-like penalty term. This transforms the problem into a strictly convex and more robust optimization problem, which can be solved in short time using the Sinkhorn algorithm as long as the overall number of unknowns remains moderate. In theory this approach, called entropic optimal transport (EOT), is also valid for two-marginal problems in high dimension or general multi-marginal problems.\nUnfortunately, this approach corresponds to smearing out the transport plan, yielding a huge amount of configurations in its support: The true optimizer of\n\u2217MP was partially funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) through the Collaborative Research Center TRR 109 \u201cDiscretization in Geometry and Dynamics\u201c, Projektnummer 195170736\nthe EOT problem has the same support as the full product measure of its marginals. Thus the support size scales polynomially in the support size of the marginals, and exponentially in the number of marginals. Recent approaches alleviate this problem by truncation or multi-scale methods [11] or \u2013 most recently \u2013 low-rank approximation [10, 12].\nAn alternative approach was proposed in [7, 6]. Rather than looking for further refinements of interior point methods, which struggle to solve huge programs, one goes back to the original linear program, and exploits that the OT program possesses extremely sparse solutions. If the i-th marginal is supported on \u2113i points, there exist optimal plans with support size less than the sum of the \u2113i, rather than their product [6].\nThe standard approach to solve linear programs is the simplex algorithm and its descendants, most promisingly Column Generation [8]. The latter tackles large LPs by iteratively solving smaller (\u201creduced\u201d) problems on a dynamically evolving subset of all variables. In optimal transport, every variable of the LP corresponds to a possible configuration in the product of the marginal domains, so Column Generation corresponds to solving the OT problem on a subset of the product of the marginal domains. Thus Column Generation can in principle exploit sparsity and find the exact optimal plan of the full problem, provided a sparse superset of its support is known. But in practice such a superset has to be found.\nThere are two obstructions for doing so. First, in Column Generation the generation of new variables is originally done by solving a second optimization problem, the so called pricing problem. Unfortunately the pricing problem for large problems is still expensive; in the multi-marginal case it has been proven to be NP hard [1, 7]. For LPs of moderate size this problem can be alleviated by generating new configurations at random, but in this randomized version one needs to try many configurations, again leading to an unacceptable slowdown for large problems. Second, the algorithm increases the size of the LP in each iteration step, making the iterations slower and slower and lacking any convergence guarantee until the size has reached the original LP size that one wanted to avoid!\nFor these reasons, [7] invented\n\u2022 a genetic search rule, restricting the number of possible proposals from all configurations to an update of one entry in one active configuration\n\u2022 a genetic tail-clearing rule which discards those configurations which have been inactive the longest, to keep the overall support size at a fixed small multiple of the size of sparse optimizers.\nThe resulting algorithm, which was termed Genetic Column Generation (GenCol), exhibited in several application examples of interest a spectacularly accelerated convergence to global optimizers. A theoretical explanation has hitherto been lacking.\nIn this paper we present a rigorous proof of convergence to a global optimizer in the case of two marginals. The fact that GenCol cannot get stuck in a local optimizer is far from obvious since the upper bound on the support size makes the reduced problem non-convex. The proof relies on the concept of ccyclical monotonicity which is well known in the theory of optimal transport. It\nfinds here a beautiful application and yields an intuitive understanding of the algorithm.\nOur arguments, while rigorously and non-trivially reducing the storage cost, do not yield a rigorous bound on the convergence speed, which \u2013 in numerical examples \u2013 is observed to be exponential [7, 6]. By contrast, for Sinkhorn as well as some classical LP algorithms requiring access to the full state space, the convergence speed has been rigorously estimated. See [5] for Sinkhorn, [3] for max-flow min-cut, and [2] for cutting plane applied to perfect matching. Let us also mention the numerical study [4] which compares the runtimes of some standard two-marginal OT algorithms.\nIn section 6 we analyze the multi-marginal case, for which the GenCol algorithm was originally proposed. We rigorously justify convergence of the algorithm to a global optimizer provided the search rule finds any possible configuration with positive probability. Hence with such a search rule, GenCol rigorously reduces the storage complexity from exponential to linear in the number of variables. However the price to pay is that the number of search steps might be exponentially large. By contrast, GenCol with the efficient search rule from [7, 6] - with its one-entry-at-a-time update which requires only quadratically many search steps in the number of marginals - might fail to converge to a global optimizer, at least for general costs. See section 6.2 for a counterexample. The design and analysis of updating rules for the multi-marginal case which are both efficient and yield rigorous global convergence for practically relevant costs is an interesting open question."
        },
        {
            "heading": "2 c-Cyclical Monotonicity",
            "text": "Given two probability measures \u00b51, \u00b52 on Polish spaces X respectively Y , the optimal transport problem is the following:\nminimize F [\u03b3] :=\n\u222b\nX\u00d7Y\nc(x, y) d\u03b3(x, y) over \u03b3 \u2208 P(X \u00d7 Y )\nsubject to\n{\n\u03b3(A\u00d7 Y ) = \u00b51(A) for all measurable A \u2282 X\n\u03b3(X \u00d7B) = \u00b52(B) for all measurable B \u2282 Y.\nwhere P denotes the set of probability measures. Solutions to the constraints are called transport plans. Optimality of a transport plan \u03b3 can be characterized by a condition on its support, called c-cyclical monotonicity.\nDefinition 2.1 (see e.g. [9], Def. 1.36). Given a function c : X\u00d7Y \u2192 R\u222a{+\u221e}, we say that a set \u0393 \u2282 X \u00d7 Y is c-cyclically monotone (c-CM) if for every k \u2208 N, every permutation \u03c3 : {1, . . . , k} \u2192 {1, . . . , k}, and every finite set of points {(x1, y1), ..., (xk, yk)} \u2282 \u0393 we have\nk\u2211\ni=1\nc(xi, yi) \u2264 k\u2211\ni=1\nc(xi, y\u03c3(i)).\nWhile it is easy to see (at least in the discrete case) that this is a necessary condition on the support of an optimal plan, it turns out to also be sufficient.\nTheorem 2.2 (see e.g. [9], Thm. 1.49). Suppose X and Y are Polish spaces and c : X \u00d7 Y \u2192 R is uniformly continuous and bounded. Given \u03b3 \u2208 P(X \u00d7 Y ), if supp(\u03b3) is c-CM then \u03b3 is an optimal transport plan between its marginals \u00b51 = (\u03c01)\u266f\u03b3 and \u00b52 = (\u03c02)\u266f\u03b3 for the cost c."
        },
        {
            "heading": "3 Sparsity of optimal plans",
            "text": "The support of optimal transport plans is typically a much smaller set than the product of the supports of the marginals. Rather than going into classical variants for convex costs like Brenier\u2019s theorem and their interesting relation to c-cyclical monotonicity, we focus directly on a discrete version for general costs which informed the design of the GenCol algorithm and is useful for its analysis.\nFor X and Y discrete, |X | = \u21131, |Y | = \u21132, the objective function F becomes a finite sum and the OT problem a linear program in standard form:\nminimize \u3008c, \u03b3\u3009 := \u2211\n(x,y)\u2208X\u00d7Y\nc(x, y)\u03b3(x, y) over \u03b3 : X \u00d7 Y \u2192 [0,\u221e)\nsubject to \u03b3 \u2208 \u03a0(\u00b51, \u00b52) :\u21d4\n{\u2211\ny\u2208Y \u03b3(x0, y) = \u00b51(x0)\u2200x0 \u2208 X \u2211\nx\u2208X \u03b3(x, y0) = \u00b52(y0)\u2200y0 \u2208 Y,\n(OT)\nwhere the measures \u00b51, \u00b52 and \u03b3 were identified with their densities with respect to the counting measures on their domains.\nTheorem 3.1. Suppose X and Y are discrete with |X | = \u21131, |Y | = \u21132. Then any extreme point of the Kantorovich polytope \u03a0(\u00b51, \u00b52) is supported on at most \u21131+ \u21132\u22121 points. In particular, for any cost c : X\u00d7Y \u2192 R and any marginals, the OT problem (OT) possesses an optimizer supported on at most \u21131 + \u21132 \u2212 1 points.\nThis can be deduced from well known results on extremal solutions in linear programming. For a self-contained and simple proof using geometry of convex polytopes see [6]."
        },
        {
            "heading": "4 Genetic Column Generation Algorithm",
            "text": "The algorithm doesn\u2019t deal with the full OT problem but only its restrictions to certain subsets of X \u00d7 Y whose size is of the order of the support size of optimizers from Theorem 3.1.\nWe call a subset \u2126 \u2282 X\u00d7Y a feasible subset of configurations if \u03a0(\u00b5, \u03bd)\u2229{\u03b3 : supp(\u03b3) \u2282 \u2126} is non-empty. Given such a subset, we define the reduced problem to be\nminimize \u3008c, \u03b3\u3009 over \u03b3 : X \u00d7 Y \u2192 [0,\u221e)\nsubject to \u03b3 \u2208 \u03a0(\u00b51, \u00b52)\nand supp(\u03b3) \u2286 \u2126.\n(ROT)\nBecause \u03b3 is a discrete measure, supp(\u03b3) is the set of all (x, y) \u2208 X \u00d7 Y with \u03b3(x, y) 6= 0. Thus the reduced problem amounts to reducing the variables in the linear program to the values of \u03b3 on configurations in \u2126 (and setting the values\noutside \u2126 to zero), and not changing the constraints. As the values outside \u2126 no longer need to be considered, this shrinks the size of the program to that of \u2126.\nBefore we come to genetic column generation, let us describe classical column generation. Unlike genetic column generation it does not restrict the size of \u2126, and works as follows. Given a feasible initial set \u2126, the first step is to solve the reduced problem. The second step is to generate a new configuration (x\u2032, y\u2032) /\u2208 \u2126 which is added to \u2126 and improves the solution. The two steps are iterated until no more improving configurations exist.\nThe second step relies on the dual of the reduced problem (ROT),\nmaximize \u3008\u00b51, u1\u3009+ \u3008\u00b52, u2\u3009 overu1 : X \u2192 R, u2 : Y \u2192 R such that u1(x) + u2(y) \u2264 c(x, y) \u2200(x, y) \u2208 \u2126. (D-ROT)\nIn comparison, the dual of the full problem (OT) has the same objective function, but more constraints:\nmaximize \u3008\u00b51, u1\u3009+ \u3008\u00b52, u2\u3009 overu1 : X \u2192 R, u2 : Y \u2192 R such that u1(x) + u2(y) \u2264 c(x, y) \u2200(x, y) \u2208 X \u00d7 Y. (D-OT)\nHence every dual optimizer for the full problem is admissible in the reduced problem (D-ROT), but a dual optimizer for the reduced problem might violate a constraint of the full problem (D-OT). If, however, a dual optimizer for the reduced problem is admissible for (D-OT) then it is already optimal for (D-OT):\nLemma 4.1. Let (\u03b3\u22c6, (u\u22c61, u \u22c6 2)) be a pair of optimizers for the reduced problems (ROT, D-ROT). If (u\u22c61, u \u22c6 2) is admissible for the dual of the full problem (D-OT), then \u03b3\u22c6 is optimal for (OT).\nFor a proof of this classical result translated into the present context and language of OT see [6]. Hence new configurations (x\u2032, y\u2032) /\u2208 \u2126 can be sought by checking if they violate the dual constraint of the full problem (D-OT), i.e. if they satisfy the following acceptance criterion:\nu\u22c61(x \u2032) + u\u22c62(y \u2032)\u2212 c(x\u2032, y\u2032) > 0. (Acc)\nDue to economic interpretations this difference is called gain. In classical column generation this gain is maximized over all configurations, constituting the socalled pricing problem.\nThe following difficulties arise when applying column generation to large LPs, as already pointed out in the Introduction. (i) The pricing problem is too expensive; and the empirical strategy of instead generating configurations (x\u2032, y\u2032) /\u2208 \u2126 independently at random until one of them satisfies (Acc) requires too many trials, especially in the multi-marginal case. (ii) Regardless of how one searches for new configurations, the subset \u2126 grows in each iteration step, making the iterations slower and slower and lacking any convergence guarantee until the size has reached the original LP size that one wanted to avoid.\nGenetic column generation [7, 6] tackles these difficulties as follows. (i) Motivated by machine learning protocols in unsupervised learning, the algorithm first proposes new configurations originating from currently active configurations, i.e. (x, y) \u2208 supp(\u03b3) \u2282 \u2126: one picks an active configuration at\nrandom (\u201cparent\u201d), then proposes an offspring (\u201cchild\u201d) by changing one entry of the parent configuration. Explicitly,\ngiven a parent (x, y) \u2208 supp(\u03b3), pick a random child in ( supp(\u00b51)\u00d7{y} ) \u222a ( {x}\u00d7supp(\u00b52) ) .\n(1)\nThe offspring is then accepted if its gain is positive. The rough analogy to ML is that the proposal step mimics an SGD step and the acceptance mimics learning from an adversary (in this case, the current dual). In fact, the proposal step in the first version of GenCol was even more similar to SGD, in that entries were points on a regular grid and children were proposed from neighbouring sites of parents.\n(ii) The size of \u2126 is restricted to remain of the order of the support size of optimizers from Theorem 3.1. More precisely, one introduces a hyperparameter \u03b2 > 1 and a tail clearing rule which guarantees that\n|\u2126| \u2264 \u03b2 \u00b7 (\u21131 + \u21132). (2)\nTail clearing means that whenever, after accepting a child, \u2126 violates (2), the oldest unused configurations are removed. In practice, one chooses 3 . \u03b2 . 5 and discards a batch of \u21131 + \u21132 configurations whenever |\u2126| exceeds \u03b2 \u00b7 (\u21131 + \u21132). The hyperparameter \u03b2 does not depend on the sizes \u21131 and \u21132 of X and Y .\nSee Algorithm 1 for a summary of the algorithm.\nAlgorithm 1 Genetic Column Generation\nRequire: Marginals \u00b51, \u00b52; feasible set \u2126 satisfying (2); hyperparameter \u03b2 > 1 1: while TRUE do 2: (\u03b3\u22c6, u\u22c6)\u2190 solution to (ROT), (D-ROT) 3: repeat\n4: Sample a parent in supp(\u03b3\u22c6) and a child (x\u2032, y\u2032) 5: until u\u22c61(x \u2032)+u\u22c62(y \u2032) > c(x\u2032, y\u2032) (Acc) or all possible offspring were tried 6: if \u00ac(Acc) then 7: return (\u03b3\u22c6, u\u22c6) optimal 8: end if\n9: Accept the child: \u2126\u2190 \u2126 \u222a {(x\u2032, y\u2032)} 10: if |\u2126| > \u03b2 \u00b7 (\u21131 + \u21132) then 11: remove oldest inactive configurations from \u2126 12: end if 13: end while\nIt is not clear why the algorithm should find a global optimum. Can it happen \u2013 due to the tail clearing \u2013 that it instead gets stuck in a local minimum?\nThe answer is No, as shown in the next section. Note that (at least in the two-marginal case; see section ... for discussion of the multi-marginal case) every configuration (x\u2032, y\u2032) /\u2208 \u2126 which belongs to the product of the supports of \u00b51 and \u00b52 is proposed by Algorithm 1 with strictly positive probability, so the genetic proposal of updates is not a restriction. However, the tail-clearing turns the original, convex state space \u03a0(\u00b51, \u00b52) into the nonconvex state space \u03a0(\u00b51, \u00b52) \u2229 {\u03b3 \u2208 P(X \u00d7 Y ) | | supp \u03b3| \u2264 \u03b2 \u00b7 (\u21131 + \u21132), making the question of global convergence nontrivial."
        },
        {
            "heading": "5 Convergence",
            "text": "Before giving the proof of convergence, we must specify line 2 (solving the reduced problem and its dual) and line 4 (sample a parent and a child) of Algorithm 1 more precisely.\nLine 2. First, in degenerate cases optimal plans may not be unique, so for convergence it is mandatory that \u03b3\u2217 in Algorithm 1 is updated only if the previous plan is no longer minimizing. Second, we require the linear programming solver to provide a solution \u03b3\u2217 which satisfies the support size bound from Theorem 3.1. If one uses the simplex algorithm with a warm start, both these requirements are automatically guaranteed.\nLine 4. Second, to rigorously implement the second stopping criterion in line 5, one does not sample parents and children in each trial independently, but draws random permutations covering all possibilities and then tries them one after another until the stopping criterion is satisfied.\nTheorem 5.1. Let X and Y be discrete with |X | = \u21131, |Y | = \u21132, let c : X\u00d7Y \u2192 R be any cost, let \u00b51 \u2208 P(X), \u00b52 \u2208 P(Y ) be any marginals, and let \u2126 \u2282 X \u00d7 Y be any feasible subset of configurations. For any optimal solution \u03b3\u22c6 for the reduced problem (ROT) which is an extreme point of the Kantorovich polytope and which is not optimal for the full problem (OT), the GenCol proposal and acceptance routine (lines 2\u20139 of Algorithm 1) as detailed above finds with positive probability in consecutive steps a superset \u2126\u0303 \u2283 \u2126, whose size exceeds that of \u2126 by at most \u21131 + \u21132 \u2212 1 elements, which reduces the total cost:\nmin \u03b3 : supp(\u03b3)\u2286\u2126\u0303 F [\u03b3] < min \u03b3 : supp(\u03b3)\u2286\u2126 F [\u03b3].\nProof. Consider an optimal solution (\u03b30, u0) for the reduced problem (ROT) with \u03b30 extremal which is not optimal for the full problem. By Theorem 3.1 \u03b30 is sparse with at most \u21131 + \u21132 \u2212 1 non-zero entries (active configurations). In the following we write u0 = (u01, u 0 2). Due to complementary slackness\nu01(x) + u 0 2(y)\u2212 c(x, y) = 0 \u2200(x, y) \u2208 supp(\u03b3 0).\nBecause \u03b30 is not optimal for the full problem (OT), by Theorem 2.2 there exists a family \u0393 = {(x1, y1), ..., (xk, yk)} \u2282 supp(\u03b30) and a permutation \u03c3 \u2208 Sk such that\nk\u2211\ni=1\nc(xi, yi) > k\u2211\ni=1\nc(xi, y\u03c3(i)).\nNote that the bound | supp(\u03b3)| \u2264 \u21131+\u21132\u22121 yields the upper bound k \u2264 \u21131+\u21132\u22121. Because {(x1, y1), ..., (xk, yk)} \u2282 supp(\u03b3 0), complementary slackness implies\nu01(x1) + u 0 2(y1)\u2212 c(x1, y1) = 0 (3.1)\n...\nu01(xk) + u 0 2(yk)\u2212 c(xk, yk) = 0. (3.k)\nAfter summation,\nk\u2211\ni=1\n(\nu01(xi) + u 0 2(yi)\n) \u2212 k\u2211\ni=1\nc(xi, yi)\n\ufe38 \ufe37\ufe37 \ufe38\n> k\u2211\ni=1\nc(xi,y\u03c3(i))\n= 0 (4)\n=\u21d2 k\u2211\ni=1\n(\nu01(xi) + u 0 2(y\u03c3(i))\u2212 c(xi, y\u03c3(i))\n)\n> 0 (5)\n=\u21d2 max i\u2208{1,...,k}\n{ u01(xi) + u 0 2(y\u03c3(i))\u2212 c(xi, y\u03c3(i)) } > 0. (6)\nBecause, in the two-marginal case, all configurations (x\u2032, y\u2032) /\u2208 \u2126 are proposed by GenCol with positive probability, the element of \u0393 where the maximum in (6) is realized, let us call it (xi1 , y\u03c3(i1)), is proposed with positive probability, and accepted. In the next step the reduced OT problem is resolved on \u2126 extended by this element, yielding a new optimal pair (\u03b31, (u11, u 1 2)) and two cases.\nCase 1: The optimal plan changes: \u03b31 6= \u03b30. In that case, due to the rule that the plan only changes when it must, the optimal cost decreases and we are done.\nCase 2: The optimal plan does not change, \u03b31 = \u03b30. But the dual solution (u11, u 1 2) must have changed. Because \u03b3\n1 = \u03b30, we have (x1, y1), . . . , (xk, yk) \u2208 supp(\u03b31) and eqs. (3.1) - (3.k) still hold true with u01, u 0 2 replaced by u 1 1, u 1 2. But now, since u1 must satisfy the dual constraints on the enlarged configuration set, we also have\nu11(xi1 ) + u 1 2(y\u03c3(i1))\u2212 c(xi1 , y\u03c3(i1)) \u2264 0.\nSince eqs. (4)\u2013(6) all are also still true with u01, u 0 2 replaced by u 1 1, u 1 2, we conclude that\nmax i\u2208{1,...,k}\\{i1}\n{ u11(xi) + u 1 2(y\u03c3(i))\u2212 c(xi, y\u03c3(i)) } > 0. (7)\nBut in the next step, again either the optimal plan changes or the element of \u0393 realizing the maximum in (7) is proposed with positive probability and accepted, and so on. After k enlargement steps of \u2126, either a change of optimal plan has occurred in some step, or all elements of \u0393 have been accepted with positive probability. But then Case 1 occurs, since the mass min{\u03b30(xi, yi), i = 1, . . . , k} > 0 can be moved from {(xi, yi)}ki=1 to {(xi, y\u03c3(i)} k i=1, decreasing the total cost.\nOne can alternatively see that Case 1 occurs once all elements of \u0393 have been accepted by considering the dual solution uk: otherwise we would have\nuk1(xi) + u k 2(y\u03c3(i))\u2212 c(xi, y\u03c3(i)) \u2264 0 \u2200i \u2208 {1, ..., k},\nbut on the other hand (5) must hold with u01, u 0 2 replaced by u k 1 , u k 2 , a contradiction. Convergence of Algorithm 1 now follows as an easy consequence.\nCorollary 5.2. Suppose X and Y are discrete spaces of finite cardinality, and the hyperparameter \u03b2 is \u2265 2. For any marginals, any cost function, and any feasible initial set \u2126 \u2282 X \u00d7 Y , GenCol converges with probability 1 to an exact solution of the OT problem (OT).\nProof. The total cost is monotonically decreasing. Moreover X\u00d7Y is finite and every non-optimal plan is improved with positive probability: Since \u03b2 \u2265 2, after a tail-clearing the algorithm allows to add more than \u21131 + \u21132\u2212 1 configurations, and by Theorem 5.1 this suffices to find a plan with lower cost. Therefore the algorithm converges with probability 1.\nRemarks.\n1. Note the generality of the cost function.\n2. Shorter families of new configurations are found with higher probability.\n3. Longer tails increase the probability to find also long families, but slow down the simplex algorithm to solve the LP. In practice, a value slightly larger than the minimal value from theory (e.g. \u03b2 = 3) works well, and was in fact used on empirical grounds in [6]."
        },
        {
            "heading": "6 The multi-marginal case",
            "text": "The algorithm was originally introduced for multi-marginal problems [7, 6], and its adaptation to this case is straightforward. One now has N marginals \u00b51, . . . , \u00b5N on discrete spaces X1, . . . , XN of sizes \u21131, . . . , \u2113N . Plans are nonnegative functions on the product space X1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 XN to [0,\u221e), and one seeks to\nminimize \u3008c, \u03b3\u3009 := \u2211\n(x1,...,xN)\u2208X1\u00d7...\u00d7XN\nc(x1, ..., xN )\u03b3(x1, ..., xN )\nsubject to \u03b3 \u2208 \u03a0(\u00b51, ..., \u00b5N ).\n(MMOT)\nThis problem possesses an optimizer \u03b3 : X1 \u00d7 ... \u00d7XN \u2192 R supported on at most 1+\n\u2211N i=1(\u2113i\u2212 1) points. Starting from a feasible set of configurations \u2126 \u2282\nX1\u00d7\u00b7 \u00b7 \u00b7\u00d7XN satisfying |\u2126| \u2264 \u03b2 \u00b7 (\u21131+ . . .+ \u2113N), sampling of new configurations works as before: one picks an active configuration (parent) and proposes a child related to the parent, which is accepted if the gain \u2211N\ni=1 ui(xi) \u2212 c(x1, ..., xN ) is positive, where u = (u1, . . . , uN ), ui : Xi \u2192 R, is the current dual solution. Tail clearing is carried out whenever |\u2126| exceeds \u03b2 \u00b7 (\u21131 + ... + \u2113N ). Once a child has been accepted and tail clearing has been carried out if necessary, the reduced primal and dual problems on \u2126 are re-solved.\nTwo obvious generalizations of the search rule for children suggest themselves. Either children are proposed by fixing all but one entry or changing all but one entry of the parent configuration:\nGiven a parent (x1, . . . , xN ) \u2208 supp(\u03b3), pick a random child in N\u22c3\ni=1\n{x1} \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 {xi\u22121} \u00d7 supp(\u00b5i)\u00d7 {xi+1} \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 {xN} (8)\nor\nGiven a parent (x1, . . . , xN ) \u2208 supp(\u03b3), pick a random child in N\u22c3\ni=1\nsupp(\u00b51)\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 supp(\u00b5i\u22121)\u00d7 {xi} \u00d7 supp(\u00b5i+1)\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 supp(\u00b5N ). (9)\nFor N = 2 both rules reduce to (1). In practice, GenCol with the search rule (8) turned out to be an extremely fast and accurate method to solve high-dimensional OT problems. In various test examples with up to \u223c 1030 variables, it converged to a global optimum of the full problem using active sets of only a few thousand unknowns. However, it has the drawback that global convergence might fail; see section 6.2 for a counterexample. By contrast, for (9) we can prove global convergence. Unfortunately, this rule has the drawback that it is inefficient in practice due to the huge search space.\nLet us now see how much of the rigorous analysis from section 5 can be extended to the multi-marginal case. An extension of Theorem 5.1 on the number of steps needed to find an improving configuration is possible, but a different argument is required because the MMOT equivalent of c-cyclical monotonicity fails to provide a practical upper bound on the length of a cycle."
        },
        {
            "heading": "6.1 Tail clearing for MMOT",
            "text": "As before, let \u2126 \u2282 X1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 XN be a feasible subset of configurations r = (r1, ..., rN ), \u03b3 the current solution of the reduced problem (i.e. of (MMOT) with X1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 XN replaced by \u2126), and u1, ..., uN the current Kantorovich potentials. Assume \u03b3 is not optimal, then there exists \u03b3\u0303 with C[\u03b3\u0303] < C[\u03b3] and supp(\u03b3\u0303) \u2264 \u2211\ni(\u2113i \u2212 1) + 1, as shown in [6]. By construction supp(\u03b3\u0303) \u2229 \u2126 c 6= \u2205,\nbecause otherwise \u03b3 would not have been a current optimal solution. Hence there exists \u2126\u0303 \u2283 supp(\u03b3\u0303) with |\u2126\u0303| < \u2211\ni \u2113i.\nDefine \u2126\u0303\u2217 = {r \u2208 \u2126\u0303 : r /\u2208 \u2126}. We claim that there exists r\u2032 \u2208 \u2126\u0303\u2217, such that the dual certificate is violated, i.e. (u1\u2295\u00b7 \u00b7 \u00b7\u2295uN)(r\u2032) := u1(r\u20321)+ . . .+uN (r \u2032 N ) > c(r\u2032). We argue by contradiction. Assume u1 \u2295 \u00b7 \u00b7 \u00b7 \u2295 uN \u2264 c on \u2126\u0303. Then,\n\u222b\n\u2126\nc d\u03b3 =\n\u222b\n\u2126\nu1 \u2295 \u00b7 \u00b7 \u00b7 \u2295 uN d\u03b3 =\n\u222b\nX1\u00d7\u00b7\u00b7\u00b7\u00d7XN\nu1 \u2295 \u00b7 \u00b7 \u00b7 \u2295 uN d\u03b3\n=\nN\u2211\ni=1\n\u222b\nXi\nui d\u00b5i =\n\u222b\nX1\u00d7\u00b7\u00b7\u00b7\u00d7XN\nu1 \u2295 \u00b7 \u00b7 \u00b7 \u2295 uN d\u03b3\u0303\n=\n\u222b\n\u2126\u0303\nu1 \u2295 \u00b7 \u00b7 \u00b7 \u2295 uN d\u03b3\u0303 \u2264\n\u222b\n\u2126\u0303\nc d\u03b3\u0303.\nNow analogously to the argument for the two marginal case, re-solve the reduced problem on \u2126 \u222a {r\u2032}. Then either\ninf{C[\u03b7] : \u03b7 \u2208 \u03a0(\u00b51, . . . , \u00b5N ), supp(\u03b7) \u2282 \u2126 \u222a {r \u2032}} < C[\u03b3],\nor equality holds. In the latter case, \u03b3 is still optimal, but the dual solutions (u\u0303i) must be changed to satisfy the dual certificate u\u03031 \u2295 \u00b7 \u00b7 \u00b7 \u2295 u\u0303N \u2264 c on \u2126 \u222a {r\u2032}. By the same argument as before, we again find a configuration in \u2126\u0303\\(\u2126\u222a\n{r\u2032}) violating the dual certificate. Repeating, after at most |\u2126\u0303\u2217| steps, all configurations are added and \u03b3\u0303 is now an accessible solution, lowering the total cost.\nThis shows that, also in the multi-marginal case, the size of the reduced problems can be limited by \u03b2\u00b7(\u21131+. . .+\u2113N), justifying the tail-clearing procedure and memory efficiency of the algorithm. We summarize this finding in the following theorem.\nTheorem 6.1. Let \u03b3 be an extremal optimal solution for the reduced multimarginal optimal transport problem, and let (ui) be corresponding Kantorovich potentials. Assume \u03b3 is not optimal for the full MMOT problem. Then there exist configurations r(1), . . . , r(k) \u2208 \u2126c, k < \u2211\ni \u2113i, such that\ninf \u03b3\u0303\u2208\u03a0(\u00b51,...,\u00b5N )\nsupp(\u03b3\u0303)\u2282\u2126\u222a{r(1),...,r(k)}\nC[\u03b3\u0303] < C[\u03b3],\nand r(1), . . . , r(k) are accepted by GenCol\u2019s acceptance criterion.\nHowever, the improving configurations r(i) must be proposed by the genetic search rule. Thus we can only obtain a global convergence result for (9).\nCorollary 6.2. Suppose X1, . . . , XN are discrete spaces of finite cardinality, and the hyperparameter \u03b2 is \u2265 2. For any marginals, any cost function, and any feasible initial set \u2126 \u2282 X1\u00d7 \u00b7 \u00b7 \u00b7 \u00d7XN , GenCol with the search rule (9) converges with probability 1 to an exact solution of the multi-marginal OT problem (MMOT)."
        },
        {
            "heading": "6.2 A Counterexample",
            "text": "Our global convergence proof cannot simply be transferred to the multi-marginal case with the efficient search rule (8) where children differ from an active configuration by only 1 entry. We present a simple counterexample. Let X1 = X2 = X3 = {1, 2, 3} and N = 3. Let further\n\u00b51 = \u00b52 = \u00b53 :=\n3\u2211\nx=1\n1 3 \u03b4x.\nThe cost function is chosen to be\nc(x1, x2, x3) :=\n \n\n0 x1 = x2 = x3\n1 (x1 6= x2) \u2227 (x1 6= x3) \u2227 (x2 6= x3)\n2 else.\nThen the transport plan\n\u03b30 = 1\n3 (\u03b4(1,2,3) + \u03b4(2,3,1) + \u03b4(3,1,2))\nis a stationary state for GenCol. Obviously the global optimal plan is\n\u03b3\u22c6 = 1\n3 (\u03b4(1,1,1) + \u03b4(2,2,2) + \u03b4(3,3,3)).\nGenCol proposes new configurations by updating one entry of one active configuration. Independently of the dual solution, all possible configurations that can be proposed are\n(1, 2, 2), (1, 3, 3), (2, 2, 3), (3, 2, 3), (2, 1, 1), (2, 2, 1), (2, 3, 2), (2, 3, 3),\n(1, 3, 1), (3, 3, 1), (3, 2, 2), (3, 3, 2), (3, 1, 1), (3, 1, 3), (1, 1, 2), (2, 1, 2).\nThe cost for all of them is 2, while the cost for all active configurations in \u03b30 is only 1. Hence for any subset of the configurations listed above added to the active configurations in \u03b30, the optimal plan is again \u03b30. Therefore the current solution \u03b30 never changes and \u03b3\n\u2217 cannot be reached. The example is designed so that one would have to update two entries of an active configuration to reduce the cost. Any update in just one entry increases the cost.\nSome interesting properties of this example are:\n\u2022 The problem is symmetric (i.e., all marginals are equal and the cost is symmetric in its variables), like the Coulomb OT problem arising in electronic structure.\n\u2022 One can replace {1, 2, 3} by a convex independent set (i.e. a set all of whose points are extreme points), in which case the cost c can even be chosen convex."
        },
        {
            "heading": "7 Conclusions",
            "text": "We rigorously justified the GenCol algorithm in the two-marginal case for arbitrary costs and marginals, showing that it avoids non-minimizing stationary states despite maintaining sparsity.\nFor the multi-marginal case, we rigorously justified the algorithm provided the search rule finds the required configurations described in Theorem 6.1 with positive probability. Thus GenCol rigorously reduces the storage cost from exponential to linear in the number of marginals. However, the number of search steps might be exponentially large.\nIt is an interesting open problem whether the efficient search rule (8) (which only requires quadratically many search steps in the number of marginals) or any similarly efficient modification can be rigorously justified, at least for costs of practical interest like the Coulomb cost or the Wasserstein barycenter cost."
        }
    ],
    "title": "Convergence proof for the GenCol algorithm in the case of two-marginal optimal transport",
    "year": 2023
}