{
    "abstractText": "A simple model to study subspace clustering is the high-dimensional k-Gaussian mixture model where the cluster means are sparse vectors. Here we provide an exact asymptotic characterization of the statistically optimal reconstruction error in this model in the high-dimensional regime with extensive sparsity, i.e. when the fraction of non-zero components of the cluster means \u03c1, as well as the ratio \u03b1 between the number of samples and the dimension are xed, while the dimension diverges. We identify the information-theoretic threshold below which obtaining a positive correlation with the true cluster means is statistically impossible. Additionally, we investigate the performance of the approximate message passing (AMP) algorithm analyzed via its state evolution, which is conjectured to be optimal among polynomial algorithm for this task. We identify in particular the existence of a statistical-to-computational gap between the algorithm that require a signal-to-noise ratio \u03bbalg \u2265 k/ \u221a \u03b1 to perform better than random, and the information theoretic threshold at \u03bbit \u2248 \u221a \u2212k\u03c1 log \u03c1/ \u221a \u03b1. Finally, we discuss the case of sub-extensive sparsity \u03c1 by comparing the performance of the AMP with other sparsity-enhancing algorithms, such as sparse-PCA and diagonal thresholding.",
    "authors": [
        {
            "affiliations": [],
            "name": "Luca Pesce"
        },
        {
            "affiliations": [],
            "name": "Bruno Loureiro"
        },
        {
            "affiliations": [],
            "name": "Florent Krzakala"
        },
        {
            "affiliations": [],
            "name": "Lenka Zdeborov\u00e1"
        }
    ],
    "id": "SP:9f0fee584ab2afebc4ab29207b74c891109f99e7",
    "references": [
        {
            "authors": [
                "Phillip Pope",
                "Chen Zhu",
                "Ahmed Abdelkader",
                "Micah Goldblum",
                "Tom Goldstein"
            ],
            "title": "The intrinsic dimension of images and its impact on learning, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Thibault Lesieur",
                "Florent Krzakala",
                "Lenka Zdeborov\u00e1"
            ],
            "title": "Constrained low-rank matrix estimation: phase transitions, approximate message passing and applications",
            "venue": "Journal of Statistical Mechanics: Theory and Experiment,",
            "year": 2017
        },
        {
            "authors": [
                "Marc Lelarge",
                "L\u00e9o Miolane"
            ],
            "title": "Fundamental limits of symmetric low-rank matrix estimation",
            "venue": "In Satyen Kale and Ohad Shamir, editors, Proceedings of the 30th Conference on Learning Theory, COLT 2017,",
            "year": 2017
        },
        {
            "authors": [
                "L\u00e9o Miolane"
            ],
            "title": "Fundamental limits of low-rank matrix estimation: the non-symmetric case, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Yash Deshpande",
                "Andrea Montanari"
            ],
            "title": "Information-theoretically optimal sparse pca",
            "venue": "IEEE International Symposium on Information Theory,",
            "year": 2014
        },
        {
            "authors": [
                "Alyson K Fletcher",
                "Sundeep Rangan"
            ],
            "title": "Iterative reconstruction of rank-one matrices in noise",
            "venue": "Information and Inference: A Journal of the IMA,",
            "year": 2018
        },
        {
            "authors": [
                "Michael Celentano",
                "Andrea Montanari",
                "Yuchen Wu"
            ],
            "title": "The estimation error of general rst order methods, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Michael Celentano",
                "Chen Cheng",
                "Andrea Montanari"
            ],
            "title": "The high-dimensional asymptotics of rst order methods with random data, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Iain M. Johnstone",
                "Arthur Yu Lu"
            ],
            "title": "On consistency and sparsity for principal components analysis in high dimensions",
            "venue": "Journal of the American Statistical Association,",
            "year": 2009
        },
        {
            "authors": [
                "Jiashun Jin",
                "Zheng Tracy Ke",
                "Wanjie Wang"
            ],
            "title": "Phase transitions for high dimensional clustering and related problems, 2016",
            "year": 2016
        },
        {
            "authors": [
                "Lance Parsons",
                "Ehtesham Haque",
                "Huan Liu"
            ],
            "title": "Subspace clustering for high dimensional data: a review",
            "venue": "SIGKDD Explor.,",
            "year": 2004
        },
        {
            "authors": [
                "Jinho Baik",
                "G\u00e9rard Ben Arous",
                "Sandrine"
            ],
            "title": "P\u00e9ch\u00e9. Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices",
            "venue": "The Annals of Probability,",
            "year": 2005
        },
        {
            "authors": [
                "Mohamad Dia",
                "Nicolas Macris",
                "Florent Krzakala",
                "Thibault Lesieur",
                "Lenka Zdeborov\u00e1"
            ],
            "title": "Mutual information for symmetric rank-one matrix estimation: A proof of the replica formula",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Ahmed El Alaoui",
                "Michael I Jordan"
            ],
            "title": "Detection limits in the high-dimensional spiked rectangular model",
            "venue": "In Conference On Learning Theory,",
            "year": 2018
        },
        {
            "authors": [
                "Ahmed El Alaoui",
                "Florent Krzakala",
                "Michael Jordan"
            ],
            "title": "Fundamental limits of detection in the spiked wigner model",
            "venue": "The Annals of Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Ryosuke Matsushita",
                "Toshiyuki Tanaka"
            ],
            "title": "Low-rank matrix reconstruction and clustering via approximate message passing",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2013
        },
        {
            "authors": [
                "Thibault Lesieur",
                "Caterina de Bacco",
                "Jess Banks",
                "Florent Krzakala",
                "Cris Moore",
                "Lenka Zdeborova"
            ],
            "title": "Phase transitions and optimal algorithms in high-dimensional gaussian mixture clustering",
            "venue": "54th Annual Allerton Conference on Communication, Control, and Computing (Allerton),",
            "year": 2016
        },
        {
            "authors": [
                "Mohamed Ndaoud"
            ],
            "title": "Sharp optimal recovery in the two-component gaussian mixture model, 2020",
            "year": 2020
        },
        {
            "authors": [
                "S. Lloyd"
            ],
            "title": "Least squares quantization in pcm",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 1982
        },
        {
            "authors": [
                "Martin Azizyan",
                "Aarti Singh",
                "Larry Wasserman"
            ],
            "title": "Minimax theory for high-dimensional gaussian mixtures with sparse mean separation",
            "year": 2013
        },
        {
            "authors": [
                "Nicolas Verzelen",
                "Ery Arias-Castro"
            ],
            "title": "Detection and feature selection in sparse mixture models, 2016",
            "year": 2016
        },
        {
            "authors": [
                "Jianqing Fan",
                "Han Liu",
                "Zhaoran Wang",
                "Zhuoran Yang"
            ],
            "title": "Curse of heterogeneity: Computational barriers in sparse mixture models and phase retrieval, 2018",
            "year": 2018
        },
        {
            "authors": [
                "Matthew Brennan",
                "Guy Bresler"
            ],
            "title": "Average-case lower bounds for learning sparse mixtures, robust estimation and semirandom adversaries, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Martin Azizyan",
                "Aarti Singh",
                "Larry Wasserman"
            ],
            "title": "E cient sparse clustering of high-dimensional non-spherical gaussian mixtures",
            "year": 2014
        },
        {
            "authors": [
                "David L. Donoho",
                "Arian Maleki",
                "Andrea Montanari"
            ],
            "title": "Message-passing algorithms for compressed sensing",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2009
        },
        {
            "authors": [
                "F. Krzakala",
                "M. M\u00e9zard",
                "F. Sausset",
                "Y.F. Sun",
                "L. Zdeborov\u00e1"
            ],
            "title": "Statistical-physics-based reconstruction in compressed sensing",
            "venue": "Phys. Rev. X,",
            "year": 2012
        },
        {
            "authors": [
                "Jean Barbier",
                "Florent Krzakala",
                "Nicolas Macris",
                "Leo Miolane",
                "Lenka Zdeborova"
            ],
            "title": "Optimal errors and phase transitions in high-dimensional generalized linear models",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2019
        },
        {
            "authors": [
                "Benjamin Aubin",
                "Bruno Loureiro",
                "Antoine Maillard",
                "Florent Krzakala",
                "Lenka Zdeborov\u00e1"
            ],
            "title": "The spiked matrix model with generative priors",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2021
        },
        {
            "authors": [
                "Benjamin Aubin",
                "Bruno Loureiro",
                "Antoine Baker",
                "Florent Krzakala",
                "Lenka Zdeborov\u00e1"
            ],
            "title": "Exact asymptotics for phase retrieval and compressed sensing with random generative priors",
            "venue": "Proceedings of The First Mathematical and Scienti c Machine Learning Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Adel Javanmard",
                "Andrea Montanari"
            ],
            "title": "State evolution for general approximate message passing algorithms, with applications to spatial coupling",
            "year": 2012
        },
        {
            "authors": [
                "Ryosuke Matsushita",
                "Toshiyuki Tanaka"
            ],
            "title": "Low-rank matrix reconstruction and clustering via approximate message passing",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2013
        },
        {
            "authors": [
                "Andrea Montanari",
                "Ramji Venkataramanan"
            ],
            "title": "Estimation of low-rank matrices via approximate message passing",
            "venue": "The Annals of Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Marco Mondelli",
                "Christos Thrampoulidis",
                "Ramji Venkataramanan"
            ],
            "title": "Optimal combination of linear and spectral estimators for generalized linear models, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Federico Ricci-Tersenghi",
                "Guilhem Semerjian",
                "Lenka Zdeborov\u00e1"
            ],
            "title": "Typology of phase transitions in bayesian inference problems",
            "venue": "Physical Review E,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "\u221a \u03b1 to perform better than random, and the information theoretic threshold\nat \u03bbit \u2248 \u221a \u2212k\u03c1 log \u03c1/ \u221a \u03b1. Finally, we discuss the case of sub-extensive sparsity \u03c1 by comparing the performance of the AMP with other sparsity-enhancing algorithms, such as sparse-PCA and diagonal thresholding."
        },
        {
            "heading": "1 Introduction",
            "text": "With the growing size of modern data, clustering techniques play an important role in reducing the dimensionality of the features used in modern Machine Learning pipelines. Indeed, in many tasks of interest ranging from DNA sequence analysis to image classi cation, the relevant features are known to live in a lower-dimensional space (intrinsic dimension) than their raw acquisition format (extrinsic dimension) [1]. In these cases, identifying these features can help saving computational resources while signi cantly improving on learning performance. But given a corrupted embedding of low-dimensional features in a high-dimensional space, is it always statistically possible to retrieve them? And if yes - how can reconstruction be achieved e ciently in practice? In this manuscript we address these two fundamental questions in a simple model for subspace clustering: a k-cluster Gaussian mixture model with sparse centroids. In this model, the low-dimensional hidden features are given by the sparse centroids, which are embedded in a higher dimensional space and corrupted by additive Gaussian noise. We assume that the number of non-zero components of the centroids as well as the number of samples scales linearly with the dimension of the embedding space. Given a nite sample from the mixture, the goal of the statistician is to cluster the data, i.e. estimate the centroids (or features) as well as possible."
        },
        {
            "heading": "2 Model & setting",
            "text": "Let x\u03bd \u2208 Rd, \u03bd \u2208 [n] := {1, \u00b7 \u00b7 \u00b7 , n} denote n i.i.d. data points drawn from an isotropic k-cluster Gaussian mixture:\nx\u03bd \u223ci.i.d. \u2211 c\u2208C pcN (\u221a \u03bb/s\u00b5c, Id ) , \u03bd \u2208 [n] (1)\nwhere pc are the class probabilities, C is the index set (|C| = k), \u00b5c \u2208 Rd are the s-sparse vectors representing the means of the clusters and \u03bb is a measure of the signal-to-noise ratio (SNR) for this model. In the following, we focus in the balanced case for which pc = 1/k. We select the subspace of relevant features thanks to the introduction of the vector\nar X\niv :2\n20 5.\n13 52\n7v 2\n[ st\nat .M\nL ]\n1 D\nec 2\n02 2\nvi \u2208 Rk . The projection of all the cluster means, on a given dimension i \u2208 [d], will be completely determined by a linear combination of the components of vi. We consider for this purpose a Gauss-Bernoulli distribution:\nvi \u223ci.i.d. \u03c1N(0, Ik) + (1\u2212 \u03c1)\u03b40 (2)\nwhere \u03c1 := s/d is the density of non-zero elements. For the convenience of the theoretical analysis that follows, it will be useful to work with a particular encoding of the class labels C. For a given sample \u03bd \u2208 [n] belonging to the class c \u2208 C, de ne the following label indicator vector u\u03bdc \u2208 {\u2212 1k , k\u22121 k } k:\nu\u03bdc = 1 k (\u22121, . . . ,\u22121, k \u2212 1\ufe38 \ufe37\ufe37 \ufe38\nc-th element\n,\u22121, . . . ,\u22121) (3)\nFor a given draw (x\u03bd)\u03bd\u2208[n], de ne the matrices X \u2208 Rd\u00d7n with columns given by x\u03bd and U \u2208 Rn\u00d7k,V \u2208 Rd\u00d7k with rows given by u\u03bdc and vi.\nA crucial point that we shall exploit in this paper is that, with this notation, our model for subspace clustering can be written as a matrix factorization problem, where the data has been generated as:\nX = \u221a \u03bb\ns VU> + W (4)\nwith W \u2208 Rd\u00d7n a Gaussian matrix with elements Wi\u03bd \u223c N(0, 1). This formulation is completely equivalent to the one in eq. (1), by identifying the cluster means as given component-wise by: \u00b5(i)c = v>i uc, i \u2208 [d]. The centroids will have, in expectation, only s non-zero components; as discussed in the introduction, the s-sparse class means {\u00b5c}c\u2208C can be thought of as low-dimensional features embedded in a higher-dimensional space Rd, that have been corrupted by isotropic additive Gaussian noise with variance \u223c \u03bb\u22121. Given a nite draw (x\u03bd)\u03bd\u2208[n] generated from this model with class means V? and labels U?, the goal of the statistician is to perform clustering, i.e. to estimate U? from X, which is equivalent to retrieving the class label for each sample in the data. However, note that there is a clear class symmetry in this problem: the labels can only be estimated up to a permutation \u03c0 \u2208 Sym(C) reshu ing the columns of U?. Taking this symmetry into account we can assess the performance of an estimator U\u0302 through the averaged symmetrized mean-squared error:\nMSE(U\u0302) = min \u03c0\u2208Sym(C)\n1 n E||\u03c0(U\u0302)\u2212 U?||F (5)\nwhere || \u00b7 ||F denotes the matrix Frobenius norm. In particular, we will be interested in characterizing reconstruction in the proportional high-dimensional limit where the number of samples n, the ambient dimension d and the sparsity level s go to in nity n, d, s \u2192 \u221e at xed density \u03c1 = s/d, sample complexity \u03b1 = n/d, number of clusters k \u2265 2 and signal strength \u03bb > 0.\nNote that the clustering problem above is closely related to the problem of estimating the class means / features V?. Indeed, written as in eq. (4) the problem of estimating both the labels and centroids (U?,V?) boils down to a low-rank matrix factorization problem. In this manuscript, we have chosen to frame the discussion in terms of the clustering, but all our results could be presented also in terms of the reconstruction of the class means.\nIn this manuscript we provide a sharp asymptotic characterization of when reconstruction, as measured by positive correlation with the ground truth, is possible in high-dimensions for this model, both statistically and algorithmically. In particular, our main contributions are: \u2022 We map the subspace clustering problem to an asymmetric matrix factorization problem. We compute the closed-form asymptotic formula for the performance of the Bayesian-optimal estimator in the high-dimensional limit, building on a statistical physics inspired approach [2] that has been rigorously proven in this case [3, 4] . This allows us to provide a sharp threshold bellow which reconstruction of the features is statistically impossible as a function of the parameters of the model. \u2022 To estimate the algorithmic limitations of reconstruction, we analyse a tailored approximate message passing (AMP) algorithm [2, 5, 6] approximating the posterior marginals in this problem, and derive the associated state evolution equations characterising its asymptotic performance. Such algorithms are optimal among rst order methods [7, 8]) and therefore their reconstruction threshold provides a bound on the algorithmic complexity clustering in our model. \u2022 The two results above allow us to paint a full picture of the statistical-to-algorithmic trade-o s in high-dimensional subspace clustering for the sparse k-Gaussian mixture model, and in particular to identify an algorithmically hard region of the sparsity level (1 \u2212 \u03c1) vs. signal-to-noise ratio (\u03bb) plane for which reconstruction is possible statistically but not algorithmically, see Fig. 1. Further, we provide a detailed analysis in the high sparsity regime (\u03c1 \u2192 0+) of how the algorithmically hard region grows as we increase the number of clusters and the sparsity level. In particular, the information theoretical transition arises at \u03bbit \u2248 \u221a \u2212k\u03c1 log \u03c1/ \u221a \u03b1, and the algorithmic one at \u03bbalg \u2265 k/\u221a\u03b1.\n\u2022 The analysis for AMP optimality relies on the nite \u03c1 assumption as n, d \u2192 \u221e. We thus also investigate the complementary case when the number of non-zero components is of the order s . n and indeed see that sparse principal component analysis (SPCA) and Diagonal thresholding [9] can perform better than random in this region. We nd, however, that this requires s \u2264 \u221a n (thus \u03c1 = o(1)). We rephrase our ndings in terms of existing literature on the subspace clustering for two-classes Gaussian mixtures [10, 11].\nRelated works: Subspace clustering is a well-studied topic in classical statistics, with a wide range of methods used in practice, see [12] for a review. Closer to this work is the theoretical line of work studying the limitations of clustering in high-dimensions. Baik, Ben Arous and P\u00e9ch\u00e9 have shown that PCA for Gaussian mixture clustering fails to correlate with the mixture means below certain threshold known as the BBP transition [13]. For dense Gaussian means, the statistical and algorithmic limitations of clustering were analysed in di erent regimes of interest. Our approach to study subspace clustering relies on a mapping to a low-rank matrix factorization problem. Low-rank matrix factorization has been extensively studied in the literature, and its asymptotic Bayes-optimal performance was characterized in [2\u20135, 14\u201316]. The construction of AMP algorithms and the associated state evolutions for matrix factorization was done in [2, 5, 6, 17]. In this work, we leverage these general results on matrix factorization. The closest to our work is perhaps [2, 18], where the authors characterize the asymptotic reconstruction thresholds for the dense case (\u03c1 = 1) in the proportional limit where the number of samples and input dimensions diverge at a constant rate. Non-asymptotic results were also discussed in [19] which considered a modi cation of Lloyd\u2019s algorithm [20] achieving minimax optimal rate and proving a computational lower bound. To the best of our knowledge, the case in which the means are sparse has only been analysed in the regime where the number of non-zero components is sub-extensive with respect to the input dimension. Lower bounds for the statistically optimal recovery threshold in this regime were given in [21, 22], while computational lower bounds are studied in [23, 24]. Clustering of two component Gaussian mixtures has been studied in order to shed light on comparison between statistical and algorithmical tractability in a sparse scenario in [10, 11, 25]. In particular, [10] conjectured and [11] proved algorithmic bounds for this problem. They claim that even below the BBP threshold they can build an algorithm achieving exponentially small misclustering error, given that (up-to log factors) we have s . \u221a n. We relate our ndings to their work exploring the extreme sparsity regime in detail."
        },
        {
            "heading": "3 Main theoretical results",
            "text": "In this section, we introduce the two main technical results allowing us to characterize the limitations of clustering reconstruction (both statistically and algorithmically) for the model introduced above in the proportional high-dimensional\nlimit.\nStatistical reconstruction: First, note that up to the permutation symmetry, the estimator minimizing the averaged mean-squared error in eq. (5) admits a closed-form solution given by the marginals of the posterior distribution:\nU\u0302bo = arg min U\u2208Rn\u00d7k MSE(U) = E [U|X] (6)\nwhere the posterior distribution for the model de ned in eq. (4) explicitly reads:\nP (U |X) = 1 Z(X) n\u220f \u03bd=1 Pu(u\u03bd) \u222b Rk d\u220f i=1 (dviPv(vi)) n\u220f \u03bd=1 d\u220f i=1 e \u2212 12 ( X\u03bdi\u2212 \u221a \u03bb su > \u03bd vi )2 (7)\nand for convenience we de ned the vectors u\u03bd ,vi \u2208 Rk which are the rows of U,V, and with the prior distribution Pu being the uniform distribution over the indicator vectors de ned in eq. (3) and Pv given by the Gauss-Bernouilli distribution de ned in eq. (2).\nAlthough in principle it would be possible to compute the minimum mean-squared error (MMSE) given by the Bayes-optimal estimator in eq. (6) by sampling from the posterior from eq. (7), this is impractical when d, n are large. For instance, simply computing the di erent integrals involved in the evidence Z scale exponentially with the dimensions. The rst result consists precisely in a closed-form solution for the asymptotic performance of the Bayes-optimal estimator:\nMain theoretical result 1. In the proportional high-dimensional limit where n, d, s \u2192 \u221e with xed ratios \u03c1 = s/d, \u03b1 = n/d and xed \u03bb, k, the minimum mean-squared error for the reconstruction of U \u2208 Rn\u00d7k is given by:\nlim n\u2192\u221e MMSE = k \u2212 1 k \u2212 TrM?u (8)\nwhere M?u \u2208 Rk\u00d7k is the solution of the following minimization problem:\nM?u = arg min Mu\u2208Rk\u00d7k\n{ max\nMv\u2208Rk\u00d7k \u03a6rs (Mu,Mv)\n} . (9)\nwith:\n\u03a6rs(Mu,Mv) = \u03b1\u03bbTrMuMv\n2\u03c1 \u2212 Ev\u2217,w\n[ logZv ( \u03b1\u03bbMu \u03c1 , \u03b1\u03bbMuu\u2217 \u03c1 + \u221a \u03b1\u03bbMu \u03c1 w )]\n\u2212 \u03b1Eu\u2217,w [ logZu ( \u03bbMv \u03c1 , \u03bbMvv\u2217 \u03c1 + \u221a \u03bbMv \u03c1 w )] (10)\nwhere we introduced u\u2217 \u223c Pu,v\u2217 \u223c Pv,w \u223c N(0, Ik), and we de ned Zu/v as:\nZu(A, b) = 1\nk \u2211 c\u2208C exp ( b>uc \u2212 1 2 u>c Auc ) (11)\nZv(A, b) = 1\u2212 \u03c1+ \u03c1 exp ( b>(Ik + A)\u22121b\n2\n)\u221a det (Ik + A) \u22121 (12)\nResult 1 follows from our mapping of the subspace clustering problem introduced in Sec. 2 to a low-rank matrix factorization form eq. (4). Indeed, this mapping allows us to leverage a closed-form formula characterizing the asymptotic MMSE for low-rank matrix estimation with generic priors that was derived heuristically [2, 18] using the replica method from Statistical Physics and was rigorously proven in a series of works [3\u20135, 14] in the context of subspace clustering. Our contribution resides in making this connection and drawing the consequences for the subspace clustering problem, a non-trivial endeavour given the complexity of the resulting formulas. Note that the minimization problem eq. (9) is fundamentally di erent from the one in eq. (6). Indeed, the rst involves only low-dimensional variables, and can be easily solved in a computer, while the latter is a high-dimensional problem which is computationally intractable for large d, n. The other parameter M?v \u2208 Rk\u00d7k solving eq. (9) can be used to characterize the MMSE reconstruction error on V.\nAlgorithmic reconstruction: While result 1 allow us to sharply characterize when clustering is statistically possible, it does not provide us a practical way to estimate the true class labels U? \u2208 Rn\u00d7k from the data X \u2208 Rd\u00d7n. In order to provide a bound in the algorithmic complexity clustering, we consider an approximate message passing (AMP) algorithm for our problem. Message passing algorithms are a class of rst order algorithms (scaling as O(nd), the dimensions of the input matrix X) designed to approximate the marginals of a target posterior distribution, and which have two very important features. First, for a large class of random problems (such as the clustering problem studied here) AMP provides the best known rst order method in terms of estimation performance [26\u201330], and has been rigorously proven to be the optimal for certain problems [7, 8]. Second, the asymptotic performance of AMP can be tracked by a set of low-dimensional state evolution equations [31], meaning that its reconstruction performance can be sharply computed without having to run a high-dimensional instance of the problem. For low-rank matrix factorization problems an associated AMP algorithm can be derived [5, 6, 17, 32, 33]. Therefore, yet again we leverage the mapping of subspace clustering to a low-rank matrix factorization problem to derive the associated AMP algorithm 1 with denoising functions \u03b7v, \u03b7u:\n\u03b7u(A, b) = 1\u2211k c=1 exp ( b>uc \u2212 u > c Auc 2 ) k\u2211 c=1 uc exp ( b>uc \u2212 u>c Auc 2 ) (13)\n\u03b7v(A, b) = (Ik +A)\u22121b \u03c1+ (1\u2212 \u03c1) \u221a det (Ik + A) exp ( \u2212b>(Ik+A)\u22121b\n2 ) (14) As mentioned above, one of they key features of Algorithm 1 is that its asymptotic performance can be tracked exactly by a set of low-dimensional equations.\nMain theoretical result 2. In the proportional high-dimensional limit where n, d, s \u2192 \u221e with xed ratios \u03c1 = s/d, \u03b1 = n/d and xed \u03bb, k, the correlation between the ground truth (U?,V?) and the AMP estimators (U\u0302 t amp, V\u0302 t amp) at iterate t,\nMtu = 1\nn U>? U\u0302\nt amp, M t v =\n1 n V>? V\u0302 t amp (15)\nsatisfy the following state evolution equations:\nMt+1u = Eu\u2217\u223cPu,\u03be\u223cN(0k,Ik) [ \u03b7u ( \u03b1\u03bbMu \u03c1 , \u03b1\u03bbMuu\u2217 \u03c1 + \u221a \u03b1\u03bbMu \u03c1 w ) u>\u2217 ] (16)\nMt+1v = Ev\u2217\u223cPv,\u03be\u223cN(0k,Ik) [ \u03b7v ( \u03bbMv \u03c1 , \u03bbMvv\u2217 \u03c1 + \u221a \u03bbMv \u03c1 w ) v>\u2217 ] (17)\nMoreover, the asymptotic performance of is simply given by:\nlim n\u2192\u221e\nMSE(U\u0302 t amp) = k \u2212 1 k \u2212 TrMtu (18)\nResult 2 is a consequence from the general theory connecting AMP algorithms to their state evolution (SE) [31]. In the context of low-rank matrix factorization, a derivation of the state evolution equations above from Algorithm 1 were rst provided for the rank-one case in [5, 6] and were extended to general rank and denoising functions in [2, 18]. A crucial observation is that Result 2 is intimately related to Result 1. Indeed, the state evolution equations (17) coincide exactly with running gradient descent on the potential de ned in eq. (10). While the performance of the statistically optimal estimator U\u0302bo is given by the xed point with the minimal value of the potential \u03a6rs, the performance of the AMP estimator U\u0302amp is described by the closest minima to the initialization Mt=0u . Therefore, studying both the statistical and algorithmic limitations of clustering in high-dimensions boils down to the study of the minima of the potential \u03a6rs, see App. A for further details. The identi cation of a subspace clustering problem with a matrix factorization one allows us to leverage the rich literature from this eld. However, analyzing these formulas is far from trivial and consitutes a considerable technical challenge. The major di culty is to nd a suitable parametrization of the overlap matrices that reduces the number of parameters to be tracked, we discuss this in detail in Sec. 5. Moreover, we nd the scaling ansatz in the high-sparsity limit that closes the equations on amenable quantities in order to nd analytically the statistical-to-computational gap for the large-rank setting, see Sec. 6. However, we stress that we did not take all the necessary precautions to claim full rigor. e.g. prove formally that the minimum is unique (although we checked all these both analytically and numerically).\nAlgorithm 1 low-rAMP Input: Data X \u2208 Rd\u00d7n Initialize v\u0302t=0i , u\u0302t=0\u03bd \u223c N(0k, Ik), \u03c3\u0302t=0u,\u03bd = 0k\u00d7k , \u03c3\u0302t=0v,i = 0k\u00d7k . for t \u2264 tmax do\nAtu = \u03bbs ( U\u0302t )> U\u0302, Atv = \u03bbs ( V\u0302t )> V\u0302\nBtv = \u221a \u03bb sXU\u0302 t \u2212 \u03bbs n\u2211 \u03bd=1 \u03c3tu,\u03bdV\u0302 t\u22121, Btu = \u221a \u03bb sX >V \u2212 \u03bbs d\u2211 i=1 \u03c3tv,iU\u0302 t\u22121 Take {btv,i \u2208 Rk}di=1, {btu,\u03bd \u2208 Rk}n\u03bd=1 rows of Btv,Btu v\u0302t+1i = \u03b7v(Atv, btv,i), u\u0302t+1\u03bd = \u03b7u(Atu, btu,\u03bd) \u03c3\u0302t+1v,i = \u2202b\u03b7v(Atv, btv,i), \u03c3\u0302t+1u,\u03bd = \u2202b\u03b7u(Atu, btv,\u03bd) Here U\u0302t \u2208 Rn\u00d7k, V\u0302t \u2208 Rd\u00d7k,Btu \u2208 Rn\u00d7k,Btv \u2208 Rd\u00d7k,Atu \u2208 Rk\u00d7k,Atv \u2208 Rk\u00d7k\nend for Return: Estimators v\u0302amp,i, u\u0302amp,\u03bd \u2208 Rk, \u03c3\u0302u,\u03bd , \u03c3\u0302v,i \u2208 Rk\u00d7k"
        },
        {
            "heading": "4 Reconstruction limits for sparse clustering",
            "text": "These results allow us to paint a full picture for when sparse subspace clustering is possible in the model de ned in Sec. 2 as a function of the quantity of data \u03b1, the sparsity 1\u2212\u03c1, the number of clusters k and the SNR \u03bb. Figure 1 summarizes the di erent reconstruction regimes in the (\u03c1, \u03bb) plane for a two-clusters problem at xed sample complexity \u03b1=2, also known as a phase diagram. Moreover the general considerations on the reconstruction limits for sparse clustering, given by analyzing the two-clusters problem, are easily generalizable to the general mixture case and not restricted to that particular model, see Sec. 5. For a xed sparsity 1\u2212\u03c1, we identify the following regions in Fig. 1: Impossible phase: There is not enough information in the data matrix X handled to the statistician to assign cluster membership better than chance for any algorithm. The Bayes-optimal MMSE is not better than a random guess. Clustering (reconstruction of U? better than chance) is impossible.\nHard phase: The MMSE is non-trivial, and clustering is statistically possible to some extent, but the best known polynomial time algorithm, AMP, fails to correlate better than chance with the true cluster assignment U?. Any polynomial-time algorithm is conjectured to fail in this region.\nEasy phase: In the easy phase, not only clustering is statistically possible, but AMP is able to achieve positive correlation with U?. One can also investigate when AMP achieves the Bayes-optimal MMSE (instead of just positive correlation) leading to the same transition (except from a subtle correction very close to the tri-critical point, see the discussion in App. A). In Fig. 2, we investigate the di erent phases presented above by varying the sparsity level 1 \u2212 \u03c1 and SNR \u03bb at xed \u03b1. We compare the performance of AMP with popular spectral algorithms: Principal Component Analysis (PCA) and Sparse Principal Component Analysis (SPCA). We can initialize AMP and the SE equations in two di erent ways: we call uninformative initialization a choice for the rst iterates of AMP and SE which assumes no knowledge on the ground truth values; conversely with informative initialization we consider that the statistician has some prior knowledge on the the ground truth signal. Note that in a real-life scenario the statistician does not have access to the ground truth. Yet, as a theoretical tool the informative initialization provides important information about the algorithmically hard phase, see App A for a discussion. The initialization strategy we considered in the uninformed case for Algorithm 1 is not the only possible choice, there are smarter ways of initialising which can lead to a considerable improvement without explicitly assuming any information about the signal, e.g. spectral initialization [34]. Looking at Fig. 2 we see that SE with uninformed initialization tracks AMP. Morover we note that increasing the sparsity level, i.e. decreasing \u03c1, the problem becomes algorithmically harder. This is re ected in a discontinous jump in the MSE at \u03bbalg which becomes larger. We observe along the same lines a neat advantage by imposing the sparsity constraint in the spectral algorithm (SPCA) with respect to vanilla one (PCA) as the sparsity grows. We discuss the details on the numerical simulations in App. D and the code is available at https://github.com/lucpoisson/SubspaceClustering."
        },
        {
            "heading": "5 Stability analysis and algorithmic threshold",
            "text": "In this section we provide a detailed analytical derivation of the threshold \u03bbalg characterizing the algorithmic reconstruction as a function of the number of clusters and the sparsity. First, note due to the permutation symmetry of the clusters\nthe overlap matrices admit the following parametrization:\nMtu = mtu k Ik \u2212 mtu k2 Jk Mtv = mtvIk \u2212 mtv k Jk (19)\nwhere Jk is the k \u00d7 k-matrix with all elements equal to one. This parametrization is preserved under the SE iterations. Therefore, inserting it in the SE equations yield equations for (mtu,mtv):\nmt+1u = f (k) u (\u03bbm t v/\u03c1) m t+1 v = f (k) v (\u03b1\u03bbm t u/\u03c1) (20)\nwhere we introduced the following update functions:\nf (k)u (z) := k\nk \u2212 1 E\u03c9\u223cN(0k,Ik)\n[ ez+w1 \u221a z\ne\u03bbm t v+w1 \u221a z + \u2211k l=2 e wl \u221a z ] \u2212 1 k \u2212 1\n(21)\nf (k)v (z) := \u03c12z\nk(k + z) \u222b +\u221e 0 Sk\u22121 (2\u03c0) k 2\n\u03bek+1e\u2212\u03be 2/2\n\u03c1+ (1\u2212 \u03c1)(k+zk ) k 2 e\u2212\n\u03be2z 2k\nd\u03be (22)\nwhere Sk\u22121 is the surface of the k-dimensional unitary hypersphere. Recall that (mtu,mtv)\u2208 [0, 1]2 fully characterize the reconstruction performance, with (m\u221eu ,m\u221ev )=(0, 0) corresponding to the performance of a random guess. Conversely, (m\u221eu ,m \u221e v )=(1, \u03c1) corresponds to perfect reconstruction of the cluster membership and the sparse cluster means. One can check that the trivial xed point (mu,mv)=(0, 0) is always a xed point of the SE equations. Moreover, note that its stability is crucially connected to the algorithmic threshold \u03bbalg. Indeed, if the trivial xed point (mu,mv)=(0, 0) is stable, AMP with an uninformed initialization will always converge to this point - and therefore achieve random guessing performance. To study its stability, we expand the update functions around (mu,mv)=(0, 0) up to the second order. Expressing everything in terms of mu, we obtain:\nmt+1u = Fse ( mtu ) = \u03bb2\u03b1\nk2 mtu +\n( (k \u2212 4)\u03b12\u03bb4\n2k4 \u2212 \u03bb\n3\u03b12\nk3\n)( mtu )2 + o ( (mtu) 2 )\n(23)\nThis immediately tells us that the trivial xed point becomes unstable at the algorithmic threshold \u03bbalg = k/\u221a\u03b1. This transition is a well-known result in random matrix theory and goes under the name of BBP transition [13]. Despite the\nfact that we expand around the trivial xed point, the perturbative method also provides information also about the Bayesoptimal performance, thanks to the general properties that the phase diagrams in Bayes-optimal inference problems must respect [35]. Indeed, a su cient criterion for the presence of an algorithmically hard phase requires at the algorithmic threshold: F \u2032\u2032se(0) > 0. The local study predicts that the phase diagram will present an algorithmically hard region for k > khard = 4+2 \u221a \u03b1. The criterion nevertheless is not necessary in this setting, as we immediately see from the phase diagram for the two-components GMM in Fig. 1: we clearly observe an algorithmically hard region for high-sparsity although k < khard. In fact, the information-theoretic threshold \u03bbit, de ned as the value of the SNR at which the problem becomes statistically possible, cannot be found simply thanks to the expansion around the trivial xed point. Indeed the exact computation of \u03bbit requires evaluating the potential function \u03a6 in eq. (10): one needs to nd the threshold value of the SNR at which the informative xed point has a lower value of the free energy with respect to the uninformative xed point. The technical discussion on the calculation of\u03bbit, which we see plotted in Fig. 1 for the two-classes case, is given in App. B, while a general overview on the di erent thresholds is given in App. A. It is interesting to stress that the linearization of the SE equation around the trivial xed point in eq. (23) coincide to study the gradient of the potential \u03a6rs in eq. (10) around that point, and hence the stability of the trivial xed point is determined by the potential landscape. This connection is at the roots of the conjectured link between the hardness of an inference task and AMP: the statistical-to-computational gap opens up as the xed point with minimal value of the potential in eq. (10), describing the performance of the Bayes-optimal estimator, is attained far from a locally stable region around the uninformative xed point, where the AMP algorithm gets stuck."
        },
        {
            "heading": "6 Large sparsity regime",
            "text": "We characterize in this section the scaling of the thresholds in the very sparse (and most interesting) regime when \u03c1\u2192 0+. We highlight the main passages of the computation and focus on the principal results, see Appendix C for a detailed analysis. The starting point is the following change of variables:\nmu = m\u0303u\n\u221a \u2212\u03c1 log \u03c1\n\u03b1 mv = m\u0303v\u03c1 \u03bb = C(k)k\n\u221a \u2212\u03c1 log \u03c1\n\u03b1 (24)\nInserting these expressions into eq. (20), we obtain simpli ed SE equations for the rescaled overlaps (m\u0303u, m\u0303v) without any residual dependence on (\u03c1, \u03b1):\nm\u0303u = C(k)m\u0303v m\u0303v = Tk (C(k)m\u0303u) (25)\nwhere we introduced the auxiliary function Tk(\u00b7) de ned as:\nTk(z) = \u222b +\u221e 0 Sk\u22121 k(2\u03c0)k/2 \u03bek+1e\u2212\u03be 2/2\u0398 ( z\u03be2 2 \u2212 1 ) d\u03be\nwhere \u0398(\u00b7) is the Heaviside theta function. By considering the large k expansion of Tk(\u00b7), we can derive the scaling of the IT threshold with (k, \u03c1, \u03b1), see Appendix C for more details. Putting together with our previous result for \u03bbalg from Sec. 5, we obtain the following fundamental result:\n\u03bbit \u2248 \u221a \u2212k\u03c1 log \u03c1\n\u03b1 \u03bbalg = k\u221a \u03b1 . (26)\nThese equations completely characterize the behavior at large rank & small (but nite) sparsity. The statistical-tocomputational gap, where AMP is not able to exploit the information on the sparse nature of the cluster means, grows with both the sparsity and the rank. In App. C, we further show that the large rank expression for the IT threshold \u03bbit is accurate already at moderate k \u2248 10, see Fig. 9. It is interesting at this point to reframe our results on the high sparsity regime in the context of the existing literature. The results for the IT transition for the detection of two-classes sparse GMM were discussed in [22, 23], and we obtain a consistent scaling in the small \u03c1 behaviour. Despite this fact, the algorithmic bounds of di erent relevant works for the same problem [10, 11, 23] seem, at rst sight, to not agree with our ndings. In particular [11] proves rigorously the existence of an algorithm that achieves minimax rate under the BBP threshold for clustering of two-classes sparse GMM. This apparent inconsistency is related to di erent sparsity regimes analyzed. Indeed, here we investigate the extensive sparsity regime, i.e. \u03c1 = O(1), while the guarantees for the e cient algorithms working under the BBP threshold require a very high sparsity level, i.e. \u03c1 = o(1). This di erence is crucial. In Fig. 3 we illustrate it by considering the performance of two popular algorithms for this problem: Diagonal Thresholding (DT) [9] and SPCA. For extremely large sparsity, i.e. s = O(1), these algorithms indeed provide estimators with positive correlation with the true classes below the BBP threshold! However, as soon as we increase the density of non-zero components the performance strongly deteriorates. In fact, the transition to random chance performance takes place as the number of non-zero components approach s . \u221a n, in agreement with the literature [10, 11]."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this work we considered the problem of clustering k homogeneous Gaussian mixtures with sparse means. Mapping this to a low-rank matrix factorization problem, we have provided an exact asymptotic characterization of the MMSE in the high dimensional limit. The Bayes-optimal performance was compared to AMP, the best known polynomial time algorithm for this problem in the studied regime. In the large sparsity regime, we uncovered a large statistical-tocomputational gap as the sparsity level grows, and unveiled the existence of a computationally hard phase. In particular, we have shown that the SNR threshold below which recovery is statistically impossible is given by \u03bbit \u2248 \u221a \u2212k\u03c1 log \u03c1/ \u221a \u03b1, while the one for which AMP positively correlates with the ground truth classes is given by \u03bbalg \u2265 k/\u221a\u03b1. Our result for the existence of an algorithmically hard region was compared with the existing literature for this problem, solving an apparent contradiction due to the scaling assumption of the sparsity level with the dimension of the features. We corroborated our ndings with the help of algorithms for subspace clustering such as sparse principal component analysis and diagonal thresholding. The mapping of the subspace clustering problem to a low-rank matrix factorization one is exible and extending this work to more general scenarios is de nitely interesting. The main limitation of the current setting is the Gaussian assumption for the noise. One natural idea to go beyond this limitation is to consider other avours of message passing algorithms, such as vector AMP (VAMP) [36] to study more general rotationally invariant noise distributions. A further prospective direction is to consider inhomegeneous mixture models, i.e. pc 6= 1/k in eq. (1), in order to study the e ect of unbalancedness on the statistical-to-computational gap."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank Maria Re netti for discussions. This work started as a part of the doctoral course Statistical Physics For Optimization and Learning taught at EPFL in spring 2021. We acknowledge funding from the ERC under the European Union\u2019s Horizon 2020 Research and Innovation Program Grant Agreement 714608-SMiLe, and by the Swiss National Science Foundation grant SNFS OperaGOST, 200021_200390."
        },
        {
            "heading": "A Analysis of the thresholds",
            "text": "We identi ed in Sec. 4 di erent reconstruction phases for the subspace clustering problem, characterizing completely the Bayes-optimal and algorithmical performances. We detail in this section the de nition of the reconstruction phases and the consequences for the computational and statistical limits of subspace clustering. We discuss in particular an interesting link between the thresholds separating these phases and the potential function \u03a6rs in eq. (10). First, we enlarge the picture on the reconstruction phases we o ered in Sec. 3. Along with the impossible, hard and easy phases we can de ne a further region. We call the Alg-Bayes phase, the region of parameters in which the performance of AMP is, not only achieving positive correlation with the ground truth, but achieves the Bayes-optimal performance. We summarize now the complete description:\nImpossible phase: There is not enough information in the data matrix X handled to the statistician in order to assign cluster membership better than chance for any algorithm, and the Bayes-optimal MMSE is not better than a random guess. Clustering (i.e. reconstruction of U? better than chance) is impossible.\nHard phase: The MMSE is non-trivial, and clustering is statistically possible to some extent, but the best known polynomial time algorithm, AMP, fails to correlate better than chance with the true cluster assignment U?. Any polynomial-time algorithm is conjectured to fail in this region.\nEasy phase: In the easy phase, not only clustering is statistically possible, but AMP is able to achieve positive correlation with U?.\nAlg-Bayes phase: In the alg-Bayes phase AMP is able to achieve Bayes-optimal positive correlation with U?.\nFollowing the introduction of the alg-Bayes phase, we nd an enriched version of the phase diagram for the two-classes subspace clustering at xed \u03b1, see Fig. 4. When we cross from one region to an other we have a phase transition. Each phase transition is characterized by di erent thresholds: values of the parameters which signal the onset of a new phase. In Fig. 4 we see di erent thresholds which were not present in the previous plot in Fig. 1: {\u03bbdyn, \u03bbalg-Bayes, \u03bbjump-Bayes}. In order to de ne these quantities, it is useful to highlight the relationship between the thresholds and the minima of the \"free energy\" \u03a6rs in eq. (10). Let us x a sparsity level 1\u2212 \u03c1, say \u03c1 = 0.05, and move vertically on the y-axis starting from the bottom on the y-axis, i.e. \u03bb\u221a\u03b1/k 1. As we increase the value of the SNR we can identify the following thresholds:\n\u2022 \u03bb < \u03bbdyn: The only minimum of eq. (9) is the trivial minimum corresponding to zero correlation Mu = 0 with U?. Therefore, below this threshold reconstruction is impossible.\n\u2022 \u03bb \u2208 (\u03bbdyn, \u03bbit): A second minima with higher \u03a6rs (i.e. a local minimum) and correlation appears, but the trivial minimum Mu = 0 is still the global one. Therefore, AMP with a uninformed initialization Mt=0 = 0 will converge to the trivial minimum, and reconstruction is only possible with a strong informed initialization. We call the threshold value for the emergence of this local minimum the dynamical spinodal \u03bbdyn.\n\u2022 \u03bb \u2208 (\u03bbit, \u03bbalg): As the SNR is increased, the local minumum goes down in energy \u03a6rs, and at a certain \u03bbit, it crosses the trivial minimum. Therefore, in this interval the non-trivial minimum is the global one, while the trivial minimum becomes local. Although reconstruction is statistically possible in this region, AMP with a uninformed initialization Mt=0 = 0 is stuck at the trivial minima. Therefore, in this region we enter the hard phase.\n\u2022 \u03bb \u2208 (\u03bbalg, \u03bbalg-Bayes) As the SNR is further increased, AMP with a uninformed initialization starts to achieve positive correlation with U?, although strictly lower than the Bayes-optimal estimator. In terms of the potential \u03a6rs, this corresponds to the trivial minimum continuously becoming a local maximum, and another local minimum corresponding to higher correlation continuously appearing. This new local minima coexists with the global one, which corresponds to the Bayes-optimal performance. We enter the easy phase.\n\u2022 \u03bb > \u03bbalg-Bayes: Finally, as the SNR is further increased the local minima disappears, and there is only one minimum with high-correlation with the signal left. In this region, AMP with a uninformed initialization achieves the same performance as the Bayes-optimal estimator. We enter the alg-Bayes phase.\nThe discussion above is summarized in Fig. 7. We plot toghether with the evolution of the minima of \u03a6rs, the performance of SE with both informative and uninformative initialization to analyze the behaviour of the MSE.\nThe full characterization of the subspace clustering problem both from an algorithmic and statistical perspective, boils down to the analysis of the evolution of the critical points of \u03a6rs as we vary the meaningful parameters in the problem. We note from Fig. 4 that the performance of AMP is, when it achieves positive correlation with the ground truth, almost everywhere Bayes-optimal apart from a small region around the tri-critical point. This point is de ned - at xed (\u03b1, k)- as the tuple of parameters (\u03bbT (\u03b1, k) , \u03c1T (\u03b1, k)), such that the \"spinodal\" thresholds meet, i.e. \u03bbalg-Bayes = \u03bbdyn. We discuss why these thresholds are called spinodals, and how to compute them practically in Sec. B. We can analyze the vicinity of the tri-critical point to analyze the non-trivial interplay between the easy and alg-Bayes phase, where AMP does not achieve always Bayes-optimal performance. Imagine to repeat the same steps as before considering the zoom around the tri-critical point of the phase diagram, see Fig. 6. Fix a sparsity level 1\u2212 \u03c1, say \u03c1 = 0.202, and move vertically on the y-axis starting from the bottom on the y-axis, i.e. \u03bb\u221a\u03b1/k 1. As we increase the SNR we can repeat the previous analysis, obtaining now: \u2022 \u03bb < \u03bbalg: The only minimum of eq. (9) is the trivial minimum corresponding to zero correlation Mu = 0 with U?. Therefore, below this threshold reconstruction is impossible.\n\u2022 \u03bb \u2208 (\u03bbalg, \u03bbdyn): The trivial minimum becomes unstable and AMP achieves positive correlation with the ground truth. The minimum is unique and also SE with a positive initialization would end up there. The phase is alg-Bayes.\n\u2022 \u03bb \u2208 (\u03bbdyn, \u03bbjump-bayes): As the SNR is increased, a new local minimum appears. The reconstruction phase is still alg-Bayes since the non-trivial minimum has higher free energy than the global one.\n\u2022 \u03bb \u2208 (\u03bbjump-Bayes, \u03bbalg-Bayes) As the SNR is further increased, the free energy of the informative minimum goes down and becomes equal to the uninformative one. We enter the easy phase, nevertheless AMP achieves positive correlation with the truth, the Bayes optimal performance is superior. The Bayes-optimal MSE have a rst order phase transition at \u03bbjump-Bayes, hence the name jump-Bayes.\n\u2022 \u03bb > \u03bbalg-Bayes: Finally, as the SNR is further increased the local minima disappears, and there is only one minimum with high-correlation with the signal left. In this region, AMP with a uninformed initialization achieves the same performance as the Bayes-optimal estimator. We enter the alg-Bayes phase. The analysis of the evolution of the minima of \u03a6rs and the consequences on the MSE is done in Fig. 7."
        },
        {
            "heading": "B Building the phase diagram",
            "text": "We build in this section the phase diagram for the two-classes GMM shown in Fig. 1 , explaining the steps which are easily generalizable to the general mixture case. First we note that we can simplify the model for two-mixtures GMM in eq. (4) even further by mapping it to an easier rank k = 1 version of the matrix factorization problem. It su ces to replace the matrices (U, V ) in eq. (4) by the following quantities:\nu \u223c Rad(n) \u2208 {\u22121,+1}n v \u223ci.i.d. \u03c1N(0, Id) + (1\u2212 \u03c1)\u03b40 \u2208 Rd (27)\nThe two formulations of the problem are indeed formally equivalent up to a rescaling of the parameters such that at xed \u03b1 the quantity \u03bb/k is the same in two settings. The mapping simplify signi cantly the computation. First, in order to compute the Bayes-optimal performance in eq. (8), we must compute the partition functions Zu/v in eqs. (11),(12) for the new simpli ed model. We shall exploit the following general relation, as a function of the prior distribution on (U,V):\nZu/v(A, b) = Ex\u223cPu/v [ exp ( \u2212b>x+ x >Ax 2 )] (28)\nthus exploiting the explicit expression of the prior in eq. (27) we obtain:\nZu(A, b) = e \u2212A2 cosh b Zv(A, b) = 1\u2212 \u03c1+ \u03c1\u221a 1 +A exp\n( b2\n2(1 +A)\n) (29)\nWe study the computational limits for the subspace clustering problem deriving the associated AMP to this simpli ed low-rank matrix factorization problem. We have to compute the denoising functions for the simpli ed model, written in eqs. (13),(14) for the general rank case. We shall exploit the general formula relating them with the partition functions computed above:\n\u03b7u/v(A, b) = \u2202b logZu/v(A, b) (30)\nthus using the prior for the simpli ed k = 1 model in eq. (27) we obtain:\n\u03b7u(A, b) = tanh(b) \u03b7v(A, b) = \u03c1b\n1 +A\n1\n\u03c1+ (1\u2212 \u03c1) \u221a 1 +Ae\u2212 b2 2(1+A)\n(31)\nwhere now (A, b) \u2208 R2. We can write at this point the SE equations for the overlaps (mtu,mtv) which are now scalar variables. Let us introduce the following notation:\nmt+1u = Eu\u2217\u223cPu,\u03be\u223cN(0,1) [ \u03b7u ( \u03bbmtv \u03c1 , \u03bbmtvu\u2217 \u03c1 + \u221a \u03bbmtv \u03c1 \u03be ) u\u2217 ] := U(\u03bbmtv/\u03c1) (32)\nmt+1v = Ev\u2217\u223cPv,\u03be\u223cN(0,1) [ \u03b7v ( \u03b1\u03bbmtu \u03c1 , \u03b1\u03bbmtuv\u2217 \u03c1 + \u221a \u03b1\u03bbmtu \u03c1 \u03be ) v\u2217 ] := V(\u03b1\u03bbmtu/\u03c1) (33)\nThe perturbative method we presented in Sec 5 would pinpoint easily the expected algorithmical threshold \u03bbalg = 1/\u221a\u03b1. Despite this, as we discussed in Sec. 5, it would not guarantee the presence of an algorithmically hard region since the number of cluster k < khard.\nWe have to resort to numerical computations for nding the exact values of the thresholds since also in this simple case we do not have a closed-form update for the iterates (mtu,mtv), although we will treat them analytically in App. C. We keep in mind the picture in Figs. (5,7) and compute the di erent thresholds there de ned. Let us consider rst \u03bbit, the IT threshold, de ned as the SNR at which the problem becomes statistically possible. We see in Fig. 5 that it coincides with the SNR level at which the free energy of the two minima (if present) are equal. We analyze for simplicity sparsity levels in which \u03bbdyn < \u03bbalg, i.e. we refer to Fig. 5, otherwise the criterion above would de ne equivalently \u03bbbayes-Jump. We compute the di erence of free energy \u2206\u03a6 between the two minima introducing the path \u03b3 : R\u2192 R2 which follows the state evolution equations:\n\u03b3(t) = (t,V(\u03b1\u03bbt))\nWe can use the fundamental theorem of calculus to obtain the di erence of free energy between the trivial xed point (mu,mv) = (0, 0) and a non-trivial one (mu,mv) = ( x,V ( \u03b1\u03bb(x)x) ) at overlap mu = x as follows:\n\u2206\u03a6(x) = \u222b x 0 dq d\u03a6 dt ( mu(t) = q,mv(t) = V(\u03b1\u03bb(x)q);\u03bb(x), \u03c1, \u03b1 ) (34)\nwhere we introduced \u03bb(x) as the SNR needed in order to have at xed (\u03c1, \u03b1) an overlap mu = x de ned by the self-consistent equation:\nx = U ( \u03bb(x)V(\u03b1\u03bb(x)x) ) (35)\nPlugging in the expression of the derivative we identify the IT threshold \u03bbit as the minimal SNR such that the following equation is satis ed: \u222b x(\u03bbit)\n0\ndqV\u2032(\u03b1\u03bbitq)[q \u2212 U(\u03bbitV(\u03b1\u03bbitq))] = 0 (36)\nLet us consider now the Bayes-algorithmical and dynamical thresholds, always referring to their pictorial representation in Figs. (5,7). From a practical standpoint they are stationary point of the the function \u03bb(mu), solution of eq. (35). A reader with some statistical physics background may recognize a parallel with the theory of real gases. The curve \u03bb(mu), exactly as the Pressure-Volume curve p(v) for real gases, is composed of two branches called stable and unstable branch de ned from the value of the derivative \u2202\u03bb/\u2202m (resp. \u2202p/\u2202v). The operative de nition of these thresholds as critical points of the curve \u03bb(mu) allows us to easily compute them numerically, see Fig. 8 to observe their evolution as a function of the sparsity level. We see that as we increase the sparsity, i.e. decrease \u03c1, the statistical-to-computation gap, measured visually by the distance between the two critical points, increase. Moreover the IT threshold collapse with the dynamical one. The same happens with the Bayes-algorithmic threshold, approaching \u03bbalg \u2248 1."
        },
        {
            "heading": "C Scaling behaviour at large sparsity",
            "text": "We do not have in general an analytical expression for the iterates of the SE equation (mtu,mtv) appearing in eq. (20), thus we introduced in Sec. 6 a change of variables that allows us to approach analytically the problem in the large sparsity regime. We study in this section the consequences of this scaling assumption. Let us rewrite it here:\nmu = m\u0303u\n\u221a \u2212\u03c1 log \u03c1\n\u03b1 mv = m\u0303v\u03c1 \u03bb = C(k)k\n\u221a \u2212\u03c1 log \u03c1\n\u03b1 (37)\nConsider the update of the parameter mv under this parametrization:\nmv = \u03c1m\u0303v = f (k) v ( \u2212 m\u0303uC(k) log \u03c1 ) (38)\nWe can rewrite the right hand side in the following way:\n\u03c1m\u0303v = \u03c1 \u2212m\u0303uC(k) log \u03c1\n(k \u2212 m\u0303uC(k) log \u03c1) \u222b +\u221e 0 Sk\u22121 (2\u03c0) k 2\n\u03c1\u03bek+1e\u2212\u03be 2/2\n\u03c1+ (1\u2212 \u03c1)(k\u2212m\u0303uC(k) log \u03c1k ) k 2 \u03c1m\u0303uC(k)\u03be2/2\nd\u03be (39)\nwhere Sk\u22121(1) is the surface of the k\u2212dimensional unitary hypersphere. Working in the small \u03c1 limit allows us to exploit a concentration in measure over which we integrate on the right hand side. The exponent of \u03c1 in the denominator, i.e. m\u0303uC(k)\u03be 2\n2 , will determine the large sparsity behaviour. If the exponent is greater than one, \u03c1 m\u0303uC(k)\u03be 2/2 will go to zero, otherwise it will diverge in the limit \u03c1\u2192 0. Thus we obtain:\nmv \u2248 \u03c1 \u222b +\u221e 0 Sk\u22121(1) k(2\u03c0)k/2 \u03bek+1e\u2212\u03be 2/2\u0398 ( m\u0303uC(k) 2 \u03be2 \u2212 1 ) := \u03c1Tk (m\u0303uC(k)) (40)\nwhere we introduced \u0398(x) as the Heavyside theta. Plugging in the above expression into the equation de ning mtu we have:\nmu = f (k) u (\u03bb\u03c1Tk (m\u0303uC(k))) (41)\nthus approximating for small \u03c1 the function f (k)u , expressing everything in terms of mu and plugging in the scaling ansatz for mu in eq. (37), we obtain the simpli ed SE in the large sparsity regime:\nm\u0303u = C(k)Tk (m\u0303uC(k)) (42)\nWe can repeat the analysis done in the previous appendix to nd the thresholds in this limit. The condition de ning the IT threshold \u03bbit written in eq. (34) simpli es to:\u222b x\u0303\n0\ndq\u0303 T \u2032k (Ck(x\u0303)q\u0303) q\u0303 = \u222b x\u0303 0 dq\u0303 T \u2032k (Ck(x\u0303)q\u0303)Ck(x\u0303)Tk (Ck(x\u0303)q\u0303) (43)\nwhere we de ned Ck(x\u0303) as the value of the coe cient C(k), solution of eq. (42) when the overlap is xed at mu =\u221a \u2212\u03c1 log \u03c1 \u03b1 x\u0303. This task is much easier to solve. Likewise, the computation of the dynamical spinodal simpli es greatly. We need to nd the minimum SNR such that eq. (42) has a non trivial solution. By introducing the auxiliary variable y = Ck(y)m\u0303 we rewrite eq. (42) as:\nC2k(y) = y\nTk(y) (44)\nthus the minimal SNR to obtain a non trivial solution, de ned by the coe cient Cdyn(k), to obtain a non trivial solution of the equation above is given by:\nCdyn(k) = min y\u2208R+\n\u221a y\nTk(y) (45)\nAt this stage we still need to resort to numerical inspection in order to nd the coe cient Cit, Cdyn, but we can investigate analytically the large k behaviour. By considering the leading order of the function Tk(\u00b7), one can see that Tk(z) \u2248 \u0398 ( z \u2212 2k+1 ) . By plugging in this expression into the de nition of the coe cients ( Cit(k), Cdyn(k) ) in eqs. (43),(45) we obtain the following asymptotic result:\nCit(k) \u2248 \u221a 4\nk + 1 Cdyn(k) \u2248\n\u221a 2\nk + 1 (46)\nthus plugging them into the scaling assumption in eq. (37) we obtain the following scaling for the thresholds:\n\u03bbit \u2248 \u221a 4k2\nk + 1\n\u221a \u2212\u03c1 log \u03c1\n\u03b1 \u03bbdyn \u2248\n\u221a 2k2\nk + 1\n\u221a \u2212\u03c1 log \u03c1\n\u03b1 (47)\nThe evolution of the coe cients Cit, Cdyn as a function of the number of clusters is summarized in Fig. 9. We see in Fig. 9 that the large rank expansion is quite accurate also at moderate k, especially for the IT threshold. The easy phase and the alg-Bayes phase merge, hence we will not analyze the distinction between \u03bbalg and \u03bbalg-Bayes in this regime."
        },
        {
            "heading": "D Details on numerical simulations",
            "text": "We discuss in this section the details behind the numerical simulations presented in Sec. 4. The code is available in the Github repository associated to the manuscript. First we stress an important point on the convergence of low-rAMP algorithms. Increasing the sparsity of the problem, i.e. decreasing \u03c1, the convergence of AMP becomes more di cult. In order to solve this problem is useful to implement damping to stabilize the iteration, see the modi ed AMP in Algorithm 2. We compared the performance of AMP with di erent popular algorithm in the literature. The rst general-purpose algorithm we considered for subspace clustering is a modi cation of the sparse PCA algorithm (SPCA). Let us consider a data matrix Y \u2208 Rn\u00d7d, where as in our notation n is the number of samples and d is the feature dimension. In the SPCA problem, the statistician wants to nd directions in the space which maximize the variance of our dataset by constraining the cardinality of the new basis vectors. In vanilla PCA instead we try to nd directions, called principal components {e\u0302m}dm=1, which maximize the variance not caring if they will be given by linear combination of all the features of\nour problem: e\u0302m = \u2211d i=1 \u03b1 (m) i ei, where we called {ei}di=1 the canonical basis vectors. In SPCA we want that some of the coe cients \u03b1(m)i (called \"loadings\" in the literature) to be zero, favouring interpretability of the optimal estimator. By formulating in a variational way the problem the sparsity of the estimator is enhanced using LASSO regularization. We write the pseudocode for the program we used in the two-class subspace clustering problem in Algorithm. 3. The unregularized problem, i.e. \u0393 = 0, is equivalent to vanilla PCA. The comparison of the performances of the two spectral algorithms has been done in Fig. 2 and we have a clear advantage in imposing the cardinality constraint as the sparsity level increase. We considered in the sub-extensive sparsity regime in Sec. 6 the Diagonal Thresholding algorithm (DT). The main idea is to search for spatial directions with the largest variance, and threshold the sample covariance matrix accordingly, hence the name Diagonal Thresholding. The pseudocode for the algorithm we used in the two-classes subspace clustering is given in Algorithm. 4.\nAlgorithm 2 low-rAMP with damping Input: Data X \u2208 Rd\u00d7n Initialize v\u0302t=0i , u\u0302t=0\u03bd \u223c N(0k, Ik), \u03c3\u0302t=0u,\u03bd = 0k\u00d7k , \u03c3\u0302t=0v,i = 0k\u00d7k . for t \u2264 tmax do\nAtmpu = \u03bbs ( U\u0302t )> U\u0302, Atmpv = \u03bbs ( V\u0302t )> V\u0302\nBtmpv = \u221a \u03bb sXU\u0302 t \u2212 \u03bbs n\u2211 \u03bd=1 \u03c3tu,\u03bdV\u0302 t\u22121, Btmpu = \u221a \u03bb sX >V \u2212 \u03bbs d\u2211 i=1 \u03c3tv,iU\u0302 t\u22121 Damping step with damping coe cient \u03b3: Atu = (1\u2212 \u03b3)A tmp u + \u03b3At\u22121u Atv = (1\u2212 \u03b3)A tmp v + \u03b3At\u22121v Btu = (1\u2212 \u03b3)B tmp u + \u03b3Bt\u22121u Btv = (1\u2212 \u03b3)B tmp v + \u03b3Bt\u22121v Take {btv,i \u2208 Rk}di=1, {btu,\u03bd \u2208 Rk}n\u03bd=1 rows of Btv,Btu v\u0302t+1i = \u03b7v(Atv, btv,i), u\u0302t+1\u03bd = \u03b7u(Atu, btu,\u03bd) \u03c3\u0302t+1v,i = \u2202b\u03b7v(Atv, btv,i), \u03c3\u0302t+1u,\u03bd = \u2202b\u03b7u(Atu, btv,\u03bd) Here U\u0302t \u2208 Rn\u00d7k, V\u0302t \u2208 Rd\u00d7k,Btu \u2208 Rn\u00d7k,Btv \u2208 Rd\u00d7k,Atu \u2208 Rk\u00d7k,Atv \u2208 Rk\u00d7k\nend for Return: Estimators v\u0302amp,i, u\u0302amp,\u03bd \u2208 Rk, \u03c3\u0302u,\u03bd , \u03c3\u0302v,i \u2208 Rk\u00d7k\nAlgorithm 3 SPCA Input: Data Y \u2208 Rn\u00d7d Initialize \u2206sparsity = 1,\u0393 = 10\u22123 while |\u2206sparsity| \u2265 1 do\nSolve variational problem: (C\u0302, D\u0302) = arg min C\u2208Rn,D\u2208Rd\n{ \u2225\u2225Y\u2212 CDT\u2225\u2225\nF + \u0393 \u2016D\u20161}\nCompute rst sparse principal component D Compute the estimated sparsity s\u0302 = \u2211d i=1(1\u2212 \u03b4v\u0302i,0) Compute sparsity mismatch \u2206sparsity = \u03c1d\u2212 s\u0302 If \u2206sparsity < 0 decrease \u0393, otherwise increase it\nend while Project the data matrix onto the rst sparse principal component: P = YD \u2208 Rn Compute cluster membership assignment: U\u0302 = sign(P) Return: MSE(U\u0302)\nAlgorithm 4 Diagonal Thresholding Input: Data Y \u2208 Rn\u00d7d Compute the sample covariance matrix K\u0302 = 1n \u2211n \u03bd=1 y\u03bdy > \u03bd\nFind the directions with the s largest variance, with s = b\u03c1dc. Call the subset of indices corresponding to the directions above S. Create K\u0303:\nK\u0303ij = {\nK\u0302ij if (i, j) \u2208 S 0 otherwise\nCompute the largest eigenvector of the thresholded matrix K\u0303 and call it v\u0302. Project the data matrix onto the rst sparse principal component: p = Yv\u0302 \u2208 Rn Compute cluster membership assignment: u\u0302 = sign(p) Return: MSE(u\u0302)"
        }
    ],
    "title": "Subspace",
    "year": 2022
}