{
    "abstractText": "A person\u2019s facial hairstyle, such as presence and size of beard, can significantly impact face recognition accuracy. There are publicly-available deep networks that achieve reasonable accuracy at binary attribute classification, such as beard / no beard, but few if any that segment the facial hair region. To investigate the effect of facial hair in a rigorous manner, we first created a set of fine-grained facial hair annotations to train a segmentation model and evaluate its accuracy across African-American and Caucasian face images. We then use our facial hair segmentations to categorize image pairs according to the degree of difference or similarity in the facial hairstyle. We find that the False Match Rate (FMR) for image pairs with different categories of facial hairstyle varies by a factor of over 10 for African-American males and over 25 for Caucasian males. To reduce the bias across image pairs with different facial hairstyles, we propose a scheme for adaptive thresholding based on facial hairstyle similarity. Evaluation on a subject-disjoint set of images shows that adaptive similarity thresholding based on facial hairstyles of the image pair reduces the ratio between the highest and lowest FMR across facial hairstyle categories for African-American from 10.7 to 1.8 and for Caucasians from 25.9 to 1.3. Facial hair annotations and facial hair segmentation model will be publicly available.",
    "authors": [
        {
            "affiliations": [],
            "name": "Kagan Ozturk"
        },
        {
            "affiliations": [],
            "name": "Grace Bezold"
        },
        {
            "affiliations": [],
            "name": "Aman Bhatta"
        },
        {
            "affiliations": [],
            "name": "Haiyu Wu"
        },
        {
            "affiliations": [],
            "name": "Kevin Bowyer"
        }
    ],
    "id": "SP:99897e3d456b48d00bb6596b19599389aa62b8e8",
    "references": [
        {
            "authors": [
                "Salem Hamed Abdurrahim",
                "Salina Abdul Samad",
                "Aqilah Baseri Huddin"
            ],
            "title": "Review on the effects of age, gender, and race demographics on automatic face recognition",
            "venue": "The Vis. Comput.,",
            "year": 2018
        },
        {
            "authors": [
                "Radhakrishna Achanta",
                "Appu Shaji",
                "Kevin Smith",
                "Aurelien Lucchi",
                "Pascal Fua",
                "Sabine S\u00fcsstrunk"
            ],
            "title": "Slic superpixels compared to state-of-the-art superpixel methods",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2012
        },
        {
            "authors": [
                "V\u0131\u0301tor Albiero",
                "Kevin W Bowyer"
            ],
            "title": "Is face recognition sexist? No, gendered hairstyles and biology are",
            "venue": "In Proceedings of the British Machine Vision Conference (BMVC),",
            "year": 2020
        },
        {
            "authors": [
                "V\u0131\u0301tor Albiero",
                "Kai Zhang",
                "Kevin W. Bowyer"
            ],
            "title": "How does gender balance in training data affect face recognition accuracy",
            "venue": "IEEE International Joint Conference on Biometrics (IJCB),",
            "year": 2020
        },
        {
            "authors": [
                "V\u0131\u0301tor Albiero",
                "Kai Zhang",
                "Michael C King",
                "Kevin W Bowyer"
            ],
            "title": "Gendered differences in face recognition accuracy explained by hairstyles, makeup, and facial morphology",
            "venue": "IEEE Transactions on Information Forensics and Security,",
            "year": 2021
        },
        {
            "authors": [
                "Xiang An",
                "Xuhan Zhu",
                "Yuan Gao",
                "Yang Xiao",
                "Yongle Zhao",
                "Ziyong Feng",
                "Lan Wu",
                "Bin Qin",
                "Ming Zhang",
                "Debing Zhang"
            ],
            "title": "Partial fc: Training 10 million identities on a single machine",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Aman Bhatta",
                "V\u0131\u0301tor Albiero",
                "Kevin W Bowyer",
                "Michael C King"
            ],
            "title": "The gender gap in face recognition accuracy is a hairy problem",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Jacqueline G. Cavazos",
                "P. Jonathon Phillips",
                "Carlos D. Castillo",
                "Alice J. O\u2019Toole"
            ],
            "title": "Accuracy comparison across face recognition algorithms: Where are we on measuring race bias",
            "venue": "IEEE Transactions on Biometrics, Behavior, and Identity Science,",
            "year": 2021
        },
        {
            "authors": [
                "Jiankang Deng",
                "Jia Guo",
                "Jing Yang",
                "Niannan Xue",
                "Irene Kotsia",
                "Stefanos Zafeiriou"
            ],
            "title": "Arcface: Additive angular margin loss for deep face recognition",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Pawel Drozdowski",
                "Christian Rathgeb",
                "Antitza Dantcheva",
                "Naser Damer",
                "Christoph Busch"
            ],
            "title": "Demographic bias in biometrics: A survey on an emerging challenge",
            "venue": "IEEE Transactions on Technology and Society,",
            "year": 2020
        },
        {
            "authors": [
                "Markos Georgopoulos",
                "James Oldfield",
                "Mihalis A Nicolaou",
                "Yannis Panagakis",
                "Maja Pantic"
            ],
            "title": "Mitigating demographic bias in facial datasets with style-based multi-attribute transfer",
            "venue": "International Journal of Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "G Givens",
                "JR Beveridge",
                "BA Draper",
                "P Grother",
                "PJ Phillips"
            ],
            "title": "How features of the human face affect recognition: a statistical comparison of three face recognition algorithms",
            "venue": "In Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
            "year": 2004
        },
        {
            "authors": [
                "Patrick Grother"
            ],
            "title": "Face recognition vendor test (FRVT) part 8: Summarizing demographic differentials",
            "venue": "Technical report,",
            "year": 2022
        },
        {
            "authors": [
                "Patrick Grother",
                "Mei Ngan",
                "Kayee Hanaoka"
            ],
            "title": "Face recognition vendor test (fvrt): Part 3, demographic effects",
            "venue": "National Institute of Standards and Technology Gaithersburg, MD,",
            "year": 2019
        },
        {
            "authors": [
                "W Guo",
                "P Aarabi"
            ],
            "title": "Hair segmentation using heuristicallytrained neural networks",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Tero Karras",
                "Timo Aila",
                "Samuli Laine",
                "Jaakko Lehtinen"
            ],
            "title": "Progressive growing of gans for improved quality, stability, and variation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Minchul Kim",
                "Anil K Jain",
                "Xiaoming Liu"
            ],
            "title": "Adaface: Quality adaptive margin for face recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Brendan F Klare",
                "Mark J Burge",
                "Joshua C Klontz",
                "Richard W Vorder Bruegge",
                "Anil K Jain"
            ],
            "title": "Face recognition performance: Role of demographic information",
            "venue": "IEEE Transactions on information forensics and security,",
            "year": 2012
        },
        {
            "authors": [
                "T. Hoang Ngan Le",
                "Khoa Luu",
                "Marios Savvides"
            ],
            "title": "Fast and robust self-training beard/moustache detection and segmentation",
            "venue": "In 2015 International Conference on Biometrics (ICB),",
            "year": 2015
        },
        {
            "authors": [
                "T Hoang Ngan Le",
                "Khoa Luu",
                "Keshav Seshadri",
                "Marios Savvides"
            ],
            "title": "Beard and mustache segmentation using sparse classifiers on self-quotient images",
            "venue": "In 2012 19th IEEE International Conference on Image Processing,",
            "year": 2012
        },
        {
            "authors": [
                "T Hoang Ngan Le",
                "Khoa Luu",
                "Chenchen Zhu",
                "Marios Savvides"
            ],
            "title": "Semi self-training beard/moustache detection and segmentation simultaneously",
            "venue": "Image and Vision Computing,",
            "year": 2017
        },
        {
            "authors": [
                "Alex Levinshtein",
                "Cheng Chang",
                "Edmund Phung",
                "Irina Kezele",
                "Wenzhangzhi Guo",
                "Parham Aarabi"
            ],
            "title": "Real-time deep hair matting on mobile devices",
            "venue": "In 2018 15th Conference on Computer and Robot Vision (CRV),",
            "year": 2018
        },
        {
            "authors": [
                "Ziwei Liu",
                "Ping Luo",
                "Xiaogang Wang",
                "Xiaoou Tang"
            ],
            "title": "Deep learning face attributes in the wild",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2015
        },
        {
            "authors": [
                "Boyu Lu",
                "Jun-Cheng Chen",
                "Carlos D Castillo",
                "Rama Chellappa"
            ],
            "title": "An experimental evaluation of covariates effects on unconstrained face verification",
            "venue": "IEEE Transactions on Biometrics, Behavior, and Identity Science,",
            "year": 2019
        },
        {
            "authors": [
                "Umar Riaz Muhammad",
                "Michele Svanera",
                "Riccardo Leonardi",
                "Sergio Benini"
            ],
            "title": "Hair detection, segmentation, and hairstyle classification in the wild",
            "venue": "Image and Vision Computing,",
            "year": 2018
        },
        {
            "authors": [
                "Minh Hoai Nguyen",
                "Jean-Francois Lalonde",
                "Alexei A. Efros",
                "Fernando De la Torre"
            ],
            "title": "Image-based shaving",
            "venue": "Computer Graphics Forum,",
            "year": 2008
        },
        {
            "authors": [
                "Omkar M. Parkhi",
                "Andrea Vedaldi",
                "Andrew Zisserman"
            ],
            "title": "Deep face recognition",
            "venue": "In Proceedings of the British Machine Vision Conference (BMVC),",
            "year": 2015
        },
        {
            "authors": [
                "P.Jonathon Phillips",
                "Harry Wechsler",
                "Jeffery Huang",
                "Patrick J. Rauss"
            ],
            "title": "The feret database and evaluation procedure for face-recognition algorithms",
            "venue": "Image and Vision Computing,",
            "year": 1998
        },
        {
            "authors": [
                "Karl Ricanek Jr.",
                "Tamirat Tesafaye"
            ],
            "title": "Morph: A longitudinal image database of normal adult age-progression",
            "venue": "In Proceedings of the 7th International Conference on Automatic Face and Gesture Recognition,",
            "year": 2006
        },
        {
            "authors": [
                "Florian Schroff",
                "Dmitry Kalenichenko",
                "James Philbin"
            ],
            "title": "Facenet: A unified embedding for face recognition and clustering",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Ignacio Serna",
                "Aythami Morales",
                "Julian Fierrez",
                "Manuel Cebrian",
                "Nick Obradovich",
                "Iyad Rahwan"
            ],
            "title": "Algorithmic discrimination: Formulation and exploration in deep learningbased face biometrics",
            "venue": "arXiv preprint arXiv:1912.01842,",
            "year": 1912
        },
        {
            "authors": [
                "Keshav Seshadri",
                "Marios Savvides"
            ],
            "title": "Robust modified active shape model for automatic facial landmark annotation of frontal faces",
            "venue": "In 2009 IEEE 3rd International Conference on Biometrics: Theory, Applications, and Systems,",
            "year": 2009
        },
        {
            "authors": [
                "Yehu Shen",
                "Zhenyun Peng",
                "Yaohui Zhang"
            ],
            "title": "Image based hair segmentation algorithm for the application of automatic facial caricature synthesis",
            "venue": "The Scientific World Journal,",
            "year": 2014
        },
        {
            "authors": [
                "Michele Svanera",
                "Umar Riaz Muhammad",
                "Riccardo Leonardi",
                "Sergio Benini"
            ],
            "title": "Figaro, hair detection and segmentation in the wild",
            "venue": "In 2016 IEEE International Cohttp://dictionary.cambridge.org/nference on Image Processing (ICIP),",
            "year": 2016
        },
        {
            "authors": [
                "Philipp Terh\u00f6rst",
                "Jan Niklas Kolf",
                "Marco Huber",
                "Florian Kirchbuchner",
                "Naser Damer",
                "Aythami Morales Moreno",
                "Julian Fierrez",
                "Arjan Kuijper"
            ],
            "title": "A comprehensive study on face recognition biases beyond demographics",
            "venue": "IEEE Transactions on Technology and Society,",
            "year": 2021
        },
        {
            "authors": [
                "William Thong",
                "Cees GM Snoek"
            ],
            "title": "Feature and label embedding spaces matter in addressing image classifier bias",
            "venue": "In Proceedings of the British Machine Vision Conference (BMVC),",
            "year": 2021
        },
        {
            "authors": [
                "Haitao Wang",
                "Stan Z Li",
                "Yangsheng Wang"
            ],
            "title": "Face recognition under varying lighting conditions using self quotient image",
            "venue": "In Sixth IEEE International Conference on Automatic Face and Gesture Recognition,",
            "year": 2004
        },
        {
            "authors": [
                "Hao Wang",
                "Yitong Wang",
                "Zheng Zhou",
                "Xing Ji",
                "Dihong Gong",
                "Jingchao Zhou",
                "Zhifeng Li",
                "Wei Liu"
            ],
            "title": "Cosface: Large margin cosine loss for deep face recognition",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognitionn,",
            "year": 2018
        },
        {
            "authors": [
                "Haiyu Wu",
                "V\u0131\u0301tor Albiero",
                "KS Krishnapriya",
                "Michael C King",
                "Kevin W Bowyer"
            ],
            "title": "Face recognition accuracy across demographics: Shining a light into the problem",
            "venue": "arXiv preprint arXiv:2206.01881,",
            "year": 2022
        },
        {
            "authors": [
                "Haiyu Wu",
                "Grace Bezold",
                "Aman Bhatta",
                "Kevin W Bowyer"
            ],
            "title": "Logical consistency and greater descriptive power for facial hair attribute learning",
            "venue": "arXiv preprint arXiv:2302.11102,",
            "year": 2023
        },
        {
            "authors": [
                "Xingkun Xu",
                "Yuge Huang",
                "Pengcheng Shen",
                "Shaoxin Li",
                "Jilin Li",
                "Feiyue Huang",
                "Yong Li",
                "Zhen Cui"
            ],
            "title": "Consistent instance false positive improves fairness in face recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Yongzhe Yan",
                "Stefan Duffner",
                "Xavier Naturel",
                "Anthony Berthelier",
                "Christophe Garcia",
                "Christophe Blanc",
                "Thierry Chateau"
            ],
            "title": "Two-stage human hair segmentation in the wild using deep shape prior",
            "venue": "Pattern Recognition Letters,",
            "year": 2020
        },
        {
            "authors": [
                "Ho-Sub Yoon",
                "Seong-Woo Park",
                "Jang-Hee Yoo"
            ],
            "title": "Realtime hair segmentation using mobile-unet",
            "year": 2021
        },
        {
            "authors": [
                "Changqian Yu",
                "Jingbo Wang",
                "Chao Peng",
                "Changxin Gao",
                "Gang Yu",
                "Nong Sang"
            ],
            "title": "Bisenet: Bilateral segmentation network for real-time semantic segmentation",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Recent developments in deep neural networks (DNNs) have enabled learning complex feature hierarchies from raw data. With the availability of large datasets and computational power, end-to-end representation learning has become the most prominent method in computer vision as well as many other domains. Face recognition has been one of the most popular computer vision problems for researchers over the past decades, and face recognition has found extensive use in various real-world applications.\nAccuracy of face verification improved with the advent\nof DNNs [11,20,32,35,43]. However, studies have demonstrated that the accuracy of these models varies between demographic groups [1,10,12,16,21,36,40,41,44]. Although the initial approach to identify the root cause of these disparities involves examination of training data, even when the data is balanced among demographic groups, the models may still yield different impostor (images of two different individuals) and genuine (different images of the same individual) distributions [4, 13, 46].\nBeyond demographics, several face attributes, including facial hair, are investigated for possible causes of bias in face recognition (see Section 2). Facial hair can readily be used to change appearance in a significant way; e.g., a person can shave their long beard to have a clean-shaven face. At the same time, compared to other occlusions such as masks, sunglasses, hat, etc., one can be requested to take off their face mask to take a border-crossing photo, but it is not feasible to ask someone to shave at the border-crossing kiosk.\nPrevious works analyze the effect of facial hair on face recognition systems using a binary label, categorizing images as clean-shaven or facial hair. We believe the degree of similarity or difference in facial hairstyles can have a significant effect on the match score for a pair of images. In this work, we present the first experimental analysis to understand the effect of facial hair size on similarity scores. First, a semantic segmentation model is trained to label facial hair pixels. CelebA-HQ [19] and MORPH [34] datasets are used to create 2550 manual annotations of the extent of facial hair. A cross-dataset evaluation is conducted, also comparing accuracy across demographic groups. Then, pretrained ArcFace [11] and AdaFace [20] models are used to extract features and calculate similarity scores for different groups with varying extent of facial hair. We show that a large FMR variation can be observed on image pairs defined by facial hairstyle and this effect can be significantly reduced by employing adaptive thresholding.\nThis paper is organized as follows. In Section 2, we review related work on facial hair segmentation and its impact on face recognition. In Section 3, we describe our\nar X\niv :2\n30 8.\n15 74\n0v 1\n[ cs\n.C V\n] 3\n0 A\nug 2\n02 3\ndataset and experimental setup for building the segmentation model. Section 4 introduces the face matchers and dataset we use to analyze the effects of facial hair on similarity scores. Then, findings on FMR variation and proposed thresohlding method based on facial hair is presented. Finally, in Section 5, we conclude with a summary and suggestions for future research."
        },
        {
            "heading": "2. Related Work",
            "text": "Segmentation of facial hair region. In the literature, \u201chair segmentation\u201d typically refers to hair growing from the scalp (head) and does not include facial hair [17, 26, 30, 38, 39, 47, 48]. Early work on facial hair segmentation by Ngyuen et al. [31] proposes a layer extraction method to automatically decompose facial attributes such as beard and glasses. After decomposition, they apply post-processing to finalize the segmentation. They provide visual examples of segmentation results, but no quantitative results.\nIn [24], several steps are applied to detect beard and mustache regions. First, 79 facial landmarks are obtained using a modified Active Shape Model [37]. Next, the SelfQuotient [42] algorithm is used to eliminate illumination ar-\ntifacts. Then, four regions of interest (mustache, left beard, right beard and middle beard) are defined. A binary sparse classifier is used to label these regions as skin or facial hair. In the follow-up work [23], the Self-Quotient step is removed to reduce the complexity. Instead, they build a feature vector consisting of Histogram of Gabor (HoG) and Histogram of Oriented Gradient of Gabor (HOGG) at different directions and frequencies, to detect facial hair on image patches. They report that this method is faster but less accurate than their previous work. These two approaches are later combined with the SLIC [2] superpixel algorithm in [25]. Neither the annotations nor the segmentation model from this work have been made publicly-available. Also, the accuracy figures are for coarse subregions of the face. For example, if an image is marked as hair in the left beard area of the image and any hair is found in the left beard area, it is counted as a binary match. The spatial extent of the match is not considered.\nImpact of facial hair on recognition accuracy. In an earlier work [14] before the deep learning era, effects of facial hair with several facial attributes are analyzed. Three\nwell-known algorithms with cosine similarity are applied to measure the distance between 2,144 face images from FERET [33]. They only use a binary label to mark images as facial hair or no facial hair. Their results suggest that when facial hair is present in one image and not another, face recognition accuracy improves.\nIn [28], effects of 7 covariates, including facial hair, are analyzed using 5 DNNs. Four binary labels are used for facial hair: no facial hair, mustache, goatee and beard. They report that state-of-the-art (SOTA) deep models are able to handle facial hair variations, and facial hair does not change the key features of faces.\nIn [40], two popular deep learning models are employed to investigate the influence of facial hair on face verification. \u201cNo beard\u201d and \u201c5 o\u2019clock shadow\u201d binary attributes are compared, and they report that \u201c5 o\u2019clock shadow\u201d results in much higher recognition rates.\nThe effects of scalp hair and facial hair on face recognition are investigated in [8]. While a segmentation network is utilized to segment scalp hair, they use the combination of Microsoft Face API [29] and Amazon Rekognition [6] to make a binary classification of images as clean-shaven or facial hair. They report that the accuracy of classifying clean-shaven is lower for African-American than for Caucasian, and suggest that a better algorithm is needed to detect facial hair accurately across demographic groups.\nIn work similar to ours [45], a facial hair attribute dataset is built and a facial hair classifier is trained to explore the effect of facial hair. They use binary labels to define beard area as clean-shaven, chin area and side to side and do not take into account the specific extent of beard region.\nNovelty of this work. Previous works studying how facial hair impacts recognition accuracy only considered binary facial hair attributes such as beard/no-beard, whereas we propose to segment the facial hair region in each image to characterize facial hairstyle similarity of a pair. No previous work has examined the accuracy of facial hair segmentation across demographic groups, whereas we compare accuracy of our segmenter across Caucasian and African-American. We present the first experimental results to show that an adaptive thresholding scheme can reduce the FMR variation across categories of hairstyle-defined images pairs."
        },
        {
            "heading": "3. Facial Hair Segmentation",
            "text": ""
        },
        {
            "heading": "3.1. Training Segmentation Model",
            "text": "The Bilateral Segmentation Network (BiSeNet) [49] is used to train our facial hair segmenter. This architecture is designed to encode spatial information through Spatial Path and Context Path to provide sufficient receptive field. We did not observe accuracy improvements with Spatial Path, so we only use Context Path. The implementation [9] is\ntrained to classify each pixel as facial hair or not facial hair. Accuracy is measured on two datasets as Intersection over Union (IoU) of the detected facial hair region with the manually specified facial hair region (\u201cground truth\u201d).\nCelebA-HQ [19] is used to create facial hair annotations in 1024 \u00d7 1024 resolution. CelebA-HQ images correspond to a subset of CelebA [27], with steps applied to obtain consistent quality and center the images on the facial region. These steps include: artifact removal, 4x superresolution, mirror padding, Gaussian filtering, cropping and resampling to 1024\u00d7 1024.\nA total of 2550 images were manually annotated (using labelme [22]) to outline the extent of the facial hair region. To establish the boundary between scalp hair and facial hair, facial hair above the bottom of the ear was not annotated. During annotation, we observe that it is difficult to annotate a region consistently as facial hair or not if facial hair is very short. Consequently, we expect noisier annotations for short facial hair. Poor lighting and low-quality images also result in noisier annotations. To minimize the effect of noisy labels on accuracy evaluation, we use two labels to mark the images: facial hair and five o\u2019clock shadow. The remaining areas of the images are considered as not facial hair. This allows us to divide our data into 3 parts: (1) images with only the facial hair label, (2) images with both facial hair and five o\u2019clock shadow labels, and (3) images with only five o\u2019clock shadow label. Our training set consists of images from all three subsets. However, the validation and test sets do not include images with five o\u2019clock shadow. We create a second test set with only five o\u2019clock shadow to report accuracy separately for this.\nCelebA-HQ images with facial hair annotations are used for training (2000 images), validation (150 images) and test (200 images) sets. Additionally, 1500 clean-shaven images in CelebA-HQ, including both male and female identities, are also used during training to learn clean-shaven faces. CelebA-HQ images and annotations are downsized to 512 \u00d7 512 as we did not observe improvement in accuracy with higher resolution. Scaling (0.75, 1.25, 1.5), horizontal flipping, rotation (90\u25e6, 270\u25e6) and color jitter are used to augment the training data. 480 \u00d7 480 random crops are then obtained to feed the network. Additionally, we apply random JPEG compression as we observe improvements on MORPH images with this additional mode of augmentation."
        },
        {
            "heading": "3.2. Results",
            "text": "The MORPH dataset [34] was created to support research on face aging and it has also become heavily used in studying demographic variation in accuracy [3\u20135,8,45]. For a cross-dataset evaluation, 100 images of African-American males (AAM) and 100 images of Caucasian males (CM) are annotated in 480 \u00d7 400 resolution. We use a facial hair attribute classifier [45] to select 10 images in each of 10\ncategories, for each of African-American males and Caucasian males in MORPH (200 images total). Facial hair categories [45] include chinArea short mustacheConnected, chinArea medium mustacheConnected, chinArea short noMustache, chinArea short isolated, sideToSide short mustacheConnected, sideToSide medium mustacheConnected, sideToSide short isolated, sideToSide long mustacheConnected, mustacheOnly long and mustacheOnly short. Annotations were made independently by two annotators, in order to be able to report the IoU between two independent manual annotations (see Table 1).\nAccuracy is reported on two datasets. Note that we expect lower IoU accuracy on MORPH images because training and validation sets consist only of CelebA-HQ images. Also, image quality of MORPH images is lower than CelebA-HQ (our CelebA-HQ annotations include finer details of facial hair; however, it is not easy to capture details on MORPH, especially if facial hair is short, because of image compression). Since the predictions on five o\u2019clock shadow are not as consistent as on longer facial hair, we divide the CelebA-HQ test set into 150 images labeled with only facial hair label and another 50 images with only five o\u2019clock shadow label. Figure 1 shows examples of CelebAHQ images with manual annotations and model predictions.\nTable 1 lists mean and standard deviation of IoU accuracy for the 5 models trained and tested with 5 different data splits. To analyze the impact of training set size, we also measured the performance of networks trained with 500, 1000, and 1500 facial hair images. These subsets\nwere randomly selected 3 times from a total of 2000 facial hair images for each of the 5 data splits. The model achieves 84.85% IoU on the CelebA-HQ facial hair set which consists of 150 images with only facial hair label. IoU decreases to 64.3% for CelebA-HQ five o\u2019clock shadow which is composed of 50 images annotated with five o\u2019clock shadow label. The reason for this drop in accuracy is the ambiguity of annotating near clean-shaven facial hair length as facial hair or not facial hair (Figure 1). Note that IoU is reported for facial hair in Table 1. IoU of not facial hair label is measured as 98.74% for test. This reflects the fact that higher IoU is generally expected on matching larger regions, as the disagreements on the boundary become a smaller fraction of the total area. We didn\u2019t observe mislabelling for our test set of 200 clean-shaven images.\nIoU accuracy on CelebA-HQ test images is shown in Table 2 as a function of facial hair ratio. Facial hair ratio is the (number of pixels annotated as facial hair) / (size of im-\nage), ranging from 0 to about 35% (see Figure 2). The same number of pixels incorrectly predicted as facial hair can be a large fraction of a small facial hair region or a small fraction of a large facial hair region. Thus, there is an overall trend of higher IoU accuracy for larger facial hair regions.\nThe results show that IoU on African-Americans (79.1%) is slightly worse than Caucasians (81.77%). One reason for this small difference is that, even though we use a facial attribute classifier [45] to balance the test sets for these 2 demographic groups, average facial hair ratios of ground truths is 0.063 for Caucasians and 0.054 for AfricanAmericans. Note that, it is expected to observe lower IoU accuracy for smaller facial hair areas (see Table 2). In addition to the performance of the proposed model, IoU accuracy of two human annotators is also reported in Table 1. Surprisingly, annotations by two different persons only achieve 78.39% IoU. This reflects the ambiguity of facial hair definition, especially in the presence of short facial hair, poor lighting and low quality images."
        },
        {
            "heading": "4. Face Matching",
            "text": "In this work, we use two face recognition models to observe if the impact of facial hair on verification is consistent for two well-known SOTA face recognition models. ArcFace [11] is one of the most popular models in recent years. A loss function \u201cAdditive Angular Margin Loss\u201d is\nproposed to increase the discriminative power of features and help stabilize training. The version with ResNet100 [18] backbone trained on Glint360K [7] is used in our experiments. AdaFace [20], which assigns different importance to samples based on image quality, is a more recent model that achieves SOTA accuracy on several datasets. It avoids focusing on images that are unidentifiable because of low quality, instead emphasizing hard-yet-recognizable instances."
        },
        {
            "heading": "4.1. Impact of Facial Hair on Accuracy",
            "text": "The curated version of MORPH in [5] is used to analyze the effect of facial hair size on face verification for AfricanAmerican and Caucasian males. Face representations of 35,276 images of 8,835 Caucasian males and 56,245 images of 8,839 African-American males are obtained using the ArcFace and AdaFace models. Cosine similarity is used to obtain matching scores. Facial hair regions are detected by the segmentation model. Then, the number of facial hair pixels is divided by the total number of pixels in the image to calculate facial hair ratio for creating pair groups. Figure 3 shows the distribution of facial hair ratio across AfricanAmerican and Caucasian male images.\nIt is visually observed that near clean-shaven length hair can cause the segmentation network to predict small areas as facial hair. Furthermore, since we determined not to label facial hair above the bottom of the ear, face images\nwith sideburns may also lead to predictions of small regions. For this reason, we count the images as clean-shaven if the facial hair ratio is < 0.001. Two other thresholds, 0.1 and 0.15 are set to analyze the impact of the extent of the facial hair region. In our experiments, we build the following impostor and authentic pair groups: clean-shaven vs. clean-shaven (cl vs cl), clean-shaven vs. facial hair (cl vs fh) and facial hair vs. facial hair (fh vs fh). Four facial hair ranges; (< 0.001), (\u2265 0.001 & < 0.1), (\u2265 0.1) and (\u2265 0.15) are depicted respectively as cl, fh S, fh L1 and fh L2 in the Figure 4.\nSimilarity score distributions of the different pair groups are shown in Figure 4 for African-Americans and Caucasians. The pair group of cl vs cl is included in the plots in the first and second columns to act as a baseline for comparison. It can be observed by looking at the cl vs fh L1, cl vs fh S and cl vs cl pairs of both demographics that, the greater amount of facial hair size that differs between the images leads to lower similarity scores on genuine pairs (Figure 4 left column). The same effect can be observed on impostor pairs of AfricanAmericans. However, Caucasian impostor distributions are similar to each other.\nOn the other hand, if both images in a genuine pair have a great amount of facial hair gen fh L2 vs fh L2, similarity scores get closer to gen cl vs cl if it is not higher. However, imp fh L2 vs fh L2 impostor distributions of 2 demographic groups are shifted to the higher scores significantly compared to imp cl vs cl impostors. For these reasons, it can be claimed that adaptive thresholds can be used, according to the amount of facial hair, to improve verification performance. For example, if the reference image has a great amount of facial hair, we\ncan expect lower scores for a clean-shaven query face image and much higher similarity scores for a comparison with an image that has a similar amount of facial hair as the reference image (e.g. mean of gen cl vs fh L1 is 0.67 compared to mean of gen fh L2 vs fh L2 that is 0.74 for Caucasians).\nFalse Match Rate (FMR) and False Non-Match Rate (FNMR) analysis is employed across pair groups to assess the facial hair impact quantitatively. First, we find a threshold value to have a 1-in-10,000 FMR for Caucasians and African-Americans separately. We observe that, FMR of imp fh L2 vs fh L2 pairs increases to 4.23 \u00d7 10\u22124 for African-Americans and 9.47 \u00d7 10\u22124 for Caucasians. Note that, imp cl vs fh L1 has the lowest FMR among our pair groups (3.17\u00d710\u22125 for AAM and 3.9\u00d710\u22125 for CM). An Inequity ratio of (max FMR across groups)/(min FMR across groups) proposed by the NIST FRVT report (see section 2.2 of [15]) is used to measure differences between pair groups. Using this measure for the facial hair categories indicates a hairstyle-based inequity of 24 for CM and 13 for AAM. These results suggest that partitioning pairs into subgroups based on facial hair size can be useful to detect subsets with high FMR and help maintaining the same FMR rate across all groups by determining threshold values according to facial hair.\nDisparity between the similarity scores of AfricanAmericans and Caucasians is further investigated in Figure 5. Since the plots obtained using AdaFace and ArcFace are quite similar, we only use the ArcFace model in Figure 5. It can be seen that, both demographics reach an Equal Error Rate around 5 \u00d7 10\u22125 indicating there is no significant gap in verification performance (see Figure 5a). Even though African-American pairs have higher FMR for all three groups, they have lower FNMR ((a) all pairs, (b) cl vs cl and (c) cl vs fh L1 in Figure 5). It is important to note that, their FNMR are getting closer to each other for cl vs cl and moving away from each other for cl vs fh L1. Since we have low error rates, a larger dataset with more genuine pairs is needed for a more accurate analysis."
        },
        {
            "heading": "4.2. Adaptive Threshold to Mitigate Bias",
            "text": "We conduct an experiment to show that facial hair predictions can be used to find adaptive thresholds for pair groups, created according to the size of facial hair, to alleviate the facial hair effect on targeted FMR. We first randomly divide the MORPH dataset into two parts: validation and test. Note that they have the same number of disjoint subjects, but they can have a different number of pairs according the number of clean-shaven and facial hair images. To have a 1-in-10,000 FMR, a global threshold is found by looking the all impostor similarity scores in the validation set. Also, threshold values for three subgroups,\ncl vs cl, cl vs fh L1 and fh L2 vs fh L2 are determined to achieve our FMR target on each group. Then, these threshold values are used on the disjoint test set to report results. The experimental protocol is applied separately for African-Americans and Caucasians.\nTable 3 shows the FMR on the test set using threshold values determined by the validation set. Mean and standard deviation of the 5 random splits are reported. Ratios of the worst and best FMR [15] across facial hair groups is also given in the last column. Due to low number of impostor pairs, zero FMR is observed in two splits for African-Americans using adaptive threshold, thus mean and standard deviation is reported using the other three data splits. Results show that there can be a 10 times difference between the highest FMR and lowest FMR across groups based on the facial hair for African-Americans and 25 times difference for Caucasians (similar results are observed for the whole dataset before validation-test split, 13 for African-Americans and 24 for Caucasians. See Section 4.1). We show that this impact can be diminished using an adaptive threshold that is determining a threshold based on the facial hair size. The ratio between the worst and best FMR for African-American reduces from 10.70 to 1.78 and from 25.87 to 1.27 for Caucasians."
        },
        {
            "heading": "5. Conclusions",
            "text": "To investigate how facial hair impacts recognition accuracy, we first train a semantic segmentation model to segment facial hair. IoU accuracy of the trained model is evaluated cross-dataset, and compared across African-American and Caucasian. This is the only facial hair segmenter to be evaluated across demographics, and the implementation is made available to the research community.\nUsing facial hair segmentations from our model, we analyze the impostor and genuine distribution based on the extent of facial hair in the pair of images. We find that the greater the difference in facial hair size between two images in an impostor or genuine pair, the lower the similarity score. Categories of facial hairstyles are defined for a pair of images, and large differences in FMR are observed between categories.\nOur findings demonstrate that utilizing a variable threshold determined by the amount of facial hair present can substantially mitigate the FMR bias due to facial hairstyle. The FMR max-min ratio for both African-American and Caucasian drops from nearly 10 and 25, respectively, to below 2 for both demographics. The variable threshold also makes it harder for someone to use facial hair to change their appearance in order to create a false non-match result."
        }
    ],
    "title": "Beard Segmentation and Recognition Bias",
    "year": 2023
}