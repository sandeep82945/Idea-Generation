{
    "abstractText": "Information retrieval (IR) or knowledge retrieval, is a critical component for many downstream tasks such as open-domain question answering (QA). It is also very challenging, as it requires succinctness, completeness, and correctness. In recent works, dense retrieval models have achieved state-of-the-art (SOTA) performance on in-domain IR and QA benchmarks by representing queries and knowledge passages with dense vectors and learning the lexical and semantic similarity. However, using single dense vectors and end-to-end supervision are not always optimal because queries may require attention to multiple aspects and event implicit knowledge. In this work, we propose an information retrieval pipeline that uses entity/event linking model and query decomposition model to focus more accurately on different information units of the query. We show that, while being more interpretable and reliable, our proposed pipeline significantly improves passage coverages and denotation accuracies across five IR and QA benchmarks. It will be the go-to system to use for applications that need to perform IR on a new domain without much dedicated effort, because of its superior interpretability and cross-domain performance. 1",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiaodong Yu"
        },
        {
            "affiliations": [],
            "name": "Ben Zhou"
        },
        {
            "affiliations": [],
            "name": "Dan Roth"
        }
    ],
    "id": "SP:06be84aa1b82d514ed8dc63dc9b254e064a727cf",
    "references": [
        {
            "authors": [
                "Christopher Clark",
                "Kenton Lee",
                "Ming-Wei Chang",
                "Tom Kwiatkowski",
                "Michael Collins",
                "Kristina Toutanova."
            ],
            "title": "BoolQ: Exploring the surprising difficulty of natural yes/no questions",
            "venue": "Proceedings of the 2019 Conference of the North American Chap-",
            "year": 2019
        },
        {
            "authors": [
                "Nicola De Cao",
                "Gautier Izacard",
                "Sebastian Riedel",
                "Fabio Petroni."
            ],
            "title": "Autoregressive entity retrieval",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "ArXiv, abs/1810.04805.",
            "year": 2019
        },
        {
            "authors": [
                "Mor Geva",
                "Daniel Khashabi",
                "Elad Segal",
                "Tushar Khot",
                "Dan Roth",
                "Jonathan Berant."
            ],
            "title": "Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies",
            "venue": "Transactions of the Association for Computational Linguistics, 9:346\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Kelvin Guu",
                "Kenton Lee",
                "Zora Tung",
                "Panupong Pasupat",
                "Ming-Wei Chang."
            ],
            "title": "Realm: Retrievalaugmented language model pre-training",
            "venue": "ArXiv, abs/2002.08909.",
            "year": 2020
        },
        {
            "authors": [
                "Gautier Izacard",
                "Mathilde Caron",
                "Lucas Hosseini",
                "Sebastian Riedel",
                "Piotr Bojanowski",
                "Armand Joulin",
                "Edouard Grave"
            ],
            "title": "Unsupervised dense information retrieval with contrastive learning",
            "year": 2021
        },
        {
            "authors": [
                "Gautier Izacard",
                "Edouard Grave."
            ],
            "title": "Distilling knowledge from reader to retriever for question answering",
            "venue": "ArXiv, abs/2012.04584.",
            "year": 2020
        },
        {
            "authors": [
                "Mandar Joshi",
                "Eunsol Choi",
                "Daniel Weld",
                "Luke Zettlemoyer"
            ],
            "title": "TriviaQA: A large scale distantly",
            "year": 2017
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas O\u011fuz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for open-domain question answering",
            "venue": "arXiv preprint arXiv:2004.04906.",
            "year": 2020
        },
        {
            "authors": [
                "Uszkoreit",
                "Quoc Le",
                "Slav Petrov."
            ],
            "title": "Natural questions: A benchmark for question answering research",
            "venue": "Transactions of the Association for Computational Linguistics, 7:452\u2013466.",
            "year": 2019
        },
        {
            "authors": [
                "Jimmy Lin",
                "Xueguang Ma",
                "Sheng-Chieh Lin",
                "JhengHong Yang",
                "Ronak Pradeep",
                "Rodrigo Nogueira."
            ],
            "title": "Pyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations",
            "venue": "Proceedings of the 44th Annual",
            "year": 2021
        },
        {
            "authors": [
                "Xueguang Ma",
                "Kai Sun",
                "Ronak Pradeep",
                "Jimmy J. Lin."
            ],
            "title": "A replication study of dense passage retriever",
            "venue": "ArXiv, abs/2104.05740.",
            "year": 2021
        },
        {
            "authors": [
                "Rodrigo Nogueira",
                "Kyunghyun Cho."
            ],
            "title": "Passage re-ranking with bert",
            "venue": "ArXiv, abs/1901.04085.",
            "year": 2019
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Zhilin Yang",
                "Peng Qi",
                "Saizheng Zhang",
                "Yoshua Bengio",
                "William Cohen",
                "Ruslan Salakhutdinov",
                "Christopher D. Manning."
            ],
            "title": "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
            "venue": "Proceedings of the 2018 Conference on Empiri-",
            "year": 2018
        },
        {
            "authors": [
                "Xiaodong Yu",
                "Wenpeng Yin",
                "Nitish Gupta",
                "Dan Roth."
            ],
            "title": "Event linking: Grounding event mentions to wikipedia",
            "venue": "ArXiv, abs/2112.07888.",
            "year": 2021
        },
        {
            "authors": [
                "Roth"
            ],
            "title": "Learning to decompose: Hypothetical",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Open-domain information retrieval is a task to retrieve relevant information for any type of queries. An ideal IR model should provide short, accurate and complete information to the downstream models to perform tasks such as open-domain question answering (QA). More importantly, an opendomain IR system should show good cross-dataset performance for it to be truly open-domain, and ideally be interpretable at the same time to provide diverse information and improvement directions for\n\u2217 Equal Contribution. 1Data and code are available here: https://github.com/\nCogComp/decomp-el-retriever.\ndownstream models. Recently, dense retrieval models (Karpukhin et al., 2020; Izacard et al., 2021) achieves SOTA performances on open-domain QA benchmarks by learning the dense representation of the query and the passages to measure the lexical and semantic similarities. The passages with higher similarity to the query representation will be retrieved.\nWhile these approaches achieve relatively high performances when supervised with in-domain training data, they are less ideal to be applied to out-of-domain queries. This is mainly due to their single vector representations and the end-to-end training schemes. For example, in Fig. 1, we observe that questions may require symbolic reasoning processes. That is, to answer if McKenna Grace voted for Joe Biden in the 2020 election, we would need to perform information retrieval on implicit dimensions such as the legal age to be allowed to vote and other events that may change Grace\u2019s political stances. However, understanding and performing this process is inherently difficult for existing retrievers that rely on single dense vectors (Karpukhin et al., 2020). Even if they can perform such reasoning processes in a latent way through the representations, it is much less reliable compared with a symbolic process that first explicitly understands what to do and then retrieve relevant information. Furthermore, it is almost impossible ar X iv :2 30 8.\n04 75\n6v 1\n[ cs\n.C L\n] 9\nA ug\n2 02\n3\nto interpret the retrieval process embedded in dense vectors, which hinders research and applications that aim to improve from the mistakes.\nIn this paper, we propose an information retrieval pipeline that addresses the aforementioned issues, which tackles questions that require attention to multiple dimensions in an interpretable and reliable fashion. Specifically, our pipeline first tries to decompose the input query into relevant dimensions and associate it with hypothetical events that may contribute to the final answer. With the relevant events, we build on top of the advances of event-linking models (Yu et al., 2021) and link the hypothetical events to real-world information from a traceable knowledge base. Compared with existing methods such as Contriever (Izacard et al., 2021) and DPR (Karpukhin et al., 2020), our pipeline does not require any large-scale pre-training, domain-specific training or parameter tuning. It is more interpretable as future work can examine the intermediate generations of hypothetical events and improve individual components based on more fine-grained observations.\nAt the same time, we show that our pipeline has more reliable cross-domain performances. On a test suite composed of five question-answering datasets, our proposed pipeline improves an average of 6% compared with the state-of-the-art retriever FiD-KD (Izacard and Grave, 2020) in crossdomain supervision settings, and over 10% compared with the unsupervised but heavily pre-trained Contriever model. Our proposed pipeline is the best-performing method, while being interpretable and reliable, for any applications that need to perform information retrieval overnight without much in-domain efforts and considerations."
        },
        {
            "heading": "2 Related Work",
            "text": "Traditionally, people use sparse vector methods, like TF-IDF and BM25, to calculate the lexical similarity between queries and documents. Though BM25 is still one of the best unsupervised retrieving methods, the lack of semantic understanding limits its performance. After the success of pretrained language models like BERT (Devlin et al., 2019), dense representations show a better semantic understanding of queries and documents. People start to use dense vectors from BERT to train a bi-encoder model as the retriever (Karpukhin et al., 2020) or a cross-encoder model as the ranker (Nogueira and Cho, 2019). Many works use dif-\nferent methods trying to improve the retrieval performance of the bi-encoder. Just to name a few: Karpukhin et al. (2020) use annotated Question Answering pairs of queries and documents to train a bi-encoder model with hard negatives mined from BM25. Guu et al. (2020) combine the bi-encoder training into the pre-training of BERT. Izacard et al. (2021) propose an unsupervised bi-encoder retriever by sampling two spans within the same document to create positive pairs. Though different works focus on different parts of the bi-encoder training, using a single vector to represent all the information of the query remains the same, and the effort is always to make the dense vector better understand the query. In our work, we want to point out that encoding the query into a single vector is less optimal, and we aim to use different modules to retrieve passages that are relevant to specific information units of the query, which makes the retrieval process more interpretable and reliable."
        },
        {
            "heading": "3 Method",
            "text": "In this section, we describe our information retrieval pipeline, as overviewed in Fig. 2. In general, our information retrieval is a pipeline that can be described as linking -> decomposition -> linking. Specifically, we first conduct entity and event linking on the input query question to retrieve relevant documents of explicit entities and events in the query. Then, we apply a decomposition model on the query to generate potential supporting events and conduct event linking again. The idea is that our core retriever, the event-linking model, works on identifying similar events from a knowledge base. That being said, the decomposition process is designed to associate the query with similar supporting events, which will be properly utilized by the event-linking system."
        },
        {
            "heading": "3.1 Title Generation",
            "text": "As shown in Figure 2, given input question/query Q, we first use the entity linking system GENRE (De Cao et al., 2021) to acquire ten titles2 and the event linking system EveLink (Yu et al., 2021) to acquire another five titles from Wikipedia. Then, we use the decomposition model and follow previous settings (Zhou et al., 2022) to generate five sets of decompositions, each containing three sentences. For each of these sentences from decomposition, we use the event linking model to acquire another\n2We get ten since this is the default of the model we use.\nfive titles.3 We use the joint set of the resulting titles as the starting point for further passage-level information retrieval. Both entity linking model and event linking model are trained on Wikipedia hyperlinks, without any in-domain supervision."
        },
        {
            "heading": "3.2 Passage Selection",
            "text": "Since both the entity linking model and event linking model only retrieve Wikipedia titles, instead of a specific passage, to fairly compare with the previous dense retriever, we build a simple passage selection module to select passages from the Wikipedia pages generated in the previous step. Coarse Filtering. We split Wikipedia pages into DPR-styled text blocks, each with 100 words, and then use BM-25 implemented by (Ma et al., 2021) as an efficient method to select 200 passages as candidates. In practice, we observe that 200 passages are almost the same as document-level coverages. Fine-grained Ranking. To re-rank coarse passage selections and get more fine-grained selections, we train a T5-large model based on existing datasets\u2019 evidence annotations. Specifically, the model takes an input sequence that is the concatenation of a question and a piece of context. When the context contributes to the solution-finding process, the output sequence will be a positive token, and if the context is irrelevant to the question, the output sequence will be a negative token. We propose two versions of our pipeline that use different source datasets. The first version uses HotpotQA annota-\n3Five is a magic number, and we do not tune this parameter on target datasets.\ntions, with the annotated sentences relevant to the question as positive contexts and other randomly selected sentences as negative contexts. We randomly sample the same amount of negative contexts as the positive ones and result in 430k question-context pairs. The second version uses NaturalQuestions as the supervision source. Specifically, we use the hard negatives provided in Karpukhin et al. (2020) for the NQ dataset. To clarify the comparisons, we down-sample the NQ supervision to the same size as the HotpotQA model. Both versions are trained with 1 epoch and other default parameters set by the Transformers package (Wolf et al., 2020)."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets and Metrics",
            "text": "We use several QA datasets that require information retrieval (IR) as our test bench. Specifically, we consider NaturalQuestion (NQ) (Kwiatkowski et al., 2019), TriviaQA (TQA) (Joshi et al., 2017), HotpotQA (Hotpot) (Yang et al., 2018), BoolQA (Clark et al., 2019) and StrategyQA (Geva et al., 2021). Following previous work (Izacard et al., 2021), we use the official development sets of each dataset for evaluation purposes. As we do not use any parameters that are specifically tuned or adjusted according to benchmark performances, we do not separately prepare a development set. Table 1 lists the number of instances we use for evaluation from each dataset. For the first three datasets, we consider the percentage of answers successfully contained by the top-K retrieved texts, where we denote top-K as data@K (also known as recall@K). We remove all yes/no questions from HotpotQA for accurate recall calculation. For BoolQA and StrategyQA, since the answers are binary (yes/no), we employ an entailment model from Zhou et al. (2022) to derive the final answer and evaluate accuracies."
        },
        {
            "heading": "4.2 Baselines",
            "text": "We consider three IR baselines: Contriever (Izacard et al., 2021), DPR (Karpukhin et al., 2020) and\nFiD-KD (Izacard and Grave, 2020). To fairly compare different methods, we aim to report results that are not directly supervised with in-domain training data. Contriever is an unsupervised model. DPR is trained on NQ. For FiD-KD, since there are multiple released models, we report cross-dataset performances using the TQA-trained system for NQ, and the NQ-trained system for all other datasets. 4"
        },
        {
            "heading": "4.3 Results",
            "text": "Table 2 shows the IR performances of our proposed pipeline compared to state-of-the-art not-directlysupervised IR systems. Our proposed pipeline outperforms the unsupervised Contriever model by an average of 19% across three datasets with recall@5 results. Compared with the cross-domain DPR model, our pipeline improves a drastic 18% on TriviaQA, and 23% on HotpotQA. Even the more competitive FiD-KD model suffers from the cross-dataset transfer, surpassed by our proposed pipeline for 8% on TriviaQA, and 18% on HotpotQA. In general, we show that our proposed pipeline gets better open-domain performances without in-domain supervision and is potentially more reliable for real-world applications."
        },
        {
            "heading": "4.4 Boosting with Correction",
            "text": "Many work use large language models (LLM) to perform knowledge extraction for downstream tasks. However, since LLM generations are not guaranteed to be truthful, we still need to perform a traceable information retrieval for reliability-\n4We use DPR and FiD-KD implemented by (Lin et al., 2021).\ncritical tasks, similar to the pipeline we propose in this work. In order to both utilize the memorization capability of LLMs and produce traceable and trustworthy information, we study if our pipeline can be further improved with LLM correction on the generated decompositions. Specifically, we follow Zhou et al. (2022) and ask GPT-3 to fix any factual errors in the decompositions generated by T5, after which we conduct the same linking and retrieval processes. Table 3 compares the performances of our pipeline after GPT-3 correction on HotpotQA, the most challenging benchmark in our evaluation suite. We observe a 6% improvement on both recall@5 and recall@20. This shows the versatility of our pipeline, which can be easily improved by the advancements of individual components, instead of proposing new frameworks and spending a considerable amount of resources on training or pre-training each time."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this work, we propose an information retrieval (IR) pipeline built on the advances of query decomposition and event linking. Specifically, we associate the input query with generated events relevant to the query through the decomposition step and then link these events to real-world facts in a knowledge base with an event-linking model. We show that with simple passage selection through BM25 and cross-domain QA supervision, our pipeline outperforms existing state-of-the-art unsupervised or cross-domain IR models on five datasets by an average of 6%. Compared with existing methods, our pipeline does not involve heavy pre-training, parameter tuning, or domain-specific supervision. It is an easy-to-use method that achieves reliable and stable performances without much effort for any new domain. It can also be used with large language models to retrieve faithful and traceable information that is even more accurate."
        },
        {
            "heading": "6 Limitations",
            "text": "\u2022 Though both entity linking and event linking models do not use any human-annotated data, our passage selection model is still supervised. How to build an unsupervised but effection passage selection model could be an interesting future work.\n\u2022 In this work, we only focus on the retriever without a reader. Combining our retrieval pipeline with a reader to evaluate the final exact match accuracy could be a good QA system."
        }
    ],
    "title": "Building Interpretable and Reliable Open Information Retriever for New Domains Overnight",
    "year": 2023
}