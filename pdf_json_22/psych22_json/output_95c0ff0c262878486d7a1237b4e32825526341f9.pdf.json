{
    "abstractText": "The ability to associate touch with sight is essential for tasks that require physically interacting with objects in the world. We propose a dataset with paired visual and tactile data called Touch and Go, in which human data collectors probe objects in natural environments using tactile sensors, while simultaneously recording egocentric video. In contrast to previous efforts, which have largely been confined to lab settings or simulated environments, our dataset spans a large number of \u201cin the wild\u201d objects and scenes. We successfully apply our dataset to a variety of multimodal learning tasks: 1) self-supervised visuo-tactile feature learning, 2) tactile-driven image stylization, i.e., making the visual appearance of an object more consistent with a given tactile signal, and 3) predicting future frames of a tactile signal from visuo-tactile inputs. * Indicates equal contribution 36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks. ar X iv :2 21 1. 12 49 8v 2 [ cs .C V ] 2 9 N ov 2 02 2",
    "authors": [
        {
            "affiliations": [],
            "name": "Fengyu Yang"
        },
        {
            "affiliations": [],
            "name": "Chenyang Ma"
        },
        {
            "affiliations": [],
            "name": "Jiacheng Zhang"
        },
        {
            "affiliations": [],
            "name": "Jing Zhu"
        },
        {
            "affiliations": [],
            "name": "Wenzhen Yuan"
        },
        {
            "affiliations": [],
            "name": "Andrew Owens"
        }
    ],
    "id": "SP:20f943b31f74523b09d838243af15d45b7daa4ac",
    "references": [
        {
            "authors": [
                "Edward H Adelson"
            ],
            "title": "On seeing stuff: the perception of materials by humans and machines",
            "venue": "In Human vision and electronic imaging VI,",
            "year": 2001
        },
        {
            "authors": [
                "Relja Arandjelovic",
                "Andrew Zisserman"
            ],
            "title": "Look, listen and learn",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2017
        },
        {
            "authors": [
                "Mohammad Babaeizadeh",
                "Chelsea Finn",
                "Dumitru Erhan",
                "Roy H Campbell",
                "Sergey Levine"
            ],
            "title": "Stochastic variational video prediction",
            "venue": "arXiv preprint arXiv:1710.11252,",
            "year": 2017
        },
        {
            "authors": [
                "David Bau",
                "Alex Andonian",
                "Audrey Cui",
                "YeonHwan Park",
                "Ali Jahanian",
                "Aude Oliva",
                "Antonio Torralba"
            ],
            "title": "Paint by word",
            "venue": "In arXiv:2103.10951,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Burka",
                "Siyao Hu",
                "Stuart Helgeson",
                "Shweta Krishnan",
                "Yang Gao",
                "Lisa Anne Hendricks",
                "Trevor Darrell",
                "Katherine J Kuchenbecker"
            ],
            "title": "Proton: A visuo-haptic data acquisition system for robotic learning of surface properties",
            "venue": "In 2016 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI),",
            "year": 2016
        },
        {
            "authors": [
                "Alex Burka",
                "Abhinav Rajvanshi",
                "Sarah Allen",
                "Katherine J Kuchenbecker"
            ],
            "title": "Proton 2: Increasing the sensitivity and portability of a visuo-haptic surface interaction recorder",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2017
        },
        {
            "authors": [
                "L Alexander"
            ],
            "title": "Burka. Instrumentation, data, and algorithms for visually understanding haptic surface properties",
            "venue": "Dissertations available from ProQuest,",
            "year": 2018
        },
        {
            "authors": [
                "Roberto Calandra",
                "Andrew Owens",
                "Dinesh Jayaraman",
                "Justin Lin",
                "Wenzhen Yuan",
                "Jitendra Malik",
                "Edward H. Adelson",
                "Sergey Levine"
            ],
            "title": "More than a feeling: Learning to grasp and regrasp using vision and touch",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2018
        },
        {
            "authors": [
                "Roberto Calandra",
                "Andrew Owens",
                "Manu Upadhyaya",
                "Wenzhen Yuan",
                "Justin Lin",
                "Edward H Adelson",
                "Sergey Levine"
            ],
            "title": "The feeling of success: Does touch sensing help predict grasp outcomes",
            "venue": "Conference on Robot Learning (CoRL),",
            "year": 2017
        },
        {
            "authors": [
                "Moitreya Chatterjee",
                "Anoop Cherian"
            ],
            "title": "Sound2sight: Generating visual dynamics from sound and context",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Vivian Chu",
                "Ian McMahon",
                "Lorenzo Riano",
                "Craig G McDonald",
                "Qin He",
                "Jorge Martinez Perez- Tejada",
                "Michael Arrigo",
                "Trevor Darrell",
                "Katherine J Kuchenbecker"
            ],
            "title": "Robotic learning of haptic adjectives through physical interaction",
            "venue": "Robotics and Autonomous Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Shaowei Cui",
                "Rui Wang",
                "Junhang Wei",
                "Jingyi Hu",
                "Shuo Wang"
            ],
            "title": "Self-attention based visualtactile fusion learning for predicting grasp outcomes",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2020
        },
        {
            "authors": [
                "Mark R. Cutkosky",
                "Robert D. Howe",
                "William R. Provancher"
            ],
            "title": "Force and Tactile Sensors, pages 455\u2013476",
            "year": 2008
        },
        {
            "authors": [
                "Virginia R de Sa"
            ],
            "title": "Learning classification with unlabeled data",
            "venue": "Advances in neural information processing systems,",
            "year": 1994
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A largescale hierarchical image database",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Emily L. Denton",
                "Rob Fergus"
            ],
            "title": "Stochastic video generation with a learned prior",
            "venue": "In ICML,",
            "year": 2018
        },
        {
            "authors": [
                "Karan Desai",
                "Justin Johnson"
            ],
            "title": "Virtex: Learning visual representations from textual annotations",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Nima Fazeli",
                "Miquel Oller",
                "Jiajun Wu",
                "Zheng Wu",
                "Joshua B Tenenbaum",
                "Alberto Rodriguez"
            ],
            "title": "See, feel, act: Hierarchical learning for complex manipulation skills with multisensory fusion",
            "venue": "Science Robotics,",
            "year": 2019
        },
        {
            "authors": [
                "Chelsea Finn",
                "Ian Goodfellow",
                "Sergey Levine"
            ],
            "title": "Unsupervised learning for physical interaction through video prediction",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Hang Gao",
                "Huazhe Xu",
                "Qi-Zhi Cai",
                "Ruth Wang",
                "Fisher Yu",
                "Trevor Darrell"
            ],
            "title": "Disentangling propagation and generation for video prediction",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Ruohan Gao",
                "Yen-Yu Chang",
                "Shivani Mall",
                "Li Fei-Fei",
                "Jiajun Wu"
            ],
            "title": "Objectfolder: A dataset of objects with implicit visual, auditory, and tactile representations",
            "venue": "CoRL,",
            "year": 2021
        },
        {
            "authors": [
                "Ruohan Gao",
                "Zilin Si",
                "Yen-Yu Chang",
                "Samuel Clarke",
                "Jeannette Bohg",
                "Li Fei-Fei",
                "Wenzhen Yuan",
                "Jiajun Wu"
            ],
            "title": "Objectfolder 2.0: A multisensory object dataset for sim2real transfer",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Geng",
                "Max Hamilton",
                "Andrew Owens"
            ],
            "title": "Comparing correspondences: Video prediction with correspondence-wise losses",
            "venue": "Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Paul Guerrero",
                "Milos Hasan",
                "Kalyan Sunkavalli",
                "Radomir Mech",
                "Tamy Boubekeur",
                "Niloy Mitra"
            ],
            "title": "Matformer: A generative model for procedural materials",
            "venue": "ACM Trans. Graph.,",
            "year": 2022
        },
        {
            "authors": [
                "Yu Guo",
                "Cameron Smith",
                "Milo\u0161 Ha\u0161an",
                "Kalyan Sunkavalli",
                "Shuang Zhao"
            ],
            "title": "Materialgan: Reflectance capture using a generative svbrdf model",
            "venue": "ACM Trans. Graph.,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "arXiv preprint arXiv:1512.03385,",
            "year": 2015
        },
        {
            "authors": [
                "Phillip Isola",
                "Jun-Yan Zhu",
                "Tinghui Zhou",
                "Alexei A. Efros"
            ],
            "title": "Image-to-image translation with conditional adversarial networks",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "Justin Johnson",
                "Alexandre Alahi",
                "Li Fei-Fei"
            ],
            "title": "Perceptual losses for real-time style transfer and super-resolution, 2016",
            "year": 2016
        },
        {
            "authors": [
                "Justin Johnson",
                "Agrim Gupta",
                "Li Fei-Fei"
            ],
            "title": "Image generation from scene graphs",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Micah Johnson",
                "Forrester Cole",
                "Alvin Raj",
                "Edward Adelson"
            ],
            "title": "Microgeometry capture using an elastomeric sensor",
            "venue": "ACM Trans. Graph.,",
            "year": 2011
        },
        {
            "authors": [
                "Micah K Johnson",
                "Edward H Adelson"
            ],
            "title": "Retrographic sensing for the measurement of surface texture and shape",
            "venue": "In 2009 IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Armand Joulin",
                "Laurens van der Maaten",
                "Allan Jabri",
                "Nicolas Vasilache"
            ],
            "title": "Learning visual features from large weakly supervised data",
            "venue": "In European Conference on Computer Vision,",
            "year": 2016
        },
        {
            "authors": [
                "Zhanat Kappassov",
                "Juan-Antonio Corrales",
                "V\u00e9ronique Perdereau"
            ],
            "title": "Tactile sensing in dexterous robot hands \u2014 review",
            "venue": "Robotics and Autonomous Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Bruno Korbar",
                "Du Tran",
                "Lorenzo Torresani"
            ],
            "title": "Co-training of audio and video representations from self-supervised temporal synchronization",
            "year": 2018
        },
        {
            "authors": [
                "Mike Lambeta",
                "Po wei Chou",
                "Stephen Tian",
                "Brian Yang",
                "Benjamin Maloon",
                "Victoria Rose Most",
                "Dave Stroud",
                "Raymond Santos",
                "Ahmad Byagowi",
                "Gregg Kammerer",
                "Dinesh Jayaraman",
                "Roberto Calandra"
            ],
            "title": "Digit: A novel design for a low-cost compact high-resolution tactile sensor with application to in-hand manipulation",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2020
        },
        {
            "authors": [
                "Susan J Lederman",
                "Roberta L Klatzky"
            ],
            "title": "Hand movements: A window into haptic object recognition",
            "venue": "Cognitive Psychology,",
            "year": 1987
        },
        {
            "authors": [
                "Michelle A Lee",
                "Yuke Zhu",
                "Krishnan Srinivasan",
                "Parth Shah",
                "Silvio Savarese",
                "Li Fei-Fei",
                "Animesh Garg",
                "Jeannette Bohg"
            ],
            "title": "Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks",
            "venue": "arXiv preprint arXiv:1810.10191,",
            "year": 2018
        },
        {
            "authors": [
                "Seung Hyun Lee",
                "Wonseok Roh",
                "Wonmin Byeon",
                "Sang Ho Yoon",
                "Chan Young Kim",
                "Jinkyu Kim",
                "Sangpil Kim"
            ],
            "title": "Sound-guided semantic image manipulation",
            "venue": "arXiv preprint arXiv:2112.00007,",
            "year": 2021
        },
        {
            "authors": [
                "Hendrik PA Lensch",
                "Michael Goesele",
                "Yung-Yu Chuang",
                "Tim Hawkins",
                "Steve Marschner",
                "Wojciech Matusik",
                "Gero Mueller"
            ],
            "title": "Realistic materials in computer graphics",
            "venue": "In ACM SIGGRAPH 2005 Courses, pages 1\u2013es",
            "year": 2005
        },
        {
            "authors": [
                "Tingle Li",
                "Yichen Liu",
                "Andrew Owens",
                "Hang Zhao"
            ],
            "title": "Learning visual styles from audio-visual associations",
            "venue": "European Conference on Computer Vision (ECCV),",
            "year": 2022
        },
        {
            "authors": [
                "Yunzhu Li",
                "Jun-Yan Zhu",
                "Russ Tedrake",
                "Antonio Torralba"
            ],
            "title": "Connecting touch and vision via cross-modal prediction",
            "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Justin Lin",
                "Roberto Calandra",
                "Sergey Levine"
            ],
            "title": "Learning to identify object instances by touch: Tactile recognition via multimodal matching",
            "venue": "In 2019 International Conference on Robotics and Automation (ICRA),",
            "year": 2019
        },
        {
            "authors": [
                "William Lotter",
                "G. Kreiman",
                "David D. Cox"
            ],
            "title": "Deep predictive coding networks for video prediction and unsupervised learning",
            "venue": "ArXiv,",
            "year": 2017
        },
        {
            "authors": [
                "Dhruv Mahajan",
                "Ross Girshick",
                "Vignesh Ramanathan",
                "Kaiming He",
                "Manohar Paluri",
                "Yixuan Li",
                "Ashwin Bharambe",
                "Laurens Van Der Maaten"
            ],
            "title": "Exploring the limits of weakly supervised pretraining",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Andrew Melnik",
                "Luca Lach",
                "Matthias Plappert",
                "Timo Korthals",
                "Robert Haschke",
                "Helge Ritter"
            ],
            "title": "Tactile sensing and deep reinforcement learning for in-hand manipulation tasks",
            "venue": "In IROS Workshop on Autonomous Object Manipulation,",
            "year": 2019
        },
        {
            "authors": [
                "Pedro Morgado",
                "Nuno Vasconcelos",
                "Ishan Misra"
            ],
            "title": "Audio-visual instance discrimination with cross-modal agreement",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Adithyavairavan Murali",
                "Yin Li",
                "Dhiraj Gandhi",
                "Abhinav Gupta"
            ],
            "title": "Learning to grasp without seeing",
            "venue": "In International Symposium on Experimental Robotics,",
            "year": 2018
        },
        {
            "authors": [
                "Jiquan Ngiam",
                "Aditya Khosla",
                "Mingyu Kim",
                "Juhan Nam",
                "Honglak Lee",
                "Andrew Y Ng"
            ],
            "title": "Multimodal deep learning",
            "venue": "In ICML,",
            "year": 2011
        },
        {
            "authors": [
                "Andrew Owens",
                "Alexei A. Efros"
            ],
            "title": "Audio-visual scene analysis with self-supervised multisensory features",
            "venue": "Eur. Conf. Comput. Vis.,",
            "year": 2018
        },
        {
            "authors": [
                "Andrew Owens",
                "Phillip Isola",
                "Josh McDermott",
                "Antonio Torralba",
                "Edward H. Adelson",
                "William T. Freeman"
            ],
            "title": "Visually indicated sounds",
            "venue": "In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Andrew Owens",
                "Jiajun Wu",
                "Josh H McDermott",
                "William T Freeman",
                "Antonio Torralba"
            ],
            "title": "Ambient sound provides supervision for visual learning",
            "venue": "In Eur. Conf",
            "year": 2016
        },
        {
            "authors": [
                "Taesung Park",
                "Alexei A. Efros",
                "Richard Zhang",
                "Jun-Yan Zhu"
            ],
            "title": "Contrastive learning for conditional image synthesis",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Viorica Patraucean",
                "Ankur Handa",
                "Roberto Cipolla"
            ],
            "title": "Spatio-temporal video autoencoder with differentiable memory",
            "venue": "arXiv preprint arXiv:1511.06309,",
            "year": 2015
        },
        {
            "authors": [
                "Senthil Purushwalkam",
                "Abhinav Gupta",
                "Danny M Kaufman",
                "Bryan Russell"
            ],
            "title": "Bounce and learn: Modeling scene dynamics with real-world bounces",
            "year": 1904
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "arXiv preprint arXiv:2102.12092,",
            "year": 2021
        },
        {
            "authors": [
                "Scott Reed",
                "Zeynep Akata",
                "Xinchen Yan",
                "Lajanugen Logeswaran",
                "Bernt Schiele",
                "Honglak Lee"
            ],
            "title": "Generative adversarial text to image synthesis",
            "venue": "In International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Philipp Ruppel",
                "Yannick Jonetzko",
                "Michael G\u00f6rner",
                "Norman Hendrich",
                "Jianwei Zhang"
            ],
            "title": "Simulation of the syntouch biotac sensor",
            "venue": "In International Conference on Intelligent Autonomous Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Shuran Song",
                "Andy Zeng",
                "Johnny Lee",
                "Thomas Funkhouser"
            ],
            "title": "Grasping in the wild: Learning 6dof closed-loop grasping from low-cost demonstrations",
            "venue": "Robotics and Automation Letters,",
            "year": 2020
        },
        {
            "authors": [
                "Subramanian Sundaram",
                "Petr Kellnhofer",
                "Yunzhu Li",
                "Jun-Yan Zhu",
                "Antonio Torralba",
                "Wojciech Matusik"
            ],
            "title": "Learning the signatures of the human grasp using a scalable tactile",
            "venue": "glove. Nature,",
            "year": 2019
        },
        {
            "authors": [
                "Stephen Tian",
                "Frederik Ebert",
                "Dinesh Jayaraman",
                "Mayur Mudigonda",
                "Chelsea Finn",
                "Roberto Calandra",
                "Sergey Levine"
            ],
            "title": "Manipulation by feel: Touch-based control with deep predictive models",
            "venue": "In 2019 International Conference on Robotics and Automation (ICRA),",
            "year": 2019
        },
        {
            "authors": [
                "Yonglong Tian",
                "Dilip Krishnan",
                "Phillip Isola"
            ],
            "title": "Contrastive multiview coding",
            "venue": "In European conference on computer vision,",
            "year": 2020
        },
        {
            "authors": [
                "Ruben Villegas",
                "Arkanath Pathak",
                "Harini Kannan",
                "Dumitru Erhan",
                "Quoc V. Le",
                "Honglak Lee"
            ],
            "title": "High fidelity video prediction with large stochastic recurrent neural networks, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Shaoxiong Wang",
                "Mike Lambeta",
                "Po-Wei Chou",
                "Roberto Calandra"
            ],
            "title": "Tacto: A fast, flexible, and open-source simulator for high-resolution vision-based tactile sensors",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2022
        },
        {
            "authors": [
                "Ting-Chun Wang",
                "Ming-Yu Liu",
                "Andrew Tao",
                "Guilin Liu",
                "Jan Kautz",
                "Bryan Catanzaro"
            ],
            "title": "Few-shot video-to-video synthesis",
            "venue": "arXiv preprint arXiv:1910.12713,",
            "year": 2019
        },
        {
            "authors": [
                "Ting-Chun Wang",
                "Ming-Yu Liu",
                "Jun-Yan Zhu",
                "Andrew Tao",
                "Jan Kautz",
                "Bryan Catanzaro"
            ],
            "title": "High-resolution image synthesis and semantic manipulation with conditional gans, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Ting-Chun Wang",
                "Ming-Yu Liu",
                "Jun-Yan Zhu",
                "Andrew Tao",
                "Jan Kautz",
                "Bryan Catanzaro"
            ],
            "title": "High-resolution image synthesis and semantic manipulation with conditional gans",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Zhou Wang",
                "Alan C Bovik",
                "Hamid R Sheikh",
                "Eero P Simoncelli"
            ],
            "title": "Image quality assessment: from error visibility to structural similarity",
            "venue": "IEEE transactions on image processing,",
            "year": 2004
        },
        {
            "authors": [
                "Youngsun Wi",
                "Pete Florence",
                "Andy Zeng",
                "Nima Fazeli"
            ],
            "title": "Virdo: Visio-tactile implicit representations of deformable objects",
            "venue": "arXiv preprint arXiv:2202.00868,",
            "year": 2022
        },
        {
            "authors": [
                "Yue Wu",
                "Rongrong Gao",
                "Jaesik Park",
                "Qifeng Chen"
            ],
            "title": "Future video synthesis with object motion prediction",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Wenzhen Yuan",
                "Siyuan Dong",
                "Edward Adelson"
            ],
            "title": "Gelsight: High-resolution robot tactile sensors for estimating geometry and force",
            "venue": "Sensors, 17:2762,",
            "year": 2017
        },
        {
            "authors": [
                "Wenzhen Yuan",
                "Siyuan Dong",
                "Edward H Adelson"
            ],
            "title": "Gelsight: High-resolution robot tactile sensors for estimating geometry and force",
            "year": 2017
        },
        {
            "authors": [
                "Wenzhen Yuan",
                "Chenzhuo Zhu",
                "Andrew Owens",
                "Mandayam A Srinivasan",
                "Edward H Adelson"
            ],
            "title": "Shape-independent hardness estimation using deep learning and a gelsight tactile sensor",
            "venue": "In International Conference on Robotics and Automation (ICRA),",
            "year": 2017
        },
        {
            "authors": [
                "Qiang Zhang",
                "Yunzhu Li",
                "Yiyue Luo",
                "Wan Shou",
                "Michael Foshey",
                "Junchi Yan",
                "Joshua B Tenenbaum",
                "Wojciech Matusik",
                "Antonio Torralba"
            ],
            "title": "Dynamic modeling of hand-object interactions via tactile sensing",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2021
        },
        {
            "authors": [
                "Richard Zhang",
                "Phillip Isola",
                "Alexei A. Efros",
                "Eli Shechtman",
                "Oliver Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric, 2018",
            "year": 2018
        },
        {
            "authors": [
                "Xilong Zhou",
                "Nima Khademi Kalantari"
            ],
            "title": "Adversarial single-image svbrdf estimation with hybrid training",
            "venue": "In Computer Graphics Forum,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "The ability to associate touch with sight is essential for tasks that require physically interacting with objects in the world. We propose a dataset with paired visual and tactile data called Touch and Go, in which human data collectors probe objects in natural environments using tactile sensors, while simultaneously recording egocentric video. In contrast to previous efforts, which have largely been confined to lab settings or simulated environments, our dataset spans a large number of \u201cin the wild\u201d objects and scenes. We successfully apply our dataset to a variety of multimodal learning tasks: 1) self-supervised visuo-tactile feature learning, 2) tactile-driven image stylization, i.e., making the visual appearance of an object more consistent with a given tactile signal, and 3) predicting future frames of a tactile signal from visuo-tactile inputs.\n* Indicates equal contribution\n36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks.\nar X\niv :2"
        },
        {
            "heading": "1 Introduction",
            "text": "As humans, our ability to correlate touch with sight is an essential component of understanding the physical properties of the objects around us. While recent advances in other areas of multimodal learning have been fueled by large datasets, the difficulty of collecting high-quality data has made it challenging for the community to develop similarly effective visuo-tactile models.\nAn intuitively appealing solution is to offload data collection to robots [8, 9, 42], which can acquire enormous amounts of data by repeatedly probing objects around them. However, this approach captures only a narrow, \u201crobot-centric\u201d slice of the visuo-tactile world. The data typically is limited to a specific environment (e.g., a robotics lab), and it fundamentally suffers from a chicken-and-egg problem, as the robot must already be capable of touching and manipulating the objects it acquires data from. In practice, this often amounts to recording data from tabletops, typically with small objects the robots can safely grasp. Recent work has also turned to simulation [21, 22], such as by modeling special cases where tactile interactions can be accurately simulated (e.g., rigid objects). Yet this approach, too, is highly limited. Real objects squish, deform, and bend in complex ways, and their seemingly simple surfaces can hide complicated microgeometry, such as weaves of fabric and tiny pores. Obtaining a full understanding of vision and touch, beyond simple robotic manipulation tasks, requires modeling these subtle visuo-tactile properties.\nWe argue that many aspects of the visuo-tactile world are currently best learned by observing physical interactions performed by humans. Humans can easily access a wide range of spaces and objects that would be very challenging for robots. By capturing data from objects in situ, the recorded sensory signals more closely match how the objects would be encountered in the wild. Inspired by this idea, we present a dataset, called Touch and Go, in which human data collectors walk through a variety of environments, probing objects with tactile sensors and simultaneously recording their actions on video. Our dataset spans a wide range of indoor and outdoor environments, such as classrooms, gyms, streets, and hiking trails. The objects and \u201cstuff\u201d [1] they contain are thus significantly more diverse than those of existing datasets, making it well-suited to self-supervised learning, and to tasks that require an understanding of material properties, such as visual synthesis tasks.\nWe apply our dataset to a variety of multimodal learning tasks. First, we learn tactile features through self-supervised learning, by training a model to associate images with touch. We find that the learned features significantly outperform supervised ImageNet [15] features on a robotic manipulation task, and on recognizing materials in our dataset (Fig. 1b). Second, we propose a novel task of tactile-driven image stylization: making an image \u201cfeel more like\u201d a given tactile input. To solve this problem, we adapt the recent method of Li et al. [41] to generate an image whose structure matches an input image but whose style is likely to co-occur with the given tactile information. This task evaluates the ability to learn cross-modal associations \u2013 i.e., how an object feels from how it looks and vice versa. The resulting model can successfully change the texture of an input image, such as by adding bumps to a smooth surface to match the tactile information recorded from a rock (Fig. 1c). Finally, we study multimodal models for future touch prediction: predicting future frames of a touch sensor\u2019s recording, given both visual and tactile signals. We show that visual information improves these predictions over touch alone (Fig. 1d)."
        },
        {
            "heading": "2 Related Work",
            "text": "Simulated vision and touch. A variety of methods have simulated low-dimensional Biotac [59] and fabric-based tactile sensing [46]. Other work has proposed to simulate high-dimensional tactile data, based on visual tactile sensors such as GelSight [73, 32, 31]. Wang et al. [65] simulated visual and tactile data for robotic grasps of rigid objects. Gao et al. [21, 22] proposed a dataset of simulated visual, tactile, and audio data, derived from CAD models with only rigid deformation. In contrast to these works, we collect our data from real objects and scenes, which contain non-rigid deformation, microgeometry, and wider variations in visual appearance.\nRobotic vision and touch. Researchers have proposed a variety of methods that use visual and touch signals for robotic applications [9, 8, 13, 34, 37, 48, 43, 12, 70, 11, 18]. Several of these have proposed visuo-tactile datasets. Calandra et al. created a dataset for multimodal grasping [9] and regrasping [8] with a robotic arm. Li et al. [42] collected data from a robotic arm synthesis, and proposed a model based on generative adversarial networks [24] for cross-modal translation. Murali\net al. [48] proposed a dataset for tactile-only grasping. These datasets have largely been confined to specific environments (e.g., a lab space containing the robots), and only contain objects provided to them by humans that the robots are capable of interacting with. Consequently, they contain a small number of object instances (each less than 200).\nHuman-collected multimodal data. We take inspiration from work that collects data by having humans physically interact with objects in situ. Song et al. [60] proposed a human-collected grasping dataset. In contrast, our focus is on having humans collect rich multimodal sensory data that is well-suited to self-supervised learning. Our approach is similar to Owens et al. [51], which learns audio-visual associations by probing objects with a drumstick [51]. In contrast, we collect touch instead of sound, and record data in an approximately egocentric manner as they move from object to object. Later work predicts the trajectory of a bounced ball [55]. Sundaram et al. [61] proposed a glove that records tactile signals, and collected a dataset of human grasps for 26 objects in a lab setting. Other work predicts hand pose from touch [75]. Burka et al. combined several haptic sensors [5, 6]. They then demonstrated the resulting sensor by collecting a preliminary (currently unreleased) dataset of 357 real-world surfaces, and training a model to predict human ratings of 4 surface properties from touch. By contrast, we have significantly larger and more diverse data from indoor and outdoor scenes (rather than flat, largely indoor surfaces), use a rich vision-based tactile sensor (GelSight), and demonstrate our dataset on cross-modal prediction tasks.\nMultimodal feature learning. In seminal work, de Sa [14] proposed to learn from correlating sight from sound. A variety of methods of been proposed for training deep networks to learn features from audio-visual correlations [49, 51, 52, 2, 35, 50, 47], from images and depth [63], and from vision and language [33, 45, 17, 56], and matching images and touch [38]. We adapt the contrastive model of Tian et al. [63] to visuo-tactile learning.\nMultimodal image prediction. A variety of methods have been proposed for predicting images from another modality, such as by using text or labels [58, 30, 4, 57] or sound [39, 10, 41]. Li et al. [41] proposed a model for audio-driven stylization, i.e. learning to restyle images to better match an audio signal. We adapt this model to tactile-driven stylization, creating a model that is conditioned on tactile inputs instead of sound. We also take inspirations from work on future video prediction [16, 23, 54, 66, 20, 71, 67, 3, 64]. In particular, Tian et al. [62] trains an action-conditioned video prediction method to estimate future tactile signals, using a GelSight sensor controlled by a CNC machine. In contrast, we predict future tactile signals from natural objects, and show that visual information can improve the prediction quality.\nCross-modal image stylization. Many areas of multimodal perception have used cross-modal image stylization to evaluate whether models can capture associations between modalities (e.g. Textto-image stylization, Audio-visual stylization). We adapt the cross-modal stylization method of Li et al. [41] to tactile-driven stylization, a task that requires learning visual-tactile associations \u2013 i.e., how an object feels from how it looks and vice versa. This direction is also related to work in computer graphics that synthesizes images that have specific material properties [40, 25, 26, 77]. In contrast to these works, we synthesize images with material properties that are captured implicitly from a touch signal.\n3 The Touch and Go Dataset We collect a dataset of natural vision-and-touch signals. Our dataset contains multimodal data recorded by humans, who probe objects in their natural locations with a tactile sensor. To more easily train and analyze models on this dataset, we also collect material labels and identify the frames within the press."
        },
        {
            "heading": "3.1 Collecting a natural visuo-tactile dataset",
            "text": "To acquire our dataset, human data collectors (the authors) walked through a variety of environments, probing the objects with a tactile sensor. To obtain images that show clear, zoomed-in images of the objects being touched, two people collected data at once: one who presses the tactile sensor onto an object, and another who records an \u201capproximately egocentric\u201d video (see supplement for a visualization) of their hand. The two data collectors moved from object to object in the space as part of a single, continuously recording video, touching the objects around them. We show examples from our dataset in Fig. 2. The captured data varies heavily in material properties (e.g., soft/hard, smooth/rough), geometries, and semantics.\nCapturing procedure. To ensure that our dataset captures the natural variation of real-world vision and touch, we collect both rigid and deformable objects in indoor and outdoor scenes. These scenes include rooms in university buildings, such as classrooms and hallways, apartments, hiking trails, playgrounds, and streets. We show example footage from our model in Fig. 1, and in the supplement. The data collectors selected a variety of objects in each scene to press including chairs, walls, ground, sofa, table, etc. in indoor scenes, and grass, rock, tree, sand etc. in outdoor scenes, pressing each one approximately 3 times. Each press lasted for 0.7 sec on average. If an object contains multiple materials (e.g., a chair with a cushion and a plastic arm), collectors generally directed their presses to each one. The data collectors also aimed to touch object parts with complex geometry, rather than flat surfaces. To avoid capturing human faces, in public spaces the captures point the camera toward the ground when moving between objects. Since the GelSight may provide information about force implicitly [73], and explicit force readings are not required for many visuo-tactile tasks, we do not use a separate force sensor.\nHardware. For the tactile sensor, we use GelSight [32], the variant designed for robotic manipulation [73]. This is a vision-based tactile sensor, approximately 1.5cm in diameter, in which a camera observes the deformation of a curved elastomer gel illuminated by multiple colored light sources, with markers embedded inside it. When the sensor is pressed against an object, the gel deforms, which results in changes to the reflected illumination. The color conveys the surface normal of the object being touched, similar to photometric stereo. These black dots are \u201cmarkers\" that are physically embedded within the GelSight\u2019s elastomer gel. Thus, GelSight records a video in which surface orientation, depth, and shear can be estimated by analyzing the appearance of each video frame. The tactile sensor\u2019s recordings are recorded concurrently with visual images from an ordinary webcam (both at approximately 27 Hz). Both videos (tactile and visual) are recorded on a single computer."
        },
        {
            "heading": "3.2 Annotating the dataset",
            "text": "To make it easier to analyze results and train models, we provide annotations for material categories and frames within the press.\nDetecting the press. As the data collectors move between objects, the tactile sensor does not make contact with anything. Thus, for convenience, we provide the subset of frames within the touch, so that applications can use trimmed videos (Sec. 4). To obtain these timings, we train a detector for press detection. We (the authors) hand-label 10k frames and train a binary ResNet-18 classifier [27] that operates on GelSight images. On our test set of 2k hand-labeled frames, our classifier obtains 97% accuracy (chance is around 66%). To further ensure high accuracy, we hire workers to review the frames within the touch and correct errors.\n2Following common practice, the tactile images are enhanced for visualization purpose. Contrast and sharpness are increased by 30% and saturation is increased by 20%.\nLabeling materials. We label the material category for all (visual) video frames that our detector predicts within the press, using a labeling scheme similar to [51]. Online workers assign a label from a list of categories. If an object is not in this category list, the workers will label it other material. To ensure accuracy, we have 5 workers label each image. We show the distribution of labels in Fig. 2."
        },
        {
            "heading": "3.3 Dataset analysis",
            "text": "We analyze the contents of our dataset and compare it to other works.\nData distribution. In Fig. 2, we show statistics of labeled materials and scene types, and provide qualitative results from the dataset. It contains approximately 13.9k detected touches and approximately 3971 individual object instances. Since we do not explicitly label instances, we obtain the latter number by exhaustively counting the objects in 10% of the videos and extrapolating. Our dataset is relatively balanced between indoor (52.2%) and outdoor (47.8%) scenes. We found that several categories, namely synthetic fabric, tile, paper, and leather, are only present in our indoor scenes, while tree, grass, plant, and sand are only present in outdoor scenes. The remaining materials exist in both scenes. We provide more details in the supplement.\nComparison to other datasets. In Table 1, we compare our dataset to several previously proposed visuo-tactile datasets collected by robots, by humans, or through simulation. Our dataset contains approximately 4\u00d7 as many object instances as the second-largest dataset, the simulation-based ObjectFolder 2.0 [22], and 11\u00d7 larger compared with the human-collected dataset by Burka et al. [7]. Compared to the existing robot-collected datasets, ours contains more touches (e.g., 1.15\u00d7 more than VisGel [42] and 1.5\u00d7 more than The Feeling of Success [9]). Our dataset also contains data from more diverse scenes than prior work, with a mixture of natural indoor and outdoor scenes. In contrast, robot-collected datasets [8, 9, 42] are confined to a single lab space containing the robot.\nQualitative comparison to other datasets We show qualitative examples of data from other datasets in Fig. 3 to help understand the differences between our dataset and those of previous work: Object Folder 2.0 [22], which contains virtual objects, and two robotic datasets: Feeling of Success [9], and VisGel [42]. We show examples from indoor scenes, since the other datasets do not contain outdoor scenes, and with rigid materials (since the virtual scenes do not contain deformable materials). Each row illustrates objects which are composed of similar materials, along with their corresponding GelSight images. As can be seen, the robot-centric datasets [42, 9] are confined to a fixed space. Their objects are also smaller than those in our dataset, since they must be capable of being grasped by the robot\u2019s gripper. Synthetic datasets [21, 22] do not contain complex microgeometry, and their rigid objects do not deform when pressed."
        },
        {
            "heading": "4 Applications",
            "text": "To evaluate the effectiveness of our dataset, we perform tasks that are designed to span a variety of application domains, including representation learning, image synthesis, and future prediction."
        },
        {
            "heading": "4.1 Multimodal self-supervised representation learning",
            "text": "We ask, first, whether we can use the multimodal data to learn representations for the tactile modality by associating touch with sight. We then ask how well the learned representations convey material properties from our dataset, and whether they are useful for robotic learning tasks whose data has been collected in other works [8, 9]. The latter task requires a significant amount of generalization, since the objects manipulated by robots while our dataset is collected by humans.\nObject Folder: 5, 60, ?, 475 Feeling of Success: 1, 179, 193, 214 VisGel: 2_2004_133, 4_4244_178, 8_8003_190, 9_9003_197\nOur goal is to learn a representation that captures the information and correspondences between visual and tactile images, which can be useful for downstream tasks. Given the visual and tactile datasets, XI and XT , we aim to extract the corresponding visual-tactile pairs, {xiI ,xiT } and mismatched pairs {xiI ,x j T } using the Contrastive Multiview Coding (CMC) model proposed by Tian et al. [63]. The detailed procedure is shown below.\nFor each visual-tactile image pair, we first encode visual images and tactile inputs as L2-normalized embeddings using two networks, where zI = f\u03b8I (xI) for visual images and zT = f\u03b8T (xT ) for tactile images. Recall that our goal is to find the corresponding sample of the other modality, given a set S that contains both the corresponding example and K \u2212 1 random examples. When matching a visual example xiI to tactile, the loss for matching visual images to tactile images is:\nLXI ,XTcontrast = \u2212log exp(f\u03b8I (x i I) \u00b7 f\u03b8T (xiT )/\u03c4)\u2211K\nj=1 exp(f\u03b8I (x i I) \u00b7 f\u03b8T (x j T )/\u03c4)\n(1)\nwhere \u03c4 = 0.07 is a constant and j indexes the tactile examples in S. Analogously, we can obtain a loss in which tactile examples are matched to images, LXT ,XIcontrast. We minimize both losses:\nL(XI , XT ) = LXI ,XTcontrast + L XT ,XI contrast (2)\nThe training details and hyperparameters are provided in the supplementary material."
        },
        {
            "heading": "4.2 Tactile-driven image stylization",
            "text": "Touch provides complementary information that may not be easily conveyed through other modalities that are commonly used to drive image stylization, such as language and sound. For example, touch can precisely define how smooth/rough a surface ought to be, and express the subtle shape of its microgeometry. A model that can successfully predict these properties from visuo-tactile data therefore ought to be able to translate between modalities. Inspired by the audio-driven image stylization of Li et al. [41], we propose the task of tactile-driven image stylization: making an image look as though it \u201cfeels like\u201d a given touch signal.\nFollowing Li et al. [41], we base our approach on contrastive unpaired translation (CUT) [53]. Given an input image xI and a tactile example xT , our goal is to manipulate the image via a generator such that the manipulated pair x\u0302I = G(xI ,xT ) is more likely to co-occur in the dataset. Our model consists of an image translation network that is conditioned on a tactile example (a GelSight image). The loss function encourages the model to preserve the image structure, which is enforced using an image-based contrastive loss [53], while adjusting the image style such that the resulting textures are more likely to co-occur with the tactile signal, which is measured using a visuo-tactile discriminator.\nMaking sight consistent with touch. We train the model with a discriminator D that distinguishes between real (and fake) visuo-tactile pairs. During training time, we shuffle the dataset to generate the\nset Sn containing mismatched image-tactile pairs {xI ,x\u2032T } \u2208 Sn. Likewise, we define the original dataset as Sm which contains matched image-tactile pairs {xI ,xT } \u2208 Sm. In formal terms, the visual-tactile adversarial loss can be written as:\nLVT = E{xI ,xT }\u223cSm logD(xI ,xT ) + E{xI ,x\u2032T }\u223cSn log(1\u2212D(G(xI ,x \u2032 T ),x \u2032 T )) (3)\nLoss. We combine our visuo-tactile discriminator with the structure-preserving loss used in Li et al. [42], which was originally proposed by CUT [53]. This loss, which we call LCUT, works by training a contrastive learning model that puts patches in the input and predicted images into correspondence, such that patches at the same positions are close in an embedding space. Please see the supplement for more details. The overall loss is:\nLTDIS = LVT + LCUT. (4)"
        },
        {
            "heading": "4.3 Multimodal future touch prediction",
            "text": "Inspired by the challenges of action-conditional future touch prediction [62], we use our dataset to ask whether visual data can improve our estimates of future tactile signals: i.e., what will this object feel like in a moment? Visual information can help touch prediction in a number of ways, such as by conveying material properties (e.g., deformation) and and geometry. It can also provide information about which action is being performed. One may thus also consider it as an implicit form of action conditioning [19], since the visual information provides information analogous to actions.\nWe adapt the video prediction architecture of Geng et al. [23] to this task. This model is based on residual networks [27, 29] with 3D space-time convolutions (please see the supplement for architecture details). We predict multiple frames by autoregressively feeding our output images back to the original model. Given a series of paired visual and tactile images from times 1 to t, {(x1I ,x1T ), ..., (xtI ,xtT )}, our goal is to predict the subsequent tactile image, x\u0302 (t+1) T . We train the model with L1 and perceptual loss, following [23]."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Self-supervised feature learning",
            "text": "We train a self-supervised model that learns to associate images with touch. We evaluate this learned representation on two downstream tasks: robot grasping and material understanding tasks. .\nRobotic grasping task. For the robot grasping task, we use the experimental setup and dataset of Calandra et al. [9]. Thus, the task requires generalizing to data recorded in a very different environment, and with different GelSight sensors. The goal of this task is to predict whether a robotic arm will successfully grasp an object, based on inputs from two GelSight images recorded before and after grasping. Since there is no standard training/test split from [9], we split their objects randomly into training/test. The resulting dataset contains 68 objects and 5921 touches for training, 16 objects and 1204 touches for validation, and 21 objects and 1204 touches for testing. Similar to the tactile-only model from Calandra et al. [9], we compute features for each of the 4 tactile images (before/after images for 2 tactile sensors) using our self-supervised model. We concatenate these features together and train a linear classifier to solve this binary classification task.\nMaterial understanding tasks. We evaluate whether the learned features convey material properties. Given tactile features, we recognize: 1) material categories, 2) hard vs. soft surfaces, and 3) smooth vs. rough surfaces. Following [51], we re-categorize material categories to generate soft/hard labels and hire online workers to label the smooth/rough according to the visual image. Since the smoothness and roughness may vary within a material category, we hire online workers to label the smooth vs. rough according to the visual image. To avoid providing our self-supervised learning model with object instances that appear in the linear probing experiment, we split the dataset into an unlabeled set containing 5172 touches (51.7%), a labeled training set of 3921 touches (39.2%), and a labeled test set of 923 touches (9.1%). We split the dataset by video (rather than by frame) to avoid having the same (or nearly same) object appear in both training and test.\nImplementation details. We train our model for 240 epochs, using the optimization parameters from CMC [63], after adjusting the learning rate schedule to compensate for longer training. We set the weight decay to be 10\u22124. We train our model with the batch size of 128 on 4 Nvidia 2080-Ti GPUs. For the downstream classification tasks, we froze our network weights and obtained visual features by performing global average pooling on the final convolutional layer. We follow the approach of [63] for learning the linear classifier.\nComparison to other feature sets. We show downstream classification results in Table 2. To evaluate the effectiveness of our dataset, we compare our learned features to those of several other approaches. These include using supervised ImageNet [15] features, which are commonly used to represent GelSight images [74, 9, 8], and visual CMC [63] features trained on ImageNet, which treats the L and ab color channels of an image as different modalities. We see that our model obtains significantly better performance than these models on both tasks, due to its ability to learn from real tactile data. These results suggest that our dataset provides a useful signal for training self-supervised tactile representations. We also show that increasing the material categories leading to much better downstream performance."
        },
        {
            "heading": "5.2 Tactile-driven image stylization",
            "text": "We use our model to modify the style of an image to match a touch signal.\nImplementation details. Following Li et al. [41], during training we sample a random image from the dataset, along with a second visuo-tactile pair, and use the pair to restyle the image, using the loss in Eq. 4. We provide architectural details in the supplement. For the discriminator we adopt the PatchGAN architecture [28]. The architecture of discriminator follows [41], which concatenates the two input images channel-wise, and passes the combined images to the discriminator. We train our model on 4 Nvidia 2080-Ti GPUs for 100 epochs with a batch size of 8 and the learning rate of 0.0002. We augment the vision images with random cropping and horizontal flipping.\nExperimental setup. Following [41], we evaluate our model by restyling images in our dataset with random tactile inputs, both of which are taken from the test set. We use evaluation metrics that measure the consistency of the manipulated image with the example used for conditioning. First, we measure the similarity of the manipulated image and the tactile example used for conditioning. Similar to [41], we use our trained CMC model, by taking the dot product between visual and tactile\nembeddings. Second, we compare material prediction consistency between the manipulated image with the (held out) conditional image. We use our material classifier to categorize the predicted and conditioning images, and measure the rate at which they agree. Since this is a novel task, we create a second variation of our model for comparison, following [78]. This model performs the stylization using CycleGAN, rather than CUT (see supplement for details).\nQuantitative results. Quantitative results are shown in Table 3. Here, the \u201cbaseline\u201d indicates results from original image before stylization. We can see that the CUT-based method obtains higher\nCMC similarity than a CycleGAN-based method [78]. In terms of material classification consistency, our model consistently outperforms CycleGAN-based method [78].\nQualitative results. In Fig. 7, we show results from our model. Our model successfully manipulates images to match tactile inputs, such as by making surfaces rougher or smoother, or by creating \u201chybrid\u201d materials (e.g., adding grass to a surface). These results are obtained without having access to the tactile example\u2019s corresponding image, suggesting that the model has learned which physical properties are shared between sight and touch."
        },
        {
            "heading": "5.3 Multimodal video prediction",
            "text": "We evaluate our model for predicting future tactile signals. We compare a tactile-only model to a multimodal visuo-tactile model, and show that the latter obtains better performance.\nExperimental setting. We evaluate the effectiveness of multimodal inputs using three context frames to predict the next frame under two different time horizons: skipping 3 and 5 frames between\ncontexts. Following [23], we adopt three evaluation metrics: MAE, SSIM [69] and LPIPS [76]. We provide training hyperparameters in the supplement.\nMultimodal vs. single-modal prediction. We show the quantitative results in Table 4. Here, we adopt two video prediction baselines [16] and [23]. We can see that, by incorporating our dataset\u2019s visual signal, the models gain a constant performance increase under different evaluation metrics for both model, under both experimental settings. The gap becomes larger for longer time horizon, suggesting that visual information may be more helpful in this case."
        },
        {
            "heading": "6 Discussion",
            "text": "We proposed Touch and Go, a human-collected visuo-tactile dataset. Our dataset comes from realworld objects, and is significantly more diverse than prior datasets. We demonstrate its effectiveness on a variety of applications that involve robotic manipulation, material understanding, and image synthesis. In the tradition of previous work [51], we see our work as a step toward human-collected multimodal data collection, in which humans equipped with multiple sensors collect diverse dataset by recording themselves physically interacting with the world. We hope this data will enable researchers to study diverse visuo-tactile learning applications, beyond the \u201crobotics-centric\u201d domains that are often the focus of previous efforts.\nLimitations. Collecting diverse tactile data is an ongoing challenge, since it requires physically being present in the locations where data is collected. While adding human collectors improves diversity in many ways, our dataset was mainly collected in one geographic location (near University of Michigan\u2019s campus). Consequently, the data we recorded may not generalize to all spaces. The use of humans in the data collection process also potentially introduces bias, which differs from \u201crobotic\u201d or \u201cvirtual data\u201d bias. For example, humans may choose unrepresentative parts of the objects to probe, and do not perform actions with consistent force. The humans who recorded the dataset may also not be representative of the general population, which may introduce bias (e.g., in skin tone).\nAcknowledgements. We thank Xiaofeng Guo and Yufan Zhang for the extensive help with the GelSight sensor, and thank Daniel Geng, Yuexi Du and Zhaoying Pan for the helpful discussions. This work was supported in part by Cisco Systems and Wang Chu Chien-Wen Research Scholarship."
        },
        {
            "heading": "A Project webpage",
            "text": "We\u2019ve provided a webpage for our dataset, which contains a link to the dataset. We also provide additional examples from our dataset (c.f., Fig 2 of the main paper)."
        },
        {
            "heading": "B Dataset file structure",
            "text": "Our dataset is currently available through our webpage (and directly via this link). For long-term maintenance, we will upload our dataset to University of Michigan\u2019s EECS web servers after acceptance.\nThe touch_and_go directory contains a dataset directory of raw videos, extract_frame.py that convert raw videos to frames, label.txt of material labels for frames within the press, and category_reference.txt of the name for each category in label.txt.\nEach raw video folder in the Dataset folder consists of six items: \u2022 video.mp4: Raw RGB video recording the interaction of human probing objects.\n\u2022 gelsight.mp4: Raw GelSight (tactile) video for objects.\n\u2022 time1.npy: The recording time for each frame in \u201cvideo.mp4\u201d.\n\u2022 time2.npy: The recording time for each frame in \u201cgelsight.mp4\u201d.\n\u2022 video_frame: The folder containing all the frames in \u201cvideo.mp4\u201d. (Generated after running extract_frame.py)\n\u2022 gelsight_frame: The folder containing all the frames in \u201cgelsight.mp4\u201d. (Generated after running extract_frame.py)\nWe have provided qualitative examples of the videos on our project page. To view the videos at full resolution, please download them."
        },
        {
            "heading": "C Egocentric recording setup",
            "text": "As shown in Fig. 6, we use a webcam to record the RGB video and a GelSight sensor to capture the tactile signals, which are both connected to one laptop computer. To obtain images that show clear, zoomed-in images of the objects being touched, two people collected data at once: one who presses the tactile sensor onto an object, and another who records an \u201capproximately egocentric\u201d video. Alternatively, one person may record both signals, while another holds the computer, providing them with a view of what they are pressing via the screen. In this way, they can ensure that the objects they are probing appear approximately in the center of the recorded images and increase the stability of the recording."
        },
        {
            "heading": "D Category list",
            "text": "We conclude the objects appeared in our dataset into 20 categories according to their material property. All these categories are listed with decreasing number of quantities in terms of the number of touches. Label Num. denotes the number in the label.txt representing each category.\nE Implementation details for self-supervised learning\nWhen training the contrastive multiview coding (CMC) model, we use a learning rate of 0.03 and train for 240 epochs. We use SGD as our optimizer and set the weight decay to be 10\u00d7 10\u22124 and the momentum to be 0.9. We use a batch size of 128 on 4 Nvidia 2080-Ti GPUs. For the linear probing stage in both downstream tasks, we fixed the weight of our pretrained backbone and adopt the global average pooling at the last layer followed by a linear classifier. We use a learning rate of 0.01 for ResNet-18 and 0.1 for ResNet-50. For both material classification and robot grasping, we train the linear classifiers with 60 epochs and a batch size of 256."
        },
        {
            "heading": "F Details for Tactile-driven image stylization",
            "text": "Architecture. Our model consists of a multi-modal generator, a tactile-visual texture discriminator and a patch-wise structure discriminator. We can further break up our multi-modal generator into three components, an image encoder Genc_I, a tactile encoder Genc_T and a decoder Gdec. Given our dataset that contains unpaired instances Sn = {xI ,x\u2032T }, the output image x\u0302I can be expressed as x\u0302I = G(xI ,x \u2032 T ) = Gdec(concat(Genc_I(xI),Genc_T(x \u2032 T))).\nStructure preserving loss (LCUT). Our goal in this tactile-guided image stylization is to restyle the source image with the textures that are associated with the target tactile input while preserving the source structure. Following previous approaches [53, 41], we introduce an a noise contrastive estimation (NCE) loss [53] on the image encoder Genc_I that helps preserve the structural information between the visual input xI and the generated image x\u0302I .\nThis loss is motivated by recent contrastive learning to maximize the probability for the neural network to select the corresponding patch in both the original image xI and the generated image x\u0302I . Specifically, we select a query patch from the generated x\u0302I , one positive patch and N negative patches from the original image xI . Then we encode these patches into a K dimensional vectors by a\nMLP so that query vector q, positive vector \u03c5+ belong to RK and negative vectors \u03c5\u2212 \u2208 RN\u00d7K :\nl(\u03c5,\u03c5+,\u03c5\u2212) = \u2212log exp(q\u00b7\u03c5\n+\n\u03c4 )\nexp(q\u00b7\u03c5 + \u03c4 ) + \u2211N n=1 exp( q\u00b7\u03c5\u2212 \u03c4 )\n(5)\nwhere \u03c4 is the temperature parameter.\nSince our image encoder is a multi-layer convolutional network, we take advantage of multiple feature stacks generated from different layers. Specifically, we select L layers of feature stacks and pass them into a MLP M and the output is M(Glenc_I(xI)) = {v1l ,v2l , ...,vNl ,v N+1 l }, where l \u2208 {1, ..., L\u22121, L}. Here, we denotesGlenc_I(xI) as the feature stacks at layer l. Similarity, we apply this to the generated image x\u0302I so that we get our query vector for each layer, which can be represented as {q1l , q2l , ..., qNl , q N+1 l }. Thus, for each sample index n at layer l, we let vnl \u2208 RN\u00d7Cl as the positive samples and other features v(N+1)\\nl \u2208 RN\u00d7Cl as negative samples, where Cl indicates the channel of the layer l. Thus our multi-layer NCE loss can be represented as the following:\nLCUT = ExI\u223cSn L\u2211 l=1 N+1\u2211 n=1 l(qnl ,\u03c5 n l ,\u03c5 (N+1)\\n l ) (6)\nwhere Sn contains mismatched image-tactile pairs {xI ,x\u2032T }, as defined in our main text.\nImplementation details. Our image encoder and decoder of the generator are fully convolutional neural networks consisting of 9 blocks of ResNet-based CNN bottlenecks. The first convolution layer is set to 7 \u00d7 7 and the rest are set to 3 \u00d7 3. For the tactile encoder, we adopt a ResNet18 [27] backbone pretrained on the ImageNet [15]. For the discriminator we adopt the PatchGAN architecture [28]. To compute the NCE loss, we extract features from five different layers: the input image layer, the first and second downsampling convolution layer and the first and fifth residual blocks. We train our model on 4 Nvidia 2080-Ti GPUs for 100 epochs with the batch size of 8 and learning rate of 0.0002. For input visual images, we use a random crop and horizontal flip."
        },
        {
            "heading": "G More results for tactile-drive image stylization",
            "text": ""
        },
        {
            "heading": "H Details for multimodal future touch prediction",
            "text": "Overall Architecture Following [23], our model adopts widely-used residual network from [68] while replacing the 2D convolution to 3D convolution, which utilizes a encoder-decoder architecture. To adapt for multimodal prediction, we introduce two encoders for tactile inputs and visual inputs with identical structures but different weights. Then we concatenate these features along the channel and feed them into the decoder consisting of transposed convolution layers, similar to the architecture of tactile-driven stylization.\nTraining details For the video prediction task, we train our model using Adam Optimizer with the learning rate of 2 \u00d7 10\u22124 for all experiments. We utilize the batch size of 8 on 4 Nvidia 2080-Ti GPUs and train for 30 epochs. We initialize the weights from a Gaussian distribution with the mean 0 and std of 0.02. To obtain multi-frame prediction, we recursively feed our output images back to the original model. During this process, the loss are backward through the entire chain of recursive functions and gradients are accumulated, following [23, 44].\nEvaluation Metrics Following [23], we adopt three evaluation metrics: MAE, SSIM and LPIPS. Structural similarity (SSIM) is a similarity metric to quantify image quality degradation. The higher the SSIM, the better the generated frame. Learned Perceptual Image Patch Similarity (LPIPS) measures the distance between image patches. The lower the LPIPS, the higher the similarity."
        },
        {
            "heading": "I Datasheet",
            "text": "Motivation"
        },
        {
            "heading": "Q1. For what purpose was the dataset created?",
            "text": "Answer: The goal of this dataset is to provide training data for multimodal learning systems that learn to associate the sight of objects with their corresponding tactile data (i.e., how they \u201cfeel\u201d). In contrast to previous efforts, our dataset contains a large number of in-the-wild recordings from indoor and outdoor scenes."
        },
        {
            "heading": "Q2. Who created this dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?",
            "text": "Answer: Six researchers at the University of Michigan and Carnegie Mellon University (affiliated as of 2022) have created the dataset: Fengyu Yang, Chenyang Ma, Jiacheng Zhang, Jing Zhu, Wenzhen Yuan and Andrew Owens."
        },
        {
            "heading": "Q3. Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number.",
            "text": "Answer: Our dataset is funded in part by Cisco Systems and The University of Michigan."
        },
        {
            "heading": "Q4. Any other comments?",
            "text": "Answer: No.\nComposition"
        },
        {
            "heading": "Q5. What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?",
            "text": "Answer: Each instance is a visuo-tactile image pair containing the visual image and its corresponding tactile signal, i.e. the result of someone pressing the object with a GelSight tactile sensor."
        },
        {
            "heading": "Q6. How many instances are there in total (of each type, if appropriate)?",
            "text": "Answer: There are in total approximately 246k visuo-tactile image (frame) pairs of about 13.9k touches in our dataset."
        },
        {
            "heading": "Q7. Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?",
            "text": "Answer: Yes. We have provided the full dataset."
        },
        {
            "heading": "Q8. What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images) or features?",
            "text": "Answer: The raw data consists of videos recorded by human collectors and the corresponding tactile videos. The RGB videos and tactile videos are synchronously recorded, and compressed with a video codec.\nQ9. Is there a label or target associated with each instance?\nAnswer: Yes. We label all frames where human are probing an object with its material label.\nQ10. Is any information missing from individual instances?\nAnswer: No."
        },
        {
            "heading": "Q11. Are relationships between individual instances made explicit (e.g., users\u2019 movie ratings, social network links)?",
            "text": "Answer: Since we walk through scenes, recording objects around us, the objects in a video are close in space. Tactile signals from the same materials or objects are likely to be similar.\nQ12. Are there recommended data splits (e.g., training, development/validation, testing)?\nAnswer: As illustrated in the main text, different tasks require different train/val/test splits. In general, to avoid having the same (or nearly the same) images appear in both training and test set, we recommend splitting the dataset by video (rather than by touch or by frame). We will provide the splits used in our experiments."
        },
        {
            "heading": "Q13. Are there any errors, sources of noise, or redundancies in the dataset?",
            "text": "Answer: It is a challenging task to infer the material according to the RGB images. We have at least 5 people label each image, though still possible to have some images correctly labeled for its material category.\nQ14. Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?\nAnswer: The data is self-contained."
        },
        {
            "heading": "Q15. Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals non-public communications)?",
            "text": "Answer: No."
        },
        {
            "heading": "Q17. Does the dataset relate to people?",
            "text": "Answer: No.\nCollection Process"
        },
        {
            "heading": "Q18. How was the data associated with each instance acquired?",
            "text": "Answer: The data is directly collected by two people (authors) walking through a variety of environments, probing objects with tactile sensors and simultaneously recording their actions on videos."
        },
        {
            "heading": "Q19. What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)?",
            "text": "Answer: We collect our dataset using a RGB camera and a GelSight tactile sensor. Details of the hardware is illustrated in the main text.\nQ20. If the dataset is a sample from a larger set, what was the sampling strategy?\nAnswer: No, the dataset is not a sample from a larger set."
        },
        {
            "heading": "Q21. Who was involved in data collection process (e.g., students, crowd-workers, contractors) and how were they compensated (e.g., how much were crowd-workers paid)?",
            "text": "Answer: Our dataset is collected by authors of this paper."
        },
        {
            "heading": "Q22. Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)?",
            "text": "Answer: The dataset is collected across the winter, spring and summer (February 2022 to June 2022). The objects in our dataset are taken from scenes at the specific time (and season) in which the data was collected."
        },
        {
            "heading": "Q23. Were any ethical review processes conducted (e.g., by an institutional review board)?",
            "text": "Answer: Our dataset only contains natural scenes, with no humans subjects (including no humans on screen). It therefore does not qualify as human subjects research."
        },
        {
            "heading": "Q24. Does the dataset relate to people?",
            "text": "Answer: No.\nPreprocessing, Cleaning, and/or Labeling"
        },
        {
            "heading": "Q25. Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)?",
            "text": "Answer: Yes. We collect our raw data in the format of RGB and GelSight videos. To facilitate training and downstream tasks, we preprocess the raw videos by converting them into frames, detecting the frames within the press, and label the pressed frames by their material. Detailed description are in the Dataset section of the main text."
        },
        {
            "heading": "Q26. Was the \u201craw\u201d data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)?",
            "text": "Answer: Yes. We save the original videos for unanticipated future uses of other tasks.\nQ27. Is the software used to preprocess/clean/label the instances available?\nAnswer: Yes. The source code to extract frames is available on our webpage."
        },
        {
            "heading": "Q28. Any other comments?",
            "text": "Answer: No.\nUses"
        },
        {
            "heading": "Q29. Has the dataset been used for any tasks already?",
            "text": "Answer: Yes. As illustrated in the main text, we apply our dataset to a variety of multimodal learning tasks. First, we learn tactile features through self-supervised learning, by training a model to associate images with touch. Secondly, we use our dataset to perform material classification task via GelSight Images. Thirdly, we propose a novel task of tactile-driven image stylization: making an image \u201cfeel more like\u201d a given tactile input. Finally, we study multimodal models for future touch prediction: predicting future frames of a touch sensor\u2019s recording, given both visual and tactile signals.\nQ30. Is there a repository that links to any or all papers or systems that use the dataset?\nAnswer: We do not have a repository to record all papers using our dataset. However, we can track these papers via Google Scholar."
        },
        {
            "heading": "Q31. What (other) tasks could the dataset be used for?",
            "text": "Answer: Our dataset is potentially suitable for tasks that require visual, tactile, or visuo-tactile understanding, such as visual-tactile image translation, shape/hardness estimation, etc.\nQ32. Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?\nAnswer: Our dataset was mainly collected in one geographic location (near University of Michigan\u2019s campus). Consequently, the data we recorded may not generalize to all spaces. The use of humans in the data collection process also potentially introduces bias, which differs from \u201crobotic\u201d or \u201cvirtual data\u201d bias. It was also recorded by a relatively small number of human collectors. The way that they interacted with the objects may therefore not be fully representative."
        },
        {
            "heading": "Q33. Are there any tasks for which the dataset should not be used?",
            "text": "Answer: Our dataset is designed for visuo-tactile learning tasks. It may be not appropriate for tasks outside this domain."
        },
        {
            "heading": "Q34. Any other comments?",
            "text": "Answer: No."
        },
        {
            "heading": "Q35. Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?",
            "text": "Answer: Yes. Our dataset is publicly available."
        },
        {
            "heading": "Q36. How will the dataset will be distributed (e.g., tarball on website, API, GitHub)",
            "text": "Answer: Our dataset contains a link to a Google Drive directory that contains all of the raw videos, a \u201cextract_frame.py\u201d file to convert videos into frames and a separate \u201clabel.txt\u201d file containing all material labels for frames within the press (See B for more details)."
        },
        {
            "heading": "Q37. When will the dataset be distributed?",
            "text": "Answer: We have currently provided all raw data, including videos, tactile recordings, labels, and code. Our dataset will be officially released starting by October 2022 (e.g., in an easy-to-download format and with full documentation)."
        },
        {
            "heading": "Q38. Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?",
            "text": "Answer: Our dataset is distributed under the license of CC BY."
        },
        {
            "heading": "Q39. Do any export controls or other regulatory restrictions apply to the dataset or to individual instances?",
            "text": "Answer: No.\nQ40. Any other comments?\nAnswer: No.\nMaintenance"
        },
        {
            "heading": "Q41. Who will be supporting/hosting/maintaining the dataset?",
            "text": "Answer: Our dataset is currently hosted on a public Google Drive directory. We will also mirror the dataset using a web server provided by The University of Michigan, so that it will be available indefinitely."
        },
        {
            "heading": "Q42. How can the owner/curator/manager of the dataset be contacted (e.g., email address)?",
            "text": "Answer: The email of authors of our dataset is available on the project webpage.\nQ43. Is there an erratum?\nAnswer: No. If we notice errors in the future, we will put them in an erratum."
        },
        {
            "heading": "Q44. Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?",
            "text": "Answer: There is no routine update plan for our dataset. To correct labeling errors, please contact authors of our dataset.\nQ45. If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were individuals in question told that their data would be retained for a fixed period of time and then deleted)?\nAnswer: No. Our dataset is not related to people."
        },
        {
            "heading": "Q46. Will older versions of the dataset continue to be supported/hosted/maintained?",
            "text": "Answer: No. We only maintain the latest dataset unless there is a significant update.\nQ47. If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?\nAnswer: We have provided information about how the data was collected, including the sensors and the dataset collection procedure. Thus, those who want to collect similar data can easily do so."
        }
    ],
    "title": "Touch and Go: Learning from Human-Collected Vision and Touch",
    "year": 2022
}