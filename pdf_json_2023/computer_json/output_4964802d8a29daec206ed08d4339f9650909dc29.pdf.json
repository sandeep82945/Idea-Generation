{
    "abstractText": "We formalize and interpret the geometric structure of d-dimensional fully connected ReLU layers in neural networks. The parameters of a ReLU layer induce a natural partition of the input domain, such that the ReLU layer can be significantly simplified in each sector of the partition. This leads to a geometric interpretation of a ReLU layer as a projection onto a polyhedral cone followed by an affine transformation, in line with the description in [5, doi:10.48550/arXiv.1905.08922] for convolutional networks with ReLU activations. Further, this structure facilitates simplified expressions for preimages of the intersection between partition sectors and hyperplanes, which is useful when describing decision boundaries in a classification setting. We investigate this in detail for a feed-forward network with one hidden ReLU-layer, where we provide results on the geometric complexity of the decision boundary generated by such networks, as well as proving that modulo an affine transformation, such a network can only generate d different decision boundaries. Finally, the effect of adding more layers to the network is discussed.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jonatan Vallin"
        },
        {
            "affiliations": [],
            "name": "Karl Larsson"
        },
        {
            "affiliations": [],
            "name": "Mats G. Larson"
        }
    ],
    "id": "SP:9d99f5a7930f9cdef8c5e8ea4a42018daf07dc51",
    "references": [
        {
            "authors": [
                "M. Alfarra",
                "A. Bibi",
                "H. Hammoud",
                "M. Gaafar",
                "B. Ghanem"
            ],
            "title": "On the decision boundaries of neural networks: A tropical geometry perspective",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 45(4):5027\u2013 5037",
            "year": 2022
        },
        {
            "authors": [
                "R. Balestriero",
                "R. Baraniuk"
            ],
            "title": "A spline theory of deep learning",
            "venue": "Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proc. Mach. Learn. Res., pages 374\u2013 383",
            "year": 2018
        },
        {
            "authors": [
                "R. Basri",
                "D.W. Jacobs"
            ],
            "title": "Efficient representation of low-dimensional manifolds using deep networks",
            "venue": "International Conference on Learning Representations",
            "year": 2017
        },
        {
            "authors": [
                "Y. Bengio",
                "O. Delalleau"
            ],
            "title": "On the expressive power of deep architectures",
            "venue": "Algorithmic Learning Theory, pages 18\u201336. Springer Berlin Heidelberg",
            "year": 2011
        },
        {
            "authors": [
                "S. Carlsson"
            ],
            "title": "Geometry of deep convolutional networks",
            "venue": "arXiv:1905.08922",
            "year": 2019
        },
        {
            "authors": [
                "S. Carlsson",
                "H. Azizpour",
                "A.S. Razavian",
                "J. Sullivan",
                "K. Smith"
            ],
            "title": "The preimage of rectifier network activities",
            "venue": "5th International Conference on Learning Representations ",
            "year": 2017
        },
        {
            "authors": [
                "S.R. Dubey",
                "S.K. Singh",
                "B.B. Chaudhuri"
            ],
            "title": "Activation functions in deep learning: A comprehensive survey and benchmark",
            "venue": "Neurocomputing, 503:92\u2013108",
            "year": 2022
        },
        {
            "authors": [
                "A. Fawzi",
                "S.-M. Moosavi-Dezfooli",
                "P. Frossard",
                "S. Soatto"
            ],
            "title": "Empirical study of the topology and geometry of deep networks",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3762\u20133770",
            "year": 2018
        },
        {
            "authors": [
                "X. Glorot",
                "A. Bordes",
                "Y. Bengio"
            ],
            "title": "Deep sparse rectifier neural networks",
            "venue": "Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, volume 15 of Proc. Mach. Learn. Res., pages 315\u2013323",
            "year": 2011
        },
        {
            "authors": [
                "I. Goodfellow",
                "D. Warde-Farley",
                "M. Mirza",
                "A. Courville",
                "Y. Bengio"
            ],
            "title": "Maxout networks",
            "venue": "Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proc. Mach. Learn. Res., pages 1319\u20131327",
            "year": 2013
        },
        {
            "authors": [
                "B. Hanin",
                "D. Rolnick"
            ],
            "title": "Deep ReLU networks have surprisingly few activation patterns",
            "venue": "Proceedings of the 33rd International Conference on Neural Information Processing Systems (NeurIPS\u201919), volume 32 of Adv. Neural Inf. Process. Syst.",
            "year": 2019
        },
        {
            "authors": [
                "B. Hanin",
                "M. Sellke"
            ],
            "title": "Approximating continuous functions by ReLU nets of minimal width",
            "venue": "arXiv:1710.11278",
            "year": 2017
        },
        {
            "authors": [
                "B. Liu",
                "M. Shen"
            ],
            "title": "Some geometrical and topological properties of DNNs\u2019 decision boundaries",
            "venue": "Theor. Comput. Sci., 908:64\u201375",
            "year": 2022
        },
        {
            "authors": [
                "G. Mont\u00fafar",
                "R. Pascanu",
                "K. Cho",
                "Y. Bengio"
            ],
            "title": "On the number of linear regions of deep neural networks",
            "venue": "Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS\u201914), volume 26 of Adv. Neural Inf. Process. Syst.",
            "year": 2014
        },
        {
            "authors": [
                "S.-M. Moosavi-Dezfooli",
                "A. Fawzi",
                "J. Uesato",
                "P. Frossard"
            ],
            "title": "Robustness via curvature regularization",
            "venue": "and vice versa. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9078\u20139086",
            "year": 2019
        },
        {
            "authors": [
                "G. Naitzat",
                "A. Zhitnikov",
                "L.-H. Lim"
            ],
            "title": "Topology of deep neural networks",
            "venue": "Journal of Machine Learning Research, 21(184):1\u201340",
            "year": 2020
        },
        {
            "authors": [
                "L. Zhang",
                "G. Naitzat",
                "L.-H. Lim"
            ],
            "title": "Tropical geometry of deep neural networks",
            "venue": "Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proc. Mach. Learn. Res., pages 5824\u20135832",
            "year": 2018
        },
        {
            "authors": [
                "Z. Zhang",
                "H. Zha"
            ],
            "title": "Nonlinear dimension reduction via local tangent space alignment",
            "venue": "Intelligent Data Engineering and Automated Learning, pages 477\u2013481. Springer Berlin Heidelberg",
            "year": 2003
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The most popular non-linear activation functions in deep learning today are variants of the Rectified Linear Unit (ReLU) \u2013 in its standard form defined ReLU(x) = max(0, x). This popularity is due to its state-of-the-art performance, both regarding efficiency, thanks to computational simplicity, and training, thanks to mostly linear behavior and alleviation of issues such as vanishing gradients [7, 9]. The focus of this paper are fully-connected layers with ReLU activations, herein denoted ReLU layers, on the from\nT (x) = ReLU(Ax+ b) (1.1)\nwhere the ReLU is applied component-wise and the matrix A and vector b are the layer\u2019s training parameters. These are fundamental building blocks of common deep network architectures, for instance, convolutional and feed-forward networks, and an increased theoretical understanding of their behavior is key for fully understanding the properties of the architectures that include them.\nContributions. In [5] Carlsson provides a geometric description for the action of a layer on the form (1.1), and uses it to give a procedure for computing preimages of deep convolutional networks. We here further formalize and interpret this geometric\nar X\niv :2\n31 0.\n03 48\n2v 2\n[ cs\n.L G\n] 8\nN ov\n2 02\n3\ndescription, which we then use to show properties of decision boundaries generated by feed-forward networks. Our main contributions are summarized in the following points.\n\u2022 We formalize the geometric description of a ReLU layer in [5] by introducing a detailed set notation for the natural partitions of the layer domain and codomain induced by the parameters. This facilitates explicit expressions for the images and preimages of the layer and a geometric interpretation of a ReLU layer as a projection onto a polyhedral cone followed by an affine transformation. The description is generalized to include contracting ReLU layers where the dimension is reduced. In an upcoming paper, this geometric description will serve as a basis for deriving error bounds for approximating hypersurfaces by decision boundaries of deep ReLU networks.\nWe utilize the formalized description in a binary classification problem, where the decision boundary separating two classes is defined as the zero-contour of a d-dimensional fully-connected feed-forward network with one hidden ReLU layer T : Rd \u2192 Rd and a final affine transformation L : Rd \u2192 R. With the exception of very specific parameter configurations, we prove that:\n\u2022 The number of linear pieces of the decision boundary is precisely 2d \u2212 2m, where m is an integer given by the parameters in the ReLU layer via the geometric description.\n\u2022 Modulo an affine transformation, such a network can only generate d different decision boundaries.\nFurther, we discuss how the class of decision boundaries is affected by adding additional hidden ReLU layers to the network.\nPrevious Works. While there is an abundance of empirical studies of various properties of networks with ReLU layers, we here mainly focus on theoretical results for finite ReLU networks on the form\nF (x) = L \u25e6 T (N) \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 T (1)(x) (1.2)\nThe basis of many studies, including the present work, is the fundamental observation that ReLU networks are continuous piecewise linear functions. A measure of geometric complexity is the number of linear pieces such networks produce, which can grow exponentially in the number of layers [14]. These pieces are however highly dependent of each other, and in practice, deep networks may only use a portion of their theoretical maximum expressiveness [11]. By specific parameter choices ReLU networks can be constructed to represent the maximum operation, from which it can be deduced that any continuous piecewise linear function can be represented by a ReLU network with sufficiently many parameters [2,10,12], which in turn implies that such networks can approximate smooth functions.\nFrom a geometric point of view, a deep neural network can be seen as a sequence of mappings that gradually transforms seemingly geometrically complex input data to something manageable [4], where each layer in general simplifies the data\u2019s shape [3,18], or even it\u2019s topology [16]. A geometric description of how a ReLU layer on the form (1.1) transforms the data, as a projection onto a polyhedral cone followed by an affine transformation, is given in [5]. This description is based on a dual basis induced by a geometric interpretation of the layer\u2019s parameters [6], and is utilized for computing preimages of convolutional networks.\nAs the decision boundaries of deep neural networks characterize the learned classifier, it is essential to understand their mathematical properties including their geometry and complexity. For instance, the works [8, 15] present a connection between geometrical properties of the decision boundaries and the robustness of the classifier. The authors provide evidence that there is a strong relation between the sensitivity of perturbation of the input data and large curvature of the decision boundary of the network. Along this line, the authors of [13] use tools from differential geometry to derive sufficient conditions on the network parameters for producing flat or developable decision boundaries. They also provide a method to compute topological properties of the decision boundary.\nAnother geometric viewpoint is presented in [17], where ReLU networks are described in terms of tropical geometry. This provides a connection between properties of the network and tropical geometric objects, where for instance the ReLU layer (1.1) is characterized by the tropical zonotopes. This work was recently extended in [1] where they provide a geometrical description of the decision boundary for a shallow network model. They prove that the decision boundary is contained in the convex hull of two zonotopes derived from the network parameters.\nOutline. In Section 2, we describe the structure of the mapping defined by a standard ReLU layer. Inspired by [5], we introduce a convenient dual basis obtained through the parameters in the layer, and using this dual basis, we construct a partition of the input space, allowing us to describe the action of the mapping explicitly. In Section 3, we utilize this description in a classification setting. We characterize in detail the geometry of decision boundaries generated by a shallow ReLU network and provide a high-level description of the effect on decision boundaries when adding more layers to a network. In Section 4, we summarize our findings."
        },
        {
            "heading": "2 The Geometrical Structure of a ReLU Layer",
            "text": "In this section, we analyze the geometrical structure of fully-connected ReLU layers and derive expressions for their preimages. The map of a ReLU layer (1.1) can be written\nT : Rd \u220b x 7\u2192 ReLU(Ab(x)) \u2208 Rd+ (2.1)\nwhere Rd+ denotes the non-negative orthant in Rd and Ab : Rd \u2192 Rd is the affine map\nAb(x) = Ax+ b (2.2)\nwith parameters A \u2208 Rd\u00d7d and b \u2208 Rd.\nGeometry of the Affine Map. We begin by giving a geometric interpretation of the parameters in the affine map (2.2). Let row i in the affine map be denoted \u03c1i(x), i.e.,\n\u03c1i(x) = ai \u00b7 x+ bi = (Ax+ b)i for i = 1, . . . , d (2.3)\nwhere ai \u2208 Rd is the i:th row of the matrix A and bi \u2208 R is the i:th element of the vector b. The zero levels for {\u03c1i}di=1 define hyperplanes\nPi = {x \u2208 Rd : \u03c1i(x) = 0} for i = 1, . . . , d (2.4)\nwith normals ai, and we let the sign of \u03c1i define half-spaces\nUi,+ = {x \u2208 Rd : \u03c1i(x) > 0}, Ui,\u2212 = {x \u2208 Rd : \u03c1i(x) < 0} (2.5)\nNote that \u03c1i is a scaled version of the signed distance function associated with Pi, which is positive on Ui,+ (the half-space into which ai is directed) and negative on Ui,\u2212. Further, assume that {ai}di=1 spans Rd so that the hyperplanes are in general position. Then, the intersection of the hyperplanes is a point x0 = \u2229di=1Pi, which is the unique solution to the linear system of equations\nAx0 + b = 0 (2.6)\nLet I = {1, . . . , d} and define Li to be the line Li = \u22c2\nj\u2208I\\{i}\nPj (2.7)\npassing through x0. Since the hyperplanes are assumed to be in general position, the line Li and the hyperplane Pi will only coincide at x0. Nevertheless, for j \u0338= i we have that Li \u2282 Pj by definition. In accordance with the work in [5], for i \u2208 I we will let a\u2217i \u2208 Rd be a vector parallel to Li directed such that x0 + a \u2217 i \u2208 Ui,+. Hence, aj \u00b7 a\u2217i = 0 for j \u0338= i and ai \u00b7 a\u2217i > 0, and by assigning a length to each vector a\u2217i these vectors become uniquely determined, which we summarize in the following definition.\nDefinition 2.1 (Dual Basis). Given an invertible matrix A \u2208 Rd\u00d7d with rows ai \u2208 Rd, we define the set of vectors {a\u2217i : i \u2208 I} satisfying\naj \u00b7 a\u2217i = \u03b4ij for i, j \u2208 I (2.8)\nand denote this set the dual basis of A.\nSince the vectors {ai : i \u2208 I} are assumed to be linearly independent, the vectors in the dual basis {a\u2217i : i \u2208 I} will also be linearly independent, and hence the dual basis is also a basis in Rd. The dual basis will be useful for describing the action of a ReLU layer (2.1). Figure 1 depicts the geometrical construction of the dual basis. Algebraically, the vector a\u2217i is the i:th column vector of the inverse matrix A\n\u22121 and, hence, the action of the matrix A on the vector a\u2217i is simply\nAa\u2217i = ei (2.9)\nwhere ei is the i:th basis vector in the standard Euclidean basis. By expanding x \u2208 Rd in the dual basis, such that\nx = x0 + \u2211 i\u2208I \u03bbia \u2217 i (2.10)\nwith coefficients \u03bbi \u2208 R, and applying the affine map (2.2) we have\nAb(x) = Ax+ b = Ax0 + b\ufe38 \ufe37\ufe37 \ufe38 =0 + \u2211 i\u2208I \u03bbi(Aa \u2217 i ) = \u2211 i\u2208I \u03bbiei = [\u03bb1, \u03bb2, . . . , \u03bbd] T (2.11)\nThus, applying the affine map yields a vector with the coefficients of the expansion in the dual basis {a\u2217i : i \u2208 I} as its elements."
        },
        {
            "heading": "2.1 Partition of the Domain and Codomain",
            "text": "In this section, we will introduce a partition of the ReLU layer domain \u2014 the Rd input, and a partition of the ReLU layer codomain \u2013 the Rd output. These partitions will be useful in describing the action of the layer. Recall that I = {1, . . . , d} and consider two disjoint index subsets I+, I\u2212 \u2286 I where I+ \u2229 I\u2212 = \u2205, denoted by the pairing I = (I+, I\u2212). Let I be the set of all such pairings.\nDomain Partition. For a given (I+, I\u2212) \u2208 I we define the following subset of Rd\nS(I+,I\u2212) = { x \u2208 Rd : x = x0 + \u2211 i\u2208I+ \u03b1ia \u2217 i \u2212 \u2211 i\u2208I\u2212 \u03b1ia \u2217 i , with \u03b1i > 0 } (2.12)\nThe family of all such sets\nS = {SI \u2282 Rd : I \u2208 I} (2.13)\nwill serve useful in describing the action of the ReLU layer in different parts of the domain. We first verify that S constitutes a partition of Rd. Consider a general point x \u2208 Rd expanded in the dual basis (2.10), and define the pairing\n(I+, I\u2212) = ( {i \u2208 I : \u03bbi > 0}, {i \u2208 I : \u03bbi < 0} ) (2.14)\nThe point x \u2208 Rd can then be expanded on the form\nx = x0 + \u2211 i\u2208I \u03bbia \u2217 i = x0 + \u2211 i\u2208I+ \u03b1ia \u2217 i \u2212 \u2211 i\u2208I\u2212 \u03b1ia \u2217 i (2.15)\nwith coefficients \u03b1i = |\u03bbi| > 0 for i \u2208 I+ \u222a I\u2212, and we recognize this as the structure of points in (2.12). Hence, each point in Rd belongs to precisely one SI , which in turn means that S defines a partition of Rd, such that\nRd = \u22c3 I\u2208I SI and \u2205 = SI \u2229 SJ for I \u0338= J (2.16)\nWhen we refer to the dimension of a set S(I+,I\u2212) \u2208 S we mean the dimension of the subspace span({a\u2217i : i \u2208 I+ \u222a I\u2212}), and since the dual vectors a\u2217i are assumed to be linearly independent we simply get dim(S(I+,I\u2212)) = |I+ \u222a I\u2212|. By a simple combinatorial argument, it is easy to verify that the number of k dimensional sets in the partition S is precisely equal to ( d k ) 2k, so the total number of sets in S is\n|S| = d\u2211\nk=0\n( d\nk\n) 2k = 3d (2.17)\nby the binomial theorem. An example of a partition S generated by a dual basis in R2 is illustrated in Figure 2.\nCodomain Partition. We will now introduce another partition of Rd that will be useful in describing the geometrical structure of the codomain of the ReLU layer. Since x0 and {a\u2217i : i \u2208 I} are derived from the parameters A and b in (2.1), it is easy to verify that in the canonical case, when A = Id (the d \u00d7 d identity matrix) and b = 0, we have x0 = 0 and a \u2217 i = ei. We will use the hat symbol to denote sets defined by (2.12) in this specific setting, that is\nS\u0302(I+,I\u2212) =\n{ x \u2208 Rd : x = \u2211 i\u2208I+ \u03b2iei \u2212 \u2211 i\u2208I\u2212 \u03b2iei, with \u03b2i > 0 } (2.18)\nFor instance, we have S\u0302(I,\u2205) = Rd++ \u2014 the strictly positive orthant in Rd, and S\u0302(\u2205,\u2205) = {0}. In accordance with the domain partition above, the family of sets on the form (2.18) also generates a partition of Rd, which we denote by S\u0302. Examples of partitions S\u0302 and S of R3 are depicted in Figure 3.\nAffine Equivalence. The affine map Ab : Rd \u2192 Rd induces a one-to-one correspondence between the sets in S and those in S\u0302 in the sense that for all I \u2208 I, the restriction Ab : SI \u2192 S\u0302I is a bijection. Indeed, for any x \u2208 SI = S(I+,I\u2212) we have that x = x0 + \u2211 i\u2208I+ \u03b1ia \u2217 i \u2212 \u2211 i\u2208I\u2212 \u03b1ia \u2217 i for some \u03b1i > 0 by (2.12). From equation (2.11) it follows\nAx+ b = \u2211 i\u2208I+ \u03b1iei \u2212 \u2211 i\u2208I\u2212 \u03b1iei \u2208 S\u0302(I+,I\u2212) (2.19)\nFurther, the inverse A\u22121b : S\u0302I \u2192 SI is given by A \u22121 b (y) = A \u22121(y \u2212 b) and for every y \u2208 S\u0302(I+,I\u2212) with y = \u2211 i\u2208I+ \u03b2iei \u2212 \u2211 i\u2208I\u2212 \u03b2iei for some \u03b2i > 0 it holds\nA\u22121(y \u2212 b) = x0 + \u2211 i\u2208I+ \u03b2ia \u2217 i \u2212 \u2211 i\u2208I\u2212 \u03b2ia \u2217 i \u2208 S(I+,I\u2212) (2.20)\nbecause of the definition of x0 and recalling that a \u2217 i is the i:th column vector in A \u22121. Thus, the affine map takes every set SI in S to the corresponding set S\u0302I in S\u0302 and vice versa for the inverse. In that sense, the pairings (I+, I\u2212) are invariant under the affine map due to our construction of the partitions S and S\u0302.\nClosure and Boundary. The closure of a set S(I+,I\u2212) is given by\nS(I+,I\u2212) = { x \u2208 Rd : x = x0 + \u2211 i\u2208I+ \u03b1ia \u2217 i \u2212 \u2211 i\u2208I\u2212 \u03b1ia \u2217 i , with \u03b1i \u2265 0 } (2.21)\ni.e., the conical hull of the set {a\u2217i : i \u2208 I+} \u222a {\u2212a\u2217i : i \u2208 I\u2212} translated by x0. In particular, S(I,\u2205) is a polyhedral cone with apex x0 with supporting hyperplanes Pi, i \u2208 I, as illustrated in Figure 4. Therefore, in the canonical case we get S\u0302(I,\u2205) = Rd+. We can also express the closure in terms of other sets in S. To see this, we introduce a partial order \u2aaf on I. For I,J \u2208 I with I = (I+, I\u2212), J = (J+, J\u2212) we define J \u2aaf I if and only if J+ \u2286 I+ and J\u2212 \u2286 I\u2212. Further, we write J \u227a I if J \u2aaf I and J \u0338= I. This gives the following compact expressions for the closure respectively the boundary of a set SI\nSI = \u22c3 J\u2aafI SJ and \u2202SI = \u22c3 J\u227aI SJ (2.22)"
        },
        {
            "heading": "2.2 Image of a ReLU Layer",
            "text": "When considering the entire map T , the situation is slightly more complicated due to the application of the ReLU function. In general, several sets in S will be mapped to the same set in S\u0302. As we will see the boundary \u2202S\u0302(I,\u2205) of S\u0302(I,\u2205) = Rd+ given by\n\u2202S\u0302(I,\u2205) = \u22c3\nJ\u227a(I,\u2205) S\u0302J = \u22c3 J\u2282I S\u0302(J,\u2205) (2.23)\nwill be an important object when studying the structure of T and therefore we also introduce the partition \u2202S\u0302 = {S\u0302J : J \u227a (I, \u2205)} of this boundary. Along the same lines, we define \u2202S. Lemma 2.1 below describes the action of T on sets in S.\nLemma 2.1 (Image Structure of a ReLU Layer). Given a set S(I+,I\u2212) \u2208 S it holds\nT (S(I+,I\u2212)) = S\u0302(I+,\u2205) (2.24)\nProof. Consider a point x \u2208 S(I+,I\u2212), which by (2.12) has the expansion\nx = x0 + \u2211 i\u2208I+ \u03b1ia \u2217 i \u2212 \u2211 i\u2208I\u2212 \u03b1ia \u2217 i (2.25)\nwith coefficients \u03b1i > 0. Applying the ReLU layer (2.1) and using equation (2.19) we get\nT (x) = ReLU(Ax+ b, 0) (2.26)\n= max (\u2211 i\u2208I+ \u03b1iei \u2212 \u2211 i\u2208I\u2212 \u03b1iei, 0 ) (2.27)\n= \u2211 i\u2208I+ \u03b1iei \u2208 S\u0302(I+,\u2205) (2.28)\nwhere the last equality holds by the definition of the Euclidian basis vectors ei and that \u03b1i > 0. Conversely, for any y = \u2211 i\u2208I+ \u03b2iei \u2208 S\u0302(I+,\u2205) where \u03b2i > 0 we have\nA\u22121(y \u2212 b)\u2212 \u2211 i\u2208I\u2212 \u03b1ia \u2217 i \u2208 S(I+,I\u2212) (2.29)\nfor any choice of \u03b1i > 0 according to equation (2.20). Also,\nT ( A\u22121(y \u2212 b)\u2212 \u2211 i\u2208I\u2212 \u03b1ia \u2217 i ) = y (2.30)\nand hence for all y \u2208 S\u0302(I+,\u2205) there is an x \u2208 S(I+,I\u2212) such that T (x) = y. \u25a0\nLemma 2.1 reveals that whenever I+ \u2282 I we have\nT (S(I+,I\u2212)) = S\u0302(I+,\u2205) \u2208 \u2202S\u0302 (2.31)\nand in particular when I+ = I we get\nT (S(I,\u2205)) = S\u0302(I,\u2205) (2.32)\nHence, T reduces to the affine map Ab when restricted to S(I,\u2205) and therefore we will refer to S(I,\u2205) as the affine sector in S. In fact, T acts affinely on all points in the polyhedral cone S(I,\u2205). Moreover, by the lemma we also see that\ndim(T (S(I+,I\u2212))) = dim(S\u0302(I+,\u2205)) = |I+| \u2264 |I+ \u222a I\u2212| = dim(S(I+,I\u2212)) (2.33)\nHence, the dimension of the images of S(I+,I\u2212) with I\u2212 \u0338= \u2205 under the map T is reduced. The only sets in S with preserved dimension are those that are subsets of S(I,\u2205). Especially,\nthe only set with a non-zero measure in Rd for which the dimension is preserved is the affine sector S(I,\u2205) on which T acts affinely. All other sets in S with non-zero measure will be mapped onto some lower dimensional set in \u2202S\u0302. Since points in a dataset will generally belong to a subset of the d-dimensional sets (those with non-zero measure) in S and all of them, but one, will be mapped to some lower dimensional set on the boundary \u2202S\u0302(I,\u2205) it is clear that T has contracting properties. Thus, iteratively applying maps of the form (2.1) as is done in deep fully-connected ReLU networks will efficiently contract the input data.\nGeometric Interpretation. The ReLU layer (2.1) is constructed as the affine map Ab followed by the ReLU activation function, which is a projection Rd 7\u2192 S\u0302I,\u2205 = Rd+. Using the geometric structure defined above, we will now give an alternative construction where the order of these operations is reversed, as a projection onto a polyhedral cone followed by the affine map Ab. We know that a point x \u2208 S(I+,I\u2212) with expansion x = x0 +\u2211\ni\u2208I+ \u03b1ia \u2217 i \u2212 \u2211 i\u2208I\u2212 \u03b1ia \u2217 i , \u03b1i > 0, is mapped to the point y = \u2211 i\u2208I+ \u03b1iei \u2208 S\u0302(I+,\u2205) \u2282 R d +.\nWe can split this transformation into two steps x0 + \u2211 i\u2208I+ \u03b1ia \u2217 i \u2212 \u2211 i\u2208I\u2212\n\u03b1ia \u2217 i\ufe38 \ufe37\ufe37 \ufe38\n\u2208S(I+,I\u2212)\n\u03c07\u2212\u2192 x0 + \u2211 i\u2208I+\n\u03b1ia \u2217 i\ufe38 \ufe37\ufe37 \ufe38\n\u2208S(I+,\u2205)\u2282S(I,\u2205)\nAb7\u2212\u2192 \u2211 i\u2208I+\n\u03b1iei\ufe38 \ufe37\ufe37 \ufe38 \u2208S\u0302(I+,\u2205)\u2282R d +\n(2.34)\nThis suggests that we can decompose the ReLU layer T : Rd \u2192 Rd+ as\nT = ReLU \u25e6 Ab = Ab \u25e6 \u03c0 (2.35)\nwhere \u03c0 : Rd \u2192 S(I,\u2205) is a surjective projection mapping the input space Rd onto the polyhedral cone S(I,\u2205) and Ab : S(I,\u2205) \u2192 Rd+ is the bijective affine map given by (2.2) mapping the polyhedral cone onto the non-negative orthant Rd+. Note that (2.35) gives a description of how Ab commutes with the ReLU. The projection \u03c0 is piecewise defined on the sectors in S. For a point x \u2208 S(I+,I\u2212) we define\n\u03c0(x) = \u03c0 ( x0 + \u2211 i\u2208I+ \u03b1ia \u2217 i \u2212 \u2211 i\u2208I\u2212 \u03b1ia \u2217 i ) = x0 + \u2211 i\u2208I+ \u03b1ia \u2217 i (2.36)\nwhich clearly is a projection since \u03c0 \u25e6 \u03c0(x) = \u03c0(x). For points x \u2208 S(I,\u2205) the projection \u03c0 acts trivially, i.e., \u03c0(x) = x, whereas points outside the cone will be mapped to some part of the cone boundary. Apart from a translation, the non-linear properties of T are entirely captured by the projection \u03c0 since Ab is affine. The geometrical structure of the decomposition of T is depicted in Figure 5. From the construction of \u03c0 it is clear that a sector S(I+,I\u2212) with I\u2212 \u0338= \u2205 will be projected onto S(I+,\u2205) \u2208 \u2202S. Hence, the |I+ \u222a I\u2212|dimensional set S(I+,I\u2212) is projected onto a |I+|-dimensional part of the boundary of the polyhedral cone. Moreover, from the definition (2.36) of \u03c0 we see that the projection is parallel to the subspace span{a\u2217i : i \u2208 I\u2212}. The action of \u03c0 in R3 is illustrated in Figure 6.\nRemark 2.1 (Contracting ReLU Layers). By a minor modification, this geometrical description also extends to ReLU layers where the input dimension is reduced, i.e., T : Rd \u2192 Rm+ where d > m. In this scenario, A \u2208 Rm\u00d7d will have fewer rows than columns and we only get m hyperplanes in Rd, defined as in (2.4). Assuming the rows of A are\nlinearly independent, the hyperplanes will not intersect in a point but in a (d \u2212 m)dimensional affine subspace \u22c2m i=1 Pi of Rd. However, if we define the m-dimensional\nsubspace V = span({a1, a2, . . . , am}) we get that V \u2229 (\u22c2m i=1 Pi ) is a single point x0 \u2208 V . Similarly, if we let I = {1, 2, . . . ,m} we can, for each i \u2208 I, define the dual vector a\u2217i parallel to the line\nLi = V \u2229 ( \u22c2 j\u2208I\\{i} Pj ) (2.37)\nand scaled such that ai \u00b7 a\u2217j = \u03b4ij for i, j \u2208 I. These m dual vectors will also be linearly independent and therefore they will be a basis of the subspace V . Using these dual vectors, we proceed as before by defining a partition of the subspace V using the sets\nS(I+,I\u2212) = { x \u2208 Rd : x = x0 + \u2211 i\u2208I+ \u03b1ia \u2217 i \u2212 \u2211 i\u2208I\u2212 \u03b1ia \u2217 i , \u03b1i > 0 } (2.38)\nfor disjoint index sets I+, I\u2212 \u2286 I = {1, 2, . . . ,m}. In this case we get that S(I,\u2205) \u2282 V is an m-dimensional cone with apex at x0 embedded in Rd. Let V \u22a5 be the orthogonal complement to V in Rd, i.e.,\nV \u22a5 = {x \u2208 Rd : x \u00b7 v = 0, \u2200v \u2208 V } (2.39)\nand let {wi}i\u2208I\u22a5 , I\u22a5 = {m + 1, . . . , d}, be some basis to V \u22a5. Since Rd = V \u2295 V \u22a5 and x0 \u2208 V , each point x \u2208 Rd has the expansion\nx = x0 + \u2211 i\u2208I\n\u03bbia \u2217 i\ufe38 \ufe37\ufe37 \ufe38\n\u2208V\n+ \u2211 i\u2208I\u22a5\n\u03bbiwi\ufe38 \ufe37\ufe37 \ufe38 \u2208V \u22a5\n, where \u03bbi \u2208 R (2.40)\nBecause the rows of A are vectors in V whereas wi \u2208 V \u22a5 we by the definition of the orthogonal complement (2.39) have Awi = 0, and it follows that T is invariant to components in V \u22a5 such that\nT (x) = T ( x0 + \u2211 i\u2208I \u03bbia \u2217 i + \u2211 i\u2208I\u22a5 \u03bbiwi ) = T ( x0 + \u2211 i\u2208I \u03bbia \u2217 i ) (2.41)\nWe can incorporate this into our geometric description by prepending an orthogonal projection onto the subspace V \u2282 Rd. This gives a decomposition of T : Rd \u2192 Rm+ as\nx = x0 + \u2211 i\u2208I+ \u03b1ia \u2217 i \u2212 \u2211 i\u2208I\u2212 \u03b1ia \u2217 i + \u2211 i\u2208I\u22a5\n\u03bbiwi\ufe38 \ufe37\ufe37 \ufe38 \u2208Rd\nPV7\u2212\u2192 x0 + \u2211 i\u2208I+ \u03b1ia \u2217 i \u2212 \u2211 i\u2208I\u2212\n\u03b1ia \u2217 i\ufe38 \ufe37\ufe37 \ufe38\n\u2208S(I+,I\u2212)\u2282V\n\u03c07\u2212\u2192 x0 + \u2211 i\u2208I+\n\u03b1ia \u2217 i\ufe38 \ufe37\ufe37 \ufe38\n\u2208S(I+,\u2205)\u2282S(I,\u2205)\nAb7\u2212\u2192 \u2211 i\u2208I+\n\u03b1iei\ufe38 \ufe37\ufe37 \ufe38 \u2208S\u0302(I,\u2205)=Rm+\n(2.42)\nwhere PV : Rd \u2192 V is the orthogonal projection onto V , whereafter the same geometrical description (2.34) as in the case of preserved dimension is used with the difference that the cone projection takes place in the m-dimensional subspace V \u2282 Rd."
        },
        {
            "heading": "2.3 Preimage of a ReLU Layer",
            "text": "The preimage of a set \u03c9\u0302 \u2286 Rd under the ReLU layer T is the set of all elements in the domain Rd that T maps into \u03c9\u0302, and we denote the preimage by T\u22121(\u03c9\u0302). Based on Lemma 2.1 and its proof, we will here express preimages under T using the geometrical structure of the domain and codomain detailed above. Firstly, since T maps Rd onto Rd+ = S\u0302(I,\u2205), the preimage of any point y /\u2208 S\u0302(I,\u2205) will be empty. Secondly, equation (2.30) in the proof of Lemma 2.1 shows that the set{\nx \u2208 Rd : x = A\u22121(y \u2212 b)\u2212 \u2211 i\u2208I\u2212 \u03b1ia \u2217 i , \u03b1i > 0 } (2.43)\ncontains all points in S(I+,I\u2212) that are mapped to a specific point y \u2208 S\u0302(I+,\u2205) \u2282 S\u0302(I,\u2205). In other words, the set in (2.43) contains the preimage of y intersected with S(I+,I\u2212). Thirdly, Lemma 2.1 reveals that the ReLU layer is invariant with respect to I\u2212, i.e.,\nT (S(I+,I\u2212)) = T (S(I+,J\u2212)) \u2200J\u2212 \u2286 I \\ I+ (2.44)\nHence, the complete preimage of a single point y \u2208 S\u0302(I+,\u2205) is given by\nT\u22121(y) = \u22c3\nK\u2286I\\I+\n{ x \u2208 Rd : x = A\u22121(y \u2212 b)\u2212 \u2211 i\u2208K \u03b1ia \u2217 i , \u03b1i > 0 } (2.45)\n= { x \u2208 Rd : x = A\u22121(y \u2212 b)\u2212 \u2211 i\u2208I\\I+ \u03b1ia \u2217 i , \u03b1i \u2265 0 } (2.46)\nIf y \u2208 S\u0302(I+,\u2205), its complete preimage is spanned by {a\u2217i : i \u2208 I \\ I+}, so\ndim(T\u22121(y)) = d\u2212 dim(S\u0302(I+,\u2205)) = codim(S\u0302(I+,\u2205)) (2.47)\nHence, points intersecting a lower dimensional facet of \u2202S\u0302 will generate a preimage of higher dimension than points intersecting a higher dimensional facet. Figure 7 illustrates the structure of the preimages for different points in R3+. The preimage of an entire set S\u0302I \u2208 S\u0302 is expressed in the following lemma.\nLemma 2.2 (Preimage Structure under a ReLU Layer). The preimage of the set S\u0302(J,\u2205) \u2208 S\u0302 under T , where J \u2286 I, is given by\nT\u22121(S\u0302(J,\u2205)) = \u22c3\nK\u2286I\\J\nS(J,K) (2.48)\nProof. Starting from (2.46), the preimage of the entire set S\u0302(J,\u2205) can computed as\nT\u22121(S\u0302(J,\u2205)) = \u22c3\ny\u2208S\u0302(J,\u2205)\n{ x \u2208 Rd : x = A\u22121(y \u2212 b)\u2212 \u2211 i\u2208I\\J \u03b1ia \u2217 i , \u03b1i \u2265 0 } (2.49)\n= \u22c3\nx\u2032\u2208S(J,\u2205)\n{ x \u2208 Rd : x = x\u2032 \u2212 \u2211 i\u2208I\\J \u03b1ia \u2217 i , \u03b1i \u2265 0 } (2.50)\n= \u22c3\nx\u2032\u2208S(J,\u2205) \u22c3 K\u2286I\\J { x \u2208 Rd : x = x\u2032 \u2212 \u2211 i\u2208K \u03b1ia \u2217 i , \u03b1i > 0 } (2.51)\n= \u22c3\nK\u2286I\\J \u22c3 x\u2032\u2208S(J,\u2205) { x \u2208 Rd : x = x\u2032 \u2212 \u2211 i\u2208K \u03b1ia \u2217 i , \u03b1i > 0 } (2.52)\n= \u22c3\nK\u2286I\\J\nS(J,K) (2.53)\nwhich concludes the proof. \u25a0\nUsing Lemma 2.2 we can also derive expressions for preimages of the closures of sets in S\u0302. By the definition of the closure (2.22), we obtain\nT\u22121 ( S\u0302(J,\u2205) ) = T\u22121 ( \u22c3 K\u2286J S\u0302(K,\u2205) ) = \u22c3 K\u2286J T\u22121(S\u0302(K,\u2205))\n= \u22c3 K\u2286J ( \u22c3 L\u2286I\\K S(K,L) ) = \u22c3 K\u2286J S(K,I\\K)\n(2.54)\nNow, given a subset \u03c9\u0302 \u2286 S\u0302(J,\u2205) we get\nT\u22121(\u03c9\u0302) = { x \u2208 Rd : x = x\u2032 \u2212 \u2211 i\u2208I\\J \u03b1ia \u2217 i , \u03b1i \u2265 0, x\u2032 \u2208 \u03c9 } (2.55)\nwhere \u03c9 = A\u22121b (\u03c9\u0302), i.e, the preimage of \u03c9\u0302 under the affine transformation. To keep the notation consistent, we will continue labeling quantities related to sets seen as subsets of the codomain of T using the hat symbol (e.g, S\u0302I and \u03c9\u0302), while the corresponding quantities related to the inverse image of the same subset under the affine map Ab will be labeled in the same way but without the hat (e.g., SI and \u03c9). In the special case when \u03c9\u0302 \u2286 S\u0302(I,\u2205) the preimage is simply given by\nT\u22121(\u03c9\u0302) = { x \u2208 Rd : x = x\u2032, x\u2032 \u2208 \u03c9 } = \u03c9 (2.56)\nas T reduces to an invertible affine map on S(I,\u2205). For a general set \u03c9\u0302 \u2286 Rd+ its preimage under T is completely described in terms of the dual basis {a\u2217i : i \u2208 I} and its intersection with the sets in \u2202S\u0302. Geometrically, the preimage of \u03c9\u0302 is obtained by first mapping the intersection \u03c9\u0302 \u2229Rd+ to the cone, i.e., to \u03c9 \u2229 S(I,\u2205) and then the parts on the boundary of the cone will be extended outwards in directions given by a subset of the dual vectors as illustrated in Figure 8."
        },
        {
            "heading": "3 Application to Feed-Forward Networks",
            "text": "In this section, we investigate how the results above can be applied in a binary classification setting, where the decision boundary separating two classes is formulated as the zero contour to a feed-forward ReLU network.\nDefinition 3.1 (Fully-Connected ReLU Network). A function F : Rdin \u2192 Rdout is called a fully-connected ReLU network of widths d1, . . . , dN and depth N if it can be written as the composition\nF (x) = L \u25e6 T (N) \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 T (1)(x) (3.1)\nwhere L : RdN \u2192 Rdout is an affine function and T (k) : Rdk\u22121 \u2192 Rdk+ , k = 1, . . . , N , d0 = din are functions of the form\nT (k)(x) = ReLU(A(k)x+ b(k)) (3.2)\nparameterized by A(k) \u2208 Rdk\u00d7dk\u22121 , b(k) \u2208 Rdk .\nSuch a network architecture is illustrated in Figure 9. From Definition 3.1 it is easy to see that a fully-connected ReLU network is a continuous piecewise linear function on some polygonal partition of the input domain. Changing the internal parameters A(k), b(k) will affect not only the function values computed by the network but also the polygonal partition it is subordinate to in a nontrivial manner. For simplicity, we will let dout = 1 so the network computes a real-valued function, which is a convenient choice when solving binary classification problems, and we restrict ourselves to networks of constant width equal to the input dimension, that is, din = d1 = d2 = . . . = dN = d. Our approach also generalizes to networks where din \u2265 d1 \u2265 d2 \u2265 . . . \u2265 dN , see Remark 2.1.\nLemma 3.1 (Canonical Network Structure). Any network on the form (3.1) where the parameters in each layer are such that the dual basis is well defined (by Definition 2.1, or Remark 2.1 in the case of a contracting layer) is equivalent to the network\nF (x) = L\u0303 \u25e6 \u03c0\u0303(N) \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 \u03c0\u0303(1)(x) (3.3)\nwhere \u03c0\u0303(k) is the projection onto a polyhedral cone defined by the parameters A\u0303(k) \u2208 Rdk\u00d7d1, b\u0303(k) \u2208 Rdk of the affine function\nA\u0303b (k) (x) = A\u0303(k)x+ b\u0303(k) = A (k) b \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 A (1) b (x) (3.4)\nand where L\u0303(x) = L \u25e6 A\u0303b (N) (x).\nProof. This follows directly from the commutating property (2.35) of a ReLU layer and the fact that compositions of affine functions are affine functions. \u25a0"
        },
        {
            "heading": "3.1 Classification Model Problem",
            "text": "LetX1, X2 \u2282 Rd be two sets, each identified with one of two classes, such thatX1\u2229X2 = \u2205. In general, these two underlying sets are unknown and we are only given a set of samples from them. The classification problem can be formulated as finding a real-valued network F separating the two sets in the following sense{\nF (x) > 0, if x \u2208 X1 F (x) < 0, if x \u2208 X2\n(3.5)\nPoints are then classified based on the sign after evaluating them using F . The conditions above split the input domain Rd into the super- and sublevel sets, {x \u2208 Rd : F (x) > 0} and {x \u2208 Rd : F (x) < 0}, called the decision regions of F . These regions define the network classifier in the sense that points will be classified by the network based on which region in Rd they belong to. The decision regions are separated by the hypersurface\n\u0393 = {x \u2208 Rd : F (x) = 0} (3.6)\ncalled the decision boundary of the network F . Mathematically, \u0393 is precisely the preimage of {0} under F : Rd \u2192 R. Consequently, a binary classification problem is solved if and only if F changes sign over \u0393, separating the sets X1 and X2.\nCondition on ReLU Layers. Our geometric interpretation of how fully-connected ReLU layers affect the data in this classification setting gives a fundamental condition on each such layer. The polyhedral cone defined through the parameters of the first layer will intersect Rd where the two disjoint sets X1 and X2 live. The sets will then be projected onto parts of the cone, as described above, and mapped affinely to Rd+. In this way, the data will evolve through the network by repeatedly applying such transformations for each ReLU layer present in the network. During the actual training of the network, the parameters of each layer are optimized which geometrically means that the shapes of the associated polyhedral cones are changing. Now, let X\u03031 and X\u03032 denote the images of X1 and X2, respectively, under the composition of the first n ReLU layers in a network. Then, if we apply one more ReLU layer T we must have\n\u03c0(X\u03031) \u2229 \u03c0(X\u03032) = \u2205 (3.7)\nwhere \u03c0 is the projection defined by T . Otherwise, the two data sets identified with different classes will be mixed; thus, the binary classification problem cannot be solved. The condition in (3.7) restricts the possible positions and orientations of the cones identified with each ReLU layer. Thus, if a network solves the binary classification problem, then (3.7) is a necessary condition for each layer in that network."
        },
        {
            "heading": "3.2 Decision Boundaries for Shallow Networks",
            "text": "We continue by analyzing decision boundaries for networks with one hidden layer. Consider a network F : Rd \u2192 R on the form\nF (x) = L \u25e6 T (x) (3.8)\nwhere the hidden layer T is a fully-connected ReLU layer (2.1) and the output layer L : Rd \u2192 R is an affine transformation\nL(y) = a\u0302L \u00b7 y + b\u0302L (3.9)\nwith parameters a\u0302L \u2208 Rd and b\u0302L \u2208 R. Note that the non-linear behavior of this network, defined by the composition Rd T\u2212\u2192 Rd+\nL\u2212\u2192 R, is contained in the hidden layer T . By the analysis above T reduces to (2.28) on each sector in S, and likewise, its preimage T\u22121 on each sector in S\u0302 reduces to (2.55). We are now interested in detailing the decision boundary (3.6) when F (x) is given by a shallow network (3.8). The output of T will lie\non S\u0302(I,\u2205) while the input to L generating a zero output will lie on the hyperplane\nP\u0302 = ker(L) = {y \u2208 Rd : a\u0302L \u00b7 y + b\u0302L = 0} (3.10)\nHence, we realize that the decision boundary can be expressed as the preimage T\u22121 of\nthe intersection between P\u0302 and S\u0302(I,\u2205). By further decomposing S\u0302(I,\u2205) into sectors in S\u0302, we arrive at the following expression for the decision boundary to a shallow network.\n\u0393 = {x \u2208 Rd : L \u25e6 T (x) = 0} (3.11) = T\u22121 ( P\u0302 \u2229 S\u0302(I,\u2205) ) (3.12)\n= T\u22121 ( P\u0302 \u2229 \u22c3 J\u2286I S\u0302(J,\u2205) ) (3.13)\n= \u22c3 J\u2286I T\u22121 ( P\u0302 \u2229 S\u0302(J,\u2205) ) (3.14)\nSince we are only interested in non-degenerate cases where \u0393 is an actual (d \u2212 1)- dimensional surface, i.e., not empty nor filling d-dimensional sectors in S, we assume the intersection P\u0302 \u2229 S\u0302(I,\u2205) to be non-empty. This implies 0 /\u2208 P\u0302 \u21d4 b\u0302L \u0338= 0 and to simplify our description below, we also assume that b\u0302L < 0, which we can do without loss of generality since if this is not the case we can factor out \u22121 from L that we instead incorporate into T .\nThe hypersurface \u0393 is a continuous piecewise linear surface where each index subset J \u2286 I corresponds to one linear piece of \u0393 that by (2.55) can be expressed\nT\u22121 ( P\u0302 \u2229 S\u0302(J,\u2205) ) = { x \u2208 Rd : x = x\u2032 \u2212 \u2211 i\u2208I\\J \u03bbia \u2217 i , \u03bbi \u2265 0, x\u2032 \u2208 P \u2229 S(J,\u2205) } (3.15)\nwhere P is the domain hyperplane given by the preimage of P\u0302 under the affine transformation (2.2). More explicitly, this hyperplane can be expressed\nP = {x \u2208 Rd : Ab(x) \u2208 P\u0302} = {x \u2208 Rd : aL \u00b7 x+ bL = 0} (3.16)\nwhere aL \u2208 Rd and bL \u2208 R are defined\naL = AT a\u0302L and bL = a\u0302L \u00b7 b+ b\u0302L (3.17)\nHence, \u0393 is completely described by the intersections P \u2229 S(J,\u2205) for J \u2286 I and the dual basis {a\u2217i : i \u2208 I}. Of particular interest are the preimages of the intersections with the (d\u2212 1)-dimensional faces\nT\u22121 ( P\u0302 \u2229 S\u0302(I\\{i},\u2205) ) = { x \u2208 Rd : x = x\u2032 \u2212 \u03bbia\u2217i , \u03bbi \u2265 0, x\u2032 \u2208 P \u2229 S(I\\{i},\u2205) } (3.18)\nwhere i \u2208 I. Since the remaining pieces are linear transitions between these parts \u0393 is completely determined by the preimages in (3.18). Figure 10 shows an example of how \u0393 is generated given a hyperplane as the kernel of an affine map L.\nAccording to (3.18), \u03b1\u2217i is a tangent vector to T \u22121(P\u0302 \u2229S\u0302(I\\{i})) whose direction relative to the central linear piece P \u2229 S(I,\u2205) (with normal direction aL) gives an indication on how \u0393 curves. Hence, the signs of the scalar products\n{a\u2217i \u00b7 aL : i \u2208 I} (3.19)\ncharacterize the geometry of the surface. If they are all of one sign, the surface is a boundary of a convex set. Otherwise, it\u2019s a saddle surface in the sense that some pieces are curved towards the central piece and others away from it.\nIntersection Values. Since 0 /\u2208 P\u0302 by assumption it follows that x0 /\u2208 P and hence the hyperplane P is completely determined by its intersections with the lines Li = {x0+ ta\u2217i : t \u2208 R}. Assuming general position of P (not parallel with any of the lines Li) there, for each i \u2208 I, is a parameter ti \u0338= 0 specifying where the line Li intersects the hyperplane P such that\nx0 + tia \u2217 i \u2208 P (3.20)\nHere we require at least one ti > 0, because if all ti < 0 then P does not intersect S(I,\u2205) at all and hence \u0393 = \u2205, which breaks our assumption that \u0393 is a (d \u2212 1)-dimensional surface. Inserting the intersection points (3.20) in the hyperplane equation (3.16) gives\n0 = aL \u00b7 (x0 + tia\u2217i ) + bL = ti(aL \u00b7 a\u2217i ) + aL \u00b7 x0 + bL, i \u2208 I (3.21)\nlayer T . (a) A hyperplane P\u0302 intersecting the non-negative orthant S\u0302(I,\u2205) = Rd+. The set P\u0302 \u2229 (Rd \\ Rd+) will have an empty preimage. (b) The central piece P\u0302 \u2229 S\u0302(I,\u2205) is transformed by the inverse of the affine map to P \u2229 S(I,\u2205). Each non-empty intersection of P with the sets in \u2202S will generate a linear piece spanned by a subset of the dual vectors. The union of all these linear pieces defines \u0393.\nBy the assumption that b\u0302L < 0 and the relations (3.17) it follows that aL \u00b7 x0 + bL < 0, which in combination with (3.21) gives the inequality\nti(a L \u00b7 a\u2217i ) > 0 (3.22)\nand hence, we conclude that the sign of ti is the same as the sign of a L \u00b7 a\u2217i . We also see that\naL \u00b7 a\u2217i = AT a\u0302L \u00b7 a\u2217i = a\u0302L \u00b7 Aa\u2217i = a\u0302L \u00b7 ei = a\u0302Li (3.23)\nwhere a\u0302Li is the i:th component of the vector a\u0302 L. Note that a\u0302L is an actual training parameter in L, whose components\u2019 signs directly determine how \u0393 curves relative to the central piece and the signs of the values ti. The values of ti can also be calculated from the parameters in L through the equation\nti = \u2212b\u0302L\na\u0302Li (3.24)\nIntersections. In terms of these values we will now describe the intersections P \u2229S(J,\u2205) used in (3.15), the expression for the linear pieces of \u0393. Let J \u2286 I and recall from (2.12) that x \u2208 S(J,\u2205) has the expansion x = x0 + \u2211 j\u2208J \u03b1ja \u2217 j with coefficients \u03b1j > 0. The\nintersection P \u2229 S(J,\u2205) is the set of points x \u2208 S(J,\u2205) satisfying\n0 = aL \u00b7 x+ bL (3.25) = aL \u00b7 ( x0 + \u2211 i\u2208J \u03b1ia \u2217 i ) + bL (3.26)\n= aL \u00b7 x0 + \u2211 j\u2208J \u03b1ja L \u00b7 a\u2217j + bL (3.27)\n= (aL \u00b7 x0 + bL) ( 1\u2212 \u2211 j\u2208J \u03b1j tj ) (3.28)\nwhere we in the last equality used the identity aL \u00b7 a\u2217j = \u2212a L\u00b7x0+bL\ntj deduced from (3.21).\nSince x0 /\u2208 P \u21d4 (aL \u00b7x0+ bL) \u0338= 0, the second parenthesis in (3.28) must be zero, yielding the additional condition \u2211\nj\u2208J\n\u03b1j tj = 1 (3.29)\non the coefficients \u03b1j > 0. This means that the intersection P \u2229 S(J,\u2205) can be expressed\nP \u2229 S(J,\u2205) = { x \u2208 Rd : x = x0 + \u2211 j\u2208J \u03b1ja \u2217 j , \u03b1j > 0, \u2211 j\u2208J \u03b1j tj = 1 } (3.30)\nIntroducing the index set J\u2212 = {j \u2208 J : tj < 0}, we note that in case J\u2212 = J the condition (3.29) cannot be fulfilled, and hence the intersection P \u2229 S(J,\u2205) = \u2205. At the other extreme, when J\u2212 = \u2205, all terms in (3.29) are strictly positive and we can deduce that \u03b1j < tj, which implies that the intersection is non-empty and bounded. In the remaining case where \u2205 \u2282 J\u2212 \u2282 J , we have both positive and negative terms in (3.29), which implies that the intersection is non-empty and unbounded. Due to the additional condition (3.29), all non-empty intersections will be sets of dimension |J | \u2212 1.\nCombining (3.30) with (3.15) gives the following expression for the preimages we are interested in. For all J \u2286 I such that J\u2212 \u0338= J we get non-empty preimages\nT\u22121 ( P\u0302 \u2229 S\u0302(J,\u2205) ) = (3.31){\nx \u2208 Rd : x = x0 + \u2211 j\u2208J \u03b1ja \u2217 j \u2212 \u2211 i\u2208I\\J \u03bbia \u2217 i , \u03bbi \u2265 0, \u03b1j > 0, \u2211 j\u2208J \u03b1j tj = 1\n}\nand if J\u2212 = J the preimage is empty. Using these descriptions we can now calculate the number of linear pieces of \u0393.\nTheorem 3.1 (Number of Linear Pieces). The number of linear pieces of the decision boundary \u0393 = {x \u2208 Rd : L \u25e6 T (x) = 0} of a fully-connected ReLU network F : Rd \u2192 R with one layer is\n2d \u2212 2m (3.32)\nwhere m = |{i \u2208 I : ti < 0}|.\nProof. By (3.15) we see that each non-empty intersection P \u2229 S(J,\u2205) will generate a unique linear piece of \u0393. Hence, the number of non-empty intersections will determine the number of linear pieces of \u0393. We have showed that the only empty intersections are those P \u2229 S(J,\u2205) where J \u2286 {i \u2208 I : ti < 0}. The number of such subsets is exactly 2m where m = |{i \u2208 I : ti < 0}| and since |I| = d we conclude that the total number of linear pieces is exactly 2d \u2212 2m. \u25a0\nTheorem 3.1 holds as long as A has full rank and the hyperplane P is in general position as defined before. Thus, the number of linear pieces is directly determined by m, i.e., the number of negative ti, i \u2208 I. Since ti = \u2212b\u0302 L\na\u0302Li the number of linear pieces can\nbe directly computed from the training parameters in the affine function L. However, we saw earlier that the signs of the values ti also control how \u0393 curves relative to the central piece P \u2229 S(I,\u2205). Thus, maximizing the number of linear pieces will restrict the potential complexity of the geometry of \u0393. For example, when m = 0 (i.e, all ti are positive), we will maximize the number of linear pieces of \u0393 to 2d \u2212 1 but then all the dot products {a\u2217i \u00b7 n}i\u2208I are positive (since the sign of a\u2217i \u00b7 n is the same as the sign of ti according to (3.21)), so \u0393 will be a convex hypersurface. This shows that there is a trade-off between the complexity in terms of the number of linear pieces and the complexity in terms of the curvature of the decision boundary.\nDefinition 3.2 (Canonical Decision Boundaries). Let F\u0302 be a shallow network (3.8)\nwhere the parameters in the ReLU layer (2.1), which we denote T\u0302 , are A = Id and b = 0. In this special case a\u2217i = ei, x0 = 0, S = S\u0302, and any hyperplane P = P\u0302 . By (3.20), there for each hyperplane in general position are values {t\u2032i \u2208 R : i \u2208 I} such that the points t\u2032iei \u2208 Rd define the intersection between the hyperplane and the affine sector. For each m \u2208 {0, 1, . . . , d \u2212 1}, we let Pm = P\u0302m be the unique hyperplane yielding intersection values\nt\u2032i = { \u22121 if i \u2264 m 1 if i > m\n(3.33)\nNote that we include a prime in the notation for variables defining canonical decision boundaries to simplify later comparisons to arbitrary decision boundaries. Assuming the final affine transformation L in our network F\u0302 is such that ker(L) = P\u0302m, the decision boundary induced by F\u0302 (x) = 0 is\n\u0393\u0302m = \u22c3 J\u2286I T\u0302\u22121 ( P\u0302m \u2229 S\u0302(J,\u2205) ) (3.34)\nwhere we by (3.31) can express the preimage of each intersection T\u0302\u22121 ( P\u0302m \u2229 S\u0302(J,\u2205) ) = (3.35){\nx \u2208 Rd : x = \u2211 j\u2208J \u03b1\u2032jej \u2212 \u2211 i\u2208I\\J \u03bb\u2032iei, \u03bb \u2032 i \u2265 0, \u03b1\u2032j > 0, \u2211 j\u2208J \u03b1\u2032j t\u2032j = 1\n}\nWe call { \u0393\u0302m : m = 0, . . . , d\u2212 1 } the set of canonical decision boundaries.\nBy definition, there are d canonical decision boundaries, which for the case d = 3 are illustrated in Figure 11. As can be seen from (3.33) the integer m conforms with the prior\ndefinition m = |{i \u2208 I : ti < 0}|, and thus we can conclude that \u0393\u0302m consists of 2d \u2212 2m linear pieces according to Theorem 3.1. We will next show that every decision boundary induced by a shallow network (3.8) is equivalent to one canonical decision boundary in the following sense.\nDefinition 3.3 (Equivalence of Decision Boundaries). Two decision boundaries \u0393, \u0393\u2032 \u2282 Rd are equivalent if there exists an invertible affine map M : Rd \u2192 Rd such that M(\u0393\u2032) = \u0393.\nTheorem 3.2. A fully-connected ReLU network F : Rd \u2192 R with one hidden layer can only generate d non-equivalent decision boundaries.\nProof. As above we consider the generic setting when the network F generates a decision boundary \u0393, whose intersection values {ti \u2208 R : i \u2208 I} in (3.20) are such that ti \u0338= 0 and m = |{i \u2208 I : ti < 0}| < d. We will now show that this \u0393 is equivalent to the canonical decision boundary \u0393\u0302m of Definition 3.2 by constructing an invertible affine transformation M that maps \u0393\u0302m onto \u0393. Consider the map M : Rd \u2192 Rd given by\nM(x) = x0 + A \u22121DQ\u03c3x (3.36)\nwhere x0 = \u2212A\u22121b is the projection cone apex, D is a diagonal matrix with elements Dii = |ti|, and Q\u03c3 is a permutation matrix. The action of the linear part of this map on a basis vector ei by (2.9) is\nA\u22121DQ\u03c3ei = A \u22121|t\u03c3(i)|e\u03c3(i) = |t\u03c3(i)|a\u2217\u03c3(i) (3.37)\nThe permutation matrix Q\u03c3 is constructed such that it realizes a permutation Q\u03c3ei = e\u03c3(i) where the bijective map \u03c3 : I 7\u2192 I has the effect of sorting the intersection values of \u0393, i.e., that t\u03c3(i) \u2264 t\u03c3(i+1) for i = 1, . . . , d\u22121. By this construction the signs of the permuted intersection values correspond to those of the canonical decision boundary such that sign(t\u03c3(j)) = sign(t \u2032 j) and since t \u2032 j \u2208 {\u22121, 1} we have the relation\nt\u2032j|t\u03c3(j)| = t\u03c3(j) (3.38)\nWe will now show that the action of (3.36) is such that M(\u0393\u0302m) = \u22c3 J\u2286I M ( T\u0302\u22121 ( P\u0302m \u2229 S\u0302(J,\u2205) )) = \u22c3 K\u2286I T\u0302\u22121 ( P\u0302 \u2229 S\u0302(K,\u2205) ) = \u0393 (3.39)\nand, hence, that \u0393 and \u0393\u0302m are equivalent according to Definition 3.3. For some J \u2286 I, consider the expression of the intersection preimage T\u0302\u22121 ( P\u0302m \u2229 S\u0302(J,\u2205) ) given in (3.35). Letting M act on the coordinate expansion in (3.35) and using (3.37) give\nM(x) = x0 + A \u22121DQ\u03c3 (\u2211 j\u2208J \u03b1\u2032jej \u2212 \u2211 i\u2208I\\J \u03bb\u2032iei ) (3.40)\n= x0 + \u2211 j\u2208J \u03b1\u03c3(j)|t\u03c3(j)|a\u2217\u03c3(j) \u2212 \u2211 i\u2208I\\J \u03bb\u03c3(i)|t\u03c3(i)|a\u2217\u03c3(i) (3.41)\n= x0 + \u2211\nj\u2208\u03c3(J)\n\u03b1j|tj|a\u2217j \u2212 \u2211\ni\u2208I\\\u03c3(J)\n\u03bbi|ti|a\u2217i (3.42)\nwhere we defined \u03b1\u03c3(j) = \u03b1 \u2032 j|t\u03c3(j)|, \u03bb\u03c3(i) = \u03bb\u2032i|t\u03c3(i)| and \u03c3(J) = {\u03c3(j) : j \u2208 J}. In terms of these definitions, we can also reformulate the constraints in (3.35) as\n\u03bb\u2032i \u2265 0 \u21d4 \u03bb\u03c3(i) \u2265 0, \u03b1\u2032j > 0 \u21d4 \u03b1\u03c3(j) > 0, \u2211 j\u2208J \u03b1\u2032j t\u2032j = 1 \u21d4 \u2211 j\u2208\u03c3(J) \u03b1j tj = 1 (3.43)\nThe first two equivalences hold since |t\u03c3(i)| > 0 and the last equivalence follows from the calculation \u2211\nj\u2208J\n\u03b1\u2032j t\u2032j = \u2211 j\u2208J \u03b1\u03c3(j) t\u2032j|t\u03c3(j)| = \u2211 j\u2208J \u03b1\u03c3(j) t\u03c3(j) = \u2211 j\u2208\u03c3(J) \u03b1j tj\n(3.44)\nwhere we in the second equality used (3.38). Comparing (3.40)\u2013(3.44) to the expression for a general intersection preimage (3.31) we realize that\nM ( T\u0302\u22121 ( P\u0302m \u2229 S\u0302(J,\u2205) )) = T\u0302\u22121 ( P\u0302 \u2229 S\u0302(\u03c3(J),\u2205) ) (3.45)\nSince \u03c3 is a bijection we have \u03c3(J) \u0338= \u03c3(K) for J,K \u2286 I with J \u0338= K and for every K \u2286 I there is a J \u2286 I such that \u03c3(J) = K. Hence, we conclude that (3.39) holds.\nWe end this proof by noting that the d canonical decision boundaries are non-equivalent. Since an invertible affine map acting on a decision boundary cannot change the number of linear pieces, which by Theorem 3.1 is different for each m, any pair of canonical decision boundaries \u0393\u0302m1 , \u0393\u0302m2 with m1 \u0338= m2 cannot be equivalent. This gives us that for a fullyconnected ReLU network with one hidden layer, there are only d different non-equivalent decision boundaries \u0393 (one for each value of m). \u25a0\nThis result implies that if \u0393 \u2286 Rd is a decision boundary separating two classes of points X1 and X2, then there is an affine map M : Rd \u2192 Rd such that the transformed sets M(X1) and M(X2) are separated by \u0393m for some 0 \u2264 m < d. In this sense, it is enough to analyze the properties of the canonical decision boundaries since every other possible decision boundary can be obtained through an affine transformation."
        },
        {
            "heading": "3.3 Decision Boundaries for Deep Networks",
            "text": "When appending additional ReLU layers to the network any precise complexity estimates for decision boundary, such as those provided for shallow networks in Theorem 3.1 and Theorem 3.2, are difficult to show. Instead, we here provide some more general commentary on the effect of network depth.\nConsider a network on the form (3.1) with N hidden ReLU layers and width d. Define \u0393(k) = { x \u2208 Rd : L \u25e6 T (N) \u25e6 . . . \u25e6 T (k)(x) = 0 } , where 1 \u2264 k \u2264 N (3.46)\nWe can interpret \u0393(k) as the decision boundary of the network when we have removed the first k \u2212 1 layers, so the actual decision boundary is simply \u0393 = \u0393(1). Removing all but one hidden layer gives a shallow network, which means \u0393(N) is characterized in the previous section. Moreover, we have the recursive relation\n\u0393(k) = T\u2212(k) ( \u0393(k+1) \u2229 S\u0302(I,\u2205) ) (3.47)\nwhere T\u2212(k)(\u03c9\u0302) denotes the preimage of \u03c9\u0302 under the ReLU layer T (k). Clearly, only the\nparts of \u0393(k+1) intersecting S\u0302(I,\u2205) will contribute to \u0393 (k). Parts of \u0393(k+1) entirely inside S\u0302(I,\u2205) will be obtained through an affine transformation so any additional complexity of the hypersurface \u0393(k) is due to the intersections of \u0393(k+1) with the boundary \u2202S\u0302(I,\u2205). Unlike a shallow network, where every non-empty intersection of ker(L) with a boundary part S\u0302(J,\u2205) \u2208 \u2202S\u0302 generates one addidional linear piece, the intersection of \u0393(k+1) with S\u0302(J,\u2205) can yield more than one additional linear piece in \u0393 (k). The reason is that the intersection \u0393(k+1) \u2229 S\u0302(J,\u2205) is typically piecewise linear where each linear piece will generate an additional new linear piece in \u0393k. However, all those linear pieces emerging from the intersection with the same boundary part S\u0302(J,\u2205) will not be independent since they are spanned by the same set of dual basis vectors.\nIn effect, the number of linear pieces of the decision boundary \u0393 can be very large in a network with multiple layers, but they are not entirely independent of each other. Even though one layer can induce more than d linear pieces, all of them are spanned by subsets of only d new vectors, namely the dual basis induced by the parameters of T (k), see Definition 2.1. The overall complexity of \u0393 emerges from these dual bases which will span the additional linear pieces from each layer, together with the increasing complexity\nof the intersections of \u0393(k) with S\u0302(I,\u2205) as we successively proceed through the layers."
        },
        {
            "heading": "4 Conclusion",
            "text": "We have provided a detailed geometric description of the structure of a fully-connected ReLU layer by introducing a partition of the input space using a dual basis derived from the layer parameters. With this framework, we can describe the action of such a layer as\na projection of Rd onto a polyhedral cone followed by an invertible affine transformation mapping the cone onto the non-negative orthant Rd+. Most of the sectors in our partition will be projected onto lower-dimensional parts of the boundary of the cone, and hence, it is apparent that networks with multiple ReLU layers can contract data efficiently. However, in a classification setting the cones associated with each layer must be constructed such that the sets of points from different classes are not mixed.\nWith this geometrical description, we can compute preimages of sets in terms of their intersections with Rd+ and the dual vectors. The decision boundary for a network with a single hidden layer can be expressed as the preimage of a hyperplane. This allowed us to characterize the complexity of the decision boundary in terms of the number of linear pieces and the geometry as their relative orientation. We can conclude that there is a trade-off between the number of linear pieces and the dependency between them. In particular, maximizing the number of linear pieces will always result in a convex decision boundary.\nWe have also classified the possible decision boundaries for a fully-connected ReLU network with a single hidden layer by posing mild conditions on the parameters. All such decision boundaries can be mapped by an affine transformation to one of the d canonical decision boundaries we described in detail. Lastly, we briefly discussed the effect of adding more layers to the network. Specifically, the number of linear pieces of a decision boundary of a network can grow very fast with the number of layers, but the majority of these pieces are highly dependent.\nIn an upcoming paper, we provide a geometric approximation theory for deep ReLU networks relevant to the binary classification setting. More precisely, we show that a sufficiently regular hypersurface can be approximated by the decision boundary of a deep ReLU network to any desired accuracy. That construction will heavily depend on the geometric description of ReLU layers provided in this paper.\nAcknowledgement. This research was supported in part by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation; the Swedish Research Council Grants Nos. 2017-03911, 2021-04925; and the Swedish Research Programme Essence."
        }
    ],
    "title": "The Geometric Structure of Fully-Connected ReLU Layers",
    "year": 2023
}