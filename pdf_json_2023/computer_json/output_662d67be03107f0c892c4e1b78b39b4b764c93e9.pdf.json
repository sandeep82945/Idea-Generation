{
    "abstractText": "Denoising diffusion models have recently achieved state-of-the-art performance in many image-generation tasks. They do, however, require a large amount of computational resources. This limits their application to medical tasks, where we often deal with large 3D volumes, like high-resolution three-dimensional data. In this work, we present a number of different ways to reduce the resource consumption for 3D diffusion models and apply them to a dataset of 3D images. The main contribution of this paper is the memory-efficient patch-based diffusion model PatchDDM, which can be applied to the total volume during inference while the training is performed only on patches. While the proposed diffusion model can be applied to any image generation tasks, we evaluate the method on the tumor segmentation task of the BraTS2020 dataset and demonstrate that we can generate meaningful three-dimensional segmentations.",
    "authors": [
        {
            "affiliations": [],
            "name": "Florentin Bieder"
        },
        {
            "affiliations": [],
            "name": "Julia Wolleb"
        },
        {
            "affiliations": [],
            "name": "Alicia Durrer"
        },
        {
            "affiliations": [],
            "name": "Robin Sandk\u00fchler"
        },
        {
            "affiliations": [],
            "name": "Philippe C. Cattin"
        }
    ],
    "id": "SP:584b6824c01ae71e948749825044734f2650b3e0",
    "references": [
        {
            "authors": [
                "Tomer Amit",
                "Eliya Nachmani",
                "Tal Shaharbany",
                "Lior Wolf"
            ],
            "title": "Segdiff: Image segmentation with diffusion probabilistic models",
            "venue": "arXiv preprint arXiv:2112.00390,",
            "year": 2021
        },
        {
            "authors": [
                "Spyridon Bakas",
                "Hamed Akbari",
                "Aristeidis Sotiras",
                "Michel Bilello",
                "Martin Rozycki",
                "Justin S Kirby",
                "John B Freymann",
                "Keyvan Farahani",
                "Christos Davatzikos"
            ],
            "title": "Advancing the cancer genome atlas glioma MRI collections with expert segmentation labels and radiomic features",
            "venue": "Scientific data,",
            "year": 2017
        },
        {
            "authors": [
                "Spyridon Bakas",
                "Mauricio Reyes",
                "Andras Jakab",
                "Stefan Bauer",
                "Markus Rempfler",
                "Alessandro Crimi",
                "Russell Takeshi Shinohara",
                "Christoph Berger",
                "Sung Min Ha",
                "Martin Rozycki"
            ],
            "title": "Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the BRATS challenge",
            "venue": "arXiv preprint arXiv:1811.02629,",
            "year": 2018
        },
        {
            "authors": [
                "Arpit Bansal",
                "Eitan Borgnia",
                "Hong-Min Chu",
                "Jie S. Li",
                "Hamid Kazemi",
                "Furong Huang",
                "Micah Goldblum",
                "Jonas Geiping",
                "Tom Goldstein"
            ],
            "title": "Cold Diffusion: Inverting Arbitrary Image Transforms",
            "venue": "Without Noise. arXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Zolnamar Dorjsembe",
                "Sodtavilan Odonchimed",
                "Furen Xiao"
            ],
            "title": "Three-Dimensional Medical Image Synthesis with Denoising Diffusion Probabilistic Models",
            "venue": "In Medical Imaging with Deep Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Xutao Guo",
                "Yanwu Yang",
                "Chenfei Ye",
                "Shang Lu",
                "Yang Xiang",
                "Ting Ma"
            ],
            "title": "Accelerating Diffusion Models via Pre-segmentation Diffusion Sampling for Medical Image Segmentation",
            "venue": "arXiv preprint arXiv:2210.17408,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep Residual Learning for Image Recognition",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising Diffusion Probabilistic Models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Fabian Isensee",
                "Paul F Jaeger",
                "Simon AA Kohl",
                "Jens Petersen",
                "Klaus H Maier-Hein"
            ],
            "title": "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation",
            "venue": "Nature methods,",
            "year": 2021
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Timo Aila",
                "Samuli Laine"
            ],
            "title": "Elucidating the Design Space of Diffusion-Based Generative Models",
            "venue": "arXiv preprint arXiv:2206.00364,",
            "year": 2022
        },
        {
            "authors": [
                "Amirhossein Kazerouni",
                "Ehsan Khodapanah Aghdam",
                "Moein Heidari",
                "Reza Azad",
                "Mohsen Fayyaz",
                "Ilker Hacihaliloglu",
                "Dorit Merhof"
            ],
            "title": "Diffusion models for medical image analysis: A comprehensive survey",
            "venue": "arXiv preprint arXiv:2211.07804,",
            "year": 2022
        },
        {
            "authors": [
                "Curran Associates",
                "Inc",
                "2018. Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "year": 2018
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily Denton",
                "Seyed Kamyar Seyed Ghasemipour",
                "Burcu Karagol Ayan",
                "S Sara Mahdavi",
                "Rapha Gontijo Lopes"
            ],
            "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
            "venue": "arXiv preprint arXiv:2205.11487,",
            "year": 2022
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising Diffusion Implicit Models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Julia Wolleb",
                "Florentin Bieder",
                "Robin Sandk\u00fchler",
                "Philippe C Cattin"
            ],
            "title": "Diffusion Models for Medical Anomaly Detection",
            "venue": "arXiv preprint arXiv:2203.04306,",
            "year": 2022
        },
        {
            "authors": [
                "Julia Wolleb",
                "Robin Sandkuehler",
                "Florentin Bieder",
                "Philippe Valmaggia",
                "Philippe C. Cattin"
            ],
            "title": "Diffusion Models for Implicit Image Segmentation Ensembles",
            "venue": "Proceedings of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Junde Wu",
                "Huihui Fang",
                "Yu Zhang",
                "Yehui Yang",
                "Yanwu Xu"
            ],
            "title": "MedSegDiff: Medical Image Segmentation with Diffusion Probabilistic Model",
            "venue": "arXiv preprint arXiv:2211.00611,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Keywords: diffusion models, three-dimensional, supervised segmentation"
        },
        {
            "heading": "1. Introduction",
            "text": "Denoising diffusion models (Ho et al., 2020; Nichol and Dhariwal, 2021) have lately shown an impressive performance in image generation and experienced increasing popularity in medical image analysis (Kazerouni et al., 2022). However, the processing of large threedimensional (3D) volumes, which often is required in medical applications, is still a challenge. Limitations related to the computational resources only allow the processing of small 3D volumes, which impedes the processing of high-resolution magnetic resonance (MR) or computer tomography (CT) scans.\nContribution In this work, we introduce architectural changes to the state-of-the-art diffusion model implementation (Nichol and Dhariwal, 2021), enabling to train on large 3D volumes with commonly available GPUs. We adapt the U-Net-like architecture to improve the speed and memory efficiency. Furthermore, we propose a novel method illustrated in Figure 1. With this method, the diffusion model is trained only on coordinate-encoded patches of the input volume, which reduces the memory consumption and speeds up the training process. During sampling, the proposed method allows processing of large volumes in their full resolution without needing to split them up into patches. To evaluate our method, we perform diffusion model based image segmentation (Wolleb et al., 2022b) that has previously been proposed for 2D segmentation on the BraTS2020 dataset (Menze et al., 2014; Bakas et al., 2017, 2018).\n\u00a9 2023 F. Bieder, J. Wolleb, A. Durrer, R. Sandku\u0308hler & P.C. Cattin.\nar X\niv :2\n30 3.\n15 28\n8v 1\n[ cs\n.C V\n] 2\n7 M\nar 2\nRelated Work Denoising diffusion models have seen a quick adoption in research, replacing the more traditional generative models in many tasks such as such as unconditional and conditional image generation (Ho et al., 2020; Song et al., 2021; Nichol and Dhariwal, 2021), text-to-image translation (Nichol et al., 2021; Saharia et al., 2022b; Ramesh et al., 2021; Kim et al., 2022) and inpainting (Ramesh et al., 2021; Nichol et al., 2021). Diffusion models have also been used for various applications in the medical field, for instance, for anomaly detection (Wolleb et al., 2022a), synthetic image generation (Dorjsembe et al., 2022; Peng et al., 2022) and segmentation (Guo et al., 2022; Wu et al., 2022; Wolleb et al., 2022b). Medical images, however, often are 3D volumes, such as MR- or CT-scans. These volumes create challenges regarding memory consumption of processing methods. Consequently, many of the current methods are limited to two-dimensional (2D) slices only (Wu et al., 2022; Wolleb et al., 2022b; Guo et al., 2022) or to 3D volumes restricted to a limited resolution of at most 128\u00d7128\u00d7128 (Khader et al., 2022; Peng et al., 2022; Dorjsembe et al., 2022). To the best of our knowledge, we are the first to tackle the challenge of applying denoising diffusion models to large 3D volumes."
        },
        {
            "heading": "2. Method",
            "text": "We explore how denoising diffusion implicit models (DDIMs) presented in Section 2.1 can be improved regarding memory efficiency and time consumption. The required architectural changes are presented in Section 2.2. We present the training and sampling scheme of our method PatchDDM in Section 2.3. For evaluation, we use the segmentation approach using denoising diffusion models presented in Section 2.4."
        },
        {
            "heading": "2.1. Denoising Diffusion Models",
            "text": "In the following, we will use the notation introduced by (Ho et al., 2020). Denoising diffusion models rely on an iterative noising and denoising process. The forward noising process q is\ngiven by q(xt | xt\u22121) = N (xt; \u221a 1\u2212 \u03b2txt\u22121, \u03b2tI) (1)\nwhere \u03b2t is a predefined sequence of variances. We can directly compute xt from a given x0 with\nq(xt | x0) := N (xt; \u221a \u03b1tx0, (1\u2212 \u03b1t)I) (2)\nwith \u03b1t := 1 \u2212 \u03b2t and \u03b1t := \u220ft\ns=1 \u03b1s. This corresponds to degrading the input image by adding Gaussian noise. For image generation tasks we are interested in the reverse process p\u03d1.\np\u03d1,t(xt\u22121 | xt) = N (xt\u22121;\u00b5\u03d1,t(xt),\u03a3\u03d1,t(xt)) (3) Both \u00b5\u03d1,t and \u03a3\u03d1,t can be estimated by a U-Net-based network \u03b5\u03d1,t with parameters \u03d1. The loss used to train the network \u03b5\u03d1,t can be written as\n\u2016\u03b5\u2212 \u03b5\u03d1,t(xt, t)\u20162 = \u2016\u03b5\u2212 \u03b5\u03d1,t( \u221a \u03b1tx0 + \u221a 1\u2212 \u03b1t\u03b5, t)\u20162, with \u03b5 \u223c N (0, I). (4)\nUsing the DDIM (Song et al., 2021) sampling scheme, we can define\nxt\u22121 = \u221a \u03b1t\u22121\n( xt \u2212 \u221a 1\u2212 \u03b1t\u03b5\u03d1,t(xt)\u221a\n\u03b1t\n) + \u221a 1\u2212 \u03b1t\u22121\u03b5\u03d1,t(xt), (5)\nwhere \u03b5\u03d1(xt) is the output of the network. This sampling scheme has the advantage that the denoising process is deterministic and we do not need to sample random vectors in every step. Thus, the only source of stochasticity during inference originates from the random initial sample xT which is sampled from N (0, I). During inference, a sequence of images xi for i = T, T \u2212 1, . . . , 0 of decreasing noise level is being generated, the initial xT is sampled from a standard normal distribution N (0, I)."
        },
        {
            "heading": "2.2. Architecture",
            "text": "We adapt the 2D-U-Net-based network architecture proposed by (Ho et al., 2020; Nichol and Dhariwal, 2021) and used by (Nichol et al., 2021; Bansal et al., 2022; Wu et al., 2022; Song et al., 2021) for the application on 3D data. The previously proposed architecture features two or three residual convolutional blocks at each down- and upsampling step. Furthermore, it uses attention blocks at multiple resolutions as well as in the bottleneck. (Saharia et al., 2022a) determined that adding global self attention can slightly improve the quality of the generated images as compared to an increase in convolutional blocks. For 3D data, the attention blocks use disproportionally more memory, which made it infeasible to use on current hardware, which is why we removed them completely. The second fundamental change we implemented was the use of additive skip connections, as shown in Figure 2. In the previous architecture as well as in the original U-Net implementation (Ronneberger et al., 2015), the skip connection uses concatenation to combine xs from the encoder with the upsampled tensors xu from the lower resolution path of the decoder. This implies that the decoder requires significantly more resources than the encoder, especially at the highest resolution levels. To alleviate this issue, we propose to average them as x = 12(xs + xu). Unlike in ResNet (He et al., 2016), where the skip connections are added, we found the\naveraging to be crucial for avoiding numerical issues like exploding gradients. Intuitively this can be justified by considering xs and xu as random variables with xu, xs iid. N (0, \u03c32). Therefore, xu+xs \u223c N (0, 2\u03c32), that is, with each concatenation the variance doubles, while averaging preserves the variance.\nThe savings in memory from replacing the concatenation with the averaging allow us to increase the network width, i.e. the number of channels within the whole network, by a factor of 1.61, while preserving the total memory usage. Furthermore, the resulting network architecture allows for training on varying input sizes. This property is crucial or our proposed patch-based method. For all of our experiments, we use the same network configuration."
        },
        {
            "heading": "2.3. Patch-based Approach with Coordinate-encoding",
            "text": "To benefit from the lower requirements of computational resources but still to operate on the original resolution, we propose a novel patch-based training method named PatchDDM that trains on randomly sampled patches of the input but can afterwards be applied to the full resolution volume during inference. This means for the training we can benefit from using smaller inputs, which means we need less computations per iteration as well as less memory. For the inference, however, we can pass the entire input volume at once without having to sample patches and reassemble them. This means no boundary artifacts do to the separate padding of the patches within the CNN are introduces, and also the stitching artifacts that that can appear in traditional patch-based approaches are eliminated.\nTo add information about the position of the patch, we condition the network on the position of the sampled patch. We implemented this by concatenating a grid of Cartesian coordinates to the input. Each coordinate is represented by one channel as a linear gradient ranging from -1 to 1. This is similar to the method proposed in (Liu et al., 2018). They propose to add the coordinates as additional channels for before all convolutions.\nIn our case, we append the coordinates to the whole input just like in (Liu et al., 2018), but then sample a patch, where the coordinates serve as a position encoding for the sampled patch. An overview of this coordinate encoding is given in Figure 1. For the BraTS2020 data, the subject is centered within the volume. We use a patch sampling strategy, assigning a higher probability to the center of the volume, as shown in Figure 6 in the Appendix A.\nBaseline methods For our ablation study, we use two baselines with the same network as our proposed approach, but without patch-based training. Furthermore we also performed an experiment with our patch based approach but without the proposed coordinate encoding. The training did not converge and did not produce any usable result. Therefore, we will not report any metrics from this experiment. The two baseline methods are the following:\n\u2022 Training on full resolution (FullRes): We implemented a distributed version of the proposed architecture that splits the task to two GPUs if necessary. This allows for training directly on full resolution (2563) data, given that the expensive specialized GPU hardware is available.\n\u2022 Training on half resolution (HalfRes): A straightforward way to reduce the requirements in terms of computational resources is training the model on downsampled data. In our experiments, we downsampled the input image before passing it to the network, but then upsampled the output of the network again to evaluate the performance on the full size. For three spatial dimensions (i.e. 3D) this means that reducing the input size from 2563 to 1283 results in a reduction of a factor of 8 in terms of memory and computation time, allowing this model to be run on widely available GPUs."
        },
        {
            "heading": "2.4. Denoising Diffusion Models with Ensembling for Segmentation",
            "text": "In order to generate the segmentation of an input image b, we need to condition the generation of the segmentation mask x0 on that given image b. We will follow the method proposed by (Wolleb et al., 2022b), where the input images b are being concatenated to every xt as a condition. It was shown that ensembling several predicted segmentation masks per input image increases the segmentation performance (Amit et al., 2021; Wolleb et al., 2022b). An overview of this segmentation approach is given in Figure 3. An advantage of the denoising diffusion based segmentation approach is the implicit ensembling we get when using different samples xT from the noise distribution N (0, I), which can be used to increase the performance and estimate the uncertainty. To evaluate the performance of our proposed method PatchDDM described in Section 2.3 and the two baseline methods FullRes and HalfRes, we apply our method to a segmentation task as proposed in (Wolleb et al., 2022b). Therefore, we train our diffusion model to generate semantic segmentation masks."
        },
        {
            "heading": "3. Experiments",
            "text": "Dataset For our experiments, we used the BraTS2020 dataset (Menze et al., 2014; Bakas et al., 2017, 2018). It contains 369 head MR-scans, each including four sequences (T1, T1ce, T2, FLAIR) with a resolution of 1 \u00d7 1 \u00d7 1 mm3, resulting in a total scan size of 240\u00d7 240\u00d7 155, which we padded to a size of 256\u00d7 256\u00d7 256. The background voxels were set to zero and the range between the first and 99th percentile was normalized to [0, 1]. We used an 80%/10%/10% split for training, validation and testing. The label masks consist of three classes, namely the Gadolinium-enhancing tumor, the peritumoral edema, and the necrotic and non-enhancing tumor core. For the binary segmentation experiments, all three classes were merged into one.\nTraining Details We performed our experiments on NVIDIA A100 GPUs with 40GB of memory each. To directly train on the full resolution 2563 images, we distribute the model over 2 GPUs. The methods HalfRes and PatchDDM were trained on one GPU only. The optimizer we used was AdamW (Loshchilov and Hutter, 2017) with the default parameters. We chose the learning rate lr = 10\u22125 by optimizing the average Dice coefficient on the validation set after 150k optimization steps over a range of values between 10\u22126 and 10\u22123. We trained the models for the same amount of time for all experiments (420h). For the evaluation, we selected the best-performing models based on the average Dice score on the validation set based on a single evaluation, i.e., without ensembling. For the denoising process, we set the number of steps to T = 1000 and use the affine variance schedule proposed in (Ho et al., 2020) with \u03b21 = 0.02, \u03b2T = 10 \u22124.\nAccelerated Sampling By default we need T = 1000 denoising steps for the inference. As shown in (Song et al., 2021), we can interpret the the DDIM denoising step (5) as the Euler discretization of an ordinary differential equation (ODE). This insight motivates the use of larger step sizes with respect to t during inference, which allows for accelerated sampling. The drawback is that the output quality deteriorates with fewer samples. We investigate how we can trade off fewer sampling steps (larger step sizes) and ensembling (more samples)."
        },
        {
            "heading": "4. Results",
            "text": "In the following, we will assess the performance of our proposed model and compare it to the two baseline approaches. For each model, we computed the average Dice score on the validation set and used this to choose the best-performing checkpoint. We provide some qualitative outputs in Figure 7 in the Appendix B. To assess the training progress, we display the Dice score as well as the HD95 (Hausdorff distance, 95th percentile) of PatchDDM over the course of the training in Figure 8 in the Appendix C. The metrics of best-performing checkpoint with respect to the Dice score when using a single evaluation (no ensembling) is reported in Table 2 in Appendix D along with the score of the state of the art nnU-Net (Isensee et al., 2021)."
        },
        {
            "heading": "4.1. Segmentation Ensembling",
            "text": "To evaluate the impact of ensembling, we compute the Dice- and HD95-score of the three methods (PatchDDM, FullRes, HalfRes) with resepect to ensemble size, see Figure 4. Both scores significantly improve using ensembles for our proposed PatchDDM and the FullRes method. In Table 3 in the Appendix E the metrics for different ensemble sizes are provided. The curves show that ensembling can further improve the performance and get very close to the best performing ensembles with an ensemble size of as small as five to nine."
        },
        {
            "heading": "4.2. Computational Resources & Time Requirements",
            "text": "We report the memory consumption and the time required for one model evaluation for all comparing methods. As displayed in Table 1, the training of FullRes needs close to 80GB of memory. This requires at the time of writing still highly expensive hardware. The other baseline HalfRes as well as our proposed PatchDDM method both need less than 12GB for training and can therefore be trained on much cheaper and widely available hardware. The reduced resolution also results in a reduction in the number of computations, and therefore a larger number of optimization steps that can be performed in a given time interval. A drawback of our proposed method is the increased memory consumption and reduced speed during inference, both of which are comparable to the FullRes model."
        },
        {
            "heading": "4.3. Ensembling and Accelerated Sampling",
            "text": "Figure 5 shows the trade-off between the ensemble size and the number of sampling steps. With as little as 20 sampling steps (i.e. a step size of 50), the performance is already close to the results obtained with T = 1000 steps, implying a speedup of a factor of 50. But even with fewer step sizes, we can trade the number of steps for a greater ensemble size to achieve a similar performance. Consequently, for a fixed budget of network evaluations (i.e. steps), we can profit from using ensembling with accelerated sampling."
        },
        {
            "heading": "5. Discussion",
            "text": "We propose PatchDDM, a novel patch-based diffusion model architecture that allows the training of diffusion models on high-resolution 3D datasets. This enables denoising diffusion models to be used for image analysis and -processing tasks in medicine on commonly available hardware. We could demonstrate the effectiveness by applying it to a recently developed segmentation framework for medical images. In the future, we would like to investigate the performance of our proposed approach for tasks involving image generation. Furthermore, we will investigate the role of the patch size used and whether it can be made smaller for processing even higher resolution volumes. In order to preserve high quality, (Karras et al., 2022) proposed using higher-order ODE solvers, like the Heun, method when choosing larger step sizes. This might further reduce the number of iterations needed. Finally, it would be interesting to investigate an extension of this segmentation framework that includes multiple classes."
        },
        {
            "heading": "Acknowledgments",
            "text": "We are grateful for the support of the Novartis FreeNovation initiative and the Uniscientia Foundation (project #147-2018). We would also like to thank the NVIDIA Corporation for donating a GPU that was used for our experiments."
        },
        {
            "heading": "Appendix B. Qualitative Results",
            "text": ""
        },
        {
            "heading": "Appendix D. Single Evaluation Scores",
            "text": ""
        },
        {
            "heading": "Appendix C. Training Progress",
            "text": ""
        },
        {
            "heading": "Appendix E. Ensembling Scores",
            "text": ""
        }
    ],
    "title": "Diffusion Models for Memory-efficient Processing of 3D Medical Images",
    "year": 2023
}