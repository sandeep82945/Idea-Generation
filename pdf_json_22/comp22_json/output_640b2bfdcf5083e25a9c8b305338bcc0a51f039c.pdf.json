{
    "abstractText": "Widespread applications of deep neural networks (DNNs) benefit from DNN testing to guarantee their quality. In the DNN testing, numerous test cases are fed into the model to explore potential vulnerabilities, but they require expensive manual cost to check the label. Therefore, test case prioritization is proposed to solve the problem of labeling cost, e.g., activation-based and mutation-based prioritization methods. However, most of them suffer from limited scenarios (i.e. high confidence adversarial or false positive cases) and high time complexity. To address these challenges, we propose the concept of the activation graph from the perspective of the spatial relationship of neurons. We observe that the activation graph of cases that triggers the model\u2019s misbehavior significantly differs from that of normal cases. Motivated by it, we design a test case prioritization method based on the activation graph, ActGraph, by extracting the high-order node features of the activation graph for prioritization. ActGraph explains the difference between the test cases to solve the problem of scenario limitation. Without mutation operations, ActGraph is easy to implement, leading to lower time complexity. Extensive experiments on three datasets and four models demonstrate that ActGraph has the following key characteristics. (i) Effectiveness and generalizability : ActGraph shows competitive performance in all of the natural, adversarial and mixed scenarios, especially in RAUC-100 improvement (\u223c \u00d71.40). (ii) Efficiency : ActGraph does not use complex mutation operations and runs in less time",
    "authors": [
        {
            "affiliations": [],
            "name": "Jinyin Chen"
        },
        {
            "affiliations": [],
            "name": "Jie Ge"
        },
        {
            "affiliations": [],
            "name": "Haibin Zheng"
        }
    ],
    "id": "SP:ae24e563ecc4cb76a939ed21c3ed009a67e2534c",
    "references": [
        {
            "authors": [
                "H. Zhang",
                "X. Ma"
            ],
            "title": "Misleading attention and classification: An adversarial attack to fool object detection models in the real world",
            "venue": "Comput. Secur",
            "year": 2022
        },
        {
            "authors": [
                "Y. Duan",
                "X. Zhou",
                "J. Zou",
                "J. Qiu",
                "J. Zhang",
                "Z. Pan"
            ],
            "title": "Mask-guided noise restriction adversarial attacks for image classification",
            "venue": "Comput. Secur",
            "year": 2021
        },
        {
            "authors": [
                "A.A. Alhogail",
                "A. Alsabih"
            ],
            "title": "Applying machine learning and natural language processing to detect phishing",
            "venue": "email. Comput. Secur",
            "year": 2021
        },
        {
            "authors": [
                "K. Kim",
                "J.S. Kim",
                "S. Jeong",
                "J. Park",
                "H.K. Kim"
            ],
            "title": "Cybersecurity for autonomous vehicles: Review of attacks and defense",
            "venue": "Comput. Secur",
            "year": 2021
        },
        {
            "authors": [
                "D.T. Danny Yadron"
            ],
            "title": "Tesla driver dies in first fatal crash while using autopilot mode. https://www.theguardian.com/technology/2016/ jun/30/tesla-autopilot-death-self-driving-car-elon-musk",
            "venue": "Nature",
            "year": 2021
        },
        {
            "authors": [
                "K. Pei",
                "Y. Cao",
                "J. Yang",
                "S. Jana"
            ],
            "title": "Deepxplore: Automated whitebox testing of deep learning systems",
            "venue": "Proceedings of the 26th Symposium on Operating Systems Principles,",
            "year": 2017
        },
        {
            "authors": [
                "J. Guo",
                "Y. Jiang",
                "Y. Zhao",
                "Q. Chen",
                "J. Sun"
            ],
            "title": "Dlfuzz: differential fuzzing testing of deep learning systems",
            "venue": "Proceedings of the 2018 ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,",
            "year": 2018
        },
        {
            "authors": [
                "A. Odena",
                "C. Olsson",
                "D.G. Andersen",
                "I.J. Goodfellow"
            ],
            "title": "Tensorfuzz: Debugging neural networks with coverage-guided fuzzing",
            "venue": "Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Y. Tian",
                "K. Pei",
                "S. Jana",
                "B. Ray"
            ],
            "title": "Deeptest: automated testing of deepneural-network-driven autonomous cars",
            "venue": "Proceedings of the 40th International Conference on Software Engineering,",
            "year": 2018
        },
        {
            "authors": [
                "L. Ma",
                "F. Juefei-Xu",
                "F. Zhang",
                "J. Sun",
                "M. Xue",
                "B. Li",
                "C. Chen",
                "T. Su",
                "L. Li",
                "Y. Liu",
                "J. Zhao",
                "Y. Wang"
            ],
            "title": "Deepgauge: multi-granularity testing criteria for deep learning systems",
            "venue": "Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Feng",
                "Q. Shi",
                "X. Gao",
                "J. Wan",
                "C. Fang",
                "Z. Chen"
            ],
            "title": "Deepgini: prioritizing massive tests to enhance the robustness of deep neural networks",
            "venue": "ISSTA \u201920: 29th ACM SIGSOFT International Symposium on Software Testing and Analysis, Virtual Event,",
            "year": 2020
        },
        {
            "authors": [
                "W. Shen",
                "Y. Li",
                "L. Chen",
                "Y. Han",
                "Y. Zhou",
                "B. Xu"
            ],
            "title": "Multiple-boundary clustering and prioritization to promote neural network retraining",
            "venue": "Nature",
            "year": 2021
        },
        {
            "authors": [
                "J. Kim",
                "R. Feldt",
                "S. Yoo"
            ],
            "title": "Guiding deep learning system testing using surprise adequacy",
            "venue": "Proceedings of the 41st International Conference on Software Engineering,",
            "year": 2019
        },
        {
            "authors": [
                "T. Byun",
                "V. Sharma",
                "A. Vijayakumar",
                "S. Rayadurgam",
                "D.D. Cofer"
            ],
            "title": "Input prioritization for testing neural networks",
            "venue": "IEEE International Conference On Artificial Intelligence Testing, AITest 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Z. Wang",
                "H. You",
                "J. Chen",
                "Y. Zhang",
                "X. Dong",
                "W. Zhang"
            ],
            "title": "Prioritizing test inputs for deep neural networks via mutation analysis",
            "venue": "IEEE/ACM International Conference on Software Engineering, ICSE 2021,",
            "year": 2021
        },
        {
            "authors": [
                "H. Zhu",
                "P.A.V. Hall",
                "May"
            ],
            "title": "J.H.R.: Software unit test coverage and adequacy",
            "venue": "ACM Comput. Surv. 29(4),",
            "year": 1997
        },
        {
            "authors": [
                "Y.K. Malaiya",
                "M.N. Li",
                "J.M. Bieman",
                "R. Karcich"
            ],
            "title": "Software reliability growth with test coverage",
            "venue": "IEEE Trans. Reliab",
            "year": 2002
        },
        {
            "authors": [
                "C. Cadar",
                "D. Dunbar",
                "D.R. Engler"
            ],
            "title": "KLEE: unassisted and automatic generation of high-coverage tests for complex systems programs",
            "venue": "8th USENIX Symposium on Operating Systems Design and Implementation,",
            "year": 2008
        },
        {
            "authors": [
                "Z. Li",
                "X. Ma",
                "C. Xu",
                "C. Cao"
            ],
            "title": "Structural coverage criteria for neural networks could be misleading",
            "venue": "Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results, ICSE (NIER)",
            "year": 2019
        },
        {
            "authors": [
                "F. Harel-Canada",
                "L. Wang",
                "M.A. Gulzar",
                "Q. Gu",
                "M. Kim"
            ],
            "title": "Is neuron coverage a meaningful measure for testing deep neural networks",
            "venue": "ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Dong",
                "P. Zhang",
                "J. Wang",
                "S. Liu",
                "J. Sun",
                "J. Hao",
                "X. Wang",
                "L. Wang",
                "J.S. Dong",
                "D. Ting"
            ],
            "title": "There is limited correlation between coverage and robustness for deep neural networks",
            "year": 1911
        },
        {
            "authors": [
                "N. Carlini",
                "D.A. Wagner"
            ],
            "title": "Towards evaluating the robustness of neural networks",
            "venue": "IEEE Symposium on Security and Privacy, SP 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Y. LeCun",
                "L. Bottou",
                "Y. Bengio",
                "P. Haffner"
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proc. IEEE 86(11),",
            "year": 1998
        },
        {
            "authors": [
                "J. Gilmer",
                "S.S. Schoenholz",
                "P.F. Riley",
                "O. Vinyals",
                "G.E. Dahl"
            ],
            "title": "Neural message passing for quantum chemistry",
            "venue": "Proceedings of the 34th International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "T. Liu"
            ],
            "title": "Learning to rank for information retrieval",
            "venue": "J. (eds.) Proceeding of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2010
        },
        {
            "authors": [
                "Z. Li",
                "X. Ma",
                "C. Xu",
                "C. Cao",
                "J. Xu",
                "J. L\u00fc"
            ],
            "title": "Boosting operational DNN testing efficiency through conditioning",
            "venue": "Proceedings of the ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2019,",
            "year": 2019
        },
        {
            "authors": [
                "J. Chen",
                "Z. Wu",
                "Z. Wang",
                "H. You",
                "L. Zhang",
                "M. Yan"
            ],
            "title": "Practical accuracy estimation for efficient deep neural network testing",
            "venue": "ACM Trans. Softw. Eng. Methodol",
            "year": 2021
        },
        {
            "authors": [
                "W. Ma",
                "M. Papadakis",
                "A. Tsakmalis",
                "M. Cordy",
                "Y.L. Traon"
            ],
            "title": "Test selection for deep learning systems",
            "venue": "ACM Trans. Softw. Eng. Methodol",
            "year": 2021
        },
        {
            "authors": [
                "Y. Chen",
                "Z. Wang",
                "D. Wang",
                "Y. Yao",
                "Z. Chen"
            ],
            "title": "Behavior patterndriven test case selection for deep neural networks",
            "venue": "IEEE International Conference On Artificial Intelligence Testing, AITest 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Z. Li",
                "X. Ma",
                "C. Xu",
                "J. Xu",
                "C. Cao",
                "J. Lu"
            ],
            "title": "Operational calibration: debugging confidence errors for dnns in the field",
            "venue": "ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering,",
            "year": 2020
        },
        {
            "authors": [
                "G. Naitzat",
                "A. Zhitnikov",
                "L. Lim"
            ],
            "title": "Topology of deep neural networks",
            "venue": "J. Mach. Learn. Res",
            "year": 2020
        },
        {
            "authors": [
                "D. Filan",
                "S. Casper",
                "S. Hod",
                "C. Wild",
                "A. Critch",
                "S. Russell"
            ],
            "title": "Clusterability in neural networks. CoRR abs/2103.03386 (2021) https://arxiv",
            "year": 2021
        },
        {
            "authors": [
                "B. Rieck",
                "M. Togninalli",
                "C. Bock",
                "M. Moor",
                "M. Horn",
                "T. Gumbsch",
                "K.M. Borgwardt"
            ],
            "title": "Neural persistence: A complexity measure for deep neural networks using algebraic topology",
            "venue": "In: 7th International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "F. Vahedian",
                "R. Li",
                "P. Trivedi",
                "D. Jin",
                "D. Koutra"
            ],
            "title": "Convolutional neural network dynamics: A graph perspective",
            "year": 2021
        },
        {
            "authors": [
                "Y. Zhao",
                "H. Zhang"
            ],
            "title": "Quantitative performance assessment of CNN units via topological entropy calculation",
            "venue": "The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "T. Chen",
                "C. Guestrin"
            ],
            "title": "Xgboost: A scalable tree boosting system",
            "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data",
            "year": 2021
        },
        {
            "authors": [
                "A. Krizhevsky"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Master\u2019s thesis,",
            "year": 2009
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for largescale image recognition",
            "venue": "3rd International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "N. Papernot",
                "P.D. McDaniel",
                "S. Jha",
                "M. Fredrikson",
                "Z.B. Celik",
                "A. Swami"
            ],
            "title": "The limitations of deep learning in adversarial settings",
            "venue": "IEEE European Symposium on Security and Privacy,",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "1\n(\u223c \u00d71/50) than the state-of-the-art method. The code of ActGraph is open-sourced at https:// github.com/Embed-Debuger/ActGraph.\nKeywords: Deep Neural Network, Test Prioritization, Deep Learning Testing, Activation Graph, Label"
        },
        {
            "heading": "1 Introduction",
            "text": "Deep neural networks (DNNs) are widely applied in object recognition [1], image classification [2], natural language processing [3], autonomous vehicles [4], etc. But it is still threatened by uncertain inputs. For example, a car in autopilot mode recognized a white truck as a cloud in the sky and caused a serious traffic accident [5]. Therefore, it is crucial to test the DNN to find vulnerabilities before deployment.\nIn the testing phase, numerous diverse test cases are fed into the DNN in order to evaluate the reliability of the model. These test cases require expensive manual labeling and verification. To reduce the cost of labeling, a feasible solution is to prioritize test cases to find more vulnerabilities. These test cases are more likely to expose DNN vulnerabilities are marked in advance, improving the efficiency of DNN testing.\nSome test case prioritization methods have been proposed to address the labeling cost problem. They can be divided into three categories, including Neuron Coverage (NC)-based [6\u201310], model activation-based [11\u201314] and mutation-based [15] test case prioritization methods. NC-based methods draw on the concept of test coverage [16\u201318] from the traditional software testing to measure the adequacy of test cases. But some studies [19\u201321] have pointed out that it isn\u2019t strongly correlated between NC and misclassified inputs, so NC cannot be effectively applied to prioritization.\nModel activation-based prioritization methods can be divided into confidence-based and embedding-based test case prioritization methods. Confidence-based methods [11, 12] extract the probability distribution of test cases in the DNN confidence layer. They believe that the correctly classified cases should output higher probabilities, but misclassified cases should output multiple similar probabilities. This assumption limits their application scenarios. For example, Carlini-Wagner (C&W) [22] adversarial cases can cause the DNN to output high confidence in the wrong class, so the effect of confidence-based methods will be significantly reduced. On the other hand, embedding-based prioritization methods [13, 14] generally extract the hidden layer outputs of test cases. They prioritize test cases with inconsistencies based on the differences between adversarial and normal cases. However, they cannot effectively prioritize false positive (FP) cases, because the confusion of hidden layer features directly leads to the misclassification of the model.\nMutation-based test case prioritization, i.e., PRioritizing test inputs via Intelligent Mutation Analysis (PRIMA), is the state-of-the-art (SOTA)\nmethod, which uses mutation operations to make test cases as different as possible to produce different prediction results, arguing that it is easier to reveal the DNN vulnerabilities. The time complexity of PRIMA is O (nm1 + nm2N\u03b8), where n is the total number of test cases, m1 is the number of sample mutations, m2 is the number of model mutations and N\u03b8 is the number of parameters in the model mutation. Sincem1,m2 and N\u03b8 are usually very large, PRIMA has to take higher time complexity than activation-based methods (O(n log n)).\nBased on the above analysis, the existing test case prioritization methods show limitations in two aspects: (i) Limited Scenarios. Model activationbased prioritization methods have limited application scenarios. Specifically, confidence-based prioritization methods consider that misclassified cases have multiple similar probabilities, thus they are less effective for adversarial cases with high confidence. On the other hand, embedding-based prioritization methods are less effective for FP cases, due to that the embedding features of FP cases are confused with the wrong classes. (ii) High Complexity. Mutationbased prioritization has a high time complexity since most of them require numerous mutations, memory read and write, and mutation query operations. Therefore, its time complexity is higher than activation-based methods.\nTherefore, to address the prioritization challenges, we explore the in-depth relationship between the test cases and the model\u2019s dynamic features. Although the existing model activation-based prioritization methods have extracted features in the hidden layer or the confidence layer, but they overlook some more fine-grained features, such as the neuron\u2019s activation. Therefore, we propose graph-level neuron activation features for test cases, to extract the activation graph between DNN layers. The activation graph is defined as the connection relationship of neurons. We study the differences in the activation graph for test cases. An illustration is carried out shown in Fig. 1. The activation graphs for the normal, FP, and C&W adversarial test cases are shown. It is a LeNet-5 [23] trained on MNIST [23]. Fig. 1 (a) and (b) are normal test cases with class labels 9 and 7, respectively. Fig. 1 (c) and (d) are misclassified as 9, which are FP and adversarial cases, respectively. We only show weighted edge values greater than 0.4, in order to show the activation graph clearly. The size of the node is determined by its node feature value. We observe the distribution differences in activation graphs for the test cases. Specifically, the edges of the last layer of the activation graph of the normal cases are only connected to the correct node, as shown in Fig. 1 (a) and (b). On the contrary, the misclassified cases will connect not only the correct node but also the wrong node in the last layer of the activation graph, as shown in Fig. 1 (c) and (d). Comparing the activation graph of normal case (Fig. 1 (b)) and adversarial case (Fig. 1 (d)), we observe that the similar distribution of edges in shallow layers (L1, L2), but the differences distribution of edges gradually increase in deeper layers (L3, L4 and L5).\nTherefore, we propose a model activation graph-based test case prioritization, namely ActGraph. ActGraph regards the neurons of DNN as nodes, and\nthe adjacency matrix as the connection relationship between nodes. Then, the node features and the adjacency matrix are aggregated by message passing [24], the aggregated node features will contain the features of neighbor nodes and the structural information between nodes, which can be effectively used for test case prioritization. ActGraph has the following two key characteristics.\n(i) Effectiveness and generalizability. ActGraph extracts the finer-grained activation features of the test cases on the model, and converts the model activations into the spatial relationship of neurons, which can solve the limitation problem of the scene based on the model activation method. ActGraph can prioritize multiple types of test cases by learning one type of cases, which is more general than model activation-based methods.\n(ii) Efficiency. ActGraph uses the graph to extract high-level node features instead of mutation operations, which is much more efficient than the mutation-based approaches.\nIn addition, ActGraph builds a ranking model using Learning-to-Rank (L2R) [25], which can effectively prioritize test cases by learning the node features of test cases. According to the priority results of the ActGraph, the test cases that trigger the vulnerability can be marked earlier, thereby greatly improving the efficiency of DNN testing and effectively saving development time.\nThe main contributions are summarized as follows.\n\u2022 By identifying the limitations of existing prioritization methods, we propose the activation graph, and find that cases that trigger model vulnerabilities and normal cases in the activation graph. \u2022 Motivated by the observation, we propose a novel test case prioritization method, namely ActGraph. It extracts the spatial relationship between neuron nodes, calculates the center node feature in the activation graph, and adopts L2R model to intelligently combine center node feature to achieve efficient test case prioritization.\n\u2022 Comprehensive experiments have been conducted on three datasets and four models to verify the effectiveness and efficiency of ActGraph. It outperforms the SOTA baselines in both natural and adversarial scenarios, especially in RAUC-100 (\u223c \u00d71.40).\nThe remainder of this paper is organized as follows. We describe the related work in Section 2. In Section 3, we describe the designed ActGraph method in detail. Experimental results are provided in Section 4. Threats to validity are described in Section 5 and conclusions are made in Section 6."
        },
        {
            "heading": "2 Related Works",
            "text": ""
        },
        {
            "heading": "2.1 Test Case Prioritization for DNNs",
            "text": "Some DNN test case prioritization methods are proposed to solve the labelingcost problem. According to the different dynamic features of DNNs for test cases, they can be categorized into: NC-based [6\u201310], model activationbased [11\u201314] and mutation-based [15] test case prioritization methods.\nNC-based test case prioritization. NC borrows the concept of test coverage from traditional software testing to measure the adequacy of test cases, which was first proposed by Pei et al. [6]. They argue that DNNs with higher coverage are more secure and reliable when the input is uncertain. Based on this, Guo et al. [7] continuously mutate the input slightly, with maximizing the NC and the predicted difference between the original and the mutated input to guide the generation of test cases. But some studies [19\u201321] have pointed out that NC and misclassified inputs do not have a strong correlation, so cannot be effectively applied to test case prioritization.\nModel activation-based test case prioritization. Test case prioritization methods based on model activation can be divided into confidence-based and embedding feature-based. Confidence-based prioritization methods generally obtain the confidence output of the DNN. From a statistical point of view, they assume that correctly classified test cases should have high probability, and misclassified or abnormal test cases have multiple similar probabilities. For example, Feng et al. [11] proposed DeepGini based on the Gini impurity, which can quickly identify test samples that may lead to DNN misclassification. Shen et al. [12] proposed Multiple-Boundary Clustering and Prioritization (MCP) to uniformly select high-priority test cases from all boundary regions by dividing test cases into multiple boundary regions. Embedding feature-based prioritization methods believe that the deep layer of the DNN can better represent the high-level features of the input cases, so the last layer of hidden layer features of the DNN output is generally obtained. Kim et al. [13] proposed Surprise Adequacy (SA) to measure the adequacy of test cases for DNN testing by calculating the difference in hidden layer features between test cases and training data, including Likelihood-based Surprise Adequacy (LSA) and Distance-based Surprise Adequacy (DSA). LSA refers to estimating the density of the embedding features of the test cases on the training data, while\nDSA defines the Euclidean distance between the embedding features of the test cases and the training data. They show that DSA is more suitable for classification models.\nMutation-based test case prioritization. Wang et al. [15] proposed PRIMA, which prioritizes those test cases that produce different prediction results through fuzzy mutation operations (including input mutation and model mutation), arguing that this is more likely to reveal DNN vulnerabilities.\nIn addition, some test case sampling methods are proposed to improve the efficiency of DNN testing [26\u201330], to estimate the accuracy of DNN by sampling a few test cases. Our work aims to identify more bug-revealing test cases earlier by prioritizing test cases."
        },
        {
            "heading": "2.2 Graph Structure of DNNs",
            "text": "Several studies have constructed DNNs as Graphs (DAG) to explore the performance of DNNs, such as interpretability, generalization and performance analysis. Naitzat et al. [31] demonstrated the superiority of ReLu activation by studying the variation of the Betty number of two classes of DNN. Filan [32] et al. proposed to directly represent the fully connected layer as a weighted undirected graph, where each neuron corresponds to a node. Rieck [33] et al. proposed neural persistence, a measure of the topological complexity of network structures that can give a criterion for early stopping of training. They proposed \u201dUnroll\u201d, and converting convolutional layers into graphs. Then, Vahedian et al. [34] proposed a \u201cRolled\u201d graph representation of convolutional layers to solve the DNN performance prediction problem by capturing the early DNN dynamics during the training phase. To maintain the semantic meaning of the convolutional layers, they represent each filter as a node and link the filters in successive layers by weighted edges. Zhao et al. [35] proposed feature entropy to quantitatively identify individual neuron states of a specific class.\nIn general, most of the existing DAG studies focus on the relationship between the structural parameters of the DNN and the overall behavior of the DNN. ActGraph and the existing DAG mainly have the following differences: i) The methods of constructing the graph are different. The graph of ActGraph uses the DNN structure and multi-layer activation, unlike past works that only consider the DNN structure; ii) The application scenarios are different. ActGraph is used for test case prioritization in DNN testing."
        },
        {
            "heading": "3 Approach",
            "text": ""
        },
        {
            "heading": "3.1 Overview",
            "text": "Existing DAG studies have been able to convert the DNN into the graph, discussing the dynamic properties of the DNN model [32\u201335]. They used an undirected weighted graph to treat neurons as nodes and the weights of the model as weighted edges between nodes. But they did not consider expressing the activation information of case to graph. Different from them, we use a\ndirected weighted graph to take the model weights as the skeleton of the graph, and map the activation values to the graph, so that the feature of the test case is expressed on the graph. For convenience, the definitions of symbols used in this paper are listed in Table 1.\nWe propose a novel test case prioritization method based on model activation graph, namely ActGraph. It consists of three stages. i) Test Case Activation: the test cases are fed into the trained DNN, and each layer of the DNN outputs activation values (Section 3.2); ii) Feature Extraction: activation graphs are constructed based on the activations of DNN\u2019s each layer, and the adjacency matrix and node features are extracted on the activation graphs. Finally the center node features are obtained by message passing aggregation (Section 3.3); iii) Ranking Model Building : ActGraph adopts the framework of L2R to build a ranking model, which can utilize the center node features, for prioritizing test cases (Section 3.4). The framework of ActGraph is shown in Fig. 2."
        },
        {
            "heading": "3.2 Test Case Activation",
            "text": "ActGraph is a model activation-based test case prioritization method that runs during the test phase. Test cases are input to the DNN, and each layer outputs activation values. In order to facilitate the construction of the graph, the weights and activations of each neuron are averaged, and the weights and activations of each layer are normalized.\nFor a trained DNN and a test case x. DNN has L layers, nli is the i-th neuron of the l-th layer. The case x is input to the DNN, and get the output of each layer of neurons in the DNN. The activation value \u03d5li of n l i is calculated\nas:\n\u03d5li(x) = 1\nHeightl \u00d7Widthl\n\u2211\nHeightl\n\u2211\nWidthl\nF li (x) (1)\nwhere F li (x) \u2208 R Heightl\u00d7Widthl\u00d7Channell is the feature map of nli output when x is input. For the convolutional layer, the output dimension is Heightl \u00d7 Widthl, and the dimension of the fully connected layer is 1\u00d7 1.\nThe neuron activation value \u03d5l(x) of each layer performs the max-min normalization, which normalizes \u03d5l(x) to the [0,1] range, is calculated as:\n\u03d5l(x) = \u03d5l(x)\u2212min(\u03d5l(x))\nmax(\u03d5l(x))\u2212min(\u03d5l(x)) (2)\nFor the neuron nli, its neuron weight is calculated as:\nwl\u22121,lj,i = 1\nHeight\u03b8l \u00d7Width \u03b8 l\n\u2211\nHeight\u03b8 l\n\u2211\nWidth\u03b8 l\n\u03b8l\u22121,lj,i (3)\nwhere \u03b8l\u22121,lj,i \u2208 R Height\u03b8 l \u00d7Width\u03b8 l represents the weight parameter between the neuron nl\u22121j and the neuron n l i. The dimension of the neuron weight of the convolutional layer of the l-th layer is Height\u03b8l \u00d7Width \u03b8 l , and the dimension of the weight of the fully connected layer is 1\u00d7 1. Normalize the neuron weight wl\u22121,l of each layer, and convert wl\u22121,l of each layer to the range of [0,1], is calculated as:\nwl\u22121,l = wl\u22121,l \u2212min(wl\u22121,l)\nmax(wl\u22121,l)\u2212min(wl\u22121,l) (4)\nTo reduce the computational cost, ActGraph only obtains the neuron activations and their weights of the last K layers of the DNN."
        },
        {
            "heading": "3.3 Feature Extraction",
            "text": "ActGraph adopts the L2R framework, and builds a ranking model for each DNN model. Since L2R requires a set of features like other supervised machine learning [25]. ActGraph extracts a set of features from the activation values of the test cases and the structural information of the model. In this section, we propose the steps of feature extraction for test cases in ActGraph.\nAs shown in Fig. 1, the weighted edges can significantly represent the differences of distribution between different test cases, but cannot clearly express the characteristics of neurons. Therefore, we would like to extract more effective node features from the activation graph for prioritization. The message passing of Graph Neural Network (GNN) can aggregate the features of the current node and neighbor nodes, which is similar to the data flow of DNN, that is, the activation values of the previous layer are passed to the next layer. Specifically, we use a directed activation graph, and extract the weighted indegrees of nodes as node features. The weighted in-degrees are low-order node features that represent the importance of the nodes. Further, we aggregate node features and the adjacency matrix to obtain higher-order node features by message passing, namely center node feature. For the explanation of its effectiveness, we describe it in detail in Section 4.4.\nBecause the activation values of the DNN have the data flow, we construct the DNN as a directed weighted graph. Let D = (V,E) be a directed weighted graph whose node set is V and its edge set is E, where V is the neuron set of DNN, and E is the set of the directed weighted edges, as follows:\nAj,i =\n{\nwj,i \u00d7 \u03d5i , vj \u2208 \u0393 \u2212 D(vi) 0 , vj /\u2208 \u0393 \u2212 D(vi) (5)\nwhere A is the adjacency matrix ofD, vi is the i-th node ofD, wj,i is the weight between vj and vi, and \u0393 \u2212\nD(vi) = {vj | vj \u2208 V (D) \u2227 \u27e8vj , vi\u27e9 \u2208 E(D) \u2227 vj \u0338= vi} is the set of predecessors of vi.\nWe use weighted in-degree as node features (nf). Degree is the simplest and most effective feature for nodes, which captures the connectivity of nodes. The vi\u2019s weight is the sum of the weights of adjacent input edges, calculated as follows:\nnfi = \u2211 j Aj,i , vj \u2208 \u0393 \u2212 D(vi) (6)\nwhere nfi is the node feature value of vi. Weighted in-degree is a low-order node feature. Therefore, we use the message passing of GNN to aggregate the adjacency matrix and node features of the activation graph to obtain the center node feature (cnf ), which is calculated as follows:\ncnf = AGG(A, nf) (7)\nwhere the aggregation function AGG() can use Sum(),Max(), and Average(). We use the Sum() function.\nAfter calculating the cnf of all nodes, ActGraph only takes the cnf of the last two layers. Because we believe that the deeper activation of the model can fully express the high-dimensional characteristics of test cases. The two-layer cnf needs at least four layers of weight and activation, so we set K=4. The reason is described in Section 3.5."
        },
        {
            "heading": "3.4 Ranking Model Building",
            "text": "ActGraph adopts the XGBoost algorithm [36], which is an optimized distributed gradient reinforcement learning algorithm, and establishes an L2Rbased ranking model.\nThe cnf of the validation set obtained by Eq. (7) is used as the training set of the ranking model, and according to the DNN\u2019s prediction of the sample, it is labeled as 0 (prediction is correct) or 1 (prediction is wrong). The loss function for training the ranking model is as follows:\nobj(cnf, y) = l(y, y\u0302) +\nT \u2211\nt=1\n\u2126(ft) (8)\nwhere y\u0302 = \u2211T\nt=1 ft(cnf), ft(cnf) is the predicted value of the t-th tree, y is 0 or 1, T is the number of trees, and \u2126 is regularization. In summary, the process of training ranking model is shown in Algorithm 1."
        },
        {
            "heading": "3.5 Utility Analysis of Center Node Feature",
            "text": "In this section, we analyze the utility of cnf and explain how the K value of ActGraph is determined. Let a DNN with N neurons. Its output value of each layer is activated by a case x. The activation value \u03d5 and the weight W can be expressed as:\n\u03d5 = [ \u03d50(x) \u03d51(x) ... \u03d5N\u22121(x) ]\n1\u00d7N (9)\nW =\n\n   w0,0 w0,1 ... w0,N\u22121 w1,0 w1,1 ... w1,N\u22121 ... ... ... ...\nwN\u22121,0 wN\u22121,1 ... wN\u22121,N\u22121\n\n  \nN\u00d7N\n(10)\nwhere \u03d5 and W are layer normalized by Eq. (2) and Eq. (4). Then the adjacency matrix A is calculated by Eq. (5) as:\nA =\n\n  \n\u03d50 \u00b7 w0,0 \u03d51 \u00b7 w0,1 ... \u03d5N\u22121 \u00b7 w0,N\u22121 \u03d50 \u00b7 w1,0 \u03d51 \u00b7 w1,1 ... \u03d5N\u22121 \u00b7 w1,N\u22121\n... ... ... ...\n\u03d50 \u00b7 wN\u22121,0 \u03d51 \u00b7 wN\u22121,1 ... \u03d5N\u22121 \u00b7 wN\u22121,N\u22121\n\n  \nN\u00d7N\n(11)\nThen the node feature nf is calculated by Eq. (6) as:\nnf = [ \u03d50 \u2211j wj,0 ... \u03d5N\u22121 \u2211j wj,N\u22121 ]\n1\u00d7N (12)\nAlgorithm 1 ActGraph\nRequire: A DNN f1 to be tested; The last K layers selected; Validation dataset xc \u2208 X = {x1, x2, ...}; The set of center node feature cnf . Ensure: A ranking model f2. 1: for l in K do 2: Obtain neuron weight wl\u22121,l by Eq. (3). 3: Normalize the neuron weight wl\u22121,l by Eq. (4). 4: end for 5: cnf = {\u2205}. 6: for xc in X do 7: for l in K do 8: Obtain neuron activation \u03d5l(xc) by Eq. (1). 9: Normalize the neuron activation \u03d5l(xc) by Eq. (2). 10: end for 11: Initialize directed weighted graph Dc. 12: Extract Adjacency matrix Ac by Eq. (5). 13: Calculate node feature nfc by Eq. (6). 14: Aggregate center node feature cnfc by Eq. (7). 15: cnf \u2190 cnf \u222a cnfc. 16: end for 17: Train the ranking model f2 by Eq. (8). 18: return f2.\nwhere nf is the in-degree of the activation graph. A node feature nfi is \u03d5i \u2211j wj,i, which indicates that the nfi value is obtained by aggregating the activation value and input edges of vi. Finally, the center node feature cnf is calculated by computing A and nf by Eq. (7) as:\ncnf = [ \u2211z \u03d5zwz,0nfz ... \u2211z \u03d5zwz,N\u22121nfz ]\n1\u00d7N (13)\nwhere the cnfi of vi is \u2211z\n\u03d5zwz,infz. Intuitively, cnfi is aggregated by the activation values and the nf value of neurons in the upper layer of vi.\nTherefore, ActGraph requires at least three layers of network to aggregate so that the cnf of the last layer is valid (not zero). In the experiment, we use the last four layers of DNN, i.e. K=4, to obtain the effective cnf of the last two layers."
        },
        {
            "heading": "4 EXPERIMENT",
            "text": "We evaluate ActGraph through answering the following five research questions (RQs).\n\u2022 RQ1: Does ActGraph show the SOTA prioritization performance in both natural and adversarial scenarios?\n\u2022 RQ2: Does ActGraph show the competitive generalizability in mixed scenarios? \u2022 RQ3: How to interpret ActGraph\u2019s utility by t-SNE and heatmap visualization? \u2022 RQ4: How is the stability of ActGraph under different hyperparameters (i.e. trainset size and training parameters of ActGraph)? \u2022 RQ5: Does ActGraph efficient in time complexity?"
        },
        {
            "heading": "4.1 Setup",
            "text": "Platform. The experiments were conducted on a server equipped with Intel XEON 6240 2.6GHz X 18C (CPU), Tesla V100 32GiB (GPU), 16GiB DDR4RECC 2666MHz (Memory), Ubuntu 16.04 (OS), Python 3.6, Keras-2.2.4, Tensorflow-gpu-1.9.0, Xgboost-1.5.2.\nDatasets. We conduct experiment on MNIST [23], CIFAR-10 [37] and CIFAR-100 [37]. MNIST contains 60,000 28\u00d728 gray-scale images, and each image is marked with numbers from 0 to 9. CIFAR-10 includes 60,000 32\u00d732 three-channel RGB-color images, which are divided into ten classes equally. CIFAR-100 includes 60,000 32\u00d732 three-channel RGB-color images, which are divided into one hundred classes equally. For each dataset, 40,000 are used for training, 10,000 for validation and 10,000 for testing.\nModels. For MNIST, we use LeNet-5 [23] for prioritization. On CIFAR-10, VGG16 [38] and ResNet18 [39] are adopted. On even larger dataset CIFAR100, we adopt VGG19 [38] model. The datasets and models configurations are shown in Table 2.\nData Preparation. To verify that ActGraph is able to prioritize various test cases that trigger model bugs. We use a variety of data operations for data generation, and generate a variety of different types of datasets to make the DNN misclassify, including adversarial and natural noise cases. The natural operations include image rotation, translation and flipping, collectively referred to as Rotate. And the adversarial operations include C&W [22] and Jacobianbased Saliency Map Attacks (JSMA) [40]. C&W attack leads the model to output false label with high confidence. JSMA can change only a few pixels to implement the attack, so the power of the disturbance is small.\nWe construct the Testset to prioritize and the Trainset for training the ranking model. For Original, Testset comes from the testset of the original dataset, and Trainset comes from the validation set of the original dataset. However, the DNN has high accuracy and only a few misclassified samples in Trainset. Therefore, these samples need to be repeatedly sampled until the\nbalance of positive and negative samples reaches 5,000 to 5,000, and Testset remains unchanged. For other types of data (Rotate, JSMA, C&W and Mix), the ratio of Trainset is 5,000 to 5,000, that is, it consists of 5,000 normally classified samples and 5,000 manipulated misclassified samples, and the ratio of Testset is 8,000 to 2,000. Mix is randomly sampled from four types of sets.\nBaselines. We adopt the model activation-based and mutation-based prioritization algorithms as the baselines in our experiment, including DeepGini [11], MCP [12], DSA [13] and PRIMA [15]. The parameters for these algorithms are configured following their settings reported in the respective papers. In addition, in order to explore the impact of ActGraph extracted graph-level features on test case prioritization, we extracted and concatenated the confidence output and the last hidden output as our baseline, namely Act.\nRAUC = AUC(rank)\nAUC(ideal rank) \u00d7 100% (14)\nRAUC-n is the RAUC for the first n test cases, as follows:\nRAUC(n) = AUC(rank, n)\nAUC(ideal rank, n) \u00d7 100% (15)\nThe value range is [0, 1], and 1 indicates the best ranking result. RAUC-100, RAUC-500, RAUC-1000 and RAUC-ALL are used in the experiments.\nImplementation Details. Our experiments have the following settings: (1) For XGBoost ranking algorithm in ActGraph, we set Learning rate to be 0.1, Colsample bytree to be 0.3, and Max depth to be 5; (2) In order to reduce the computational cost, we set K as 4 and take the cnf of the last two layers; (3) For all image data, we normalize the range of each pixel to [0, 1]."
        },
        {
            "heading": "4.2 Effectiveness of ActGraph",
            "text": "In the section, we find the answer to RQ1, by comparing ActGraph with 5 baseline algorithms to verify the effectiveness of ActGraph in natural and adversarial scenarios.\nImplementation details. (1) Each model and dataset is set with two natural scenarios (Original and Rotate) and two adversarial scenarios (C&W and JSMA). (2) The training type is the same as the test type. (3) The size of trainset of ranking model is 2,000, which contains 1,000 positive samples and 1,000 negative samples.\nResults and analysis. The results are shown in Table 3. The bold indicates the optimal results of different methods under the same type scenario and the same metric. In the total 64 results, ActGraph performs the best with 42 best results (65.63%), followed by DeepGini with 13 best results (20.31%) and PRIMA with 7 best results (10.94%). Then, we average the four metrics, in which ActGraph is the best, followed by PRIMA. Specifically, the average results of ActGraph are 0.865\u223c0.939, which are 0.80%\u223c5.96% higher than PRIMA, 2.06%\u223c13.19% higher than DeepGini, 8.53%\u223c13.50% higher than\nAct, 18.78%\u223c21.28% higher than MCP, and 11.14%\u223c24.58% higher than DSA. In the 32 results of natural scenarios, ActGraph gets 17 best results, DeepGini gets 13 best results, and PRIMA gets 2 best results. In the 32 results of adversarial scenarios, ActGraph gets 25 best results, PRIMA gets 5 best results, MCP gets 1 best results and DSA gets 1 best results.\nBecause the time and cost of prioritization is limited, the number of test cases that can be labeled is often small. This also means that RAUC-100 is more important than RAUC-ALL for the test case prioritization approaches. In RAUC-100, the average result of ActGraph is 0.871, 13.19% higher than DeepGini, 5.96% higher than PRIMA, 20.20% higher than MCP, 24.36% higher than DSA, and 13.50% higher than Act. These results show that DeepGini has better effects than PRIMA in natural scenarios, PRIMA has better effects than DeepGini in adversarial scenarios, and the average results of PRIMA are better than DeepGini. ActGraph shows the SOTA effect in both adversarial and natural scenarios, especially in RAUC-100.\nSpecifically, confidence-based methods perform better on FP cases, and embedding-based methods perform better on adversarial cases. In Original, DeepGini performs best with 11 best results. But in C&W scenario, ActGraph has 13 best results. Especially in RAUC-100, ActGraph is 9.41%\u223c52.51% higher than DeepGini. On the contrary, DSA is an embedding-based method, which performs better in adversarial scenarios than in natural scenarios. These results also confirm the previous hypothesis that the confidence-based methods work well for natural scenarios and the embedding-based methods work well for adversarial scenarios.\nThen, ActGraph outperforms Act, which illustrates that cnf is more effective than model activation feature. Because cnf not only has the information of neuron activation characteristics, but also the node connection relationship between neurons. In particular, we show that ActGraph also outperforms Act in generalizability in Section 4.3.\nAnswer to RQ1: ActGraph outperforms the baseline methods (i.e., DeepGini, PRIMA, MCP, DSA and Act) in natural and adversarial scenarios. ActGraph gets 78.13% best results in the adversarial scenarios and 53.13% best results in the natural scenarios. In RAUC-100, the average results of ActGraph are 5.96%\u223c24.36% higher than the baseline methods."
        },
        {
            "heading": "4.3 Generalizability of ActGraph",
            "text": "In the section, we find the answer to RQ2, validating the performance of ActGraph for prioritization of multiple types of test cases, especially with limited training knowledge. Limited training knowledge means that the trainset of the ranking model contains only one type of cases, which can trigger DNN vulnerabilities, but multiple types of test cases need to be prioritized in the testing phase.\nImplementation details. (1) Five training types are set up to explore the generalizability of ActGraph to Mix testset. (2) The size of trainset of ranking model is 2,000, which contains 1,000 positive cases and 1,000 negative cases. (3) RAUC-100 and RAUC-500 are evaluated in the experiment, since the time and cost of prioritization is limited.\nResults and analysis. The results are shown in Table 4. Since DeepGini and MCP are unsupervised methods, their prioritization results are not affected by the type of testset. In the total 40 results, ActGraph shows 29 best results (72.5%), followed by PRIMA with 8 best results (20%), MCP with 2 best results and DeepGini with zero best results. Then, we average the 4 metrics, and ActGraph performs the best, followed by PRIMA. The average results of ActGraph are 0.878\u223c0.898, which are 3.58%\u223c4.65% higher than PRIMA, 9.02%\u223c11.08% higher than DeepGini, 6.46%\u223c6.49% higher than Act, 22.70%\u223c23.54% higher than DSA, and 10.90%\u223c16.09% higher than MCP. Especially, ActGraph performs better in RAUC-100, is 4.65%\u223c22.70% higher than the other baseline methods.\nIn the RAUC-100 of VGG16, ResNet18 and VGG19, the variance of PRIMA from 0.0062 to 0.0095, DSA from 0.0123 to 0.0163, Act from 0.0005 to 0.0184. The variance of ActGraph achieve 0.0041, 0.0028 and 0.0027, respectively. As the result, ActGraph performs more consistently than other supervised learning baseline methods. This shows the stable effectiveness of ActGraph for different types of test cases in limited training knowledge. This\nalso shows that, for the DNN model under test, the ranking model of ActGraph does not need to be retrained frequently, because it can perform stably on different types of test cases, which indicates the generalizability of ActGraph.\nAnswer to RQ2: ActGraph outperforms the baseline methods (i.e., DeepGini, PRIMA, MCP, DSA and Act) in mixed scenarios. ActGraph has 72.5% of the best results, especially in average RAUC-100, which is 4.65%\u223c22.70% higher than the baseline methods. In addition, ActGraph shows better stability than the baseline methods by calculating the variance of RAUC-100. The variance of the baseline methods are 3.39 to 6.57 times that of ActGraph."
        },
        {
            "heading": "4.4 Interpretability of ActGraph",
            "text": "In the section, we find the answer to RQ3, explaining why ActGraph can be used effectively for prioritization. We show the visualization of test cases with high prioritization, and carry out qualitative analysis and quantitative analysis.\nImplementation details. (1) The t-SNE visualization for qualitative analysis and the heat map for quantitative analysis. (2) We use mixed cases to analyze the intra-class and inter-class distances of the features of ActGraph and baseline algorithms.\nTest cases visualization. The visualization of test cases with high prioritization is shown in Fig. 3. Intuitively, for the FP and Rotate test cases, they are also difficult to recognize by humans. For example, the images \u201c7\u201d and \u201c5\u201d are incomplete; The image \u201cdog\u201d is too bright, and the hair is too long, which blocks the basic features of dog; The colors of the images \u201ccat\u201d, \u201cflatfish\u201d, \u201cseal\u201d and \u201chorse\u201d are similar to the environment. In particular, the \u201ctractor\u201d and \u201ccamel\u201d are rotated 180 degrees resulting in the blue sky at the bottom of the image, thus identifying them as \u201clobster\u201d and \u201cshark\u201d. For JSMA and C&W, some images are also broken or blurred, such as \u201c9\u201d and \u201c2\u201d in the first line and \u201cpine tree\u201d in the third line. Most of the images are clear, but adding unobserved adversarial perturbations causes DNN output errors. This shows that ActGraph can pick up weak adversarial perturbations.\nQualitative analysis. The visualization of t-SNE is the first row of Fig. 4 for qualitative analysis. Intuitively, the features of Confidence and Embedding are confused, and the distances between different types are relatively close. In PRIMA, Clean and FP are close in distance, JSMA and C&W are close in distance, and Rotate is in the middle between natural and adversarial cases. This shows that although PRIMA can distinguish between FP and adversarial cases, it is difficult to distinguish between Clean and FP cases. In ActGraph, Rotate and Clean are distinguished, and most of the FP and Rotate overlap, only a few FP and Clean intersect, and the distance between JSMA and C&W is also farther than PRIMA. This shows that the center node feature of ActGraph not only has better prioritization performance for adversarial cases, but also has better prioritization effect on natural cases and FP cases than existing methods.\nQuantitative analysis. We calculate intra-class and inter-class distances on the t-SNE for five types of cases, represented by the heatmap, for quantitative analysis, where the distance measure used Euclidean distance. The heat map is the second row of Fig. 4. For intra-class distance, ActGraph is 6.46\u223c12.78. Except JSMA, the intra-class distance of ActGraph is smaller than other methods. For inter-class distance, intuitively, the distance between natural cases (Clean, FP and Rotate) and adversarial cases (JSMA and C&W) of ActGraph is all the farthest, with the farthest distance being 40.44, which is 1.54\u223c2.80 times of the other three methods. This shows that ActGraph can distinguish natural samples from adversarial samples better than other methods. In addition, the distance from FP to Clean of ActGraph is 16.12, the distance from FP to Rotate is 11.77, and the difference is 4.35, while the difference of Confidence is -0.85, Embedding is 1.41, and PRIMA is 0.61. This shows that the FP of ActGraph is closer to Rotate and farther from Clean, and the effect of prioritization will be better. Also, ActGraph\u2019s JSMA to C&W distance is farther than other methods.\nAnswer to RQ3: ActGraph has smaller intra-class distances and larger inter-class distances than the baseline methods. For inter-class distances, the maximum inter-class distance of ActGraph is 1.54\u223c2.80 times that of the baseline methods, and the average inter-class distance of ActGraph is 1.12\u223c1.36 times that of the baseline methods. For inter-class distances, the minimum inter-class distance of the baseline methods are 1.52\u223c2.04 times that of ActGraph and the average inter-class distance are 1.28\u223c1.47 times that of ActGraph."
        },
        {
            "heading": "4.5 Sensitivity Analysis of Parameters",
            "text": "In this section, we find the answer to RQ4. We analyze the influence of the size of trainset and training parameters of ActGraph.\nImplementation details. (1) We set RAUC-100 as the evaluation metric. (2) It is a VGG16 trained on CIFAR-10. (3) The type of trainset and testset are both Mix.\nInfluence of trainset size. The result shows in Fig. 5. DeepGini and MCP are unsupervised methods, so are not affected by the size of the trainset. In LeNet-5 and VGG (VGG16 and VGG19), ActGraph exhibits the best prioritization performance, and the performance increases with the size of the dataset. In ResNet18, ActGraph performs better than most baseline algorithms. Importantly, ActGraph is less affected by changes in trainset size, while the remaining three baseline algorithms (i.e. PRIMA, DSA and Act) are more affected by changes in trainset size. This shows that ActGraph can learn effective features from a small number of cases.\nInfluence of training parameters. We investigate the influence of main parameters in ActGraph, including Max depth (the maximum tree depth for each XGBoost model), Colsample bytree (the sampling ratio of columns of features when constructing each tree) and Learning rate (the boosting learning rate) in the XGBoost ranking algorithm. Fig. 6 shows the effectiveness of ActGraph under different parameter settings in RAUC-100 across the four models. We found that, ActGraph performs stably under different parameter settings and our default settings are proper.\nAnswer to RQ4: For the trainset size, we set four scenarios ranging from 500 to 2,000. ActGraph has stable performance and increases with the increase\nof the trainset size. For training hyperparameters, ActGraph is also stable. These results indicate that our previous parameter settings are appropriate."
        },
        {
            "heading": "4.6 Time Complexity",
            "text": "In this section, we find the answer to RQ5, referring to the prioritization time cost.\nImplementation details. We measure average running time for ranking 10,000 test cases by ActGraph and baselines. We run each method for 5 times, and the average is identified as the final result.\nResults and analysis. Firstly, we theoretically analyze the complexity of ActGraph according to different steps.\nThe time complexity of ActGraph includes acquiring activation values of multi-layer neurons, calculating ap, calculating nf and calculating cnf, so its time complexity is\nT \u223c O(t\u00d7 V ) +O(t\u00d7 E) +O(t\u00d7 E) +O(t\u00d7 E) \u223c O(t\u00d7 E) (16)\nwhere t is the number of samples, V is the number of neurons, and E is the number of edges between neurons.\nFurther, we analyze the efficiency of ActGraph from the real running time. According to the Table 5, the running time of ActGraph is acceptable, the time cost of ActGraph increases due to the increase of the total number of neurons and edges. In general, ActGraph is much faster than PRIMA, but is inferior to other methods. The reason is that we search more layers and neurons. Besides, ActGraph calculates the weighted edges between neurons, and calculates the center node features in the activation graph. PRIMA runs on average 50 times longer than ActGraph.\nAnswer to RQ5: The average running time for ActGraph to prioritize 10,000 cases is about four minutes, which is acceptable."
        },
        {
            "heading": "5 Threates to Validity",
            "text": "The depth of the graph. We explain why K=4 and take only two layers of cnf in Section 3.5. Too large K value (more than 4) and the layer number of cnf (more than 2) will not only increase the time cost, but also introduce noise from the shallow layer. And the value of K too small to obtain the valid cnf value. Therefore, it is appropriate to set K=4, which is also confirmed by the comprehensive experimental results.\nTime cost. The time cost of ActGraph may be affected by the neuron number of the DNN. We demonstrate that the ActGraph\u2019s runtime is acceptable by applying three popular DNN models with significant differences in the number of neurons.\nAccess to DNNs. ActGraph is a white-box method of prioritizing test cases. It needs to access the DNN for model weight parameters and deep activation of test cases. It is widely accepted that DNN testing could have full knowledge of the target model in software engineering."
        },
        {
            "heading": "6 Conclusion",
            "text": "Aiming at the problems of limited application scenarios and high time cost of existing test case prioritization methods, we propose a test case prioritization method based on the DNN activation graph, named ActGraph. We observe that the activation graphs of cases that trigger model vulnerabilities different from those of normal cases significantly. Motivated by it, ActGraph extracts the node features and adjacency matrix of test cases by building an activation graph, and uses the message passing mechanism to aggregate node features and adjacency matrix to obtain more effective center node features for test case prioritization. Extensive experiments have verified the effectiveness of ActGraph, which outperforms the SOTA method in both natural and adversarial scenarios, especially in RAUC-100 (\u223c \u00d71.40). And when the number of test cases is 10,000, the actual running time of the SOTA method is 50 times that of ActGraph. The experiments show that ActGraph has significantly better performance in terms of effectiveness, generalizability, and efficiency.\nIn the future, we will improve our ActGraph approach and apply it to more popular tasks and models, such as the transformer model for natural language processing and the long short-term memory model for time series forecasting.\nAcknowledgments. This research was supported by the National Natural Science Foundation of China, (No. 62072406) the National Key Laboratory of Science and Technology on Information System Security (No. 61421110502), the National Natural Science Foundation of China (No. 62103374)."
        }
    ],
    "title": "ActGraph: Prioritization of Test Cases Based on Deep Neural Network Activation Graph",
    "year": 2023
}