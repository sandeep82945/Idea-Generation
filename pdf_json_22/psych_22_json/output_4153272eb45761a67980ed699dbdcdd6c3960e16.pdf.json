{
    "abstractText": "Automated co-located human-human interaction analysis has been addressed by the use of nonverbal communication as measurable evidence of social and psychological phenomena. We survey the computing studies (since 2010) detecting phenomena related to social traits (e.g., leadership, dominance, personality traits), social roles/relations, and interaction dynamics (e.g., group cohesion, engagement, rapport). Our target is to identify the nonverbal cues and computational methodologies resulting in effective performance. This survey differs from its counterparts by involving the widest spectrum of social phenomena and interaction settings (free-standing conversations, meetings, dyads, and crowds). We also present a comprehensive summary of the related datasets and outline future research directions which are regarding the implementation of artificial intelligence, dataset curation, and privacy-preserving interaction analysis. Some major observations are: the most often used nonverbal cue, computational method, interaction environment, and sensing approach are speaking activity, support vector machines, and meetings composed of 3-4 persons equipped with microphones and cameras, respectively; multimodal features are prominently performing better; deep learning architectures showed improved performance in overall, but there exist many phenomena whose detection has never been implemented through deep models. We also identified several limitations such as the lack of scalable benchmarks, annotation reliability tests, cross-dataset experiments, and explainability analysis.",
    "authors": [
        {
            "affiliations": [],
            "name": "CIGDEM BEYAN"
        },
        {
            "affiliations": [],
            "name": "ALESSANDRO VINCIARELLI"
        }
    ],
    "id": "SP:097da0553c36380150cbf311961a202d4b09051b",
    "references": [
        {
            "authors": [
                "Maedeh Aghaei",
                "Mariella Dimiccoli",
                "Petia Radeva"
            ],
            "title": "2016. With whom do I interact? Detecting social interactions in egocentric photo-streams",
            "venue": "In Proc. of IEEE ICPR",
            "year": 2016
        },
        {
            "authors": [
                "E.S. Aimar",
                "P. Radeva",
                "M. Dimiccoli"
            ],
            "title": "Social Relation Recognition in Egocentric Photostreams",
            "venue": "In Proc. of IEEE ICIP",
            "year": 2019
        },
        {
            "authors": [
                "Xavier Alameda-Pineda",
                "Yan Yan",
                "Elisa Ricci",
                "Oswald Lanz",
                "Nicu Sebe"
            ],
            "title": "Analyzing Free-Standing Conversational Groups: A Multimodal Approach",
            "venue": "In Proc. of ACMMM",
            "year": 2015
        },
        {
            "authors": [
                "Stefano Alletto",
                "Giuseppe Serra",
                "Simone Calderara",
                "Rita Cucchiara"
            ],
            "title": "Understanding social relationships in egocentric vision",
            "venue": "Pattern Recognition 48,",
            "year": 2015
        },
        {
            "authors": [
                "Stefano Alletto",
                "Giuseppe Serra",
                "Simone Calderara",
                "Francesco Solera",
                "Rita Cucchiara"
            ],
            "title": "From Ego to Nos-Vision: Detecting Social Relationships in First-Person Views",
            "venue": "In Proc. of IEEE CVPRw",
            "year": 2014
        },
        {
            "authors": [
                "Rawan Alsarrani",
                "Anna Esposito",
                "Alessandro Vinciarelli"
            ],
            "title": "Thin Slices of Depression: Improving Depression Detection Performance Through Data Segmentation",
            "venue": "In Proc. of ICASSP",
            "year": 2022
        },
        {
            "authors": [
                "Oya Aran",
                "Daniel Gatica-Perez"
            ],
            "title": "Fusing Audio-Visual Nonverbal Cues to Detect Dominant People in Small Group Conversations",
            "venue": "In Proc. of ICPR",
            "year": 2010
        },
        {
            "authors": [
                "Oya Aran",
                "Daniel Gatica-Perez"
            ],
            "title": "Cross-domain personality prediction: from video blogs to small group meetings",
            "venue": "In Proc. of ACM ICMI",
            "year": 2013
        },
        {
            "authors": [
                "Oya Aran",
                "Daniel Gatica-Perez"
            ],
            "title": "One of a kind: inferring personality impressions in meetings",
            "venue": "In Proc. of ACM ICMI",
            "year": 2013
        },
        {
            "authors": [
                "Umut Avci",
                "Oya Aran"
            ],
            "title": "Effect of nonverbal behavioral patterns on the performance of small groups",
            "venue": "In Workshop on Understanding Modeling Multiparty Multimodal Interactions",
            "year": 2014
        },
        {
            "authors": [
                "Umut Avci",
                "Oya Aran"
            ],
            "title": "Predicting the Performance in Decision-Making Tasks: From Individual Cues to Group Interaction",
            "venue": "IEEE Trans. Multimedia 18,",
            "year": 2016
        },
        {
            "authors": [
                "Chongyang Bai",
                "Maksim Bolonkin",
                "Srijan Kumar",
                "Jure Leskovec",
                "Judee Burgoon",
                "Norah Dunbar",
                "V.S. Subrahmanian"
            ],
            "title": "Predicting dominance in multi-person videos",
            "venue": "In Proc. of IJCAI",
            "year": 2019
        },
        {
            "authors": [
                "T. Baltrusaitis",
                "P. Robinson",
                "L-P. Morency"
            ],
            "title": "OpenFace: An open source facial behavior analysis toolkit",
            "venue": "In Proc. of IEEE WACV",
            "year": 2016
        },
        {
            "authors": [
                "Sophia Bano",
                "Tamas Suveges",
                "Jianguo Zhang",
                "Stephen J. Mckenna"
            ],
            "title": "Multimodal Egocentric Analysis of Focused Interactions",
            "venue": "IEEE Access",
            "year": 2018
        },
        {
            "authors": [
                "Sophia Bano",
                "Jianguo Zhang",
                "Stephen J. McKenna"
            ],
            "title": "Finding Time Together: Detection and Classification of Focused Interaction in Egocentric Video",
            "venue": "In Proc. of IEEE CVPR workshops",
            "year": 2017
        },
        {
            "authors": [
                "Roman Bednarik",
                "Shahram Eivazi",
                "Michal Hradis"
            ],
            "title": "Gaze and conversational engagement in multiparty video conversation: an annotation scheme and classification of high and low levels of engagement",
            "venue": "In Proc. Workshop on eye gaze in intelligent human machine interaction",
            "year": 2012
        },
        {
            "authors": [
                "Cigdem Beyan",
                "Matteo Bustreo",
                "Muhammad Shahid",
                "Gian Luca Bailo",
                "Nicolo Carissimi",
                "Alessio Del Bue"
            ],
            "title": "Analysis of Face-Touching Behavior in Large Scale Social Interaction Dataset",
            "year": 2020
        },
        {
            "authors": [
                "Cigdem Beyan",
                "Francesca Capozzi",
                "Cristina Becchio",
                "Vittorio Murino"
            ],
            "title": "Identification of Emergent Leaders in a Meeting Scenario Using Multiple Kernel Learning",
            "venue": "In Proc. of ACM ICMI-ASSP4MI",
            "year": 2016
        },
        {
            "authors": [
                "Cigdem Beyan",
                "Francesca Capozzi",
                "Cristina Becchio",
                "Vittorio Murino"
            ],
            "title": "Prediction of the Leadership Style of an Emergent Leader Using Audio and Visual Nonverbal Features",
            "venue": "IEEE Trans. Multimedia 20,",
            "year": 2018
        },
        {
            "authors": [
                "Cigdem Beyan",
                "Nicolo Carissimi",
                "Francesca Capozzi",
                "Sebastiano Vascon",
                "Matteo Bustreo",
                "Antonio Pierro",
                "Cristina Becchio",
                "Vittorio Murino"
            ],
            "title": "Detecting Emergent Leader in a Meeting Environment Using Nonverbal Visual Features Only",
            "venue": "In Proc. of ACM ICMI",
            "year": 2016
        },
        {
            "authors": [
                "Cigdem Beyan",
                "Vasiliki-Maria Katsageorgiou",
                "Vittorio Murino"
            ],
            "title": "Moving as a Leader: Detecting Emergent Leadership in Small Groups using Body Pose",
            "venue": "In Proc. of ACMMM",
            "year": 2017
        },
        {
            "authors": [
                "Cigdem Beyan",
                "Vasiliki-Maria Katsageorgiou",
                "Vittorio Murino"
            ],
            "title": "A Sequential Data Analysis Approach to Detect Emergent Leaders in Small Groups",
            "venue": "IEEE Trans. on Multimedia 21,",
            "year": 2019
        },
        {
            "authors": [
                "Cigdem Beyan",
                "Muhammad Shahid",
                "Vittorio Murino"
            ],
            "title": "Investigation of Small Group Social Interactions Using Deep Visual Activity-Based Nonverbal Features",
            "venue": "In Proc. of ACMMM",
            "year": 2018
        },
        {
            "authors": [
                "Cigdem Beyan",
                "Andrea Zunino",
                "Muhammad Shahid",
                "Vittorio Murino"
            ],
            "title": "Personality Traits Classification Using Deep Visual Activity-Based Nonverbal Features of Key-Dynamic Images",
            "venue": "IEEE Trans. on Affective Comput. 12,",
            "year": 2021
        },
        {
            "authors": [
                "Indrani Bhattacharya",
                "Michael Foley",
                "Christine Ku",
                "Ni Zhang",
                "Tongtao Zhang",
                "Cameron Mine",
                "Manling Li",
                "Heng Ji",
                "Christoph Riedl",
                "Brooke Foucault Welles",
                "Richard J. Radke"
            ],
            "title": "The Unobtrusive Group Interaction (UGI) Corpus",
            "venue": "In Proc. of ACM MMSys",
            "year": 2019
        },
        {
            "authors": [
                "Indrani Bhattacharya",
                "Michael Foley",
                "Ni Zhang",
                "Tongtao Zhang",
                "Christine Ku",
                "Cameron Mine",
                "Heng Ji",
                "Christoph Riedl",
                "Brooke Foucault Welles",
                "Richard J. Radke"
            ],
            "title": "A Multimodal-Sensor-Enabled Room for Unobtrusive Group Meeting Analysis",
            "venue": "In Proc. of ACM ICMI",
            "year": 2018
        },
        {
            "authors": [
                "Beatrice Biancardi",
                "Lou Maisonnave-Couterou",
                "Pierrick Renault",
                "Brian Ravenet",
                "Maurizio Mancini",
                "Giovanna Varni"
            ],
            "title": "The WoNoWa Dataset: Investigating the Transactive Memory System in Small Group Interactions",
            "venue": "In Proc. of ACM ICMI",
            "year": 2020
        },
        {
            "authors": [
                "Sanjay Bilakhia",
                "Stavros Petridis",
                "Anton Nijholt",
                "Maja Pantic"
            ],
            "title": "The MAHNOB Mimicry Database: A database of naturalistic human interactions",
            "venue": "Pattern Recognition Letters",
            "year": 2015
        },
        {
            "authors": [
                "Hakan Bilen",
                "Basura Fernando",
                "Efstratios Gavves",
                "Andrea Vedaldi",
                "Stephen Gould"
            ],
            "title": "Dynamic Image Networks for Action Recognition",
            "venue": "In Proc. of IEEE CVPR",
            "year": 2016
        },
        {
            "authors": [
                "McKenzie Braley",
                "Gabriel Murray"
            ],
            "title": "The Group Affect and Performance (GAP) Corpus",
            "venue": "In Proc. of ACM GIFT",
            "year": 2018
        },
        {
            "authors": [
                "Andreas Bulling",
                "Ulf Blanke",
                "Bernt Schiele"
            ],
            "title": "A Tutorial on Human Activity Recognition Using Body-Worn Inertial Sensors",
            "venue": "ACM Comput. Surv. 46,",
            "year": 2014
        },
        {
            "authors": [
                "Laura Cabrera-Quiros",
                "Demetriou Demetriou",
                "Ekin Gedik",
                "Leander van der Meij",
                "Hayley Hung"
            ],
            "title": "The MatchNMingle dataset: a novel multi-sensor resource for the analysis of social interactions and group dynamics in-the-wild during free-standing conversations and speed dates",
            "venue": "IEEE Trans. on Affective Comput",
            "year": 2018
        },
        {
            "authors": [
                "Laura Cabrera-Quiros",
                "Ekin Gedik",
                "Hayley Hung"
            ],
            "title": "Estimating Self-Assessed Personality from Body Movements and Proximity in Crowded Mingling Scenarios",
            "venue": "In Proc. of ACM ICMI",
            "year": 2016
        },
        {
            "authors": [
                "Zhe Cao",
                "Gines Hidalgo Martinez",
                "Tomas Simon",
                "Shih-En Wei",
                "Yaser Sheikh"
            ],
            "title": "OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields",
            "venue": "IEEE Trans. PAMI",
            "year": 2019
        },
        {
            "authors": [
                "Guillaume Chanel",
                "Christian M\u00fchl"
            ],
            "title": "Connecting brains and bodies: applying physiological computing to support social interaction",
            "venue": "Interacting with Computers 27,",
            "year": 2015
        },
        {
            "authors": [
                "Jonathan P Chang",
                "Caleb Chiam",
                "Liye Fu",
                "Andrew Z Wang",
                "Justine Zhang",
                "Cristian Danescu-Niculescu-Mizil"
            ],
            "title": "Convokit: A toolkit for the analysis of conversations",
            "year": 2020
        },
        {
            "authors": [
                "Theodora Chaspari",
                "Brian Baucom",
                "Adela C"
            ],
            "title": "Quantifying EDA synchrony through joint sparse representation: a case-study of couples\u2019 interactions",
            "venue": "Timmons",
            "year": 2015
        },
        {
            "authors": [
                "Theodora Chaspari",
                "Adela C Timmons",
                "Brian R"
            ],
            "title": "Exploring sparse representation measures of physiological synchrony for romantic couples",
            "venue": "Baucom",
            "year": 2017
        },
        {
            "authors": [
                "Chih-Wei Chen",
                "Rodrigo Cilla Ugarte",
                "Chen Wu",
                "Hamid Aghajan"
            ],
            "title": "Discovering social interactions in real work environments",
            "venue": "In Proc. of IEEE FG",
            "year": 2011
        },
        {
            "authors": [
                "Marco Cristani",
                "Loris Bazzani",
                "Giulia Paggetti",
                "Andrea Fossati",
                "Diego Tosato",
                "Alessio Del Bue",
                "Gloria Menegaz",
                "Vittorio Murino"
            ],
            "title": "Social interaction discovery by statistical analysis of F-formations",
            "venue": "In Proc. of BMVC",
            "year": 2011
        },
        {
            "authors": [
                "Marco Cristani",
                "Giulia Paggetti",
                "Alessandro Vinciarelli",
                "Loris Bazzani",
                "Gloria Menegaz",
                "Vittorio Murino"
            ],
            "title": "Towards Computational Proxemics: Inferring Social Relations from Interpersonal Distances",
            "venue": "In Proc. of PASSAT/SocialComp",
            "year": 2011
        },
        {
            "authors": [
                "Marco Cristani",
                "R. Raghavendra",
                "Alessio Del Bue",
                "Vittorio Murino"
            ],
            "title": "Human behavior analysis in video surveillance: A Social Signal Processing perspective",
            "venue": "Neurocomputing",
            "year": 2013
        },
        {
            "authors": [
                "Wen Dong",
                "Bruno Lepri",
                "Fabio Pianesi",
                "Alex Pentland"
            ],
            "title": "Modeling functional roles dynamics in small group interactions",
            "venue": "IEEE Trans. Multimedia 15,",
            "year": 2013
        },
        {
            "authors": [
                "Dario Dotti",
                "Esam Ghaleb",
                "Stylianos Asteriadis"
            ],
            "title": "Temporal Triplet Mining for Personality Recognition",
            "venue": "In Proc. of IEEE FG",
            "year": 2020
        },
        {
            "authors": [
                "Dario Dotti",
                "Mirela Popa",
                "Stylianos Asteriadis"
            ],
            "title": "Being the Center of Attention: A Person-Context CNN Framework for Personality Recognition",
            "year": 2020
        },
        {
            "authors": [
                "Metehan Doyran",
                "Arjan Schimmel",
                "P\u0131nar Baki",
                "K\u00fcbra Ergin",
                "Bat\u0131kan T\u00fcrkmen",
                "Alm\u0131la Akda\u011f Salah",
                "Sander CJ Bakkes",
                "Heysem Kaya",
                "Ronald Poppe",
                "Albert Ali Salah"
            ],
            "title": "MUMBAI: multi-person, multimodal board game affect and interaction analysis dataset",
            "venue": "Journal on Multimodal User Interfaces 15,",
            "year": 2021
        },
        {
            "authors": [
                "Vanessa Echeverria",
                "Roberto Martinez-Maldonado",
                "Simon Buckingham Shum"
            ],
            "title": "Towards Collaboration Translucence: Giving Meaning to Multimodal Group Data",
            "venue": "In Proc. of CHI. 1\u201316. Manuscript submitted to ACM",
            "year": 2019
        },
        {
            "authors": [
                "Sergio Escalera",
                "Oriol Pujol",
                "Petia Radeva",
                "Jordi Vitria",
                "M. Teresa Anguera"
            ],
            "title": "Automatic detection of dominance and expected interest",
            "venue": "In EURASIP Journal on Advances in Signal Processing",
            "year": 2010
        },
        {
            "authors": [
                "Florian Eyben",
                "Klaus R. Scherer",
                "Bjorn W. Schuller"
            ],
            "title": "The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research and Affective Computing",
            "venue": "IEEE Trans. on Affective Comput. 7,",
            "year": 2016
        },
        {
            "authors": [
                "Florian Eyben",
                "Felix Weninger",
                "Florian Gross",
                "Bj\u00f6rn Schuller"
            ],
            "title": "Recent Developments in OpenSMILE, the Munich Open-Source Multimedia Feature Extractor",
            "venue": "In Proc. of ACMMM",
            "year": 2013
        },
        {
            "authors": [
                "Victor Escorcia",
                "Juan Carlos Niebles"
            ],
            "title": "ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding",
            "venue": "In Proc. of CVPR",
            "year": 2015
        },
        {
            "authors": [
                "Sheng Fang",
                "Catherine Achard",
                "Severine Dubuisson"
            ],
            "title": "Personality classification and behaviour interpretation: an approach based on feature categories",
            "venue": "In Proc. of ACM ICMI",
            "year": 2016
        },
        {
            "authors": [
                "Alireza Fathi",
                "Jessica K. Hodgins",
                "James M. Rehg"
            ],
            "title": "Social interactions: A first-person perspective",
            "venue": "In Proc. of IEEE CVPR",
            "year": 2012
        },
        {
            "authors": [
                "Rodolfo Migon Favaretto",
                "Paulo Knob",
                "Soraia Raupp Musse",
                "Felipe Vilanova",
                "Angelo Brandelli Costa"
            ],
            "title": "Detecting personality and emotion traits in crowds from video sequences",
            "venue": "Mach. Vis. Appl",
            "year": 2019
        },
        {
            "authors": [
                "Sebastian Feese",
                "Bert Arnrich",
                "Gerhard Troster",
                "Bertolt Meyer",
                "Klaus Jonas"
            ],
            "title": "Detecting Posture Mirroring in Social Interactions with Wearable Sensors",
            "venue": "Proc. of Int. Symp. on Wearable Computers,",
            "year": 2011
        },
        {
            "authors": [
                "Sebastian Feese",
                "Bert Arnrich",
                "Gerhard Tr\u00f6ster",
                "Bertolt Meyer",
                "Klaus Jonas"
            ],
            "title": "Quantifying Behavioral Mimicry by Automatic Detection of Nonverbal Cues from Body Motion",
            "venue": "In Proc. of IEEE SocialCom/PASSAT",
            "year": 2012
        },
        {
            "authors": [
                "Sebastian Feese",
                "Amir Muaremi",
                "Bert Arnrich",
                "Gerhard Tr\u00f6ster",
                "Bertolt Meyer",
                "Klaus Jonas"
            ],
            "title": "Discriminating Individually Considerate and Authoritarian Leaders by Speech Activity Cues",
            "venue": "In Proc. of IEEE SocialCom/PASSAT",
            "year": 2011
        },
        {
            "authors": [
                "Nikolaos Flemotomos",
                "Pavlos Papadopoulos",
                "James Gibson",
                "Shrikanth S Narayanan"
            ],
            "title": "Combined Speaker Clustering and Role Recognition in Conversational Speech",
            "venue": "In Proc. of INTERSPEECH",
            "year": 2018
        },
        {
            "authors": [
                "Simon Flutura",
                "Johannes Wagner",
                "Florian Lingenfelser",
                "Andreas Seiderer",
                "Elisabeth Andr\u00e9"
            ],
            "title": "MobileSSI: Asynchronous Fusion for Social Signal Interpretation in the Wild",
            "venue": "In Proc. of ACM ICMI",
            "year": 2016
        },
        {
            "authors": [
                "Gaurav Fotedar",
                "Aditya Gaonkar P",
                "Saikat Chatterjee",
                "Prasanta Kumar Ghosh"
            ],
            "title": "Automatic recognition of social roles using long term role transitions in small group interactions",
            "venue": "In Proc. of INTERSPEECH",
            "year": 2016
        },
        {
            "authors": [
                "Riccardo Franceschini",
                "Enrico Fini",
                "Cigdem Beyan",
                "Alessandro Conti",
                "Federica Arrigoni",
                "Elisa Ricci"
            ],
            "title": "Multimodal Emotion Recognition with Modality-Pairwise Unsupervised Contrastive Loss",
            "venue": "In Proc. of ICRP",
            "year": 2022
        },
        {
            "authors": [
                "Tian Gan",
                "Yongkang Wong",
                "Daqing Zhang",
                "Mohan S. Kankanhalli"
            ],
            "title": "Temporal Encoded F-Formation System for Social Interaction Detection",
            "venue": "In Proc. of ACMMM",
            "year": 2013
        },
        {
            "authors": [
                "Daniel Gatica-Perez"
            ],
            "title": "Automatic Nonverbal Analysis of Social Interaction in Small Groups: A Review",
            "venue": "Image and Vision Computing 27,",
            "year": 2009
        },
        {
            "authors": [
                "Ekin Gedik",
                "Hayley Hung"
            ],
            "title": "Detecting Conversing Groups Using Social Dynamics from Wearable Acceleration: Group Size Awareness",
            "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol",
            "year": 2018
        },
        {
            "authors": [
                "A.L. Georgescu",
                "S. Koeroglu",
                "AF de C Hamilton",
                "K. Vogeley",
                "C.M. Falter-Wagner",
                "W. Tschacher"
            ],
            "title": "Reduced nonverbal interpersonal synchrony in autism spectrum disorder independent of partner diagnosis: a motion energy study",
            "venue": "Molecular autism 11,",
            "year": 2020
        },
        {
            "authors": [
                "Panayiotis G Georgiou",
                "Matthew P Black",
                "Shrikanth S Narayanan"
            ],
            "title": "Behavioral signal processing for understanding (distressed) dyadic interactions: some recent developments",
            "venue": "In ACM workshop on Human gesture and behavior understanding",
            "year": 2011
        },
        {
            "authors": [
                "Shreya Ghosh",
                "Abhinav Dhall",
                "Nicu Sebe",
                "Tom Gedeon"
            ],
            "title": "Predicting group cohesiveness in images",
            "venue": "In Proc. of IJCNN",
            "year": 2019
        },
        {
            "authors": [
                "Theodoros Giannakopoulos"
            ],
            "title": "pyAudioAnalysis: An Open-Source Python Library for Audio Signal Analysis",
            "venue": "PloS one 10,",
            "year": 2015
        },
        {
            "authors": [
                "Michele Girolami",
                "Fabio Mavilia",
                "Franca Delmastro"
            ],
            "title": "Sensing social interactions through BLE beacons and commercial mobile devices",
            "venue": "Pervasive and Mobile Computing",
            "year": 2020
        },
        {
            "authors": [
                "Georg Groh",
                "Alexander Lehmann",
                "Jonas Reimers",
                "Marc Rene Friess",
                "Loren Schwarz"
            ],
            "title": "Detecting Social Situations from Interaction Geometry",
            "venue": "In IEEE Int. Conf. on Social Computing",
            "year": 2010
        },
        {
            "authors": [
                "Lauren V Hadley",
                "W Owen Brimijoin",
                "William M Whitmer"
            ],
            "title": "Speech, movement, and gaze behaviours during dyadic conversation in noise",
            "venue": "Scientific reports 9,",
            "year": 2019
        },
        {
            "authors": [
                "Juan Lorenzo Hagad",
                "Roberto Legaspi",
                "Masayuki Numao",
                "Merlin Suarez"
            ],
            "title": "Predicting Levels of Rapport in Dyadic Interactions through Automatic Detection of Posture and Posture Congruence",
            "venue": "In Proc. of IEEE SocialCom/PASSAT",
            "year": 2011
        },
        {
            "authors": [
                "Fasih Haider",
                "Saturnino Luz"
            ],
            "title": "A System for Real-Time Privacy Preserving Data Collection for Ambient Assisted Living",
            "venue": "In Proc. of Interspeech",
            "year": 2019
        },
        {
            "authors": [
                "Yuyun Huang",
                "Emer Gilmartin",
                "Nick Campbell"
            ],
            "title": "Speaker Dependency Analysis, Audiovisual Fusion Cues and a Multimodal BLSTM for Conversational Engagement Recognition",
            "venue": "INTERSPEECH",
            "year": 2017
        },
        {
            "authors": [
                "Hayley Hung",
                "Gokul Chittaranjan"
            ],
            "title": "The Idiap Wolf Corpus: Exploring Group Behaviour in a Competitive Role-Playing Game",
            "venue": "In Proc. of ACMMM",
            "year": 2010
        },
        {
            "authors": [
                "Hayley Hung",
                "Daniel Gatica-Perez"
            ],
            "title": "Estimating Cohesion in Small Groups Using Audio-Visual Nonverbal Behavior",
            "venue": "IEEE Trans. on Multimedia",
            "year": 2010
        },
        {
            "authors": [
                "Hayley Hung",
                "Yan Huang",
                "Gerald Friedland",
                "Daniel Gatica-Perez"
            ],
            "title": "Estimating Dominance in Multi-Party Meetings Using Speaker Diarization",
            "venue": "IEEE Trans. Audio, Speech, Language Process. 19,",
            "year": 2011
        },
        {
            "authors": [
                "Hayley Hung",
                "Gabriel Murray",
                "Giovanna Varni",
                "Nale Lehmann-Willenbrock",
                "Fabiola H. Gerpott",
                "Catharine Oertel"
            ],
            "title": "Workshop on Interdisciplinary Insights into Group and Team Dynamics",
            "venue": "In Proc. of ACM ICMI",
            "year": 2020
        },
        {
            "authors": [
                "Shoichi Inaba",
                "Yoshimitsu Aoki"
            ],
            "title": "Conversational Group Detection Based on Social Context Using Graph Clustering Algorithm",
            "venue": "In Proc. of SITIS",
            "year": 2016
        },
        {
            "authors": [
                "Ryo Ishii",
                "Kazuhiro Otsuka",
                "Shiro Kumano",
                "Junji Yamato"
            ],
            "title": "Analysis of Respiration for Prediction of \"Who Will Be Next Speaker and When?\" In Multi-Party Meetings",
            "venue": "ICMI",
            "year": 2014
        },
        {
            "authors": [
                "Dineshbabu Jayagopi",
                "Taemie Kim",
                "Alex Pentland",
                "Daniel Gatica-Perez"
            ],
            "title": "Privacy-sensitive recognition of group conversational context with sociometers",
            "venue": "Springer Multimedia Systems",
            "year": 2012
        },
        {
            "authors": [
                "Dinesh Babu Jayagopi",
                "Daniel Gatica-Perez"
            ],
            "title": "Mining group nonverbal conversational patterns using probabilistic topic models",
            "venue": "IEEE Trans. Multimedia 12,",
            "year": 2010
        },
        {
            "authors": [
                "Cao Jie",
                "Pan Peng"
            ],
            "title": "Recognize the most dominant person in multi-party meetings using nontraditional features",
            "venue": "In Proc. of ICIS",
            "year": 2010
        },
        {
            "authors": [
                "David Johnson",
                "Gabriel Murray"
            ],
            "title": "Clustering and Multimodal Analysis of Participants in Task-Based Discussions",
            "venue": "In Companion Publication of ACM ICMI",
            "year": 2021
        },
        {
            "authors": [
                "Hanbyul Joo",
                "Tomas Simon",
                "Xulong Li"
            ],
            "title": "Panoptic Studio: A Massively Multiview System for Social Interaction Capture",
            "venue": "IEEE Trans. on PAMI 41,",
            "year": 2019
        },
        {
            "authors": [
                "Kyriaki Kalimeri",
                "Bruno Lepri",
                "Oya Aran",
                "Dinesh Babu Jayagopi",
                "Daniel Gatica-Perez",
                "Fabio Pianesi"
            ],
            "title": "Modeling dominance effects on nonverbal behaviors using granger causality",
            "venue": "In Proc. of ACM ICMI",
            "year": 2012
        },
        {
            "authors": [
                "Kyriaki Kalimeri",
                "Bruno Lepri",
                "Taemie Kim",
                "Fabio Pianesi",
                "Alex Sandy Pentland"
            ],
            "title": "Automatic Modeling of Dominance Effects Using Granger Causality",
            "venue": "Human Behavior Understanding,",
            "year": 2011
        },
        {
            "authors": [
                "Kyriaki Kalimeri",
                "Bruno Lepri",
                "Fabio Pianesi"
            ],
            "title": "Causal-modelling of personality traits: extraversion and locus of control",
            "venue": "In 2nd Inter. Workshop on Social Signal Processing",
            "year": 2010
        },
        {
            "authors": [
                "F. Kamper"
            ],
            "title": "An Empirical Study of Gaussian Belief Propagation and Application in the Detection of F-Formations",
            "venue": "In Proc. of ACMMM",
            "year": 2017
        },
        {
            "authors": [
                "Oyku Kapcak",
                "Jose Vargas-Quiros",
                "Hayley Hung"
            ],
            "title": "Estimating Romantic, Social, and Sexual Attraction by Quantifying Bodily Coordination using Wearable Sensors",
            "venue": "In Proc. of ACII",
            "year": 2019
        },
        {
            "authors": [
                "Kleomenis Katevas",
                "Hamed Haddadi",
                "Laurissa Tokarchuk",
                "Richard G. Clegg"
            ],
            "title": "Detecting Group Formations Using IBeacon Technology",
            "venue": "In Proc. of ACM UbiComp",
            "year": 2016
        },
        {
            "authors": [
                "Kleomenis Katevas",
                "Katrin H\u00e4nsel",
                "Richard Clegg",
                "Ilias Leontiadis",
                "Hamed Haddadi",
                "Laurissa Tokarchuk"
            ],
            "title": "Finding Dory in the Crowd: Detecting Social Interactions Using Multi-Modal Mobile Sensing",
            "venue": "In Proc. of SenSys-ML",
            "year": 2019
        },
        {
            "authors": [
                "Tatsuya Kawahara",
                "Soichiro Hayashi",
                "Katsuya Takanashi"
            ],
            "title": "Estimation of interest and comprehension level of audience through multi-modal behaviors in poster conversations",
            "venue": "In Proc. of INTERSPEECH",
            "year": 2013
        },
        {
            "authors": [
                "Shahid Nawaz Khan",
                "Maitree Leekha",
                "Jainendra Shukla",
                "Rajiv Ratn Shah"
            ],
            "title": "Vyaktitv: A multimodal peer-to-peer hindi conversations based dataset for personality assessment",
            "venue": "In IEEE BigMM",
            "year": 2020
        },
        {
            "authors": [
                "Eunju Kim",
                "Sumi Helal",
                "Diane Cook"
            ],
            "title": "Human Activity Recognition and Pattern Discovery",
            "venue": "IEEE Pervasive Comput. 9,",
            "year": 2010
        },
        {
            "authors": [
                "Ahmet Alp Kindiroglu",
                "Lale Akarun",
                "Oya Aran"
            ],
            "title": "Multi-domain and multi-task prediction of extraversion and leadership from meeting videos",
            "venue": "EURASIP JIVP",
            "year": 2017
        },
        {
            "authors": [
                "Mikkel Baun Kj\u00e6rgaard",
                "Henrik Blunck"
            ],
            "title": "Tool Support for Detection and Analysis of Following and Leadership Behavior of Pedestrians from Mobile Sensing Data",
            "venue": "Pervasive Mob. Comput",
            "year": 2014
        },
        {
            "authors": [
                "M.A. Klados",
                "P. Konstantinidi",
                "R. Dacosta-Aguayo",
                "V.-D. Kostaridou",
                "A. Vinciarelli",
                "M. Zervakis"
            ],
            "title": "Automatic recognition of personality profiles using EEG functional connectivity during emotional processing",
            "venue": "Brain sciences 10,",
            "year": 2020
        },
        {
            "authors": [
                "Panagiotis Koromilas",
                "Theodoros Giannakopoulos"
            ],
            "title": "Unsupervised Multimodal Language Representations using Convolutional Autoencoders",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "M. Koutsombogera",
                "C. Vogel"
            ],
            "title": "Modeling Collaborative Multimodal Behavior in Group Dialogues: The MULTISIMO Corpus",
            "year": 2018
        },
        {
            "authors": [
                "Uliyana Kubasova",
                "Gabriel Murray",
                "McKenzie Braley"
            ],
            "title": "Analyzing Verbal and Nonverbal Features for Predicting Group Performance",
            "venue": "In Proc. of INTERSPEECH",
            "year": 2019
        },
        {
            "authors": [
                "S. Kumano",
                "K. Otsuka",
                "R. Ishii",
                "J. Yamato"
            ],
            "title": "Automatic gaze analysis in multiparty conversations based on Collective First-Person Vision",
            "venue": "In Proc. of IEEE FG,",
            "year": 2015
        },
        {
            "authors": [
                "Juha M. Lahnakoski",
                "Paul A.G. Forbes",
                "Cade McCall",
                "Leonhard Schilbach"
            ],
            "title": "Unobtrusive tracking of interpersonal orienting and distance predicts the subjective quality of social interactions",
            "venue": "Royal Society Open Science",
            "year": 2020
        },
        {
            "authors": [
                "Catherine Lai",
                "Gabriel Murray"
            ],
            "title": "Predicting Group Satisfaction in Meeting Discussions",
            "venue": "In Proc. of the Workshop on Modeling Cognitive Processes from Multimodal Data",
            "year": 2018
        },
        {
            "authors": [
                "Chi-Chun Lee",
                "Matthew Black",
                "Athanasios"
            ],
            "title": "Quantification of prosodic entrainment in affective spontaneous spoken interactions of married couples",
            "venue": "Katsamanis",
            "year": 2010
        },
        {
            "authors": [
                "Chi-Chun Lee",
                "Athanasios Katsamanis",
                "Brian R Baucom",
                "Panayiotis G Georgiou",
                "Shrikanth S Narayanan"
            ],
            "title": "Using measures of vocal entrainment to inform outcome-related behaviors in marital conflicts",
            "venue": "In Proc. of APSIPA",
            "year": 2012
        },
        {
            "authors": [
                "Bruno Lepri",
                "Kyriaki Kalimeri",
                "Fabio Pianesi"
            ],
            "title": "Honest Signals and Their Contribution to the Automatic Analysis of Personality Traits A Comparative Study",
            "venue": "Human Behavior Understanding, ser. Lecture Notes in Computer Science,",
            "year": 2010
        },
        {
            "authors": [
                "B. Lepri",
                "J. Staiano",
                "G. Rigato",
                "K. Kalimeri",
                "A. Finnerty",
                "F. Pianesi",
                "N. Sebe",
                "A. Pentland"
            ],
            "title": "The SocioMetric Badges Corpus: A Multilevel Behavioral Dataset for Social Behavior in Complex Organizations",
            "venue": "In Proc. of IEEE SocialCom/PASSAT",
            "year": 2012
        },
        {
            "authors": [
                "Bruno Lepri",
                "Ramanathan Subramanian",
                "Kyriaki Kalimeri",
                "Jacopo Staiano",
                "Fabio Pianesi",
                "Nicu Sebe"
            ],
            "title": "Connecting Meeting Behavior with Extraversion-A Systematic Study",
            "venue": "IEEE Trans. on Affective Comput",
            "year": 2012
        },
        {
            "authors": [
                "Bruno Lepri",
                "Ramanathan Subramanian",
                "Kyriaki Kalimeri",
                "Jacopo Staiano",
                "Fabio Pianesi",
                "Nicu Sebe"
            ],
            "title": "Employing social gaze and speaking activity for automatic determination of the Extraversion trait",
            "venue": "In Proc. of ACM ICMI-MLMI",
            "year": 2013
        },
        {
            "authors": [
                "Hang Li",
                "Julien Epps",
                "Siyuan Chen"
            ],
            "title": "Think before you speak: An investigation of eye activity patterns during conversations using eyewear",
            "venue": "Int. Journal of Human-Computer Studies",
            "year": 2020
        },
        {
            "authors": [
                "Yun-Shao Lin",
                "Chi-Chun Lee"
            ],
            "title": "Using Interlocutor-Modulated Attention BLSTM to Predict Personality Traits in Small Group Interaction",
            "venue": "In Proc. of ACM ICMI",
            "year": 2016
        },
        {
            "authors": [
                "Yun-Shao Lin",
                "Chi-Chun Lee"
            ],
            "title": "Predicting Performance Outcome with a Conversational Graph Convolutional Network for Small Group Interactions",
            "venue": "In Proc. of IEEE ICASSP",
            "year": 2020
        },
        {
            "authors": [
                "Xinchen Liu",
                "Wu Liu",
                "Meng Zhang",
                "Jingwen Chen",
                "Lianli Gao",
                "Chenggang Yan",
                "Tao Mei"
            ],
            "title": "Social Relation Recognition From Videos via Multi-Scale Spatial-Temporal Reasoning",
            "venue": "In Proc. of CVPR",
            "year": 2019
        },
        {
            "authors": [
                "Nichola Lubold",
                "Heather Pon-Barry"
            ],
            "title": "Acoustic-prosodic entrainment and rapport in collaborative learning dialogues",
            "venue": "In Proc. ACM Multimodal Learning Analytics Workshop and Grand Challenge",
            "year": 2014
        },
        {
            "authors": [
                "Scott M Lundberg",
                "Su-In Lee"
            ],
            "title": "A Unified Approach to Interpreting Model Predictions",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jinna Lv",
                "Wu Liu",
                "Lili Zhou",
                "Bin Wu",
                "Huadong Ma"
            ],
            "title": "Multi-stream Fusion Model for Social Relation Recognition from Videos",
            "venue": "In MultiMedia",
            "year": 2018
        },
        {
            "authors": [
                "Jinna Lv",
                "Bin Wu"
            ],
            "title": "Spatio-Temporal Attention Model Based on Multi-view for Social Relation Understanding",
            "venue": "In MultiMedia Modeling",
            "year": 2019
        },
        {
            "authors": [
                "Lucien Maman",
                "Elenora Ceccaldi",
                "Nale Lehmann-Willenbrock",
                "Laurence Likforman-Sulem",
                "Mohamed Chetouani",
                "Gualtiero Volpe",
                "Giovanna Varni"
            ],
            "title": "GAME-ON: A Multimodal Dataset for Cohesion and Group Analysis",
            "venue": "IEEE Access",
            "year": 2020
        },
        {
            "authors": [
                "N. Marquardt",
                "K. Hinckley",
                "S. Greenberg"
            ],
            "title": "Cross-Device Interaction via Micro-Mobility and f-Formations",
            "venue": "In Proc. of ACM UIST",
            "year": 2012
        },
        {
            "authors": [
                "Aleksandar Matic",
                "Venet Osmani",
                "Oscar Mayora-Ibarra"
            ],
            "title": "Mobile Monitoring of Formal and Informal Social Interactions at Workplace",
            "venue": "In Proc. of ACM UbiComp",
            "year": 2014
        },
        {
            "authors": [
                "Yash Mehta",
                "Navonil Majumder",
                "Alexander Gelbukh",
                "Erik Cambria"
            ],
            "title": "Recent trends in deep learning based personality detection",
            "venue": "Artificial Intelligence Review",
            "year": 2020
        },
        {
            "authors": [
                "Chreston Miller",
                "Christa Miller"
            ],
            "title": "Timing is Everything: Identifying Diverse Interaction Dynamics in Scenario and Non-Scenario Meetings",
            "venue": "In International Conference on eScience",
            "year": 2019
        },
        {
            "authors": [
                "Juan Abdon Miranda Correa",
                "Mojtaba Khomami Abadi",
                "Nicu Sebe",
                "Ioannis Patras"
            ],
            "title": "AMIGOS: A Dataset for Affect, Personality and Mood Research on Individuals and Groups",
            "venue": "IEEE Trans. on Affective Comput",
            "year": 2018
        },
        {
            "authors": [
                "Go Miura",
                "Shogo Okada"
            ],
            "title": "Task-Independent Multimodal Prediction of Group Performance Based on Product Dimensions",
            "venue": "In Proc. of ACM ICMI",
            "year": 2019
        },
        {
            "authors": [
                "Ali Mollahosseini",
                "Behzad Hasani",
                "Mohammad H Mahoor"
            ],
            "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
            "venue": "IEEE Trans. on Affective Comput. 10,",
            "year": 2017
        },
        {
            "authors": [
                "Philipp Muller",
                "Michael Xuelin Huang",
                "Andreas Bulling"
            ],
            "title": "Detecting Low Rapport During Natural Interactions in Small Groups from Non-Verbal Behaviour",
            "venue": "In Proc. of Inter. Conf. on Intelligent User Interfaces",
            "year": 2018
        },
        {
            "authors": [
                "Philipp Matthias Muller",
                "Andreas Bulling"
            ],
            "title": "Emergent Leadership Detection Across Datasets",
            "year": 2019
        },
        {
            "authors": [
                "Skanda Muralidhar",
                "Marianne Schmid Mast",
                "Daniel Gatica-Perez"
            ],
            "title": "A Tale of Two Interactions: Inferring Performance in Hospitality Encounters from Cross-Situation Social Sensing",
            "venue": "ACM Interact. Mob. Wearable Ubiquitous Technol",
            "year": 2018
        },
        {
            "authors": [
                "Gabriel Murray",
                "Catharine Oertel"
            ],
            "title": "Predicting Group Performance in Task-Based Interaction",
            "venue": "In Proc. of ACM ICMI",
            "year": 2018
        },
        {
            "authors": [
                "Iftekhar Naim",
                "M. Iftekhar Tanveer",
                "Daniel Gildea",
                "Mohammed Ehsan Hoque"
            ],
            "title": "Automated prediction and analysis of job interview performance: The role of what you say and how you say it",
            "venue": "In Proc. of IEEE FG,",
            "year": 2015
        },
        {
            "authors": [
                "Marjolein C. Nanninga",
                "Yanxia Zhang",
                "Nale Lehmann-Willenbrock",
                "Zolt\u00e1n Szl\u00e1vik",
                "Hayley Hung"
            ],
            "title": "Estimating Verbal Expressions of Task and Social Cohesion in Meetings by Quantifying Paralinguistic Mimicry",
            "venue": "In Proc. of ACM ICMI",
            "year": 2017
        },
        {
            "authors": [
                "Laurent Son Nguyen",
                "Denise Frauendorfer",
                "Marianne Schmid Mast",
                "Daniel Gatica-Perez"
            ],
            "title": "Hire me: Computational Inference of Hirability in Employment Interviews Based on Nonverbal Behavior",
            "venue": "IEEE Trans. Multimedia 16,",
            "year": 2014
        },
        {
            "authors": [
                "Radoslaw Niewiadomski",
                "Lea Chauvigne",
                "Maurizio Mancini",
                "Antonio Camurri"
            ],
            "title": "Towards a Model of Nonverbal Leadership in Unstructured Joint Physical Activity",
            "venue": "In Proc. of MOCO. Manuscript submitted to ACM Co-Located Human-Human Interaction Analysis using Nonverbal Cues: A Survey",
            "year": 2018
        },
        {
            "authors": [
                "Fumio Nihei",
                "Yukiko I. Nakano",
                "Yuki Hayashi",
                "Hung-Hsuan Hung",
                "Shogo Okada"
            ],
            "title": "Predicting Influential Statements in Group Discussions Using Speech and Head Motion Information",
            "venue": "In Proc. of ACM ICMI",
            "year": 2014
        },
        {
            "authors": [
                "Catharine Oertel",
                "Fred Cummins",
                "Jens Edlund",
                "Petra Wagner",
                "Nick Campbell"
            ],
            "title": "D64: a corpus of richly recorded conversational interaction",
            "venue": "Journal on Multimodal User Interfaces",
            "year": 2012
        },
        {
            "authors": [
                "Catharine Oertel",
                "Kenneth A. Funes Mora",
                "Joakim Gustafson",
                "Jean-Marc Odobez"
            ],
            "title": "Deciphering the Silent Participant: On the Use of Audio-Visual Cues for the Classification of Listener Categories in Group Discussions",
            "venue": "In Proc. of ACM ICMI",
            "year": 2015
        },
        {
            "authors": [
                "Catharine Oertel",
                "Kenneth A. Funes Mora",
                "Samira Sheikhi",
                "Jean-Marc Odobez",
                "Joakim Gustafson"
            ],
            "title": "Who Will Get the Grant? A Multimodal Corpus for the Analysis of Conversational Behaviours in Group Interviews",
            "year": 2014
        },
        {
            "authors": [
                "Catharine Oertel",
                "Giampiero Salvi"
            ],
            "title": "A Gaze-Based Method for Relating Group Involvement to Individual Engagement in Multimodal Multiparty Dialogue",
            "venue": "In Proc. of ACM ICMI",
            "year": 2013
        },
        {
            "authors": [
                "Catharine Oertel",
                "Giampiero Salvi",
                "Jana Gotze",
                "Jens Edlund",
                "Joakim Gustafson",
                "Mattias Heldner"
            ],
            "title": "n.d.]. The KTH games corpora: How to catch a werewolf",
            "venue": "In In Multimodal-Corpora:Beyond Audio and Gaze,",
            "year": 2013
        },
        {
            "authors": [
                "Ayumi Ohnishi",
                "Kaoru Saito",
                "Tsutomu Terada",
                "Masahiko Tsukamoto"
            ],
            "title": "Toward Interest Estimation from Head Motion Using Wearable Sensors: A Case Study in Story Time for Children",
            "venue": "In Human-Computer Interaction. Interaction Contexts",
            "year": 2017
        },
        {
            "authors": [
                "Shogo Okada",
                "Oya Aran",
                "Daniel Gatica-Perez"
            ],
            "title": "Personality Trait Classification via Co-Occurrent Multiparty Multimodal Event Discovery",
            "venue": "In Proc. of ACM ICMI",
            "year": 2015
        },
        {
            "authors": [
                "Shogo Okada",
                "Laurent Son Nguyen",
                "Oya Aran",
                "Daniel Gatica-Perez"
            ],
            "title": "Modeling Dyadic and Group Impressions with Intermodal and Interperson Features",
            "venue": "ACM Trans. Multimedia Comput. Commun. Appl. 15,",
            "year": 2019
        },
        {
            "authors": [
                "Toshiki Onishi",
                "Arisa Yamauchi",
                "Ryo Ishii",
                "Yushi Aono",
                "Akihiro Miyata"
            ],
            "title": "Analyzing Nonverbal Behaviors along with Praising",
            "venue": "In Proc. of ACM ICMI",
            "year": 2020
        },
        {
            "authors": [
                "Ehsan Othman",
                "Frerk Saxen",
                "Dmitri Bershadskyy",
                "Philipp Werner",
                "Ayoub Al-Hamadi",
                "Joachim Weimann"
            ],
            "title": "Predicting Group Contribution Behaviour in a Public Goods Game from Face-to-Face Communication",
            "venue": "Sensors 19,",
            "year": 2019
        },
        {
            "authors": [
                "Cristina Palmero",
                "Javier Selva",
                "Sorina Smeureanu",
                "Julio C.S. Jacques"
            ],
            "title": "Context-Aware Personality Inference in Dyadic Scenarios: Introducing the UDIVA Dataset",
            "venue": "Junior",
            "year": 2021
        },
        {
            "authors": [
                "Giancarlo Paoletti",
                "Jacopo Cavazza",
                "Cigdem Beyan",
                "Alessio Del Bue"
            ],
            "title": "Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning",
            "year": 2021
        },
        {
            "authors": [
                "Giancarlo Paoletti",
                "Jacopo Cavazza",
                "Cigdem Beyan",
                "Alessio Del Bue"
            ],
            "title": "Unsupervised Human Action Recognition with Skeletal Graph Laplacian and Self-Supervised Viewpoints Invariance",
            "venue": "In Proc. of BMVC",
            "year": 2021
        },
        {
            "authors": [
                "Sunghyun Park",
                "Jonathan Gratch",
                "Louis-Philippe Morency"
            ],
            "title": "I already know your answer: Using nonverbal behaviors to predict immediate outcomes in a dyadic negotiation",
            "venue": "In Proc. ACM ICMI",
            "year": 2012
        },
        {
            "authors": [
                "Sunghyun Park",
                "Stefan Scherer",
                "Jonathan Gratch",
                "Peter Carnevale",
                "Louis-Philippe Morency"
            ],
            "title": "Mutual behaviors during dyadic negotiation: Automatic prediction of respondent reactions",
            "venue": "In Proc. of ACII",
            "year": 2013
        },
        {
            "authors": [
                "Debprakash Patnaik",
                "P.S. Sastry",
                "K.P. Unnikrishnan"
            ],
            "title": "Inferring Neuronal Network Connectivity from Spike Data: A Temporal Data Mining Approach",
            "venue": "Sci. Program",
            "year": 2008
        },
        {
            "authors": [
                "Alex Pentland"
            ],
            "title": "Social Signal Processing",
            "venue": "IEEE Signal Processing Magazine 24,",
            "year": 2007
        },
        {
            "authors": [
                "Vignesh Ramanathan",
                "Bangpeng Yao",
                "Li Fei-Fei"
            ],
            "title": "Social Role Discovery in Human Events",
            "venue": "In Proc. of IEEE CVPR",
            "year": 2013
        },
        {
            "authors": [
                "Sowmya Rasipuram",
                "Dinesh Babu Jayagopi"
            ],
            "title": "Automatic multimodal assessment of soft skills in social interactions: a review",
            "venue": "Multim. Tools Appl",
            "year": 2020
        },
        {
            "authors": [
                "Elisa Ricci",
                "Jagannadan Varadarajan",
                "Ramanathan Subramanian",
                "Samuel Rota Bul\u00f2",
                "Narendra Ahuja",
                "Oswald Lanz"
            ],
            "title": "Uncovering Interactions and Interactors: Joint Estimation of Head, Body Orientation and F-Formations from Surveillance Videos",
            "venue": "In Proc. of IEEE ICCV",
            "year": 2015
        },
        {
            "authors": [
                "Jorge Rios-Martinez",
                "Anne Spalanzani",
                "Christian Laugier"
            ],
            "title": "From Proxemics Theory to Socially-Aware Navigation: A Survey",
            "venue": "Int. Journal of Social Robotics",
            "year": 2015
        },
        {
            "authors": [
                "Alessio Rosatelli",
                "Ekin Gedik",
                "Hayley Hung"
            ],
            "title": "Detecting F-formations Roles in Crowded Social Scenes with Wearables: Combining Proxemics Dynamics using LSTMs",
            "venue": "In Proc. of ACII",
            "year": 2019
        },
        {
            "authors": [
                "J.B. Rotter"
            ],
            "title": "Generalized expectancies for internal versus external control of reinforcement",
            "venue": "Psychological monographs",
            "year": 1966
        },
        {
            "authors": [
                "Viktor Rozgic",
                "Bo Xiao",
                "Athanasios Katsamanis",
                "Brian R Baucom",
                "Panayiotis G Georgiou",
                "Shrikanth S Narayanan"
            ],
            "title": "A new multichannel multi modal dyadic interaction database",
            "venue": "In Proc. of INTERSPEECH",
            "year": 2010
        },
        {
            "authors": [
                "Sina Sajadmanesh",
                "Daniel Gatica-Perez"
            ],
            "title": "Locally Private Graph Neural Networks",
            "venue": "In Proc. of ACM CCS",
            "year": 2021
        },
        {
            "authors": [
                "Dairazalia Sanchez-Cortes",
                "Oya Aran",
                "Dinesh Babu Jayagopi",
                "Marianne Schmid Mast",
                "Daniel Gatica-Perez"
            ],
            "title": "Emergent Leaders through Looking and Speaking: from Audio-Visual Data to Multimodal Recognition",
            "venue": "Journal on Multimodal User Interfaces",
            "year": 2012
        },
        {
            "authors": [
                "Dairazalia Sanchez-Cortes",
                "Oya Aran",
                "Marianne Schmid Mast",
                "Daniel Gatica-Perez"
            ],
            "title": "Identifying emergent leadership in small groups using nonverbal communicative cues",
            "venue": "In Proc. of ACM ICMI-MLMI. 8\u201310. Manuscript submitted to ACM",
            "year": 2010
        },
        {
            "authors": [
                "Dairazalia Sanchez-Cortes",
                "Oya Aran",
                "Marianne Schmid Mast",
                "Daniel Gatica-Perez"
            ],
            "title": "A Nonverbal Behavior Approach to Identify Emergent Leaders in Small Groups",
            "venue": "IEEE Trans. Multimedia 14,",
            "year": 2012
        },
        {
            "authors": [
                "Ashtosh Sapru",
                "Herve Bourlard"
            ],
            "title": "Automatic Social Role Recognition In Professional Meetings Using Conditional Random Fields",
            "venue": "In Proc. of INTERSPEECH",
            "year": 2013
        },
        {
            "authors": [
                "Ashtosh Sapru",
                "Herve Bourlard"
            ],
            "title": "Automatic recognition of emergent social roles in small group interactions",
            "venue": "IEEE Trans. Multimedia 17,",
            "year": 2015
        },
        {
            "authors": [
                "Ramprasaath R. Selvaraju",
                "Michael Cogswell",
                "Abhishek Das",
                "Ramakrishna Vedantam",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization",
            "venue": "In Proc. of IEEE ICCV",
            "year": 2017
        },
        {
            "authors": [
                "Francesco Setti",
                "Oswald Lanz",
                "Roberta Ferrario",
                "Vittorio Murino",
                "Marco Cristani"
            ],
            "title": "Multi-scale f-formation discovery for group detection",
            "venue": "In Proc. of IEEE ICIP",
            "year": 2013
        },
        {
            "authors": [
                "M. Shahid",
                "C. Beyan",
                "V. Murino"
            ],
            "title": "S-VVAD: Visual Voice Activity Detection by Motion Segmentation",
            "venue": "In Proc. of IEEE WACV",
            "year": 2021
        },
        {
            "authors": [
                "G. Sharma",
                "S. Ghosh",
                "A. Dhall"
            ],
            "title": "Automatic Group Level Affect and Cohesion Prediction in Videos",
            "venue": "In Proc. of ACII",
            "year": 2019
        },
        {
            "authors": [
                "Julio C. Silveira Jacques Junior",
                "Yagmur G\u00fc\u00e7l\u00fct\u00fcrk",
                "Marc Perez"
            ],
            "title": "First Impressions: A Survey on Vision-based Apparent Personality Trait Analysis",
            "venue": "IEEE Trans.on Affective Comput",
            "year": 2019
        },
        {
            "authors": [
                "Francesco Solera",
                "Simone Calderara",
                "Rita Cucchiara"
            ],
            "title": "Learning to identify leaders in crowd",
            "venue": "In IEEE CVPRw",
            "year": 2015
        },
        {
            "authors": [
                "Yale Song",
                "Louis-Philippe Morency",
                "Randall Davis"
            ],
            "title": "Multimodal human behavior analysis: learning correlation and interaction across modalities",
            "venue": "In Proc. ACM ICMI",
            "year": 2012
        },
        {
            "authors": [
                "Ruchir Srivastava",
                "Jiashi Feng",
                "Sujoy Roy",
                "Shuicheng Yan",
                "Terence Sim"
            ],
            "title": "Don\u2019t Ask Me What i\u2019m like, Just Watch and Listen",
            "venue": "In Proc. of ACMMM",
            "year": 2012
        },
        {
            "authors": [
                "Jacopo Staiano",
                "Bruno Lepri",
                "Kyriaki Kalimeri",
                "Nicu Sebe",
                "Fabio Pianesi"
            ],
            "title": "Contextual Modeling of Personality States? Dynamics in Face-to-Face Interactions",
            "venue": "In Proc. of PASSAT/SocialCom",
            "year": 2011
        },
        {
            "authors": [
                "Jacopo Staiano",
                "Bruno Lepri",
                "Ramanathan Subramanian",
                "Nicu Sebe",
                "Fabio Pianesi"
            ],
            "title": "Automatic modeling of personality states in small group interactions",
            "venue": "In Proc. of ACMMM",
            "year": 2011
        },
        {
            "authors": [
                "Alexandros Stergiou",
                "Ronald Poppe"
            ],
            "title": "Analyzing human\u2013human interactions: A survey",
            "venue": "CVIU",
            "year": 2019
        },
        {
            "authors": [
                "Ramanathan Subramanian",
                "Jagannadan Varadarajan",
                "Elisa Ricci",
                "Oswald Lanz",
                "Stefan Winkler"
            ],
            "title": "Jointly Estimating Interactions and Head, Body Pose of Interactors from Distant Social Scenes",
            "venue": "In Proc. of ACMMM",
            "year": 2015
        },
        {
            "authors": [
                "Ramanathan Subramanian",
                "Yan Yan",
                "Jacopo Staiano",
                "Oswald Lanz",
                "Nicu Sebe"
            ],
            "title": "On the relationship between head pose social attention and personality prediction for unstructured and dynamic group interactions",
            "venue": "In Proc. of ACM ICMI",
            "year": 2013
        },
        {
            "authors": [
                "Noriko Suzuki",
                "Tosirou Kamiya",
                "Ichiro Umata",
                "Sadanori Ito",
                "Shoichiro Iwasawa",
                "Mamiko Sakata",
                "Katsunori Shimohara"
            ],
            "title": "Detection of Division of Labor in Multiparty Collaboration. In Human Interface and the Management of Information. Information and Interaction for Learning, Culture, Collaboration and Business",
            "year": 2013
        },
        {
            "authors": [
                "Lucia Teijeiro-Mosquera",
                "Joan-Isaac Biel",
                "Jose Luis Alba-Castro",
                "Daniel Gatica-Perez"
            ],
            "title": "What Your Face Vlogs About: Expressions of Emotion and Big-Five Traits Impressions in YouTube",
            "venue": "IEEE Trans. on Affective Computing",
            "year": 2015
        },
        {
            "authors": [
                "Juan R. Terven",
                "Bogdan Raducanu",
                "Mar\u00eda Elena Meza de Luna",
                "Joaqu\u00edn Salas"
            ],
            "title": "Head-gestures mirroring detection in dyadic social interactions with computer vision-based wearable",
            "venue": "devices. Neurocomputing",
            "year": 2016
        },
        {
            "authors": [
                "Sanket Kumar Thakur",
                "Cigdem Beyan",
                "Pietro Morerio",
                "Alessio Del Bue"
            ],
            "title": "Predicting Gaze from Egocentric Social Interaction Videos and IMU Data",
            "venue": "In ACM ICMI",
            "year": 2021
        },
        {
            "authors": [
                "Tony Tung",
                "Randy Gomez",
                "Tatsuya Kawahara",
                "Takashi Matsuyama"
            ],
            "title": "Group Dynamics and Multimodal Interaction Modeling Using a Smart Digital Signage",
            "venue": "In Proc. of ECCVw",
            "year": 2012
        },
        {
            "authors": [
                "T. Tung",
                "R. Gomez",
                "T. Kawahara",
                "T. Matsuyama"
            ],
            "title": "Multiparty Interaction Understanding Using Smart Multimodal Digital Signage",
            "venue": "IEEE Trans.s on Human-Machine Systems 44,",
            "year": 2014
        },
        {
            "authors": [
                "Fabio Valente",
                "Samuel Kim",
                "Petr Motlicek"
            ],
            "title": "Annotation and recognition of personality traits in spoken conversations from the AMI meetings corpus",
            "venue": "In Proc. of INTERSPEECH",
            "year": 2012
        },
        {
            "authors": [
                "Jagannadan Varadarajan",
                "Ramanathan Subramanian",
                "Samuel Rota Bul\u00f2",
                "Narendra Ahuja",
                "Oswald Lanz",
                "Elisa Ricci"
            ],
            "title": "Joint Estimation of Human Pose and Conversational Groups from Social Scenes",
            "venue": "Proc. of ICCV",
            "year": 2018
        },
        {
            "authors": [
                "Arno Veenstra",
                "Hayley Hung"
            ],
            "title": "Do they like me? Using video cues to predict desires during speed-dates",
            "venue": "In IEEE ICCV Workshops",
            "year": 2011
        },
        {
            "authors": [
                "Alessandro Vinciarelli",
                "Gelareh Mohammadi"
            ],
            "title": "A Survey of Personality Computing",
            "venue": "IEEE Trans. on Affective Comput. 5,",
            "year": 2014
        },
        {
            "authors": [
                "Alessandro Vinciarelli",
                "Maja Pantic",
                "Herv\u00e9 Bourlard"
            ],
            "title": "Social signal processing: Survey of an emerging domain",
            "venue": "Image and Vision Computing 27,",
            "year": 2009
        },
        {
            "authors": [
                "Alessandro Vinciarelli",
                "Fabio Valente",
                "Sree Harsha Yella",
                "Ashtosh Sapru"
            ],
            "title": "Understanding Social Signals in Multi-party Conversations: Automatic Recognition of Socio-Emotional Roles in the AMI Meeting Corpus",
            "venue": "In Proc. of IEEE SMC",
            "year": 2011
        },
        {
            "authors": [
                "Limin Wang",
                "Yuanjun Xiong",
                "Zhe Wang",
                "Yu Qiao",
                "Dahua Lin",
                "Xiaoou Tang",
                "Luc Van Gool"
            ],
            "title": "Temporal segment networks: Towards good practices for deep action recognition",
            "year": 2016
        },
        {
            "authors": [
                "William Yang Wang",
                "Fadi Biadsy",
                "Andrew Rosenberg",
                "Julia Hirschberg"
            ],
            "title": "Automatic detection of speaker state: Lexical, prosodic, and phonetic approaches to level-of-interest and intoxication classification",
            "venue": "Computer Speech & Language 27,",
            "year": 2013
        },
        {
            "authors": [
                "Yanbang Wang",
                "Pan Li",
                "Chongyang Bai",
                "VS Subrahmanian",
                "Jure Leskovec"
            ],
            "title": "Generic Representation Learning for Dynamic Social Interaction",
            "venue": "In Proc. of ACM SIGKDD Int. Conf. on KDDM MLG Workshop",
            "year": 2020
        },
        {
            "authors": [
                "Bo Xiao",
                "Daniel Bone",
                "Maarten Van Segbroeck",
                "Zac E Imel",
                "David C Atkins",
                "Panayiotis G Georgiou",
                "Shrikanth S Narayanan"
            ],
            "title": "Modeling therapist empathy through prosody in drug addiction counseling",
            "venue": "In Proc. of ISCA",
            "year": 2014
        },
        {
            "authors": [
                "Bo Xiao",
                "Panayiotis Georgiou",
                "Brian Baucom",
                "Shrikanth Narayanan"
            ],
            "title": "Multimodal detection of salient behaviors of approach-avoidance in dyadic interactions",
            "venue": "In Proc. of ACM ICMI",
            "year": 2012
        },
        {
            "authors": [
                "Bo Xiao",
                "Panayiotis G Georgiou",
                "Zac E Imel",
                "David C Atkins",
                "Shrikanth S Narayanan"
            ],
            "title": "Modeling therapist empathy and vocal entrainment in drug addiction counseling",
            "venue": "In Proc. of INTERSPEECH",
            "year": 2013
        },
        {
            "authors": [
                "Bo Xiao",
                "Zac E Imel",
                "David C Atkins",
                "Panayiotis G Georgiou",
                "Shrikanth S Narayanan"
            ],
            "title": "Analyzing speech rate entrainment and its relation to therapist empathy in drug addiction counseling",
            "venue": "In Proc. of ISCA",
            "year": 2015
        },
        {
            "authors": [
                "Sumi Yasuyuki",
                "Yano Masaharu",
                "Nishida Toyoaki"
            ],
            "title": "Analysis Environment of Conversational Structure with Nonverbal Multimodal Data",
            "venue": "In Proc. of ACM ICMI-MLMI",
            "year": 2010
        },
        {
            "authors": [
                "A. Zadeh",
                "P. Pu"
            ],
            "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
            "venue": "In Proc. of ACL",
            "year": 2018
        },
        {
            "authors": [
                "Gloria Zen",
                "Bruno Lepri",
                "Elisa Ricci",
                "Oswald Lanz"
            ],
            "title": "Space Speaks: Towards Socially and Personality Aware Visual Surveillance",
            "venue": "In Proc. of ACM MPVA",
            "year": 2010
        },
        {
            "authors": [
                "Lingyu Zhang",
                "Indrani Bhattacharya",
                "Mallory"
            ],
            "title": "Multiparty Visual Co-Occurrences for Estimating Personality Traits in Group Meetings",
            "venue": "In Proc. of IEEE WACV",
            "year": 2020
        },
        {
            "authors": [
                "Lingyu Zhang",
                "Mallory Morgan",
                "Indrani et al. Bhattacharya"
            ],
            "title": "Improved Visual Focus of Attention Estimation and Prosodic Features for Analyzing Group Interactions",
            "venue": "In Proc. of ACM ICMI",
            "year": 2019
        },
        {
            "authors": [
                "Lingyu Zhang",
                "Richard J. Radke"
            ],
            "title": "A Multi-Stream Recurrent Neural Network for Social Role Detection in Multiparty Interactions",
            "venue": "IEEE Journal of Selected Topics in Signal Processing 14,",
            "year": 2020
        },
        {
            "authors": [
                "Yanxia Zhang",
                "Jeffrey Olenick",
                "Chu-Hsiang Chang",
                "Steve W.J. Kozlowski",
                "Hayley Hung"
            ],
            "title": "The I in Team: Mining Personal Social Interaction Routine with Topic Models from Long-Term Team Data",
            "venue": "In Proc. of IUI",
            "year": 2018
        },
        {
            "authors": [
                "Yanxia Zhang",
                "Jeffrey Olenick",
                "Chu-Hsiang Chang",
                "Steve W.J. Kozlowski",
                "Hayley Hung"
            ],
            "title": "TeamSense: Assessing Personal Affect and Group Cohesion in Small Teams through Dyadic Interaction and Behavior Analysis with Wearable Sensors",
            "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol",
            "year": 2018
        },
        {
            "authors": [
                "Yanhao Zhang",
                "Lei Qin",
                "Shengping Zhang",
                "Hongxun Yao",
                "Qingming Huang"
            ],
            "title": "Formation Period Matters: Towards Socially Consistent Group Detection via Dense Subgraph Seeking",
            "venue": "In Proc. of ACM ICMR",
            "year": 2015
        },
        {
            "authors": [
                "Shun-Chang Zhong",
                "Yun-Shao Lin",
                "Chun-Min Chang",
                "Yi-Ching Liu",
                "Chi-Chun Lee"
            ],
            "title": "Predicting Group Performances Using a Personality Composite-Network Architecture During Collaborative Task",
            "venue": "In Proc. of INTERSPEECH",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "\u00a9 [Beyan et al.] [2023]. This is the author\u2019s version of the work. It is posted here for your personal use. Not for redistribution.\nThe definitive version was published in ACM Computing Surveys URL: https://dl.acm.org/journal/csur DOI: https://doi.org/10.1145/3626516\nCo-Located Human-Human Interaction Analysis using Nonverbal Cues: A Survey\nCIGDEM BEYAN, Dept. of Management, Information and Production Engineering, University of Bergamo, Italy\nALESSANDRO VINCIARELLI, School of Computing Science, University of Glasgow, UK\nALESSIO DEL BUE, Pattern Analysis and Computer Vision, Istituto Italiano di Tecnologia, Italy\nAutomated co-located human-human interaction analysis has been addressed by the use of nonverbal communication as measurable evidence of social and psychological phenomena. We survey the computing studies (since 2010) detecting phenomena related to social traits (e.g., leadership, dominance, personality traits), social roles/relations, and interaction dynamics (e.g., group cohesion, engagement, rapport). Our target is to identify the nonverbal cues and computational methodologies resulting in effective performance. This survey differs from its counterparts by involving the widest spectrum of social phenomena and interaction settings (free-standing conversations, meetings, dyads, and crowds). We also present a comprehensive summary of the related datasets and outline future research directions which are regarding the implementation of artificial intelligence, dataset curation, and privacy-preserving interaction analysis. Some major observations are: the most often used nonverbal cue, computational method, interaction environment, and sensing approach are speaking activity, support vector machines, and meetings composed of 3-4 persons equipped with microphones and cameras, respectively; multimodal features are prominently performing better; deep learning architectures showed improved performance in overall, but there exist many phenomena whose detection has never been implemented through deep models. We also identified several limitations such as the lack of scalable benchmarks, annotation reliability tests, cross-dataset experiments, and explainability analysis.\nCCS Concepts: \u2022 Human-centered computing \u2192 Collaborative and social computing; \u2022 Computing methodologies \u2192 Machine learning.\nAdditional Key Words and Phrases: interaction analysis, social signals, human behavior understanding, nonverbal communication\nACM Reference Format: Cigdem Beyan, Alessandro Vinciarelli, and Alessio Del Bue. 2023. Co-Located Human-Human Interaction Analysis using Nonverbal Cues: A Survey. ACM Comput. Surv. 1, 1, Article 1 (January 2023), 39 pages. https://doi.org/10.1145/3626516"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Human Behavior Understanding (HBU) is an important topic in research domains; video surveillance, social robotics, detection of mental health issues, human-computer interaction, and many others. Until now, HBU has mostly been performed in terms of Human Activity Recognition (HAR), a task in which behavior means a sequence of pre-defined action classes such as running, walking, or sitting [42]. Despite such a definition of behavior, HAR is a problem far\nAuthors\u2019 addresses: Cigdem Beyan, cigdem.beyan@unibg.it, Dept. of Management, Information and Production Engineering, University of Bergamo, Via A. Einstein 2, Dalmine, 24044, Italy; Alessandro Vinciarelli, Alessandro.Vinciarelli@glasgow.ac.uk, School of Computing Science, University of Glasgow, Glasgow, G12 8QQ, UK; Alessio Del Bue, Alessio.DelBue@iit.it, Pattern Analysis and Computer Vision, Istituto Italiano di Tecnologia, Enrico Melen 83, Genoa, 16152, Italy.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. Manuscript submitted to ACM\nManuscript submitted to ACM 1\nar X\niv :2\n20 7.\n10 57\n4v 2\n[ cs\n.H C\n] 4\nO ct\n2 02\n3\nfrom being solved [48] and still faces major challenges. For instance, different people can perform the same action in different ways [31], different actions can leave similar traces in a given sensor [31, 96], there can be obtrusion, the number of people in a recording can change significantly (from an individual to crowds), the sensors can be inaccurate or the environmental conditions can change. Furthermore, only a few studies address HAR in a fully unsupervised way [149], i.e., without knowing in advance what are the actions to be recognized. Additionally, a major limitation is that HAR approaches do not consider the social context in which humans perform actions and therefore they do not understand what such actions mean. For example, a HAR approach can understand that a person touches his/her own face by reasoning on the spatial relation between hands, face, and body parts [17]. However, the same HAR approach does not necessarily try to understand whether self-touching results from thinking, being embarrassed, or mimicking another person (a behavior typically related to liking and rapport [57]). In this respect, current HAR approaches appear to lack Social Intelligence, i.e., the ability to make sense of human behavior in socially meaningful terms.\nThe problem of interpreting observable behavior in social terms gave rise to Social Signal Processing (SSP) [154, 192], an interdisciplinary domain at the crossroad between technology and human sciences (psychology and sociology). In particular, SSP research integrates social psychology concepts into Artificial Intelligence (AI) and focuses on automatic detection, interpretation, and synthesis of social signals, nonverbal behavioral cues (e.g., gestures, eye gaze, body posture, vocal characteristics, interpersonal distances, facial expressions) capable to convey socially-relevant information. The core idea underlying SSP and more generally, Social AI (the collective name encompassing all domains aimed at endowing machines with social intelligence) is that observable human behavior is not just a sequence of actions but the physical, machine-detectable trace of social and psychological phenomena that cannot be sensed and observed directly. For example, from a Social AI point of view, a smile is not just the activation of a few facial muscles that move the lip corners up but the evidence that, with a certain probability, an individual is experiencing the emotion of happiness or is showing a friendly attitude.\nScope. The aim of this paper is to review the SSP-oriented approaches for HBU. We focus on nonverbal behavior analysis in co-located social interactions and survey the research efforts tackling automated detection of a social and psychological phenomenon. Reviewed works are clustered into three: 1) detection of social traits, e.g., leadership, dominance, personality traits (Sec. 3.1), 2) social role recognition and social relations detection (Sec. 3.2) and 3) interaction dynamics analysis to detect group cohesion, empathy, rapport and so forth (Sec. 3.3). Each study is discussed to provide answers to the following main research questions: 1) What are the nonverbal cues allowing to perform effective automated detection of a phenomenon? 2) What are the computational methodologies performing effective automated detection of a phenomenon through nonverbal signal representations?\nIt is important to mention that even for the detection of the same social and psychological phenomena, the studies can differ from each other in terms of the sensor(s) they used, the interaction setting (e.g., dyads, small groups), and scenario (e.g., meetings, free-standing conversations) that their methodology was tested on. On the other hand, one should notice that such differences could determine the way automated interaction analysis is computationally performed through nonverbal signals. Therefore, our review also acknowledges the type of sensors used and specifies the interaction environment. We concentrate on the (technical) novelties and limitations of the reviewed works considering the time of the publication. For each phenomenon, we share our key observations and tend to highlight what has not yet been performed (e.g., the adaptation of deep models, and usage of multimodal cues). The further paper inclusion/exclusion criteria can be summarized as follows. Remote/online interactions, which have been getting more attention lately, are not in our scope. The interactants are always humans, therefore, we do not concentrate on human-robot, -virtual agents, or similar Manuscript submitted to ACM\ninteractions. All studies discussed rely on nonverbal signals and perform automated detection of any aforementioned social and psychological phenomenon. If a study also utilizes linguistic features together with nonverbal signals, that study was also considered as in the scope of this survey. Emotion recognition and synthesis (which are the topics of Affective Computing rather than SSP, see [35] for more information) are other important subjects, that can be realized through the analysis of nonverbal behaviors. However, this paper does not target such applications, unless their (or any other affective states\u2019) identification was addressed together with a social and psychological phenomenon. Besides, there exist several health-related applications (e.g., detection of depression [6], autism spectrum disorder [66]), performed by automated nonverbal behavior analysis during the co-located human-human interactions. Such works are also out of our scope. Limited attempts to analyze interactions in images (i.e., spatial modeling only) were not considered because of the importance of modeling spatiotemporal data in HBU [79]. Moreover, single-person settings, such as vlogs [183] or presentations (unless there is at least one other person who is co-located and being interacted with) are excluded. We consider the peer-reviewed research efforts published in the conference proceedings or journals (technical reports, and pre-prints are not covered) since 2010. The reader can refer to Appendix A, describing and illustrating the selection process of the papers reviewed in this work.\nRelated Surveys in SSP Domain. This section summarizes prior related review papers in order to motivate the need for our survey paper and to demonstrate our differences and contributions with respect to them. One of the earliest surveys of nonverbal behavior (also called non-linguistic behavior) analysis was [192]. That work introduced, besides the concept of SSP, a taxonomy of nonverbal behavioral cues (grouped into physical appearance, gestures and posture, face and eye behavior, vocal behavior, and use of space and environment) and an initial indication of the social phenomena (i.e., personality, status, dominance, persuasion, regulation, and rapport) that it was possible to detect through automatic analysis of nonverbal cues. Furthermore, the survey showed how several technologies developed until then could be used for SSP purposes (e.g., speech analysis, computer vision, and biometry). In the same year, the studies about the automatic analysis of small group conversations in terms of nonverbal communication were reviewed in [64]. That work illustrated computational models of interaction management, internal states, personality traits, and social relationships. In parallel, the survey papers on nonverbal behavior analysis for video surveillance [42] and automatic personality perception, recognition, and synthesis based on, e.g., written texts, nonverbal behaviors, mobile data, wearable devices, and online games [191] was published. At the time the aforementioned surveys were released, most approaches were unimodal and based such as on the use of cameras or microphones to capture social interactions. In the meantime, the application of multimodal approaches has become more common, presumably, because the cost of wearable sensors keeps decreasing and devices such as smartphones and smartwatches (equipped with sensors such as cameras, microphones, accelerometers, and gyroscopes) have seamlessly integrated everyday life. The use of multiple sensors, sensor networks, and multimodal methodologies brought novel perspectives and new findings. In particular, [147] provided a review of mobile SSP applications covering mobile sensing, social interaction detection, behavioral cues extraction, social signal inference, and social behavior understanding. Further significant improvements were raised which resulted from the recent advances in AI and especially from the development of deep learning methods. In this respect, [179] reviewed human-human interaction studies covering some of the SSP research in which Convolutional Neural Networks (CNNs) recognize social behavior in videos. Furthermore, [123] focused on deep learning-based personality detection. Other survey papers were; [173] summarizing the progress in personality trait analysis in visual computing and [156] focusing on automatic analysis of social interactions for soft skills prediction.\nManuscript submitted to ACM\nOur Contributions. This paper has the following threefold contributions. 1) The types of social and psychological phenomena this review covers are much more than the context included by the related survey papers (notice that the research efforts on some phenomena, e.g., vocal entrainment, group performance, satisfaction, and quality of interactions have never been surveyed before). We also take into account all sensor technologies used and incorporate all possible interaction settings (e.g., dyads, small groups) and scenarios (e.g., meetings, free-standing conversations) in addition to discussing each paper in terms of the nonverbal cues and the computational methodologies. These make our survey majorly different from Junior et al. [173] considering only cameras and visual domain, Cristani et al. [42] focusing only on video surveillance, Gatica-Perez [64] concentrating on small group conversations, [173] limiting its scope to the recognition of personality traits, and Mehta et al. [123] focusing only on deep learning methods. Moreover, given its submission date, our survey is able to include more recent literature. Please notice that several related surveys [42, 64, 191, 192] were published before 2015. 2) We provide a review of the related datasets in terms of their scenario, group size, the total number of participants, annotations, sensors, and public availability information. Such a comprehensive summary is not available in the literature, and we argue that it can foster future efforts in data collection by helping to spot what has not been analyzed yet. Additionally, it can improve awareness regarding the datasets so that future studies can use them in their experimental analysis. 3) We propose several future research directions regarding dataset collection, privacy-preserving social interaction analysis, and the usage of Artificial Intelligence. Such proposals have not been introduced in any SSP-oriented paper before.\nOrganization. The rest of the paper is structured as follows. We first introduce the preliminaries in Sec. 2, which are necessary to be able to make the review of existing literature. Our survey is given in Sec. 3. In Sec. 4, we examine the datasets having the annotations regarding the social and psychological phenomena inspected in Sec. 3. Sec. 5 draws some possible future research directions motivated by challenges and the observations out of our review. Finally, Sec. 6 summarizes the key observations, limitations, and future research directions."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "We first define the concepts of dyads and groups given that human-human social interaction in which interactants share the same environment is in the form of one of them (Sec. 2.1). We refer to multiple interaction environments, including structured settings (such as meetings), semi-structured settings (such as mingling gatherings), or in-the-wild settings (for instance crowds). In Sec. 2.2, we describe the nonverbal cues used by the reviewed studies. Sec. 2.3 reports the sensors utilized in co-located human-human social interaction analysis, particularly for the automated detection of social and psychological phenomena. Finally, Sec. 2.4 exhibits the computational methods utilized by the reviewed papers."
        },
        {
            "heading": "2.1 Dyads, Groups and Interaction Environments",
            "text": "The expression dyad refers to two persons involved in social interaction while the expression group refers to cases in which the number of interactants is at least three. Dyad and group members feel like a part of a super-individual union and whenever collaborating towards results possibly subject to evaluation, share the responsibility of success [79]. Interaction takes place through both verbal and nonverbal communication affected by the number of interacting people. In particular, small groups (3-6 people) tend to be more dynamic than larger ones [64]. For this reason, it is important to highlight that some studies consider dyads as groups of two persons, but dyads are different from groups in many aspects affecting the way automatic interaction analysis is performed [64, 79]. For instance, 1) people tend to form and dissolve dyads more often than groups, 2) dyad and group interactants experience different emotions, 3) the group members can interact with Manuscript submitted to ACM\nan individual person or a subgroup while dyad members can interact only with the other dyad member and furthermore, 4) some social phenomena (such as group cohesion) cannot be studied in dyads [79].\nThe reviewed papers mostly focused on interactions in small groups, in particular the meetings including 3-4 participants that sit around a table to perform a predefined task. Such a setting was used to infer a large number of social phenomena from nonverbal communication. Instead, fewer studies addressed the interactions in mingling events [32] (also referred to as free-standing conversations [3]) involving a larger number of participants that dynamically (and even frequently) form dyads/groups and move from one dyad/group to the other without restrictions. People involved in such scenarios interact spontaneously and therefore the social interactions show more articulated and in-the-wild dynamics. It is important to notice that free-standing conversations might require the detection of groups before addressing the detection of social and psychological phenomena. This is different from the meetings that reviewed papers were set up in the way that dyad/group members are constant and hence performing automated group detection is not needed. There has been extensive literature on automatic group detection. In most cases, the focus was given to the detection of spatial arrangements and the most important cues were interpersonal distance and relative body orientation [71]. Furthermore, many works addressed the detection of F-formations [40, 63, 65, 90, 92, 121, 157, 159, 170, 209], which are spatial arrangements that people spontaneously form during free-standing conversations. In the F-formations context, a person communicating with another tends to maintain close proximity and have a shared focus of attention. Most methodologies detecting F-formations used standard RGB videos captured by distant cameras [40, 80, 90, 157, 170, 189, 209] while Gan et al. [63] utilized Kinect depth sensors as well. Alternatively, Katevas et al. [70, 92] detected stationary interactions inside the crowds by relying on smartphones\u2019 Bluetooth and motion activity sensors. In [65], pairwise F-formations using the data collected from a single body-worn tri-axial accelerometer were detected. On the other hand, there exist a few attempts to detect the social and psychological phenomena in crowds [55, 98, 174, 182] such as by processing the indoor/outdoor surveillance systems\u2019 data. Fig. 1 presents some scenes taken from publicly available datasets demonstrating different interaction environments."
        },
        {
            "heading": "2.2 Nonverbal Cues",
            "text": "The nonverbal cues used by the reviewed studies include vocal behavior (Sec. 2.2.1), body activity (Sec. 2.2.2), eye gaze and visual focus of attention (Sec. 2.2.3), facial expressions (Sec. 2.2.4), proxemics (Sec. 2.2.5) and physical appearance (Sec. 2.2.6). Fig. 2 illustrates some methodologies and/or sensors to automatically extract a number of nonverbal cues.\n2.2.1 Vocal Behavior. Such behavior corresponds to everything in speech except words, including the use of vocalizations (e.g., fillers, laughter, and sobbing), pauses, and turn-taking. Speaking activity and prosody are the two aspects of Manuscript submitted to ACM\nvocal behavior that are most commonly used in the literature.\nSpeaking Activity. The most common form of speaking activity detection is referred to as speaker diarization, which is the automatic segmentation of speech recordings into turns (i.e., time intervals) in which only one of the persons involved in an interaction is speaking. In other words, the speaker diarization detects who speaks and when. If the identity of the speaker is not necessary, speaking activity detection can be performed by simply performing a speech/non-speech segmentation, i.e., by automatically identifying the intervals of time in which there is at least one person speaking. Both tasks above are typically addressed through audio processing methodologies, but there have been approaches that used alternative cues too (e.g., the movement of the speakers [171]). In most cases, the output of speaker diarization or speech/non-speech segmentation is represented in terms of statistical properties of turns (e.g., total length, mean, maximum, mean, interquartile range, histogram, fraction), speaking time length, overlapping speech time, turn-taking order, the number of successful/unsuccessful interruptions, and speaker floor grab, the fraction of time non-overlapping speech accounts for and the back-channel time during turns of different speakers. Such properties can be extracted for each individual involved in an interaction and/or at the group level. Examples of the latter case (typically called group conversational features) include group speaking time length and its statistics (e.g., total, mean) or the total number of conversational events (e.g., successful/unsuccessful interruptions, back-channel utterances).\nProsody. Such cues account for the way people talk and in most cases correspond to how loud people speak (is captured through the energy of the speech signal) and their intonation (is captured through the fundamental frequency of the speech signal). In addition, prosody-related cues include speaking rhythm (captured by utterance timing). The features corresponding to these cues typically include statistics of signal energy, fundamental frequency (e.g., variation, maximum, mean), spectral features (formants, bandwidths, spectrum intensity), speaking rate (e.g., number of syllables per second), local variability of the speech signal (e.g., jitter and shimmer) and Mel-Frequency Cepstral Coefficients (MFCCs). Due to the large number of speech features aimed at capturing vocal behavior, there have been attempts to identify standard feature sets through the application of publicly available packages (e.g., OpenSMILE [51]) or meta-analysis of the literature (e.g., the Geneva Minimalistic Acoustic Parameter Set (GeMAPS) [210]).\n2.2.2 Body Activity. The expression body activity refers to postures, movements of the upper body, arms, hands, and gestures. In most cases, the input data consists of videos and the first step is the detection of humans and/or their body parts, followed by the actual detection of the movements. The literature proposes publicly available packages (e.g., OpenPose [34]) that can perform human body part detection, and their output is often used to represent movements with suitable terms for further use in the analysis of social interactions. There are multiple approaches to represent movements such as spatiotemporal processing of body pose [21], weighted motion energy images (wMEI) [166], dynamic images [24] and optical flow images [23, 119]. Once the mentioned representations are available, it is possible to extract features such as statistics of movement\u2019s energy (e.g., mean and standard deviation), hand activity histograms, number of the body pose changes and variance of body orientation. An alternative approach is to apply data-driven techniques such as CNNs [24]. Finally, object recognition and HAR were used to obtain the number of face-touching events, arm-folding, arm diagonal behavior, hand gesticulations [57], head/hand/arm/body fidgeting head nodding [138], the interaction between people and a predefined set of objects and object manipulation [115, 155].\n2.2.3 Eye Gaze & Visual Focus of Attention. The Visual Focus of Attention (VFOA) can be defined as the point in space a person is looking at and it can be detected by estimating either gaze direction or head pose (some works, Manuscript submitted to ACM\nfor example, [180], use head and body pose as an indicator of attention). Compared to the head pose, the eye gaze direction tends to provide more accurate VFOA information. However, it is more difficult to estimate, and it requires one to constrain, at least to a certain extent, the movement of people. In particular, it requires the use of invasive sensors such as eye trackers that capture high-resolution images of the eyes but need to be worn and might not allow a person to freely move the head. For this reason, there are many studies (e.g., [20, 97, 181, 205]) that detected the VFOA by using the head pose. Such an approach was further encouraged by the availability of open-source packages aimed at head orientation measurement (e.g., OpenFace [13]). The head pose corresponds to 3-angles (tilt or pitch, pan or yaw, and roll) that allow one to identify a vector normal to the face of a person. The direction of such a vector is used as an estimate of the gaze direction and leads to the identification of the VFOA. Once the VFOA of all people participating in an interaction is known, it is possible to calculate several features that were shown to account for different social and psychological phenomena. As examples of such features, the distribution of the time when an individual is in the VFOA of others, the amount of time two people are in the VFOA of each other at the same moment, the amount of time the VFOA corresponds to a given object and the number of times the VFOA of an individual corresponds to another one can be given.\n2.2.4 Facial Expressions. Facial expressions were specifically included in the analysis of dyads (e.g., interviews) or meetings. Their usage was relatively less than speaking activity, prosody, eye gaze/VFOA, and body activity such that Manuscript submitted to ACM\nthe number of studies covering them is seven (which are [12, 129, 130, 132, 176, 196, 206]) whereas the number of works analyzing the speaking activity, prosody, eye gaze/VFOA, and body activity is 53, 47, 32 and 49, respectively. Some phenomena whose automated detection was performed based on facial expressions are; rapport [128], leadership [129, 196], dominance [12], personality traits [176] and social roles [206]). Facial expression analysis includes detecting facial landmarks and facial action units (FAUs). A popular tool to extract such information is OpenFace [13] providing 68 facial landmarks and 17 distinct FAUs for a face typically captured by a close-up camera.\n2.2.5 Proxemics. This is the domain studying the social meaning of interpersonal distances especially when the movement of people is not constrained and therefore social, cultural, and psychological phenomena are the only factors underlying the physical distance between two individuals [158]. Free-standing conversations (conv.) are the settings in which proxemics play the most important role and individual distances were shown to account for the detection of multiple social phenomena including social roles [3], leadership [98, 182], engagement [190] and personality traits [33, 44, 55, 181, 181]. The sensors used to detect the proximity were various from cameras to smartphones and smart badges. In the literature, a social group corresponds to individuals exhibiting similar motion characteristics such as having similar trajectories, being close to each other, or having similar motion orientation. A convenient way to define social groups is by detecting F-formations as described in the previous section.\n2.2.6 Physical Appearance. This includes height, body shape, skin/hair color, clothes, and make-up of a person. It was addressed in terms of physical attractiveness, sympathy, and appreciation in [134]. Besides, the appearance of a person and contextual objects are explored to distinguish different social relations in [115]. By the time deep learning methods (especially CNNs) have been used in the context of this paper, physical appearance is (implicitly) being considered more, by performing feature learning from RGB images. One corresponding implementation can be seen in [206]."
        },
        {
            "heading": "2.3 Sensors",
            "text": "Sensors and their arrangement determine the way automatic interaction analysis is performed. In particular, the cues that can be detected depend on the sensors being used for data acquisition, and correspondingly, the robustness of cue detection changes significantly with the modalities and sensors used. For what concerns vocal behavior, the most frequent sensors are microphones of different types: close-talk (CTM), headset (HSM), tabletop (TTM), lapel (LAM), microphone arrays (ARM), omnidirectional (ODM), four-channel cardioid (FCM) and other distantly placed microphones such as single-channel far-field (FFM). In the case of body activity, eye gaze, VFOA, facial expressions, proximity, and the physical appearance of the participants, the main sensors are cameras. Depending on the setting and scenario, the literature shows the use of cameras: close-up (CUC), Red Green Blue Depth (RGBD), web (WC), 360 degrees (C360), frame-synchronized (FSC), digital movie (DMC), top-view (TVC), wearable (WCM), narrative (NC), pan and tilt zoom (PTZ), far-field (FFC) or a combination of them. CUC has been used mostly in meeting environments, for instance, in [22, 129] for soft skills analysis and in [146] for social group performance analysis whereas TVC has been utilized in proximity analysis such as in [190].\nAlthough the most common sensors are microphones and cameras, other sensing technologies have also attracted researchers. For example, smartphones were the preferred sensors in [60] as they represent the core communication device in daily life. In fact, smartphones can be used to efficiently monitor co-located human-human interactions in dyads and groups because they are equipped with a wide array of sensors such as microphones, cameras, accelerometers (ACC), gyroscopes (GYRO), magnetometers (MAG) and proximity (Prox) detectors. In a similar vein, sociometer badges (SB) are small portable devices that include microphones, infrared beam detectors, Bluetooth detectors, and ACC sensors. Manuscript submitted to ACM\nThey were used for the analysis of free-standing conversations in [3, 44, 45, 82, 88, 207, 208]. Eye trackers (ET) and smart glasses (SG) have been very precise devices for capturing visual attention, in particular for dyadic [184] or standing interactions [182]. Recently, they were also used during rather more complex co-located interactions in which the wearer moves a lot and interacts with multiple objects/humans in indoor and outdoor environments [185]. External ACC and Prox were used for the analysis of mingling scenarios [33] while ACC was used for the analysis of small group round-table meetings [136] as well. Wearable inertial measurement units (IMU) [58, 104, 185] and motion capture systems (MoCAP) [135, 136, 182] were typically preferred to realize highly robust body motion analysis. Head-mounted wearables were used to capture the speech, body movement, and gaze [72]. As an example, [112] showed that eye activity determined by head-mounted wearable could be used to infer a person\u2019s communication load and conversational state that is to be applied for turn-taking prediction. Other wearables such as respiration sensors [81] were shown to be useful for predicting the next speaker in co-located multi-party meetings. Biological sensors combined with sensors capturing nonverbal behaviors also supply an accurate automated understanding of various social phenomena. For example, [99] realized automatic personality recognition on a dataset [125] captured by electroencephalogram (EEG) and annotated based on the observations of the nonverbal behaviors."
        },
        {
            "heading": "2.4 Computational Methods",
            "text": "The computational methods considered in this survey refer to machine learning or deep learning supervised and unsupervised approaches. While supervised methods were frequently utilized, pioneer works on some topics such as modeling leadership [164\u2013166] and dominance [7, 78] applied unsupervised methods that are rule-based or ranking-based or based on the fusion of features. Another popular unsupervised method was Granger causality, a multivariate time series representation framework applied in [87, 88] for modeling dominance. Finally, Gaussian Mixture Models (GMM) and Latent Dirichlet Allocation (LDA), further unsupervised approaches, were applied for engagement analysis in [141] and leadership style detection in [83]. The supervised methods were used for performing classification or regression. The most popular classification method was Support Vector Machine (SVM), which was typically combined with a radial basis function (RBF) kernel or a linear kernel. Earlier works used Naive Bayes (NB) [58, 73, 77, 82, 104, 122, 133], Locally Weighted Naive Bayes (LW-NB) [181] and k-Nearest Neighbors (k-NN) [190] as well. Bayesian Networks (BN) [89, 193], Boosting [102, 188], Boosted Trees (BT) [131], Decision Tress (DT) [204], Random Forest (RF) [12, 97, 102, 131, 134, 136, 144, 146, 181, 208], Collective Classifier (CC) [166] and Multiple Kernel Learning (MKL; and its variations such as Localized MKL (LMLK)) [21\u201324] combined with other classifiers such as SVM, were other popular methods. Some of these classifiers were merged within Multi-task Learning (MTL) [97]. In case of sequential data exist, the most commonly applied methodologies include Influence Model (IM) [177], Hidden Markov Models (HMM) - including variations such as Linear-chain HMM (LC-HMM) [177] - and Conditional Random Fields (CRF) - including variants such as linear-chain CRF (LC-CRF) [61, 155, 167, 168, 177]. A similarly wide spectrum of approaches can be observed in the case of regression methods that include linear regression (linR), Support Vector Regression (SVR) [105, 126], Random Forest Regression (RFR) [105], Ridge Regression (RR) [8, 9, 53, 97, 134, 143, 144, 176], Multiple Regression (MR) [26, 182] and Logistic Regression (LR) [33, 91, 118, 136, 208].\nThe latest years have witnessed the success of Deep Neural Networks (DNN) that were increasingly more often used, from shallow models such as Multilayer Perceptron (MLP) [12, 73] to deep architectures such as Convolutional Neural Networks (CNN) [23, 45, 118], Fully Connected Neural Networks (FCNN) [206] and Graph Convolutional Networks (GCNN) [114]. Deep Boltzmann Machines (DBM) were applied as an unsupervised feature learning method to jointly model body pose features in [21]. For sequential data processing, Long Short-Term Memory Networks (LSTM) Manuscript submitted to ACM\n[2, 24, 113, 119, 172, 206, 210], a type of Recurrent Neural Network (RNN), were applied for various problems and often in combination with CNNs. In [22], sequential data processing was performed with Conditional Restricted Boltzmann Machines (CRBM) [22] and a combination of RNNs with Restricted Boltzmann Machines (RNN-RBM) [22]. On the other hand, [148] presented a Transformer-based context-aware model to analyze face-to-face dyadic interactions. The next section discusses in depth the computational methods and particularly their effectiveness together with the nonverbal cues utilized.\nWhen considering the computational methods, it becomes essential to emphasize their evaluation, including the specific evaluation metrics employed. In this regard, there are no distinct evaluation metrics beyond the conventional ones used in standard classification (e.g., accuracy, f-score, recall, precision) and regression (e.g., mean square error) evaluations, which are applicable to various machine and deep learning methods. Consequently, there is a lack of standardization across studies regarding evaluation metrics. It is worth noting that accuracy is the most commonly used metric for classification tasks, while mean square error is frequently utilized for regression analysis."
        },
        {
            "heading": "3 REVIEW: METHODOLOGIES TO DETECT SOCIAL AND PSYCHOLOGICAL PHENOMENA",
            "text": "Recalling that, this paper reviews the studies performing automated detection of social and psychological phenomena, which are realized through automatic nonverbal behavior analysis of co-located human-human social interactions, we divided the existing literature into three broad categories. These are namely social traits (Sec. 3.1), social roles or relations (Sec. 3.2), and interaction dynamics (Sec. 3.3). Below, we discuss each study in text and further summarize them in Tables 1, 2, and 3. The tables are recommended to be read together with the corresponding text. Our discussions start with a brief definition of social and psychological phenomena. We then focus on the effectiveness of the nonverbal cues and the computational methods applied along with the corresponding sensors and the interaction environment.\nManuscript submitted to ACM\nTable 1 \u2013 continued from previous page Ref. Sensor Nonverbal Cue Scenario (Group Size) Computational Method [19] LAM, CUC Speaking/head/body activity, prosody,\nVFOA Meetings (4) SVM, MKL\nDominance [7] HSM, LAM, ARM, CUC Speaking/head/body activity Meetings (4) Rank-based fusion [84] HSM, LAM, ARM Speaking activity, energy Meetings (4) Rule-based [49] HSM, WC Speaking/body/face/mouth activity Dyadic interactions Adaboost [78] OTM Speaking activity Meetings (4) Rule-based [88] SB Body movement energy Meetings (4) Granger Causality [87] CTM, CUC Speaking energy, body activity Meetings (4) Granger Causality [12] ARM, WC Speaking activity, VFOA, FAU, emo-\ntion scores, MFCCs Meetings (3-4, 5-8) MLP, RF\n[196] ARM, WC MFCCs, VFOA, FAU, speaking activity, emotion intensity Meetings (3-4) Temporal network-diffusion CNN Personality traits [108] CTM, ODM, CUC, WC Conversational activity, emphasis, influ-\nence, mimicry, fidgeting Meetings (4) SVM w/ rbf\n[111] CTM, ODM, CUC, WC Speaking activity, VFOA Meetings (4) SVM w/ rbf [89] CTM, ODM, CUC, WC Conversational activity, emphasis, influ-\nence, mimicry, fidgeting Meetings (4) BN\n[178] CTM, ODM, CUC, WC Conversational activity, emphasis, VFOA Meetings (4) SVM w/{linear, rbf} [177] CTM, ODM, CUC, WC Conversational activity, emphasis, VFOA Meetings (4) IC-HMM, LC-CRF, IM [188] HSM Speaking activity, prosody Meetings (4) Boosting [176] DMC Speaking activity, prosody, facial ex-\npressions Movies RR, SVM w/{linear, sparse, rbf}\n[110] CTM, ODM, WC Speaking activity, VFOA Meetings (4) SVM w/rbf [8] WC Visual activity Meetings (3,4) RR, linear SVM [9] ARM, WC Speaking/head/body activity, prosody,\nVFOA Meetings (3-4) RR, linear SVM\n[181] FSC VFOA, proximity Indoor free-standing conv. (6) Linear SVM, RF, LW-NB [143] ARM, WC Speaking / body activity, prosody, VFOA Meetings (3-4) RR, linear SVM [33] ACC, Prox Speaking activity, body movement energy, proximity Mingle events (30-32) LR [24] WC Dynamic images [29] Meetings (3-4) CNN+LSTM w/{softmax, SVM, LMKL} [55] TVC Proximity, velocity, trajectory angular variation Indoor/outdoor surveillance (crowd) Rule-based [44] FSC, SB Body pose, proximity Indoor free-standing conv. (18) Metric learning [45] FSC, SB Body pose, proximity, social group motion Indoor free-standing conv. (18) CNN [148] LAM, CUC, ODM Face, raw video, raw audio Dyadic interactions 2D-CNN, R(2+1)D network, Transformer Personality traits together with leadership/dominance/competence/liking detection [53] ARM, WC Speaking / head / body activity Meetings (3-4) RR, SVM w/ rbf [97] ARM, WC Speaking / head / body activity, VFOA,\nprosody Meetings (3-4) SVM, RR, RF, MTL w/ regression\n[113] ARM Speaking activity Meetings (3-4) Bidirectional LSTM [23] LAM, ARM, WC Optical flow images Meetings (3-4) CNN w/ {softmax, SVM, LMKL}\nContinued on next page\nManuscript submitted to ACM\nTable 1 \u2013 continued from previous page Ref. Sensor Nonverbal Cue Scenario (Group Size) Computational Method [204] CUC VFOA, body activity and hand-head /\nface relative position Meetings (3-4) Multiple DT\nHirability [134] ARM, CUC Speaking/body/head activity, prosody,\nhead nods, smile, eye gaze, physical appearance\nReal job interviews (dyad) Ordinary least-squares, RR, RF\n[132] CUC Prosody, facial expressions Mock interviews (dyad) SVR, Lasso, LR, Gaussian Process Hirability together with other soft skills [130] Kinect, ARM Speaking activity, prosody, body mo-\ntion, head nods, facial expressions, Job interviews and conv. in reception desk (dyad) SVR, RFR\n[144] ARM, WC Speaking/head/body activity, prosody, VFOA Meetings (3-4), job interviews (dyad) RR, linear SVM, RF"
        },
        {
            "heading": "3.1 Social Traits",
            "text": "The social traits addressed by the computing studies include competence, likeability, hirability, personality, dominance, and leadership. The task is the automatic prediction of such traits based on the self-assessments or the external observers\u2019 judgments. This section defines each trait and then discusses the approaches, findings, novelties, and limitations of each relevant work. Furthermore, a detailed summary of reviewed papers can be seen in Table 1.\nLeadership and its style. A leader is a person who has authority and power over a group of people and can exert his/her dominance, influence, and control over them [20]. Emergent leaders, on the other hand, are the ones who naturally show these characteristics in a group. Reviewed works investigated the leadership and its style for their automated detection.\nEmergent leadership has been studied comprehensively to classify a person as the most leader, the least leader, or other [18, 20\u201322, 129, 164\u2013166]. As the interaction environment, small group meetings, composed of 3-4 participants sitting around a table, were preferred the most [18, 20\u201322, 26, 129, 164\u2013166]. There exist only a few study performing detection of the designated leaders (also called role-based leaders) such that [98, 174, 182] focusing on crowds and [135] targeting the interactions among dyads. Overall, the most frequently used modalities were audio and video, which were captured by ARM/LAM and CUC, respectively. Differently, Kinect was preferred in [26], IMU and ET were used in [182], and MoCAP was utilized in [135, 182]. The nonverbal signal and the computational method employed the most were; speaking activity and SVM, respectively. Sanchez et al. [164\u2013166] presented the first publicly available dataset for emergent leadership. Additionally, they deeply examined the effectiveness of several feature sets based on speaking activity, head and body activity, prosody, and gaze together with several learning models and proposed a novel method called Collective Classifier. Besides, the first study investigating the leadership in standing groups composed of six persons was [182], showing the effectiveness of eye gaze, movement, and proximity modeled with stepwise MR. The works [98, 174] focused on crowds and presented novel approaches based on PageRank-based graph link analysis and structural SVM, respectively. The analysis in [174] was limited to the overall performance of the model without further discovering the individual contribution of each feature. Several studies [22, 129, 164, 166] showed that using multimodal features improves inference performance. For example, it was found that visual information augments acoustic information [166]. Beyan et al. [18] demonstrated that VFOA and speaking activity-based features can perform better than head and body activity-based cues. Furthermore, effective joint modeling of multimodal cues was studied in [20\u201322], indicating the state-of-the-art (SOTA) performance of MKL methods. Even though earlier art applied rule-based or rank-based models [98, 164\u2013166], deep architectures have been also applied in [21, 22]. Notice that, there is not yet an end-to-end pipeline. Pose-based visual activity modeling was prominent in [21, 129, 135, 182]. For instance, a novel methodology encodingManuscript submitted to ACM\nthe spatiotemporal body postures within an unsupervised learning approach was presented in [21]. That method became the SOTA body activity representation for emergent leadership. As the only attempt, Muller et al. [129] performed an analysis across different leadership datasets, testing the generalizability of an SVM trained on one dataset to others, showing that cross-dataset leader detection is possible by using pose and VFOA features.\nRegarding leadership style detection, Jayagopi et al. [83] investigated autocratic leaders, participative leaders (egalitarian), or free-rein leaders (those who allow group members to make decisions) by analyzing the participants\u2019 speaking activity in meetings. The focus was given to designated leaders in [83]. Instead, Beyan et al. [19] aimed to detect emergent leaders\u2019 style, which was classified either as autocratic (a person who is directive, tends to exert strong control and decision making) or democratic (a person who tends to involve group members into decision-making process). Also, a wider set of cues including speaking activity, prosody, head/body activity, and VFOA were modeled within unimodal and multimodal learning schemes in [19]. The demonstrated results were more favorable towards the speaking activity and VFOA [19]. Differently, Feese et al. [58] discriminated between individually considerate leaders (defined as leaders who pay attention to their followers and listen to them effectively) and autocratic leaders using data from IMU sensors. That has been the unique work [58] showing the effectiveness of body posture mirroring that was modeled with NB. However, it was limited due to not presenting comparisons with other nonverbal signals. Overall, one can observe that there has been no attempt to perform leadership style prediction using deep models.\nDominance. It is seen as a trait or it is the hierarchical position of a person in a group [87]. As a trait (the case of our interest), dominance characterizes people that influence others and the automatic detection of the dominant person in a dyad/group was the aim of the reviewed studies. Following social psychology, dominance leaves traces in terms of speaking time, turn management, interruptions, pitch, body activity, facial expression, VFOA, and eye gaze. For this reason, computing studies have focused mostly on such nonverbal cues [7, 12, 49, 78, 84, 87, 88, 196]. The most widely investigated interaction setting was meetings composed of 3-4 people [7, 78, 84, 87, 88, 196]. Instead, Escalera et al. [49] examined dyads, and Chongyang et al. [12] studied larger groups of up to eight people. The usage of the microphones (HSM, KAM, ARM), and cameras (CUC, WC) hold the majority while the wearable SB was preferred in [88].\nThe research efforts in the dominance context mainly focused on improving automatic detection performance. To do so, the studies proposed different nonverbal cue combinations with peculiar computational models. Overall, the speaking activity cues are the most effective ones while the body activity cues follow them. The deep models in [12, 196] boosted the SOTA performance significantly. Aran et al. [7] applied multimodal fusion of speaking, head, and body activity cues at the feature extraction level and at the classifier level via score and rank level fusion. The results showed that the visual information is complementary to audio and multimodal fusion is needed to achieve better dominance estimation performance. Jie et al. [84] focused on speaking length and speaking energy to recognize the most and the least dominant persons in multi-party conversations. The simple rule-based method was found effective compared to SVMs. However, applying feature extraction in a semi-automatic way is a limitation of [84]. Escalera et al. [49] presented a set of movement-based features extracted from body, face, and mouth activities in order to define a higher set of interaction indicators that were modeled with AdaBoost and demonstrated that the speaking length is the preferred feature to detect dominant people. Motivated by this finding, Hung et al. [78] examined how the performance of a speaker diarization method affects the dominance estimation and found that reducing the signal-to-noise ratio of the input source and reducing the computational complexity of the speaker diarization algorithm both leads to worse performance. The novelty of [87, 88] lies in using Granger Causality, which took into account the causal effects the dominant subjects\u2019 speaking and body activities have on the behavior of the other participants. Chongyang et al. [12] not only estimated the most Manuscript submitted to ACM\ndominant person in a group but also inferred the more dominant person in a pair of people by proposing the Dominance Rank algorithm. That study [12] proved the effectiveness of the facial action units, emotion scores (the intensity of eight emotions and two facial traits (smile, and open eyes)), MFCCs, speaking and gazing probabilities modeled with an MLP. Yanbang et al. [196] used the features\u2019 of [12] and presented a better-performing temporal network. That model\u2019s effectiveness is thanks to containing a diffusion block capable of extracting patterns from highly dynamic interactions and being able to learn subtle and local patterns by integrating a set of poolings.\nPersonality Traits. Personality is a latent construct that accounts for individuals\u2019 behavior, characteristics, emotions, and motivations [123, 191]. The reviewed computing studies focused on personality traits for automatic recognition. Different personality models were proposed in social psychology. However, in technological research [8, 9, 24, 44, 45, 55, 143, 177, 178, 188] the most popular model has been the Big-Five traits, which correspond to openness to experience (the person who is curious, original and have wide imagination and interest), conscientiousness (the person who is efficient, organized, reliable, responsible and thoughtful), extraversion (the person who is active, energetic, talkative and assertive), agreeableness (the person who is trustworthy, straightforward, kind, forgiving and generous) and neuroticism (the person who is nervous, tense, sensitive, unstable and worrying). Big-Five traits have been often referred to as OCEAN traits because of their initials. Overall, extraversion was the most often investigated trait as it can be strongly displayed during group interactions and consequently resulted in more reliable annotations than other personality traits [9]. On the other hand, Locus of Control (LoC) is defined as the outcomes of a person\u2019s actions are primarily the results of her own actions or based on the events/influences outside of his/her control [160] and it was examined in [89, 108]. HEXACO traits include honesty in the Big-Five cluster and they were studied in [33].\nThe automatic recognition of these traits was repeatedly performed with speaking activity, VFOA, prosody, and body activity-based cues. Three nonverbal behavior groups were constructed based on the categories described in Sec. 2.2 in [89, 108, 111, 177, 178]. These were: i) conversational activity which is an indicator of interest and engagement consists of the audio energy in a time frame, length of the voiced segments, length of the speaking segments, the fraction of the speaking time, and voicing rate; ii) emphasis referring to the speaker\u2018s motivation such that the consistency can be interpreted as focus and its variability can be interpreted as openness, including the prosodic features formant frequency, confidence in formant frequency, spectral entropy, values of the larger autocorrelation peaks, number of the larger auto-correlation peaks, location of the larger auto-correlation peaks, and time derivative of energy and iii) influence that is related to dominance and represented by the ratio of overlapping speech segments to the total segments. The majority of the works preferred investigating the meetings involving 3-4 people [8, 9, 24, 89, 108, 110, 111, 143, 177, 178, 188]. Consequently, the most frequently used sensors were CTM, ARM, ODM, CUC, and WC. Some others tested their method, which uses speaking activity, prosody, and facial expressions, on a dataset composed of movies [176], in free-standing conversations using interpersonal distances [33, 44, 45, 181] and in indoor/outdoor surveillance videos where people engage in social gatherings [55]. Palmero et al. [148] focused on dyadic interactions and used facial features, raw audio, and several metadata (interlocutors\u2019 characteristics, order of task, task difficulty, interlocutors\u2019 relationship).\nThe most important results and findings are summarized as follows. For extraversion and LoC identification, using activity and emphasis features together improved the performance [108]. Speaking activity and gaze features were effective in predicting extraversion, and the classification performance depended on the length of the observation time [111]. Extraversion was the best-recognized personality quality which was detected by using vocal behavioral cues (such as pitch) and VFOA in [178]. Following that, openness was the second best-recognized quality, and the corresponding result was obtained when the speech energy derivative was used. The usage of IM resulted in better performance than Manuscript submitted to ACM\nHMMs and became an alternative approach to perform Big-Five traits detection [177]. Non-linguistic features (prosody, speech activity, overlaps, and interruptions) could outperform linguistic features (words n-gram and dialog acts) for the prediction of high/low extraversion, consciousness, and neuroticism traits [188]. Okada et al. [143] proposed the usage of higher-level features representing inter-modal and inter-person relationships, whose patterns are identified as co-occurring events based on a clustering algorithm. This novel representation of features demonstrated improved results compared to earlier works. The way features were fused (early fusion) in [143] was altered to late fusion in a follow-up paper of authors [144], resulting in even better performance. Aran et al. [8] used vlog datasets as the source and the face-to-face small group meetings as the target to train a ridge regression model, which was later tested on the target dataset. As the target data used was labeled, that approach contradicts unsupervised domain adaptation, still, it has been contributing to the literature by showing the generalizability of the body activity cues across datasets. Later on, the effectiveness of using thin-sliced impressions was compared with the whole meeting impressions of external observers in [9]. The results implied that for the extraversion trait, predictions based on the whole meeting are feasible with a slight decrease compared to the predictions based on the thin-sliced data. Whereas for the openness trait, using the whole meeting provides better performance. Moreover, body activity features could achieve higher accuracy for extraversion detection in comparison to audio features, and the performance of energy features was found to be higher than other prosodic features [9]. VFOA features were good at predicting extraversion and neuroticism while the head pose errors had more impact on extraversion detection in cocktail party scenarios in [181]. The data of body-worn triaxial ACC to extract the individual\u2019s speaking status was merged with movement energy-related cues to estimate the personality in crowded mingling scenarios in [33]. That study [33] relied on transductive parameter transfer across speech-related body movements of some participants to label the speech turns of others. Beyan et al. [23] proposed a novel pipeline composed of optical flow image computation (a representation of body activity), CNN-based feature learning, feature encoding performed through covariance matrices in Riemann space and the classification with SVM and LMKL. Such an approach [23] showed significantly better results compared to the prior visual activity-based and audio-visual features for high/low extraversion classification. Instead of optical flow images, dynamic image representation [29] was the input of a CNN+LSTM network aiming to detect the spatiotemporal saliency to determine key-dynamic images in [24]. As a novelty, only the feature embeddings belonging to the automatically detected test key-dynamic images were used for personality prediction and such an implementation improved the performance compared to using all images in a test video. Another study [44] proposed an architecture composed of two CNNs, jointly modeled with a triplet loss. The novel inputs of that model were images embedding the skeleton data over time and the interpersonal distance representing the global dynamics of a scene over time. That proposed method [44] surpassed several prior arts including LSTM modeling body motion and proxemics features over time. The same model was extended in [45] by adding the social group descriptor in the image format and a third CNN-based encoder, resulting in better scores than [44]. The facial features extracted from the person-in-interest and the scene features extracted from other interlocutors were used together with the raw audio signal in [148]. That study was the first work using Transformers for personality trait recognition. However, using several CNNs as the backbones made the whole pipeline computationally heavy.\nAmong all social phenomena, personality traits have gathered the highest attention from computing studies. Deep learning approaches are the SOTA for this research line. However, it is hard to make a conclusion on the effectiveness of a computational model over another as the studies were rarely tested on the same datasets. Due to this fact, the comprehensive highlights are exhibited only in terms of nonverbal cues as follows. Speaking activity and prosody-based features were found effective for the automatic recognition of several personality traits. As an alternative, body activity features have been very promising even in semi-structured scenarios such as poster sessions (a type of free-standing Manuscript submitted to ACM\nconversation). There exist several recent efforts representing the body activity over time in terms of images to process them with CNNs [23, 24, 44, 45]. There is no doubt that multimodal (e.g., acoustic and vision) approaches dominate this research line due to their remarkably better performance than unimodal methods.\nCompetence & Liking. A person who is perceived as competent has the experience, skills, knowledge, and intelligence while a person who is perceived as liking is kind, friendly, sympathetic, considerate, and/or well-disposed [166]. Such social traits were examined together with leadership and/or personality traits by computing studies. The entire efforts shown for the automated detection of such traits undertook the interactions during meetings composed of 3-4 persons [53, 97, 113]. Thereupon, the sensors used were bounded with microphones (ARM, LAM) and cameras (WC, CUC). Fang et al. [53] studied perceived competence, liking, dominance, leadership, and the rank of dominance together with Big-Five personality traits. The effectiveness of the speaking activity, energy and pitch, body motion, and audio interruptions were investigated with regression and SVM. As the novelty, that study [53] approached the problem in terms of: intra-personal features (i.e. related to only one participant), dyadic features (i.e. related to a pair of participants), and one-vs-all features (i.e. related to one participant versus the other members of the group). The intra-personal and one-vs-all features were found more effective for all traits and social impressions considered. Kindiroglu et al. [97] also studied the same traits and social impressions. Similar to [8] a vlog dataset was transferred to a small group meeting dataset, but different from [8], [97] applied multi-task learning. Such an implementation resulted in improved performance with a limited size of target data. Lin et al. [113] presented a novel attention mechanism (called interlocutor-modulated) injected into a bi-directional LSTM. That method modeled the vocal behaviors of both the target speaker and his contextual interlocutors and showed better performance than [53, 97]. The key conclusions are: i) the dominant effectiveness of the speaking activity-based cues is prominent and ii) the proposed method of [113] is the current SOTA.\nHirability. It is defined in terms of communicative competence, persuasion skills, work conscientiousness, and stress resistance [144]. As expected, this trait was investigated during interviews (i.e., dyadic conversations) but also in small group meetings. Both settings were equipped with microphones (i.e., ARM) and cameras (i.e., Kinect, WC, CUC). The automatic detection of hirability is a social trait in which the facial expressions (as well as higher level cues inferred from facial expressions such as smile) were relatively more considered [130, 132]. Speaking, body and head activities (particularly head nods), and VFOA were the other features examined. Studies used rather standard regression methods (e.g., SVR and LR), and the deep learning models have never been tested. The important findings can be encapsulated into three folds. i) Multimodal nonverbal cues perform better than unimodal audio cues (speaking activity, prosody) or unimodal video-based features (VFOA, head, and body activities) [134, 144]. ii) Out of the above-mentioned cues, the most predictive ones are the applicant\u2019s audio cues and the interviewer\u2019s visual cues. This shows that the interviewerproduced responses could condition the behavior of the job applicant, i.e., by displaying visual back-channels [134]. iii) The higher speaking turn duration and head nods are positively correlated to higher ratings for hirability, competence, sociability, persuasiveness, clearness, and positiveness while verbal content is less important [130].\nManuscript submitted to ACM\nTable 2 \u2013 continued from previous page Ref. Sensor Nonverbal Cue Scenario (Group Size) Computational Method [193] HSM, LAM Speaking activity, prosody Meetings (4) Dynamic BN [167] HSM, LAM Speaking activity, prosody Meetings (4) CRF [43] CTM, ODM, WC Speaking activity, hand/body fidgeting Meetings (4) IM [168] HSM OpenSMILE acoustic features [51], speaking activity Meetings (4) CRF [138] CTM, Kinect Speaking activity, visual backchannels, VFOA Meetings (4) SVM w/ RBF [61] HSM OpenSMILE acoustic features [51], speaking activity Meetings (4) HCRF [3] FSC, SB Head / body pose Indoor free-standing conv.\n(18) Alternating Direction Optimization\n[59] FFM MFCCs Motivational interview (dyad) Hierarchical Agglomerative Clustering, SVM [206] HSM, CUC Facial expressions, head pose, eye gaze, body movement, appearance, prosody, and audio short-term metrics [69] Meetings (4) LSTM+FCNN Social Relations Detection [155] DMC Body motion, human-object int., facial images YouTube videos CRF [118] DMC Spectrograms, motion in RGB and optical flow Movies CNN w/LR [119] DMC Motion in images, optical flow, face images Movies CNN w/LSTM [2] WCM (NC) RGB images Indoor/outdoor free-sitting\nconv. CNN+LSTM\n[115] DMC Physical appearance, proximity, human-object int. Movies Pyramid GCN"
        },
        {
            "heading": "3.2 Social Roles and Relations",
            "text": "Below, we present a deep review of computing studies on automated social roles and social relation detection, also summarized in Table 2.\nSocial Roles. The roles considered in studies published before 2010 typically correspond to specific functions role-played in a given social context (such as a project manager in a meeting). Therefore, they are scenario-specific. However, a model trained in a dataset having a fixed social context might not be generalizable and subsequently might not be successfully applied to other datasets or real-world environments. Considering this, Vinciarelli et al. [193] defined the following socio-emotional roles that are independent of any particular scenario or corpus and several works [43, 61, 167, 168, 206] proposed methods to detect such roles automatically. Protagonist is a person who takes the floor, leads the conversation, and asserts his/her authority, supporter is a person who is cooperative, shows attention and acceptance while providing support, neutral is a person who does not express his/her ideas and accepts others\u2019 ideas, gatekeeper is the moderator who encourages the communication between the group members and attacker is a person who does not agree with others\u2019 ideas, does not respect the status of others and attacks other speakers. Alameda-Pineda et al. [3] defined social attractor (a person who attracts the attention of the other members in the group) to be detected in a real-world workshop session where people have discussions in front of posters. In such a context, the poster presenters were labeled as social attractors. Oertel et al. [138] instead studied the automated detection of attentive listener (a person who grabs the floor after the current speaker), side participant (potential future speakers that do not grab the floor after the current speaker) and bystander (a person who is not expected to speak in the near future but the rest of the group is aware of them).\nThe automated recognition of social roles was largely addressed in meetings composed of 4-persons [43, 61, 138, 167, 168, 193, 206]. Few studies focused on other settings such as indoor free-standing conversations [3] and dyads [59]. Microphones (HSM, LAM, CTM, ODM) were frequently incorporated to capture the relevant data followed by the cameras (WC, Kinect) and the usage of SB in free-standing conversations. The integration of such sensors implies that vocal behavior (speaking activity and prosody) was relatively well-investigated in this respect, which indeed can be observed in [43, 59, 61, 138, 167, 168, 193, 206]. An exception is Alameda-Pineda et al. [3] which relied only onManuscript submitted to ACM\nhead and body postures. Others utilized a multimodal cue set such that the facial expressions, head pose, eye gaze, body movement, appearance, and prosody were modeled in [206] while in addition to speaking activity Oertel et al. [138] and Dong et al. [43] involved visual backchannels and hand/body fidgeting, respectively. The core findings of the reviewed works are summarized as follows. The effectiveness of turn-taking patterns, turn duration, and prosodic features with Bayesian BN modeling was shown in [193]. Sapru et al. [167, 168] applied a novel and more effective method than [193]. They combined turn-taking patterns, speech duration, prosody, and lexical information to integrate the statistical dependencies between roles across adjacent segments of meetings with CRFs. For the first attempt, in [43] the importance of the temporal dependencies among a) the roles played by the same subject, b) the time properties of the roles played by each individual, and c) the mutual constraints among the roles of different group members were considered through an IM framework, showing notable results. Fotedar et al. [61] relied on a generative model where the participants\u2019 role transition was used and the likelihood of the feature vector for a role was generated by CRF. Such an approach showed improvements in the detection of roles; gatekeeper and protagonist compared to earlier works. One limitation of that study is assuming the availability of all slices from a meeting for every participant and therefore not being resilient to missing information. Oertel et al. [138] showed that speaking activity, visual/verbal backchannels, and gaze patterns observed in dyadic interactions remain the same in multi-party interactions during the detection of the role of silent participant. Such a conclusion is very interesting, still, the results should have been confirmed on a larger dataset as well as better examine the effect of data segment size on the findings. Recent work presented the usage of co-occurrence features and successive occurrence features in thin time windows to model the behavior of a person, as well as the responses of that person by using multi-stream RNN [206]. That pipeline has been original for role detection and resulted in several conclusions such that: a) VFOA is the most effective cue out of others and facial action units: lip-tightener, lip-tightener, lip-stretcher, and lip-corner-depressor are more descriptive among all action units, b) the head yaw has the largest importance among all head orientations, c) rhythm of the speech plays an important role while the entropy of energy, speaking status, spectral entropy and spectral roll off are among the important cues [206]. The optimization procedure; alternating-direction method of multipliers (ADMM) was first time applied for role recognition in [3]. Such a model can be favorable for the estimation of any phenomena by solving the matrix completion problem with ADMM, independent of the nonverbal cues considered. Flemotomos et al. [59] demonstrated that in dyads, using speech signals yields superior results compared to the independent use of turn-level classifiers which do not take speaker-specific variabilities. However, the experimental analysis was performed only with hierarchical agglomerative clustering and SVM.\nSocial Relations. Automated detection of social relations was majorly addressed based on computer vision techniques. The used sensors are different types of cameras and some of them recorded the audio too. The datasets composed of movies [115, 118, 119] or YouTube videos [155] were used for the validation of the proposed methods. One exception is [2] investigating indoor/outdoor free-sitting conversations. Ramanathan et al. [155] recognized social relations during events such as a birthday party where the relations are birthday person, parents, friends, and guests or a wedding where the roles are bride, groom, priest, and bridesmaids by modeling person-specific features; body motion, face images and human-object interactions through a customized CRF. The novelty of that work [155] was taking into account the interrole interactions in CRF, showing considerably better performance with respect to k-means and baseline CRF (i.e., not considering the inter-role interactions). Social relations were split into subjective relations (dominant, competitive, trusting, warm, friendly, etc.) and objective relations (working relation, kinship relation, etc.) in [118, 119]. A multi-stream CNN was proposed to model audio and high-level semantic information in videos. The feature embeddings were further fused with LR in [118] while [119] exploited LSTM with multiple attention units, eventually outperforming LR. Liu et al. [115] Manuscript submitted to ACM\nrecognized social relations: attachment (relations: parent-offspring roles), mating (relations: couple), hierarchical power (relations: leader-subordinate, service (e.g., customer-waiter, passenger-driver), reciprocity (relations: sibling, friend) and coalitions groups (relations: colleague, opponent) with a framework called Multi-scale Spatial-Temporal Reasoning (MSTR). MSTR adapts temporal segment network (TSN [194]), a triple graph model representing the visual relations between persons and objects as well as proposing Pyramid Graph Convolutional Network performing temporal reasoning with multiscale receptive fields. That method [115] demonstrated better results than several complex architectures: TSN with/without spatiotemporal features, GCN, and its variations. Differently, a method to recognize social relations from egocentric video streams (notice that this has been the unique work processing first-person videos for social relation detection) composed of several interactions such as father-child, mother-child, lovers, colleagues were presented in [2]. Features learned from full, facial, or full-body images with various CNNs were compared with an LSTM-based classifier. Overall, several efforts in this context targeted integrating the advances of deep learning methods and even brought in novel technical perspectives, promoting more accurate social relation detection.\nTable 3 \u2013 continued from previous page Ref. Sensor Nonverbal Cue Scenario (Group Size) Computational Method [104] Kinect, IMU Proximity, face/body orientation, VFOA Meetings (dyad) NB, SVM Vocal Entrainment Detection [106] ARM Prosody (pitch, energy) Couples\u2019 int. (dyad) Statistical sequence modeling [67] ARM Prosody (pitch, energy) Couples\u2019 int. (dyad) SVM [107] ARM Prosody (pitch, intensity), speech rate, MFCCs Couples\u2019 int. (dyad) Correlation analysis Group Cohesion Estimation [77] HSM, CUC Speaking activity, prosody, head / body motion Meetings (4) NB, SVM [133] HSM Speaking rate and frequency Meetings (3-8) NB [208] SB Speaking activity, energy and consistency of the move-\nment Meetings (6) LR, linear SVM, RF\n[172] NA OpenSMILE acoustic features [51], appearance Youtube videos CNN, LSTM Rapport / Empathy Detection [73] CUC Posture Meetings (dyad) NB, SVM, MLP [151] CUC Head nod/shake/tilt, eye gaze, smile, self-touch Meetings (dyad) SVM [175] CUC, LAM Head nod / shake, forefinger raise / raise-like / wag, hand\nwag/scissor, shoulder shrug, prosody Political debates (5) Canonical Correlation w/\nMulti-view Hidden CRF [199] FFM Prosody (pitch, jitter, vocal energy, shimmer) Motivational Interviewing\n(dyad) Linear SVM\n[152] CUC, CTM Smile, posture, VFOA, eye gaze, prosody (voice quality, pitch, energy, spectral stationarity) Meetings (dyad) Linear SVM [197] FFM MFCCs, pitch, turn-taking cues Motivational interviewing (dyad) LR, linR [116] Unidirectional CTM Prosody (intensity, pitch, voice quality, and speaking rate) Collaborative problemsolving (dyad) Correlation analysis [200] FFM Speech rate Motivational interviewing (dyad) Linear SVM [128] LAM, CUC, FSC, CTM Speaking activity, prosody, hand motion, head orientation, FAU Meetings (3-4) SVM w/ RBF"
        },
        {
            "heading": "3.3 Interaction Dynamics",
            "text": "Interaction dynamics is a complex phenomenon related to individuals\u2019 traits and their roles in the dyad/group. Its analysis includes but is not limited to, the detection of group conversational context, engagement, involvement, group cohesion, empathy, and rapport. Moreover, vocal entrainment is a well-known conversational phenomenon in which the interactants show a synchronization of words and/or speaking style during their conversation. It has a high correlation with engagement, rapport, and even empathy. Through nonverbal behavior analysis, it is possible to detect whether there is a high/low group performance [210], quantify interaction quality [104], or predict group satisfaction level [105]. Below, we review the corresponding computing studies in depth while their summarization is given in Table 3.\nGroup Conversational Context. Computing studies performed the automatic recognition of group conversations in terms of the context: brainstorming vs. decision-making [82], formal vs. informal [122], focused vs. unfocused [15] and scenario vs. non-scenario [124] interactions. Given that each study focused on different contexts and was tested on different datasets and scenarios, we review them independently by avoiding comparing them in terms of the performance of the used nonverbal cues and computational models. Still, it is noticeable that i) every study utilized the speaking activity as a cue, ii) three of four works applied SVM for prediction, and iii) three of four works used wearable sensors. In [82], brainstorming and decision-making interactions were discriminated by analyzing nonverbal behaviors of individuals (e.g., total speaking length, total speaking turn) and groups (e.g., group speaking length distribution, group speaking turn distribution) when the data was collected with a privacy-sensitive mobile sociometer during meetings. It was found that Manuscript submitted to ACM\nthe fraction of silence, the fraction of overlapped speech, and group speaking length are the most effective features. That study [82] differs from others in terms of the sensor utilized, i.e., it did not apply the classical setup of meetings with microphones and cameras. Consequently, participants were able to move freely (notice that authors did not report any sensor error) and were not obliged to sit at a table as majorly happens in several other meeting datasets. On the other hand, the workplace social interactions were analyzed in [122] by using smartphones and ACC. Among interpersonal distance, relative body orientation, and speech-based nonverbal features, the speaking activity was the most dominant one. Since the dataset used in [122] was in the wild and long-durational, the findings of that study are remarkable. The speaking activity was quantified in terms of a) participant\u2019s involvement, b) the cycles of high/low activity, and c) interruptions to discriminate scenario and non-scenario meetings with parallel episodes [153] technique in [124]. That study was unique due to the computational method it used. Bano et al. [15] defined focused interactions as the ones the co-present individuals have mutual VFOA, establishing face-to-face engagement and direct conversation. They proposed an online SVM-based classifier to distinguish unfocused from focused interactions in egocentric videos. Visual face track scores, camera motion profiles, and speaking activity showed the best performance when they were fused. That has been the only study detecting in-the-wild indoor/outdoor interactions captured from the first-person perspective and adapting an online learning algorithm.\nEngagement, Involvement and Interest Level. The computing approaches performing automated detection of engagement, involvement, and interest level were tested on: meetings [16, 140], speed-dates [91, 190] and poster sessions [94]. The spectrum of nonverbal cues used is wide and ranges from proximity [190] to motion [91, 190], gaze analysis [16, 94, 140] and acoustic features [195]. Cameras and microphones were dominantly used, instead, a few studies utilized the data sensed by the eye trackers and ACC. The findings of the reviewed research are as follows. Veenstra et al. [190] showed that the video-based features (position, proximity, and body motion) perform better than audio-only systems using standard SVM and k-NN for the engagement detection in speed dates (dyads composed of a male and a female). Xiao et al. [198] distinctively relied on Approach-Avoidance (AA) coding that measures the involvement and immediacy and specifically focuses on the salient events in dyadic interactions, which trigger change points in AA code in time. Within this approach, body motion, pose, and vocal energy features were modeled by an SVM, concluding that visual cues are more reliable compared to vocal energy to make decisions on salient AA events. Bednarik et al. [16] demonstrated that gaze patterns: fixations, and saccades captured with eye trackers and modeled with an SVM are effective in detecting engagement and its level (i.e., no interest, following, responding, conversing, influencing, governing the discussion) in meeting scenarios. A limitation of that work [16] is not performing an ablation study for the window length (fixed to 15 seconds) in which the analysis was being performed. Oertel et al. [140] successfully distinguished the levels of group involvement with GMMs and the gaze patterns (e.g., the fraction of subjects looking at other subjects, mutual gaze, and the maximum fraction of subjects looking at the same target) in a role-playing game. Kawahara et al. [94] showed that the visual cues (gaze occurrence frequency, gaze duration) and particularly the gaze occurrence frequency contribute much more than verbal backchannels (e.g., \"yeah\", \"ok\") to estimate the interest level of the audience in poster sessions where people stand. Wang et al. [195] found out that acoustic features (e.g., low-level acoustic energy, speech duration, voice quality features, MFCCs) represented for each utterance dominate all lexical features to automatically detect the level of interest. The conclusions of that study are debatable since the language of the used datasets was German while the model used for prosodic analysis was trained in English. To automatically detect the attraction (notice that this is relevant to engagement) in speed dates using movement features that were captured by single body-worn ACC, Kapcak et al. [91] first time proposed motion convergence (measuring if two people\u2019s behavior style is symmetric) and proved its effectiveness with LR. The research Manuscript submitted to ACM\nefforts so far discussed in this subsection relied on annotations based on the perception of third persons. Even though studies significantly differ from each other in terms of the datasets used for model evaluation, one can still conclude that: i) overall, visual cues performed better than acoustic features, ii) acoustic features in some cases were preferable to lexical features, and iii) gaze activity and body motion were among the most effective cues to detect the phenomena of interest.\nGroup Performance. Automatic detection of group performance is a task investigated mostly in small groups (3-4 people) in which people make a decision regarding a (pre-defined) task [10, 11, 102, 114, 126, 136, 210]. That topic is important since an intelligent detection system can provide information aimed at enhancing the performance and efficiency of a group and its members. This can be used by managers to better understand the dynamics of their groups, improve the productivity of meetings, and prevent organizations from wasting time and money [102]. Meeting environments constituted to capture relevant data, were equipped with microphones (ARM, LAM, HSM) and/or cameras (WC). The findings, technical novelties, and limitations of the reviewed works are summarized as follows. Nihei et al. [136] showed that the influential statements in group discussions, which affect the discussion flow and are highly related to group performance, can be automatically predicted in terms of prosody, VFOA, and head motion features modeled with SVM, RF, and LR. By modeling audio-visual behavioral features with multivariate binary IM, [10] demonstrated that different group performance clusters have different interaction types. The same authors extended their feature space in [11] by involving self-reported features regarding personality, perception-related, and hierarchy in the group and showed that self-reported features were important while the group-looking cues and the influence cues (i.e., the confidence score IM model produces) were major predictors for group performance while IM outperformed HMM, on average. Linguistic features leveraged the performance of acoustic features (MFCCs, associated delta features, jitter, shimmer, PCM loudness, F0 envelope, F0 contour, voicing probability, and log power of Mel-frequency bands) to predict group performance in task-based interactions in [131]. The same study also presented an interesting data augmentation strategy. First, a model with the target dataset was trained and used to annotate an auxiliary dataset. Then, a new model for the prediction of group performance was trained with the target and the auxiliary datasets. That final model was used to make predictions on the test split of the target dataset, resulting in improved performance compared to not applying data augmentation. We believe that such an approach can be adapted for other traits\u2019 automatic detection to increase the model\u2019s accuracy. All aforementioned efforts were limited to being tested on relatively small-scale datasets. Instead, Zhong et al. [210] proposed a novel network composed of DNN and BLSTM with an attention mechanism, which models the vocal behaviors and personality to predict the group performances during collaborative problem-solving tasks, tested on a large data collection. Including personality attributes showed 14% performance improvement while vocal behaviors were more significant in the high-performing groups than the low-performing groups [210]. Lin et al. [114] utilized conversation dynamics as the graph to aggregate group members\u2019 speech and lexical behaviors within a deep model surpassing the SOTA results of several datasets. The experimental analysis in [114] was extensive compared to its counterparts and as being validated on three diverse datasets while performing the best out of all, it provided more robustness. It is notable that nonverbal features tend to be used together with lexical features for the detection of group performance. This situation is different from other phenomena (reviewed in previous subsections) that were detected only with nonverbal cues. Overall, using the features representing personality in addition to the acoustic features improved the detection performance. The only work analyzing the long-term interactions collected from wearable sensors in this domain is [207], which analyzed the continuous tracks of six persons for four months in a mission with SB. That study showed that considering behaviors of individuals at different temporal resolutions contributes.\nManuscript submitted to ACM\nGroup Satisfaction. Estimating group satisfaction automatically is crucial for developing strategies for computeraided decision-making and human-robot interaction while also being important to understanding the cognitive state of individuals. However, this topic (at least after 2010) has not been studied much within co-located human-human social interaction analysis. Lai et al. [105] performed regression analysis showing that combining acoustic, lexical, and turn-taking features improves performance while prosodic features are good at attention satisfaction prediction and voice quality is more informative for predicting information overload. The weak points of that paper [105] are; not validating their findings on other datasets and not proposing novel computational methods or nonverbal cues. On the other hand, a recent short paper [85] presented the estimation of group satisfaction by clustering the features composed of pronoun usage and coordination (extracted by Convokit [36], a measure which reflects the change in speaking patterns to become more linguistically similar to others within groups). That approach is innovative because of its concept and also relies on clustering rather than predicting a particular outcome of interest. However, its experimental analysis is limited to a single dataset. It is noticeable that both studies focused on speaking activity and there exists no work investigating the effectiveness of several other cues (especially vision-based) for the automatic group satisfaction estimation.\nQuality of Interactions. There exist studies that addressed the automated estimation of the quality of co-located humanhuman social interactions using physiological signals such as [37, 38] which processed electrodermal activity. On the other hand, it is well-known that physiological activity is correlated with nonverbal behaviors and vice versa. Though, surprisingly, there exists only one work [104], which implemented detection of the quality of interactions through nonverbal behavior analysis. Lahnakoski et al. [104] considered movement synchrony in addition to several other nonverbal features modeled with NB and SVM. It was exhibited that a) proxemic behaviors best predict the quality of the interactions rather than interpersonal movement synchrony, b) increased distance between participants predicts lower enjoyment, c) increased joint orientation towards each other during cooperation is correlated with increased effort, and d) the interpersonal distance is not informative to estimate the quality of the interaction. These results were obtained by processing the data captured with Kinect and IMU sensors. The related literature lacks the use of acoustic features, multimodal cues, and deep models while all these options have the potential to boost the detection performance.\nGroup Cohesion. Research in social and organizational psychology has shown that good cohesion between group members is correlated with team effectiveness, productivity, and performance. Cohesion is split into two: social and task cohesion. Social cohesion refers to the attractiveness (likeliness) of group members towards each other [133]. This can be extracted by asking questions such as whether the teammates appeared to be involved/engaged in the discussion or have a good rapport. Task cohesion, on the other hand, is related to how much a group is reaching its goals in a shared way. Therefore, one can measure task cohesion by asking questions such as whether the group members share responsibilities and goals or whether the teammates are collaborative [77]. Meetings have been suitable environments to perform automatic group cohesion detection [77, 133, 208]. Unlike other works, [172] analyzed diverse interactions by processing YouTube videos containing events such as interviews, festivals, and parties. The major findings and novelties are discussed as follows. Hung et al. [77] estimated the high/low group cohesion using audio (e.g., speaking energy, turn duration, speaking time), video and audiovisual cues (e.g., upper body motion during the overlapping speech, motion when not speaking) with NB and SVM. The best-performing feature was found as a turn-taking feature which is the accumulation of the total pause duration between each individual\u2019s turns. Instead, mimicry features in terms of speaking rate and frequency (defined as how well subsequent samples from subjects\u2019 audio fit the learned distributions from the other participants) performed better than the turn-taking features of [77] when they were modeled with a Gaussian Manuscript submitted to ACM\nNB, moreover, for social cohesion, the performance of the turn-taking and mimicry features were found comparable [133]. Differently, aggregating the behavior patterns extracted from individuals\u2019 SB data with the group-level interactions improved the effectiveness of assessing group cohesion in meetings composed of up to six persons in [208]. Sharma et al. [172] proposed a two-head (one for visual data and the other for acoustic data) DNN architecture composed of CNNs and LSTMs. To obtain visual and acoustics embeddings, the Inception V3 model pre-trained on GAF-cohesion database [68] and OpenSMILE [51] tool were used, respectively. That work was unique as it is based on deep learning for group cohesion detection tasks. Overall, all works included acoustic features, implying that they are effective for such an automated task, and the performance could further be boosted by the integration of head and body movement cues [77, 208].\nVocal Entrainment. Interaction synchrony in human-human conversations can occur naturally and spontaneously. Vocal entrainment is the phenomenon in which the interactants show a synchronization of speaking style during their conversation. The automatic detection of such a phenomenon was investigated in dyads when both participants were wearing ARM [67, 106, 106]. Reviewed works focused on discovering the effective acoustic features with statistical modeling [106], in terms of correlations [107] and with SVM [67]. There exists no work presenting a data-driven approach and/or processing the raw audio data without requiring an additional feature extraction step. Lee et al. [106] exhibited the success of turn-wise entrainment measures obtained from prosodic cues, specifically pitch and energy to understand the overall attitude of the interacting partners in problem-solving interactions. Georgiou et al. [67] built an SVM classifier with the features of [106] to analyze distressed dyadic interactions in terms of vocal entrainment and concluded that including video-based cues or body sensors\u2019 data can improve the task. Furthermore, KCCA in [107] showed that there exists a statistically significant relationship between vocal entrainment and withdrawal, which were explained based on the variations in the overall vocal engagement level in the problem-solving interactions. The experiments in all studies were limited to a single dataset targeting a very specific scenario: married couples\u2019 interactions.\nRapport and Empathy. Empathy is an important skill in developing rapport with a person [200]. It means close and harmonious relationships where the interaction patterns are synchronized while attention, positivity, and coordination are necessary [128]. Rapport is a complex social behavior, and low rapport can result in interpersonal conflicts and decreased collaboration. The data was captured with microphones (LAM, FFM, CTM) and cameras (CUC, FSC) to perform automatic detection of rapport and empathy during the meeting of 2-persons [73, 151, 152], 3-4 participants [128], political debates of up to 5-persons [175], motivational interviews of 2-persons [197, 199, 200] and collaborative problem-solving in dyads [116]. This implies that related works used different datasets, which does not allow us to make a performance comparison among them. Therefore, we discuss the findings with respect to the effectiveness of the nonverbal cues, as follows. Posture mirroring showed a powerful performance in detecting the rapport with NB, SVM, and MLP in [73]. Muller et al. [128] analyzed facial expressions, hand motion, gaze, the speaker turns, and prosody to detect low rapport, where facial features performed on average the best and incorporating the participant\u2019s personalities (which is a particular novelty of this work) as a prior knowledge contributed positively. MFCCs, pitch, and turn-taking features alone and combined were found significantly and highly correlated with empathy, respectively [199]. High pitch and energy were found negatively correlated with empathy [197]. In [200], the average absolute difference of turn-level speech rates between two people, silence duration, and other statistics of speech was exhibited as correlated with empathy. Furthermore, for automatically predicting high/low empathy with SVMs, speech rate cues were found effective, meaning that vocal entrainment contributes to empathy modeling [200]. On the other hand, [116] demonstrated the correlation between entrainment and rapport. In detail, the speakers appeared to entrain primarily by matching their\nManuscript submitted to ACM\nprosody on a turn-by-turn basis and the pitch was the most significant prosodic feature people entrain on when rapport was present. What differs [116] from [197, 199, 200] is the scenario relied on: motivational interviewing vs. collaborative problem-solving. The recognition of the agreement and disagreements is also related to empathy and rapport. Multi-view HCRF was presented to be effective in learning interactions in terms of the audio-visual cues (head nod, head shake, forefinger raise, forefinger raise-like, forefinger wagging, hand wag, hands scissor, shoulder shrug) and acoustic features (fundamental frequency and energy) when KCCA was used to capture the nonlinear hidden dynamics in [175]. Using head nodding, head shaking, head tilting, eye gaze, smiling, and self-touching together was suggested in [151] to predict the acceptances or rejections of proposals in a dyadic negotiation. The same authors [152] detected that symmetric smiles, symmetric postures, mutual gaze, symmetric and asymmetric head orientation, asymmetric eye gaze, asymmetric voice pitch, asymmetric voice quality, and asymmetric spectral stationarity are useful in predicting the respondent reactions. In conclusion, prosody, speaking, head, and body activities are competent for rapport and empathy detection. Future research can consider applying audio-visual deep learning methods.\nTable 4 \u2013 continued from previous page Ref. Scenarios GS / # Par. Annotations Sensors Pub\nMATRICS [136] Role-based meetings 4 / 40 Personality traits (S), (not)influential statements (E) MoCAP, ACC, Kinect, ET, FSC, WC, HSM KTH-Idiap Interview. [139]\nRole-play group interviews 4 / 20 Eye gaze (E), voice activity (E), personality traits (S), interest-level/performance/potential related questionnaire (E), performance (E) Kinect, GoPro, CTM \u2713\nMAHNOB [28] Dyadic discussions and negotiations 2 / 60 Mimicry of head gestures, hand gestures, body movement and facial expressions (E), reflective mimicry (E) CUC, HSM, FFM \u2713 SALSA [3] Indoor free-standing conv. 18 Personality traits (S), position (E), head/body orientation (E), F-formation (A)\nFSC, SB (microphone, infrared beam detector, Bluetooth detector, and ACC) \u2713\nPAVIS Leadership [17, 20] Meeting (survival task) 4 / 64 Leadership (S, E), designate leaders (S), leadership style (E), voice activity (E), face-touching behavior (E) LAM, CUC \u2713 MobileSSI [60] Free discussions in the pub 3 Voice activity (E), laughter (E) LAM, smartphone (ACC) [184] Meeting, topic-defined conv. 2 / 48 Head nodding (E), conv. usefulness (E), sustained concentration (E), competence (E), satisfaction level (E) Smart glasses, CUC [133] Business meeting 3-8 / 107 Cohesion (E) ARM, HSM, HD cameras, ACC [142] Storytelling events 14 Interest level (E), observed action (E) ACC, Gyro, HD camera MatchNMingle [32] Indoor free-standing conv. and speed dates\n8-2 / 94 Personality traits (S), self-control scale (S), sociosexual orientation inventory (S), social cues (E), social cautions (E), F-formations (E) Triaxial ACC, Prox., GoPro, TVC \u2713\nMULTISIMO [101] Meeting (solving a quiz) 3 / 49 Personality traits (S, E), experience (S), speaking activity (A), dominance (E), transcripts (E), turn-taking (A), emotions (A) CUC, C360, HSM, ODM, Kinect \u2713 AMIGOS [125] Watching videos 4 / 20 Personality traits (S), mood (S), valence (S, E), arousal (S, E), dominance (S), liking (S), familiarity (S), emotions (S) EEG, ECG and skin conductance sensors, CUC, Kinect \u2713 MPII Group Interaction [128]\nMeeting, topic-defined conv. 3-4 / 78 Rapport (E), leadership (S), dominance (S), competence (S), liking (S), personality traits (S) LAM, CUC, FSC, CTM \u2713\nFocused Interaction [14] Daily-life indoor/outdoor int. 19 sessions (No)focused interaction (E), voice activity (A) GoPro, ACC, GYRO, MAG, smartphone (GPS) \u2713 UGI [26] Meeting (survival tasks) 4 / 36 Perceived leadership (E), perceived contribution (E) Time-of-flight sensors, Kinect, LAM \u2713 SRIV [118] Movies and TV dramas 69 movies, 3124 videos Subjective and objective social relations (E) DMC TeamSense [208] Four-month simulation of a space exploration mission 6 Roles (E), affective states (S), team cohesiveness (S)\nSB (microphone, infrared beam detector, Bluetooth detector, and ACC)\nPanoptic [86] Standing game-play 3-12 Roles (E), speaking activity (E) HD and VGA cameras, Kinect \u2713\nContinued on next page\nManuscript submitted to ACM\nTable 4 \u2013 continued from previous page Ref. Scenarios GS / # Par. Annotations Sensors Pub\nGAP [30] Meetings (survival task) 2-3 / 37 Group satisfaction (E), group decision-making (E), dialogue acts (E), sentiment (E), transcription (E) WC, handy cam \u2713 VGAF [172] Web videos 1004 videos Group-level emotion (E), group-level cohesion (E) DMC \u2713 EgoSocialRelation [1, 2] Daily-life indoor and outdoor int. 8 first-person / 100 Sequence annotation (E), face locations (E), social int. state (E), social relations (E) Narrative camera \u2713 [93] Social networking event 22 Social groups (E) Smartphone (iBeacon Prox., ACC, Gravity, rotation rate), HD cameras ViSR [115] Movies 8000 clips Social relations (E) DMC \u2713 [47] Healthcare simulations 4 / 9 Roles (E), collaboration translucence (E) Skin conductance\nsensor, ACC, ARM, HD camera\nResistance Game [12] Game-play 5-8 / 233 Roles (E), dominance (E) HD video camera \u2713 NTHULP [210] Role-based meetings 3 / 194 Roles (E), personality traits (S), group perfor-\nmance outcome (E) CUC, LAM\n[146] Game-play, standing conv. 4 / 96 Transcriptions (E), text binary content parameters (E), group contribution (E) CUC, HSM GAME-ON [120] Game-play, standing conv. 3 / 51 Cohesion (S), leadership (S), emotional state (S), warmth and competences (S)\nIMU (ACC, GYRO, MAG), infrared, RGB cameras, HSM \u2713\nWoNoWa [27] Indoor free-standing conv. performing a pre-defined task 3 / 45 Voice activity (A,E), quantity of motion (A), head position and rotation (A), chronemics (E), proxemics (E), kinesics (E), leadership (S), degree of acquaintance (S), warmth and competence (E), perceived transactive memory system (E) HD handy recorder, HSM \u2713 [145] Topic-predefined meeting, sitting participants 2 / 34 Dialogue data (E), praising skills (E) CUC, HSM [104] Topic pre-defined conv., cooperative and competitive game-play\n2 / 18 gaze (E), head location/orientation (E), enjoyment (E), choice (E), felt pressure (E), effort (E), personality traits (E) Kinect, IMU\n[204] Meeting (survival task) 3-4 / 48 Personality traits (S), contribution level (E), leadership (E) CUC, Kinect VYAKTITV [95] Daily-life conv. 2 / 25 Personality traits (S) CUC, FFM UDIVA [148] Competitive and collaborative\ntasks 2 / 147 personality traits (S, E), sociodemographics (S), mood (S), fatigue (S), social relation (S) LAM, CUC, ODM, egocentric CAM, wearable heart rate monitor \u2713\nMUMBAI [46] Board-game 4 / 58 Personality (S), game-playing experience (S), emotions (E), int. types, e.g., cooperative (E) CUC \u2713"
        },
        {
            "heading": "4 DATASETS",
            "text": "The reviewed works in Sec. 3 have been conducted using datasets that typically differ from each other in terms of a) the scenarios (e.g., interviewing, brainstorming in a meeting, poster presentation, game playing), b) interaction settings (e.g., the number of people involves, indoor/outdoor, standing/sitting), c) annotations (e.g., self-assessments of participants, the assessment made by observers), and d) the sensing technology used. In Table 4, we present a summary of co-located human-human social interaction datasets (listed chronologically), supplying annotations to detect social and psychological phenomena. As seen, the majority of the efforts were given to small group interactions, typically composed of four Manuscript submitted to ACM\npersons as well as dyads. When earlier datasets were more based on role-play interactions, recently, several datasets were collected in less unconstrained environments (e.g., poster sessions, speed dates, pub discussions). There exist attempts to curate datasets from YouTube videos, movies, or from a first-person (egocentric) perspective. More discussions on datasets and our proposals regarding data collection can be seen in Sec. 5.2."
        },
        {
            "heading": "5 PROPOSED FUTURE RESEARCH DIRECTIONS",
            "text": "We propose several future research directions in the context of the automatic detection of social and psychological phenomena in co-located human-human social interactions through nonverbal behavior analysis. They are mostly motivated by the limitations of the works reviewed in Sec. 3 and Sec. 4. The content is divided into three: a) Artificial Intelligence (Sec. 5.1), b) dataset collection (Sec. 5.2), and c) privacy-preserving social interaction analysis (Sec. 5.3)."
        },
        {
            "heading": "5.1 Artificial Intelligence",
            "text": "This paper shows that the progress in the field of co-located human-human social interaction analysis is considerable while various AI concepts have been well integrated. The automatic detection of several social and psychological phenomena was addressed by (data-driven) deep learning methodologies, which show better performance compared to machine learning methods with hand-crafted features. Still, the deep models have not been applied for the detection of several phenomena. These are hirability detection, group conversational context classification, group satisfaction detection, estimating the quality of social interactions, vocal entrainment detection, and rapport/empathy detection. On the other hand, the adoption of deep learning has required the need for large-scale datasets. However, the labeling cost is presumably a limiting factor. In this respect, utilizing domain adaptation methods could be considered to label the new datasets. To date, only a few attempts have shown to perform analyses across datasets [8, 129], although there have been several datasets aiming to detect the same or related social phenomena, collected with very similar sensor setups and scenarios while using the same nonverbal signals. An alternative approach can be relying on unsupervised pre-training where the labels are not needed to learn the feature embeddings, and only a few annotated data can be used to evaluate the performance of the methods. Unsupervised pre-training was recently tested on several domains, e.g., HAR [150], multimodal emotion recognition [62, 100] and sentiment analysis [100]. Notice that, the last two domains both used nonverbal signals to make decisions and the effectiveness of unsupervised pre-training in [62, 100, 150] was on par with or even better than the several fully-supervised SOTA. Unfortunately, unsupervised pre-training has not been yet integrated into the automated detection of social and psychological phenomena in co-located human-human social interactions.\nAI models have been used to make important decisions regarding diverse social and psychological phenomena, and it is important to understand how models make predictions. This issue becomes even more important considering the fact that the SSP domain is very multi-disciplinary. To address this, explainability analysis (refers to the details and reasons for the prediction of a model that should be clear and easy to understand) should be performed. Some methodologies, like SVMs, MLPs, CNNs, and RNNs, lack algorithmic transparency and interpretability. Therefore, utilizing visualization techniques on images (such as heatmaps, attribution), using attention mechanisms (for instance Grad-CAM [169]), or applying feature visualization at different levels of a certain network are good alternatives to improve the trained models\u2019 explainability. Likewise, SHAP (SHapley Additive exPlanations) [117] allows us to understand how each feature impacts the model\u2019s outcome, and to extract the salient segments and features driving a model to make a decision. However, we noticed that the related art has so far stayed behind in addressing the explainability and interpretability of their proposed methods as only a few studies, e.g., [17, 24] have adapted the aforementioned or similar tools, allowing to achieve a better level of communication across experts of different disciplines. It is also essential to discuss the challenges in transferring Manuscript submitted to ACM\nhuman social interaction research into daily life. Many real-world challenges, such as missing sensors in the inference time and domain-shift problems, have not been investigated by the reviewed works. The integration of online learning methods and adaptive models could be considered with this respect. The authors also speculate that once less restrictive (i.e., in-the-wild) datasets are collected, the new methodologies trained on them can be more resilient to handle such real-life challenges."
        },
        {
            "heading": "5.2 Dataset Collection",
            "text": "Sec. 4 shows that major efforts were performed in collecting and annotating related datasets. However, not all these resources are publicly available (see Table 4 for details), and it is still relatively challenging to access data suitable for the experimental activity. This is a very important problem that should be addressed as it does not allow researchers to perform a comparative study. One possible reason for not supplying publicly available datasets is that collection and dissemination of human data is an activity that must respect rigorous ethical standards ensuring the protection of individuals\u2019 privacy. The most suitable solution is the involvement of ethical committees operating in the institutions that collect the data and the definition of standard protocols to be respected by the whole research community.\nThe size of the existing datasets tends to be limited compared to other research areas. As an example, if we consider the Affective Computing domain, one can see much larger datasets (e.g., CMU-MOSEI [202] having 23500 video clips, AffectNet [127] having millions of annotated images). Whereas the reviewed studies used datasets having on the average up to 50 participants and the number of videos reaches up to 3000 even though collected through crowd-sourcing (see Table 4). This is a problem for the application of AI methods such as deep learning that typically requires large amounts of data to be effectively trained. Furthermore, models trained on limited material might not generalize well, and thus perform as desired in some conditions but not in others. In this respect, creating large-scale benchmarks such as ImageNet [162] (a large visual database designed for use in visual object recognition) or ActivityNet [52] (benchmark aims at covering a wide range of complex human activities) would contribute to the field positively. The creation of such benchmarks would allow testing and more importantly comparing different methodologies to extract and/or learn in a data-driven way the nonverbal behaviors, which are further used for the detection of various social and psychological phenomena. Another important thing to consider for collecting data is to rely on real-world scenarios. It is also interesting to collect long duration data where participants are involved in multiple tasks and diverse environments and interact with various people for several days or weeks. Interpersonal behavior differences especially the ones arising due to ethnic diversity are a challenge for social behavior analysis. Therefore, future work can focus on collecting datasets including subjects with diverse cultural backgrounds as well.\nThe experiments were often made by use of psychometric questionnaires. In some cases, they are self-assessments provided by the participants whereas in others they are assessments about the participants provided by external observers. In both cases, the questionnaire outcomes are used as labels for the evaluation of the approaches. Therefore, in the case of assessments made by multiple observers, it would be important to analyze the reliability of the judgments, i.e., whether the different observers actually agree beyond the chance level. The aforementioned reliability analysis is still not in the common practice of reviewed datasets. For instance, studies [4, 5, 12, 14, 15, 25, 26, 47, 55, 73, 101, 103, 118, 119, 122, 136, 145, 155, 174, 182, 186, 187, 201, 204] neither discussed the reliability of the annotations nor showed the trustfulness of the ground-truth data by applying quantitative inter-rater agreement analysis.\nManuscript submitted to ACM"
        },
        {
            "heading": "5.3 Privacy-preserving Social Interaction Analysis",
            "text": "Recording and storing signals from sensors might violate the privacy of a person especially when consent has not been explicitly obtained. Besides, preserving the privacy of individuals is essential once the research is transferred into real-world applications. There exist related works considering the usage of privacy-sensitive sensors such as [82] utilizing mobile sociometer. However, our review shows that there is no study performing privacy-preserving nonverbal signal processing. Haider and Luz [74] presented a low-cost system that was used to extract audio features while the actual spoken content was protected. Sajadmanesh and Gatica-Perez [163] developed a privacy-preserving GCNN learning algorithm based on Local Differential Privacy, which is important to apply for the problems where graph nodes contain sensitive features that need to be kept private while they could be beneficial for a central server for training the GCNN. But, neither these studies [74, 163] nor others have been integrated into the detection of social and psychological phenomena. Future research could focus on answering the questions: a) what are the privacy-preserving nonverbal signal representations? (furthermore, is not using features extracted from appearance enough, is using the audio features having low linguistic information enough?), b) What are the privacy-preserving sensors, c) How to prove that one approach is more privacy-sensitive than another? (for instance, can lower intelligibility be used for that?), and d) Can we preserve the performance when using more privacy-sensitive features?"
        },
        {
            "heading": "6 CONCLUSIONS",
            "text": "This survey paper has gathered together the research efforts shown regarding the automatic analysis of nonverbal cues displayed in co-located human-human social interactions. We assembled peer-reviewed papers since 2010, which proposed methodologies to automatically analyze and estimate social and psychological phenomena including social traits, social roles, social relations, and several others related to interaction dynamics. Our article significantly differs from the earlier surveys due to its broad coverage. Additionally, we provide a review of related datasets. We also propose several future research directions, not yet being addressed by the SSP-oriented HBU studies. Our generalizable key observations are outlined as follows. i) The studies investigating the same phenomena diverge from each other in terms of the computational methodology they proposed, the nonverbal cues they used, the interaction environment and/or scenarios they tackled, or the sensors they utilized. Addressing one or a couple of such items in a different way from the prior art made the more recent novel/original. ii) Several reviewed papers showed that using multimodal features improves inference performance. iii) The applied computational methods are very varied. Some studies presented novel pipelines rather than only applying mainstream methods. In general, the deep models improved the performance significantly for the automatic detection of some phenomena. However, SVM was the classifier that was utilized the most. iv) Automatic detection of personality traits was studied the most out of all social phenomena. v) The speaking activity was the most preferable nonverbal feature group. vi) Meeting environments, composed of 3-4 persons, were the most commonly investigated interaction scenario. Consequently, microphones and cameras were used more than other sensors to capture the data. vii) The proposed methods, targeting the detection of the same phenomena, were rarely tested on common datasets.\nGiven the above observations, we further determine the following limitations. a) Several datasets are not publicly available. This impairs the reproducibility of the results as well as not allows researchers to compare their method\u2019s performance with others. b) There are datasets that lack annotation reliability analysis. Usage of them might result in misleading findings. c) The datasets\u2019 scalability is not at the level of some other research domains using Artificial Intelligence. This can limit the application of advanced learning methods. Moreover, the models trained on limited material might not generalize well, thus they can perform well in some conditions but not in others. d) The possible\nManuscript submitted to ACM\neffectiveness of the deep models has not been investigated for several phenomena. e) There were a limited number of attempts to perform cross-dataset analysis [8, 129], although there have been datasets supplying annotations for the same/related social phenomena, which were collected with similar sensor setups and scenarios allowing to extraction of the same nonverbal signals. Presenting such an analysis can give an idea regarding the potential performance of a proposed method during its real-life implementation as cross-domain analysis simulates the domain-shift problem. f) Reviewed works lack explainability analysis and the usage of tools to improve interpretability. g) We have not observed a discussion about how to transfer the presented human social interaction research into daily life by handling deployment time challenges such as domain-shift, and missing sensor data.\nMotivated by the key observations and the limitations, we propose the future research directions: 1) adapting unsupervised feature pre-training, which can lower the need for labeled data compared to fully supervised methods, 2) integration of the domain adaptation methods for efficient data collection, 3) collecting more in-the-wild, long durational datasets, which would improve the generalizability of the developed models, 4) creating benchmark datasets to be used to compare different methodologies to extract and/or learn the nonverbal behaviors in a data-driven manner, 5) developing privacy-preserving nonverbal behavior representations, sensors, and computational models, and 6) implementing online learning methods and adaptive models in order to handle real-life challenges in a better way. We believe that the research reviewed in this paper and the future work adapting our recommendations would result in effective Human Behavior Understanding that can be integrated into systems such as intelligent vehicles and social robots."
        },
        {
            "heading": "A APPENDIX: PAPER SELECTION METHODOLOGY",
            "text": "Fig. 3 demonstrates the paper selection methodology applied in this survey. We retrieved 1758 documents from SCOPUS with the search phrases \u201chuman\u201d AND \u201cnonverbal\u201d AND \u201cinteraction\u201d on titles, abstracts, and keywords when the date was selected higher or equal to 2010. Following that, we applied for an initial review of the titles and the abstract allowing us to discard the studies: a) detecting/extracting nonverbal cues without performing social and psychological phenomena, b) performing affective computing only, c) focusing on human-robot, virtual agents interactions, d) not applied on co-located interactions, e) focusing on health-related applications. As a consequence of this initial review, we obtained 486 documents. Such documents were categorized into surveys, datasets, and social and psychological phenomena methodology papers. We further made a detailed review in order to discard the papers such as not including a computational method (see Fig. 3 for more details). Meanwhile, we eliminated non-computing studies (e.g., psychology papers) as well. At that stage, we figured out that we missed some recent studies, perhaps because the proceedings they were involved in were not yet synchronized with SCOPUS. To confirm this, we additionally checked each paper\u2019s citations in GOOGLE SCHOLAR by limiting our review to the date \"<\" 2021. That allowed us to include 5 more peer-reviewed research papers matching our scope. Finally, our survey includes 116 computing studies in addition to 51 dataset papers. We also discuss 9 related surveys allowing us to express our contributions with respect to them."
        }
    ],
    "title": "Co-Located Human-Human Interaction Analysis using Nonverbal Cues: A Survey",
    "year": 2023
}