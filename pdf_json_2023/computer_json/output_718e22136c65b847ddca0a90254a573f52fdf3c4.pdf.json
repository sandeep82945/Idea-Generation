{
    "abstractText": "In recent years, self-supervised learning (SSL) has emerged as a promising approach for extracting valuable representations from unlabeled data. One successful SSL method is contrastive learning, which aims to bring positive examples closer while pushing negative examples apart. Many current contrastive learning approaches utilize a parameterized projection head. Through a combination of empirical analysis and theoretical investigation, we provide insights into the internal mechanisms of the projection head and its relationship with the phenomenon of dimensional collapse. Our findings demonstrate that the projection head enhances the quality of representations by performing contrastive loss in a projected subspace. Therefore, we propose an assumption that only a subset of features is necessary when minimizing the contrastive loss of a mini-batch of data. Theoretical analysis further suggests that a sparse projection head can enhance generalization, leading us to introduce SparseHead a regularization term that effectively constrains the sparsity of the projection head, and can be seamlessly integrated with any self-supervised learning (SSL) approaches. Our experimental results validate the effectiveness of SparseHead, demonstrating its ability to improve the performance of existing contrastive methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zeen Song"
        },
        {
            "affiliations": [],
            "name": "Xingzhe Su"
        },
        {
            "affiliations": [],
            "name": "Jingyao Wang"
        },
        {
            "affiliations": [],
            "name": "Wenwen Qiang"
        },
        {
            "affiliations": [],
            "name": "Changwen Zheng"
        },
        {
            "affiliations": [],
            "name": "Fuchun Sun"
        }
    ],
    "id": "SP:6420926e579c5794fb5cd783e8d23e493b4c1a61",
    "references": [
        {
            "authors": [
                "Jordan T. Ash",
                "Surbhi Goel",
                "Akshay Krishnamurthy",
                "Dipendra Misra"
            ],
            "title": "Investigating the Role of Negatives in Contrastive Representation",
            "year": 2021
        },
        {
            "authors": [
                "Pranjal Awasthi",
                "Nishanth Dikkala",
                "Pritish Kamath"
            ],
            "title": "Do More Negative Samples Necessarily Hurt in Contrastive Learning",
            "year": 2022
        },
        {
            "authors": [
                "Randall Balestriero",
                "Yann LeCun"
            ],
            "title": "Contrastive and Non-Contrastive Self-Supervised Learning Recover Global and Local Spectral Embedding Methods",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Adrien Bardes",
                "Jean Ponce",
                "Yann LeCun"
            ],
            "title": "VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning, January 2022",
            "year": 2022
        },
        {
            "authors": [
                "Yoshua Bengio",
                "Aaron Courville",
                "Pascal Vincent"
            ],
            "title": "Representation learning: A review and new perspectives",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2013
        },
        {
            "authors": [
                "Kevin Beyer",
                "Jonathan Goldstein",
                "Raghu Ramakrishnan",
                "Uri Shaft"
            ],
            "title": "When is \u201cnearest neighbor",
            "venue": "Database Theory\u2014ICDT\u201999: 7th International Conference Jerusalem, Israel, January",
            "year": 1999
        },
        {
            "authors": [
                "Jinkun Cao",
                "Ruiqian Nai",
                "Qing Yang",
                "Jialei Huang",
                "Yang Gao"
            ],
            "title": "An Empirical Study on Disentanglement of Negative-free Contrastive Learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Mathilde Caron",
                "Ishan Misra",
                "Julien Mairal",
                "Priya Goyal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Unsupervised learning of visual features by contrasting cluster assignments",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Rich Caruana"
            ],
            "title": "Multitask learning",
            "venue": "Machine learning,",
            "year": 1997
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A Simple Framework for Contrastive Learning of Visual Representations",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Kaiming He"
            ],
            "title": "Exploring Simple Siamese Representation Learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Ching-Yao Chuang",
                "Joshua Robinson",
                "Yen-Chen Lin",
                "Antonio Torralba",
                "Stefanie Jegelka"
            ],
            "title": "Debiased contrastive learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Adam Coates",
                "Andrew Ng",
                "Honglak Lee"
            ],
            "title": "An Analysis of Single-Layer Networks in Unsupervised Feature Learning",
            "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,",
            "year": 2011
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "ImageNet: A large-scale hierarchical image database",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Linus Ericsson",
                "Henry Gouk",
                "Chen Change Loy",
                "Timothy M. Hospedales"
            ],
            "title": "Self-Supervised Representation Learning: Introduction, advances, and challenges",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2022
        },
        {
            "authors": [
                "Aleksandr Ermolov",
                "Aliaksandr Siarohin",
                "Enver Sangineto",
                "Nicu Sebe"
            ],
            "title": "Whitening for Self-Supervised Representation Learning, May 2021",
            "year": 2021
        },
        {
            "authors": [
                "Chelsea Finn",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Claudia Flexeder"
            ],
            "title": "Generalized Lasso Regularization for Regression Models",
            "venue": "PhD thesis, Institut fu\u0308r Statistik,",
            "year": 2010
        },
        {
            "authors": [
                "Quentin Garrido",
                "Yubei Chen",
                "Adrien Bardes",
                "Laurent Najman",
                "Yann Lecun"
            ],
            "title": "On the duality between contrastive and non-contrastive self-supervised learning",
            "venue": "arXiv preprint arXiv:2206.02574,",
            "year": 2022
        },
        {
            "authors": [
                "Songwei Ge",
                "Shlok Mishra",
                "Chun-Liang Li",
                "Haohan Wang",
                "David Jacobs"
            ],
            "title": "Robust contrastive learning using negative samples with diminished semantics",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jean-Bastien Grill",
                "Florian Strub",
                "Florent Altch\u00e9",
                "Corentin Tallec",
                "Pierre Richemond",
                "Elena Buchatskaya",
                "Carl Doersch",
                "Bernardo Avila Pires",
                "Zhaohan Guo",
                "Mohammad Gheshlaghi Azar",
                "Bilal Piot",
                "koray kavukcuoglu",
                "Remi Munos",
                "Michal Valko"
            ],
            "title": "Bootstrap Your Own Latent - A New Approach to Self- Supervised Learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Michael Gutmann",
                "Aapo Hyv\u00e4rinen"
            ],
            "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models",
            "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,",
            "year": 2010
        },
        {
            "authors": [
                "Jeff Z. HaoChen",
                "Colin Wei",
                "Adrien Gaidon",
                "Tengyu Ma"
            ],
            "title": "Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum Contrast for Unsupervised Visual Representation Learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Georgia Gkioxari",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Mask r-cnn",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Olivier Henaff"
            ],
            "title": "Data-efficient image recognition with contrastive predictive coding",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Tianyu Hua",
                "Wenxiao Wang",
                "Zihui Xue",
                "Sucheng Ren",
                "Yue Wang",
                "Hang Zhao"
            ],
            "title": "On feature decorrelation in self-supervised learning",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Huiwon Jang",
                "Hankook Lee",
                "Jinwoo Shin"
            ],
            "title": "Unsupervised Meta-learning via Few-shot Pseudosupervised Contrastive Learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Li Jing",
                "Pascal Vincent",
                "Yann LeCun",
                "Yuandong Tian"
            ],
            "title": "Understanding Dimensional Collapse in Contrastive Self-supervised Learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Yannis Kalantidis",
                "Mert Bulent Sariyildiz",
                "Noe Pion",
                "Philippe Weinzaepfel",
                "Diane Larlus"
            ],
            "title": "Hard negative mixing for contrastive learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Ching-Yun Ko",
                "Jeet Mohapatra",
                "Sijia Liu",
                "Pin-Yu Chen",
                "Luca Daniel",
                "Lily Weng"
            ],
            "title": "Revisiting contrastive learning through the lens of neighborhood component analysis: An integrated framework",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Simon Kornblith",
                "Jonathon Shlens",
                "Quoc V Le"
            ],
            "title": "Do better imagenet models transfer better",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Ya Le",
                "Xuan Yang"
            ],
            "title": "Tiny imagenet visual recognition challenge",
            "venue": "CS 231N,",
            "year": 2015
        },
        {
            "authors": [
                "Dong Bok Lee",
                "Seanie Lee",
                "Kenji Kawaguchi",
                "Yunji Kim",
                "Jihwan Bang",
                "Jung-Woo Ha",
                "Sung Ju Hwang"
            ],
            "title": "Self-Supervised Set Representation Learning for Unsupervised Meta-Learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Hankook Lee",
                "Kibok Lee",
                "Kimin Lee",
                "Honglak Lee",
                "Jinwoo Shin"
            ],
            "title": "Improving transferability of representations via augmentation-aware self-supervision",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jiangmeng Li",
                "Wenwen Qiang",
                "Yanan Zhang",
                "Wenyi Mo",
                "Changwen Zheng",
                "Bing Su",
                "Hui Xiong"
            ],
            "title": "Metamask: Revisiting dimensional confounder for self-supervised learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jiangmeng Li",
                "Wenwen Qiang",
                "Changwen Zheng",
                "Bing Su",
                "Hui Xiong"
            ],
            "title": "Metaug: Contrastive learning via meta feature augmentation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Yazhe Li",
                "Roman Pogodin",
                "Danica J Sutherland",
                "Arthur Gretton"
            ],
            "title": "Self-supervised learning with kernel dependence maximization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In European conference on computer vision,",
            "year": 2014
        },
        {
            "authors": [
                "Xin Liu",
                "Zhongdao Wang",
                "Yali Li",
                "Shengjin Wang"
            ],
            "title": "Self-supervised learning via maximum entropy coding",
            "venue": "arXiv preprint arXiv:2210.11464,",
            "year": 2022
        },
        {
            "authors": [
                "Renkun Ni",
                "Manli Shu",
                "Hossein Souri",
                "Micah Goldblum",
                "Tom Goldstein"
            ],
            "title": "The Close Relationship Between Contrastive Learning and Meta-Learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Wenwen Qiang",
                "Jiangmeng Li",
                "Changwen Zheng",
                "Bing Su",
                "Hui Xiong"
            ],
            "title": "Interventional Contrastive Learning with Meta Semantic Regularizer",
            "venue": "In Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Geoffrey Roeder",
                "Luke Metz",
                "Durk Kingma"
            ],
            "title": "On linear identifiability of learned representations",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Sebastian Ruder"
            ],
            "title": "An overview of multi-task learning in deep neural networks",
            "venue": "arXiv preprint arXiv:1706.05098,",
            "year": 2017
        },
        {
            "authors": [
                "Nikunj Saunshi",
                "Jordan Ash",
                "Surbhi Goel",
                "Dipendra Misra",
                "Cyril Zhang",
                "Sanjeev Arora",
                "Sham Kakade",
                "Akshay Krishnamurthy"
            ],
            "title": "Understanding Contrastive Learning Requires Incorporating Inductive Biases",
            "year": 2022
        },
        {
            "authors": [
                "Nikunj Saunshi",
                "Orestis Plevrakis",
                "Sanjeev Arora",
                "Mikhail Khodak",
                "Hrishikesh Khandeparkar"
            ],
            "title": "A Theoretical Analysis of Contrastive Unsupervised Representation Learning",
            "venue": "In Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Yonglong Tian",
                "Dilip Krishnan",
                "Phillip Isola"
            ],
            "title": "Contrastive Multiview Coding, December 2020",
            "year": 2020
        },
        {
            "authors": [
                "Robert Tibshirani"
            ],
            "title": "Regression shrinkage and selection via the lasso",
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological),",
            "year": 1996
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation Learning with Contrastive Predictive Coding, January 2019",
            "year": 2019
        },
        {
            "authors": [
                "Julius Von K\u00fcgelgen",
                "Yash Sharma",
                "Luigi Gresele",
                "Wieland Brendel",
                "Bernhard Sch\u00f6lkopf",
                "Michel Besserve",
                "Francesco Locatello"
            ],
            "title": "Self-supervised learning with data augmentations provably isolates content from style",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Haoxiang Wang",
                "Han Zhao",
                "Bo Li"
            ],
            "title": "Bridging multi-task learning and meta-learning: Towards efficient training and effective adaptation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Tongzhou Wang",
                "Phillip Isola"
            ],
            "title": "Understanding contrastive representation learning through alignment and uniformity on the hypersphere",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Yifei Wang",
                "Qi Zhang",
                "Yisen Wang",
                "Jiansheng Yang",
                "Zhouchen Lin"
            ],
            "title": "Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Zixin Wen",
                "Yuanzhi Li"
            ],
            "title": "The Mechanism of Prediction Head in Non-contrastive Self-supervised Learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Xi Weng",
                "Lei Huang",
                "Lei Zhao",
                "Rao Muhammad Anwer",
                "Salman Khan",
                "Fahad Shahbaz Khan"
            ],
            "title": "An Investigation into Whitening Loss for Self-supervised Learning, October 2022",
            "year": 2022
        },
        {
            "authors": [
                "Jure Zbontar",
                "Li Jing",
                "Ishan Misra",
                "Yann LeCun",
                "Stephane Deny. Barlow"
            ],
            "title": "Twins: Self-Supervised Learning via Redundancy Reduction",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Xiaohua Zhai",
                "Joan Puigcerver",
                "Alexander Kolesnikov",
                "Pierre Ruyssen",
                "Carlos Riquelme",
                "Mario Lucic",
                "Josip Djolonga",
                "Andre Susano Pinto",
                "Maxim Neumann",
                "Alexey Dosovitskiy"
            ],
            "title": "A large-scale study of representation learning with the visual task adaptation benchmark",
            "venue": "arXiv preprint arXiv:1910.04867,",
            "year": 2019
        },
        {
            "authors": [
                "Chaoning Zhang",
                "Kang Zhang",
                "Chenshuang Zhang",
                "Trung X. Pham",
                "Chang D. Yoo",
                "In So Kweon"
            ],
            "title": "How Does SimSiam Avoid Collapse Without Negative Samples? A Unified Understanding with Selfsupervised Contrastive Learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Yu Zhang",
                "Qiang Yang"
            ],
            "title": "An overview of multi-task learning",
            "venue": "National Science Review,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 7.\n08 91\n3v 2\n[ cs\n.L G\n] 1"
        },
        {
            "heading": "1 Introduction",
            "text": "The objective of self-supervised learning (SSL) lies in the extraction of valuable representations from unlabeled data[15, 5, 43]. One prominent method that has achieved significant success in SSL is contrastive learning[10, 52, 24]. This approach involves considering augmented samples as positive pairs, while others within the same batch serve as negative samples. The primary goal of contrastive learning is to minimize the distances between positive pairs, while simultaneously maximizing the distances between negative pairs.\nOne of the most famous framework of contrastive learning is SimCLR [10], which has achieved stateof-the-art performance in visual representation learning tasks. One particularly intriguing aspect of SimCLR is the introduction of a nonlinear projection head, which significantly enhances the quality of the learned representations. Empirical observations reveal that the quality of the features learned prior to the projection head, referred to as the representation, surpasses that of the features obtained after the projection head, known as the embedding, in downstream classification tasks (with an improvement of over 10%) [10, Section 4.2].\n\u2217Equal Contribution \u2020Corresponding author\nPreprint. Under review.\nDespite the effectiveness of the projection head, there is a limited understanding of its internal mechanism due to the lack of existing research on this topic. However, some recent works [28, 30] argue that the poor performance of embeddings is a result of a phenomenon called dimensional collapse. This phenomenon occurs when the learned embeddings span a low-dimensional space, leading to a loss of information.\nTo gain a deeper understanding of the role of the projection head, we conducted a series of empirical experiments, which are presented in Section 4. In these experiments, we visualized the log-scaled eigenvalues of the learned representations and embeddings obtained from different combinations of feature extractors and projection heads. Our findings are as follows: i) The inclusion of a projection head, whether non-linear or linear, leads to significantly larger eigenvalues in the learned representations compared to representations trained without a projection head. ii) When a projection head is incorporated, the eigenvalues of the embeddings are noticeably lower than those of the representations. iii) Even with the presence of a projection head, in cases where the representation space is larger, the dimensional collapse phenomenon cannot be entirely mitigated.\nBuilding upon the aforementioned findings, this paper addresses the core assumption that minimizing the contrastive loss of a mini-batch of samples can be achieved using only a subset of features from the representation space. This assumption is visually illustrated in Figure 2. To further explore the implications of this assumption, we conduct a theoretical analysis in Section 5, where we investigate the relationship between the sparsity of the projection head, the generalization ability, and the discriminative performance. This analysis serves as a motivation for our proposed method called SparseHead, which is a regularization term designed to effectively constrain the sparsity of the projection head. In Section 7, we evaluate the performance of SparseHead in various downstream tasks, aiming to enhance the quality of learned representations.\nOur contributions can be summarized as follows:\n\u2022 We empirically demonstrate the significant impact of incorporating a projection head in contrastive learning. This leads to improved quality of learned representations compared to representations trained without a projection head. Additionally, we highlight the effectiveness of projecting the representation into a lower-dimensional space. However, we also identify the limitations of simply adding a projection head.\n\u2022 We undertake a theoretical analysis to explore the impact of the sparsity of the projection head. Our investigation provides evidence supporting the claim that a sparse projection head can improve the generalization capabilities of learned representations. Additionally, we validate the correctness of the key assumption by demonstrating the discriminative ability of the learned representations in lower-dimensional spaces. Notably, we establish a lower bound for the discriminative ability, which is determined by the sparsity constraint hyperparameter.\n\u2022 We introduce SparseHead, a regularization term designed to effectively constrain the sparsity of the projection head. SparseHead can be seamlessly integrated with any selfsupervised learning (SSL) approaches. Through a series of experiments, we validate the effectiveness of SparseHead and demonstrate its ability to enhance both generalization and discriminative performance in learned representations."
        },
        {
            "heading": "2 Related Works",
            "text": "Contrastive Learning in Practice. Contrastive learning emerged as a powerful technique in the field of unsupervised representation learning, initially introduced from a mutual information perspective [22, 11, 24, 33, 52]. Notably, SimCLR [10] made significant strides in narrow the performance gap between unsupervised and supervised methods. However, when used in practical application, the performance of SimCLR is hindered by the need for a large number of negative samples. To relax the need of numerous negative samples, negative-related techniques, such as BYOL [21], SimSiam [11], MoCHi[31], PBNS[20], and DCL[12] have been introduced, which utilize stop-gradient and asymmetric predictor approaches, respectively. Another set of solutions aims to overcome the trivial solution problem. Barlow Twins [59], SSL-HSIC [41], VICReg [4], W-MSE [16], and CWRGP [58] suggest constraining the covariance matrix of the sample features to be a unit matrix. Additionally, the augmentation technique in contrastive learning has also been explored. MetAug [40] proposes a learnable augmentation method for contrastive learning. AugSelf[38] proposes a\naugmentaion-aware approach. In our work, we propose a novel sparsity constraint on the projection head to enhance generalization, distinguishing our approach from previous works in this field.\nTheoretical Findings. Contrastive learning has not only led to methodological advancements but has also provided significant theoretical insights into its underlying principles [61, 57, 3, 7]. The pioneers of theoretical guarantees for contrastive loss were Saunshi et al. [49], who explored the impact of negative samples on contrastive learning performance. Ash et al. [1] delved into the role of negatives and introduced the \"collision-coverage\" problem within the same framework. However, the work of Awasthi et al. [2] demonstrated that, under certain circumstances, increasing the number of negative samples does not necessarily impair performance. Understanding the optimization objectives of contrastive learning, Wang et al. [55] established that the contrastive loss aims to optimize both alignment and uniformity. Furthermore, the phenomenon of dimensional collapse in contrastive learning has been analyzed by DirectCLR [30] and MetaMask [39]. The augmentation graph and graph connectivity have also been proposed as theoretical analyses [23, 56, 3]. Despite these contributions, Saunshi et al. [48] argue that the augmentation of overlap alone is insufficient to explain the success of contrastive learning. Exploring the causal effects of contrastive learning, SSLPICS [53] and ICL-MSR [45] have investigated the causal effects and developed structural causal models to enhance performance. Garrido et al. [19] examined the duality between contrastive and non-contrastive self-supervised learning. Additionally, Wen et al. [57] have focused on understanding the mechanism of the prediction head in non-contrastive self-supervised learning. In this paper, we contribute to the theoretical analysis by providing a detailed examination of the properties of the projection head in contrastive learning."
        },
        {
            "heading": "3 Preliminary Study",
            "text": "Given a dataset D = {x} and a data augmentations distribution A, the learning objective of contrastive learning can be written as:\nmin \u03b8,\u03c6 Lcontrastive(\u03b8, \u03c6;D,A) := EBx\u223cD[\u2113(\u03b8, \u03c6;Zx)], (1)\ns.t. Zx = { h\u03c6(f\u03b8(a l(x))) | x \u2208 Bx, a l \u223c A, l \u2208 {1, 2} } .\nIn Equation 1, Bx is a mini-batch sampled from D, f\u03b8 denotes the feature extractor and h\u03c6 is the projection head. \u2113 is the InfoNCE loss of this mini-batch of samples:\n\u2113(\u03b8, \u03c6;Zx) = \u2212 \u2211\nz l\u2208Zx\nlog( exp(cos(zl, z3\u2212l)/\u03c4) \u2211\nz\u2208Zx\\{zl} exp(cos(zl, z)/\u03c4)\n) (2)\nWhere cos(u,v) = uTv/\u2016u\u2016\u2016v\u2016 and \u03c4 is a hyperparameter for temperature scaling.\nThe feature extractor f\u03b8 is parameterized as a backbone neural network, and the projection head h\u03c6 is implemented as a multi-layer MLP or a single-layer weight matrix. We denote the extracted Representation as r = f\u03b8(x) \u2208 R d. And the projected Embeddings as z = h\u03c6(r) \u2208 R m. Equation 2 implies that the similarity between zl and z3\u2212l is maximized while the similarities between zl and negative samples Zx \\ {z l} are minimized.\nAfter the training phase, the projection head h\u03c6 is discarded. The trainable weights in feature extractor f\u03b8 is frozen and a coefficient matrix W\u0304 is trained on the training set Dtrain = {(x, y)} of downstream task, the label y can be either class indices or realvalues, depending on the property of tasks. The performance of f\u03b8 on the downstream task can be evaluated with maximum likelihood estimation (MLE) on the test set Dtest = {(x, y)} as below:\nLdownstream(W\u0304 ; f\u03b8,Dtest,Dtrain) = E(x,y)\u223cDtest[\u2212 logP (y; W\u0304f\u03b8(x))], (3)\ns.t. W\u0304 = argmax W\u0304 E(x,y)\u223cDtrain [logP (y; W\u0304f\u03b8(x))]"
        },
        {
            "heading": "4 Empirical Analyses",
            "text": "To gain an intuitive understanding of the role of the projection head in contrastive learning, we conducted a series of experiments. We utilized the SimCLR method [10] on the STL-10 dataset [13] to train the feature extractor f\u03b8 with various projection heads h\u03c6. Three types of projection heads were explored: a fixed identity matrix h\u03c6 = I , a single linear layer h\u03c6 = W , and a multi-layer non-linear fully-connected layer. The architecture of the feature extractor f\u03b8 employed ResNet-18 and ResNet-50 [26]. The dimensions of the learned representations and embeddings were set as d = m = 512 for ResNet-18 and d = m = 2048 for ResNet-50. Next, we collected all the embedding and representation vectors from the test set X test, denoted as R and Z, respectively. Covariance matrices Cr \u2208 R d\u00d7d and Cz \u2208 R m\u00d7m were computed for the representation and embedding vectors, respectively. To gain insights into the structures of these covariance matrices, we performed Eigenvalue Decomposition (EVD) on them, resulting in eigenvalues \u03c3r and \u03c3z for Cr and Cz . Figure 1 illustrates the log-scaled eigenvalue spectra of Cr and Cz under different h\u03c6 and f\u03b8.\nFirstly, as depicted in Figure 1a, we observe the impact of different projection heads on the rank of Cr. When no projection head is used (h\u03c6 = I), numerous eigenvalues are zero, indicating that the\nrank of Cr is less than d. This observation suggests the occurrence of dimensional collapse. Secondly, Figures 1b and 1c demonstrate that when a linear or nonlinear projection head is employed, the rank of Cr is equal to d, thereby avoiding dimensional collapse. However, we note that the rank of Cz is lower than the rank of Cr in these cases, implying a loss of information in the embedding space. Lastly, Figure 1d highlights that even with the presence of a projection head, dimensional collapse can still occur in larger networks and representation spaces. Observations are summarized as follows: 1) The representation vectors R contain more informative features compared to the embedding vectors Z; 2) Performing contrastive learning on the embedding space can yield improvements in representation learning; 3) Despite the presence of a projection head, the issue of dimensional collapse cannot be completely mitigated.\nThese observations provide the motivation to propose the core assumption of this paper, which asserts that minimizing the contrastive loss of a mini-batch of samples can be achieved by using only a subset of features from the representation space.\nWe illustrate this idea in Figure 2, where a dog image serves as the original sample and is cropped twice to obtain the anchor and positive sample. The negative samples consist of images of other animals. Remarkably, in this scenario, the features extracted from the animal subject in the original image are adequate for discerning between the positive and negative samples. In the subsequent subsection, we formalize this hypothesis within the framework of multi-task learning."
        },
        {
            "heading": "5 Theoretical Analyses",
            "text": ""
        },
        {
            "heading": "5.1 Bridging contrastive learning and unsupervised meta-learning",
            "text": "Multi-task learning (MTL) is a subfield of machine learning that aims to enhance the performance and generalization of a learning system by simultaneously training it on multiple related tasks[47, 9]. The fundamental concept behind MTL is to learn a task-agnostic feature extractor capable of capturing relevant features for each task from a shared input space, enabling the learning system to handle multiple tasks effectively[62]. Meta-learning builds upon MTL by enabling the model to rapidly adapt to new tasks that were not encountered during training[17, 54]. It focuses on developing models that can learn from a distribution of tasks and generalize to new, unseen tasks efficiently. Contrastive learning (CL) shares a similar objective with MTL and meta-learning, aiming to learn transferable representations in a task-agnostic manner from unlabeled data. CL can be viewed as the unsupervised counterpart of meta-learning, and its learning objective is closely related to metricbased meta-learning [44, 29, 37].\nIf we consider the anchor zl as a query sample and Zx as the support set, Equation 2 can be seen as equivalent to an N -way 1-shot few-shot classification task. In this case, we can focus on the linear head h\u03c6 = W , and introduce the indicator variable y1 = I(z l, z3\u2212l) to denote whether zl and z3\u2212l form a positive pair. With these considerations, Equation 2 can be alternatively expressed as follows:\n\u2113(\u03b8, \u03c6;Zx) = \u2212 \u2211\nz l\u2208Zx\nlogP (y 1 ;Wf\u03b8(a l(x))) (4)\nCombining Equation 4 and Equation 3, we can conclude that the goal of contrastive learning is to learn a task-agnostic feature extractor f\u03b8\u2217 that apply to all tasks. Hence, we define the task t as a tuple t = (Bx,ABx) sampled from the distribution P (t), where ABx represents the i.i.d. augmentation for mini-batch Bx."
        },
        {
            "heading": "5.2 Main Results",
            "text": "We begin by assuming the existence of a ground-truth (GT) feature extractor f\u03b8\u2217 , which generates the ground-truth representation f\u03b8\u2217(x) for any given sample x \u2208 R\nX . The objective of multi-task learning is to learn a feature extractor f\n\u03b8\u0302 that approximates the GT feature extractor. To formalize\nthis idea, we introduce the concept of the ground-truth equivalent representation (GTE) as follows:\nDefinition 5.1. (Ground-truth equivalent representation) A learned encoder function f \u03b8\u0302 : RX \u2192 Rd is said to be equivalent w.r.t the ground-truth representation f\u03b8\u2217 when there exists an diagonal matrix \u039b and a permutation matrix P such that, for all x \u2208 X , f\n\u03b8\u0302 = \u039bPf\u03b8\u2217(x).\nRoeder et al. [46] demonstrated that contrastive learning can achieve linear identifiability for any feature extractor f\u03b8 with parameters \u03b8 \u2208 \u0398. In other words, for any x \u2208 X , there exists an invertible matrix L such that f \u03b8\u0302 (x) = Lf\u03b8\u2217(x), where {\u03b8\n\u2217, \u03b8\u0302} \u2286 \u0398 and f\u03b8\u2217 represents the optimal feature extractor. According to Definition 5.1, this implies that the ground-truth equivalent (GTE) representation f\n\u03b8\u0302 is a specific instance of linear identifiability, with L = \u039bP . We can easily prove\nthat the GTE representation does not adversely affect generalization, as defined in Definition 5.1, by considering the following:\nProposition 5.2. (Generalization via GTE representation)\n\u03b8\u0302 = argmin \u03b8\u0303 Et\u223cP (t)Ex\u2208Btx \u2212 logP (y 1\n;W tf\u03b8\u0303(x)) (5)\nW t = \u2212 argmin W\u0303 Ex\u2208Bx logP (y1; W\u0303f\u03b8\u0303(x)) (6)\nwhere W t is the task-specific ground-truth classifier and \u2200x \u2208 X , f \u03b8\u0302 = \u039bPf\u03b8\u2217(x).\nThe detailed proof is illustrated in Appendix. Based on the previous section, we concluded that only a subset of features is used to compare losses for all tasks. We can thus make the following hypothesis about the L2,0 norm of task-specific ground-truth classifiers W t:\nAssumption 5.3. (Task specific classifier) for a given task t and a learned feature extractor f \u03b8\u0302 , the L2,0 norm of GT projection head \u2016W t\u20162,0 < d, indicating that the task-specific head is sparse, and only a subset of features are used for task t.\nLet St be the support of the matrix W t, i.e. the set of features that are relevant for predicting y in the t-th task: St := {j \u2208 D|\u2016W t:j\u20162,0 6= 0}, where D := {i} d i=1. Note that S t is unknown to the learner. We assume that W follows a distribution P (W ), and the support S follows a distribution P (S). We can write P (W ) as a mixture of conditional distributions: P (W ) = \u2211\nS\u2208P(D) P (S)P (W |S). Here,\nP(D) = {S|S \u2286 D} is the power set of D. We also define S := {S \u2208 P(D)|P (S) > 0} as the set of non-trivial supports. The support S and W satisfy the following assumptions.\nAssumption 5.4. (Intra-support sufficient variance) \u2200S \u2208 S and all u \u2208 R|S|\\{0}. EP (W )[W:Su = 0|S] = 0.\nAssumption 5.4 indicates that the distribution P (W |S) does not lie within a specific subspace, thereby indicating that all support S \u2208 S are non-trivial.\nAssumption 5.5. (Non-trivial Feature) \u2200j \u2208 D, \u22c3\nS\u2208S|j 6\u2208S S = D \\ {j}.\nAssumption 5.5 asserts that for every feature j, there exists a collection of supports S such that the union of these supports includes all features except for feature j itself. This implies that the union of supports is sufficiently diverse to cover all features, and no feature appears in all tasks. Building upon Assumptions 5.4 and 5.5, we can establish a proof that the following bi-level optimization problem guarantees the achievement of the ground-truth equivalent (GTE) representation.\nTheorem 5.6. (GTE via sparsity) Let \u03b8\u0302 be a minimizer of\nminEt\u223cP (t)Ex\u2208Btx \u2212 logP (y1; W\u0302 tf\u03b8\u0303(x)) (7) s.t. \u2200W \u2208 W , W\u0302 t \u2208 argmin W\u0303s.t.\n\u2016W\u0303\u20162,0\u2264\u2016W\u20162,0\nEx\u2208Bx \u2212 logP (y; W\u0303f\u03b8\u0302(x)) (8)\nIf f \u03b8\u0302 is continuous for all \u03b8\u0302, f \u03b8\u0302 is equivalent to ground-truth feature extractor f\u03b8\u2217 .\nThe detailed proof is provided in Appendix. This section presents theoretical analyses to showcase that imposing sparsity constraints on the projection head can encourage the feature extractor to learn ground-truth equivalent (GTE) representations, which in turn enhance generalization performance."
        },
        {
            "heading": "5.3 Discriminant Analyses",
            "text": "In order to provide further insight into the justification for Assumption 5.3, we delve deeper into its rationale. Figure 2 visually illustrates the intuition behind this assumption. In this subsection,\nwe specifically explore the discriminative capability of low-rank features. As we have seen in the previous section, contrastive learning involves treating each batch as a classification problem, where the objective is to make the embeddings of the same class closer and the embeddings of different classes farther apart. Thus, the discriminability of the embeddings plays a crucial role in representation learning. It has been proven in [6] that as dimensionality increases, the distance to the nearest data point approaches the distance to the farthest data point. To quantify the discriminativeness of the feature extractor, we can utilize the min-max distance ratio, which is defined as:\nM\u03b8,\u03c6(d) = \u03b4max(d)\u2212 \u03b4min(d)\n\u03b4min(d) (9)\nHere, \u03b4max(d) = max{\u2016z \u2212 zi\u20162 | i = 1, . . . , n} and \u03b4 min(d) = min{\u2016z \u2212 zi\u20162 | i = 1, . . . , n} represent the maximum and minimum distances between the embedding vector z and the other samples, respectively. When the dimensionality of the representation approaches infinity, [6] propose the following conclusion.\nLemma 5.7. For any given embeddings z, z1, z2, . . . , zn \u2208 R d generated by feature extractor f\u03b8 and h\u03c6 of i.i.d. random data points xi \u2208 X , we have that\nP ( lim d\u2192\u221e M\u03b8,\u03c6(d) = 0) = 1 (10)\nAccording to Lemma 5.7, as the dimensionality increases infinitely, the similarity between pairwise instances becomes less effective in providing contrast for distinguishing positive and negative pairs. In such cases, it is reasonable for the learning objective to prioritize low-dimensional embeddings over high-dimensional ones. From Section 5.2, we can conclude that the incorporation of a sparse projection head has the potential to improve the generalization capabilities of representations by projecting them into a low-dimensional subspace. This is achieved through the inclusion of the\nregularization term \u03bb\u2016W\u0303\u20162,1 in the optimization problem 12, which effectively limits the dimensionality of the embedding z. Expanding on this observation, we present a proof to demonstrate that the discriminative measurement M\u03b8,\u03c6(d) is lower bounded by the regularization parameter \u03bb in the following manner:\nTheorem 5.8. (lower bounded min-max distance ratio) For any given embeddings z, z1, z2, . . . , zn \u2208 R d generated by learned feature extractor f \u03b8\u0302 and h \u03c6\u0302 of i.i.d. random data points xi \u2208 X , we have that\nP ( lim d\u2192\u221e M \u03b8\u0302,\u03c6\u0302 (d) > \u03bbC(X )) = 1 (11)\nwhere M \u03b8\u0302,\u03c6\u0302 is the min-max distance ratio from the learned embedding z with Equation 12 andC(X ) is a constant only related to original samples.\nThe detailed proof is provided in the Appendix. Theorem 5.8 implies that the learned embedding can successfully capture the intrinsic similarity during the minimization of the learning objective, thereby resulting in improved performance on downstream tasks."
        },
        {
            "heading": "6 Method",
            "text": "Theorem 5.6 establishes the link between sparsity and the GTE representations. However, the intractable nature of the L2,0 norm motivates us to employ a relaxation technique. Specifically, we adopt the least absolute shrinkage and selection operator (lasso) [51, 18], which serves as an approximation to the L2,0 norm. By doing so, we arrive at the following corollary.\nCorollary 6.1. (Optimization with lasso) The Theorem 5.6 can be relaxed as the following optimization problem with a L2,1 regularization term.\nmin \u03b8\u0302\n\u2212 1\nNT\nT \u2211\nt=1\n\u2211\nx\u2208Btx\nlogP (y 1 ; W\u0302 tf \u03b8\u0302 (al(x))) (12)\ns.t. \u2200t \u2208 [T ], W\u0302 t \u2208 argmin W\u0303\n\u2212 1\nN\n\u2211\nx\u2208Btx\nlogP (y 1 ; W\u0303f \u03b8\u0302 (al(x))) + \u03bb\u2016W\u0303\u20162,1 (13)\nWe therefore propose a regularization method called SparseHead. Specifically, we introduce a regularization term that encourages sparsity in the projection head. If the projection head is linear, represented as h\u03c6 = W , we apply the L2,1 norm as the regularization term, given by \u03bb\u2016W\u20162,1. On the other hand, if the projection head is non-linear, we use the L2,1 norm of the weight matrix in the last layer as the regularization term. In the upcoming section, we present a series of experiments to demonstrate the effectiveness of the proposed SparseHead regularization term. These experiments provide insights into how sparsity promotion can improve the performance of self-supervised learning."
        },
        {
            "heading": "7 Experiments",
            "text": ""
        },
        {
            "heading": "7.1 Experimental Setup",
            "text": "Datasets. We conducted experiments on the following datasets: CIFAR-10, CIFAR-100[35], STL10[13], Tiny ImageNet[36], ImageNet 100[50], ImageNet[14] and COCO[42].\nImplementation Details. In the linear evaluation task, we employed the ResNet-18 network on four datasets: CIFAR-10, CIFAR-100, STL-10, and Tiny ImageNet. For ImageNet and ImageNet100, we utilized the Resnet-50 network. The encoder was trained for 1000 epochs on CIFAR-10 and CIFAR100, using a learning rate of 3 \u00d7 10\u22123. On STL-10, we trained the encoder for 2000 epochs with a learning rate of 2 \u00d7 10\u22123, while on Tiny ImageNet, we trained it for 1000 epochs with a learning rate of 2 \u00d7 10\u22123. Regarding ImageNet and ImageNet100, we followed the settings outlined in [10, 21, 59] and trained for 1000 epochs. The Adam optimizer [32] was employed with a weight decay of 1 \u00d7 10\u22126. All models utilized a projection head, consisting of an MLP with two fully connected layers, batch normalization, and ReLU activation function. Notably, in SimCLR, BYOL, and Barlow-Twins, we included the SparseHead regularization term within the projection head. The temperature parameter in Equation 2 was set to \u03c4 = 0.5, and the sparsity regularization parameter in Equation 12 \u03bb = 1\u00d7 10\u22124.\nLinear evaluation. We test the generalization performance of the learned feature extractor following the same procedure as Equation 3 by freezing the f\u03b8 and train a classifier W on top of it. Additionally, for datasets that are small to medium-sized, we also evaluate the accuracy of a k-nearest neightbors classifier with k = 5. Table 1 presents the results on small, medium, and large-\nsized datasets. We incorporate the SparseHead into three models, namely SimCLR+SparseHead, BYOL+SparseHead, and Barlow Twins+SparseHead. From Table 1, it is evident that the performance of our proposed methods surpasses that of the baseline. Notably, the proposed methods exhibits the best results by enhancing the linear evaluation accuracy by more than 1%. Specifically, SimCLR+SparseHead boosts the top 1 validation accuracy of SimCLR in ImageNet by 2.12%, while BYOL+SparseHead enhances BYOL by 1.05%.\nSemi-supervised learning. We also assess the fine-tuning performance of the self-supervised pretrained ResNet-50 model on a classification task with a subset of ImageNet\u2019s training set using labeled information. We adopt the semi-supervised approach introduced in [34, 10, 60, 27], and employ the same fixed splits of 1% and 10% of labeled ImageNet training data, respectively. The test set\u2019s top 1 and top 5 accuracies are reported, and the models are trained for 1000 epochs, as demonstrated in Table 2. When only 1% of the labels are available, the Barlow Twins+SparseHead model attains a top 1/top 5 accuracy of 56.2%/79.5%, improving the accuracy by 1.3%/0.1% points compared to the 200 pre-training epochs setting. The semi-supervised classification results suggest that SparseHead learns superior representations for image-level prediction.\nTransfer learning. We evaluate the performance of SparseHead on the COCO dataset [42] using the instance segmentation and object detection tasks. Following the common transfer learning setting employed by existing methods [59, 21], we adopt Mask R-CNN [25] with a 1\u00d7 schedule and the same backbone as Faster R-CNN. The results, presented in Table 3, demonstrate that the proposed method significantly enhances the performance of downstream tasks."
        },
        {
            "heading": "7.2 Ablation Study",
            "text": "The effective representations. To demonstrate the improvement of generalization ability and solve the dimension collapse problem by the sparse projection head, we trained a feature extractors ResNet-50 on the STL-10 dataset. The representation dimension of ResNet-50 is d = 2048. We obtained the eigenvalue spectra on the test set in the same way as the motivation experiment in Sec. 4. The eigenvalue spectra of the representation vectors and embedding vectors with the sparse constraint and without the sparse constraint are shown in the following figure.\nFigure 3 shows that when the higher dimensional feature space, the issue of dimension collapse persists even when a projection head is employed. However, by controlling the sparsity of the projection head, we observe that the effective dimension of the embedding decreases, while the effective dimension of the representation increases. This observation confirms the correctness of our previous conclusion."
        },
        {
            "heading": "8 Conclusion",
            "text": "In this work, we perform empirical studies to propose the key assumption that only a subset of features are used when minimizing the contrastive loss of a batch of samples. Then, we theoretically showed that constraining the sparsity of projection head can improve the genralization and discriminative of the learned feature extractor network. Inspired by our theory, we proposed SparseHead, a regulator that constrain the sparsity of projection head and can be seamlessly integrated with any\nself-supervised learning (SSL) approaches. Finally, by a series of experiments, we showed that our proposed SparseHead regulator significantly improve the performance of existing self-supervised method on linear evaluation and Semi-Supervised tasks.\nLimitations and Broader Impact. The regularization hyperparameter\u03bb is pre-defined in this paper, improvements might be obtained by learning this parameter in an adaptive way. Another limitation is that our experiments are conducted only on ResNet architectures. Future research might incorporate architectural improvements, and demonstrate the advantage of the proposed SparseHead."
        }
    ],
    "title": "Towards the Sparseness of Projection Head in Self-Supervised Learning",
    "year": 2023
}