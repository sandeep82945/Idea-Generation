{
    "abstractText": "With great practical value, the study of Multidomain Neural Machine Translation (NMT) mainly focuses on using mixed-domain parallel sentences to construct a unified model that allows translation to switch between different domains. Intuitively, words in a sentence are related to its domain to varying degrees, so that they will exert disparate impacts on the multi-domain NMT modeling. Based on this intuition, in this paper, we devote to distinguishing and exploiting word-level domain contexts for multi-domain NMT. To this end, we jointly model NMT with monolingual attention-based domain classification tasks and improve NMT as follows: 1) Based on the sentence representations produced by a domain classifier and an adversarial domain classifier, we generate two gating vectors and use them to construct domain-specific and domain-shared annotations, for later translation predictions via different attention models; 2) We utilize the attention weights derived from target-side domain classifier to adjust the weights of target words in the training objective, enabling domain-related words to have greater impacts during model training. Experimental results on ChineseEnglish and English-French multi-domain translation tasks demonstrate the effectiveness of the proposed model. Source codes of this paper are available on Github https://github.com/DeepLearnXMU/WDCNMT.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiali Zeng"
        },
        {
            "affiliations": [],
            "name": "Jinsong Su"
        },
        {
            "affiliations": [],
            "name": "Huating Wen"
        },
        {
            "affiliations": [],
            "name": "Yang Liu"
        },
        {
            "affiliations": [],
            "name": "Jun Xie"
        },
        {
            "affiliations": [],
            "name": "Yongjing Yin"
        },
        {
            "affiliations": [],
            "name": "Jianqiang Zhao"
        }
    ],
    "id": "SP:4dede284f26e15ebe58b3f07e0964a0184d0e2d6",
    "references": [
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio."
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "Proc. of ICLR 2015.",
            "year": 2015
        },
        {
            "authors": [
                "Miguel Ballesteros",
                "Chris Dyer",
                "Noah A. Smith."
            ],
            "title": "Improved transition-based parsing by modeling characters instead of words with lstms",
            "venue": "Proc. of EMNLP 2015.",
            "year": 2015
        },
        {
            "authors": [
                "Boxing Chen",
                "Colin Cherry",
                "George Foster",
                "Samuel Larkin."
            ],
            "title": "Cost weighting for neural machine translation domain adaptation",
            "venue": "Proc. of the First Workshop on Neural Machine Translation.",
            "year": 2017
        },
        {
            "authors": [
                "Wenhu Chen",
                "Evgeny Matusov",
                "Shahram Khadivi",
                "Jan-Thorsten Peter."
            ],
            "title": "Guided alignment training for topic-aware neural machine translation",
            "venue": "CoRR abs/1607.01628.",
            "year": 2016
        },
        {
            "authors": [
                "Xinchi Chen",
                "Zhan Shi",
                "Xipeng Qiu",
                "Xuanjing Huang."
            ],
            "title": "Adversarial multi-criteria learning for chinese word segmentation",
            "venue": "Proc. of ACL 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Schwenk",
                "Yoshua Bengio."
            ],
            "title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation",
            "venue": "Proc. of EMNLP 2014.",
            "year": 2014
        },
        {
            "authors": [
                "Chenhui Chu",
                "Raj Dabre",
                "Sadao Kurohashi."
            ],
            "title": "An empirical comparison of domain adaptation methods for neural machine translation",
            "venue": "Proc. of ACL 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan H. Clark",
                "Alon Lavie",
                "Chris Dyer."
            ],
            "title": "One system, many domains: Open-domain statistical machine translation via feature augmentation",
            "venue": "Proc. of AMTA 2012.",
            "year": 2012
        },
        {
            "authors": [
                "Daxiang Dong",
                "Hua Wu",
                "Wei He",
                "Dianhai Yu",
                "Haifeng Wang."
            ],
            "title": "Multi-task learning for multiple language translation",
            "venue": "Proc. of ACL 2015.",
            "year": 2015
        },
        {
            "authors": [
                "M. Amin Farajian",
                "Marco Turchi",
                "Matteo Negri",
                "Marcello Federico."
            ],
            "title": "Multi-domain neural machine translation through unsupervised adaptation",
            "venue": "Proc. of WMT 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Markus Freitag",
                "Yaser Al-Onaizan."
            ],
            "title": "Fast domain adaptation for neural machine translation",
            "venue": "CoRR abs/1612.06897.",
            "year": 2016
        },
        {
            "authors": [
                "Andrew C. Heusser",
                "Kirsten Ziman",
                "Lucy L.W. Owen",
                "Jeremy R. Manning."
            ],
            "title": "Hypertools: a python toolbox for gaining geometric insights into high-dimensional data",
            "venue": "Journal of Machine Learning Research.",
            "year": 2018
        },
        {
            "authors": [
                "M Huck",
                "A Birch",
                "B Haddow."
            ],
            "title": "Mixeddomain vs",
            "venue": "multi-domain statistical machine translation. In Proc. of MT Summit 2015.",
            "year": 2015
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Lei Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "Proc. of ICLR 2015.",
            "year": 2015
        },
        {
            "authors": [
                "Catherine Kobus",
                "Josep Crego",
                "Jean Senellart."
            ],
            "title": "Domain control for neural machine translation",
            "venue": "CoRR abs/1612.06140.",
            "year": 2016
        },
        {
            "authors": [
                "Jiwei Li",
                "Xinlei Chen",
                "Eduard Hovy",
                "Dan Jurafsky."
            ],
            "title": "Visualizing and understanding neural models in nlp",
            "venue": "Proc. of NAACL 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Pengfei Liu",
                "Xipeng Qiu",
                "Xuanjing Huang."
            ],
            "title": "Domain separation networks",
            "venue": "Proc. of NIPS 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Pengfei Liu",
                "Xipeng Qiu",
                "Xuanjing Huang."
            ],
            "title": "Adversarial multi-task learning for text classification",
            "venue": "Proc. of ACL 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Yang Liu",
                "Sujian Li",
                "Xiaodong Zhang",
                "Zhifang Sui."
            ],
            "title": "Implicit discourse relation classification via multi-task neural networks",
            "venue": "Proc. of ACL 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Minh-Thang Luong",
                "Christopher D Manning."
            ],
            "title": "Stanford neural machine translation systems for spoken language domains",
            "venue": "Proc. of IWSLT 2015.",
            "year": 2015
        },
        {
            "authors": [
                "Kalchbrenner Nal",
                "Blunsom Phil."
            ],
            "title": "Recurrent continuous translation models",
            "venue": "Proc. of EMNLP 2013.",
            "year": 2013
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proc. of ACL 2002.",
            "year": 2002
        },
        {
            "authors": [
                "Reid Pryzant",
                "Denny Britz",
                "Q Le."
            ],
            "title": "Effective domain mixing for neural machine translation",
            "venue": "Proc. of WMT 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Hassan Sajjad",
                "Nadir Durrani",
                "Fahim Dalvi",
                "Yonatan Belinkov",
                "Stephan Vogel."
            ],
            "title": "Neural machine translation training in a multi-domain scenario",
            "venue": "CoRR abs/1708.08712.",
            "year": 2017
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Neural machine translation of rare words with subword units",
            "venue": "Proc. of ACL 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Rico Sennrich",
                "Holger Schwenk",
                "Walid Aransa."
            ],
            "title": "A multi-domain translation model framework for statistical machine translation",
            "venue": "Proc. of ACL 2013.",
            "year": 2013
        },
        {
            "authors": [
                "Christophe Servan",
                "Josep Crego",
                "Jean Senellart."
            ],
            "title": "Domain specialization: a post-training domain adaptation for neural machine translation",
            "venue": "CoRR abs/1612.06141.",
            "year": 2016
        },
        {
            "authors": [
                "Jinsong Su",
                "Zhixing Tan",
                "Deyi Xiong",
                "Rongrong Ji",
                "Xiaodong Shi",
                "Yang Liu."
            ],
            "title": "Lattice-based recurrent neural network encoders for neural machine translation",
            "venue": "Proc. of AAAI 2017, pages 3302\u2013 3308.",
            "year": 2017
        },
        {
            "authors": [
                "Jinsong Su",
                "Jiali Zeng",
                "Deyi Xiong",
                "Yang Liu",
                "Mingxuan Wang",
                "Jun Xie."
            ],
            "title": "A hierarchyto-sequence attentional neural machine translation model",
            "venue": "IEEE/ACM Trans. Audio, Speech & Language Processing, 26(3):623\u2013632.",
            "year": 2018
        },
        {
            "authors": [
                "Ilya Sutskever",
                "Oriol Vinyals",
                "Quoc V Le."
            ],
            "title": "Sequence to sequence learning with neural networks",
            "venue": "Proc. of NIPS 2014.",
            "year": 2014
        },
        {
            "authors": [
                "Sander Tars",
                "Mark Fishel."
            ],
            "title": "Multi-domain neural machine translation",
            "venue": "CoRR abs/1805.02282.",
            "year": 2018
        },
        {
            "authors": [
                "Liang Tian",
                "Derek F. Wong",
                "Lidia S. Chao",
                "Paulo Quaresma",
                "Francisco Oliveira",
                "Shuo Li",
                "Yiming Wang",
                "Yi Lu."
            ],
            "title": "Um-corpus: A large english-chinese parallel corpus for statistical machine translation",
            "venue": "Proc. of LREC 2014.",
            "year": 2014
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Proc. of NIPS 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Rui Wang",
                "Andrew Finch",
                "Masao Utiyama",
                "Eiichiro Sumita."
            ],
            "title": "Sentence embedding for neural machine translation domain adaptation",
            "venue": "Proc. of ACL 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Rui Wang",
                "Masao Utiyama",
                "Lemao Liu",
                "Kehai Chen",
                "Eiichiro Sumita."
            ],
            "title": "Instance weighting for neural machine translation domain adaptation",
            "venue": "Proc. of EMNLP 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Biao Zhang",
                "Deyi Xiong",
                "Jinsong Su",
                "Hong Duan."
            ],
            "title": "A context-aware recurrent encoder for neural machine translation",
            "venue": "IEEE/ACM Trans. Audio, Speech & Language Processing, 25(12):2424\u20132432.",
            "year": 2017
        },
        {
            "authors": [
                "Jian Zhang",
                "Liangyou Li",
                "Andy Way",
                "Qun Liu."
            ],
            "title": "Topic-informed neural machine translation",
            "venue": "Proc. of COLING 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Qingyu Zhou",
                "Nan Yang",
                "Furu Wei",
                "Ming Zhou."
            ],
            "title": "Selective encoding for abstractive sentence summarization",
            "venue": "Proc. of ACL 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Barret Zoph",
                "Deniz Yuret",
                "Jonathan May",
                "Kevin Knight."
            ],
            "title": "Transfer learning for low-resource neural machine translation",
            "venue": "Proc. of EMNLP 2016.",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 447\u2013457 Brussels, Belgium, October 31 - November 4, 2018. c\u00a92018 Association for Computational Linguistics\n447\ndomain Neural Machine Translation (NMT) mainly focuses on using mixed-domain parallel sentences to construct a unified model that allows translation to switch between different domains. Intuitively, words in a sentence are related to its domain to varying degrees, so that they will exert disparate impacts on the multi-domain NMT modeling. Based on this intuition, in this paper, we devote to distinguishing and exploiting word-level domain contexts for multi-domain NMT. To this end, we jointly model NMT with monolingual attention-based domain classification tasks and improve NMT as follows: 1) Based on the sentence representations produced by a domain classifier and an adversarial domain classifier, we generate two gating vectors and use them to construct domain-specific and domain-shared annotations, for later translation predictions via different attention models; 2) We utilize the attention weights derived from target-side domain classifier to adjust the weights of target words in the training objective, enabling domain-related words to have greater impacts during model training. Experimental results on ChineseEnglish and English-French multi-domain translation tasks demonstrate the effectiveness of the proposed model. Source codes of this paper are available on Github https://github.com/DeepLearnXMU/WDCNMT."
        },
        {
            "heading": "1 Introduction",
            "text": "In recent years, neural machine translation (NMT) has achieved great advancement (Nal and Phil, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). However, two difficulties are encountered in the practical applications of NMT. On the one hand, training a NMT model for a spe-\n\u2217Corresponding author\ncific domain requires a large quantity of parallel sentences in such domain, which is often not readily available. Hence, the much more common practice is to construct NMT models using mixed-domain parallel sentences. In this way, the domain-shared translation knowledge can be fully exploited. On the other hand, the translated sentences often belong to multiple domains, thus requiring a NMT model general to different domains. Since the textual styles, sentence structures and terminologies in different domains are often remarkably distinctive, whether such domainspecific translation knowledge is effectively preserved could have a direct effect on the performance of the NMT model. Therefore, how to simultaneously exploit the exclusive and shared translation knowledge of mixed-domain parallel sentences for multi-domain NMT remains a challenging task.\nTo tackle this problem, recently, researchers have carried out many constructive and in-depth studies (Kobus et al., 2016; Zhang et al., 2016; Pryzant et al., 2017; Farajian et al., 2017). However, most of these studies mainly focus on the utilization of domain contexts as a whole in NMT, while ignoring the discrimination of domain contexts at finer-grained level. In each sentence, some words are closely associated with its domain, while others are domain-independent. Intuitively, these two kinds of words play differ-\nent roles in multi-domain NMT, nevertheless, they are not being distinguished by the current models. Take the sentence shown in Figure 1 for example. The Chinese words \u201c\u2018 \u00ac\u201d(congress), \u201c\u00c6 Y\u201d(bills), \u201c \\\u201d(inclusion), and \u201c\u00c6\u00a7\u201d(agenda) are frequently used in Laws domain and imply the Laws style of the sentence, while other words in this sentence are common in all domains and they mainly indicate the semantic meaning of the sentence. Thus, it is reasonable to distinguish and encode these two types of words separately to capture domain-specific and domain-shared contexts, allowing the exclusive and shared knowledge to be exploited without any interference from the other. Meanwhile, the English words \u201cpriority\u201d,\u201cgovernment\u201d, \u201cbill\u201d and \u201cagenda\u201d are also closely related to Laws domain. To preserve the domain-related text style and idioms in generated translations, it is also reasonable for our model to pay more attention to these domain-related words than the others during model training. On this account, we believe that it is significant to distinguish and explore word-level domain contexts for multi-domain NMT.\nIn this paper, we propose a multi-domain NMT model with word-level domain context discrimination. Specifically, we first jointly model NMT with monolingual attention-based domain classification tasks. In source-side domain classification and adversarial domain classification tasks, we perform two individual attention operations on source-side annotations to generate the domainspecific and domain-shared vector representations of source sentence, respectively. Meanwhile, an attention operation is also placed on target-side hidden states to implement target-side domain classification. Then, we improve NMT with the following two approaches:\n(1) According to the sentence representations produced by source-side domain classifier and adverisal domain classifier, we generate two gating vectors for each source annotation. With these two gating vectors, the encoded information of source annotation is selected automatically to construct domain-specific and domain-shared annotations, both of which are used to guide translation predictions via two attention mechanisms;\n(2) Based on the attention weights of the target words from target-side domain classifier, we employ word-level cost weighting strategy to refine our model training. In this way, domain-specific\ntarget words will be assigned greater weights than others in the objective function of our model.\nOur work demonstrates the benefits of separate modeling of the domain-specific and domainshared contexts, which echoes with the successful applications of the multi-task learning based on shared-private architecture in many tasks, such as discourse relation recognition (Liu et al., 2017b), word segmentation (Chen et al., 2017b), text classification (Liu et al., 2017a), and image classification (Liu et al., 2016). Overall, the main contributions of our work are summarized as follows:\n\u2022 We propose to construct domain-specific and domain-shared source annotations from ini-\ntial annotations, of which effects are respectively captured for translation predictions.\n\u2022 We propose to adjust the weights of target words in the training objective of NMT ac-\ncording to their relevance to different domains.\n\u2022 We conduct experiments on large-scale multi-domain Chinese-English and English-\nFrench datasets. Experimental results demonstrate the effectiveness of our model."
        },
        {
            "heading": "2 Model",
            "text": "Figure 2 illustrates the architecture of our model, which includes a neural encoder equipped with a domain classifier and an adversarial domain classifier, and a neural decoder with two attention models and a target-side domain classifier."
        },
        {
            "heading": "2.1 Neural Encoder",
            "text": "As shown in the lower part of Figure 2, our encoder leverages the sentence representations produced by these two classifiers to construct domain-specific and domain-shared annotations from initial ones, preventing the exclusive and shared translation knowledge from interfering with each other. In our encoder, the input sentence x=x1, x2, ..., xN are first mapped to word vectors and then fed into a bidirectional GRU (Cho et al., 2014) to obtain \u2212\u2192 h = \u2212\u2192 h 1, \u2212\u2192 h 2, ..., \u2212\u2192 h N and \u2190\u2212 h = \u2190\u2212 h 1, \u2190\u2212 h 2, ..., \u2190\u2212 h N in the left-to-right and right-to-left directions, respectively. These two sequences are then concatenated as hi = { \u2212\u2192 h \u22a4i , \u2190\u2212 h \u22a4i } \u22a4 to form the word-level semantic representation of the input sentence.\nDomain Classifier and Adversarial Domain Classifier. With annotations {hi} N i=1, we employ\ntwo attention-like aggregators to generate the semantic representations of sentence x, denoted by the vectors Er(x) and Es(x), respectively. Based on these two vectors, we employ the same neural network to model two classifiers with different context modeling objectives:\nOne is a domain classifier that aims to distinguish different domains in order to generate domain-specific source-side contexts. It is trained using the objective function J sdc(x;\u03b8 s dc ) = log p(d|x;\u03b8s dc ), where d is the domain tag of x and \u03b8s dc is its parameter set. The other is an adversarial domain classifier capturing source-side domainshared contexts. To this end, we train it using the following adversarial loss functions:\nJ s1adc(x;\u03b8 s1 adc) = log p(d|x;\u03b8 s1 adc,\u03b8 s2 adc), (1) J s2adc(x;\u03b8 s2 adc) = H(p(d|x;\u03b8 s1 adc,\u03b8 s2 adc)), (2)\nwhere H(p(\u00b7))=\u2212 \u2211K\nk=1 pk(\u00b7) log pk(\u00b7) is an entropy of distribution p(\u00b7) with K domain labels, \u03b8s1 adc and \u03b8s2 adc denote the parameters of softmax layer and the generation layer of Es(x) in this classifier, respectively. By this means, Er(x) and Es(x) are expected to encode the domain-specific and domain-shared semantic representations of x, respectively. It should be noted that our utilization of domain classifiers is similar to adversarial training used in (Pryzant et al., 2017) which injects\ndomain-shared contexts into annotations. However, by contrast, we introduce domain classifier and adversarial domain classifier simultaneously to distinguish different kinds of contexts for NMT more explicitly.\nHere we describe only the modeling procedure of the domain classifier, while it is also applicable to the adversarial domain classifier. Specifically, Er(x) is defined as follows:\nEr(x) = N\u2211\ni=1\n\u03b1ihi, (3)\nwhere \u03b1i = exp(ei)\u2211N i\u2032 exp(ei\u2032) ,\nei = (va) \u22a4 tanh(Wahi),\nand va and Wa are the relevant attention parameters. Then, we feed Er(x) into a fully connected layer with ReLU function (Ballesteros et al., 2015), and then pass its output through a softmax layer to implement domain classification\np(\u00b7|x;\u03b8sdc)\n=softmax(W s\u22a4dc ReLU(Er(x)) + b s dc), (4)\nwhere W sdc and b s dc are softmax parameters.\nDomain-Specific and Domain-Shared Annotations. Since domain-specific and domain-shared contexts have different effects on NMT, and thus\nshould be distinguished and separately captured by NMT model. Specifically, we first leverage the sentence representations Er(x) and Es(x) to generate two gating vectors, gri and g s i , for annotation hi in the following way:\ngri = sigmoid(W (1) gr Er(x) +W (2) gr hi + bgr),\n(5)\ngsi = sigmoid(W (1) gs Es(x) +W (2) gs hi + bgs),\n(6)\nwhere W \u2217gr, W \u2217 gs, bgr and bgs denote the relevant matrices and bias, respectively. With these two vectors, we construct domain-specific and domain-shared annotations hri and h s i from hi:\nhri = g r i \u2299 hi, (7) hsi = g s i \u2299 hi. (8)"
        },
        {
            "heading": "2.2 Neural Decoder",
            "text": "The upper half of Figure 2 illustrates the architecture of our decoder. In particular, with the attention weights of target words from the domain classifier, we employ word-level cost weighting strategy to refine model training.\nFormally, our decoder applies a nonlinear function g(\u2217) to define the conditional probability of translation y=y1, y2, ..., yM :\np(y|x) = M\u220f\nj=1\np(yj |x, y<j) = M\u220f\nj=1\ng(yj\u22121, sj , c r j , c s j),\n(9)\nwhere the vector sj denotes the GRU hidden state. It is updated as\nsj = GRU(sj\u22121, yj\u22121, c r j , c s j). (10)\nHere the vectors crj and c s j represent the domainspecific and domain-shared contexts, respectively.\nDomain-Specific and Domain-Shared Context Vectors. When generating yj , we define c r j as a weighted sum of the domain-specific annotations {hri }:\ncrj = N\u2211\ni=1 exp(erj,i)\u2211N i\u2032=1 exp(e r j,i\u2032) \u00b7 hri , (11)\nwhere erj,i = a(sj\u22121, h r i ),\nand a(*) is a feedforward neural network. Meanwhile, we produce csj from the domain-shared annotations {hsi} as in Eq. 11. By introducing c r j\nand csj into sj , our decoder is able to distinguish and simultaneously exploit two types of contexts for translation predictions.\nDomain Classifier. We equip our decoder with a domain classifier with parameters \u03b8tdc, which maximizes the training objective i.e., J tdc(y;\u03b8 t dc ) = log p(d|y;\u03b8t dc ). To do this, we also apply attention operation to produce the domain-aware semantic representation Er(y) of y,\nEr(y) = M\u2211\nj=1\n\u03b2jsj , (12)\nwhere \u03b2j = exp(ej)\u2211M j\u2032 exp(ej\u2032) ,\nej = (vb) \u22a4 tanh(Wbsj),\nand vb and Wb are the related parameters. Likewise, we stack a domain classifier on top of Er(y). Note that this classifier is only used in model training to infer attention weights of target words. These weights measure their semantic relevance to different domains and can be utilized to adjust their cost weights in NMT training objective.\nNMT Training Objective with Word-Level Cost Weighting. Formally, we define the objective function of NMT as follows:\nJnmt(x,y;\u03b8nmt)\n= M\u2211\nj=1\n(1 + \u03b2j) log p(yj |x, y<j ;\u03b8nmt), (13)\nwhere \u03b2j is the attention weight of yj obtained by Eq. (12), and \u03b8nmt denotes the parameter set of NMT. By this scaling strategy, domainspecific words are emphasized with a bonus, while domain-shared words are updated as usual.\nPlease note that scaling costs with a multiplicative scalar essentially changes the magnitude of parameter update but without changing its direction (Chen et al., 2017a). Besides, although our scaling strategy is similar to the cost weighting proposed by Chen et al. (2017a), our approach differs from it in two aspects: First, we employ wordlevel cost weighting rather than sentence-level one to refine NMT training; Second, our approach is less time-consuming for multi-domain NMT."
        },
        {
            "heading": "2.3 Overall Training Objective",
            "text": "Given a mixed-domain training corpus D = {(x,y, d)}, we train the proposed model accord-\ning to the following objective function:\nJ (D;\u03b8) = \u2211\n(x,y,d)\u2208D\n{Jnmt(x,y;\u03b8nmt)\n+ J sdc(x;\u03b8 s dc) + J t dc(y;\u03b8 t dc) (14) + J s1adc(x;\u03b8 s1 adc) + \u03bb \u00b7 J s2 adc(x;\u03b8 s2 adc)}\nwhere Jnmt(\u2217), J s dc(\u2217), J t dc(\u2217) and J s\u2217 adc(\u2217) are the objective functions of NMT, source-side domain classifier, target-side domain classifier, and source-side adversarial domain classifier, respectively, \u03b8={\u03b8nmt, \u03b8 s dc , \u03b8t dc , \u03b8s1 adc , \u03b8s2 adc }, and \u03bb is the hyper-parameter for adversarial learning.\nParticularly, to ensure encoding accuracy of domain-shared contexts, we follow Chen et al. (2017b) to adopt an alternative two-phase strategy in training, where we alternatively optimize J (D;\u03b8) with \u03b8s1 adc and {\u03b8-\u03b8s1 adc } respectively fixed at a time."
        },
        {
            "heading": "3 Experiment",
            "text": "To investigate the effectiveness of our model, we conducted multi-domain translation experiments on Chinese-English and English-French datasets."
        },
        {
            "heading": "3.1 Setup",
            "text": "Datasets. For Chinese-English translation, our data comes from UM-Corpus (Tian et al., 2014) and LDC1. To ensure data quality, we chose only the parallel sentences with domain label Laws, Spoken, and Thesis from UM-Corpus, and the LDC bilingual sentences related to News domain as our dataset. We used randomly selected sentences from UM-Corpus and LDC as development set, and combined the test set of UM-Corpus and randomly selected sentences from LDC to construct our test set. For English-French translation, we conducted experiments on the datasets of OPUS corpus2, containing sentence pairs from Medical, News, and Parliamentary domains. We also divided these datasets into training, development and test sets. Table 1 provides the statistics of the corpora used in our experiments.\nWe performed word segmentation on Chinese sentences using Stanford Segmenter3, and tokenized English and French sentences using MOSES script4. Then, we employed Byte Pair\n1https://www.ldc.upenn.edu/. 2http://opus.nlpl.eu/ 3https://nlp.stanford.edu/ 4http://www.statmt.org/moses/\nEncoding (Sennrich et al., 2016) to convert all words into subwords. The translation quality was evaluated by case-sensitive BLEU (Papineni et al., 2002).\nContrast Models. Since our model is essentially a standard attentional NMT model enhanced with word-level domain contexts, we refer to it as +WDC. We compared it with the following models, namely:\n\u2022 OpenNMT5. A famous open-source NMT system used widely in the NMT community\ntrained on mix-domain training set.\n\u2022 DL4NMT-single (Bahdanau et al., 2015). A reimplemented attentional NMT trained on a\nsingle domain dataset.\n\u2022 DL4NMT-mix (Bahdanau et al., 2015). A reimplemented attentional NMT trained on\nmix-domain training set.\n\u2022 DL4NMT-finetune (Luong and Manning, 2015). A reimplemented attentional NMT\nwhich is first trained using out-of-domain training corpus and then fine-tuned using indomain dataset.\n\u2022 +Domain Control (+DC) (Kobus et al., 2016). It directly introduces embeddings of\nsource domain tag to enrich annotations of encoder.\n\u2022 +Multitask Learning (+ML1) (Dong et al., 2015). It adopts a multi-task learning frame-\nwork that shares encoder representation and separates the decoder modeling of different domains.\n\u2022 +Multitask Learning (+ML2) (Pryzant et al., 2017). This model jointly trains\n5http://opennmt.net/.\nNMT with domain classification via multitask learning.\n\u2022 +Adversarial Discriminative Mixing (+ADM) (Pryzant et al., 2017). It employs\nadversarial training to achieve the domain adaptation in NMT.\n\u2022 +Target Token Mixing (+TTM) (Pryzant et al., 2017). This model is similar to\n+DC, with the only difference that it enriches source annotations by adding target-side domain tag rather than source-side one.\nNote that our model uses two annotation sequences, thus we also compared it with the aforementioned models with two times of hidden state size (2\u00d7hd). To further examine the effectiveness of the proposed components in our model, we also provided the performance of the following variants of our model:\n\u2022 +WDC(S). It only exploits the source-side word-level domain contexts for multi-domain\nNMT.\n\u2022 +WDC(T). It only employ word-level cost weighting on the target side to refine the\nmodel training.\nImplementation Details. Following the common practice, we only used the training sentences within 50 words to efficiently train NMT models. Thus, 85.40% and 88.96% of the Chinese-English and English-French parallel sentences were covered in our experiments. In addition, we set the vocabulary size for Chinese-English and EnglishFrench as 32,000 and 32,000, respectively. In doing so, our vocabularies covered 99.97% Chinese words and 99.99% English words of the ChineseEnglish corpus, and almost 100% English words and 99.99% French words of the English-French corpus, respectively.\nWe applied Adam (Kingma and Ba, 2015) to train models and determined the best model parameters based on the model performance on development set. The used hyper-parameter were set as follows: \u03b21 and \u03b22 of Adam as 0.9 and 0.999, word embedding dimension as 500, hidden layer size as 1000, learning rate as 5\u00d710\u22124, batch size as 80, gradient norm as 1.0, dropout rate as 0.1, and beamsize as 10. Other settings were set following (Bahdanau et al., 2015)."
        },
        {
            "heading": "3.2 Results on Chinese-English Translation",
            "text": "We first determined the optimal hyper-parameter \u03bb (see Eq. (14)) on the development set. To do this, we gradually varied \u03bb from 0.1 to 1.0 with an increment of 0.1 in each step. Since our model achieved the best performance when \u03bb=0.1, hence, we set \u03bb=0.1 for all experiments thereafter.\nTable 2 shows the overall experimental results. Using almost the same hyper-parameters, our reimplemented DL4NMT outperforms OpenNMT in all domains, demonstrating that our baseline is competitive in performance. Moreover, on all test sets of different domains, our model significantly outperforms other contrast models no matter which hyper-parameters they use. Furthermore, we arrive at the following conclusions:\nFirst, our model surpasses DL4NMT-single, DL4NMT-mix and DL4NMT-finetune, all of which are commonly used in domain adaptation for NMT. Please note that DL4NMT-finetune requires multiple adapted NMT models to be constructed, while ours is a unified one that works well in all domains.\nSecond, compared with +DC, +ML2 and +ADM which all exploit source-side domain contexts for multi-domain NMT, our +WDC(S) still\nexhibits better performance. This is because that these models focus on one aspect of domain contexts, while our model considers both domainspecific and domain-shared contexts on the source side.\nThird, +WDC(T) also outperforms DL4NMT, revealing that it is reasonable and effective to emphasize domain-specific words in model training..\nLast, +WDC achieves the best performance when compared with both +WDC(S) and +WDC(T). Therefore, we believe that word-level domain contexts on the both sides are complementary to each other, and utilizing them simultaneously is beneficial to multi-domain NMT."
        },
        {
            "heading": "3.3 Experimental Analysis",
            "text": "Furthermore, we conducted several visualization experiments to empirically analyze the individual effectiveness of the added model components."
        },
        {
            "heading": "3.3.1 Visualizations of Gating Vectors",
            "text": "We first visualized the gating vectors gri and g s i to quantify their effects on extracting domainspecific and domain-shared contexts from initial source-side annotations. Since both gri and g s i are high dimension vectors, which are difficult to be visualized directly, we followed Li et al. (2016) and Zhou et al. (2017) to visualize their individual contributions to the final output, which can be\napproximated by their first derivatives.\nFigure 3 shows the first derivative heat maps for two example sentences in Laws and Thesis domain, respectively. We can observe that without any loss of semantic meanings from source sentences, most of the domain-specific words are strengthened by gri , while most of the domainshared words, especially function words, are focused by gsi . This result is consistent with our expectation for the function of two gating vectors."
        },
        {
            "heading": "3.3.2 Visualizations of Sentence Representations and Annotations",
            "text": "Furthermore, we applied the hypertools (Heusser et al., 2018) to visualize the sentence representations Er(x) and Es(x), and the domain-specific and domain-shared annotation sequences {hri } N i=1 and {hsi} N i=1. Here we represent each annotation sequence with its average vector in the figure.\nAs shown in Figure 4 (a) and (b), the sentence representation vectors and the average annotation vectors of different domains are clearly distributed in different regions. By contrast, their distributions are much more concentrated in Figure 4 (c) and (d). Thus, we conclude that our model is able to distinctively learn domain-specific and domainshared contexts. Moreover, from Figure 4 (b), we observe that the sentence representation vectors of Laws domain does not completely coincide with\nthose of the other domains, this may be caused by the more formal and consistent sentence styles in Laws domain."
        },
        {
            "heading": "3.3.3 Illustrations of Domain-Specific Target Words",
            "text": "Lastly, for each domain, we presented the top ten target words with the highest weights learned by our target-side domain classifier. To do this, we calculated the average attention weight of each word in the training corpus as its corresponding domain weight.\nAs is clearly shown in Table 3 that most listed target words are closely related to their domains. This result validates the aforementioned hypothesis that some words are domain-dependent while others are domain-independent, and our targetside domain classifier is capable of distinguishing them with different attention weights."
        },
        {
            "heading": "3.4 Results on English-French Translation",
            "text": "Likewise, we determined the optimal \u03bb=0.1 on the development set. Table 4 gives the results of English-French multi-domain translation. Similar to the previous experimental result in Section 3.2, our model continues to achieve the best performance compared to all contrast models using two different hidden state size settings, which demonstrates again that our model is effective and general to different language pairs in multi-domain NMT."
        },
        {
            "heading": "4 Related Work",
            "text": "In this work, we study on multi-domain machine translation in the field of domain adaptation for machine translation, which has attracted great attention since SMT (Clark et al., 2012; Huck et al.,\n2015; Sennrich et al., 2013). As for NMT, the dominant strategies for domain adaptation generally fall into two categories:\nThe first category is to transfer out-of-domain knowledge to in-domain translation. The conventional method is fine-tuning, which first trains the model on out-of-domain dataset and then finetunes it on in-domain dataset (Luong and Manning, 2015; Zoph et al., 2016; Servan et al., 2016). Freitag and Al-Onaizan (2016) proceeded further by ensembling the fine-tuned model with the original one. Chu et al. (2017) fine-tuned the model using the mix of in-domain and out-of-domain training corpora. From the perspective of data selection, Chen et al. (2017a) scaled the top-level costs of NMT system according to each training sentence\u2019s similarity to the development set. Wang et al. (2017a) explored the data selection strategy based on sentence embeddings for NMT domain adaptation. Moreover, Wang et al. (2017b) further proposed several sentence and domain weighting methods with a dynamic weight learning strategy. However, these approaches usually only perform well on target domain while being highly time consuming in transferring translation knowledge to all the constitute domains.\nThe second category is to directly use a mixed-\ndomain training corpus to construct NMT model for the translated sentences derived from different domains. In this aspect, Kobus et al. (2016) incorporated domain information into NMT by appending a domain indicator token to each source sequence. Similarly, Johnson et al. (2016) added an artificial token to the input sequence to indicate the required target language. Contrastingly, Farajian et al. (2017) utilized the similarity between each test sentence and the training instances to dynamically set the hyper-parameters of the learning algorithm and update the generic model on the fly. Pryzant et al. (2017) proposed three novel models: discriminative mixing that jointly models NMT with domain classification, adversarial discriminative mixing, and target token mixing which appends a domain token to the target sequence. Sajjad et al. (2017) explored data concatenation, model stacking, data selection and multi-model ensemble to train multi-domain NMT. By exploiting domain as a tag or a feature, Tars and Fishel (2018) treated text domains as distinct languages in order to use multi-lingual approaches when implementing multi-domain NMT. Inspired by topicbased SMT, some researchers resorted to incorporating topical contexts into NMT. Chen et al. (2016) used the topic information of input sentence as an additional input to decoder. Zhang et al. (2016) enhanced the word representation by adding its topic embedding. However, these methods require to have explicit document boundaries between training data, which unfortunately do not exist in most datasets.\nOverall, our work is related to the second type of approach with (Pryzant et al., 2017) and (Chen et al., 2017a) most related to ours. Unlike (Pryzant et al., 2017) applying adversarial training to only capture domain-shared translation knowledge, we further exploit domain-specific translation knowledge for multi-domain NMT. Also, in sharp contrast to (Chen et al., 2017a), our model not only exploits the source-side word-level domain contexts differently, but also employs a word-level cost weighting strategy for multi-domain NMT."
        },
        {
            "heading": "5 Conclusion and Future Work",
            "text": "In this work, we have explored how to utilize word-level domain contexts for multi-domain NMT. By jointly modeling NMT and domain classification tasks, we utilize the sentence representations of source-side domain classifier and ad-\nversarial domain classifier to construct domainspecific and domain-shared source annotations, which are then exploited by decoder. Moreover, using the attentional weights of target-side domain classifier, we adjust the weights of target words in the training objective to refine model training. Experimental results and in-depth analyses demonstrate the effectiveness of the proposed model.\nIn the future, we would like to extend the proposed word-level cost weighting strategy to source words. Besides, our method is also general to other NMT models. Therefore, we plan to apply our method to the NMT with complex architectures, for example, lattice-to-sequence NMT (Su et al., 2017), hierarchy-to-sequence NMT (Su et al., 2018), NMT with context-aware encoder (Zhang et al., 2017) and Transformer (Vaswani et al., 2017) and so on."
        },
        {
            "heading": "Acknowledgments",
            "text": "The authors were supported by National Natural Science Foundation of China (No. 61672440), the Fundamental Research Funds for the Central Universities (Grant No. ZK1024), and Scientific Research Project of National Language Committee of China (Grant No. YB135-49). We also thank the reviewers for their insightful comments."
        }
    ],
    "title": "Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination",
    "year": 2018
}