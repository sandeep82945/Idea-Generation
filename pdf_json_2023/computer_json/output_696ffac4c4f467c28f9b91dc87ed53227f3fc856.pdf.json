{
    "abstractText": "This paper explores an expression-related selfsupervised learning (SSL) method (ContraWarping) to perform expression classification in the 5th Affective Behavior Analysis in-the-wild (ABAW) competition. Affective datasets are expensive to annotate, and SSL methods could learn from large-scale unlabeled data, which is more suitable for this task. By evaluating on the Aff-Wild2 dataset, we demonstrate that ContraWarping outperforms most existing supervised methods and shows great application potential in the affective analysis area. Codes will be released on: https: //github.com/youqingxiaozhua/ABAW5.",
    "authors": [
        {
            "affiliations": [],
            "name": "Fanglei Xue"
        },
        {
            "affiliations": [],
            "name": "Yifan Sun"
        },
        {
            "affiliations": [],
            "name": "Yi Yang"
        }
    ],
    "id": "SP:e673852a41fe4c7ba558b449e91327dfd0526bf4",
    "references": [
        {
            "authors": [
                "Emad Barsoum",
                "Cha Zhang",
                "Cristian Canton Ferrer",
                "Zhengyou Zhang"
            ],
            "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
            "venue": "In Proceedings of the 18th ACM International Conference on Multimodal Interaction,",
            "year": 2016
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A Simple Framework for Contrastive Learning of Visual Representations",
            "venue": "In ICML. arXiv,",
            "year": 2020
        },
        {
            "authors": [
                "Jiankang Deng",
                "Jia Guo",
                "Niannan Xue",
                "Stefanos Zafeiriou"
            ],
            "title": "Arcface: Additive angular margin loss for deep face recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Jean-Bastien Grill",
                "Florian Strub",
                "Florent Altch\u00e9",
                "Corentin Tallec",
                "Pierre H. Richemond",
                "Elena Buchatskaya",
                "Carl Doersch",
                "Bernardo Avila Pires",
                "Zhaohan Daniel Guo",
                "Mohammad Gheshlaghi Azar",
                "Bilal Piot",
                "Koray Kavukcuoglu",
                "R\u00e9mi Munos",
                "Michal Valko"
            ],
            "title": "Bootstrap your own latent: A new approach to self-supervised Learning",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "SL Happy",
                "Aurobinda Routray"
            ],
            "title": "Automatic facial expression recognition using features of salient facial patches",
            "venue": "IEEE transactions on Affective Computing,",
            "year": 2014
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked Autoencoders Are Scalable Vision Learners",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Mingjie He",
                "Jie Zhang",
                "Shiguang Shan",
                "Xilin Chen"
            ],
            "title": "Enhancing Face Recognition With Self-Supervised 3D Reconstruction",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Jae-Yeop Jeong",
                "Yeong-Gi Hong",
                "Daun Kim",
                "Yuchul Jung",
                "Jin-Woo Jeong"
            ],
            "title": "Facial expression recognition based on multi-head cross attention network",
            "venue": "arXiv preprint arXiv:2203.13235,",
            "year": 2022
        },
        {
            "authors": [
                "Dimitrios Kollias"
            ],
            "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Dimitrios Kollias"
            ],
            "title": "Abaw: learning from synthetic data & multi-task learning challenges",
            "venue": "In European Conference on Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Dimitrios Kollias",
                "Attila Schulc",
                "Elnar Hajiyev",
                "Stefanos Zafeiriou"
            ],
            "title": "Analysing affective behavior in the first abaw 2020 competition",
            "venue": "In 2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG",
            "year": 2020
        },
        {
            "authors": [
                "Dimitrios Kollias",
                "Viktoriia Sharmanska",
                "Stefanos Zafeiriou"
            ],
            "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
            "venue": "arXiv preprint arXiv:1910.11111,",
            "year": 1910
        },
        {
            "authors": [
                "Dimitrios Kollias",
                "Viktoriia Sharmanska",
                "Stefanos Zafeiriou"
            ],
            "title": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
            "venue": "arXiv preprint arXiv:2105.03790,",
            "year": 2021
        },
        {
            "authors": [
                "Dimitrios Kollias",
                "Panagiotis Tzirakis",
                "Alice Baird",
                "Alan Cowen",
                "Stefanos Zafeiriou"
            ],
            "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
            "venue": "arXiv preprint arXiv:2303.01498,",
            "year": 2023
        },
        {
            "authors": [
                "Dimitrios Kollias",
                "Panagiotis Tzirakis",
                "Mihalis A Nicolaou",
                "Athanasios Papaioannou",
                "Guoying Zhao",
                "Bj\u00f6rn Schuller",
                "Irene Kotsia",
                "Stefanos Zafeiriou"
            ],
            "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
            "venue": "International Journal of Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Dimitrios Kollias",
                "Stefanos Zafeiriou"
            ],
            "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
            "venue": "arXiv preprint arXiv:1910.04855,",
            "year": 1910
        },
        {
            "authors": [
                "Dimitrios Kollias",
                "Stefanos Zafeiriou"
            ],
            "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
            "venue": "arXiv preprint arXiv:2103.15792,",
            "year": 2021
        },
        {
            "authors": [
                "Dimitrios Kollias",
                "Stefanos Zafeiriou"
            ],
            "title": "Analysing affective behavior in the second abaw2 competition",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Shan Li",
                "Weihong Deng",
                "JunPing Du"
            ],
            "title": "Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "Yifan Li",
                "Haomiao Sun",
                "Zhaori Liu",
                "Hu Han"
            ],
            "title": "Affective behaviour analysis using pretrained model with facial priori, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Yong Li",
                "Jiabei Zeng",
                "Shiguang Shan",
                "Xilin Chen"
            ],
            "title": "Self- Supervised Representation Learning From Videos for Facial Action Unit Detection",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Delian Ruan",
                "Yan Yan",
                "Shenqi Lai",
                "Zhenhua Chai",
                "Chunhua Shen",
                "Hanzi Wang"
            ],
            "title": "Feature decomposition and reconstruction learning for effective facial expression recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Ying Shu",
                "Yan Yan",
                "Si Chen",
                "Jing-Hao Xue",
                "Chunhua Shen",
                "Hanzi Wang"
            ],
            "title": "Learning Spatial-Semantic Relationship 3 for Facial Attribute Recognition With Limited Labeled Data",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Kai Wang",
                "Xiaojiang Peng",
                "Jianfei Yang",
                "Shijian Lu",
                "Yu Qiao"
            ],
            "title": "Suppressing uncertainties for large-scale facial expression recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Kai Wang",
                "Xiaojiang Peng",
                "Jianfei Yang",
                "Debin Meng",
                "Yu Qiao"
            ],
            "title": "Region attention networks for pose and occlusion robust facial expression recognition",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Fanglei Xue",
                "Yifan Sun",
                "Yi Yang"
            ],
            "title": "Unsupervised facial expression representation learning with contrastive local warping, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Fanglei Xue",
                "Zichang Tan",
                "Yu Zhu",
                "Zhongsong Ma",
                "Guodong Guo"
            ],
            "title": "Coarse-to-fine cascaded networks with smooth predicting for video facial expression recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Fanglei Xue",
                "Qiangchang Wang",
                "Guodong Guo"
            ],
            "title": "Trans- FER: Learning Relation-aware Facial Expression Representations with Transformers",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision, Mar",
            "year": 2021
        },
        {
            "authors": [
                "Fanglei Xue",
                "Qiangchang Wang",
                "Zichang Tan",
                "Zhongsong Ma",
                "Guodong Guo"
            ],
            "title": "Vision transformer with attentive pooling for robust facial expression recognition",
            "venue": "IEEE Transactions on Affective Computing,",
            "year": 2022
        },
        {
            "authors": [
                "Stefanos Zafeiriou",
                "Dimitrios Kollias",
                "Mihalis A Nicolaou",
                "Athanasios Papaioannou",
                "Guoying Zhao",
                "Irene Kotsia"
            ],
            "title": "Aff-wild: Valence and arousal \u2018in-the-wild\u2019challenge",
            "venue": "In Computer Vision and Pattern Recognition Workshops (CVPRW),",
            "year": 2017
        },
        {
            "authors": [
                "Tenggan Zhang",
                "Chuanhe Liu",
                "Xiaolong Liu",
                "Yuchen Liu",
                "Liyu Meng",
                "Lei Sun",
                "Wenqiang Jiang",
                "Fengyuan Zhang",
                "Jinming Zhao",
                "Qin Jin"
            ],
            "title": "Multi-task learning framework for emotion recognition in-the-wild",
            "venue": "In European Conference on Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Wei Zhang",
                "Zunhu Guo",
                "Keyu Chen",
                "Lincheng Li",
                "Zhimeng Zhang",
                "Yu Ding"
            ],
            "title": "Prior aided streaming network for multi-task affective recognitionat the 2nd abaw2 competition",
            "venue": "arXiv preprint arXiv:2107.03708,",
            "year": 2021
        },
        {
            "authors": [
                "Wei Zhang",
                "Xianpeng Ji",
                "Keyu Chen",
                "Yu Ding",
                "Changjie Fan"
            ],
            "title": "Learning a Facial Expression Embedding Disentangled From Identity",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Wei Zhang",
                "Zhimeng Zhang",
                "Feng Qiu",
                "Suzhen Wang",
                "Bowen Ma",
                "Hao Zeng",
                "Rudong An",
                "Yu Ding"
            ],
            "title": "Transformer-based multimodal information fusion for facial expression analysis",
            "venue": "arXiv preprint arXiv:2203.12367,",
            "year": 2022
        },
        {
            "authors": [
                "Yuhang Zhang",
                "Chengrui Wang",
                "Weihong Deng"
            ],
            "title": "Relative Uncertainty Learning for Facial Expression Recognition",
            "venue": "In NeurIPS 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Yuhang Zhang",
                "Chengrui Wang",
                "Xu Ling",
                "Weihong Deng"
            ],
            "title": "Learn From All: Erasing Attention Consistency for Noisy Label Facial Expression Recognition",
            "venue": "In ECCV. arXiv,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Affective computing aims to automatically recognize expressions from static images or videos. With affective computing, people could build applications in society analysis, human-computer interaction systems, driver fatigue monitoring, and so on. For the past few years, many methods [6, 22, 25, 28, 31, 38, 39] have been proposed to recognize expressions. However, these methods all rely on precise human annotations to learn. Although some of them [27, 38, 39] could learn from noisy labels, they can not learn from unlabeled data. Unfortunately, expressions are subjective and subtle, making annotation a large-scale expression database very expensive and limiting the scale of current databases.\nRecently, some researchers proposed some selfsupervised learning methods to learn from unlabeled data. Contrastive learning-based methods (such as SimCLR [2], MoCo [8], BYOL [5], etc.) learn image features from different views of the Siamese network. Differently, MAE [7] try to reconstruct a masked image to learn semantic features. Some works also adopt these ideas for face tasks. SSPL [26]\n*Work was down when Fanglei Xue was an intern at Baidu Research. \u2020Corresponding author.\nlearns the spatial-semantic relationship of face images by correct rotated patches, face parsing, and area classification tasks. He et al. [10] try to benefit the face recognition task by adopting a 3D reconstruction task. TCAE [24] and FaceCycle [36] learn face representation by disentangling pose, expression, and identity features from each other. Most recently, a contrastive learning method ContraWarping [29] is proposed to learn expression-related features by directly simulating muscle movements. All these methods demonstrate their effectiveness in static image databases [1,20,22].\nAff-wild2 [12\u201321, 33] is a large-scale video database for ABAW competitions. It annotated 548 videos, around 2.7M frames, into eight pre-defined categories: anger, disgust, fear, happiness, sadness, surprise, neutral, and other. Thanks to the release of this database, we conduct experiments to explore the effectiveness of ContraWarping on this in-the-wild video database. By directly fine-tuning the pre-trained weights from ContraWarping, we get the performance of the validation set of Expression (Expr) Classification Challenge with a Res-50 backbone, significantly outperforming the supervised one."
        },
        {
            "heading": "2. Related Works",
            "text": "Many inspirational methods have been proposed in previous ABAW competitions. We investigate some expression classification methods and multi-task learning methods which, including the Expr task.\nZhang et al. [34] ensemble multiple 2D backbones to extract features for every single frame and concatenate these features to a temporal encoder to explore temporal features. By combining regression layers and classification layers, it learns from multi-task annotation and ranks first in the ABAW4 challenge. It also used MAE pre-trained weights to enhance its performance. Li et al. [23] also use MAE pre-trained weights combined with AffectNet supervised pre-trained weights and ranked 2nd in ABAW4. Zhang et al. [37] proposed a transformer-based fusion module to fuse multi-modality features from audio, image, and word information. Jeong et al. [11] extended the DAN model and\nar X\niv :2\n30 3.\n10 51\n1v 1\n[ cs\n.C V\n] 1\n8 M\nar 2\n02 3\nS1 S2 S3 S4\nbackbone\nConv x2 FC\nfear anger disgust happiness sadness surprise \u2026 fla2en\nFigure 1. The pipeline of our method. The first two stages of the backbone is freezed.\nachieved 2nd in ABAW3. Xue et al. [30] utilized a coarseto-fine cascade network with a temporal smoothing strategy and ranked 3rd in ABAW3. Zhang et al. [35] found that AU, VA, and Expr representations are intrinsically associated with each other and proposed a streaming network for multi-task learning."
        },
        {
            "heading": "3. Method",
            "text": "Since this paper focuses on exploring the efficiencies of different SSL methods, we adopt a simple framework to perform frame-wise classification. Architecture and implementation details are described below."
        },
        {
            "heading": "3.1. Architecture",
            "text": "Following [34], we simply adopt 2D backbones (such ResNet [9], ViT [4], etc.) to extract features from every frames. The pipeline of our architecture is illustrated in\nFig. 1. The face image is proceeded by a backbone to extract feature maps. After that, instead of adopting global average pooling, we adopt two convolutional layers with stride 2 to explore spacial relationships and then flatten via the spacial dimension to keep spatial information. One fullyconnected (FC) layer is further attached to generate the final classification results.\nTo evaluate the effectiveness of different pre-trained weights, we freeze the first two stages of the backbone and fine-tune the last two stages as well as the new-added layers."
        },
        {
            "heading": "3.2. Implementation",
            "text": "We adopt random cropping and horizontal flip for data augmentation to prevent over-fitting. The model is finetuned with the SGD optimizer for 8000 iters. The learning rate is set to 5e-3 with a cosine decay. The batch size is set to 128. Since the adjacent frames in the video are very similar, we randomly sample one frame of every ten frames for training. The average f1 Score across all eight categories on the validation set is reported.\nBy default, the Res-50 [9] network without the last classifier is adopted as the backbone. The kernel size of two\nBackbone Pre-trained F1-score\nIR-50 [3] Sup. MS1M 30.78 APViT [32] Sup. MS1M 35.48 APViT [32] Sup. RAF-DB 35.63\nRes-18 [9] ContraWarping 33.69 Res-50 [9] ContraWarping 37.57\nTable 1. Results with different backbones and pre-trained weights. Sup. indicates supervised pre-training with manually annotated labels.\ndown-sampling convolution layers is set to 2, and the hidden dimension is 256."
        },
        {
            "heading": "4. Experiments",
            "text": "To investigate the effectiveness of ContraWarping on this in-the-wild video database, we conduct experiments with several backbones and pre-trained weights on the validation set of ABAW5. As illustrated in Tab. 1, models with more parameters are not always better. APViT [32] is a recently proposed state-of-the-art method that combines both CNN and ViT for feature extraction. It boosts IR-50 from 30.78 to 35.48. However, it fails to outperform Res-50 with ContraWarping pre-trained, which achieves 37.57 on the validation set. The ContraWarping could increase the performance significantly. Even a simple Res-18 could outperform IR-50 with 33.69, indicating that ContraWarping pretraining is more suitable for expression analysis."
        },
        {
            "heading": "5. Conclusion",
            "text": "In this paper, we adopt a simple pipeline to evaluate the effectiveness of ContraWarping, a self-supervised learning method for affective analysis on Aff-Wild2. The ContraWarping could learn expression-related features from unlabeled data by simulating muscle movements. Experiments on Aff-Wild2 indicate that models initialized with ContraWarping pre-trained weights could extract more in-\nformative features and performs better than supervised ones."
        }
    ],
    "title": "Exploring Expression-related Self-supervised Learning for Affective Behaviour Analysis",
    "year": 2023
}