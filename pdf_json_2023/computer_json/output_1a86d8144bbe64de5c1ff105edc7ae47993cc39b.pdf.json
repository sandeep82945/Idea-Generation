{
    "abstractText": "Limited by expensive pixel-level labels, polyp segmentation models are plagued by data shortage and suffer from impaired generalization. In contrast, polyp bounding box annotations are much cheaper and more accessible. Thus, to reduce labeling cost, we propose to learn a weakly supervised polyp segmentation model (i.e.,WeakPolyp) completely based on bounding box annotations. However, coarse bounding boxes contain too much noise. To avoid interference, we introduce the mask-to-box (M2B) transformation. By supervising the outer box mask of the prediction instead of the prediction itself, M2B greatly mitigates the mismatch between the coarse label and the precise prediction. But, M2B only provides sparse supervision, leading to non-unique predictions. Therefore, we further propose a scale consistency (SC) loss for dense supervision. By explicitly aligning predictions across the same image at different scales, the SC loss largely reduces the variation of predictions. Note that our WeakPolyp is a plug-and-play model, which can be easily ported to other appealing backbones. Besides, the proposed modules are only used during training, bringing no computation cost to inference. Extensive experiments demonstrate the effectiveness of our proposed WeakPolyp, which surprisingly achieves a comparable performance with a fully supervised model, requiring no mask annotations at all. Codes are available at https://github.com/weijun88/WeakPolyp.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jun Wei"
        },
        {
            "affiliations": [],
            "name": "Yiwen Hu"
        },
        {
            "affiliations": [],
            "name": "Shuguang Cui"
        },
        {
            "affiliations": [],
            "name": "S.Kevin Zhou"
        },
        {
            "affiliations": [],
            "name": "Zhen Li"
        }
    ],
    "id": "SP:4bf1f944c92fd77c87f9469361a8978ec5e1b22a",
    "references": [
        {
            "authors": [
                "L. Cai",
                "M. Wu",
                "L. Chen",
                "W. Bai",
                "M. Yang",
                "S. Lyu",
                "Q. Zhao"
            ],
            "title": "Using guided self-attention with local information for polyp segmentation",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 629\u2013638",
            "year": 2022
        },
        {
            "authors": [
                "M. Cheng",
                "Z. Kong",
                "G. Song",
                "Y. Tian",
                "Y. Liang",
                "J. Chen"
            ],
            "title": "Learnable orientedderivative network for polyp segmentation",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 720\u2013730. Springer",
            "year": 2021
        },
        {
            "authors": [
                "B. Dong",
                "W. Wang",
                "D.P. Fan",
                "J. Li",
                "H. Fu",
                "L. Shao"
            ],
            "title": "Polyp-pvt: Polyp segmentation with pyramid vision transformers",
            "venue": "arXiv preprint arXiv:2108.06932",
            "year": 2021
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S. Gelly",
                "J. Uszkoreit",
                "N. Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "ICLR",
            "year": 2021
        },
        {
            "authors": [
                "D.P. Fan",
                "G.P. Ji",
                "T. Zhou",
                "G. Chen",
                "H. Fu",
                "J. Shen",
                "L. Shao"
            ],
            "title": "Pranet: Parallel reverse attention network for polyp segmentation",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 263\u2013273",
            "year": 2020
        },
        {
            "authors": [
                "S. Gao",
                "M. Cheng",
                "K. Zhao",
                "X. Zhang",
                "M. Yang",
                "P.H.S. Torr"
            ],
            "title": "Res2net: A new multi-scale backbone architecture",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell. 43(2), 652\u2013662",
            "year": 2021
        },
        {
            "authors": [
                "H. Itoh",
                "M. Misawa",
                "Y. Mori",
                "M. Oda",
                "S.E. Kudo",
                "K. Mori"
            ],
            "title": "Sun colonoscopy video database",
            "venue": "http://amed8k.sundatabase.org/",
            "year": 2020
        },
        {
            "authors": [
                "G.P. Ji",
                "Y.C. Chou",
                "D.P. Fan",
                "G. Chen",
                "D. Jha",
                "H. Fu",
                "L. Shao"
            ],
            "title": "Progressively normalized self-attention network for video polyp segmentation",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
            "year": 2021
        },
        {
            "authors": [
                "G.P. Ji",
                "G. Xiao",
                "Y.C. Chou",
                "D.P. Fan",
                "K. Zhao",
                "G. Chen",
                "L. Van Gool"
            ],
            "title": "Video polyp segmentation: A deep learning perspective",
            "venue": "Machine Intelligence Research",
            "year": 2022
        },
        {
            "authors": [
                "M. Misawa",
                "Kudo",
                "S.e.",
                "Y. Mori",
                "K. Hotta",
                "K. Ohtsuka",
                "T. Matsuda",
                "S. Saito",
                "T. Kudo",
                "T. Baba",
                "F Ishida"
            ],
            "title": "Development of a computer-aided detection system for colonoscopy and a publicly accessible large colonoscopy video database (with video)",
            "venue": "Gastrointestinal endoscopy 93(4), 960\u2013967",
            "year": 2021
        },
        {
            "authors": [
                "T.C. Nguyen",
                "T.P. Nguyen",
                "G.H. Diep",
                "A.H. Tran-Dinh",
                "T.V. Nguyen",
                "M.T. Tran"
            ],
            "title": "Ccbanet: Cascading context and balancing attention for polyp segmentation",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 633\u2013643. Springer",
            "year": 2021
        },
        {
            "authors": [
                "J.G.B. Puyal",
                "K.K. Bhatia",
                "P. Brandao",
                "O.F. Ahmad",
                "D. Toth",
                "R. Kader",
                "L. Lovat",
                "P. Mountney",
                "D. Stoyanov"
            ],
            "title": "Endoscopic polyp segmentation using a hybrid 2d/3d cnn",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 295\u2013305",
            "year": 2020
        },
        {
            "authors": [
                "C. Rother",
                "V. Kolmogorov",
                "A. Blake"
            ],
            "title": " grabcut\" interactive foreground extraction using iterated graph cuts",
            "venue": "ACM transactions on graphics (TOG) 23(3), 309\u2013314",
            "year": 2004
        },
        {
            "authors": [
                "Y. Shen",
                "X. Jia",
                "M.Q.H. Meng"
            ],
            "title": "Hrenet: A hard region enhancement network for polyp segmentation",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 559\u2013568. Springer",
            "year": 2021
        },
        {
            "authors": [
                "N.K. Tomar",
                "D. Jha",
                "U. Bagci",
                "S. Ali"
            ],
            "title": "Tganet: Text-guided attention for improved polyp segmentation",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 151\u2013160",
            "year": 2022
        },
        {
            "authors": [
                "J. Wang",
                "Q. Huang",
                "F. Tang",
                "J. Meng",
                "J. Su",
                "S. Song"
            ],
            "title": "Stepwise feature fusion: Local guides global",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 110\u2013120",
            "year": 2022
        },
        {
            "authors": [
                "W. Wang",
                "E. Xie",
                "X. Li",
                "D.P. Fan",
                "K. Song",
                "D. Liang",
                "T. Lu",
                "P. Luo",
                "L. Shao"
            ],
            "title": "Pvtv2: Improved baselines with pyramid vision transformer",
            "venue": "Computational Visual Media 8(3), 1\u201310",
            "year": 2022
        },
        {
            "authors": [
                "J. Wei",
                "Y. Hu",
                "G. Li",
                "S. Cui",
                "S. Kevin Zhou",
                "Z. Li"
            ],
            "title": "Boxpolyp: Boost generalized polyp segmentation using extra coarse bounding box annotations",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 67\u201377",
            "year": 2022
        },
        {
            "authors": [
                "J. Wei",
                "Y. Hu",
                "R. Zhang",
                "Z. Li",
                "S.K. Zhou",
                "S. Cui"
            ],
            "title": "Shallow attention network for polyp segmentation",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 699\u2013708",
            "year": 2021
        },
        {
            "authors": [
                "R. Zhang",
                "P. Lai",
                "X. Wan",
                "D.J. Fan",
                "F. Gao",
                "X.J. Wu",
                "G. Li"
            ],
            "title": "Lesion-aware dynamic kernel for polyp segmentation",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 99\u2013109",
            "year": 2022
        },
        {
            "authors": [
                "R. Zhang",
                "G. Li",
                "Z. Li",
                "S. Cui",
                "D. Qian",
                "Y. Yu"
            ],
            "title": "Adaptive context selection for polyp segmentation",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 253\u2013262",
            "year": 2020
        },
        {
            "authors": [
                "Y. Zhang",
                "H. Liu",
                "Q. Hu"
            ],
            "title": "Transfuse: Fusing transformers and cnns for medical image segmentation",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 14\u201324",
            "year": 2021
        },
        {
            "authors": [
                "X. Zhao",
                "L. Zhang",
                "H. Lu"
            ],
            "title": "Automatic polyp segmentation via multi-scale subtraction network",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 120\u2013130. Springer",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Keywords: Polyp segmentation \u00b7 Weak Supervision \u00b7 Colorectal cancer"
        },
        {
            "heading": "1 Introduction",
            "text": "Colorectal Cancer (CRC) has become a major threat to health worldwide. Since most CRCs originate from colorectal polyps, early screening for polyps is necessary. Given its significance, automatic polyp segmentation models [5,8,16,18] have been designed to aid in screening. For example, ACSNet [21], HRENet [14], LDNet [20] and CCBANet [11] propose to use convolutional neural networks to\n\u2020 Equal contributions\nar X\niv :2\n30 7.\n10 91\n2v 1\n[ cs\n.C V\n] 2\n0 Ju\nextract multi-scale contexts for robust predictions. LODNet [2], PraNet [5], and MSNet [23] aim to improve the model\u2019s discrimination of polyp boundaries. SANet [19] eliminates the distribution gap between the training set and the testing set, thus improving the model generalization. Recently, TGANet [15] introduces text embeddings to enhance the model\u2019s discrimination. Furthermore, Transfuse [22], PPFormer [1], and Polyp-Pvt [3] introduce the Transformer [4] backbone to extract global contexts, achieving a significant performance gain.\nAll above models are fully supervised and require pixel-level annotations. However, pixel-by-pixel labeling is time-consuming and expensive, which hampers practical clinical usage. Besides, many polyps do not have well-defined boundaries. Pixel-level labeling inevitably introduces subjective noise. To address the above limitations, a generalized polyp segmentation model is urgently needed. In this paper, we achieve this goal by a weakly supervised polyp segmentation model (named WeakPolyp) that only uses coarse bounding box annotations. Fig. 1(a) shows the differences between our WeakPolyp and fully supervised models. Compared with fully supervised ones, WeakPolyp requires only a bounding box for each polyp, thus dramatically reducing the labeling cost. More meaningfully, WeakPolyp can take existing large-scale polyp detection datasets to assist the polyp segmentation task. Finally, WeakPolyp does not require the labeling for polyp boundaries, avoiding the subjective noise at source. All these advantages make WeakPolyp more clinically practical.\nHowever, bounding box annotations are much coarser than pixel-level ones, which can not describe the shape of polyps. Simply adopting these box annotations as supervision introduces too much background noise, thereby leading to suboptimal models. As a solution, BoxPolyp [18] only supervises the pixels with high certainty. However, it requires a fully supervised model to predict the uncertainty map. Unlike BoxPolyp, our WeakPolyp completely follows the weakly supervised form that requires no additional models or annotations. Surprisingly, just by redesigning the supervision loss without any changes to the model structure, WeakPolyp achieves comparable performance to its fully supervised counterpart. Fig. 1(b) visualizes some predicted results by WeakPolyp.\nWeakPolyp is mainly enabled by two novel components: mask-to-box (M2B) transformation and scale consistency (SC) loss. In practice, M2B is applied to transform the predicted mask into a box-like mask by projection and backprojection. Then, this transformed mask is supervised by the bounding box annotation. This indirect supervision avoids the misleading of box-shape bias of annotations. However, many regions in the predicted mask are lost in the projection and therefore get no supervision. To fully explore these regions, we propose the SC loss to provide a pixel-level self-supervision while requiring no annotations at all. Specifically, the SC loss explicitly reduces the distance between predictions of the same image at different scales. By forcing feature alignment, it inhibits the excessive diversity of predictions, thus improving the model generalization.\nIn summary, our contributions are three-fold: (1) We build the WeakPolyp model completely based on bounding box annotations, which largely reduces the labeling cost and achieves a comparable performance to full supervision. (2) We propose the M2B transformation to mitigate the mismatch between the prediction and the supervision, and design the SC loss to improve the robustness of the model against the variability of the predictions. (3) Our proposed WeakPolyp is a plug-and-play option, which can boost the performances of polyp segmentation models under different backbones."
        },
        {
            "heading": "2 Method",
            "text": "Model Components. Fig. 2 depicts the components of WeakPolyp, including the segmentation phase and the supervision phase. For the segmentation phase, we adopt Res2Net [6] as the backbone. For input image I \u2208 RH\u00d7W , Res2Net extracts four scales of features {fi|i = 1, ..., 4} with the resolutions [ H2i+1 , W 2i+1 ]. Considering the computational cost, only f2, f3 and f4 are utilized. To fuse them, we first apply a 1\u00d71 convolutional layer to unify the channels of f2, f3, f4 and then use the bilinear upsampling to unify their resolutions. After being transformed to the same size, f2, f3, f4 are added together and fed into one 1\u00d7 1 convolutional layer for final prediction. Instead of the segmentation phase, our contributions primarily lie in the supervision phase, including mask-to-box (M2B) transformation and scale consistency (SC) loss. Notably, both M2B and SC are independent of the specific model structure. Model Pipeline. For each input image I, we first resize it into two different scales: I1 \u2208 Rs1\u00d7s1 and I2 \u2208 Rs2\u00d7s2 . Then, I1 and I2 are sent to the segmentation model and get two predicted masks P1 and P2, both of which have been resized to the same size. Next, an SC loss is proposed to reduce the distance between P1 and P2, which helps suppress the variation of the prediction. Finally, to fit the bounding box annotations (B), P1 and P2 are sent to M2B and converted into box-like masks T1 and T2. With T1/T2 and B, we calculate the binary cross entropy (BCE) loss and Dice loss, without worrying about noise interference."
        },
        {
            "heading": "2.1 Mask-to-Box (M2B) Transformation",
            "text": "One naive method to achieve the weakly supervised polyp segmentation is to use the bounding box annotation B to supervise the predicted mask P1/P2. Unfortunately, models trained in this way show poor generalization. Because there is a strong box-shape bias in B. Training with this bias, the model is forced to predict the box-shape mask, unable to maintain the polyp\u2019s contours. To solve this, we innovatively use B to supervise the bounding box mask (i.e.,T1/T2) of P1/P2, rather than P1/P2 itself. This indirect supervision separates P1/P2 from B so that P1/P2 is not affected by the shape bias of B while obtaining the position and extent of polyps. But how to implement the transformation from P1/P2 to T1/T2? We design the M2B module, which consists of two steps: projection and back-projection, as shown in Fig. 2. Projection. As shown in Eq. 1, given a predicted mask P \u2208 [0, 1]H\u00d7W , we project it horizontally and vertically into two vectors Pw \u2208 [0, 1]1\u00d7W and Ph \u2208 [0, 1]H\u00d71. In this projection, instead of using mean pooling, we use max pooling to pick the maximum value for each row/column in P . Because max pooling can completely remove the shape information of the polyp. After projection, only the position and scope of the polyp are stored in Pw and Ph.\nPw = max(P, axis = 0) \u2208 [0, 1]1\u00d7W , Ph = max(P, axis = 1) \u2208 [0, 1]H\u00d71 (1)\nBack-projection. Based on Pw and Ph, we construct the bounding box mask of the polyp by back-projection. As shown in Eq. 2, Pw and Ph are first repeated into P \u2032 w and P \u2032\nh with the same size as P . Then, we element-wisely take the minimum of P \u2032 w and P \u2032\nh to achieve the bounding box mask T . As shown in Fig. 2, T no longer contains the contours of the polyp.\nP \u2032 w = repeat(Pw, H, axis = 0) \u2208 [0, 1]H\u00d7W P \u2032\nh = repeat(Ph,W, axis = 1) \u2208 [0, 1]H\u00d7W\nT = min(P \u2032 w, P \u2032 h) \u2208 [0, 1]H\u00d7W (2)\nSupervision. By M2B, P1 and P2 are transformed into T1 and T2, respectively. Because both T1/T2 and B are box-like masks, we directly calculate the supervision loss between them without worrying about the misguidance of box-shape bias. Specifically, we follow [19,5] to adopt BCE loss LBCE and Dice loss LDice for model supervision, as shown in Eq. 3.\nLSum = LBCE(T1, B) + LBCE(T2, B)\n2 + LDice(T1, B) + LDice(T2, B) 2\n(3)\nPriority. By simple transformation, M2B turns the noisy supervision into a noise-free one, so that the predicted mask is able to preserve the contours of the polyp. Notably, M2B is differentiable, which can be easily implemented with PyTorch and plugged into the model to participate in gradient backpropagation."
        },
        {
            "heading": "2.2 Scale Consistency (SC) Loss",
            "text": "In M2B, most pixels in P are ignored in the projection, thus only a few pixels with high response values are involved in the supervision loss. This sparse supervision may lead to non-unique predictions. As shown in Fig. 3, after M2B projection, five predicted masks with different response values can be transformed into the same bounding box mask. Therefore, we consider introducing the SC loss to achieve dense supervision without annotations, which reduces the degree of freedom of predictions. Method. As shown in Fig. 2, due to the non-uniqueness of the prediction and the scale difference between I1 and I2, P1 and P2 differ in response values. But P1 and P2 come from the same image I1. They should be exactly the same. Given this, as shown in Eq. 4, we build the dense supervision LSC by explicitly\nreducing the distance between P1 and P2, where (i, j) is the pixel coordinates. Note that only pixels inside bounding box are involved in LSC to emphasize more on polyp regions. Despite its simplicity, LSC brings pixel-level constraints to compensate for the sparsity of LSum, thus reducing the variety of predictions.\nLSC = \u2211 (i,j)\u2208box |P i,j 1 \u2212 P i,j 2 |\u2211\n(i,j)\u2208box 1 (4)"
        },
        {
            "heading": "2.3 Total Loss",
            "text": "As shown in Eq. 5, combining LSum and LSC together, we get WeakPolyp model. Note that WeakPolyp simply replaces the supervision loss without making any changes to the model structure. Therefore, it is general and can be ported to other models. Besides, LSum and LSC are only used during training. In inference, they will be removed, thus having no effect on the speed of the model.\nLTotal = LSum + LSC (5)"
        },
        {
            "heading": "3 Experiments",
            "text": "Datasets. Two large polyp datasets are adopted to evaluate the model performance, including SUN-SEG [9] and POLYP-SEG. SUN-SEG originates from [7,10], which consists of 19,544 training images, 17,070 easy tesing images, and 12,522 hard testing images. POLYP-SEG is our private polyp segmentation dataset,\nwhich contains 15,916 training images and 4,040 testing images. Note that, during training, only bounding box annotations are adopted in our WeakPolyp. Training Settings. WeakPolyp is implemented using PyTorch. All input images are uniformly resized to 352\u00d7352. For data augmentation, random flip, random rotation, and multi-scale training are adopted. The whole network is trained in an end-to-end way with an AdamW optimizer. Initial learning rate and batch size are set to 1e-4 and 16, respectively. We train the entire model for 16 epochs. Quantitative Comparison. Table. 1 compares the model performance under different supervisions, backbones, and datasets. The overall performance order is gt>WeakPolyp>box>grabcut. The model supervised by grabcut [13] masks performs the worst, because the foreground and background of polyp images are similar. Grabcut can not well distinguish between them, resulting in poor masks. Our WeakPolyp predictably outperforms the model supervised by box masks because it is not affected by the box-shape bias of the annotations. Interestingly, WeakPolyp even surpasses the fully supervised model on SUN-SEG, which indicates that there is a lot of noise in the pixel-level annotations. But WeakPolyp does not require pixel-level annotations so it avoids noise interference. Visual Comparison. Fig. 4 visualizes some predictions based on different supervisions. Compared with other counterparts, WeakPolyp not only highlights\nthe polyp shapes but also suppresses the background noise. Even for challenging scenarios, WeakPolyp still handles well and generates accurate masks. Ablation Study. To investigate the importance of each component in WeakPolyp, we evaluate the model on both Res2Net-50 and PVTv2-B2 for ablation studies. As shown in Table 2, all proposed modules are beneficial for the final predictions. Combining all these modules, our model achieves the highest performance. Compared with Fully Supervised Methods. Table. 3 shows our WeakPolyp is even superior to many previous fully supervised methods: PraNet [5], SANet [19], 2/3D [12] and PNS+ [9], which shows the excellent application prospect of weakly supervised learning in the polyp field."
        },
        {
            "heading": "4 Conclusion",
            "text": "Limited by expensive labeling cost, pixel-level annotations are not readily available, which hinders the development of the polyp segmentation field. In this paper, we propose the WeakPolyp model completely based on bounding box annotations. WeakPolyp requires no pixel-level annotations, thus avoiding the interference of subjective noise labels. More importantly, WeakPolyp even achieves a comparable performance to the fully supervised models, showing the great potential of weakly supervised learning in the polyp segmentation field. In future, we will introduce temporal information into weakly supervised polyp segmentation to further reduce the model\u2019s dependence on labeling."
        },
        {
            "heading": "5 Acknowledgement",
            "text": "This work was supported in part by Shenzhen General Program No. JCYJ20220530143600001, by the Basic Research Project No. HZQB-KCZYZ2021067 of Hetao Shenzhen HK S&T Cooperation Zone, by Shenzhen-Hong Kong Joint Funding No. SGDX20211123112401002, by Shenzhen Outstanding Talents Training Fund, by Guangdong Research Project No. 2017ZT07X152 and No. 2019CX01X104, by the Guangdong Provincial Key Laboratory of Future Networks of Intelligence (Grant No. 2022B1212010001), by the Guangdong Provincial Key Laboratory of Big Data Computing, The Chinese University of Hong\nKong, Shenzhen, by the NSFC 61931024&81922046, by zelixir biotechnology company Fund, by Tencent Open Fund."
        }
    ],
    "title": "WeakPolyp: You Only Look Bounding Box for Polyp Segmentation",
    "year": 2023
}