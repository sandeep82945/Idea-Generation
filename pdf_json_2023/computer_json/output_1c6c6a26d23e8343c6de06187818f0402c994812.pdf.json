{
    "abstractText": "Despite serving as the foundation models for a wide range of NLP benchmarks, pre-trained language models have shown limited capabilities of acquiring implicit commonsense knowledge from self-supervision alone, compared to learning linguistic and factual knowledge that appear more explicitly in the surface patterns in text. In this work, we introduce commonsense knowledge transfer, a framework to transfer the commonsense knowledge stored in a neural commonsense knowledge model to a generalpurpose pre-trained language model. It first exploits general texts to form queries for extracting commonsense knowledge from the neural commonsense knowledge model and then refines the language model with two selfsupervised objectives: commonsense mask infilling and commonsense relation prediction, which align human language with the underlying commonsense knowledge. Empirical results show that our approach consistently improves the model\u2019s performance on downstream tasks that require commonsense reasoning. Moreover, we find that the improvement is more significant in the few-shot setting. This suggests that our approach helps language models better transfer to downstream tasks without extensive supervision by injecting commonsense knowledge into their parameters.",
    "authors": [
        {
            "affiliations": [],
            "name": "Wangchunshu Zhou"
        },
        {
            "affiliations": [],
            "name": "Ronan Le Bras"
        },
        {
            "affiliations": [],
            "name": "Yejin Choi"
        }
    ],
    "id": "SP:cda33cc3a398ca2e28d1c17b59ce071b9b74fe6e",
    "references": [
        {
            "authors": [
                "Emily M. Bender",
                "Alexander Koller."
            ],
            "title": "Climbing towards NLU: on meaning, form, and understanding in the age of data",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Chaitanya Malaviya",
                "Keisuke Sakaguchi",
                "Ari Holtzman",
                "Hannah Rashkin",
                "Doug Downey",
                "Scott Wen-tau Yih",
                "Yejin Choi."
            ],
            "title": "Abductive commonsense reasoning",
            "venue": "ICLR.",
            "year": 2020
        },
        {
            "authors": [
                "Yonatan Bisk",
                "Rowan Zellers",
                "Ronan Le Bras",
                "Jianfeng Gao",
                "Yejin Choi."
            ],
            "title": "Piqa: Reasoning about physical commonsense in natural language",
            "venue": "ThirtyFourth AAAI Conference on Artificial Intelligence (AAAI).",
            "year": 2020
        },
        {
            "authors": [
                "Antoine Bosselut",
                "Hannah Rashkin",
                "Maarten Sap",
                "Chaitanya Malaviya",
                "Asli Celikyilmaz",
                "Yejin Choi."
            ],
            "title": "COMET: commonsense transformers for automatic knowledge graph construction",
            "venue": "ACL (1), pages 4762\u20134779. Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Alexis Conneau",
                "Douwe Kiela."
            ],
            "title": "Senteval: An evaluation toolkit for universal sentence representations",
            "venue": "LREC.",
            "year": 2018
        },
        {
            "authors": [
                "Wanyun Cui",
                "Xingran Chen."
            ],
            "title": "Enhancing language models with plug-and-play large-scale commonsense",
            "venue": "CoRR, abs/2109.02572.",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL-HLT (1), pages 4171\u20134186. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "William B. Dolan",
                "Chris Brockett."
            ],
            "title": "Automatically constructing a corpus of sentential paraphrases",
            "venue": "IWP@IJCNLP.",
            "year": 2005
        },
        {
            "authors": [
                "WA Falcon."
            ],
            "title": "Pytorch lightning",
            "venue": "GitHub. Note: https://github.com/PyTorchLightning/pytorchlightning, 3.",
            "year": 2019
        },
        {
            "authors": [
                "Bin He",
                "Xin Jiang",
                "Jinghui Xiao",
                "Qun Liu."
            ],
            "title": "Kgplm: Knowledge-guided language model pretraining via generative and discriminative learning",
            "venue": "CoRR, abs/2012.03551.",
            "year": 2020
        },
        {
            "authors": [
                "John Hewitt",
                "Christopher D. Manning."
            ],
            "title": "A structural probe for finding syntax in word representations",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2019
        },
        {
            "authors": [
                "Geoffrey E. Hinton",
                "Oriol Vinyals",
                "Jeffrey Dean."
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "CoRR, abs/1503.02531.",
            "year": 2015
        },
        {
            "authors": [
                "Pedram Hosseini",
                "David A. Broniatowski",
                "Mona T. Diab."
            ],
            "title": "Commonsense knowledge-augmented pretrained language models for causal reasoning classification",
            "venue": "CoRR, abs/2112.08615.",
            "year": 2021
        },
        {
            "authors": [
                "Jena D. Hwang",
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Jeff Da",
                "Keisuke Sakaguchi",
                "Antoine Bosselut",
                "Yejin Choi."
            ],
            "title": "comet-) atomic 2020: On symbolic and neural commonsense knowledge graphs",
            "venue": "AAAI, pages 6384\u20136392. AAAI Press.",
            "year": 2021
        },
        {
            "authors": [
                "Yoon Kim",
                "Alexander M. Rush."
            ],
            "title": "Sequencelevel knowledge distillation",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages 1317\u20131327. The",
            "year": 2016
        },
        {
            "authors": [
                "Tassilo Klein",
                "Moin Nabi."
            ],
            "title": "Towards zero-shot commonsense reasoning with self-supervised refinement of language models",
            "venue": "CoRR, abs/2109.05105.",
            "year": 2021
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Shiyang Li",
                "Jianshu Chen",
                "Dian Yu."
            ],
            "title": "Teaching pretrained models with commonsense reasoning: A preliminary kb-based approach",
            "venue": "CoRR, abs/1909.09743.",
            "year": 2019
        },
        {
            "authors": [
                "Bill Yuchen Lin",
                "Xinyue Chen",
                "Jamin Chen",
                "Xiang Ren."
            ],
            "title": "Kagnet: Knowledge-aware graph networks for commonsense reasoning",
            "venue": "EMNLP/IJCNLP (1), pages 2829\u20132839. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Bill Yuchen Lin",
                "Wangchunshu Zhou",
                "Ming Shen",
                "Pei Zhou",
                "Chandra Bhagavatula",
                "Yejin Choi",
                "Xiang Ren."
            ],
            "title": "Commongen: A constrained text generation challenge for generative commonsense reasoning",
            "venue": "Findings of the Association for Com-",
            "year": 2020
        },
        {
            "authors": [
                "Ye Liu",
                "Yao Wan",
                "Lifang He",
                "Hao Peng",
                "Philip S. Yu"
            ],
            "title": "Kg-bart: Knowledge graph-augmented bart for generative commonsense reasoning",
            "year": 2020
        },
        {
            "authors": [
                "Ye Liu",
                "Yao Wan",
                "Lifang He",
                "Hao Peng",
                "Philip S. Yu."
            ],
            "title": "KG-BART: knowledge graph-augmented BART for generative commonsense reasoning",
            "venue": "AAAI, pages 6418\u20136425. AAAI Press.",
            "year": 2021
        },
        {
            "authors": [
                "Christopher D. Manning",
                "Kevin Clark",
                "John Hewitt",
                "Urvashi Khandelwal",
                "Omer Levy."
            ],
            "title": "Emergent linguistic structure in artificial neural networks trained by self-supervision",
            "venue": "Proc. Natl. Acad. Sci. USA, 117(48):30046\u201330054.",
            "year": 2020
        },
        {
            "authors": [
                "William Merrill",
                "Yoav Goldberg",
                "Roy Schwartz",
                "Noah A. Smith"
            ],
            "title": "Provable limitations of acquiring meaning from ungrounded form: What will future language models understand? CoRR, abs/2104.10809",
            "year": 2021
        },
        {
            "authors": [
                "Todor Mihaylov",
                "Peter Clark",
                "Tushar Khot",
                "Ashish Sabharwal."
            ],
            "title": "Can a suit of armor conduct electricity? a new dataset for open book question answering",
            "venue": "arXiv preprint arXiv:1809.02789.",
            "year": 2018
        },
        {
            "authors": [
                "Matthew E Peters",
                "Mark Neumann",
                "Robert L Logan IV",
                "Roy Schwartz",
                "Vidur Joshi",
                "Sameer Singh",
                "Noah A Smith."
            ],
            "title": "Knowledge enhanced contextual word representations",
            "venue": "arXiv preprint arXiv:1909.04164.",
            "year": 2019
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Patrick S.H. Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander H. Miller"
            ],
            "title": "Language models as knowledge bases",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "arXiv preprint arXiv:1910.10683.",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "Squad: 100, 000+ questions for machine comprehension of text",
            "venue": "EMNLP.",
            "year": 2016
        },
        {
            "authors": [
                "Adam Roberts",
                "Colin Raffel",
                "Noam Shazeer"
            ],
            "title": "How much knowledge can you pack into the parameters of a language model",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Melissa Roemmele",
                "Cosmin Adrian Bejan",
                "Andrew S. Gordon."
            ],
            "title": "Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
            "venue": "Logical Formalizations of Commonsense Reasoning, Papers from the 2011 AAAI Spring Sym-",
            "year": 2011
        },
        {
            "authors": [
                "Maarten Sap",
                "Ronan Le Bras",
                "Emily Allaway",
                "Chandra Bhagavatula",
                "Nicholas Lourie",
                "Hannah Rashkin",
                "Brendan Roof",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "ATOMIC: an atlas of machine commonsense for if-then reasoning",
            "venue": "AAAI, pages 3027\u20133035.",
            "year": 2019
        },
        {
            "authors": [
                "Maarten Sap",
                "Hannah Rashkin",
                "Derek Chen",
                "Ronan Le Bras",
                "Yejin Choi."
            ],
            "title": "Socialiqa: Commonsense reasoning about social interactions",
            "venue": "EMNLP.",
            "year": 2019
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Y. Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "EMNLP.",
            "year": 2013
        },
        {
            "authors": [
                "Robyn Speer",
                "Joshua Chin",
                "Catherine Havasi"
            ],
            "title": "Conceptnet 5.5: An open multilingual graph of general knowledge",
            "venue": "In AAAI,",
            "year": 2017
        },
        {
            "authors": [
                "Alon Talmor",
                "Jonathan Herzig",
                "Nicholas Lourie",
                "Jonathan Berant."
            ],
            "title": "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
            "venue": "arXiv preprint arXiv:1811.00937.",
            "year": 2018
        },
        {
            "authors": [
                "Lifu Tu",
                "Garima Lalwani",
                "Spandana Gella",
                "He He."
            ],
            "title": "An empirical study on robustness to spurious correlations using pre-trained language models",
            "venue": "Trans. Assoc. Comput. Linguistics, 8:621\u2013633.",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "NIPS, pages 5998\u20136008.",
            "year": 2017
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R. Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Ruize Wang",
                "Duyu Tang",
                "Nan Duan",
                "Zhongyu Wei",
                "Xuanjing Huang",
                "Cuihong Cao",
                "Daxin Jiang",
                "Ming Zhou"
            ],
            "title": "K-adapter: Infusing knowledge into pre-trained models with adapters",
            "venue": "arXiv preprint arXiv:2002.01808",
            "year": 2020
        },
        {
            "authors": [
                "Alex Warstadt",
                "Amanpreet Singh",
                "Samuel R. Bowman."
            ],
            "title": "Neural network acceptability judgments",
            "venue": "TACL.",
            "year": 2019
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel R. Bowman."
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "NAACL-HLT.",
            "year": 2018
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander M. Rush."
            ],
            "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing",
            "venue": "ArXiv, abs/1910.03771.",
            "year": 2019
        },
        {
            "authors": [
                "Wenhan Xiong",
                "Jingfei Du",
                "William Yang Wang",
                "Veselin Stoyanov."
            ],
            "title": "Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Canwen Xu",
                "Wangchunshu Zhou",
                "Tao Ge",
                "Ke Xu",
                "Julian J. McAuley",
                "Furu Wei."
            ],
            "title": "Blow the dog whistle: A chinese dataset for cant understanding with common sense and world knowledge",
            "venue": "NAACL-HLT, pages 2139\u20132145. Association for",
            "year": 2021
        },
        {
            "authors": [
                "Zhi-Xiu Ye",
                "Qian Chen",
                "Wen Wang",
                "Zhen-Hua Ling."
            ],
            "title": "Align, mask and select: A simple method for incorporating commonsense knowledge into language representation models",
            "venue": "CoRR, abs/1908.06725.",
            "year": 2019
        },
        {
            "authors": [
                "Zhengyan Zhang",
                "Xu Han",
                "Zhiyuan Liu",
                "Xin Jiang",
                "Maosong Sun",
                "Qun Liu."
            ],
            "title": "Ernie: Enhanced language representation with informative entities",
            "venue": "arXiv preprint arXiv:1905.07129.",
            "year": 2019
        },
        {
            "authors": [
                "Wangchunshu Zhou",
                "Dong-Ho Lee",
                "Ravi Kiran Selvam",
                "Seyeon Lee",
                "Xiang Ren."
            ],
            "title": "Pre-training text-to-text transformers for concept-centric common sense",
            "venue": "ICLR. OpenReview.net.",
            "year": 2021
        },
        {
            "authors": [
                "Xuhui Zhou",
                "Yue Zhang",
                "Leyang Cui",
                "Dandan Huang."
            ],
            "title": "Evaluating commonsense in pretrained language models",
            "venue": "AAAI, pages 9733\u20139740. AAAI Press.",
            "year": 2020
        },
        {
            "authors": [
                "Xuhui Zhou",
                "Yue Zhang",
                "Leyang Cui",
                "Dandan Huang."
            ],
            "title": "Evaluating commonsense in pretrained language models",
            "venue": "AAAI, pages 9733\u20139740.",
            "year": 2020
        },
        {
            "authors": [
                "Yukun Zhu",
                "Ryan Kiros",
                "Richard S. Zemel",
                "Ruslan Salakhutdinov",
                "Raquel Urtasun",
                "Antonio Torralba",
                "Sanja Fidler."
            ],
            "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
            "venue": "2015 IEEE Interna-",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "In this work, we introduce commonsense knowledge transfer, a framework to transfer the commonsense knowledge stored in a neural commonsense knowledge model to a generalpurpose pre-trained language model. It first exploits general texts to form queries for extracting commonsense knowledge from the neural commonsense knowledge model and then refines the language model with two selfsupervised objectives: commonsense mask infilling and commonsense relation prediction, which align human language with the underlying commonsense knowledge.\nEmpirical results show that our approach consistently improves the model\u2019s performance on downstream tasks that require commonsense reasoning. Moreover, we find that the improvement is more significant in the few-shot setting. This suggests that our approach helps language models better transfer to downstream tasks without extensive supervision by injecting commonsense knowledge into their parameters."
        },
        {
            "heading": "1 Introduction",
            "text": "Recent advances in pre-trained language models have transformed the landscape of natural language processing. Self-supervised pre-training objectives including masked language modeling (Devlin et al., 2019) and masked span infilling (Lewis et al., 2020) enable pre-trained models to acquire linguistic (Hewitt and Manning, 2019; Manning et al., 2020) and factual knowledge (Petroni et al., 2019) by modeling the distribution of naturally occurring texts.\n\u2217Work done while interning at the Allen Institute for AI\nHowever, most of these objectives are limited to exploiting the surface form of human language, and the lack of grounded supervision calls into question how well these representations can ever capture meaning (Bender and Koller, 2020), not to mention the underlying commonsense knowledge which is often reasoned implicitly and does not appear in the surface form of human language (Merrill et al., 2021; Zhou et al., 2020a; Hwang et al., 2021). On the other hand, commonsense reasoning is important for building generalizable models because it enables the model to reason about a great number of events, causes, and effects, while observing only a small fraction of them. The ineffectiveness of self-supervised language model pre-training on acquiring commonsense knowledge makes them require a relatively large number of labeled examples to succeed in a downstream task and prune to overfit task-specific correlations (Tu et al., 2020).\nTherefore, equipping pre-trained language models with commonsense reasoning ability has attracted much attention. To this end, two distinct lines of research focus on improving commonsense reasoning ability of pre-trained language models.\nar X\niv :2\n30 6.\n02 38\n8v 1\n[ cs\n.C L\n] 4\nJ un\n2 02\n3\nThe first one focuses on incorporating external commonsense knowledge graph for commonsense reasoning (Lin et al., 2019; Liu et al., 2021; Cui and Chen, 2021) while the other attempts to inject commonsense knowledge into the parameters of pretrained models (Li et al., 2019; Zhou et al., 2021; Klein and Nabi, 2021). In this work we focus on the second type of method because it alleviates the need for external knowledge bases for training and inference on downstream tasks, thus simpler, more efficient, and not limited by the coverage issue of external knowledge bases.\nPrior work injects commonsense knowledge into pre-trained models either on symbolic commonsense knowledge graphs with manually defined rules (Li et al., 2019) or masked language modeling (Hosseini et al., 2021) or on general text corpus with concept-centric self-supervised objectives (Zhou et al., 2021). The former method is limited by the coverage of knowledge graphs and human-written rules. It also fails to make use of large-scale diverse natural text corpus. Therefore, the training is limited to short and synthetic commonsense tuples, which affects its generalization ability on diverse downstream tasks. The latter method, however, only captures surface-level order relations between concepts and fails to learn commonsense relations between concepts such as cause, effect, intent, requirement, etc., which are crucial for commonsense reasoning but often implicitly reasoned, thus do not appear in the surface form of natural language.\nIn this work, we propose commonsense knowledge transfer, an alternative framework to refine a general purpose pre-trained model\u2019s commonsense reasoning ability. In contrast to previous work, it aims to transfer the commonsense knowledge stored in a neural commonsense knowledge model (e.g., COMET (Bosselut et al., 2019)) to a general purpose pre-trained model on large scale general text corpus. In this way, our approach combines the best of both worlds from prior art: the dense and informative commonsense knowledge from commonsense knowledge graphs and the accessibility of large-scale diverse general corpus.\nCommonsense knowledge transfer is conceptually related to knowledge distillation (KD) (Hinton et al., 2015) since they both aim to transfer knowledge from a knowledge-rich model to another model that lacks it. However, different from conventional KD, in commonsense knowl-\nedge transfer, the source model (i.e., neural commonsense model) and the target model (i.e., pretrained model) are heterogeneous. Moreover, instead of simply mimicking the teacher model, commonsense knowledge transfer requires the target model to learn specialized knowledge from the source model while retaining its own capability. This poses unique challenges since the knowledge transfer can not be accomplished by simply matching the logits or feature distribution between the student and the teacher. To this end, we propose to first extract commonsense knowledge in textual form from the source model and then exploit the extracted knowledge to form self-supervised training data for the target model. As illustrated in Figure 1, commonsense knowledge transfer first exploits general texts to form queries for retrieving commonsense knowledge from the neural commonsense knowledge model. Then it refines a pretrained model with two self-supervised objectives that align the surface form of human language with its underlying commonsense inference: commonsense text infilling and commonsense relation prediction. The former objective concatenates natural text with its commonsense inference to form an input example, masks certain spans in it, and trains the model to reconstruct the original input. The latter method instead trains the model to distinguish valid commonsense inference from carefully constructed spurious commonsense inference given the original text and commonsense relation. Refining a pre-trained model by multi-tasking on both generation (former) and understanding (latter) tasks enables the model to better adapt to different kinds of downstream tasks.\nWe refine T5 (Raffel et al., 2020) with commonsense knowledge transfer and fine-tune the resulting model downstream tasks requiring commonsense reasoning ability in both the fully supervised setting and few-shot settings where only a percentage of labeled examples are available. Experimental results show substantial improvements in downstream tasks requiring commonsense reasoning, especially in the few-shot setting, demonstrating the effectiveness of our approach."
        },
        {
            "heading": "2 Methodology",
            "text": "Our proposed commonsense knowledge transfer framework consists of a neural commonsense knowledge model (e.g., COMET) and a pre-trained model (e.g., T5). The goal of commonsense knowl-\nedge transfer is to transfer the commonsense knowledge from the neural commonsense knowledge model (i.e., source model) to the pre-trained model (i.e., target model) so that it can generalize better to downstream tasks requiring commonsense reasoning ability.\nCompared to conventional knowledge transfer methods such as knowledge distillation, commonsense knowledge transfer faces a unique challenge: the source model and the target model are heterogeneous because they are trained on different data with different objectives. As such, we can not simply feed a batch of data to both of the models and train the target model to match the source model\u2019s logits or feature distribution. To alleviate this problem, we propose a two-stage knowledge transfer scheme as illustrated in Figure 1. To be specific, we first use natural texts to form queries for retrieving commonsense knowledge (in text form) from the neural commonsense knowledge model. We then construct training data with two novel commonsense-related self-supervised objectives based on the retrieved commonsense knowledge and the corresponding natural text. Finally, we train the target model on the constructed training data to inject commonsense knowledge retrieved from the source model. We describe our method to extract commonsense knowledge from a neural commonsense knowledge model and the proposed commonsense-related self-supervised objectives in detail in this section."
        },
        {
            "heading": "2.1 Commonsense Knowledge Extraction",
            "text": "We first describe the source model, i.e., neural commonsense knowledge model, in the commonsense knowledge transfer framework. It is a transformer (Vaswani et al., 2017) language model trained on commonsense knowledge graphs like ATOMIC (Sap et al., 2019a) and ConceptNet (Speer et al., 2017) with the objective of predicting the object (i.e., commonsense inference) with the subject (i.e., natural text) and relation as input. For example, given a commonsense tuple (s=\u201ctake a nap\", r=Causes, o=\u201chave energy\"), the neural commonsense knowledge model is trained to generate o given s and r as inputs. After training, it can generate accurate, representative knowledge for new, unseen entities and events.\nTo extract commonsense knowledge stored in a neural commonsense knowledge model, we use a natural sentence as the subject s (e.g., he wants to\ncook a meal) and concatenate it with a randomly selected commonsense relation r (e.g., xNeed) from a pre-defined set to form a prompt (e.g., he wants to cook a meal xNeed ). We then feed the prompt to the neural commonsense knowledge model and use it to generate a commonsense inference (e.g., to buy ingredients). In this way, the commonsense knowledge generation process resembles the way in which the neural commonsense knowledge model is trained. As such, we can get commonsense inferences of relatively high qualities.\nUsing a neural commonsense knowledge model as a knowledge source has two advantages. On one hand, compared to the previous method (Li et al., 2019) using a symbolic commonsense knowledge graph, a neural commonsense knowledge model can generalize to unseen subjects, thus enabling us to refine the target pre-trained model on large-scale natural text corpus together with its commonsense inferences. As such, the resulting model can better adapt to downstream tasks which are formulated in diverse natural texts. On the other hand, compared to another method (Zhou et al., 2021) that only uses plain text and is thus limited to the surface form of naturally occurring text, the use of a neural commonsense knowledge model provides much denser commonsense knowledge including a diverse set of commonsense relations between natural texts and the underlying commonsense knowledge."
        },
        {
            "heading": "2.2 Commonsense Knowledge Injection",
            "text": "After commonsense knowledge extraction, we need to inject the extracted commonsense knowledge into the target model. A straightforward solution is to use sequence-level knowledge distillation (Kim and Rush, 2016) and continually train the student to generate retrieved commonsense inference given the original text and commonsense relation. However, this can be sub-optimal due to the domain discrepancy between commonsense knowledge and natural text, which introduces the catastrophic forgetting problem (Kirkpatrick et al., 2017) and hurts the performance on downstream tasks, which is also recently confirmed by Cui and Chen (2021).\nTo better inject the extracted commonsense knowledge into a pre-trained model without suffering from catastrophic forgetting so that its capability on general NLP tasks is retained (or even improved), we propose two commonsense-related self-supervised objectives: commonsense text infilling and commonsense relation prediction. The\nformer objective is generative while the latter is a discriminative objective. We refine the pre-trained model by multi-tasking on both objectives so that the model can better adapt to tasks requiring either generative or discriminative commonsense reasoning ability.\nCommonsense Text Infilling Commonsense text infilling is a simple extension to the conventional text infilling objective used for pre-training BART and T5. It transforms each sentence to a commonsense tuple similar to that in a commonsense knowledge graph by appending the commonsense relation and the generated commonsense inference. We then mask text spans in the commonsense tuple by randomly selecting one masking scheme among text masking, commonsense masking, bidirectional masking, and relation masking. As illustrated in Fig 2, these masking strategies selectively mask different components in the input commonsense tuple and lead to different optimization objectives. Specifically, these masking schemes mask either spans in natural text (P(s|s\u0303, r, o)), commonsense inference (P(o|s, r, o\u0303)), natural text/commonsense inference (P(s, o|s\u0303, r, o\u0303)), or commonsense relation (P(r|s, r\u0303, o)), respectively. We then train the model to predict the masked spans autoregressively. The diverse masking strategies provide more diverse training signals compared to random masking, thus enabling the model to better align the surface form of human language and the underlying commonsense knowledge.\nIn addition, unlike the conventional practice in masked span infilling objective that randomly masks text spans with the same probability, we pro-\nCommonsense Relation Prediction\nInput: he plans to cook a meal for himself, what is needed for that?\nOptions: (object) Subject: Relation"
        },
        {
            "heading": "A. to buy ingredients he plans to cook a meal for himself. xNeed",
            "text": ""
        },
        {
            "heading": "B. to eat food he plans to cook a meal for himself. xWant",
            "text": ""
        },
        {
            "heading": "C. to get prepared I don\u2019t want to fail the next exam xNeed",
            "text": ""
        },
        {
            "heading": "D. to find a job she wants to save money for a car xNeed Output: A",
            "text": "Figure 3: Illustration of the commonsense relation prediction objective. We train the pre-trained model to predict the correct commonsense inference given the subject and relation from three distractors generated with either different subjects or relations as inputs.\npose to mask text spans including concepts (tokens recognized as nouns or verbs by a Spacy POS tagger) with a higher probability so that the model will be trained to predict concepts more frequently compared to non-content words that are generally not related to commonsense reasoning.\nCommonsense Relation Prediction While the commonsense text infilling objective encourages the pre-trained model to align natural texts and their commonsense inferences, it is always trained on valid commonsense tuples. This can be suboptimal because we also want the model to be capable of discriminating invalid commonsense inferences, which is important for many commonsenserelated downstream tasks.\nTo this end, we introduce a commonsense relation prediction task that trains the model to distinguish the correct commonsense inference corresponding to the input sentence and the commonsense relation from distractors. To be specific, the commonsense relation prediction objective is formulated as a multi-choice QA problem with an input sentence as the context, a commonsense relation as the question, and a set of four commonsense inferences as options. The set of options consists of one correct commonsense inference, which is generated by the neural commonsense model with the input sentence and commonsense relation as input, and three carefully curated distractors (i.e., negative examples) generated by the same neural commonsense knowledge model with different inputs. As illustrated in Figure 3, among the three distractors, one is generated with an input composed of the same sentence and a different commonsense relation, and another two are generated with an input composed of different sentences\nwith the same commonsense relation. In this way, the model learns to align the natural texts with valid commonsense knowledge while also distinguishing commonsense inferences that do not make sense. Moreover, this objective is formulated as a multi-choice QA task that closely resembles several downstream commonsense-related tasks such as CommonsenseQA and SOCIALIQA, thus enabling easier transfer especially when labeled training examples are scarce."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Experimental Settings",
            "text": "Models In our experiments we apply commonsense knowledge transfer to refine T5 (Raffel et al., 2019), a popular model pre-trained with the text infilling objective. We experiment with both T5base and T5-large, which consist of 220 million and 774 million parameters respectively, as the target model in the commonsense knowledge transfer framework. We do not experiment with extremely large models like T5-11B because of the resource constraints and the fact that these models are hard to deploy in real-world applications. We use COMET-ATOMIC2020, a state-of-the-art neural commonsense knowledge model that can generate accurate, representative knowledge for new, unseen entities and events, as the source model. It is initialized with BART and continually trained on ATOMIC2020 (Hwang et al., 2021), a new general purpose commonsense knowledge graph. Data We randomly sample a subset consisting of 10 million sentences from the English Wikipedia and the BookCorpus (Zhu et al., 2015), which is used for pre-training BERT and its variants. We se-\nlect a set of representative commonsense relations including intent, reason, effect, need, want, and react from relations used to train COMET-ATOMIC2020. For each sentence, we randomly sample two relations and retrieve the corresponding commonsense explanation from COMET2020. We randomly select one relation-explanation pair to form the input example and leave another as the distractor for the commonsense relation prediction objective.\nTraining We refine the pre-trained models on the self-supervised examples constructed with the sampled 10 million sentences for 100k steps with a batch size of 1024, a maximum sequence length of 256, and a learning rate of 5e-5/2e-5 for base-size and large-size models respectively with a linear warm-up for the first 8,000 updates. After knowledge transfer, we fine-tune the models on downstream tasks by formulating the tasks into text-totext problems. Pre-training and fine-tuning details are included in the Appendix.\nEvaluation We evaluate the continual pre-trained models on downstream tasks that require commonsense reasoning including CommonsenseQA (Talmor et al., 2018), OpenbookQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), aNLI (Bhagavatula et al., 2020), COPA (Roemmele et al., 2011), and SOCAILIQA (Sap et al., 2019b) In addition to the conventional fully supervised setting, we also test our approach in the few-shot setting by varying the percentage of labeled examples from the original training set used for fine-tuning. The idea is that limited labeled examples can only help the model understand the task but are insufficient for the model to acquire enough commonsense knowledge to solve the task. As such, it requires the\nmodel to store enough commonsense knowledge in its parameters to succeed in the few-shot setting. For both the settings, we report the results on the official development set and tune the hyperparameters based on the models\u2019 performance on an in-house split dev set. We report the mean and variance of 3 individual runs with different random seeds because most datasets are relatively small, which makes the variance in results non-negligible.\nBaselines We compare our approach with methods that continually train a pre-trained model with different objectives. We divide the baselines into two categories based on the source of their supervision. The first category includes methods that only exploit general text corpus, including (1) T5 + TI that continually pre-trains the public checkpoint of T5 with the same text infilling objective for more steps, (2) T5 + SSM that also continual pretrains T5 with the text infilling objective, but use salient span masking (Roberts et al., 2020) instead of random masking for data construction, (3) (T5 + KD) that uses sequence-level knowledge distillation (Kim and Rush, 2016) for knowledge transfer, where the student model is trained with the teacher output (i.e., P(o|s, r)), and (4) CALM (Zhou et al., 2021) that uses novel self-supervised objectives to construct concept-centric self-supervision from general text corpus. The second category instead exploits CSKG, including (5) T5 + CSKG (TI) train T5 with the text infilling objective on tuples in a CSKG, and (6) T5 + CSKG (Rule) (Li et al., 2019) that uses manually defined rules to construct training examples from a CSKG. We also include a COMET baseline where we directly fine-tune the pre-trained COMET-ATOMIC2020 model for the downstream tasks to verify the necessity of commonsense knowledge transfer, and a CKT w/ GPT2 baseline where the commonsense inferences are generated by a pre-trained GPT-2 large model to verify whether the gain comes from transferring the commonsense knowledge from COMET, or simply from data augmentation from another generative model. For a fair comparison, we use the same data\nand training steps compared to our approach for baselines from the first category and use ATOMIC2020, on which the teacher model in our framework is pre-train on, as the commonsense knowledge graph. For reference, we also include some popular knowledge-enhanced pre-trained models including ERNIE (Zhang et al., 2019) and KnowBERT (Peters et al., 2019)."
        },
        {
            "heading": "3.2 Fully-supervised Results",
            "text": "We first present results in the fully-supervised setting. Results on base-size models are presented in Table 1. We can see that our approach yields significant improvement compared to the T5 baseline (up to 4 absolute scores) and consistently outperform CALM, the state-of-the-art method of injecting commonsense knowledge into PTLMs.\nIn addition, we observe that simply using continual training with the original text-infilling objective or its variant with salient span masking only marginally improves the performance. Surprisingly, training with text infilling on a commonsense knowledge graph leads to degraded performance compared to the T5 baseline. We suspect this is because the commonsense tuples in commonsense knowledge graphs are generally too short and simple, making the pre-trained model unable to reason within relatively long contexts which is crucial for most downstream tasks. Moreover, we find that continually pre-training with training data constructed with commonsense tuples in a commonsense knowledge graph following manually de-\nsigned rules leads to improvements in certain tasks. However, the improvement is inconsistent across different tasks and it even hurts the performance on certain tasks, which may be because the rules for constructing training data are tailored for certain tasks like CSQA. The inferior performance of using commonsense knowledge graphs as data sources also confirms the need of using natural text corpus during continual pre-training for better adapting to diverse downstream tasks. We also find directly applying sequence-level KD and training the student to mimic the teacher on the commonsense tuple generation task fails to improve the performance because the task is not general enough and thus cannot transfer to diverse downstream tasks well. Moreover, directly fine-tuning COMET or using GPT-2 as the commonsense knowledge source results in very poor performance. This confirms the necessity of commonsense knowledge transfer and shows that it is actually transferring commonsense knowledge instead of simple text augmentation.\nTo further confirm the effectiveness of commonsense knowledge transfer, we apply it to T5-large and compare it to competitive baselines in the basesize experiments. The results are presented in Table 2. We can see that our approach consistently outperforms T5-large and CALM-large. This suggests that our approach can successfully generalize to large-size pre-trained models."
        },
        {
            "heading": "3.3 Few-shot Results",
            "text": "Injecting commonsense knowledge into pre-trained models is important because it enables the model to reason and generalize to unseen examples while observing only a few labeled examples. To this end, we fine-tune the compared models with different fractions of labeled training data to investigate the transition of the behavior of our model and baselines from the low-resource regime to the fullysupervised setting (Fig. 4). We observe that the performance improvement of our approach compared to the baselines is more significant in the low-resource regime. This shows that commonsense knowledge transfer can successfully transfer commonsense knowledge into pre-trained models so that they can generalize well while seeing only a small part of training data. This may also help the model reduce the risk/tendency of fitting the spurious correlations in the annotated datasets and thus generalize better."
        },
        {
            "heading": "3.4 Analysis",
            "text": "To better understand the proposed commonsense knowledge transfer framework and the role of its different components, we conduct an ablation study about the impact of different proposed objectives, the impact of multi-tasking the commonsenserelated self-supervised objective versus sequentially training, and the impact of the size of natural text corpus used for transfer (see Table 3).\nImpact of Objectives We find that both the proposed objectives contribute to the performance improvement of our approach. The commonsense text infilling objective is shown to be more critical than the commonsense relation prediction task. We suspect this is because commonsense text infilling resembles the vanilla text infilling objective with which the T5 models are pre-trained, thus preventing the model from catastrophic forgetting. In addition, all four masking strategies are beneficial, and their contribution varies for different downstream tasks. This confirms the necessity of a diverse masking scheme. Moreover, our strategy for constructing distractors outperforms the random counterpart, demonstrating the necessity of hard negative examples for the commonsense relation prediction task. Multi-task versus Sequential Transfer As for the training order between the two objectives, we find that starting from the commonsense text infilling task and then switching to the commonsense relation prediction task performs similarly with our multi-tasking strategy while significantly outperforming its counterpart training with the reverse direction. We think this is because the commonsense text infilling objective resembles the original pre-training while the commonsense relation prediction is more similar to downstream tasks. We opt for the multi-tasking strategy for simplicity. Impact of Corpus Size We find that commonsense knowledge transfer significantly outperforms both the T5 baseline and the competitive CALM method with only 10 percent of the full data used for distillation. Nevertheless, the performance improvement also confirms that our approach can benefit from the accessibility of large-scale natural texts. For base-size models, the performance improvements seem to saturate after 10 million sentence pairs. However, we anticipate that larger-size models may still benefit from a larger amount of data, and leave this for future work."
        },
        {
            "heading": "4 Related Work",
            "text": ""
        },
        {
            "heading": "Knowledge-augmented Pre-trained Models A",
            "text": "number of recent works have examined the problem of incorporating world knowledge with the pretrained models. A number of works use an external knowledge base to incorporate entity knowledge with pre-trained models (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; Liu et al., 2020). However, these approaches require specialized re-\nsources like knowledge bases which are non-trivial to seek, thus limiting the domain they can be applied to. Xiong et al. (2020) proposed a novel entity replacement detection objective that incorporates Wikipedia to encode world knowledge into a BERTlike pre-trained model. He et al. (2020) proposed a generative and discriminative framework that pretrains the model to complete and correct knowledge spans. The aforementioned approaches generally focus on factual knowledge of entities while our work mainly focuses on commonsense knowledge. Commonsense Reasoning for NLP Several recent studies (Talmor et al., 2018; Sap et al., 2019b; Zhou et al., 2020b; Lin et al., 2020; Xu et al., 2021) evaluate the performance of several pre-trained language models on tasks that require commonsense reasoning and find that it is still very hard for pre-trained language models to match or exceed human-level performance. Therefore, approaches to improve the commonsense reasoning ability of pre-trained language models has attracted much attention. These approaches can be divided into two categories. The first category focuses on incorporating an external commonsense knowledge graph for commonsense reasoning. For example, Lin et al. (2019), Cui and Chen (2021), and Liu et al. (2021) propose to exploit structured symbolic commonsense knowledge graphs to perform commonsense reasoning. The second one instead attempts to inject commonsense knowledge into the parameters of pre-trained models. For example, Ye et al. (2019); Li et al. (2019) proposed to use manually designed rules to construct commonsense related training examples from commonsense knowledge graphs. Zhou et al. (2021) instead only relies on general text corpus and proposed two concept-centric self-supervised objectives to refine pre-trained models with commonsense knowledge."
        },
        {
            "heading": "5 Conclusion",
            "text": "We introduce commonsense knowledge transfer, a framework to transfer the commonsense knowledge stored in a neural commonsense knowledge model to a general-purpose pre-trained model. Our method extracts commonsense knowledge from the source model to construct self-supervised training data for the target model. Empirical results show that our approach consistently outperforms previous methods for improving the commonsense reasoning ability of pre-trained models that exploit either symbolic knowledge graphs or texts alone."
        },
        {
            "heading": "Limitations",
            "text": "In our experiments, we use T5-base and T5-large models as the target model since they are widelyused, representative pre-trained seq2seq models and use COMET-ATOMIC2020 as the commonsense knowledge source. However, there are other pretrained seq2seq models such as BART, and neural commonsense models such as COMET that we did not experiment with. Moreover, we only experimented with 10 million randomly sampled sentences from the English Wiki and BookCorpus datasets. It would be interesting to investigate whether continually pre-training with a larger scale dataset can further improve the performance."
        },
        {
            "heading": "Ethical Considerations",
            "text": "Our work focuses on improving the commonsense reasoning ability of pre-trained language models. It probably does not introduce extra ethical concerns. However, in commonsense knowledge extraction, the neural commonsense knowledge model may generate unexpected (e.g., biased) commonsense inferences, and training with these inferences may lead to additional bias in the pre-trained model. Nevertheless, all pre-trained language models contain bias and should be examined."
        },
        {
            "heading": "A Pre-training and Fine-tuning Details",
            "text": ""
        },
        {
            "heading": "A.1 Pre-Training Details",
            "text": "We implement our models using Pytorchlightning (Falcon, 2019) and Hugginface\u2019s Pytorch Transformers (Wolf et al., 2019). For pre-training phase, we use the AdamW optimizer with maximum sequence length 256, train batch size 8, gradient accumulation 8, warmup steps 8000, weight decay 0.01 and adam epsilon 1e-6. We train the models with 8 V100 GPUs and FP32 precision. The model is pre-trained for 10 epochs. We searched for the best learning rate for our model out of [5e-6, 2e-5, 5e-5, 1e-4]."
        },
        {
            "heading": "A.2 Fine-Tuning Details",
            "text": "For fine-tuning, we use 4 V100 GPUs and use FP32. For all tasks, we use the AdamW optimizer with learning rate from [1e-5, 2e-5, 5e-5, 1e-4, 2e-4], maximum sequence length 256, batch size from [4, 8, 16, 32]. For all tasks, we use a warmup fraction of 0.01, and max epoch of 20."
        },
        {
            "heading": "B Additional Analysis",
            "text": ""
        },
        {
            "heading": "B.1 Qualitative Analysis",
            "text": "To better understand the proposed method, we present a case study in Figure 5. We can see that both the objectives introduced in the CALM model and the salient span masking (SSM) strategy fail to exploit the underlying commonsense rationale beyond the surface form of texts while our approach directly aligns texts with the corresponding commonsense inferences with different commonsense relations. That explains why commonsense knowledge transfer can effectively improve a pre-trained model\u2019s performance on downstream tasks requiring commonsense reasoning ability."
        },
        {
            "heading": "B.2 Experimental Results on GLUE",
            "text": "To verify that commonsense knowledge transfer is suitable for general-purpose pre-trained models, we fine-tune our model on the GLUE benchmark (Wang et al., 2019). Specifically, we test on MRPC (Dolan and Brockett, 2005), QQP1 and STS-B (Conneau and Kiela, 2018) for Paraphrase Similarity Matching; SST-2 (Socher et al., 2013) for Sentiment Classification; MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016) and RTE (Wang et al., 2019) for the Natural Language\n1https://www.quora.com/q/quoradata/ First-Quora-Dataset-Release-Question-Pairs\nInference; CoLA (Warstadt et al., 2019) for Linguistic Acceptability.\nThe results are shown on Table 4, we can see that after commonsense knowledge transfer, the resulting model\u2019s general natural language understanding ability is comparable with the original T5-base model. This shows that our approach does not affect the model\u2019s general transfer ability and thus can be applied to general-purpose language models."
        },
        {
            "heading": "B.3 Experiments with BART",
            "text": "To demonstrate the versatility of commonsense knowledge transfer for different backbones, we conduct additional experiments using BART as the backbone model. The results are shown in Table 5. We can see that commonsense knowledge trans-\nfer also consistently improves the BART model, demonstrating the versatility of our approach."
        },
        {
            "heading": "B.4 Experiments on CommonGEN",
            "text": "We also experiment on the CommonGEN dataset, a generative commonsense reasoning dataset where the model is required to take several keywords as inputs and output a sentence that makes sense. The results are shown in Table 6. We can see that our approach performs similarly with the CALM model, which includes the CommonGEN task objective as one of the pre-training tasks.\nB.5 Impact of Pre-training Data Size We also conduct experiments to investigate the sample-efficiency of commonsense knowledge transfer. We present the trend of performance improvement in Figure 6. We can see that our method achieves significant performance improvement upon the T5 baseline with only 10% of the total training data, which confirms the sampleefficiency of commonsense knowledge transfer."
        }
    ],
    "title": "Commonsense Knowledge Transfer for Pre-trained Language Models",
    "year": 2023
}