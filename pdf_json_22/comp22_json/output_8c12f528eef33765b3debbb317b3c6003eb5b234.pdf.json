{
    "abstractText": "In this paper, we derive variational formulas for the asymptotic exponents of the concentration and isoperimetric functions in the product Polish probability space. These formulas are expressed in terms of relative entropies (which are from information theory) and optimal transport cost functionals (which are from optimal transport theory). Our results verify an intimate connection among information theory, optimal transport, and concentration of measure or isoperimetric inequalities. In the concentration regime, the corresponding variational formula is in fact a dimension-free bound on the exponent of the concentration function. The proofs in this paper are based on information-theoretic and optimal transport techniques. Our results generalize Alon, Boppana, and Spencer\u2019s in [4], Gozlan and L\u00e9onard\u2019s [14], and Ahlswede and Zhang\u2019s in [3].",
    "authors": [
        {
            "affiliations": [],
            "name": "Lei Yu"
        }
    ],
    "id": "SP:db1337f32548c8404978c4776b22c72e03ba796a",
    "references": [
        {
            "authors": [
                "R. Ahlswede",
                "P. G\u00e1cs",
                "J. K\u00f6rner"
            ],
            "title": "Bounds on conditional probabilities with applications in multi-user communication",
            "venue": "Z. Wahrscheinlichkeitstheorie verw. Gebiete, 34(3):157\u2013177",
            "year": 1976
        },
        {
            "authors": [
                "R. Ahlswede",
                "E.-H. Yang",
                "Z. Zhang"
            ],
            "title": "Identification via compressed data",
            "venue": "IEEE Transactions on Information Theory, 43(1)",
            "year": 1997
        },
        {
            "authors": [
                "R. Ahlswede",
                "Z. Zhang"
            ],
            "title": "Asymptotical isoperimetric problem",
            "venue": "Proceedings of the 1999 IEEE Information Theory and Communications Workshop, pages 85\u201387. IEEE",
            "year": 1999
        },
        {
            "authors": [
                "N. Alon",
                "R. Boppana",
                "J. Spencer"
            ],
            "title": "An asymptotic isoperimetric inequality",
            "venue": "Geometric & Functional Analysis, 8(3):411\u2013436",
            "year": 1998
        },
        {
            "authors": [
                "D. Berend",
                "P. Harremo\u00ebs",
                "A. Kontorovich"
            ],
            "title": "Minimum KL-divergence on complements of L1 balls",
            "venue": "IEEE Transactions on Information Theory, 60(6):3172\u20133177",
            "year": 2014
        },
        {
            "authors": [
                "D.P. Bertsekas",
                "S.E. Shreve"
            ],
            "title": "Stochastic optimal control: the discrete-time case",
            "venue": "volume 5. Athena Scientific",
            "year": 1996
        },
        {
            "authors": [
                "T.M. Cover",
                "J.A. Thomas"
            ],
            "title": "Elements of Information Theory",
            "venue": "Wiley-Interscience, 2nd edition",
            "year": 2006
        },
        {
            "authors": [
                "I. Csisz\u00e1r"
            ],
            "title": "I-divergence geometry of probability distributions and minimization problems",
            "venue": "The Annals of Probability, pages 146\u2013158",
            "year": 1975
        },
        {
            "authors": [
                "I. Csisz\u00e1r",
                "J. K\u00f6rner"
            ],
            "title": "Information Theory: Coding Theorems for Discrete Memoryless Systems",
            "venue": "Cambridge University Press",
            "year": 2011
        },
        {
            "authors": [
                "A. Dembo"
            ],
            "title": "Information inequalities and concentration of measure",
            "venue": "The Annals of Probability, pages 927\u2013939",
            "year": 1997
        },
        {
            "authors": [
                "A. Dembo",
                "O. Zeitouni"
            ],
            "title": "Large Deviations Techniques and Applications",
            "venue": "Springer, 2nd edition",
            "year": 1998
        },
        {
            "authors": [
                "A.L. Gibbs",
                "F.E. Su"
            ],
            "title": "On choosing and bounding probability metrics",
            "venue": "International statistical review, 70(3):419\u2013435",
            "year": 2002
        },
        {
            "authors": [
                "N. Gozlan"
            ],
            "title": "A characterization of dimension free concentration in terms of transportation inequalities",
            "venue": "The Annals of Probability, 37(6):2480\u20132498",
            "year": 2009
        },
        {
            "authors": [
                "N. Gozlan",
                "C. L\u00e9onard"
            ],
            "title": "A large deviation approach to some transportation cost inequalities",
            "venue": "Probability Theory and Related Fields, 139(1):235\u2013283",
            "year": 2007
        },
        {
            "authors": [
                "N. Gozlan",
                "C. L\u00e9onard"
            ],
            "title": "Transport inequalities",
            "venue": "a survey. Markov Processes and Related Fields, 16:635\u2013736",
            "year": 2010
        },
        {
            "authors": [
                "M. Hayashi",
                "V.Y.F. Tan"
            ],
            "title": "Minimum rates of approximate sufficient statistics",
            "venue": "IEEE Transactions on Information Theory, 64(2):875\u2013888",
            "year": 2017
        },
        {
            "authors": [
                "M. Ledoux"
            ],
            "title": "The concentration of measure phenomenon",
            "venue": "Number 89. American Mathematical Soc.",
            "year": 2001
        },
        {
            "authors": [
                "K. Marton"
            ],
            "title": "A simple proof of the blowing-up lemma",
            "venue": "IEEE Transactions on Information Theory, 32(3):445\u2013446",
            "year": 1986
        },
        {
            "authors": [
                "K. Marton"
            ],
            "title": "Bounding d\u0304-distance by informational divergence: a method to prove measure concentration",
            "venue": "The Annals of Probability, 24(2):857\u2013866",
            "year": 1996
        },
        {
            "authors": [
                "L. Nirenberg"
            ],
            "title": "Topics in nonlinear functional analysis",
            "venue": "volume 6. American Mathematical Soc.",
            "year": 1974
        },
        {
            "authors": [
                "M. Raginsky",
                "I. Sason"
            ],
            "title": "Concentration of Measure Inequalities in Information Theory",
            "venue": "Communications and Coding, volume 10 of Foundations and Trends in Communications and Information Theory. Now Publishers Inc",
            "year": 2013
        },
        {
            "authors": [
                "M. Talagrand"
            ],
            "title": "Concentration of measure and isoperimetric inequalities in product spaces",
            "venue": "Publications Math\u00e9matiques de l\u2019Institut des Hautes Etudes Scientifiques, 81(1):73\u2013205",
            "year": 1995
        },
        {
            "authors": [
                "M. Talagrand"
            ],
            "title": "Transportation cost for gaussian and other product measures",
            "venue": "Geometric & Functional Analysis, 6(3):587\u2013600",
            "year": 1996
        },
        {
            "authors": [
                "I. Vajda"
            ],
            "title": "Note on discrimination information and variation (corresp.)",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 1970
        },
        {
            "authors": [
                "T. van Erven",
                "P. Harremo\u00ebs"
            ],
            "title": "R\u00e9nyi divergence and Kullback-Leibler divergence",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2014
        },
        {
            "authors": [
                "C. Villani"
            ],
            "title": "Topics in optimal transportation",
            "venue": "Number 58. American Mathematical Soc.",
            "year": 2003
        },
        {
            "authors": [
                "C. Villani"
            ],
            "title": "Optimal transport: old and new",
            "venue": "volume 338. Springer Science & Business Media",
            "year": 2008
        },
        {
            "authors": [
                "L. Yu"
            ],
            "title": "Asymptotics of Strassen\u2019s optimal transport problem",
            "venue": "To appear in Annales de l\u2019Institut Henri Poincar\u00e9 (B) Probabilit\u00e9s et Statistiques. Available at arXiv preprint arXiv:1912.02051",
            "year": 2022
        },
        {
            "authors": [
                "L. Yu",
                "V.Y.F. Tan"
            ],
            "title": "On exact and\u221e-R\u00e9nyi common information",
            "venue": "IEEE Transactions on Information Theory, 66(6):3366\u20133406",
            "year": 2020
        },
        {
            "authors": [
                "C. Zalinescu"
            ],
            "title": "Convex analysis in general vector spaces",
            "venue": "World scientific",
            "year": 2002
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n20 5.\n07 59\n6v 2\n[ m\nat h.\nPR ]\n1 6\nSe p\nIndex terms\u2014 Concentration of measure, isoperimetric inequality, optimal transport, informationtheoretic method, exponent"
        },
        {
            "heading": "1 Introduction",
            "text": "Concentration of measure in a probability metric space refers to a phenomenon that a slight enlargement of any measurable set of not small probability will always have large probability. In the language of functional analysis, it is equivalent to a phenomenon that the value of any Lipschitz function is concentrated around its medians. The concentration of measure phenomenon was pushed forward in the early 1970s by V. Milman in the study of the asymptotic geometry of Banach spaces. It was then studied in depth by V. Milman and many other authors including Gromov, Maurey, Pisier, Schechtman, Talagrand, Ledoux, etc. In particular, Talagrand [22] studied the concentration of measure in product spaces equipped with product probability measures, and derived a variety of concentration of measure inequalities for these spaces. In information theory, concentration of measure is known as the blowing-up lemma [1, 18], which was employed by G\u00e1cs, Ahlswede, and K\u00f6rner to prove the strong converses of two coding problems in information theory.\nIt is worth mentioning that Marton is the first to introduce information-theoretic techniques, especially transportation-entropy inequalities, in the study of the concentration of measure [18], which yields an elegant and short proof for this phenomenon. By developing a new transportation-entropy inequality, Talagrand extended her idea to the case of Gaussian measure and Euclidean metric [23]. Since then, such a textbook beautiful argument became popular and emerged in many books, e.g., [17, 21, 26]. By replacing the \u201clinear\u201d transportation-entropy inequality in Marton\u2019s argument with the \u201cnonlinear\u201d version, Gozlan and L\u00e9onard obtained the sharp dimension-free bound on the concentration function [14]. In other words, their bound corresponds to the asymptotic exponent of the concentration function. Furthermore, Gozlan [13] also used Marton\u2019s argument to prove the equivalence between the Gaussian bound on the concentration function and\n\u2217L. Yu is with the School of Statistics and Data Science, LPMC, KLMDASR, and LEBPS, Nankai University, Tianjin 300071, China (e-mail: leiyu@nankai.edu.cn). This work was supported by the NSFC grant 62101286 and the Fundamental Research Funds for the Central Universities of China (Nankai University).\nTalagrand\u2019s transportation-entropy inequality. Dembo [10] provided a new kind of transportation-entropy inequalities, and used them to recover several results of Talagrand [22].\nAhlswede and Zhang [3] focused on the isoperimetric regime of the concentration problem, in which they assumed the set to be small enough such that its enlargement is small as well. In fact, in this regime, the problem turns into an isoperimetric problem where the difference between the enlargement and the original set is regarded as the \u201cboundary\u201d of the set. They characterized the asymptotic exponents for this problem by using information-theoretic methods.\nIn this paper, we investigated the concentration (or isoperimetric) problem in the product Polish space. Specifically, we minimize the probability of the t-enlargement (or t-neighborhood) At of a set A under the condition that the probability of A is given. Here, different from the common setting in concentration of measure, the probability of A is not necessarily restricted to be around 1/2. The probability of A could be small or large. We use Marton\u2019s idea to derive a dimension-free bound (expressed in the variational form) for this problem in the concentration regime, which is exponentially sharp when the probability of A is not too large (not close to 1) and the probability of At is close to 1. This bound sharpens Gozlan and L\u00e9onard\u2019s bound [14] especially for the setting in which the probability of A is exponentially small. It also sharpens the well known Talagrand\u2019s concentration inequality in [22]. Furthermore, based on Ahlswede and Zhang the inherently typical subset lemma [2, 3], we also characterize the asymptotic exponent for the problem in the isoperimetric regime in which the probabilities of A and At exponentially vanish.\nWe now introduce the mathematical formulation. Let X and Y be Polish spaces. Let \u03a3(X ) and \u03a3(Y) be respectively the Borel \u03c3-algebras on X and Y that are generated by the topologies on X and Y. Let P(X ) and P(Y) denote the sets of probability measures (or distributions) on X and Y respectively. Let PX \u2208 P(X ) and PY \u2208 P(Y). In other words, PX and PX are respectively the distributions of two random variables X and Y . Let c : X \u00d7 Y \u2192 [0,+\u221e) be lower semi-continuous, which is called a cost function. Denote Xn as the n-fold product space of X . For the product space Xn \u00d7 Yn and given c, we consider an additive cost function cn on Xn \u00d7 Yn given by\ncn(x n, yn) :=\nn \u2211\ni=1\nc(xi, yi) for (x n, yn) \u2208 Xn \u00d7 Yn,\nwhere c given above is independent of n. Obviously, cn is lower semi-continuous since c is lower semicontinuous.\nFor a set A \u2286 Xn, denote its t-enlargement under c as\nAt := \u22c3\nxn\u2208A {yn \u2208 Yn : cn(xn, yn) \u2264 t}.\nTo address the measurability of At, we assume that either of the following two conditions holds throughout this paper.\n1. For lower semi-continuous c, we restrict A to a closed set.\n2. If X and Y are the same Polish space and c = dp, where p > 0 and d is a metric on this Polish space, then A can be any Borel set.\nFor the first case, since Xn and Yn are Polish, for closed A, the set At is an analytic set and hence, universally measurable. If we extend P\u2297nY to the collection of analytic sets, then P \u2297n Y (A\nt) is well defined. Hence, for this case, we by default adopt this extension to avoid the measurability problem. For the second case, for any Borel set A, At is always Borel (since it is countable intersections of Borel sets \u22c3\nxn\u2208A{yn \u2208 Yn : cn(xn, yn) < t+ 1k}, k = 1, 2, ...).\nDefine the isoperimetric function as for a \u2208 [0, 1], t \u2265 0,\n\u0393(n)(a, t) := inf A:P\u2297nX (A)\u2265a P\u2297nY (A t), (1)\nwhere the set A is assumed to satisfy either of two conditions above. We call (a, t) 7\u2192 1 \u2212 \u0393(n)(a, t) as the concentration function, which is a generalization of the usual concentration function t 7\u2192 1\u2212\u0393(n)(12 , t) in the theory of concentration of measure. Define the isoperimetric and concentration exponents respectively as1 for \u03b1 \u2265 0,\nE (n) 0 (\u03b1, \u03c4) := \u2212\n1 n log \u0393(n)(e\u2212n\u03b1, n\u03c4)\nE (n) 1 (\u03b1, \u03c4) := \u2212\n1 n log ( 1\u2212 \u0393(n)(e\u2212n\u03b1, n\u03c4) ) . (2)\nIn fact,\n\u0393(n)(e\u2212n\u03b1, n\u03c4) = e\u2212nE (n) 0 (\u03b1,\u03c4) = 1\u2212 e\u2212nE(n)1 (\u03b1,\u03c4).\nIn this paper, our results involve the optimal transport (OT) cost functional, which is introduced now. The coupling set of (PX , PY ) is defined as\n\u03a0(PX , PY ) :=\n{\nPXY \u2208 P(X \u00d7 Y) : PXY (A\u00d7 Y) = PX(A), \u2200A \u2208 \u03a3(X ), PXY (X \u00d7B) = PY (B), \u2200B \u2208 \u03a3(Y)\n}\n.\nDistributions in \u03a0(PX , PY ) are termed couplings of (PX , PY ). The OT cost between PX and PY is defined as2\nC(PX , PY ) := min PXY \u2208\u03a0(PX ,PY ) E(X,Y )\u223cPXY [c(X,Y )]. (3)\nAny PXY \u2208 \u03a0(PX , PY ) attaining C(PX , PY ) is called an OT plan. The minimization problem in (3) is called the Monge\u2013Kantorovich\u2019s OT problem [26]. The functional (PX , PY ) \u2208 P(X )\u00d7P(Y) 7\u2192 C(PX , PY ) \u2208 [0,+\u221e) is called the OT (cost) functional. If X and Y are the same Polish space and c = dp, where p \u2265 1 and d is a metric on this Polish space, then Wp(PX , PY ) := (C(PX , PY ))1/p is the so-called p-th Wasserstein distance between PX and PY . For the n-dimensional case, Wp(PXn , PY n) := (C(PXn , PY n)) 1/p with cn(x n, yn) = \u2211n i=1 d\np(xi, yi) is the p-th Wasserstein distance between PXn and PY n for the product metric dn(x n, yn) = cn(x n, yn)1/p where p \u2265 1.\nFurthermore, for another distribution PW on a Polish space W , the conditional coupling set of Markov kernels (or transition probabilities) PX|W and PY |W is defined as\n\u03a0(PX|W , PY |W ) :=\n{\nPXY |W \u2208 P(X \u00d7 Y|W) : PXY |W=w \u2208 \u03a0(PX|W=w , PY |W=w), \u2200w \u2208 W\n}\n,\nwhere P(X \u00d7Y|W) denotes the set of Markov kernels from W to X \u00d7Y. The conditional OT cost between Markov kernels (or transition probability measures) PX|W and PY |W given PW is defined as\nC(PX|W , PY |W |PW ) := min PXY |W\u2208\u03a0(PX|W ,PY |W ) E(X,Y,W )\u223cPXY |WPW [c(X,Y )], (4)\nwhere PXY |WPW denotes the joint probability measure induced by PW and PXY |W . The conditional OT cost can be alternatively expressed as3\nC(PX|W , PY |W |PW ) = EPW [C(PX|W , PY |W )].\nThe measurability of w 7\u2192 C(PX|W=w , PY |W=w) follows since it is the composition of measurable functions w 7\u2192 (PX|W=w, PY |W=w) and (QX , QY ) 7\u2192 C(QX , QY ).\n1Throughout this paper, the base of log is e. Our results are still true if the bases are chosen to other values, as long as the bases of the logarithm and exponential are the same.\n2The existence of the minimizers are well-known; see, e.g., [26, Theorem 1.3]. Furthermore, when the (joint) distribution of the random variables involved in an expectation is clear from context, we will omit the subscript \u201c(X, Y ) \u223c PXY \u201d.\n3In other words, the minimization in (4) can be taken in a pointwise way for each w. For optimal P (w) XY attaining C(PX|W=w, PY |W=w), the measurability of w 7\u2192 P (w)XY (B), B \u2208 \u03a3(X \u00d7 Y) can be addressed by measurable selection theorems, e.g., [6, Proposition 7.50]."
        },
        {
            "heading": "1.1 Dimension-Free Bound on Concentration Exponent",
            "text": "We first provide a dimension-free bound on the concentration exponent E (n) 1 . To this end, for two distributions P,Q defined on the same space, we denote D(Q\u2016P ) := \u222b\nlog(dQdP )dQ as the Kullback-Leibler (KL) divergence or relative entropy of Q from P . Denote the conditional version D(QX|W \u2016PX|W |QW ) = D(QX|WQW \u2016PX|WQW ).\nGiven PX , PY , and c, we define\n\u03c6(\u03b1, \u03c4) := inf QX\u2208P(X ),QY \u2208P(Y):\nD(QX\u2016PX )\u2264\u03b1,C(QX ,QY )>\u03c4\nD(QY \u2016PY ). (5)\nDenote \u03c6\u0306(\u03b1, \u03c4) as the lower convex envelope of \u03c6(\u03b1, \u03c4), which can be also expressed as\n\u03c6\u0306(\u03b1, \u03c4) = inf QX|W ,QY |W ,QW :\nD(QX|W \u2016PX |QW )\u2264\u03b1, C(QX|W ,QY |W |QW )>\u03c4\nD(QY |W \u2016PY |QW ), (6)\nwhere W is an auxiliary random variable defined on a Polish space. However, by Carath\u00e9odory\u2019s theorem, the alphabet size of QW can be restricted to be no larger than 4. In fact, the alphabet size can be further restricted to be no larger than 3, since it suffices to consider the boundary points of the convex hull of\n{( D(QX|W=w\u2016PX), D(QY |W=w\u2016PY ), C(QX|W=w, QY |W=w) )}\nw\u2208W .\nWe now provide a dimension-free bound4 for E (n) 1 . The proof is provided in Section 2.\nTheorem 1 (Dimension-Free Bound). It holds that for \u03b1, \u03c4 \u2265 0,\nE (n) 1 (\u03b1, \u03c4) \u2265 \u03c6\u0306(\u03b1, \u03c4). (7)\nThis bound is elegant in the following sense. This is a bound for the concentration exponent, but is expressed in terms of two fundamental quantities from other fields\u2014\u201crelative entropy\u201d which comes from information theory (or large deviations theory) and \u201coptimal transport cost\u201d which comes from the theory of optimal transport. Hence, this shows an intimate connection among concentration of measure, information theory, and optimal transport. Furthermore, in the following subsection, we will show that this bound is asymptotically tight under certain conditions.\nThe first bound like the one in (7) was first derived by Marton [18, 19], which was improved by Gozlan and L\u00e9onard [14]. However, their bounds are only valid for the case that c is a metric or the composition of a metric with a convex function, since the triangle inequality is used as a key step in their proofs. Our proof relies on the chain rule for OT costs, instead of the triangle inequality, leading to that our bound in (7) is better than Gozlan and L\u00e9onard\u2019s. When their bounds are valid and \u03b1 is close to zero, e.g., \u03b1 = 1n log 2 (i.e., a = 12 ), our bound and theirs do not differ too much, and as n\u2192\u221e, they coincide asymptotically. However, if \u03b1 is bounded away from zero, our bound is usually asymptotically tight but theirs are not. We next provide more details above Gozlan and L\u00e9onard\u2019s bound. To this end, we first introduce a mild assumption.\nAssumption 1: (Positivity Condition) \u03d5\u0306X(\u03c4) is strictly positive for all sufficiently small (equivalently for all) \u03c4 > 0, where\n\u03d5X(\u03c4) := inf QX :C(PX ,QX)>\u03c4\nD(QX\u2016PX). (8)\nAn equivalent statement of Assumption 1 is that given PX , if C(PX , QX) is bounded away from zero, then so is D(QX\u2016PX). In other words, given PX , convergence in information (i.e., D(QX\u2016PX)\u2192 0) implies convergence in optimal transport (i.e., C(PX , QX)\u2192 0).\n4The terminology \u201cdimension-free bound\u201d here indicates that the tuple of the normalized enlargement parameter \u03c4 , the (normalized) exponent of P\u2297n\nX (A), and the (normalized) exponent of 1\u2212P\u2297n Y (An\u03c4 ) verifies the same inequality for all n. This\nconcept is weaker than that in [13] and reduces to the latter when P\u2297n X (A) is fixed to be around 1/2, c is set to d2, and the bound on the exponent of 1\u2212P\u2297n Y\n(An\u03c4 ) in the inequality satisfied by the tuple is the quadratic form. Hence, the \u201cdimension-free bound\u201d here could be satisfied by a much larger class of probability metric spaces, than that in [13].\nThe function \u03d5X(\u03c4) characterizes the best possible tradeoff between the relative entropy and the OT cost. Assumption 1 holds if the following \u201clinear\u201d5 transportation-entropy inequality holds:\nWp(PX , QX) \u2264 \u221a \u03b3D(QX\u2016PX), \u2200QX (9) for some constant \u03b3. It is well-known that (9) is valid when \u03b3 = 2 and d is the Hamming metric or when \u03b3 = 2, d is the Euclidean metric, and PX is the standard Gaussian measure. The former is known as Csisz\u00e1r\u2013Kullback\u2013Pinsker (CKP) inequality, and the latter is known as Talagrand inequality.\nThe generalized inverse of \u03d5\u0306X is for \u03b1 \u2265 0, \u03d5\u0306\u2212X(\u03b1) := inf {\u03c4 \u2265 0 : \u03d5X(\u03c4) \u2265 \u03b1}\n= inf { \u03c4 \u2265 0 : D(QX|W \u2016PX |QW ) \u2265 \u03b1, \u2200QXW : C(PX , QX|W |QW ) > \u03c4 } = inf { \u03c4 \u2265 0 : C(PX , QX|W |QW ) \u2264 \u03c4, \u2200QXW : D(QY |W \u2016PX |QW ) < \u03b1 }\n= sup QXW :D(QX|W \u2016PX |QW )<\u03b1\nC(PX , QX|W |QW )\n= \u201c\u03baX(\u03b1),\nwhere \u03baX(\u03b1) := sup\nQX :D(QX\u2016PX )<\u03b1 C(PX , QX). (10)\nAssumption 1 can be equivalently stated as \u201c\u03baX(\u03b1)\u2192 0 as \u03b1\u2192 0. We now introduce Gozlan and L\u00e9onard\u2019s bound. We assume that X and Y are the same Polish space and PX = PY , the cost function c = d p, and a = 12 . For this case, C = W p p . Then, by using the triangle inequality, it can be obtained that\n\u03c6\u0306(\u03b1, \u03c4) \u2265 inf QX|W ,QY |W ,QW :\nWpp (PX ,QX|W |QW )\u2264\u201c\u03baX (\u03b1), Wpp (QX|W ,QY |W |QW )>\u03c4\nD(QY |W \u2016PY |QW )\n\u2265 inf QX|W ,QY |W ,QW :\nWp(PX ,QY |W |QW )> p \u221a \u03c4\u2212 p \u221a \u201c\u03baX (\u03b1)\nD(QY |W \u2016PY |QW )\n\u2265 \u03d5\u0306X(( p \u221a \u03c4 \u2212 p \u221a \u201c\u03baX(\u03b1)) p). (11)\nBy Theorem 1, the expression in (11) is a lower bound on E (n) 1 (\u03b1, \u03c4), which is just Gozlan and L\u00e9onard\u2019s bound [14]. Under Assumption 1, as \u03b1\u2192 0, this bound converges to \u03d5\u0306X(\u03c4). Moreover, under the assumption of the transportation-entropy inequality, it holds that \u03d5\u0306X(\u03c4) \u2264 \u03c4 2/p\n\u03b3 , which implies that (11) is further lower bounded by\n( p \u221a \u03c4\u221a \u03b3 \u2212\u221a\u03b1 )2 , (12)\nrecovering the Gaussian bound. Refer to [18, 19, 22, 23] for many such bounds. Note that the enlargement under c = dp is\nAn\u03c4 := \u22c3\nxn\u2208A {yn : p\n\u221a \u221a \u221a \u221a n \u2211\ni=1\ndp(xi, yi) \u2264 r},\nwhere r = p \u221a n\u03c4 . So, when expressed in r, the bound in (12) corresponds to 1 \u2212 \u0393(n)(e\u2212n\u03b1, n\u03c4) \u2265\nexp\n[\n\u2212n ( r p \u221a n \u221a \u03b3 \u2212\u221a\u03b1 )2 ] . For p = 1, it reduces to exp [ \u2212n ( r n \u221a \u03b3 \u2212 \u221a \u03b1 )2 ] , and for p = 2, it reduces\nto exp\n[\n\u2212n (\nr\u221a n\u03b3 \u2212\n\u221a \u03b1 )2 ] .\n5Rigorously speaking, Wp is bounded by a linear function of \u221a D, rather than a linear function of D."
        },
        {
            "heading": "1.2 Improvement of Talagrand\u2019s Concentration Inequality",
            "text": "The lower bound in (12) with \u03b3 = 2 holds for many pairs of (PX , c). Talagrand [22] showed that this bound (in fact, the one with a worse factor) for the Hamming metric and p = 1 is implied by the following inequality (see [22, p. 86]):\nP\u2297nX ((A t)c)1\u2212\u03bbP\u2297nX (A) \u03bb \u2264 e\u2212\u03bb(1\u2212\u03bb)2n\u03c42 , \u2200A, \u03bb \u2208 [0, 1]. This kind of inequalities are the so-called Talagrand\u2019s concentration inequalities. We now provide an improvement of this inequality which will be shown to be exponentially sharp.\nGiven PX , PY , and c, we define for \u03c4, \u03bb \u2265 0,\n\u03c6\u03bb(\u03c4) := inf QX ,QY :C(QX ,QY )>\u03c4\n(1\u2212 \u03bb)D(QY \u2016PY ) + \u03bbD(QX\u2016PX).\nDenote \u03c6\u0306\u03bb(\u03c4) as the lower convex envelope of \u03c6\u03bb(\u03c4).\nTheorem 2 (Improvement of Talagrand\u2019s Concentration Inequality). It holds that for any \u03c4, \u03bb \u2265 0, t = n\u03c4, and any A,\nP\u2297nY ((A t)c)1\u2212\u03bbP\u2297nX (A) \u03bb \u2264 e\u2212n\u03c6\u0306\u03bb(\u03c4). (13) Proof.\n\u2212 1 n log ( P\u2297nY ((A t)c)1\u2212\u03bbP\u2297nX (A) \u03bb ) \u2265 inf \u03b1\u22650 \u03bb\u03b1+ (1\u2212 \u03bb)\u03c6\u0306(\u03b1, \u03c4) (14)\n= inf \u03b1\u22650,QX|W ,QY |W ,QW : D(QX|W \u2016PX |QW )\u2264\u03b1, C(QX|W ,QY |W |QW )>\u03c4\n\u03bb\u03b1+ (1 \u2212 \u03bb)D(QY |W \u2016PY |QW )\n= inf QX|W ,QY |W ,QW :\nC(QX|W ,QY |W |QW )>\u03c4\n\u03bbD(QX|W \u2016PX |QW ) + (1\u2212 \u03bb)D(QY |W \u2016PY |QW )\n= \u03c6\u0306\u03bb(\u03c4).\nFrom the alternative expression of \u03c6\u0306\u03bb(\u03c4) in (14) and for each \u03b1, choosing \u03bb such that \u03bb\n\u03bb\u22121 is a subgradient\nof \u03b1\u2032 7\u2192 \u03c6\u0306(\u03b1\u2032, \u03c4) at \u03b1, we obtain the inequality in (7) from the inequality in (14) (or equivalently, the one in (13)). Hence, Theorem 2 is in fact equivalent to Theorem 1, and the asymptotic tightness of (7) is equivalent to the exponential sharpness of (13)."
        },
        {
            "heading": "1.3 Asymptotics of Concentration Exponent",
            "text": "We next prove that the bound in Theorem 1 is asymptotically tight under the following assumptions. Denote the L\u00e9vy\u2013Prokhorov metric on P(X ) as L(Q\u2032X , QX) = inf{\u03b4 > 0 : Q\u2032X(A) \u2264 QX(A\u03b4) + \u03b4, \u2200 closed A \u2286 X} with A\u03b4 := \u22c3\nx\u2208A{x\u2032 \u2208 X : d(x, x\u2032) < \u03b4}, which is compatible with the weak topology. Assumption 2: We assume that there is a function \u03b4(\u01eb) : (0,\u221e)\u2192 (0,\u221e) vanishing as \u01eb \u2193 0 such that\ninf Q\u2032X :L(QX ,Q \u2032 X )\u2264\u01eb\nC(Q\u2032X , QY ) \u2265 C(QX , QY )\u2212 \u03b4(\u01eb)\nholds for all (QX , QY ). In other words, infQ\u2032X :L(QX ,Q\u2032X )\u2264\u01ebC(Q \u2032 X , QY ) \u2192 C(QX , QY ) as \u01eb \u2193 0 uniformly for all (QX , QY ). Obviously, if the optimal transport cost functional (QX , QY ) 7\u2192 C(QX , QY ) is uniformly continuous under the L\u00e9vy\u2013Prokhorov metric (which was assumed by the author in [28] in studying the asymptotics of Strassen\u2019s optimal transport problem), then Assumption 2 holds. The following two examples satisfying Assumption 2 were provided in [28].\n1. (Countable Alphabet and Bounded Cost) X and Y are countable sets and c is bounded (i.e., supx,y c(x, y) < \u221e).\n2. (Wasserstein Distance Induced by a Bounded Metric)6 X = Y is a Polish space equipped with a bounded metric d, i.e., supx,y d(x, y) < \u221e. The cost function is set to c = dp for p \u2265 1, and hence, C =W pp .\nThe following theorem shows that the bound in Theorem 1 is asymptotically tight under Assumption 2. The proof is provided in Section 3. For a function f : [0,\u221e)k \u2192 [0,\u221e] with k \u2265 1, denote the effective domain of f as\ndomf = { xk \u2208 [0,\u221e)k : f(xk) <\u221e } .\nBy definition, domf\u0306 = dom \u201cf = domf if f is monotonous in each parameter (given others).\nTheorem 3 (Asymptotics of E (n) 1 ). The following hold.\n1. Under Assumption 2, for any (\u03b1, \u03c4) in the interior of dom\u03c6\u0306, it holds that limn\u2192\u221eE (n) 1 (\u03b1, \u03c4) = \u03c6\u0306(\u03b1, \u03c4).\n2. Let (an) be a sequence such that e \u2212o(n) \u2264 an \u2264 1 \u2212 e\u2212o(n) (and hence \u03b1n = \u2212 1n log an \u2192 0). Then,\nunder Assumption 2, it holds that for any \u03c4 in the interior of dom\u03d5\u0306,\nlim \u03b1\u21930 \u03c6\u0306(\u03b1, \u03c4) \u2264 lim inf n\u2192\u221e E (n) 1 (\u03b1n, \u03c4) \u2264 lim sup n\u2192\u221e E (n) 1 (\u03b1n, \u03c4) \u2264 \u03d5\u0306(\u03c4),\nwhere \u03d5(\u03c4) := \u03c6(0, \u03c4) = inf\nQY :C(PX ,QY )>\u03c4 D(QY \u2016PY ). (15)\nThe condition e\u2212o(n) \u2264 an \u2264 1\u2212 e\u2212o(n) implies that the sequence (an) does not approach 0 or 1 too fast, in the sense that the sequence (an) is sandwiched between a sequence that subexponentially approaches zero and a sequence that subexponentially approaches one.\nThe most interesting case might be the case that (X , PX) and (Y, PY ) are the same Polish probability space and the cost function c is set to dp with p \u2265 1 and d denoting a metric on this space. In other words, C = W pp . We now remove Assumption 2 from Theorem 3 and obtain the following theorem. Furthermore, to further simplify Statement 2 of Theorem 3, we need Assumption 1. The proof of the following theorem is provided in Section 4.\nTheorem 4 (Asymptotics of E (n) 1 for Wasserstein Distances). Assume that X = Y is a Polish space equipped with a metric d. Assume PX = PY and c = d p for p \u2265 1. Then, the following hold.\n1. For any (\u03b1, \u03c4) in the interior of dom\u03c6\u0306, it holds that limn\u2192\u221eE (n) 1 (\u03b1, \u03c4) = \u03c6\u0306(\u03b1, \u03c4).\n2. Let (an) be a sequence such that e \u2212o(n) \u2264 an \u2264 1\u2212 e\u2212o(n) (and hence \u03b1n = \u2212 1n log an \u2192 0). Then, for\nany \u03c4 in the interior of dom\u03d5\u0306X , it holds that\nlim sup n\u2192\u221e\nE (n) 1 (\u03b1n, \u03c4) \u2264 \u03d5\u0306X(\u03c4),\nand under Assumption 1,\nlim inf n\u2192\u221e\nE (n) 1 (\u03b1n, \u03c4) \u2265 \u03d5\u0306X(\u03c4),\nwhere \u03d5X is defined in (8). In particular, under Assumption 1, for any \u03c4 in the interior of dom\u03d5\u0306X ,\nlim n\u2192\u221e\nE (n) 1 (\u03b1n, \u03c4) = \u03d5\u0306X(\u03c4).\n6The second example satisfying Assumption 1 follows by the fact that the Wasserstein distance induced by a bounded metric d is equivalent to the L\u00e9vy\u2013Prokhorov metric in the sense that Lp+1 \u2264 W pp \u2264 Lp + dpmaxL where dmax = supx,x\u2032\u2208X d(x, x\u2032) is the diameter of X [12].\nStatement 2 in Theorem 4 is not new; see Proposition 4.6 and Theorem 5.4 in [15]. A consequence of these results is that given (\u03b1n) such that \u03b1n \u2192 0 as n \u2192 \u221e, lim infn\u2192\u221eE(n)1 (\u03b1n, \u03c4) > 0 holds for all 0 < \u03c4 < \u03c4max (i.e., exponential convergence) if and only if Assumption 1 holds.\nIn fact, for this setting of a = 12 , Alon, Boppana, and Spencer in [4] provided an alternative expression for limn\u2192\u221e E (n) 1 (\u03b1n, \u03c4) when X is finite (Assumption 1 is automatically satisfied for this case). The equivalence between their expression and \u03d5\u0306X(\u03c4) is proven in Section 1.5. Example (Hamming Metric): When X = Y and c is the Hamming metric, i.e., c(x, y) = 1{x 6=y}, by duality of Wasserstein metric, we have\n\u03d5(\u03c4) = inf QY :\u2016QY \u2212PX\u2016TV>\u03c4\nD(QY \u2016PY ),\nwhere \u2016QY \u2212PX\u2016TV := supAQY (A)\u2212PX(A) is the total variation (TV) distance. For the case of PX = PY , such a function was investigated in [5,24]. It was shown in [24] that \u03d5(\u03c4) \u2265 L(\u03c4) := minp\u2208[0,1\u2212\u03c4 ]D2(p\u2016p+ \u03c4) for any PX = PY . Here L is increasing in \u03c4 , and L(0) = 0, L(1) = +\u221e. Moreover, the lower bound L(\u03c4) is tight when PX = PY is atomless [5]. In other words, atomless distributions PX = PY are the worst in the sense that their concentration exponents are the smallest among all distributions. For the case of PX = PY , by using the inequality \u03d5(\u03c4) \u2265 L(\u03c4), we can obtain a bound on \u03c6(\u03b1, \u03c4) as follows: \u03c6(\u03b1, \u03c4) \u2265 L([\u03c4 \u2212 L\u22121(\u03b1)]+) where [x]+ := max{x, 0}. This bound is also tight for atomless distributions PX = PY and any \u03b1 \u2265 0, 0 \u2264 \u03c4 < 1. (Note that by definition, \u03c6(\u03b1, 1) =\u221e for all \u03b1 \u2265 0.) Moreover, when \u03c4 \u2191 1, this bound approaches L(1\u2212L\u22121(\u03b1)) which is finite for all \u03b1 > 0 and infinite for \u03b1 = 0. This indicates a significant difference between the case \u03b1 = 0 and the case \u03b1 > 0."
        },
        {
            "heading": "1.4 Asymptotics of Isoperimetric Exponent",
            "text": "We next derive the asymptotic expression of E (n) 0 (\u03b1, \u03c4). Define\n\u03c8(\u03b1, \u03c4) := sup QXW :D(QX|W \u2016PX |QW )\u2264\u03b1 inf QY |XW :E[c(X,Y )]\u2264\u03c4\nD(QY |W \u2016PY |QW ), (16)\nwith the supremum taken over all W defined on finite alphabets. The alphabet size of W can be restricted to be no larger than 2, which will be proven in Section 1.5 by using the dual expression for \u03c8. Based on \u03c8, the asymptotic expression of E (n) 0 is characterized in the following theorem.\nTheorem 5 (Asymptotics of E (n) 0 ). Assume that X is a compact metric space (which hence is Polish) and Y is an arbitrary Polish space. Assume that c is bounded. Under Assumption 2, for any (\u03b1, \u03c4) in the interior of dom\u03c8, it holds that\nlim \u03b1\u2032\u2191\u03b1 \u03c8(\u03b1\u2032, \u03c4) \u2264 lim inf n\u2192\u221e E (n) 0 (\u03b1, \u03c4) \u2264 lim sup n\u2192\u221e E (n) 0 (\u03b1, \u03c4) \u2264 lim \u03c4 \u2032\u2191\u03c4 \u03c8(\u03b1, \u03c4 \u2032). (17)\nThe proof of Theorem 5 is provided in Section 5. By checking our proof, the lower bound in (17) does not require the compactness of X and the boundness of c, but requires Assumption 2. Furthermore, to make it consistent with the expression of \u03c6 which is expressed in terms of relative entropies and the OT cost, the infimization in (16) can be written as the infimization over QY |W such that C(QX|W , QY |W |QW ) \u2264 \u03c4 .\nA special case of Theorem 5 in which X ,Y are both finite was proven by Ahlswede and Zhang [3] as a direct consequence of the inherently typical subset lemma [2]. In fact, the proof of Theorem 5 is also based on the inherently typical subset lemma, but requires more technical treatments since the space is much more general."
        },
        {
            "heading": "1.5 Dual Formulas",
            "text": "We now provide dual formulas for \u03c8 in (16) and variants of \u03c6 in (5) and \u03d5 in (15). The main tool used in deriving dual formulas is the Kantorovich duality for the optimal transport cost and the duality for the Iprojection. In the following, for a measurable function f : X \u2192 R, we adopt the notation PX(f) = \u222b\nX f dPX .\nWe define a variant of \u03c6 as for \u03b1, \u03c4 \u2265 0, \u03c6\u2265(\u03b1, \u03c4) := inf\nQX\u2208P(X ),QY \u2208P(Y): D(QX\u2016PX )\u2264\u03b1,C(QX ,QY )\u2265\u03c4\nD(QY \u2016PY ).\nThen, \u03c6\u2265(\u03b1, \u03c4) \u2264 \u03c6(\u03b1, \u03c4) \u2264 lim\u03c4 \u2032\u2193\u03c4 \u03c6\u2265(\u03b1, \u03c4 \u2032). Hence, for all (\u03b1, \u03c4) in the interior of dom\u03c6\u0306, \u03c6\u0306\u2265(\u03b1, \u03c4) = \u03c6\u0306(\u03b1, \u03c4). We next derive a dual formula for \u03c6\u2265.\nTheorem 6. For all \u03c4, \u03b1 \u2265 0,\n\u03c6\u2265(\u03b1, \u03c4) = inf (f,g)\u2208Cb(X )\u00d7Cb(Y):\nf+g\u2264c\nsup \u03bb>0,\u03b7>0\n\u03bb\u03c4 \u2212 logPY (e\u03bbg)\u2212 \u03b7\u03b1 \u2212 \u03b7 logPX(e \u03bb \u03b7 f ).\nMoreover, for all (\u03b1, \u03c4) in the interior of dom\u03c6\u0306, \u03c6\u0306\u2265(\u03b1, \u03c4) = \u03c6\u0306(\u03b1, \u03c4).\nDefine a variant of \u03d5 as\n\u03d5\u2265(\u03c4) := \u03c6\u2265(0, \u03c4) = inf QY :C(PX ,QY )\u2265\u03c4\nD(QY \u2016PY ).\nAs a consequence of Theorem 6, we have a dual formula for \u03d5\u2265.\nCorollary 1. For all \u03c4 \u2265 0, \u03d5\u2265(\u03c4) = inf\n(f,g)\u2208Cb(X )\u00d7Cb(Y): f+g\u2264c\nsup \u03bb\u22650\n\u03bb(\u03c4 \u2212 PX(f))\u2212 logPY (e\u03bbg).\nMoreover, for all \u03c4 in the interior of dom\u03d5\u0306, \u03d5\u0306\u2265(\u03c4) = \u03d5\u0306(\u03c4).\nWhen PX = PY , we define a variant of \u03d5X as\n\u03d5X,\u2265(\u03c4) := inf QX :C(PX ,QX)\u2265\u03c4\nD(QX\u2016PX).\nFor this case, we can write \u03d5X,\u2265 as follows.\nProposition 1. When PX = PY and c = d with d being a metric, we have for any 0 \u2264 \u03c4 < \u03c4max, \u03d5X,\u2265(\u03c4) = inf\n1-Lip f :PX(f)=0 sup \u03bb\u22650\n\u03bb\u03c4 \u2212 logPX(e\u03bbf ). (18)\nMoreover, for all \u03c4 in the interior of dom\u03d5\u0306X , \u03d5\u0306X,\u2265(\u03c4) = \u03d5\u0306X(\u03c4).\nBased on the dual formula in (18), we next show the equivalence between our formula \u03d5\u0306X(\u03c4) and Alon, Boppana, and Spencer\u2019s in [4]. When (X , PX) and (Y, PY ) are the same finite metric probability space, the cost function c is set to the metric d on this space, and a is set to 12 (equivalently, \u03b1n = 1 n log 2), Alon, Boppana, and Spencer in [4] proved an alternative expression for limn\u2192\u221e E (n) 1 (\u03b1n, \u03c4) which is\nr(\u03c4) := sup \u03bb\u22650\n\u03bb\u03c4 \u2212 LG(\u03bb).\nHere G = (X , d, PX) denotes the metric probability space we consider, and LG(\u03bb) denotes the maximum of logPX(e\n\u03bbf ) over all 1-Lipschitz functions7 f : X \u2192 R with PX(f) = 0. Theorem 7. For a finite metric probability space G = (X , d, PX ) and all \u03c4 > 0, \u03d5\u0306X(\u03c4) = r(\u03c4).\nWe now provide a dual formula for \u03c8.\nTheorem 8. For all \u03c4, \u03b1 \u2265 0,\n\u03c8(\u03b1, \u03c4) = sup fw+gw\u2264c,\u2200w\u2208{0,1} sup \u03bb\u22650 inf \u03b7>0 sup w\u2208{0,1}\n\u03b7\u03b1 + \u03b7 logPX(e \u03bb \u03b7 fw)\u2212 \u03bb\u03c4 \u2212 logPY (e\u2212\u03bbgw ),\nwhere (fw, gw) \u2208 Cb(X ) \u00d7 Cb(Y), \u2200w. Moreover, the alphabet size of W in the definition of \u03c8 (in (16)) can be restricted to be no larger than 2.\n7Call f : X \u2192 R 1-Lipschitz if |f(x)\u2212 f(x\u2032)| \u2264 d(x, x\u2032) for all x, x\u2032 \u2208 X ."
        },
        {
            "heading": "1.6 Connection to Strassen\u2019s Optimal Transport",
            "text": "We have characterized or bounded the concentration and isoperimetric exponents. Our results extend Alon, Boppana, and Spencer\u2019s in [4], Gozlan and L\u00e9onard\u2019s [14], and Ahlswede and Zhang\u2019s in [3]. Furthermore, the concentration or isoperimetric function is closely related to Strassen\u2019s optimal transport problem, in which we aim at characterizing\nG (n) t (PX , PY ) := min\nPXnY n\u2208\u03a0(P\u2297nX ,P \u2297n Y )\nP{cn(Xn, Y n) > t}\nfor t \u2265 0. By Strassen\u2019s duality,\nG (n) t (PX , PY ) = sup closed A\u2286X P\u2297nX (A)\u2212 P\u2297nY (At) (19)\n= sup a\u2208[0,1]\na\u2212 \u0393(n)(a, t).\nTherefore, if \u0393(n)(a, t) is characterized, then so is G (n) t (PX , PY ). In fact, the asymptotic exponents of G (n) t (PX , PY ) were already characterized by the author in [28]. Moreover, it has been shown in [28] that it suffices to restrict A in the supremum in (19) to be \u201cexchangeable\u201d (or \u201cpermutation-invariant\u201d). In other words, A could be specified by a set B of empirical measures in the way that a sequence xn is in A if and only if its empirical measure is in B. Hence, the supremum in (19) can be written as an optimization over empirical measures. From this point, we observe that if a 7\u2192 \u0393(n)(a, t) is convex, then the set A in the definition of \u0393(n)(a, t) (see (1)) can be also restricted to be \u201cexchangeable\u201d. We conjecture that this conclusion holds not only for this special case, but also for any other cases. If this is true, then central limit theorems can be applied to derive the limit of \u0393(n)(a, tn) with a fixed and tn set to a sequence approaching C(PX , PY ) in the order of 1/ \u221a n, just like central limit results in derived in [28]."
        },
        {
            "heading": "1.7 Notations and Organization",
            "text": "Throughout this paper, for a topological space Z, we use \u03a3(Z) to denote the Borel \u03c3-algebra on Z generated by the topology of Z. Hence (Z,\u03a3(Z)) forms a measurable space. For this measurable space, we denote the set of probability measures on (Z,\u03a3(Z)) as P(Z). If we equip P(Z) with the weak topology, then the resultant space is a Polish space as well. For brevity, we denote it as (P(Z),\u03a3(P(Z))).\nAs mentioned at the beginning of the introduction, X and Y are Polish spaces, and PX and PY are two probability measures defined respectively on X and Y. We also use QX , RX to denote another two probability measures on X . The probability measures PX , QX , RX can be thought as the push-forward measures (or the distributions) induced jointly by the same measurable function X (random variable) from an underlying measurable space to X and by different probability measures P,Q,R defined on the underlying measurable space. Without loss of generality, we assume that X is the identity map, and P,Q,R are the same as PX , QX , RX . So, PX , QX , RX could be independently specified to arbitrary probability measures. We say that all probability measures induced by the underlying measure P, together with the corresponding measurable spaces, constitute the P-system. So, PX is in fact the distribution of the random variable X in the P-system, where the letter \u201cP \u201d in the notation PX refers to the system and the subscript \u201cX\u201d refers to the random variable. When emphasizing the random variables, we write X \u223c PX to indicate that X follows the distribution PX in the P-system. For a random variable (a measurable function) f from X to another measurable space Z, the distribution Pf(X) of f in different systems is clearly different, e.g., it is PX \u25e6 f\u22121 in the P-system, but it is QX \u25e6 f\u22121 in the Q-system.\nWe use PX \u2297 PY to denote the product of PX and PY , and P\u2297nX (resp. P\u2297nY ) to denote the n-fold product of PX (resp. PY ). For a probability measure PX and a regular conditional distribution (transition probability or Markov kernel) PY |X from X to Y, we denote PXPY |X as the joint probability measure induced by PX and PY |X . We denote PY or PX \u25e6 PY |X as the marginal distribution on Y of the joint distribution PXPY |X . Moreover, we can pick up probability measures or transition probabilities from different\nprobability systems to constitute a joint probability measure, e.g., PXQY |X . For a distribution PX on X and a measurable subset A \u2286 X , PX(\u00b7|A) denotes the conditional probability measure given A. For brevity, we write PX(x) := PX({x}), x \u2208 X . In particular, if X \u223c PX is discrete, the restriction of PX to the set of singletons corresponds to the probability mass function of X in the P-system. We denote xn = (x1, x2, \u00b7 \u00b7 \u00b7 , xn) \u2208 Xn as a sequence in Xn. Given xn, denote xki = (xi, xi+1, \u00b7 \u00b7 \u00b7 , xk) as a subsequence of xn for 1 \u2264 i \u2264 k \u2264 n, and xk := xk1 . For a probability measure PXn on Xn, we use PXk|Xk\u22121 to denote the regular conditional distribution of Xk given X\nk\u22121 induced by PXn . For a measurable function f : X \u2192 R, sometimes we adopt the notation PX(f) = \u222b\nX f dPX . Given n \u2265 1, the empirical measure (also known as type for the finite alphabet case in information\ntheory [9, 11]) for a sequence xn \u2208 Xn is\nLxn := 1\nn\nn \u2211\ni=1\n\u03b4xi\nwhere \u03b4x is Dirac mass at the point x \u2208 X . For a pair of sequences (xn, yn) \u2208 Xn \u00d7 Yn, the empirical joint measure Lxn,yn and empirical conditional measure Lyn|xn are defined similarly. Obviously, empirical measures (or empirical joint measures) for n-length sequences are discrete distributions whose probability masses are multiples of 1/n.\nWe use B\u03b4(z) := {z\u2032 \u2208 Z : d(z, z\u2032) < \u03b4} and B\u03b4](z) := {z\u2032 \u2208 Z : d(z, z\u2032) \u2264 \u03b4} to respectively denote an open ball and a closed ball. We use A, Ao, and Ac := Z\\A to respectively denote the closure, interior, and complement of the set A \u2286 Z. Denote the sublevel set of the relative entropy (or the divergence \u201cball\u201d) as D\u01eb](PX) := {QX : D(QX\u2016PX) \u2264 \u01eb} for \u01eb \u2265 0. The L\u00e9vy\u2013Prokhorov metric, the TV distance, and the relative entropy admit the following relation: For any QX , PX ,\n\u221a\n2D(QX\u2016PX) \u2265 \u2016QX \u2212 PX\u2016TV \u2265 L(QX , PX), (20)\nwhich implies for \u01eb \u2265 0, D\u221a2\u01eb](PX) \u2286 B\u01eb](PX).\nThe first inequality in (20) is known as Pinsker\u2019s inequality, and the second inequality follows by definition [12].\nFor (X,Y ) \u223c QXY , the mutual information between X and Y is denoted as IQ(X ;Y ) = D(QXY \u2016QX \u2297 QY ). Denote the conditional mutual information as\nIQ(X ;Y |W ) = EQW [D(QXY |W \u2016QX|W \u2297QY |W )].\nFor discrete random variables (X,Y ) \u223c QXY , the (Shannon) entropy\nHQ(X) = \u2212 \u2211\nx\nQX(x) logQX(x),\nand the conditional (Shannon) entropy\nHQ(X |Y ) = \u2212 \u2211\nx,y\nQXY (x, y) logQX|Y (x|y).\nIn fact, for discrete random variables, IQ(X ;Y ) = HQ(X)\u2212HQ(X |Y ). We use f(n) = on(1) to denote that f(n) \u2192 0 as n \u2192 +\u221e. We denote inf \u2205 := +\u221e, sup \u2205 := \u2212\u221e, and [k] := {1, 2, ..., k}. Denote g\u0306 as the lower convex envelope of a function g, and \u201cg as the upper concave envelope of g.\nThroughout this paper, we use the following convention.\nConvention 1. When we write an optimization problem with probability measures as the variables, we by default require that those probability measures satisfy that all the relative entropies and integrals in constraint functions and the objective function exist and also are finite. If there is no such a distribution, by default, the value of the optimization problem is set to \u221e if the optimization is an infimization, and set to \u2212\u221e if the optimization is a supremization.\nThis paper is organized as follows. In Section 2-5, we respectively prove Theorems 1-5. The proofs for dual formulas are given in Section 6."
        },
        {
            "heading": "2 Proof of Theorem 1",
            "text": "Let A \u2286 X be a measurable subset. Denote t = n\u03c4 . Denote QXn = P\u2297nX (\u00b7|A) and QY n = P\u2297nY (\u00b7|(At)c). For two sets A,B, denote cn(A,B) = infxn\u2208A,yn\u2208B cn(xn, yn). We first claim that\nC(QXn , QY n) > \u03c4.\nWe now prove it. If cn(A, (A t)c) is attained by some pair (x\u2217n, y\u2217n), then\nC(QXn , QY n) \u2265 cn(A, (At)c) = cn(x\u2217n, y\u2217n) > \u03c4.\nWe next consider the case that cn(A, (A t)c) is not attained. Denote the optimal coupling that attains the infimum in the definition of C(QXn , QY n) as QXnY n (the existence of this coupling is well known). Therefore,\nC(QXn , QY n) = EQcn(X n, Y n).\nBy definition, cn(x n, yn) > \u03c4 for all xn \u2208 A, yn \u2208 B. Since any probability measure on a Polish space is tight, we have that for any \u01eb > 0, there exists a compact set F such that QXnY n(F ) > 1 \u2212 \u01eb. By the lower semi-continuity of c and compactness of F , we have that inf(xn,yn)\u2208F cn(x\nn, yn) is attained, and hence, inf(xn,yn)\u2208F cn(x n, yn) > \u03c4 , i.e., there is some \u03b4 > 0 such that cn(x n, yn) \u2265 \u03c4 + \u03b4 for all (xn, yn) \u2208 F . This further implies that C(QXn , QY n) \u2265 (1\u2212 \u01eb)(\u03c4 + \u03b4) + \u01eb\u03c4 > \u03c4. Hence, the claim above is true. Furthermore, by definition of QXn , QY n , we then have\n1 n D(QXn\u2016P\u2297nX ) = \u2212 1 n logP\u2297nX (A) 1 n D(QY n\u2016P\u2297nY ) = \u2212 1 n logP\u2297nY ((A t)c).\nTherefore,\nE (n) 1 (\u03b1, \u03c4) = \u2212\n1 n log\n(\n1\u2212 inf A:P\u2297nX (A)\u2265e\u2212n\u03b1 P\u2297nY (A t)\n)\n\u2265 inf QXn ,QY n : 1 nD(QXn\u2016P \u2297n X )\u2264\u03b1,\nC(QXn ,QY n )>\u03c4\n1 n D(QY n\u2016P\u2297nY ). (21)\nNote that this lower bound depends on the dimension n. We next single-letterize this bound, i.e., make it independent of n. To this end, we need the chain rule for relative entropies and the chain rule for OT costs. For relative entropies, we have the chain rule:\nD(QXn\u2016P\u2297nX ) = n \u2211\nk=1\nD(QXk|Xk\u22121\u2016PX |QXk\u22121) (22)\nD(QY n\u2016P\u2297nY ) = n \u2211\nk=1\nD(QYk|Y k\u22121\u2016PY |QY k\u22121).\nFor OT costs, we have a similar chain rule.\nLemma 1 (\u201cChain Rule\u201d for OT Costs). For any probability measures QXn , QY n on two Polish spaces,\nC(QXn , QY n) \u2264 n \u2211\nk=1\nC(QXk|Xk\u22121 , QYk|Y k\u22121 |QXk\u22121 , QY k\u22121),\nwhere\nC(QXk|Xk\u22121 , QYk|Y k\u22121 |QXk\u22121 , QY k\u22121) := sup\nQ Xk\u22121Y k\u22121 \u2208\u03a0(Q Xk\u22121 ,Q Y k\u22121 )\nC(QXk|Xk\u22121 , QYk|Y k\u22121 |QXk\u22121Y k\u22121).\nProof. We need the following \u201cchain rule\u201d for coupling sets, which is well-known in OT theory; see the proof in, e.g., [29, Lemma 9].\nLemma 2 (\u201cChain Rule\u201d for Coupling Sets). For any regular conditional distributions (PXi|Xi\u22121W , PYi|Y i\u22121W ), i \u2208 [n] and any QXiYi|Xi\u22121Y i\u22121W \u2208 \u03a0(PXi|Xi\u22121W , PYi|Y i\u22121W ), i \u2208 [n], we have\nn \u220f\ni=1\nQXiYi|Xi\u22121Y i\u22121W \u2208 \u03a0 (\nn \u220f\ni=1\nPXi|Xi\u22121W , n \u220f\ni=1\nPYi|Y i\u22121W ) .\nBy the lemma above, we have\nC(QXn , QY n)\n= inf QXnY n\u2208\u03a0(QXn ,QY n )\nn \u2211\nk=1\nEc(Xk, Yk)\n\u2264 inf QXn\u22121Y n\u22121\u2208\n\u03a0(QXn\u22121 ,QY n\u22121)\n[ n\u22121 \u2211\nk=1\nEc(Xk, Yk) + inf QXnYn|Xn\u22121Y n\u22121\u2208\n\u03a0(QXn|Xn\u22121 ,QYn|Y n\u22121)\nEc(Xn, Yn) ]\n(23)\n\u2264 inf QXn\u22121Y n\u22121\u2208\n\u03a0(QXn\u22121 ,QY n\u22121)\n[ n\u22121 \u2211\nk=1\nEc(Xk, Yk)\n+ sup QXn\u22121Y n\u22121\u2208\n\u03a0(QXn\u22121 ,QY n\u22121)\ninf QXnYn|Xn\u22121Y n\u22121\u2208\n\u03a0(QXn|Xn\u22121 ,QYn|Y n\u22121)\nEc(Xn, Yn) ]\n= inf QXn\u22121Y n\u22121\u2208\n\u03a0(QXn\u22121 ,QY n\u22121)\n[ n\u22121 \u2211\nk=1\nEc(Xk, Yk) ] + C(QXn|Xn\u22121 , QYn|Y n\u22121 |QXn\u22121 , QY n\u22121)\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\n\u2264 n \u2211\nk=1\nC(QXk|Xk\u22121 , QYk|Y k\u22121 |QXk\u22121 , QY k\u22121), (24)\nwhere in (23), Lemma 2 is applied.\nWe continue the proof of Theorem 1. From (22), we know that for anyQXn such that 1 nD(QXn\u2016P\u2297nX ) \u2264 \u03b1,\nthere must exist nonnegative numbers (\u03b1k) such that\nD(QXk|Xk\u22121\u2016PX |QXk\u22121) \u2264 \u03b1k and 1n \u2211n k=1 \u03b1k = \u03b1. Similarly, from (24), we know that for (QXn , QY n) such that 1 nC(QXn , QY n) > \u03c4 , there must exist nonnegative numbers (\u03c4k) such that\nC(QXk|Xk\u22121 , QYk|Y k\u22121 |QXk\u22121 , QY k\u22121) > \u03c4k and 1n \u2211n k=1 \u03c4k = \u03c4 . These lead to that for some sequence of nonnegative pairs ((\u03b1k, \u03c4k)) such that 1 n \u2211n k=1 \u03b1k = \u03b1, 1 n \u2211n k=1 \u03c4k = \u03c4 , we have\nE (n) 1 (\u03b1, \u03c4) \u2265\n1\nn\nn \u2211\nk=1\n\u03c6k(\u03b1k, \u03c4k, QXk\u22121 , QY k\u22121),\nwhere\n\u03c6k(\u03b1k, \u03c4k, QXk\u22121 , QY k\u22121) := inf Q\nXk|X k\u22121 ,QYk|Y k\u22121 :\nD(Q Xk|X k\u22121\u2016PX |QXk\u22121 )\u2264\u03b1k, C(Q\nXk|X k\u22121 ,QYk|Y k\u22121\n|Q Xk\u22121 ,Q Y k\u22121 )>\u03c4k\nD(QYk|Y k\u22121\u2016PY |QY k\u22121).\nWe now simplify the expression of \u03c6k(\u03b1k, \u03c4k, QXk\u22121 , QY k\u22121). Note that\nC(QXk|Xk\u22121 , QYk|Y k\u22121 |QXk\u22121 , QY k\u22121) > \u03c4k if and only if there exists a coupling QXk\u22121Y k\u22121 of (QXk\u22121 , QY k\u22121) such that\nC(QXk|Xk\u22121 , QYk|Y k\u22121 |QXk\u22121Y k\u22121) > \u03c4k.\nTherefore,\n\u03c6k(\u03b1k, \u03c4k, QXk\u22121 , QY k\u22121)\n= inf Q\nXk|X k\u22121 ,QYk|Y k\u22121 ,Q Xk\u22121Y k\u22121\n\u2208\u03a0(Q Xk\u22121 ,Q Y k\u22121 ):\nD(Q Xk|X k\u22121\u2016PX |QXk\u22121 )\u2264\u03b1k, C(Q\nXk|X k\u22121 ,QYk|Y k\u22121\n|Q Xk\u22121Y k\u22121 )>\u03c4k\nD(QYk|Y k\u22121\u2016PY |QY k\u22121)\n\u2265 inf Q\nXk|X k\u22121 ,QYk|Y k\u22121 ,Q Xk\u22121Y k\u22121 :\nD(Q Xk|X\nk\u22121Y k\u22121 \u2016PX |QXk\u22121Y k\u22121 )\u2264\u03b1k,\nC(Q Xk|X k\u22121Y k\u22121 ,Q Yk|X k\u22121Y k\u22121\n|Q Xk\u22121Y k\u22121 )>\u03c4k\nD(QYk|Xk\u22121Y k\u22121\u2016PY |QXk\u22121Y k\u22121) (25)\n\u2265 inf Q\nXk|X k\u22121Y k\u22121 ,Q Yk|X k\u22121Y k\u22121 ,Q Xk\u22121Y k\u22121 : D(Q Xk|X k\u22121Y k\u22121 \u2016PX |QXk\u22121Y k\u22121 )\u2264\u03b1k,\nC(Q Xk|X k\u22121Y k\u22121 ,Q Yk|X k\u22121Y k\u22121\n|Q Xk\u22121Y k\u22121 )>\u03c4k\nD(QYk|Xk\u22121Y k\u22121\u2016PY |QXk\u22121Y k\u22121) (26)\nwhere\n\u2022 in (25), we denote QXk|Xk\u22121Y k\u22121 = QXk|Xk\u22121 , QYk|Xk\u22121Y k\u22121 = QYk|Y k\u22121 , (27)\nand at the same time, we relax the coupling QXk\u22121Y k\u22121 of (QXk\u22121 , QY k\u22121) to any joint distribution,\n\u2022 in (26) we optimize over (QXk|Xk\u22121Y k\u22121 , QYk|Xk\u22121Y k\u22121) directly, instead over (QXk|Xk\u22121 , QYk|Y k\u22121). (In other words, we remove the constraints given in (27) from the optimization in (26).)\nRecall the expression of \u03c6\u0306(\u03b1, \u03c4) in (6). If we substitute W \u2190 (Xk\u22121, Y k\u22121), X \u2190 Xk, Y \u2190 Yk into (26), then we obtain the expression in (6). In other words, (26) is further lower bounded by \u03c6\u0306(\u03b1k, \u03c4k). Therefore,\nE (n) 1 (\u03b1, \u03c4) \u2265\n1\nn\nn \u2211\nk=1\n\u03c6\u0306(\u03b1k, \u03c4k) \u2265 \u03c6\u0306(\u03b1, \u03c4)."
        },
        {
            "heading": "3 Proof of Theorem 3",
            "text": "Statement 1 (Case \u03b1 > 0): From the dimension-free bound in Theorem 1, lim infn\u2192\u221eE (n) 1 (\u03b1, \u03c4) \u2265 \u03c6\u0306(\u03b1, \u03c4). We next prove lim supn\u2192\u221eE (n) 1 (\u03b1, \u03c4) \u2264 \u03c6\u0306(\u03b1, \u03c4).\nWe assume that W is finite, and without loss of generality, we assume W = [|W|] = {1, 2, \u00b7 \u00b7 \u00b7 , |W|}. Let \u01eb > 0. Let ( QW , QX|W , QY |W ) be an optimal pair attaining \u03c6\u0306(\u03b1\u2212 \u01eb, \u03c4 + \u01eb) + \u01eb. That is,\nD(QX|W \u2016PX |QW ) \u2264 \u03b1\u2212 \u01eb C(QX|W , QY |W |QW ) > \u03c4 + \u01eb D(QY |W \u2016PY |QW ) \u2264 \u03c6\u0306(\u03b1\u2212 \u01eb, \u03c4 + \u01eb) + \u01eb.\nWithout loss of generality, we assume supp(QW ) = [m].\nFor each n, let Q (n) W be an empirical measure of an n-length sequence such that supp(Q (n) W ) \u2286 [m] and\nQ (n) W \u2192 QW as n\u2192\u221e. LetQ (n) XW := Q (n) W QX|W , Q (n) YW := Q (n) W QY |W . Letw n = (1, \u00b7 \u00b7 \u00b7 , 1, 2, \u00b7 \u00b7 \u00b7 , 2, \u00b7 \u00b7 \u00b7 ,m, \u00b7 \u00b7 \u00b7 ,m) be an n-length sequence, where i appears ni := nQ (n) W (i) times. Hence, the empirical measure of w n is Q (n) W .\nDenote Aw := B\u01eb\u2032](QX|W=w) for w \u2208 [m], and A := {RX|W : RX|W=w \u2208 Aw, \u2200w \u2208 [m]} for \u01eb > 0. Denote A as the set of sequences xn such that the empirical conditional measure Lxn|wn \u2208 A. In other words,\nA = {xn : Lxn|wn \u2208 A} = m \u220f\nw=1\n{xnw : Lxnw \u2208 Aw} .\nFor each w, Aw is closed. Since the empirical measure map \u2113 is continuous under the weak topology, {xnw : Lxnw \u2208 Aw} is closed in Xnw . Therefore, A is closed in Xn.\nSimilarly, denote Bw := B\u01eb\u2032](QY |W=w) for w \u2208 [m], and B := {RY |W : RY |W=w \u2208 Bw, \u2200w \u2208 [m]} for \u01eb > 0. Denote B = {yn : Lyn|wn \u2208 B}. So, B is closed in Yn.\nBy Sanov\u2019s theorem,\nlim sup n\u2192\u221e \u2212 1 n logP\u2297nX (A) = \u2211\nw\nQW (w) lim sup n\u2192\u221e \u2212 1 nw logP\u2297nwX {xnw : Lxnw \u2208 Aw}\n\u2264 \u2211\nw\nQW (w) inf RX\u2208Aow\nD(RX\u2016PX)\n\u2264 \u2211\nw\nQW (w)D(QX|W=w\u2016PX)\n= D(QX|W \u2016PX |QW ) \u2264 \u03b1\u2212 \u01eb.\nHence, \u2212 1n logP\u2297nX (A) \u2264 \u03b1 for all sufficiently large n. Similarly,\n\u2212 1 n logP\u2297nY (B) \u2264 \u03c6\u0306(\u03b1\u2212 \u01eb, \u03c4 + \u01eb) + 2\u01eb (28)\nfor all sufficiently large n.\nObserve that8\nB0 := { (RY |W=w)w\u2208W \u2208 P(Y)m : C(QX|W , RY |W |QW ) > \u03c4 + \u01eb }\nis open in P(Y)m equipped with the product topology. Since QY |W \u2208 B0, B0 contains the product of Fw, w \u2208 [m] for some open sets Fw \u2286 P(Y) such that QY |W=w \u2208 Fw. So Bw \u2286 Fw, \u2200w, for sufficiently small \u01eb\u2032, which means in this case, B \u2286 B0. Then, we obtain that\nEQW inf RX\u2208AW ,RY \u2208BW C(RX , RY ) = inf RX|W\u2208A,RY |W\u2208B\nC(RX|W , RY |W |QW )\n\u2265 inf RY |W\u2208B C(QX|W , RY |W |QW )\u2212 \u03b4(\u01eb\u2032) (29) \u2265 \u03c4 + \u01eb\u2212 \u03b4(\u01eb\u2032), (30)\nwhere \u03b4(\u01eb\u2032) is positive and vanishes as \u01eb\u2032 \u2193 0, (29) follows by Assumption 2, and (30) follows from B \u2286 B0. 8This is because RY |W 7\u2192 C(QX|W , RY |W |QW ) is the weighted sum of lower semi-continuous functions RY |W=w 7\u2192 C(QX|W=w, RY |W=w). So, C(QX|W , RY |W |QW ) is lower semi-continuous as well in P(Y)m equipped with the product topology. Hence, its strict superlevel sets are open.\nUsing these equations, we obtain that for Lxn|wn \u2208 A,Lyn|wn \u2208 B,\n1 n cn(x n, yn) = ELxn,yn,wn c(X,Y )\n\u2265 C(Lxn|wn ,Lyn|wn |Lwn) \u2265 inf\nRX|W\u2208A,RY |W\u2208B C(RX|W , RY |W |Lwn)\n= EW\u223cLwn inf RX\u2208AW ,RY \u2208BW C(RX , RY ) \u2192 EW\u223cQW inf RX\u2208AW ,RY \u2208BW C(RX , RY ) > \u03c4 + \u01eb\u2212 \u03b4(\u01eb\u2032).\nSo, if we let \u01eb > 0 be fixed and \u01eb\u2032 > 0 be sufficiently small such that \u01eb > \u03b4(\u01eb\u2032), then for sufficiently large n, we have 1ncn(x n, yn) > \u03c4 . Hence,\nlim sup n\u2192\u221e\nE (n) 1 (\u03b1, \u03c4) \u2264 \u03c6\u0306(\u03b1 \u2212 \u01eb, \u03c4 + \u01eb) + \u01eb.\nSince \u03c6\u0306 is convex, it is continuous on (0,+\u221e)2. We hence have that for all \u03b1, \u03c4 > 0,\nlim sup n\u2192\u221e\nE (n) 1 (\u03b1, \u03c4) \u2264 \u03c6\u0306(\u03b1, \u03c4).\nStatement 2 (Case \u03b1n \u2192 0): The lower bound follows by the dimension-free bound in Theorem 1. We next prove the upper bound. For this case, we set \u03b1 = 0 in the proof above, and re-choose ( QW , QY |W )\nas an optimal pair attaining \u03d5\u0306(\u03c4 + \u01eb) + \u01eb. That is,\nC(PX , QY |W |QW ) > \u03c4 + \u01eb D(QY |W \u2016PY |QW ) \u2264 \u03d5\u0306(\u03c4 + \u01eb) + \u01eb.\nOn one hand, we choose A := B\u01eb\u2032](PX) for \u01eb \u2032 > 0, and then have\nlim sup n\u2192\u221e \u2212 1 n log ( 1\u2212 P\u2297nX (A) ) \u2265 inf QX\u2208Ac D(QX\u2016PX)\n\u2265 inf QX :L(QX ,PX )\u2265\u01eb\u2032/2 D(QX\u2016PX) \u2265 \u01eb\u20322/8,\nwhere the last inequality follows since\nD(QX\u2016PX) \u2265 L(QX , PX)2/2.\nHence, for fixed \u01eb\u2032 > 0, P\u2297nX (A)\u2192 1 as n\u2192 +\u221e exponentially fast. On the other hand, we remain the choices of Bw and B. Similarly to (28), we obtain\n\u2212 1 n logP\u2297nY (B) \u2264 \u03d5\u0306(\u03c4 + \u01eb) + 2\u01eb\nfor all sufficiently large n. Similarly to the above, it can be shown that 1ncn(x\nn, yn) > \u03c4 for sufficiently large n. We hence have that for all \u03c4 > 0,\nlim sup n\u2192\u221e\nE (n) 1 (\u03b1n, \u03c4) \u2264 \u03d5\u0306(\u03c4)."
        },
        {
            "heading": "4 Proof of Theorem 4",
            "text": "Statement 1 (Case \u03b1 > 0): From the dimension-free bound in Theorem 1, lim infn\u2192\u221eE (n) 1 (\u03b1, \u03c4) \u2265 \u03c6\u0306(\u03b1, \u03c4). We next prove lim supn\u2192\u221eE (n) 1 (\u03b1, \u03c4) \u2264 \u03c6\u0306(\u03b1, \u03c4).\nLet s > 0, and ds = min{d, s}. Then, ds is a bounded metric on X . This is just the second example given below Assumption 2 which satisfies Assumption 2. So, by Theorem 3, when we set c = dps, we have lim supn\u2192\u221e E (n) 1,s (\u03b1, \u03c4) \u2264 \u03c6\u0306s(\u03b1, \u03c4), where E (n) 1,s (\u03b1, \u03c4) is the quantity E (n) 1 (\u03b1, \u03c4) given in (2) but defined for c = dps, and similarly, \u03c6\u0306s(\u03b1, \u03c4) is the \u03c6\u0306(\u03b1, \u03c4) defined for c = d p s . Explicitly,\n\u03c6\u0306s(\u03b1, \u03c4) = inf QX|W ,QY |W ,QW :\nD(QX|W \u2016PX |QW )\u2264\u03b1, Cs(QX|W ,QY |W |QW )>\u03c4\nD(QY |W \u2016PY |QW ) (31)\nwhere Cs(QX|W , QY |W |QW ) is the OT cost for c = dps. Observe that for the same A,\nAt = \u22c3\nxn\u2208A {yn \u2208 Yn :\nn \u2211\ni=1\ndp(xi, yi) \u2264 t}\n\u2286 \u22c3\nxn\u2208A {yn \u2208 Yn :\nn \u2211\ni=1\ndps(xi, yi) \u2264 t} =: Ats\nSo, E (n) 1 (\u03b1, \u03c4) \u2264 E (n) 1,s (\u03b1, \u03c4). Hence, lim supn\u2192\u221e E (n) 1 (\u03b1, \u03c4) \u2264 \u03c6\u0306s(\u03b1, \u03c4). Taking limit as s \u2192 \u221e, we obtain lim supn\u2192\u221e E (n) 1 (\u03b1, \u03c4) \u2264 lims\u2192\u221e \u03c6\u0306s(\u03b1, \u03c4). To prove Statement 1, it suffices to show that lims\u2192\u221e \u03c6\u0306s(\u03b1, \u03c4) = \u03c6\u0306(\u03b1, \u03c4) for \u03b1, \u03c4 > 0. On one hand, \u03c6\u0306s(\u03b1, \u03c4) \u2265 \u03c6\u0306(\u03b1, \u03c4) since Cs(QX|W , QY |W |QW ) \u2264 C(QX|W , QY |W |QW ). So, it suffices to prove lims\u2192\u221e \u03c6\u0306s(\u03b1, \u03c4) \u2264 \u03c6\u0306(\u03b1, \u03c4) for \u03b1, \u03c4 > 0.\nLet \u01eb > 0. Let ( QW , QX|W , QY |W ) be an optimal pair attaining \u03c6\u0306(\u03b1, \u03c4) + \u01eb. That is,\nD(QX|W \u2016PX |QW ) \u2264 \u03b1 C(QX|W , QY |W |QW ) > \u03c4 D(QY |W \u2016PY |QW ) \u2264 \u03c6\u0306(\u03b1, \u03c4) + \u01eb.\nLemma 3. Given (QX , QY ), lim s\u2192\u221e Cs(QX , QY ) = C(QX , QY ).\nProof. Obviously, Cs(QX , QY ) \u2264 C(QX , QY ). Hence, lims\u2192\u221e Cs(QX , QY ) \u2264 C(QX , QY ). By Kantorovich duality [27, Theorem 5.10] (also given in Lemma 5),\nC(QX , QY ) = sup (f,g)\u2208Cb(X )\u00d7Cb(Y):f+g\u2264c\n\u222b\nX f dQX +\n\u222b\nY g dQY\nwhere Cb(X ) denotes the collection of bounded continuous functions f : X \u2192 R. Given \u01eb > 0, let (f\u2217, g\u2217) \u2208 Cb(X ) \u00d7 Cb(Y) be such that\nf\u2217 + g\u2217 \u2264 c \u222b\nX f\u2217 dQX +\n\u222b\nY g\u2217 dQY \u2265 C(QX , QY )\u2212 \u01eb.\nThen, by the boundness, f\u2217 + g\u2217 \u2264 cs for all sufficiently large s. By Kantorovich duality again,\nCs(QX , QY ) = sup (f,g)\u2208Cb(X )\u00d7Cb(Y):f+g\u2264cs\n\u222b\nX f dQX +\n\u222b\nY g dQY . (32)\nFor sufficiently large s, (f\u2217, g\u2217) is a feasible solution to (32). Hence,\nCs(QX , QY ) \u2265 \u222b\nX f\u2217 dQX +\n\u222b\nY g\u2217 dQY \u2265 C(QX , QY )\u2212 \u01eb.\nSince \u01eb > 0 is arbitrary, lims\u2192\u221e Cs(QX , QY ) \u2265 C(QX , QY ), completing the proof.\nSince by definition, the conditional OT cost is the weighted sum of the unconditional version, given (\nQW , QX|W , QY |W ) , we immediately have\nlim s\u2192\u221e\nCs(QX|W , QY |W |QW ) = C(QX|W , QY |W |QW ) > \u03c4.\nSo, for sufficiently large s, Cs(QX|W , QY |W |QW ) > \u03c4 which means that ( QW , QX|W , QY |W )\nis a feasible solution to the infimization in (31) with \u03b1 substituted by \u03b1\u2212 \u01eb. Therefore,\nlim s\u2192\u221e\n\u03c6\u0306s(\u03b1, \u03c4) \u2264 D(QY |W \u2016PY |QW ) \u2264 \u03c6\u0306(\u03b1, \u03c4) + \u01eb.\nLetting \u01eb \u2193 0, we obtain lims\u2192\u221e \u03c6\u0306s(\u03b1, \u03c4) \u2264 \u03c6\u0306(\u03b1, \u03c4). This completes the proof. Statement 2 (Case \u03b1n \u2192 0): The proof for the upper bound is similar to the above for Statement 1, and hence is omitted here. We next prove lim infn\u2192\u221eE (n) 1 (\u03b1n, \u03c4) \u2265 \u03d5\u0306X(\u03c4). From the dimension-free bound in 3, we have for fixed \u03c4 , E (n) 1 (\u03b1n, \u03c4) \u2265 \u03c6\u0306(\u03b1n, \u03c4). Under the condition D(QX|W \u2016PX |QW ) \u2264 \u03b1n, we have\nC(QX|W \u2016PX |QW ) \u2264 \u201c\u03baX(\u03b1n).\nRecall that \u03baX is given in (10). By Assumption 2, \u201c\u03baX(\u03b1) \u2192 0 as \u03b1 \u2192 0. By the triangle inequality (since for this case, C1/p(\u00b7, \u00b7) is a Wasserstein metric), we then have that for (QX|W , QY |W , QW ) satisfying the constraints in (6),\nC1/p(PX , QY |W |QW ) \u2265 C1/p(QX|W , QY |W |QW )\u2212 C1/p(QX|W , PX |QW ) > \u03c41/p \u2212 \u201c\u03baX(\u03b1n)1/p.\nWe finally obtain \u03c6\u0306(\u03b1n, \u03c4) \u2265 \u03d5\u0306X ( (\u03c41/p \u2212 \u201c\u03baX(\u03b1n)1/p)p ) .\nLetting n\u2192\u221e, lim infn\u2192\u221e \u03c6\u0306(\u03b1n, \u03c4) \u2265 \u03d5\u0306X(\u03c4) for \u03c4 > 0. Hence, lim infn\u2192\u221eE(n)1 (\u03b1n, \u03c4) \u2265 \u03d5\u0306X(\u03c4)."
        },
        {
            "heading": "5 Proof of Theorem 5",
            "text": ""
        },
        {
            "heading": "5.1 Upper Bound",
            "text": ""
        },
        {
            "heading": "5.1.1 Finite X",
            "text": "We first consider that X is a finite metric space. For this case, we extend Ahlswede, Yang, and Zhang\u2019s method [2, 3] to the case in which Y is an arbitrary Polish space (but X is still a finite metric space). We divide the proof into four steps.\nStep 1: Inherently Typical Subset Lemma In our proof, we utilize the inherently typical subset lemma in [2, 3]. We now introduce this lemma. Let\nA be any subset of Xn. For any 0 \u2264 i \u2264 n\u2212 1, define\nAi = { xi \u2208 X i : xi is a prefix of some element of A } ,\nwhich is the projection of A to the space X i of the first i components.\nDefinition 1. A \u2286 Xn is called m-inherently typical if there exist a set Wm with |Wm| \u2264 (m+ 1)|X | and n mappings \u03c6i : Ai \u2192Wm, i \u2208 [0 : n\u2212 1] such that the following hold: (i) There exists a distribution (empirical measure) QXW such that for any x n \u2208 A,\nLxnwn = QXW\nwhere wn is a sequence defined by wi = \u03c6i(x i\u22121) for all 1 \u2264 i \u2264 n. Such a sequence is called a sequence associated with xn through (\u03c6i). (ii)\nHQ(X |W )\u2212 log2m m \u2264 1 n log |A| \u2264 HQ(X |W ). (33)\nFor an m-inherently typical set A, let QXn be the uniform distribution on A. We now give another interpretation of the m-inherently typical set in the language of sufficient statistics. Let Wi = \u03c6i(X\ni\u22121). First, observe that\n1 n log |A| = HQ(Xn)\n= n \u2211\ni=1\nHQ(Xi|X i\u22121)\n=\nn \u2211\ni=1\nHQ(Xi|X i\u22121,Wi)\n= HQ(XK |XK\u22121,WK ,K)\nwhere K is a random time index uniformly distributed over [n] which is independent of Xn. Moreover,\nQXK ,WK = E(Xn,Wn)\u223cQXn,Wn [QXK ,WK |Xn,Wn ]\n= E(Xn,Wn)\u223cQXn,Wn [LXn,Wn ]\n= QX,W . (34)\nHence, the inequalities in (33) can be rewritten as\n0 \u2264 IQ(XK ;XK\u22121,K|WK) \u2264 log2m\nm .\nThe first inequality holds trivially since mutual information is nonnegative. For sufficiently large m, the bound log 2 m m is sufficiently small. Hence, IQ(XK ;X\nK\u22121,K|WK) is close to zero. In this case, XK and (XK\u22121,K) are approximately conditionally independent given WK . In other words, WK is an approximate sufficient statistic for \u201cunderlying parameter\u201d XK ; refer to [7, Section 2.9] for sufficient statistics and [16] for approximate versions.\nAs for m-inherent typical sets, one of the most important results is the inherently typical subset lemma, which concerns the existence of inherent typical sets. Such a lemma was proven by Ahlswede, Yang, and Zhang [2, 3].\nLemma 4 (Inherently Typical Subset Lemma). For any m \u2265 216|X |2, n satisfying ( (m+ 1)5|X |+4 ln(n+ 1) ) /n \u2264 1, and any A \u2286 Xn, there exists an m-inherently typical subset A\u0303 \u2286 A such that\n0 \u2264 1 n log |A| |A\u0303| \u2264 |X |(m+ 1)|X | log(n+ 1) n .\nStep 2: Multi-letter Bound\nFor any A \u2286 Xn, denote AQX := A \u2229 {xn : Lxn = QX} for empirical measure QX . Since A = \u22c3 QX AQX\nand the number of distinct types is no more than (n+ 1)|X |, by the pigeonhole principle, we have\nP\u2297nX (AQX ) \u2265 P\u2297nX (A)(n+ 1)\u2212|X |\nfor some empirical measure QX . By the lemma above, given m \u2265 216|X |2, for all sufficiently large n, there exists an m-inherently typical subset A\u0303 \u2286 AQX such that |A\u0303| \u2265 |AQX | \u00b7 (n+ 1)\u2212b where b = |X |(m+1)|X |. Observe that for anyB \u2286 {xn : Lxn = QX}, we have P\u2297nX (B) = |B|en \u2211\nx QX (x) logPX (x). Hence,\nP\u2297nX (A\u0303) \u2265 P\u2297nX (AQX )(n+ 1)\u2212b \u2265 P\u2297nX (A)(n+ 1)\u2212b \u2032\nwhere b\u2032 = b+ |X | = |X |(1 + (m+ 1)|X |). Let QXn be the uniform distribution on A\u0303. Then, (34) and (37) still hold, and moreover,\nD(QXn\u2016P\u2297nX ) = \u2212 1 n logP\u2297nX (A\u0303) \u2264 \u2212 1 n logP\u2297nX (A) + on(1).\nIf P\u2297nX (A) \u2265 e\u2212n\u03b1, we have D(QXn\u2016P\u2297nX ) \u2264 \u03b1+ on(1). (35)\nDenote t = n\u03c4 . Let QY n|Xn be a conditional distribution such that given each x n, QY n|Xn=xn is concentrated on the cost ball Bt(x n) := {yn : cn(xn, yn) \u2264 t}. Then, we have that QY n := QXn \u25e6 QY n|Xn is concentrated on At, which implies that \u2212 1n logP\u2297nY (At) \u2264 1nD0(QY n\u2016P\u2297nY ) \u2264 1nD(QY n\u2016P\u2297nY ). Here D0(Q\u2016P ) := \u2212 logP{dQdP > 0} is the R\u00e9nyi divergence of order 0, which is no greater than the relative entropy D(Q\u2016P ) [25]. Since QY n|Xn is arbitrary, we have\n\u2212 1 n logP\u2297nY (A t) \u2264 inf QY n|Xn :cn(Xn,Y n)\u2264t a.s. 1 n D(QY n\u2016P\u2297nY ).\nTaking supremum of the RHS over all QXn satisfying (34), (37), and (35), we have\nE (n) 0 (\u03b1, \u03c4) \u2264 \u03b7n(\u03b1, \u03c4) := sup\nQXn ,QXW : 1 nD(QXn\u2016P \u2297n X )\u2264\u03b1+on(1),\nQXnWn{(xn,wn):Lxn,wn=QXW }=1, IQ(XK ;X K\u22121,K|WK)=om(1)\ninf QY n|Xn :\ncn(X n,Y n)\u2264t a.s.\n1 n D(QY n\u2016P\u2297nY ). (36)\nwhere Wi = \u03c6i(X i\u22121). The condition QXnWn{(xn, wn) : Lxn,wn = QXW } = 1 implies QXK ,WK = QXW .\nStep 3: Single-letterizing the Cost Constraint We next make a special choice of QY n|Xn . Let \u03b4 > 0 and let QY |XW be a conditional distribution such\nthat\n\u00b5 := EQXW QY |XW c(X,Y ) \u2264 \u03c4 \u2212 \u03b4.\nThen, for all (xn, wn) with type QXW with wi = \u03c6i(x i\u22121) and for Y n \u223c Q\u2297nY |X,W (\u00b7|xn, wn), it holds that\nEcn(x n, Y n) =\nn \u2211\nk=1\nEc(xk, Yk)\n=\nn \u2211\nk=1\n\u00b5(xk, wk)\n= nEQXW \u00b5(X,W ) = n\u00b5,\nwhere \u00b5(x,w) := EQY |(X,W )=(x,w)c(x, Y ). Hence, by Chebyshev\u2019s inequality, the probability\n\u01ebn := Q{Y n /\u2208 {xn}t} = Q{cn(xn, Y n) > n\u03c4}\n\u2264 E [ (cn(x n, Y n)\u2212 n\u00b5)2 ]\nn2(\u03c4 \u2212 \u00b5)2\n=\n\u2211n k=1 E [ (c(xk, Yk)\u2212 \u00b5(xk, wk))2 ]\nn2(\u03c4 \u2212 \u00b5)2\n= EQXW Var(c(X,Y )|X,W )\nn(\u03c4 \u2212 \u00b5)2\n\u2264 Var(c(X,Y )) n(\u03c4 \u2212 \u00b5)2 \u2264 E [ c(X,Y )2 ]\nn(\u03c4 \u2212 \u00b5)2 .\nRecall that Q denotes the underlying probability measure that induces Q\u2297nY |X,W . Since E [ c(X,Y )2 ] is bounded, the last line (and also \u01ebn) vanishes as n\u2192\u221e. However, such a product distribution Q\u2297nY |X,W does not satisfy the constraint cn(X\nn, Y n) \u2264 t a.s. So, we cannot substitute it into (36) directly. We next construct a conditional version of Q\u2297nY |X,W and then substitute this conditional version into (36). Denote Q\u0302Y n|XnWn as a distribution given by\nQ\u0302Y n|(Xn,Wn)=(xn,wn) =\n(\nn \u220f\nk=1\nQY |(X,W )=(xk,wk)\n)\n(\u00b7|{xn}t)\nfor all xn and wi = \u03c6i(x i\u22121). Denote\nQ\u0303Y n|(Xn,Wn)=(xn,wn) =\n(\nn \u220f\nk=1\nQY |(X,W )=(xk,wk)\n)\n(\u00b7|({xn}t)c).\nWe can rewrite Q\u2297nY |XW as a mixture:\nQ\u2297nY |XW (\u00b7|xn, wn) = (1 \u2212 \u01ebn)Q\u0302Y n|(Xn,Wn)=(xn,wn) + \u01ebnQ\u0303Y n|(Xn,Wn)=(xn,wn).\nFor the same input distributionQXn , the output distributions of channelsQ \u2297n Y |XW , Q\u0302Y n|Xn,Wn , and Q\u0303Y n|Xn,Wn are respectively denoted as QY n , Q\u0302Y n , and Q\u0303Y n , which satisfy\nQY n = (1\u2212 \u01ebn)Q\u0302Y n + \u01ebnQ\u0303Y n .\nDenote J \u223c QJ := Bern(\u01ebn), and QY n|J=1 = Q\u0302Y n , QY n|J=0 = Q\u0303Y n . Then,\nQY n = QJ(1)QY n|J=1 +QJ(0)QY n|J=0.\nObserve that\nD(QY n|J\u2016P\u2297nY |QJ) = (1\u2212 \u01ebn)D(Q\u0302Y n\u2016P\u2297nY ) + \u01ebnD(Q\u0303Y n\u2016P\u2297nY ) \u2265 (1\u2212 \u01ebn)D(Q\u0302Y n\u2016P\u2297nY ).\nOn the other hand, D(QY n|J\u2016P\u2297nY |QJ) = D(QY n\u2016P\u2297nY ) +D(QJ|Y n\u2016QJ |QY n),\nand D(QJ|Y n\u2016QJ |QY n) = IQ(J ;Y n) \u2264 HQ(J) \u2264 log 2.\nHence,\nD(Q\u0302Y n\u2016P\u2297nY ) \u2264 D(QY n\u2016P\u2297nY ) + log 2\n1\u2212 \u01ebn .\nBy choosing QY n|Xn in (36) such that QY n|Xn=xn = Q\u0302Y n|(Xn,Wn)=(xn,wn) for all x n where wi = \u03c6i(x i\u22121), we then have\n\u03b7n (\u03b1, \u03c4) \u2264 sup QXn ,QXW : 1 nD(QXn\u2016P \u2297n X )\u2264\u03b1+on(1),\nQXK,WK=QXW ,\nIQ(XK ;X K\u22121,K|WK)=om(1)\ninf QY |XW :\nEQXW QY |XW c(X,Y )\u2264\u03c4\u2212\u03b4\n1 n D(Q\u0302Y n\u2016P\u2297nY )\n\u2264 sup QXn ,QXW : 1 nD(QXn\u2016P \u2297n X )\u2264\u03b1+on(1),\nQXK,WK=QXW ,\nIQ(XK ;X K\u22121,K|WK)=om(1)\ninf QY |XW :\nEQXW QY |XW c(X,Y )\u2264\u03c4\u2212\u03b4\nD(QY n\u2016P\u2297nY ) n(1\u2212 \u01ebn) + on(1).\nStep 4: Single-letterizing Divergences We next complete the single-letterization. By standard information-theoretic techniques, we obtain that\n1 n D(QY n\u2016P\u2297nY ) = 1 n\nn \u2211\nk=1\nD(QYk|Y k\u22121\u2016PY |QY k\u22121)\n\u2264 1 n\nn \u2211\nk=1\nD(QYk|Xk\u22121Y k\u22121\u2016PY |QXk\u22121Y k\u22121)\n= 1\nn\nn \u2211\nk=1\nD(QYk|Xk\u22121\u2016PY |QXk\u22121)\n+ 1\nn\nn \u2211\nk=1\nD(QYk|Xk\u22121Y k\u22121\u2016QYk|Xk\u22121 |QXk\u22121Y k\u22121)\n= 1\nn\nn \u2211\nk=1\nD(QYk|Xk\u22121\u2016PY |QXk\u22121) + 1\nn\nn \u2211\nk=1\nIQ(Yk;Y k\u22121|Xk\u22121)\n= 1\nn\nn \u2211\nk=1\nD(QYk|Xk\u22121\u2016PY |QXk\u22121) (37)\n= D(QYK |XK\u22121K\u2016PY |QXK\u22121K) = D(QYK |XK\u22121KWK\u2016QYK |WK |QXK\u22121KWK ) +D(QYK |WK\u2016PY |QWK ) = IQ(YK ;X\nK\u22121,K|WK) +D(QYK |WK\u2016PY |QWK ) = D(QYK |WK\u2016PY |QWK ) + om(1) (38) = D(QY |W \u2016PY |QW ) + om(1),\nwhere\n\u2022 (37) follows since under the distribution QXnWnQ \u2297n Y |XW , W k is a function of Xk\u22121, and moreover, Yk\nand Y k\u22121 are conditionally independent given (Xk\u22121,W k) for each k;\n\u2022 (38) follows since under the distribution QK \u2297QXnWnQ\u2297nY |XW with QK = Unif[n], (K,XK\u22121) and YK are conditionally independent given (XK ,WK), and hence,\nIQ(YK ;X K\u22121,K|WK) \u2264 IQ(XK ;XK\u22121,K|WK) = om(1);\n\u2022 in the last line, QY |W is induced by the distribution QXWQY |XW , and the last line follows since QYK|W is induced by the distribution QXKWKQY |XW , and hence, QYK |W = QY |W . (Recall that QXKWK = QXW .)\nSimilarly, we have\n1 n D(QXn\u2016P\u2297nX ) = 1 n\nn \u2211\nk=1\nD(QXk|Xk\u22121\u2016PX |QXk\u22121)\n= D(QXK |XK\u22121K\u2016PX |QXK\u22121K) = IQ(XK ;X\nK\u22121K|W ) +D(QX|W \u2016PX |QW ) \u2265 D(QX|W \u2016PX |QW ).\nHence,\n\u03b7n (\u03b1, \u03c4) \u2264 1\n1\u2212 \u01ebn sup\nQXW : D(QX|W \u2016PX |QW )\u2264\u03b1+on(1)\ninf QY |XW :\nEQc(X,Y )\u2264\u03c4\u2212\u03b4\nD(QY |W \u2016PY |QW ) + om(1) + on(1)\n= 1\n1\u2212 \u01ebn \u03c8m(\u03b1 + on(1), \u03c4 \u2212 \u03b4) + om(1) + on(1),\nwhere \u03c8m is defined similarly as \u03c8 but with W restricted to concentrate on the alphabet Wm satisfying |Wm| \u2264 (m+ 1)|X |.\nLetting n\u2192\u221e and \u03b4 \u2193 0, we obtain\nlim sup n\u2192\u221e\nE (n) 0 (\u03b1, \u03c4) \u2264 lim sup\n\u03b1\u2032\u2193\u03b1,\u03c4 \u2032\u2191\u03c4 \u03c8m(\u03b1\n\u2032, \u03c4 \u2032) + om(1). (39)\nWe next prove that \u03c8m is upper semicontinuous. Define\ng(\u03c4,QXW ) := inf QY |XW :EQXW QY |XW c(X,Y )\u2264\u03c4\nD(QY |W \u2016PY |QW ).\nIt is easy to see that g(\u03c4,QXW ) is convex in (\u03c4,QXW ). Since given m, QXW is defined on a finite alphabet, QXW is in a probability simplex (which is relatively compact). Hence, g(\u03c4,QXW ) is upper semicontinuous in (\u03c4,QXW ). By definition,\n\u03c8m(\u03b1, \u03c4) = sup QXW :D(QX|W \u2016PX |QW )\u2264\u03b1 g(\u03c4,QXW )\nLet (\u03b1k, \u03c4k) be a sequence converging (\u03b1, \u03c4) as k \u2192 \u221e. For each k, let Q(k)XW attain (or approximately attain) \u03c8m(\u03b1k, \u03c4k). Hence, passing to a subsequence, we can obtain a convergent sequence (Q (k) XW ), which is assumed to converge to Q\u2217XW . Hence,\nD(Q\u2217X|W \u2016PX |Q\u2217W ) = lim k\u2192\u221e D(Q (k) X|W \u2016PX |Q (k) W ) \u2264 lim k\u2192\u221e \u03b1k = \u03b1.\nMoreover,\ng(\u03c4,Q\u2217XW ) \u2265 lim sup k\u2192\u221e g(\u03c4k, Q (k) XW ) = lim sup k\u2192\u221e \u03c8m(\u03b1k, \u03c4k).\nTherefore,\n\u03c8m(\u03b1, \u03c4) = sup QXW :D(QX|W \u2016PX |QW )\u2264\u03b1 g(\u03c4,QXW )\n\u2265 g(\u03c4,Q\u2217XW ) \u2265 lim sup\nk\u2192\u221e \u03c8m(\u03b1k, \u03c4k).\nHence, \u03c8m is upper semicontinuous. By (39) and the upper semicontinuity of \u03c8m, and letting m\u2192\u221e, we obtain\nlim sup n\u2192\u221e E (n) 0 (\u03b1, \u03c4) \u2264 limm\u2192\u221e\u03c8m(\u03b1, \u03c4) = \u03c8(\u03b1, \u03c4)."
        },
        {
            "heading": "5.1.2 Compact X",
            "text": "We next consider compact X . We first introduce a result implied by Assumption 2. By choosing QX , Q\u2032X , QY as Dirac measures \u03b4x, \u03b4x\u2032 , \u03b4y in Assumption 2 and by the fact that L(\u03b4x, \u03b4x\u2032) = d(x, x\n\u2032) when d(x, x\u2032) \u2264 1, there exists a function \u03b4(\u01eb) : (0,\u221e)\u2192 (0,\u221e) vanishing as \u01eb \u2193 0 such that\ninf x\u2032:d(x,x\u2032)\u2264\u01eb\nc(x\u2032, y) \u2265 c(x, y)\u2212 \u03b4(\u01eb) (40)\nholds for all (x, y). In other words, infx\u2032:d(x,x\u2032)\u2264\u01eb c(x \u2032, y)\u2192 c(x, y) as \u01eb \u2193 0 uniformly for all (x, y). Note that without Assumption 2, Equation (40) is still true if c = d.\nSince X is compact, for any r > 0, it can be covered by a finite number of open balls {Br(xi)}ki=1. Denote Ei := Br(xi)\\ \u22c3i\u22121 j=1Br(xj), i \u2208 [k], which are measurable. Hence, {Ei} k i=1 forms a partition of X , and Ei is a subset of Br(xi). For each i, we choose a point zi \u2208 Ei. Consider Z := {z1, z2, \u00b7 \u00b7 \u00b7 , zk} as a sample space, and define a probability mass function PZ on Z given by PZ(zi) = PX(Ei), \u2200i \u2208 [k]. In other words, Z \u223c PZ is a quantized version of X \u223c PX in the sense that Z = zi if X \u2208 Ei for some i.\nFor a vector in := (i1, i2, ..., in) \u2208 [k]n, denote Ein := \u220fn l=1Eil . Consequently, {Ein : in \u2208 [k]n} forms a partition of Xn. Similarly, for Xn \u223c P\u2297nX , we denote Zn as a random vector where Zi is the quantized version of Xi, i \u2208 [n]. Obviously, Zn \u223c P\u2297nZ .\nFor any measurable set A \u2286 Xn, denote I := {in \u2208 [k]n : Ein \u2229 A 6= \u2205}. Denote A\u0302 := \u22c3 in\u2208I Ein which is a superset of A, i.e., A \u2286 A\u0302. On the other hand, for each in \u2208 I and any \u03c4\u0302 > 0, the t\u0302-enlargement of Ein with t\u0302 := n\u03c4\u0302 satisfies that\nE t\u0302in = {yn : cn(xn, yn) \u2264 t\u0302, \u2203xn \u2208 Ein} \u2286 {yn : cn(xn, yn) \u2264 t\u0302, d(xi, x\u0302i) \u2264 r, \u2200i \u2208 [n], \u2203x\u0302n \u2208 A, \u2203xn \u2208 Xn} (41) = {yn : inf\nxn:d(xi,x\u0302i)\u2264r,\u2200i\u2208[n] cn(x\nn, yn) \u2264 t\u0302, \u2203x\u0302n \u2208 A}\n= {yn : n \u2211\ni=1\ninf xi:d(xi,x\u0302i)\u2264r\nc(xi, yi) \u2264 t\u0302, \u2203x\u0302n \u2208 A}\n\u2286 {yn : n \u2211\ni=1\nc(x\u0302i, yi) \u2264 n(\u03c4\u0302 + \u03b4(r)), \u2203x\u0302n \u2208 A} (42)\n= An(\u03c4\u0302+\u03b4(r)),\nwhere\n\u2022 (41) follows from the fact that \u2203xn \u2208 Ein implies d(xi, x\u0302i) \u2264 r, \u2200i \u2208 [n] for some x\u0302n \u2208 A, xn \u2208 Xn;\n\u2022 in (42) \u03b4(r) is a positive function of r which vanishes as r \u2193 0, and (42) follows by (40). Hence,\nA\u0302t\u0302 = \u22c3\nin\u2208I E t\u0302in \u2286 An(\u03c4\u0302+\u03b4(r))\nIf we choose \u03c4\u0302 = \u03c4 \u2212 \u03b4(r), then A\u0302n\u03c4\u0302 \u2286 An\u03c4 . Combining this with A \u2286 A\u0302 implies\nP\u2297nY (A n\u03c4 ) \u2265 P\u2297nY (A\u0302n\u03c4\u0302 )\nP\u2297nX (A) \u2264 P\u2297nX (A\u0302),\nwhich further imply that\ninf A:P\u2297nX (A)\u2265a\nP\u2297nY (A n\u03c4 ) \u2265 inf A:P\u2297nX (A\u0302)\u2265a P\u2297nY (A\u0302 n\u03c4\u0302 )\n= inf I\u2286[k]n:P\u2297nX ( \u22c3 in\u2208I Ein)\u2265a\nP\u2297nY (( \u22c3\nin\u2208I Ein)\nn\u03c4\u0302 )\n= inf B\u2286Zn:P\u2297nZ (B)\u2265a\nP\u2297nY (B n\u03c4\u0302 ),\nwhere Bn\u03c4\u0302 = {yn : cn(zn, yn) \u2264 n\u03c4\u0302 , \u2203zn \u2208 B}. Therefore,\nE (n) 0 (\u03b1, \u03c4 |PX ) \u2264 E (n) 0 (\u03b1, \u03c4\u0302 |PZ),\nwhere E (n) 0 (\u00b7, \u00b7|PX) is the exponent E (n) 0 defined for distribution pair (PX , PY ), and E (n) 0 (\u00b7, \u00b7|PZ) is the exponent E (n) 0 defined for (PZ , PY ).\nDenote \u03c8(\u00b7, \u00b7|PX) as the function \u03c8 defined for (PX , PY ), and \u03c8(\u00b7, \u00b7|PZ) as the one defined for (PZ , PY ). Since Z is a finite metric space (with discrete metric), by the result proven in Section 5.1.1, we have\nlim sup n\u2192\u221e\nE (n) 0 (\u03b1, \u03c4\u0302 |PZ) \u2264 \u03c8(\u03b1, \u03c4\u0302 |PZ).\nTherefore,\nlim sup n\u2192\u221e\nE (n) 0 (\u03b1, \u03c4 |PX ) \u2264 \u03c8(\u03b1, \u03c4\u0302 |PZ) = \u03c8(\u03b1, \u03c4 \u2212 \u03b4(r)|PZ ). (43)\nWe next show that \u03c8(\u03b1\u2032, \u03c4 \u2032 + \u03b4(r)|PZ ) \u2264 \u03c8(\u03b1\u2032, \u03c4 \u2032|PX) for any \u03b1\u2032 \u2265 0, \u03c4 \u2032 > 0. For any QZ|W , we define a mixture distribution QX|W such that for each w,\nQX|W=w = k \u2211\ni=1\nQZ|W (zi|w)PX(\u00b7|Ei),\nwhich implies\ndQX|W dPX\n(x|w) = k \u2211\ni=1\nQZ|W (zi|w) 1Ei(x)\nPX(Ei) =\nk \u2211\ni=1\nQZ|W (zi|w) 1Ei(x)\nPZ (zi) , \u2200x. (44)\nFor such QX|W , D(QX|W \u2016PX |QW ) = D(QZ|W \u2016PZ |QW ). (45) Note that for such a construction, Z \u223c QZ can be seen as a quantized version of X \u223c QX . By (40), we have that c(X,Y ) \u2265 c(Z, Y ) \u2212 \u03b4(r) a.s. where Z is the quantized version of (and also a function of) X . We hence have that for QX|W constructed above,\nC(QX|W , QY |W |QW ) = min QXY |W\u2208\u03a0(QX|W ,QY |W ) EQWQXY |W [c(X,Y )]\n\u2265 min QXY |W\u2208\u03a0(QX|W ,QY |W ) EQWQXY |W [c(Z, Y )]\u2212 \u03b4(r) \u2265 min QZY |W\u2208\u03a0(QZ|W ,QY |W ) EQWQZY |W [c(Z, Y )]\u2212 \u03b4(r) = C(QZ|W , QY |W |QW )\u2212 \u03b4(r).\nTherefore,\ninf QY |W :C(QX|W ,QY |W |QW )\u2264\u03c4 \u2032\nD(QY |W \u2016PY |QW )\n\u2265 inf QY |W :C(QZ|W ,QY |W |QW )\u2264\u03c4 \u2032+\u03b4(r) D(QY |W \u2016PY |QW ).\nTaking supremum over QZW such that D(QZ|W \u2016PZ |QW ) \u2264 \u03b1\u2032, we obtain\nsup QZW :D(QZ|W \u2016PZ |QW )\u2264\u03b1\u2032 inf QY |W :C(QX|W ,QY |W |QW )\u2264\u03c4 \u2032\nD(QY |W \u2016PY |QW ) \u2265 \u03c8(\u03b1\u2032, \u03c4 \u2032 + \u03b4(r)|PZ )\nwhere QX|W at the LHS above is induced by QZ|W as shown in (44). By (45), the LHS above is in turn upper bounded by \u03c8(\u03b1\u2032, \u03c4 \u2032|PX) (by replacing the supremum above with the supremum over QXW such that D(QX|W \u2016PX |QW ) \u2264 \u03b1\u2032). Hence,\n\u03c8(\u03b1\u2032, \u03c4 \u2032 + \u03b4(r)|PZ ) \u2264 \u03c8(\u03b1\u2032, \u03c4 \u2032|PX).\nFor \u03c4 > 2\u03b4(r) (when \u03c4 > 0 and r is sufficiently small), substituting \u03b1\u2032 \u2190 \u03b1, \u03c4 \u2032 \u2190 \u03c4 \u2212 2\u03b4(r) into the above inequality, we have \u03c8(\u03b1, \u03c4 \u2212 \u03b4(r)|PZ ) \u2264 \u03c8(\u03b1, \u03c4 \u2212 2\u03b4(r)|PX). (46) Combining (43) and (46) and letting r \u2193 0, we have\nlim sup n\u2192\u221e\nE (n) 0 (\u03b1, \u03c4 |PX ) \u2264 lim \u03c4 \u2032\u2191\u03c4 \u03c8(\u03b1, \u03c4 \u2032|PX)."
        },
        {
            "heading": "5.2 Lower Bound",
            "text": "The proof for the lower bound is similar to that for Statement 1 of Theorem 3 given in Section 3. Let \u01eb > 0. Let QWX be such that |supp(QW )| < \u221e and D(QX|W \u2016PX |QW ) \u2264 \u03b1 \u2212 \u01eb. Without loss of generality, we assume supp(QW ) = [m]. For each n, let Q (n) W be an empirical measure of an n-length sequence (i.e., n-type) such that supp(Q (n) W ) \u2286 [m] and Q (n) W \u2192 QW as n \u2192 \u221e. Let Q (n) XW = Q (n) W QX|W . Let wn = (1, \u00b7 \u00b7 \u00b7 , 1, 2, \u00b7 \u00b7 \u00b7 , 2, \u00b7 \u00b7 \u00b7 ,m, \u00b7 \u00b7 \u00b7 ,m) be an n-length sequence, where i appears ni := nQ(n)W (i) times. Hence, the empirical measure of wn is Q\n(n) W . Let \u01eb \u2032 > 0. Denote Aw := B\u01eb\u2032](QX|W=w) for w \u2208 [m], and A := {RX|W : RX|W=w \u2208 Aw, \u2200w \u2208 [m]}. Denote\nA = {xn : Lxn|wn \u2208 A} = m \u220f\nw=1\n{xnw : Lxnw \u2208 Aw} .\nAs shown in Section 3, A is closed in Xn, and \u2212 1n logP\u2297nX (A) \u2264 \u03b1 for all sufficiently large n. Denote t = n\u03c4 . Observe that\nAt = { yn : \u2203xn, Lxn|wn \u2208 A, cn(xn, yn) \u2264 t }\n= { yn : \u2203xn, Lxn|wn \u2208 A, ELxn,yn,wn c(X,Y ) \u2264 \u03c4 } \u2286 { yn : \u2203xn, Lxn|wn \u2208 A, C(Lxn|wn ,Lyn|wn |Lwn) \u2264 \u03c4 } \u2286 {\nyn : \u2203RX|W \u2208 A, C(RX|W ,Lyn|wn |Q(n)W ) \u2264 \u03c4 } .\nHence, we have\nAt \u2286 { yn : Lyn|wn \u2208 B } ,\nwhere B = {RY |W : C(RX|W , RY |W |Q(n)W ) \u2264 \u03c4, \u2203RX|W \u2208 A}. (47)\nWe next compute the exponent of P\u2297nY (At) by using Sanov\u2019s theorem. To this end, we first convert the probability P\u2297nY (At) to a probability of the (random) empirical measure LY nWn with (Y n,Wn) \u223c P\u2297nY \u2297Q\u2297nW . Define\nB := {(wn, yn) : Lyn|wn \u2208 B,Lwn = Q(n)W }.\nNote that for distinct wn, w\u0302n with the same empirical measure Q (n) W , there is a permutation \u03c3 such that w\u0302n =\u03c3(wn). Moreover, the set {(w\u0302n, yn) : Lyn|w\u0302n \u2208 B} is the resultant set by permuting each elements in {(wn, yn) : Lyn|wn \u2208 B} via \u03c3. On the other hand, the distribution P\u2297nY \u2297Q\u2297nW is permutation-invariant (or exchangeable). Hence, we have\nP\u2297nY \u2297Q\u2297nW {(w\u0302n, yn) : Lyn|w\u0302n \u2208 B} = P\u2297nY \u2297Q\u2297nW {(wn, yn) : Lyn|wn \u2208 B}.\nMoreover, the events in the probability at the LHS and RHS are mutually exclusive. Hence, for any wn with empirical measure Q (n) W ,\nP\u2297nY \u2297Q\u2297nW {(wn, yn) : Lyn|wn \u2208 B} = 1 |{wn : Lwn = Q(n)W }| P\u2297nY \u2297Q\u2297nW (B).\nFurthermore, given wn, the LHS above is Q\u2297nW (w n)P\u2297nY { yn : Lyn|wn \u2208 B } . Hence,\nP\u2297nY { yn : Lyn|wn \u2208 B } = 1 |{wn : Lwn = Q(n)W }| \u00b7Q\u2297nW (wn) P\u2297nY \u2297Q\u2297nW (B)\n= 1\nQ\u2297nW {wn : Lwn = Q (n) W }\nP\u2297nY \u2297Q\u2297nW (B).\nNote that W is finite. By the finite alphabet version of Sanov\u2019s theorem [7],\nlim n\u2192\u221e \u2212 1 n logQ\u2297nW {wn : Lwn = Q (n) W } = 0.\nHence,\nlim inf n\u2192\u221e \u2212 1 n logP\u2297nY (Bwn) = lim infn\u2192\u221e \u2212 1 n logP\u2297nY \u2297Q\u2297nW (B) =: E.\nWe next estimate E. For sufficiently large n, Q (n) W belongs to B\u01eb\u2032](QW ). So,\nB \u2286 {(wn, yn) : Lyn|wn \u2208 B,Lwn \u2208 B\u01eb\u2032](QW )}.\nBy Sanov\u2019s theorem,\nE \u2265 inf RWY \u2208B\u2032 D(RYW \u2016PY \u2297QW ), (48)\nwhere B\u2032 := {RWY : RW \u2208 B\u01eb\u2032](QW ), QY |W \u2208 B}. To simplify this lower bound, denoting\nB\u0302 := {RY |W : C(QX|W , RY |W |QW ) \u2264 \u03c4 + 2\u01eb},\nwe claim that for sufficiently small \u01eb\u2032, it holds that\nB\u2032 \u2286 B\u0302\u2032 := {RWY : RW \u2208 B\u01eb\u2032](QW ), RY |W \u2208 B\u0302}, (49)\nand B\u0302\u2032 is closed. We next prove this claim. By Assumption 2, for any RY |W , it holds that given \u01eb \u2032\u2032 > 0, for sufficiently small \u01eb\u2032,\ninf RX|W\u2208A\nC(RX|W , RY |W |Q(n)W ) \u2265 C(QX|W , RY |W |Q (n) W )\u2212 \u01eb\u2032\u2032.\nNote that the minimization in the conditional optimal transport can be taken in a pointwise way for each condition W = w. Combining this with the condition that c is bounded, we have that RW 7\u2192 C(RX|W , RY |W |RW ) is continuous. So, given \u01eb\u2032\u2032 > 0, for sufficiently large n,\nC(QX|W , RY |W |Q(n)W ) \u2265 C(QX|W , RY |W |QW )\u2212 \u01eb\u2032\u2032.\nThis implies that given \u01eb\u2032\u2032, for sufficiently small \u01eb\u2032, B \u2286 B\u0302. (Recall B is defined in (47).) Hence, B\u2032 \u2286 B\u0302\u2032. We next prove that for sufficiently small \u01eb, B\u0302\u2032 is closed. Let (R(k)WY ) be an arbitrary sequence drawn from B\u0302\u2032, which converges to R\u2217WY (under the weak topology). Obviously, R (k) W \u2192 R\u2217W = QW and R (k) Y |W=w \u2192 R\u2217Y |W=w for each w. By the lower semi-continuity of RY 7\u2192 C(RX , RY ), we have that\nlim inf k\u2192\u221e\nC(QX|W=w, R (k) Y |W=w) \u2265 C(QX|W=w, R\u2217Y |W=w).\nHence,\nlim inf k\u2192\u221e\nC(QX|W , R (k) Y |W |QW ) \u2265 C(QX|W , R\u2217Y |W |QW ).\nOn the other hand, by the choice of (R (k) WY ), C(QX|W , R (k) Y |W |QW ) \u2264 \u03c4 +2\u01eb\u2032\u2032. Hence, C(QX|W , R\u2217Y |W |QW ) \u2264 \u03c4 + 2\u01eb\u2032\u2032. That is, R\u2217WY \u2208 B\u0302\u2032. Hence, B\u0302\u2032 is closed. This completes the proof of the claim above. By (49) and (48),\nE \u2265 inf RWY \u2208B\u0302\u2032 D(RYW \u2016PY \u2297QW )\n= inf RWY :RW\u2208B\u01eb\u2032](QW ),RY |W\u2208B\u0302\nD(RY |W \u2016PY |RW ) +D(RW \u2016QW )\nLetting \u01eb\u2032 \u2193 0 and by the continuity of RW \u2208 P([m]) 7\u2192 D(RW \u2016QW ), we obtain\nE \u2265 \u03b2 := lim \u01eb\u2032\u21930 inf RW\u2208B\u01eb\u2032](QW ),RY |W :C(QX|W ,RY |W |QW )\u2264\u03c4+2\u01eb\u2032\u2032 D(RY |W \u2016PY |QW )\nLet (R (k) W , R (k) Y |W ) be such that\nR (k) W \u2208 B 1k ](QW ), C(QX|W , R (k) Y |W |QW ) \u2264 \u03c4 + 2\u01eb\u2032\u2032,\nD(R (k) Y |W \u2016PY |QW ) \u2264 \u03b2 +\n1 k .\nSince R (k) W is in the probability simplex, by passing to a subsequence, we assume R (k) W \u2192 QW . Since sublevel sets of the relative entropy RY 7\u2192 D(RY \u2016PY ) are compact, by the fact that for each w, D(RY |W=w\u2016PY ) is finite, passing to a subsequence, we have R\n(k) Y |W=w \u2192 R\u2217Y |W=w. By the lower semi-continuity of the relative\nentropy and the optimal transport cost functional, we have\nlim inf k\u2192\u221e\nD(R (k) Y |W \u2016PY |QW ) \u2265 D(R\u2217Y |W \u2016PY |QW ),\nlim inf k\u2192\u221e\nC(QX|W , R (k) Y |W |QW ) \u2265 C(QX|W , R\u2217Y |W |QW ).\nHence, R\u2217Y |W satisfies that\nC(QX|W , R \u2217 Y |W |QW ) \u2264 \u03c4 + 2\u01eb\u2032\u2032\nD(R\u2217Y |W \u2016PY |QW ) \u2264 \u03b2.\nTherefore, E \u2265 g(\u03c4 + 2\u01eb\u2032\u2032, QXW ), where\ng(t, QXW ) := inf QY |W :C(QX|W ,QY |W |QW )\u2264t\nD(QY |W \u2016PY |QW )\n= inf QY |XW :E[c(X,Y )]\u2264t\nD(QY |W \u2016PY |QW ).\nSince QXW is arbitrary distribution on X \u00d7W satisfying D(QX|W \u2016PX |QW ) \u2264 \u03b1\u2212 \u01eb, taking supremum over all such distributions, we obtain\nlim inf n\u2192\u221e\nE (n) 0 (\u03b1, \u03c4) \u2265 sup QXW :D(QX|W \u2016PX |QW )\u2264\u03b1\u2212\u01eb g(\u03c4 + 2\u01eb\u2032\u2032, QXW )\n= \u03c8W(\u03b1 \u2212 \u01eb, \u03c4 + 2\u01eb\u2032\u2032). where \u03c8W is defined similarly as \u03c8 in (16) but with the alphabet W fixed. Letting \u01eb \u2193 0 and \u01eb\u2032\u2032 \u2193 0, we obtain\nlim inf n\u2192\u221e\nE (n) 0 (\u03b1, \u03c4) \u2265 lim\n\u01eb\u21930 lim \u01eb\u2032\u2032\u21930\n\u03c8W(\u03b1\u2212 \u01eb, \u03c4 + 2\u01eb\u2032\u2032)\n= sup \u01eb,\u01eb\u2032\u2032>0\n\u03c8W(\u03b1\u2212 \u01eb, \u03c4 + 2\u01eb\u2032\u2032).\nNote that given W , sup \u01eb\u2032\u2032>0 \u03c8W(\u03b1\u2212 \u01eb, \u03c4 + 2\u01eb\u2032\u2032)\n= sup QXW :D(QX|W \u2016PX |QW )\u2264\u03b1\u2212\u01eb sup \u01eb\u2032\u2032>0\ng(\u03c4 + 2\u01eb\u2032\u2032, QXW )\n= sup QXW :D(QX|W \u2016PX |QW )\u2264\u03b1\u2212\u01eb lim \u01eb\u2032\u2032\u21930 inf QY |XW :E[c(X,Y )]\u2264\u03c4+\u01eb\nD(QY |W \u2016PY |QW )\n= sup QXW :D(QX|W \u2016PX |QW )\u2264\u03b1\u2212\u01eb inf QY |XW :E[c(X,Y )]\u2264\u03c4\nD(QY |W \u2016PY |QW ) (50)\n= \u03c8W(\u03b1\u2212 \u01eb, \u03c4), where (50) follows since given QXW , \u03c4 7\u2192 infQY |XW :E[c(X,Y )]\u2264\u03c4 D(QY |W \u2016PY |QW ) is convex, and hence, continuous on (0,+\u221e). Hence,\nlim inf n\u2192\u221e\nE (n) 0 (\u03b1, \u03c4) \u2265 sup\nfinite W sup \u01eb>0\n\u03c8W(\u03b1\u2212 \u01eb, \u03c4)\n= sup \u01eb>0 sup finite W\n\u03c8W(\u03b1\u2212 \u01eb, \u03c4)\n= lim \u01eb\u21930\n\u03c8(\u03b1\u2212 \u01eb, \u03c4)."
        },
        {
            "heading": "6 Proofs of Dual Formulas",
            "text": "It is well known that the OT cost admits the following duality.\nLemma 5 (Kantorovich Duality). [27, Theorem 5.10] It holds that\nC(QX , QY ) = sup (f,g)\u2208Cb(X )\u00d7Cb(Y):f+g\u2264c QX(f) +QY (g),\nwhere Cb(X ) denotes the collection of bounded continuous functions f : X \u2192 R. We also need the following duality for the I-projection, which is well-known if the space is Polish since both sides in (51) correspond to the large deviation exponent.\nLemma 6 (Duality for the I-Projection). Let f : X \u2192 R be a measurable bounded above function. Then, it holds that for any real \u03c4 ,\ninf Q:Q(f)\u2265\u03c4 D(Q\u2016P ) = sup \u03bb\u22650 \u03bb\u03c4 \u2212 logP (e\u03bbf ), (51)\nand for any real \u03b1 \u2265 0, sup\nQ:D(Q\u2016P )\u2264\u03b1 Q(f) = inf \u03b7>0 \u03b7\u03b1+ \u03b7 logP (e(1/\u03b7)f ). (52)\nThe sup\u03bb\u22650 in (51) can be replaced by sup\u03bb>0.\nThis lemma is a direct consequence of the following lemma. The following lemma can be easily verified by definition.\nLemma 7. [8] For a measurable bounded above function f and \u03bb \u2265 0, define a probability measure Q\u03bb with density\ndQ\u03bb dP = e\u03bbf P (e\u03bbf ) ,\nthen\nD(Q\u2016P )\u2212D(Q\u03bb\u2016P ) = D(Q\u2016Q\u03bb) + \u03bb (Q(f)\u2212Q\u03bb(f)) \u2265 \u03bb (Q(f)\u2212Q\u03bb(f)) .\nThe function f in Lemmas 6 and 7 can be assumed to be unbounded, but P (e\u03bbf ) should be finite for Lemma 7, P (e\u03bbf ) should be finite for \u03bb \u2265 0 such that Q\u03bb(f) = \u03c4 for (51) in Lemma 6, and P (e(1/\u03b7)f ) should be finite for \u03b7 > 0 such that D(Q1/\u03b7\u2016P ) = \u03b1 for (52) in Lemma 6,\nThe conditional version of Lemma 6 is as follows.\nLemma 8. Let W be a finite set and f : X \u00d7W \u2192 R be a measurable bounded above function. Let PW be a probability measure on W. Then, for any real \u03c4 , it holds that\ninf QX|W :PWQX|W (f)\u2265\u03c4 D(QX|W \u2016PX|W |PW ) = sup \u03bb\u22650 \u03bb\u03c4 \u2212 PW (logPX|W (e\u03bbf )),\nand for any real \u03b1 \u2265 0, it holds that\nsup QX|W :D(QX|W \u2016PX|W |QW )\u2264\u03b1 PWQX|W (f) = inf \u03b7>0\n\u03b7\u03b1+ \u03b7PW (logPX|W (e (1/\u03b7)f )).\nBased on the duality lemmas above, we prove Theorem 6.\nProof of Theorem 6. By the definition of \u03c6\u2265 and by the Kantorovich duality,\n\u03c6\u2265(\u03b1, \u03c4) = inf QX ,QY ,f,g:f+g\u2264c, QX(f)+QY (g)\u2265\u03c4, D(QX\u2016PX )\u2264\u03b1\nD(QY \u2016PY )\n= inf QX ,f,g:f+g\u2264c, D(QX\u2016PX )\u2264\u03b1 inf QY :QX (f)+QY (g)\u2265\u03c4\nD(QY \u2016PY ). (53)\nBy Lemma 6,\n\u03c6\u2265(\u03b1, \u03c4) = inf f,g:f+g\u2264c, inf QX :D(QX\u2016PX )\u2264\u03b1 sup \u03bb>0\n\u03bb(\u03c4 \u2212QX(f))\u2212 logPY (e\u03bbg). (54)\nThe objective function in (54) is linear in \u03bb and also linear in QX , and moreover, {QX : D(QX\u2016PX) \u2264 \u03b1} is compact. So, by the minimax theorem, the second infimization and the supremization can be swapped [30, Theorem 2.10.2]. Hence, the inf-sup part in (53) is equal to\nsup \u03bb>0 inf QX :D(QX\u2016PX )\u2264\u03b1\n\u03bb(\u03c4 \u2212QX(f))\u2212 logPY (e\u03bbg).\nwhich by Lemma 6, can be rewritten as\nsup \u03bb>0 \u03bb(\u03c4 \u2212 inf \u03b7>0 (\u03b7\u03b1+ \u03b7 logPX(e (1/\u03b7)f )))\u2212 logPY (e\u03bbg).\nSubstituting this into (54) completes the proof.\nProof of Proposition 1. By the Kantorovich\u2013Rubinstein formula [27, (5.11)],\n\u03d5X,\u2265(\u03c4) = inf QX ,1-Lip f :PX (f)=0, QX(f)\u2265\u03c4\nD(QX\u2016PX)\n= inf 1-Lip f :PX(f)=0 inf QX sup \u03bb\u22650\nD(QX\u2016PX) + \u03bb(\u03c4 \u2212QX(f))\n= inf 1-Lip f :PX(f)=0 sup \u03bb\u22650 inf QX\nD(QX\u2016PX) + \u03bb(\u03c4 \u2212QX(f))\n= inf 1-Lip f :PX(f)=0 sup \u03bb\u22650\n\u03bb\u03c4 \u2212 logPX(e\u03bbf ).\nProof of Theorem 7. It is easy to see that \u03d5\u0306X,\u2265(\u03c4) = \u03d5\u0306X(\u03c4). If we swap the inf and sup in (18), then we will obtain r(\u03c4). However, this is infeasible in general.\nObviously, from (18), \u03d5X,\u2265(\u03c4) \u2265 r(\u03c4), and by definition, r(\u03c4) is convex. So, taking the lower convex envelope, we obtain \u03d5\u0306X,\u2265(\u03c4) \u2265 r(\u03c4). It remains to prove \u03d5\u0306X,\u2265(\u03c4) \u2264 r(\u03c4). We next do this.\nBy [4, Theorem 3.10], given any \u03c4 \u2265 0, there is a \u03bb\u2217 such that r(\u03c4) = \u03bb\u2217\u03c4\u2212LG(\u03bb\u2217). Because the function \u03bb 7\u2192 \u03bb\u03c4 \u2212 LG(\u03bb) has a maximum at \u03bb\u2217, its right derivative at \u03bb\u2217 is at most 0, and its left derivative is at least 0. In other words, we have LlG(\u03bb\n\u2217) \u2264 \u03c4 \u2264 LrG(\u03bb\u2217). Because LrG(\u03bb\u2217) \u2265 \u03c4 , there must be a function g : X \u2192 R such that Lg(\u03bb\u2217) = LG(\u03bb\u2217) and L\u2032g(\u03bb\u2217) \u2265 \u03c4 . Because LlG(\u03bb\u2217) \u2264 \u03c4 , there must be a function h : X \u2192 R such that Lh(\u03bb\u2217) = LG(\u03bb\u2217) and L\u2032h(\u03bb\u2217) \u2264 \u03c4 . Hence for any \u01eb > 0, there are positive integer n and nonnegative integer k such that |\u03c4\u0302 \u2212 \u03c4 | \u2264 \u01eb, where\n\u03c4\u0302 := pL\u2032g(\u03bb \u2217) + (1 \u2212 p)L\u2032h(\u03bb\u2217)\nand p = kn .\nLet Xn \u223c P\u2297nX . Denote f : Xn \u2192 R by\nf(xn) =\nk \u2211\ni=1\ng(xi) +\nn \u2211\ni=k+1\nh(xi).\nSince g, h are 1-Lipschitz, so is f (on the product space). Then, for any \u03bb \u2265 0,\nLf (\u03bb) = kLg(\u03bb) + (n\u2212 k)Lh(\u03bb).\nThen,\nr(\u03c4) = \u03bb\u2217\u03c4 \u2212 LG(\u03bb\u2217) \u2264 \u03bb\u2217\u03c4\u0302 \u2212 (pLg(\u03bb\u2217) + (1\u2212 p)Lh(\u03bb\u2217)) + \u03bb\u2217\u01eb = sup\n\u03bb\u22650 \u03bb\u03c4\u0302 \u2212 (pLg(\u03bb) + (1 \u2212 p)Lh(\u03bb)) + \u03bb\u2217\u01eb (55)\n= sup \u03bb\u22650 \u03bb\u03c4\u0302 \u2212 1 n Lf(\u03bb) + \u03bb \u2217\u01eb\n\u2265 inf 1-Lip f\u0302 :P\u2297nX (f\u0302)=0 sup \u03bb\u22650\n\u03bb\u03c4\u0302 \u2212 1 n Lf\u0302 (\u03bb) + \u03bb \u2217\u01eb\n= 1\nn \u03d5n(n\u03c4\u0302) + \u03bb\n\u2217\u01eb (56)\n\u2265 \u03d5\u0306X,\u2265(\u03c4\u0302 ) + \u03bb\u2217\u01eb, (57)\nwhere\n\u2022 (55) follows since the objective function in it is strictly convex in \u03bb and its derivative is zero at \u03bb\u2217;\n\u2022 \u03d5n in (56) given by\n\u03d5n(t) = inf QXn\u2208P(Xn):C(P\u2297nX ,QXn )\u2265t\nD(QXn\u2016P\u2297nX )\nis the n-dimensional extension of \u03d5X,\u2265, and (56) follows by Proposition 1 for the n-dimensional version \u03d5n;\n\u2022 (57) follows the single-letterization argument same to that used for (21).\nLastly, letting \u01eb\u2192 0, we have \u03c4\u0302 \u2192 \u03c4 . By the continuity of \u03d5\u0306X,\u2265 and (57), we have r(\u03c4) \u2265 \u03d5\u0306X,\u2265(\u03c4).\nProof of Theorem 8. We first give a dual formula for\n\u03b8(\u03c4,QXW ) := inf QY |W :C(QX|W ,QY |W |QW )\u2264\u03c4\nD(QY |W \u2016PY |QW ).\nObserve that\n\u03b8(\u03c4,QXW ) = inf QY |W :C(QX|W ,QY |W |QW )\u2264\u03c4\nD(QY |W \u2016PY |QW )\n= inf QY |W sup \u03bb\u22650\nD(QY |W \u2016PY |QW ) + \u03bb(C(QX|W , QY |W |QW )\u2212 \u03c4)\n= sup \u03bb\u22650 inf QY |W\nD(QY |W \u2016PY |QW ) + \u03bb(EQW [C(QX|W (\u00b7|W ), QY |W (\u00b7|W ))]\u2212 \u03c4)\n= sup \u03bb\u22650 inf QY |W EQW [D(QY |W (\u00b7|W )\u2016PY ) + \u03bb( sup f+g\u2264c QX|W (f |W ) +QY |W (g|W )\u2212 \u03c4)] (58)\n= sup \u03bb\u22650\n\u2211\nw\nQW (w)[ inf QY |W=w sup f+g\u2264c\nD(QY |W=w\u2016PY )\n+ \u03bb(QX|W=w(f) +QY |W=w(g)\u2212 \u03c4)] (59) = sup\n\u03bb\u22650\n\u2211\nw\nQW (w)[ sup f+g\u2264c inf QY |W=w\nD(QY |W=w\u2016PY )\n+ \u03bb(QX|W=w(f) +QY |W=w(g)\u2212 \u03c4)] (60) = sup\n\u03bb\u22650 sup fw+gw\u2264c,\u2200w EQW\n[ \u03bb(QX|W (fW )\u2212 \u03c4) \u2212 logPY (e\u2212\u03bbgW ) ] , (61)\nwhere\n\u2022 (58) follows by the Kantorovich duality, in (59) infQY |W is taken in a pointwise way;\n\u2022 the inf and sup are swapped in (60) which follows by the general minimax theorem [20, Theorem 5.2.2] by identifying that 1) the optimal value of the sup-inf in (60) is finite (since upper bounded by \u03bb(C(QX|W=w, PY )\u2212 \u03c4)), and 2) by choosing f, g as zero functions, the objective subfunction turns to be QY |W=w 7\u2192 D(QY |W=w\u2016PY )\u2212 \u03bb\u03c4 whose sublevels are compact under the weak topology;\n\u2022 (61) follows by Lemma 7 (and the supremum over f, g is moved outside of the expectation).\nSubstituting this dual formula for \u03b8 to \u03c8, we obtain\n\u03c8(\u03b1, \u03c4) = sup QXW :D(QX|W \u2016PX |QW )\u2264\u03b1 \u03b8(\u03c4,QXW )\n= sup \u03bb\u22650 sup fw+gw\u2264c,\u2200w sup QXW :D(QX|W \u2016PX |QW )\u2264\u03b1 EQW\n[ \u03bb(QX|W (fW )\u2212 \u03c4) \u2212 logPY (e\u2212\u03bbgW ) ]\n= sup \u03bb\u22650 sup fw+gw\u2264c,\u2200w\u2208{0,1} sup QX|W ,p\u2208[0,1]:D(QX|W \u2016PX |Bern(p))\u2264\u03b1\nEW\u223cBern(p) [ \u03bb(QX|W (fW )\u2212 \u03c4)\u2212 logPY (e\u2212\u03bbgW ) ]\n(62)\n= sup \u03bb\u22650 sup fw+gw\u2264c,\u2200w\u2208{0,1} sup p\u2208[0,1] inf \u03b7>0 \u03b7\u03b1+\n\u03b7EW\u223cBern(p) logPX(e (1/\u03b7)(\u03bb(fW \u2212\u03c4)\u2212logPY (e\u2212\u03bbgW ))) (63)\n= sup \u03bb\u22650 sup fw+gw\u2264c,\u2200w\u2208{0,1} inf \u03b7>0 \u03b7\u03b1+ \u03b7 max w\u2208{0,1}\nlogPX(e (1/\u03b7)(\u03bb(fw\u2212\u03c4)\u2212logPY (e\u2212\u03bbgw ))) (64)\n= sup fw+gw\u2264c,\u2200w\u2208{0,1} sup \u03bb\u22650 inf \u03b7>0 max w\u2208{0,1}\n\u03b7\u03b1+ \u03b7 logPX(e (\u03bb/\u03b7)fw )\u2212 \u03bb\u03c4 \u2212 logPY (e\u2212\u03bbgw ),\nwhere in (62), by Carath\u00e9odory\u2019s theorem, the alphabet size of QW can be restricted to be no larger than 2, (63) follows by Lemma 8, and (64) follows by the minimax theorem."
        }
    ],
    "title": "Exact Exponents for Concentration and Isoperimetry in Product Polish Spaces",
    "year": 2022
}