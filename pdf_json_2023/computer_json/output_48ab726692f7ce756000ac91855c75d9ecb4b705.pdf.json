{
    "abstractText": "Novel view synthesis of satellite images holds a wide range of practical applications. While recent advances in the Neural Radiance Field have predominantly targeted pin-hole cameras, and models for satellite cameras often demand sufficient input views. This paper presents rpcPRF, a Multiplane Images (MPI) based Planar neural Radiance Field for Rational Polynomial Camera (RPC). Unlike coordinate-based neural radiance fields in need of sufficient views of one scene, our model is applicable to single or few inputs and performs well on images from unseen scenes. To enable generalization across scenes, we propose to use reprojection supervision to induce the predicted MPI to learn the correct geometry between the 3D coordinates and the images. Moreover, we remove the stringent requirement of dense depth supervision from deep multiview-stereo-based methods by introducing rendering techniques of radiance fields. rpcPRF combines the superiority of implicit representations and the advantages of the RPC model, to capture the continuous altitude space while learning the 3D structure.Given an RGB image and its corresponding RPC, the end-to-end model learns to synthesize the novel view with a new RPC and reconstruct the altitude of the scene. When multiple views are provided as inputs, rpcPRF exerts extra supervision provided by the extra views.On the TLC dataset from ZY-3, and the SatMVS3D dataset with urban scenes from WV-3, rpcPRF outperforms state-of-the-art nerf-based methods by a significant margin in terms of image fidelity, reconstruction accuracy, and efficiency, for both singleview and multiview task.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tongtong Zhang"
        },
        {
            "affiliations": [],
            "name": "Yuanxiang Li"
        }
    ],
    "id": "SP:842b1ae186f35bc59bacbf08771e4429f2e080cb",
    "references": [
        {
            "authors": [
                "C. De Franchis",
                "E. Meinhardt-Llopis",
                "J. Michel",
                "J.-M. Morel",
                "G. Facciolo"
            ],
            "title": "An automatic and modular stereo pipeline for pushbroom images",
            "venue": "ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "K. Zhang",
                "N. Snavely",
                "J. Sun"
            ],
            "title": "Leveraging vision reconstruction pipelines for satellite imagery",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, 2019, pp. 0\u20130.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Yao",
                "Z. Luo",
                "S. Li",
                "T. Fang",
                "L. Quan"
            ],
            "title": "Mvsnet: Depth inference for unstructured multi-view stereo",
            "venue": "Proceedings of the European conference on computer vision (ECCV), 2018, pp. 767\u2013783.",
            "year": 2018
        },
        {
            "authors": [
                "F. Wang",
                "S. Galliani",
                "C. Vogel",
                "P. Speciale",
                "M. Pollefeys"
            ],
            "title": "Patchmatchnet: Learned multi-view patchmatch stereo",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 14 194\u201314 203.",
            "year": 2021
        },
        {
            "authors": [
                "X. Gu",
                "Z. Fan",
                "S. Zhu",
                "Z. Dai",
                "F. Tan",
                "P. Tan"
            ],
            "title": "Cascade cost volume for high-resolution multi-view stereo and stereo matching",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 2495\u20132504.",
            "year": 2020
        },
        {
            "authors": [
                "K. Luo",
                "T. Guan",
                "L. Ju",
                "Y. Wang",
                "Z. Chen",
                "Y. Luo"
            ],
            "title": "Attentionaware multi-view stereo",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 1590\u20131599.",
            "year": 2020
        },
        {
            "authors": [
                "R. Weilharter",
                "F. Fraundorfer"
            ],
            "title": "Atlas-mvsnet: Attention layers for feature extraction and cost volume regularization in multi-view stereo",
            "venue": "2022 26th International Conference on Pattern Recognition (ICPR). IEEE, 2022, pp. 3557\u20133563.",
            "year": 2022
        },
        {
            "authors": [
                "J. Shade",
                "S. Gortler",
                "L.-w. He",
                "R. Szeliski"
            ],
            "title": "Layered depth images",
            "venue": "Proceedings of the 25th annual conference on Computer graphics and interactive techniques, 1998, pp. 231\u2013242.",
            "year": 1998
        },
        {
            "authors": [
                "T. Zhou",
                "R. Tucker",
                "J. Flynn",
                "G. Fyffe",
                "N. Snavely"
            ],
            "title": "Stereo magnification: Learning view synthesis using multiplane images",
            "venue": "arXiv preprint arXiv:1805.09817, 2018.",
            "year": 1805
        },
        {
            "authors": [
                "R.A. Rosu",
                "S. Behnke"
            ],
            "title": "Neuralmvs: Bridging multi-view stereo and novel view synthesis",
            "venue": "2022 International Joint Conference on Neural Networks (IJCNN). IEEE, 2022, pp. 1\u20137.",
            "year": 2022
        },
        {
            "authors": [
                "A. Chen",
                "Z. Xu",
                "F. Zhao",
                "X. Zhang",
                "F. Xiang",
                "J. Yu",
                "H. Su"
            ],
            "title": "Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 124\u201314 133.",
            "year": 2021
        },
        {
            "authors": [
                "K. Deng",
                "A. Liu",
                "J.-Y. Zhu",
                "D. Ramanan"
            ],
            "title": "Depth-supervised nerf: Fewer views and faster training for free",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 12 882\u201312 891.",
            "year": 2022
        },
        {
            "authors": [
                "M.M. Johari",
                "Y. Lepoittevin",
                "F. Fleuret"
            ],
            "title": "Geonerf: Generalizing nerf with geometry priors",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 18 365\u201318 375.",
            "year": 2022
        },
        {
            "authors": [
                "J. Chibane",
                "A. Bansal",
                "V. Lazova",
                "G. Pons-Moll"
            ],
            "title": "Stereo radiance fields (srf): Learning view synthesis for sparse views of novel scenes",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 7911\u20137920.",
            "year": 2021
        },
        {
            "authors": [
                "D. Derksen",
                "D. Izzo"
            ],
            "title": "Shadow neural radiance fields for multi-view satellite photogrammetry",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 1152\u20131161.",
            "year": 2021
        },
        {
            "authors": [
                "R. Mar\u0131\u0301",
                "G. Facciolo",
                "T. Ehret"
            ],
            "title": "Sat-nerf: Learning multi-view satellite photogrammetry with transient objects and shadow modeling using rpc cameras",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 1311\u20131321.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Yu",
                "S. Gao"
            ],
            "title": "Fast-mvsnet: Sparse-to-dense multi-view stereo with learned propagation and gauss-newton refinement",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 1949\u20131958.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Yao",
                "Z. Luo",
                "S. Li",
                "T. Shen",
                "T. Fang",
                "L. Quan"
            ],
            "title": "Recurrent mvsnet for high-resolution multi-view stereo depth inference",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 5525\u20135534.",
            "year": 2019
        },
        {
            "authors": [
                "S. Galliani",
                "K. Lasinger",
                "K. Schindler"
            ],
            "title": "Gipuma: Massively parallel multi-view stereo reconstruction",
            "venue": "Publikationen der Deutschen Gesellschaft f\u00fcr Photogrammetrie, Fernerkundung und Geoinformation e. V, vol. 25, no. 361-369, p. 2, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Q. Xu",
                "W. Tao"
            ],
            "title": "Planar prior assisted patchmatch multi-view stereo",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 07, 2020, pp. 12 516\u201312 523.",
            "year": 2020
        },
        {
            "authors": [
                "K. Luo",
                "T. Guan",
                "L. Ju",
                "H. Huang",
                "Y. Luo"
            ],
            "title": "P-mvsnet: Learning patch-wise matching confidence aggregation for multi-view stereo",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 10 452\u201310 461.",
            "year": 2019
        },
        {
            "authors": [
                "J.J. Park",
                "P. Florence",
                "J. Straub",
                "R. Newcombe",
                "S. Lovegrove"
            ],
            "title": "Deepsdf: Learning continuous signed distance functions for shape representation",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 165\u2013174.",
            "year": 2019
        },
        {
            "authors": [
                "R. Chabra",
                "J.E. Lenssen",
                "E. Ilg",
                "T. Schmidt",
                "J. Straub",
                "S. Lovegrove",
                "R. Newcombe"
            ],
            "title": "Deep local shapes: Learning local sdf priors for detailed 3d reconstruction",
            "venue": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXIX 16. Springer, 2020, pp. 608\u2013625.",
            "year": 2020
        },
        {
            "authors": [
                "V. Sitzmann",
                "M. Zollh\u00f6fer",
                "G. Wetzstein"
            ],
            "title": "Scene representation networks: Continuous 3d-structure-aware neural scene representations",
            "venue": "Advances in Neural Information Processing Systems, vol. 32, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Xie",
                "T. Takikawa",
                "S. Saito",
                "O. Litany",
                "S. Yan",
                "N. Khan",
                "F. Tombari",
                "J. Tompkin",
                "V. Sitzmann",
                "S. Sridhar"
            ],
            "title": "Neural fields in visual computing and beyond",
            "venue": "Computer Graphics Forum, vol. 41, no. 2. Wiley Online Library, 2022, pp. 641\u2013676.",
            "year": 2022
        },
        {
            "authors": [
                "K. Zhang",
                "G. Riegler",
                "N. Snavely",
                "V. Koltun"
            ],
            "title": "Nerf++: Analyzing and improving neural radiance fields",
            "venue": "arXiv preprint arXiv:2010.07492, 2020.",
            "year": 2010
        },
        {
            "authors": [
                "M. Niemeyer",
                "J.T. Barron",
                "B. Mildenhall",
                "M.S. Sajjadi",
                "A. Geiger",
                "N. Radwan"
            ],
            "title": "Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 5480\u20135490.",
            "year": 2022
        },
        {
            "authors": [
                "A. Yu",
                "V. Ye",
                "M. Tancik",
                "A. Kanazawa"
            ],
            "title": "pixelnerf: Neural radiance fields from one or few images",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 4578\u20134587.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Wei",
                "S. Liu",
                "Y. Rao",
                "W. Zhao",
                "J. Lu",
                "J. Zhou"
            ],
            "title": "Nerfingmvs: Guided optimization of neural radiance fields for indoor multi-view 16 stereo",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 5610\u20135619.",
            "year": 2021
        },
        {
            "authors": [
                "B. Roessle",
                "J.T. Barron",
                "B. Mildenhall",
                "P.P. Srinivasan",
                "M. Nie\u00dfner"
            ],
            "title": "Dense depth priors for neural radiance fields from sparse input views",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 12 892\u201312 901.",
            "year": 2022
        },
        {
            "authors": [
                "B. Kaya",
                "S. Kumar",
                "F. Sarno",
                "V. Ferrari",
                "L. Van Gool"
            ],
            "title": "Neural radiance fields approach to deep multi-view photometric stereo",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022, pp. 1965\u20131977.",
            "year": 2022
        },
        {
            "authors": [
                "R. Martin-Brualla",
                "N. Radwan",
                "M.S. Sajjadi",
                "J.T. Barron",
                "A. Dosovitskiy",
                "D. Duckworth"
            ],
            "title": "Nerf in the wild: Neural radiance fields for unconstrained photo collections",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 7210\u20137219.",
            "year": 2021
        },
        {
            "authors": [
                "P.P. Srinivasan",
                "R. Tucker",
                "J.T. Barron",
                "R. Ramamoorthi",
                "R. Ng",
                "N. Snavely"
            ],
            "title": "Pushing the boundaries of view extrapolation with multiplane images",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 175\u2013184.",
            "year": 2019
        },
        {
            "authors": [
                "J. Navarro",
                "N. Sabater"
            ],
            "title": "Deep view synthesis with compact and adaptive multiplane images",
            "venue": "Signal Processing: Image Communication, vol. 107, p. 116763, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. DuVall",
                "J. Flynn",
                "M. Broxton",
                "P. Debevec"
            ],
            "title": "Compositing light field video using multiplane images",
            "venue": "ACM SIGGRAPH 2019 Posters, 2019, pp. 1\u20132.",
            "year": 2019
        },
        {
            "authors": [
                "S. Wizadwongsa",
                "P. Phongthawee",
                "J. Yenphraphai",
                "S. Suwajanakorn"
            ],
            "title": "Nex: Real-time view synthesis with neural basis expansion",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 8534\u20138543.",
            "year": 2021
        },
        {
            "authors": [
                "J. Li",
                "Z. Feng",
                "Q. She",
                "H. Ding",
                "C. Wang",
                "G.H. Lee"
            ],
            "title": "Mine: Towards continuous depth mpi with nerf for novel view synthesis",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 12 578\u201312 588.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Wu",
                "Z. Zou",
                "Z. Shi"
            ],
            "title": "Remote sensing novel view synthesis with implicit multiplane representations",
            "venue": "IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 1\u201313, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Q. Xu",
                "W. Tao"
            ],
            "title": "Learning inverse depth regression for multiview stereo with correlation cost volume",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 07, 2020, pp. 12 508\u2013 12 515.",
            "year": 2020
        },
        {
            "authors": [
                "J. Gao",
                "J. Liu",
                "S. Ji"
            ],
            "title": "A general deep learning based framework for 3d reconstruction from multi-view stereo satellite images",
            "venue": "ISPRS Journal of Photogrammetry and Remote Sensing, vol. 195, pp. 446\u2013461, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "J. Lu",
                "Y. Li",
                "Z. Zuo"
            ],
            "title": "Satmvs: A novel 3d reconstruction pipeline for remote sensing satellite imagery",
            "venue": "Proceedings of the International Conference on Aerospace System Science and Engineering 2021. Springer, 2022, pp. 521\u2013538.",
            "year": 2021
        },
        {
            "authors": [
                "C. Godard",
                "O. Mac Aodha",
                "M. Firman",
                "G.J. Brostow"
            ],
            "title": "Digging into self-supervised monocular depth estimation",
            "venue": "Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 3828\u2013 3838.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014novel view synthesis, neural radiance field, multi-plane image, rational polynomial camera.\nI. INTRODUCTION\nDeep-learning-based large-scale satellite photogrammetry has undergone significant development in both remote sensing and computer vision. Novel view synthesis is a computer vision and graphics technique that involves generating new images of a scene or object from viewpoints that were not part of the original data acquisition. To efficiently represent the 3D scene, neural radiance field (NeRF) has revolutionized the Novel View Synthesis (NVS) task since its introduction by providing more flexible and lightweight solutions.\nObtaining a dense altitude map is another crucial task that provides reliable and straightforward 3D representations. Traditional 3D reconstruction methods often rely on multiview or monocular inputs to explicitly recover 3D or 2.5D representations. For satellite photogrammetry, the two pioneering pipelines S2P [1] and Sat-COLMAP [2] are robust but require considerable time for one scene, especially in the dense matching phase. In contrast, learning-based multiview stereo\nTongtong Zhang, Yuanxiang Li are with the Department of Information and Control, School of Aeronautics and Astronautics, Shanghai Jiao Tong University.\n(MVS) methods directly perform depth or normal regression [3], [4], without demanding extensive computations during pairwise matching. However, computations are still expensive in these approaches. For example, to ensure depth resolution, the regressed cost volume must be cascaded several times [5] or composed in an attention structure [6], [7]. In addition, MVS approaches typically require large-scale datasets with dense depth supervision, which may be impractical if the given data are only from optical satellite sensors. Moreover, the mechanism of constructing the cost volumes restricts the model performance on objects with sharp edges such as buildings in urban areas. As for the 2.5D representations, such as Layered Depth Images (LDI) [8] and Multiplane Images (MPI) [9], the product resolutions are often limited by the discrete sampling frequency.\nTo circumvent the shortcomings of explicit representations, we resort to the differentiable rendering techniques. To date, most successors to NeRF adopt the same idea, namely a coordinate-based multilayer-perceptron (MLP), to estimate the components per volume for image construction, such as RGB and transmittance. Combining NeRF with other clues gives decent results, such as [10]\u2013[14], and the satellite versions [15], [16] reconstruct a scene without depth supervision.\nNevertheless, there are still two factors that limit the canonical neural radiance field for pushbroom photogrammetry. The first factor is the gap between the camera models. Although\nar X\niv :2\n31 0.\n07 17\n9v 1\n[ cs\n.C V\n] 1\n1 O\nct 2\n02 3\n2 the RPC can be approximated by the pinhole camera, the conversion not only adds time costs, but also introduces unavoidable fitting errors that increase with image size. By far, most neural radiance fields are designed for perspective cameras, with the exception of Sat-NeRF [16]. By changing the sampling strategies, Sat-NeRF can sample rays in geodetic coordinates, the sampling process is one-off, and it takes a long time to perform the iterative localization step for each position unless caching rays beforehand.\nThe second factor of NeRF-based methods is the generalization ability. NeRF models such as Sat-NeRF and S-NeRF [15] are trained and evaluated within one scene, and fail in unseen cases. The decoder-based radiance fields are not designed to capture the corresponding geometry between stereo pairs, but only for querying volume information. However, typical pushbroom sensors have a limited number of shots to keep the photometric properties the same, e.g., the single-line-camera mounted on WorldView-3 (WV-3) takes multiple shots on the same scene within a limited time window.\nTherefore we propose the efficient implicit model rpcPRF, by replacing the coordinated decoder with an encoder-decoder architecture to model a planar-density-based field. The convolutional encoder extracts high-level image features as prior that are concatenated with the embedded altitude to pass to the decoder, and rigorous reprojection losses are exerted as geometric supervision. Instead of rendering per ray, the frustum is sampled in continuous 3D space and warped to the target view by tensor contraction with inverse rational polynomial coefficients. Rendering is then performed on each frustum to estimate the RGB color of the novelty view image, as well as the altitude of the scene. As a byproduct of the NVS task, the altitude estimation from the network is based on the inference of the ray termination, according to the RGB cues. Due to the inconsistency of brightness and altitude, the best model for estimating the rendered RGB may not yield the correct altitude map at some typical position, such as buildings cast by shadows and lawns with reflective light, so we add some sparse points as supervision for the altitude estimation task.\nIn summary, both the single-view and the multiview versions of rpcPRF solve the issues raised by NeRF models, and they are evaluated on the two satellite multiview stereo benchmark datasets, TLC and Sat-MVS3DM. In the ablation study, we demonstrate the effectiveness of our loss design and the MPI warping module. The main contributions of this paper are:\n\u2022 It is the first work that applies MPI to pushbroom cameras and uses a planar radiance field to synthesize novel views with new RPC. \u2022 rpcPRF is capable of single-view novel view synthesis, and self-supervised altitude estimation with no need for dense ground truth altitude. \u2022 The proposed model generalizes well on unseen scenes, and can be also apply to sparse multiple views. \u2022 rpcPRF significantly outperforms existing RPC-based neural rendering models on NVS, while achieving higher inference efficiency, with single or multiple views."
        },
        {
            "heading": "II. RELATED WORK",
            "text": ""
        },
        {
            "heading": "A. Deep multiview stereo",
            "text": "Given a set of images with known camera parameters, MVS aims to recover the dense geometry of the given scene. Among the learning-based MVS approaches, there are two lines of work, some of which even surpass traditional MVS methods. The first line of work explicitly encodes the stereo pair structure into cost volumes with a learnable image feature encoder and regularizes them with a regression network. But the feature extraction for 3D cost volume is generally time-consuming and memory-consuming, especially when using 3D convolution [3]. Some later works have sparsified the cost volume [17], and some replace the 3D convolution with a recurrent neural network [18] or some attention modules [6]. Another line of work adopts a randomized, iterative patchmatch approach to calculate the depth for the nearest neighbors [19], [20], while the end-to-end differentiable version [4] inherits the advantages. The combination of the two ideas is also welladapted to deep learning settings [21]. Nonetheless, most of these works require burdensome supervision of dense depth maps, and explicit intermediate results incur huge memory and computation costs.\nB. Implicit deep 3D representations\nImplicit Neural Representations (INR) have flourished in these years. Rather than recording the scene as explicit signals, such as depth maps, point clouds, meshes, or voxels, INR tries to learn continuous mappings to represent the signals, which are implemented as neural networks. Common INR paradigms include a coordinate-based decoder that takes coordinate positions as input and returns values at the queried locations, providing more flexibility for both computation and memory. Surfaces are commonly presented as level sets [22], [23]; scenes as different novel views [24], or volumes that can be queried flexibly [25], [26]. However, the decoder architecture performs weakly in geometric modeling. Geometry can be incorporated by additionally regularizing the appearance and geometry of patches rendered from unobserved viewpoints, such as RegNeRF [27], or by introducing an encoder. PixelNeRF [28], for instance, derives a discontinuous scene representation conditioned on the prior encoded by the CNN with only a few images as input. GeoNeRF [13] and MVSNeRF [11] leverage the explicit cost volume to encode the geometry. Most of these methods introduce additional computation while rendering the image per ray, which costs more time and computation than plain NeRF."
        },
        {
            "heading": "C. NeRF for stereo and reconstruction",
            "text": "Multiview-Stereo and implicit representations complement each other in scene reconstruction. generally, MVS provides the geometry guidance for the radiance field, such as sparse depth map guidance [29], or immediate explicit geometrical feature representations as prior [11], [13]. Structure from Motion (SfM) can also help in exerting depth maps [12], [30] to provide geometry supervision. Estimating the correspondences based on similar image regions in stereo images can\n3 also provide strong supervision of the scene geometry. [14] learns pair-wise similarities via NeRF, and [31] utilize surface normals additionally for confirming the correspondences.\nSpecific to satellites, S-NeRF [15] learns the impact of solar rays while jointly inferring the geometry, and Sat-NeRF [16] first adapts NeRF to pushbroom photogrammetry. As an extension of S-NeRF, Sat-NeRF takes full advantage of metadata of satellite images, by estimating transient objects in addition, similar to NeRF in the wild [32]. However, the models are all designed for one specific scene with sufficient multiple views."
        },
        {
            "heading": "D. Application of MPI",
            "text": "Owing to the straightforward operations, MPI can be rendered in real-time, with many follow-ups for stereo matching [33], novel view synthesis [34], video [9], [35], etc. MPI is typically designed for a stereo pair with a narrow baseline [9], extracted from the input Plane Sweep Volumes (PSVs) with the estimated alpha constraining the visibility. Despite the advantages of MPI in rendering efficiency and quality, the tradeoff between memory usage and the potential rendering ability remains to be a challenge, since the MPI disparity sampling frequency linearly affects the range of views [33]. For this issue, [34] adaptively sparsifies the MPI representations to remove the redundancy by introducing a sparse loss term. More radically, with the aid of INR, the continuous representation of the discrete MPI extends to the continuous depth space. Nex [36] combines neural rendering with MPI to fulfill the real-time rendering task; MINE [37] constructs MPI in the continuous 3D space by introducing the renderer in NeRF; imMPI [38] extends MINE by pretraining across scenes and fine-tuning on a single scene, to construct a full light field for aerial images of one scene.\nHowever, these models can not be directly used on satellite photogrammetry. Except for the discrepancy of the imaging model, the generalization ability across scenes still remains to be settled. The most relevant work imMPI [38] claimed to render novel views for remote sensing views by modeling the per-scene planar radiance field, but the images come from perspective cameras. Although the scene priors extracted from cross-scene initialization speed up the per-scene optimization, the model can not generalize to new scenes."
        },
        {
            "heading": "III. PLANAR NEURAL RADIANCE FIELD FOR RPC MODEL",
            "text": "This section firstly elaborates on the proposed rpcPRF with single-view, as the pipeline illustrated in Fig. 2. Apart from single-view input, rpcPRF with multiple views is described in Section III-D."
        },
        {
            "heading": "A. Efficient implicit 3D representation",
            "text": "1) Conventional MPI: MPI takes a set of fronto-parallel planes at a fixed range of depths as the basis for global scene synthesis [9]. Different from LDI [8], MPI fixes the layers at sampled depth.\nThe 3D frustum of the scene is a collection of Ns RGBA layers {(Czi , \u03b1zi)}, i \u2208 {1, . . . , Ns}, where Czi(x, y) denotes\nthe RGB value of the position [x, y, zi] with x, y indexing the pixels and zi indexing the ith sampled depth, while \u03b1zi(x, y) refers to the alpha map at [x, y, zi]. The canonical MPI renderer uses alpha composition on the RGBA set above to yield the image estimation I\u0302 and the disparity map estimation D\u0302: I\u0302(x, y) = D\u2211 i=1 (Czi(x, y)\u03b1zi(x, y) D\u220f j=i+1 (1\u2212 \u03b1zj (x, y)))\nD\u0302(x, y) = D\u2211 i=1 (z\u22121i \u03b1zi(x, y) D\u220f j=i+1 (1\u2212 \u03b1zi(x, y)))\n(1)\nThen the homography warping is applied to the frustum to render novel views for the perspective cameras.\n2) Height sampling and embedding: As metadata determined in advance, the height range [hnear, hfar] of the region is far larger than common NeRF settings. To perform proper sampling along the epipolar line, as [39] suggested, we sample Ns height hypotheses between the lowest altitude hfar and the highest altitude hnear on reciprocal altitude space:\n1\nhi =\n1\nhfar + i\u2212 1 Ns ( 1 hnear \u2212 1 hfar ). (2)\nTo better capture the comprehensive feature of the large scale height space to initialize the MPI space, we adopt a 1- dim positional embedding to the relative height sample index hi, i \u2208 {0, 1, 2, . . . , Ns \u2212 1} by the frequency encoding:\n\u03b3(hi) =[sin(2 0\u03c0hi), cos(2 0\u03c0hi), . . . , (3)\nsin(2L\u22121\u03c0hi), cos(2 L\u22121\u03c0hi)]\nAs features in the altitude space, the altitude embeddings are concatenated with multi-scale features extracted with the encoder later.\n3) Planar radiance field: The theoretical analysis by [33] based on signal processing shows that the range of view is limited by the depth sampling frequency. Increasing the sampling frequency helps in representing the stereo, yet the MPI still fails to model the continuous 3D space of arbitrary depth.\nTo benefit from the neural rendering techniques for representing a continuous 3D space, MINE [37] extends the volume rendering of a ray in eq. 4 to planar volume rendering, i.e., for the RGBA set {(Chi , \u03c3hi)} at the sampled height set hi, i \u2208 {1, 2, . . . , Ns} for pixel (x, y), Chi being the color and \u03c3hi being the volume density, the extension of the composition in eq. 1 becomes: I\u0302(x, y) = N\u2211 i=1 Ti(x, y)(1\u2212 exp(\u2212\u03c3hi(x, y)\u03b4hi))Czi(x, y)\nH\u0302(x, y) = N\u2211 i=1\nTi(x, y)(1\u2212 exp(\u2212\u03c3hi(x, y)\u03b4hi))hi (4)\nwhere \u03b4hi(x, y) = \u2225P(x, y, hi+1)\u22a4 \u2212 P(x, y, hi)\u22a4\u22252 denotes the distance between the ith plane to the (i+1)th plane in the pinhole camera coordinate, with P(\u00b7) being the conversion from perspective 3D coordinate to the Cartesian coordinate.\nTi(x, y) = exp(\u2212 i\u22121\u2211 j=1 \u03c3hi(x, y)\u03b4hj ), x \u2208 [0,W ], y \u2208 [0, H]\n4\nof an image with size H \u00d7 W indicates the accumulated transmittance from the first plane P(x, y, h1) to the ith plane P(x, y, hi), i.e., the probability of a ray travels from (x, y, h1) to (x, y, hi) without hitting the object on the ground surface.\n4) Encoder and decoder: To construct the RGBA set {(Chi , \u03c3hi)}, i \u2208 {1, 2, . . . , D} as the discretized planar radiance field, we adopt an encoder-decoder architecture as [37] illustrated in Fig. 3. The encoder extracts features from the RGB image and output feature priors at 5 different scales, feati, i \u2208 {1, . . . , 5}, with Resnet-18 as the backbone. Then the feature set is merged with the embedded heights from eq. ?? and fed to the decoder to produce the 4-channel MPI at 5 scales. As shown in the factor graph in Fig. 3, the encoder runs one time for RGB images from different views in a batch, and the decoder forwards 5 times for the 5 scales of MPI."
        },
        {
            "heading": "B. Adapting to RPC protocol",
            "text": "The imaging model of the satellite with RPC serves as an accurate model to project the ground to the corresponding image at pixel level. The RPC model offers a compact and precise representation of the geometry between ground and generalized sensors. It is widely adopted in high resolution satellite photogrammetry, owing to the high approximation accuracy and the independency of the sensor and platform. The RPC model relates the ground coordinate and the image pixel coordinate with the ratios of polynomials. For a 3D point in the normalized world coordinate and its projected 2D point on the image from the sensor, the projection is approximated with polynomials as tensors:\n samp = P1(lat, lon, hei) P2(lat, lon, hei) = A(num)X A(den)X line = P3(lat, lon, hei)\nP4(lat, lon, hei) = B(num)X B(den)X\n(5)\nwhere lat, lon, hei denote the latitude, longitude and height of the 3D point, samp and line are the row and the column of the pixels, and for this point in the object space, its cubic tensor X = {Xijk} has rank 3, where Xijk = [1, heii, latj , lonk], i, j, k \u2208 {0, 1, 2, 3}. And Pt(\u00b7), t \u2208 {1, 2, 3, 4} represents the cubic polynomials which formulate two ratios to fit the projection from the world to the\n5 image. The polynomials are represented by coefficient tensors A(num),A(den),B(num),B(den) \u2208 R4\u00d74 as numerators and denominators respectively. More explicitly, taking P1(\u00b7) for example, there is P1(lat, lon, hei) = A(num)X = 3\u2211\ni=0 3\u2211 j=0 3\u2211 k=0 Aijk(num)hei ilatj lonk,\n(6) Inversely, the conversion from image to the ground, i.e. localization, can also be approximated by Pt(\u00b7), t \u2208 {5, 6, 7, 8}, where the polynomial for one point can also be represented as tensor contraction, with coefficients on the numerator and denominator as C(num), C(den),D(num),D(den) \u2208 R4\u00d74 respectively, and its cubic tensor Y = {Yilm}, with Yilm = [1, heii, sampl, linem], i, l,m \u2208 {0, 1, 2, 3} denotes the point in the image space: lat = P5(samp, line, hei) P6(samp, line, hei) = C(num)Y C(den)Y lon = P7(samp, line, hei)\nP8(samp, line, hei) = D(num)Y D(den)Y\n(7)\nWith the two-way conversion of one ground point and its corresponding image pixel established, we try to establish the conversion from the reference MPI to the target MPI as batches of points.\n1) Differentiable RPC warping in advance: In Conventional MPI settings, the novel view is rendered via eq. 4 with the MPI of the warped reference view via a homography tensor in the camera coordinate. With the epipolar geometry in homogeneous space, the homography tensor is computed in batches for the source view by composing the intrinsic and extrinsic features of the pinhole camera. The homography tensor maps from the reference camera coordinates to the object space and then projects the 3D points onto the source view with a compact tensor operation. Inspired by [40], we also warp the MPI from the reference view to the source views via projection and localization computed in the quaternion cubic homogeneous polynomial space.\nIn the following, we set up MPI warping for RPC in the the Geodetic Coordinate System (GCS). Again the forward and inverse mappings as tensors in eq. 5 and eq. 7 become tensors of more ranks, allowing for batched computation of points. The MPI warping from the reference to the target takes two steps. Firstly virtual grids in GCS are obtained via localization with the reference RPC tensors, and secondly we project the virtual grids to the target MPI via the target RPC tensors. Let B denotes the batch size, Nmpi denotes the number of points in one set of MPI, for the nth point M bnilm = {(1, (heibn)i, (sampbn)l, (linebn)m)}, i, l,m \u2208 {0, 1, 2, 3} in the bth batch of MPI points in the image space as a tensor of rank 5 M (ref) = {M bnilm}, the tensor composed of the corresponding points in the object space is denoted as G = {Gbnijk} with Gbnijk = {1, (heibn)i, (lat(bn))j , (lon(bn))k}, i, j, k \u2208 {0, 1, 2, 3}, and b \u2208 {1, . . . , B}, n \u2208 {1, . . . , Nmpi}. The tensor index notation of the target MPI in the image space M (tgt) is similar to M (ref), as shown in Fig. 4, where \u2298 denotes the element-wise division of tensors.\nFig. 4. The factor graph of RPC tensor contractions in the batchified conversion of MPI pixels from reference view to target view.\nIn the first step, the localization process is implemented as contraction of RPC coefficient tensor C(num), C(den),D(num),D(den) \u2208 RB\u00d7Nm\u00d74\u00d74\u00d74 and the MPI tensor M (ref) \u2208 RB\u00d7Nm\u00d74\u00d74\u00d74 in the reference image space. For the latitude tensor lat \u2208 RB\u00d7Nmpi and longitude tensor lon \u2208 RB\u00d7Nmpi respectively, the localization process in (1) of Fig. 4 written with tensor notations can be given as:\nlat = C(num)M (ref) \u2298 C(den)M (ref) \u225c Floc(M (ref), RPC(ref)) (8)\nlon = D(num)M (ref) \u2298D(den)M (ref) \u225c Floc(M (ref), RPC(ref))\nwhere Floc is defined as the tensor localization function which suits fine to parallel computation with CUDA.\nThen the tensor G of correspondent 3D points in the object space is given by concatenation of the latitude tensor and longitude tensor as G = {hei|samp|line}, then we extend the points to its cubic form by combining them with different exponents as Gbnijk = {1, (heibn)i, (lat(bn))j , (lon(bn))k}, i, j, k \u2208 {0, 1, 2, 3}, for projection to the target image space with the target RPC. The projection from object space to the target MPI in the second step with tensor notations becomes:\nsamp = A(num)G \u2298 A(den)G \u225c Fproj(G, RPC(tgt)) (9) line = B(num)G \u2298 B(den)G \u225c Fproj(G, RPC(tgt))\nwhere Fproj is defined as the tensor projection function. Therefore, the representations above not only support the twoway conversion in batch, but the coefficients can also be prepared in advance of the network training or inference process, further reducing time and memory consumption, especially in the localization step. The range of altitude in both TLC and SatMVS3DM can be easily queried. Since we observe a much smaller altitude range than that calculated from the RPC, the altitude sample space and the virtual grid from which we precompute the RPC tensors are based on the former range.\n6 2) Warping for novel view: With the inverse RPC tensor calculated beforehand, and the MPI:\n{(Chi(samp(ref), line(ref)), (10) \u03c3hi(samp (ref), line(ref)))}, i \u2208 {1, . . . , Ns}\nfrom the decoder for the reference view, for every batch b we get a set of 3D points of size Nmpi = Ns \u00d7Hs \u00d7Ws as reference MPI, so we have the tensor representation M (ref) \u2208 RB\u00d7Ns\u00d7Hs\u00d7Ws . where Hs and Ws are the height and width of the MPI at scale s, and Ns is the number of samplings in the altitude space.\nNow there are two ways to obtain the novel view. The instinct one is to use Fproj on the intermediate altitude estimation, as shown in Fig. 5 (a). We can obtain the altitude estimation H\u0302(tmp) = {h\u0302(tmp)} with eq. 4, and use it as a temporary ground truth height map. Using the tensor localization equation with the precalculated inverse coefficients C(ref),D(ref) of reference RPC in 9, we have the corresponding tensor G of points in the object space. However, relying too much on the intermediate results impairs reliability, as intermediate altitude estimates may be imprecise due to unavoidable factors such as shadowing, reflectivity, etc.\nThe second way is to warp the whole frustum, and then apply planar volume rendering to the warped MPI in Fig. 5 (b). The reference MPI is treated as a batch of points in the image space as a tensor M (ref), with batch size to be the sampling number D, the two-step warping process follows Fig. 4 to produce M (tgt). Then we obtain the RGB and height estimation via eq. 4 with M (tgt).\nDespite the convenience of the first way, we validate in the experiments the superiority of the second way, which demonstrates the effectiveness of differentiable MPI warping in both novel view synthesis and altitude estimation."
        },
        {
            "heading": "C. Loss design",
            "text": "1) Loss functions: We mainly use two classes of loss functions as supervision according to their functionality, namely, image quality supervision and geometric supervision.\na) Image Quality Supervision: Given only the image supervision I(ref)gt and I (tgt) gt as the ground-truth RGB image of the reference and target view, of size H\u00d7W with 3 channel,\nto induce better render quality for the model, we take L1 RGB loss:\nLrefRGB = \u2211 \u2225I\u0302(ref) \u2212 I(ref)gt \u2225, (11)\nLrefSSIM = 1\u2212 SSIM(I\u0302 (ref), I (ref) gt ) (12)\nand the structural loss based on the structural similarity index: LtgtRGB = \u2211 \u2225I\u0302(tgt) \u2212 I(tgt)gt \u2225, (13)\nLtgtSSIM = 1\u2212 SSIM(I\u0302 (tgt), I (tgt) gt ) (14)\nb) Geometric Supervision: We adopt two kinds of reprojection errors as geometric supervision. After the height of the reference view H\u0302 is rendered, the first loss project it back to the reference view as I\u0303reproj = Fproj(H\u0302, rpc(ref)) and get the L1 RGB loss:\nLrefreproj = \u2225I\u0303reproj \u2212 I (ref) gt \u2225 (15)\nthe other reprojection loss project the points back to the target view as I\u0303reproj = Fproj(H\u0302, rpc(tgt)) and yield another L1 RGB loss:\nLtgtreproj = \u2225I\u0303reproj \u2212 I (tgt) gt \u2225 (16)\nc) Sparse Altitude Supervision: Optionally, for the altitude estimation task, instead of penalizing the model weights with dense ground truth altitude, we apply sparse ground truth 3D points to facilitate the altitude estimation. The models with image quality losses and geometric losses can yield the best estimation of the probability density of ray terminations, based on the hue clues. But the hue clues can be misleading when the colors are brighter at a low altitude, which commonly appear in photos over cities. Moreover, sparse supervision is much easier to obtain than dense supervision, with SfM or specified control points. For single-view tasks, only the reference view altitude supervision is used:\nLrefpts = 1 |#P ref | \u2211\n(r,c,a)\u2208P ref (ln\n1 H\u0302ref (r, c) \u2212 ln 1 a ) (17)\nFor multiview tasks, the target view altitude supervision can be applied optionally:\nLtgtpts = 1 |#P tgt| \u2211\n(r,c,a)\u2208P tgt (ln\n1 H\u0302tgt(r, c) \u2212 ln 1 a ) (18)\nwhere #P ref and #P tgt indicate the number of used 3D points of the reference view and the target view, H\u0302ref and H\u0302tgt denote the synthesized altitude of the reference view and the target view, and r, c, a index row, column and altitude of the height map repectively.\n2) The total loss: For single-view input, on the novel view synthesis task and height estimation task, we take merely the RGB image and the corresponding RPC of the reference view as input, and render the height estimation H\u0302 , reference view image I\u0302(ref), and the novel view image I\u0302tgt from the model. The overall loss is given by:\nLtotal = \u03bbRGBLrefRGB+\u03bbSSIML ref SSIM+\u03bbreprojL ref reproj (19)\n7\nwhere \u03bbRGB , \u03bbSSIM , \u03bbreproj are the coefficients for the loss respectively. For altitude estimation task, we further impose sparse points as supervision:\nLtotal = \u03bbRGBLrefRGB + \u03bbSSIML ref SSIM (20)\n+ \u03bbreprojLrefreproj + \u03bbptsL ref pts\nwhere \u03bbpts is the weight for points loss."
        },
        {
            "heading": "D. rpcPRF for multiple views",
            "text": "1) Pipeline: rpcPRF is not limited to a single view if there are more images provided. Rather than forwarding the encoder and decoder many times, multiple views provide stronger supervision on target view synthesis without increasing total parameters. The difference between rpcPRF on single-view and multiple views are shown in Fig. 6:\n2) The total loss: For multiview input, on the tasks novel view synthesis and height estimation, the input consists of the RGB images of the reference and the target views, and the loss consists of the penalizations on the reference view and the target view:\nLtotal\u2212m = \u03bbRGB(LrefRGB + L tgt RGB)\n+ \u03bbSSIM (LrefSSIM + L tgt SSIM )"
        },
        {
            "heading": "IV. EXPERIMENTAL RESULTS AND ANALYSIS",
            "text": "This section presents quantitative and qualitative evaluations of our methods on the two representative datasets for RPCbased satellite photogrammetry, TLC [40] and SatMVS3D [41]. First, we introduce the details of the datasets and our preprocessing. Second, we present model setups for view synthesis and altitude estimation on single-view tasks and multiview tasks. Then we explicitly specify the details of the comparison with the state-of-the-art methods on RPC\nphotogrammetry, together with the ablation studies. Finally, we conduct ablation studies on the proposed MPI warping module and losses, and then we analyze how the hyperparameters affect the models."
        },
        {
            "heading": "A. Data preparation",
            "text": "a) TLC: The TLC SatMVS dataset are collected from the triple-line-camera mounted on ZY-3 satellite 1. The optical images are naturally organized as triples of 2.1 m in resolution for the nadir view and 2.5 m in resolution for the two side views. The original TLC dataset provides ready-made splits for training and testing, from which we select parts of the dataset, and further split the original training sets into a training set of 800 triples and a validation set of 100 triples, the test set are of 200 triples. All of the ground truth RGB images and the height maps are composed of 384 x 768 pixels. In addition, we remove the NO-DATA value of the ground truth height map to the minimum height of the valid value, which takes a large proportion. We then recalculate the RPC with the numerator and denominator for samp and line in localization. The virtual grid for this precomputation shrinks to the altitude range with a larger latitude and longitude range. For single-view novel view synthesis and altitude estimation, the triple are reduced to the nadir view. And for the multiview task we select the nadir as the reference view and the other two as the target views.\nb) SatMVS3DM: The SatMVS3DM dataset is an adaptation of the original Multi-View Stereo 3D Mapping Challenge (MVS3DM) challenge dataset for the MVS task. Originally the panchromatic and multispectral images from MVS3DM challenge 2 focus on a 100 km2 area near San\n1http://gpcv.whu.edu.cn/data/whu tlc.html 2https://spacenet.ai/iarpa-multiview-stereo-3d-mapping/\n8 Fernando, Argentina, collected over a span of two years by the WV-3 satellite with a resolution of 30cm per pixel in nadir views. The SatMVS3D dataset converts the ground truth point cloud from UTM to geodetic coordinates, which are then projected onto image coordinates to obtain the ground truth height map. On the basis of SatMVS3D, we filter out the noise of the height map, which deviates far from the ground surface, hence guaranteeing a legitimate altitude range for sampling. For every scene, we take an image size of 384 \u00d7 480. As a supplementary of the selected triple-stereo from SatMVS3DM, we randomly generate groups of subsets of each scene with sizes of 5, 7 and 9, which corresponds the notation g3, g5, g7, g9 in Table I. The original ground truth of 20cm airborne lidar ground truth data for a subset of 20 square kilometers of the same area. There are 6 sites of different sizes within the dataset, the split and the abbreviation information is detailed in Table II."
        },
        {
            "heading": "B. Experiments setup",
            "text": "1) Evaluation metrics: For altitude estimation, after the conversion of the height map to the DSM, we further quantitatively evaluate the reconstruction quality with common metrics:\n\u2022 The mean absolute error (MAE): the average of the L1 distance over all the grid units between the ground truth and the estimated height map; \u2022 Median Height Error (ME): The median of the absolute L1 distance over all the grid units between the ground truth and the estimated height map; \u2022 Error below thresholds: the percentage of grid units with an L1 distance error below the thresholds of 1.0 m (approximately equivariant to the ground sample distance (GSD)), 5.0m and 7.5 m (< 1.0m, < 2.5m and < 7.5m); \u2022 The inference time (time): the inference time from feeding a test image to the network until the height map is inferred.\nFor novel view synthesis, we take the common image quality indicators, the learned perceptual image patch similarity (LPIPs), the peak signal-to-noise ratio (PSNR), and structural similarity index measure (SSIM) as metrics. LPIPS metric utilizes a VGG network to evaluate image similarity at the feature level, while PSNR and SSIM evaluate the image similarity at the pixel level.\n2) Training detatils: We trained the proposed rpcPRF with various combination of losses, and we take a ResNet-18 as encoder, a height decoder with similar structure as Monodepth2 [42]. The rpcPRF models are implemented with PyTorch, and the compared models NeRF and Sat-NeRF are implemented with Pytorch Lightning. All of the models are trained on an NVIDIA RTX 3090 (24GB) GPU. For rpcPRF, we adopt Adam optimizer with a multi-step scheduler for the learning rate. The initial learning rates for the encoder and decoder are set to be 0.0001 and 0.0002, and the milestones for the schedulers vary according to the number of parameters and the epoch settings. The maximum number of epochs is 100 for the multiview setting and 50 for the single-view setting For comparison fairness, we utilize the own settings of the compared models to ensure their performance, besides we modify the dataloader and ray batch size to apply to our problem."
        },
        {
            "heading": "C. Comparison with State-Of-the-Art on novel view synthesis",
            "text": "1) Comparison methods: To date, the novel view synthesis works based on neural radiance fields have been rarely designed for pushbroom cameras. The derivation of the two representative works, Shadow-NeRF (S-NeRF) [15] and SatNeRF [16] both require extra meta-information, such as solar supervision for calculating the irradiance field. Since the two cross-scene multiview datasets we chose have no more information, we adapt our relative comparison as follows:\n\u2022 NeRF(RPC): As a corner stone of neural implicit representations, NeRF models the scene volume with MLP networks as a mapping function, which means that the functions to query volumes require per-scene optimization. For the sake of fairness, we train the MLP functions across scenes, which is the basic setting of our proposed model. Both validation and testing are performed on unseen scenes from the training set, with the same ratio as our rpcPRF split shown in Table I, but with the input format as rays and pixels instead of the entire image. The model parameters are set the same as in S-NeRF except for the lighting condition modules.Moreover, we adopt the ray sampling procedure similar as Sat-NeRF for the most plain NeRF comparison without modeling the shadows. \u2022 Sat-NeRF: Despite meta-information deficiency, we extend the existing information for NeRF-based models as much as possible, based on RPC ray sampling. Firstly, we preserve the embedding vector tj learned as the function of the image index j \u2208 {1, 2, 3}, to encode the effect caused by the relative position, since there is no more geometry encoded. Secondly, we hold the uncertainty coefficient related to the probability of explaining whether\n9\nthe transient object explains the color. Since the features of these two factors are concatenated with the sun direction tensor and then fed to the corresponding network, we set the sun directions for all the input images to be perpendicular to the ground, serving as an empty placeholder. The other training settings follow Sat-NeRF, e.g., the stopping rule of training is the inflection point of validation PSNR, and the dataset splitting follows our model. \u2022 rpcPRF-s: The single-view version of rpcPRF is abbreviated as rpcPRF-s, trained with the loss in eq. 19. Instead of validation PSNR, the training of rpcPRF-s stops at the inflection point of the total loss. \u2022 rpcPRF-m: The multiview version of rpcPRF is abbreviated as rpcPRF-m, trained with the loss in eq. 21.\nAll the models above are designed for the different triples of images, and our goal is to compare the generalization ability of the models on unseen scenarios. All the models are trained on the 6 sites of SatMVS3DM dataset, with splits and abbreviation details in Table II. All the metrics are computed as averages across each site itself.\n2) Image quality comparison and discussion: Table III shows the quantitative comparison between rpcPRF and NeRF-based methods on different sites of Sat-MVS3DM, Table IV on TLC, and Fig. 9 shows the qualitative comparison between these methods on Sat-MVS3DM, and Fig. 10 shows the result on TLC dataset.\nAs can be seen from Table III, Fig. 9 and Fig. 10, the two NeRF-based methods fail to generalize across scenes, while both rpcPRF-s nd rpcPRF-m take the lead on the three metrics\nfor all the scenes. The innate design of the former two methods does not encode the geometry, which means that it performs poorly on the reconstruction task. In the original setting for Sat-NeRF [16], the goal is to learn the volumes to construct structure within a specific scene and render the novel view of the same scene, while rpcPRF aims to learn the geometry of the underlying scene via RPC. Sat-NeRF performs tenuously better than S-NeRF in this case, which shows the importance of image indexing embeddings to some extent, but the indexing embeddings still fail to capture the geometry. In contrast, both the single-view and multiview version perform well on unseen cases. The single-view rpcPRF slightly outperforms PSNR, SSIM and LPIPS.\n3) Efficiency comparison: In addition to the image quality assessment, we also compare the models by memory and computation consumption, time cost, and the model parameters. It is important to note that rpcPRF-s and rpcPRF-m differ in inputs and the training process, so the memory and computation consumption is the same at inference time, thus three models are available for comparison in Table V:\n10\nof images. NeRF-based models are rendered per ray, so the temporary memory actually depends on the batch size of rays, while rpcPRF series rendered per image, so we set the batch size of S-NeRF and Sat-NeRF to be 3 \u00d7 H \u00d7 W . Table V shows that the proposed rpcPRF takes less computation than Sat-NeRF, with faster inference process. Besides, for most of current single-gpu settings, the batch size of 3\u00d7H\u00d7W rays is unattainable, so the total inference time of rpcPRF is actually far less than NeRF-based models."
        },
        {
            "heading": "D. Single-view rpcPRF and ablation study",
            "text": "This section explores the impacts of the key factors on rpcPRF. The first consideration is to validate the effectiveness\nof the differentiable frustum warping module for novel view synthesis, and we take the novel view as a direct projection from the estimated altitude for comparison III. The second consideration is to validate the impact of different losses combinations on the single-view rpcPRF for the two tasks.\n1) Model notations: The baseline notations for novel view synthesis are listed below:\n\u2022 bp: The baseline single-view method without reprojection loss, and novel view projected from the estimated altitude. \u2022 bp-tgt: The baseline single-view method with the total loss in eq. 19, with the novel view projected from the estimated altitude. \u2022 bp-ref: The single-view method with the total loss in\n11\neq. 19, with the novel view projected from the estimated altitude. \u2022 bp-tgt-lpips:The same as bp-tgt but with LPIPs loss during training. \u2022 bp: The single-view method without reprojection loss, and the novel view rendered from the warped frustum. \u2022 bf-tgt: The baseline single-view method with the total loss in eq. 19, with the novel view rendered from the warped frustum. \u2022 bf-ref: The single-view method with the total loss in eq. 19, with the novel view projected from estimated altitude. \u2022 bf-tgt-lpips:The same as bf but with LPIPS loss during training.\nbp series denote the methods of directly obtaining the novel view image via projecting from the estimated altitude. bf series\ndenote the methods of rendering the images from the warped frustum. For altitude estimation, there are additional options for losses combinations:\n\u2022 bl-pts: The baseline method with the total loss in eq. 19 for a single view, with sparse altitude supervision from a randomly selected point cloud of size 100. \u2022 bf-pts: The same loss setting with bl-pts but the novel view is synthesized from the warped frustum.\n2) Novel view synthesis: We compare the models quantitatively with the same image quality metrics in Table III, the results are shown in Table VI, and the visualizations in Fig. 7. We test the models on the subset of the Masterpovitional3 (MP3) subset and compute the average values of all the metrics.\n12\nTable VI shows the crucial role of the reprojection loss in the single-view case. Both the supervisions of projections to the target view and the reference view take effects on improving the synthesized image quality on bp series and bf series. Besides, directly projecting the synthesized source view via RPC (bp series), yields worse results than bf series, and the best results are all achieved by warping the whole frustum with supervision in eq. 19, which shows the significance of frustum warping. Additionally, adding LPIPS supervision alongside reprojection loss during training guarantees sound image quality during training, but the generalization ability is weakened a lot.\n3) Altitude estimation: Unlike single-view learning settings with a scale factor and layered smooth loss [37], satellite photogrammetry does not appear in a layered fashion, and the depth range is larger a lot than the driving or indoor scenarios. We evaluate single-view rpcPRF with different loss combina-\ntions for altitude estimation on the subset MasterProvisional3 (MP3) from the Sat-MVS3DM dataset. The quantitative results are recorded in Table VII and the qualitative comparison is presented in Fig. 11. Table VII reveals the crucial role of sparse point supervision. The estimated altitude is the expectation value of the altitude at which the ray stops, which means that the accurate transparency map determines the real altitude. However, the transparency maps output by the model tend to reveal the best probability density to synthesize the RGB image, thus the shadows, the different textures patches in the RGB image may mislead the model for right transparency map. Sparse ground truth point cloud helps the model to regress to the correct transparency maps, with better reconstruction results."
        },
        {
            "heading": "E. Multiview rpcPRF and ablation study",
            "text": "Similar as the single-view settings, this section studies on the impacts of the same two factors, the method for obtaining the novel view and the losses combinations.\n1) Model Notations:\n\u2022 bp: The baseline method with the total loss in eq. 21 for multiple view, with the novel view projected from estimated altitude. \u2022 bp-reproj: The same as bp with the supervision of Lreproj . \u2022 bp-lpips: The same as bp but with the LPIPS loss during training.\n13\n\u2022 bf: The baseline method with the total loss in eq. 21 for multiple views, and the novel view is synthesized from the warped frustum. \u2022 bf-reproj: The same as bf but without the supervision of Lreproj . \u2022 bf-lpips: The same as bf but with LPIPS loss during training.\nBesides, the optional losses for multiview altitude estimation are again set to be:\n\u2022 bl-pts: The baseline method with the total loss in eq. 21 for multiview, and the novel view is projected from estimated altitude. \u2022 bf-pts: The same loss setting with bl-pts but the novel view is synthesized from the warped frustum.\n2) Novel view synthesis: We compare the models quantitatively with the same image quality metrics in Table III, the results are shown in Table VIII, and the visualizations are in Fig. 8.\nAs shown in Table VIII, rendering a novel view from the warped frustum still takes the lead, and the self-reprojection loss helps rpcPRF to achieve the highest PSNR, while increasing SSIM and decreasing LPIPS. LPIPS supervision also boosts multiview rpcPRF performance, but reprojection loss enables a more comprehensive enhancement.\n3) Altitude estimation: We list the quantitative results in Table IX and qualitative results in Fig. 12. The sparse point supervision again helps the multiview version of rpcPRF to achieve more accurate altitude estimation for all the metrics in Table IX.\nF. Investigation on the number of views This part studies the effect of Input Number of views. As shown in Table X, the synthesized image quality increases\n14\nslightly with more of input views, according to the three metrics. In most cases, single and triple inputs are enough, for overall consideration.\nG. Investigation on the number of altitude samples This part analyzes the sensitivity of rpcPRF to the altitude sample frequency of the MPI. To construct the MPI with the altitude decoder, we sample altitude of size 16, 24, 32, 48 respectively to feed the decoder and to construct the warping grids. Therein the decoder yields MPI with corresponding sizes, and we quantitatively compare the effect on SatMVS3DM dataset for Novel Synthesis in Table XI.\nApparently, the memory consumption and model parameters increase rapidly with the number of samples, and the\ncorresponding FLOPs grows the fastest especially. Meanwhile the synthesized image quality has no radical changes with the altitude samples.\nH. Investigation on the number of sparse points as supervision\nThis part investigates the number of sparse points used as supervision for the single-view altitude estimation tasks. Except for the number of points, all the compared models follow the setting of bf-pts. Table XII shows that the performance of altitude estimation improves when the number of points for supervision is relatively small, but too many points negatively affect the model performance.\n15"
        },
        {
            "heading": "V. CONCLUSION",
            "text": "In this paper, we propose rpcPRF, a generalization of MPI and NeRF models for rational polynomial camera photogrammetry, with single-view and multiple view versions. Given a single image or a triple of images, rpcPRF synthesize novel image from a novel rpc for the new view. rpcPRF is the first work to use MPI for pushbroom sensors, for which we introduce the frustum warping module. Other than training and testing within one scene, we propose to use self-reprojection supervision, thus rpcPRF is able to synthesize a novel view for an entirely new scene. We conduct extensive evaluations on the single-view and multiview versions of rpcPRF. The proposed rpcPRF shows competence in synthesized image quality, with much faster inference speed and lower computation costs. We also demonstrate by the experiments the effectiveness of the proposed frustum warping module. Moreover, rpcPRF is able to synthesize altitude maps without dense depth supervision. With sparse altitude clues, both single-view and multiview rpcPRF series can reconstruct the altitude map. In the future, we will further explore strategies to reduce the training memory caused by intermediate computation of MPI at different scales. The efficiency of inferring the ray termination is also a pivotal topic for further research, for which hierarchical sampling on MPI might be a good choice."
        }
    ],
    "title": "rpcPRF: Generalizable MPI Neural Radiance Field for Satellite Camera",
    "year": 2023
}