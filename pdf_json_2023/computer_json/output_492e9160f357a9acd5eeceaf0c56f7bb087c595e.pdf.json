{
    "abstractText": "Automatic segmentation of medical images is a key step for diagnostic and interventional tasks. However, achieving this requires large amounts of annotated volumes, which can be tedious and timeconsuming task for expert annotators. In this paper, we introduce DeepEdit, a deep learning-based method for volumetric medical image annotation, that allows automatic and semi-automatic segmentation, and clickbased refinement. DeepEdit combines the power of two methods: a noninteractive (i.e. automatic segmentation using nnU-Net, UNET or UNETR) and an interactive segmentation method (i.e. DeepGrow), into a single deep learning model. It allows easy integration of uncertaintybased ranking strategies (i.e. aleatoric and epistemic uncertainty computation) and active learning. We propose and implement a method for training DeepEdit by using standard training combined with user interaction simulation. Once trained, DeepEdit allows clinicians to quickly segment their datasets by using the algorithm in auto segmentation mode or by providing clicks via a user interface (i.e. 3D Slicer, OHIF). We show the value of DeepEdit through evaluation on the PROSTATEx dataset for prostate/prostatic lesions and the Multi-Atlas Labeling Beyond the Cranial Vault (BTCV) dataset for abdominal CT segmentation, using state-of-the-art network architectures as baseline for comparison. DeepEdit could reduce the time and effort annotating 3D medical images compared to DeepGrow alone. Source code is available at https://github.com/Project-MONAI/MONAILabel",
    "authors": [
        {
            "affiliations": [],
            "name": "Andres Diaz-Pinto"
        },
        {
            "affiliations": [],
            "name": "Pritesh Mehta"
        },
        {
            "affiliations": [],
            "name": "Sachidanand Alle"
        },
        {
            "affiliations": [],
            "name": "Muhammad Asad"
        },
        {
            "affiliations": [],
            "name": "Richard Brown"
        },
        {
            "affiliations": [],
            "name": "Vishwesh Nath"
        },
        {
            "affiliations": [],
            "name": "Alvin Ihsani"
        },
        {
            "affiliations": [],
            "name": "Michela Antonelli"
        },
        {
            "affiliations": [],
            "name": "Daniel Palkovics"
        },
        {
            "affiliations": [],
            "name": "Csaba Pinter"
        },
        {
            "affiliations": [],
            "name": "Ron Alkalay"
        },
        {
            "affiliations": [],
            "name": "Steve Pieper"
        },
        {
            "affiliations": [],
            "name": "Holger R. Roth"
        },
        {
            "affiliations": [],
            "name": "Daguang Xu"
        },
        {
            "affiliations": [],
            "name": "Prerna Dogra"
        },
        {
            "affiliations": [],
            "name": "Tom Vercauteren"
        },
        {
            "affiliations": [],
            "name": "Andrew Feng"
        },
        {
            "affiliations": [],
            "name": "Abood Quraini"
        },
        {
            "affiliations": [],
            "name": "Sebastien Ourselin"
        },
        {
            "affiliations": [],
            "name": "Jorge Cardoso"
        }
    ],
    "id": "SP:d0ca784976c7555076cc3c1d740e35504446e8ba",
    "references": [
        {
            "authors": [
                "O. Ronneberger",
                "P. Fischer",
                "T. Brox"
            ],
            "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation,",
            "venue": "in International Conference on Medical Image Computing and Computer-Assisted Intervention,",
            "year": 2015
        },
        {
            "authors": [
                "F. Milletari",
                "N. Navab",
                "S.-a. Ahmadi"
            ],
            "title": "V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation,",
            "year": 2016
        },
        {
            "authors": [
                "F. Isensee",
                "P.F. Jaeger",
                "S.A. Kohl",
                "J. Petersen",
                "K.H. Maier-Hein"
            ],
            "title": "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation,",
            "venue": "Nature Methods,",
            "year": 2020
        },
        {
            "authors": [
                "Y. He",
                "D. Yang",
                "H. Roth",
                "C. Zhao",
                "D. Xu"
            ],
            "title": "DiNTS: Differentiable Neural Net- work Topology Search for 3D Medical Image Segmentation,",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "A. Hatamizadeh",
                "Y. Tang",
                "V. Nath",
                "D. Yang",
                "A. Myronenko",
                "B. Landman",
                "H.R. Roth",
                "D. Xu"
            ],
            "title": "UNETR: Transformers for 3D Medical Image Segmenta- tion,",
            "venue": "IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),",
            "year": 2022
        },
        {
            "authors": [
                "M. Antonelli",
                "A. Reinke",
                "S. Bakas",
                "K. Farahani",
                "A. Kopp-Schneider",
                "B.A. Land- man",
                "G. Litjens",
                "B. Menze",
                "O. Ronneberger",
                "R.M. Summers"
            ],
            "title": "The medical segmentation decathlon,",
            "venue": "Nature Communications,",
            "year": 2022
        },
        {
            "authors": [
                "A. Hatamizadeh",
                "V. Nath",
                "Y. Tang",
                "D. Yang",
                "H.R. Roth",
                "D. Xu"
            ],
            "title": "Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images,",
            "venue": "in International MICCAI Brainlesion Workshop,",
            "year": 2022
        },
        {
            "authors": [
                "T. Sakinis",
                "F. Milletari",
                "H. Roth",
                "P. Korfiatis",
                "P.M. Kostandy",
                "K. Philbrick",
                "Z. Akkus",
                "Z. Xu",
                "D. Xu",
                "B.J. Erickson"
            ],
            "title": "Interactive segmentation of medical images through fully convolutional neural networks,",
            "venue": "arXiv preprint arXiv:1903.08205,",
            "year": 1903
        },
        {
            "authors": [
                "F. Zhao",
                "X. Xie"
            ],
            "title": "An Overview of Interactive Medical Image Segmentation,",
            "venue": "Annals of the British Machine Vision Association,",
            "year": 2013
        },
        {
            "authors": [
                "J. Shi",
                "J. Malik"
            ],
            "title": "Normalized cuts and image segmentation,",
            "venue": "IEEE Transac- tions on Pattern Analysis and Machine Intelligence,",
            "year": 2000
        },
        {
            "authors": [
                "L. Grady",
                "T. Schiwietz",
                "S. Aharon",
                "R. Westermann"
            ],
            "title": "Random walks for in- teractive organ segmentation in two and three dimensions: Implementation and validation,",
            "venue": "Medical Image Computing and Computer-Assisted Intervention,",
            "year": 2005
        },
        {
            "authors": [
                "Y. Boykov",
                "G. Funka-Lea"
            ],
            "title": "Graph cuts and efficient N-D image segmentation,",
            "venue": "International Journal of Computer Vision, vol. 70,",
            "year": 2006
        },
        {
            "authors": [
                "N. Xu",
                "B. Price",
                "S. Cohen",
                "J. Yang",
                "T. Huang"
            ],
            "title": "Deep Interactive Object Selection,",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "E. Agustsson",
                "J.R. Uijlings",
                "V. Ferrari"
            ],
            "title": "Interactive Full Image Segmentation by Considering All Regions Jointly,",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "G. Wang",
                "M.A. Zuluaga",
                "W. Li",
                "R. Pratt",
                "P.A. Patel",
                "M. Aertsen",
                "T. Doel",
                "A.L. David",
                "J. Deprest",
                "S. Ourselin",
                "T. Vercauteren"
            ],
            "title": "DeepIGeoS: A Deep Inter- active Geodesic Framework for Medical Image Segmentation,",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "P.A. Yushkevich",
                "J. Piven",
                "H. Cody Hazlett",
                "R. Gimpel Smith",
                "S. Ho",
                "J.C. Gee",
                "G. Gerig"
            ],
            "title": "User-guided 3D active contour segmentation of anatomical struc- tures: Significantly improved efficiency and reliability,",
            "venue": "Neuroimage, vol. 31,",
            "year": 2006
        },
        {
            "authors": [
                "M. Kass",
                "A. Witkin",
                "D. Terzopoulos"
            ],
            "title": "Snakes: Active Contour Models,",
            "venue": "Journal of Computer Vision, pp",
            "year": 1988
        },
        {
            "authors": [
                "M. Nolden",
                "S. Zelzer",
                "A. Seitel",
                "D. Wald",
                "M. M\u00fcller",
                "A.M. Franz",
                "D. Maleike",
                "M. Fangerau",
                "M. Baumhauer",
                "L. Maier-Hein",
                "K.H. Maier-Hein",
                "H.P. Meinzer",
                "I. Wolf"
            ],
            "title": "The medical imaging interaction toolkit: Challenges and advances: 10 years of open-source development,",
            "venue": "International Journal of Computer Assisted Radiology and Surgery,",
            "year": 2013
        },
        {
            "authors": [
                "K.K. Maninis",
                "S. Caelles",
                "J. Pont-Tuset",
                "L. Van Gool"
            ],
            "title": "Deep Extreme Cut: From Extreme Points to Object Segmentation,",
            "venue": "Proceedings of the IEEE Com- puter Society Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "R. Adams",
                "L. Bischof"
            ],
            "title": "Seeded Region Growing,",
            "venue": "IEEE Transactions on Pat- tern Analysis and Machine Intelligence,",
            "year": 1994
        },
        {
            "authors": [
                "S. Osher",
                "J.A. Sethian"
            ],
            "title": "Fronts propagating with curvature-dependent speed: Algorithms based on Hamilton-Jacobi formulations,",
            "venue": "Journal of Computational Physics,",
            "year": 1988
        },
        {
            "authors": [
                "P. Mehta",
                "M. Antonelli",
                "S. Singh",
                "N. Grondecka",
                "E.W. Johnston",
                "H.U. Ahmed",
                "M. Emberton",
                "S. Punwani",
                "S. Ourselin"
            ],
            "title": "AutoProstate: Towards Automated Reporting of Prostate MRI for Prostate Cancer Assessment Using Deep Learning,",
            "venue": "Cancers, vol. 13,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Keywords: Interactive Segmentation \u00b7 Deep Learning \u00b7 CNNs\nar X\niv :2\n30 5.\n10 65\n5v 1\n[ ee"
        },
        {
            "heading": "1 Introduction",
            "text": "Inspired by the landmark contributions of 2D U-Net[1], 3D U-Net[2], and VNet[3], Convolutional Neural Networks (CNN) have become high-performing methods for automatic segmentation of medical images [4,5,6]. Medical image segmentation challenges, such as the Medical Segmentation Decathlon (MSD) [7], has helped steer methodological innovations and performance improvements for CNN-based methods. At the time of writing, one of the first positions on the live leaderboard 7 for MSD is held by the nnU-Net [4], a segmentation pipeline based on U-Net that automatically configures to any new medical image segmentation task. More recently, transformer-based [8] networks introduced by Hatamizadeh et. al. (Swin UNETR [9] and UNETR [6]), have further improved on nnUNET\u2019s performance, achieving state-of-the-art performance on the MSD segmentation tasks.\nDespite their outstanding performance, automatic segmentation algorithms have not yet reached the desired level of performance needed for certain clinical applications [10]. In particular, automatic segmentation accuracy can be impacted by patient variation, acquisition differences, image artifacts [11] and limited amount of training data. In an attempt to address these challenges, interactive segmentation methods that accept user guidance to improve segmentation have been proposed [12,13,14,15]. Normalized cuts [12], random walks [13], graph-cuts [14], and geodesics [15] have been proposed for interactive segmentation using bounding-box or scribbles-based user interactions. However, a major limitation of these classical methods is that they only succeed in addressing simpler segmentation problems where objects have clear structural boundaries, and require extensive user interaction for more complex segmentation cases containing ambiguity in object boundaries [10].\nA number of deep learning-based interactive segmentation methods based have been proposed for improving the robustness of interactive image segmentation [16,17]. In [16], user foreground and background clicks were converted into euclidean distance maps, and subsequently learned from as additional input channels to a CNN. Inspired by the aforementioned studies and other incremental works, interactive methods for medical image segmentation based on deep learning have been recently proposed [18,10,19]. In [18], a boundingbox and scribble-based CNN segmentation pipeline was proposed, whereby a user-provided bounding box is first used to assist the CNN in foreground segmentation. This was followed by image-specific fine-tuning using user-provided scribbles. Due to the inclusion of user interaction within a CNN, this method provided greater robustness and accuracy than state-of-the-art for segmenting previously unseen objects, while also using fewer user interactions than existing interactive segmentation methods. In contrast, Sakinis et al. [10] proposed a click-based method, motivated in part by the work of [16]. In their work, Gaussian-smoothed foreground and background clicks were added as input chan-\n7 https://decathlon-10.grand-challenge.org/evaluation/challenge/\nleaderboard/\nnels to an encoder-decoder CNN. Experiments on multiple-organ segmentation in CT volumes showed that their method delivers 2D segmentations in a fast and reliable manner, generalizes well to unseen structures, and accurately segments organs with few clicks. An alternate method that first performs an automatic CNN segmentation step, followed by an optional refinement through user clicks or scribbles, was proposed by [19]. Their method, named DeepIGeoS, achieved substantially improved performance compared to automatic CNN on 2D placenta and 3D brain tumour segmentation, and higher accuracy with fewer interactions than traditional interactive segmentation methods.\nAutomatic and semi-automatic segmentation methods are available as part of open-source software packages for medical imaging analysis: ITK-SNAP [20] which offers semi-automatic active contour segmentation [21]; 3D Slicer [22] and MITK [23] offer automatic, boundary-points-based [24]; DeepGrow [10] segmentation through the NVIDIA Clara AI-Assisted Annotation Extension; as well as other classic semi-automatic segmentation methods such as region growing [25] and level sets [26].\nWe propose DeepEdit, a method that combines an automatic and a semiautomatic approach for 3D medical images into a single deep learning-based model. DeepEdit has three working modes: first, it can be used in click-free inference mode (similar to a regular segmentation network), providing fullyautomatic segmentation predictions which can be used as a form of initialisation; second, it allows users to provide clicks to initialise and guide a semi-automatic segmentation model; lastly, given an initial segmentation, DeepEdit can be used to refine and improve the initial prediction by providing editing clicks. DeepEdit training process is similar to the algorithm proposed by Sakinis et al. [10] (DeepGrow) - Gaussian-smoothed clicks for all labels and background are generated and added as input to the backbone CNN, but removes the minimum-click limitation of DeepGrow. Contrary to DeepGrow, our proposed DeepEdit model allows the prediction of an automatic segmentation-based initialisation without userprovided clicks, which can then be further edited by providing clicks. Lastly, the proposed model can also be used for multi-label segmentation problems, allowing the user to generate/segment all labels simultaneously instead of one label at a time.\nThe flexibility offered by embedding these three functionalities (auto segmentation, semi-automatic segmentation and label refinement) allows DeepEdit to be integrated into an active learning pipeline. For instance, it could be used in automatic segmentation mode for aleatoric and/or epistemic uncertainty computation to rank unlabeled images (See Fig. 1(b)).\nIn order to show the performance of DeepEdit, we present applications for single and multiple label segmentation for annotating the datasets: prostate, prostatic lesion, and abdominal organ segmentation."
        },
        {
            "heading": "2 Proposed Method",
            "text": "The DeepEdit architecture is based on a backbone that can be any segmentation network (i.e. UNET, nnU-Net [4], UNETR, SwinUNETR[9]). The main difference resides in how this backbone is trained and the number of channels in the input tensor. For training, the input tensor could be either the image with zeroed tensors (automatic segmentation mode) or the image with tensors representing label and background clicks provided by the user (interactive mode). In Fig. 1, DeepEdit is presented in its training and inference mode.\nAs shown in Fig. 1, DeepEdit can integrate an active learning strategy in which the trained model is used to rank the unlabelled volumes from the most uncertain to the least uncertain. Every time the expert annotator fetches an image, DeepEdit present the one with more uncertainty, allowing the model to learn from the most challenging cases first."
        },
        {
            "heading": "2.1 User interaction and simulated clicks",
            "text": "Our proposed method embeds three approaches: automatic segmentation, semiautomatic segmentation and interactive segmentation. This means, for some iterations, DeepEdit is trained click-free, and for others is trained as DeepGrow (clicks are simulated and included in the input tensor as extra channels). As DeepGrow relies on clicks provided by a user or agent, we simulated those following the similar approach presented in Sakinis\u2019 work - voxels where clicks are located are set to one and smoothed with a Gaussian filter. This is done for the positive label, if single label task or, in general, for all labels and background."
        },
        {
            "heading": "2.2 Training DeepEdit",
            "text": "As previously mentioned, the training process of the DeepEdit algorithm involves click-free iterations and iterations with simulated clicks. As shown in Fig. 1(a), the input of the network is a concatenation of multiple tensors: the image, a tensor containing clicks simulated for each label and a tensor containing clicks simulated for the background. Our proposed algorithm mixes two types of training loops: a) click-free training iterations - meaning that for some iterations, the tensors representing the labels and background clicks are zeros (training for the automatic segmentation); b) simulated-click based training iterations - where labels and background clicks are simulated and placed in the tensors. We sample from a uniform distribution with probability p to determine which iteration is click-free and which one uses simulated clicks. An additional hyper-parameter, the number of simulated clicks per iteration, is set by the user as a function of the task complexity. These two training loops allow DeepEdit to be used fully automatically, semi-automatic, and as a segmentation refinement approach. We developed all these new transforms for click simulation and mixed training in MONAI [10,27]."
        },
        {
            "heading": "3 Experimental Results",
            "text": "In order to demonstrate the flexibility and value of DeepEdit, and the impact of the number of simulated clicks, a set of experiments were performed on the PROSTATEx and Multi Atlas Labeling Beyond The Cranial Vault (BTCV) datasets. We present the impact of the number of clicks in the prostate, prostatic lesion, and abdominal organs (BTCV dataset) segmentation. For both single and multilabel segmentation experiments, we used a learning rate of 1e-4, batch size equal to 1 and Adam optimizer. The following MONAI transforms were used to train and validate DeepEdit: intensity normalization, random flipping (vertically and horizontally), random shift intensity and random rotation.\nAll our experiments have been implemented using the MONAI Core library [27] (version 0.8.1) and MONAI Label platform (version 0.3.1). All source code for DeepEdit algorithm and Active Learning strategies have been made publicly available and documented at https://github.com/Project-MONAI/MONAILabel as part of the MONAI Label repository."
        },
        {
            "heading": "3.1 Prostate Segmentation Tasks",
            "text": "DeepEdit applications were built for whole prostate segmentation and prostatic lesion segmentation. Experiments were run using the PROSTATEx Challenge training dataset, [28], hereby referred to as the PROSTATEx dataset. For both single- and multi-label tasks, experiments were conducted to compare the segmentation performance of DeepEdit as the hyperparameter controlling the number of training iterations with zero simulated clicks is varied. We compared DeepEdit on different click-free training iterations: DeepEdit-0 (equivalent to\nDeepGrow), DeepEdit-0.25 and DeepEdit-0.50, meaning that 0, 25 and 50 percent of the training iterations were click-free. Ten-fold cross-validation was performed for both tasks. Segmentation quality was assessed using the Dice coefficient. As in [10], segmentation performance at inference time was assessed using simulated clicks instead of user mouse-clicks to objectively assess how segmentation quality improves as clicks are added; segmentation performance was assessed at 0, 1, 5, and 10 simulated inference clicks. The presented results are an average of three repetitions to account for variability in simulated inference click placement.\nWhole Prostate Segmentation The whole prostate segmentation task concerns the segmentation of the prostate on T2-weighted MRI (T2WI). Eleven patients from the PROSTATEx dataset were excluded due to inconsistencies between T2WI and the ground-truth segmentations, leaving a total of 193 patients for use in experiments.\nT2WI were pre-processed by resampling to a common resolution of 0.5mm \u00d7 0.5mm \u00d7 3.0mm, normalization using per-image whitening, and cropping/padding to a common size of 320\u00d7320 \u00d732.\nA comparison of DeepEdit-0 (equivalent to DeepGrow), DeepEdit-0.25, and DeepEdit-0.5 is shown in Table 1. Furthermore, the distributions of Dice scores are shown in Fig. 2. DeepEdit-0.5 was found to have the highest click-free mean Dice score (0.908), while DeepEdit-0 gave the highest mean Dice scores at 1 to 10 simulated inference clicks.\nProstatic Lesion Segmentation The prostatic lesion segmentation task concerns the segmentation of lesions within the prostate using T2WI, apparent diffusion coefficient (ADC) map, and computed high b-value diffusion-weighted MRI (DWI). Since our experiments were conducted using the PROSTATEx dataset, we used the PROSTATEx definition of a lesion, i.e., a prostatic lesion is defined as any area of suspicion attributed to a Prostate Imaging-Reporting and Data System (PI-RADS) score by the expert radiologist (anonymous clinician) who read and reported PROSTATEx dataset cases; all lesions in the PROSTATEx dataset were scored PI-RADS \u2265 2. Four patients from the PROSTATEx dataset were excluded due to not containing contoured lesions in the ground truth (the\nassessment metrics would have been undefined), leaving a total of 200 patients with a total of 299 lesions for use in experiments.\nA b-value, b = 2000, was selected for computing high b-value DWI; computed b2000 (Cb2000) DWI were generated using DWI acquired at lower b-values, extrapolated by assuming a monoexponential model for the per-voxel observed signal. ADC map and Cb2000 DWI were registered to T2WI to account for voluntary/involuntary patient movement between acquisitions and differences in resolution. T2WI and Cb2000 DWI were normalised by dividing voxel intensities by the interquartile mean of central gland (CG) voxel intensities [29]; ADC maps were not normalised as they contain a quantitative measurement. T2WI, ADC map, and Cb2000 DWI were resampled to a common resolution of 0.5 mm \u00d7 0.5 mm \u00d7 3 mm. Then, whole prostate masks were used to crop the prostate region on all MR modalities; a margin was applied in each direction to reduce the likelihood of prostate tissue being discarded. Next, a cropping/padding transformation was used to ensure a common spatial size of 256 \u00d7 256 \u00d7 32.\nA comparison of DeepEdit-0 (equivalent to DeepGrow), DeepEdit-0.25, and DeepEdit-0.5 is shown in Table 2. Furthermore, the distributions of Dice scores are shown in Fig 3. As in the whole prostate segmentation task, DeepEdit-0.5 gave the highest click-free mean Dice score (0.272), while DeepEdit-0 (equivalent to DeepGrow) gave the highest mean Dice scores at 1 to 10 simulated inference clicks."
        },
        {
            "heading": "3.2 Abdominal Organ Segmentation",
            "text": "A second set of experiments using the UNETR [6] as backbone were performed on the BTCV dataset. For this, we used 23 images for training, 6 for validation, and an image size of 128\u00d7128\u00d7128. As the previous analysis, we compared DeepEdit\ntrained with 0% click-free training iterations (equivalent to DeepGrow), 25% click-free training iterations, and 50% click-free training iterations.\nIn Table 3, we show the obtained results on the validation set for 0, 1, 5, and 10 simulated clicks. As a fair comparison, we trained and validated a UNETR and the DeepEdit using the same images, same transforms and for the same number of epochs (200).\nAs it is shown in Table 3, any DeepEdit configuration performs slightly better than the UNETR on the validation set when simulated clicks are provided.\nAdditional qualitative results obtained from DeepEdit are presented in the supplementary material. We show how DeepEdit could also be applied on two additional clinical problems: segmentation of metastatic spines and teeth segmentation for treatment planning in reconstructive periodontal surgery."
        },
        {
            "heading": "4 Conclusion",
            "text": "In this study, we introduce DeepEdit, a method that enables an uncertaintydriven active learning workflow for labelling medical images using a framework that combines deep learning-based automatic segmentation and interactive edits. Compared to previous interactive approaches, DeepEdit can be easily integrated into any 3D medical segmentation pipeline that includes active learning strategies. Using DeepEdit, biologists/clinicians can 1) obtain an automatic segmentation that can later be modified or refined by providing clicks through a user interface (e.g., 3D Slicer, OHIF), or 2) provide clicks to get a segmentation (semi-automatic segmentation). This could significantly reduce the time clinicians/biologists spend on annotating more datasets, which translates in less cost and effort spent on this process."
        }
    ],
    "title": "DeepEdit: Deep Editable Learning for Interactive Segmentation of 3D Medical Images",
    "year": 2023
}