{
    "abstractText": "Compared to conventional artificial neurons that produce dense and real-valued responses, biologically-inspired spiking neurons transmit sparse and binary information, which can also lead to energy-efficient implementations. Recent research has shown that spiking neural networks can be trained like standard recurrent neural networks using the surrogate gradient method. They have shown promising results on speech command recognition tasks. Using the same technique, we show that they are scalable to large vocabulary continuous speech recognition, where they are capable of replacing LSTMs in the encoder with only minor loss of performance. This suggests that they may be applicable to more involved sequence-to-sequence tasks. Moreover, in contrast to their recurrent non-spiking counterparts, they show robustness to exploding gradient problems without the need to use gates.",
    "authors": [
        {
            "affiliations": [],
            "name": "Alexandre Bittar"
        },
        {
            "affiliations": [],
            "name": "Philip N. Garner"
        }
    ],
    "id": "SP:77d40b7de581d89d6db9ca64d840cdcc071f656e",
    "references": [
        {
            "authors": [
                "W. Maass"
            ],
            "title": "Networks of spiking neurons: The third generation of neural network models",
            "venue": "Neural networks, vol. 10, no. 9, pp. 1659\u20131671, 1997. DOI: 10 . 1016 / S0893 - 6080(97)00011-7.",
            "year": 1997
        },
        {
            "authors": [
                "Z.F. Mainen",
                "T.J. Sejnowski"
            ],
            "title": "Reliability of spike timing in neocortical neurons",
            "venue": "Science, vol. 268, no. 5216, pp. 1503\u20131506, 1995. DOI: 10.1126/science.7770778.",
            "year": 1995
        },
        {
            "authors": [
                "R. Van Rullen",
                "S.J. Thorpe"
            ],
            "title": "Rate coding versus temporal order coding: What the retinal ganglion cells tell the visual cortex",
            "venue": "Neural computation, vol. 13, no. 6, pp. 1255\u20131283, 2001. DOI: 10.1162/08997660152002852.",
            "year": 2001
        },
        {
            "authors": [
                "D.A. Butts",
                "C. Weng",
                "J. Jin",
                "C.-I. Yeh",
                "N.A. Lesica",
                "J.-M. Alonso",
                "G.B. Stanley"
            ],
            "title": "Temporal precision in the neural code and the timescales of natural vision",
            "venue": "Nature, vol. 449, no. 7158, pp. 92\u201395, 2007. DOI: 10.1038/ nature06105.",
            "year": 2007
        },
        {
            "authors": [
                "T. Gollisch",
                "M. Meister"
            ],
            "title": "Rapid neural coding in the retina with relative spike latencies",
            "venue": "science, vol. 319, no. 5866, pp. 1108\u20131111, 2008. DOI: 10.1126/science. 1149639.",
            "year": 2008
        },
        {
            "authors": [
                "W. Gerstner",
                "W.M. Kistler"
            ],
            "title": "Spiking neuron models: Single neurons, populations, plasticity",
            "venue": "Cambridge university press,",
            "year": 2002
        },
        {
            "authors": [
                "P. Blouw",
                "X. Choo",
                "E. Hunsberger",
                "C. Eliasmith"
            ],
            "title": "Benchmarking keyword spotting efficiency on neuromorphic hardware",
            "venue": "Proceedings of the 7th annual neuro-inspired computational elements workshop, 2019, pp. 1\u20138. DOI: 10.1145/3320288.3320304.",
            "year": 2019
        },
        {
            "authors": [
                "A. Bittar",
                "P.N. Garner"
            ],
            "title": "A surrogate gradient spiking baseline for speech command recognition",
            "venue": "Frontiers in Neuroscience, vol. 16, Aug. 2022. DOI: 10.3389/fnins. 2022.865897.",
            "year": 2022
        },
        {
            "authors": [
                "S. Hochreither",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, Nov. 1997. DOI: 10.1162/neco.1997.9.8.1735.",
            "year": 1997
        },
        {
            "authors": [
                "P. Warden"
            ],
            "title": "Speech commands: A dataset for limited-vocabulary speech recognition, Apr. 2018",
            "year": 2018
        },
        {
            "authors": [
                "E.O. Neftci",
                "H. Mostafa",
                "F. Zenke"
            ],
            "title": "Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks",
            "venue": "IEEE Signal Processing Magazine, vol. 36, no. 6, pp. 51\u201363, 2019. DOI: 10.1109/MSP.2019.2931595.",
            "year": 2019
        },
        {
            "authors": [
                "S.B. Shrestha",
                "G. Orchard"
            ],
            "title": "SLAYER: Spike layer error reassignment in time",
            "venue": "Advances in Neural Information Processing Systems, vol. 31, Curran Associates, Inc., 2018, pp. 1412\u20131421. [Online]. Available: https : / / proceedings . neurips . cc / paper / 2018 / file / 82f2b308c3b01637c607ce05f52a2fed - Paper.pdf.",
            "year": 2018
        },
        {
            "authors": [
                "F.C. Bauer",
                "G. Lenz",
                "S. Haghighatshoar",
                "EX S. Sheik"
            ],
            "title": "ODUS: Stable and efficient training of spiking neural networks, 2022",
            "year": 2022
        },
        {
            "authors": [
                "J. Wu",
                "E. Y\u0131lmaz",
                "M. Zhang",
                "H. Li",
                "K.C. Tan"
            ],
            "title": "Deep spiking neural networks for large vocabulary automatic speech recognition",
            "venue": "Frontiers in neuroscience, vol. 14, p. 199, 2020. DOI: 10.3389/fnins.2020.00199.",
            "year": 2020
        },
        {
            "authors": [
                "W. Ponghiran",
                "K. Roy"
            ],
            "title": "Spiking neural networks with improved inherent recurrence dynamics for sequential learning",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, 2022, pp. 8001\u20138008. DOI: 10.1609/ aaai.v36i7.20771.",
            "year": 2022
        },
        {
            "authors": [
                "J.S. Garofolo",
                "L.F. Lamel",
                "W.M. Fisher",
                "J.G. Fiscus",
                "D.S. Pallett",
                "N.L. Dahlgren"
            ],
            "title": "DARPA TIMIT acousticphonetic continous speech corpus CD-ROM. NIST speech disc 1-1.1",
            "venue": "NIST, Gaithersburg, MD, USA, NISTIR 4930, Feb. 1993. DOI: 10.35111/17gk-bn40.",
            "year": 1993
        },
        {
            "authors": [
                "V. Panayotov",
                "G. Chen",
                "D. Povey",
                "S. Khudanpur"
            ],
            "title": "Librispeech: An ASR corpus based on public domain audio books",
            "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), IEEE, 2015, pp. 5206\u20135210. DOI: 10.1109/ICASSP.2015. 7178964.",
            "year": 2015
        },
        {
            "authors": [
                "S. Ioffe",
                "C. Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift, Feb",
            "year": 2015
        },
        {
            "authors": [
                "Y. Dan",
                "M.-M. Poo"
            ],
            "title": "Spike timing-dependent plasticity: From synapse to perception",
            "venue": "Physiological reviews, vol. 86, no. 3, pp. 1033\u20131048, 2006. DOI: 10.1152/physrev. 00030.2005.",
            "year": 2006
        },
        {
            "authors": [
                "F. Zenke",
                "S. Ganguli"
            ],
            "title": "Superspike: Supervised learning in multilayer spiking neural networks",
            "venue": "Neural computation, vol. 30, no. 6, pp. 1514\u20131541, 2018. DOI: 10.1162/neco_ a_01086.",
            "year": 2018
        },
        {
            "authors": [
                "G. Bellec",
                "D. Salaj",
                "A. Subramoney",
                "R. Legenstein",
                "W. Maass"
            ],
            "title": "Long short-term memory and learning-to-learn in networks of spiking neurons",
            "venue": "Advances in Neural Information Processing Systems, vol. 31, Curran Associates, Inc., 2018, pp. 1412\u20131421. [Online]. Available: https : / / proceedings . neurips . cc / paper / 2018 / file / c203d8a151612acf12457e4d67635a95 - Paper.pdf.",
            "year": 2018
        },
        {
            "authors": [
                "J. Kaiser",
                "H. Mostafa",
                "E. Neftci"
            ],
            "title": "Synaptic plasticity dynamics for deep continuous local learning (DECOLLE)",
            "venue": "Frontiers in Neuroscience, vol. 14, p. 424, 2020. DOI: 10.3389/fnins.2020.00424.",
            "year": 2020
        },
        {
            "authors": [
                "B. Yin",
                "F. Corradi",
                "S. M"
            ],
            "title": "Boht\u00e9, Accurate and efficient time-domain classification with adaptive spiking recurrent neural networks, 2021",
            "year": 2021
        },
        {
            "authors": [
                "A. Paszke",
                "S. Gross",
                "S. Chintala"
            ],
            "title": "Automatic differentiation in pytorch",
            "venue": "NIPS Workshops, 2017. [Online]. Available: https://openreview.net/pdf?id= BJJsrmfCZ.",
            "year": 2017
        },
        {
            "authors": [
                "M. Ravanelli",
                "T. Parcollet",
                "P. Plantinga"
            ],
            "title": "SpeechBrain: A general-purpose speech toolkit, 2021",
            "year": 2021
        },
        {
            "authors": [
                "A. Graves",
                "S. Fern\u00e1ndez",
                "F. Gomez",
                "J. Schmidhuber"
            ],
            "title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks",
            "venue": "Proceedings of the 23rd international conference on Machine learning, 2006, pp. 369\u2013376. DOI: 10.1145/1143844. 1143891.",
            "year": 2006
        },
        {
            "authors": [
                "R. Sennrich",
                "B. Haddow",
                "A. Birch"
            ],
            "title": "Neural machine translation of rare words with subword units",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014 spiking neurons, speech recognition, deep learning, surrogate gradient, bio-inspired computing\n1. INTRODUCTION\nArtificial Neural Networks (ANNs) have become ubiquitous in modern speech technologies. It is currently common practice to use ANNs from the second generation, as defined by Maass [1], in which the elementary units process real-valued inputs with an activation function and transmit real-valued outputs. This form of analogue communication actually differs from what is observed inside biological neural networks, where the information is encoded and transmitted in the form of sparse and binary sequences of events called spike trains. Activation functions in ANNs can be interpreted as firing rate approximations of an underlying spiking mechanism, but this neglects the precise spike timings, whose particular importance in the visual cortex and in auditory neurons has been demonstrated by several neuroscience studies [2]\u2013[5].\nSpiking Neural Networks (SNNs) are based on physiologically plausible neuron models [6] that lead to more\nThis project received funding under NAST: Neural Architectures for Speech Technology, Swiss National Science Foundation, grant number 185010.\nrealistic neuronal dynamics and represent the third generation of ANNs [1]. Whilst they can lead to energy-efficient hardware implementations, and have notably gained interest around keyword-spotting devices [7], SNNs are typically harder to train and have not yet caught up with the ANN performance on speech processing tasks in general. Nevertheless, SNNs have recently achieved concrete progress on speech command recognition tasks [8]. They are now able to compete with larger gated recurrent neural networks such as LSTMs [9] on the Google Speech Commands data set [10], which represents roughly 30 hours of speech data in the form of short command words. This recent success is mostly due to the surrogate gradient method [11], that allows SNNs to be trained via gradient descent like ANNs, making them compatible with modern deep learning frameworks. There even exist CUDA implementations of the surrogate gradient backpropagation [12], [13] that can speed up training on GPUs.\nDespite this apparent compatibility, their usage has so far been restricted to relatively simple tasks and small networks, when compared to current end-to-end ANN architectures. In particular, it appears that this method has not yet been thoroughly tested in an advanced sequence-to-sequence learning scenario. In this study, we take the method that has been shown to be successful on speech command recognition, and apply it to the more challenging task of large vocabulary continuous speech recognition (LVCSR). Even though the scalability of surrogate gradient SNNs to multi-layered architectures has been assessed [8], this was purely done within the constraints of rather simple tasks compared to LVCSR. Indeed, speech command recognition involves very short audio samples, for which the complete input sequence needs to be labelled with a single class. On the other hand, LVCSR must handle long speech utterances and predict the corresponding arrangements of tokens using encoder-decoder architectures that typically involve far more components and parameters. Wu et al. [14] have used SNNs on LVCSR tasks, however, instead of the surrogate gradient approach, they use a tandem learning rule, that relies on sharing weights with non-spiking ANNs that approximate the spike counts, thereby discarding meaningful information about spike timings in the learning mechanism. The only piece of research involving surrogate ar X iv :2 21 2.\n01 18\n7v 2\n[ cs\n.C L\n] 1\n6 Fe\nb 20\n23\ngradient SNNs on LVCSR that we were able to find is the very recent work of Ponghiran and Roy [15]. Inspired by the LSTM, they define a custom version of SNNs that combines a forget gate with multi-bit outputs instead of binary spikes, and reach error rates of 19.72% and 11.75% on TIMIT [16] and LibriSpeech [17] (100 hours) respectively.\nIn this work, we come back to the standard most commonly used leaky integrate-and-fire (LIF) neuron model without gates or multi-bit outputs and show that it is already capable of replacing LSTM layers with only minor losses of performance on LVCSR tasks. Thereby we aim to,\n1. Assess the compatibility and scalability of standard surrogate gradient SNNs as part of end-to-end, sequenceto-sequence, deep architectures on LVCSR tasks.\n2. Test the representational capabilities of SNNs as speech encoders, in which the information is reduced to a sparse and binary form.\n3. Confirm that as opposed to recurrent ANNs, SNNs can be robust to exploding gradient problems without resorting to gates.\nMore generally, our aim is not to improve upon the current ANN state-of-the-art results, but to assess the current capabilities of surrogate gradient SNNs on more advanced deep learning tasks. The results appear promising for the inclusion of SNNs into energy-efficient speech technologies.\n2. SINGLE NEURON MODEL\nThe behaviour of a single spiking neuron can be captured by the LIF neuron model, whose dynamics are characterised by four phases, (i) the integration of stimuli, (ii) a decay back to rest in the absence of stimuli, (iii) the emission of a short pulse when a critical threshold value is attained, and (iv) a recovery period after a spike is emitted. It can be implemented in discrete time as,\nI[t] = BN ( Wx[t] ) + V s[t\u2212 1] (1a)\nu[t] = \u03b1 ( u[t\u2212 1]\u2212 s[t\u2212 1] ) + ( 1\u2212 \u03b1 ) I[t] (1b)\ns[t] = ( u[t] \u2265 \u03d1 ) . (1c)\nHere the stimulus I[t] is a linear combination of inputs x[t] from feedforward connections W and outputs s[t \u2212 1] from recurrent connections V , where batch normalisation [18] is applied to the former. The membrane potential u[t] integrates the incoming stimuli, but also naturally decays back to its rest state at zero. This leaky behaviour is characterized by a trainable coefficient \u03b1 \u2208 [0, 1]. In order to reach its threshold value of \u03d1 = 1 and produce a spike s[t] = 1, the neuron must therefore receive a critical amount of excitatory stimuli within a sufficiently short time window. After a spike is emitted, the membrane potential is reset to a lower value. Most of\nthe time, the neuron therefore stays silent, i.e., s[t] = 0, and no information is transmitted. These dynamics are illustrated in Figure 1, where the membrane potential is shown in blue. In an equivalent nonspiking recurrent neural network, the stimulus I[t] is also computed as in Equation (1a), but instead of using the spiking dynamics of Equations (1b) and (1c), an activation value is simply computed using a nonlinear function g(\u00b7), such as the rectified linear unit (ReLU) or sigmoid,\ny[t] = g ( I[t] ) . (2)\nThe real-valued outputs y[t] can then be interpreted as mean spike counts or firing rates over some arbitrary period of time. Compared to such standard artificial neurons commonly used in deep learning, a spiking neuron has several particularities,\n\u2022 the transmission of sparse binary events instead of dense real-valued sequences, which can reduce the number of operations and the energy consumption.\n\u2022 an additional parameter \u03b1, responsible for how long the neuron will remember certain information.\n\u2022 in principle a better robustness to exploding gradient problems due to the sparse and binary properties of the variable to which recurrence is applied.\n3. SURROGATE GRADIENT METHOD\nIn order to train spiking neural networks, one can either emulate biologically inspired learning rules, such as spike timing dependent plasticity (STDP) [19], or employ gradient based methods, which have recently proven to be extremely successful for training deep artificial neural networks. In this paper, with the objective of making SNNs compatible with modern deep learning frameworks, we concentrate on the ubiquitous stochastic gradient descent (SGD) method. This also allows training of SNNs and ANNs jointly as hybrid architectures inside the same framework.\nAs a result of the non-differentiable threshold operation defined in Equation (1c), SNNs are not directly compatible with SGD. Nevertheless, as reviewed by Neftci et al. [11], the surrogate gradient method allows replacement of the undefined gradient with a surrogate during the backward pass. This in turn allows SNNs to be trained like recurrent neural\nnetworks (RNNs) using the Back-Propagation Through Time (BPTT) algorithm, which is a generalisation of gradient descent for handling sequential data. As presented in the introduction, this method has recently proven to be reliable on speech command recognition tasks [8], but its applicability to deeper networks with the more involved task of LVCSR remains to be assessed.\nDifferent choices of functions have been tested to define the pseudo-derivative of the Heaviside step function [12], [20]\u2013[23]. All share the characteristics of being positive when the potential u[t] is close to the threshold value \u03d1 and vanishing otherwise. In this work, similarly to [8], [22], we use the so-called boxcar function,\n\u2202s[t] \u2202u[t] = { 0.5 if |u[t]\u2212 \u03d1| \u2264 0.5 0 otherwise,\n(3)\nwhich we manually set as the custom backward pass of Equation (1c) and exploit auto-differentiation inside the deep learning framework PyTorch [24].\n4. EXPERIMENTS\nUsing the SpeechBrain framework [25], we perform experiments on three speech recognition tasks with increasing amounts of training data, TIMIT (4 hours), LibriSpeech100 (100 hours) and LibriSpeech960 (960 hours). In our baseline, we use LSTMs that have proven representational capabilities for speech. We have no hypothesis that SNNs can replace all of LSTMs capabilities. Nevertheless we would expect simpler recurrence to suffice for at least some of the layers."
        },
        {
            "heading": "4.1. Baseline architecture",
            "text": "On all three tasks, the inputs consist of 40 mel-filterbank features that are extracted from the utterance waveforms. They are passed through convolutional layers, where time pooling is applied before reaching four bidirectional recurrent LSTM layers of 512 hidden units each. Some additional linear layers output the desired phoneme or subword probabilities. On TIMIT, these probabilities, which correspond to 40 different phoneme labels (including a blank class), directly go into a connectionist temporal classifier (CTC) loss [26]. On LibriSpeech (both 100 and 960 hours), the probabilities correspond to 5000 byte-pair encoding subword units [27]. In addition to CTC loss, an attention-based RNN decoder is also used during training with Negative Log-Likelihood (NLL) loss applied to the predicted sequence. Finally, at inference time, a pretrained Transformer-based language model is used instead of the RNN decoder."
        },
        {
            "heading": "4.2. Gradually replacing LSTMs",
            "text": "The LSTM layers inside the encoder are the only part of the end-to-end training pipeline that we focus on for our experi-\nments. We gradually replace them with SNN layers defined using Equations (1a)-(1c), and reach the architecture illustrated in Figure 2. In order to compare with equivalent nonspiking layers, we also make similar experiments by gradually replacing the LSTM layers with standard non-gated RNN layers instead of the recurrent spiking layers. Note that both are referred to as RNNs in the tables. For instance, 1 RNN - 3 LSTM means that we replaced the first LSTM layer with a non-gated RNN and kept the three remaining ones as they were. For each RNN-LSTM configuration, we therefore train two instances of the architecture, once with spiking neurons and once with nonspiking neurons in the RNN layers."
        },
        {
            "heading": "4.3. Credible intervals on error rates",
            "text": "On TIMIT, the test set consists of 192 sentences that contain a total of n = 7193 phonemes. On LibriSpeech, the clean test set, which is used for both 100h and 960h experiments, represents 2620 sentences for a total of n = 52576 words. When testing an ASR model, the error rate corresponds to the fraction of words or phonemes that were incorrectly predicted by the model. The number of successes in such an experiment with binary outcomes can be modelled as a binomial distribution. For trivial priors, the posterior of the binomial distribution is beta distributed. Using the equal-tailed 95% credible intervals on the posterior distribution, we get error bars between \u00b10.84% and \u00b10.88% for the reported phoneme error rates (PERs) on TIMIT, and about \u00b10.21% for the reported word error rates (WERs) on LibriSpeech."
        },
        {
            "heading": "4.4. TIMIT experiments",
            "text": "On TIMIT, as presented in Table 1, we observe that gradually replacing the LSTM layers by non-gated recurrent spiking layers only hinders the accuracy by less than two percent. Similar results are obtained when replacing the LSTM layers by standard nonspiking RNNs instead. This first shows that surrogate gradient SNNs are capable of being trained as part of an end-to-end hybrid architecture. Moreover, they perform similarly to their nonspiking RNN counterpart, which suggests that sparse, binary information is sufficient to encode speech information. It is worth noting that even if LSTMs remain marginally better, they appear to be over-parameterized when used on such a small data set."
        },
        {
            "heading": "4.5. LibriSpeech experiments",
            "text": "The results on the 100h version are presented in Table 2. We first notice that replacing the first LSTM layer with either a spiking or a nonspiking RNN layer only marginally degrades the performance, even though a non-gated layer involves four times fewer trainable parameters compared to a LSTM layer. When replacing more than one LSTM layer, nonspiking RNNs suffered from exploding gradient issues, whereas equivalent spiking networks remained stable. This confirms that spiking RNNs are more robust to the problems of exploding gradients, compared to standard non-gated RNNs. We can assume that this is due to the sparse nature of the information that prevents gradients from excessively accumulating. On top of being able to learn where nonspiking RNNs diverged, SNNs were actually capable of replacing LSTM layers. The error rates using SNNs remain close to a 1% difference about the LSTM performance, except when\nreplacing all LSTM layers. This suggests that the gating mechanism present in LSTMs may be important for an adequate processing of speech information, even if only one layer may suffice. It is worth noting that we also compared with fewer LSTM layers and ensured that adding SNNs was indeed beneficial. For instance, two SNN layers followed by two LSTM layers did outperform two LSTM layers on their own. These results were left out of the tables for clarity.\nThe results on the complete 960h training set presented in Table 3 show the same tendency as with the 100h reduced version. Here nonspiking RNNs suffered from exploding gradients and were not able to replace a single LSTM layer, whereas spiking RNNs were even capable of replacing all of them. Again, keeping one LSTM layer significantly improves the performance, which supports our hypothesis that SNNs cannot replace all of the LSTMs capabilities.\n5. CONCLUSION\nIn the introduction, we set three goals for our work on spiking neural networks. After successfully training them in the context of LVCSR, we can conclude that surrogate gradient SNNs are compatible with large end-to-end, sequence-to-sequence modern architectures. This answers our first and more general goal of assessing their scalability to more advanced tasks and deeper networks. Secondly, on all tasks, spiking layers, which involve four times fewer trainable parameters, were shown to be capable of replacing LSTMs with only minor losses in performance. Even though keeping a single LSTM layer still significantly helped reducing the error rate, this demonstrates that the information inside neural networks can efficiently be reduced to sparse and binary events without considerably affecting the network encoding capabilities. Thirdly, recurrent SNNs proved to be robust to exploding gradient problems without requiring gates, which was not possible using standard RNNs on large data sets. More generally, these findings contribute to making SNNs viable deep learning components and promising tools for energy-efficient speech technology.\n6. REFERENCES\n[1] W. Maass, \u201cNetworks of spiking neurons: The third generation of neural network models,\u201d Neural networks, vol. 10, no. 9, pp. 1659\u20131671, 1997. DOI: 10 . 1016 / S0893 - 6080(97)00011-7.\n[2] Z. F. Mainen and T. J. Sejnowski, \u201cReliability of spike timing in neocortical neurons,\u201d Science, vol. 268, no. 5216, pp. 1503\u20131506, 1995. DOI: 10.1126/science.7770778.\n[3] R. Van Rullen and S. J. Thorpe, \u201cRate coding versus temporal order coding: What the retinal ganglion cells tell the visual cortex,\u201d Neural computation, vol. 13, no. 6, pp. 1255\u20131283, 2001. DOI: 10.1162/08997660152002852.\n[4] D. A. Butts, C. Weng, J. Jin, C.-I. Yeh, N. A. Lesica, J.-M. Alonso, and G. B. Stanley, \u201cTemporal precision in the neural code and the timescales of natural vision,\u201d Nature, vol. 449, no. 7158, pp. 92\u201395, 2007. DOI: 10.1038/ nature06105.\n[5] T. Gollisch and M. Meister, \u201cRapid neural coding in the retina with relative spike latencies,\u201d science, vol. 319, no. 5866, pp. 1108\u20131111, 2008. DOI: 10.1126/science. 1149639.\n[6] W. Gerstner and W. M. Kistler, Spiking neuron models: Single neurons, populations, plasticity. Cambridge university press, 2002. DOI: 10.1017/CBO9780511815706.\n[7] P. Blouw, X. Choo, E. Hunsberger, and C. Eliasmith, \u201cBenchmarking keyword spotting efficiency on neuromorphic hardware,\u201d in Proceedings of the 7th annual neuro-inspired computational elements workshop, 2019, pp. 1\u20138. DOI: 10.1145/3320288.3320304.\n[8] A. Bittar and P. N. Garner, \u201cA surrogate gradient spiking baseline for speech command recognition,\u201d Frontiers in Neuroscience, vol. 16, Aug. 2022. DOI: 10.3389/fnins. 2022.865897.\n[9] S. Hochreither and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, Nov. 1997. DOI: 10.1162/neco.1997.9.8.1735.\n[10] P. Warden, Speech commands: A dataset for limited-vocabulary speech recognition, Apr. 2018. arXiv: 1804.03209 [cs.CL].\n[11] E. O. Neftci, H. Mostafa, and F. Zenke, \u201cSurrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks,\u201d IEEE Signal Processing Magazine, vol. 36, no. 6, pp. 51\u201363, 2019. DOI: 10.1109/MSP.2019.2931595.\n[12] S. B. Shrestha and G. Orchard, \u201cSLAYER: Spike layer error reassignment in time,\u201d in Advances in Neural Information Processing Systems, vol. 31, Curran Associates, Inc., 2018, pp. 1412\u20131421. [Online]. Available: https : / / proceedings . neurips . cc / paper / 2018 / file / 82f2b308c3b01637c607ce05f52a2fed - Paper.pdf.\n[13] F. C. Bauer, G. Lenz, S. Haghighatshoar, and S. Sheik, EXODUS: Stable and efficient training of spiking neural networks, 2022. arXiv: 2205.10242 [cs.NE].\n[14] J. Wu, E. Y\u0131lmaz, M. Zhang, H. Li, and K. C. Tan, \u201cDeep spiking neural networks for large vocabulary automatic speech recognition,\u201d Frontiers in neuroscience, vol. 14, p. 199, 2020. DOI: 10.3389/fnins.2020.00199.\n[15] W. Ponghiran and K. Roy, \u201cSpiking neural networks with improved inherent recurrence dynamics for sequential learning,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, 2022, pp. 8001\u20138008. DOI: 10.1609/ aaai.v36i7.20771.\n[16] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, D. S. Pallett, and N. L. Dahlgren, \u201cDARPA TIMIT acousticphonetic continous speech corpus CD-ROM. NIST speech disc 1-1.1,\u201d NIST, Gaithersburg, MD, USA, NISTIR 4930, Feb. 1993. DOI: 10.35111/17gk-bn40.\n[17] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: An ASR corpus based on public domain audio books,\u201d in 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), IEEE, 2015, pp. 5206\u20135210. DOI: 10.1109/ICASSP.2015. 7178964.\n[18] S. Ioffe and C. Szegedy, Batch normalization: Accelerating deep network training by reducing internal covariate shift, Feb. 2015. arXiv: 1502.03167 [cs.LG].\n[19] Y. Dan and M.-M. Poo, \u201cSpike timing-dependent plasticity: From synapse to perception,\u201d Physiological reviews, vol. 86, no. 3, pp. 1033\u20131048, 2006. DOI: 10.1152/physrev. 00030.2005.\n[20] F. Zenke and S. Ganguli, \u201cSuperspike: Supervised learning in multilayer spiking neural networks,\u201d Neural computation, vol. 30, no. 6, pp. 1514\u20131541, 2018. DOI: 10.1162/neco_ a_01086.\n[21] G. Bellec, D. Salaj, A. Subramoney, R. Legenstein, and W. Maass, \u201cLong short-term memory and learning-to-learn in networks of spiking neurons,\u201d in Advances in Neural Information Processing Systems, vol. 31, Curran Associates, Inc., 2018, pp. 1412\u20131421. [Online]. Available: https : / / proceedings . neurips . cc / paper / 2018 / file / c203d8a151612acf12457e4d67635a95 - Paper.pdf.\n[22] J. Kaiser, H. Mostafa, and E. Neftci, \u201cSynaptic plasticity dynamics for deep continuous local learning (DECOLLE),\u201d Frontiers in Neuroscience, vol. 14, p. 424, 2020. DOI: 10.3389/fnins.2020.00424.\n[23] B. Yin, F. Corradi, and S. M. Bohte\u0301, Accurate and efficient time-domain classification with adaptive spiking recurrent neural networks, 2021. arXiv: 2103.12593 [ccs.NE].\n[24] A. Paszke, S. Gross, S. Chintala, et al., \u201cAutomatic differentiation in pytorch,\u201d NIPS Workshops, 2017. [Online]. Available: https://openreview.net/pdf?id= BJJsrmfCZ.\n[25] M. Ravanelli, T. Parcollet, P. Plantinga, et al., SpeechBrain: A general-purpose speech toolkit, 2021. arXiv: 2106.04624 [eess.AS].\n[26] A. Graves, S. Ferna\u0301ndez, F. Gomez, and J. Schmidhuber, \u201cConnectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks,\u201d in Proceedings of the 23rd international conference on Machine learning, 2006, pp. 369\u2013376. DOI: 10.1145/1143844. 1143891.\n[27] R. Sennrich, B. Haddow, and A. Birch, Neural machine translation of rare words with subword units, 2015. arXiv: 1508.07909 [cs.CL]."
        }
    ],
    "title": "SURROGATE GRADIENT SPIKING NEURAL NETWORKS AS ENCODERS FOR LARGE VOCABULARY CONTINUOUS SPEECH RECOGNITION",
    "year": 2023
}