{
    "abstractText": "Personalized drug response prediction is an approach for tailoring effective therapeutic strategies for patients based on their tumors\u2019 genomic characterization. The current study introduces a new listwise Learning-to-rank (LTR) model called Inversion Transformer-based Neural Ranking (ITNR). ITNR utilizes genomic features and a transformer architecture to decipher functional relationships and construct models that can predict patient-specific drug responses. Our experiments were conducted on three major drug response data sets, showing that ITNR reliably and consistently outperforms state-of-the-art LTR models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shahabeddin Sotudian"
        },
        {
            "affiliations": [],
            "name": "Ioannis Ch. Paschalidis"
        },
        {
            "affiliations": [],
            "name": "Shahabeddin Sotudiana"
        },
        {
            "affiliations": [],
            "name": "Ioannis Ch. Paschalidisa"
        }
    ],
    "id": "SP:eb88e0745237fbdb3afa11b00973e3bef06cba11",
    "references": [
        {
            "authors": [
                "Delora Baptista",
                "Pedro G Ferreira",
                "Miguel Rocha"
            ],
            "title": "Deep learning for drug response prediction in cancer",
            "venue": "Briefings in bioinformatics,",
            "year": 2021
        },
        {
            "authors": [
                "Omid Bazgir",
                "Ruibo Zhang",
                "Saugato Rahman Dhruba",
                "Raziur Rahman",
                "Souparno Ghosh",
                "Ranadip Pal"
            ],
            "title": "Representation of features as images with neighborhood dependencies for compatibility with convolutional neural networks",
            "venue": "Nature communications,",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Bruch"
            ],
            "title": "An alternative cross entropy loss for learning-torank",
            "venue": "In Proceedings of the Web Conference",
            "year": 2021
        },
        {
            "authors": [
                "Christopher JC Burges"
            ],
            "title": "From ranknet to lambdarank to lambdamart: An overview",
            "year": 2010
        },
        {
            "authors": [
                "Jinyu Chen",
                "Louxin Zhang"
            ],
            "title": "A survey and systematic assessment of computational methods for drug response prediction. Briefings in ITNR: Inversion Transformer-based Neural Ranking for Cancer Drug Recommendations: Preprint submitted to ElsevierPage",
            "venue": "GDSC Fold",
            "year": 2021
        },
        {
            "authors": [
                "Austin Clyde",
                "Tom Brettin",
                "Alexander Partin",
                "Maulik Shaulik",
                "Hyunseung Yoo",
                "Yvonne Evrard",
                "Yitan Zhu",
                "Fangfang Xia",
                "Rick Stevens"
            ],
            "title": "A systematic approach to featurization for cancer drug sensitivity predictions with deep learning",
            "venue": "arXiv preprint arXiv:2005.00095,",
            "year": 2020
        },
        {
            "authors": [
                "Marco Cuturi",
                "Olivier Teboul",
                "Jean-Philippe Vert"
            ],
            "title": "Differentiable ranking and sorting using optimal transport",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Carlos De Niz",
                "Raziur Rahman",
                "Xiangyuan Zhao",
                "Ranadip Pal"
            ],
            "title": "Algorithms for Drug Sensitivity Prediction",
            "year": 2016
        },
        {
            "authors": [
                "Paul Geeleher",
                "Nancy J. Cox",
                "R. Stephanie Huang"
            ],
            "title": "Clinical drug response can be predicted using baseline gene expression levels and in vitrodrug sensitivity in cell lines",
            "venue": "Genome Biology,",
            "year": 2014
        },
        {
            "authors": [
                "Aditya Grover",
                "Eric Wang",
                "Aaron Zweig",
                "Stefano Ermon"
            ],
            "title": "Stochastic optimization of sorting networks via continuous relaxations",
            "venue": "arXiv preprint arXiv:1903.08850,",
            "year": 2019
        },
        {
            "authors": [
                "Saad Haider",
                "Raziur Rahman",
                "Souparno Ghosh",
                "Ranadip Pal"
            ],
            "title": "A Copula Based Approach for Design of Multivariate Random Forests for Drug Sensitivity Prediction",
            "venue": "PLoS ONE,",
            "year": 2015
        },
        {
            "authors": [
                "Boran Hao",
                "Yang Hu",
                "Shahabeddin Sotudian",
                "Zahra Zad",
                "William G Adams",
                "Sabrina A Assoumou",
                "Heather Hsu",
                "Rebecca G Mishuris",
                "Ioannis Ch. Paschalidis"
            ],
            "title": "Development and validation of predictive models for covid-19 outcomes in a safety-net hospital population",
            "venue": "Journal of the American Medical Informatics Association,",
            "year": 2022
        },
        {
            "authors": [
                "Boran Hao",
                "Shahabeddin Sotudian",
                "Taiyao Wang",
                "Tingting Xu",
                "Yang Hu",
                "Apostolos Gaitanidis",
                "Kerry Breen",
                "George C Velmahos",
                "Ioannis Ch. Paschalidis"
            ],
            "title": "Early prediction of level-of-care requirements in patients with covid-19",
            "venue": "Elife, 9:e60519,",
            "year": 2020
        },
        {
            "authors": [
                "In Sock Jang",
                "Elias Chaibub Neto",
                "Justin Guinney",
                "Stephen H. Friend",
                "Adam A. Margolin"
            ],
            "title": "Systematic assessment of analytical methods for drug sensitivity prediction from cancer cell line data",
            "venue": "In Pacific Symposium on Biocomputing,",
            "year": 2013
        },
        {
            "authors": [
                "In Sock Jang",
                "Elias Chaibub Neto",
                "Justin Guinney",
                "Stephen H Friend",
                "Adam A Margolin"
            ],
            "title": "Systematic assessment of analytical methods for drug sensitivity prediction from cancer cell line data",
            "venue": "In Biocomputing",
            "year": 2014
        },
        {
            "authors": [
                "Hiba Kobeissi",
                "Saeed Mohammadzadeh",
                "Emma Lejeune"
            ],
            "title": "Enhancing mechanical metamodels with a generative model-based augmented training dataset",
            "venue": "Journal of Biomechanical Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Amar Koleti",
                "Raymond Terryn",
                "Vasileios Stathias",
                "Caty Chung",
                "Daniel J Cooper",
                "John P Turner",
                "Du\u0161ica Vidovi\u0107",
                "Michele Forlin",
                "Tanya T Kelley",
                "Alessandro D\u2019Urso"
            ],
            "title": "Data portal for the library of integrated network-based cellular signatures (lincs) program: integrated access to diverse large-scale cellular perturbation response data",
            "venue": "Nucleic acids research,",
            "year": 2018
        },
        {
            "authors": [
                "JungHo Kong",
                "Doyeon Ha",
                "Juhun Lee",
                "Inhae Kim",
                "Minhyuk Park",
                "Sin-Hyeog Im",
                "Kunyoo Shin",
                "Sanguk Kim"
            ],
            "title": "Network-based machine learning approach to predict immunotherapy response in cancer patients",
            "venue": "Nature communications,",
            "year": 2022
        },
        {
            "authors": [
                "Jun Li",
                "Qing Lu",
                "Yalu Wen"
            ],
            "title": "Multi-kernel linear mixed model with adaptive lasso for prediction analysis on high-dimensional multiomics",
            "venue": "data. Bioinformatics,",
            "year": 2020
        },
        {
            "authors": [
                "Tie-Yan Liu"
            ],
            "title": "Learning to rank for information retrieval",
            "year": 2011
        },
        {
            "authors": [
                "Przemys\u0142aw Pobrotyn",
                "Tomasz Bartczak",
                "Miko\u0142aj Synowiec",
                "Rados\u0142aw Bia\u0142obrzeski",
                "Jaros\u0142aw Bojar"
            ],
            "title": "Context-aware learning to rank with self-attention",
            "venue": "arXiv preprint arXiv:2005.10084,",
            "year": 2020
        },
        {
            "authors": [
                "Przemys\u0142aw Pobrotyn",
                "Rados\u0142aw"
            ],
            "title": "Bia\u0142obrzeski. Neuralndcg: Direct optimisation of a ranking metric via differentiable relaxation of sorting",
            "venue": "arXiv preprint arXiv:2102.07831,",
            "year": 2021
        },
        {
            "authors": [
                "Sebastian Prillo",
                "Julian Eisenschlos"
            ],
            "title": "Softsort: A continuous relaxation for the argsort operator",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Tao Qin",
                "Tie-Yan Liu",
                "Hang Li"
            ],
            "title": "A general approximation framework for direct optimization of information retrieval measures",
            "venue": "Information retrieval,",
            "year": 2010
        },
        {
            "authors": [
                "Tao Qin",
                "Tie-Yan Liu",
                "Jun Xu",
                "Hang Li"
            ],
            "title": "Letor: A benchmark collection for research on learning to rank for information retrieval",
            "venue": "Information Retrieval,",
            "year": 2010
        },
        {
            "authors": [
                "Gregory Riddick",
                "Hua Song",
                "Susie Ahn",
                "Jennifer Walling",
                "Diego Borges-Rivera",
                "Wei Zhang",
                "Howard A. Fine"
            ],
            "title": "Predicting in vitro drug sensitivity using Random Forests",
            "year": 2011
        },
        {
            "authors": [
                "Xiaoqing Ru",
                "Xiucai Ye",
                "Tetsuya Sakurai",
                "Quan Zou"
            ],
            "title": "Application of learning to rank in bioinformatics tasks",
            "venue": "Briefings in Bioinformatics,",
            "year": 2021
        },
        {
            "authors": [
                "Ali Akbar Sadat Asl",
                "Mohammad Mahdi Ershadi",
                "Shahabeddin Sotudian",
                "X Li",
                "S Dick"
            ],
            "title": "Fuzzy expert systems for prediction of icu admission in patients with covid-19",
            "venue": "Intelligent Decision Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Theodore Sakellaropoulos",
                "Konstantinos Vougas",
                "Sonali Narang",
                "Filippos Koinis",
                "Athanassios Kotsinas",
                "Alexander Polyzos",
                "Tyler J Moss",
                "Sarina Piha-Paul",
                "Hua Zhou",
                "Eleni Kardala"
            ],
            "title": "A deep learning framework for predicting response to therapy in cancer",
            "venue": "Cell reports,",
            "year": 2019
        },
        {
            "authors": [
                "Aman Sharma",
                "Rinkle Rani"
            ],
            "title": "Drug sensitivity prediction framework using ensemble and multi-task learning",
            "venue": "International Journal of Machine Learning and Cybernetics,",
            "year": 2020
        },
        {
            "authors": [
                "Richard Sinkhorn"
            ],
            "title": "A relationship between arbitrary positive matrices and doubly stochastic matrices",
            "venue": "The annals of mathematical statistics,",
            "year": 1964
        },
        {
            "authors": [
                "Shahabeddin Sotudian",
                "Aaron Afran",
                "Christina A LeBedis",
                "Anna F Rives",
                "Ioannis Ch Paschalidis",
                "Michael DC Fishman"
            ],
            "title": "Social determinants of health and the prediction of missed breast imaging appointments",
            "venue": "BMC Health Services Research,",
            "year": 2022
        },
        {
            "authors": [
                "Shahabeddin Sotudian",
                "Ruidi Chen",
                "Ioannis Ch. Paschalidis"
            ],
            "title": "Distributionally robust multi-output regression ranking",
            "venue": "arXiv preprint arXiv:2109.12803,",
            "year": 2021
        },
        {
            "authors": [
                "Shahabeddin Sotudian",
                "Israel T Desta",
                "Nasser Hashemi",
                "Shahrooz Zarbafian",
                "Dima Kozakov",
                "Pirooz Vakili",
                "Sandor Vajda",
                "Ioannis Ch. Paschalidis"
            ],
            "title": "Improved cluster ranking in protein\u2013protein docking using a regression approach",
            "venue": "Computational and structural biotechnology journal,",
            "year": 2021
        },
        {
            "authors": [
                "Shahabeddin Sotudian",
                "Ioannis Ch. Paschalidis"
            ],
            "title": "Machine learning for pharmacogenomics and personalized medicine: A ranking model for drug sensitivity prediction",
            "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics,",
            "year": 2021
        },
        {
            "authors": [
                "Shahabeddin Sotudian",
                "Mohammad Hossein Fazel Zarandi"
            ],
            "title": "Interval type-2 enhanced possibilistic fuzzy c-means clustering for gene expression data analysis",
            "venue": "arXiv preprint arXiv:2101.00304,",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Xuanhui Wang",
                "Cheng Li",
                "Nadav Golbandi",
                "Michael Bendersky",
                "Marc Najork"
            ],
            "title": "The lambdaloss framework for ranking metric optimization",
            "venue": "In Proceedings of the 27th ACM international conference on information and knowledge management,",
            "year": 2018
        },
        {
            "authors": [
                "Qiang Wu",
                "Christopher JC Burges",
                "Krysta M Svore",
                "Jianfeng Gao"
            ],
            "title": "Adapting boosting for information retrieval measures",
            "venue": "Information Retrieval,",
            "year": 2010
        },
        {
            "authors": [
                "Fangfang Xia",
                "Jonathan Allen",
                "Prasanna Balaprakash",
                "Thomas Brettin",
                "Cristina Garcia-Cardona",
                "Austin Clyde",
                "Judith Cohn",
                "James Doroshow",
                "Xiaotian Duan",
                "Veronika Dubinkina"
            ],
            "title": "A cross-study analysis of drug response prediction in cancer cell lines",
            "venue": "Briefings in bioinformatics,",
            "year": 2022
        },
        {
            "authors": [
                "Fen Xia",
                "Tie-Yan Liu",
                "Jue Wang",
                "Wensheng Zhang",
                "Hang Li"
            ],
            "title": "Listwise approach to learning to rank: theory and algorithm",
            "venue": "In Proceedings of the 25th International Conference on Machine Learning,",
            "year": 2008
        },
        {
            "authors": [
                "Mohammad Hossein Fazel Zarandi",
                "Shahabeddin Sotudian",
                "Oscar Castillo"
            ],
            "title": "A new validity index for fuzzy-possibilistic c-means clustering",
            "venue": "arXiv preprint arXiv:2005.09162,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Highlights ITNR: Inversion Transformer-based Neural Ranking for Cancer Drug Recommendations Shahabeddin Sotudian, Ioannis Ch. Paschalidis\n\u2022 The proposed framework is a transformer-based model to predict drug responses using RNAseq gene expression profile, drug descriptors and drug fingerprints.\n\u2022 ITNR utilizes a Context-Aware-Transformer architecture as its scoring function that ensures the modeling of inter-item dependencies.\n\u2022 We introduced a novel loss function using the concept of Inversion and Approximate Permutation matrices. \u2022 Our computational results indicated that our method leads to substantially improved performance when compared to the\nbaseline methods across all performance metrics, which can lead to selecting highly effective personalized treatment.\nITNR: Inversion Transformer-based Neural Ranking for Cancer Drug Recommendations Shahabeddin Sotudiana, Ioannis Ch. Paschalidisa,b,\u2217 aDepartment of Electrical and Computer Engineering, Division of Systems Engineering, Boston University, Boston, MA, USA bDepartment of Biomedical Engineering, and Faculty of Computing and Data Sciences, Boston University, Boston, MA, USA\nA R T I C L E I N F O Keywords: Learning-to-rank Transformers Inversion Drug Response Prediction Approximate Permutation\nA B S T R A C T Personalized drug response prediction is an approach for tailoring effective therapeutic strategies for patients based on their tumors\u2019 genomic characterization. The current study introduces a new listwise Learning-to-rank (LTR) model called Inversion Transformer-based Neural Ranking (ITNR). ITNR utilizes genomic features and a transformer architecture to decipher functional relationships and construct models that can predict patient-specific drug responses. Our experiments were conducted on three major drug response data sets, showing that ITNR reliably and consistently outperforms state-of-the-art LTR models."
        },
        {
            "heading": "1. Introduction",
            "text": "Conventional Machine Learning (ML) algorithms are generally designed to minimize regression or classification errors [12, 13, 16, 32, 42]. Many real-world applications, on the other hand, deemphasize prediction accuracy and prioritize the correct ordering among all the instances [33, 34]. The aim of Learning-to-rank (LTR) methods is to apply ML techniques to solve ranking problems and predict the optimal ordering among the instances according to their degrees of relevance, importance, or preference as defined in the specific application.\nRanking plays a central role in a wide variety of applications, including document retrieval, online advertisement, drug discovery, machine translation, feature selection, document summarization, definition search, question answering, and recommendation systems, among others [27, 25]. In these applications, it is highly desirable to design a model that places relevant/important items at the top of the ranking list. In this work, and without loss of generality, we focus on cancer Drug Response Prediction (DRP) where the goal is to prescribe an optimal therapeutic option for each patient based on their cancer\u2019s unique molecular fingerprints (i.e., the goal of \u201cprecision oncology\u201d). Not every patient responds to medical treatment in the same way. Effectiveness of a medication is influenced by various factors including physiological, pathological, environmental, and genetic factors [40]. Since in-vitro experiments are exceedingly costly and time-consuming, DRP algorithms could serve as promising strategies for the accurate prediction of optimal drug therapies based upon the personalized molecular profiles of patient tumors [35].\nThe confluence of efficient computational tools and a significant number of samples has ushered in a new generation of ML models for drug recommendation. The literature offers a variety of traditional approaches from\n\u2217Corresponding author sotudian@bu.edu (S. Sotudian); yannisp@bu.edu (.I.Ch. Paschalidis)\nORCID(s):\nSupport Vector Machines (SVMs); Principal Component Regression; Ridge, LASSO, and Elastic Net Regressions; to more advanced methods such as multiple-output [33], multiple-kernel [19], and multiple-task learning [30] techniques. As for traditional methods, comprehensive comparative studies [15, 35] demonstrated that Elastic Net or Ridge regression-based models will most likely yield the most accurate predictors. On the other hand, deep learning (DL) models have demonstrated their superiority in capturing the non-linear and complex relationships of biological data better than the traditional algorithms [1]. Representative DL-based algorithms include [2, 29, 21]. These works, [5, 1] and [40] provided a comprehensive analysis of DRP models and other related topics such as data integration, feature selection, experimental settings, combination therapy, and so on.\nIn the current work, we seek to use an under-explored approach for drug response prediction problems, namely transformer models. The application of transformer-based techniques in LTR signifies the high capability of these models in various applications [21]. Equipped with this perspective, we make the following contributions. We developed our LTR framework using a context-aware scoring function and an inversion-based loss function. Unlike the majority of LTR models whose scoring functions score items separately, transformer models (i.e., a popular self-attention-based neural machine translation architecture) allow for the modeling of inter-item dependencies. We adopt the Context-AwareTransformer [21] which is a special case of the encoder part of the transformer. Additionally, this architecture takes into account the inter-dependency of scores between items in the computation of items\u2019 scores. We also proposed a loss function using the concept of Inversion and Approximate Permutation matrices.\nIn experiments, our framework yields state-of-the-art results in three major drug response data sets, showing that our model maintains a consistently good performance under various experimental settings. Thus, the proposed\nITNR: Inversion Transformer-based Neural Ranking for Cancer Drug Recommendations: Preprint submitted to ElsevierPage 1 of 9\nITNR: Inversion Transformer-based Neural Ranking for Cancer Drug Recommendations\narchitecture can capture local context information and crossitem interactions that lead to a reliable drug recommendation system.\nNotational conventions: We use boldfaced lowercase letters to denote vectors, ordinary lowercase letters to denote scalars, boldfaced uppercase letters to denote matrices, and calligraphic capital letters to denote sets. All vectors are column vectors. For space saving reasons, we write \ud835\udc31 to denote the column vector (\ud835\udc651,\u2026 , \ud835\udc65dim(\ud835\udc31)), where dim(\ud835\udc31) isthe dimension of \ud835\udc31. We use prime to denote the transpose, and J\ud835\udc41K for the set {1,\u2026 , \ud835\udc41} for any integer \ud835\udc41 ."
        },
        {
            "heading": "2. Materials and Methods",
            "text": ""
        },
        {
            "heading": "2.1. Problem Formulation",
            "text": "Data in a drug ranking problem consist of a set of triples (cell line, drug, drug response score). A cell line-drug pair is represented by a feature vector. Our ultimate goal is to select the most effective drugs from a set of drugs based on their response. We characterize a drug ranking data set with tuples {(\ud835\udc17\ud835\udc5e ,\ud835\udf3d\ud835\udc5e)}\ud835\udc47\ud835\udc5e=1 where \ud835\udc5e \u2208 J\ud835\udc47 K indexes cell lines,and \ud835\udc17\ud835\udc5e and \ud835\udf3d\ud835\udc5e represent the list of cell line-drug pairs and corresponding drug response scores, respectively. For the \ud835\udc5e- th cell line, we have \ud835\udc5b\ud835\udc5e drugs. \ud835\udc17\ud835\udc5e \u2208 IR\ud835\udc5b\ud835\udc5e\u00d7(\ud835\udc41\ud835\udc37+\ud835\udc41\ud835\udc36 ) has rows (\ud835\udc31\ud835\udc5e1,\u2026 , \ud835\udc31\n\ud835\udc5e \ud835\udc5b\ud835\udc5e ), each of which is a (\ud835\udc41\ud835\udc37+\ud835\udc41\ud835\udc36 )-dimensional cellline-drug vector, formed as the concatenation of an (\ud835\udc41\ud835\udc36 )-dimensional cell line feature vector (i.e., gene expression for cell lines) and an (\ud835\udc41\ud835\udc37)-dimensional drug feature vector. The vector \ud835\udf3d\ud835\udc5e = (\ud835\udf03\ud835\udc5e1 ,\u2026 , \ud835\udf03\ud835\udc5e\ud835\udc5b\ud835\udc5e ) \u2208 IR \ud835\udc5b\ud835\udc5e + contains the corresponding ground-truth drug response scores. The drug response scores are in [0, 1] and a higher score (in our data) implies a more effective drug. Our goal is to learn a scoring function \ud835\udc53 \u2236 IR(\ud835\udc41\ud835\udc37+\ud835\udc41\ud835\udc36 ) \u2192 IR from a training data set with given drug response scores that minimizes the empirical loss:\n\u0302(\ud835\udc53 ) \u225c 1\u2211 \ud835\udc5e \ud835\udc5b\ud835\udc5e\n\ud835\udc47 \u2211\n\ud835\udc5e=1\n\ud835\udc5b\ud835\udc5e \u2211 \ud835\udc51=1 \ud835\udcc1 ( \ud835\udf03\ud835\udc5e\ud835\udc51 , \ud835\udc53 (\ud835\udc31 \ud835\udc5e \ud835\udc51) ) , (1)\nwhere \ud835\udcc1 \u2236 IR \u00d7 IR \u2192 IR is a loss function. For a new cell line-drug matrix \ud835\udc17\ud835\udc61 \u2208 IR\ud835\udc5b\ud835\udc61\u00d7(\ud835\udc41\ud835\udc37+\ud835\udc41\ud835\udc36 ), we can obtain the predicted ranking list by ranking the rows in \ud835\udc17\ud835\udc61 based on their inferred ranking scores ?\u0302?\ud835\udc61 = (\ud835\udc53 (\ud835\udc31\ud835\udc611),\u2026 , \ud835\udc53 (\ud835\udc31\ud835\udc61\ud835\udc5b\ud835\udc61 )). Twopivotal elements of an LTR framework are a scoring function and a loss function. In subsequent subsections, we describe the construction of our LTR framework using a contextaware scoring function and an inversion-based loss function."
        },
        {
            "heading": "2.2. Multi-Headed Self-Attention Scoring Function",
            "text": "Most algorithms in the DRP literature are trained to optimize a loss function that may not capture the interactions among drugs [35]. Moreover, they score them individually at inference-time without regard to any mutual influences among the drugs. Given that context-aware models based on transformers [21] have been successfully used for document\nretrieval, we adopt a similar approach of applying a contextaware ranker for our scoring function. The transformer architecture proposed by [21] plays the role of the Multi-Headed Self-Attention (MHSA) scoring function in our LTR framework. The self-attention mechanism of this architecture aims to handle long-range and inter-item dependencies. Since the MHSA scoring function scores a drug by considering all other drugs applicable to a cell line, it fully captures the interactions among drugs. We consider the list\u2019s items as our tokens where item features are token embeddings. We feed these embeddings to an Encoder Layer. The attention mechanism is the core of the encoder layer to catalyze learning the higher-order representations of items in the list. Mapping query (\ud835\udc2a\ud835\udc56), key(\ud835\udc24\ud835\udc56), and value (\ud835\udc2f\ud835\udc56) vectors to a higher level representationby taking a weighted sum of the values over all items is the essence of the self-attention mechanism. We use the Scaled Dot-Product form to compute attention as follows:\n\u03a8(\ud835\udc10,\ud835\udc0a,\ud835\udc15) = softmax ( \ud835\udc10\ud835\udc0a\u2032 \u221a\n\ud835\udc51\ud835\udc5a\n)\n\ud835\udc15, (2)\nwhere \ud835\udc10 is a query matrix that contains all items (queries) in the list; \ud835\udc0a and \ud835\udc15 are the key and value matrices, respectively. To empower the model to leverage the order of the input tokens, we use fixed positional encodings for the input embeddings as follows:\n\ud835\udec0(\ud835\udc5d,2\ud835\udc56) = sin(\ud835\udc5d\u221510000(2\ud835\udc56\u2215\ud835\udc51\ud835\udc5a)), \ud835\udec0(\ud835\udc5d,2\ud835\udc56+1) = cos(\ud835\udc5d\u221510000(2\ud835\udc56\u2215\ud835\udc51\ud835\udc5a)),\n(3)\nwhere \ud835\udc5d refers to the position, and \ud835\udc56 is the dimension. Here, \ud835\udec0 is a matrix and positional encoding is a system to encode each position into a vector. For instance, given that the model uses 256 dimensions for positional encoding (\ud835\udc51\ud835\udc5a = 256), werepresent each element of the feature vector (i.e., token) as a 256-dimensional vector. In the display above, \ud835\udc5d is an integer from 0 to a pre-defined maximum number of tokens minus 1. Since we have 256 dimensions, we can define 128 pairs of sine and cosine values. Accordingly, the value of \ud835\udc56 goes from 0 to 127. By ensembling multiple attention modules (a.k.a Multi-Head Attention (MHA)), we can improve the ability of the model to learn representations from various subspaces of the input data. MHA can be expressed as:\nMHA(\ud835\udc10,\ud835\udc0a,\ud835\udc15) = Concat(\ud835\udeb21,\ud835\udeb22, ...,\ud835\udeb2\ud835\udc5f)\ud835\udc16\ud835\udc0e, \ud835\udeb2\ud835\udc56 = \u03a8(\ud835\udc10\ud835\udc16 \ud835\udc10 \ud835\udc56 ,\ud835\udc0a\ud835\udc16 \ud835\udc0a \ud835\udc56 ,\ud835\udc15\ud835\udc16 \ud835\udc15 \ud835\udc56 ),\n(4)\nwhere in the above equations, each head \ud835\udeb2\ud835\udc56 (out of \ud835\udc5f heads)refers to the \ud835\udc56-th attention mechanism of Equation (2), and \ud835\udc16\ud835\udc10\ud835\udc56 \u2208 IR\n\ud835\udc51\ud835\udc5a\u00d7\ud835\udc51\ud835\udc5e , \ud835\udc16\ud835\udc0a\ud835\udc56 \u2208 IR\ud835\udc51\ud835\udc5a\u00d7\ud835\udc51\ud835\udc58 , \ud835\udc16\ud835\udc15\ud835\udc56 \u2208 IR\ud835\udc51\ud835\udc5a\u00d7\ud835\udc51\ud835\udc63 , and \ud835\udc16\ud835\udc0e \u2208 IR\ud835\udc5f\ud835\udc51\ud835\udc63\u00d7\ud835\udc51\ud835\udc5a are learnable matrices. Moreover, \ud835\udc5f refers to the number of parallel attention layers or heads. Here, Concat(\u22c5) represents a concatenation operation. It is extremely advantageous to perform the self-attention operation several times and concatenate the outputs. The main problem is the growing size of the resulting output vector. This can\nITNR: Inversion Transformer-based Neural Ranking for Cancer Drug Recommendations: Preprint submitted to ElsevierPage 2 of 9\nITNR: Inversion Transformer-based Neural Ranking for Cancer Drug Recommendations\nbe solved by linearly projecting the matrices \ud835\udc10, \ud835\udc0a, and \ud835\udc15 to \ud835\udc51\ud835\udc5e , \ud835\udc51\ud835\udc58, and \ud835\udc51\ud835\udc63-dimensional spaces \ud835\udc5f times, respectively.Note that we typically use \ud835\udc51\ud835\udc5e = \ud835\udc51\ud835\udc58 = \ud835\udc51\ud835\udc63 = \ud835\udc51\ud835\udc5a\u2215\ud835\udc5f. We referthe interested reader to [37] for more information.\nIn our transformer model, we use a more complex architecture by stacking multiple encoder blocks. The main components of an encoder block including an MHA layer with a skip connection, layer normalization, time-distributed feed-forward layer, and dropout layer can be seen in Figure 1. For more information regarding these components, please refer to [37]. Specifically, the encoder part of the Transformer includes \ud835\udc41 encoder blocks, \ud835\udc3b heads, and hidden dimension \ud835\udc51\u210e. First, a shared fully connected (FC) inputlayer of size \ud835\udc51fc is applied to each item. Then, we feed hiddenrepresentations to the encoder part of the transformer. Our final scoring model can be achieved by iteratively stacking multiple encoder blocks where the output of each block is fed into the next one. The final model can be expressed as follows:\n\ud835\udc1f (\ud835\udc31) = \ud835\udf0c(\u03a01(\u2026\u03a0\ud835\udc41 (\ud835\udf0c(\ud835\udc31)))), (5) where \ud835\udf0c(\u22c5) represents a projection onto a fully-connected layer and \u03a0\ud835\udc56(\u22c5) refers to a single encoder block. We definea single encoder block \u03a0\ud835\udc56(\u22c5) as:\n\u03a0\ud835\udc56(\ud835\udc31) = \ud835\udc41(\ud835\udc33 +\ud835\udc37(\ud835\udf0c(\ud835\udc33))), \ud835\udc33 = \ud835\udc41(\ud835\udc31 +\ud835\udc37(\ud835\udefc(\ud835\udc31)))\n(6)\nwhere \ud835\udefc(\u22c5) is the MHA module, \ud835\udc41(\u22c5) refers to the layer normalisation, and \ud835\udc37(\u22c5) is the dropout layer. Eventually, a score for each item is calculated using a shared fullyconnected layer. Figure 1 provides a schematic overview of our LTR framework."
        },
        {
            "heading": "2.3. Inv-Rank Loss Function",
            "text": "In the previous subsection, we presented the scoring function of our ranking framework. The architecture of this model allows the networks to exploit local features. More importantly, the final score for each item will be calculated by considering all other items on the list. Now, we can use the scores and the ground truth labels to optimize any desired ranking loss. It has been demonstrated that the sorting operator can be approximated by the induced permutation matrix, \ud835\udc43\ud835\udc60\ud835\udc5c\ud835\udc5f\ud835\udc61(\ud835\udc2c) [21].We use the concept of permutation matrices to define our proposed loss function. Permutation matrices are both doubly-stochastic (i.e., a square matrix with entries in [0, 1] where every row and column sum to one) and unimodal (i.e., a square matrix with entries in [0, 1] where each row sums to one, but also has the constraint that the maximizing entry in every row should have a unique column index) [10, 21]. Assume \ud835\udc00\ud835\udc2c represents the matrix of absolute pairwise scoredifferences of \ud835\udc2c with the \ud835\udc56, \ud835\udc57-th element given by \ud835\udc00\ud835\udc2c[\ud835\udc56, \ud835\udc57] = |\ud835\udc60\ud835\udc56\u2212 \ud835\udc60\ud835\udc57|. Grover et al. [10] proposed a continuous relaxation of the permutation matrix \ud835\udc43\ud835\udc60\ud835\udc5c\ud835\udc5f\ud835\udc61(\ud835\udc2c) in the space of unimodal row-stochastic matrices. The \ud835\udc56-th row of \ud835\udc43\ud835\udc60\ud835\udc5c\ud835\udc5f\ud835\udc61(\ud835\udc2c) can be\ncomputed as follows: \ud835\udc43\ud835\udc60\ud835\udc5c\ud835\udc5f\ud835\udc61(\ud835\udc2c)[\ud835\udc56, \u2236](\ud835\udf0f) = softmax[((\ud835\udc5b+ 1\u2212 2\ud835\udc56)\ud835\udc2c\u2212\ud835\udc00\ud835\udc2c\ud835\udfcf\u2215\ud835\udf0f], (7)\nwhere \ud835\udfcf is an all-one vector, \ud835\udc5b is the number of items in a list, and \ud835\udf0f behaves like a temperature knob that controls the degree of approximation and the variance of the gradients (i.e., lower \ud835\udf0f leads to better approximation and higher variance). Moreover, softmax is a function that scales the values in the list and transforms them into values between 0 and 1 such that all values in the returned list sum to 1 (i.e., they can be interpreted as probabilities). Essentially, if we left-multiply a column vector of scores by its \ud835\udc43\ud835\udc60\ud835\udc5c\ud835\udc5f\ud835\udc61(\ud835\udc2c), we can achievethe approximated sorted list [22]. Note that we perform Sinkhorn scaling [31] to obtain doubly stochastic permutation matrices. The Sinkhorn scaling method normalizes all rows and columns; this process is repeated until convergence is achieved (i.e., 30 iterations or the greatest gap between the sum of a row or column and one must be maintained below 10\u22126, whichever occurs first). Assume we use the MHSA scoring function to calculate scores \ud835\udc2c\ud835\udc5e = \ud835\udc53 (\ud835\udc17\ud835\udc5e) for the \ud835\udc5e-th cell line \ud835\udc17\ud835\udc5e . We also have the ground truth labels \ud835\udf3d\ud835\udc5e for the cell line. We define true and predicted Weighted Approximate Permutation (WAP) matrices using Equation (7) as:\n\u2022 \ud835\udc12\ud835\udc43 = \ud835\udc43\ud835\udc60\ud835\udc5c\ud835\udc5f\ud835\udc61(\ud835\udc2c\ud835\udc5e)\u2297\ud835\udc30(\ud835\udf3d\ud835\udc5e): predicted WAP matrix inducedby the scores \ud835\udc2c\ud835\udc5e , \u2022 \ud835\udc12\ud835\udc47 = \ud835\udc43\ud835\udc60\ud835\udc5c\ud835\udc5f\ud835\udc61(\ud835\udf3d\ud835\udc5e) \u2297 \ud835\udc30(\ud835\udf3d\ud835\udc5e): true WAP matrix induced bythe ground truth labels \ud835\udf3d\ud835\udc5e ,\nwhere \ud835\udc30(\ud835\udf3d\ud835\udc5e) = (\ud835\udc54(\ud835\udf03\ud835\udc5e1), \ud835\udc54(\ud835\udf03\ud835\udc5e2),\u2026 , \ud835\udc54(\ud835\udf03\ud835\udc5e\ud835\udc5b\ud835\udc5e )) is the drug im-portance vector, and \ud835\udc54(\ud835\udc65) = 2\ud835\udc65 \u2212 1 is a famous LTR gain function. Here, \u2297 indicates the multiplication of columns of \ud835\udc43\ud835\udc60\ud835\udc5c\ud835\udc5f\ud835\udc61(\u22c5) by elements of the vector \ud835\udc30(\u22c5) (i.e., multiply thefirst column by the first element, second column by the second element and so on). The WAP matrix (i.e., \ud835\udc12\ud835\udc43 and \ud835\udc12\ud835\udc47 ) captures the relative position of a drug in a list. Eachcolumn of this matrix refers to a drug and the non-zero element of a column represents the predicted/true position of a drug in a ranking list. Note that if the ground truth labels of multiple drugs in a list are the same, then we have multiple ideal positions for each of them. Thus, multiple elements of those columns (i.e., drugs) will be non-zero which shows the potential predicted/true positions of those drugs in the list. We use \ud835\udc30 to force our model to focus on more sensitive drugs (i.e., it gives higher weights to more sensitive drugs) rather than insensitive drugs. Consequently, the model pushes sensitive drugs to the top of the ranking list. We use the concept of Inversion to define our loss function. Inversion can be defined as a pair of elements that are out of their correct order in a permutation. Let \ud835\udf0b be a permutation. If \ud835\udc56 < \ud835\udc57 and \ud835\udf0b(\ud835\udc56) > \ud835\udf0b(\ud835\udc57) , either the pair of places (\ud835\udc56, \ud835\udc57) or the pair of elements (\ud835\udf0b(\ud835\udc56), \ud835\udf0b(\ud835\udc57)) is called an inversion of \ud835\udf0b. We define \ud835\udeaf to capture inversions in a permutation matrix as follows:\n\ud835\udeaf(\ud835\udc2c\ud835\udc5e ,\ud835\udf3d\ud835\udc5e) = ( \ud835\udc08\u2032 \u2297 \ud835\udf37 )\u2032 , (8)\nITNR: Inversion Transformer-based Neural Ranking for Cancer Drug Recommendations: Preprint submitted to ElsevierPage 3 of 9\nITNR: Inversion Transformer-based Neural Ranking for Cancer Drug Recommendations\nits output through \ud835\udc41 encoders blocks. Finally, another fully-connected layer is used to calculate scores.\nwhere \ud835\udc08 = \ud835\udc12\ud835\udc43\ud835\udc12\u2032\ud835\udc47 \u2212\ud835\udc12\ud835\udc47 \ud835\udc12\u2032\ud835\udc47 and \ud835\udf37 = (1, 2, 3,\u2026 , \ud835\udc5b\ud835\udc5e). Precisely, \ud835\udc08 measures the deviation between predicted and true WAPs and penalizes the model for any inversion. The rows of a WAP matrix represent ranks. There exist various degrees of inversion. For instance, there is a big difference between (\ud835\udf0b(1), \ud835\udf0b(3)) and (\ud835\udf0b(1), \ud835\udf0b(7)). Our model should penalize (\ud835\udf0b(1), \ud835\udf0b(7)) more than (\ud835\udf0b(1), \ud835\udf0b(3)). To that end, we multiply the rows of the \ud835\udc08 matrix by the elements of \ud835\udf37. Therefore, our model places greater emphasis on high inversions. Eventually, we define the Inv-Rank loss of all cell lines in our training data set as follows:\n\u0302(\ud835\udc53 ) \u225c 1\u2211 \ud835\udc5e \ud835\udc5b\ud835\udc5e\n\ud835\udc47 \u2211\n\ud835\udc5e=1\n\ud835\udc5b\ud835\udc5e \u2211 \ud835\udc51=1 |\ud835\udeaf\ud835\udc51(\ud835\udc2c\ud835\udc5e ,\ud835\udf3d\ud835\udc5e) \u2032 \ud835\udfcf|, (9)\nwhere\ud835\udeaf\ud835\udc51(\ud835\udc2c\ud835\udc5e ,\ud835\udf3d\ud835\udc5e) is the \ud835\udc51-th column of\ud835\udeaf(\ud835\udc2c\ud835\udc5e ,\ud835\udf3d\ud835\udc5e). Since \ud835\udc12\ud835\udc43 isdifferentiable with respect to the elements of \ud835\udc2c [10], it is easy to show that the proposed loss function is a differentiable function of scores. Therefore, the SGD method can be used to optimize the loss function."
        },
        {
            "heading": "2.4. Data sets and Pre-processing Steps",
            "text": "The current study focused on single-drug response prediction and we designed, trained, and evaluated all models using the cell line data and drug sensitivity data from the Predictive Oncology Model & Data Clearinghouse hosted at the National Cancer Institute [40]. We used three main drug-cell line data sets from the Cancer Cell Line Encyclopedia (CCLE), the Genomics of Drug Sensitivity in Cancer (GDSC), and the Genentech Cell Line Screening Initiative (gCSI) studies; and molecular descriptors generated using\nthe Dragon 7.0 and the Mordred software packages [40]. Specifically, we represented a cell line using its RNAseq gene expression profile [36]; and we used drug descriptors and drug fingerprints to characterize a drug. The Area Under the drug response Curve (AUC) was used to quantify drug sensitivity. AUC \u2208 [0, 1] can be compared across studies and a lower value indicates higher drug sensitivity. We modified drug sensitivities so that a higher value indicates a more effective drug (i.e., one minus AUC). The details of the data sets can be found in [40]. To follow the best practice of LTR [25], the continuous drug responses were converted to graded ones. Accordingly, we classified drugs into three categories 0 (i.e., \u201cinsensitive\u201d), 1 (i.e., \u201csensitive\u201d), and 2 (i.e., \u201chighly sensitive\u201d). For the \ud835\udc58-th cell line, let \ud835\udc4380and \ud835\udc4390 denote the 80-th and 90-th percentiles of its drugresponse values {\ud835\udc5f1\ud835\udc58, \ud835\udc5f2\ud835\udc58,\u2026 , \ud835\udc5f\ud835\udc41\ud835\udc37\ud835\udc58}, respectively. Then, thedrug relevance score for the \ud835\udc56-th drug, ?\u0302?\ud835\udc56\ud835\udc58 (\ud835\udc56 = 1,\u2026 , \ud835\udc41\ud835\udc37),can be computed as:\n?\u0302?\ud835\udc56\ud835\udc58 =\n\u23a7\n\u23aa\n\u23a8\n\u23aa\n\u23a9\n2, if \ud835\udc5f\ud835\udc56\ud835\udc58 \u2265 \ud835\udc4390, 1, if \ud835\udc4380 \u2264 \ud835\udc5f\ud835\udc56\ud835\udc58 < \ud835\udc4390, 0, otherwise.\nWe also performed several pre-processing steps to reduce the training complexity and improve the overall performance. We standardize the features by subtracting the mean and scaling to unit variance [28]. Originally, RNAseq gene expressions are represented by approximately 17,000 features. Several studies [6] demonstrated that the LINCS1000 gene set [17] can outperform or achieve similar performance compared to any other superset of LINCS1000. Therefore, we\nITNR: Inversion Transformer-based Neural Ranking for Cancer Drug Recommendations: Preprint submitted to ElsevierPage 4 of 9\nITNR: Inversion Transformer-based Neural Ranking for Cancer Drug Recommendations\nonly included LINCS1000 genes in our analysis. Moreover, the data sets include 3838 molecular descriptors and 1024 path fingerprint features to represent a drug. To further reduce the dimensionality of our data and select informative features, we used a regression model as the feature selection method. Specifically, we considered drug-cell line vectors (i.e., 5862 features including 1000 genes, 3838 molecular descriptors, and 1024 path fingerprint features) and the response values (i.e., one minus AUCs) as independent and dependent variables, respectively. First, the regression model is applied to these variables and the importance of each feature is obtained through regression coefficients. After standardizing the variables, a larger absolute regression coefficient indicates that this specific variable has more influence on drug sensitivity (i.e., dependent variable). We have two groups of features, namely 1000 cell line features and 4862 drug features. We kept 500 genes out of 1000 gene features with the highest coefficient values. Similarly, we selected 500 features out of the 4862 drug features. Consequently, we used 1000 selected gene-drug features in our experiments. We repeated this procedure for all three data sets."
        },
        {
            "heading": "2.5. Performance metrics",
            "text": "Two main LTR evaluation metrics, namely NDCG@k and MRR@k are used to assess the performance of the models. Let \ud835\udc37(\ud835\udc60) = 1\u2215log(1 + \ud835\udc60) be a discount function, \ud835\udc54(\ud835\udc60) = 2\ud835\udc60 \u2212 1, a monotonically increasing gain function, and \ud835\udc5b = {(\ud835\udc311, \ud835\udc661), ..., (\ud835\udc31\ud835\udc5b, \ud835\udc66\ud835\udc5b)} a set of items orderedaccording to their ground-truth labels, with \ud835\udc31\ud835\udc56 and \ud835\udc66\ud835\udc56 beingan item feature vector and score, respectively. Moreover, let \u0303\ud835\udc5b be a ranked list for \ud835\udc5b according to the \ud835\udc32 scores. Wedefine the Discounted Cumulative Gain (DCG) of \u0303\ud835\udc5b as \u03a6(\u0303\ud835\udc5b) =\n\u2211\ud835\udc5b \ud835\udc5f=1 \ud835\udc54(\ud835\udc66\ud835\udf0b\ud835\udc5f )\ud835\udc37(\ud835\udc5f), where \ud835\udf0b\ud835\udc5f is the index of theitem ranked at position \ud835\udc5f of \u0303\ud835\udc5b. The Ideal DCG (IDCG), \u03a6\ud835\udc3c (\ud835\udc5b) is the DCG score of the ideal ranking result. NDCGnormalizes DCG by the IDCG and can be calculated by \u03a6\ud835\udc41 (\u0303\ud835\udc5b) = \u03a6(\u0303\ud835\udc5b)\u2215\u03a6\ud835\udc3c (\ud835\udc5b) \u2208 [0, 1]. To force the metricto focus on the top-k items, we use NDCG@k, which is the top-k version of NDCG, where the discount function is \ud835\udc37(\ud835\udc60) = 0 for \ud835\udc60 > \ud835\udc58. Mean Reciprocal Rank (MRR) puts a high focus on the most relevant item of a list. Assume \ud835\udc5f\ud835\udc56 denotes the rank of the most relevant item in the \ud835\udc56-thlist, then the reciprocal rank is defined as \ud835\udc45\ud835\udc56 = 1\u2215\ud835\udc5f\ud835\udc56. For \ud835\udc41 lists, the MRR is the mean of the N reciprocal ranks, MRR = 1\ud835\udc41\n\u2211\ud835\udc41 \ud835\udc56=1\ud835\udc45\ud835\udc56. MRR@k is simply the top-k version ofMRR. The significant difference between MRR and NDCG is that NDCG distinguishes between \u201cpartially sensitive\u201d and \u201chighly sensitive\u201d drugs while MRR only focuses on the most sensitive drugs. From now on, we use NDCG@k and MRR@k to denote the mean NDCG@k and the mean MRR@k (i.e., the mean of the performance metric for all lists in our test set)."
        },
        {
            "heading": "2.6. Experimental settings and hyper-parameter optimization",
            "text": "We conducted our experiments based on the standard supervised LTR framework [20]. The authors of LETOR [25]\npartitioned the LTR data sets into five parts for five-fold cross-validation where three parts were used for training, one part for validation (i.e., tuning the hyperparameters of the learning algorithms), and the remaining part for evaluating the performance of the learned model. We followed the same procedure, partitioning our data sets into five-folds, and conducting five-fold cross-validation to train the models. The hyperparameters were tuned on the validation sets (i.e., we optimized the hyperparameters to maximize the NDCG score) and the average on the test sets over the 5 folds was reported in the various tables. For more details on the parameter-tuning procedure and experimental settings, please refer to Appendix."
        },
        {
            "heading": "2.7. Competing Methods",
            "text": "We compared our proposed ranking algorithm with three types of algorithms, namely Transformer-based Neural Ranking (TNR), Deep Neural Networks (DNN), and traditional models. In recent years, the attention mechanism in transformers began a revolution in deep neural networks that led to major advances in the performance of many models obtained in this fashion. We compared our model with four TNR models (i.e., NDCGLoss2++ [38], ListMLE [41], ApproxNDCG [24], and RankNet [4]) to ensure its superior performance and stability compared to similar methodologies. Second, we also compared ITNR with other deep learning-based models. To that end, we compared ITNR with the so-called DNN-Sakellaropoulos (DNN-S) [29]. DNN-S has been reported as one of the best DNN models in the DRP literature [18, 35]. Third, several DRP comparative studies [11, 8, 26] reported tree-based models as one of the best-performing algorithms for drug recommendation. Specifically, LambdaMARTMAP [39] hasbeen shown repeatedly to surpass other LTR methods such as Coordinate Ascent, Random Forests, BoltzRank, RankBoost, AdaRank, SoftRank, and so on [8, 3, 38]. Finally, we compared ITNR with Elastic Net Regression (ENR) as a traditional DRP model. Multiple studies [14, 35, 9] have suggested that elastic net will most likely yield the most accurate predictors for drug response prediction."
        },
        {
            "heading": "3. Experimental Results",
            "text": "In Table 1, we summarized the performance of the models on various DRP data sets. We report NDCG@k and MRR@k, both computed out-of-sample (i.e., test set not used for training the model). The average on the test set over the 5 folds was reported. Bold and underlined numbers indicate the best performance among all methods for each metric. Bold numbers demonstrate the second-best performance among all methods for each metric. The best model for the CCLE data set (474 unique samples and 24 unique drugs) is ITNR with an NDCG@10 of 94.03% and an MRR@10 of 93.90%, with the ENR model close behind (NDCG@10 of 93.85% and MRR@10 of 93.64%). While DNN-S achieved the highest MRR among the baseline models, ITNR outperformed it by a relatively large margin. Moreover, although NDCG@5 of TNR-RankNet\nITNR: Inversion Transformer-based Neural Ranking for Cancer Drug Recommendations: Preprint submitted to ElsevierPage 5 of 9"
        },
        {
            "heading": "Performance Comparison of Ranking Methods on CCLE, GDSC, and gCSI Data Sets.",
            "text": "is higher than our model, it does not have comparable performance considering other evaluation metrics. On the gCSI data set (357 unique samples and 16 unique drugs), ITNR demonstrates consistent performance improvement overall baseline models across all performance metrics and any of the chosen rank cutoffs. Notably, we observe a 2.6% performance improvement compared to the average of baseline models (i.e., 78.34%) in terms of NDCG@5. Moreover, ENR which performed really well on the CCLE data set demonstrated poor performance on the gCSI data set. TNR-RankNet and LambdaMART models achieved moderate performance and are the second-best methods. The models trained on gCSI did not generalize well. This was not surprising as gCSI had the smallest number of drugs and was thus prone to overfitting. Nevertheless, our method is able to maintain its high performance.\nIn general, the results for these two data sets indicate that ITNR significantly outperforms other competing methods. All models except ENR and LambdaMART are Deep Learning (DL) based models. Since DL models are complex and have many learnable parameters, they tend to overfit easier than traditional models. Due to this, DL algorithms perform better when trained on large amounts of data. To that end, we also conducted experiments on the GDSC data set (670 unique cell lines and 233 unique drugs) which is one of the largest public DRP data sets. Generally, DL models outperformed other traditional methods (i.e., ENR), since a\nmodel like ENR may not fully capture the structural information within drugs. Among transformer-based baselines, we observed that ITNR achieved the best performance, with 83.36% for NDCG@5 and 92.74% for MRR@5. NDCG@25 of LambdaMART is higher than our model. However, this model is not even the second-best model considering other evaluation metrics. Among the competing models, TNRRankNet outperformed other methods by a relatively large margin. All in all, ITNR consistently outperforms all baseline methods across all metrics and data sets. In our experiment on three DRP data sets, TNR-RankNet and LambdaMART demonstrated reasonably good overall performance and they are the second-best methods. ITNR is not only able to push the most sensitive drugs to the top of the ranking list, but it can put them in the right order. Further, it can also generalize better and achieve superior performance on large DRP data sets like GDSC."
        },
        {
            "heading": "4. Discussion and Conclusion",
            "text": "Our study presented a novel transformer-based model, called Inversion Transformer-based Neural Ranking (ITNR), to predict cancer drug response. Our model used the wellknown Transformer architecture to extract a better drugcell line representation. We also developed a novel loss function based on the concept of inversion and approximate\nITNR: Inversion Transformer-based Neural Ranking for Cancer Drug Recommendations: Preprint submitted to ElsevierPage 6 of 9\nITNR: Inversion Transformer-based Neural Ranking for Cancer Drug Recommendations\npermutation matrices. Our results suggest that the transformer network with multi-head attention is suitable for modeling the interactions of drug substructure and multiomics data. Extensive experimental results demonstrated that our model is more effective than the current state-ofthe-art methods highlighting the predictive capability of our model and its potential translational value in personalized medicine. There are several directions for future work. In the current model, we use an approximated permutation matrix (i.e., NeuralSort) as the sorting operator. We can replace NeuralSort with another approximation of a sorting operator such as the Optimal Transport [7] and SoftSort [23]. Furthermore, the current model disregards the side effects and toxicity of drugs when predicting the best medication option. We can further optimize our recommendations by including the toxicity of drugs in our predictions.\nCRediT authorship contribution statement Shahabeddin Sotudian: Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Validation, Visualization, Writing \u2013 Original Draft Preparation. Ioannis Ch. Paschalidis: Conceptualization, Formal analysis, Funding acquisition, Investigation, Methodology, Project administration, Resources, Supervision, Writing - review and editing."
        },
        {
            "heading": "Appendix",
            "text": ""
        },
        {
            "heading": "A. Hyper-parameter optimization",
            "text": "The list of hyper-parameters and their values for all ranking algorithms can be found in Table 2. In this table, \ud835\udc3f is the list length used for training (note that a list was either padded or sub-sampled to that length), \ud835\udc51fc is the dimensionof the linear projection, \ud835\udc41 is the number of encoder blocks, \ud835\udc51\u210e is the transformer hidden dimension, \ud835\udc3b is the numberof attention heads, \ud835\udc37\ud835\udc5f refers to the hidden dropout ratio, \ud835\udc35 is the batch size, \ud835\udc38 is the number of epochs, \ud835\udcc12 is theparameter of the \ud835\udcc12-norm penalty for the DNN-S model, \ud835\udf02is the learning rate, \ud835\udc37\ud835\udc5a\ud835\udc4e\ud835\udc65 is the maximum depth of a tree, \u210e\ud835\udc5a\ud835\udc56\ud835\udc5b is the minimum sum of the instance weight (Hessian)needed in a leaf, \ud835\udc41\ud835\udc47 is the number of estimators, \ud835\udefc is theregularization strength of the model, and \ud835\udc45\ud835\udcc11 controls thecontribution of the \ud835\udcc11 and \ud835\udcc12 penalties in the ENR model.The details of hyper-parameter settings of all methods for the five folds can be found in Tables 3, 4, 5, and 6."
        },
        {
            "heading": "The List of Hyper-parameters and Their Values.",
            "text": ""
        },
        {
            "heading": "Best Parameters - Transformer Models",
            "text": ""
        },
        {
            "heading": "Best Parameters - DNN-S",
            "text": "Data Sets Folds \ud835\udc35 \ud835\udc38 \ud835\udc37\ud835\udc5f \ud835\udcc12\nCCLE\nFold 1 25 50 0.3 0.0001 Fold 2 25 50 0.5 0.01 Fold 3 25 50 0.1 0.01 Fold 4 25 50 0.3 0.001 Fold 5 25 50 0.5 0.01\nGDSC\nFold 1 25 50 0.3 0.001 Fold 2 25 50 0.5 0.01 Fold 3 25 50 0.3 0.01 Fold 4 25 50 0.3 0.001 Fold 5 25 50 0.3 0.01\ngCSI\nFold 1 25 50 0.3 0.001 Fold 2 25 50 0.3 0.001 Fold 3 25 50 0.3 0.01 Fold 4 25 50 0.3 0.01 Fold 5 25 50 0.3 0.01\n[3] Sebastian Bruch. An alternative cross entropy loss for learning-torank. In Proceedings of the Web Conference 2021, pages 118\u2013126, 2021. [4] Christopher JC Burges. From ranknet to lambdarank to lambdamart: An overview. Learning, 11(23-581):81, 2010. [5] Jinyu Chen and Louxin Zhang. A survey and systematic assessment of computational methods for drug response prediction. Briefings in\nITNR: Inversion Transformer-based Neural Ranking for Cancer Drug Recommendations: Preprint submitted to ElsevierPage 7 of 9\nITNR: Inversion Transformer-based Neural Ranking for Cancer Drug Recommendations"
        },
        {
            "heading": "Best Parameters - LambdaMART",
            "text": ""
        },
        {
            "heading": "Best Parameters - ENR",
            "text": ""
        },
        {
            "heading": "Data Sets Folds \ud835\udefc \ud835\udc45\ud835\udcc11",
            "text": "Hyunseung Yoo, Yvonne Evrard, Yitan Zhu, Fangfang Xia, and Rick Stevens. A systematic approach to featurization for cancer drug sensitivity predictions with deep learning. arXiv preprint arXiv:2005.00095, 2020. [7] Marco Cuturi, Olivier Teboul, and Jean-Philippe Vert. Differentiable ranking and sorting using optimal transport. Advances in neural information processing systems, 32, 2019. [8] Carlos De Niz, Raziur Rahman, Xiangyuan Zhao, and Ranadip Pal. Algorithms for Drug Sensitivity Prediction. Algorithms, 9(4):77, December 2016. Number: 4 Publisher: Multidisciplinary Digital Publishing Institute. [9] Paul Geeleher, Nancy J. Cox, and R. Stephanie Huang. Clinical drug response can be predicted using baseline gene expression levels and in vitrodrug sensitivity in cell lines. Genome Biology, 15(3):R47, March 2014. [10] Aditya Grover, Eric Wang, Aaron Zweig, and Stefano Ermon. Stochastic optimization of sorting networks via continuous relaxations. arXiv preprint arXiv:1903.08850, 2019. [11] Saad Haider, Raziur Rahman, Souparno Ghosh, and Ranadip Pal. A Copula Based Approach for Design of Multivariate Random Forests for Drug Sensitivity Prediction. PLoS ONE, 10(12), December 2015. [12] Boran Hao, Yang Hu, Shahabeddin Sotudian, Zahra Zad, William G Adams, Sabrina A Assoumou, Heather Hsu, Rebecca G Mishuris, and Ioannis Ch. Paschalidis. Development and validation of predictive models for covid-19 outcomes in a safety-net hospital population. Journal of the American Medical Informatics Association, 2022.\n[13] Boran Hao, Shahabeddin Sotudian, Taiyao Wang, Tingting Xu, Yang Hu, Apostolos Gaitanidis, Kerry Breen, George C Velmahos, and Ioannis Ch. Paschalidis. Early prediction of level-of-care requirements in patients with covid-19. Elife, 9:e60519, 2020. [14] In Sock Jang, Elias Chaibub Neto, Justin Guinney, Stephen H. Friend, and Adam A. Margolin. Systematic assessment of analytical methods for drug sensitivity prediction from cancer cell line data. In Pacific Symposium on Biocomputing, pages 63\u201374. WORLD SCIENTIFIC, November 2013. [15] In Sock Jang, Elias Chaibub Neto, Justin Guinney, Stephen H Friend, and Adam A Margolin. Systematic assessment of analytical methods for drug sensitivity prediction from cancer cell line data. In Biocomputing 2014, pages 63\u201374. World Scientific, 2014. [16] Hiba Kobeissi, Saeed Mohammadzadeh, and Emma Lejeune. Enhancing mechanical metamodels with a generative model-based augmented training dataset. Journal of Biomechanical Engineering, 144(12):121002, 2022. [17] Amar Koleti, Raymond Terryn, Vasileios Stathias, Caty Chung, Daniel J Cooper, John P Turner, Du\u0161ica Vidovi\u0107, Michele Forlin, Tanya T Kelley, Alessandro D\u2019Urso, et al. Data portal for the library of integrated network-based cellular signatures (lincs) program: integrated access to diverse large-scale cellular perturbation response data. Nucleic acids research, 46(D1):D558\u2013D566, 2018. [18] JungHo Kong, Doyeon Ha, Juhun Lee, Inhae Kim, Minhyuk Park, Sin-Hyeog Im, Kunyoo Shin, and Sanguk Kim. Network-based machine learning approach to predict immunotherapy response in cancer patients. Nature communications, 13(1):1\u201315, 2022. [19] Jun Li, Qing Lu, and Yalu Wen. Multi-kernel linear mixed model with adaptive lasso for prediction analysis on high-dimensional multiomics data. Bioinformatics, 36(6):1785\u20131794, 2020. [20] Tie-Yan Liu. Learning to rank for information retrieval. 2011. [21] Przemys\u0142aw Pobrotyn, Tomasz Bartczak, Miko\u0142aj Synowiec, Ra-\ndos\u0142aw Bia\u0142obrzeski, and Jaros\u0142aw Bojar. Context-aware learning to rank with self-attention. arXiv preprint arXiv:2005.10084, 2020. [22] Przemys\u0142aw Pobrotyn and Rados\u0142aw Bia\u0142obrzeski. Neuralndcg: Direct optimisation of a ranking metric via differentiable relaxation of sorting. arXiv preprint arXiv:2102.07831, 2021. [23] Sebastian Prillo and Julian Eisenschlos. Softsort: A continuous relaxation for the argsort operator. In International Conference on Machine Learning, pages 7793\u20137802. PMLR, 2020. [24] Tao Qin, Tie-Yan Liu, and Hang Li. A general approximation framework for direct optimization of information retrieval measures. Information retrieval, 13(4):375\u2013397, 2010. [25] Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. Letor: A benchmark collection for research on learning to rank for information retrieval. Information Retrieval, 13(4):346\u2013374, 2010. [26] Gregory Riddick, Hua Song, Susie Ahn, Jennifer Walling, Diego Borges-Rivera, Wei Zhang, and Howard A. Fine. Predicting in vitro drug sensitivity using Random Forests. Bioinformatics, 27(2):220\u2013 224, January 2011. Publisher: Oxford Academic. [27] Xiaoqing Ru, Xiucai Ye, Tetsuya Sakurai, and Quan Zou. Application of learning to rank in bioinformatics tasks. Briefings in Bioinformatics, 2021. [28] Ali Akbar Sadat Asl, Mohammad Mahdi Ershadi, Shahabeddin Sotudian, X Li, and S Dick. Fuzzy expert systems for prediction of icu admission in patients with covid-19. Intelligent Decision Technologies, (Preprint):1\u201310, 2022. [29] Theodore Sakellaropoulos, Konstantinos Vougas, Sonali Narang, Filippos Koinis, Athanassios Kotsinas, Alexander Polyzos, Tyler J Moss, Sarina Piha-Paul, Hua Zhou, Eleni Kardala, et al. A deep learning framework for predicting response to therapy in cancer. Cell reports, 29(11):3367\u20133373, 2019. [30] Aman Sharma and Rinkle Rani. Drug sensitivity prediction framework using ensemble and multi-task learning. International Journal of Machine Learning and Cybernetics, 11(6):1231\u20131240, 2020. [31] Richard Sinkhorn. A relationship between arbitrary positive matrices and doubly stochastic matrices. The annals of mathematical statistics, 35(2):876\u2013879, 1964.\nITNR: Inversion Transformer-based Neural Ranking for Cancer Drug Recommendations: Preprint submitted to ElsevierPage 8 of 9\nITNR: Inversion Transformer-based Neural Ranking for Cancer Drug Recommendations\n[32] Shahabeddin Sotudian, Aaron Afran, Christina A LeBedis, Anna F Rives, Ioannis Ch Paschalidis, and Michael DC Fishman. Social determinants of health and the prediction of missed breast imaging appointments. BMC Health Services Research, 22(1):1\u201311, 2022. [33] Shahabeddin Sotudian, Ruidi Chen, and Ioannis Ch. Paschalidis. Distributionally robust multi-output regression ranking. arXiv preprint arXiv:2109.12803, 2021. [34] Shahabeddin Sotudian, Israel T Desta, Nasser Hashemi, Shahrooz Zarbafian, Dima Kozakov, Pirooz Vakili, Sandor Vajda, and Ioannis Ch. Paschalidis. Improved cluster ranking in protein\u2013protein docking using a regression approach. Computational and structural biotechnology journal, 19:2269\u20132278, 2021. [35] Shahabeddin Sotudian and Ioannis Ch. Paschalidis. Machine learning for pharmacogenomics and personalized medicine: A ranking model for drug sensitivity prediction. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 2021. [36] Shahabeddin Sotudian and Mohammad Hossein Fazel Zarandi. Interval type-2 enhanced possibilistic fuzzy c-means clustering for gene expression data analysis. arXiv preprint arXiv:2101.00304, 2021. [37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [38] Xuanhui Wang, Cheng Li, Nadav Golbandi, Michael Bendersky, and Marc Najork. The lambdaloss framework for ranking metric optimization. In Proceedings of the 27th ACM international conference on information and knowledge management, pages 1313\u20131322, 2018. [39] Qiang Wu, Christopher JC Burges, Krysta M Svore, and Jianfeng Gao. Adapting boosting for information retrieval measures. Information Retrieval, 13(3):254\u2013270, 2010. [40] Fangfang Xia, Jonathan Allen, Prasanna Balaprakash, Thomas Brettin, Cristina Garcia-Cardona, Austin Clyde, Judith Cohn, James Doroshow, Xiaotian Duan, Veronika Dubinkina, et al. A cross-study analysis of drug response prediction in cancer cell lines. Briefings in bioinformatics, 23(1):bbab356, 2022. [41] Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. Listwise approach to learning to rank: theory and algorithm. In Proceedings of the 25th International Conference on Machine Learning, pages 1192\u20131199, 2008. [42] Mohammad Hossein Fazel Zarandi, Shahabeddin Sotudian, and Oscar Castillo. A new validity index for fuzzy-possibilistic c-means clustering. arXiv preprint arXiv:2005.09162, 2020.\nITNR: Inversion Transformer-based Neural Ranking for Cancer Drug Recommendations: Preprint submitted to ElsevierPage 9 of 9"
        }
    ],
    "title": "ITNR: Inversion Transformer-based Neural Ranking for Cancer Drug Recommendations",
    "year": 2023
}