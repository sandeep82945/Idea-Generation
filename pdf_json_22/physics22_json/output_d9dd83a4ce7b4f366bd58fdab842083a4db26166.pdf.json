{
    "abstractText": "Non-equilibrium chemistry is a key process in the study of the InterStellar Medium (ISM), in particular the formation of molecular clouds and thus stars. However, computationally it is among the most difficult tasks to include in astrophysical simulations, because of the typically high (>40) number of reactions, the short evolutionary timescales (about 104 times less than the ISM dynamical time) and the characteristic non-linearity and stiffness of the associated Ordinary Differential Equations system (ODEs). In this proof of concept work, we show that Physics Informed Neural Networks (PINN) are a viable alternative to traditional ODE time integrators for stiff thermo-chemical systems, i.e. up to molecular hydrogen formation (9 species and 46 reactions). Testing different chemical networks in a wide range of densities (\u22122 < log n/cm\u22123 < 3) and temperatures (1 < logT/K < 5), we find that a basic architecture can give a comfortable convergence only for simplified chemical systems: to properly capture the sudden chemical and thermal variations a Deep Galerkin Method is needed. Once trained (\u223c 103 GPUhr), the PINN well reproduces the strong non-linear nature of the solutions (errors < \u223c 10%) and can give speed-ups up to a factor of\u223c 200with respect to traditional ODE solvers. Further, the latter have completion times that vary by about \u223c 30% for different initial n and T , while the PINN method gives negligible variations. Both the speed-up and the potential improvement in load balancing imply that PINN-powered simulations are a very palatable way to solve complex chemical calculation in astrophysical and cosmological problems.",
    "authors": [
        {
            "affiliations": [],
            "name": "L. Branca"
        },
        {
            "affiliations": [],
            "name": "A. Pallottini"
        }
    ],
    "id": "SP:34faaab3d37b298b0e0c77bfc91eb986354793e6",
    "references": [
        {
            "authors": [
                "M Abadi"
            ],
            "title": "TensorFlow: Large-Scale Machine Learning on Het",
            "year": 2015
        },
        {
            "authors": [
                "M. J"
            ],
            "title": "Automatic differentiation in machine learning: a survey",
            "year": 2015
        },
        {
            "authors": [
                "C. Hindmarsh A"
            ],
            "title": "ODEPACK: Ordinary differential equation solver",
            "year": 2019
        },
        {
            "authors": [
                "M C"
            ],
            "title": "Adam: A Method for Stochastic Optimization",
            "venue": "Chemistry A,",
            "year": 2020
        },
        {
            "authors": [
                "A. Kumar",
                "T. Fisher R"
            ],
            "title": "DeepXDE: A deep",
            "venue": "Mathematical Programming,",
            "year": 2013
        },
        {
            "authors": [
                "U. Maio",
                "K. Dolag",
                "B. Ciardi",
                "L. Tornatore"
            ],
            "title": "learning library for solving differential equations (arXiv:1907.04502",
            "venue": "Lupi A.,",
            "year": 2019
        },
        {
            "authors": [
                "G. Ucci",
                "A. Ferrara",
                "A. Pallottini",
                "S. Gallerani"
            ],
            "title": "PINN to solve ISM chemistry",
            "venue": "Reference Manual. CreateS-",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Key words: ISM: evolution, molecules \u2013 methods: numerical \u2013 software: development"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Thermal and chemical evolution are crucial processes in astrophysical and cosmological environments. Gas cooling, heating, ionization, and photo-dissociation are vital drivers of the evolution of the interstellar (ISM) and intergalactic (IGM) medium. Chemical processes are widely included in theoretical works and numerical simulations, playing key roles in determining the evolution in the early Universe history (Galli & Palla 1998; Glover & Abel 2008), the IGM during the Epoch of the Reionization (Maio et al. 2007; Theuns et al. 1998), galaxy formation and evolution (Pallottini et al. 2017; Lupi 2019), and Giant Molecular Cloud (GMC Kim et al. 2018; Decataldo et al. 2019).\nThe typical chemical network involves hydrogen, helium, (possibly individual) metals, and molecules, by further coupling of all these various species with the radiation field (i.e. photoheating and photoionization), dynamics (i.e. shocks) and interaction with dust; moreover, going to progressively smaller scales, the chemical processes become more and more complex. To follow a non-equilibrium chemical and thermal evolution in a numerical simulation is necessary an additional set of Ordinary Differential Equations (ODEs) that describe the coupling of the various species.\n\u2605 lorenzo.branca@sns.it\nVarious numerical schemes and implementations have been developed to accomplish this task: krome (Grassi et al. 2014), xdeload (Nejad 2005), astrochem (Kumar & Fisher 2013), alchemic (Semenov et al. 2010), grackle (Smith et al. 2017), and ggchempy (Ge 2022). The strategy/approximations adopted in a specific implementation can vary, however all schemes have to face similar key problems: i) the chemical ODE system is often stiff and ii) the typical time-scales are much shorter than the dynamical/hydrodynamic time , e.g. \u0394\ud835\udc61\ud835\udc50\u210e\ud835\udc52\ud835\udc5a 10\u22124\u0394\ud835\udc61\u210e\ud835\udc66\ud835\udc51\ud835\udc5f\ud835\udc5c. Thus, adopting robust multistep implicit schemes is needed in order to numerically solve the ODEs with procedural methods (Byrne & Hindmarsh 1987). Such schemes often rely on matrix inversion, which \u2013 at face value \u2013 has a computational complexity that scales as O(\ud835\udc413\ud835\udc60\ud835\udc5d\ud835\udc52\ud835\udc50), with \ud835\udc41\ud835\udc60\ud835\udc5d\ud835\udc52\ud835\udc50 being the number of species in the network. The matrix associated with chemical ODEs is often sparse, which ameliorate the burden of the inversion (Grassi et al. 2021); nonetheless, systems experience a fast growth of the computational cost in including progressively more complex chemical network, ultimately making the CPU time spent on chemistry a relevant fraction of a hydrodynamical simulation. Further, the precise computational time for the inversion is difficult to estimate given the ODEs, which thus can spoil the load-balancing of the typical astrophysical code.\nThus \u2013 in order to try to overcome these limitations \u2013 it is interesting to consider emulators as possible fast alternatives to a\n\u00a9 2022 The Authors\nar X\niv :2\n21 1.\n15 68\n8v 1\n[ as\ntr o-\nph .G\nA ]\n2 8\nN ov\n2 02\nprocedural resolution of chemical networks. The usage of emulators in astrophysics, based on machine learning (ML) and deep learning techniques, has steadily increased its importance in recent years (LeCun et al. 2015). However, so far ML applications in astrophysics are mainly limited to data driven inferences as parameters or classifications; for example Ucci et al. (2018) use decision trees to infer key ISM physical properties from emission line ratios, Chardin et al. (2019) emulates radiative transfer calculation using Epoch of Reionization simulations as a training dataset, Prelogovi\u0107 et al. (2022) infer astrophysical parameters from 21 cm light cone images by adopting recurrent neural networks, and Dropulic et al. (2021) shows how to predict stellar line-of-sight velocity from Gaia observations of the Milky Way (MW) by training on phase space mock data sets. Currently, there are few attempts to alleviate the cost of computing chemistry by using auto-encoders: Grassi et al. (2021) tries to reduce the complexity of chemical ODE with high dimensionality by compression in a latent space of a smaller dimension; while Grassi et al. (2021) showcased the approach for isothermal models, its generalization seems non-trivial. On the other hand, Holdship et al. (2021) recently implement a fast emulator that includes temperature evolution; it works combining auto-encoders (to reduce the dimensionality) and emulators to follow the temperature and abundances evolution. All these solutions seem encouraging, promoting future usage of emulators for thermo-chemistry in hydrodynamic simulations. However, as these are supervised learning strategies, they require the creation of datasets containing the solution of the ODEs system using a traditional solver.\nAn intriguing alternative consist in direct approximation of the solution for the chemical system with a neural network (NN), so that once the training is complete the chemical evolution can be directly computed at lower computational cost. Solving differential equations with NN can be done adopting the Neural Ordinary Differential Equation (NODE)model (Chen et al. 2018), that useRecurrent Neural Networks with an high number of layers to emulate the discretization of a differential operator. Amore general approach consist in using Physics Informed Neural Network (PINN) (Raissi et al. 2019). The PINN method is very flexible and has been used to solve a large class of differential system associated with wide range of physical problems, as 2D acoustic wave equations (Moseley et al. 2020), turbulent fluid-dynamics (Hennigh et al. 2020) or full radiative transfer calculation (Mishra & Molinaro 2021).\nIn this work we choose to focus on the PINN, in part because of its flexibility and in part since the mode seems simpler to extend with respect to the original implementation presented in Raissi et al. (2019), by using the eXtended Physics Informed Neural Network (X-PINN, Hu et al. 2022). While various implementation of the PINN frameworks have been developed (Lu et al. 2019; Haghighat & Juanes 2021; Rackauckas et al. 2019; Hennigh et al. 2020), applications in the astrophysical and cosmological context has yet to be explored, except in Chantada et al. (2022), where PINN (in this works named Cosmological Informed neural Networks, CINN) are used to solve the background dynamics of the universe for four different models and then to perform statistical analyses to estimate the values of each model\u2019s parameters with observational data.\nTo our knowledge, only Ji et al. (2021) performs an in-depth study of stiff chemical systems via PINN, by showcasing the solution of standard ODE benchmarks, as the ROBER (3 non-linear ODEs) and POLLU problems (20 non-linear ODEs); however, the adopted reaction coefficients are constant, while for the typical ISM network they can vary by several orders of magnitude, mainly due\nto temperature1; such dependence is key for astrophysical problems and promotes the system from chemical to thermo-chemical, thus making its resolution more complex.\nIn this proof of concept work, we show that it is possible to train a PINN to emulate a complex and realistic chemical network that can be used to simulate the ISM. Our aim is to showcase the PINN scheme in a non-trivial astrophysical context, by proposing an efficient and accurate alternative to the procedural solvers.\nIn Sec. 2 we introduce chemical networks and their usage in astrophysical context (Sec. 2.1),we present the general framework of the PINNmethod (Sec. 2.2), andwe benchmark PINN for simplified ODE systems (Sec. 2.3). Then, in Sec. 3, we detail and implement the further modelling needed to adopt the PINN framework to treat realistic ISM chemical networks. Sec. 4 presents the results of our models in terms of accuracy, efficiency, and in comparison with procedural solvers. In Sec. 5 we give our conclusions."
        },
        {
            "heading": "2 METHODS OVERVIEW",
            "text": "In this Sec. we provide a summary of the thermal and chemical evolution of the ISM (Sec. 2.1), present the procedural tool used as reference to test our results, and introduce the usage of neural network solvers, in particular focusing on PINN algorithm (Sec. 2.2), we benchmark the model for a chemical and thermo-chemical like ODEs system (Sec. 2.3)."
        },
        {
            "heading": "2.1 Chemistry of the Interstellar medium",
            "text": "Given a chemical network, following its evolution entails knowing the density of each species and the temperature at each time step. This chemical and thermal evolution can be described by an ODEs system, with one equation for each species involved and one for the temperature evolution. Each equation is given by a sum of terms that depends on the chemical reaction involved.\nIn general, reactions depends on the number density of each\n1 Formally, this is correctwhen only two body reactions are considered; for a general chemical network, further variation are present, e.g. photoionization reactions are proportional to the radiation field and, in cosmic ray induced reactions, their flux plays a similar role.\nMNRAS 000, 1\u201316 (2022)\nspecies (n = \ud835\udc5b1, . . . , \ud835\udc5b\ud835\udc41\ud835\udc60\ud835\udc5d\ud835\udc52\ud835\udc50 ), the temperature (\ud835\udc47) and the radiative flux (ionizing photons, photo-dissociation photons, cosmic rays, ...) The ODEs system for the chemical species can be written as (e.g. Grassi et al. 2014) \u00a4\ud835\udc5b\ud835\udc58 = \u2211\ufe01\n\ud835\udc57\u2208reaction\ud835\udc58 \u00a9\u00ab\ud835\udc4e \ud835\udc57 \u220f \ud835\udc5f \u2208reactant \ud835\udc57 \ud835\udc5b\ud835\udc5f ( \ud835\udc57) \u00aa\u00ae\u00ac , (1) where \ud835\udc5b\ud835\udc58 is the k-th species, and \ud835\udc4e \ud835\udc57 are the rate coefficients for all the reactions considered in the chemical network.\nConsidering only 2-body reactions and photo-reactions2, eq. 1 can be re-written as:\n\u00a4\ud835\udc5b\ud835\udc58 = \ud835\udc34 \ud835\udc56 \ud835\udc57 \ud835\udc58 \ud835\udc5b\ud835\udc56\ud835\udc5b \ud835\udc57 + \ud835\udc35\ud835\udc56\ud835\udc58\ud835\udc5b\ud835\udc56 , (2)\nwhere \ud835\udc34\ud835\udc56 \ud835\udc57 \ud835\udc58 = \ud835\udc34\ud835\udc56 \ud835\udc57 \ud835\udc58 (\ud835\udc47, n) are 2-body reaction coupling coefficients, \ud835\udc35\ud835\udc56 \ud835\udc58 = \ud835\udc35\ud835\udc56 \ud835\udc58 (F) describe the photo-reactions rates, with F quantifying the photon and cosmic ray flux in various energy bins. The evolution of the thermal state of the gas is accounted evolving the gas temperature, that depends on the heating and cooling process (chemical, radiative, ...):\n\u00a4\ud835\udc47 = (\ud835\udefe \u2212 1) \ud835\udc58\ud835\udc4f \u2211 \ud835\udc56 \ud835\udc5b\ud835\udc56 (\u0393 \u2212 \u039b) , (3)\nwhere \ud835\udc58\ud835\udc4f is the Boltzmann constant, \ud835\udefe is the gas adiabatic index, \u0393 = \u0393(\ud835\udc47, n,F) and \u039b = \u039b(\ud835\udc47, n,F) are the heating and cooling functions, respectively.\nIn this work we focus on a ISM chemical network originally presented in Bovino et al. (2016) and that has been used for studies on molecular cloud scales (Decataldo et al. 2019, 2020) and the on evolution of high-redshift galaxies (Pallottini et al. 2017, 2019, 2022). The network has \ud835\udc41\ud835\udc60\ud835\udc5d\ud835\udc52\ud835\udc50 = 9 species: e\u2212,H\u2212,H,H+, He,He+, He++, H2, and H+2 . Following Bovino et al. (2016), the evolution of the species is regulated by 46 reactions (for a schematic view, see Fig. 1), involving dust processes, i.e. H2 formation on dust grains (Jura 1975), photo-chemistry, and cosmic rays ionization. In particular, the rates are taken from Bovino et al. (2016): reactions 1 to 31, 53, 54, and from 58 to 61 in their tables B.1 and B.2, photo-reactions P1 to P9 in their table 2.\nFor the temperature evolution (eq. 3) we account for the following processes: photoelectric heating from dust (Bakes & Tielens 1994), cosmic rays heating (Cen 1992), photo heating, heating/cooling due to exothermic/endothermic reactions, metal line cooling (Shen et al. 2013), Compton cooling from the CMB, molecular H2 cooling (Glover & Abel 2008), and atomic cooling (Cen 1992). For simplicity, in this work, we adopt a constant solar value for the metallicity (\ud835\udc4d = \ud835\udc4d , Asplund et al. (2009)) and dust to gas ratio ( \ud835\udc53\ud835\udc51 = 0.3, Hirashita & Ferrara (2002)).\nRecall that, the two body reactions (\ud835\udc34\ud835\udc56 \ud835\udc57 \ud835\udc58 , eq. 2) depends only on density n and the temperature\ud835\udc47 , however the coefficients of (\ud835\udc35\ud835\udc56 \ud835\udc58 , eq. 2), and the heating and cooling terms (\u0393 and \u039b, eq. 3) additionally depends on the flux F.\nFor simplicity, in this work we consider a Spectral Energy Distribution (SED) of UV/Xray background from Haardt & Madau (2012) at redshift \ud835\udc67 = 0 and adoptMW like cosmic ray flux with rate \ud835\udf01\ud835\udc50\ud835\udc5f = 3\u00d710\u221217s\u22121. This SED is not completely appropriate for the typical ISMconditions, (e.g. seeDraine 1978, for theMW), however we adopt it so that all photo-ionizations (H+ \ud835\udefeh\ud835\udf08>13.6eV \u2192 H+ + e,\n2 Neglecting secondary higher order processes, also reaction involving cosmic rays can be approximated with the same formalism, see e.g. Bovino et al. (2016).\n...) in our chemical network are active, i.e. this choice allow us to robustly test all the reactions in the model.\nVarious implementations/schemes can be adopted to solve a chemical network. As a reference for this work, we adopt the flexible code krome3 (Grassi et al. 2014), which is a framework that \u2013 given an input chemical network \u2013 generates the code to solve the associated ODE system. To solve the system krome use lsodes, which is included in odepack (Hindmarsh 2019). lsodes is an implicit, robust, multistep, iterative high order solver (5 by default) that can take advantage of the sparsity of the Jacobian matrix of the ODEs. The default krome relative and absolute tolerances are fixed at 10\u22124 and 10\u221220 respectively.\nIn this work, we adopt krome i) to build the ODEs structure (eq.s 2 and 3) for our PINN scheme (Sec. 3) and ii) to test our results during the validation phase (Sec. 4)."
        },
        {
            "heading": "2.2 Physics Informed Neural Network",
            "text": "In general, we can write a set of partial differential equations (PDE)/ODE4 in the form\nD(\ud835\udc62(x)) = \ud835\udc53 (x), \u2200x \u2208 \u03a9, (4a) B(\ud835\udc62(x)) = \ud835\udc54(x), \u2200x \u2208 \ud835\udf15\u03a9 , (4b)\nwhere x is the set of independent variables,D is the differential operator of the PDE/ODE, B is a constrain operator \u2013 i.e. it represents the boundary/initial conditions (BC/IC) \u2013 and \ud835\udc62(x) is the solution of the PDE/ODE system.\nOur aim is to approximate the solution of the system with a neural network (see Wang & Raj 2017, for a review analyzing the theoretical and practical aspects). In principle, this is possible because of the universal approximation theorem (Cybenko 1989), since multi-layer feed-forward neural networks are capable of approximating any Borel measurable function (Hornik et al. 1989). Namely, it is formally possible to replace the PDE/ODE solution"
        },
        {
            "heading": "3 https://bitbucket.org/tgrassi/krome/src/master/",
            "text": "4 While in the present we are focusing on ODE systems associated with a chemical network, the PINN scheme can also be applied to PDE.\nMNRAS 000, 1\u201316 (2022)\n\ud835\udc62(x) with the output of the neural network \ud835\udc62\ud835\udc5b\ud835\udc52\ud835\udc61 (x, \ud835\udf03), which can be written as\n\ud835\udc62\ud835\udc5b\ud835\udc52\ud835\udc61 (x, \ud835\udf03) = W\ud835\udc5b (\ud835\udf19\ud835\udc5b\u22121 \u25e6 \ud835\udf19\ud835\udc5b\u22122 \u25e6 ... \u25e6 \ud835\udf191\ud835\udf190) (x) + b\ud835\udc5b , (5)\ni.e. the composition of the action of successive layers \ud835\udf19\ud835\udc56\n\ud835\udf19\ud835\udc56 (x) = \ud835\udf0e(W\ud835\udc56x + b\ud835\udc56) , (6)\nwhere \ud835\udf0e is the activation function,W\ud835\udc56 is the matrix of the weights and b\ud835\udc56 is the bias vector. It is convenient to clusterW\ud835\udc56 and b\ud835\udc56 as\n\ud835\udf03 = {W, b} , (7)\ni.e. the set of the parameters to be trained. For the PINN method, the necessary condition is that \ud835\udc62\ud835\udc5b\ud835\udc52\ud835\udc61 is derivable at least \ud835\udc5d times \u2013 i.e. \ud835\udc62\ud835\udc5b\ud835\udc52\ud835\udc61 (x, \ud835\udf03) \u2208 C\ud835\udc5d \u2013 with \ud835\udc5d being the maximum derivative order for the operator D in eq. 4a.\nUsing the auto-differentiation (Gunes Baydin et al. 2015) it is possible to define the partial (or total) derivative \ud835\udf15\ud835\udc56\ud835\udc62\ud835\udc5b\ud835\udc52\ud835\udc61 (\ud835\udc65\ud835\udc56 , \ud835\udf03) by selecting a proper activation function \ud835\udf0e, e.g. a typically a sigmoid or hyperbolic tangent. The activation functions are often chosen among smooth (C\u221e) analytical functions, so that the derivatives calculated by auto-differentiation are correct at machine precision, which is important for the numerical stability in the evaluation of the PDE/ODE terms.\nThe core of the PINN scheme consist in evaluating the residual in the space of the neural network parameters \ud835\udf03 (eq. 4a). The aim is to optimize the loss function L, that can be defined as follows:\nL \ud835\udc53 (\ud835\udf03) = \ud835\udc51\u03a9 (D(\ud835\udc62\ud835\udc5b\ud835\udc52\ud835\udc61 (x, \ud835\udf03)) \u2212 \ud835\udc53 (x, \ud835\udf03)), (8a) L\ud835\udc54 (\ud835\udf03) = \ud835\udc51 \u2032 \ud835\udf15\u03a9 (B(\ud835\udc62\ud835\udc5b\ud835\udc52\ud835\udc61 (x, \ud835\udf03)) \u2212 \ud835\udc54(x, \ud835\udf03)), (8b) L\ud835\udc61\ud835\udc5c\ud835\udc61 (\ud835\udf03) = L \ud835\udc53 (\ud835\udf03) + L\ud835\udc54 (\ud835\udf03) (8c)\nwhere \ud835\udc51\u03a9 and \ud835\udc51 \u2032 \ud835\udf15\u03a9 are positive metrics that can be used to evaluate the distance to the exact solution \ud835\udc62 and IC/BC, e.g. the \ud835\udc3f2 norm (\u2016 \u00b7 \u201622). Including the ODE/PDEs residuals in the loss function is the reason why the algorithm is called physics informed.\nIn this context the residuals takes the place of what labels would be in a context of supervised learning, making the algorithm completely data independent (i.e. in this case, the solutions obtained with a numerical solver). Note that the metrics in eq.s 8 can be very general, e.g. for a ODE/PDE system each equation (and associated IC/BC term) can be weighted individually (see later Sec. 3.3, in particular eq. 25).\nOnce the loss function is defined, an optimization algorithm is applied on the parameters set \ud835\udf03. Various solution have been proposed for different problem, the most classical being the Stochastic Gradient Descent (SGD), i.e. a Gradient Descent (GD) algorithm applied on a random subset of the parameter space (Robbins & Monro 1951). The general idea beyond the GD method is to evaluate the gradient in a set of points of the domain and then mediate on them to find the optimal direction to minimize the loss function. However, when the domain is to huge (in terms of memory or in computational cost for the loss function gradient evaluation) or when the algorithm is distributed on more than one device (i.g. multi GPUs), the SGD is preferable: the method is equivalent to GD but on a subset of points (usually named batch). The parameters are update as follows:\n\ud835\udf03\ud835\udc56+1 = \ud835\udf03\ud835\udc56 \u2212 \ud835\udf02\u2207L(\ud835\udf03\ud835\udc56) , (9)\nwhere \ud835\udf02 is a scalar called learning rate. In this work we use the more sophisticated SGD variant with adaptive learning and gradient momentum rate ADAM (Kingma & Ba 2014) where the parameters\nupdate is defined as:\n\ud835\udf03\ud835\udc56+1 = \ud835\udf03\ud835\udc56 \u2212 \ud835\udf02 \ud835\udc5a\n\u221a \ud835\udc63 + \ud835\udf16 , (10)\nwhere\ud835\udc5a and \ud835\udc63 are the first and second moments of the gradient, and \ud835\udf16 is the smoothing term. The procedure can be considered concluded if the algorithm finds a set of parameters named \ud835\udf03\u2217 such that for a given positive scalar \ud835\udeff the following condition is met:\nL\ud835\udc61\ud835\udc5c\ud835\udc61 (\ud835\udf03\u2217) < \ud835\udeff , (11a)\nwhich implies that:\n\ud835\udc62\ud835\udc5b\ud835\udc52\ud835\udc61 (x, \ud835\udf03\u2217) ' \ud835\udc62(x) (11b)\nwhere \ud835\udeff is the required tolerance, that could be fixed a priori or determined on the fly, i.e. if the loss function is not decreasing for a large number of epochs.\nIn Fig. 2 we show the sketch of the feed-forward architecture of the PINN that is used in this work. To summarize, the proposed method follows these steps:\n0) We define a sampling of the input parameter space X and we randomly initialize the PINN parameters \ud835\udf03. 1) Given \ud835\udf03, we evaluate the solution of the PDE/ODEs system (eq.s 4) on the input space X using the PINN, i.e. \ud835\udc62 = NN\ud835\udf03 (X) 2) We compute the partial derivatives that appear in the PDE/ODEs via automatic differentiation, thus we evaluate the residuals in the loss function defined in eq. 8. 3) We update the values of parameters \ud835\udf03 with the optimizer in eq. 10, repeating 1)-3) until the convergence is reached (eq. 11).\nThe architecture is shown in Fig. 2 is the first introduced in the literature, and therefore the simplest. However, in recent years, more complex schemes have been drawn, such as: Fourier Network (FN, Tancik et al. (2020)) and its variations Modified Fourier Network (MFN, Wang et al. (2021)), Highway Fourier Network, inspired by Highway Network first introduced in (Srivastava et al. 2015) that are designed explicitly to take in account high frequency variations of the solution. Other variants are: SInusoidal REpresentation NEtwork (SIREN Sitzmann et al. (2020)), mainly designed for periodic-like solutions, MeshfreeFlowNet Jiang et al. (2020)), designed for super-resolution tasks and the Deep Galerkin Method (DGM (Sirignano & Spiliopoulos 2018)) which is inspired by Long Short Term Memory (LSTM) architecture but optimizes for PDEs computing."
        },
        {
            "heading": "2.3 Model benchmark",
            "text": "Before trying to solve the ISM chemistry via the PINN method, we validate the algorithm on a simplified and easily reproducible problem.\nFor this benchmark, we select an ODE system similar in shape to eq. 2. To have a formal description of the system, we rearrange the coefficient tensor as follows:\n\ud835\udc40 \ud835\udc57 \ud835\udc58 = \ud835\udc34\ud835\udc56 \ud835\udc57 \ud835\udc58 \ud835\udc5b\ud835\udc56 + \ud835\udc35\ud835\udc56\ud835\udc58 , (12)\nso that we write the ODE system in a compact (and standard) way\n\u00a4\ud835\udc5b\ud835\udc58 = \ud835\udc40 \ud835\udc57\n\ud835\udc58 \ud835\udc5b \ud835\udc57 , (13)\nwhere n = (n1, n2, n3) is a 3-dimensional vector of fake species, and \ud835\udc40 is the rate coefficients matrix.\nTo give a proof of the performance of the model, we adopt the\nMNRAS 000, 1\u201316 (2022)\nfollowing shape for the matrix of coefficients\n\ud835\udc40 = ( \u2212(\ud835\udc581 + \ud835\udc583\ud835\udc5b3) 0 \ud835\udc583\ud835\udc5b1 \ud835\udc581 + +2\ud835\udc583\ud835\udc5b3 \u2212\ud835\udc582 2\ud835\udc583\ud835\udc5b1\n\u2212\ud835\udc583\ud835\udc5b0 \ud835\udc581 \u2212\ud835\udc583\ud835\udc5b0\n) (14a)\nand consider two different cases. For case 1) we select rate coefficients as\nk\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc60 = [0.8, 0.5, 0.2] , (14b)\ni.e. the coupling matrix depend only on density \ud835\udc40 = \ud835\udc40 (n), which mimics a temperature-independent system. For 2) we adopt\nk\ud835\udc61\u2212\ud835\udc51\ud835\udc52\ud835\udc5d = [0.8 \u2212 sin(\ud835\udc61), 0.5 + cos(\ud835\udc61), 0.2 + sin(2\ud835\udc61)] , (14c)\nso that \ud835\udc40 = \ud835\udc40 (n, \ud835\udc61), which mimics a temperature-dependent system. The initial conditions of the fake species for both models are n\ud835\udc56\ud835\udc5b1 = 1, n \ud835\udc56\ud835\udc5b 2 = 0.001 and n \ud835\udc56\ud835\udc5b 3 = 0.1. For both models we consider the fake time interval [0,5). A key obstacle in solving a chemical (or thermo-chemical) evolution of a system is represented by stiffness. The ratio S can be used to quantify the stiffness of the system\nS = |Re(\ud835\udf06) |\ud835\udc5a\ud835\udc4e\ud835\udc65|Re(\ud835\udf06) |\ud835\udc5a\ud835\udc56\ud835\udc5b , (15)\nwhere Re(\ud835\udf06)\ud835\udc5a\ud835\udc4e\ud835\udc65/\ud835\udc5a\ud835\udc56\ud835\udc5b are the largest/smallest value of the real part of eigenvalues of the matrix. For both our test cases the coefficients change as the system evolves, thus we compute the mean stiffness ratio in the fake time interval, obtaining \u3008S(\ud835\udc40 (n))\u3009 ' 2.8\u00d7103 and \u3008S(\ud835\udc40 (n, \ud835\udc61))\u3009 ' 2.9\u00d7105 for 1) and 2), respectively. The difference in the stiffness ratio reflects the increase in complexity for a variable temperature system.\nThus, for the model with no explicit time dependence, we choose a neural network architecture with 6 layers of 64 neurons each. We adopt 210 ' 103 training points and \u2013 to assure convergence of the loss function \u2013 we run the training for 1.5 \u00d7 104 epochs, which takes \u223c 0.2CPUhr on a INTEL i7-9700 CPU using the ADAM algorithm introduced in Sec. 2.2. Instead, for the model with explicit time dependence, the setup consists of 6 layers of 128 neurons each, 213 ' 8 \u00d7 103 training points, and 3 \u00d7 104 epochs (\u223c 1.1CPUhr), reflecting the increased complexity. For this benchmark we have selected a feed-forward architecture, that is the simpler setup which can achieve satisfactory results. In order to compare our model with a procedural solver, we solve the systems with the Backward Differentiation Formula (BDF)method adopting the scypy implementation (Virtanen et al. 2019).\nIn Fig. 3 we show the benchmark validation. Qualitatively, the reconstruction of the trained models (dashed line) seem good in both cases. Quantitatively, we can define the errors as5\n\u0394 = \ud835\udc66\ud835\udc35\ud835\udc37\ud835\udc39 \u2212 \ud835\udc66\ud835\udc41\ud835\udc41 . (16)\nThe evolution of the error is show in the bottom panels of Fig. 3. For the model without/with explicit time dependence the mean errors are \u3008\u0394\u3009 ' 10\u22123 and 2 \u00d7 10\u22123 respectively, which we consider a satisfactory result for our benchmark. Interestingly, Fig. 3 shows no evidence of a growth of the error during the time evolution, as it might be expected from a regular procedural method. Such behaviour is intrinsic of the PINN method, as the training algorithm aims to minimizing the residuals on the entire time domain simultaneously."
        },
        {
            "heading": "3 NEURAL NETWORKS FOR ISM CHEMISTRY",
            "text": "While the benchmark shows good potential in applying of the PINN method for ISM chemistry, multiple aspects need additional considerations. Specifically, the initial conditions must be generalized (Sec. 3.2), the chemical network requires further consideration (Sec. 3.1), the loss function requires modifications (Sec. 3.3), the neural network must be improved, and the training process needs more careful attention (Sec. 3.4)."
        },
        {
            "heading": "3.1 ODE structure of the chemical networks",
            "text": "For the selected chemical network (Sec. 2.1), the associated ODEs system has non-trivial dependence of the coefficients on the temperature. This is determined by the elements of the interaction matrix \ud835\udc40\ud835\udc56 \ud835\udc58 (defined in eq. 12): an example is shown in Fig. 4, where we plot a subset of the matrix element as a function of temperature. In the allowed temperature range, the selected \ud835\udc40\ud835\udc56\n\ud835\udc58 can vary by more than\nabout 10 order of magnitude. The stiffness of the system is higher with respect to the benchmark (eq. 14); for instance, for normal ISM densities (i.e. the IC used in Sec. 3.2), the stiffness can reach values of S \u223c 1016 for \ud835\udc47 < 2.5 \u00d7 104K and S \u223c 104 for \ud835\udc47 > 2.5 \u00d7 104K. This is a hint that i) the network architecture must be expanded ii)\n5 For the simple benchmark, it is convenient to avoid using relative error, since all the fake species densities are order unit.\nMNRAS 000, 1\u201316 (2022)\nthe training will be more expensive, also because of the generalized initial conditions.\nGiven these expectations and in order to explore different strategies for both the neural network and training, for the chemical system we consider both a molecular network (introduced in Sec. 2.1) and a reduced atomic network. The atomic network is a simplified version of the molecular one, namely, it does not include the chemistry of molecular hydrogen and its cooling, simplifying both the reaction network and the temperature evolution. With respect to molecular, in atomic the number of species decreases from 9 to 7 and the number of reactions decreases from 46 to 24."
        },
        {
            "heading": "3.2 Setting up the initial conditions",
            "text": "The PINN model adopted in the benchmark (Sec. 2.3) is solved only with a specific set of fixed initial conditions: this is a major limitation. Adopting the same approach for ISM chemistry would considerably limit the applicability of the model, since training is expensive and would be needed for each different thermodynamic configuration.We can overcome this limit by generalizing themodel as follows.\nRecalling eq. 4, in the case of ODEs the operator it follows that B(x) = (\ud835\udc651 (\ud835\udc610), \ud835\udc652 (\ud835\udc610), ..., \ud835\udc65\ud835\udc58 (\ud835\udc610)). Formally, to generalize for arbitrary initial conditions, we can promote the initial values from a scalar to a function \ud835\udc65\ud835\udc56 (\ud835\udc610) \u2192 \ud835\udc53\ud835\udc56 (\ud835\udc610) that map the ICs in a desire range, e.g. for temperature, \ud835\udc53\ud835\udc47 (\ud835\udc610) = [20, 106]K. The procedure has been proposed by Flamant et al. (2020) and greatly increases the dimensionality of the problem; however, as shown by Mishra & Molinaro (2020), the PINN does not suffer too much from the curse of dimensionality, e.g. can be trained in a 100-dimensional space in case of heat equation. Summarizing, we can use this strategy to vary both the initial thermodynamic state (\ud835\udc47\ud835\udc56\ud835\udc5b, \ud835\udc5b\ud835\udc56\ud835\udc5b) and the fractions of each species, \ud835\udc36\ud835\udc58 . We expect the procedure to increase the training time, however note that \u2013 once the training is completed \u2013 the computational time for predictions is mostly unaltered, since the latter depends only on the network architecture, i.e. the number of neurons and links.\nOnce trained our model maps the input space (time and initial\nconditions) to the evolution of the desired quantities:\n(\ud835\udc61, IC) \u2192 PINN(\ud835\udc61, IC) = (n(\ud835\udc61, IC), \ud835\udc47 (\ud835\udc61, IC)) . (17)\nStating eq. 17 differently, knowing the initial densities and temperature at the initial time \ud835\udc61 = \ud835\udc61\ud835\udc56\ud835\udc5b, the PINN return the evolved quantities at \ud835\udc61 = \ud835\udc61\ud835\udc5c\ud835\udc62\ud835\udc61 :\n[\ud835\udc5b\ud835\udc58 (\ud835\udc61 = \ud835\udc61\ud835\udc56\ud835\udc5b), \ud835\udc47 (\ud835\udc61 = \ud835\udc61\ud835\udc56\ud835\udc5b)] \u2192PINN(\ud835\udc61\ud835\udc5c\ud835\udc62\ud835\udc61 , \ud835\udc5b\ud835\udc58 (\ud835\udc61 = \ud835\udc61\ud835\udc56\ud835\udc5b), \ud835\udc47 (\ud835\udc61 = \ud835\udc61\ud835\udc56\ud835\udc5b)) = = [\ud835\udc5b\ud835\udc58 (\ud835\udc61 = \ud835\udc61\ud835\udc5c\ud835\udc62\ud835\udc61 ), \ud835\udc47 (\ud835\udc61 = \ud835\udc61\ud835\udc5c\ud835\udc62\ud835\udc61 )] (18)\nwith \ud835\udc58 indexing the species. Note that the adopted generalization for the IC (eq. 17) is not directly applicable to arbitrary PDE systems, which require more sophisticated and problem dependent techniques (i.e. Nakamura et al. 2021).\nTherefore, in this paper we distinguish 2 type of model: single models, that have fixed initial condition, and general model. Although single models are subclasses of the general models, in the spirit of proof of concept it is convenient to keep the cases separate, since different strategies are adopted to achieve convergence in the training.\nFor single models, we adopt \ud835\udc47 = 103K as the initial temperature for both networks (atomic/molecular), while the total density (\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc61 \u2261 \u2211 \ud835\udc58 \ud835\udc5b\ud835\udc58 ) and individual abundances (\ud835\udc36\ud835\udc58 \u2261 \ud835\udc5b\ud835\udc58/ \u2211 \ud835\udc58 \ud835\udc5b\ud835\udc58 ) are set as follows. atomic: \ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc61 = 90.4cm\u22123,\ud835\udc36H\u2212 = 0.0015,\ud835\udc36H = 0.69, \ud835\udc36He = 0.288, \ud835\udc36H+ = 0.0069, \ud835\udc36He+ = 0.0029 and \ud835\udc36He++ = 0.0008; molecular: \ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc61 = 100cm\u22123, \ud835\udc36H = 0.6241, \ud835\udc36H\u2212 = 0.001, \ud835\udc36H2 = 0.104, \ud835\udc36He = 0.26, \ud835\udc36H+ = 0.0062, \ud835\udc36H+2 = 0.001, \ud835\udc36He+ = 0.0026 and \ud835\udc36He++ = 0.0007. In both cases, \ud835\udc36e\u2212 is set to ensure charge neutrality.\nFor general models, the initial values for the temperature, total density, and individual abundances span the following ranges:\n20 \u2264 \ud835\udc47 K \u2264 106 (19a)\n10\u22122 \u2264 \ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc61 cm\u22123 \u2264 103 (19b)"
        },
        {
            "heading": "10\u22126 <\u223c \ud835\udc36\ud835\udc58 <\u223c 1 , (19c)",
            "text": "where \ud835\udc36\ud835\udc58 are chosen such that global charge neutrality is respected and the total hydrogen (helium) fraction is \ud835\udc4b ' 70% (\ud835\udc4c ' 30%)."
        },
        {
            "heading": "3.3 Loss function definition",
            "text": "A crucial aspect in any ML approach is the design of the loss function.Directly evaluating themean square errors using themetric in eq. 8 cannot capture the fine structure of the underlying solution. This is mainly driven by the large dynamical range spanned by the density of different species. For instance, for typical ISM conditions H\u2212 is usually about 8 order of magnitude lower than H: using a simple (e.g. uniformly weighted) loss function would ignore the variation of H\u2212. Ideally, to maximize the convergence efficiency, all components of the loss function must be of the same order of magnitude. However, unlike in a supervised learning problem, the code does not know in advance the abundances (and temperature) during the evolution.\nWith this in mind, we have adopted the following strategy to model the loss function. The first step consists in considering the evolution of the logarithm of the abundances (eq. 2) and temperature (eq. 3); this \u201cfeature normalization\u201d is a relatively standard technique for ML. Moreover, we solve the ODEs system in a logarithmic time scale; with this precaution we can better capture the sudden (\ud835\udc61 <\u223c 102 yr) large (more than one order of magnitude) variations that species can experience for some initial conditions.\nMNRAS 000, 1\u201316 (2022)\nTo summarize, in linear space the ODE in eq. 2 has the following residuals:\nR\ud835\udc5b\ud835\udc58 = \u00a4\ud835\udc5b\ud835\udc58 \u2212 (\ud835\udc34 \ud835\udc56 \ud835\udc57 \ud835\udc58 \ud835\udc5b \ud835\udc57 + \ud835\udc35\ud835\udc56\ud835\udc58 )\ud835\udc5b\ud835\udc56 . (20)\nWe perform the change of variables:\n\ud835\udc66\ud835\udc58 = log(\ud835\udc5b\ud835\udc58/cm\u22123) (21a) \ud835\udc47 = log(\ud835\udc47/eV) (21b) \ud835\udf0f = log(\ud835\udc61/s) . (21c)\nThus eq. 2 can be rewritten as\n\u00a4\ud835\udc66\ud835\udc58 = 10\ud835\udf0f\n10\ud835\udc66\ud835\udc58 (\ud835\udc34\ud835\udc56 \ud835\udc57 \ud835\udc58 \ud835\udc5b \ud835\udc57 + \ud835\udc35\ud835\udc56\ud835\udc58 )\ud835\udc5b\ud835\udc56 . (22)\nand the residuals are naturally defined as\nR\ud835\udc66\ud835\udc58 = \u00a4\ud835\udc66\ud835\udc58 \u2212 10\ud835\udf0f\n10\ud835\udc66\ud835\udc58 (\ud835\udc34\ud835\udc56 \ud835\udc57 \ud835\udc58 \ud835\udc5b \ud835\udc57 + \ud835\udc35\ud835\udc56\ud835\udc58 )\ud835\udc5b\ud835\udc56 . (23)\nThe relation between the residuals computed in the logarithmic space and in the linear space is\nR\ud835\udc66\ud835\udc58 = \ud835\udc61\n\ud835\udc5b\ud835\udc58 R\ud835\udc5b\ud835\udc58 . (24)\nDividing the residuals by \ud835\udc5b\ud835\udc58 , the loss function should be balanced, even when the sum is extended to species with order of magnitude difference between abundances (e.g. H\u2212 vs \ud835\udc3b).\nDespite these precautions, the loss function remains complex and consequently the training procedure can present mythologies, e.g. excessive stiffness in the parameters update. To mitigate this problem, the weights \ud835\udf06\ud835\udc56 are modified during the training, using the procedure detailed in Wang et al. (2021). Summarizing, we write the loss function as\nL(\ud835\udf03) = \ud835\udc41\u2211\ufe01 \ud835\udc56=1 \ud835\udf06\ud835\udc56L\ud835\udc56 (\ud835\udf03) , (25)\nwhere the sum is extended to \ud835\udc41 = 2(\ud835\udc41\ud835\udc60\ud835\udc5d\ud835\udc52\ud835\udc50 + 1) to account for the ODE residuals for the chemical species and the temperature (eq. 8a) and initial conditions (eq. 8b). The terms \ud835\udf06\ud835\udc56 are adaptively regulated utilizing the back-propagated gradient statistics during model training. By experimenting different approaches, we noticed that it is convenient to adopt such adaptive regulation of the weights of the loss, mainly because it improves the convergence during the initial phases of training; this is later discussed in Sec. 4.1."
        },
        {
            "heading": "3.4 Network architecture and training strategy",
            "text": "In general, the optimization of neural network topology and the hyperparameters tuning is a complex task. Although there are tools designed for hyperparameters optimization, an accurate calibration of these is usually not practical, especially if the training procedure is time expensive. So we perform an optimization based on small variations of a fiduciary setting.\nFor our models, we adopt either a simple Feed-forward Neural Network (FNN, Fig. 2) or a more advanced DGM architecture (Sirignano & Spiliopoulos 2018). A DGM follows essentially the same 0)-3) steps described in Sec. 2.2, however the parameters update is more complex (see sketch in Fig. 5 ). At depth \ud835\udc56\ud835\udc61\u210e first we calculate the output of a standard dense layer\n\ud835\udf19\ud835\udc56 (x) = \ud835\udf0e(W\ud835\udc56x + b\ud835\udc56) . (26)\nThe result is then processed through a DGM layer by computing\n\ud835\udc4d\ud835\udc56 , \ud835\udc3a\ud835\udc56 , \ud835\udc45\ud835\udc56 , and \ud835\udc3b\ud835\udc56 as follows:\n\ud835\udc4d\ud835\udc56 = \ud835\udf0e(U(\ud835\udc67)\ud835\udc56 x + W (\ud835\udc67)\ud835\udf19\ud835\udc56 + b(\ud835\udc67)\ud835\udc56 ) (27a) \ud835\udc3a\ud835\udc56 = \ud835\udf0e(U(\ud835\udc54)\ud835\udc56 x + W (\ud835\udc54)\ud835\udf19\ud835\udc56 + b(\ud835\udc54)\ud835\udc56 ) (27b) \ud835\udc45\ud835\udc56 = \ud835\udf0e(U(\ud835\udc5f )\ud835\udc56 x + W (\ud835\udc5f )\ud835\udf19\ud835\udc56 + b(\ud835\udc5f )\ud835\udc56 ) (27c) \ud835\udc3b\ud835\udc56 = \ud835\udf0e(U(\u210e)\ud835\udc56 x + W (\u210e) (\ud835\udf19\ud835\udc56 \ud835\udc45\ud835\udc56) + b(\u210e)\ud835\udc56 ) , (27d)\nwhere\ud835\udc48 andW(...) are weight matrices, b are biases, and represent the Hadamard (element-wise) product. For depth (\ud835\udc56 + 1)\ud835\udc61\u210e , the outputs are then combined via\n\ud835\udf19\ud835\udc56+1 (x) = (1 \u2212 \ud835\udc3a\ud835\udc56) \ud835\udc3b\ud835\udc56 + \ud835\udc4d\ud835\udc56 \ud835\udf19\ud835\udc56 (28)\nto define the next dense layer. A DGM layer requires roughly eight timesmorememory than standard FNN, since there are eight weight matrices per layer. The main advantage is the ability to capture the sharp turns of the underlying solution, as argued in Sirignano & Spiliopoulos (2018).\nFor the activation function, we have chosen an adaptive version of the sigmoid function, \ud835\udf0e(\ud835\udc4e, \ud835\udc65):\n\ud835\udf0e(\ud835\udc4e, \ud835\udc65) = 1 1 + \ud835\udc52\u2212\ud835\udc4e\ud835\udc65 (29)\nwhere \ud835\udc65 is the input value and \ud835\udc4e is a NN adaptive parameter, which is an additional parameter that is optimized during the training. As shown in Jagtap et al. (2020), this is a useful strategy for dynamical problems that present a wide range of time scales, thus particularly in our case where the reaction rate that can vary by several order of magnitude (see Fig. 4). The overall architecture design is summarized in Fig. 5.\nTo fully exploit our hardware, the learning procedure is distributed on multi-GPUs (up to 4) and the train domain is divided in several mini-batches (4-32 per GPU). The optimizer is ADAM, with initial learning rate, \ud835\udf02 and decaying scheduling dependent on the specific model 6, which needs a gradient aggregation correction for the parameters update. Furthermore, the initial learning rate, \ud835\udf02, is subjected to a gradual warm-up, following Goyal et al. (2018), which makes \ud835\udf02 less dependent on the user initialization.\nThe final aspect to address is setup for the training points. The higher the number of training point, the better the variability of the solution is captured. However, increasing such number does increase the training time and/or the requiredmemory. It is therefore necessary to find a good trade-off between the amount of training points and computational cost. The number is chosen empirically, however, in the most general case of variable initial conditions described in Sec. 3.2, is shown in (De Ryck & Mishra 2021) that it grows less then exponentially. This result makes us confident that, net of our hardware availability, it is theoretically possible to make eq. 8 converge as the number of chemical species increases. In this workwe find that\u223c 104\u2212105 points perGPU for a single batch gives a reasonable balance. The training points distribution follow the Halton sequence (Halton & Smith 1964); to increase the sampling density, we change the points cloud during the training after a fixed number of iterations, here fixed to 1000 epochs. In total we consider 4 cases, which originates from the combination of single/general models (IC) withmolecular/atomic chemical networks. A summary of the parameters for eachmodel is given in Tab. 1. The development of our codes, was perfermod using modulus (Hennigh et al. 2020),\n6 Note that the learning rate \ud835\udf02 can be tuned manually on-the-fly for stability reasons.\nMNRAS 000, 1\u201316 (2022)\na tensorflow (Abadi et al. 2015) based tool specific for PINN design."
        },
        {
            "heading": "4 RESULTS",
            "text": "To present our results, first we analyze the training procedure, by studying the convergence of the various model (Sec. 4.1) and detailing the trends for individual ions (Sec. 4.2). Then, we validate our models against procedural solvers by adopting krome as our ground truth: first we focus on the single atomic network (Sec. 4.3), then we give a comparative analysis of the remaining models (Sec. 4.4)."
        },
        {
            "heading": "4.1 Training: convergence",
            "text": "An overview of the PINN training can be appreciated in Fig. 6. In the upper panel, we plotL as a function of the training epochs for all models. For all models, the trend of loss functions during the training phase appears qualitatively similar in shape, in particular with a sharp decline in the early epochs. In general, more complex models (from single atomic to general molecular) require more training resources to reach an acceptable convergence. In particular the single atomic model is much easier to emulate, in fact the convergence is about two orders of magnitude better with \u223c 5 \u00d7 105 less training epochs. Furthermore,\nRecall that the training is stopped when L is below a certain threshold, however there is no generally accepted value for\nsuch threshold. Being able to check the residual interactively, it is possible to have an on-the-fly evaluation of the reliability of the approximation. Additionally, note that if the loss function is stable for a long period (\u223c 105epochs), it is possible \u2013 and convenient \u2013 to stop the training and change some hyper-parameters. In particu-\nMNRAS 000, 1\u201316 (2022)\nlar, this is an important optimization strategy to adopt in the final stages of the training: when the ADAM algorithm is no longer able to lower the loss function, it is possible to switch to the second order method L-BFGS that uses the hessian matrix; this algorithm is slower but more powerful in the minimization, thus can be used during the last epochs to refine the convergence (Liu & Nocedal 1989; Schraudolph et al. 2007); we adopt such strategy and report the effect later in Sec. 4.3. further, to quantify the importance of the adaptive weights (see Sec. 3.3), we perform a control training for the single molecularmodel; without adaptive weights, after \u223c 3.2\u00d7105 epochs the loss function value is \u223c 20\u00d7 larger with respect to results shown in Fig. 6.\nIn the lower panel of Fig. 6 we plot the PDF of the sum of the residuals at the last training epoch. In general the peak is around \u223c 10\u22121.5 for all models except , with high value tails that can reach up to \u223c 10\u22121, with larger residuals for models with higher complexity. Table 1 summarizes the main hyper-parameters and the convergence time for each model. Apart from the single atomic model, the training time is around thousands of hours (despite the high performance of the GPU used). However, real time can be linearly reduced with the use of multi-GPUs. We also noticed that the single molecular and general atomic models exhibit similar results in terms of convergence (same number of layers and similar training time). While it is a convenient and compact comparison, summing the residual for each chemical species does hide some interesting aspects of the convergence."
        },
        {
            "heading": "4.2 Species by species convergence",
            "text": "Unlike the most ML applications, a simple evaluation of the loss function (both for training and validation points) it is not enough to guarantee the goodness of the model for PINN algorithm. Indeed, the loss function gives a domain-wide average, thus does not imply local convergence; this aspect is particularly important since we adopt multi-components loss function thus \u2013 despite the precautions taken (Sec. 3.3) \u2013 some chemical species or a particular set of IC might be not well reconstructed.\nFig. 7 shows an example of the training of each of the components of the loss function in the general molecular case. Up to ' 1.2 \u00d7 106 training epochs, all the different components follow a similar decreasing trend. After ' 4 \u00d7 106 the model start to converge, and the different components saturates at different values; in particular, we can see that the temperature reconstruction dominates the loss function for this specific model. To assess the situation, it is convenient to look at the distribution of residuals when the training is completed.\nIn the upper panels of Fig. 8 we show the various components of the loss function of the for both the atomic and molecular single model cases. For the single atomic model, all individual residuals are smaller than about 10\u22122, with the exception of a negligible contribution for the high values tail of the distributions. For the single molecular model, in general similar values are present, but the high values tails are in general larger, as expected from different values of the loss functionwhen the training is stopped; in particular, the high values tail of temperature evolution surpasses 10\u22122.\nRecall that, for general models, ICs span 20 \u2264 \ud835\udc47\ud835\udc56\ud835\udc5b/K \u2264 105 and 10\u22122 \u2264 \ud835\udc5b\ud835\udc56\ud835\udc5b/cm\u22123 \u2264 103, and the fraction of each species is selected to respect charge neutrality and the constraint \ud835\udc4b ' 0.7 and\nMNRAS 000, 1\u201316 (2022)\nMNRAS 000, 1\u201316 (2022)\n\ud835\udc4c ' 0.3, i.e. they have larger number of dimension with respect to single models. Looking on the bottom-left panel of Fig. 8, the residual distributions for all species in the case of general atomic model are centered around (or below for negative particles) 10\u22122 with tails that does not exceed 10\u22121. For general molecular model (bottom-right panel of Fig. 8), the residuals of shown a similar behavior except temperatures show similar errors except temperature distribution, centered at \u223c 10\u22120.5 (compare with Fig. 7). From Fig. 8 we can draw the following conclusions: (i) except for single model the temperature evolution residuals are the most difficult to minimize and (ii) the residual distribution are for different species are about the same order of magnitude, which implies a good balance of the loss function."
        },
        {
            "heading": "4.3 Validation of the single atomic network",
            "text": "To validate the training, we compare with the results from krome (Grassi et al. 2014), that it is used to build the procedural solvers for both the atomic and molecular chemical networks (see Sec. 2 for a description of the code).\nIn Fig. 9 we show the evolution of the single atomic model for 1Myr. We note a qualitative good agreement with procedural solvers in terms of logarithm of the density except for He++ and H\u2212. We have obtained a very good reconstruction of the initial conditions (which are a soft constraint in the model) and in general the algorithm mixes the variations in density with precision that decreases in cases of species with sharp turns (in log-space).\nThe choice of adopting logarithmic time (eq. 21c) plays an important role here; while it has no immediate advantage in terms of the loss function (eq. 24), it helps in the recovery of abundance evolution which starts far from the equilibrium, which consequently have a very steep \u2013 and thus difficult to reproduce \u2013 evolution. In practical terms, the logarithmic time scale has a smoothing effect on these sharp turns and convergence is thus facilitated. However, despite the smoothing effect, the fastest varying species have the largest relative errors.\nNote that in the case show in Fig. 9, a non-negligible benefit comes from refining the training with a L-BFGS optimizer (Liu & Nocedal 1989; Schraudolph et al. 2007). This change lowered the training curve only by a factor ' 1/3; this is expected: with respect to ADAM, L-BFGS is of higher order but is more prone to get stuck in local minima when the ODEs are stiff (Lu et al. 2019). Importantly, the adoption of the L-BFGS refinement has significantly improved the validation with krome. In particular, before He++ and H\u2212 showed errors two order of magnitude larger. Further, by using L-BFGS, the reconstruction of the species at early time and at equilibrium is improved.\nTo be more quantitative, it is convenient to define both relative (\u0394\ud835\udc5f ) and fractional (\u0394 \ud835\udc53 ) differences:\n\u0394 \ud835\udc53 = |\ud835\udc5b\ud835\udc41\ud835\udc41 \u2212 \ud835\udc5b\ud835\udc3e\ud835\udc45 |\n\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc61 (30a)\n\u0394\ud835\udc5f = |\ud835\udc5b\ud835\udc41\ud835\udc41 \u2212 \ud835\udc5b\ud835\udc3e\ud835\udc45 |\n\ud835\udc5b\ud835\udc3e\ud835\udc45 , (30b)\nwhere \ud835\udc5b\ud835\udc41\ud835\udc41 are the values predicted with the trained models and \ud835\udc5b\ud835\udc3e\ud835\udc45 are computed with krome. In the bottom panel of Fig. 9 we show the distribution of relative and fraction errors. While for most of the species the relative error are reasonably small (\u0394\ud835\udc5f <\u223c 10\u22121), the relative errors are dominated by H\u2212, which peaks at \u0394\ud835\udc5f \u223c 1 and He++, which has a very high values tail. However, these two species have low abundances, thus the fractional errors of all species are small, i.e. \u0394 \ud835\udc53 <\u223c 10\u22122.\nIn terms of usage, it seems encouraging that the dominant relative errors affect the less abundant species, since they lead to negligible fractional errors; however, low abundance species can be important in some situations, i.e. H\u2212 abundance is critical in computing H2 in low metallicity/dust environments. For a proper usage, such errors should be removed with further training and/or by reconsidering the architecture via a change of hyperparameters setup."
        },
        {
            "heading": "4.4 Model comparison",
            "text": "Our other models are compared with krome in Fig. 10, where we show the PDF of the relative error \u0394\ud835\udc5f . For convenience, a summary of the quantiles of the distributions is reported in Tab. 2.\nResults for the single molecular model are shown in the left panel of Fig. 10. Overall we found a good reconstruction, with the 75% of the points with a relative error log(\u0394\ud835\udc5f ) < \u22120.84. However the goodness of the reconstruction presents a species to species variance, in particular we found \u0394\ud835\udc5f ' 1 in the most of cases for He++, similarly to what we found in the single atomic model. As this feature is present for both chemical network, it might be a sign that our models fails to accurately capture the ionization of He+, which has a very small rate given the hardness of the impinging radiation field.\nFocusing on general atomic model (central panel of Fig. 10), we note that overall the 75% of the predictions have relative errors smaller than log(\u0394\ud835\udc5f ) < \u22120.89; this behaviour is similar to the single molecularmodel, which have been trained for about the same number of epochs. While general atomic has a wider range of IC, single molecular havemore reactions; however, with respect to other techniquesML is less affected by the curse of dimensionality, thus it is not straightforward to predict a hierarchy of complexity between different models, i.e. larger reactions set vs larger IC parameter space.\nIn the right panel of Fig. 10 we show the relative errors distribution for the general molecular model. We note an overall greater difficulty to emulate the procedural solvers, with the 75% of prediction is smaller than log(\u0394\ud835\udc5f ) < 0.05, with larger errors on temperature, negatively charged ions and hydrogen. For most of the species, errors for general molecular are higher with respect to previously seen models, as a consequence of the increase of complexity for both the number of reactions and parameter space for IC; however He++ presents very small errors, thus overall the PINN convergence rate of individual species seems to be not to be directly linked to the reaction rate of the chemical system.\nWhile up to this point we have considered only 1D PDF, it is interesting to see if the emulation power of general models is\nMNRAS 000, 1\u201316 (2022)\ndifferent depending on the initial position in the\ud835\udc47\u2212\ud835\udc5b plane (Figs. 11 and 12). Compared to atomicmodels we have a better reconstruction of the evolution of H and H+ in the molecular models. Although we do not have a definitive explanation for this improvement, we are led to think that the inclusion of H2 and H+2 has some black-box\neffect that allows the model to better reconstruct the evolution of hydrogen.\nIn Fig. 11 we show the \u0394\ud835\udc5f distribution of general atomic computed at \u223c 1Myr as a function of the temperature and gas density. In Fig. 11, we note a good reconstruction over the entire parameter space for negative ions, while for what concerns hydrogen (H and\nMNRAS 000, 1\u201316 (2022)\nMNRAS 000, 1\u201316 (2022)\nH+) and helium (He, He+ and He++) we have a good agreement at high densities which gradually worsens at low densities. While this is not shown in the figure, the error at low density is dominated by positively charged ions for both hydrogen and helium. Regarding the evolution of the temperature, we generally have a very good reconstruction, except when the initial temperature is \ud835\udc47\ud835\udc56\ud835\udc5b ' 6\u00d7103K.\nSimilarly to general atomic, in Fig. 12 we show the relative errors to the general molecular model\nRelative to general atomic, the errors for general molecular are larger (as also expected from Fig. 10), but have a very different behaviour in the \ud835\udc5b-\ud835\udc47 plane. Both the H, H+ and He, He+, He++ groups have flat distributions of errors, and show relatively a good agreement with krome. Electrons have in general higher errors, with two \ud835\udc5b-\ud835\udc47 narrow stripes of lower errors, likely caused by a higher concentration of training points. Themolecular hydrogen has high relative errors in the low density regime, where its density is negligible, and the temperature shows a tension that gradually rises with density. Overall, there seem to be no causal connection between regions with large errors and the underlying chemical system.\nSummarizing, despite the inherent difficulty of the problem (especially in general cases) for a proof of concept work these are encouraging results, as regions where errors are high can be cured with a longer training. In view of a coupling with numerical simulations and to ease the convergence rate of the training, an exploration the hyperparameters space of the network architecture is needed, along with an adaptive addition of training points in the regions of parameter space where errors are higher.\nIt is interesting to conclude our analysis by comparing the timing of the PINN network compared to procedural solvers. For these tests both codes run on similar machines, specifically PINN runs using a single INTELXEONGold 6240 CPUwith a frequency of 2.60GHz, the procedural solver krome adopts a single INTEL\ni7-9700 CPU with a frequency 3.00GHz. For single models, we checked the evolution of system using 105 different end time \ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc51 from 10yr to 1Myr. With krome, we find that single atomic (single molecular) models have a completion time7 of from 0.05s (0.09s) to 10.56s (11.16s) with increasing \ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc51 . With the PINN, we obtain a speed up of a factor \u223c 207 (\u223c 116) for single atomic (single molecular). For the general models we prepare a 3D grid that is uniformly sampled in log-spaced: the grid features different initial species fractions, \ud835\udc36\ud835\udc58 \u2208 [10\u22126, 1) (100 points), \ud835\udc47\ud835\udc56\ud835\udc5b \u2208 [20, 106]K (512 points), and \ud835\udc5b\ud835\udc56\ud835\udc5b \u2208 [10\u22122, 103]cm\u22123 (512 points); different initial conditions are evolved up to \ud835\udc61end = 1Myr. With respect to the krome solver, for the PINN we have a speed-up of a factor of \u223c 108 and \u223c 91 for the general atomic and general molecular respectively. The speed-up is better in cases of the simpler thermochemical system. However, with exploration of the hyperparameter space of the PINN, it should be possible to obtain lighter networks, i.e. needing fewer algebraic operations to compute the output, to achieve a further the speed-up for complex chemical networks.\nIt is important to note that varying the initial conditions, the general atomic (general molecular) krome yields a model to model standard deviation of \u223c 24% (\u223c 30%) with respect to the mean completion time. For the PINN, the variance is negligible, i.e. less than 10\u22125%. If exploited, this is a critical advantage over procedural solvers, since the latter are prone to yield load balancing problems, while the usage of the PINN can improve the scaling of parallel numerical codes with hydrodynamic coupled with chemistry.\nFinally, we compare with other works have tried to adopt machine learning techniques to solve the chemical evolution, i.e. latent_ode (Grassi et al. 2021) and chemulator (Holdship et al.\n7 Note that a non-negligible amount of time is spent in the warm-up of the solver.\nMNRAS 000, 1\u201316 (2022)\n2021). With respect to the chemical network adopted here, these models emulate more complex chemical networks in a higher density range (log \ud835\udc5b/cm\u22123 >\u223c 1), i.e. with 33 and 29 species for chemulator and latent_ode, respectively, even tough latent_ode have no temperature dependence nor evolution. Differently to thesemethods, our PINN is completely unsupervised, i.e. the PINN does not requires a pre-computed dataset for the solutions (chemulator) or a procedural solver in latent space (latent_ode). Further, while both the present and latent_ode are developedwith hydrodynamic code coupling in mind, chemulator is tough to be a faster alternative to photoionization models (R\u00f6llig et al. 2007), i.e. its structure is too computationally demanding to be included in numerical simulations (Holdship et al. 2021).\nOverall our PINN method has better validation errors with respect to latent_ode and comparable to chemulator. For the speed-up, the PINN yields performances that are slightly above the one of latent_ode (\u00d765), with both ML techniques being tested against procedural ODE solvers. The speed-up for chemulator is much better (\u00d750000), but it is obtained against the photoionization code uclchem (Holdship et al. 2017), which is a much more complex program with respect to an ODE solver. Final caveat, these comparisons should be taken with care, as all these models are still at a proof of concept stage, thus further optimizations are possible."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "Chemical processes are key in regulating the evolution of the interstellar (ISM) and intergalactic medium. However, in cosmological and astrophysical simulations, finding solutions for thermochemistry networks is numerically costly: the systems are stiff and there are orders of magnitude of differences between the time scale of chemical reactions and the ones of astrophysical processes (e.g. gravity and fluid dynamics). This requires the usage of robust, highorder, multi step backward integrator for ordinary differential equations (ODE), potentially leading to computational bottlenecks for the numerical simulations.\nIn this work, we explored the possibility to use trained unsupervised physics informed neural networks (PINN) as an alternative to solve or ameliorate such problems. The main idea consists in expressing the underlying ODEs solution with a neural network (NN) and treat the problem in a variational way, i.e. by minimizing the residuals of the chemical system. This procedure (Sec. 2) has been first introduced in Raissi et al. (2019) but has never been adopted in the context of thermo-chemistry for astrophysical problems.\nWe first tested the method using benchmark cases (Sec. 2.3), then we developed the necessary technical solutions for our case study (Sec. 3.1). We adopted two different thermo-chemical networks that can solve the ISM chemistry with and without molecular hydrogen formation. We build PINN with fixed and arbitrary initial conditions in single and generalmodels, respectively. Our main results can be summarized as follows.\n\u2022 A simple feed-forward architecture cannot reconstruct the evolution of a realistic thermo-chemical network. The minimal setup to achieve good results consist in adopting: a Deep Galerkin Method as Neural Network architecture coupled with adaptive sigmoid activation function, adaptive weights in the loss function, an annealing learning rate, andADAMoptimization algorithm (possibly followed by L-BFGS, see Sec.s 3.3 and 3.4).Moreover, a considerable benefit consists in solving the equations in log-space, both for the abundance and time; while the former is a standard normalization technique,\na logarithmic time helps for far-from equilibrium situations, as it increase the time sampling.\n\u2022 Even when running on state-of-the-art GPUs, for all models the training time for convergence is relatively large when compared to typical PINN cases of study that are in the literature. We train the models for \u223c 3 \u00d7 106 epochs, with a total computational time that can vary from ' 102GPUhr to ' 2 \u00d7 103GPUhr. Unless dedicated resources are allocated, these relative long training times make it expensive to scan the hyperparameter space of the network to improve the convergence and validation.\n\u2022 As expected, the simplest realistic case (single atomic) costs much less respect to the most complex case (general molecular), both in terms of training time, number of training points and dimension of the network itself (and thus associated memory). However, the hierarchy of complexity between the other model is not clear. On the one hand, the general atomicmodel is multidimensional, thus i) the problem becomemore stiff as the reaction rates varymorewildly and ii) the curse of dimensions starts to play a role, even tough traditionally ML is less affected with respect to other techniques. On the other hand, in the case of single molecular model, there are about double the reactions and molecular heating and cooling processes. It is unclear which of these two factors is dominant in hindering a fast convergence.\n\u2022 We find overall a good agreement with procedural solver. For almost all models, more than 75% of the points with relative errors less than 15% and 50% of the prediction smaller than 1%; for single atomic we have errors smaller than 17% in 90% of cases. In the case of general models we obtain better results in the high density region of the thermodynamic parameter space.\n\u2022 For all models, we obtained a significant speed-up respect to the procedural solvers, from \u223c 200 for the more simple model to \u223c 90 for the more complex. Furthermore, the very low variance of computational time in different thermodynamic conditions can potentially solve load balancing problems that can occur in the context of massively parallel codes.\nAlthough the chemical networks emulated in this work are relatively small (up to 9 ions and 46 reactions), we do not expect to have excessive problems in increasing the number of species, mainly because, with respect to other methods, the PINN suffer less from the curse of dimensionality (i.e. see Mishra & Molinaro (2020) for the PINN method in up to \u223c 100 dimensions). Thud, it would be interesting to test the PINN for possible application to wider chemical networks, i.e. explicit non equilibrium metals evolution following carbon and oxygen chemistry (Glover et al. 2010): it would entail tracing the evolution of 32 chemical species and 218 reactions; this would represent an interesting possibility for future developments, which should also give a fairer benchmark with respect to other attempts done in emulating chemistry with different ML techniques (Grassi et al. 2021).\nLimited to our knowledge, the present work is the first case of PINN application for systems of ODEswith this level of complexity. Thus, this proof-of-concept work is a necessary step before using the trained models as emulators in simulations. In the range of applicability tested here, we can conclude that the models can be used as emulator without a significant loss of precision, as long as further refinement is included for those \ud835\udc5b-\ud835\udc47 regions where some of the species tracked by the network experience a loss of precision. For the future, the promising speed-up (up to \u223c 200) and the absence of variance in the completion time of the calculation make the PINN a very palatable tool for the solution of chemical networks in astrophysical simulations.\nMNRAS 000, 1\u201316 (2022)"
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "AP acknowledges support from the ERC Advanced Grant INTERSTELLAR H2020/740120. We gratefully acknowledge computational resources of the Center for High Performance Computing (CHPC) at SNS. We acknowledge the use of the Python programming language (Van Rossum & Drake 2009), Matplotlib (Hunter 2007), NumPy (van der Walt et al. 2011), and Scipy (Virtanen et al. 2019).\nData availability\nThe derived data generated in this research will be shared on reasonable requests to the corresponding author.\nThis paper has been typeset from a TEX/LATEX file prepared by the author."
        }
    ],
    "title": "Neural networks: solving the chemistry of the interstellar medium",
    "year": 2022
}