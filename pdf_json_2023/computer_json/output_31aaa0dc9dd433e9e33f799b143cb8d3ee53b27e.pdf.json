{
    "abstractText": "Researchers in explainable artificial intelligence have developed numerous methods for helping users understand the predictions of complex supervised learning models. By contrast, explaining the uncertainty of model outputs has received relatively little attention. We adapt the popular Shapley value framework to explain various types of predictive uncertainty, quantifying each feature\u2019s contribution to the conditional entropy of individual model outputs. We consider games with modified characteristic functions and find deep connections between the resulting Shapley values and fundamental quantities from information theory and conditional independence testing. We outline inference procedures for finite sample error rate control with provable guarantees, and implement efficient algorithms that perform well in a range of experiments on real and simulated data. Our method has applications to covariate shift detection, active learning, feature selection, and active feature-value acquisition.",
    "authors": [
        {
            "affiliations": [],
            "name": "David S. Watson"
        },
        {
            "affiliations": [],
            "name": "Joshua O\u2019Hara"
        }
    ],
    "id": "SP:6c2c3d156dd46a21842a368af6bd43aca23f9c5a",
    "references": [
        {
            "authors": [
                "Kjersti Aas",
                "Martin Jullum",
                "Anders L\u00f8land"
            ],
            "title": "Explaining individual predictions when features are dependent: More accurate approximations to Shapley values",
            "venue": "Artif. Intell.,",
            "year": 2021
        },
        {
            "authors": [
                "Julius Adebayo",
                "Justin Gilmer",
                "Michael Muelly",
                "Ian Goodfellow",
                "Moritz Hardt",
                "Been Kim"
            ],
            "title": "Sanity checks for saliency maps",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Javier Antor\u00e1n",
                "Umang Bhatt",
                "Tameem Adel",
                "Adrian Weller",
                "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
            ],
            "title": "Getting a CLUE: A method for explaining uncertainty estimates",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Stephen Bates",
                "Emmanuel Cand\u00e8s",
                "Lihua Lei",
                "Yaniv Romano",
                "Matteo Sesia"
            ],
            "title": "Testing for outliers with conformal p-values",
            "venue": "The Annals of Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "Blair Bilodeau",
                "Natasha Jaques",
                "Pang Wei Koh",
                "Been Kim"
            ],
            "title": "Impossibility theorems for feature attribution",
            "venue": "arXiv preprint,",
            "year": 2022
        },
        {
            "authors": [
                "Sebastian Bordt",
                "Ulrike von Luxburg"
            ],
            "title": "From Shapley values to generalized additive models and back",
            "venue": "In Proceedings of The 26th International Conference on Artificial Intelligence and Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "Craig Boutilier",
                "Nir Friedman",
                "Moises Goldszmidt",
                "Daphne Koller"
            ],
            "title": "Context-specific independence in Bayesian networks",
            "venue": "In Proceedings of The 12th Conference on Uncertainty in Artificial Intelligence,",
            "year": 1996
        },
        {
            "authors": [
                "Hugh Chen",
                "Joseph D. Janizek",
                "Scott Lundberg",
                "Su-In Lee"
            ],
            "title": "True to the model or true to the data",
            "year": 2006
        },
        {
            "authors": [
                "Hugh Chen",
                "Ian C. Covert",
                "Scott M. Lundberg",
                "Su-In Lee"
            ],
            "title": "Algorithms to estimate Shapley value feature attributions",
            "venue": "arXiv preprint,",
            "year": 2022
        },
        {
            "authors": [
                "Jianbo Chen",
                "Le Song",
                "Martin Wainwright",
                "Michael Jordan"
            ],
            "title": "Learning to explain: An informationtheoretic perspective on model interpretation",
            "venue": "In Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Jianbo Chen",
                "Le Song",
                "Martin J. Wainwright",
                "Michael I. Jordan"
            ],
            "title": "L-Shapley and C-Shapley: Efficient model interpretation for structured data",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Tianqi Chen",
                "Carlos Guestrin"
            ],
            "title": "XGBoost: A scalable tree boosting system",
            "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
            "year": 2016
        },
        {
            "authors": [
                "Thomas M. Cover",
                "Joy A. Thomas"
            ],
            "title": "Elements of information theory",
            "year": 2006
        },
        {
            "authors": [
                "Ian Covert",
                "Su-In Lee"
            ],
            "title": "Improving kernelSHAP: Practical Shapley value estimation using linear regression",
            "venue": "In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Ian Covert",
                "Scott M. Lundberg",
                "Su-In Lee"
            ],
            "title": "Understanding global feature contributions with additive importance measures",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Ian Covert",
                "Scott M. Lundberg",
                "Su-In Lee"
            ],
            "title": "Explaining by removing: A unified framework for model explanation",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2021
        },
        {
            "authors": [
                "Dheeru Dua",
                "Casey Graff"
            ],
            "title": "UCI machine learning repository, 2017",
            "venue": "URL http://archive.ics.uci. edu/ml",
            "year": 2017
        },
        {
            "authors": [
                "Yoav Freund",
                "H Sebastian Seung",
                "Eli Shamir",
                "Naftali Tishby"
            ],
            "title": "Selective sampling using the query by committee algorithm",
            "venue": "Machine learning,",
            "year": 1997
        },
        {
            "authors": [
                "Jerome H. Friedman"
            ],
            "title": "Multivariate adaptive regression splines",
            "venue": "The Annals of Statistics,",
            "year": 1991
        },
        {
            "authors": [
                "Yarin Gal",
                "Zoubin Ghahramani"
            ],
            "title": "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning",
            "venue": "In Proceedings of The 33rd International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Andrew Gelman",
                "John B. Carlin",
                "Hal S. Stern",
                "David B. Dunson",
                "Aki Vehtari",
                "Donald Rubin"
            ],
            "title": "Bayesian data analysis",
            "year": 1995
        },
        {
            "authors": [
                "Chuan Guo",
                "Geoff Pleiss",
                "Yu Sun",
                "Kilian Q Weinberger"
            ],
            "title": "On calibration of modern neural networks",
            "venue": "In Proceedings of the International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Kilian Hendrickx",
                "Lorenzo Perini",
                "Dries Van der Plas",
                "Wannes Meert",
                "Jesse Davis"
            ],
            "title": "Machine learning with a reject option: A survey",
            "venue": "arXiv preprint arXiv:2107.11277,",
            "year": 2021
        },
        {
            "authors": [
                "Andrew Herren",
                "P. Richard Hahn"
            ],
            "title": "Statistical aspects of SHAP: Functional ANOVA for model interpretation",
            "venue": "arXiv preprint,",
            "year": 2022
        },
        {
            "authors": [
                "Tom Heskes",
                "Evi Sijben",
                "Ioan Gabriel Bucur",
                "Tom Claassen"
            ],
            "title": "Causal Shapley values: Exploiting causal knowledge to explain individual predictions of complex models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Giles Hooker",
                "Lucas Mentch",
                "Siyu Zhou"
            ],
            "title": "Unrestricted permutation forces extrapolation: variable importance requires at least one more model, or there is no free variable importance",
            "venue": "Statistics and Computing,",
            "year": 2021
        },
        {
            "authors": [
                "Torsten Hothorn",
                "Achim Zeileis"
            ],
            "title": "Predictive distribution modeling using transformation forests",
            "venue": "J. Comput. Graph. Stat.,",
            "year": 2021
        },
        {
            "authors": [
                "Neil Houlsby",
                "Ferenc Husz\u00e1r",
                "Zoubin Ghahramani",
                "M\u00e1t\u00e9 Lengyel"
            ],
            "title": "Bayesian active learning for classification and preference learning",
            "venue": "arXiv preprint,",
            "year": 2011
        },
        {
            "authors": [
                "Xuanxiang Huang",
                "Joao Marques-Silva"
            ],
            "title": "The inadequacy of Shapley values for explainability",
            "venue": "arXiv preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Eyke H\u00fcllermeier",
                "Willem Waegeman"
            ],
            "title": "Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods",
            "venue": "Mach. Learn.,",
            "year": 2021
        },
        {
            "authors": [
                "Dominik Janzing",
                "Lenon Minorics",
                "Patrick Bloebaum"
            ],
            "title": "Feature relevance quantification in explainable AI: A causal problem",
            "venue": "In Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Neil Jethani",
                "Mukund Sudarshan",
                "Yindalon Aphinyanaphongs",
                "Rajesh Ranganath"
            ],
            "title": "Have we learned to explain?: How interpretability methods can learn to encode predictions in their interpretations",
            "venue": "In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Neil Jethani",
                "Adriel Saporta",
                "Rajesh Ranganath"
            ],
            "title": "Don\u2019t be fooled: label leakage in explanation methods and the importance of their quantitative evaluation",
            "venue": "In Proceedings of The 26th International Conference on Artificial Intelligence and Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "Amir-Hossein Karimi",
                "Gilles Barthe",
                "Borja Balle",
                "Isabel Valera"
            ],
            "title": "Model-agnostic counterfactual explanations for consequential decisions",
            "venue": "In The 23rd International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Robert Kaufman"
            ],
            "title": "Heteroskedasticity in regression",
            "venue": "SAGE, London,",
            "year": 2013
        },
        {
            "authors": [
                "Andreas Kirsch",
                "Joost van Amersfoort",
                "Yarin Gal"
            ],
            "title": "BatchBALD: Efficient and diverse batch acquisition for deep bayesian active learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Sofia Ira Ktena",
                "Alykhan Tejani",
                "Lucas Theis",
                "Pranay Kumar Myana",
                "Deepak Dilipkumar",
                "Ferenc Husz\u00e1r",
                "Steven Yoo",
                "Wenzhe Shi"
            ],
            "title": "Addressing delayed feedback for continuous training with neural networks in CTR prediction",
            "venue": "In Proceedings of the 13th ACM Conference on Recommender Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Himabindu Lakkaraju",
                "Ece Kamar",
                "Rich Caruana",
                "Jure Leskovec"
            ],
            "title": "Faithful and customizable explanations of black box models",
            "venue": "In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society,",
            "year": 2019
        },
        {
            "authors": [
                "Balaji Lakshminarayanan",
                "Alexander Pritzel",
                "Charles Blundell"
            ],
            "title": "Simple and scalable predictive uncertainty estimation using deep ensembles",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Tor Lattimore",
                "Csaba Szepesv\u00e1ri"
            ],
            "title": "Bandit algorithms",
            "year": 2020
        },
        {
            "authors": [
                "Jing Lei",
                "Max G\u2019Sell",
                "Alessandro Rinaldo",
                "Ryan J Tibshirani",
                "Larry Wasserman"
            ],
            "title": "Distribution-Free Predictive Inference for Regression",
            "venue": "Journal of the American Statistical Association,",
            "year": 2018
        },
        {
            "authors": [
                "Dan Ley",
                "Umang Bhatt",
                "Adrian Weller"
            ],
            "title": "Diverse, global and amortised counterfactual explanations for uncertainty estimates",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Zachary Lipton"
            ],
            "title": "The mythos of model interpretability",
            "venue": "Commun. ACM,",
            "year": 2018
        },
        {
            "authors": [
                "Scott M. Lundberg",
                "Su-In Lee"
            ],
            "title": "A unified approach to interpreting model predictions",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Scott M. Lundberg",
                "Gabriel Erion",
                "Hugh Chen",
                "Alex DeGrave",
                "Jordan M. Prutkin",
                "Bala Nair",
                "Ronit Katz",
                "Jonathan Himmelfarb",
                "Nisha Bansal",
                "Su-In Lee"
            ],
            "title": "From local explanations to global understanding with explainable AI for trees",
            "venue": "Nat. Mach. Intell.,",
            "year": 2020
        },
        {
            "authors": [
                "Christoph Luther",
                "Gunnar K\u00f6nig",
                "Moritz Grosse-Wentrup"
            ],
            "title": "Efficient SAGE estimation via causal structure learning",
            "venue": "In Proceedings of The 26th International Conference on Artificial Intelligence and Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "Sisi Ma",
                "Roshan Tourani"
            ],
            "title": "Predictive and causal implications of using shapley value for model interpretation",
            "venue": "In Proceedings of the 2020 KDD Workshop on Causal Discovery,",
            "year": 2020
        },
        {
            "authors": [
                "Christopher Meek"
            ],
            "title": "Strong completeness and faithfulness in Bayesian networks",
            "venue": "In Proceedings of The 11th Conference on Uncertainty in Artificial Intelligence,",
            "year": 1995
        },
        {
            "authors": [
                "Luke Merrick",
                "Ankur Taly"
            ],
            "title": "The explanation game: Explaining machine learning models using shapley values",
            "venue": "In CD-MAKE,",
            "year": 2020
        },
        {
            "authors": [
                "Ramaravind K. Mothilal",
                "Amit Sharma",
                "Chenhao Tan"
            ],
            "title": "Explaining machine learning classifiers through diverse counterfactual explanations",
            "venue": "In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency,",
            "year": 2020
        },
        {
            "authors": [
                "Kevin Murphy"
            ],
            "title": "Probabilistic Machine Learning: An Introduction",
            "year": 2022
        },
        {
            "authors": [
                "Masashi Okamoto"
            ],
            "title": "Distinctness of the eigenvalues of a quadratic form in a multivariate sample",
            "venue": "The Annals of Statistics,",
            "year": 1973
        },
        {
            "authors": [
                "Lars H.B. Olsen",
                "Ingrid K. Glad",
                "Martin Jullum",
                "Kjersti Aas"
            ],
            "title": "Using shapley values and variational autoencoders to explain predictive models with dependent mixed features",
            "venue": "Journal of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Lars Henry Berge Olsen",
                "Ingrid Kristine Glad",
                "Martin Jullum",
                "Kjersti Aas"
            ],
            "title": "A comparative study of methods for estimating conditional Shapley values and when to use them",
            "venue": "arXiv preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Ian Osband",
                "Charles Blundell",
                "Alexander Pritzel",
                "Benjamin Van Roy"
            ],
            "title": "Deep exploration via bootstrapped DQN",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Yaniv Ovadia",
                "Emily Fertig",
                "Jie Ren",
                "Zachary Nado",
                "David Sculley",
                "Sebastian Nowozin",
                "Joshua Dillon",
                "Balaji Lakshminarayanan",
                "Jasper Snoek"
            ],
            "title": "Can you trust your model\u2019s uncertainty? evaluating predictive uncertainty under dataset shift",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Art B. Owen"
            ],
            "title": "Sobol\u2019 indices and Shapley value",
            "venue": "SIAM/ASA Journal on Uncertainty Quantification,",
            "year": 2014
        },
        {
            "authors": [
                "George Papamakarios",
                "Eric Nalisnick",
                "Danilo Jimenez Rezende",
                "Shakir Mohamed",
                "Balaji Lakshminarayanan"
            ],
            "title": "Normalizing flows for probabilistic modeling and inference",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2021
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga",
                "Alban Desmaison",
                "Andreas Kopf",
                "Edward Yang",
                "Zachary DeVito",
                "Martin Raison",
                "Alykhan Tejani",
                "Sasank Chilamkurthy",
                "Benoit Stiner",
                "Lu Fang",
                "Junjie Bai",
                "Soumith Chintala"
            ],
            "title": "Pytorch: an imperative style, high-performance deep learning library",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Apostolos F. Psaros",
                "Xuhui Meng",
                "Zongren Zou",
                "Ling Guo",
                "George Em Karniadakis"
            ],
            "title": "Uncertainty quantification in scientific machine learning: Methods, metrics, and comparisons",
            "venue": "J. Comput. Phys.,",
            "year": 2023
        },
        {
            "authors": [
                "John R. Quinlan"
            ],
            "title": "Induction of decision trees",
            "venue": "Mach. Learn,",
            "year": 1986
        },
        {
            "authors": [
                "John R. Quinlan. C"
            ],
            "title": "Programs for Machine Learning",
            "year": 1993
        },
        {
            "authors": [
                "Carl Edward Rasmussen",
                "Christopher K.I. Williams"
            ],
            "title": "Gaussian processes for machine learning",
            "year": 2006
        },
        {
            "authors": [
                "Marco T\u00falio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin"
            ],
            "title": "Why should I trust you?\": Explaining the predictions of any classifier",
            "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
            "year": 2016
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin"
            ],
            "title": "Anchors: High-precision model-agnostic explanations",
            "year": 2018
        },
        {
            "authors": [
                "Robert A. Rigby",
                "D.M. Stasinopoulos"
            ],
            "title": "Generalized additive models for location, scale and shape",
            "venue": "Journal of the Royal Statistical Society: Series C (Applied Statistics),",
            "year": 2005
        },
        {
            "authors": [
                "Burr Settles"
            ],
            "title": "Active learning literature survey",
            "venue": "Technical Report 1648,",
            "year": 2009
        },
        {
            "authors": [
                "Mohammad Hossein Shaker",
                "Eyke H\u00fcllermeier"
            ],
            "title": "Aleatoric and epistemic uncertainty with random forests",
            "venue": "Advances in Intelligent Data Analysis XVIII,",
            "year": 2020
        },
        {
            "authors": [
                "Dylan Slack",
                "Anna Hilgard",
                "Sameer Singh",
                "Himabindu Lakkaraju"
            ],
            "title": "Reliable post hoc explanations: Modeling uncertainty in explainability",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Lewis Smith",
                "Yarin Gal"
            ],
            "title": "Understanding measures of uncertainty for adversarial example detection",
            "venue": "In Proceedings of The 33rd International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Kacper Sokol",
                "Peter Flach"
            ],
            "title": "LIMEtree: Interactively customisable explanations based on local surrogate multi-output regression trees",
            "venue": "arXiv preprint,",
            "year": 2020
        },
        {
            "authors": [
                "Peter L. Spirtes",
                "Clark N. Glymour",
                "Richard Scheines"
            ],
            "title": "Causation, Prediction, and Search",
            "year": 2000
        },
        {
            "authors": [
                "Niranjan Srinivas",
                "Andreas Krause",
                "Sham Kakade",
                "Matthias Seeger"
            ],
            "title": "Gaussian process optimization in the bandit setting: No regret and experimental design",
            "venue": "In Proceedings of the 27th International Conference on Machine Learning,",
            "year": 2010
        },
        {
            "authors": [
                "Mukund Sundararajan",
                "Amir Najmi"
            ],
            "title": "The many Shapley values for model explanation",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Mukund Sundararajan",
                "Ankur Taly",
                "Qiqi Yan"
            ],
            "title": "Axiomatic attribution for deep networks",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning - Volume 70,",
            "year": 2017
        },
        {
            "authors": [
                "Richard Sutton",
                "Andrew G. Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "year": 2018
        },
        {
            "authors": [
                "Natasa Tagasovska",
                "David Lopez-Paz"
            ],
            "title": "Single-model uncertainties for deep learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Jacopo Teneggi",
                "Beepul Bharti",
                "Yaniv Romano",
                "Jeremias Sulam"
            ],
            "title": "From Shapley back to Pearson: Hypothesis testing via the Shapley value",
            "venue": "arXiv preprint,",
            "year": 2022
        },
        {
            "authors": [
                "Janek Thomas",
                "Andreas Mayr",
                "Bernd Bischl",
                "Matthias Schmid",
                "Adam Smith",
                "Benjamin Hofner"
            ],
            "title": "Gradient boosting for distributional regression: Faster tuning and improved variable selection via noncyclical updates",
            "venue": "Statistics and Computing,",
            "year": 2018
        },
        {
            "authors": [
                "Caroline Uhler",
                "Garvesh Raskutti",
                "Peter B\u00fchlmann",
                "Bin Yu"
            ],
            "title": "Geometry of the faithfulness assumption in causal inference",
            "venue": "The Annals of Statistics,",
            "year": 2013
        },
        {
            "authors": [
                "Guy Van den Broeck",
                "Anton Lykov",
                "Maximilian Schleich",
                "Dan Suciu"
            ],
            "title": "On the tractability of SHAP explanations",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Vladimir Vovk",
                "Alexander Gammerman",
                "Glenn Shafer"
            ],
            "title": "Algorithmic Learning in a Random World",
            "year": 2005
        },
        {
            "authors": [
                "Erik \u0160trumbelj",
                "Igor Kononenko"
            ],
            "title": "An efficient explanation of individual classifications using game theory",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2010
        },
        {
            "authors": [
                "Sandra Wachter",
                "Brent Mittelstadt",
                "Chris Russell"
            ],
            "title": "Counterfactual explanations without opening the black box: Automated decisions and the GDPR",
            "venue": "Harvard J. Law Technol.,",
            "year": 2018
        },
        {
            "authors": [
                "David S. Watson"
            ],
            "title": "Conceptual challenges for interpretable machine",
            "venue": "learning. Synthese,",
            "year": 2022
        },
        {
            "authors": [
                "David S Watson",
                "Luciano Floridi"
            ],
            "title": "The explanation game: a formal framework for interpretable machine",
            "venue": "learning. Synthese,",
            "year": 2021
        },
        {
            "authors": [
                "David S. Watson",
                "Limor Gultchin",
                "Ankur Taly",
                "Luciano Floridi"
            ],
            "title": "Local explanations via necessity and sufficiency: Unifying theory and practice",
            "venue": "Minds Mach.,",
            "year": 2022
        },
        {
            "authors": [
                "Brian Williamson",
                "Jean Feng"
            ],
            "title": "Efficient nonparametric statistical inference on population feature importance using shapley values",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Lisa Wimmer",
                "Yusuf Sale",
                "Paul Hofman",
                "Bernd Bischl",
                "Eyke H\u00fcllermeier"
            ],
            "title": "Quantifying aleatoric and epistemic uncertainty in machine learning: Are conditional entropy and mutual information appropriate measures",
            "venue": "In Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Jinsung Yoon",
                "James Jordon",
                "Mihaela van der Schaar"
            ],
            "title": "INVASE: Instance-wise variable selection using neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Jiji Zhang",
                "Peter L. Spirtes"
            ],
            "title": "Strong faithfulness and uniform consistency in causal inference",
            "venue": "In Proceedings of The 19th Conference on Uncertainty in Artificial Intelligence,",
            "year": 2003
        },
        {
            "authors": [
                "Xingyu Zhao",
                "Wei Huang",
                "Xiaowei Huang",
                "Valentin Robu",
                "David Flynn"
            ],
            "title": "Baylime: Bayesian local interpretable model-agnostic explanations",
            "venue": "In Proceedings of the 37th Conference on Uncertainty in Artificial Intelligence,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Machine learning (ML) algorithms can solve many prediction tasks with greater accuracy than classical methods. However, some of the most popular and successful algorithms, such as deep neural networks, often produce models with millions of parameters and complex nonlinearities. The resulting \u201cblack box\u201d is essentially unintelligible to humans. Researchers in explainable artificial intelligence (XAI) have developed numerous methods to help users better understand the inner workings of such models (see Sect. 2).\nDespite the rapid proliferation of XAI tools, the goals of the field have thus far been somewhat narrow. The vast majority of methods in use today aim to explain model predictions, i.e. point estimates. But these are not necessarily the only model output of interest. Predictive uncertainty can also vary widely across the feature space, in ways that may have a major impact on model performance and human decision making. Such variation makes it risky to rely on the advice of a black box, especially when generalizing to new environments. Discovering the source of uncertainty can be an important first step toward reducing it.\nQuantifying predictive uncertainty has many applications in ML. For instance, it is an essential subroutine in any task that involves exploration, e.g. active learning [18, 36], multi-armed bandits [73, 40], and reinforcement learning more generally [55, 76]. Other applications of predictive\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\nar X\niv :2\n30 6.\n05 72\n4v 2\n[ st\nat .M\nL ]\nuncertainty quantification include detecting covariate shift [77] and adversarial examples [70], as well as classification with reject option [23]. Our method aims to expand the scope of XAI to these varied domains by explaining predictive uncertainty via feature attributions.\nKnowing the impact of individual features on local uncertainty can help drive data collection and model design. It can be used to detect the source of a suspected covariate shift, select informative features, and test for heteroskedasticity. Our attribution strategy makes use of the Shapley value framework for XAI [83, 44, 74, 49, 9], a popular approach inspired by cooperative game theory, which we adapt by altering the characteristic function and augment with inference procedures for provable error rate control. The approach is fully model-agnostic and therefore not limited to any particular function class.\nOur main contributions are threefold: (1) We describe modified variants of the Shapley value algorithm that can explain higher moments of the predictive distribution, thereby extending its explanatory utility beyond mere point estimates. We provide an information theoretic interpretation of the resulting measures and study their properties. (2) We introduce a split conformal inference procedure for Shapley variables with finite sample coverage guarantees. This allows users to test the extent to which attributions for a given feature are concentrated around zero with fixed type I error control. (3) We implement model-specific and model-agnostic variants of our method and illustrate their performance in a range of simulated and real-world experiments, with applications to feature selection, covariate shift detection, and active learning."
        },
        {
            "heading": "2 Related Work",
            "text": "XAI has become a major subfield of machine learning in recent years. The focus to date has overwhelmingly been on explaining predictions in supervised learning tasks, most prominently via feature attributions [64, 44, 75], rule lists [65, 38, 71], and counterfactuals [84, 50, 34]. Despite obvious differences between these methods, all arguably share the same goal of identifying minimal conditions sufficient to alter predictions in some pre-specified way [87]. Resulting explanations should be accurate, simple, and relevant for the inquiring agent [86].\nQuantifying inductive uncertainty is a fundamental problem in probability theory and statistics, although machine learning poses new challenges and opportunities in this regard [30]. The classical literature on this topic comes primarily from Bayesian modeling [21] and information theory [13], which provide a range of methods for analyzing the distribution of random variables. More recent work on conformal inference [82, 41, 4] has expanded the toolkit for practitioners.\nImportant application areas for these methods include active learning (AL) and covariate shift detection. In AL, the goal is to selectively query labels for unlabeled instances aimed to maximize classifier improvement under a given query budget. Methods often select instances on which the model has high epistemic (as opposed to aleatoric) uncertainty [67], as for example in BatchBALD [36]. This is especially valuable when labels are sparse and costly to collect while unlabeled data is widely available. In covariate shift detection, the goal is to identify samples that are abnormal relative to the in-distribution observations that the classifier has seen during training. It is well-known that neural networks can be overconfident [22], yielding predictions with unjustifiably high levels of certainty on test samples. Addressing this issue is an active area of research, and a variety of articles take a perspective of quantifying epistemic uncertainty [30, 56]. In safety-critical applications, the degree of model uncertainty can be factored into the decision making, for example by abstaining from prediction altogether when confidence is sufficiently low [23].\nVery little work has been done on explaining predictive uncertainty. A notable exception is the CLUE algorithm [3, 42], a model-specific method designed for Bayesian deep learning, which generates counterfactual samples that are maximally similar to some target observation but optimized for minimal conditional variance. This contrasts with our feature attribution approach, which is model-agnostic and thereby the first to make uncertainty explanations available to function classes beyond Bayesian deep learning models.\nPredictive uncertainty is often correlated with prediction loss, and therefore explanations of model errors are close relatives of our method. LossSHAP [45] is an extension of Lundberg and Lee [44]\u2019s SHAP algorithm designed to explain the pointwise loss of a supervised learner (e.g., squared error or cross entropy). Though this could plausibly help identify regions where the model is least certain\nabout predictions, it requires a large labelled test dataset, which may not be available in practice. By contrast, our method only assumes access to some unlabelled dataset of test samples, which is especially valuable when labels are slow or expensive to collect. For instance, LossSHAP is little help in learning environments where covariate shift is detectable before labels are known [94]. This is common, for example, in online advertising [37], where an impression today may lead to a conversion next week but quick detection (and explanation) of covariate shift is vital.\nPrevious authors have explored information theoretic interpretations of variable importance measures; see [16, Sect. 8.3] for a summary. These methods often operate at global resolutions\u2014e.g., Sobol\u2019 indices [57] and SAGE [15]\u2014whereas our focus is on local explanations. Alternatives such as INVASE [90] must be trained alongside the supervised learner itself and are therefore not modelagnostic. L2X [10], REAL-X [32], and SHAP-KL [33] provide post-hoc local explanations, but they require surrogate models to approximate a joint distribution over the full feature space. Chen et al. [11] propose an information theoretic variant of Shapley values for graph-structured data, which we examine more closely in Sect. 4."
        },
        {
            "heading": "3 Background",
            "text": "Notation. We use uppercase letters to denote random variables (e.g., X) and lowercase for their values (e.g., x). Matrices and sets of random variables are denoted by uppercase boldface type (e.g., X) and vectors by lowercase boldface (e.g., x). We occasionally use superscripts to denote samples, e.g. x(i) is the ith row of X. Subscripts index features or subsets thereof, e.g. XS = {Xj}j\u2208S and x (i) S = {x (i) j }j\u2208S , where S \u2286 [d] = {1, . . . , d}. We define the complementary subset S = [d]\\S.\nInformation Theory. Let p, q be two probability distributions over the same \u03c3-algebra of events. Further, let p, q be absolutely continuous with respect to some appropriate measure. We make use of several fundamental quantities from information theory [13], such as entropy H(p), cross entropy H(p, q), KL-divergence DKL(p \u2225 q), and mutual information I(X;Y ) (all formally defined in Appx. B.1). We use shorthand for the (conditional) probability mass/density function of the random variable Y , e.g. pY |xS := p(Y | XS = xS). We speak interchangeably of the entropy of a random variable and the entropy of the associated mass/density function: H(Y | x) = H(pY |x). We call this the local conditional entropy to distinguish it from its global counterpart, H(Y | X), which requires marginalization over the joint space X \u00d7 Y .\nShapley Values. Consider a supervised learning model f trained on features X \u2208 X \u2286 Rd to predict outcomes Y \u2208 Y \u2286 R. We assume that data are distributed according to some fixed but unknown distribution D. Shapley values are a feature attribution method in which model predictions are decomposed as a sum: f(x) = \u03d50 + \u2211d j=1 \u03d5(j,x), where \u03d50 is the baseline expectation (i.e., \u03d50 = ED[f(x)]) and \u03d5(j,x) denotes the Shapley value of feature j at point x. To define this quantity, we require a value function v : 2[d] \u00d7 Rd 7\u2192 R that quantifies the payoff associated with subsets S \u2286 [d] for a particular sample. This characterizes a cooperative game, in which each feature acts as a player. A common choice for defining payoffs in XAI is the following [83, 44, 74, 49, 9]:\nv0(S,x) := ED [ f(x) | XS = xS ] ,\nwhere we marginalize over the complementary features S in accordance with reference distribution D. For any value function v, we may define the following random variable to represent j\u2019s marginal contribution to coalition S at point x:\n\u2206v(S, j,x) := v(S \u222a {j},x)\u2212 v(S,x).\nThen j\u2019s Shapley value is just the weighted mean of this variable over all subsets:\n\u03d5v(j,x) := \u2211\nS\u2286[d]\\{j}\n|S|! (d\u2212 |S| \u2212 1)! d!\n[ \u2206v(S, j,x) ] . (1)\nIt is well known that Eq. 1 is the unique solution to the attribution problem that satisfies certain desirable properties, including efficiency, symmetry, sensitivity, and linearity [74] (for formal statements of these axioms, see Appx. B.2.)"
        },
        {
            "heading": "4 Alternative Value Functions",
            "text": "To see how standard Shapley values can fall short, consider a simple data generating process with X,Z \u223c U(0, 1)2 and Y \u223c N (X,Z2). Since the true conditional expectation of Y is X , this feature will get 100% of the attributions in a game with payoffs given by v0. However, just because Z receives zero attribution does not mean that it adds no information to our predictions\u2014on the contrary, we can use Z to infer the predictive variance of Y and calibrate confidence intervals accordingly. This sort of higher order information is lost in the vast majority of XAI methods.\nWe consider information theoretic games that assign nonzero attribution to Z in the example above, and study the properties of resulting Shapley values. We start in an idealized scenario in which we have: (i) oracle knowledge of the joint distribution D; and (ii) unlimited computational budget, thereby allowing complete enumeration of all feature subsets.\nINVASE [90] is a method for learning a relatively small but maximally informative subset of features S \u2282 [d] using the loss function DKL(pY |x \u2225 pY |xS ) + \u03bb|S|, where \u03bb is a regularization penalty. Jethani et al. [33]\u2019s SHAP-KL adapts this loss function to define a new game:\nvKL(S,x) := \u2212DKL(pY |x \u2225 pY |xS ),\nwhich can be interpreted as \u22121 times the excess number of bits one would need on average to describe samples from Y | x given code optimized for Y | xS . Chen et al. [11] make a similar proposal, replacing KL-divergence with cross entropy:\nvCE (S,x) := \u2212H(pY |x, pY |xS ).\nThis value function is closely related to that of LossSHAP [45], which for likelihood-based loss functions can be written:\nvL(S,x) := \u2212 log p(Y = y | xS),\nwhere y denotes the true value of Y at the point x. As Covert et al. [16] point out, this is equivalent to the pointwise mutual information I(y;xS), up to an additive constant. However, vL requires true labels for Y , which may not be available when evaluating feature attributions on a test set. By contrast, vCE averages over Y , thereby avoiding this issue: vCE (S,x) = \u2212EY |x [ vL(S,x) ] . We reiterate that in all cases we condition on some fixed value of x and do not marginalize over the feature space X . This contrasts with global feature attribution methods like SAGE [15], which can be characterized by averaging vL over the complete joint distribution p(X, Y ).\nIt is evident from the definitions that vKL and vCE are equivalent up to an additive constant not depending on S, namely H(pY |x). This renders the resulting Shapley values from both games identical (all proofs in Appx. A.)1\nProposition 4.1. For all features j \u2208 [d], coalitions S \u2286 [d]\\{j}, and samples x \u223c DX :\n\u2206KL(S, j,x) = \u2206CE (S, j,x)\n= \u222b Y p(y | x) log p(y | xS , xj) p(y | xS) dy.\nThis quantity answers the question: if the target distribution were pY |x, how many more bits of information would we get on average by adding xj to the conditioning event xS? Resulting Shapley values summarize each feature\u2019s contribution in bits to the distance between Y \u2019s fully specified local posterior distribution p(Y | x) and the prior p(Y ).\nProposition 4.2. With v \u2208 {vKL, vCE}, Shapley values satisfy \u2211d j=1 \u03d5v(j,x) = DKL(pY |x \u2225 pY ).\nWe introduce two novel information theoretic games, characterized by negative and positive local conditional entropies:\nvIG(S,x) := \u2212H(Y | xS), vH(S,x) := H(Y | xS).\n1All propositions in this section can be adapted to the classification setting by replacing the integral with a summation over labels Y .\nThe former subscript stands for information gain; the latter for entropy. Much like vCE , these value functions can be understood as weighted averages of LossSHAP payoffs over Y , however this time with expectation over a slightly different distribution: vIG(S,x) = \u2212vH(S,x) = \u2212EY |xS [ vL(S,x) ] . The marginal contribution of feature j to coalition S is measured in bits of local conditional mutual information added or lost, respectively (note that \u2206IG = \u2212\u2206H ). Proposition 4.3. For all features j \u2208 [d], coalitions S \u2286 [d]\\{j}, and samples x \u223c DX :\n\u2206IG(S, j,x) = I(Y ;xj | xS).\nThis represents the decrease in Y \u2019s uncertainty attributable to the conditioning event xj when we already know xS . This quantity is similar (but not quite equivalent) to the information gain, a common optimization objective in tree growing algorithms [61, 62]. The difference again lies in the fact that we do not marginalize over X , but instead condition on a single instance. Resulting Shapley values summarize each feature\u2019s contribution in bits to the overall local information gain.\nProposition 4.4. Under vIG , Shapley values satisfy \u2211d j=1 \u03d5IG(j,x) = I(Y ;x).\nIn the classic game v0, out-of-coalition features are eliminated by marginalization. However, this will not generally work in our information theoretic games. Consider the modified entropy game, designed to take d-dimensional input:\nvH\u2217(S,x) := ED [ H(pY |x) | XS = xS ] .\nThis game is not equivalent to vH , as shown in the following proposition.\nProposition 4.5. For all coalitions S \u2282 [d] and samples x \u223c DX :\nvH(S,x)\u2212 vH\u2217(S,x) = DKL(pY |XS ,xS \u2225 pY |xS ).\nThe two value functions will tend to diverge when out-of-coalition features XS inform our predictions about Y , given prior knowledge of xS . Resulting Shapley values represent the difference in bits between the local and global conditional entropy.\nProposition 4.6. Under vH\u2217 , Shapley values satisfy \u2211d j=1 \u03d5H\u2217(j,x) = H(Y | x)\u2212H(Y | X).\nIn other words, \u03d5H\u2217(j,x) is j\u2019s contribution to conditional entropy at a given point, compared to a global baseline that averages over all points.\nThese games share an important and complex relationship to conditional independence structures. We distinguish here between global claims of conditional independence, e.g. Y \u22a5\u22a5X | Z, and local or context-specific independence (CSI), e.g. Y \u22a5\u22a5X | z. The latter occurs when X adds no information about Y under the conditioning event Z = z [7] (see Appx. B.1 for an example).\nTheorem 4.7. For value functions v \u2208 {vKL, vCE , vIG , vH }, we have:\n(a) Y \u22a5\u22a5Xj | XS \u21d4 supx\u2208X |\u2206v (S, j,x)| = 0. (b) Y \u22a5\u22a5Xj | xS \u21d2 \u2206v (S, j,x) = 0. (c) The set of distributions such that \u2206v(S, j,x) = 0 \u2227 Y \u22a5\u0338\u22a5Xj | xS is Lebesgue measure zero.\nItem (a) states that Y is conditionally independent of Xj given XS if and only if j makes no contribution to S at any point x. Item (b) states that the weaker condition of CSI is sufficient for zero marginal payout. However, while the converse does not hold in general, item (c) states that the set of counterexamples is small in a precise sense\u2014namely, it has Lebesgue measure zero. A similar result holds for so-called unfaithful distributions in causality [72, 92], in which positive and negative effects cancel out exactly, making it impossible to detect certain graphical structures. Similarly, context-specific dependencies may be obscured when positive and negative log likelihood ratios cancel out as we marginalize over Y . Measure zero events are not necessarily harmless, especially when working with finite samples. Near violations may in fact be quite common due to statistical noise [80]. Together, these results establish a powerful, somewhat subtle link between conditional independencies and information theoretic Shapley values. Similar results are lacking for the standard value function v0\u2014with the notable exception that conditional independence implies zero marginal payout [46]\u2014an inevitable byproduct of the failure to account for predictive uncertainty."
        },
        {
            "heading": "5 Method",
            "text": "The information theoretic quantities described in the previous section are often challenging to calculate, as they require extensive conditioning and marginalization. Computing some O(2d) such quantities per Shapley value, as Eq. 1 requires, quickly becomes infeasible. (See [81] for an in-depth analysis of the time complexity of Shapley value algorithms.) Therefore, we make several simplifying assumptions that strike a balance between computational tractability and error rate control.\nFirst, we require some uncertainty estimator h : X 7\u2192 R\u22650. Alternatively, we could train new estimators for each coalition [88]; however, this can be impractical for large datasets and/or complex function classes. In the previous section, we assumed access to the true data generating process. In practice, we must train on finite samples, often using outputs from the base model f . In the regression setting, this may be a conditional variance estimator, as in heteroskedastic error models [35]; in the classification setting, we assume that f outputs a pmf over class labels and write fy : Rd 7\u2192 [0, 1] to denote the predicted probability of class y \u2208 Y . Then predictive entropy is estimated via the plug-in formula ht(x) := \u2212 \u2211 y\u2208Y fy(x) log fy(x), where the subscript t stands for total.\nIn many applications, we must decompose total entropy into epistemic and aleatoric components\u2014i.e., uncertainty arising from the model or the data, respectively. We achieve this via ensemble methods, using a set of B basis functions, {f1, . . . , fB}. These may be decision trees, as in a random forest [68], or subsets of neural network nodes, as in Monte Carlo (MC) dropout [20]. Let f by(x) be the conditional probability estimate for class y given sample x for the bth basis function. Then aleatoric uncertainty is given by ha(x) := \u2212 1B \u2211B b=1 \u2211 y\u2208Y f b y(x) log f b y(x). Epistemic uncertainty is simply the difference [28], he(x) := ht(x)\u2212 ha(x).2 Alternative methods may be appropriate for specific function classes, e.g. Gaussian processes [63] or Bayesian deep learning models [51]. We leave the choice of which uncertainty measure to explain up to practitioners. In what follows, we use the generic h(x) to signify whichever estimator is of relevance for a given application.\nWe are similarly ecumenical regarding reference distributions. This has been the subject of much debate in recent years, with authors variously arguing that D should be a simple product of marginals [31]; or that the joint distribution should be modeled for proper conditioning and marginalization [1]; or else that structural information should be encoded to quantify causal effects [25]. Each approach makes sense in certain settings [8, 85], so we leave it up to practitioners to decide which is most appropriate for their use case. We stress that information theoretic games inherit all the advantages and disadvantages of these samplers from the conventional XAI setting, and acknowledge that attributions should be interpreted with caution when models are forced to extrapolate to off-manifold data [26]. Previous work has shown that no single sampling method dominates, with performance varying as a function of data type and function class [53]; see [9] for a discussion.\nFinally, we adopt standard methods to efficiently sample candidate coalitions. Observe that the distribution on subsets implied by Eq. 1 induces a symmetric pmf on cardinalities |S| \u2208 {0, . . . , d\u22121} that places exponentially greater weight at the extrema than it does at the center. Thus while there are over 500 billion coalitions at d = 40, we can cover 50% of the total weight by sampling just over 0.1% of these subsets (i.e., those with cardinality \u2264 9 or \u2265 30). To reach 90% accuracy requires just over half of all coalitions. In fact, under some reasonable conditions, sampling \u0398(n) coalitions is asymptotically optimal, up to a constant factor [88]. We also employ the paired sampling approach of Covert and Lee [14] to reduce variance and speed up convergence still further.\nSeveral authors have proposed inference procedures for Shapley values [88, 69, 93]. These methods could in principle be extended to our revised games. However, existing algorithms are typically either designed for local inference, in which case they are ill-suited to make global claims about feature relevance, or require global value functions upfront, unlike the local games we consider here. As an alternative, we describe a method for aggregating local statistics for global inference. Specifically, we test whether the random variable \u03d5(j,x) tends to concentrate around zero for a given j. We take a conformal approach [82, 41] that provides the following finite sample coverage guarantee.\nTheorem 5.1 (Coverage). Partition n training samples {(x(i), y(i))}ni=1 \u223c D into two equalsized subsets I1, I2 where I1 is used for model fitting and I2 for computing Shapley values. Fix\n2The additive decomposition of total uncertainty into epistemic and aleatoric components has recently been challenged [89]. While alternative formulations are possible, we stick with this traditional view, which is widely used in deep ensembles and related methods for probabilistic ML [39, 56, 60].\na target level \u03b1 \u2208 (0, 1) and estimate the upper and lower bounds of the Shapley distribution from the empirical quantiles. That is, let q\u0302lo be the \u2113th smallest value of \u03d5(j,x(i)), i \u2208 I2, for \u2113 = \u2308(n/2+1)(\u03b1/2)\u2309, and let q\u0302hi be the uth smallest value of the same set, for u = \u2308(n/2+1)(1\u2212\u03b1/2)\u2309. Then for any test sample x(n+1) \u223c D, we have:\nP ( \u03d5(j,x(n+1)) \u2208 [q\u0302lo, q\u0302hi] ) \u2265 1\u2212 \u03b1.\nMoreover, if Shapley values have a continuous joint distribution, then the upper bound on this probability is 1\u2212 \u03b1+ 2/(n+ 2).\nNote that this is not a conditional coverage claim, insomuch as the bounds are fixed for a given Xj and do not vary with other feature values. However, Thm. 5.1 provides a PAC-style guarantee that Shapley values do not exceed a given (absolute) threshold with high probability, or that zero falls within the (1\u2212 \u03b1)\u00d7 100% confidence interval for a given Shapley value. These results can inform decisions about feature selection, since narrow intervals around zero are necessary (but not sufficient) evidence of uninformative predictors. This result is most relevant for tabular or text data, where features have some consistent meaning across samples; it is less applicable to image data, where individual pixels have no stable interpretation over images."
        },
        {
            "heading": "6 Experiments",
            "text": "Full details of all datasets and hyperparameters can be found in Appx. C, along with supplemental experiments that did not fit in the main text. Code for all experiments and figures can be found in our dedicated GitHub repository.3 We use DeepSHAP to sample out-of-coalition feature values in neural network models, and TreeSHAP for boosted ensembles. Alternative samplers are compared in a separate simulation experiment below. Since our goal is to explain predictive entropy rather than information, we use the value function vH\u2217 throughout, with plug-in estimators for total, epistemic, and/or aleatoric uncertainty."
        },
        {
            "heading": "6.1 Supervised Learning Examples",
            "text": "First, we perform a simple proof of concept experiment that illustrates the method\u2019s performance on image, text, and tabular data.\nImage Data. We examine binary classifiers on subsets of the MNIST dataset. Specifically, we train deep convolutional neural nets to distinguish 1 vs. 7, 3 vs. 8, and 4 vs. 9. These digit pairs tend to look similar in many people\u2019s handwriting and are often mistaken for one another. We therefore expect relatively high uncertainty in these examples, and use a variant of DeepSHAP to visualize the pixel-wise contributions to predictive entropy, as estimated via MC dropout. We compute attributions for epistemic and aleatoric uncertainty, visually confirming that the former identifies regions of the image that most increase or reduce uncertainty (see Fig. 1A).\nApplying our method, we find that epistemic uncertainty is reduced by the upper loop of the 9, as well as by the downward hook on the 7. By contrast, uncertainty is increased by the odd angle of the 8 and its small bottom loop. Aleatoric uncertainty, by contrast, is more mixed across the pixels, reflecting irreducible noise.\nText Data. We apply a transformer network to the IMDB dataset, which contains movie reviews for some 50,000 films. This is a sentiment analysis task, with the goal of identifying positive vs. negative reviews. We visualize the contribution of individual words to the uncertainty of particular predictions as calculated using the modified DeepSHAP pipeline, highlighting how some tokens tend to add or remove predictive information.\nWe report results for two high-entropy examples in Fig. 1B. In the first review, the model appears confused by the sentence \u201cThis is not Great Cinema but I was most entertained,\u201d which clearly conveys some ambiguity in the reviewer\u2019s sentiment. In the second example, the uncertainty comes from several sources including unexpected juxtapositions such as \u201claughing and crying\u201d, as well as \u201cliar liar...you will love this movie.\u201d\n3https://github.com/facebookresearch/infoshap.\nTabular Data. We design a simple simulation experiment, loosely inspired by [1], in which Shapley values under the entropy game have a closed form solution (see Appx. C.1 for details). This allows us to compare various approaches for sampling out-of-coalition feature values. Variables X are multivariate normally distributed with a Toeplitz covariance matrix \u03a3ij = \u03c1|i\u2212j| and d = 4 dimensions. The conditional distribution of outcome variable Y is Gaussian, with mean and variance depending on x. We exhaustively enumerate all feature subsets at varying sample sizes and values of the autocorrelation parameter \u03c1.\nFor imputation schemes, we compare KernelSHAP, maximum likelihood, copula methods, empirical samplers, and ctree (see [54] for definitions of each, and a benchmark study of conditional sampling strategies for feature attributions). We note that this task is strictly more difficult than computing Shapley values under the standard v0, since conditional variance must be estimated from the residuals of a preliminary model, itself estimated from the data. No single method dominates throughout, but most converge on the true Shapley value as sample size increases. Predictably, samplers that take conditional relationships into account tend to do better under autocorrelation than those that do not."
        },
        {
            "heading": "6.2 Covariate Shift and Active Learning",
            "text": "To illustrate the utility of our method for explaining covariate shift, we consider several semi-synthetic experiments. We start with four binary classification datasets from the UCI machine learning repository [17]\u2014BreastCancer, Diabetes, Ionosphere, and Sonar\u2014and make a random 80/20 train/test split on each. We use an XGBoost model [12] with 50 trees to estimate conditional probabilities and the associated uncertainty. We then perturb a random feature from the test set, adding a small amount of Gaussian noise to alter its underlying distribution. Resulting predictions have a large degree of entropy, and would therefore be ranked highly by an AL acquisition function. We compute information theoretic Shapley values for original and perturbed test sets. Results are visualized in Fig. 3.\nOur method clearly identifies the source of uncertainty in these datapoints, assigning large positive or negative attributions to perturbed features in the test environment. Note that the distribution shifts\nare fairly subtle in each case, rarely falling outside the support of training values for a given feature. Thus we find that information theoretic Shapley values can be used in conjunction with covariate shift detection algorithms to explain the source of the anomaly, or in conjunction with AL algorithms to explain the exploratory selection procedure."
        },
        {
            "heading": "6.3 Feature Selection",
            "text": "Another application of the method is as a feature selection tool when heteroskedasticity is driven by some but not all variables. For this experiment, we modify the classic Friedman benchmark [19], which was originally proposed to test the performance of nonlinear regression methods under signal sparsity. Outcomes are generated according to:\nY = 10 sin(\u03c0X1X2) + 20(X3 \u2212 0.5)2 + 10X4 + 5X5 + \u03f5y,\nwith input features X \u223c U(0, 1)10 and standard normal residuals \u03f5y \u223c N (0, 12). To adapt this DGP to our setting, we scale Y to the unit interval and define:\nZ = 10 sin(\u03c0X6X7) + 20(X8 \u2212 0.5)2 + 10X9 + 5X10 + \u03f5z,\nwith \u03f5z \u223c N (0, Y\u0303 2), where Y\u0303 denotes the rescaled version of Y . Note that Z\u2019s conditional variance depends exclusively on the first five features, while its conditional mean depends only on the second five. Thus with f(x) = E[Z | x] and h(x) = V[Z | x], we should expect Shapley values for f to concentrate around zero for {X6, . . . , X10}, while Shapley values for h should do the same for {X1, . . . , X5}. We draw 2000 training samples and fit f using XGBoost with 100 trees. This provides estimates of both the conditional mean (via predictions) and the conditional variance (via observed residuals \u03f5\u0302y). We fit a second XGBoost model h with the same hyperparameters to predict log(\u03f5\u03022y). Results are reported on a test set of size 1000. We compute attributions using TreeSHAP [45] and visualize results in Fig. 4A. We find that Shapley values are clustered around zero for unimportant features in each model, demonstrating the method\u2019s promise for discriminating between different modes of predictive information. In a supplemental experiment, we empirically evaluate our conformal coverage guarantee on this same task, achieving nominal coverage at \u03b1 = 0.1 for all features (see Appx. C.3).\nAs an active feature-value acquisition example, we use the same modified Friedman benchmark, but this time increase the training sample size to 5000 and randomly delete some proportion of cells in the design matrix for X. This simulates the effect of missing data, which may arise due to entry errors or high collection costs. XGBoost has native methods for handling missing data at training and test time, although resulting Shapley values are inevitably noisy. We refit the conditional variance estimator h and record feature rankings with variable missingness.\nThe goal in active feature-value acquisition is to prioritize the variables whose values will best inform future predictions subject to budgetary constraints. Fig. 4B shows receiver operating characteristic (ROC) curves for a feature importance ranking task as the frequency of missing data increases from zero to 50%. Importance is estimated via absolute Shapley values. Though performance degrades with increased missing data, as expected, we find that our method reliably ranks important features above unimportant ones in all trials. Even with fully half the data missing, we find an AUC of 0.682, substantially better than random."
        },
        {
            "heading": "7 Discussion",
            "text": "Critics have long complained that Shapley values (using the conventional payoff function v0) are difficult to interpret in XAI. It is not always clear what it even means to remove features [43, 2], and large/small attributions are neither necessary nor sufficient for important/unimportant predictors, respectively [5, 29]. In an effort to ground these methods in classical statistical notions, several authors have analyzed Shapley values in the context of ANOVA decompositions [24, 6] or conditional independence tests [47, 78], with mixed results. Our information theoretic approach provides another window into this debate. With modified value functions, we show that marginal payoffs \u2206v(S, j,x) have an unambiguous interpretation as a local dependence measure. Still, Shapley values muddy the waters somewhat by averaging these payoffs over coalitions.\nThere has been a great deal of interest in recent years on functional data analysis, where the goal is to model not just the conditional mean of the response variable E[Y | x], but rather the entire distribution P (Y | x), including higher moments. Distributional regression techniques have been developed for additive models [66], gradient boosting machines [79], random forests [27], and neural density estimators [58]. Few if any XAI methods have been specifically designed to explain such models, perhaps because attributions would be heavily weighted toward features with a significant impact on the conditional expectation, thereby simply reducing to classic measures. Our method provides one possible way to disentangle those attributions and focus attention on higher moments. Future work will explore more explicit connections to the domain of functional data.\nOne advantage of our approach is its modularity. We consider a range of different information theoretic games, each characterized by a unique value function. We are agnostic about how to estimate the relevant uncertainty measures, fix reference distributions, or sample candidate coalitions. These are all active areas of research in their own right, and practitioners should choose whichever combination of tools works best for their purpose.\nHowever, this flexibility does not come for free. Computing Shapley values for many common function classes is #P-hard, even when features are jointly independent [81]. Modeling dependencies to impute values for out-of-coalition features is a statistical challenge that requires extensive marginalization. Some speedups can be achieved by making convenient assumptions, but these may incur substantial errors in practice. These are familiar problems in feature attribution tasks. Our method inherits the same benefits and drawbacks."
        },
        {
            "heading": "8 Conclusion",
            "text": "We introduced a range of methods to explain conditional entropy in ML models, bringing together existing work on uncertainty quantification and feature attributions. We studied the information theoretic properties of several games, and implemented our approach in model-specific and modelagnostic algorithms with numerous applications. Future work will continue to examine how XAI can go beyond its origins in prediction to inform decision making in areas requiring an explorationexploitation trade-off, such as bandits and reinforcement learning.\nAcknowledgments and Disclosure of Funding\nWe thank Matthew Shorvon for his feedback on an earlier draft of this paper. We are also grateful to the reviewers for their helpful comments."
        },
        {
            "heading": "A Proofs",
            "text": "Proof of Prop. 4.1. Substituting vKL into the definition of \u2206v(S, j,x) gives: \u2206KL(S, j,x) = \u2212DKL(pY |x \u2225 pY |xS ,xj ) +DKL(pY |x \u2225 pY |xS ).\nRearranging and using the definition of KL-divergence, we have:\n\u2206KL(S, j,x) = E Y |x\n[ log p(y | x)\u2212 log p(y | xS) ] \u2212 E\nY |x\n[ log p(y | x)\u2212 log p(y | xS , xj) ] .\nCleaning up in steps:\n\u2206KL(S, j,x) = E Y |x\n[ log p(y | x)\u2212 log p(y | xS)\u2212 log p(y | x) + log p(y | xS , xj) ] = E\nY |x\n[ log p(y | xS , xj)\u2212 log p(y | xS) ] =\n\u222b Y p(y | x) log p(y | xS , xj) p(y | xS) dy.\nSubstituting vCE into the definition of \u2206v(S, j,x) gives:\n\u2206CE (S, j,x) = \u2212H(pY |x, pY |xS ,xj ) +H(pY |x, pY |xS ).\nRearranging and using the definition of cross entropy, we have:\n\u2206CE (S, j,x) = H(pY |x, pY |xS )\u2212H(pY |x, pY |xS\u222a{j})\n= E Y |x\n[ \u2212 log p(y | xS) ] \u2212 E\nY |x\n[ \u2212 log p(y | xS , xj) ] = E\nY |x\n[ log p(y | xS , xj)\u2212 log p(y | xS) ] =\n\u222b Y p(y | x) log p(y | xS , xj) p(y | xS) dy.\nProof of Prop. 4.2. Since the Shapley value \u03d5v(j,x) is just the expectation of \u2206v(S, j,x) under a certain distribution on coalitions S \u2286 [d]\\{j} (see Eq. 1), it follows from Prop. 4.1 that feature attributions will be identical under vKL and vCE . To show that resulting Shapley values sum to the KL-divergence between p(Y | x) and p(Y ), we exploit the efficiency property:\nd\u2211 j=1 \u03d5KL(j,x) = vKL([d],x)\u2212 vKL(\u2205,x)\n= \u2212DKL(pY |x \u2225 pY |x) +DKL(pY |x \u2225 pY ) = DKL(pY |x \u2225 pY ).\nThe last step exploits Gibbs\u2019s inequality, according to which DKL(p \u2225 q) \u2265 0, with DKL(p \u2225 q) = 0 iff p = q.\nProof of Prop. 4.3. Substituting vIG into the definition of \u2206v(S, j,x) gives: \u2206IG(S, j,x) = \u2212H(Y | xS , xj) +H(Y | xS)\n= H(Y | xS)\u2212H(Y | xS , xj) = I(Y ;xj | xS)\n= \u222b Y p(y, xj | xS) log p(y, xj | xS) p(y | xS) p(xj | xS) dy.\nIn the penultimate line, we exploit the equality I(Y ;X) = H(Y )\u2212H(Y | X), by which we define mutual information (see Appx. B.1).\nProof of Prop. 4.4. We once again rely on efficiency and the definition of mutual information in terms of marginal and conditional entropy:\nd\u2211 j=1 \u03d5IG(j,x) = vIG([d],x)\u2212 vIG(\u2205,x)\n= \u2212H(Y | x) +H(Y ) = H(Y )\u2212H(Y | x) = I(Y ;x).\nProof of Prop. 4.5. Let b(S,x) denote the gap between local conditional entropy formulae that take |S|and d-dimensional input, respectively. Then we have:\nb(S,x) = vH(S,x)\u2212 vH\u2217(S,x) = H(Y | xS)\u2212 E\nX S |xS\n[ H(Y | x) | XS = xS ] = \u2212 E\nY |xS\n[ log p(y | xS) ] + E\nX S ,Y |xS\n[ log p(y | xS ,xS) ] = E\nX S ,Y |xS\n[ log p(y | xS ,xS)\u2212 log p(y | xS) ] =\n\u222b X\nS\n\u222b Y p(xS , y | xS) log p(y | xS ,xS) p(y | xS) dxS dy\n= DKL(p(Y | XS ,xS) \u2225 p(Y | xS)).\nProof of Prop. 4.6. Exploiting the efficiency property, we immediately have: d\u2211\nj=1\n\u03d5H\u2217(j,x) = vH\u2217([d],x)\u2212 vH\u2217(\u2205,x)\n= H(Y | x)\u2212 E DX\n[ H(Y | x) ] = H(Y | x)\u2212H(Y | X).\nBy contrast, Shapley values for the original entropy game vH sum to H(Y | x)\u2212H(Y ) = \u2212I(Y ;x). Thus whereas the baseline for an empty coalition S = \u2205 is the prior entropy H(Y ) under vH , the corresponding baseline for the d-dimensional version vH\u2217 is the global posterior entropy H(Y | X).\nProof of Thm. 4.7. Begin with item (a). Note that the conditional independence statement Y \u22a5\u22a5Xj | XS holds iff, for all points (x, y) \u223c D, we have:\np(y | xS , xj) = p(y | xS) and p(y, xj | xS) = p(y | xS) p(xj | xS).\nThe former guarantees that marginal payouts evaluate to zero for v \u2208 {vKL, vCE}; the latter does the same for v \u2208 {vIG, vH}. This follows because the log ratio in each formula evaluates to zero when numerator and denominator are equal.\nOf course, conditional independence is also sufficient for zero marginal payout with more familiar value functions such as v0. But item (a) makes an additional claim\u2014that the converse holds as well, i.e. that conditional independence is necessary for zero marginal payout across all x. This follows from the definitions of the value functions themselves. Observe:\nE x\u223cDX\n[ \u2206KL(S, j,x) ] = E\n(x,y)\u223cD\n[ log\np(y | xS , xj) p(y | xS)\n]\n= E DX\n[ E\nY |xS ,xj\n[ log\np(y | xS , xj) p(y | xS) ]] = E\nDX\n[ DKL(pY |xS ,xj \u2225 pY |xS ) ] By Gibbs\u2019s inequality, the KL-divergence between two distributions is zero iff they are equal, so setting this value to zero for all x satisfies the first definition of conditional independence above. For the latter, we simply point out that:\nE x\u223cDX\n[ \u2206IG(S, j,x) ] = I(Y ;Xj | XS).\nSince conditional mutual information equals zero iff the relevant variables are conditionally independent, this satisfies the second definition above.\nItem (b) states that CSI, which is strictly weaker than standard conditional independence, is also sufficient for zero marginal payout at a given point x. This follows directly from the sufficiency argument above.\nThe converse relationship is more complex, however. Call a distribution conspiratorial if there exists some S, j,x such that \u2206v (S, j,x) = 0 \u2227 Y \u22a5\u0338\u22a5Xj | xS for some v \u2208 {vKL, vCE , vIG, vH}. Such distributions are so named because the relevant probabilities must coordinate in a very specific way to guarantee summation to zero as we marginalize over Y . As a concrete example, consider the following data generating process:\nX \u223c Bern(0.5), Z \u223c Bern(0.5), Y \u223c Bern(0.3 + 0.4X \u2212 0.2Z).\nWhat is the contribution of X to coalition S = \u2205 when X = 1 and Z = 1? In this case, we have neither global nor context-specific independence, i.e. Y \u22a5\u0338\u22a5X . Yet, evaluating the payoffs in a KL-divergence game, we have:\n\u2206KL(S, j,x) = \u2211 y P (y | X = 1, Z = 1) log P (y | X = 1) P (y)\n= 0.5 log 0.4\n0.6 + 0.5 log\n0.6\n0.4 = 0.\nIn this case, we find that negative and positive values of the log ratio cancel out exactly as we marginalize over Y . (Similar examples can be constructed for all our information theoretic games.) This shows that CSI is sufficient but not necessary for \u2206v(S, j,x) = 0.\nHowever, just because conspiratorial distributions are possible does not mean that they are common. Item (c) states that the set of all such distributions has Lebesgue measure zero. Our proof strategy here follows that of Meek [48], who demonstrates a similar result in the case of unfaithful distributions, i.e. those whose (conditional) independencies are not entailed by the data\u2019s underlying graphical structure. This is an important topic in the causal discovery literature (see, e.g., [91, 92]).\nFor simplicity, assume a discrete state space X \u00d7 Y , such that the data generating process is fully parametrized by a table of probabilities T for each fully specified event. Fix some S, j such that Y \u22a5\u0338\u22a5Xj | xS. Let C be the number of possible outcomes, Y = {y1, . . . , yC}. Define vectors p,q, r of length C such that, for each c \u2208 [C]:\npc = p(yc | x), qc = p(yc | xS , xj)\u2212 p(yc | xS), rc = log p(yc | xS , xj) p(yc | xS) ,\nand stipulate that p(yc | xS) > 0 for all c \u2208 [C] to avoid division by zero. Observe that these variables are all deterministic functions of the parameters encoded in the probability table T . By the assumption of local conditional dependence, we know that \u2225r\u22250 > 0. Yet for our conspiracy to obtain, the data must satisfy p \u00b7 r = 0. A well-known algebraic lemma of Okamoto [52] states that if a polynomial constraint is non-trivial (i.e., if there exists some settings for which it does not hold), then the subset of parameters for which it does hold has Lebesgue measure zero. The log ratio r is not polynomial in T , but the difference q is. The latter also has a strictly positive L0 norm by the assumption of local conditional independence. Crucially, the resulting dot product intersects with our previous dot product at zero. That is, though differences are in general a non-injective surjective function of log ratios, we have a bijection at the origin whereby p \u00b7 r = 0 \u21d4 p \u00b7 q = 0. Thus the conspiracy requires nontrivial constraints that are linear in p,q\u2014themselves polynomial in the system parameters T\u2014so we conclude that the set of conspiratorial distributions has Lebesgue measure zero.\nProof of Thm. 5.1. Our proof is an application of the split conformal method (see [41, Thm. 2.2]). Whereas that method was designed to bound the distance between predicted and observed outcomes for a regression task, we adapt the argument to measure the concentration of Shapley values for a given feature. To achieve this, we replace out-of-sample absolute residuals with out-of-sample Shapley values and drop the symmetry assumption, which will not generally apply when features are informative. The result follows immediately from the exchangeability of \u03d5(j,x(i+1)) and \u03d5(j,x(i)), i \u2208 I2, which is itself a direct implication of the i.i.d. assumption. Since bounds are calculated so as to cover (1\u2212 \u03b1)\u00d7 100% of the distribution, it is unlikely that new samples will fall outside this region. Specifically, such exceptions occur with probability at most \u03b1. This amounts to a sort of PAC guarantee, i.e. that Shapley values will be within a data-dependent interval with probability at least 1\u2212 \u03b1. The interval can be trivially inverted to compute an associated p-value.\nWe identify three potential sources of error in estimating upper and lower quantiles of the Shapley distribution: (1) learning the conditional entropy model h from finite data; (2) sampling values for out-of-coalition features S; (3) sampling coalitions S. Convergence rates as a function of (1) and (2) are entirely dependent on the selected subroutines. With consistent methods for both, conformal prediction bands are provably close to the oracle band (see [41, Thm. 8]). As for (3), Williamson and Feng [88] show that with efficient estimators for (1) and (2), as well as an extra condition on the minimum number of subsets, sampling \u0398(n) coalitions is asymptotically optimal, up to a constant factor."
        },
        {
            "heading": "B Addenda",
            "text": "This section includes extra background material on information theory and Shapley values.\nB.1 Information Theory\nLet p, q be two probability distributions over the same \u03c3-algebra of events. Further, let p, q be absolutely continuous with respect to some appropriate measure. The entropy of p is defined as H(p) := Ep[\u2212 log p],\ni.e. the expected number of bits required to encode the distribution.4 The cross entropy of p and q is defined as H(p, q) := Ep[\u2212 log q], i.e. the expected number of bits required to encode samples from p using code optimized for q. The KL-divergence between p and q is defined as DKL(p \u2225 q) := Ep[log p/q], i.e. the cost in bits of modeling p with q. These three quantities are related by the formula DKL(p \u2225 q) = H(p, q)\u2212H(p). The reduction in Y \u2019s uncertainty attributable to X is also called the mutual information, I(Y ;X) := H(Y )\u2212H(Y | X). This quantity is nonnegative, with I(Y ;X) = 0 if and only if the variables are independent.\nHowever, conditioning on a specific value of X may increase uncertainty in Y , in which case the local posterior entropy exceeds the prior. Thus it is possible that H(Y | x) > H(Y ) for some x \u2208 X . For example, consider the following data generating process:\nX \u223c Bern(0.8), Y \u223c Bern(0.5 + 0.25X).\nIn this case, we have P (Y = 1) = 0.7, P (Y = 1 | X = 0) = 0.5, and P (Y = 1 | X = 1) = 0.75. It is easy to see that even though the marginal entropy H(Y ) exceeds the global conditional entropy H(Y | X), the local entropy at X = 0 is larger than either quantity, H(Y | X = 0) > H(Y ) > H(Y | X). In other words, conditioning on the event X = 0 increases our uncertainty about Y .\nSimilarly, there may be cases in which I(Y ;X | Z) > 0, but I(Y ;X | z) = 0. This is what Boutilier et al. [7] call context-specific independence (CSI). For instance, if X,Z \u2208 {0, 1}2 and Y := max{X,Z}, then we have Y \u22a5\u0338\u22a5X | Z, but Y \u22a5\u22a5X | (Z = 1) since Y \u2019s value is determined as soon as we know that either parent is 1.\nB.2 The Shapley Axioms\nFor completeness, we here list the Shapley axioms.\nEfficiency. Shapley values sum to the difference in payoff between complete and null coalitions: d\u2211\nj=1\n\u03d5(j,x) = v([d],x)\u2212 v(\u2205,x).\nSymmetry. If two players make identical contributions to all coalitions, then their Shapley values are equal:\n\u2200S \u2286 [d]\\{i, j} : v(S \u222a {i},x) = v(S \u222a {j},x) \u21d2 \u03d5(i,x) = \u03d5(j,x).\nSensitivity. If a player makes zero contribution to all coalitions, then its Shapley value is zero:\n\u2200S \u2286 [d]\\{j} : v(S \u222a {j},x) = v(S,x) \u21d2 \u03d5(j,x) = 0.\nLinearity. The Shapley value for a convex combination of games can be decomposed into a convex combination of Shapley values. For any a, b \u2208 R and value functions v1, v2, we have:\n\u03d5a\u00b7v1+b\u00b7v2(j,x) = a\u03d5v1(j,x) + b\u03d5v2(j,x)."
        },
        {
            "heading": "C Experiments",
            "text": "C.1 Datasets.\nThe MNIST dataset is available online.5 The IMDB dataset is available on Kaggle.6 For the tabular data experiment in Sect. 6.1, we generate Y according to the following process:\n\u00b5(x) := \u03b2\u22a4x, \u03c32(x) := exp(\u03b3\u22a4x), Y | x \u223c N ( \u00b5(x), \u03c32(x) ) .\nCoefficients \u03b2,\u03b3 are independent Rademacher distributed random vectors of length 4.\nThe BreastCancer, Diabetes, Ionosphere, and Sonar datasets are all distributed in the mlbench package, which is available on CRAN.7\n4Though the term \u201cbit\u201d is technically reserved for units of information measured with logarithmic base 2, we use the word somewhat more loosely to refer to any unit of information.\n5http://yann.lecun.com/exdb/mnist/. 6https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews. 7https://cran.r-project.org/web/packages/mlbench/index.html.\nC.2 Models.\nAll neural network training was conducted in PyTorch [59]. For the MNIST experiment, we train a deep neural network with the following model architecture: (1) A convolutional layer with 10 filters of size 5\u00d7 5, followed by max pooling of size 2\u00d7 2, ReLU activation, and a dropout layer with probability 0.3. (2) A convolutional layer with 20 filters of size 5\u00d7 5, followed by a dropout layer, max pooling of size 2\u00d7 2, ReLU activation, and a dropout layer with probability 0.3. (3) Fully connected (dense) layer with 320 input features and 50 output units, followed by ReLU activation and a dropout layer. (4) Fully connected layer with 50 input features and 10 output units, followed by softmax activation. We train with a batch size of 128 for 20 epochs at a learning rate of 0.01 and momentum 0.5. For Monte Carlo dropout, we do 50 forward passes to sample B = 50 subnetworks. For the IMDB experiment, we use a pre-trained BERT model from the Hugging Face transformers library.8 All hyperparameters are set to their default values. All XGBoost models are trained with the default hyperparameters, with the number of training rounds cited in the text.\nC.3 Coverage\nTo empirically test our conformal coverage guarantee, we compute quantiles for out-of-sample Shapley values on the modified Friedman benchmark. Results for conditional expectation and conditional variance are reported in Table 1, with target level \u03b1 = 0.1. Note that what constitutes a \u201csmall\u201d or \u201clarge\u201d interval is context-dependent. The conditional variance model is fit to \u03f52y , which has a tighter range than Z, leading to smaller Shapley values on average. However, nominal coverage is very close to the target 90% throughout, illustrating how the conformal method can be used for feature selection and outlier detection.\n8https://huggingface.co/docs/transformers/model_doc/bert.\nC.4 Extended MNIST\nWe supplement the original MNIST experiment with extra examples, including a mean column for the true label (Fig. 5A) and augmenting the binary problem with the classic 10-class task (Fig. 5B). We find that mean and entropy are highly associated in the binary setting, as the analytic formulae suggest, while the relationship is somewhat more opaque in the 10-class setting, since we must average over a larger set of signals."
        }
    ],
    "title": "Explaining Predictive Uncertainty with Information Theoretic Shapley Values",
    "year": 2023
}