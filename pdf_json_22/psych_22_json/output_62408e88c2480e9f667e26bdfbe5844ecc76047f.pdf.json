{
    "abstractText": "Personality determines a wide variety of human daily and working behaviours, and is crucial for understanding human internal and external states. In recent years, a large number of automatic personality computing approaches have been developed to predict either the apparent personality or self-reported personality of the subject based on non-verbal audio-visual behaviours. However, the majority of them suffer from complex and dataset-specific pre-processing steps and model training tricks. In the absence of a standardized benchmark with consistent experimental settings, it is not only impossible to fairly compare the real performances of these personality computing models but also makes them difficult to be reproduced. In this paper, we present the first reproducible audio-visual benchmarking framework to provide a fair and consistent evaluation of eight existing personality computing models (e.g., audio, visual and audio-visual) and seven standard deep learning models on both self-reported and apparent personality recognition tasks. Building upon a set of benchmarked models, we also investigate the impact of two previously-used long-term modelling strategies for summarising short-term/frame-level predictions on personality computing results. We conduct a comprehensive investigation into all the benchmarked models to demonstrate their capabilities in modelling personality traits on two publicly available datasets, audio-visual apparent personality (ChaLearn First Impression) and self-reported personality (UDIVA) datasets. The experimental results conclude: (i) apparent personality traits, inferred from facial behaviours by most benchmarked deep learning models, show more reliability than self-reported ones; (ii) visual models frequently achieved superior performances than audio models on personality recognition; (iii) non-verbal behaviours contribute differently in predicting different personality traits; and (iv) our reproduced personality computing models generally achieved worse performances than their original reported results. We make all the code and settings of this personality computing benchmark publicly available at https://github.com/liaorongfan/DeepPersonality.",
    "authors": [
        {
            "affiliations": [],
            "name": "Rongfan Liao"
        },
        {
            "affiliations": [],
            "name": "Siyang Song"
        },
        {
            "affiliations": [],
            "name": "Hatice Gunes"
        }
    ],
    "id": "SP:73171e4349e65b0804400c369c39e6d2845b1da8",
    "references": [
        {
            "authors": [
                "P.J. Corr",
                "G. Matthews"
            ],
            "title": "The Cambridge handbook of personality psychology",
            "venue": "IEEE TRANSACTIONS ON ,",
            "year": 2020
        },
        {
            "authors": [
                "S. Youn",
                "R.J. Faber"
            ],
            "title": "Impulse buying: its relation to personality traits and cues",
            "venue": "ACR North American Advances, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "A. Sano",
                "A.J. Phillips",
                "Z.Y. Amy",
                "A.W. McHill",
                "S. Taylor",
                "N. Jaques",
                "C.A. Czeisler",
                "E.B. Klerman",
                "R.W. Picard"
            ],
            "title": "Recognizing academic performance, sleep quality, stress level, and mental health using personality traits, wearable sensors and mobile phones",
            "venue": "2015 IEEE 12th International Conference on Wearable and Implantable Body Sensor Networks (BSN). IEEE, 2015, pp. 1\u20136.",
            "year": 2015
        },
        {
            "authors": [
                "S. Jaiswal",
                "S. Song",
                "M. Valstar"
            ],
            "title": "Automatic prediction of depression and anxiety from behaviour and personality attributes",
            "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII). IEEE, 2019, pp. 1\u20137.",
            "year": 2019
        },
        {
            "authors": [
                "S.J.B. Asendorpf"
            ],
            "title": "Wilpers, \u201cPersonality effects on social relationships.",
            "venue": "Journal of personality and social psychology,",
            "year": 1998
        },
        {
            "authors": [
                "E.M.D.A. Briley"
            ],
            "title": "Tucker-Drob, \u201cGenetic and environmental continuity in personality development: a meta-analysis.",
            "venue": "Psychological bulletin,",
            "year": 2014
        },
        {
            "authors": [
                "S. Afzal",
                "B. Dempsey",
                "C. D\u2019Helon",
                "N. Mukhi",
                "M. Pribic",
                "A. Sickler",
                "P. Strong",
                "M. Vanchiswar",
                "L. Wilde"
            ],
            "title": "The personality of ai systems in education: experiences with the watson tutor, a oneon-one virtual tutoring system",
            "venue": "Childhood Education, vol. 95, no. 1, pp. 44\u201352, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "D. Mansour",
                "A.B. Bhardwaj",
                "A. Chopra"
            ],
            "title": "Relating ocean (big five) to job satisfaction in aviation",
            "venue": "2021 International Conference on Computational Intelligence and Knowledge Economy (ICCIKE). IEEE, 2021, pp. 285\u2013289.",
            "year": 2021
        },
        {
            "authors": [
                "H. Kaya",
                "F. Gurpinar",
                "A. Ali Salah"
            ],
            "title": "Multi-modal score fusion and decision trees for explainable automatic job candidate screening from video cvs",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops, 2017, pp. 1\u20139.",
            "year": 2017
        },
        {
            "authors": [
                "F. Xia",
                "N.Y. Asabere",
                "H. Liu",
                "Z. Chen",
                "W. Wang"
            ],
            "title": "Socially aware conference participant recommendation with personality traits",
            "venue": "IEEE Systems Journal, vol. 11, no. 4, pp. 2255\u20132266, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "H. Eysenck"
            ],
            "title": "Dimensions of personality: 16, 5 or 3?\u2014criteria for a taxonomic paradigm",
            "venue": "Personality and Individual Differences, vol. 12, no. 8, pp. 773\u2013790, 1991. [Online]. Available: https:// www.sciencedirect.com/science/article/pii/019188699190144Z",
            "year": 1991
        },
        {
            "authors": [
                "J.T. Mitchell",
                "N.A. Kimbrel",
                "N.E. Hundt",
                "A.R. Cobb",
                "R.O. Nelson-Gray",
                "C.M. Lootens"
            ],
            "title": "An analysis of reinforcement sensitivity theory and the five-factor model",
            "venue": "European Journal of Personality: Published for the European Association of Personality Psychology, vol. 21, no. 7, pp. 869\u2013887, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "F. De Fruyt",
                "L. Van de Wiele",
                "C. Van Heeringen"
            ],
            "title": "Cloninger\u2019s psychobiological model of temperament and character and the five-factor model of personality",
            "venue": "Personality and individual differences, vol. 29, no. 3, pp. 441\u2013452, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "R.R. McCrae",
                "P.T. Costa Jr."
            ],
            "title": "The five-factor theory of personality.",
            "year": 2008
        },
        {
            "authors": [
                "V. Ponce-L\u00f3pez",
                "B. Chen",
                "M. Oliu",
                "C. Corneanu",
                "A. Clap\u00e9s",
                "I. Guyon",
                "X. Bar\u00f3",
                "H.J. Escalante",
                "S. Escalera"
            ],
            "title": "Chalearn lap 2016: First round challenge on first impressions-dataset and results",
            "venue": "European conference on computer vision. Springer, 2016, pp. 400\u2013418.",
            "year": 2016
        },
        {
            "authors": [
                "C. Ventura",
                "D. Masip",
                "A. Lapedriza"
            ],
            "title": "Interpreting cnn models for apparent personality trait regression",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops, 2017, pp. 55\u201363.",
            "year": 2017
        },
        {
            "authors": [
                "S. Song",
                "Z. Shao",
                "S. Jaiswal",
                "L. Shen",
                "M. Valstar",
                "H. Gunes"
            ],
            "title": "Learning graph representation of person-specific cognitive processes from audio-visual behaviours for automatic personality recognition",
            "venue": "arXiv preprint arXiv:2110.13570, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M.A. Moreno-Armend\u00e1riz",
                "C.A.D. Mart\u0131\u0301nez",
                "H. Calvo",
                "M. Moreno-Sotelo"
            ],
            "title": "Estimation of personality traits from portrait pictures using the five-factor model",
            "venue": "IEEE Access, vol. 8, pp. 201 649\u2013201 665, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "D. Helm",
                "M. Kampel"
            ],
            "title": "Single-modal video analysis of personality traits using low-level visual features",
            "venue": "2020 Tenth International Conference on Image Processing Theory, Tools and Applications (IPTA). IEEE, 2020, pp. 1\u20136.",
            "year": 2020
        },
        {
            "authors": [
                "S. Eddine Bekhouche",
                "F. Dornaika",
                "A. Ouafi",
                "A. Taleb-Ahmed"
            ],
            "title": "Personality traits and job candidate screening via analyzing facial videos",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops, 2017, pp. 10\u201313.",
            "year": 2017
        },
        {
            "authors": [
                "M.K. Tellamekala",
                "T. Giesbrecht",
                "M. Valstar"
            ],
            "title": "Dimensional affect uncertainty modelling for apparent personality recognition",
            "venue": "IEEE Transactions on Affective Computing, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "F. G\u00fcrp\u0131nar",
                "H. Kaya",
                "A.A. Salah"
            ],
            "title": "Combining deep facial and ambient features for first impression estimation",
            "venue": "European conference on computer vision. Springer, 2016, pp. 372\u2013385.",
            "year": 2016
        },
        {
            "authors": [
                "L. Zhang",
                "S. Peng",
                "S. Winkler"
            ],
            "title": "Persemon: a deep network for joint analysis of apparent personality, emotion and their relationship",
            "venue": "IEEE Transactions on Affective Computing, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "C. Beyan",
                "A. Zunino",
                "M. Shahid",
                "V. Murino"
            ],
            "title": "Personality traits classification using deep visual activity-based nonverbal features of key-dynamic images",
            "venue": "IEEE Transactions on Affective Computing, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "C.-L. Zhang",
                "H. Zhang",
                "X.-S. Wei",
                "J. Wu"
            ],
            "title": "Deep bimodal regression for apparent personality analysis",
            "venue": "European conference on computer vision. Springer, 2016, pp. 311\u2013324.",
            "year": 2016
        },
        {
            "authors": [
                "A. Subramaniam",
                "V. Patel",
                "A. Mishra",
                "P. Balasubramanian",
                "A. Mittal"
            ],
            "title": "Bi-modal first impressions recognition using temporally ordered deep audio and stochastic visual features",
            "venue": "European conference on computer vision. Springer, 2016, pp. 337\u2013 348.",
            "year": 2016
        },
        {
            "authors": [
                "F. G\u00fcrpinar",
                "H. Kaya",
                "A.A. Salah"
            ],
            "title": "Multimodal fusion of audio, scene, and face features for first impression estimation",
            "venue": "2016 23rd International conference on pattern recognition (ICPR). IEEE, 2016, pp. 43\u201348.",
            "year": 2016
        },
        {
            "authors": [
                "Y. Li",
                "J. Wan",
                "Q. Miao",
                "S. Escalera",
                "H. Fang",
                "H. Chen",
                "X. Qi",
                "G. Guo"
            ],
            "title": "Cr-net: A deep classification-regression network for multimodal apparent personality analysis.",
            "venue": "International Journal of Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "S. Aslan",
                "U. G\u00fcd\u00fckbay"
            ],
            "title": "Multimodal video-based apparent personality recognition using long short-term memory and convolutional neural networks",
            "venue": "arXiv preprint arXiv:1911.00381, 2019.",
            "year": 1911
        },
        {
            "authors": [
                "X.-S. Wei",
                "C.-L. Zhang",
                "H. Zhang",
                "J. Wu"
            ],
            "title": "Deep bimodal regression of apparent personality traits from short video sequences",
            "venue": "IEEE Transactions on Affective Computing, vol. 9, no. 3, pp. 303\u2013315, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Y. G\u00fc\u00e7l\u00fct\u00fcrk",
                "U. G\u00fc\u00e7l\u00fc",
                "M.A. van Gerven",
                "R. van Lier"
            ],
            "title": "Deep impression: Audiovisual deep residual networks for multimodal apparent personality trait recognition",
            "venue": "European conference on computer vision. Springer, 2016, pp. 349\u2013358.",
            "year": 2016
        },
        {
            "authors": [
                "S. Song",
                "S. Jaiswal",
                "L. Shen",
                "M. Valstar"
            ],
            "title": "Spectral representation of behaviour primitives for depression analysis",
            "venue": "IEEE Transactions on Affective Computing, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Song",
                "S. Jaiswal",
                "E. Sanchez",
                "G. Tzimiropoulos",
                "L. Shen",
                "M. Valstar"
            ],
            "title": "Self-supervised learning of person-specific facial dynamics for automatic personality recognition",
            "venue": "IEEE Transactions on Affective Computing, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "D. Curto",
                "A. Clap\u00e9s",
                "J. Selva",
                "S. Smeureanu",
                "J. Junior",
                "C. Jacques",
                "D. Gallardo-Pujol",
                "G. Guilera",
                "D. Leiva",
                "T.B. Moeslund"
            ],
            "title": "Dyadformer: A multi-modal transformer for long-range modeling of dyadic interactions",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 2177\u20132188.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Shao",
                "S. Song",
                "S. Jaiswal",
                "L. Shen",
                "M. Valstar",
                "H. Gunes"
            ],
            "title": "Personality recognition by modelling person-specific cognitive processes using graph representation",
            "venue": "Proceedings of the 29th ACM International Conference on Multimedia, 2021, pp. 357\u2013366.",
            "year": 2021
        },
        {
            "authors": [
                "H. Hayat",
                "C. Ventura",
                "A. Lapedriza"
            ],
            "title": "On the use of interpretable cnn for personality trait recognition from audio.",
            "venue": "CCIA,",
            "year": 2019
        },
        {
            "authors": [
                "Y. G\u00fc\u00e7l\u00fct\u00fcrk",
                "U. G\u00fc\u00e7l\u00fc",
                "M.A.J. van Gerven",
                "R. van Lier"
            ],
            "title": "Deep impression: Audiovisual deep residual networks for multimodal apparent personality trait recognition",
            "venue": "Computer Vision \u2013 ECCV 2016 Workshops, p. 349\u2013358, 2016. [Online]. Available: http://dx.doi.org/10.1007/978-3-319-49409-8 28",
            "year": 2016
        },
        {
            "authors": [
                "C. Palmero",
                "J. Selva",
                "S. Smeureanu",
                "J.C.J. Junior",
                "A. Clap\u00e9s",
                "A. Mosegu\u0131",
                "Z. Zhang",
                "D. Gallardo",
                "G. Guilera",
                "D. Leiva"
            ],
            "title": "Context-aware personality inference in dyadic scenarios: Introducing the udiva dataset.",
            "venue": "WACV (Workshops),",
            "year": 2021
        },
        {
            "authors": [
                "J.C.S.J. Junior",
                "Y. G\u00fc\u00e7l\u00fct\u00fcrk",
                "M. P\u00e9rez",
                "U. G\u00fc\u00e7l\u00fc",
                "C. Andujar",
                "X. Bar\u00f3",
                "H.J. Escalante",
                "I. Guyon",
                "M.A. Van Gerven",
                "R. Van Lier"
            ],
            "title": "First impressions: A survey on vision-based apparent personality trait analysis",
            "venue": "IEEE Transactions on Affective Computing, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Joo",
                "F.F. Steen",
                "S.-C. Zhu"
            ],
            "title": "Automated facial trait judgment and election outcome prediction: Social dimensions of face",
            "venue": "Proceedings of the IEEE international conference on computer vision, 2015, pp. 3712\u20133720. IEEE TRANSACTIONS ON , VOL. 14, NO. 8, AUGUST 2015 16",
            "year": 2015
        },
        {
            "authors": [
                "A. Dhall",
                "J. Hoey"
            ],
            "title": "First impressions-predicting user personality from twitter profile images",
            "venue": "International Workshop on Human Behavior Understanding. Springer, 2016, pp. 148\u2013158.",
            "year": 2016
        },
        {
            "authors": [
                "B. Zhou",
                "A. Khosla",
                "A. Lapedriza",
                "A. Oliva",
                "A. Torralba"
            ],
            "title": "Learning deep features for discriminative localization",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 2921\u20132929.",
            "year": 2016
        },
        {
            "authors": [
                "O. Celiktutan",
                "H. Gunes"
            ],
            "title": "Continuous prediction of perceived traits and social dimensions in space and time",
            "venue": "Image Processing (ICIP), 2014 IEEE International Conference on. IEEE, 2014, pp. 4196\u2013 4200.",
            "year": 2014
        },
        {
            "authors": [
                "O. Celiktutan",
                "F. Eyben",
                "E. Sariyanidi",
                "H. Gunes",
                "B. Schuller"
            ],
            "title": "Maptraits 2014-the first audio/visual mapping personality traits challenge-an introduction: Perceived personality and social dimensions",
            "venue": "Proceedings of the 16th International Conference on Multimodal Interaction. ACM, 2014, pp. 529\u2013530.",
            "year": 2014
        },
        {
            "authors": [
                "F. Eyben",
                "M. W\u00f6llmer",
                "B. Schuller"
            ],
            "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
            "venue": "Proceedings of the 18th ACM international conference on Multimedia, 2010, pp. 1459\u20131462.",
            "year": 2010
        },
        {
            "authors": [
                "R. Qin",
                "W. Gao",
                "H. Xu",
                "Z. Hu"
            ],
            "title": "Modern physiognomy: an investigation on predicting personality traits and intelligence from the human face",
            "venue": "Science China Information Sciences, vol. 61, no. 5, p. 058105, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "A. Cafaro",
                "J. Wagner",
                "T. Baur",
                "S. Dermouche",
                "M. Torres Torres",
                "C. Pelachaud",
                "E. Andr\u00e9",
                "M. Valstar"
            ],
            "title": "The noxi database: multimodal recordings of mediated novice-expert interactions",
            "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction, 2017, pp. 350\u2013359.",
            "year": 2017
        },
        {
            "authors": [
                "O. Celiktutan",
                "E. Skordos",
                "H. Gunes"
            ],
            "title": "Multimodal humanhuman-robot interactions (mhhri) dataset for studying personality and engagement",
            "venue": "IEEE Transactions on Affective Computing, vol. 10, no. 4, pp. 484\u2013497, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "H. Salam",
                "V. Manoranjan",
                "J. Jiang",
                "O. Celiktutan"
            ],
            "title": "Learning personalised models for automatic self-reported personality recognition",
            "venue": "Understanding Social Behavior in Dyadic and Small Group Interactions. PMLR, 2022, pp. 1\u201321.",
            "year": 2022
        },
        {
            "authors": [
                "A. Paszke",
                "S. Gross",
                "F. Massa",
                "A. Lerer",
                "J. Bradbury",
                "G. Chanan",
                "T. Killeen",
                "Z. Lin",
                "N. Gimelshein",
                "L. Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "Advances in neural information processing systems, vol. 32, pp. 8026\u20138037, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "K. Zhang",
                "Z. Zhang",
                "Z. Li",
                "Y. Qiao"
            ],
            "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
            "venue": "IEEE Signal Processing Letters, vol. 23, no. 10, pp. 1499\u20131503, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "N. Boyko",
                "O. Basystiuk",
                "N. Shakhovska"
            ],
            "title": "Performance evaluation and comparison of software for face recognition, based on dlib and opencv library",
            "venue": "2018 IEEE Second International Conference on Data Stream Mining & Processing (DSMP). IEEE, 2018, pp. 478\u2013482.",
            "year": 2018
        },
        {
            "authors": [
                "C. Suman",
                "S. Saha",
                "A. Gupta",
                "S.K. Pandey",
                "P. Bhattacharyya"
            ],
            "title": "A multi-modal personality prediction system",
            "venue": "Knowledge-Based Systems, vol. 236, p. 107715, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H.-Y. Suen",
                "K.-E. Hung",
                "C.-L. Lin"
            ],
            "title": "Tensorflow-based automatic personality recognition used in asynchronous video interviews",
            "venue": "IEEE Access, vol. 7, pp. 61 018\u201361 023, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Ruder"
            ],
            "title": "An overview of gradient descent optimization algorithms",
            "venue": "arXiv preprint arXiv:1609.04747, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "H. Hayat",
                "C. Ventura",
                "\u00c0. Lapedriza"
            ],
            "title": "On the use of interpretable cnn for personality trait recognition from audio",
            "venue": "CCIA, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "T. Giannakopoulos"
            ],
            "title": "pyaudioanalysis: An open-source python library for audio signal analysis",
            "venue": "PloS one, vol. 10, no. 12, p. e0144610, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "B. McFee",
                "C. Raffel",
                "D. Liang",
                "D.P. Ellis",
                "M. McVicar",
                "E. Battenberg",
                "O. Nieto"
            ],
            "title": "librosa: Audio and music signal analysis in python",
            "venue": "Proceedings of the 14th python in science conference, vol. 8. Citeseer, 2015, pp. 18\u201325.",
            "year": 2015
        },
        {
            "authors": [
                "J. Hu",
                "L. Shen",
                "S. Albanie",
                "G. Sun",
                "E. Wu"
            ],
            "title": "Squeeze-andexcitation networks",
            "venue": "2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Wang",
                "K. Sun",
                "T. Cheng",
                "B. Jiang",
                "C. Deng",
                "Y. Zhao",
                "D. Liu",
                "Y. Mu",
                "M. Tan",
                "X. Wang",
                "W. Liu",
                "B. Xiao"
            ],
            "title": "Deep high-resolution representation learning for visual recognition",
            "venue": "2020.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Liu",
                "Y. Lin",
                "Y. Cao",
                "H. Hu",
                "Y. Wei",
                "Z. Zhang",
                "S. Lin",
                "B. Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "2021.",
            "year": 2021
        },
        {
            "authors": [
                "K. Hara",
                "H. Kataoka",
                "Y. Satoh"
            ],
            "title": "Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2018
        },
        {
            "authors": [
                "C. Feichtenhofer",
                "H. Fan",
                "J. Malik",
                "K. He"
            ],
            "title": "Slowfast networks for video recognition",
            "venue": "2019.",
            "year": 2019
        },
        {
            "authors": [
                "C. Yang",
                "Y. Xu",
                "J. Shi",
                "B. Dai",
                "B. Zhou"
            ],
            "title": "Temporal pyramid network for action recognition",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.",
            "year": 2020
        },
        {
            "authors": [
                "R. Girdhar",
                "J. Carreira",
                "C. Doersch",
                "A. Zisserman"
            ],
            "title": "Video action transformer network",
            "venue": "2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Song",
                "L. Shen",
                "M. Valstar"
            ],
            "title": "Human behaviour-based automatic depression analysis using hand-crafted statistics and deep learned spectral features",
            "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018). IEEE, 2018, pp. 158\u2013165.",
            "year": 2018
        },
        {
            "authors": [
                "J.-I. Biel",
                "D. Gatica-Perez"
            ],
            "title": "Voices of vlogging",
            "venue": "Fourth International AAAI Conference on Weblogs and Social Media, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "D. Sanchez-Cortes",
                "O. Aran",
                "M.S. Mast",
                "D. Gatica-Perez"
            ],
            "title": "A nonverbal behavior approach to identify emergent leaders in small groups",
            "venue": "IEEE Transactions on Multimedia, vol. 14, no. 3, pp. 816\u2013 832, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "J.A. Miranda-Correa",
                "M.K. Abadi",
                "N. Sebe",
                "I. Patras"
            ],
            "title": "Amigos: A dataset for affect, personality and mood research on individuals and groups",
            "venue": "IEEE Transactions on Affective Computing, vol. 12, no. 2, pp. 479\u2013493, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J.A.M. Correa",
                "M.K. Abadi",
                "N. Sebe",
                "I. Patras"
            ],
            "title": "Amigos: A dataset for affect, personality and mood research on individuals and groups",
            "venue": "IEEE Transactions on Affective Computing, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M.K. Rothbart",
                "S.A. Ahadi",
                "K.L. Hershey",
                "P. Fisher"
            ],
            "title": "Investigations of temperament at three to seven years: The children\u2019s behavior questionnaire",
            "venue": "Child development, vol. 72, no. 5, pp. 1394\u2013 1408, 2001.",
            "year": 2001
        },
        {
            "authors": [
                "L.K. Ellis",
                "M.K. Rothbart"
            ],
            "title": "Revision of the early adolescent temperament questionnaire",
            "venue": "Poster presented at the 2001 biennial meeting of the society for research in child development, Minneapolis, Minnesota. Citeseer, 2001.",
            "year": 2001
        },
        {
            "authors": [
                "C.J. Soto",
                "O.P. John"
            ],
            "title": "The next big five inventory (bfi-2): Developing and assessing a hierarchical model with 15 facets to enhance bandwidth, fidelity, and predictive power.",
            "venue": "Journal of personality and social psychology,",
            "year": 2017
        },
        {
            "authors": [
                "M.C. Ashton",
                "K. Lee"
            ],
            "title": "The hexaco\u201360: A short measure of the major dimensions of personality",
            "venue": "Journal of personality assessment, vol. 91, no. 4, pp. 340\u2013345, 2009.",
            "year": 2009
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Self-reported (true) personality recognition, Apparent personality (impression) recognition, Audio-visual personality computing benchmark, Spatio-temporal modelling, Deep Learning\nF"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "P ERSONALITY defines a characteristic sets of human cog-nitive processes and emotional patterns that evolve from various biological and environmental factors [1], which are well associated with a wide range of human behaviours and status such as purchasing behaviours [2], health conditions [3], [4], social relationships [5], and even criminal activities [6]. Consequently, automatic personality computing systems have drawn a lot of attention in recent years, and have been frequently developed for real-world human behaviour understanding applications, including computer assisted tutoring systems [7], human resource management [8], job interviews [9], and recommendation systems [10]. In these systems, the complex personality is usually described by trait-based models [11], [12], [13], [14] which focus on modelling personality aspects that are stable over time for the target person but differ in others [15].\nExisting automatic personality computing approaches can be roughly categorized into two types: (i) automatic personality recognition that recognises the target subject\u2019s\n\u2022 Rongfan Liao is with SONY China Software Center. E-mail: rongfan.liao@sony.com \u2022 Siyang Song and Hatice Gunes are with the AFAR Lab, Department of Computer Science and Technology, University of Cambridge, Cambridge, CB3 0FT, United Kingdom. E-mail: ss2796@cam.ac.uk, Hatice.Gunes@cl.cam.ac.uk (\u2217 Corresponding Author: Siyang Song, E-mail: ss2796@cam.ac.uk)\nManuscript received September 18, 2022.\nself-reported personality traits (also called true personality traits); and (ii) automatic personality perception recognition that predicts external human observers\u2019 impression on the target subject (also called first impression or apparent personality). The majority of these approaches were focused on automatic apparent personality recognition and were evaluated on the ChaLearn First Impression dataset [16]. They recognise apparent personality using either human visual behaviours (e.g., facial behaviours) [17], [18], [19], [20], [21], [22], [23], [24], [24], [25] or audio-visual behaviours [26], [27], [28], [29], [30]. More specifically, a large part of these [17], [21], [22], [26], [31], [32] infer personality traits from every single frame or short segment [19], and then make the clip-level personality prediction by combining all frame/segment-level predictions, where video-level personality labels were used as the frame/short segmentlevel labels to train models. While subjects with different personalities can behave similarly in a frame or a short video segment, models trained by such strategies would be problematic as they pair similar behaviour samples with different personality labels [33]. Alternatively, recent studies frequently emphasized that long-term behaviours enable more reliable personality inference [25], [29], [34], as personality traits are stable over time [15]. These approaches propose to encode a clip-level personality representation from long-term behaviours of the subject, and generally achieve superior recognition results than most frame/segment-level ar X iv :2 21 0.\n09 13\n8v 1\n[ cs\n.C V\n] 1\n7 O\nct 2\n02 2\napproaches. However existing approaches usually employ different pre-processing, post-processing and training strategies. Such differences lead to their reported results not being fairly reflected by their models\u2019 capabilities in recognizing personality traits. Meanwhile, very few number of deep learning-based approaches [18], [35], [36] have been proposed to recognize self-reported personality traits, and consequently there is lack of a comprehensive understanding for the capability of existing widely-developed apparent personality recognition approaches and standard deep learning models in recognizing self-reported personality traits. In summary, there is no comprehensive study that fairly demonstrates and compares the performances of existing personality computing models and widely-used deep learning models on both apparent personality and selfreported personality recognition (Research gap 1). In addition, only a small number of these studies made their code publicly available, which are even built on different platforms, e.g., Keras [19], [20], TensorFlow [37], caffe [24], Torch [27] and Chainer [38] (listed in Table 1), where some of these codes are incomplete and unimplementable. As a result, it is difficult for other researchers to reproduce or extend most of them (Research gap 2).\nIn this paper, we bridge the aforementioned research gaps by introducing two main contributions: (i) we benchmark eight existing audio-visual or visual only automatic apparent personality recognition approaches as well as seven widely-used static or spatio-temporal deep learning models with standardized pre-processing, training and post-processing strategies, on both widely-used selfreported personality (i.e., UDIVA [39]) and apparent personality datasets (ChaLearn First Impression [40]). The reported results provide a fair evaluation and comparison of all these models\u2019 capabilities for both apparent personality and selfreported personality recognition tasks (addressing research gap 1); and (ii) we make the code of all benchmarked models as well as their standardized pre-processing, training and post-processing scripts publicly available to further the science of automatic personality recognition. This provides new researchers entering this field a set of strong personality computing baselines, which will facilitate their exploration of new models and will enable them to apply these to new datasets in personality computing. To the best of our knowledge, this is the first study that benchmarks and fairly evaluates the existing personality computing approaches and standard deep learning models on both self-reported and apparent personality recognition tasks, with consistent and reproducible experimental settings."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "In this section, we systematically review and summarize recent audio-visual automatic apparent personality recognition (Sec. 2.1) and self-reported personality recognition (Sec. 2.2) approaches."
        },
        {
            "heading": "2.1 Automatic apparent personality Recognition",
            "text": "Existing approaches frequently attempt to recognize apparent personality from non-verbal facial behaviours, where a\nlarge part of them infer apparent personality only from a single facial display [17], [19], [20]. Joo et al. [41] conduct studies on face images of 650 American politicians of white ethnicity, which extracts Histogram of oriented gradients (HOG) features as the low-level static facial representation to predict personality traits. Dhall et al. [42] utilize both hand-crafted and deep-learned features to describe Twitter profile facial images and infer personality traits from them, where background information was also considered. Moreno-Armenda\u0301riz et al. [19] first extract one portrait picture as the representation for each video, and then train a CNN to predict video-level perception. In addition, although facial videos are provided, some studies also directly infer apparent personality from static facial appearance without considering temporal dynamics. For example, Ventura et al. [17] use Classification Activation Map (CAM) [43] to investigate the relationship between facial expressions and apparent personality, which feeds each frame to a CNN to infer video-level perception traits.\nWhile very few studies [37] infer personality traits from only audio behaviours, the majority of the studies focus on audio-visual methods that frequently show enhanced performances than the corresponding single modality (e.g., visual-based or audio-based) systems. Besides some early studies [44], [45] investigated the relationship between frame-level facial cues and apparent personality traits in the context of human-robot interactions, most existing approaches attempts to predict video-level personality traits. Zhang et al. [26] first choose 6 images from each video, and extend the Descriptor Aggregation Network (DAN) to deep learn facial perception features at the frame-level. Meanwhile, a linear regressor is used to process log filter bank-based clip-level audio representation. The late fusion scheme is employed to get the final prediction by averaging audio and visual predictions. Guccluturk et al. [32] propose a two-stream ResNet to deep learn both audio and visual personality cues at the frame-level, which are combined at fully connected (FC) layer to predict perceptions. Subramaniam et al. [27] divide each audio-visual clip into several short segments, where one facial frame is selected from each segment to produce a segment-level visual feature. Then, the segment-level audio-visual features are combined via a FC layer to make the apparent personality prediction. To recognize apparent personality from clip-level (long-term) audio-visual behaviours, Gu\u0308rpinar et al. [28] employ a pretrained network to extract frame-level facial emotion features. The video-level visual representation is then obtained by computing statistics of all frame-level features while cliplevel audio feature is extracted via OpenSMILE [46]. The final prediction is made by the weighted average of audio and visual predictions. As discussed in Sec. 1, these static or short-term behaviour-based approaches usually re-use video-level personality labels as the frame/segment-level labels during the training, which may result in problematic models.\nTo avoid the above problem, many recent approaches are proposed to recognize apparent personality from longterm behaviours (i.e., applying video-level behaviour representation to recognize video-level personality traits). For example, Beyan et al. [25] summarize a video into a set of dynamic images, and then select a small part of key dynamic\nimages based on their spatio-temporal saliency. Then, the produced key dynamic image sequence is used as the videolevel representation for the perception prediction. Helm et al. [20] extract a face image sequence by down-sampling the target video, whose features is used as the video-level facial behaviour representation. Then, a 3D-CNN is employed to infer apparent personality from this video-level representation. Gu\u0308rp\u0131nar et al. [23] jointly extract facial expressionrelated features from all aligned face images and the ambient information from the first frame of a down-sampled video, both of which are combined as the video-level visual representation for apparent personality prediction. Li et al. [29] propose to down-sample each video into 32 frames, and extract video-level visual features from the corresponding facial region sequence as well as the whole image sequence. In addition, the long-term audio features and text features are also combined with the visual features for apparent personality recognition. Since the above long-term behaviour modelling approach depend on down-sample videos, Song et al. [34] propose a self-supervised learning strategy to use all frames for video-level representation construction. This method trains a person-specific layer for each individual using facial behaviours of the whole video, whose weights are then used as the video-level personality representation."
        },
        {
            "heading": "2.2 Automatic Self-reported Personality Recognition",
            "text": "Compared to apparent personality, only a small number of studies attempt to infer self-reported personality traits. Qin et al. [47] extract five types of hand-crafted features (i.e., HOG, Local Binary Pattern (LBP), Gabor, Scale-invariant feature transform (SIFT) and Generalized Search Trees (Gist)) from each face image, which are fed to standard regressors (e.g., decision tree) to estimate self-reported personality (16PF) and intelligence. Besides such frame-level self-reported personality recognition approaches, recently proposed audio-visual self-reported personality analysis datasets [39], [48] lead some studies to develop video-level self-reported personality recognition approaches. Similar to apparent personality recognition solutions, Curto et al. [35] propose a Transformer-based model to extract individual and interpersonal behaviour features from each short dyadic interaction segment using variable time windows, which can jointly recognize self-reported personality traits for both individuals. In addtion, Celiktutan et al. [49] specifically investigated the self-reported personality recognition under the human-robot interaction scenarios. Since personality is well associated with human cognitive process, Song and Shao et al. [18], [36] propose the first audio-visual approach that employs Neural Architecture Search (NAS) to explore a\nperson-specific network for each subject using all available frames, whose architecture and parameters are encoded as the person-specific cognitive process graph representation for self-reported personality recognition. This NAS-based idea was then followed by [50], which also search a personalized deep learning architectures for each subject to recognize Big Five personality traits."
        },
        {
            "heading": "3 THE PROPOSED BENCHMARKING FRAMEWORK",
            "text": "In this paper, we propose a benchmarking framework that aims to provide a rigorous and reproducible evaluation of the existing personality computing models and the widelyused deep learning models for both automatic self-reported and apparent personality recognition. This framework will equip the future researchers with a set of strong audiovisual personality computing baselines, with standardized data pre-processing, training and post-processing strategies. Specifically, this section introduces our benchmarking framework by presenting its coding infrastructure (Sec. 3.1), benchmarked models and their settings (Sec. 3.2), and datasets used for evaluation (Sec. 3.3). The pipeline of the proposed personality benchmarking framework is illustrated in Fig. 1."
        },
        {
            "heading": "3.1 Coding infrastructure",
            "text": "A unified experimental setting is much needed given the heterogeneity in data processing and training strategies of existing personality computing approaches. Our benchmarking framework is built on PyTorch [51] library and it unifies the components of the data input, data preprocessing, data post-processing, training, optimization and evaluation, for all deep learning model-based experiments. The only difference in experiments resides in different models\u2019 architectures and their training hyper-parameter settings as the optimal hyper-parameter settings are modeldependent. This way, the personality computing results achieved for all benchmarked models can be compared fairly."
        },
        {
            "heading": "3.1.1 Data input and pre-processing",
            "text": "For all experiments, we consistently employ the same training, validation and test sets that are predefined in the corresponding self-reported personality and apparent personality datasets for training the models and reporting the results. Instead of using complex and dataset-specific data pre-processing pipelines, we employ a widely-used and unified data pre-processing pipeline for each compared\nmodel (e.g., static models and spatio-temporal models) as follows:\n\u2022 Static image-based models: We first evenly divide each video into K short segments as suggested in [27], [31] and only select the one frame of each segment (To make experiments to be reproducible, we consistently use the first frame of each segment), where we follow the winner [31] of ChaLearn 2016 impression challenge to set K as 100 for experiments conducted on ChaLearn First Impression dataset, and empirically setK as 2000 for UDVIA dataset. We then apply MTCNN [52] to obtain an cropped and aligned face image from each selected frame. During training, validation or testing, we again follow [31] to resize the resolution of these images to 456\u00d7 256, where only a 224 \u00d7 224 sub-region in the center of the image is used as the final full frame. Meanwhile, the face region is contained in a cropped image with the resolution of 112 \u00d7 112 using the Dlib library [53]. Specifically, standard data augmentation including center crop, random horizontal flip and pixel normalization are employed to process all training images. We follow previous studies [17], [31], [32] that compute personality traits prediction of each test video by averaging all frame-level predictions. \u2022 Spatio-temporal models: For each spatio-temporal model, we benchmark two types of systems: (i) short segment-level system, where we divide each video into several segments, where each segment that contains n frames (i.e., we empirically employed n = 32 or n = 64 according to (1) the setting employed in the original publication; and (2) the experimental\nresults) is used as the input. The personality trait prediction of each test video is computed by averaging all segment-level predictions; and (ii) videolevel system, where we again follow [29] that evenly divides each video into 32 segments, and select the first frame of each segment to construct a sequence of 32 frames to represent the entire video. The sequence is then used as the input for spatio-temporal models to produce the video-level personality prediction. For both systems, MTCNN and Dlib library are again employed to obtain each full frame and its corresponding face image based on the same setting as the static image-based models. \u2022 Audio models: As most audio-visual studies did not provide the details of their raw audio signal extraction, we follow previous studies [31], [54] to use the FFmpeg tool 1 to extract the raw audio signal from original audio-visual clips for all audio-based models. The processing details for each audio model are introduced in Sec. 3.2.2."
        },
        {
            "heading": "3.1.2 Training, validation and testing protocol",
            "text": "For all benchmarked static models and short segment-based spatio-temporal models, we follow the previous studies [17], [31], [32] to re-use the clip-level label as the corresponding frame-level or short segment-level labels to train them. We also use the clip-level label as the label to train models that take clip-level sequence as the input. For all experiments, we consistently employ the same training, validation and testing protocol. Each static image-based model or short segment-based spatio-temporal model is trained on\n1. https://github.com/FFmpeg/FFmpeg\nthe training dataset with the shuffled data samples, and is evaluated on the validation set. We choose the model that obtains the best performance on the validation set as the final model for each experiment, which generates the reported testing results."
        },
        {
            "heading": "3.1.3 Evaluation metrics",
            "text": "To measure the performance of the trained models, we first employ two standard evaluation metrics (i.e., ACC and Mean Squared Error (MSE)) proposed in [16] and [39]. The ACC is defined as:\nACC = 1\u2212 1 N N\u2211 n=1 |ypn \u2212 ygn | (1)\nand the MSE can be computed as:\nMSE = 1\nN N\u2211 n=1 (ypn \u2212 ygn) 2 (2)\nwhere N indicates the number of test videos; ypi and ygi denote the prediction and ground-truth of the nth test video, respectively. We also follow recent studies [18], [22], [34] that employ Concordance Correlation Coefficient (CCC, Eq. 3) to measure the correlation between predictions and groundtruths, which is defined as:\nCCC = 2\u03c3Yp\u03c3YgPCCYp,Yg\n\u03c32Yp + \u03c3 2 Yg + ( \u00b5Yp \u2212 \u00b5Yg )2 , (3) where \u00b5Yp and \u00b5Yg denote the mean values of predictions and ground-truths of all test videos, respectively; \u03c3Yp and \u03c3Yg are the corresponding standard deviations; and PCCYp,Yg denotes the Pearson Correlation Coefficient (PCC) between Yp and Yg . In this paper, we follow previous studies that individually employ MSE and CCC to evaluate the selfreported personality recognition systems while we use ACC and CCC to evaluate the apparent personality recognition systems."
        },
        {
            "heading": "3.2 Benchmarked personality computing models",
            "text": "In this section, we provide the details of all the benchmarked models including (i) seven visual models, six audio models and five audio-visual models that have been employed by existing personality computing approaches; (ii) seven typical spatial/spatio-temporal deep learning CNN/Transformer models that have been widely-used in image or video processing tasks but have not yet been employed for personality recognition; and (iii) two previously used clip-level encoding models that allow all frame/short segment-level personality predictions of a clip to be combined for clip-level personality recognition."
        },
        {
            "heading": "3.2.1 Personality computing model inclusion and exclusion criteria",
            "text": "To the best of our knowledge, there are more than one hundred visual or audio-visual personality computing models that have been proposed in the literature. In this paper, we propose the following inclusion and exclusion criteria to benchmark the most representative personality computing models:\n\u2022 Inclusion criteria: The main criteria of this benchmark is to choose representative approaches that have been evaluated and compared in the wellknown and widely-used publicly available audiovisual personality computing dataset: ChaLearn 2016 First Impression. As a result, we first choose the Top-3 ranked models [26], [27], [32] from the ChaLearn 2016 First Impression Recognition Challenge. Then, we choose the best end-to-end audiovisual deep learning model that have been published in recent three years, respectively, which ( [24] (2019), [29] (2020), and [54] 2022) claimed that they have achieved the state-of-the-art performances on ChaLearn 2016 impression dataset and provided all details for model settings and training (the best model of 2021 [34] is not an end-to-end approach). In addition, we also reproduce two models [17], [37] that can visualize the relationship between human audio/visual non-verbal behaviours and personality traits. \u2022 Exclusion criteria: In this paper, we exclude methods that are not based on: (i) audio-visual deep learning models; (ii) an end-to-end framework, such as [18], [22], [34], [36] which require relatively complex pre-processing or pre-training; (iii) evaluated on the ChaLearn 2016 dataset [18], [35], [35], [36], [50], [55]; (iv) using the Big-five personality traits as the target recognition task; and (v) treating personality recognition as a regression task."
        },
        {
            "heading": "3.2.2 Existing personality computing models",
            "text": "We first present seven deep learning visual models, six audio models and five audio-visual models that have been proposed to recognize apparent personality traits, which are evaluated on the ChaLearn 2016 First Impression dataset [16]. For each deep learning-based model, the last FC layer has five neurons to jointly predict five personality traits.\nVisual models: The details of the seven existing personality computing visual models reproduced in this paper are explained as follows:\n\u2022 (i) Wei et al. [31]: DAN is a CNN model that infers personality from each static image (including both face and background). It consists of several convolution-ReLU blocks and an additional block that is equipped with both average-pooling and maxpooling, which is added between the last convolutional layer and the final FC layer. Before being fed to the model, each image is resized to 456 \u00d7 256 resolution. The training process is conducted using the SGD optimizer [56] with initial learning rate of 0.05, weight decay of 0.0005 and momentum of 0.8, where MSE is employed as the loss function. \u2022 (ii) Ventura et al. [17]: CAM-DAN+ has a similar architecture to the DAN network, which also infers personality from each static image. This model applies a max pooling and an average pooling as two parallel branches in addition to convolution layers, and is equipped with a Class Activation Map (CAM) module in order to visualize the most salient facial\nregions for personality recognition. The training process is conducted using the SGD optimizer [56] with initial learning rate of 0.05, weight decay of 0.0005 and momentum of 0.8, where MSE is employed as the loss function. \u2022 (iii) Subramaniam et al. [27]: The visual part of this approach consists of three 3D convolutional layers and two FC layers. It randomly takes three face images from each video as the video-level representation, and concatenates them as the clip-level input. The model is trained using the SGD optimizer with initial learning rate of 0.05, weight decay of 0.0005 and momentum of 0.9, where MSE is employed as the loss function. \u2022 (iv) Gu\u0308clu\u0308tu\u0308rk et al. [32]: The visual part of this model is a 17-layer deep residual network, where an FC layer is attached at the top of the model. It takes a static image (both face and background) as the input. The training process is conducted using the SGD optimizer with initial learning rate of 0.05, weight decay of 0.0005 and momentum of 0.9, where MSE is employed as the loss function. \u2022 (v) Zhang et al. [24]: The model first employs four convolution layers and an FC layer to extract features from each face image of the video. Then, three parallel FC layers are used to individually predict valence, arousal and personality traits at the same time. Besides, a coherence module (an FC layer) is introduced to process both images from the personality dataset and emotion dataset, allowing the final learned representation to be dataset invariant, while a RAM module that consists of two FC layers is employed to produce the apparent personality attributes from arousal and valence inputs, aiming to enhance the learned personality representations. The training process of this model is achieved by using the SGD optimizer with initial learning rate of 0.001, weight decay of 0.0005 and momentum of 0.9, while MSE is used as the loss function. \u2022 (vi) Li et al. [29]: The visual module of the CR-Net consists of two streams, where each uses a 34-layer deep residual network as the backbone to learn personality features from either down-sampled videolevel face sequences and full image sequences. Then, a CR-block is attached to first obtain personality classification features (processed by softmax) from the latent feature produced by each backbone, based on which the regression feature is extracted. The Extra Trees Regressor (ETR) is finally employed to predict personality trait intensities from the extracted regression features. To obtain the video-level face sequence and full image sequence, each video is divided into 32 segments, where a single frame is randomly selected from each segment (i.e., each video is down-sampled to 32 frames). The SGD optimizer with initial learning rate of 0.002, weight decay of 0.005 and momentum of 0.9 is employed for training, where the Bell Loss function [29] is also applied. \u2022 (vii) Suman et al. [54]: The visual part of this system is a two-stream ResNet-101 model that extracts two different visual features: ambient features (extracted\nfrom full frames) and facial features. The system consists of three steps: (i) Pre-processing: the system selects six equally spaced images from each video clip, and applies MTCNN to obtain the face area for extracting facial features; (ii) ambient and facial feature extraction: ResNet-101 pre-trained on ImageNet is re-trained to extract ambient features and facial features from each selected frame; and (iii) the ambient features and facial features of each selected frame individually are fed to a MLP (containing FC layers and a sigmoid layer) to make frame-level personality prediction. The video-level personality trait prediction is obtained by averaging all framelevel predictions. The training process is conducted using the SGD optimizer with initial learning rate of 0.01, weight decay of 0.0005 and momentum of 0.9, where MSE is employed as the loss function.\nAudio models: The details of the six existing personality computing audio models reproduced in this paper are explained as follows:\n\u2022 (i) Hayat et al. [57]: The clip-level raw audio signal is obtained from each audio-visual clip at 4000 HZ, and is represented by a 61208 dimensional vector. The obtained audio signal is normalized to have zero mean and unit variance, and converted to the frequency domain using FFT. The produced spectral signal is then fed to a network that consists of three temporal convolution layers followed by max pooling and dropout. Here, the training is conducted using SGD optimizer with initial learning rate of 0.00083, weight decay of 0.065 and a momentum of 0.9, and the MSE is applied as the loss function. \u2022 (ii) Wei et al. [31]: The clip-level raw audio signal is obtained from each audio-visual clip at 44,100 Hz, which are divided into 3059 audio frames. A 13- D MFCC feature and a 26-D logfbank feature are extracted from each audio frame. Then, the cliplevel 39767-D MFCC feature and 79534-D logfbank feature are produced by concatenating all frame-level MFCC and logfbank features, respectively. During the model training, SGD optimizer with initial learning rate of 0.05, weight decay of 0.0005 and momentum of 0.8 is applied, and the MSE is employed as the loss function. \u2022 (iii) Subramaniam et al. [27]: The audio signal of each clip is first divided into six non-overlapping segments, where the mean and standard deviation of several properties of each audio segment are extracted, and combined as a 68 dimension vector using pyAudioAnalysis [58] (please check [27] for details). We then concatenate six vectors from six segments as a matrix with size of 6 \u00d7 68, which is fed to a FC-LSTM-FC block to jointly predict five personality traits. During the model training, SGD optimizer with initial learning rate of 0.05, weight decay of 0.0005 and momentum of 0.9 is applied, and the MSE is employed as the loss function. \u2022 (iv) Gu\u0308clu\u0308tu\u0308rk et al. [32]: The entire audio signal of each clip is sampled at 16,000 HZ using the librosa library [59], resulting in a 244,832-D vector\nto represent each clip-level audio signal. For each iteration, we randomly select a continuous 50,176- D sub-vector from the clip-level vector as the cliplevel audio representation. Then, a 17-layer ResNet is employed, where a FC layer is attached at the top of it. The model training is achieved using the SGD optimizer with initial learning rate of 0.0002, weight decay of 0.0005 and momentum of 0.9 while the MSE is employed as the loss function. \u2022 (v) Li et al. [29]: The entire audio signal of each clip is converted to a 244,832-D vector using the librosa library [59], which is then used as the clip-level audio representation. The ResNet-34 model is employed to process this audio representation to extract the clip-level audio personality features, from which the CR-block and ETR are used to jointly predict five personality traits. The entire audio model is trained using the SGD optimizer with initial learning rate of 0.0002, weight decay of 0.0005 and momentum of 0.9 and the Bell loss is employed as the loss function. \u2022 (vi) Suman et al. [54]: The raw audio clip is resampled to 16 kHz. The spectrogram of the audio is then created, and these features are framed into non-overlapping examples of 0.96s. A pre-trained VGGish CNN is then introduced to extract audio personality features from each spectrogram frame. The features of all frames are finally concatenated and fed to a MLP consisting of FC layers and a sigmoid layer to make a clip-level personality prediction. The entire audio model is trained using the SGD optimizer with initial learning rate of 0.001, weight decay of 0.0005 and a momentum of 0.9, and the Bell loss is employed as the loss function.\nAudio-visual models: Based on the reproduced audio and visual systems, we also reproduce four audio-visual models that have been used for apparent personality recognition on the ChaLearn 2016 First Impression dataset as follows:\n\u2022 (i) Wei et al. [31]: Following the same settings introduced in [31], we build an audio-visual model that is made up of the corresponding visual and the audio model (the visual model (i) and the audio model (ii)) described above. The final clip-level prediction is obtained by averaging clip-level audio prediction and clip-level video prediction. \u2022 (ii) Subramaniam et al. [27]: The audio and visual clip of each subject is firstly divided into 6 segments, where each visual segment is processed by the visual model (iii) described above and each audio segment is processed by the audio model (iii) described above. Then, each pair of segment-level audio and visual latent features are concatenated as a 160-D vector. Finally, the clip-level personality prediction is obtained by feeding the audio-visual vectors of six segments to an LSTM model with 128 hidden units. Here, the LSTM contains a single hidden layer with 128 neurons. The entire model is trained using the SGD optimizer with initial learning rate of 0.05, weight decay of 0.0005 and momentum of 0.9, and MSE is employed as the loss function.\n\u2022 (iii) Gu\u0308clu\u0308tu\u0308rk et al. [32]: A two-steam ResNet model is employed, which consists of the visual model (iv) and the audio model (iv) described above. The audio feature (256-D) and the latent frame-level visual feature (256-D) are concatenated as a 512-D feature to a fully-connected layer to jointly predict the five personality traits. Specifically, the model randomly takes an audio and visual sample at each training iteration. The entire model is trained using the SGD optimizer with initial learning rate of 0.0002, weight decay of 0.0005 and momentum of 0.9, and MSE is employed as the loss function. \u2022 (iv) Li et al. [29]: A three-stream ResNet-34 model is employed, which consists of two visual streams (the visual model (vi) described above), and an audio stream (the visual model (v) described above). Specifically, the two-stream visual model takes both the face image sequence and full frame sequence as the visual inputs. The latent features produced by audio and visual streams are combined as a single 512- D vector by element-wise sum. The entire model is trained using the SGD optimizer with initial learning rate of 0.002, weight decay of 0.005 and momentum of 0.9, where the Bell loss is employed as the loss function. \u2022 (v) Suman et al. [54]: A multi-modal system that contains a visual stream (the visual model (vii) described above) which takes both face and full frames as the input, and an VGGish-based audio stream (the audio model (vii) described above) which learns personality features from the audio spectrogram. The entire model is trained using the SGD optimizer with initial learning rate of 0.001, weight decay of 0.005 and momentum of 0.9, where the Bell loss is employed as the loss function."
        },
        {
            "heading": "3.2.3 Widely-used static/spatio-temporal visual deep learning models",
            "text": "Since visual information are much more informative for personality recognition (validated in Sec. 4.1), we additional benchmark seven standard visual deep learning models that previously have not been applied to personality computing but have been widely used for static image or video analysis. Specifically, these include three models that take the static\nface/full image as the input and four models that infer personality from spatio-temporal visual data. We list all these models in Table 2."
        },
        {
            "heading": "3.2.4 Clip-level representation generation models",
            "text": "To combine frame/segment-level personality predictions of an audio-visual clip for a clip-level personality trait prediction, this section also introduces two standard methods that have been employed in existing personality computing publications. In particular, besides the widely-used strategy which simply averages all frame/segment-level predictions at the clip-level prediction (used by [17], [24], [31], [32]), we also benchmark the spectral representation [33], [67] to summarize frame/segment-level predictions at the cliplevel, which has been used by [34]. This is because the spectral representation can not only effectively encodes the temporal dependencies among all frame-level predictions of a clip, but also summarises time-series signal of an arbitrary length into a representation of a fixed size without any distortion.\n\u2022 Averaging frame/segment-level predictions (AFP): The clip-level personality prediction is obtained by averaging all frame/segment-level predictions. \u2022 Spectral representation of frame/segment-level predictions (SFP): We first encode a pair of spectral heatmaps (please see [33] for details) from the five-channel (corresponding to five traits) frame/segment-level personality trait prediction time-series. Then, we train a 1D-CNN to predict cliplevel personality traits from the produced spectral heatmaps."
        },
        {
            "heading": "3.3 Evaluation datasets",
            "text": "There are several publicly available audio-visual personality computing datasets for apparent personality recognition [16], [68], [69], and self-reported personality recognition [39], [70], [71] (please see [40] for a survey of existing personality databases). Among them, the two most widely used publicly available audio-visual self-reported personality and apparent personality datasets: the UDIVA Dataset [39] and ChaLearn First Impression dataset [16], are employed to evaluate all models described in Sec. 3.2. Both datasets provide the Big-Five personality traits (i.e., Extraversion (Ext), Agreeableness (Agr), Openness (Ope), Conscientiousness (Con), and Neuroticism (Neu)) as the label for each audiovisual clip.\nUDIVA dataset was released in 2021. It records 188 dyadic interaction clips between 147 voluntary participants, with total 90.5h of recordings. Each clip contains two audiovisual files, where each records a single participant\u2019s behaviours. For each dyadic interaction session, participants were matched based on their availability, language and three variables: gender, age group and the relationship among interlocutors. In particular, all participants were matched to enforce a close-to-uniform distribution among all possible combinations between these variables. During the recordings, participants were asked to sit at 90 degrees to the conversational partner around a table, and under the dyadic interactions based on five tasks: Talk, \u2019Animal games\u2019, Lego building, \u201cGhost blitz\u201d card game, and Gaze\nevents, where each pair of participants were interacting using one of the three languages (i.e., English, Spanish or Catalan). The self-reported personality labels are obtained for each participant based on the following rules: (i) parents of children up to 8 years old completed the Children Behavior Questionnaire (CBQ) [72]; participants from 9 to 15 years old completed the Early Adolescent Temperament Questionnaire (EATQ-R) [73]; and the rest of the participants completed both the Big Five Inventory [74] and the HonestyHumility axis of the HEXACO personality inventory [75].\nChaLearn First Impression dataset was released in 2016. It contains talking-to-the-camera 10, 000 audio-visual clips that come from 2, 764 YouTube users, where each video lasts for about 15 seconds with 30 fps. Each video is labelled with the Big-Five personality traits that are annotated by external human annotators using the Amazon Mechanical Turk. The intensity of each trait was normalized to the range of [0, 1]. The dataset provides official splits for training (6, 000 videos), validation (2, 000 videos) and test (2, 000 videos). It is the largest audio-visual dataset openly available for research purposes, which has been annotated with Big-Five apparent personality traits."
        },
        {
            "heading": "4 EXPERIMENTAL RESULTS",
            "text": "In this section, we first present the results of all benchmarked models for both self-reported personality and apparent personality traits recognition in Sec. 4.1. Then, in Sec. 4.2 we discuss the influences of different settings on personality recognition performances. Finally, we provide a systematic discussion of the benchmarking results in Sec. 4.3"
        },
        {
            "heading": "4.1 Benchmarking personality computing models",
            "text": "This section reports the self-reported and apparent personality recognition results achieved by all benchmarked models. According to Table 3 and Table 4, all benchmarked models failed to accurately infer the self-reported personality traits from human audio/visual/audio-visual behaviours, with only three visual models (e.g., Interpretimg, SENet, and HRNet) achieving more than 0.15 CCC performances. Standard audio feature extraction methods can barely extract self-reported personality-related information from non-verbal audio signals, as most of the benchmarked models achieved near-zero CCC values, while only the predictions of the VGGish achieved CCC > 0.1 with the ground-truth. Some audio models even degraded the corresponding visual systems under the audio-visual setting. The low CCC and high MSE results of almost all models on self-reported personality recognition indicate that it may difficult and unreliable for standard deep learning models to be used to directly infer human self-reported personality traits from external non-verbal behaviours.\nIn contrast, the relatively higher CCC and ACC results shown in Table 5 and Table 6 suggest that apparent personality traits can be better predicted from human nonverbal behaviours when using deep learning models, i.e., five visual models and two audio-visual models achieved CCC > 0.5. In particular, the predictions produced by HRNet and VAT from only facial behaviours have more than 0.6 correlation (measured by CCC) with the ground-truth.\nIn addition, we found that the CR-Net and VGGish models can also deep learn valuable apparent personality-related cues from non-verbal audio signals. The aforementioned results suggest that compared to self-reported personality, apparent personality is generally more feasible and easier to be directly predicted from human non-verbal behaviours. Since apparent personality is defined as the external human obverses\u2019 perception of the target subject, these results can be explained by the fact that it is straight-forward for deep learning models to be used as an external observer and make judgments based on behaviours that can be directly observed. In contrast, self-reported personality is the internal state/attribute of the target person, which is not straightforward to be directly inferred from external behaviours, and thus can be more difficult for deep learning models to simulate such processes. The detailed discussions on applying ML models for inferring self-reported personality traits can be found in [18], [36]."
        },
        {
            "heading": "4.2 Ablation studies",
            "text": "In this section, we evaluate the influences of different preprocessing, post-processing and model settings on personality recognition performances."
        },
        {
            "heading": "4.2.1 Full frames VS. Face regions",
            "text": "We compare the results achieved by feeding aligned face regions and full frames (containing both the face and backgrounds) to standard visual deep learning models in Fig. 2 and Fig. 3, where the standard visual deep learning models generally provide more reliable self-reported and apparent personality predictions by using face regions, despite the fact that neither full frames nor face regions provide reliable self-reported personality predictions. In contrast, apparent\npersonality predictions generated by all visual models using both face regions and full frames have CCC > 0.37 on average with the ground-truth, where face region-based predictions still have 1.62% CCC advantage over full framebased predictions. However, it can be seen that using face regions and full frames does not cause a large difference\nfor most models, where only HRNet is sensitive to this variable on both tasks. These results suggest that although some previous studies claimed that backgrounds and spatial contextual information can provide informative cues for personality recognition, they may also contain personalityunrelated noise that can negatively impact the personality recognition. In short, the results achieved by the benchmarked standard visual deep learning models show that the personality-related cues contained in background and spatial contextual information can not always compensate their negative impact for personality recognition."
        },
        {
            "heading": "4.2.2 Spatial VS. Spatio-temporal visual models",
            "text": "We compare the average results achieved by spatial visual models (i.e., frame-level systems) and spatio-temporal visual models (i.e., short segment-level systems and videolevel systems) on both datasets in Fig. 4. It can be observed that the average CCC performance achieved by the spatial models outperforms the spatio-temporal settings for both self-reported and apparent personality recognition tasks, i.e., while no model is reliable for inferring self-reported personality from facial behaviours, spatial models generally provide better self-reported personality predictions for all five traits than short segment-level spatio-temporal models as well as four of the five traits than video-level spatiotemporal models. Nevertheless, we found that a spatiotemporal model VAT achieved the best CCC performance in predicting apparent personality traits, with CCC > 0.6 for predicting four of the five traits, which suggest that the VAT\u2019s temporal modelling strategy is suitable to extract apparent personality-related facial behaviour cues. Overall, spatial visual models consistently achieve better average performance than spatio-temporal models in predicting all\napparent personality traits. These results indicate that the most standard spatio-temporal deep learning models (except VAT) failed to extract personality-related cues from human non-verbal spatio-temporal facial behaviours. In contrast, spatial visual models encode more reliable and discriminative personality cues from static facial expressions/identities."
        },
        {
            "heading": "4.2.3 Short segment-level VS. video-level personality modelling",
            "text": "Fig. 4 also compares short segment-level modelling and video-level modelling systems, where four widely-used spatio-temporal models introduced in Sec. 3.2.3 are benchmarked. Although both short segment-level modelling and video-level modelling systems generally failed to provide reliable self-reported personality predictions, the short segment-level systems still achieved slightly better average performances than the video-level systems for four traits (i.e., Ope, Con, Agr, and Neu traits). Moreover, the short segment-level systems have clear advantages over the\nvideo-level systems for predicting all apparent personality traits. These results suggest that the short-term non-verbal behaviours contain crucial apparent personality-related cues while the standard video-level models which ignored shortterm behaviours can not extract reliable apparent personality cues from the down-sampled long-term behaviours (i.e., down-sampling a long clip that contains around 400 frames to a segment containing 32 frames results in the lose of short-term behavioural cues)."
        },
        {
            "heading": "4.2.4 Audio VS. Visual VS. Audio-visual models",
            "text": "As shown in Fig. 5, visual models and audio-visual models on average generated superior personality recognition performances than audio models for all traits on both datasets. We found that CAM-DAN+, HRNet and SENet visual models achieved relatively promising results for both self-reported and apparent personality recognition tasks. However, the low CCC performance on self-reported personality recognition still shows that standard audio/visual machine learning models can hardly extract self-reported\npersonality-related cues from audio-visual behaviours. As a result, the five audio-visual systems displayed in Table 5 do not show clear and consistent advantages over their corresponding visual counterparts. In contrast, the audio and facial behaviours sometimes are more reliable in reflecting apparent personality traits. According to Table 5, adding the audio modality allows three out of the five audio-visual systems [29], [32], [54] achieved better performances over their corresponding visual methods in predicting apparent personality traits. However, when the extracted audio features are not well associated with apparent personality traits, they may even negatively impact the personality recognition ability of the visual system (e.g., he MFCC/logbank audio features degrade the DAN-based visual system [31]). In summary, these results suggest that (i) non-verbal facial behaviours contain more discriminative personality-related cues than non-verbal audio signals for both self-reported and apparent personality recognition tasks; and (ii) if the audio modality can individually provide reliable personality\npredictions, it is expected to further enhance the predictions of the visual system."
        },
        {
            "heading": "4.2.5 Summarising frame/segment-level personality predictions",
            "text": "We then compare the results achieved by the different strategies for summarising frame/segment-level personality predictions for obtaining clip-level personality prediction by: (i) simply averaging frame-level personality predictions as the clip-level personality prediction; and (ii) encoding a spectral representation of frame/segment-level predictions. Specifically, we first encode a pair of spectral heatmaps from the five-channel frame/segment-level personality trait predictions. Then, we train a regressor that contains seven 1D convolution layers followed by two fully connected layers to predict clip-level personality traits from the produced spectral heatmaps. As shown in Table 7, applying spectral representation to encode frame/segment-level personality predictions has consistent advantages over the averaging strategy for summarising clip-level apparent personality predictions, as the performances of all models improved us-\ning spectral representation. This also indicates that the spectral representation can capture reliable personality-related spatio-temporal cues from frame/segment-level personality predictions. The improvements of applying spectral representation for self-reported personality recognition are not as clear as recognising apparent personality traits. We assume this is because frame-level predictions of self-reported personality traits are not reliable, and thus it is difficult to capture reliable personality-related spatio-temporal cues from them."
        },
        {
            "heading": "4.3 Discussion",
            "text": "In addition to reporting the results achieved by the benchmarked models and discussing the results achieved by the different settings, this section discusses other relevant aspects of the benchmarked models, including: (i) the reproduced models generally achieving lower results than the originally reported results; (ii) personality traits having different relationships with the non-verbal behaviours; and (iii) challenges and research gaps in the current audio-visual personality computing models."
        },
        {
            "heading": "4.3.1 Lower performance than originally reported",
            "text": "As compared in Table 8, one important finding in this work is that all reproduced models failed to achieve the same or superior results as their originally reported results on the ChaLearn 2016 First Impression dataset, even when we followed the exact same pre-processing and training settings they have reported. This could be caused by various reasons, including the original papers not reporting all training/evaluation/pre-processing tricks and model details they used to obtain the reported models, or the random factors involved in training processes (e.g., the shuffled training samples). Moreover, the results achieved by most reproduced models are not as good as the corresponding systems that utilized the proposed standardised preprocessing and training steps. This validated the effectiveness of the proposed standardised pre-processing pipeline, which can be adopted by future researchers to report on their proposed models for fair comparisons."
        },
        {
            "heading": "4.3.2 Relationships between the personality traits and nonverbal behaviours",
            "text": "Fig. 4 show that self-reported Conscientiousness and Neuroticism traits are relatively reliable to be inferred by static facial displays than other traits. Self-reported Extraversion trait is less related to static facial behaviours but more associated with long-term spatio-temporal facial behaviours. In terms of apparent personality traits, it can be seen that they have similar relationship patterns with facial behaviours of three temporal scales (frame-level, short segment-level and video-level), i.e., the behaviours of all three temporal scales are more reliable to infer Conscientiousness, Extraversion, Neuroticism, and Openness traits, while all of their associations with the Agreeableness trait are clearly weaker.\nWhile facial behaviours are clearly more informative than audio behaviours in predicting all personality traits, Fig. 5 illustrates that the self-reported Conscientiousness and Neuroticism traits can be better reflected by facial behaviours while Extraversion and Openness traits are less\nClip-level representation DAN [31] ResNet [32] CRNet [61] PersEmoN [24] CAM-DAN+ [17] SENet [60] HRNet [61] Swin [62]\nassociated with facial behaviours. Again, both audio and facial behaviours are more reliable in inferring apparent Conscientiousness, Extraversion, Neuroticism, and Openness traits but are less correlated with the Agreeableness trait.\nBased on these results, we can conclude that each true or apparent personality trait has a unique relationship with human non-verbal behaviours, including their temporal scales and modalities. In particular, we found that: (i) nonverbal audio-visual behaviours are more reliable in inferring apparent personalities; (ii) facial behaviours are more informative than audio behaviours in inferring personality traits; (iii) self-reported Conscientiousness and Neuroticism traits can be easier to be inferred from non-verbal facial behaviours than other self-reported traits; and (iv) apparent Agreeableness trait is more difficult to be predicted from facial behaviours than other apparent personality traits."
        },
        {
            "heading": "4.3.3 Challenges and research gaps",
            "text": "Although an increasing number of automatic personality computing approaches have been proposed in recent years, both their reported results and our benchmarking results show that existing approaches are not very reliable for inferring personality traits, especially for self-reported personality. We can summarise the research gaps of existing personality computing approaches as follows:\n\u2022 (i) Most existing studies [17], [21], [22], [26], [31], [32], [76] attempt to infer personality traits directly from static frames or short audio-visual segments without considering contextual information. Since subjects that have different personalities can show\nsimilar behaviours in a single frame or a short audiovisual segment (e.g., different subjects can show a happy expression in a single frame), a static frame or a short audio-visual segment is not reliable for inferring personality. Also pairing static frame or a short audio-visual segment with clip-level personality labels to train models results in an ill-posed machine learning problem, making the trained model theoretically not have good generalization capability. Our results show that the SFP-based long-term modelling of frame/short segment-level predictions produced by reliable frame/short segment-based systems (e.g., most frame/short segment-level apparent personality recognition models) leads to the enhanced personality recognition results (Table 7). \u2022 (ii) Existing clip-level personality models can partially address the aforementioned ill-posed machine learning problem during model training. However, our benchmark shows that their results are worse as compared to the static frames or short audio-visual segments-based approaches. This can be explained by the fact that most clip-level spatio-temporal personality computing models [29] (including the benchmarked standard spatio-temporal models) require down-sampling the original video, i.e., a large number of frames are discarded, i.e., crucial shortterm personality-related cues are ignored during training and inference stages. While some clip-level personality computing studies [18], [34], [36] retain almost all frames in a video, these methods are timeconsuming to implement and train as they need to individually train a person-specific network for each subject. In short, there lacks an efficient clip-level audio-visual personality computing framework that can extract personality-related cues from all frames of the input data. \u2022 (iii) Most existing deep learning-based personality computing approaches simply pair the given audiovisual data and labels to train models, i.e., they do not take the personality-related domain knowledge into consideration when designing and training their models. As we can see from the benchmarked results, such strategies can not produce models that are able to provide reliable personality predictions. In other words, it is necessary for future studies to systematically review the physiological and psychological findings to concretely model the relationship between true/apparent personality traits\nand human non-verbal behaviours (e.g., the behaviours/temporal patterns that are more frequently displayed by subjects that have a certain personality traits pattern), and integrate such knowledge to design more advanced personality computing models and training strategies.\nBesides the research gaps, we also discuss the challenges for both current and future researchers in developing new and more advanced personality models as follows:\n\u2022 (i) There is a need of open-source and easily implemented code that allow researcher to improve the existing approaches or develop more advanced models based on it. \u2022 (ii) Existing approaches usually have different and complex pre-processing strategies. A large number of the published approaches do not provide full details of their pre-processing steps, model settings and training strategies due to the limited page numbers. Subsequently, it is difficult for researchers to fully reproduce their methods and results, i.e., our benchmarking results are not as the same as the results reported in their original papers. \u2022 (iii) Due to the limited number of publicly available datasets, most existing personality computing models are only evaluated on a single dataset (e.g., ChaLearn 2016 First Impression dataset). Based on the results achieved by our benchmark, we found the same model can have completely different performances in predicting self-reported or apparent personality traits on different a dataset. Therefore, we assume that personality computing models should be evaluated on more than one dataset (i.e., multiple apparent personality datasets or self-reported personality datasets) which have different data recording settings and scenarios. Otherwise, if a model achieves excellent performance on a single dataset, we can only conclude that this model is well adapted to the dataset at hand, but can not be made about its reliability and capability in predicting personality in general."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we propose the first audio-visual personality computing benchmark for both self-reported and apparent personality recognition tasks, which are evaluated on the two widely-used and publicly available personality computing datasets: the ChaLearn First Impression dataset and the ChaLearn UDIVA self-reported personality dataset. We first benchmarked seven visual models, six audio models and five audio-visual models that have been published and evaluated on the ChaLearn First Impression datasets. We also benchmarked seven visual deep learning models that are widely used for visual problems, which have not been applied to video-based personality computing before. Our open-source, easy-to-use and standardised framework can be utilized to not only develop new personality computing models but also conduct quick yet robust evaluation of new models on both self-reported and apparent personality traits recognition tasks.\nBuilding on our benchmarked models, we systematically evaluated various factors that impact personality computing. This led us to make the following conclusions: (i) using cropped and aligned face images as input generally led models to produce slightly better personality predictions than using images with background; (ii) the static models frequently achieved better performances than most spatio-temporal models for personality recognition; (iii) visual models frequently achieved superior results than audio models on personality recognition, indicating that subjects\u2019 facial behaviours contain more discriminative apparent personality-related cues than non-verbal audio behaviours; (iv) most models achieved better apparent personality recognition results than self-reported personality recognition even when different datasets were used, which suggests that subjects do not show their self-reported personality through non-verbal audio-visual behaviours; and (v) each personality trait has a unique relationship with human non-verbal behaviours (i.e., modalities and temporal scale), and thus even the same data contribute differently for personality recognition.\nWe also discussed the current research gaps and challenges in audio-visual personality computing. Our work provides a standardized data loading, pre-processing and model training framework as well as a set of reproduced personality computing models, which partially address the discussed issues. However, creating an extensive benchmark must be a common effort, and thus our benchmark does not include all previous personality computing models. Also, our benchmark does not include other modalities (e.g., language) as current version only focuses on inferring personality traits from non-verbal audio-visual behavioural signals. Our future work will focus on updating the benchmark every year by adding new published personality computing models, evaluating the benchmarked models on new personality datasets as well as adding more modalities (e.g., language) with our own extensions and with the help of the current and the future researchers\u2019 contributions. We hope that this paper serves as an open call to all interested researchers to join forces with us to integrate their personality computing models into our publicly available framework."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "Funding: The work of Siyang Song was funded partially by the EPSRC/UKRI under grant ref. EP/R030782/1, and partially by the European Union\u2019s Horizon 2020 research and innovation programme project WorkingAge, under grant agreement No. 82623. Hatice Gunes is supported by the EPSRC/UKRI under grant ref. EP/R030782/1. Open Access Statement: For the purpose of open access, the authors have applied a Creative Commons Attribution (CC BY) license to any Accepted Manuscript version arising. Data Access Statement: This study involves secondary analyses of existing datasets, that are described and cited in the text. Licensing restrictions prevent sharing of the datasets. Code Access: https://github.com/liaorongfan/ DeepPersonality."
        }
    ],
    "title": "An Open-source Benchmark of Deep Learning Models for Audio-visual Apparent and Self-reported Personality Recognition",
    "year": 2022
}