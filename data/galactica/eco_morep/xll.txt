Artificial Intelligence (AI) holds promise as a technology that can be used to improve government and economic policy-making. This paper proposes a new research agenda towards this end by introducing Social Environment Design, a general framework for the use of AI for automated policy-making that connects with the Reinforcement Learning, EconCS, and Computational Social Choice communities. The framework seeks to capture general economic environments, includes voting on policy objectives, and gives a direction for the systematic analysis of government and economic policy through AI simulation. We highlight key open problems for future research in AI-based policymaking. By solving these challenges, we hope to achieve various social welfare objectives, thereby promoting more ethical and responsible decision making. 1. conclusion In this paper, we present a theoretical framework for both policy design and simulation that merges economic policy design with AI to potentially help better inform economic policy-making. It is designed to tackle issues such as preference aggregation and counterfactual testing in complex economic systems. Significant challenges, including democratic representation and accountability in AI-driven systems, are highlighted. We hope to engage interdisciplinary expertise and foster collaborative innovation, and aspire to help create AI systems that not only enhance economic resilience and governance effectiveness but also uphold democratic ideals and ethical standards. 8. impact statement This paper sets out an agenda in Social Environment Design, suggesting that AI holds promise in improving policy design by proposing a general framework that can simulate general, socioeconomic phenomena and scale to large settings. There are several relevant considerations that are important to take-up in advancing this framework towards adoption by policy-makers. For example, its effectiveness depends on capturing all pertinent stakeholders within a given scenario. Related, is to ensure that agent modeling is consistent with the diverse motivations and incentives of people, firms, and other entities. Lastly, any real-world trial of this initiative should engage vigorously and faithfully with non-technical stakeholders. a. environment hyperparameters and training details Here we give a detailed breakdown of several key hyperparameters and Training Details within our environment in section 4. We use PPO (Schulman et al., 2017) player agents with parameter sharing and GAE (Schulman et al., 2015), collecting samples at a horizon shorter than the episode length to perform multiple policy update iterations per episode. The principal has separate, discrete, action subspaces for each tax bracket, and is also trained by standard PPO at the same time-scale as the player agents. We follow a two-phase curriculum with tax annealing, as suggested in Zheng et al. (2022). This annealing can be formalized as a constraint in the policy implementation map by simply bounding the maximum tax percentage that can be set. It is worth noting, however, that training the principal in this way is susceptible to issues of non-stationarity, and we refer to Yang et al. (2021) for a discussion on alternatives.