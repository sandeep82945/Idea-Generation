{
    "abstractText": "Safeguarding data from unauthorized exploitation is vital for privacy and security, especially in recent rampant research in security breach such as adversarial/membership attacks. To this end, unlearnable examples (UEs) have been recently proposed as a compelling protection, by adding imperceptible perturbation to data so that models trained on them cannot classify them accurately on original clean distribution. Unfortunately, we find UEs provide a false sense of security, because they cannot stop unauthorized users from utilizing other unprotected data to remove the protection, by turning unlearnable data into learnable again. Motivated by this observation, we formally define a new threat by introducing learnable unauthorized examples (LEs) which are UEs with their protection removed. The core of this approach is a novel purification process that projects UEs onto themanifold of LEs. This is realized by a new jointconditional diffusion model which denoises UEs conditioned on the pixel and perceptual similarity between UEs and LEs. Extensive experiments demonstrate that LE delivers state-of-the-art countering performance against both supervised UEs and unsupervised UEs in various scenarios, which is the first generalizable countermeasure to UEs across supervised learning and unsupervised learning. Our code is available at https://github.com/jiangw-0/LE_JCDP.",
    "authors": [
        {
            "affiliations": [],
            "name": "Wan Jiang"
        },
        {
            "affiliations": [],
            "name": "Yunfeng Diao"
        },
        {
            "affiliations": [],
            "name": "He Wang"
        },
        {
            "affiliations": [],
            "name": "Jianxin Sun"
        },
        {
            "affiliations": [],
            "name": "Meng Wang"
        },
        {
            "affiliations": [],
            "name": "Richang Hong"
        }
    ],
    "id": "SP:97fb80384e5944f7e190ec67e8d8b9e8708acd06",
    "references": [
        {
            "authors": [
                "Omri Avrahami",
                "Ohad Fried",
                "Dani Lischinski"
            ],
            "title": "Blended latent diffusion",
            "venue": "arXiv preprint arXiv:2206.02779",
            "year": 2022
        },
        {
            "authors": [
                "Abeba Birhane",
                "Vinay Uday Prabhu"
            ],
            "title": "Large image datasets: A pyrrhic win for computer vision",
            "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV)",
            "year": 2021
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In International conference on machine learning",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Haoqi Fan",
                "Ross Girshick",
                "Kaiming He"
            ],
            "title": "Improved baselines with momentum contrastive learning",
            "venue": "arXiv preprint arXiv:2003.04297",
            "year": 2020
        },
        {
            "authors": [
                "Terrance DeVries",
                "Graham W Taylor"
            ],
            "title": "Improved regularization of convolutional neural networks with cutout",
            "venue": "arXiv preprint arXiv:1708.04552",
            "year": 2017
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Hadi M Dolatabadi",
                "Sarah Erfani",
                "Christopher Leckie"
            ],
            "title": "The Devil\u2019s Advocate: Shattering the Illusion of Unexploitable Data using Diffusion Models",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Lijie Fan",
                "Sijia Liu",
                "Pin-Yu Chen",
                "Gaoyuan Zhang",
                "Chuang Gan"
            ],
            "title": "When does contrastive learning preserve adversarial robustness from pretraining to finetuning? Advances in neural information processing systems",
            "year": 2021
        },
        {
            "authors": [
                "Ji Feng",
                "Qi-Zhi Cai",
                "Zhi-Hua Zhou"
            ],
            "title": "Learning to confuse: generating training time adversarial data with auto-encoder",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Liam Fowl",
                "Micah Goldblum",
                "Ping-yeh Chiang",
                "Jonas Geiping",
                "Wojciech Czaja",
                "Tom Goldstein"
            ],
            "title": "Adversarial examples make strong poisons",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Shaopeng Fu",
                "Fengxiang He",
                "Yang Liu",
                "Li Shen",
                "Dacheng Tao"
            ],
            "title": "Robust Unlearnable Examples: Protecting Data Privacy Against Adversarial Learning",
            "venue": "In International Conference on Learning Representations. OpenReview.net",
            "year": 2022
        },
        {
            "authors": [
                "Robert Geirhos",
                "J\u00f6rn-Henrik Jacobsen",
                "Claudio Michaelis",
                "Richard Zemel",
                "Wieland Brendel",
                "Matthias Bethge",
                "Felix A Wichmann"
            ],
            "title": "Shortcut learning in deep neural networks",
            "venue": "Nature Machine Intelligence",
            "year": 2020
        },
        {
            "authors": [
                "Jean-Bastien Grill",
                "Florian Strub",
                "Florent Altch\u00e9",
                "Corentin Tallec",
                "Pierre Richemond",
                "Elena Buchatskaya",
                "Carl Doersch",
                "Bernardo Avila Pires",
                "Zhaohan Guo",
                "Mohammad Gheshlaghi Azar"
            ],
            "title": "Bootstrap your own latent-a new approach to self-supervised learning",
            "venue": "Advances in neural information processing systems",
            "year": 2020
        },
        {
            "authors": [
                "Yandong Guo",
                "Lei Zhang",
                "Yuxiao Hu",
                "Xiaodong He",
                "Jianfeng Gao"
            ],
            "title": "MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition",
            "venue": "In European Conference on Computer",
            "year": 2016
        },
        {
            "authors": [
                "Hao He",
                "Kaiwen Zha",
                "Dina Katabi"
            ],
            "title": "Indiscriminate Poisoning Attacks on Unsupervised Contrastive Learning",
            "venue": "In International Conference on Learning Representations",
            "year": 2023
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition",
            "year": 2016
        },
        {
            "authors": [
                "Philipp Henzler",
                "Jeremy Reizenstein",
                "Patrick Labatut",
                "Roman Shapovalov",
                "Tobias Ritschel",
                "Andrea Vedaldi",
                "David Novotny"
            ],
            "title": "Unsupervised learning of 3d object categories from videos in the wild",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2021
        },
        {
            "authors": [
                "Kashmir Hill",
                "Aaron Krolik"
            ],
            "title": "How photos of your kids are powering surveillance technology",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "In Proceedings of the 34th International Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Hanxun Huang",
                "Xingjun Ma",
                "Sarah Monazam Erfani",
                "James Bailey",
                "Yisen Wang"
            ],
            "title": "Unlearnable Examples: Making Personal Data Unexploitable",
            "venue": "In International Conference on Learning Representations. OpenReview.net",
            "year": 2021
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Sungbin Lim",
                "Ildoo Kim",
                "Taesup Kim",
                "Chiheon Kim",
                "Sungwoong Kim"
            ],
            "title": "2019. Fast AutoAugment",
            "venue": "In Proceedings of the 33rd International Conference on Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Zhuoran Liu",
                "Zhengyu Zhao",
                "andMartha Larson"
            ],
            "title": "Image Shortcut Squeezing: Countering Perturbative Availability Poisons with Compression",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
            "venue": "In International Conference on Learning Representations. OpenReview.net",
            "year": 2018
        },
        {
            "authors": [
                "Brandon B May",
                "N Joseph Tatro",
                "Piyush Kumar",
                "Nathan Shnidman"
            ],
            "title": "Salient Conditional Diffusion for Defending Against Backdoor Attacks",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Luis Mu\u00f1oz-Gonz\u00e1lez",
                "Battista Biggio",
                "Ambra Demontis",
                "Andrea Paudice",
                "Vasin Wongrassamee",
                "Emil C Lupu",
                "Fabio Roli"
            ],
            "title": "Towards poisoning of deep learning algorithms with back-gradient optimization",
            "venue": "In Proceedings of the 10th ACM workshop on artificial intelligence and security",
            "year": 2017
        },
        {
            "authors": [
                "Yuval Netzer",
                "Tao Wang",
                "Adam Coates",
                "Alessandro Bissacco",
                "Bo Wu",
                "Andrew Y Ng"
            ],
            "title": "Reading digits in natural images with unsupervised feature learning",
            "year": 2011
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal"
            ],
            "title": "Improved denoising diffusion probabilistic models",
            "venue": "In International Conference on Machine Learning",
            "year": 2021
        },
        {
            "authors": [
                "Weili Nie",
                "Brandon Guo",
                "Yujia Huang",
                "Chaowei Xiao",
                "Arash Vahdat",
                "Animashree Anandkumar"
            ],
            "title": "Diffusion Models for Adversarial Purification",
            "venue": "In International Conference on Machine Learning",
            "year": 2022
        },
        {
            "authors": [
                "Omkar M Parkhi",
                "Andrea Vedaldi",
                "Andrew Zisserman",
                "CV Jawahar"
            ],
            "title": "Cats and dogs",
            "venue": "In 2012 IEEE conference on computer vision and pattern recognition",
            "year": 2012
        },
        {
            "authors": [
                "Konpat Preechakul",
                "Nattanat Chatthee",
                "Suttisak Wizadwongsa",
                "Supasorn Suwajanakorn"
            ],
            "title": "Diffusion autoencoders: Toward a meaningful and decodable representation",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Tianrui Qin",
                "Xitong Gao",
                "Juanjuan Zhao",
                "Kejiang Ye",
                "Cheng-Zhong Xu"
            ],
            "title": "Learning the Unlearnable: Adversarial Augmentations Suppress Unlearnable Example Attacks",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Miguel A Ramirez",
                "Song-Kyoo Kim",
                "Hussam Al Hamadi",
                "Ernesto Damiani",
                "Young- Ji Byon",
                "Tae-Yeon Kim",
                "Chung-Suk Cho",
                "Chan Yeob Yeun"
            ],
            "title": "Poisoning attacks and defenses on artificial intelligence: A survey",
            "year": 2022
        },
        {
            "authors": [
                "Jie Ren",
                "Han Xu",
                "Yuxuan Wan",
                "Xingjun Ma",
                "Lichao Sun",
                "Jiliang Tang"
            ],
            "title": "Transferable Unlearnable Examples",
            "venue": "In International Conference on Learning Representations",
            "year": 2023
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "High-resolution image synthesis with latent diffusion models",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Olga Russakovsky",
                "Jia Deng",
                "Hao Su",
                "Jonathan Krause",
                "Sanjeev Satheesh",
                "Sean Ma",
                "Zhiheng Huang",
                "Andrej Karpathy",
                "Aditya Khosla",
                "Michael Bernstein"
            ],
            "title": "Imagenet large scale visual recognition challenge",
            "venue": "International journal of computer vision",
            "year": 2015
        },
        {
            "authors": [
                "Vinu Sankar Sadasivan",
                "Mahdi Soltanolkotabi",
                "Soheil Feizi"
            ],
            "title": "CUDA: Convolution-based Unlearnable Datasets",
            "venue": "arXiv e-prints (2023),",
            "year": 2023
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In International Conference on Machine Learning",
            "year": 2015
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P. Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
            "venue": "In 9th International Conference on Learning Representations. OpenReview.net",
            "year": 2021
        },
        {
            "authors": [
                "Jinyi Wang",
                "Zhaoyang Lyu",
                "Dahua Lin",
                "Bo Dai",
                "Hongfei Fu"
            ],
            "title": "Guided diffusion model for adversarial purification",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Yaxing Wang",
                "Chenshen Wu",
                "Luis Herranz",
                "Joost Van de Weijer",
                "Abel Gonzalez- Garcia",
                "Bogdan Raducanu"
            ],
            "title": "Transferring gans: generating images from limited data",
            "venue": "In European Conference on Computer",
            "year": 2018
        },
        {
            "authors": [
                "Zhirui Wang",
                "Yifei Wang",
                "Yisen Wang"
            ],
            "title": "Fooling adversarial training with inducing noise",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "Rui Wen",
                "Zhengyu Zhao",
                "Zhuoran Liu",
                "Michael Backes",
                "Tianhao Wang",
                "Yang Zhang"
            ],
            "title": "Is Adversarial Training Really a Silver Bullet for Mitigating Data Poisoning",
            "venue": "In International Conference on Learning Representations",
            "year": 2023
        },
        {
            "authors": [
                "Eric Wong",
                "Leslie Rice",
                "J Zico Kolter"
            ],
            "title": "Fast is better than free: Revisiting adversarial training",
            "venue": "In International Conference on Learning Representations. OpenReview.net",
            "year": 2020
        },
        {
            "authors": [
                "Shutong Wu",
                "Sizhe Chen",
                "Cihang Xie",
                "Xiaolin Huang"
            ],
            "title": "One-pixel shortcut: on the learning preference of deep neural networks",
            "venue": "In International Conference on Learning Representations",
            "year": 2023
        },
        {
            "authors": [
                "Da Yu",
                "Huishuai Zhang",
                "Wei Chen",
                "Jian Yin",
                "Tie-Yan Liu"
            ],
            "title": "Availability attacks create shortcuts",
            "venue": "In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data",
            "year": 2022
        },
        {
            "authors": [
                "Chia-Hung Yuan",
                "Shan-HungWu"
            ],
            "title": "Neural tangent generalization attacks",
            "venue": "In International Conference on Machine Learning",
            "year": 2021
        },
        {
            "authors": [
                "Sangdoo Yun",
                "Dongyoon Han",
                "Seong Joon Oh",
                "Sanghyuk Chun",
                "Junsuk Choe",
                "Youngjoon Yoo"
            ],
            "title": "Cutmix: Regularization strategy to train strong classifiers with localizable features",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision",
            "year": 2019
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Cisse",
                "Yann N Dauphin",
                "David Lopez-Paz"
            ],
            "title": "mixup: Beyond Empirical RiskMinimization",
            "venue": "In International Conference on Learning Representations. MM \u201923, October 29\u2013November",
            "year": 2018
        },
        {
            "authors": [
                "Jiaming Zhang",
                "Xingjun Ma",
                "Qi Yi",
                "Jitao Sang",
                "Yugang Jiang",
                "Yaowei Wang",
                "Changsheng Xu"
            ],
            "title": "Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Richard Zhang",
                "Phillip Isola",
                "Alexei A Efros",
                "Eli Shechtman",
                "Oliver Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "venue": "In IEEE conference on computer vision and pattern recognition",
            "year": 2018
        },
        {
            "authors": [
                "Yunqing Zhao",
                "Henghui Ding",
                "Houjing Huang",
                "Ngai-Man Cheung"
            ],
            "title": "A closer look at few-shot image generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "CCS CONCEPTS \u2022 Security and privacy\u2192 Human and societal aspects of security and privacy; \u2022 Computing methodologies\u2192Machine learning.\nKEYWORDS Unlearnable Examples, Data Protection, Deep Neural Network\n\u2217Both authors contributed equally to this research. \u2020Corresponding authors. E-mails: diaoyunfeng@hfut.edu.cn, he_wang@ucl.ac.uk\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. MM \u201923, October 29\u2013November 3, 2023, Ottawa, ON, Canada. \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0108-5/23/10. . . $15.00 https://doi.org/10.1145/3581783.3611833\nACM Reference Format: Wan Jiang, Yunfeng Diao, He Wang, Jianxin Sun, Meng Wang, and Richang Hong. 2023. Unlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples. In Proceedings of the 31st ACM International Conference on Multimedia (MM \u201923), October 29\u2013 November 3, 2023, Ottawa, ON, Canada. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3581783.3611833"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "The abundance of \u201cfreely\u201d accessible data on the Web has been pivotal to the success of modern deep learning, such as ImageNet [37] and Ms-celeb-1m [15]. However, these datasets might include personal data collected without mutual consent [2], which has raised public concerns that private data can be utilized to create commercial models without the owner\u2019s authorization [19]. To address such concerns, growing efforts [11, 21, 47] have been made to add protection to data to prevent unauthorized usage by making the data unexploitable. These methods add imperceptible \u201cshortcut\u201d noise to the images so that the deep learning models learn no useful semantics but correspondences between noise and labels [12]. Consequently, the models trained on unexploitable data fail to classify clean data, thereby safeguarding users\u2019 privacy. Such poisoning methods are named as unlearnable example (UE) protection [21] or availability attack [47].\nWhile the growing research focuses on how to make data unexploitable [10, 16, 35, 38, 43, 51], we aim to challenge this paradigm by exposing a key vulnerability in this protection: the protection is merely effective if the unexploitable data is all that is accessible. Unfortunately, this is often not the case. Data protectors can only add the \u201cunlearnable\u201d perturbations to their own data, but they cannot prevent unauthorized users from accessing similar, unprotected data from other sources. As a result, one can study the underlying distribution of the protected examples, via studying similar newly collected (unprotected) data. Taking face recognition as an example, although unlearnable examples cannot be directly used to train classifiers, it is easy to collect new unprotected face data. As long as there is sufficient similarity between the newly collected (unprotected) data and the original clean data, it is still possible to train a classifier that can successfully classify the original clean data. In other words, unauthorized users can easily bypass data protection to learn the original data representation from newly collected unprotected data, even if the data might be small in scale, different from the clean data, lacks label annotation, and is alone not ideal for training a classifier [42, 53]. ar X iv :2\n30 5.\n09 24\n1v 5\n[ cs\n.L G\n] 3\nO ct\n2 02\n3"
        },
        {
            "heading": "MM \u201923, October 29\u2013November 3, 2023, Ottawa, ON, Canada. Wan Jiang et al.",
            "text": "To show the existence of the aforementioned vulnerability, we design a new approach that can turn unlearnable examples into learnable ones. A straightforward solution would be to design a specific training scheme that can train on unexploitable data [24, 25, 33]. This is less ideal as it merely classifies unexploitable data but not reveal much about the underlying clean data, i.e. unprotected version of the unlearnable data. We argue that an ultimate countermeasure is to infer/expose the underlying clean data by turning UEs into learnable again, which can enable further unauthorized exploitation such as standard training or representation learning [16, 35]. Therefore, the learnable unauthorized data should be independent of training scheme and can be normally used just like original training data. We refer to examples in learnable unauthorized data as learnable examples (LEs). The key idea behind obtaining learnable examples is to learn a learnable data manifold from other similar data and then project unlearnable examples onto this manifold.\nInspired by the power of the diffusion models in noise purification [30] and image generation [6], we propose a novel purification method based on diffusion models, called joint-conditional diffusion purification, to capture the mapping from the unlearnable examples to their corresponding clean samples. We first inject the unlearnable images with controlled amounts of Gaussian noises progressively, until their unlearnable perturbations are submerged by Gaussian noise. Next, we equip the denoising process with a new joint condition that speeds up noise removal while preserving image semantics. The joint condition is parameterized by both the pixel distance and the neural perception distance between the unlearnable sample and its corresponding denoised version. This is based on the observation that unlearnable examples typically exhibit small differences in pixel distance from clean samples and the differences are imperceptible to the human vision. Therefore, the denoised images should closely resemble the original samples through minimizing the visual difference from the unlearnable example.\nWe extensively evaluate our approach on both supervised and unsupervised UEs across a number of benchmark datasets, and compare it with existing countering methods. The results show LE substantially outperforms existing countermeasures and it is the only one that maintains effectiveness under both supervised learning and unsupervised learning. More importantly, unlike existing countermeasures that are tied to specific training schemes, our learnable examples are independent of them and can be used normally as the original clean training data. Surprisingly, we found that our approach still retains effectiveness even when there is a large distributional difference between the newly collected data (utilized in training a learnable data manifold) and the clean data. In other words, the distributions between training data and collected raw data can be different and we can still turn unlearnable examples into learnable. This undoubtedly further deepens our concerns about the vulnerability of unexploitable data since it does not require the collected raw data to be very similar to the unprotected version of the unlearnable examples.\nIn summary, our main contributions are: 1) We identify and demonstrate an inherent vulnerability of UE protection, by formally defining an ultimate threat to UEs called learnable examples, which can turn unlearnable examples into learnable ones. 2) We\npropose a novel purification strategy for producing learnable examples, called Joint-conditional Diffusion Purification, which purifies UEs with a diffusion model simultaneously conditioned on pixel and perceptual similarity. 3) We demonstrate that LE outperforms existing state-of-the-art countermeasures against both supervised UEs and unsupervised UEs. LE is the first generalizable countermeasure across supervised learning and unsupervised learning. 4) We empirically demonstrate that the joint-conditional diffusion model can still purify UEs even when the learned density is not the same as the clean distribution, exposing the fragility of protection by UEs."
        },
        {
            "heading": "2 RELATEDWORK",
            "text": ""
        },
        {
            "heading": "2.1 Unlearnable Examples",
            "text": "Unlearnable examples (UEs) are a type of poisoning methods [34], but aimed at defending against unauthorized data exploitation [21, 27]. The vanilla UEs define a bi-level optimization objective, which makes the optimal solution on UEs have a maximum loss on clean data [9, 21, 48]. Given its effectiveness and efficiency, many variants of UEs have been proposed, such as robust UEs [11, 44], manually designed UEs [38, 47], clustering-based UEs [51], and sparse UEs [46]. Inspired by the effectiveness of UEs in supervised learning, He et al. [8] and Ren et al. [35] investigate the impact of UEs on unsupervised learning.\nCountermeasures against UEs have only been attempted very recently [7, 24, 33]. Adversarial Training (AT) [21] has been shown to partially resist UE protection, but robust UE soon broke through this countermeasure [11, 44]. Adversarial augmentation [33], which combines various data augmentation policies with adversarial training, is further proposed to improve generalization of the unauthorizedmodel. Liu et al. [24] suggest using grayscale transformation to counter UE protection. However, these methods are associated with specific training schemes, which limits the use of unauthorized data for other training schemes and tasks. The recent arXiv paper [7] applies diffusion models to counter UEs. The major differences to our approach are that we propose a new joint-conditional diffusion model instead of a naive application of the diffusion model, tackling the trade-off between perturbation purification and image semantic retaining. Furthermore, all prior countermeasures [7, 21, 24, 33] are designed for supervised learning, but not much is known about the fragility of unsupervised UEs."
        },
        {
            "heading": "2.2 Diffusion Models",
            "text": "Diffusion models [20, 39] have surpassed Generative Adversarial Networks (GANs) [13] in the field of image generation and have achieved impressive results with conditions such as text, semantic maps or reference images [1, 32, 36]. A typical diffusion model consists of two processes: a forward process and a denoising process. The former gradually approaches Gaussian noise by iteratively adding noise to clean images, while the latter obtains real images from noise in the form of Markov chains. Since a carefully designed denoising process can defuse the ramifications of data perturbations, recent works [7, 26, 30, 41] leverage the power of diffusion models for noise purification, such as adversarial purification. However, the key difference between existing noise purification and our\nUnlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples MM \u201923, October 29\u2013November 3, 2023, Ottawa, ON, Canada.\nwork is the accessibility of training data. Existing purification methods [7, 30] assume that training data (the unprotected version of the unlearnable examples) is available for training, allowing diffusion models to learn the original data distribution in advance. However, UE makes the training data unexploitable. How to train a diffusion model(and more generally learning a similar data manifold) without access to training data poses a crucial challenge for removing UE protection. Existing purification methods have yet to explore this aspect."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": ""
        },
        {
            "heading": "3.1 Problem Statement",
            "text": "In this subsection, we first give the definition of UEs, and then define the typical countermeasures against UE protection.\n3.1.1 Data Protector. Suppose that the data protectors have access to the original dataset and it is denoted as D\ud835\udc50 = {(\ud835\udc99 (\ud835\udc56 ) , \ud835\udc66 (\ud835\udc56 ) )}\ud835\udc41\ud835\udc56=1 with\ud835\udc41 clean samples, in which i.i.d. input-label pairs (\ud835\udc99 (\ud835\udc56 ) , \ud835\udc66 (\ud835\udc56 ) ) are drawn from the joint data distribution \ud835\udc5d\ud835\udc51 (\ud835\udc99, \ud835\udc66). The protectors\u2019 goal is to protect the data from unauthorized training after its release. To this end, they release an unlearnable datasetD\ud835\udc62 = {(?\u0303? (\ud835\udc56 ) , \ud835\udc66 (\ud835\udc56 ) )}\ud835\udc41\ud835\udc56=1 to the unauthorized users. The protector aims to make a classifier \ud835\udc53\ud835\udf03 : X \u2192 Y trained on the unlearnable dataset perform poorly on the original clean data distribution \ud835\udc5d\ud835\udc51 (\ud835\udc99, \ud835\udc66):\n\ud835\udf3d \u2217 = argmin \ud835\udf3d E(?\u0303?,\ud835\udc66) \u2208D\ud835\udc62 [L(\ud835\udc53\ud835\udf3d (?\u0303?), \ud835\udc66)] (1)\ns.t. \ud835\udc5d\ud835\udf3d \u2217 (\ud835\udc66 |\ud835\udc99) \u2260 \ud835\udc5d\ud835\udc51 (\ud835\udc66 |\ud835\udc99)\nwhere L(\u00b7) is the cross-entropy loss. Since unlearnable perturbations should not affect the normal data utility, it is assumed that ?\u0303? = \ud835\udc99 + \ud835\udf39 , where \ud835\udf39 is the \u201cinvisible\u201d unlearnable perturbations bounded by | |\ud835\udf39 | |\ud835\udc5d \u2264 \ud835\udf00.\n3.1.2 Unauthorized Data Exploiter. We assume the unauthorized parties only have access to the protected data, i.e. unlearnable examples set D\ud835\udc62 . Their goal is to train models on D\ud835\udc62 and make them generalize well on the original clean data distribution. To this end, existing countermeasures [21, 24, 33] have attempted to design a special training scheme that can train classifiers on unexploitable data. It has been shown that adversarial training (AT) [25] can be used to prevent UE protection to some extent [21], which is formulated as follows:\nargmin \ud835\udf3d E(?\u0303?,\ud835\udc66) \u2208D\ud835\udc62\n[ max\n| |\ud835\udf39\ud835\udc82\ud835\udc85\ud835\udc97 | |\ud835\udc5d\u2264\ud835\udf00 L(\ud835\udc53\ud835\udf3d (?\u0303? + \ud835\udf39\ud835\udc4e\ud835\udc51\ud835\udc63), \ud835\udc66)\n] (2)\nInspired by AT, adversarial augmentations (AA) [33] is further proposed very recently. Specially, they combine data augmentation policies with adversarial training:\nargmin \ud835\udf3d E(?\u0303?,\ud835\udc66) \u2208D\ud835\udc62 [ max T\u223cA L(\ud835\udc53\ud835\udf3d (T (?\u0303?), \ud835\udc66) ]\n(3)\nwhere T (\u00b7) is the combination of image augmentation policies from a set of all possible data augmentations A. However, ATbased methods suffer from a significant performance drop when training on robust unexploitable data [11]. Moreover, AT modifies the training schemes, leaving the unauthorized data itself still unexploitable. This forces the training data to be tied to adversarial training, which limits the use of data for other training schemes\nand tasks, such as standard training and representation learning. Last but not least, using AT to train a large model on large-scale unexploitable datasets is not desirable, given the computational complexity involved [45]."
        },
        {
            "heading": "3.2 Learnable Examples",
            "text": "Existing countering methods rely on specific training schemes. However, we argue that an ultimate threat against UEs should turn unexploitable data D\ud835\udc62 into a learnable dataset D\ud835\udc59 . D\ud835\udc59 can be used normally as the original clean training data, enabling further unauthorized exploitation such as standard straining and representation learning. Here we consider standard setting in supervised learning as an example, a model trained on D\ud835\udc59 can easily generalize well on the original clean data distribution \ud835\udc5d\ud835\udc51 (\ud835\udc99, \ud835\udc66):\n\ud835\udf3d \u2217 = argmin \ud835\udf3d E(\ud835\udc99,\ud835\udc66) \u2208D\ud835\udc59 [L(\ud835\udc53\ud835\udf3d (\ud835\udc99), \ud835\udc66)] (4)\ns.t. D\ud835\udc59 = denoise(D\ud835\udc62 ); \ud835\udc5d\ud835\udf3d \u2217 (\ud835\udc66 |\ud835\udc99) = \ud835\udc5d\ud835\udc51 (\ud835\udc66 |\ud835\udc99)\nWe refer to learnable protected examples in D\ud835\udc59 as learnable examples (LEs). Note that LEs are independent of training schemes, hence they also can be used for unauthorized unsupervised learning. Given a good denoiser denoise, we can project unlearnable examples back to the learnable data manifold to obtain corresponding LEs. A good denoiser is often achieved by learning a generative model \ud835\udc3a [30]. However, how to train a generator without access to the original training data is a tricky challenge. Our key observation is that although the original training data D\ud835\udc50 is not accessible, small-scale raw (unprotected) data without labeled annotation D\ud835\udc5f can be easily collected in the wild [18]. As long as there is sufficient similarity between D\ud835\udc5f and D\ud835\udc62 , we can learn an unconditional generator \ud835\udc3a from D\ud835\udc5f and utilize it to project UEs onto the manifold of LEs. Considering that diffusion models [20, 39] can achieve a high sample quality [6, 40] and noise purifying performance [30], we use a diffusion model for denoising. However, a naive application of the diffusion model will suffer the trade-off between noise purification and image semantic retaining. To tackle this problem, we propose a new joint-conditional diffusion purification conditioned on simultaneously measuring the pixel and perceptual similarity between UEs and corresponding denoised ones. An overview illustration is shown in Fig. 1. Next, we give details of the purification process."
        },
        {
            "heading": "3.3 Joint-conditional Diffusion Purification",
            "text": "3.3.1 DDPM for Data Purified. Diffusion model defines a Markov chain of diffusion steps to add Gaussian noise gradually to the data and then learn the reversal of the diffusion process to construct desired data samples from the noise. Given a clean data point \ud835\udc990 sampled from a data distribution \ud835\udc5e(\ud835\udc990), Denoising Diffusion Probabilistic Models (DDPM) [20] define a forward or diffusion process that follows Markov chain to gradually add Gaussian Noise to \ud835\udc990 in \ud835\udc47 steps with pre-defined variance schedule \ud835\udefd1:\ud835\udc47 \u2208 (0, 1)\ud835\udc47 :\n\ud835\udc5e(\ud835\udc991:\ud835\udc47 |\ud835\udc990) = \ud835\udc47\u220f \ud835\udc61=1 \ud835\udc5e(\ud835\udc99\ud835\udc61 |\ud835\udc99\ud835\udc61\u22121) (5)\n\ud835\udc5e(\ud835\udc99\ud835\udc61 |\ud835\udc99\ud835\udc61\u22121) = N(\ud835\udc99\ud835\udc61 ; \u221a\ufe01 1 \u2212 \ud835\udefd\ud835\udc61\ud835\udc99\ud835\udc61\u22121, \ud835\udefd\ud835\udc61 \ud835\udc70 )"
        },
        {
            "heading": "MM \u201923, October 29\u2013November 3, 2023, Ottawa, ON, Canada. Wan Jiang et al.",
            "text": "If we define \ud835\udefc\ud835\udc61 = 1 \u2212 \ud835\udefd\ud835\udc61 , \ud835\udefc\ud835\udc61 = \u220f\ud835\udc61\n\ud835\udc60=1 \ud835\udefc\ud835\udc60 , we can reformulate the diffuse process via a single step:\n\ud835\udc5e(\ud835\udc99\ud835\udc61 |\ud835\udc990) = N(\ud835\udc99\ud835\udc61 ; \u221a\ufe01 \ud835\udefc\ud835\udc61\ud835\udc990, (1 \u2212 \ud835\udefc\ud835\udc61 )\ud835\udc70 ) (6)\nThe reverse process is also a Markov process that learning a model \ud835\udc5d\ud835\udf11 to estimate these conditional probabilities \ud835\udc5e(\ud835\udc99\ud835\udc61\u22121 |\ud835\udc99\ud835\udc61 ):\n\ud835\udc5d\ud835\udf11 (\ud835\udc990:\ud835\udc47 ) = \ud835\udc5d (\ud835\udc99\ud835\udc47 ) \ud835\udc47\u220f \ud835\udc61=1 \ud835\udc5d\ud835\udf11 (x\ud835\udc61\u22121 |\ud835\udc99\ud835\udc61 ) (7)\n\ud835\udc5d\ud835\udf11 (\ud835\udc99\ud835\udc61\u22121 |\ud835\udc99\ud835\udc61 ) = N(\ud835\udc99\ud835\udc61\u22121; \ud835\udf41\ud835\udf11 (\ud835\udc99\ud835\udc61 , \ud835\udc61),\ud835\udf482\ud835\udc61 \ud835\udc70 )\nwhere the reverse process is started from \ud835\udc5d (\ud835\udc99\ud835\udc47 ) = N(\ud835\udc99\ud835\udc47 ; 0, \ud835\udc70 ). The mean \ud835\udf41\ud835\udf11 (\ud835\udc99\ud835\udc61 , \ud835\udc61) is a neural network parameterized by \ud835\udf11 , while the variance \ud835\udf482\ud835\udc61 can be learned by a neural network [29] or a set of time-dependent constants [20].\nNow we introduce the purification process via utilizing DDPM. Assume that we have trained a DDPM model on collected raw data D\ud835\udc5f . Suppose the unlearnable example ?\u0303?0 = \ud835\udc990 + \ud835\udf39 , we first diffuse the unlearnable image for \ud835\udc47\ud835\udc5d steps by adding Gaussian noise to submerge the unlearnable perturbation:\n?\u0303?\ud835\udc47\ud835\udc5d = \u221a\ufe03 \ud835\udefc\ud835\udc47\ud835\udc5d\ud835\udc990 + \u221a\ufe03 \ud835\udefc\ud835\udc47\ud835\udc5d\ud835\udf39 + \u221a\ufe03 1 \u2212 \ud835\udefc\ud835\udc47\ud835\udc5d \ud835\udf50 (8)\nwhere \ud835\udf50 is a standard Gaussian noise. The reverse process aims to simultaneously mitigate the added Gaussian noise and the residual unlearnable perturbation in ?\u0303?\ud835\udc47\ud835\udc5d :\n?\u0303?\u2217\ud835\udc61\u22121 = 1 \u221a \ud835\udefc\ud835\udc61 (?\u0303?\u2217\ud835\udc61 \u2212 1 \u2212 \ud835\udefc\ud835\udc61\u221a 1 \u2212 \ud835\udefc\ud835\udc61 \ud835\udf50\ud835\udf11 (?\u0303?\u2217\ud835\udc61 , \ud835\udc61)) + \ud835\udf48\ud835\udc61\ud835\udf50\ud835\udc61 (9)\nwhere ?\u0303?\u2217 \ud835\udc47\ud835\udc5d\n= ?\u0303?\ud835\udc47\ud835\udc5d . During the purification process described above, selecting the optimal purification step \ud835\udc47\ud835\udc5d is crucial. If \ud835\udc47\ud835\udc5d is too small, the unlearnable perturbation term \u221a\ufe03 \ud835\udefc\ud835\udc47\ud835\udc5d\ud835\udf39 cannot be fully submerged by Gaussian noise, while choosing too large\ud835\udc47\ud835\udc5d will lead\nto a loss of original semantic information. In our preliminary experiments, we have found that iteratively purifying the unlearnable images multiple times can mitigate this problem to some extent. Choosing a relatively small\ud835\udc47\ud835\udc5d to conduct multiple purification iterations is more effective than purifying once with a large \ud835\udc47\ud835\udc5d . The detailed iteration process is listed in Algorithm 1.\n3.3.2 Joint-conditional Diffusion Model. To further eliminate the trade-off between purification strength and semantic content retaining, we propose a novel purification process based on diffusion model, called Joint-conditional Diffusion Purification (JCDP). JCDP leverages both pixel and perception distance guidance to enable joint control on low-level and high-level semantic similarity between the purified image to the original clean one. From the perspective of the unauthorized parties, the original clean training data is not known a priori, we hence turn to retain consistency between the purified image and the unlearnable one during the reverse process. This approximation is reasonable because unlearnable example by definition is imperceptible to humans, and its unlearnable perturbation is constrained to a small \ud835\udf16-ball by pixel distance. Therefore, the purified image will closely resemble its original clean sample by encouraging it to be visually close to the unlearnable example.\nThe pixel distance can be easily calculated by mean square error (MSE). However, the true perception distance cannot be directly computed for image data. Considering that perceptual similarity can be intuitively linked to deep visual representation [52], we propose to use neural perception distance LPIPS [52] to approximate the true perception distance. We introduce an extra neural network \u210e(\u00b7) and denote its generated feature embeddings as \u03a6(\ud835\udc99). Next, according to condition this reverse process \ud835\udc5d\ud835\udf11 (\ud835\udc99\ud835\udc61\u22121 |\ud835\udc99\ud835\udc61 ) in Eq. (7) on ?\u0303?\ud835\udc61 and \u03a6(?\u0303?\ud835\udc61 ), we can obtain the denoised learnable sample from\nUnlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples MM \u201923, October 29\u2013November 3, 2023, Ottawa, ON, Canada.\nthe joint conditional distribution \ud835\udc5d\ud835\udf11 (?\u0303?\u22170:\ud835\udc47\ud835\udc5d |?\u0303?,\u03a6(?\u0303?)):\n\ud835\udc5d\ud835\udf11 (?\u0303?\u22170:\ud835\udc47\ud835\udc5d |?\u0303?,\u03a6(?\u0303?)) = \ud835\udc5d (?\u0303? \u2217 \ud835\udc47\ud835\udc5d ) \ud835\udc47\u220f \ud835\udc61=1 \ud835\udc5d\ud835\udf11 (?\u0303?\u2217\ud835\udc61\u22121 |?\u0303? \u2217 \ud835\udc61 , ?\u0303?\ud835\udc61 ,\u03a6(?\u0303?\ud835\udc61 )) (10)\nwhere ?\u0303?\ud835\udc61 is the noise version of ?\u0303?0 via \ud835\udc61-step diffusion. Using Bayes\u2019 rule, we can derive the joint conditional reverse process:\n\ud835\udc5d\ud835\udf11 (?\u0303?\u2217\ud835\udc61\u22121 |?\u0303? \u2217 \ud835\udc61 , ?\u0303?\ud835\udc61 ,\u03a6(?\u0303?\ud835\udc61 ))\n= \ud835\udc5d (\u03a6(?\u0303?\ud835\udc61 ) |?\u0303?\u2217\ud835\udc61\u22121, ?\u0303? \u2217 \ud835\udc61 , ?\u0303?\ud835\udc61 )\ud835\udc5d\ud835\udf11 (?\u0303?\u2217\ud835\udc61\u22121 |\ud835\udc99 \u2217 \ud835\udc61 , ?\u0303?\ud835\udc61 )\n\ud835\udc5d (\u03a6(?\u0303?\ud835\udc61 ) |?\u0303?\u2217\ud835\udc61 , ?\u0303?\ud835\udc61 )\n= \ud835\udc5d (?\u0303?\u2217\ud835\udc61 |?\u0303?\u2217\ud835\udc61\u22121, ?\u0303?\ud835\udc61 ,\u03a6(?\u0303?\ud835\udc61 )) \ud835\udc5d (\u03a6(?\u0303?\ud835\udc61 ) |?\u0303?\u2217\ud835\udc61\u22121, ?\u0303?\ud835\udc61 ) \ud835\udc5d (?\u0303?\u2217\ud835\udc61 |?\u0303?\u2217\ud835\udc61\u22121, ?\u0303?\ud835\udc61 ) \ud835\udc5d\ud835\udf11 (?\u0303?\u2217\ud835\udc61\u22121 |?\u0303? \u2217 \ud835\udc61 , ?\u0303?\ud835\udc61 ) \ud835\udc5d (\u03a6(?\u0303?\ud835\udc61 ) |?\u0303?\u2217\ud835\udc61 , ?\u0303?\ud835\udc61 )\n(11)\nSince the diffuse process of DDPM is a Markov process, we have:\n\ud835\udc5d\ud835\udf11 (?\u0303?\u2217\ud835\udc61\u22121 |?\u0303? \u2217 \ud835\udc61 , ?\u0303?\ud835\udc61 ,\u03a6(?\u0303?\ud835\udc61 ))=\n\ud835\udc5d (\u03a6(?\u0303?\ud835\udc61 ) |?\u0303?\u2217\ud835\udc61\u22121, ?\u0303?\ud835\udc61 )\ud835\udc5d\ud835\udf11 (?\u0303? \u2217 \ud835\udc61\u22121 |?\u0303? \u2217 \ud835\udc61 , ?\u0303?\ud835\udc61 )\n\ud835\udc5d (\u03a6(?\u0303?\ud835\udc61 ) |?\u0303?\u2217\ud835\udc61 , ?\u0303?\ud835\udc61 ) (12)\nthen we take the logarithm of both sides\nlog \ud835\udc5d\ud835\udf11 (?\u0303?\u2217\ud835\udc61\u22121 |?\u0303? \u2217 \ud835\udc61 , ?\u0303?\ud835\udc61 ,\u03a6(?\u0303?\ud835\udc61 )) = log \ud835\udc5d (\u03a6(?\u0303?\ud835\udc61 ) |?\u0303?\u2217\ud835\udc61\u22121, ?\u0303?\ud835\udc61 ) + log \ud835\udc5d\ud835\udf11 (?\u0303?\u2217\ud835\udc61\u22121 |?\u0303? \u2217 \ud835\udc61 , ?\u0303?\ud835\udc61 ) \u2212 log \ud835\udc5d (\u03a6(?\u0303?\ud835\udc61 ) |?\u0303?\u2217\ud835\udc61 , ?\u0303?\ud835\udc61 ) (13) We can approximate log \ud835\udc5d (\u03a6(?\u0303?\ud835\udc61 ) |?\u0303?\u2217\ud835\udc61 , ?\u0303?\ud835\udc61 ) using a Taylor expansion around ?\u0303?\u2217\ud835\udc61 = ?\u0303? \u2217 \ud835\udc61\u22121. This gives\nlog \ud835\udc5d\ud835\udf11 (?\u0303?\u2217\ud835\udc61\u22121 |?\u0303? \u2217 \ud835\udc61 , ?\u0303?\ud835\udc61 ,\u03a6(?\u0303?\ud835\udc61 )) \u2248 (?\u0303?\u2217\ud835\udc61\u22121 \u2212 ?\u0303? \u2217 \ud835\udc61 )\u2207?\u0303?\u2217\ud835\udc61 log \ud835\udc5d (\u03a6(?\u0303?\ud835\udc61 ) |?\u0303? \u2217 \ud835\udc61 , ?\u0303?\ud835\udc61 ) + log \ud835\udc5d\ud835\udf11 (?\u0303?\u2217\ud835\udc61\u22121 |?\u0303? \u2217 \ud835\udc61 , ?\u0303?\ud835\udc61 ) (14)\nAccording to work [39] and [6], it is proved that:\n\ud835\udc5d\ud835\udf11 (?\u0303?\u2217\ud835\udc61\u22121 |?\u0303? \u2217 \ud835\udc61 , ?\u0303?\ud835\udc61 )\n\u2248 N(?\u0303?\u2217\ud835\udc61\u22121; \ud835\udf41\ud835\udf11 (?\u0303? \u2217 \ud835\udc61 , \ud835\udc61) + \ud835\udf482\ud835\udc61\u2207?\u0303?\u2217\ud835\udc61 log\ud835\udc5d (?\u0303?\ud835\udc61 |?\u0303? \u2217 \ud835\udc61 ),\ud835\udf482\ud835\udc61 \ud835\udc70 )\n(15)\nthen we can show that \ud835\udc5d\ud835\udf11 (?\u0303?\u2217\ud835\udc61\u22121 |?\u0303? \u2217 \ud835\udc61 , ?\u0303?\ud835\udc61 ,\u03a6(?\u0303?\ud835\udc61 ))\n\u2248 N(?\u0303?\u2217\ud835\udc61\u22121; \ud835\udf41\ud835\udf11 (?\u0303? \u2217 \ud835\udc61 , \ud835\udc61) + \ud835\udf482\ud835\udc61 (\ud835\udc851 + \ud835\udc852),\ud835\udf482\ud835\udc61 \ud835\udc70 )\n\ud835\udc851 = \u2207?\u0303?\u2217\ud835\udc61 log \ud835\udc5d (?\u0303?\ud835\udc61 |?\u0303? \u2217 \ud835\udc61 ), \ud835\udc852 = \u2207?\u0303?\u2217\ud835\udc61 log \ud835\udc5d (\u03a6(?\u0303?\ud835\udc61 ) |?\u0303? \u2217 \ud835\udc61 , ?\u0303?\ud835\udc61 )\n(16)\nIn the above equation, \ud835\udc5d (?\u0303?\ud835\udc61 |?\u0303?\u2217\ud835\udc61 ) and \ud835\udc5d (\u03a6(?\u0303?\ud835\udc61 ) |?\u0303?\u2217\ud835\udc61 , ?\u0303?\ud835\udc61 ) represent how likely ?\u0303?\u2217\ud835\udc61 is close to ?\u0303?\ud835\udc61 under the data space and latent feature space in the reverse process. Following [41], we can utilize MSE as the distance metric D\ud835\udc8e to approximate \ud835\udc5d (?\u0303?\ud835\udc61 |?\u0303?\u2217\ud835\udc61 ):\n\ud835\udc5d (?\u0303?\ud835\udc61 |?\u0303?\u2217\ud835\udc61 ) = 1 \ud835\udc81 \ud835\udc52\ud835\udc65\ud835\udc5d (\ud835\udf061D\ud835\udc8e (?\u0303?\u2217\ud835\udc61 , ?\u0303?\ud835\udc61 )) (17)\nwhere \ud835\udc81 is a normalization factor and \ud835\udf061 is a scale factor that modulates the guidance strength. From Eq. (17) we have\n\ud835\udc851 = \u2207?\u0303?\u2217\ud835\udc61 log\ud835\udc5d (?\u0303?\ud835\udc61 |?\u0303? \u2217 \ud835\udc61 ) = \u2212\ud835\udf061\u2207?\u0303?\u2217\ud835\udc61 D\ud835\udc8e (?\u0303? \u2217 \ud835\udc61 , ?\u0303?\ud835\udc61 ) (18)\nSimilarly, we quantify \ud835\udc5d (\u03a6(?\u0303?\ud835\udc61 ) |\ud835\udc99\ud835\udc61 ) by adapting LPIPS as the perception distance metric D\ud835\udc91 :\n\ud835\udc5d (\u03a6(?\u0303?\ud835\udc61 ) |?\u0303?\u2217\ud835\udc61 , ?\u0303?\ud835\udc61 ) = 1 \ud835\udc81 \ud835\udc52\ud835\udc65\ud835\udc5d (\ud835\udf062D\ud835\udc91 (?\u0303?\u2217\ud835\udc61 , ?\u0303?\ud835\udc61 )) (19)\nD\ud835\udc91 (?\u0303?\u2217\ud835\udc61 , ?\u0303?\ud835\udc61 ) = | |\u03a6(?\u0303?\u2217\ud835\udc61 ) \u2212 \u03a6(?\u0303?\ud835\udc61 ) | |2 Next, we can calculate its gradient\n\ud835\udc852 = \u2207?\u0303?\u2217\ud835\udc61 log \ud835\udc5d (\u03a6(?\u0303?\ud835\udc61 ) |?\u0303? \u2217 \ud835\udc61 , ?\u0303?\ud835\udc61 ) = \u2212\ud835\udf062\u2207?\u0303?\u2217\ud835\udc61 D\ud835\udc91 (?\u0303? \u2217 \ud835\udc61 , ?\u0303?\ud835\udc61 ) (20)\nBased on the inference above, the conditional transition operator \ud835\udc5d\ud835\udf11 (?\u0303?\u2217\ud835\udc61\u22121 |?\u0303? \u2217 \ud835\udc61 , ?\u0303?\ud835\udc61 ,\u03a6(?\u0303?\ud835\udc61 )) can be approximated by a Gaussian distribution, whose mean is shifted by \ud835\udf482\ud835\udc61 (\ud835\udc851 + \ud835\udc852). Algorithm 1 outlines the inference details of JCDP.\nAlgorithm 1: Joint-conditional Diffusion Purification Input: Unlearnable example ?\u0303?0, diffusion step \ud835\udc47\ud835\udc5d per each\npurification run, number of purification iterations \ud835\udc41 , given a DDPM (\ud835\udf41\ud835\udf11 (\ud835\udc99\ud835\udc61 , \ud835\udc61),\ud835\udf482\ud835\udc61 \ud835\udc70 ), gradient scale \ud835\udf061 and \ud835\udf062.\n1 Init: \ud835\udc851 = 0 and \ud835\udc852 = 0; 2 for \ud835\udc56 \u2190 1 to \ud835\udc41 do 3 The diffusion process:\n4 \ud835\udc5e(?\u0303?1:\ud835\udc47\ud835\udc5d |?\u0303?0) = \u220f\ud835\udc47\ud835\udc5d\n\ud835\udc61=1 \ud835\udc5e(?\u0303?\ud835\udc61 |?\u0303?\ud835\udc61\u22121); 5 The reverse process: 6 for \ud835\udc61 \u2190 \ud835\udc47\ud835\udc5d to 1 do 7 \ud835\udf41,\ud835\udf482\ud835\udc61 \u2190 \ud835\udf41\ud835\udf11 (?\u0303?\u2217\ud835\udc61 , \ud835\udc61),\ud835\udf482\ud835\udc61 ; 8 \ud835\udc851 \u2190 \u2212\ud835\udf061\u2207?\u0303?\u2217\ud835\udc61 D\ud835\udc8e (?\u0303? \u2217 \ud835\udc61 , ?\u0303?\ud835\udc61 ); 9 \ud835\udc852 \u2190 \u2212\ud835\udf062\u2207?\u0303?\u2217\ud835\udc61 D\ud835\udc91 (?\u0303? \u2217 \ud835\udc61 , ?\u0303?\ud835\udc61 );\n10 ?\u0303?\u2217\ud835\udc61\u22121 \u2190 sample from N(\ud835\udf41 + \ud835\udf48 2 \ud835\udc61 (\ud835\udc851 + \ud835\udc852),\ud835\udf482\ud835\udc61 \ud835\udc70 )"
        },
        {
            "heading": "11 end",
            "text": ""
        },
        {
            "heading": "12 end",
            "text": ""
        },
        {
            "heading": "13 return ?\u0303?\u22170",
            "text": "3.3.3 Fine-tuning for Diffusion Model. We can directly train a diffusion model on newly collected (unprotected) data. However, since the size of the collected data is typically much smaller than the original training data, the generation quality may be low. Fortunately, it is almost impossible that data protectors add unlearnable perturbations to all data in the real world. Therefore, we can transfer knowledge from unprotected source domains to unauthorized target domains with limited collected data by means of fine-tuning. Specially, we first initialize DDPM with the weights of a source network pre-trained on one unprotected domain, and then fine-tune it on the newly collected data. This simple fine-tuning operation can shorten the convergence time and improve the purification performance, especially when collected raw data is limited."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 Settings",
            "text": "4.1.1 Datasets and Evaluation. We select three widely adopted benchmark datasets for LE evaluation: CIFAR-10 [22], CIFAR-100 [22], SVHN [28] and Pets [31]. CIFAR-10&100 consist of 50000 images in the training set and 10000 images in the test set. SVHN contains 73257 digit images for training and 26032 digit images for testing. Pets consists 3680 pet images for training and 3669 pet images for testing. To demonstrate the superiority of LE, we compare LE with AT [25] and the recently proposed supervised countermeasures, including ISS [24], AA [33] and AVATAR [7]. AT, ISS and AAmodify the training scheme. AVATAR uses pre-trained generative models to purify UEs and the used pre-trained models access the original training set. AA uses adversarial augmentation technology, which applies \ud835\udc3e different random augmentations for each image. For a"
        },
        {
            "heading": "MM \u201923, October 29\u2013November 3, 2023, Ottawa, ON, Canada. Wan Jiang et al.",
            "text": "fair comparison, we do not consider adding extra data in training stage so we set \ud835\udc3e = 1.\nTo demonstrate LE is a generalizable countering approach under supervised and unsupervised learning, we employ state-of-the-art supervised UE and unsupervised UE for generating unexploitable data. The supervised UE protections include Error-Minimization Noise (EMN) [21], Robust Error-Minimization Noise (REMN) [11], Linear-separable Synthetic Perturbations (LSP) [47] and Target Adversarial poisoning(TAP) [10]. For unsupervised UE, we employ Contrastive Poisoning (CP) [16] and Unlearnable Clusters (UC) [51]. Following their default setting, we generated EMN, REMN and TAP on the backbone ResNet-18 [17], and CP on three unsupervised backbones, including SimCLR [3], MoCo (v2) [4] and BYOL [14]. LSP is a manually designed UE so no backbone is required. UC is generated on backbone ResNet-18 base on surrogate model and evaluated via an self-supervised SimCLR. Following the evaluation protocol in recent countermeasures [24, 33], the perturbation radius is \ud835\udc59\u221e = 8/255 for EMN, REMN and TAP, \ud835\udc592 = 1.0 for LSP and \ud835\udc59\u221e = 16/255 for UC. To achieve the strongest poison performance, all unlearnable approaches have a 100% poisoning rate.\n4.1.2 Training Details. in the joint conditional diffusion purification process JCDP is a learning-free purification method, hence we need to train an unconditional DDPM from other unprotected data in advance. There are two settings for the collection of unprotected data: distribution match and distribution mismatch. In distribution match, we use the unprotected raw testing set (without label annotation), which belongs to the same distribution with the unprotected version of the unlearnable examples but without overlapping. Unless specified otherwise, we use it by default. The detailed settings of the distribution mismatch are reported in Sec. 4.4. ResNet-18 [17] is used as the default classifier to train on purified data. Please refer to supplementary document for the fuller training settings and implemental details."
        },
        {
            "heading": "4.2 Evaluation on Supervised UEs",
            "text": "We report the comparison results with the state-of-the-art countermeasures [7, 24, 25, 33] on all datasets in Tab. 1. First, our proposed method achieves the best test accuracy in all scenarios (countermeasures vs. datasets vs. UE methods). The only exception is REMN on CIFAR-10, where there is only a 0.6% difference with ISS. Secondly, it is apparent that other countering approaches exhibit the generalization problem across different UE methods. Specifically, AT\nand AA lead to a significant degradation in accuracy when countering REMN and LSP protection on CIFAR-10. LSP and TAP also proves to be more resilient against AVATAR and ISS than other UE methods. However, our method, LE, does not have these issues, consistently performing well across all UE protection, demonstrating its effectiveness against unforeseen UE protection. This is because our approach aims to learn a good data representation that is independent of both UE and specific training schemes. Overall, existing countermeasures might not pose an ultimate threat to unexploitable data compared to LE. Next, we provide a detailed analysis.\nComparsion with AT Methods and Data Augmentation. AT [25] can be employed to counter UE protection. A recent study has shown that Adversarial Augmentation (AA) [33], which combines data augmentation with adversarial training, can further enhance the countering performance. However, this countermeasure heavily relies on the adversarial training procedure. The robust form of error-minimizing noise [11] can easily compromise their performance by replacing the normally trained surrogate in EMN with an adversarially trained model. As shown in Tab. 1, REMN can easily break AT and significantly degrade the performance of AA on CIFAR-10. In contrast, LE transfers well across different UE protections, whose performance on REMN is only slightly lower than EMN. Furthermore, the computational complexity of AT is typically higher than standard training. For LE, the unauthorized data only needs to be denoised once, then it can be used for training models in standard setting. Hence LE reduces training time by approximately 70% compared with AT-based methods. We report the consuming time of producing learnable data using LE in supplementary document. In addition, considering that data augmentation technology is another commonly used countermeasure [7, 24], we compare our approach with 4 common used data augmentations in Tab. 2. As shown in Tab. 2, LE significantly outperforms various data augmentations by a big margin.\nUnlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples MM \u201923, October 29\u2013November 3, 2023, Ottawa, ON, Canada.\nComparsion with Image Compression. Image Shortcut Squeezing (ISS) contains three common image compression operations, including grayscale transformation (GRAY), JPEG and bit depth reduction (BDR). We compare LE with every operation in ISS and the results are shown in Tab. 3. Different from LS showing consistent countering performance across all UE protections, the optimal compression operation in ISS varies across different types of UEs, e.g. JPEG is not effective against error-minimizing noise (EMN, REMN) while GRAY is not effective against patch-based linear separable perturbation (LSP). In order to get the optimal performance, ISS has to ensemble multiple models which are applied with different compressions. As a result, LE is more efficient than ISS and has the best overall performance.\nComparsion with Noise Purification. Similar to LE, AVATAR [7] also uses diffusion model for purifying unlearnable perturbation. However, the main experiments reported in [7] utilize the original clean training data for training diffusion models. In contrast, our approach does not need to have access to the unprotected version of the unlearnable examples (original training data), which is more available in real-world scenarios. In practice, the amount of newly collected (unprotected) data we used for training diffusion model is 10000 on CIFAR-10&CIFAR-100, and 26032 on SVHN, much smaller than the amount of data used by AVATAR (50000 clean training data on CIFAR-10&100 and 73257 on SVHN). In addition, unlike a naive application of diffusion model in AVATAR, we propose a novel joint-conditional diffusionmethod to improve the purification performance. The results in Tab. 1 show our proposed method surpasses AVATAR under all UE scenarios, especially in countering against LSP. It is worth noting that the amount of data used by LE is only 1/5 of that used by AVATAR on CIFAR-10&100, and 1/3 on SVHN. We attribute the improvement to the new proposed joint-conditional diffusion model."
        },
        {
            "heading": "4.3 Evaluation on Unsupervised UEs",
            "text": "Unlike existing countering methods, which are only available in supervised learning, LE is capable of countering unsupervised UEs as well. Considering the unsupervised UEs Contrastive Poisoning (CP) [16] are designed for unsupervised contrastive learning (UCL), we evaluate the effectiveness of LE on three well-knownUCL frameworks, including SimCLR [3], MoCo (v2) [4] and BYOL [14]. Unlearnable Cluster (UC) [51] methods are not specifically designed\nfor unsupervised setting, but are robustness to unsupervised exploitation against SimCLR. We evaluate the effectiveness of LE on SimCLR for UC. As shown in Tab. 4, LE is consistently effective across various UCL algorithms. As there is no unsupervised countering method for direct comparison, we employ the contrastive learning version of AT (AdvCL) [8] and common data augmentations like Cutout [5] and Random Noise. We conduct comparisons using backbone SimCLR on CIFAR-10. The linear probing accuracy of AdvCL, Cutout and Random Noise is 79.3%, 47.7%, and 54.1% respectively, while LE achieves 86.6%, showing the superiority of LE by big margins in the context of unsupervised learning. To the best of our knowledge, LE is the first generalizable countermeasure that is effective against UEs in both supervised and unsupervised learning."
        },
        {
            "heading": "4.4 Investigating the Distribution Similarity",
            "text": "Although LE requires collecting raw unprotected data to learn a data manifold, in practice, we find collecting raw data is not difficult because the distributions of the newly collected data(surrogate distribution) can be different from the original clean distribution. To evaluate LE\u2019s tolerance to distribution mismatch, we propose to estimate the scale of distribution mismatch using semantic similarity. We set up 3 scenarios with varying levels of semantic similarities, including: (1) high semantic similarity, (2) medium semantic similarity and (3) low semantic similarity. In scenario (1), we train DDPM on clean CIFAR-100 data and use it to purify the unlearnable CIFAR-10 training set. This is because a class with high similarity to CIFAR-10 classes can always be found in CIFAR-100. For example, although there is no \u201cdog\u201d (CIFAR-10) in CIFAR-100 class set, the CIFAR-100 class set contains other semantically similar animal classes such as \u201cfox\u201d and \u201craccoon\u201d. In scenario (2), we train DDPM on clean CIFAR10 data and use it to purify the unlearnable CIFAR-100 dataset. For example, the classes in CIFAR-10 have a relatively large semantic difference from \u201cpeople\u201d classes in CIFAR-100 (\u201cbaby\u201d, \u201cboy\u201d, \u201cgirl\u201d, \u201cman\u201d, and \u201cwoman\u201d). We define such semantic similarity as medium level. In scenario (3), we train DDPM on clean SVHN and purify on unlearnable CIFAR-10&100. Scenario (3) is the most extreme case since the door digits in SVHN are totally different from the objects in CIFAR-10&100.\nFor a fair comparison, we do not use fine-tuning and keep the amount of data for training DDPM consistent with the setting"
        },
        {
            "heading": "MM \u201923, October 29\u2013November 3, 2023, Ottawa, ON, Canada. Wan Jiang et al.",
            "text": "in Sec. 4.1. The results are presented in Tab. 5. When the learned density is not far from the original clean distribution(scenario 1&2), LE can still achieve high test accuracy. Surprisingly, even when the learned data manifold is totally different from the original data distribution (scenario 3), LE can still largely improve the test accuracy. To further understand why LEs can tolerate distribution mismatch, we visualize some examples under scenario 1 and 3 in Fig. 2. Even under extreme scenario 3, LEs(S3) still retain the main original semantic features, albeit more blurred than corresponding UEs and LEs(S1). We speculate that this is because the joint-conditional terms in the reverse process (Eq. (16)) shift the mean of the learned data distribution (SVHN), pulling the purified image towards the CIFAR-10 distribution. It is hard to theoretically identify the cause and we will leave it to future research. Overall, this discovery further shatters the illusion of protected data, as it is impractical to add unlearnable noise to all unprotected images in the real world."
        },
        {
            "heading": "4.5 Ablation Studies",
            "text": "To further understand the comparative effects of different elements of our proposed method, we conduct two pairs of ablation studies: Fine-tuning vs. Training from Scratch, and Joint-conditional Diffusion Purification vs. Unconditional Diffusion Purification. \u201cTraining from Scratch\u201d means directly training DDPM on collected raw data, while fine-tuning means fine-tuning from a DDPM trained on other unprotected datasets. The results are shown in Tab. 6. Compared with training from scratch, fine-tuning DDPM can generate higherquality images already in earlier steps(reducing convergence steps from 80000 to 1000 on CIFAR-10), and higher-quality DDPM also\nhelps achieve better purification performance. Additionally, Jointconditional Diffusion Purification achieves the best defensive results. This is because joint-condition control can bring the purified image closer to the original image, while images purified by the unconditional diffusion model tend to gradually deviate from the original clean image as the purification step increases. We show a visual example in Fig. 3."
        },
        {
            "heading": "5 CONCLUSIONS",
            "text": "To systematically investigate the vulnerability of unexploitable data, we formally define a new threat called learnable examples, which can turn unlearnable examples into learnable. This is realized by a novel joint-condition diffusion purification process that projects the unlearnable examples onto a learnable data manifold, which is learned from other newly collected (unprotected) data. Notably, LE is independent of training scheme and consistently effective for both unauthorized supervised&unsupervised learning. More generally, we call for future work to design UEmethods that are not influenced by other unprotected data and use our approach to evaluate their performance. Because it is impractical to expect all the data in the world to be added \u201cunlearnable\u201d perturbations. In addition, such a UE solution can only slightly reduce the effectiveness of LE since LE can tolerate distribution mismatch to a great extent."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This project has received funding from the National Natural Science Foundation of China under grant 61932009, Fundamental Research Funds for the Central Universities (No.JZ2023HGTA0202, No.JZ2023HGQA0101).\nUnlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples MM \u201923, October 29\u2013November 3, 2023, Ottawa, ON, Canada."
        },
        {
            "heading": "MM \u201923, October 29\u2013November 3, 2023, Ottawa, ON, Canada. Wan Jiang et al.",
            "text": "[51] Jiaming Zhang, Xingjun Ma, Qi Yi, Jitao Sang, Yugang Jiang, Yaowei Wang, and Changsheng Xu. 2022. Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples. arXiv preprint arXiv:2301.01217 (2022). [52] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as a perceptual metric. In\nIEEE conference on computer vision and pattern recognition. 586\u2013595. [53] Yunqing Zhao, Henghui Ding, Houjing Huang, and Ngai-Man Cheung. 2022.\nA closer look at few-shot image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9140\u20139150."
        }
    ],
    "title": "Unlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples",
    "year": 2023
}