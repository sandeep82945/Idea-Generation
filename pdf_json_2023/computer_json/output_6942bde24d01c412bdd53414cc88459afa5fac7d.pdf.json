{
    "abstractText": "Large-scale Pretrained Language Models (LLMs), such as ChatGPT and GPT4, have shown strong abilities in multilingual translations, without being explicitly trained on parallel corpora. It is interesting how the LLMs obtain their ability to carry out translation instructions for different languages. In this paper, we present a detailed analysis by finetuning a multilingual pretrained language model, XGLM-7B, to perform multilingual translation following given instructions. Firstly, we show that multilingual LLMs have stronger translation abilities than previously demonstrated. For a certain language, the performance depends on its similarity to English and the amount of data used in the pretraining phase. Secondly, we find that LLMs\u2019 ability to carry out translation instructions relies on the understanding of translation instructions and the alignment among different languages. With multilingual finetuning, LLMs could learn to perform the translation task well even for those language pairs unseen during the instruction tuning phase.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiahuan Li"
        },
        {
            "affiliations": [],
            "name": "Hao Zhou"
        },
        {
            "affiliations": [],
            "name": "Shujian Huang"
        },
        {
            "affiliations": [],
            "name": "Shanbo Cheng"
        },
        {
            "affiliations": [],
            "name": "Jiajun Chen"
        }
    ],
    "id": "SP:d0760458d6a0450758614c0b46851635ae47dead",
    "references": [
        {
            "authors": [
                "Sweta Agrawal",
                "Chunting Zhou",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Marjan Ghazvininejad."
            ],
            "title": "Incontext examples selection for machine translation",
            "venue": "arXiv, 2212.02437.",
            "year": 2022
        },
        {
            "authors": [
                "Maruan Al-Shedivat",
                "Ankur Parikh."
            ],
            "title": "Consistency by agreement in zero-shot neural machine translation",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2019
        },
        {
            "authors": [
                "Naveen Arivazhagan",
                "Ankur Bapna",
                "Orhan Firat",
                "Roee Aharoni",
                "Melvin Johnson",
                "Wolfgang Macherey."
            ],
            "title": "The missing ingredient in zero-shot neural machine translation",
            "venue": "CoRR, 1903.07091.",
            "year": 2019
        },
        {
            "authors": [
                "Ethan Caballero",
                "Kshitij Gupta",
                "Irina Rish",
                "David Krueger."
            ],
            "title": "Broken neural scaling laws",
            "venue": "CoRR, 2210.14891.",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei"
            ],
            "title": "Scaling instruction-finetuned language models",
            "year": 2022
        },
        {
            "authors": [
                "Ahmed El-Kishky",
                "Vishrav Chaudhary",
                "Francisco Guzman",
                "Philipp Koehn."
            ],
            "title": "CCAligned: A massive collection of cross-lingual web-document pairs",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP",
            "year": 2020
        },
        {
            "authors": [
                "Xavier Garcia",
                "Yamini Bansal",
                "Colin Cherry",
                "George Foster",
                "Maxim Krikun",
                "Fangxiaoyu Feng",
                "Melvin Johnson",
                "Orhan Firat."
            ],
            "title": "The unreasonable effectiveness of few-shot learning for machine translation",
            "venue": "CoRR, 2302.01398.",
            "year": 2023
        },
        {
            "authors": [
                "Naman Goyal",
                "Cynthia Gao",
                "Vishrav Chaudhary",
                "PengJen Chen",
                "Guillaume Wenzek",
                "Da Ju",
                "Sanjana Krishnan",
                "Marc\u2019Aurelio Ranzato",
                "Francisco Guzm\u00e1n",
                "Angela Fan"
            ],
            "title": "The Flores-101 evaluation benchmark for low-resource and multilingual",
            "year": 2022
        },
        {
            "authors": [
                "Jiatao Gu",
                "Yong Wang",
                "Kyunghyun Cho",
                "Victor O.K. Li."
            ],
            "title": "Improved zero-shot neural machine translation via ignoring spurious correlations",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2019
        },
        {
            "authors": [
                "Amr Hendy",
                "Mohamed Abdelrehim",
                "Amr Sharaf",
                "Vikas Raunak",
                "Mohamed Gabr",
                "Hitokazu Matsushita",
                "Young Jin Kim",
                "Mohamed Afify",
                "Hany Hassan Awadalla"
            ],
            "title": "How good are gpt models at machine translation? a comprehensive evaluation",
            "year": 2023
        },
        {
            "authors": [
                "Wenxiang Jiao",
                "Jen tse Huang",
                "Wenxuan Wang",
                "Xing Wang",
                "Shuming Shi",
                "Zhaopeng Tu."
            ],
            "title": "Parrot: Translating during chat using large language models",
            "venue": "CoRR, 2304.02426.",
            "year": 2023
        },
        {
            "authors": [
                "Wenxiang Jiao",
                "Wenxuan Wang",
                "Jen tse Huang",
                "Xing Wang",
                "Zhaopeng Tu."
            ],
            "title": "Is chatgpt a good translator? yes with gpt-4 as the engine",
            "venue": "2301.08745.",
            "year": 2023
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B. Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei."
            ],
            "title": "Scaling laws for neural language models",
            "venue": "2001.08361.",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "CoRR, 1412.6980.",
            "year": 2017
        },
        {
            "authors": [
                "moyer",
                "Zornitsa Kozareva",
                "Mona Diab",
                "Veselin Stoyanov",
                "Xian Li"
            ],
            "title": "Few-shot learning with multilingual generative language models",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Patrick Littell",
                "David R. Mortensen",
                "Ke Lin",
                "Katherine Kairis",
                "Carlisle Turner",
                "Lori Levin."
            ],
            "title": "URIEL and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors",
            "venue": "Proceedings of the 15th Conference",
            "year": 2017
        },
        {
            "authors": [
                "Hui Liu",
                "Danqing Zhang",
                "Bing Yin",
                "Xiaodan Zhu."
            ],
            "title": "Improving pretrained models for zero-shot multi-label text classification through reinforced label hierarchy reasoning",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the",
            "year": 2021
        },
        {
            "authors": [
                "Zaid Alyafeai",
                "Albert Webson",
                "Edward Raff",
                "Colin Raffel"
            ],
            "title": "Crosslingual generalization through multitask finetuning",
            "year": 2023
        },
        {
            "authors": [
                "Lin Pan",
                "Chung-Wei Hang",
                "Haode Qi",
                "Abhishek Shah",
                "Saloni Potdar",
                "Mo Yu."
            ],
            "title": "Multilingual BERT post-pretraining alignment",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2021
        },
        {
            "authors": [
                "Holger Schwenk",
                "Vishrav Chaudhary",
                "Shuo Sun",
                "Hongyu Gong",
                "Francisco Guzm\u00e1n."
            ],
            "title": "Wikimatrix: Mining 135m parallel sentences in 1620 language pairs from wikipedia",
            "venue": "CoRR, abs/1907.05791.",
            "year": 2019
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto"
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "year": 2023
        },
        {
            "authors": [
                "David Vilar",
                "Markus Freitag",
                "Colin Cherry",
                "Jiaming Luo",
                "Viresh Ratnakar",
                "George Foster."
            ],
            "title": "Prompting palm for translation: Assessing strategies and performance",
            "venue": "2211.09102.",
            "year": 2022
        },
        {
            "authors": [
                "David Vilar",
                "Markus Freitag",
                "Colin Cherry",
                "Jiaming Luo",
                "Viresh Ratnakar",
                "George F. Foster."
            ],
            "title": "Prompting palm for translation: Assessing strategies and performance",
            "venue": "CoRR, abs/2211.09102.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y. Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V. Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "CoRR, 2109.01652.",
            "year": 2022
        },
        {
            "authors": [
                "Jian Yang",
                "Yuwei Yin",
                "Shuming Ma",
                "Haoyang Huang",
                "Dongdong Zhang",
                "Zhoujun Li",
                "Furu Wei."
            ],
            "title": "Multilingual agreement for multilingual neural machine translation",
            "venue": "Proceedings of the 59th Annual Meeting of the",
            "year": 2021
        },
        {
            "authors": [
                "Biao Zhang",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Prompting large language model for machine translation: A case study",
            "venue": "CoRR, 2301.07069.",
            "year": 2023
        },
        {
            "authors": [
                "Biao Zhang",
                "Philip Williams",
                "Ivan Titov",
                "Rico Sennrich."
            ],
            "title": "Improving massively multilingual neural machine translation and zero-shot translation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Wenhao Zhu",
                "Hongyi Liu",
                "Qingxiu Dong",
                "Jingjing Xu",
                "Lingpeng Kong",
                "Jiajun Chen",
                "Lei Li",
                "Shujian Huang."
            ],
            "title": "Multilingual machine translation with large language models: Empirical results and analysis",
            "venue": "CoRR, 2304.04675.",
            "year": 2023
        },
        {
            "authors": [
                "Zhu"
            ],
            "title": "2023). We investigate whether this also holds when using parallel sentences to finetune LLMs",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "Large-scale Pretrained Language Models (LLMs), such as ChatGPT and GPT4, have shown strong abilities in multilingual translations, without being explicitly trained on parallel corpora. It is interesting how the LLMs obtain their ability to carry out translation instructions for different languages. In this paper, we present a detailed analysis by finetuning a multilingual pretrained language model, XGLM-7B, to perform multilingual translation following given instructions. Firstly, we show that multilingual LLMs have stronger translation abilities than previously demonstrated. For a certain language, the performance depends on its similarity to English and the amount of data used in the pretraining phase. Secondly, we find that LLMs\u2019 ability to carry out translation instructions relies on the understanding of translation instructions and the alignment among different languages. With multilingual finetuning, LLMs could learn to perform the translation task well even for those language pairs unseen during the instruction tuning phase."
        },
        {
            "heading": "1 Introduction",
            "text": "The emergence of Large Pretrained Language Models (LLMs) (Brown et al., 2020; OpenAI, 2023) has revolutionized the research of machine translation (Hendy et al., 2023; Garcia et al., 2023). These models have demonstrated remarkable multilingual translation capabilities, without requiring explicit training on parallel corpora. For instance, XGLM, a medium-sized multilingual language model, outperforms supervised models using only several examples as demonstrations (Lin et al., 2022); the cutting-edge LLM GPT4 has been shown to perform comparably to commercial translation systems on multiple language pairs (Jiao et al., 2023b).\n* Equal contribution. \u2020 Corresponding author.\nMost existing researches on LLMs for machine translation focus on in-context learning (ICL), i.e. taking several parallel sentences as the demonstration to guide LLMs to perform translation (Vilar et al., 2022a; Agrawal et al., 2022; Hendy et al., 2023; Zhu et al., 2023). However, these methods rely heavily on the in-context learning ability of LLMs. For smaller models, e.g. models with only 1B or 7B parameters, the relatively weak ICL ability may result in an underestimation its potential translation ability.\nInstead of relying on the ICL abilities, we propose to investigate the ability of LLMs by directly training them to follow translation instructions. Inspired by the recent success of instruction tuning (Wei et al., 2022; Chung et al., 2022), we organize multilingual translation tasks as different instances of the translation instruction, with each instance corresponds to a specific language pair. By training the LLMs to follow these instructions, i.e. with multilingual Finetuning with Translation Instructions (mFTI), it is possible to better elicit translation ability inside LLMs.\nOur results show that by training on a mixed dataset of 1,000 sentences per language pair, mFTI outperforms the 8-shot in-context learning by near 3 BLEU on average, showing a greater potential of LLMs\u2019 translation ability than previously demonstrated (Lin et al., 2022). In addition, we also discuss how mFTI improves the LLM and which factors influence the performance.\nTo better understand why LLMs could follow these instructions, we design a mFTI setting where only a subset of the translation instructions, i.e. language pairs, are used for training. Thus LLMs need to generalize their instruction following abilities for those language pairs unseen during mFTI. Surprisingly, mFTI elicits the translation ability not only for trained language pairs, but also for those unseen during instruction training. With further experiments and analyses, we find that LLMs could\nar X\niv :2\n30 5.\n15 08\n3v 2\n[ cs\n.C L\n] 3\n0 Ju\nn 20\n23\nlearn the translation behavior in general by being trained to translate even irrelevant language pairs. It is also interesting that with mFTI, LLMs learn to directly align languages through the use of pivot languages, which also enhances the instructionfollowing ability for unseen language pairs."
        },
        {
            "heading": "2 Multilingual Finetuning with",
            "text": "Translation Instructions"
        },
        {
            "heading": "2.1 Overall Framework",
            "text": "Given a corpus of multilingual parallel sentences and their languages M = {(lsi, lti,xi,yi)}, where ls i and lti are names of the source and target language of i-th parallel sentence (xi,yi), respectively. mFTI leverages an instruction template T to organize the corpus M into a language modeling dataset D. Each sentence di in D is an instantiation of the translation instruction with a specific sentence pair: di = T (lsi, lti,xi,yi). The parameter of LLM are then optimized using a standard next-token-prediction objective on D:\nargmax \u03b8 |D|\u2211 i=1 |di|\u2211 j=1 log p\u03b8(d i j |di<j),\nwhere \u03b8 are parameters of LLMs. The instruction template adopted in this paper is\nTranslation: [ls]: x [lt]: y\nwhere the prefix \u201cTranslation:\u201d is used to indicate the translation task; the pattern \u201c[\u00b7]:\u201d is used to identify the name of the specific language. We present the study of the impact of instruction templates in Appendix A.1."
        },
        {
            "heading": "2.2 Experiment Setup",
            "text": "Backbone Language Model We consider XGLM-7B (Lin et al., 2022) as our backbone language models. XGLM-7B is a massive multilingual language model trained in an auto-regressive fashion. It is trained on a massive corpus of 500 billion tokens comprising 30 diverse languages. Low-resource languages have been up-sampled during training, making it an ideal backbone model for multilingual translation research.\nLanguages Following Lin et al. (2022), our evaluation involves 13 languages that are covered in the pretraining corpus of XGLM, i.e. English (En), German (De), French (Fr), Catalan (Ca),\nFinnish (Fi), Russian (Ru), Bulgarian (Bg), Chinese (Zh), Korean (Ko), Arabic (Ar), Swahili (Sw) and Tamil (Ta). Among these languages, En, De, Fr, Ru and Zh are high-resource languages (with ratios in the XGLM pretraining data greater 4%); Ko, Fi, Ar, Bg are medium-resource languages (with ratios between 0.5%-4%); Ca, Hi, Ta, Sw are lowresource languages (with ratios under 0.5%).\nEvaluation Datasets Following previous works (Lin et al., 2022), we evaluate translation models on the FLORES-101 dataset (Goyal et al., 2022), which provides manual translations of 1012 sentences in 101 languages.\nFinetuning Datasets Our finetuning dataset primarily comes from WikiMatrix (Schwenk et al., 2019). WikiMatrix provides a parallel corpus for 1620 different language pairs, including many nonEnglish language pairs, which enables a systematic investigation for the translation of languages other than English. We also leverage the MultiCCAligned (El-Kishky et al., 2020) corpus for language pairs that are not contained in WikiMatrix, including Hi-Sw, Ko-Sw, Ta-Sw, Sw-Hi, Sw-Ko, Sw-Ta.\nOptimization Details We finetune all models using the Adam (Kingma and Ba, 2017) optimizer with the learning rate fixed as 5e \u2212 6. We use a fixed batch size of 80 sentences and finetune models for 1 epoch or 2000 steps (depending on the size of the training corpus) for all experiments."
        },
        {
            "heading": "3 Understanding the Potential Translation Ability of LLMs",
            "text": "In this section, we first assess the overall translation performance of mFTI by comparing it to few-shot in-context learning1. We then present a detailed analysis of how the corpus for mFTI influences the translation quality."
        },
        {
            "heading": "3.1 Translation Ability of LLMs",
            "text": "We finetune XGLM on 156 language pairs spanning all 13 languages. Since our goal is to elicit the translation ability of LLMs using a small number of examples, we limit the number of parallel sentences to 1000 per language.\n1We randomly select 8 examples from the FLORES-101 dev split as the demonstration for ICL. Random selection strategy is shown to be good enough in many previous works (Vilar et al., 2022b; Zhu et al., 2023). The template we use for ICL is <src_text> = <tgt_text>, which shows good performance according to Zhu et al. (2023).\nmFTI Better Elicits Translation Ability than Few-shot ICL. Figure 1 shows the average BLEU for translation into and out of language X, respectively. BLEU scores for different language pairs are shown in Appendix A.2. It is clear that mFTI leads to better translation performances than 8-shot ICL in almost all language pairs (3 BLEU on average). For some languages, the gap is up to 8 BLEU (e.g. translating into Catalan). This demonstrates the effectiveness of mFTI in eliciting LLM\u2019s translation ability. It also shows that LLMs have a greater potential for multilingual translation than we saw with ICL (Lin et al., 2022).\nEven for translating into and out of English, mFTI still outperforms 8-shot ICL, but with a much smaller gap. This indicates that LLMs with ICL are better at performing tasks that involve English rather than other languages, but they still have the potential to perform even better.\nXGLM is still an English-centric Model. The translation performance for each language varies greatly. Considering that the number of sentences used in mFTI is the same for each language, one may suspect that the translation performance of each language largely depends on the amount of its pretraining data. For this reason, the languages in Figure 1 are listed in descending order of their data amount in the XGLM pretraining. However, there are clear fluctuations. For example, Russian and Chinese are the two languages with the largest portion of pretraining data other than English, but their translation performance is much worse than some other languages such as French.\nWe calculate the Spearman correlation between\nthe translation performance and possible influence factors, namely data amount in pretraining and similarity to English. For data amount, we use the size of the pretraining corpus reported in Lin et al. (2022). For similarity to English, we adopt the lang2vec2, which is a toolkit for querying the URIEL typological database3, to get each language\u2019s feature vector of different perspectives including geography, phylogeny, syntax, phonology and inventory4.\nAs shown in Table 1, the translation performance indeed has a positive correlation with data amount in pretraining (0.39/0.36). But the similarity between the specific language and English plays a more important role in determining the performance. All considered features demonstrate a higher correlation coefficient than the data amount in pretraining. This indicates XGLM is still a predominantly English-centric model. Based on these observations, we suggest taking the relation between different languages into consideration when collecting and sampling data for pretraining multilingual LLMs."
        },
        {
            "heading": "3.2 mFTI Enhances Direct Language Alignment",
            "text": "A distinct difference between ICL and mFTI is that mFTI could learn from more parallel sentences and update the model if needed. It is interesting to see what changes after the update.\n2https://github.com/antonisa/lang2vec 3http://www.cs.cmu.edu/~dmortens/projects/7_\nproject 4We refer readers to Littell et al. (2017) for details on how the feature vector is obtained.\nMany previous works (Zhang et al., 2023; Jiao et al., 2023b) have shown that translating by pivoting through English significantly improves ICL\u2019s translation performance. We compare performance gains of pivot translation using ICL and mFTI, respectively.\nFigure 2 presents the result. Each value in the grid is the BLEU difference before and after pivoting through English. We can first observe that pivoting through English indeed improves translation performance for ICL, up to 10 BLEU in some language pairs. However, after mFTI, the gap has been significantly reduced. Considering the fact the mFTI achieves an average 3 BLEU higher than ICL, the reduction of benefits from pivoting through English compared to direct translation may indicate a better direct alignment between languages."
        },
        {
            "heading": "3.3 Factors that affects mFTI",
            "text": "Quality of Finetuning Corpus is Crucial. During mFTI, it is important to consider the impact of the quality of finetuning corpus on translation performance. Therefore, we conduct a comparative experiment by selecting high and low-quality finetuning corpus and comparing their translation performance. The standard for selecting high and lowquality data is based on the scores carried by each sample in WikiMatrix, which is the cosine similarity of source and target sentence representations produced by LASER 5. From the filtered corpus, where each parallel sentence pair has a LASER similarity score greater than 1.04, we randomly select 100,000 parallel sentence pairs with the highest and lowest scores as the high-quality and low-\n5https://github.com/facebookresearch/LASER\nquality pool, respectively. We then construct the corresponding high-quality and low-quality corpus by randomly selecting 1000 sentences from the pools. According to the results in Table 2, finetuning with high-quality parallel sentences can improve the BLEU score by around 2 points compared to finetuning with low-quality parallel sentences, emphasizing the importance of corpus quality.\nThe Effectiveness of mFTI Scales up with Model size And Training Examples. Figure 3 shows the translation performance when varying the number of training examples per language pair (1k, 2k, 4k, 8k, 16k, 32k) and the number of model parameters (564M, 1.7B, 2.9B, 4.5B, 7.5B). As we can see, it follows a standard log-linear scaling law in terms of both the number of training examples and model size, which is consistent with findings in previous works (Kaplan et al., 2020; Caballero et al., 2023)."
        },
        {
            "heading": "4 Understanding the Ability of Carrying Out Translation Instructions",
            "text": "In this section, we present a comprehensive analysis on how mFTI improves the model\u2019s ability to carry out translation instructions.\nWe begin by presenting an overarching experiment where we intentionally withhold certain language pairs during the mFTI process, which allows us to study models\u2019 ability to carry out translation instructions under different conditions.\nFurthermore, we delve deeper into our analysis by exploring how mFTI enhances LLMs\u2019 ability to carry out translation instructions from following perspectives: better understanding of translation instructions (4.3 and 4.4) and better alignment between languages to execute translation instructions (4.5)."
        },
        {
            "heading": "4.1 Manipulating Conditions",
            "text": "In Section 3, we have presented results in a fully supervised setting, where all testing language pairs are seen during instruction tuning. To provide further insights into LLMs\u2019 generalization ability\nacross language pairs, we simulate a more realistic scenario where there may be a lack of source and/or target language sentences during the instruction tuning process.\nMore specifically, from the 13 selected languages, we hold out 6 languages as unseen languages. We further partition the rest 7 languages into three groups: Only-Source (languages only appear on the source side), Only-Target (languages only appear on the target side) and Source-Target (languages appear on both the source and target side). We then form language pairs from these partitions following the requirement of partitions. This allows us to assess mFTI\u2019s performance under the following conditions:\n\u2022 Seen Both Sides Both the source side and target side language appear in the finetuning corpus. This can be further divided to:\n\u2013 Same Direction. The same translation direction is trained during mFTI.\n\u2013 Reversed Direction. The same translation direction does not appear when training, but the reversed direction does.\n\u2013 Unseen Direction. The translation pair (neither the same nor the reverse) does not appear when training.\n\u2022 Unseen Src. Only the target language sentences appear when training.\n\u2022 Unseen Tgt. Only the source language sentences appear when training.\n\u2022 Unseen Both Sides. Neither source language nor target language sentences appear in the finetuning corpus."
        },
        {
            "heading": "4.2 mFTI Learns to Follow Translation Instruction across Conditions",
            "text": "We finetune XGLM on the corpus described in the previous section, and present the results in Table 3. Since there are 16 language directions in the training corpus, we denote the finetuned model as mFTI-16.\nmFTI-16 brings improvements on most settings, yet much less than mFTI-all Firstly we can see that mFTI-16 brings improvements on most settings except Reversed Direction, demonstrating the effectiveness of mFTI-16. However, the improvements are much less compared to mFTI-all, even\nfor the Same Direction partition. This can be attributed to fewer language pairs when finetuning, which we will discuss in Section 4.3.\nLanguage position shift between training and testing has negative effects on translation performance. The translation performance of mFTI-16 on Reversed Direction degrades by 0.8 BLEU compared to 8-shot ICL. By inspecting the translation results, we find that mFTI-16 suffers from severe off-target problems, i.e. generating translations in wrong target languages. We hypothesize that this could attribute to the shift in the relative positions of the source and target languages during training.\nSeeing target languages when finetuning is better than source languages. When there are unseen languages in the language direction, the improvement on Unseen Src is much larger compared to Unseen Tgt, which indicates the understanding of the specified target language may be more important than the source language.\nUnseen Both Sides also benefit from mFTI training. The most surprising phenomenon is that language pairs from Unseen Both Sides partition also benefit from mFTI, with an improvement of 0.7 BLEU compared to 8-shot ICL. Since mFTI-16 does not see any sentences of the source and target languages, the improvements indicate a better understanding of the translation instruction, which we will discuss in Section 4.4."
        },
        {
            "heading": "4.3 Instruction Tuning with More Language Pairs Leads to Better Translation Performance",
            "text": "Previous instruction-tuning works show that scaling the number of tasks significantly benefits the unseen tasks (Chung et al., 2022). Observing the performance gap of Same Direction between mFTI16 and mFTI-all,\nwe gradually add more language pairs to mFTI16, and plot the translation performance on each partition in Figure 4. In order to isolate the possi-\nble effect of additional monolingual sentences, we only add language pairs other than the studied 13 languages.\nIt can be seen that as the number of language pairs grows, the translation performances of all partitions generally increase, validating the importance of more language pairs. Notably, the performance of the Reversed Direction partition are significantly boosted, outperforming 8-shot ICL by a large margin when increasing the number of language pairs from 16 to 30. This emphasizes the importance of more language pairs for mFTI.\nSurprisingly, the performance of Unseen Both Sides partition improves the most. Since no language data of Unseen-both language pairs are\nadded, this indicates the ability of instructionfollowing on these language pairs has been significantly enhanced, which we will discuss in the next section."
        },
        {
            "heading": "4.4 mFTI Generalizes the Understanding of",
            "text": "Translation Instruction to Unseen Directions\nIn this section, we aim to understand how mFTI facilitates the understanding of instructions from a more fine-grained view, i.e. specific language directions and instruction-following errors.\nFor the language directions, we select Ru\u2192Fr (high resource), Bg\u2192Ar (medium resource), Ca\u2192Ta (low resource) from the Unseen-Both Sides partition to study mFTI\u2019s effectiveness under different resource settings.\nFor instruction errors, we identify the following four major problems in the translations:\n\u2022 Source Copy (SC): This error occurs when the model simply copies the source sentence as the translation without making any meaningful changes. We identify this error by calculating the sentence-level BLEU score between the translations and the source sentences. If\nthe BLEU score is above 80, it indicates that the translation is nearly identical to the source.\n\u2022 Off-target translation (OT): In this case, the model fails to produce sentences in the target language. We detect this error by using a language identification tool, such as fasttext, to determine the language of the generated translations.\n\u2022 Over/under translation (OU): This error refers to situations where the model produces translations that are significantly longer or shorter than references. We consider translations with a length ratio above 2 or below 0.5 as over- or under-translations, respectively.\n\u2022 Oscillatory hallucination (OH): This error occurs when the model gets stuck in a specific translation state and generates repeated n-grams until reaching the maximum length. We define translations with n-grams that consecutively repeat at least three times as oscillatory hallucinations.\ngreen background."
        },
        {
            "heading": "4.4.1 Adding Irrelevant Language Pairs Reduces SC, OT and OU Ratios",
            "text": "In Section 4.3, we show that additional language pairs in mFTI lead to improved BLEU scores even for the Unseen Both Sides partition. We provide an in-depth analysis here from the aforementioned fine-grained views. We plot the trends of translation and instruction-following performance, and the ratios of 4 specific instruction-following errors as the number of additional language pairs grows. The results are in Figure 5.\nMore language pairs reduces instructionfollowing errors and improves translation performance. Firstly, we can see that as more language pairs are added to the training corpus, instruction-following errors on Unseen-both language pairs are gradually reduced, leading to improvements in BLEU scores. Comparing different language pairs, we can see that high- and mediumresource language pairs generally perform better than low-resource language pairs on all four types of errors. Since all these language directions are unseen when instruction finetuning, it highlights the importance of language skills acquired during the pretraining phase.\nSC: Solved. It can be observed that after adding about 30-60 language pairs, the model learns to avoid the SC problem, indicating this is a relatively easy problem to solve.\nOU: Decreased to the level of mFTI-all. We can further see that adding more language pairs is also effective for reducing OU errors, as the error ratios significantly decrease as the number of language pairs grows. Notably, after scaling the number of language pairs to 150, the OU ratios of three language pairs are comparable to full finetuning.\nThis demonstrates the effectiveness of mFTI.\nOT: Decreased, but not to a satisfactory level. Turning to the OT ratio, we observe that it also decreases as the number of language pairs grows. However, even after scaling the number of language pairs to 150, the OT ratio still cannot be decreased to the level of mFTI-all.\nOH: No effect. Finally, we can see that with the increment in the number of language pairs, the OH ratio does not show a clear decreasing trend, which we will further discuss in the next section."
        },
        {
            "heading": "4.4.2 Joint Training with Monolingual",
            "text": "Generation Instructions Helps Reduce OH and OT Problems More Efficiently\nIn the previous section, we find that the off-target (OT) and oscillatory hallucination (OH) on some language pairs cannot be fully solved to the level of mFTI-all by adding more irrelevant language pairs. We note that both problems are only related to the target language: the OT problem can be attributed to models\u2019 inability of relating target language names to the corresponding scripts of the language, and the OH problem might be caused by the poor modeling of the target languages. We hypothesize that finetuning models on instructions of monolingual generation, i.e. given a language name, generate fluent sentences from that language, should help ease these problems.\nTo this end, we organize the monolingual sentences of the held-out languages into monolingual generation instructions. The template we adopt is \u201c[li] : y\u201d. We then finetune XGLM on the translation instruction dataset as well as these monolingual generation instructions.\nWe report the BLEU score, OT ratio and OH ratio in Table 4. Firstly we can see that adding\nmonolingual generation instructions for unseenboth language pairs can help mitigate the OT and OH problem in most scenarios, leading to better translation performance. Notably, by combining more irrelevant language pairs and monolingual sentences, the gap between mFTI-150 with monolingual sentences and mFTI-all has significantly diminished, despite the model has never seen parallel sentences of the tested language before."
        },
        {
            "heading": "4.5 mFTI Improves Language Alignment via Pivot Languages",
            "text": "Besides the understanding of translation instruction, another crucial knowledge that models must grasp to carry out the instruction is the alignment between source and target languages. However, in scenarios where direct parallel sentences are not available, models have limited access to alignment information. This situation resembles the zero-shot setting commonly studied in multilingual translation research (Gu et al., 2019; Zhang et al., 2020; Arivazhagan et al., 2019; Liu et al., 2021). In this section, we aim to investigate the ability of mFTI to establish meaningful alignments through pivot languages in this scenario.\nSpecifically, for the three Unseen Both Sides language pairs X\u2192Y studied in the previous section, i.e. Ru\u2192Fr, Bg\u2192Ar and Ca\u2192Ta, we start from the mFTI-150 setting, and add parallel sentences of X\u2192En and En\u2192Y to the training corpus. We then perform mFTI using these augmented corpora and evaluate the model\u2019s performance on test sentences that do not contain instruction-following errors. As knowledge of language alignments is the last requirement for carrying out translation instructions once the model has learned to execute correct translation behavior, the performance on these sentences serves as a reliable indicator of the model\u2019s proficiency in language alignment.\nThe result is in Table 5. First, we can see that\nmFTI-150 and 8-shot ICL perform comparably, both significantly worse than mFTI-all. Since the tested three language pairs in unseen in mFTI-150, this indicates that similar to mFTI-150, the main role of ICL is to enhance the model\u2019s understanding of the translation behavior instead of source-target alignment knowledge.\nHowever, after adding pivot parallel sentences, the model\u2019s performance (+pivot) is significantly boosted. This demonstrates the potential of mFTI to leverage pivot languages to boost direct alignment between languages and improve translation performances."
        },
        {
            "heading": "5 Related Works",
            "text": ""
        },
        {
            "heading": "5.1 LLM for MT",
            "text": "Machine translation researchers have recognized the potential of utilizing LLMs for MT, as these models acquire advanced language understanding skills during pretraining. The prevailing paradigm for leveraging LLMs for MT is in-context learning (ICL). For instance, Lin et al. (2022) demonstrated that providing 32 examples during pretraining can outperform GPT-3 and a supervised multilingual translation model. Other studies such as Vilar et al. (2022a), Agrawal et al. (2022), and Zhu et al. (2023) have investigated different factors that affect ICL\u2019s performance, including example quality, example selection strategy, and template sensitivity. Moreover, works such as Hendy et al. (2023) and Jiao et al. (2023b) have studied the translation quality of various GPT-3 models and found their performances to be comparable to commercial translation systems on high-resource language pairs. In contrast to these works, our research focuses on exploring existing LLMs\u2019 translation ability via mFTI.\nThe most similar work to ours is Jiao et al. (2023a), which finetunes an open-source LLM LLaMA (Touvron et al., 2023) on the mixes translation data and the alpaca instruction dataset (Taori et al., 2023) to make it a better translator. However, they mainly focus on the bilingual translation setting while our work investigates the multilingual generalization when finetuning LLMs to carry out translation instructions."
        },
        {
            "heading": "5.2 Generalization On Unseen Language Pairs",
            "text": "Our work also has a close relation to zero-shot translation in the multilingual translation setting,\nwhere there are no direct parallel sentences between the source and target language. There are two major problems for zero-shot translation: generating correct languages and learning universal language representations.\nFor the first problem, Gu et al. (2019); Zhang et al. (2020) leverage back-translation to add more target-language-related training data. Arivazhagan et al. (2019); Liu et al. (2021) impose regularization on the encoder/decoder to make the model more aware of the target language. Unlike their works, we discuss the off-target problem in the context of LLMs, and find adding both irrelevant language pairs and additional monolingual sentences can ease the problem to a great extent.\nFor the second problem, previous works focus on learning language-agnostic representations through additional regularization of model representations Arivazhagan et al. (2019); Pan et al. (2021), and consistency between semantic equivalent sentences(Al-Shedivat and Parikh, 2019; Yang et al., 2021). Instead, our works mainly aim to reveal the helpfulness of multilingual finetuning LLMs for unseen language pairs by internalizing the pivot language information.\nFurthermore, our discussion encompasses a more stringent version of zero-shot translation, where neither source nor target language sentences are present in the finetuning corpus. This demands a stronger generalization ability, as the model must effectively utilize the language knowledge acquired during pretraining and the translation task knowledge acquired during finetuning to generate highquality translations."
        },
        {
            "heading": "5.3 Instruction Finetuning",
            "text": "Our work focuses on finetuning a pretrained model with instructions to improve zero-shot translation performance. In previous works, several have demonstrated that without few-shot examples, it is harder for LLMs to perform well in zero-shot settings, and finetuning language models on a variety of tasks can significantly improve zero-shot performance on several tasks. For instance,Wei et al. (2022) aims to improve generalization in unseen tasks by performing instruction tuning. Muennighoff et al. (2023) further extend to finetune LLM by multilingual data instead of English data and investigate that multilingual finetuning leads to better performance on unseen tasks and unseen languages. Also, Chung et al. (2022) explore instruction tuning\nfrom the perspective of a number of tasks in finetuning corpus and LLM size. Chung et al. (2022) found that scaling these factors can dramatically improve zero-shot performance.\nIn our work, we primarily focus on the translation performance of LLM. We adopt a comprehensive approach to consider the factors mentioned above, including the scale of the finetuning corpus, the size of model parameters, and the language selection within the fine-tuning corpus, for a comprehensive analysis of the translation performance of the LLM. Additionally, we conduct a detailed analysis of the model\u2019s understanding and execution capabilities in translation tasks after instruction finetuning. We systematically track and analyze translation errors made by LLM and show that they can be solved or alleviated by adding both translation and monolingual generation instructions."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we explore Multilingual Finetuning with Translation Instructions (mFTI), to better unleash the translation ability of multilingual LLMs. Through extensive experiments, we demonstrate that by training on a mixture of 1000 sentences per language pair, mFTI achieves better performance than 8-shot ICL, indicating the untapped potential of translation ability in LLMs by previous works.\nMoreover, we systematically discuss the working mechanism of mFTI by analyzing it from the view of instruction completion. Our experiments demonstrate that mFTI helps the model better follow the instruction by introducing more language pairs and monolingual sentences, and enhances the direct language alignment by learning from pivot language pairs.\nOur paper also unveils remaining translation issues when adopting LLMs for zero-shot machine translation, i.e. over/under translation, oscillatory hallucination, and mistranslation caused by incorrect alignments. Future works should focus on acquiring more language knowledge from the pretraining phase, and designing better regularization terms to solve these problems."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 mFTI Is Less Sensitive to Templates As ICL.\nMany previous works show that ICL methods for MT are sensitive to templates (Zhang et al., 2023; Zhu et al., 2023). We investigate whether this also holds when using parallel sentences to finetune LLMs.\nIn Table 6, we list the average translation performance when using different instruction templates for mFTI. We can observe the performance variation between models trained on different templates is much smaller compared to ICL methods, in which the largest gap between the best and worst reasonable templates can be up to 3 BLEU.\nTo further analyze the template sensitivity of ICL and mFTI per language pair, we plot the standard deviation of BLEU scores across six templates in Figure 6. An interesting observation is that ICL demonstrates higher sensitivity when translating to or from English compared to other languages, which we believe deserves further research. Conversely, mFTI exhibits lower template sensitivity, rendering it a more practical choice for real-world applications.\nICL mFTI\nTranslation: [<S>]: <I> [<T>]: 11.9 16.9 Translation: <I> [<T>]: 13.0 16.7 <I> Translate from [<S>] to [<T>]: 13.9 16.6 <I> Translate to [<T>]: 13.9 16.4 The <T> translation of <S> sentence \"<I>\" is: 11.2 16.6 The <T> translation of \"<I>\" is: 11.1 16.0 <I> = 13.9 -\nTable 6: BLEU score of mFTI models using different finetuning templates. <S>, <T>, <I>, and <O> represent the source language, target language, source language sentence, and target language sentence, respectively.\nA.2 Full results Table 7 shows all 156 language pair results on 8- shot ICL and mFTI.\nFigure 6: Standard deviation of BLEU score across six different templates in mFTI and ICL."
        }
    ],
    "title": "Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions",
    "year": 2023
}