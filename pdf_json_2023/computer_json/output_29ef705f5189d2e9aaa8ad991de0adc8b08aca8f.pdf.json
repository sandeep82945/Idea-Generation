{
    "abstractText": "In recent years, the rapid development of deep learning approaches has paved the way to explore the underlying factors that explain the data. In particular, several methods have been proposed to learn to identify and disentangle these underlying explanatory factors in order to improve the learning process and model generalization. However, extracting this representation with little or no supervision remains a key challenge in machine learning. In this paper, we provide a theoretical outlook on recent advances in the field of unsupervised representation learning with a focus on auto-encoding-based approaches and on the most well-known supervised disentanglement metrics. We cover the current state-of-the-art methods for learning disentangled representation in an unsupervised manner while pointing out the connection between each method and its added value on disentanglement. Further, we discuss how to quantify disentanglement and present an in-depth analysis of associated metrics. We conclude by carrying out a comparative evaluation of these metrics according to three criteria, (i) modularity, (ii) compactness and (iii) informativeness. Finally, we show that only the Mutual Information Gap score (MIG) meets all three criteria.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ikram Eddahmani"
        },
        {
            "affiliations": [],
            "name": "Marwa El-Bouz"
        }
    ],
    "id": "SP:6651a47fb9e02edf07b2ce28e2b47f86a74dcef5",
    "references": [
        {
            "authors": [
                "Y. Bengio"
            ],
            "title": "Learning deep architectures for AI",
            "venue": "Found. Trends Mach. Learn. 2009,",
            "year": 2009
        },
        {
            "authors": [
                "I. Higgins",
                "D. Amos",
                "D. Pfau",
                "S. Racaniere",
                "L. Matthey",
                "D. Rezende",
                "A. Lerchner"
            ],
            "title": "Towards a definition of disentangled representations",
            "venue": "arXiv 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Bengio",
                "A. Courville",
                "P. Vincent"
            ],
            "title": "Representation learning: A review and new perspectives",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "year": 2013
        },
        {
            "authors": [
                "A. Szab\u00f3",
                "Q. Hu",
                "T. Portenier",
                "M. Zwicker",
                "P. Favaro"
            ],
            "title": "Challenges in disentangling independent factors of variation",
            "venue": "arXiv 2017,",
            "year": 2017
        },
        {
            "authors": [
                "R. Suter",
                "D. Miladinovic",
                "B. Sch\u00f6lkopf",
                "S. Bauer"
            ],
            "title": "Robustly Disentangled Causal Mechanisms: Validating Deep Representations for Interventional Robustness",
            "venue": "In Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "P. Le-Khac",
                "G. Healy",
                "A. Smeaton"
            ],
            "title": "Contrastive Representation Learning: A Framework and Review",
            "venue": "IEEE Access 2020,",
            "year": 1939
        },
        {
            "authors": [
                "S. Wold",
                "K. Esbensen",
                "P. Geladi"
            ],
            "title": "Principal component analysis",
            "venue": "Chemom. Intell. Lab. Syst. 1987,",
            "year": 1987
        },
        {
            "authors": [
                "D. B\u00e1scones",
                "C. Gonz\u00e1lez",
                "D. Mozos"
            ],
            "title": "Hyperspectral Image Compression Using Vector Quantization, PCA and JPEG2000",
            "venue": "Remote Sens. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "J. Stone"
            ],
            "title": "Independent component analysis: An introduction",
            "venue": "Trends Cogn. Sci",
            "year": 2002
        },
        {
            "authors": [
                "G. Naik",
                "D. Kumar"
            ],
            "title": "An overview of independent component analysis and its applications",
            "venue": "Informatica 2011,",
            "year": 2011
        },
        {
            "authors": [
                "E. Henry",
                "J. Hofrichter"
            ],
            "title": "Singular value decomposition: Application to analysis of experimental data",
            "venue": "Methods Enzymol",
            "year": 1992
        },
        {
            "authors": [
                "M. Montero",
                "C. Ludwig",
                "R. Costa",
                "G. Malhotra",
                "J. Bowers"
            ],
            "title": "The Role of Disentanglement in Generalisation",
            "venue": "Available online: https://openreview.net/forum?id=qbH974jKUVy (accessed on",
            "year": 2023
        },
        {
            "authors": [
                "Z. Shen",
                "J. Liu",
                "Y. He",
                "X. Zhang",
                "R. Xu",
                "H. Yu",
                "P. Cui"
            ],
            "title": "Towards out-of-distribution generalization: A survey",
            "venue": "arXiv 2021,",
            "year": 2021
        },
        {
            "authors": [
                "S. Duan",
                "L. Matthey",
                "A. Saraiva",
                "N. Watters",
                "C. Burgess",
                "A. Lerchner",
                "I. Higgins"
            ],
            "title": "Unsupervised model selection for variational disentangled representation learning",
            "venue": "arXiv 2019,",
            "year": 2019
        },
        {
            "authors": [
                "H. Zheng",
                "M. Lapata"
            ],
            "title": "Real-World Compositional Generalization with Disentangled Sequence-to-Sequence Learning",
            "venue": "arXiv 2022,",
            "year": 2022
        },
        {
            "authors": [
                "A. Dittadi",
                "F. Tr\u00e4uble",
                "F. Locatello",
                "M. W\u00fcthrich",
                "V. Agrawal",
                "O. Winther",
                "S. Bauer",
                "B. Sch\u00f6lkopf"
            ],
            "title": "On the transfer of disentangled representations in realistic settings",
            "venue": "arXiv 2020,",
            "year": 2020
        },
        {
            "authors": [
                "M. Montero",
                "J. Bowers",
                "R. Costa",
                "C. Ludwig",
                "G. Malhotra"
            ],
            "title": "Lost in Latent Space: Disentangled Models and the Challenge of Combinatorial Generalisation",
            "venue": "arXiv 2022,",
            "year": 2022
        },
        {
            "authors": [
                "F. Locatello",
                "M. Tschannen",
                "S. Bauer",
                "G. R\u00e4tsch",
                "B. Sch\u00f6lkopf",
                "O. Bachem"
            ],
            "title": "Disentangling factors of variation using few labels",
            "venue": "arXiv 2019,",
            "year": 2019
        },
        {
            "authors": [
                "I. Higgins",
                "L. Matthey",
                "A. Pal",
                "C. Burgess",
                "X. Glorot",
                "M. Botvinick",
                "S. Mohamed",
                "A. Lerchner"
            ],
            "title": "beta-vae: Learning basic visual concepts with a constrained variational framework",
            "venue": "In Proceedings of the International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "K. Ridgeway"
            ],
            "title": "A survey of inductive biases for factorial representation-learning",
            "venue": "arXiv 2016,",
            "year": 2016
        },
        {
            "authors": [
                "Q. Wang",
                "H. Zhou",
                "G. Li",
                "J. Guo"
            ],
            "title": "Single Image Super-Resolution Method Based on an Improved Adversarial Generation",
            "venue": "Appl. Sci. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Revell",
                "G. Madeleine"
            ],
            "title": "Poetry and Art of an Artificial Intelligence",
            "venue": "Arts 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Tsai",
                "P. Liang",
                "A. Zadeh",
                "L. Morency",
                "R. Salakhutdinov"
            ],
            "title": "Learning factorized multimodal representations",
            "venue": "arXiv 2018,",
            "year": 2018
        },
        {
            "authors": [
                "W. Hsu",
                "J. Glass"
            ],
            "title": "Disentangling by partitioning: A representation learning framework for multimodal sensory data",
            "venue": "arXiv 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Z. Xu",
                "T. Lin",
                "H. Tang",
                "F. Li",
                "D. He",
                "N. Sebe",
                "R. Timofte",
                "L. Van Gool",
                "E. Ding"
            ],
            "title": "Predict, prevent, and evaluate: Disentangled text-driven image manipulation empowered by pre-trained vision-language model",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "W. Zou",
                "J. Ding",
                "C. Wang"
            ],
            "title": "Utilizing BERT Intermediate Layers for Multimodal Sentiment Analysis",
            "venue": "In Proceedings of the 2022 IEEE International Conference on Multimedia and Expo (ICME), Taipei, Taiwan,",
            "year": 2022
        },
        {
            "authors": [
                "X. Liu",
                "P. Sanchez",
                "S. Thermos",
                "A. O\u2019Neil",
                "S. Tsaftaris"
            ],
            "title": "Learning disentangled representations in the imaging",
            "venue": "domain. Med. Image Anal. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "J. Hsieh",
                "B. Liu",
                "D. Huang",
                "L. Fei-Fei",
                "J. Niebles"
            ],
            "title": "Learning to decompose and disentangle representations for video prediction",
            "venue": "Adv. Neural Inf. Process. Syst. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "E.L. Denton"
            ],
            "title": "Unsupervised learning of disentangled representations from video",
            "venue": "Adv. Neural Inf. Process. Syst. 2017,",
            "year": 2017
        },
        {
            "authors": [
                "A. Comas",
                "C. Zhang",
                "Z. Feric",
                "O. Camps",
                "R. Yu"
            ],
            "title": "Learning disentangled representations of videos with missing data",
            "venue": "Adv. Neural Inf. Process. Syst. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "V. Guen",
                "N. Thome"
            ],
            "title": "Disentangling physical dynamics from unknown factors for unsupervised video prediction",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "K. Fan",
                "C. Joung",
                "S. Baek"
            ],
            "title": "Sequence-to-Sequence Video Prediction by Learning Hierarchical Representations",
            "venue": "Appl. Sci",
            "year": 2020
        },
        {
            "authors": [
                "Y. Zou",
                "H. Liu",
                "T. Gui",
                "J. Wang",
                "Q. Zhang",
                "M. Tang",
                "H. Li",
                "D. Wang"
            ],
            "title": "Divide and Conquer: Text Semantic Matching with Disentangled Keywords and Intents",
            "venue": "arXiv 2022,",
            "year": 2022
        },
        {
            "authors": [
                "J. Dougrez-Lewis",
                "M. Liakata",
                "E. Kochkina",
                "Y. He"
            ],
            "title": "Learning disentangled latent topics for twitter rumour veracity classification",
            "venue": "In Proceedings of the Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Q. Zhu",
                "W. Zhang",
                "T. Liu",
                "W. Wang"
            ],
            "title": "Neural stylistic response generation with disentangled latent variables",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Bangkok, Thailand,",
            "year": 2020
        },
        {
            "authors": [
                "B. Lake",
                "T. Ullman",
                "J. Tenenbaum",
                "S. Gershman"
            ],
            "title": "Building machines that learn and think like people",
            "venue": "Behav. Brain Sci. 2017,",
            "year": 2017
        },
        {
            "authors": [
                "D. Kingma",
                "M. Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv 2013,",
            "year": 2013
        },
        {
            "authors": [
                "C. Burgess",
                "I. Higgins",
                "A. Pal",
                "L. Matthey",
                "N. Watters",
                "G. Desjardins",
                "A. Lerchner"
            ],
            "title": "Understanding disentangling in \u03b2-VAE",
            "venue": "arXiv 2018,",
            "year": 2018
        },
        {
            "authors": [
                "R. Chen",
                "X. Li",
                "R. Grosse",
                "D. Duvenaud"
            ],
            "title": "Isolating Sources of Disentanglement in Variational Autoencoders",
            "venue": "arXiv 2018,",
            "year": 2018
        },
        {
            "authors": [
                "H. Kim",
                "A. Mnih"
            ],
            "title": "Disentangling by factorising",
            "venue": "arXiv 2018,",
            "year": 2018
        },
        {
            "authors": [
                "A. Kumar",
                "P. Sattigeri",
                "A. Balakrishnan"
            ],
            "title": "Variational inference of disentangled latent concepts from unlabeled observations",
            "venue": "arXiv 2017,",
            "year": 2017
        },
        {
            "authors": [
                "A. Rezaabad",
                "S. Vishwanath"
            ],
            "title": "Learning representations by maximizing mutual information in variational autoencoders",
            "venue": "In Proceedings of the 2020 IEEE International Symposium on Information Theory (ISIT),",
            "year": 2020
        },
        {
            "authors": [
                "J. Hejna",
                "A. Vangipuram",
                "K. Liu"
            ],
            "title": "Improving Latent Representations via Explicit Disentanglement",
            "venue": "Available online: http://joeyhejna.com/files/disentanglement.pdf (accessed on",
            "year": 2020
        },
        {
            "authors": [
                "F. Locatello",
                "S. Bauer",
                "M. Lucic",
                "G. R\u00e4tsch",
                "S. Gelly",
                "B. Sch\u00f6lkopf",
                "O. Bachem"
            ],
            "title": "A sober look at the unsupervised learning of disentangled representations and their evaluation",
            "venue": "arXiv 2020,",
            "year": 2020
        },
        {
            "authors": [
                "W. Cho",
                "Y. Choi"
            ],
            "title": "LMGAN: Linguistically Informed Semi-Supervised GAN with Multiple Generators",
            "venue": "Sensors 2022,",
            "year": 2022
        },
        {
            "authors": [
                "X. Chen",
                "Y. Duan",
                "R. Houthooft",
                "J. Schulman",
                "I. Sutskever",
                "P. Abbeel"
            ],
            "title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
            "venue": "Adv. Neural Inf. Process. Syst. 2016,",
            "year": 2016
        },
        {
            "authors": [
                "A. Radford",
                "L. Metz",
                "S. Chintala"
            ],
            "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
            "venue": "arXiv 2015,",
            "year": 2015
        },
        {
            "authors": [
                "Z. Lin",
                "K. Thekumparampil",
                "G. Fanti",
                "S. Oh"
            ],
            "title": "Infogan-cr: Disentangling generative adversarial networks with contrastive regularizers",
            "venue": "arXiv 2019,",
            "year": 2019
        },
        {
            "authors": [
                "T. Xiao",
                "J. Hong",
                "J. Ma"
            ],
            "title": "Dna-gan: Learning disentangled representations from multi-attribute images",
            "venue": "arXiv 2017,",
            "year": 2017
        },
        {
            "authors": [
                "I. Jeon",
                "W. Lee",
                "M. Pyeon",
                "G. Kim"
            ],
            "title": "Ib-gan: Disentangled representation learning with information bottleneck generative adversarial networks",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "L. Jing",
                "Y. Tian"
            ],
            "title": "Self-supervised visual feature learning with deep neural networks: A survey",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "year": 2020
        },
        {
            "authors": [
                "L. Ericsson",
                "H. Gouk",
                "C. Loy",
                "T. Hospedales"
            ],
            "title": "Self-supervised representation learning: Introduction, advances, and challenges",
            "venue": "IEEE Signal Process. Mag",
            "year": 2022
        },
        {
            "authors": [
                "M. Schiappa",
                "Y. Rawat",
                "M. Shah"
            ],
            "title": "Self-supervised learning for videos: A survey",
            "venue": "ACM Comput. Surv. 2022",
            "year": 2022
        },
        {
            "authors": [
                "Y. Xie",
                "T. Arildsen",
                "Z. Tan"
            ],
            "title": "Disentangled Speech Representation Learning Based on Factorized Hierarchical Variational Autoencoder with Self-Supervised Objective",
            "venue": "In Proceedings of the 2021 IEEE 31st International Workshop on Machine Learning for Signal Processing (MLSP), Gold Coast, Australia,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Zhang",
                "L. Zhang",
                "X. Zheng",
                "J. Tian",
                "J. Zhou"
            ],
            "title": "Self-supervised adversarial example detection by disentangled representation",
            "venue": "arXiv 2021,",
            "year": 2023
        },
        {
            "authors": [
                "B. Kaya",
                "R. Timofte"
            ],
            "title": "Self-supervised 2D image to 3D shape translation with disentangled representations",
            "venue": "In Proceedings of the 2020 International Conference on 3D Vision (3DV),",
            "year": 2020
        },
        {
            "authors": [
                "T. Wang",
                "Z. Yue",
                "J. Huang",
                "Q. Sun",
                "H. Zhang"
            ],
            "title": "Self-supervised learning disentangled group representation as feature",
            "venue": "Adv. Neural Inf. Process. Syst. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "F. Locatello",
                "S. Bauer",
                "M. Lucic",
                "G. Raetsch",
                "S. Gelly",
                "B. Sch\u00f6lkopf",
                "O. Bachem"
            ],
            "title": "Challenging common assumptions in the unsupervised learning of disentangled representations",
            "venue": "Int. Conf. Mach. Learn. 2019,",
            "year": 2019
        },
        {
            "authors": [
                "P. Baldi"
            ],
            "title": "Autoencoders, unsupervised learning, and deep architectures",
            "venue": "In Proceedings of the ICML Workshop on Unsupervised And Transfer Learning, Bellevue, DC, USA,",
            "year": 2012
        },
        {
            "authors": [
                "P. Vincent",
                "H. Larochelle",
                "I. Lajoie",
                "Y. Bengio",
                "P. Manzagol",
                "L. Bottou"
            ],
            "title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
            "venue": "J. Mach. Learn. Res",
            "year": 2010
        },
        {
            "authors": [
                "C. Pham",
                "S. Ladjal",
                "A. Newson"
            ],
            "title": "PCA-AE: Principal Component Analysis Autoencoder for Organising the Latent Space of Generative Networks",
            "venue": "J. Math. Imaging Vis",
            "year": 2022
        },
        {
            "authors": [
                "C. Song",
                "F. Liu",
                "Y. Huang",
                "L. Wang",
                "T. Tan"
            ],
            "title": "Auto-encoder Based Data Clustering",
            "venue": "In Proceedings of the CIARP, Havana, Cuba,",
            "year": 2013
        },
        {
            "authors": [
                "M. Gogoi",
                "S. Begum"
            ],
            "title": "Image classification using deep autoencoders",
            "venue": "In Proceedings of the 2017 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC), Tamil Nadu, India,",
            "year": 2017
        },
        {
            "authors": [
                "Y. Zhang",
                "K. Lee",
                "H. Lee"
            ],
            "title": "Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classification",
            "venue": "In Proceedings of the 33rd International Conference on Machine Learning, York City, NY, USA,",
            "year": 2016
        },
        {
            "authors": [
                "M. Hoffman",
                "D. Blei",
                "C. Wang",
                "J. Paisley"
            ],
            "title": "Stochastic variational inference",
            "venue": "J. Mach. Learn. Res. 2013,",
            "year": 2013
        },
        {
            "authors": [
                "A. Jha",
                "S. Anand",
                "M. Singh",
                "V. Veeravasarapu"
            ],
            "title": "Disentangling factors of variation with cycle-consistent variational auto-encoders",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany,",
            "year": 2018
        },
        {
            "authors": [
                "C. Doersch"
            ],
            "title": "Tutorial on variational autoencoders",
            "venue": "arXiv 2016,",
            "year": 2016
        },
        {
            "authors": [
                "D. Kingma",
                "M. Welling"
            ],
            "title": "An introduction to variational autoencoders",
            "venue": "arXiv 2019,",
            "year": 2019
        },
        {
            "authors": [
                "D. Rezende",
                "S. Mohamed",
                "D. Wierstra"
            ],
            "title": "Stochastic backpropagation and approximate inference in deep generative models",
            "venue": "Int. Conf. Mach. Learn. 2017,",
            "year": 2017
        },
        {
            "authors": [
                "A. Asperti",
                "M. Trentin"
            ],
            "title": "Balancing reconstruction error and kullback-leibler divergence in variational autoencoders",
            "venue": "IEEE Access 2020,",
            "year": 1994
        },
        {
            "authors": [
                "M. Hu",
                "Z. Liu",
                "J. Liu"
            ],
            "title": "Learning Unsupervised Disentangled Capsule via Mutual Information",
            "venue": "In Proceedings of the 2022 International Joint Conference on Neural Networks (IJCNN), Padua, Italy,",
            "year": 2022
        },
        {
            "authors": [
                "Z. Liu",
                "P. Luo",
                "X. Wang",
                "X. Tang"
            ],
            "title": "Deep learning face attributes in the wild",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision, Santiago, Chile,",
            "year": 2015
        },
        {
            "authors": [
                "M. Aubry",
                "D. Maturana",
                "A. Efros",
                "B. Russell",
                "J. Sivic"
            ],
            "title": "Seeing 3d chairs: Exemplar part-based 2d-3d alignment using a large dataset of cad models",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2014
        },
        {
            "authors": [
                "P. Paysan",
                "R. Knothe",
                "B. Amberg",
                "S. Romdhani",
                "T. Vetter"
            ],
            "title": "A 3D face model for pose and illumination invariant face recognition",
            "venue": "In Proceedings of the 2009 Sixth IEEE International Conference on Advanced Video and Signal Based Surveillance,",
            "year": 2009
        },
        {
            "authors": [
                "L. Matthey",
                "I. Higgins",
                "D. Hassabis",
                "A. dSprites Lerchner"
            ],
            "title": "Disentanglement Testing Sprites Dataset",
            "venue": "2017. Available online: https://github.com/deepmind/dsprites-dataset/",
            "year": 2022
        },
        {
            "authors": [
                "M. Hoffman",
                "M. Johnson"
            ],
            "title": "Elbo surgery: Yet another way to carve up the variational evidence lower bound",
            "venue": "In Proceedings of the Workshop in Advances in Approximate Bayesian Inference,",
            "year": 2016
        },
        {
            "authors": [
                "S. Watanabe"
            ],
            "title": "Information Theoretical Analysis of Multivariate Correlation",
            "venue": "IBM J. Res. Dev. 1960,",
            "year": 1960
        },
        {
            "authors": [
                "X. Nguyen",
                "M. Wainwright",
                "M. Jordan"
            ],
            "title": "Estimating divergence functionals and the likelihood ratio by convex risk minimization",
            "venue": "IEEE Trans. Inf. Theory",
            "year": 2010
        },
        {
            "authors": [
                "M. Sugiyama",
                "T. Suzuki",
                "T. Kanamori"
            ],
            "title": "Density-ratio matching under the Bregman divergence: A unified framework of density-ratio estimation",
            "venue": "Ann. Inst. Stat. Math",
            "year": 2011
        },
        {
            "authors": [
                "R. Harrison"
            ],
            "title": "Introduction to monte carlo simulation",
            "venue": "AIP Conf. Proc",
            "year": 2010
        },
        {
            "authors": [
                "C. Eastwood",
                "C. Williams"
            ],
            "title": "A framework for the quantitative evaluation of disentangled representations",
            "venue": "In Proceedings of the International Conference on Learning Representations, Vancouver, BC, Canada,",
            "year": 2018
        },
        {
            "authors": [
                "J. Zaidi",
                "J. Boilard",
                "G. Gagnon",
                "M. Carbonneau"
            ],
            "title": "Measuring disentanglement: A review of metrics",
            "venue": "arXiv 2020,",
            "year": 2020
        },
        {
            "authors": [
                "A. Sepliarskaia",
                "J. Kiseleva",
                "M. Rijke"
            ],
            "title": "Evaluating disentangled representations",
            "venue": "arXiv 2019,",
            "year": 2019
        },
        {
            "authors": [
                "K. Ridgeway",
                "M. Mozer"
            ],
            "title": "Learning deep disentangled embeddings with the f-statistic loss",
            "venue": "Adv. Neural Inf. Process. Syst. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "S. Zhao",
                "J. Song",
                "S. Ermon"
            ],
            "title": "Towards deeper understanding of variational autoencoding models",
            "venue": "arXiv 2017,",
            "year": 2017
        },
        {
            "authors": [
                "K. Zhang"
            ],
            "title": "On mode collapse in generative adversarial networks",
            "venue": "In Proceedings of the Artificial Neural Networks and Machine Learning\u2014ICANN 2021: 30th International Conference on Artificial Neural Networks, Bratislava, Slovakia,",
            "year": 2021
        },
        {
            "authors": [
                "A. Alemi",
                "B. Poole",
                "I. Fischer",
                "J. Dillon",
                "R. Saurous",
                "K. Murphy"
            ],
            "title": "Fixing a broken ELBO",
            "venue": "Int. Conf. Mach. Learn. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "J. Liu",
                "Z. Yuan",
                "Z. Pan",
                "Y. Fu",
                "L. Liu",
                "B. Lu"
            ],
            "title": "Diffusion Model with Detail Complement for Super-Resolution of Remote Sensing",
            "venue": "Remote Sens. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "O. Benrhouma",
                "A. Alkhodre",
                "A. AlZahrani",
                "A. Namoun",
                "W. Bhat"
            ],
            "title": "Using Singular Value Decomposition and Chaotic Maps for Selective Encryption of Video Feeds in Smart Traffic Management",
            "venue": "Appl. Sci",
            "year": 2022
        },
        {
            "authors": [
                "N. Andriyanov"
            ],
            "title": "Methods for preventing visual attacks in convolutional neural networks based on data discard and dimensionality reduction",
            "venue": "Appl. Sci",
            "year": 2021
        },
        {
            "authors": [
                "D. Samuel",
                "F. Cuzzolin"
            ],
            "title": "Svd-gan for real-time unsupervised video anomaly detection",
            "venue": "Proceedings of the British Machine Vision Conference (BMVC), Virtual,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Citation: Eddahmani, I.; Pham, C.-H.;\nNapol\u00e9on, T.; Badoc, I.; Fouefack,\nJ.-R.; El-Bouz, M. Unsupervised\nLearning of Disentangled\nRepresentation via Auto-Encoding:\nA Survey. Sensors 2023, 23, 2362.\nhttps://doi.org/10.3390/s23042362\nAcademic Editor: Paolo Gastaldo\nReceived: 14 December 2022\nRevised: 11 February 2023\nAccepted: 16 February 2023\nPublished: 20 February 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: representation learning; disentanglement; auto-encoder; generative models; neural networks; metrics"
        },
        {
            "heading": "1. Introduction",
            "text": "Data representation is a crucial and long-standing issue in machine learning, as it has a significant impact on model performance [1]. For that reason, much of the actual efforts in the machine learning community have been toward representation learning [2\u20135]. Representation learning refers to finding a low-dimensional representation that captures true underlying factors of variation that explain the data [6]. A series of traditional statistical approaches have been reported for the estimation of such a low-dimensional representation, such as Principal Component Analysis (PCA) [7,8], Independent Component Analysis (ICA) [9,10] or Single Value Decomposition (SVD) [11]. They aim to identify the underlying factors of variation in the data. However, in practice, the data to be encoded may be very large in dimension and contain factors that cannot be captured with these linear methods. Disentangled representation learning has emerged as an effective way of finding a low-dimensional space of complex data while addressing the problem of identifying the independent factors of variation. Following the definition of Bengio et al. [3]: \u201ca disentangled representation is a representation where a change in one latent variable corresponds to a change in one generative factor, while being relatively invariant to changes in other factors\u201d. As an example, a model trained on a set of face images can capture different generative factors, i.e., pose, gender, skin color or smile, and encode them into independent latent variables in the representation space. Each latent variable is sensitive to\nSensors 2023, 23, 2362. https://doi.org/10.3390/s23042362 https://www.mdpi.com/journal/sensors\na change in only one generative factor. Let zk be the latent factor controlling the facial pose, then varying zk while fixing other factors would generate images of different facial poses but with the same other generative factors (gender, skin, color, smile). The same goes for latent variables controlling other generative factors. We illustrate the notation of generative factors and latent factors in Figure 1.\nGet generative factors that explains the data\nGet latent representation\nThese representations have been useful for model generalization by discovering the causal variables in data and capturing its compositional structure [12\u201315], allowing the learning systems to understand real-world observations as humans do [16], and, therefore, representation learning could generalize to unseen scenarios. For example, a model trained to generate an image of a green square and blue triangle, because of the generalizability property, the model can also generate a blue square and green triangle [17]. Following this motivation, they have been of interest to downstream tasks, such as supervised learning, compression and data augmentation. In supervised learning, it can be used as an input feature when building classifiers or other predictors to improve predictive performance [18], reduce sample complexity [19] and offer interpretability [20]. For compression [21], disentangled representations are compact and low-dimensional, thus minimizing the cost associated with storing underlying factors of variation in data. Further, they can be used to generate novel examples not found in the original dataset [21]. Such feature learning also supports a variety of other applications, such as super-resolution [22], multimodal application [23\u201327], medical imaging [28,29], video prediction [30\u201334], natural language processing [35\u201337], transfer learning and zero-shot learning [38]. A large range of state-of-the-art methods for learning unsupervised representations is based on variational auto-encoders (VAE) [39]. Variational auto-encoders have been shown to be useful for learning high-dimensional data and inferring latent variables. However, they often fail to capture a disentangled representation of the data [20]. In order to overcome these drawbacks, several variants of VAE have been proposed [40\u201344] with the idea that they could allow better disentanglement [45,46]. Another line of work in this field is based on Generative Adversarial Networks (GAN) [47,48]. Numerous variants of GAN have been proposed and demonstrated the ability to learn a disentangled representation [49\u201353] and were reported to have comparable performance to VAE-based methods [53]. A relative field with disentangled representation learning is Self-Supervised Learning (SSL) [54\u201356]. Self-supervised learning provides a way for learning representation from unlabeled data. Recent efforts have been made toward using self-supervised algorithms in order to learn a disentangle representation [57\u201360]. However, recent studies have reported that the existing SSL methods often struggle to learn disentangled representations of the data [60]. In this paper, we aim to provide a systematic and comprehensive survey of VAE-based approaches. We attempt to shed light on some VAE variants that are considered state-ofthe-art of disentangled representations and to provide an analysis of the common idea behind all these approaches. Further, we conduct an extensive review of disentanglement metrics, where we explore what makes one metric better than another. Finally, we discuss limitations and future directions in this field of research. We sum up the recent work focusing on disentanglement and the metric proposed alongside each method in Table 1.\nTo the authors\u2019 best knowledge, [61] is the only study focusing on disentanglement representation methods from a practical point of view. In [61], the authors introduce a library to train and evaluate disentangled representations. However, a detailed description of the methods is missing. On the other hand, no other study performs this type of review focused on grouping disentanglement methods as well as metrics and presents a theoretical analysis and a detailed description of each method and their added value. The purpose of this survey is to provide researchers interested in this broad field with a comprehensive overview of state-of-the-art approaches and establish a guideline to choose a suitable approach given an objective. The rest of this paper is organized into six sections. In Section 2, we describe the main concepts that are necessary to understand the methods considered in this work. In Section 3, we review unsupervised disentanglement methods based on the auto-encoder baseline. In Section 4, we review the most well-known metrics to evaluate disentanglement. In Section 5, we present a detailed discussion of disentanglement methods and a comparison between metrics. Finally, we conclude the work and discuss its future scope in Section 6."
        },
        {
            "heading": "2. Background",
            "text": "In this section, we provide a detailed description of several notions that will be found throughout this paper."
        },
        {
            "heading": "2.1. Auto-Encoders",
            "text": "An auto-encoder [1,3,62\u201364] is a neural network architecture that is trained to reconstruct its input [65] with the least possible amount of distortion. Their main purpose is to learn a compressed meaningful representation of the data that can be used for various applications, including clustering [66] and classification [67,68].\nHere we briefly describe the auto-encoder (AE) framework: Encoder: A neural network f that maps an input (image, tensor, curve) into a hidden\nrepresentation Z capturing the significant underlying factors of the data, also called an inference model. Given a data set, {x1, . . . , xT}, for each xi, we define:\nzi = f (xi), (1)\nLatent space: A low-dimensional representation of the data. The vector z is the featurevector, also called latent code or latent dimension. z is called \u201clatent\u201d because it is a variable produced by the model from the input data.\nDecoder: A neural network g that builds back the input from its latent vectors.\nx\u0303i = g(zi), (2)\nTraining an auto-encoder consists of learning the functions f and g that minimize the error E of the reconstruction loss function4, which measures the difference between the input and its reconstruction:\narg min f ,g E[4(xi, g( f (xi)))] (3)"
        },
        {
            "heading": "2.2. Variational Auto-Encoders (VAEs)",
            "text": "Kingma et al. [39] introduce a stochastic variational inference for an auto-encoder. Variational auto-encoders attempt to describe data generation through a probabilistic modeling perspective [69]. In VAE, inputs are encoded as a distribution over latent space instead of as single points [70]. In doing so, Kingma et al. assume a posterior distribution on the latent variable zi for each data point xi denoted by inference model q\u03c6(z|x). This inference model corresponds to the probabilistic encoder, and is parameterized by \u03c6 [65]. In a similar vein, they introduce a generative model p\u03b8(x|z), which is equivalent to a probabilistic decoder determined by the parameter \u03b8: given a latent variable z it returns a distribution on the corresponding possible values x. Finally, they consider a prior distribution over the latent variables zi denoted by p\u03b8(zi), where p\u03b8(z) is a standard multivariate normal distribution N (0, I) and I is the identity matrix. Training a VAE consists of simultaneously learning the parameters \u03c6 and \u03b8. One way to estimate these parameters is to use the maximum log-likelihood (ML), a common criterion for probabilistic models. The marginal log-likelihood is the sum of each data points log p\u03b8(x1, . . . , xN) = \u2211Ni=1 log p\u03b8(xi). Each point can be rewritten as [65]:\nlog p\u03b8(xi) = DKL ( q\u03c6(z|xi)||p\u03b8(z|xi) ) + L(\u03b8, \u03c6, xi) (4)\nThe first term in Equation (4) is the Kullback\u2013Leibler (KL) divergence, which determines the distance between the approximate posterior and the true posterior, and the second term is the variational lower bound defined in [69] as:\nL(\u03b8, \u03c6, xi) = Eq\u03c6(z|xi) [ \u2212 log q\u03c6(z|x) + log p\u03b8(x, z) ] (5)\nSince the KL divergence is non-negative,L(\u03b8, \u03c6, xi) is the lower marginal log-likelihood bound, also known as the Evidence Lower Bound Objective (ELBO). This can be further expressed as:\nLVAE ' L(\u03b8, \u03c6, xi) = Eq\u03c6(z|xi)[log p\u03b8(xi|z)]\u2212DKL ( q\u03c6(z|xi)||p\u03b8(z) ) (6)\nRelying on Equation (6), one can notice that the evidence lower bound is a sum of two terms: the first term is a negative reconstruction error that needs to be maximized in order to increase the reconstruction capability of the sample, and the second term is the KL divergence that acts as a regularizer to ensure that the approximate posterior q\u03c6(z|x) remains close to the prior. Finding the model parameters \u03b8 and \u03c6 that will maximize the marginal likelihood of the data while simultaneously minimizing the KL divergence between the approximation q\u03c6(z|x) and the prior p\u03b8(z) is equivalent to maximizing the ELBO. It can thus be said that training a variational auto-encoder consists of maximizing the variational lower bound objective. The ELBO (Equation (6)) serves as the core of the variational auto-encoder and the methods we will discuss in the rest of this paper, so it is worth spending some thinking about how it can be optimized [71]:\nmax \u03c6,\u03b8\nEp(xi) [ Eq\u03c6(z|xi)[log p\u03b8(xi|z)]\u2212 DKL(q\u03c6(z|xi)||p\u03b8(z)) ] (7)\nAs is common in machine learning, the ELBO can be optimized with regard to all parameters (\u03c6 and \u03b8) using stochastic gradient descent [72], but first, more detail about q\u03c6(z|xi) is required. The usual choice is a simple factorized Gaussian encoder q\u03c6(z|xi) \u223c N ( \u00b5\u03c6(xi), \u03c3\u03c6(xi) ) , where \u00b5\u03c6(xi) and \u03c3\u03c6(xi) are the mean and standard deviation implemented via neural networks and \u03c3\u03c6(xi) is constrained to be a diagonal matrix. Under this choice, we have a KL divergence between two Gaussian distributions, which is tractable. Hence we can calculate the gradient of the last term of Equation (7). However, the first term is a bit trickier as it requires an estimation by sampling from q\u03c6(z|xi). Kingma et al. [39,72] propose to estimate the marginal likelihood lower bound of the full dataset using mini-batches of M data-points and then average the gradient over these mini-\nbatches. However, stochastic gradient descent via back-propagation cannot handle stochastic variables within the network. To solve this problem and generalize back-propagation through random sampling, they propose another way to generate samples from q\u03c6(z|xi); this solution is called the \u201creparameterization trick\u201d [39,73]. The key behind this trick is to define z as a deterministic function z = g(\u03c6, xi,\u03be), then sample from the posterior q\u03c6(z|xi) using z = \u00b5(xi) + \u03c3(xi) \u2217 \u03be. Here, \u03be is an auxiliary variable with independent marginal p(\u03be) \u223c N (0, I), and g\u03c6(.) is a vector-valued function parameterized by \u03c6. In doing so, we are keeping the stochasticity of the variables, but also, we have a deterministic function of inputs that will work for stochastic gradient descent. The architecture of a variational auto-encoder (VAE) is shown in Figure 2.\nFor ease of reference, we sum up in Table 2 the main terms and corresponding mathematical symbols used in this work."
        },
        {
            "heading": "2.3. Reconstruction Error",
            "text": "In order to make the optimization of the evidence lower bound objective (Equation (7)), the posterior q\u03c6(z|xi) is pushed to match the unit Gaussian prior p\u03b8(z) \u223c N (0, I). Since the posterior q\u03c6(z|xi) and the prior p\u03b8(z) are factorized (i.e., have diagonal covariance matrix) and the samples from q\u03c6(z|xi) are generated using the reparameterization trick, learning a representation of the data depending only on q\u03c6(z|xi) may result in a meaningless representation where only a limited number of latent variables are exploited for data reconstruction. In doing so, the amount of information that can be transmitted through the latent channels is reduced. Thus, this results in high reconstruction errors and low reconstruction fidelity [74]."
        },
        {
            "heading": "2.4. Mutual Information Theory",
            "text": "Mutual information is a fundamental quantity for measuring dependency between random variables [75]. Let (X, Z) be a couple of random variables. The mutual information (MI) between X and Z, denoted as I(X; Z), is:\nI(X; Z) = DKL(PXZ||PX \u2297 PZ) (8)\nwhere DKL is the Kullback\u2013Leibler (KL) divergence between the joint distribution and the product of the marginals."
        },
        {
            "heading": "3. Methods",
            "text": "The major challenge behind representation learning is how we can choose the model that leads to better disentanglement and thus can be useful for later downstream tasks. In this section, we present an overview of the state-of-the-art frameworks in representation learning based on auto-encoding (see Table 1)."
        },
        {
            "heading": "3.1. \u03b2-Variational Auto-Encoder",
            "text": "Higgins et al. [20] introduce a variant of variational auto-encoders [39], \u03b2-VAE, a deep generative (unsupervised) algorithm for learning disentangled representations. The authors propose to modify the evidence lower bound objective (ELBO) by up weighting the KL divergence term in Equation (7) in order to learn a disentangled representation:\nmax \u03c6,\u03b8\nEp(xi) [ Eq\u03c6(z|xi)[log p\u03b8(xi|z)]\u2212 \u03b2DKL ( q\u03c6(z|xi)||p\u03b8(z) )] (9)\nwhere \u03b2 is an adjustable hyper-parameter higher than 1. It can be noted that \u03b2-VAE with \u03b2 = 1 is equivalent to the original VAE framework [39]. Re-writing the ELBO for \u03b2-VAE:\nL\u03b2-VAE = Eq\u03c6(z|xi) [ log p\u03b8(xi|z)]\u2212 \u03b2DKL ( q\u03c6(z|xi)||p\u03b8(z) )] (10)\nSuch a penalization causes the posterior q\u03c6(z|x) to better match the factorized prior p\u03b8(z), which is associated with the need to maximize the log-likelihood of data x and push the model to learn a disentangled representation of the data. \u03b2-VAE was performed using a number of benchmarks with known ground truth factors, such as CelebA [76] (202,599 color images of celebrity faces), Chairs [77] (86,366 color images of chairs), Faces [78] (239,840 gray-scale images of 3D faces) and 2D Shape [79] (737,280 synthetic images of 2D shapes such as heart, oval and square). The size of the images in the four datasets is 64\u00d7 64 pixels."
        },
        {
            "heading": "3.2. InfoMax-Variational Auto-Encoder",
            "text": "As we can see from Equation (10), learning representation depending only on q\u03c6(z|x) may result in meaningless representations and, therefore, a poor reconstruction quality. To avoid collapsed representations, Rezaabad et al. [44] report a simple yet practical method to build a meaningful representation. They propose to extend the evidence lower bound (ELBO) with a regularizer term that maximizes the mutual information between the data and the latent representation. In so doing, the model is pushed to maximize the information about the data (input) stored in the inferred representation (latent representation) [44]. This solution is referred to as InfoMax-VAE. They end up with the following ELBO:\nmax \u03c6,\u03b8\nEq(x) [ L\u03b2-VAE ] + \u03b1Iq\u03c6(x, z) (11)\nwhere \u03b2 and \u03b1 \u2265 0 are regularization coefficients for the KL divergence and mutual information. Varying \u03b1 controls the amount of information stored in the latent representation, also known as information preference.\nFollowing [80], the mutual information is estimated by the average KL divergence between the joint and associated marginals: (Iq\u03c6(x, z) = KL ( q\u03c6(x; z)||q(x)\u2297 q\u03c6(z) ) . However, the KL divergence is hard to compute in general due to the intractable posterior, so Rezaabad et al. argue [44] that since KL divergence comes from a large class of different divergence, we can replace this term with another variational f -divergence D f (t) where t represents all possible functions. Thus, they arrive at the following equation:\nmax \u03c6,\u03b8\nEq(x) [ L\u03b2-VAE ] + \u03b1D f ( q\u03c6(x, z)||q(x)q\u03c6(z) ) (12)\nSpecifically, they choose f (t) to be t log t (more details about this choice can be found in [44]), arriving to the final ELBO for InfoMax-VAE:\nLInfoMax-VAE = L\u03b2-VAE + \u03b1 ( Eq\u03c6(x,z)[t(x, z)]\u2212Eq(x)q\u03c6(z)[exp(t(x, z)\u2212 1)] ) (13)\nThis leaves the task of evaluating Eq\u03c6(x,z) and Eq(x)q(z). They propose a simple yet practical way to do so: first of all, and thanks to the reparameterization trick [72], they draw samples from q(x), (xi, zi) \u223c q\u03c6(x, z) = q\u03c6(z|x)q(x). Afterward, to get samples from the marginal q\u03c6(z), they choose a random data point xj, followed by sampling from z \u223c q\u03c6(z|xj). The InfoMax-VAE was performed using the CelebA dataset and has been shown to be capable of learning meaningful and disentangled representation and outperforms \u03b2-VAE."
        },
        {
            "heading": "3.3. Factor Variational Auto-Encoder",
            "text": "Kim and Mnih [42] adopt another decomposition of the ELBO, specifically, they decompose the KL term in Equation (6) as Hoffman and Johnson propose in [81,82]:\nEpdata(x)[DKL(q(z|x)||p(z))] = I(x; z) + DKL(q(z)||p(z)) (14)\nwhere I(x; z) is the mutual information between x and z and q(z) = Epdata(x) [q(z|x)] = 1 N \u2211 N i=1 q(z|xi) is the latent distribution for all data. Penalizing DKL[q(z)||p(z)] encourages q(z) to match the factorized prior p(z) and, therefore, encourages disentanglement. Further, penalizing I(x; z) reduces the information about the data x kept in the latent space z, which might result in less accurate reconstructions for high values of \u03b2. Based on this observation, Kim and Mnih [42] argue that it may not be necessary or desirable to penalize the mutual information between x and z in order to have a better disentanglement. However, they propose to add an additional term to the VAE objective (Equation (6)) that penalizes the dependence of variables within the latent space [46]:\nEp(x)[Eq\u03c6(z|x)[log p\u03b8(x|z)]\u2212 DKL(q\u03c6(z|x)||p(z))]\u2212 \u03b3DKL(q(z))|| d\n\u220f i=1 q(zj) (15)\nRe-writing the ELBO for Factor-VAE:\nLFactor-VAE = LVAE \u2212 \u03b3DKL(q(z)|| d\n\u220f i=1 q(zj)) (16)\nThe second term is total correlation (TC)[83], a general measure of dependence between several random variables. This term is intractable since the estimation of both q(z) and q(zj) requires a pass through the entire data set. Hence Kim and Mnih [42] propose another alternative for optimizing this term using the density ratio trick [84,85]. The density\nratio trick consists of training a binary classifier/discriminator that returns the probability d(z) that its input was sampled from q(z) rather than from q(zj):\nTC(z) = DKL(q(z)||q(zj)) = Eq(z)][log q(z) q(zj) ] ' Eq(z)[log d(z) 1\u2212 d(z) ] (17)\nThe VAE and the discriminator are trained jointly. The VAE parameters are finetuned using the objective in Equation (16) with the total correlation term changed to its approximation in Equation (17). Factor-VAE was performed using several benchmarks, such as CelebA, Chairs and Faces. Moreover, two synthetic datasets were used: 2D Shapes and 3D Shapes containing 480,000 64\u00d7 64\u00d7 3 RGB images of 3D shapes [79]."
        },
        {
            "heading": "3.4. \u03b2-Total Correlation Variational Auto-Encoder",
            "text": "Concurrently to Kim and Mnih [42], Chen et al. [41] proposed another approach that surpasses both \u03b2-VAE performance and Factor-VAE complexity. \u03b2-TCVAE is a deep unsupervised approach for learning disentangled representation, a replacement of \u03b2-VAE, with no additional hyper-parameters during training. Chen et al. [41] proposed a different decomposition of the second term in Equation (10), arriving at the following split up:\nEp(x)[DKL(q\u03c6(z|x)||p(z))] =DKL(q(z, x)||q(z)p(x)) + DKL(q(z)||\u220f j q(zj))\n+ \u2211 j\nDKL(q(zj)||p(zj)) (18)\nThe first term is known as the index-code mutual information (MI), which denotes the mutual information between the data and the latent space. The second term is total correlation (TC). The last term is the dimension-wise KL, which primarily encourages the latent dimensions to better match their corresponding priors. According to Chen et al. [41], the total correlation term in the ELBO is the one that affects disentanglement. To verify this claim, the TC-term is evaluated using a Monte-Carlo approximation [86]. A unique integer index is assigned to each training sample, and they use the following estimator given a mini-batch of samples {n1, n2, . . . , nM}:\nEq(z)[log q(z)] ' 1 M\nM\n\u2211 i=1\n[ log\n1 NM\nM\n\u2211 j=1\nq(z(ni)|nj) ]\n(19)\nwhere q(z|ni) is close to 0 for a randomly sampled component but large if z comes from component ni. To achieve better disentanglement, the authors up-weight each term of the ELBO individually. Re-writing the \u03b2-TCVAE objective:\nL\u03b2-TCVAE = L\u03b2-VAE \u2212 \u03b1Iq(z; n)\u2212 \u03b2KL(q(z)||\u220f j q(zj)) (20)\n\u03b2-TCVAE was performed using the same datasets as Factor-VAE (CelebA, Chairs, Faces and 3D Shapes)."
        },
        {
            "heading": "3.5. DIP-Variational Auto-Encoder",
            "text": "Another line of work has argued that pushing the posterior q\u03c6(z) to match a factorized prior p(z) can lead to a better disentanglement. Kumar et al. [43] added a regularizer to the ELBO to encourage disentanglement during inference, therefore:\nLVAE \u2212 \u03bbD(q(z)||p(z)) (21)\nwhere \u03bb is a hyper-parameter controlling its effect on the evidence lower bound objective, and D is an (arbitrary) divergence. In order to estimate this term, they suggest matching the moments of these distributions. In particular, they propose to penalize the `2 distance between q\u03c6(z) and N (0, 1) in order to match their covariances.\nLet us denote:\nCovq\u03c6(z)[z] = Ep(x)Covq\u03c6(z|x)[z] + Covp(x)(Eq\u03c6(z|x)[z]) (22)\nwhere Eq\u03c6(z|x)[z] and Covq\u03c6(z|x)[z] are random variables that are functions of random variable x. Since q\u03c6(z|x) \u223c N ( \u00b5\u03c6(x), \u2211 \u03c3(x) ) , Equation (22) becomes:\nCovq\u03c6(z)[z] = Ep(x) [ \u2211 \u03c3\u03c6(x) ] + Covp(x) [ \u00b5\u03c6(x) ] (23)\nKumar et al. [43] explored two options for disentangling regularizers to get this term close to the identity matrix: (i) regularizing the deviation of Covp(x) [ \u00b5\u03c6(x) ] from the identity matrix, which they refer to as DIP-VAE-I. (ii) regularizing Covq\u03c6(x)[z], which they denote as DIP-VAE-I I. Maximizing the objective of either DIP-VAE-I Equation (24) or DIP-VAE-I I Equation (25) leads to better disentanglement.\nLDIP-VAE-I = LVAE \u2212 \u03bb1 \u2211 i 6=j\n[ Covp(x) [ \u00b5\u03c6(x) ]]2 ij \u2212 \u03bb2 \u2211\ni\n([ Covp(x) [ \u00b5\u03c6(x) ]] ii \u2212 1 )2\n(24)\nLDIP-VAE-II = LVAE \u2212 \u03bb1 \u2211 i 6=j\n[ Covq\u03c6 [z]ij ]2 \u2212 \u03bb2 \u2211\ni\n([ Covq\u03c6 [z] ] ii \u2212 1 )2\n(25)\nDIP-VAE was performed using three datasets: CelebA, 3D Chairs and 2D Shapes. DIP-VAE has been shown to be superior to \u03b2-VAE and capable of learning disentangled factors without having any conflict between disentanglement and quality reconstruction. To fairly evaluate such methods, the authors have chosen the same CNN architecture. Table 3 describes the experimental details, including the encoder and decoder architectures:"
        },
        {
            "heading": "4. Metrics",
            "text": "In order to evaluate the approaches described above and use them for downstream tasks, a metric of disentanglement is required. Most prior works relied on a visual inspection of the latent representation [87], but recently more rigorous metrics have been proposed.\nTo the authors\u2019 best knowledge, [88,89] are the only studies analyzing disentanglement metrics. In this review, we choose to focus on discussing the metrics proposed alongside each method above Table 1), clarifying their strengths and shortcomings."
        },
        {
            "heading": "4.1. Zdi f f Score",
            "text": "Higgins [20] introduced a disentanglement metric called Z-diff, also known as the \u03b2-VAE metric based on the following intuition: if one generative factor is fixed while randomly sampling all others, we will have a disentangled representation in which the latent variable corresponding to the fixed generative factor will vary less than the others. Applying this metric involves following these steps:\n1. Randomly select a generative factor fk. 2. Create a batch of couples vectors, p1 and p2, where the value of the chosen factor fk is\nkept fixed and equal within the pair while the other generative factors fk\u22121 are chosen randomly. For a batch of L samples:\np1 = (x1,1, . . . , x1,L), p2 = (x2,1, . . . , x2,L)\nwith x1,L = x2,L 3. Map each generated pair to a pair of latent variables using the inference model\nq(z|x) \u223c N(\u00b5(x), \u03c3(x)).\nz1,1 = \u00b5(x1,1), z2,1 = \u00b5(x2,1)\n4. Compute the value of the absolute linear difference between the variables related to the sample:\ne = (|z1,1 \u2212 z2,1|, . . . , |z1,L \u2212 z2,L|)\n5. The mean of all pair differences in a batch gives a single instance in the final training set. These steps are repeated for each generative factor in order to create a substantial training set. 6. Train a linear classifier on the generated training set to predict which generative factor has been fixed. 7. Zdi f f score, also known as \u03b2-VAE metric, is the accuracy of the classifier.\nIn a perfectly disentangled representation, we would expect a zero in the dimension of the training input associated with the fixed generative factor, and the classifier would learn to map the zero-value index to the factor index."
        },
        {
            "heading": "4.2. Zmin Variance Score",
            "text": "To overcome certain weaknesses of Zdi f f score, Kim and Mnih [42] introduced an unsupervised metric called Z-min Variance, also known as Factor-VAE metric. The intuition behind this metric is the same as the \u03b2-VAE metric with some improvements: a change in the way the latent representation is formed when a generative factor is fixed, in addition to the use of a specific type of classifier to predict which factor has been fixed. Calculating the Factor-VAE metric requires these steps:\n1. Randomly choose a generative factor fk. 2. Generate a batch of vectors, where the value of the selected factor fk is held fixed in\nthe batch while the other generative factors fk\u22121 are randomly selected. For a batch of L samples:\np1 = (x1,1, . . . , x1,L)\n3. Map each generated vector to latent code using the inference model:\nq(z|x) \u223c N(\u00b5(x), \u03c3(x))\n4. Normalize each variable within the latent representation using its empirical standard deviation calculated on the dataset. For a batch of L samples:\n(z1/s . . . zL/s)\n5. Calculate the empirical variance in each code of the normalized representations.\ne = (Varz1/s . . . VarzL/s)\n6. The factor index k and the latent variable index that has the lowest variance provide a training instance for the classifier. The factor index k and the index of the code dimension with the lowest variance give one training point for the classifier. These steps are repeated for each generative factor in order to create a substantial training set. 7. Train a majority vote classifier on the generated training set to predict which generative factor was fixed. 8. Zmin Variance score is equivalent to the classifier accuracy.\nBy normalizing the latent representations, the authors ensure that the argmin is insensitive to the rescaling of the representation in each latent variable. For a perfectly disentangled representation, one expects to have an empirical variance of zero in the dimension corresponding to the fixed factor."
        },
        {
            "heading": "4.3. Mutual Information Gap (MIG Score)",
            "text": "Chen et al. [41] introduced a new disentanglement metric based on mutual information theory. Mutual Information Gap (MIG) computes the mutual information (MI) between each generative factor xi and latent code zj. Higher mutual information denotes a deterministic relationship between zj and xj. The mutual information gap score can be estimated through the steps below:\n1. Calculate the mutual information between each pair of latent variables and known generative factors. 2. Each generative factor may have high mutual information with several latent variables. Therefore, for every single factor, classify latent variables according to the amount of information they stored about this factor. 3. Calculate the difference between the top two values of mutual information for each generative factor. 4. Normalize this difference by dividing by the entropy of the corresponding generative factor. 5. The Mutual Information Gap (MIG) score is equivalent to the average of these differences [41]:\nMIG(x, z) = 1 K \u2211k 1 H(xk)\n( I(zj(k) ; xk)\u2212 max\nj 6=j(k) I(zj; xk)\n) (26)\nwhere j(k) = arg maxj I(zj, xk), and K is the known generative factors."
        },
        {
            "heading": "4.4. Attribute Predictability Score (SAP)",
            "text": "In parallel to the MIG score, Kumar et al. [43] provided a metric of disentanglement also based on Mutual Information. Applying this metric requires these steps:\n1. For each generative factor, compute the R2 score of linear regression (for continuous factors) or classification score (balanced accuracy for categorical factors) of predicting a j-th generative factor using only a i-th variable in the latent representation. 2. Compute the difference between the top two most-predictive latent codes.\n3. The mean of those differences is the Attribute Predictability Score (SAP) [43].\nSAP(x, z) = 1 K \u2211k ( IiK ,K \u2212maxj#ik Ij,k ) (27)\nwhere ik = arg maxi Ii,k, and K is the number of known generative factors."
        },
        {
            "heading": "5. Discussion",
            "text": "In this section, we discuss the methods and metrics presented in this review and highlight associated opportunities and open challenges."
        },
        {
            "heading": "5.1. Methods",
            "text": "The computational methods aim to use variational encoding along with different ELBO decompositions to learn the disentangled representation of the data. The shared point between each of these methods is either up-weighting the VAE objective, Equation (10), or adding some regularizers to the VAE objective that act to match the approximate posterior q\u03c6(z) to the factorized prior p\u03b8(z). Figure 3 illustrates this idea:\nBy merely up-weighting the ELBO of VAE, the posterior q\u03c6(z|x) is pushed to correspond to the factorized prior p\u03b8(z), which results in a better disentanglement in comparison to the variational auto-encoder. \u03b2-VAE has shown acceptable performance. However, this penalization increases the tension between maximizing the data likelihood and disentanglement. As a result, there is a compromise between the accuracy of the reconstruction and the quality of disengagement within the latent representations learned. A higher value of \u03b2 allows the achievement of better disentanglement but restricts latent channel information capacity and, therefore, a loss of information as it crosses this limited capacity latent z. The loss of high-frequency details about the data leads to poor reconstruction quality. InfoMax-VAE outperforms \u03b2-VAE by constraining the latent representation so that the quantity of information kept about the observed data is maximized. In doing so, InfoMaxVAE is capable of obtaining high disentangling performance while maintaining a better reconstruction quality.\nFactor-VAE ensures independence in the latent space by penalizing the total correlation term in the ELBO. A higher value of \u03b3 leads to a lower total correlation and, therefore, encourages independence in the code distribution. On the other hand, by not penalizing the mutual information, Factor-VAE keeps the information stored in z about x. Thus the model preserves high-frequency details about the data. By doing so, Factor-VAE improves upon \u03b2-VAE and has been reported to achieve a better balance between disentanglement and reconstruction quality. However, this model mostly remains difficult to train since it calls for an auxiliary discriminator and an internal optimization loop. On the other hand, the addition of a hyper-parameter during training may affect the model stability. In \u03b2-TCVAE, the authors confirm the importance of total correlation for learning disentangled representation, and they propose an improvement in \u03b2-VAE and Factor-VAE. After breaking down the ELBO to a KL divergence term, mutual information and total correlation, they claim that adjusting \u03b2 produces the best results. Thus they set \u03b1 = \u03b3 = 1, arriving at the same objective as Factor-VAE but with a simple way to estimate the total correlation without any additional hyper-parameter for more stable training. \u03b2-TCVAE has been capable of capturing independent factors in data distribution without having any degradation in the quality of reconstruction, thus surpassing \u03b2-VAE. However, we have the same disentanglement performance as Factor-VAE but in a simple manner to compute the total correlation. DIP-VAE attempts to improve the performance of disentanglement by encouraging independence during inference. Having a disentangled prior that can be the basis for a generative disentangled model is the key idea behind DIP-VAE. DIP-VAE pushes the aggregated posterior q\u03c6(z) to correspond to a factorized prior p(z) by matching the moments of the two distributions. By doing so, DIP-VAE is capable of learning disentangled representation without introducing any trade-off between disentangling latent variables and maximizing the data likelihood. As a result, DIP-VAE has a better reconstruction quality contrary to \u03b2-VAE and with similar performance to Factor-VAE and \u03b2-TCVAE with learning disentangled representation without introducing a compromise between disentangling latent variables and the plausibility of the observed data.\nIn Table 4, we summarize the different choices of the regularizers applied for each method."
        },
        {
            "heading": "5.2. Metrics",
            "text": "It is currently unclear what exactly makes a disentangled metric better than another, but before analyzing metrics, we propose three criteria that constitute a disentangled representation, and we seek to analyze to what extent the metrics respect these criteria.\n5.2.1. Properties of a Disentangled Representation\nModularity: changes in one factor have no impact on other factors [90]. The same analogy is in the representation space, and the factors are also independent. This property is also known as disentanglement in [87].\nCompactness: the extent to which each generative factor is entered by one latent variable [88]. In other words, varying an underlying factor should have as small as possible effect on the latent space. Ideally, each generative factor is associated with only one latent code. In [87], the author refers to these criteria as completeness. Informativeness: the amount of information shared between latent variables and generative factors [87]. In other words, the value of a given factor can be precisely determined from the code. This criterion is also known as explicitness in [90].\n5.2.2. Comparison\nPrevious attempts to quantify disentangling have considered different aspects of modularity, compactness and informativeness criteria. Higgins [20] argues that a good representation is one where the modularity of the latent representation holds. To make sure this property holds, he assumes that generative factors are independent and quantify the independence of the inferred latent variables using a simple classifier. However, this metric has several weaknesses. The Zdi f f score is based on the classifier used to achieve the score. Nevertheless, the classifier could be sensitive to several hyper-parameters, such as the choice of the optimizer, the initialization of the weights and the epochs. Furthermore, there may be a perfectly disentangled representation where a generative factor corresponds to several latent variables instead of one variable. Thus Zdi f f does not satisfy the compactness property. For example, if we fix this generative factor, the corresponding latent codes will have a variation of 0. Therefore, the classifier fails to distinguish between the latent codes and returns an accuracy of less than 1. Finally, the \u03b2-VAE metric does not require any assumptions about the factor-code relationship and therefore does not satisfy informativeness criteria. Zmin Variance addresses several issues of the \u03b2-VAE metric. For instance, a majority vote classifier that predicts the fixed generative factor according to the variation of the latent variables actually allows having fewer additional hyper-parameters to optimize and, therefore, having a more reliable final score. On the other hand, and similar to the \u03b2-VAE metric, Zmin Variance depends on data with an independent factor. Furthermore, Zmin variance satisfies the compactness property by fixing the number of subsets of data and generating a training set that covers all possible combinations of factor-latent codes while maintaining a fixed factor. However, it does not require any assumptions about the factor-code relations. Thus it does not fulfill the informativeness criteria. Unlike the Zdi f f or Zmin metrics, the SAP score does not require any additional classifier and, therefore, returns a more reliable final score. The SAP metric satisfies informativeness and compactness criteria by computing the score of predicting each generative factor using a single variable in the latent representation. However, it does not penalize the modularity between generative factors. The main advantage of the Mutual Information Gap (MIG) score is that it does not require many additional hyper-parameters contrary to \u03b2-VAE and Factor-VAE. Moreover, it encourages the compactness of the representation by pushing only one latent variable to be informative about a factor. By definition, it computes the amount of information shared between latent variables and generative factors.\nTable 5 summarizes the findings from our analysis."
        },
        {
            "heading": "6. Conclusions and Future Directions",
            "text": "In this work, we conduct an extensive survey of disentangled representation learning through five state-of-the-art approaches focused on variational auto-encoders, alongside a detailed study on how to quantify disentanglement as well as conducting a comparison on the state-of-the-art of supervised disentanglement metrics. We highlighted the underlying processes of disentangled representation learning methods driven by the development and deployment of computer vision algorithms using deep neural network approaches. In fact, each method considered herein is a variant of variational auto-encoder, and more precisely, they only vary the VAE objective, known as the Evidence Lower Bound Objective (ELBO). In particular, these methods either (i) up-weight the evidence lower bound, (ii) add a regularization to the ELBO that acts to match the approximated posterior to the factorized prior or (iii) combine a regularization and overweight the ELBO. By just up-weighting the evidence lower bound objective, one can observe a clear trade-off between disentanglement and reconstruction quality, which is the case for \u03b2-VAE. On the other hand, adding a regularizer to the evidence lower bound objective (Factor-VAE, InfoMax-VAE and DIP-VAE) or up-weighting ELBO while adding a regularizer (\u03b2-TCVAE) allows better disentanglement while preserving reconstruction quality. Further, this study performs a comprehensive analysis and a fair comparison of the most well-known supervised disentanglement metrics. It considered three criteria that make one disentangled representation better than another (i) modularity, (ii) compactness and (iii) informativeness, and then compared metrics with respect to each criterion. We found that it is difficult to satisfy all three criteria at the same time. Most of the existing metrics meet one or two out of the three criteria. Only the mutual information gap score is robust to these criteria and able to give a general measure of the disentanglement quality. However, some limitations remain unresolved and could be addressed in further work in order to improve the relevance of disentangled representation learning approaches in future work. (i) Despite the empirical success, existing disentangled representation learning approaches tend to ignore the latent variables and produce unrealistic, blurry samples with a significant reconstruction error when applied to complex datasets. There are several papers that discuss the issue of latent variable collapse [91\u201394], but more analysis on this issue is needed. (ii) Most of the approaches are based on VAE or GAN, more research on other potential models, e.g., diffusion model [95], would allow new ways for disentangled representation learning. (iii) Although disentangled representation learning has achieved several successes in generalization to unseen scenarios, state-of-theart approaches for learning such representations have so far only been evaluated on small synthetic datasets. It will be interesting to explore the ability to generalize on complex real-world datasets. (iv) Finally, recent works discussed how to handle visual attacks and anomaly detection using dimensionality reduction, such as the SVD algorithm for neural networks and GAN [96\u201398]. Inspired by these works, it will be interesting to explore the application of disentangled representation learning while preventing visual attacks. Moving forward, this survey is not only useful to provide insights for researchers that are currently working in the related area but can also be used as a basis for the implementation of new approaches. Our current goal is that we can build on these methods to develop an approach capable of identifying the underlying generative factors on more challenging datasets for a real-world application in an industrial environment.\nAuthor Contributions: Conceptualization, I.E., C.-H.P., T.N. and M.E.-B.; methodology, I.E., C.-H.P, T.N. and M.E.-B.; writing\u2014original draft preparation, I.E.; writing\u2014review and editing, C.-H.P., T.N., M.E.-B. and J.-R.F.; supervision, M.E.-B. and I.B. All authors have read and agreed to the published version of the manuscript.\nFunding: This research received no external funding.\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nConflicts of Interest: The authors declare no conflict of interest."
        }
    ],
    "title": "Unsupervised Learning of Disentangled Representation via Auto-Encoding: A Survey",
    "year": 2023
}