{
    "abstractText": "Deep learning (DL) models have been popular due to their ability to learn directly from the raw data in an end-to-end paradigm, alleviating the concern of a separate error-prone feature extraction phase. Recent DL-based neuroimaging studies have also witnessed a noticeable performance advancement over traditional machine learning algorithms. But the challenges of deep learning models still exist because of the lack of transparency in these models for their successful deployment in real-world applications. In recent years, Explainable AI (XAI) has undergone a surge of developments mainly to get intuitions of how the models reached the decisions, which is essential for safety-critical domains such as healthcare, finance, and law enforcement agencies. While the interpretability domain is advancing noticeably, researchers are still unclear about what aspect of model learning a post hoc method reveals and how to validate its reliability. This paper comprehensively reviews interpretable deep learning models in the neuroimaging domain. Firstly, we summarize the current status of interpretability resources in general, focusing on the progression of methods, associated challenges, and opinions. Secondly, we discuss how multiple recent neuroimaging studies leveraged model interpretability to capture anatomical and functional brain alterations most relevant to model predictions. Finally, we discuss the limitations of the current practices and offer some valuable insights and guidance on how we can steer our future research directions to make deep learning models substantially interpretable and thus advance scientific understanding of brain disorders. Keywords\u2014 deep learning, interpretability, neuroimaging, brain dynamics, psychiatric disorders \u2217mahfuz.gsu@gmail.com \u2020vcalhoun@gsu.edu \u2021s.m.plis@gmail.com 1 ar X iv :2 30 7. 09 61 5v 1 [ cs .L G ] 1 4 Ju l 2 02 3",
    "authors": [
        {
            "affiliations": [],
            "name": "Md Mahfuzur Rahman"
        },
        {
            "affiliations": [],
            "name": "Vince Calhoun"
        },
        {
            "affiliations": [],
            "name": "Sergey Plis"
        }
    ],
    "id": "SP:787200c2a423c58b93366f8e37ce8fc747f93657",
    "references": [
        {
            "authors": [
                "David P Goldberg",
                "Peter Huxley"
            ],
            "title": "Common mental disorders: a bio-social model",
            "venue": "Tavistock/Routledge,",
            "year": 1992
        },
        {
            "authors": [
                "Vince D Calhoun",
                "Robyn Miller",
                "Godfrey Pearlson",
                "Tulay Adal\u0131"
            ],
            "title": "The chronnectome: time-varying connectivity networks as the next frontier in fMRI data",
            "venue": "discovery. Neuron,",
            "year": 2014
        },
        {
            "authors": [
                "Jing Sui",
                "Rongtao Jiang",
                "Juan Bustillo",
                "Vince Calhoun"
            ],
            "title": "Neuroimaging-based individualized prediction of cognition and behavior for mental disorders and health: methods and promises",
            "venue": "Biological psychiatry,",
            "year": 2020
        },
        {
            "authors": [
                "Peter C Mulders",
                "Philip F van Eijndhoven",
                "Aart H Schene",
                "Christian F Beckmann",
                "Indira Tendolkar"
            ],
            "title": "Resting-state functional connectivity in major depressive disorder: a review",
            "venue": "Neuroscience & Biobehavioral Reviews,",
            "year": 2015
        },
        {
            "authors": [
                "Julia M Sheffield",
                "Deanna M Barch"
            ],
            "title": "Cognition and resting-state functional connectivity in schizophrenia",
            "venue": "Neuroscience & Biobehavioral Reviews,",
            "year": 2016
        },
        {
            "authors": [
                "Eleni Zarogianni",
                "Thomas WJ Moorhead",
                "Stephen M Lawrie"
            ],
            "title": "Towards the identification of imaging biomarkers in schizophrenia, using multivariate pattern classification at a single-subject level",
            "venue": "NeuroImage: Clinical,",
            "year": 2013
        },
        {
            "authors": [
                "Petr Dluhovs",
                "Daniel Schwarz",
                "Wiepke Cahn",
                "Neeltje van Haren",
                "Ren\u00e9 Kahn",
                "Filip vSpaniel",
                "Jivr\u00ed Hor\u00e1vcek",
                "Tom\u00e1vs Kavsp\u00e1rek",
                "Hugo Schnack"
            ],
            "title": "Multi-center machine learning in imaging psychiatry: a meta-model approach",
            "year": 2017
        },
        {
            "authors": [
                "Hugo G Schnack"
            ],
            "title": "Improving individual predictions: machine learning approaches for detecting and attacking heterogeneity in schizophrenia (and other psychiatric diseases)",
            "venue": "Schizophrenia research,",
            "year": 2019
        },
        {
            "authors": [
                "Trevor Hastie",
                "Robert Tibshirani",
                "Jerome H Friedman"
            ],
            "title": "The elements of statistical learning: data mining, inference, and prediction, volume",
            "year": 2009
        },
        {
            "authors": [
                "Ali Khazaee",
                "Ata Ebrahimzadeh",
                "Abbas Babajani-Feremi"
            ],
            "title": "Application of advanced machine learning methods on resting-state fMRI network for identification of mild cognitive impairment and Alzheimer\u2019s disease",
            "venue": "Brain Imaging and Behavior,",
            "year": 2016
        },
        {
            "authors": [
                "Sergey M Plis",
                "Devon R Hjelm",
                "Ruslan Salakhutdinov",
                "Elena A Allen",
                "Henry J Bockholt",
                "Jeffrey D Long",
                "Hans J Johnson",
                "Jane S Paulsen",
                "Jessica A Turner",
                "Vince D Calhoun"
            ],
            "title": "Deep learning for neuroimaging: a validation study",
            "venue": "Frontiers in neuroscience,",
            "year": 2014
        },
        {
            "authors": [
                "Richard Bellman"
            ],
            "title": "Adaptive control processes: a guided tour princeton university",
            "venue": "press. Princeton,",
            "year": 1961
        },
        {
            "authors": [
                "Md Rahman",
                "Usman Mahmood",
                "Noah Lewis",
                "Harshvardhan Gazula",
                "Alex Fedorov",
                "Zening Fu",
                "Vince D Calhoun",
                "Sergey M Plis"
            ],
            "title": "Interpreting models interpreting brain dynamics",
            "venue": "Scientific Reports,",
            "year": 2022
        },
        {
            "authors": [
                "Adrien Payan",
                "Giovanni Montana"
            ],
            "title": "Predicting alzheimer\u2019s disease: a neuroimaging study with 3d convolutional neural networks",
            "venue": "arXiv preprint arXiv:1502.02506,",
            "year": 2015
        },
        {
            "authors": [
                "Pinkal Patel",
                "Priya Aggarwal",
                "Anubha Gupta"
            ],
            "title": "Classification of schizophrenia versus normal subjects using deep learning",
            "venue": "In Proceedings of the Tenth Indian Conference on Computer Vision, Graphics and Image Processing,",
            "year": 2016
        },
        {
            "authors": [
                "Jyoti Islam",
                "Yanqing Zhang"
            ],
            "title": "Brain mri analysis for alzheimer\u2019s disease diagnosis using an ensemble system of deep convolutional neural networks",
            "venue": "Brain informatics,",
            "year": 2018
        },
        {
            "authors": [
                "Xin Zhang",
                "Liangxiu Han",
                "Wenyong Zhu",
                "Liang Sun",
                "Daoqiang Zhang"
            ],
            "title": "An explainable 3d residual self-attention deep neural network for joint atrophy localization and alzheimer\u2019s disease diagnosis using structural mri",
            "venue": "IEEE journal of biomedical and health informatics,",
            "year": 2021
        },
        {
            "authors": [
                "Steven A Hicks",
                "Jonas L Isaksen",
                "Vajira Thambawita",
                "Jonas Ghouse",
                "Gustav Ahlberg",
                "Allan Linneberg",
                "Niels Grarup",
                "Inga Str\u00fcmke",
                "Christina Ellervik",
                "Morten Salling Olesen"
            ],
            "title": "Explaining deep neural networks for knowledge discovery in electrocardiogram analysis",
            "venue": "Scientific Reports,",
            "year": 2021
        },
        {
            "authors": [
                "Amirata Ghorbani",
                "David Ouyang",
                "Abubakar Abid",
                "Bryan He",
                "Jonathan H Chen",
                "Robert A Harrington",
                "David H Liang",
                "Euan A Ashley",
                "James Y Zou"
            ],
            "title": "Deep learning interpretation of echocardiograms",
            "venue": "NPJ digital medicine,",
            "year": 2020
        },
        {
            "authors": [
                "Pang Wei Koh",
                "Percy Liang"
            ],
            "title": "Understanding black-box predictions via influence functions",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Ruth C Fong",
                "Andrea Vedaldi"
            ],
            "title": "Interpretable explanations of black boxes by meaningful perturbation",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Geoffrey Hinton"
            ],
            "title": "Deep learning\u2014a technology with the potential to transform health",
            "venue": "care. Jama,",
            "year": 2018
        },
        {
            "authors": [
                "Satyapriya Krishna",
                "Tessa Han",
                "Alex Gu",
                "Javin Pombra",
                "Shahin Jabbari",
                "Steven Wu",
                "Himabindu Lakkaraju"
            ],
            "title": "The disagreement problem in explainable machine learning: A practitioner\u2019s perspective",
            "venue": "arXiv preprint arXiv:2202.01602,",
            "year": 2022
        },
        {
            "authors": [
                "Tessa Han",
                "Suraj Srinivas",
                "Himabindu Lakkaraju"
            ],
            "title": "Which explanation should i choose? a function approximation perspective to characterizing post hoc explanations",
            "venue": "arXiv preprint arXiv:2206.01254,",
            "year": 2022
        },
        {
            "authors": [
                "Sandra Vieira",
                "Walter HL Pinaya",
                "Andrea Mechelli"
            ],
            "title": "Using deep learning to investigate the neuroimaging correlates of psychiatric and neurological disorders: Methods and applications",
            "venue": "Neuroscience & Biobehavioral Reviews,",
            "year": 2017
        },
        {
            "authors": [
                "Li Zhang",
                "Mingliang Wang",
                "Mingxia Liu",
                "Daoqiang Zhang"
            ],
            "title": "A survey on deep learning for neuroimaging-based brain disorder analysis",
            "venue": "Frontiers in neuroscience,",
            "year": 2020
        },
        {
            "authors": [
                "Daniel T Huff",
                "Amy J Weisman",
                "Robert Jeraj"
            ],
            "title": "Interpretation and visualization techniques for deep learning models in medical imaging",
            "venue": "Physics in Medicine & Biology,",
            "year": 2021
        },
        {
            "authors": [
                "Elina Thibeau-Sutre",
                "Sasha Collin",
                "Ninon Burgos",
                "Olivier Colliot"
            ],
            "title": "Interpretability of machine learning methods applied to neuroimaging",
            "venue": "arXiv preprint arXiv:2204.07005,",
            "year": 2022
        },
        {
            "authors": [
                "Khansa Rasheed",
                "Adnan Qayyum",
                "Mohammed Ghaly",
                "Ala Al-Fuqaha",
                "Adeel Razi",
                "Junaid Qadir"
            ],
            "title": "Explainable, trustworthy, and ethical machine learning for healthcare: A survey",
            "venue": "Computers in Biology and Medicine,",
            "year": 2022
        },
        {
            "authors": [
                "Zohaib Salahuddin",
                "Henry C Woodruff",
                "Avishek Chatterjee",
                "Philippe Lambin"
            ],
            "title": "Transparency of deep neural networks for medical image analysis: A review of interpretability methods",
            "venue": "Computers in biology and medicine,",
            "year": 2022
        },
        {
            "authors": [
                "Lada Kohoutov\u00e1",
                "Juyeon Heo",
                "Sungmin Cha",
                "Sungwoo Lee",
                "Taesup Moon",
                "Tor D Wager",
                "Choong- Wan Woo"
            ],
            "title": "Toward a unified framework for interpreting machine-learning models in neuroimaging",
            "venue": "Nature protocols,",
            "year": 2020
        },
        {
            "authors": [
                "Carl G Hempel",
                "Paul Oppenheim"
            ],
            "title": "Studies in the logic of explanation",
            "venue": "Philosophy of science,",
            "year": 1948
        },
        {
            "authors": [
                "William Bechtel",
                "Adele Abrahamsen"
            ],
            "title": "Explanation: A mechanist alternative",
            "venue": "Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences,",
            "year": 2005
        },
        {
            "authors": [
                "David Lewis"
            ],
            "title": "Philosophy papers (vol",
            "year": 1986
        },
        {
            "authors": [
                "Zachary C Lipton"
            ],
            "title": "The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery",
            "year": 2018
        },
        {
            "authors": [
                "Finale Doshi-Velez",
                "Been Kim"
            ],
            "title": "Towards a rigorous science of interpretable machine learning",
            "venue": "arXiv preprint arXiv:1702.08608,",
            "year": 2017
        },
        {
            "authors": [
                "Tim Miller"
            ],
            "title": "Explanation in artificial intelligence: Insights from the social sciences",
            "venue": "Artificial intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Denis J Hilton"
            ],
            "title": "Conversational processes and causal explanation",
            "venue": "Psychological Bulletin,",
            "year": 1990
        },
        {
            "authors": [
                "Maartje MA De Graaf",
                "Bertram F Malle"
            ],
            "title": "How people explain action (and autonomous intelligent systems should too)",
            "venue": "AAAI Fall Symposium Series,",
            "year": 2017
        },
        {
            "authors": [
                "Tim Miller",
                "Piers Howe",
                "Liz Sonenberg"
            ],
            "title": "Explainable ai: Beware of inmates running the asylum or: How i learnt to stop worrying and love the social and behavioural sciences",
            "venue": "arXiv preprint arXiv:1712.00547,",
            "year": 2017
        },
        {
            "authors": [
                "Marco Ancona",
                "Enea Ceolini",
                "Cengiz \u00d6ztireli",
                "Markus Gross"
            ],
            "title": "Gradient-based attribution methods. In Explainable AI: Interpreting",
            "venue": "Explaining and Visualizing Deep Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Robert Geirhos",
                "J\u00f6rn-Henrik Jacobsen",
                "Claudio Michaelis",
                "Richard Zemel",
                "Wieland Brendel",
                "Matthias Bethge",
                "Felix A Wichmann"
            ],
            "title": "Shortcut learning in deep neural networks",
            "venue": "Nature Machine Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Riccardo Guidotti",
                "Anna Monreale",
                "Salvatore Ruggieri",
                "Franco Turini",
                "Fosca Giannotti",
                "Dino 68 Pedreschi"
            ],
            "title": "A survey of methods for explaining black box models",
            "venue": "ACM computing surveys (CSUR),",
            "year": 2018
        },
        {
            "authors": [
                "Leilani H Gilpin",
                "David Bau",
                "Ben Z Yuan",
                "Ayesha Bajwa",
                "Michael Specter",
                "Lalana Kagal"
            ],
            "title": "Explaining explanations: An overview of interpretability of machine learning",
            "venue": "IEEE 5th International Conference on data science and advanced analytics (DSAA),",
            "year": 2018
        },
        {
            "authors": [
                "Diogo V Carvalho",
                "Eduardo M Pereira",
                "Jaime S Cardoso"
            ],
            "title": "Machine learning interpretability",
            "venue": "A survey on methods and metrics. Electronics,",
            "year": 2019
        },
        {
            "authors": [
                "Alejandro Barredo Arrieta",
                "Natalia D\u00edaz-Rodr\u00edguez",
                "Javier Del Ser",
                "Adrien Bennetot",
                "Siham Tabik",
                "Alberto Barbado",
                "Salvador Garc\u00eda",
                "Sergio Gil-L\u00f3pez",
                "Daniel Molina",
                "Richard Benjamins"
            ],
            "title": "Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai",
            "venue": "Information fusion,",
            "year": 2020
        },
        {
            "authors": [
                "Pantelis Linardatos",
                "Vasilis Papastefanopoulos",
                "Sotiris Kotsiantis"
            ],
            "title": "Explainable ai: A review of machine learning interpretability methods",
            "year": 2020
        },
        {
            "authors": [
                "Wojciech Samek",
                "Gr\u00e9goire Montavon",
                "Sebastian Lapuschkin",
                "Christopher J Anders",
                "Klaus- Robert M\u00fcller"
            ],
            "title": "Explaining deep neural networks and beyond: A review of methods and applications",
            "venue": "Proceedings of the IEEE,",
            "year": 2021
        },
        {
            "authors": [
                "Gabrielle Ras",
                "Ning Xie",
                "Marcel van Gerven",
                "Derek Doran"
            ],
            "title": "Explainable deep learning: A field guide for the uninitiated",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2022
        },
        {
            "authors": [
                "Gr\u00e9goire Montavon",
                "Wojciech Samek",
                "Klaus-Robert M\u00fcller"
            ],
            "title": "Methods for interpreting and understanding deep neural networks",
            "venue": "Digital Signal Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Sandra Wachter",
                "Brent Mittelstadt",
                "Chris Russell"
            ],
            "title": "Counterfactual explanations without opening the black box: Automated decisions and the gdpr",
            "venue": "Harv. JL & Tech.,",
            "year": 2017
        },
        {
            "authors": [
                "Ramaravind K Mothilal",
                "Amit Sharma",
                "Chenhao Tan"
            ],
            "title": "Explaining machine learning classifiers through diverse counterfactual explanations",
            "venue": "In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency,",
            "year": 2020
        },
        {
            "authors": [
                "Susanne Dandl",
                "Christoph Molnar",
                "Martin Binder",
                "Bernd Bischl"
            ],
            "title": "Multi-objective counterfactual explanations",
            "venue": "In International Conference on Parallel Problem Solving from Nature,",
            "year": 2020
        },
        {
            "authors": [
                "Angela Lombardi",
                "Domenico Diacono",
                "Nicola Amoroso",
                "Alfonso Monaco",
                "Jo\u00e3o Manuel RS Tavares",
                "Roberto Bellotti",
                "Sabina Tangaro"
            ],
            "title": "Explainable deep learning for personalized age prediction with brain morphology",
            "venue": "Frontiers in neuroscience,",
            "year": 2021
        },
        {
            "authors": [
                "Shaker El-Sappagh",
                "Jose M Alonso",
                "SM Islam",
                "Ahmad M Sultan",
                "Kyung Sup Kwak"
            ],
            "title": "A multilayer multimodal detection and prediction model based on explainable artificial intelligence for alzheimer\u2019s disease",
            "venue": "Scientific reports,",
            "year": 2021
        },
        {
            "authors": [
                "Yasemin Turkan",
                "F Boray Tek"
            ],
            "title": "Convolutional attention network for mri-based alzheimer\u2019s disease classification and its interpretability analysis",
            "venue": "6th International Conference on Computer Science and Engineering (UBMK),",
            "year": 2021
        },
        {
            "authors": [
                "Ji Ye Chun",
                "Mohammad SE Sendi",
                "Jing Sui",
                "Dongmei Zhi",
                "Vince D Calhoun"
            ],
            "title": "Visualizing 69 functional network connectivity difference between healthy control and major depressive disorder using an explainable machine-learning method",
            "venue": "In 2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),",
            "year": 2020
        },
        {
            "authors": [
                "Sayan Ghosal",
                "Qiang Chen",
                "Giulio Pergola",
                "Aaron L Goldman",
                "William Ulrich",
                "Daniel R Weinberger",
                "Archana Venkataraman"
            ],
            "title": "A biologically interpretable graph convolutional network to link genetic risk pathways and neuroimaging markers of disease",
            "year": 2022
        },
        {
            "authors": [
                "Pavan Rajkumar Magesh",
                "Richard Delwin Myloth",
                "Rijo Jackson Tom"
            ],
            "title": "An explainable machine learning model for early detection of parkinson\u2019s disease using lime on datscan imagery",
            "venue": "Computers in Biology and Medicine,",
            "year": 2020
        },
        {
            "authors": [
                "Krishnakant V Saboo",
                "Chang Hu",
                "Yogatheesan Varatharajah",
                "Scott A Przybelski",
                "Robert I Reid",
                "Christopher G Schwarz",
                "Jonathan Graff-Radford",
                "David S Knopman",
                "Mary M Machulda",
                "Michelle M Mielke"
            ],
            "title": "Deep learning identifies brain structures that predict cognition and explain heterogeneity in cognitive aging",
            "year": 2022
        },
        {
            "authors": [
                "Ma Magboo",
                "A Sheila",
                "Vincent Peter C Magboo"
            ],
            "title": "Explainable ai for autism classification in children",
            "venue": "In Agents and Multi-Agent Systems: Technologies and Applications",
            "year": 2022
        },
        {
            "authors": [
                "Loveleen Gaur",
                "Mohan Bhandari",
                "Tanvi Razdan",
                "Saurav Mallik",
                "Zhongming Zhao"
            ],
            "title": "Explanationdriven deep learning model for prediction of brain tumour status using mri image data",
            "venue": "Frontiers in Genetics,",
            "year": 2022
        },
        {
            "authors": [
                "Anees Abrol",
                "Manish Bhattarai",
                "Alex Fedorov",
                "Yuhui Du",
                "Sergey Plis",
                "Vince Calhoun"
            ],
            "title": "Alzheimer\u2019s Disease Neuroimaging Initiative, et al. Deep residual learning for neuroimaging: an application to predict progression to alzheimer\u2019s disease",
            "venue": "Journal of neuroscience methods,",
            "year": 2020
        },
        {
            "authors": [
                "Cher Bass",
                "Mariana da Silva",
                "Carole Sudre",
                "Petru-Daniel Tudosiu",
                "Stephen Smith",
                "Emma Robinson"
            ],
            "title": "Icam: Interpretable classification via disentangled representations and feature attribution mapping",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Cher Bass",
                "Mariana da Silva",
                "Carole Sudre",
                "Logan ZJ Williams",
                "Petru-Daniel Tudosiu",
                "Fidel Alfaro- Almagro",
                "Sean P Fitzgibbon",
                "Matthew F Glasser",
                "Stephen M Smith",
                "Emma C Robinson"
            ],
            "title": "Icam-reg: Interpretable classification and regression with feature attribution for mapping neurological phenotypes in individual scans",
            "venue": "arXiv preprint arXiv:2103.02561,",
            "year": 2021
        },
        {
            "authors": [
                "Fabian Eitel",
                "Kerstin Ritter"
            ],
            "title": "Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI, et al. Testing the robustness of attribution methods for convolutional neural networks in mri-based alzheimer\u2019s disease classification. In Interpretability of machine intelligence in medical image computing and multimodal learning for clinical decision support, pages",
            "year": 2019
        },
        {
            "authors": [
                "Armin W Thomas",
                "Christopher R\u00e9",
                "Russell A Poldrack"
            ],
            "title": "Comparing interpretation methods in mental state decoding analyses with deep learning models",
            "venue": "arXiv preprint arXiv:2205.15581,",
            "year": 2022
        },
        {
            "authors": [
                "Armin W Thomas",
                "Hauke R Heekeren",
                "Klaus-Robert M\u00fcller",
                "Wojciech Samek"
            ],
            "title": "Analyzing neuroimaging data through recurrent deep learning models",
            "venue": "Frontiers in neuroscience,",
            "year": 2019
        },
        {
            "authors": [
                "AI Korda",
                "A Ruef",
                "S Neufang",
                "C Davatzikos",
                "S Borgwardt",
                "EM Meisenzahl",
                "N Koutsouleris"
            ],
            "title": "Identification of voxel-based texture abnormalities as new biomarkers for schizophrenia and major depressive patients using layer-wise relevance propagation on deep learning decisions",
            "venue": "Psychiatry Research: Neuroimaging,",
            "year": 2021
        },
        {
            "authors": [
                "Simon M Hofmann",
                "Frauke Beyer",
                "Sebastian Lapuschkin",
                "Ole Goltermann",
                "Markus Loeffler",
                "Klaus- Robert M\u00fcller",
                "Arno Villringer",
                "Wojciech Samek",
                "A Veronica Witte"
            ],
            "title": "Towards the interpretability of deep learning models for multi-modal neuroimaging: Finding structural changes of the ageing",
            "venue": "brain. NeuroImage,",
            "year": 2022
        },
        {
            "authors": [
                "Ramy A Zeineldin",
                "Mohamed E Karar",
                "Ziad Elshaer",
                "Christian R Wirtz",
                "Oliver Burgert",
                "Franziska Mathis-Ullrich"
            ],
            "title": "Explainability of deep neural networks for mri analysis of brain tumors",
            "venue": "International Journal of Computer Assisted Radiology and Surgery,",
            "year": 2022
        },
        {
            "authors": [
                "Christian F Baumgartner",
                "Lisa M Koch",
                "Kerem Can Tezcan",
                "Jia Xi Ang",
                "Ender Konukoglu"
            ],
            "title": "Visual feature attribution using wasserstein gans",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Martin Dyrba",
                "Arjun H Pallath",
                "Eman N Marzban"
            ],
            "title": "Comparison of cnn visualization methods to aid model interpretability for detecting alzheimer\u2019s disease",
            "venue": "In Bildverarbeitung fu\u0308r die Medizin",
            "year": 2020
        },
        {
            "authors": [
                "Christian Gerhard Tinauer"
            ],
            "title": "Explainable Deep Learning of Multi-parametric MRI Data for Alzheimer\u2019s Disease Classification",
            "venue": "PhD thesis, Graz University of Technology,",
            "year": 2020
        },
        {
            "authors": [
                "Qiu-Hua Lin",
                "Yan-Wei Niu",
                "Jing Sui",
                "Wen-Da Zhao",
                "Chuanjun Zhuo",
                "Vince D Calhoun"
            ],
            "title": "Sspnet: An interpretable 3d-cnn for classification of schizophrenia using phase maps of resting-state complexvalued fmri data",
            "venue": "Medical Image Analysis,",
            "year": 2022
        },
        {
            "authors": [
                "Jian Cui",
                "Bin Weng"
            ],
            "title": "Towards best practice of interpreting deep learning models for eeg-based brain computer interfaces",
            "venue": "arXiv preprint arXiv:2202.06948,",
            "year": 2022
        },
        {
            "authors": [
                "Kanghan Oh",
                "Young-Chul Chung",
                "Ko Woon Kim",
                "Woo-Sung Kim",
                "Il-Seok Oh"
            ],
            "title": "Classification and visualization of alzheimer\u2019s disease using volumetric convolutional neural network and transfer learning",
            "venue": "Scientific Reports,",
            "year": 2019
        },
        {
            "authors": [
                "Di Wang",
                "Nicolas Honnorat",
                "Peter T Fox",
                "Kerstin Ritter",
                "Simon B Eickhoff",
                "Sudha Seshadri",
                "Mohamad Habes"
            ],
            "title": "Deep neural network heatmaps capture alzheimer\u2019s disease patterns reported in a large meta-analysis of neuroimaging studies",
            "venue": "arXiv preprint arXiv:2207.11352,",
            "year": 2022
        },
        {
            "authors": [
                "Hemanth Manjunatha",
                "Ehsan T Esfahani"
            ],
            "title": "Extracting interpretable eeg features from a deep learning model to assess the quality of human-robot co-manipulation",
            "venue": "10th International IEEE/EMBS Conference on Neural Engineering (NER),",
            "year": 2021
        },
        {
            "authors": [
                "Kai-Yi Lin",
                "Vincent Chin-Hung Chen",
                "Yuan-Hsiung Tsai",
                "Roger S McIntyre",
                "Jun-Cheng Weng"
            ],
            "title": "Classification and visualization of chemotherapy-induced cognitive impairment in volumetric convolutional neural networks",
            "venue": "Journal of Personalized Medicine,",
            "year": 2021
        },
        {
            "authors": [
                "Hyoungshin Choi",
                "Kyoungseob Byeon",
                "Jong-eun Lee",
                "Seok-Jun Hong",
                "Bo-yong Park",
                "Hyunjin Park"
            ],
            "title": "Subgroups of eating behavior traits independent of obesity defined using functional connectivity and feature representation learning",
            "venue": "bioRxiv,",
            "year": 2022
        },
        {
            "authors": [
                "Kaustubh Supekar",
                "Carlo de Los Angeles",
                "Srikanth Ryali",
                "Kaidi Cao",
                "Tengyu Ma",
                "Vinod Menon"
            ],
            "title": "Deep learning identifies robust gender differences in functional brain organization and their dissociable links to clinical symptoms in autism",
            "venue": "The British Journal of Psychiatry,",
            "year": 2022
        },
        {
            "authors": [
                "Xiangjun Chen",
                "Zhaohui Wang",
                "Yuefu Zhan",
                "Faouzi Alaya Cheikh",
                "Mohib Ullah"
            ],
            "title": "Interpretable learning approaches in structural mri: 3d-resnet fused attention for autism spectrum disorder classification",
            "venue": "In Medical Imaging 2022: Computer-Aided Diagnosis,",
            "year": 2022
        },
        {
            "authors": [
                "Chengliang Yang",
                "Anand Rangarajan",
                "Sanjay Ranka"
            ],
            "title": "Visual explanations from deep 3d convolutional neural networks for alzheimer\u2019s disease classification",
            "venue": "In AMIA annual symposium proceedings,",
            "year": 2018
        },
        {
            "authors": [
                "Patrick McClure",
                "Dustin Moraczewski",
                "Ka Chun Lam",
                "Adam Thomas",
                "Francisco Pereira"
            ],
            "title": "Improving the interpretability of fmri decoding using deep neural networks and adversarial robustness",
            "year": 2004
        },
        {
            "authors": [
                "Caihua Wang",
                "Yuanzhong Li",
                "Yukihiro Tsuboshita",
                "Takuya Sakurai",
                "Tsubasa Goto",
                "Hiroyuki Yamaguchi",
                "Yuichi Yamashita",
                "Atsushi Sekiguchi",
                "Hisateru Tachimori"
            ],
            "title": "A high-generalizability machine learning framework for predicting the progression of alzheimer\u2019s disease using limited data",
            "venue": "NPJ digital medicine,",
            "year": 2022
        },
        {
            "authors": [
                "Andrew Ilyas",
                "Sung Min Park",
                "Logan Engstrom",
                "Guillaume Leclerc",
                "Aleksander Madry"
            ],
            "title": "Datamodels: Understanding predictions with data and data with predictions",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio"
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "arXiv preprint arXiv:1409.0473,",
            "year": 2014
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "arXiv preprint arXiv:1706.03762,",
            "year": 2017
        },
        {
            "authors": [
                "Guangqi Wen",
                "Peng Cao",
                "Huiwen Bao",
                "Wenju Yang",
                "Tong Zheng",
                "Osmar Zaiane"
            ],
            "title": "Mvs-gcn: A prior brain structure learning-guided multi-view graph convolution network for autism spectrum disorder diagnosis",
            "venue": "Computers in Biology and Medicine,",
            "year": 2022
        },
        {
            "authors": [
                "Yonghua Zhu",
                "Junbo Ma",
                "Changan Yuan",
                "Xiaofeng Zhu"
            ],
            "title": "Interpretable learning based dynamic graph convolutional networks for alzheimer\u2019s disease analysis",
            "venue": "Information Fusion,",
            "year": 2022
        },
        {
            "authors": [
                "Matthias Wilms",
                "Pauline Mouches",
                "Jordan J Bannister",
                "Deepthi Rajashekar",
                "S\u00f6nke Langner",
                "Nils D Forkert"
            ],
            "title": "Towards self-explainable classifiers and regressors in neuroimaging with normalizing flows",
            "venue": "In International Workshop on Machine Learning in Clinical Neuroimaging,",
            "year": 2021
        },
        {
            "authors": [
                "David Wood",
                "James Cole",
                "Thomas Booth"
            ],
            "title": "Neuro-dram: a 3d recurrent visual attention model for interpretable neuroimaging classification",
            "venue": "arXiv preprint arXiv:1910.04721,",
            "year": 1910
        },
        {
            "authors": [
                "Shangran Qiu",
                "Prajakta S Joshi",
                "Matthew I Miller",
                "Chonghua Xue",
                "Xiao Zhou",
                "Cody Karjadi",
                "Gary H Chang",
                "Anant S Joshi",
                "Brigid Dwyer",
                "Shuhan Zhu"
            ],
            "title": "Development and validation of an interpretable deep learning framework for alzheimer\u2019s disease",
            "year": 1920
        },
        {
            "authors": [
                "Eunho Lee",
                "Jun-Sik Choi",
                "Minjeong Kim",
                "Heung-Il Suk"
            ],
            "title": "Alzheimer\u2019s Disease Neuroimaging Initiative, et al. Toward an interpretable alzheimer\u2019s disease diagnostic model with regional abnormality representation via deep learning",
            "year": 2019
        },
        {
            "authors": [
                "Usman Mahmood",
                "Zening Fu",
                "Satrajit Ghosh",
                "Vince Calhoun",
                "Sergey Plis"
            ],
            "title": "Through the looking glass: Deep interpretable dynamic directed connectivity in resting fmri",
            "year": 2022
        },
        {
            "authors": [
                "Yu Zhao",
                "Junwei Han",
                "Lei Guo",
                "Tianming Liu"
            ],
            "title": "Modeling task fmri data via deep convolutional autoencoder",
            "venue": "In Information Processing in Medical Imaging: 25th International Conference, IPMI 2017,",
            "year": 1026
        },
        {
            "authors": [
                "Pindi Krishna Chandra Prasad",
                "Yash Khare",
                "Kamalaker Dadi",
                "PK Vinod",
                "Bapi Raju Surampudi"
            ],
            "title": "Deep learning approach for classification and interpretation of autism spectrum disorder",
            "venue": "In 2022 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2022
        },
        {
            "authors": [
                "Sukrit Gupta",
                "Yi Hao Chan",
                "Jagath C Rajapakse"
            ],
            "title": "Alzheimer\u2019s Disease Neuroimaging Initiative, et al. Decoding brain functional connectivity implicated in ad and mci",
            "venue": "In International conference on medical image computing and computer-assisted intervention,",
            "year": 2019
        },
        {
            "authors": [
                "Sukrit Gupta",
                "Yi Hao Chan",
                "Jagath C Rajapakse"
            ],
            "title": "Alzheimer\u2019s Disease Neuroimaging Initiative, et al. Obtaining leaner deep neural networks for decoding brain functional connectome in a single shot",
            "year": 2021
        },
        {
            "authors": [
                "Theerasarn Pianpanit",
                "Sermkiat Lolak",
                "Phattarapong Sawangjai",
                "Thapanun Sudhawiyangkul",
                "Theerawit Wilaiprasitporn"
            ],
            "title": "Parkinson\u2019s disease recognition using spect image and interpretable ai: A tutorial",
            "venue": "IEEE Sensors Journal,",
            "year": 2021
        },
        {
            "authors": [
                "James D Bennett",
                "Sam E John",
                "David B Grayden",
                "Anthony N Burkitt"
            ],
            "title": "Universal neurophysiological interpretation of eeg brain-computer interfaces",
            "venue": "9th International Winter Conference on Brain-Computer Interface (BCI),",
            "year": 2021
        },
        {
            "authors": [
                "Min Zhao",
                "Weizheng Yan",
                "Na Luo",
                "Dongmei Zhi",
                "Zening Fu",
                "Yuhui Du",
                "Shan Yu",
                "Tianzi Jiang",
                "Vince D Calhoun",
                "Jing Sui"
            ],
            "title": "An attention-based hybrid deep learning framework integrating brain connectivity and activity of resting-state functional mri data",
            "venue": "Medical image analysis,",
            "year": 2022
        },
        {
            "authors": [
                "Dan Jin",
                "Jian Xu",
                "Kun Zhao",
                "Fangzhou Hu",
                "Zhengyi Yang",
                "Bing Liu",
                "Tianzi Jiang",
                "Yong Liu"
            ],
            "title": "Attention-based 3d convolutional network for alzheimer\u2019s disease diagnosis and biomarkers exploration",
            "venue": "IEEE 16Th international symposium on biomedical imaging (ISBI",
            "year": 2019
        },
        {
            "authors": [
                "Guanghui Fu",
                "Jianqiang Li",
                "Ruiqian Wang",
                "Yue Ma",
                "Yueda Chen"
            ],
            "title": "Attention-based full slice brain ct image diagnosis with explanations",
            "year": 2021
        },
        {
            "authors": [
                "Dan Jin",
                "Bo Zhou",
                "Ying Han",
                "Jiaji Ren",
                "Tong Han",
                "Bing Liu",
                "Jie Lu",
                "Chengyuan Song",
                "Pan Wang",
                "Dawei Wang"
            ],
            "title": "Generalizable, reproducible, and neuroscientifically interpretable imaging biomarkers for alzheimer\u2019s disease",
            "venue": "Advanced Science,",
            "year": 2000
        },
        {
            "authors": [
                "Huzheng Yang",
                "Xiaoxiao Li",
                "Yifan Wu",
                "Siyi Li",
                "Su Lu",
                "James S Duncan",
                "James C Gee",
                "Shi Gu"
            ],
            "title": "Interpretable multimodality embedding of cerebral cortex using attention graph network for identifying bipolar disorder",
            "venue": "In International Conference on Medical Image Computing and Computer-Assisted Intervention,",
            "year": 2019
        },
        {
            "authors": [
                "Lisa Anne Hendricks",
                "Zeynep Akata",
                "Marcus Rohrbach",
                "Jeff Donahue",
                "Bernt Schiele",
                "Trevor Darrell"
            ],
            "title": "Generating visual explanations",
            "venue": "In European conference on computer vision,",
            "year": 2016
        },
        {
            "authors": [
                "Rowan Zellers",
                "Yonatan Bisk",
                "Ali Farhadi",
                "Yejin Choi"
            ],
            "title": "From recognition to cognition: Visual commonsense reasoning",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Hui Liu",
                "Qingyu Yin",
                "William Yang Wang"
            ],
            "title": "Towards explainable nlp: A generative explanation framework for text classification",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Jimmy Ba",
                "Volodymyr Mnih",
                "Koray Kavukcuoglu"
            ],
            "title": "Multiple object recognition with visual attention",
            "venue": "arXiv preprint arXiv:1412.7755,",
            "year": 2014
        },
        {
            "authors": [
                "Matthew D Zeiler",
                "Rob Fergus"
            ],
            "title": "Visualizing and understanding convolutional networks",
            "venue": "In European conference on computer vision,",
            "year": 2014
        },
        {
            "authors": [
                "David Baehrens",
                "Timon Schroeter",
                "Stefan Harmeling",
                "Motoaki Kawanabe",
                "Katja Hansen",
                "Klaus- Robert M\u00fcller"
            ],
            "title": "How to explain individual classification decisions",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2010
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrea Vedaldi",
                "Andrew Zisserman"
            ],
            "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
            "venue": "arXiv preprint arXiv:1312.6034,",
            "year": 2013
        },
        {
            "authors": [
                "Mukund Sundararajan",
                "Ankur Taly",
                "Qiqi Yan"
            ],
            "title": "Axiomatic attribution for deep networks",
            "venue": "arXiv preprint arXiv:1703.01365,",
            "year": 2017
        },
        {
            "authors": [
                "Scott M Lundberg",
                "Su-In Lee"
            ],
            "title": "A unified approach to interpreting model predictions",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin"
            ],
            "title": "Why should i trust you?\" explaining the predictions of any classifier",
            "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining,",
            "year": 2016
        },
        {
            "authors": [
                "Avanti Shrikumar",
                "Peyton Greenside",
                "Anshul Kundaje"
            ],
            "title": "Learning important features through propagating activation differences",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Gr\u00e9goire Montavon",
                "Sebastian Lapuschkin",
                "Alexander Binder",
                "Wojciech Samek",
                "Klaus-Robert M\u00fcller"
            ],
            "title": "Explaining nonlinear classification decisions with deep taylor decomposition",
            "venue": "Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Sebastian Bach",
                "Alexander Binder",
                "Gr\u00e9goire Montavon",
                "Frederick Klauschen",
                "Klaus-Robert M\u00fcller",
                "Wojciech Samek"
            ],
            "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
            "venue": "PloS one,",
            "year": 2015
        },
        {
            "authors": [
                "Jost Tobias Springenberg",
                "Alexey Dosovitskiy",
                "Thomas Brox",
                "Martin Riedmiller"
            ],
            "title": "Striving for simplicity: The all convolutional net",
            "venue": "arXiv preprint arXiv:1412.6806,",
            "year": 2014
        },
        {
            "authors": [
                "Daniel Smilkov",
                "Nikhil Thorat",
                "Been Kim",
                "Fernanda Vi\u00e9gas",
                "Martin Wattenberg"
            ],
            "title": "Smoothgrad: removing noise by adding noise",
            "venue": "arXiv preprint arXiv:1706.03825,",
            "year": 2017
        },
        {
            "authors": [
                "Ramprasaath R Selvaraju",
                "Michael Cogswell",
                "Abhishek Das",
                "Ramakrishna Vedantam",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Avanti Shrikumar",
                "Peyton Greenside",
                "Anna Shcherbina",
                "Anshul Kundaje"
            ],
            "title": "Not just a black box: Learning important features through propagating activation differences",
            "venue": "arXiv preprint arXiv:1605.01713,",
            "year": 2016
        },
        {
            "authors": [
                "Kwanseok Oh",
                "Jee Seok Yoon",
                "Heung-Il Suk"
            ],
            "title": "Learn-explain-reinforce: Counterfactual reasoning and its guidance to reinforce an alzheimer\u2019s disease diagnosis model",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Carlo Abrate",
                "Francesco Bonchi"
            ],
            "title": "Counterfactual graphs for explainable classification of brain networks",
            "venue": "In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining,",
            "year": 2021
        },
        {
            "authors": [
                "Matthias Wilms",
                "Jordan J Bannister",
                "Pauline Mouches",
                "M Ethan MacDonald",
                "Deepthi Rajashekar",
                "S\u00f6nke Langner",
                "Nils D Forkert"
            ],
            "title": "Invertible modeling of bidirectional relationships in neuroimaging with normalizing flows: Application to brain aging",
            "venue": "IEEE Transactions on Medical Imaging,",
            "year": 2022
        },
        {
            "authors": [
                "Kwanseok Oh",
                "Da-Woon Heo",
                "Ahmad Wisnu Mulyadi",
                "Wonsik Jung",
                "Eunsong Kang",
                "Kun Ho Lee",
                "Heung-Il Suk"
            ],
            "title": "Deep counterfactual-guided mri feature representation and quantitatively interpretable alzheimer\u2019s disease",
            "year": 2022
        },
        {
            "authors": [
                "Maxim Kan",
                "Ruslan Aliev",
                "Anna Rudenko",
                "Nikita Drobyshev",
                "Nikita Petrashen",
                "Ekaterina Kondrateva",
                "Maxim Sharaev",
                "Alexander Bernstein",
                "Evgeny Burnaev"
            ],
            "title": "Interpretation of 3d cnns for brain mri data classification",
            "venue": "In International Conference on Analysis of Images, Social Networks and Texts,",
            "year": 2020
        },
        {
            "authors": [
                "Hristina Uzunova",
                "Jan Ehrhardt",
                "Timo Kepp",
                "Heinz Handels"
            ],
            "title": "Interpretable explanations of black box classifiers applied on medical images by meaningful perturbations using variational autoencoders",
            "venue": "In Medical Imaging 2019: Image Processing,",
            "year": 2019
        },
        {
            "authors": [
                "Wojciech Samek",
                "Alexander Binder",
                "Gr\u00e9goire Montavon",
                "Sebastian Lapuschkin",
                "Klaus-Robert 75 M\u00fcller"
            ],
            "title": "Evaluating the visualization of what a deep neural network has learned",
            "venue": "IEEE transactions on neural networks and learning systems,",
            "year": 2016
        },
        {
            "authors": [
                "Marco Ancona",
                "Enea Ceolini",
                "Cengiz \u00d6ztireli",
                "Markus Gross"
            ],
            "title": "Towards better understanding of gradient-based attribution methods for deep neural networks",
            "venue": "arXiv preprint arXiv:1711.06104,",
            "year": 2017
        },
        {
            "authors": [
                "Junghoon Seo",
                "Jeongyeol Choe",
                "Jamyoung Koo",
                "Seunghyeon Jeon",
                "Beomsu Kim",
                "Taegyun Jeon"
            ],
            "title": "Noise-adding methods of saliency map as series of higher order partial derivative",
            "venue": "arXiv preprint arXiv:1806.03000,",
            "year": 2018
        },
        {
            "authors": [
                "Sara Hooker",
                "Dumitru Erhan",
                "Pieter-Jan Kindermans",
                "Been Kim"
            ],
            "title": "A benchmark for interpretability methods in deep neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Bolei Zhou",
                "Aditya Khosla",
                "Agata Lapedriza",
                "Aude Oliva",
                "Antonio Torralba"
            ],
            "title": "Learning deep features for discriminative localization",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Aditya Chattopadhay",
                "Anirban Sarkar",
                "Prantik Howlader",
                "Vineeth N Balasubramanian"
            ],
            "title": "Gradcam++: Generalized gradient-based visual explanations for deep convolutional networks",
            "venue": "IEEE winter conference on applications of computer vision (WACV),",
            "year": 2018
        },
        {
            "authors": [
                "Haofan Wang",
                "Zifan Wang",
                "Mengnan Du",
                "Fan Yang",
                "Zijian Zhang",
                "Sirui Ding",
                "Piotr Mardziel",
                "Xia Hu"
            ],
            "title": "Score-cam: Score-weighted visual explanations for convolutional neural networks",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops,",
            "year": 2020
        },
        {
            "authors": [
                "Andrei Kapishnikov",
                "Tolga Bolukbasi",
                "Fernanda Vi\u00e9gas",
                "Michael Terry"
            ],
            "title": "Xrai: Better attributions through regions",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Andrei Kapishnikov",
                "Subhashini Venugopalan",
                "Besim Avci",
                "Ben Wedin",
                "Michael Terry",
                "Tolga Bolukbasi"
            ],
            "title": "Guided integrated gradients: An adaptive path method for removing noise",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Ning Xie",
                "Gabrielle Ras",
                "Marcel van Gerven",
                "Derek Doran"
            ],
            "title": "Explainable deep learning: A field guide for the uninitiated",
            "year": 2020
        },
        {
            "authors": [
                "Christoph Molnar"
            ],
            "title": "Interpretable machine learning",
            "venue": "Lulu. com,",
            "year": 2020
        },
        {
            "authors": [
                "Amir-Hossein Karimi",
                "Gilles Barthe",
                "Borja Balle",
                "Isabel Valera"
            ],
            "title": "Model-agnostic counterfactual explanations for consequential decisions",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Frank R Hampel",
                "Elvezio M Ronchetti",
                "Peter Rousseeuw",
                "Werner A Stahel"
            ],
            "title": "Robust statistics: the approach based on influence functions",
            "year": 1986
        },
        {
            "authors": [
                "Samyadeep Basu",
                "Xuchen You",
                "Soheil Feizi"
            ],
            "title": "On second-order group influence functions for black-box predictions",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Benbo Zha",
                "Hong Shen"
            ],
            "title": "Ifme: Influence function based model explanation for black box decision 76 systems",
            "venue": "In International Symposium on Parallel Architectures, Algorithms and Programming,",
            "year": 2020
        },
        {
            "authors": [
                "Xiaochuang Han",
                "Byron C Wallace",
                "Yulia Tsvetkov"
            ],
            "title": "Explaining black box predictions and unveiling data artifacts through influence functions",
            "venue": "arXiv preprint arXiv:2005.06676,",
            "year": 2020
        },
        {
            "authors": [
                "Vitaly Feldman",
                "Chiyuan Zhang"
            ],
            "title": "What neural networks memorize and why: Discovering the long tail via influence estimation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Amirata Ghorbani",
                "James Zou"
            ],
            "title": "Data shapley: Equitable valuation of data for machine learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Ruoxi Jia",
                "David Dao",
                "Boxin Wang",
                "Frances Ann Hubis",
                "Nick Hynes",
                "Nezihe Merve G\u00fcrel",
                "Bo Li",
                "Ce Zhang",
                "Dawn Song",
                "Costas J Spanos"
            ],
            "title": "Towards efficient data valuation based on the shapley value",
            "venue": "In The 22nd International Conference on Artificial Intelligence and Statistics,",
            "year": 2019
        },
        {
            "authors": [
                "Julius Adebayo",
                "Justin Gilmer",
                "Michael Muelly",
                "Ian Goodfellow",
                "Moritz Hardt",
                "Been Kim"
            ],
            "title": "Sanity checks for saliency maps",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "arXiv preprint arXiv:1412.6572,",
            "year": 2014
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wojciech Zaremba",
                "Ilya Sutskever",
                "Joan Bruna",
                "Dumitru Erhan",
                "Ian Goodfellow",
                "Rob Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "arXiv preprint arXiv:1312.6199,",
            "year": 2013
        },
        {
            "authors": [
                "Leila Arras",
                "Ahmed Osman",
                "Wojciech Samek"
            ],
            "title": "Clevr-xai: a benchmark dataset for the ground truth evaluation of neural network explanations",
            "venue": "Information Fusion,",
            "year": 2022
        },
        {
            "authors": [
                "Chih-Kuan Yeh",
                "Cheng-Yu Hsieh",
                "Arun Suggala",
                "David I Inouye",
                "Pradeep K Ravikumar"
            ],
            "title": "On the (in) fidelity and sensitivity of explanations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Pascal Sturmfels",
                "Scott Lundberg",
                "Su-In Lee"
            ],
            "title": "Visualizing the impact of feature attribution",
            "venue": "baselines. Distill,",
            "year": 2020
        },
        {
            "authors": [
                "Piotr Dabkowski",
                "Yarin Gal"
            ],
            "title": "Real time image saliency for black box classifiers",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Yao Rong",
                "Tobias Leemann",
                "Vadim Borisov",
                "Gjergji Kasneci",
                "Enkelejda Kasneci"
            ],
            "title": "A consistent and efficient evaluation strategy for attribution methods",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Erico Tjoa",
                "Cuntai Guan"
            ],
            "title": "A survey on explainable artificial intelligence (xai): Toward medical xai",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Cynthia Rudin"
            ],
            "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
            "venue": "Nature Machine Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "David Leslie"
            ],
            "title": "Understanding artificial intelligence ethics and safety",
            "venue": "arXiv preprint arXiv:1906.05684,",
            "year": 1906
        },
        {
            "authors": [
                "Octavio Loyola-Gonzalez"
            ],
            "title": "Black-box vs. white-box: Understanding their advantages and weaknesses from a practical point of view",
            "venue": "IEEE Access,",
            "year": 2019
        },
        {
            "authors": [
                "Cynthia Rudin",
                "Joanna Radin"
            ],
            "title": "Why are we using black box models in ai when we don\u2019t need to? a lesson from an explainable ai competition",
            "venue": "Harvard Data Science Review,",
            "year": 2019
        },
        {
            "authors": [
                "Himabindu Lakkaraju",
                "Osbert Bastani"
            ],
            "title": " how do i fool you?\" manipulating user trust via misleading black box explanations",
            "venue": "In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society,",
            "year": 2020
        },
        {
            "authors": [
                "Thibault Laugel",
                "Marie-Jeanne Lesot",
                "Christophe Marsala",
                "Xavier Renard",
                "Marcin Detyniecki"
            ],
            "title": "The dangers of post-hoc interpretability: Unjustified counterfactual explanations",
            "year": 1907
        },
        {
            "authors": [
                "Thibault Laugel",
                "Marie-Jeanne Lesot",
                "Christophe Marsala",
                "Marcin Detyniecki"
            ],
            "title": "Issues with post-hoc counterfactual explanations: a discussion",
            "venue": "arXiv preprint arXiv:1906.04774,",
            "year": 2019
        },
        {
            "authors": [
                "Dylan Slack",
                "Sophie Hilgard",
                "Emily Jia",
                "Sameer Singh",
                "Himabindu Lakkaraju"
            ],
            "title": "Fooling lime and shap: Adversarial attacks on post hoc explanation methods",
            "venue": "In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society,",
            "year": 2020
        },
        {
            "authors": [
                "Jean-Marie John-Mathews"
            ],
            "title": "Some critical and ethical perspectives on the empirical turn of ai interpretability",
            "venue": "Technological Forecasting and Social Change,",
            "year": 2022
        },
        {
            "authors": [
                "Lok Chan"
            ],
            "title": "Explainable ai as epistemic representation",
            "venue": "Overcoming Opacity in Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Thibault Laugel",
                "Marie-Jeanne Lesot",
                "Christophe Marsala",
                "Xavier Renard",
                "Marcin Detyniecki"
            ],
            "title": "Unjustified classification regions and counterfactual explanations in machine learning",
            "venue": "In Joint European conference on machine learning and knowledge discovery in databases,",
            "year": 2019
        },
        {
            "authors": [
                "Michele Loi",
                "Andrea Ferrario",
                "Eleonora Vigan\u00f2"
            ],
            "title": "Transparency as design publicity: explaining and justifying inscrutable algorithms",
            "venue": "Ethics and Information Technology,",
            "year": 2021
        },
        {
            "authors": [
                "Jean-Marie John-Mathews"
            ],
            "title": "Critical empirical study on black-box explanations in ai",
            "venue": "arXiv preprint arXiv:2109.15067,",
            "year": 2021
        },
        {
            "authors": [
                "Andr\u00e9s P\u00e1ez"
            ],
            "title": "The pragmatic turn in explainable artificial intelligence (xai)",
            "venue": "Minds and Machines,",
            "year": 2019
        },
        {
            "authors": [
                "Nishanth Arun",
                "Nathan Gaw",
                "Praveer Singh",
                "Ken Chang",
                "Mehak Aggarwal",
                "Bryan Chen",
                "Katharina Hoebel",
                "Sharut Gupta",
                "Jay Patel",
                "Mishka Gidwani"
            ],
            "title": "Assessing the (un) trustworthiness of saliency maps for localizing abnormalities",
            "year": 2002
        },
        {
            "authors": [
                "Pieter-Jan Kindermans",
                "Sara Hooker",
                "Julius Adebayo",
                "Maximilian Alber",
                "Kristof T Sch\u00fctt",
                "Sven D\u00e4hne",
                "Dumitru Erhan",
                "Been Kim"
            ],
            "title": "The (un) reliability of saliency methods. In Explainable AI: 78 Interpreting",
            "venue": "Explaining and Visualizing Deep Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Bernease Herman"
            ],
            "title": "The promise and peril of human evaluation for model interpretability",
            "venue": "arXiv preprint arXiv:1711.07414,",
            "year": 2017
        },
        {
            "authors": [
                "Amina Adadi",
                "Mohammed Berrada"
            ],
            "title": "Peeking inside the black-box: a survey on explainable artificial intelligence (xai)",
            "venue": "IEEE access,",
            "year": 2018
        },
        {
            "authors": [
                "John Ashburner",
                "Karl J Friston"
            ],
            "title": "Voxel-based morphometry\u2014the methods",
            "year": 2000
        },
        {
            "authors": [
                "Stefan Kl\u00f6ppel",
                "Cynthia M Stonnington",
                "Carlton Chu",
                "Bogdan Draganski",
                "Rachael I Scahill",
                "Jonathan D Rohrer",
                "Nick C Fox",
                "Clifford R Jack Jr.",
                "John Ashburner",
                "Richard SJ Frackowiak"
            ],
            "title": "Automatic classification of mr scans in alzheimer\u2019s disease",
            "year": 2008
        },
        {
            "authors": [
                "Saima Farhan",
                "Muhammad Abuzar Fahiem",
                "Huma Tauseef"
            ],
            "title": "An ensemble-of-classifiers based approach for early diagnosis of alzheimer\u2019s disease: classification using structural features of brain images. Computational and mathematical methods in medicine",
            "year": 2014
        },
        {
            "authors": [
                "Hugo G Schnack",
                "Mireille Nieuwenhuis",
                "Neeltje EM van Haren",
                "Lucija Abramovic",
                "Thomas W Scheewe",
                "Rachel M Brouwer",
                "Hilleke E Hulshoff Pol",
                "Ren\u00e9 S Kahn"
            ],
            "title": "Can structural mri aid in clinical classification? a machine learning study in two independent samples of patients with schizophrenia, bipolar disorder and healthy subjects",
            "year": 2014
        },
        {
            "authors": [
                "Yuan Xiao",
                "Zhihan Yan",
                "Youjin Zhao",
                "Bo Tao",
                "Huaiqiang Sun",
                "Fei Li",
                "Li Yao",
                "Wenjing Zhang",
                "Shah Chandan",
                "Jieke Liu"
            ],
            "title": "Support vector machine-based classification of first episode drug-na\u00efve schizophrenia patients and healthy controls using structural mri",
            "venue": "Schizophrenia research,",
            "year": 2019
        },
        {
            "authors": [
                "Vaughn R Steele",
                "Vikram Rao",
                "Vince D Calhoun",
                "Kent A Kiehl"
            ],
            "title": "Machine learning of structural magnetic resonance imaging predicts psychopathic traits in adolescent offenders",
            "year": 2017
        },
        {
            "authors": [
                "Beno\u00eet Magnin",
                "Lilia Mesrob",
                "Serge Kinkingn\u00e9hun",
                "M\u00e9lanie P\u00e9l\u00e9grini-Issac",
                "Olivier Colliot",
                "Marie Sarazin",
                "Bruno Dubois",
                "St\u00e9phane Leh\u00e9ricy",
                "Habib Benali"
            ],
            "title": "Support vector machine-based classification of alzheimer\u2019s disease from whole-brain anatomical",
            "venue": "mri. Neuroradiology,",
            "year": 2009
        },
        {
            "authors": [
                "Daoqiang Zhang",
                "Yaping Wang",
                "Luping Zhou",
                "Hong Yuan",
                "Dinggang Shen"
            ],
            "title": "Alzheimer\u2019s Disease Neuroimaging Initiative, et al. Multimodal classification of alzheimer\u2019s disease and mild cognitive impairment",
            "year": 2011
        },
        {
            "authors": [
                "Barnaly Rashid",
                "Mohammad R Arbabshirani",
                "Eswar Damaraju",
                "Mustafa S Cetin",
                "Robyn Miller",
                "Godfrey D Pearlson",
                "Vince D Calhoun"
            ],
            "title": "Classification of schizophrenia and bipolar patients using static and dynamic resting-state fmri brain",
            "venue": "connectivity. Neuroimage,",
            "year": 2016
        },
        {
            "authors": [
                "Samuel Iddi",
                "Dan Li",
                "Paul S Aisen",
                "Michael S Rafii",
                "Wesley K Thompson",
                "Michael C Donohue"
            ],
            "title": "Alzheimer\u2019s Disease Neuroimaging Initiative, et al. Predicting the course of alzheimer\u2019s progression",
            "venue": "Brain informatics,",
            "year": 2019
        },
        {
            "authors": [
                "Hui Shen",
                "Lubin Wang",
                "Yadong Liu",
                "Dewen Hu"
            ],
            "title": "Discriminative analysis of resting-state functional 79 connectivity patterns of schizophrenia using low dimensional embedding of fmri",
            "year": 2010
        },
        {
            "authors": [
                "Archana Venkataraman",
                "Thomas J Whitford",
                "Carl-Fredrik Westin",
                "Polina Golland",
                "Marek Kubicki"
            ],
            "title": "Whole brain resting state functional connectivity abnormalities in schizophrenia",
            "venue": "Schizophrenia research,",
            "year": 2012
        },
        {
            "authors": [
                "Ling-Li Zeng",
                "Huaning Wang",
                "Panpan Hu",
                "Bo Yang",
                "Weidan Pu",
                "Hui Shen",
                "Xingui Chen",
                "Zhening Liu",
                "Hong Yin",
                "Qingrong Tan"
            ],
            "title": "Multi-site diagnostic classification of schizophrenia using discriminant deep learning with functional connectivity",
            "venue": "mri. EBioMedicine,",
            "year": 2018
        },
        {
            "authors": [
                "Benson Mwangi",
                "Tian Siva Tian",
                "Jair C Soares"
            ],
            "title": "A review of feature reduction techniques in neuroimaging",
            "year": 2014
        },
        {
            "authors": [
                "Arthur Mensch",
                "Julien Mairal",
                "Danilo Bzdok",
                "Bertrand Thirion",
                "Ga\u00ebl Varoquaux"
            ],
            "title": "Learning neural representations of human cognition across many fMRI studies",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Saman Sarraf",
                "Ghassem Tofighi"
            ],
            "title": "Classification of alzheimer\u2019s disease using fmri data and deep learning convolutional neural networks",
            "venue": "arXiv preprint arXiv:1603.08631,",
            "year": 2016
        },
        {
            "authors": [
                "Dong Nie",
                "Han Zhang",
                "Ehsan Adeli",
                "Luyan Liu",
                "Dinggang Shen"
            ],
            "title": "3d deep learning for multi-modal imaging-guided survival time prediction of brain tumor patients",
            "venue": "In International conference on medical image computing and computer-assisted intervention,",
            "year": 2016
        },
        {
            "authors": [
                "Usman Mahmood",
                "Md Mahfuzur Rahman",
                "Alex Fedorov",
                "Noah Lewis",
                "Zening Fu",
                "V.D. Calhoun",
                "Sergey Plis"
            ],
            "title": "Whole MILC: generalizing learned dynamics across tasks, datasets, and populations",
            "venue": "In Proceedings of the 23rd international conference on medical image computing and computer assisted intervention (MICCAI),",
            "year": 2020
        },
        {
            "authors": [
                "Anees Abrol",
                "Zening Fu",
                "Mustafa Salman",
                "Rogers Silva",
                "Yuhui Du",
                "Sergey Plis",
                "Vince Calhoun"
            ],
            "title": "Deep learning encodes robust discriminative neuroimaging representations to outperform standard machine learning",
            "venue": "Nature communications,",
            "year": 2021
        },
        {
            "authors": [
                "Kyle Young",
                "Gareth Booth",
                "Becks Simpson",
                "Reuben Dutton",
                "Sally Shrapnel"
            ],
            "title": "Deep neural network or dermatologist? In Interpretability of Machine Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision Support",
            "year": 2019
        },
        {
            "authors": [
                "Alfredo Vellido"
            ],
            "title": "The importance of interpretability and visualization in machine learning for applications in medicine and health care",
            "venue": "Neural computing and applications,",
            "year": 2019
        },
        {
            "authors": [
                "Adriano Lucieri",
                "Muhammad Naseer Bajwa",
                "Stephan Alexander Braun",
                "Muhammad Imran Malik",
                "Andreas Dengel",
                "Sheraz Ahmed"
            ],
            "title": "On interpretability of deep learning based skin lesion classifiers using concept activation vectors",
            "venue": "In 2020 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Lapuschkin",
                "Alexander Binder",
                "Gr\u00e9goire Montavon",
                "Klaus-Robert Muller",
                "Wojciech Samek"
            ],
            "title": "Analyzing classifiers: Fisher vectors and deep neural networks",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Danilo Bzdok",
                "Michael Eickenberg",
                "Olivier Grisel",
                "Bertrand Thirion",
                "Ga\u00ebl Varoquaux"
            ],
            "title": "Semi- 80 supervised factored logistic regression for high-dimensional neuroimaging data",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Ms Aayushi Bansal",
                "Dr Rewa Sharma",
                "Dr Mamta Kathuria"
            ],
            "title": "A systematic review on data scarcity problem in deep learning: solution and applications",
            "venue": "ACM Computing Surveys (CSUR),",
            "year": 2022
        },
        {
            "authors": [
                "Drew Landis",
                "William Courtney",
                "Christopher Dieringer",
                "Ross Kelly",
                "Margaret King",
                "Brittny Miller",
                "Runtang Wang",
                "Dylan Wood",
                "Jessica A Turner",
                "Vince D Calhoun"
            ],
            "title": "Coins data exchange: An open platform for compiling, curating, and disseminating neuroimaging",
            "venue": "data. NeuroImage,",
            "year": 2016
        },
        {
            "authors": [
                "Armin W Thomas",
                "Klaus-Robert M\u00fcller",
                "Wojciech Samek"
            ],
            "title": "Deep transfer learning for whole-brain fMRI analyses",
            "venue": "arXiv preprint arXiv:1907.01953,",
            "year": 1953
        },
        {
            "authors": [
                "Usman Mahmood",
                "Md Mahfuzur Rahman",
                "Alex Fedorov",
                "Zening Fu",
                "Sergey Plis"
            ],
            "title": "Transfer learning of fMRI dynamics",
            "venue": "arXiv preprint arXiv:1911.06813,",
            "year": 2019
        },
        {
            "authors": [
                "Alejandro Newell",
                "Jia Deng"
            ],
            "title": "How useful is self-supervised pretraining for visual tasks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Esten H Leonardsen",
                "Han Peng",
                "Tobias Kaufmann",
                "Ingrid Agartz",
                "Ole A Andreassen",
                "Elisabeth Gulowsen Celius",
                "Thomas Espeseth",
                "Hanne F Harbo",
                "Einar A H\u00f8gest\u00f8l",
                "Ann-Marie de Lange"
            ],
            "title": "Deep neural networks learn general and clinically relevant representations of the ageing",
            "venue": "brain. NeuroImage,",
            "year": 2022
        },
        {
            "authors": [
                "Fabian Eitel",
                "Emily Soehler",
                "Judith Bellmann-Strobl",
                "Alexander U Brandt",
                "Klemens Ruprecht",
                "Ren\u00e9 M Giess",
                "Joseph Kuchling",
                "Susanna Asseyer",
                "Martin Weygandt",
                "John-Dylan Haynes"
            ],
            "title": "Uncovering convolutional neural network decisions for diagnosing multiple sclerosis on conventional mri using layer-wise relevance propagation",
            "venue": "NeuroImage: Clinical,",
            "year": 2019
        },
        {
            "authors": [
                "Juan Miguel Valverde",
                "Vandad Imani",
                "Ali Abdollahzadeh",
                "Riccardo De Feo",
                "Mithilesh Prakash",
                "Robert Ciszek",
                "Jussi Tohka"
            ],
            "title": "Transfer learning in magnetic resonance brain imaging: a systematic review",
            "venue": "Journal of imaging,",
            "year": 2021
        },
        {
            "authors": [
                "Johannes Rieke",
                "Fabian Eitel",
                "Martin Weygandt",
                "John-Dylan Haynes",
                "Kerstin Ritter"
            ],
            "title": "Visualizing convolutional networks for mri-based diagnosis of alzheimer\u2019s disease",
            "venue": "In Understanding and Interpreting Machine Learning in Medical Image Computing Applications,",
            "year": 2018
        },
        {
            "authors": [
                "Moritz B\u00f6hle",
                "Fabian Eitel",
                "Martin Weygandt",
                "Kerstin Ritter"
            ],
            "title": "Layer-wise relevance propagation for explaining deep neural network decisions in mri-based alzheimer\u2019s disease classification",
            "venue": "Frontiers in aging neuroscience,",
            "year": 2019
        },
        {
            "authors": [
                "Rembrandt Bakker",
                "Paul Tiesinga",
                "Rolf K\u00f6tter"
            ],
            "title": "The scalable brain atlas: instant web-based access to public brain atlases and related content. Neuroinformatics",
            "year": 2015
        },
        {
            "authors": [
                "Wenxing Hu",
                "Xianghe Meng",
                "Yuntong Bai",
                "Aiying Zhang",
                "Gang Qu",
                "Biao Cai",
                "Gemeng Zhang",
                "Tony W Wilson",
                "Julia M Stephen",
                "Vince D Calhoun"
            ],
            "title": "Interpretable multimodal fusion networks 81 reveal mechanisms of brain cognition",
            "venue": "IEEE transactions on medical imaging,",
            "year": 2021
        },
        {
            "authors": [
                "Weizheng Yan",
                "Sergey Plis",
                "Vince D Calhoun",
                "Shengfeng Liu",
                "Rongtao Jiang",
                "Tian-Zi Jiang",
                "Jing Sui"
            ],
            "title": "Discriminating schizophrenia from normal controls using resting state functional network connectivity: A deep neural network and layer-wise relevance propagation method",
            "venue": "IEEE 27th International Workshop on Machine Learning for Signal Processing (MLSP),",
            "year": 2017
        },
        {
            "authors": [
                "Gidon Levakov",
                "Gideon Rosenthal",
                "Ilan Shelef",
                "Tammy Riklin Raviv",
                "Galia Avidan"
            ],
            "title": "From a deep learning model back to the brain\u2014identifying regional predictors and their relation to aging",
            "venue": "Human brain mapping,",
            "year": 2020
        },
        {
            "authors": [
                "Carlo Biffi",
                "Juan J Cerrolaza",
                "Giacomo Tarroni",
                "Wenjia Bai",
                "Antonio De Marvao",
                "Ozan Oktay",
                "Christian Ledig",
                "Loic Le Folgoc",
                "Konstantinos Kamnitsas",
                "Georgia Doumou"
            ],
            "title": "Explainable anatomical shape analysis through deep hierarchical generative models",
            "venue": "IEEE transactions on medical imaging,",
            "year": 2020
        },
        {
            "authors": [
                "Francisco J Martinez-Murcia",
                "Andres Ortiz",
                "Juan-Manuel Gorriz",
                "Javier Ramirez",
                "Diego Castillo- Barnes"
            ],
            "title": "Studying the manifold structure of alzheimer\u2019s disease: a deep learning approach using convolutional autoencoders",
            "venue": "IEEE journal of biomedical and health informatics,",
            "year": 2019
        },
        {
            "authors": [
                "Matthew Leming",
                "Juan Manuel G\u00f3rriz",
                "John Suckling"
            ],
            "title": "Ensemble deep learning on large, mixedsite fmri datasets in autism and other tasks",
            "venue": "International journal of neural systems,",
            "year": 2020
        },
        {
            "authors": [
                "Ziqi Tang",
                "Kangway V Chuang",
                "Charles DeCarli",
                "Lee-Way Jin",
                "Laurel Beckett",
                "Michael J Keiser",
                "Brittany N Dugger"
            ],
            "title": "Interpretable classification of alzheimer\u2019s disease pathologies with a convolutional neural network pipeline",
            "venue": "Nature communications,",
            "year": 2019
        },
        {
            "authors": [
                "Gareth Ball",
                "Claire E Kelly",
                "Richard Beare",
                "Marc L Seal"
            ],
            "title": "Individual variation underlying brain age estimates in typical development",
            "year": 2021
        },
        {
            "authors": [
                "Harshit Parmar",
                "Brian Nutter",
                "Rodney Long",
                "Sameer Antani",
                "Sunanda Mitra"
            ],
            "title": "Spatiotemporal feature extraction and classification of alzheimer\u2019s disease using deep learning 3d-cnn for fmri data",
            "venue": "Journal of Medical Imaging,",
            "year": 2020
        },
        {
            "authors": [
                "Chunfeng Lian",
                "Mingxia Liu",
                "Yongsheng Pan",
                "Dinggang Shen"
            ],
            "title": "Attention-guided hybrid network for dementia diagnosis with structural mr images",
            "venue": "IEEE transactions on cybernetics,",
            "year": 2020
        },
        {
            "authors": [
                "Sartaj Bhuvaji",
                "Ankita Kadam",
                "Prajakta Bhumkar",
                "Sameer Dedge",
                "Swati Kanchan"
            ],
            "title": "Brain tumor classification",
            "venue": "(mri). Kaggle,",
            "year": 2020
        },
        {
            "authors": [
                "Tal Yarkoni",
                "Russell A Poldrack",
                "Thomas E Nichols",
                "David C Van Essen",
                "Tor D Wager"
            ],
            "title": "Large-scale automated synthesis of human functional neuroimaging data",
            "venue": "Nature methods,",
            "year": 2011
        },
        {
            "authors": [
                "Markus Loeffler",
                "Christoph Engel",
                "Peter Ahnert",
                "Dorothee Alfermann",
                "Katrin Arelin",
                "Ronny Baber",
                "Frank Beutner",
                "Hans Binder",
                "Elmar Br\u00e4hler",
                "Ralph Burkhardt"
            ],
            "title": "The life-adult-study: objectives and design of a population-based cohort study with 10,000 deeply phenotyped adults in germany",
            "venue": "BMC public health,",
            "year": 2015
        },
        {
            "authors": [
                "Sina Farsiu",
                "Stephanie J Chiu",
                "Rachelle V O\u2019Connell",
                "Francisco A Folgar",
                "Eric Yuan",
                "Joseph A Izatt",
                "82 Cynthia A Toth"
            ],
            "title": "Age-Related Eye Disease Study 2 Ancillary Spectral Domain Optical Coherence Tomography Study Group, et al. Quantitative classification of eyes with and without intermediate age-related macular degeneration using optical coherence tomography",
            "year": 2014
        },
        {
            "authors": [
                "David W Shattuck",
                "Mubeena Mirza",
                "Vitria Adisetiyo",
                "Cornelius Hojatkashani",
                "Georges Salamon",
                "Katherine L Narr",
                "Russell A Poldrack",
                "Robert M Bilder",
                "Arthur W Toga"
            ],
            "title": "Construction of a 3d probabilistic atlas of human cortical structures",
            "year": 2008
        },
        {
            "authors": [
                "Oskar Maier",
                "Bjoern H Menze",
                "Janina Von der Gablentz",
                "Levin H\u00e4ni",
                "Mattias P Heinrich",
                "Matthias Liebrand",
                "Stefan Winzeck",
                "Abdul Basit",
                "Paul Bentley",
                "Liang Chen"
            ],
            "title": "Isles 2015-a public evaluation benchmark for ischemic stroke lesion segmentation from multispectral mri",
            "venue": "Medical image analysis,",
            "year": 2017
        },
        {
            "authors": [
                "Nathalie Tzourio-Mazoyer",
                "Brigitte Landeau",
                "Dimitri Papathanassiou",
                "Fabrice Crivello",
                "Octave Etard",
                "Nicolas Delcroix",
                "Bernard Mazoyer",
                "Marc Joliot"
            ],
            "title": "Automated anatomical labeling of activations in spm using a macroscopic anatomical parcellation of the mni mri single-subject brain",
            "year": 2002
        },
        {
            "authors": [
                "Dumitru Erhan",
                "Yoshua Bengio",
                "Aaron Courville",
                "Pascal Vincent"
            ],
            "title": "Visualizing higher-layer features of a deep network",
            "venue": "University of Montreal,",
            "year": 2009
        },
        {
            "authors": [
                "Laurens Van der Maaten",
                "Geoffrey Hinton"
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of machine learning research,",
            "year": 2008
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv preprint arXiv:1312.6114,",
            "year": 2013
        },
        {
            "authors": [
                "Taban Eslami",
                "Joseph S Raiker",
                "Fahad Saeed"
            ],
            "title": "Explainable and scalable machine learning algorithms for detection of autism spectrum disorder using fmri data. In Neural Engineering Techniques for Autism Spectrum Disorder",
            "year": 2021
        },
        {
            "authors": [
                "Usman Mahmood",
                "Zening Fu",
                "Vince Calhoun",
                "Sergey Plis"
            ],
            "title": "Deep dynamic effective connectivity estimation from multivariate time series",
            "venue": "arXiv preprint arXiv:2202.02393,",
            "year": 2022
        },
        {
            "authors": [
                "Kichang Kwak",
                "Marc Niethammer",
                "Kelly S Giovanello",
                "Martin Styner",
                "Eran Dayan"
            ],
            "title": "Alzheimer\u2019s Disease Neuroimaging Initiative, et al. Differential role for hippocampal subfields in alzheimer\u2019s disease progression revealed with deep learning",
            "venue": "Cerebral Cortex,",
            "year": 2022
        },
        {
            "authors": [
                "Jennifer S Polson",
                "Haoyue Zhang",
                "Kambiz Nael",
                "Noriko Salamon",
                "Bryan Y Yoo",
                "Suzie El-Saden",
                "Sidney Starkman",
                "Namkug Kim",
                "Dong-Wha Kang",
                "William Speier"
            ],
            "title": "Deep learning approaches to identify patients within the thrombolytic treatment",
            "year": 2022
        },
        {
            "authors": [
                "Guowei Zheng",
                "Yu Zhang",
                "Ziyang Zhao",
                "Yin Wang",
                "Xia Liu",
                "Yingying Shang",
                "Zhaoyang Cong",
                "Stavros I Dimitriadis",
                "Zhijun Yao",
                "Bin Hu"
            ],
            "title": "A transformer-based multi-features fusion model for prediction of conversion in mild cognitive impairment",
            "year": 2022
        },
        {
            "authors": [
                "Bin Lu",
                "Hui-Xian Li",
                "Zhi-Kai Chang",
                "Le Li",
                "Ning-Xuan Chen",
                "Zhi-Chen Zhu",
                "Hui-Xia Zhou",
                "Xue-Ying Li",
                "Yu-Wei Wang",
                "Shi-Xian Cui"
            ],
            "title": "A practical alzheimer\u2019s disease classifier via brain imaging-based deep learning on 85,721 samples",
            "venue": "Journal of Big Data,",
            "year": 2022
        },
        {
            "authors": [
                "Jeyeon Lee",
                "Brian J Burkett",
                "Hoon-Ki Min",
                "Matthew L Senjem",
                "Emily S Lundt",
                "Hugo Botha",
                "Jonathan Graff-Radford",
                "Leland R Barnard",
                "Jeffrey L Gunter",
                "Christopher G Schwarz"
            ],
            "title": "Deep learning-based brain age prediction in normal aging and dementia",
            "venue": "Nature Aging,",
            "year": 2022
        },
        {
            "authors": [
                "Yuda Bi",
                "Anees Abrol",
                "Zening Fu",
                "Jiayu Chen",
                "Jingyu Liu",
                "Vince Calhoun"
            ],
            "title": "Prediction of gender from longitudinal mri data via deep learning on adolescent data reveals unique patterns associated with brain structure and change over a two-year period",
            "venue": "arXiv preprint arXiv:2209.07590,",
            "year": 2022
        },
        {
            "authors": [
                "Tim Hahn",
                "Jan Ernsting",
                "Nils R Winter",
                "Vincent Holstein",
                "Ramona Leenings",
                "Marie Beisemann",
                "Lukas Fisch",
                "Kelvin Sarink",
                "Daniel Emden",
                "Nils Opel"
            ],
            "title": "An uncertainty-aware, shareable, and transparent neural network architecture for brain-age modeling",
            "venue": "Science advances,",
            "year": 2022
        },
        {
            "authors": [
                "Kaida Ning",
                "Pascale B Cannon",
                "Jiawei Yu",
                "Srinesh Shenoi",
                "Lu Wang",
                "Joydeep Sarkar"
            ],
            "title": "Alzheimer\u2019s Disease Neuroimaging Initiative, et al. Characterizing brain imaging features associated with adascog13 sub-scores with 3d convolutional neural networks. bioRxiv, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Tarjni Vyas",
                "Raj Yadav",
                "Chitra Solanki",
                "Rutvi Darji",
                "Shivani Desai",
                "Sudeep Tanwar"
            ],
            "title": "Deep learning-based scheme to diagnose parkinson\u2019s disease",
            "venue": "Expert Systems,",
            "year": 2022
        },
        {
            "authors": [
                "David A Wood",
                "Sina Kafiabadi",
                "Ayisha Al Busaidi",
                "Emily Guilhem",
                "Antanas Montvila",
                "Jeremy Lynch",
                "Matthew Townend",
                "Siddharth Agarwal",
                "Asif Mazumder",
                "Gareth J Barker"
            ],
            "title": "Accurate brain-age models for routine clinical mri examinations",
            "year": 2022
        },
        {
            "authors": [
                "Jennifer S Polson",
                "Haoyue Zhang",
                "Kambiz Nael",
                "Noriko Salamon",
                "Bryan Y Yoo",
                "Suzie El-Saden",
                "Sidney Starkman",
                "Namkug Kim",
                "Dong-Wha Kang",
                "William F Speier IV"
            ],
            "title": "Identifying acute ischemic stroke patients within the thrombolytic treatment window using deep learning",
            "venue": "Journal of Neuroimaging,",
            "year": 2022
        },
        {
            "authors": [
                "Valentina Bordin",
                "Davide Coluzzi",
                "Massimo W Rivolta",
                "Giuseppe Baselli"
            ],
            "title": "Explainable ai points to white matter hyperintensities for alzheimer\u2019s disease identification: a preliminary study",
            "venue": "In 2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),",
            "year": 2022
        },
        {
            "authors": [
                "Kobra Etminani",
                "Amira Soliman",
                "Anette Davidsson",
                "Jose R Chang",
                "Bego\u00f1a Mart\u00ednez-Sanchis",
                "Stefan Byttner",
                "Valle Camacho",
                "Matteo Bauckneht",
                "Roxana Stegeran",
                "Marcus Ressner"
            ],
            "title": "A 3d deep learning model to predict the diagnosis of dementia with lewy bodies, alzheimer\u2019s disease, and mild cognitive impairment using brain 18f-fdg pet",
            "venue": "European journal of nuclear medicine and molecular imaging,",
            "year": 2022
        },
        {
            "authors": [
                "Kai Gao",
                "Zhipeng Fan",
                "Jianpo Su",
                "Ling-Li Zeng",
                "Hui Shen",
                "Jubo Zhu",
                "Dewen Hu"
            ],
            "title": "Deep transfer learning for cerebral cortex using area-preserving geometry mapping",
            "venue": "Cerebral Cortex,",
            "year": 2022
        },
        {
            "authors": [
                "Jeyeon Lee",
                "Brian J Burkett",
                "Hoon-Ki Min",
                "Matthew L Senjem",
                "Ellen Dicks",
                "Nick Corriveau-Lecavalier",
                "84 Carly T Mester",
                "Heather J Wiste",
                "Emily S Lundt",
                "Melissa E Murray"
            ],
            "title": "Synthesizing images of tau pathology from cross-modal neuroimaging using deep learning",
            "venue": "bioRxiv,",
            "year": 2022
        },
        {
            "authors": [
                "Gang Qu",
                "Wenxing Hu",
                "Li Xiao",
                "Junqi Wang",
                "Yuntong Bai",
                "Beenish Patel",
                "Kun Zhang",
                "Yu-Ping Wang"
            ],
            "title": "Brain functional connectivity analysis via graphical deep learning",
            "venue": "IEEE Transactions on Biomedical Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "Kyle B See",
                "David J Arpin",
                "David E Vaillancourt",
                "Ruogu Fang",
                "Stephen A Coombes"
            ],
            "title": "Unraveling somatotopic organization in the human brain using machine learning and adaptive supervoxel-based parcellations",
            "year": 2021
        },
        {
            "authors": [
                "James Zou",
                "David Park",
                "Aubrey Johnson",
                "Xinyang Feng",
                "Michelle Pardo",
                "Jeanelle France",
                "Zeljko Tomljanovic",
                "Adam M Brickman",
                "Devangere P Devanand",
                "Jos\u00e9 A Luchsinger"
            ],
            "title": "Deep learning improves utility of tau pet in the study of alzheimer\u2019s disease",
            "venue": "Alzheimer\u2019s & Dementia: Diagnosis, Assessment & Disease Monitoring,",
            "year": 2021
        },
        {
            "authors": [
                "Suriya Murugan",
                "Chandran Venkatesan",
                "MG Sumithra",
                "Xiao-Zhi Gao",
                "B Elakkiya",
                "Muthuramalingam Akila",
                "S Manoharan"
            ],
            "title": "Demnet: a deep learning model for early diagnosis of alzheimer diseases and dementia from mr images",
            "venue": "IEEE Access,",
            "year": 2021
        },
        {
            "authors": [
                "Matthew Leming",
                "John Suckling"
            ],
            "title": "Deep learning for sex classification in resting-state and task functional brain networks from the uk biobank",
            "year": 2021
        },
        {
            "authors": [
                "Kobra Etminani",
                "Amira Soliman",
                "Stefan Byttner",
                "Anette Davidsson",
                "Miguel Ochoa-Figueroa"
            ],
            "title": "Peeking inside the box: Transfer learning vs 3d convolutional neural networks applied in neurodegenerative diseases",
            "venue": "In 2021 International Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB 2021),",
            "year": 2021
        },
        {
            "authors": [
                "Martin Dyrba",
                "Moritz Hanzig",
                "Slawek Altenstein",
                "Sebastian Bader",
                "Tommaso Ballarini",
                "Frederic Brosseron",
                "Katharina Buerger",
                "Daniel Cantr\u00e9",
                "Peter Dechent",
                "Laura Dobisch"
            ],
            "title": "Improving 3d convolutional neural network comprehensibility via interactive visualization of relevance maps: evaluation in alzheimer\u2019s disease",
            "venue": "Alzheimer\u2019s research & therapy,",
            "year": 2021
        },
        {
            "authors": [
                "David A Wood",
                "Sina Kafiabadi",
                "Aisha Al Busaidi",
                "Emily Guilhem",
                "Antanas Montvila",
                "Siddarth Agarwal",
                "Jeremy Lynch",
                "Matthew Townend",
                "Gareth Barker",
                "Sebastian Ourselin"
            ],
            "title": "Automated triaging of head mri examinations using convolutional neural networks",
            "venue": "In Medical Imaging with Deep Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Kichang Kwak",
                "Kelly S Giovanello",
                "Andrea Bozoki",
                "Martin Styner",
                "Eran Dayan"
            ],
            "title": "Alzheimer\u2019s Disease Neuroimaging Initiative, et al. Subtyping of mild cognitive impairment using a deep learning model based on brain atrophy patterns",
            "venue": "Cell Reports Medicine,",
            "year": 2021
        },
        {
            "authors": [
                "Lebo Wang",
                "Kaiming Li",
                "Xiaoping P Hu"
            ],
            "title": "Graph convolutional network for fmri analysis based on connectivity neighborhood",
            "venue": "Network Neuroscience,",
            "year": 2021
        },
        {
            "authors": [
                "Jinhyeong Bae",
                "Jane Stocks",
                "Ashley Heywood",
                "Youngmoon Jung",
                "Lisanne Jenkins",
                "Virginia Hill",
                "Aggelos Katsaggelos",
                "Karteek Popuri",
                "Howie Rosen",
                "M Faisal Beg"
            ],
            "title": "Transfer learning for predicting conversion from mild cognitive impairment to dementia of alzheimer\u2019s type based on a three-dimensional convolutional neural network",
            "venue": "Neurobiology of aging,",
            "year": 2021
        },
        {
            "authors": [
                "Rajat Mani Thomas",
                "Selene Gallo",
                "Leonardo Cerliani",
                "Paul Zhutovsky",
                "Ahmed El-Gazzar",
                "Guido Van Wingen"
            ],
            "title": "Classifying autism spectrum disorder using the temporal statistics of resting-state functional mri data with 3d convolutional neural networks",
            "venue": "Frontiers in psychiatry,",
            "year": 2020
        },
        {
            "authors": [
                "Tahjid Ashfaque Mostafa",
                "Irene Cheng"
            ],
            "title": "Parkinson\u2019s disease detection using ensemble architecture from mr images",
            "venue": "IEEE 20th International Conference on Bioinformatics and Bioengineering (BIBE),",
            "year": 2020
        },
        {
            "authors": [
                "Kichang Kwak",
                "Kelly S Giovanello",
                "Martin Styner",
                "Eran Dayan"
            ],
            "title": "Alzheimer\u2019s Disease Neuroimaging Initiative, et al. Atrophy-centered subtyping of mild cognitive impairment",
            "venue": "medRxiv,",
            "year": 2020
        },
        {
            "authors": [
                "Jihoon Oh",
                "Baek-Lok Oh",
                "Kyong-Uk Lee",
                "Jeong-Ho Chae",
                "Kyongsik Yun"
            ],
            "title": "Identifying schizophrenia using structural mri with a deep learning algorithm",
            "venue": "Frontiers in psychiatry,",
            "year": 2020
        },
        {
            "authors": [
                "Matthew Leming",
                "John Suckling"
            ],
            "title": "Stochastic encoding of graphs in deep learning allows for complex analysis of gender classification in resting-state and task functional brain networks from the uk biobank",
            "venue": "arXiv preprint arXiv:2002.10936,",
            "year": 2020
        },
        {
            "authors": [
                "Christopher West",
                "Sara Soltaninejad",
                "Irene Cheng"
            ],
            "title": "Assessing the capability of deep-learning models in parkinson\u2019s disease diagnosis",
            "venue": "In International Conference on Smart Multimedia,",
            "year": 2019
        },
        {
            "authors": [
                "Jinhyeong Bae",
                "Jane Stocks",
                "Ashley Heywood",
                "Youngmoon Jung",
                "Lisanne Jenkins",
                "Aggelos Katsaggelos",
                "Karteek Popuri",
                "M Faisal Beg",
                "Lei Wang"
            ],
            "title": "Alzheimer\u2019s Disease Neuroimaging Initiative, et al. Transfer learning for predicting conversion from mild cognitive impairment to dementia of alzheimer\u2019s type based on 3d-convolutional neural network. bioRxiv, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Jyoti Islam",
                "Yanqing Zhang"
            ],
            "title": "Understanding 3d cnn behavior for alzheimer\u2019s disease diagnosis from brain pet scan",
            "venue": "arXiv preprint arXiv:1912.04563,",
            "year": 2019
        },
        {
            "authors": [
                "Tae-Eui Kam",
                "Han Zhang",
                "Zhicheng Jiao",
                "Dinggang Shen"
            ],
            "title": "Deep learning of static and dynamic brain functional networks for early mci detection",
            "venue": "IEEE transactions on medical imaging,",
            "year": 2019
        },
        {
            "authors": [
                "Yuanyuan Liu",
                "Zhouxuan Li",
                "Qiyang Ge",
                "Nan Lin",
                "Momiao Xiong"
            ],
            "title": "Deep feature selection and causal analysis of alzheimer\u2019s disease",
            "venue": "Frontiers in neuroscience,",
            "year": 2019
        },
        {
            "authors": [
                "Soheil Esmaeilzadeh",
                "Dimitrios Ioannis Belivanis",
                "Kilian M Pohl",
                "Ehsan Adeli"
            ],
            "title": "End-to-end alzheimer\u2019s disease diagnosis and biomarker identification",
            "venue": "In International workshop on machine Learning in medical imaging,",
            "year": 2018
        },
        {
            "authors": [
                "P\u00e1l Vakli",
                "Regina J De\u00e1k-Meszl\u00e9nyi",
                "Petra Hermann",
                "Zolt\u00e1n Vidny\u00e1nszky"
            ],
            "title": "Transfer learning improves resting-state functional connectivity pattern analysis using convolutional neural networks. GigaScience",
            "year": 2018
        },
        {
            "authors": [
                "Stanislas Chambon",
                "Mathieu N Galtier",
                "Pierrick J Arnal",
                "Gilles Wainrib",
                "Alexandre Gramfort"
            ],
            "title": "A deep learning architecture for temporal sleep stage classification using multivariate and multimodal 86 time series",
            "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering,",
            "year": 2018
        },
        {
            "authors": [
                "Benjam\u00edn Guti\u00e9rrez-Becker",
                "Christian Wachinger"
            ],
            "title": "Deep multi-structural shape analysis: application to neuroanatomy",
            "venue": "In International Conference on Medical Image Computing and Computer-Assisted Intervention,",
            "year": 2018
        },
        {
            "authors": [
                "Z\u00fclfikar Aslan",
                "Mehmet Akin"
            ],
            "title": "A deep learning approach in automated detection of schizophrenia using scalogram images of eeg signals",
            "venue": "Physical and Engineering Sciences in Medicine,",
            "year": 2022
        },
        {
            "authors": [
                "Ksenia Sokolova"
            ],
            "title": "Investigating brain aging with Magnetic Resonance Imaging and Convolutional Neural Networks",
            "venue": "PhD thesis, King\u2019s College London,",
            "year": 2021
        },
        {
            "authors": [
                "Iv\u00e1n S\u00e1nchez Fern\u00e1ndez",
                "Edward Yang",
                "Marta Amengual-Gual",
                "Cristina Barcia Aguilar",
                "Paola Calvachi Prieto",
                "Jurriaan M Peters"
            ],
            "title": "Convolutional neural networks to identify malformations of cortical development: A feasibility study",
            "venue": "Seizure,",
            "year": 2021
        },
        {
            "authors": [
                "Jinlong Hu",
                "Lijie Cao",
                "Tenghui Li",
                "Shoubin Dong",
                "Ping Li"
            ],
            "title": "Gat-li: a graph attention network based learning and interpreting method for functional brain network classification",
            "venue": "BMC bioinformatics,",
            "year": 2021
        },
        {
            "authors": [
                "Zixuan Liu",
                "Ehsan Adeli",
                "Kilian M Pohl",
                "Qingyu Zhao"
            ],
            "title": "Going beyond saliency maps: Training deep models to interpret deep models",
            "venue": "In International Conference on Information Processing in Medical Imaging,",
            "year": 2021
        },
        {
            "authors": [
                "Alina Lopatina",
                "Stefan Ropele",
                "Renat Sibgatulin",
                "J\u00fcrgen R Reichenbach",
                "Daniel G\u00fcllmar"
            ],
            "title": "Investigation of deep-learning-driven identification of multiple sclerosis patients based on susceptibilityweighted images using relevance analysis",
            "venue": "Frontiers in neuroscience,",
            "year": 2020
        },
        {
            "authors": [
                "Iv\u00e1n S\u00e1nchez Fern\u00e1ndez",
                "Edward Yang",
                "Paola Calvachi",
                "Marta Amengual-Gual",
                "Joyce Y Wu",
                "Darcy Krueger",
                "Hope Northrup",
                "Martina E Bebin",
                "Mustafa Sahin",
                "Kun-Hsing Yu"
            ],
            "title": "Deep learning in rare disease. detection of tubers in tuberous sclerosis complex",
            "venue": "PLoS One,",
            "year": 2020
        },
        {
            "authors": [
                "Prabhat Garg",
                "Elizabeth Davenport",
                "Gowtham Murugesan",
                "Ben Wagner",
                "Christopher Whitlow",
                "Joseph Maldjian",
                "Albert Montillo"
            ],
            "title": "Using convolutional neural networks to automatically detect eyeblink artifacts in magnetoencephalography without resorting to electrooculography",
            "venue": "In International Conference on Medical Image Computing and Computer-Assisted Intervention,",
            "year": 2017
        },
        {
            "authors": [
                "Yosuke Fujiwara",
                "Junichi Ushiba"
            ],
            "title": "Deep residual convolutional neural networks for brain\u2013computer interface to visualize neural processing of hand movements in the human brain",
            "venue": "Frontiers in Computational Neuroscience,",
            "year": 2022
        },
        {
            "authors": [
                "Daniela Ushizima",
                "Yuheng Chen",
                "Maryana Alegro",
                "Dulce Ovando",
                "Rana Eser",
                "WingHung Lee",
                "Kinson Poon",
                "Anubhav Shankar",
                "Namrata Kantamneni",
                "Shruti Satrawada"
            ],
            "title": "Deep learning for alzheimer\u2019s disease: Mapping large-scale histological tau protein for neuroimaging biomarker",
            "venue": "validation. NeuroImage,",
            "year": 2022
        },
        {
            "authors": [
                "Mianxin Liu",
                "Han Zhang",
                "Feng Shi",
                "Dinggang Shen"
            ],
            "title": "Hierarchical graph convolutional network 87 built by multiscale atlases for brain disorder diagnosis using functional connectivity",
            "venue": "arXiv preprint arXiv:2209.11232,",
            "year": 2022
        },
        {
            "authors": [
                "Fan Zhang",
                "Bo Pan",
                "Pengfei Shao",
                "Peng Liu",
                "Shuwei Shen",
                "Peng Yao",
                "Ronald X Xu"
            ],
            "title": "Alzheimer\u2019s Disease Neuroimaging Initiative, et al. A single model deep learning approach for alzheimer\u2019s disease",
            "venue": "diagnosis. Neuroscience,",
            "year": 2022
        },
        {
            "authors": [
                "Chun-Chih Huang",
                "Intan Low",
                "Chia-Hsiang Kao",
                "Chuan-Yu Yu",
                "Tung-Ping Su",
                "Jen-Chuen Hsieh",
                "Yong-Sheng Chen",
                "Li-Fen Chen"
            ],
            "title": "Meg-based classification and grad-cam visualization for major depressive and bipolar disorders with semi-cnn",
            "venue": "In 2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),",
            "year": 2022
        },
        {
            "authors": [
                "Joan Prats-Climent",
                "Maria Teresa Gandia-Ferrero",
                "Irene Torres-Espallardo",
                "Lourdes \u00c1lvarez-Sanchez",
                "Bego\u00f1a Mart\u00ednez-Sanchis",
                "Consuelo Ch\u00e1fer-Peric\u00e1s",
                "Ignacio G\u00f3mez-Rico",
                "Leonor Cerd\u00e1-Alberich",
                "Fernando Aparici-Robles",
                "Miquel Baquero-Toledo"
            ],
            "title": "Artificial intelligence on fdg pet images identifies mild cognitive impairment patients with neurodegenerative disease",
            "venue": "Journal of Medical Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Francesca Inglese",
                "Minseon Kim",
                "Gerda M Steup-Beekman",
                "Tom WJ Huizinga",
                "Mark A Van Buchem",
                "Jeroen De Bresser",
                "Dae-Shik Kim",
                "Itamar Ronen"
            ],
            "title": "Mri-based classification of neuropsychiatric systemic lupus erythematosus patients with self-supervised contrastive learning",
            "venue": "Frontiers in neuroscience,",
            "year": 2022
        },
        {
            "authors": [
                "Linfeng Liu",
                "Siyu Liu",
                "Lu Zhang",
                "Xuan Vinh To",
                "Fatima Nasrallah",
                "Shekhar S Chandra"
            ],
            "title": "Cascaded multi-modal mixing transformers for alzheimer\u2019s disease classification with incomplete data",
            "venue": "arXiv preprint arXiv:2210.00255,",
            "year": 2022
        },
        {
            "authors": [
                "Junhao Zhang",
                "Vishwanatha M Rao",
                "Ye Tian",
                "Yanting Yang",
                "Nicolas Acosta",
                "Zihan Wan",
                "Pin-Yu Lee",
                "Chloe Zhang",
                "Lawrence S Kegeles",
                "Scott A Small"
            ],
            "title": "Detecting schizophrenia with 3d structural brain mri using deep learning",
            "venue": "arXiv preprint arXiv:2206.12980,",
            "year": 2022
        },
        {
            "authors": [
                "Adrian Tousignant Duran"
            ],
            "title": "Prediction of future multiple sclerosis disease progression using deep learning analysis of mri",
            "year": 2022
        },
        {
            "authors": [
                "Jiaguang Li",
                "Ying Wei",
                "Chuyuan Wang",
                "Qian Hu",
                "Yue Liu",
                "Long Xu"
            ],
            "title": "3-d cnn-based multichannel contrastive learning for alzheimer\u2019s disease automatic diagnosis",
            "venue": "IEEE Transactions on Instrumentation and Measurement,",
            "year": 2022
        },
        {
            "authors": [
                "Lingling Ding",
                "Ziyang Liu",
                "Ravikiran Mane",
                "Shuai Wang",
                "Jing Jing",
                "He Fu",
                "Zhenzhou Wu",
                "Hao Li",
                "Yong Jiang",
                "Xia Meng"
            ],
            "title": "Predicting functional outcome in patients with acute brainstem infarction using deep neuroimaging features",
            "venue": "European Journal of Neurology,",
            "year": 2022
        },
        {
            "authors": [
                "Burak Tasci",
                "Irem Tasci"
            ],
            "title": "Deep feature extraction based brain image classification model using preprocessed images: Pdrnet",
            "venue": "Biomedical Signal Processing and Control,",
            "year": 2022
        },
        {
            "authors": [
                "Modupe Odusami",
                "Rytis Maskeli\u016bnas",
                "Robertas Damavsevivcius"
            ],
            "title": "An intelligent system for early recognition of alzheimer\u2019s disease using neuroimaging",
            "year": 2022
        },
        {
            "authors": [
                "Yanjiao Ban",
                "Xuejun Zhang",
                "Huan Lao"
            ],
            "title": "Diagnosis of alzheimer\u2019s disease using structure highlighting key slice stacking and transfer learning",
            "venue": "Medical Physics,",
            "year": 2022
        },
        {
            "authors": [
                "Lu Yu",
                "Wei Xiang",
                "Juan Fang",
                "Yi-Ping Phoebe Chen",
                "Ruifeng Zhu"
            ],
            "title": "A novel explainable neural network for alzheimer\u2019s disease diagnosis",
            "venue": "Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Wonjun Ko",
                "Wonsik Jung",
                "Eunjin Jeon",
                "Heung-Il Suk"
            ],
            "title": "A deep generative\u2013discriminative learning for multi-modal representation in imaging genetics",
            "venue": "IEEE Transactions on Medical Imaging,",
            "year": 2022
        },
        {
            "authors": [
                "Marina Manso Jimeno",
                "Keerthi Sravan Ravi",
                "Zhezhen Jin",
                "Dotun Oyekunle",
                "Godwin Ogbole",
                "Sairam Geethanath"
            ],
            "title": "Artifactid: Identifying artifacts in low-field mri of the brain using deep learning",
            "venue": "Magnetic resonance imaging,",
            "year": 2022
        },
        {
            "authors": [
                "Esra Zihni",
                "Bryony McGarry",
                "John Kelleher"
            ],
            "title": "Moving toward explainable decisions of artificial intelligence models for the prediction of functional outcomes of ischemic stroke patients",
            "venue": "Exon Publications,",
            "year": 2022
        },
        {
            "authors": [
                "Daniel J Delbarre",
                "Luis Santos",
                "Habib Ganjgahi",
                "Neil Horner",
                "Aaron McCoy",
                "Henrik Westerberg",
                "Dieter A H\u00e4ring",
                "Thomas E Nichols",
                "Ann-Marie Mallon"
            ],
            "title": "Application of a convolutional neural network to the quality control of mri defacing",
            "year": 2022
        },
        {
            "authors": [
                "Sasmitha Dasanayaka",
                "Vimuth Shantha",
                "Sanju Silva",
                "Dulani Meedeniya",
                "Thanuja Ambegoda"
            ],
            "title": "Interpretable machine learning for brain tumour analysis using mri and whole slide images",
            "venue": "Software Impacts,",
            "year": 2022
        },
        {
            "authors": [
                "Mohamed Shaban",
                "Amy W Amara"
            ],
            "title": "Resting-state electroencephalography based deep-learning for the detection of parkinson\u2019s disease",
            "venue": "Plos one,",
            "year": 2022
        },
        {
            "authors": [
                "Miguel Altuve",
                "Ana P\u00e9rez"
            ],
            "title": "Intracerebral hemorrhage detection on computed tomography images using a residual neural network",
            "venue": "Physica Medica,",
            "year": 2022
        },
        {
            "authors": [
                "Peixin Lu",
                "Lianting Hu",
                "Ning Zhang",
                "Huiying Liang",
                "Tao Tian",
                "Long Lu"
            ],
            "title": "A two-stage model for predicting mild cognitive impairment to alzheimer\u2019s disease conversion",
            "venue": "Frontiers in Aging Neuroscience,",
            "year": 2022
        },
        {
            "authors": [
                "Jingjing Gao",
                "Mingren Chen",
                "Die Xiao",
                "Yue Li",
                "Shunli Zhu",
                "Yanling Li",
                "Xin Dai",
                "Fengmei Lu",
                "Zhengning Wang",
                "Shimin Cai"
            ],
            "title": "Classification of major depressive disorder using an attentionguided unified deep convolutional neural network and individual structural covariance network",
            "venue": "Cerebral Cortex,",
            "year": 2022
        },
        {
            "authors": [
                "Wen Yu",
                "Baiying Lei",
                "Shuqiang Wang",
                "Yong Liu",
                "Zhiguang Feng",
                "Yong Hu",
                "Yanyan Shen",
                "Michael K Ng"
            ],
            "title": "Morphological feature visualization of alzheimer\u2019s disease via multidirectional perception gan",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Chuxin Huang",
                "Weidao Chen",
                "Baiyun Liu",
                "Ruize Yu",
                "Xiqian Chen",
                "Fei Tang",
                "Jun Liu",
                "Wei Lu"
            ],
            "title": "Transformer-based deep-learning algorithm for discriminating demyelinating diseases of the central nervous system with neuroimaging",
            "venue": "Frontiers in immunology,",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Guo",
                "Jiehuan Wang",
                "Xiaoqiang Wang",
                "Wenjing Liu",
                "Hao Yu",
                "Li Xu",
                "Hengyan Li",
                "Jiangfen Wu",
                "Mengxing Dong",
                "Weixiong Tan"
            ],
            "title": "Diagnosing autism spectrum disorder in children using conventional mri and apparent diffusion coefficient based deep learning algorithms",
            "venue": "European Radiology,",
            "year": 2022
        },
        {
            "authors": [
                "Neelesh Kumar",
                "Konstantinos P Michmizos"
            ],
            "title": "A neurophysiologically interpretable deep neural 89 network predicts complex movement components from brain activity",
            "venue": "Scientific reports,",
            "year": 2022
        },
        {
            "authors": [
                "Stefan Jonas",
                "Michael M\u00fcller",
                "Andrea O Rossetti",
                "Stephan R\u00fcegg",
                "Vincent Alvarez",
                "Kaspar Schindler",
                "Fr\u00e9d\u00e9ric Zubler"
            ],
            "title": "Diagnostic and prognostic eeg analysis of critically ill patients: A deep learning study",
            "venue": "NeuroImage: Clinical,",
            "year": 2022
        },
        {
            "authors": [
                "Gulnaz Ahmed",
                "Meng Joo Er",
                "Mian Muhammad Sadiq Fareed",
                "Shahid Zikria",
                "Saqib Mahmood",
                "Jiao He",
                "Muhammad Asad",
                "Syeda Fizzah Jilani",
                "Muhammad Aslam"
            ],
            "title": "Dad-net: Classification of alzheimer\u2019s disease using adasyn oversampling technique and optimized neural network. Molecules",
            "year": 2022
        },
        {
            "authors": [
                "Jiajun Fu",
                "Meili Lu",
                "Yifan Cao",
                "Zhaohua Guo",
                "Zicheng Gao"
            ],
            "title": "Grad-cam based visualization of 3d cnns in classifying fmri",
            "venue": "In Fourteenth International Conference on Digital Image Processing (ICDIP 2022),",
            "year": 2022
        },
        {
            "authors": [
                "Houliang Zhou",
                "Lifang He",
                "Yu Zhang",
                "Li Shen",
                "Brian Chen"
            ],
            "title": "Interpretable graph convolutional network of multi-modality brain imaging for alzheimer\u2019s disease diagnosis",
            "venue": "IEEE 19th International Symposium on Biomedical Imaging (ISBI),",
            "year": 2022
        },
        {
            "authors": [
                "Michele Lo Giudice",
                "Nadia Mammone",
                "Cosimo Ieracitano",
                "Maurizio Campolo",
                "Arcangelo Ranieri Bruna",
                "Valeria Tomaselli",
                "Francesco Carlo Morabito"
            ],
            "title": "Visual explanations of deep convolutional neural network for eye blinks detection in eeg-based bci applications",
            "venue": "In 2022 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2022
        },
        {
            "authors": [
                "Ankit Kurmi",
                "Shreya Biswas",
                "Shibaprasad Sen",
                "Aleksandr Sinitca",
                "Dmitrii Kaplun",
                "Ram Sarkar"
            ],
            "title": "An ensemble of cnn models for parkinson\u2019s disease detection using datscan",
            "venue": "images. Diagnostics,",
            "year": 2022
        },
        {
            "authors": [
                "Brady J Williamson",
                "David Wang",
                "Vivek Khandwala",
                "Jennifer Scheler",
                "Achala Vagal"
            ],
            "title": "Improving deep neural network interpretation for neuroimaging using multivariate modeling",
            "venue": "SN Computer Science,",
            "year": 2022
        },
        {
            "authors": [
                "Sarmad Maqsood",
                "Robertas Damavsevivcius",
                "Rytis Maskeli\u016bnas"
            ],
            "title": "Multi-modal brain tumor detection using deep neural network and multiclass svm",
            "venue": "Medicina,",
            "year": 2022
        },
        {
            "authors": [
                "Bochong Li",
                "Ryo Oka",
                "Ping Xuan",
                "Yuichiro Yoshimura",
                "Toshiya Nakaguchi"
            ],
            "title": "Semi-automatic multiparametric mr imaging classification using novel image input sequences and 3d convolutional neural networks",
            "year": 2022
        },
        {
            "authors": [
                "Tehseen Zia",
                "Shakeeb Murtaza",
                "Nauman Bashir",
                "David Windridge",
                "Zeeshan Nisar"
            ],
            "title": "Vant-gan: adversarial learning for discrepancy-based visual attribution in medical imaging",
            "venue": "Pattern Recognition Letters,",
            "year": 2022
        },
        {
            "authors": [
                "Zhiwei Qin",
                "Zhao Liu",
                "Qihao Guo",
                "Ping Zhu"
            ],
            "title": "3d convolutional neural networks with hybrid attention mechanism for early diagnosis of alzheimer\u2019s disease",
            "venue": "Biomedical Signal Processing and 90 Control,",
            "year": 2022
        },
        {
            "authors": [
                "Muhammad Wildan Oktavian",
                "Novanto Yudistira",
                "Achmad Ridok"
            ],
            "title": "Classification of alzheimer\u2019s disease using the convolutional neural network (cnn) with transfer learning and weighted loss",
            "venue": "arXiv preprint arXiv:2207.01584,",
            "year": 2022
        },
        {
            "authors": [
                "Patrick H Luckett",
                "Luigi Maccotta",
                "John J Lee",
                "Ki Yun Park",
                "Nico UF Dosenbach",
                "Beau M Ances",
                "Robert Edward Hogan",
                "Joshua S Shimony",
                "Eric C Leuthardt"
            ],
            "title": "Deep learning resting state functional magnetic resonance imaging lateralization of temporal lobe epilepsy",
            "venue": "Epilepsia,",
            "year": 2022
        },
        {
            "authors": [
                "Noriya Watanabe",
                "Kosuke Miyoshi",
                "Koji Jimura",
                "Daisuke Shimane",
                "Ruedeerat Keerativittayayut",
                "Kiyoshi Nakahara",
                "Masaki Takeda"
            ],
            "title": "Multi-modal deep neural decoding of visual object representation in humans",
            "venue": "bioRxiv,",
            "year": 2022
        },
        {
            "authors": [
                "Venkata Ramana Murthy Oruganti",
                "Dwarikanath Mahapatra",
                "Ramanathan Subramanian"
            ],
            "title": "Outlier-based autism detection using longitudinal structural mri",
            "venue": "arXiv preprint arXiv:2202.09988,",
            "year": 2022
        },
        {
            "authors": [
                "Melanie Garcia",
                "Clare Kelly"
            ],
            "title": "Towards 3d deep learning for neuropsychiatry: predicting autism diagnosis using an interpretable deep learning pipeline applied to minimally processed structural mri data",
            "venue": "medRxiv,",
            "year": 2022
        },
        {
            "authors": [
                "Sarah C Br\u00fcningk",
                "Felix Hensel",
                "Louis P Lukas",
                "Merel Kuijs",
                "Catherine R Jutzeler",
                "Bastian Rieck"
            ],
            "title": "Back to the basics with inclusion of clinical domain knowledge-a simple, scalable and effective model of alzheimer\u2019s disease classification",
            "venue": "In Machine Learning for Healthcare Conference,",
            "year": 2021
        },
        {
            "authors": [
                "Zhongxuan Wang",
                "Letian Wu",
                "Xiangcheng Ji"
            ],
            "title": "An interpretable deep learning system for automatic intracranial hemorrhage diagnosis with ct image",
            "venue": "In Proceedings of the 2021 International Conference on Bioinformatics and Intelligent Computing,",
            "year": 2021
        },
        {
            "authors": [
                "YL Wang",
                "ZJ Zhao",
                "SY Hu",
                "FL Chang"
            ],
            "title": "Clcu-net: Cross-level connected u-shaped network with selective feature aggregation attention module for brain tumor segmentation",
            "venue": "Computer Methods and Programs in Biomedicine,",
            "year": 2021
        },
        {
            "authors": [
                "Ethan Ocasio",
                "Tim Q Duong"
            ],
            "title": "Deep learning prediction of mild cognitive impairment conversion to alzheimer\u2019s disease at 3 years after diagnosis using longitudinal and whole-brain 3d mri",
            "venue": "PeerJ Computer Science,",
            "year": 2021
        },
        {
            "authors": [
                "Yu-Ching Ni",
                "Fan-Pin Tseng",
                "Ming-Chyi Pai",
                "Ing-Tsung Hsiao",
                "Kun-Ju Lin",
                "Zhi-Kun Lin",
                "Wen-Bin Lin",
                "Pai-Yi Chiu",
                "Guang-Uei Hung",
                "Chiung-Chih Chang"
            ],
            "title": "Detection of alzheimer\u2019s disease using ecd spect images by transfer learning from fdg pet",
            "venue": "Annals of Nuclear Medicine,",
            "year": 2021
        },
        {
            "authors": [
                "Emanuel A Azcona",
                "Pierre Besson",
                "Yunan Wu",
                "Ajay S Kurani",
                "S Kathleen Bandt",
                "Todd B Parrish",
                "Aggelos K Katsaggelos"
            ],
            "title": "Alzheimer\u2019s Disease Neuroimaging Initiative, et al. Analyzing brain morphology in alzheimer\u2019s disease using discriminative and generative spiral networks",
            "venue": "bioRxiv,",
            "year": 2021
        },
        {
            "authors": [
                "SV Mishinov",
                "AI Demyanchuk",
                "EV Pushkina",
                "VV Stupak",
                "TM Fatykhov",
                "NE Russkikh",
                "DN Shtokalo"
            ],
            "title": "Identification of enlargement of the ventricular system of the brain using machine learning",
            "venue": "Biomedical Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "Merel Kuijs",
                "Catherine R Jutzeler",
                "Bastian Rieck",
                "Sarah C"
            ],
            "title": "Br\u00fcningk. Interpretability aware model training to improve robustness against out-of-distribution magnetic resonance images in alzheimer\u2019s disease classification",
            "venue": "arXiv preprint arXiv:2111.08701,",
            "year": 2021
        },
        {
            "authors": [
                "Nalin Mathur",
                "Anubha Gupta",
                "Snehlata Jaswal",
                "Rohit Verma"
            ],
            "title": "Deep learning helps eeg signals predict different stages of visual processing in the human brain",
            "venue": "Biomedical Signal Processing and Control,",
            "year": 2021
        },
        {
            "authors": [
                "Jeong-Won Jeong",
                "Soumyanil Banerjee",
                "Min-Hee Lee",
                "Nolan O\u2019Hara",
                "Michael Behen",
                "Csaba Juhasz",
                "Ming Dong"
            ],
            "title": "Deep reasoning neural network analysis to predict language deficits from psychometrydriven dwi connectome of young children with persistent language concerns",
            "venue": "Human brain mapping,",
            "year": 2021
        },
        {
            "authors": [
                "Alex H Treacher",
                "Prabhat Garg",
                "Elizabeth Davenport",
                "Ryan Godwin",
                "Amy Proskovec",
                "Leonardo Guimaraes Bezerra",
                "Gowtham Murugesan",
                "Ben Wagner",
                "Christopher T Whitlow",
                "Joel D Stitzel"
            ],
            "title": "Megnet: automatic ica-based artifact removal for meg using spatiotemporal convolutional neural networks",
            "year": 2021
        },
        {
            "authors": [
                "Manu Raju",
                "M Thirupalani",
                "S Vidhyabharathi",
                "S Thilagavathi"
            ],
            "title": "Deep learning based multilevel classification of alzheimer\u2019s disease using mri scans",
            "venue": "In IOP Conference Series: Materials Science and Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "Muhammed Ilyas Patel",
                "Shrey Singla",
                "Razeem Ahmad Ali Mattathodi",
                "Sumit Sharma",
                "Deepam Gautam",
                "Srinivasa Rao Kundeti"
            ],
            "title": "Simulating realistic mri variations to improve deep learning model and visual explanations using gradcam",
            "venue": "IEEE International Conference on Cloud Computing in Emerging Markets (CCEM),",
            "year": 2021
        },
        {
            "authors": [
                "Matthew J Leming",
                "Simon Baron-Cohen",
                "John Suckling"
            ],
            "title": "Single-participant structural similarity matrices lead to greater accuracy in classification of participants than function in autism in mri",
            "venue": "Molecular Autism,",
            "year": 2021
        },
        {
            "authors": [
                "Domenico Messina",
                "Pasquale Borrelli",
                "Paolo Russo",
                "Marco Salvatore",
                "Marco Aiello"
            ],
            "title": "Voxel-wise feature selection method for cnn binary classification of neuroimaging data",
            "venue": "Frontiers in Neuroscience,",
            "year": 2021
        },
        {
            "authors": [
                "Haoyue Zhang",
                "Jennifer S Polson",
                "Kambiz Nael",
                "Noriko Salamon",
                "Bryan Yoo",
                "Suzie El-Saden",
                "Fabien Scalzo",
                "William Speier",
                "Corey W Arnold"
            ],
            "title": "Intra-domain task-adaptive transfer learning to determine acute ischemic stroke onset time",
            "venue": "Computerized Medical Imaging and Graphics,",
            "year": 2021
        },
        {
            "authors": [
                "MohammadReza Mohebbian",
                "Ekta Walia",
                "Mohammad Habibullah",
                "Shawn Stapleton",
                "Khan A Wahid"
            ],
            "title": "Classifying mri motion severity using a stacked ensemble approach",
            "venue": "Magnetic Resonance Imaging,",
            "year": 2021
        },
        {
            "authors": [
                "Esra Zihni",
                "BL McGarry",
                "John D Kelleher"
            ],
            "title": "An analysis of the interpretability of neural networks trained on magnetic resonance imaging for stroke outcome prediction",
            "venue": "In Proc. Intl. Soc. Mag. Reson. Med,",
            "year": 2021
        },
        {
            "authors": [
                "Juan Song",
                "Jian Zheng",
                "Ping Li",
                "Xiaoyuan Lu",
                "Guangming Zhu",
                "Peiyi Shen"
            ],
            "title": "An effective 92 multimodal image fusion method using mri and pet for alzheimer\u2019s disease diagnosis",
            "venue": "Frontiers in Digital Health,",
            "year": 2021
        },
        {
            "authors": [
                "Jingjing Gao",
                "Mingren Chen",
                "Yuanyuan Li",
                "Yachun Gao",
                "Yanling Li",
                "Shimin Cai",
                "Jiaojian Wang"
            ],
            "title": "Multisite autism spectrum disorder classification using convolutional neural network classifier and individual morphological brain networks",
            "venue": "Frontiers in Neuroscience,",
            "year": 2021
        },
        {
            "authors": [
                "Tobias Hepp",
                "Dominik Blum",
                "Karim Armanious",
                "Bernhard Schoelkopf",
                "Darko Stern",
                "Bin Yang",
                "Sergios Gatidis"
            ],
            "title": "Uncertainty estimation and explainability in deep learning-based age estimation of the human brain: Results from the german national cohort mri study",
            "venue": "Computerized Medical Imaging and Graphics,",
            "year": 2021
        },
        {
            "authors": [
                "Ilker Ozsahin",
                "Boran Sekeroglu",
                "Tracy Butler"
            ],
            "title": "Classification of alzheimer\u2019s disease by using tau pet images and deep convolutional neural networks",
            "venue": "Alzheimer\u2019s & Dementia,",
            "year": 2021
        },
        {
            "authors": [
                "Kwang Hyeon Kim",
                "Hae-Won Koo",
                "Byung-Jou Lee",
                "Sang-Won Yoon",
                "Moon-Jun Sohn"
            ],
            "title": "Cerebral hemorrhage detection and localization with medical imaging for cerebrovascular disease diagnosis and treatment using explainable deep learning",
            "venue": "Journal of the Korean Physical Society,",
            "year": 2021
        },
        {
            "authors": [
                "Qi Li",
                "Mary Qu Yang"
            ],
            "title": "Comparison of machine learning approaches for enhancing alzheimer\u2019s disease",
            "venue": "classification. PeerJ,",
            "year": 2021
        },
        {
            "authors": [
                "Koichiro Yasaka",
                "Koji Kamagata",
                "Takashi Ogawa",
                "Taku Hatano",
                "Haruka Takeshige-Amano",
                "Kotaro Ogaki",
                "Christina Andica",
                "Hiroyuki Akai",
                "Akira Kunimatsu",
                "Wataru Uchida"
            ],
            "title": "Parkinson\u2019s disease: Deep learning with a parameter-weighted structural connectome matrix for diagnosis and neural circuit disorder",
            "venue": "investigation. Neuroradiology,",
            "year": 2021
        },
        {
            "authors": [
                "Jie Mei",
                "C\u00e9cilia Tremblay",
                "Nikola Stikov",
                "Christian Desrosiers",
                "Johannes Frasnelli"
            ],
            "title": "Differentiation of parkinson\u2019s disease and non-parkinsonian olfactory dysfunction with structural mri data",
            "venue": "In Medical Imaging 2021: Computer-Aided Diagnosis,",
            "year": 2021
        },
        {
            "authors": [
                "Sreelakshmi Shaji",
                "Nagarajan Ganapathy",
                "Ramakrishnan Swaminathan"
            ],
            "title": "Classification of alzheimer condition using mr brain images and inception-residual network model",
            "venue": "Current Directions in Biomedical Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "Yunyan Zhang",
                "Daphne Hong",
                "Daniel McClement",
                "Olayinka Oladosu",
                "Glen Pridham",
                "Garth Slaney"
            ],
            "title": "Grad-cam helps interpret the deep learning models trained to classify multiple sclerosis types using clinical brain magnetic resonance imaging",
            "venue": "Journal of Neuroscience Methods,",
            "year": 2021
        },
        {
            "authors": [
                "Zhonghao Fan",
                "Johann Li",
                "Liang Zhang",
                "Guangming Zhu",
                "Ping Li",
                "Xiaoyuan Lu",
                "Peiyi Shen",
                "Syed Afaq Ali Shah",
                "Mohammed Bennamoun",
                "Tao Hua"
            ],
            "title": "U-net based analysis of mri for alzheimer\u2019s disease diagnosis",
            "venue": "Neural Computing and Applications,",
            "year": 2021
        },
        {
            "authors": [
                "Sarthak Pati",
                "Siddhesh P Thakur",
                "Megh Bhalerao",
                "Spyridon Thermos",
                "Ujjwal Baid",
                "Karol Gotkowski",
                "Camila Gonzalez",
                "Orhun Guley",
                "Ibrahim Ethem Hamamci",
                "Sezgin Er"
            ],
            "title": "Gandlf: A generally nuanced deep learning framework for scalable end-to-end clinical workflows in medical imaging",
            "venue": "arXiv preprint arXiv:2103.01006,",
            "year": 2021
        },
        {
            "authors": [
                "Laura Tomaz Da Silva",
                "Nathalia Bianchini Esper",
                "Duncan D Ruiz",
                "Felipe Meneguzzi",
                "Augusto 93 Buchweitz"
            ],
            "title": "Visual explanation for identification of the brain bases for developmental dyslexia on fmri data",
            "venue": "Frontiers in computational neuroscience,",
            "year": 2021
        },
        {
            "authors": [
                "Sreevalsan S Menon",
                "K Krishnamurthy"
            ],
            "title": "Multimodal ensemble deep learning to predict disruptive behavior disorders in children",
            "venue": "Frontiers in neuroinformatics,",
            "year": 2021
        },
        {
            "authors": [
                "Yebin Youn",
                "Mingeon Kim",
                "Jiho Kim",
                "Bongkeun Kang",
                "Ghootae Kim"
            ],
            "title": "Diagnosis and visualization of intracranial hemorrhage on computed tomography images using efficientnet-based model",
            "venue": "Journal of Biomedical Engineering Research,",
            "year": 2021
        },
        {
            "authors": [
                "Varun Ullanat",
                "Vinay Balamurali",
                "Ananya Rao"
            ],
            "title": "A novel residual 3-d convolutional network for alzheimer\u2019s disease diagnosis based on raw mri scans",
            "venue": "IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES),",
            "year": 2020
        },
        {
            "authors": [
                "Wonjun Ko",
                "Wonsik Jung",
                "Eunjin Jeon",
                "Ahmad Wisnu Mulyadi",
                "Heung-Il Suk"
            ],
            "title": "Engine: Enhancing neuroimaging and genetic information by neural embedding",
            "venue": "IEEE International Conference on Data Mining (ICDM),",
            "year": 2021
        },
        {
            "authors": [
                "Xiyue Wang",
                "Tao Shen",
                "Sen Yang",
                "Jun Lan",
                "Yanming Xu",
                "Minghui Wang",
                "Jing Zhang",
                "Xiao Han"
            ],
            "title": "A deep learning algorithm for automatic detection and classification of acute intracranial hemorrhages in head ct scans",
            "venue": "NeuroImage: Clinical,",
            "year": 2021
        },
        {
            "authors": [
                "Emre Dand\u0131l",
                "Semih Karaca"
            ],
            "title": "Detection of pseudo brain tumors via stacked lstm neural networks using mr spectroscopy signals",
            "venue": "Biocybernetics and Biomedical Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "Guozhen Hu",
                "Qinjian Zhang",
                "Zhi Yang",
                "Baobin Li"
            ],
            "title": "Accurate brain age prediction model for healthy children and adolescents using 3d-cnn and dimensional attention",
            "venue": "IEEE International Conference on Bioinformatics and Biomedicine (BIBM),",
            "year": 2021
        },
        {
            "authors": [
                "Evangeline Yee",
                "Da Ma",
                "Karteek Popuri",
                "Lei Wang",
                "Mirza Faisal Beg"
            ],
            "title": "Alzheimer\u2019s Disease Neuroimaging Initiative, et al. Construction of mri-based alzheimer\u2019s disease score based on efficient 3d convolutional neural network: comprehensive validation on 7,902 images from a multi-center dataset",
            "venue": "Journal of Alzheimer\u2019s Disease,",
            "year": 2021
        },
        {
            "authors": [
                "Morteza Esmaeili",
                "Riyas Vettukattil",
                "Hasan Banitalebi",
                "Nina R Krogh",
                "Jonn Terje Geitung"
            ],
            "title": "Explainable artificial intelligence for human-machine interaction in brain tumor localization",
            "venue": "Journal of Personalized Medicine,",
            "year": 2021
        },
        {
            "authors": [
                "Yunfei Liu",
                "Xian Mo",
                "Hao Yang",
                "Yan Liu",
                "Junran Zhang"
            ],
            "title": "A pilot study of diabetes mellitus classification from rs-fmri data using convolutional neural networks",
            "venue": "Mathematical Problems in Engineering,",
            "year": 2020
        },
        {
            "authors": [
                "Taiping Qu",
                "Yangming Yue",
                "Qirui Zhang",
                "Cheng Wang",
                "Zhiqiang Zhang",
                "Guangming Lu",
                "Wei Du",
                "Xiuli Li"
            ],
            "title": "Baenet: A brain age estimation network with 3d skipping and outlier constraint loss",
            "venue": "IEEE 17th International Symposium on Biomedical Imaging (ISBI),",
            "year": 2020
        },
        {
            "authors": [
                "Maxim Kan",
                "Ruslan Aliev",
                "Anna Rudenko",
                "Nikita Drobyshev",
                "Nikita Petrashen",
                "Ekaterina Kondrateva",
                "Maxim Sharaev",
                "Alexander Bernstein",
                "Evgeny Burnaev"
            ],
            "title": "Interpretable deep learning for pattern recognition in brain differences between men and women",
            "venue": "arXiv preprint arXiv:2006.15969,",
            "year": 2020
        },
        {
            "authors": [
                "Donglin Li",
                "Jiacan Xu",
                "Jianhui Wang",
                "Xiaoke Fang",
                "Ying Ji"
            ],
            "title": "A multi-scale fusion convolutional 94 neural network based on attention mechanism for the visualization analysis of eeg signals decoding",
            "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering,",
            "year": 2020
        },
        {
            "authors": [
                "Helena Rico Pereira",
                "Jos\u00e9 Manuel Fonseca",
                "Hugo Alexandre Ferreira"
            ],
            "title": "Combination of medical imaging and demographic data for parkinson\u2019s disease diagnosis",
            "venue": "In Doctoral Conference on Computing,",
            "year": 2020
        },
        {
            "authors": [
                "Sam Nguyen",
                "Brenda Ng",
                "Alan D Kaplan",
                "Priyadip Ray"
            ],
            "title": "Attend and decode: 4d fmri task state decoding using attention models",
            "venue": "In Machine Learning for Health,",
            "year": 2020
        },
        {
            "authors": [
                "Ulysse C\u00f4t\u00e9-Allard",
                "Evan Campbell",
                "Angkoon Phinyomark",
                "Franccois Laviolette",
                "Benoit Gosselin",
                "Erik Scheme"
            ],
            "title": "Interpreting deep learning features for myoelectric control: A comparison with handcrafted features",
            "venue": "Frontiers in Bioengineering and Biotechnology,",
            "year": 2020
        },
        {
            "authors": [
                "Byung-Hoon Kim",
                "Jong Chul Ye"
            ],
            "title": "Understanding graph isomorphism network for rs-fmri functional connectivity analysis",
            "venue": "Frontiers in neuroscience,",
            "year": 2020
        },
        {
            "authors": [
                "Fengkai Ke",
                "Seungjin Choi",
                "Young Ho Kang",
                "Keun-Ah Cheon",
                "Sang Wan Lee"
            ],
            "title": "Exploring the structural and strategic bases of autism spectrum disorders with deep learning",
            "venue": "Ieee Access,",
            "year": 2020
        },
        {
            "authors": [
                "Yukinori Akiyama",
                "Takeshi Mikami",
                "Nobuhiro Mikuni"
            ],
            "title": "Deep learning-based approach for the diagnosis of moyamoya disease",
            "venue": "Journal of Stroke and Cerebrovascular Diseases,",
            "year": 2020
        },
        {
            "authors": [
                "Hidehisa Nishi",
                "Naoya Oishi",
                "Akira Ishii",
                "Isao Ono",
                "Takenori Ogura",
                "Tadashi Sunohara",
                "Hideo Chihara",
                "Ryu Fukumitsu",
                "Masakazu Okawa",
                "Norikazu Yamana"
            ],
            "title": "Deep learning\u2013derived high-level neuroimaging features predict clinical outcomes for large vessel",
            "venue": "occlusion. Stroke,",
            "year": 2020
        },
        {
            "authors": [
                "Laura Tomaz Da Silva",
                "Nathalia Bianchini Esper",
                "Duncan D Ruiz",
                "Felipe Meneguzzi",
                "Augusto Buchweitz"
            ],
            "title": "Visual explanation for identification of the brain bases for dyslexia on fmri data",
            "year": 2007
        },
        {
            "authors": [
                "Z\u00fclfikar Aslan",
                "Mehmet Ak\u0131n"
            ],
            "title": "Automatic detection of schizophrenia by applying deep learning over spectrogram images of eeg signals",
            "year": 2020
        },
        {
            "authors": [
                "Maryana Alegro",
                "Yuheng Chen",
                "Dulce Ovando",
                "Helmut Heinser",
                "Rana Eser",
                "Daniela Ushizima",
                "Duygu Tosun",
                "Lea T Grinberg"
            ],
            "title": "Deep learning for alzheimer\u2019s disease: Mapping large-scale histological tau protein for neuroimaging biomarker",
            "venue": "validation. bioRxiv,",
            "year": 2020
        },
        {
            "authors": [
                "P\u00e1l Vakli",
                "Regina J De\u00e1k-Meszl\u00e9nyi",
                "Tibor Auer",
                "Zolt\u00e1n Vidny\u00e1nszky"
            ],
            "title": "Predicting body mass index from structural mri brain images using a deep convolutional neural network",
            "venue": "Frontiers in Neuroinformatics,",
            "year": 2020
        },
        {
            "authors": [
                "Obioma Pelka",
                "Christoph M Friedrich",
                "Felix Nensa",
                "Christoph M\u00f6nninghoff",
                "Louise Bloch",
                "Karl- Heinz J\u00f6ckel",
                "Sara Schramm",
                "Sarah Sanchez Hoffmann",
                "Angela Winkler",
                "Christian Weimar"
            ],
            "title": "Sociodemographic data and apoe-\u03b54 augmentation for mri-based detection of amnestic mild cognitive impairment using deep learning systems",
            "venue": "PloS one,",
            "year": 2020
        },
        {
            "authors": [
                "Emanuel A Azcona",
                "Pierre Besson",
                "Yunan Wu",
                "Arjun Punjabi",
                "Adam Martersteck",
                "Amil Dravid",
                "Todd B Parrish",
                "S Kathleen Bandt",
                "Aggelos K Katsaggelos"
            ],
            "title": "Interpretation of brain morphology 95 in association to alzheimer\u2019s disease dementia classification using graph convolutional networks on triangulated meshes",
            "venue": "In International Workshop on Shape in Medical Imaging,",
            "year": 2020
        },
        {
            "authors": [
                "Evangeline Yee",
                "Karteek Popuri",
                "Da Ma",
                "Lei Wang",
                "Mirza Faisal Beg"
            ],
            "title": "Structural-mri-based alzheimer\u2019s disease dementia score using 3d convolutional neural networks to achieve accurate early disease prediction: Neuroimaging/optimal neuroimaging measures for early detection",
            "venue": "In 2020 Alzheimer\u2019s Association International Conference. ALZ,",
            "year": 2020
        },
        {
            "authors": [
                "Parth Natekar",
                "Avinash Kori",
                "Ganapathy Krishnamurthi"
            ],
            "title": "Demystifying brain tumor segmentation networks: interpretability and uncertainty analysis",
            "venue": "Frontiers in computational neuroscience,",
            "year": 2020
        },
        {
            "authors": [
                "Xinyang Feng",
                "Zachary C Lipton",
                "Jie Yang",
                "Scott A Small",
                "Frank A Provenzano"
            ],
            "title": "Alzheimer\u2019s Disease Neuroimaging Initiative, Frontotemporal Lobar Degeneration Neuroimaging Initiative, et al. Estimating brain age based on a uniform healthy population with deep learning and structural magnetic resonance imaging",
            "venue": "Neurobiology of aging,",
            "year": 2020
        },
        {
            "authors": [
                "Michelle Tang",
                "Pulkit Kumar",
                "Hao Chen",
                "Abhinav Shrivastava"
            ],
            "title": "Deep multimodal learning for the diagnosis of autism spectrum disorder",
            "venue": "Journal of Imaging,",
            "year": 2020
        },
        {
            "authors": [
                "Ming Chen",
                "Hailong Li",
                "Jinghua Wang",
                "Weihong Yuan",
                "Mekbib Altaye",
                "Nehal A Parikh",
                "Lili He"
            ],
            "title": "Early prediction of cognitive deficit in very preterm infants using brain structural connectome with transfer learning enhanced deep convolutional neural networks",
            "venue": "Frontiers in neuroscience,",
            "year": 2020
        },
        {
            "authors": [
                "Yunan Wu",
                "Pierre Besson",
                "Emanuel A Azcona",
                "S Kathleen Bandt",
                "Todd B Parrish",
                "Hans C Breiter",
                "Aggelos K Katsaggelos"
            ],
            "title": "Novel age-dependent cortico-subcortical morphologic interactions predict fluid intelligence: A multi-cohort geometric deep learning study",
            "venue": "bioRxiv,",
            "year": 2020
        },
        {
            "authors": [
                "Johnny Wang",
                "Maria J Knol",
                "Aleksei Tiulpin",
                "Florian Dubost",
                "Marleen de Bruijne",
                "Meike W Vernooij",
                "Hieab HH Adams",
                "M Arfan Ikram",
                "Wiro J Niessen",
                "Gennady V Roshchupkin"
            ],
            "title": "Grey matter age prediction as a biomarker for risk of dementia: A population-based study",
            "venue": "BioRxiv,",
            "year": 2019
        },
        {
            "authors": [
                "Tomomichi Iizuka",
                "Makoto Fukasawa",
                "Masashi Kameyama"
            ],
            "title": "Deep-learning-based imagingclassification identified cingulate island sign in dementia with lewy bodies",
            "venue": "Scientific reports,",
            "year": 2019
        },
        {
            "authors": [
                "Maryana Alegro",
                "Yuheng Chen",
                "Dulce Ovando",
                "Helmut Heinsen",
                "Rana Eser",
                "Duygu Tosun",
                "Lea T Grinberg"
            ],
            "title": "Deep learning based large-scale histological tau protein mapping for neuroimaging biomarker validation in alzheimer\u2019s disease",
            "venue": "bioRxiv,",
            "year": 2019
        },
        {
            "authors": [
                "Tomomichi Iizuka",
                "Makoto Fukasawa",
                "Masashi Kameyama"
            ],
            "title": "P1-348: Deep learning-based imaging classification identified cingulate island sign in dementia with lewy bodies",
            "venue": "Alzheimer\u2019s & Dementia,",
            "year": 2019
        },
        {
            "authors": [
                "Sayeri Lala"
            ],
            "title": "Convolutional neural networks for image reconstruction and image quality assessment of 2D fetal brain MRI",
            "venue": "PhD thesis, Massachusetts Institute of Technology,",
            "year": 2019
        },
        {
            "authors": [
                "Kai Gao",
                "Hui Shen",
                "Yadong Liu",
                "Lingli Zeng",
                "Dewen Hu"
            ],
            "title": "Dense-cam: Visualize the gender of brains with mri images",
            "venue": "In 2019 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2019
        },
        {
            "authors": [
                "He Chen",
                "Yan Song",
                "Xiaoli Li"
            ],
            "title": "Use of deep learning to detect personalized spatial-frequency abnormalities in eegs of children with adhd",
            "venue": "Journal of neural engineering,",
            "year": 2019
        },
        {
            "authors": [
                "Alexey Demyanchuk",
                "Ekaterina Pushkina",
                "Nikolay Russkikh",
                "Dmitry Shtokalo",
                "Sergey Mishinov"
            ],
            "title": "Hydrocephalus verification on brain magnetic resonance images with deep convolutional neural networks and\" transfer learning\" technique",
            "year": 1909
        },
        {
            "authors": [
                "Xinyang Feng",
                "Zachary C Lipton",
                "Jie Yang",
                "Scott A Small",
                "Frank A Provenzano"
            ],
            "title": "Estimating brain age based on a healthy population with deep learning and structural mri",
            "year": 1907
        },
        {
            "authors": [
                "Camilo Bermudez Noguera",
                "Shunxing Bao",
                "Kalen J Petersen",
                "Alexander M Lopez",
                "Jacqueline A Reid",
                "Andrew J Plassard",
                "David H Zald",
                "Daniel O Claassen",
                "Benoit M Dawant",
                "Bennett A Landman"
            ],
            "title": "Using deep learning for a diffusion-based segmentation of the dentate nucleus and its benefits over atlas-based methods",
            "venue": "Journal of Medical Imaging,",
            "year": 2019
        },
        {
            "authors": [
                "Johnny Wang",
                "Maria J Knol",
                "Aleksei Tiulpin",
                "Florian Dubost",
                "Marleen de Bruijne",
                "Meike W Vernooij",
                "Hieab HH Adams",
                "M Arfan Ikram",
                "Wiro J Niessen",
                "Gennady V Roshchupkin"
            ],
            "title": "Predicting brain age as a biomarker for risk of dementia",
            "venue": "Alzheimer\u2019s & Dementia,",
            "year": 2019
        },
        {
            "authors": [
                "Camilo Bermudez",
                "Andrew J Plassard",
                "Shikha Chaganti",
                "Yuankai Huo",
                "Katherine S Aboud",
                "Laurie E Cutting",
                "Susan M Resnick",
                "Bennett A Landman"
            ],
            "title": "Anatomical context improves deep learning on the brain age estimation task",
            "venue": "Magnetic Resonance Imaging,",
            "year": 2019
        },
        {
            "authors": [
                "Adam Hilbert",
                "Lucas Alexandre Ramos",
                "Hendrikus JA van Os",
                "S\u00edlvia D Olabarriaga",
                "Manon L Tolhuisen",
                "Marieke JH Wermer",
                "Renan Sales Barros",
                "Irene van der Schaaf",
                "Diederik Dippel",
                "YBWEM Roos"
            ],
            "title": "Data-efficient deep learning of radiological image data for outcome prediction after endovascular treatment of patients with acute ischemic stroke",
            "venue": "Computers in biology and medicine,",
            "year": 2019
        },
        {
            "authors": [
                "Hyunkwang Lee",
                "Sehyo Yune",
                "Mohammad Mansouri",
                "Myeongchan Kim",
                "Shahein H Tajmir",
                "Claude E Guerrier",
                "Sarah A Ebert",
                "Stuart R Pomerantz",
                "Javier M Romero",
                "Shahmir Kamalian"
            ],
            "title": "An explainable deep-learning algorithm for the detection of acute intracranial haemorrhage from small datasets",
            "venue": "Nature biomedical engineering,",
            "year": 2019
        },
        {
            "authors": [
                "Dmitry Petrov",
                "Boris A Gutman",
                "Egor Kuznetsov",
                "Christopher RK Ching",
                "Kathryn Alpert",
                "Artemis Zavaliangos-Petropulu",
                "Dmitry Isaev",
                "Jessica A Turner",
                "Theo GM van Erp",
                "Lei Wang"
            ],
            "title": "Deep learning for quality control of subcortical brain 3d shape models",
            "venue": "In International Workshop on Shape in Medical Imaging,",
            "year": 2018
        },
        {
            "authors": [
                "S\u00e9rgio Pereira",
                "Raphael Meier",
                "Victor Alves",
                "Mauricio Reyes",
                "Carlos A Silva"
            ],
            "title": "Automatic 97 brain tumor grading from mri data using convolutional neural networks and quality assessment",
            "year": 2018
        },
        {
            "authors": [
                "Vladimir Golkov",
                "Phillip Swazinna",
                "Marcel M Schmitt",
                "Qadeer A Khan",
                "Chantal MW Tax",
                "Marat Serahlazau",
                "Francesco Pasa",
                "Franz Pfeiffer",
                "Geert Jan Biessels",
                "Alexander Leemans"
            ],
            "title": "q-space deep learning for alzheimer\u2019s disease diagnosis: Global prediction and weakly-supervised localization",
            "venue": "In Proc. 27th Annu. Meeting ISMRM,",
            "year": 2018
        },
        {
            "authors": [
                "Fernando Andreotti",
                "Huy Phan",
                "Maarten De Vos"
            ],
            "title": "Visualising convolutional neural network decisions in automatic sleep scoring",
            "venue": "In CEUR Workshop Proceedings,",
            "year": 2018
        },
        {
            "authors": [
                "Xinyang Feng",
                "Jie Yang",
                "Zachary C Lipton",
                "Scott A Small",
                "Frank A Provenzano"
            ],
            "title": "Alzheimer\u2019s Disease Neuroimaging Initiative, et al. Deep learning on mri affirms the prominence of the hippocampal formation in alzheimer\u2019s disease",
            "venue": "classification. bioRxiv,",
            "year": 2018
        },
        {
            "authors": [
                "Marina Pominova",
                "Alexey Artemov",
                "Maksim Sharaev",
                "Ekaterina Kondrateva",
                "Alexander Bernstein",
                "Evgeny Burnaev"
            ],
            "title": "Voxelwise 3d convolutional and recurrent neural networks for epilepsy and depression diagnostics from structural and functional mri data",
            "venue": "IEEE International Conference on Data Mining Workshops (ICDMW),",
            "year": 2018
        },
        {
            "authors": [
                "Prabhat Garg",
                "Elizabeth Davenport",
                "Gowtham Murugesan",
                "Ben Wagner",
                "Christopher Whitlow",
                "Joseph Maldjian",
                "Albert Montillo"
            ],
            "title": "Automatic 1d convolutional neural network-based detection of artifacts in meg acquired without electrooculography or electrocardiography",
            "venue": "International Workshop on Pattern Recognition in Neuroimaging (PRNI),",
            "year": 2017
        },
        {
            "authors": [
                "Kaustubh Supekar",
                "Srikanth Ryali",
                "Rui Yuan",
                "Devinder Kumar",
                "Carlo de Los Angeles",
                "Vinod Menon"
            ],
            "title": "Robust, generalizable, and interpretable artificial intelligence\u2013derived brain fingerprints of autism and social communication symptom severity",
            "venue": "Biological Psychiatry,",
            "year": 2022
        },
        {
            "authors": [
                "Tanu Wadhera",
                "Mufti Mahmud"
            ],
            "title": "Computing hierarchical complexity of the brain from electroencephalogram signals: A graph convolutional network-based approach",
            "venue": "In 2022 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2022
        },
        {
            "authors": [
                "Noah Lewis",
                "Robyn Miller",
                "Harshvardhan Gazula",
                "Vince Calhoun"
            ],
            "title": "Fine temporal brain network structure modularizes and localizes differently in men and women: Insights from a novel explainability framework",
            "venue": "bioRxiv,",
            "year": 2022
        },
        {
            "authors": [
                "Marla Narazani",
                "Ignacio Sarasua",
                "Sebastian P\u00f6lsterl",
                "Aldana Lizarraga",
                "Igor Yakushev",
                "Christian Wachinger"
            ],
            "title": "Is a pet all you need? a multi-modal study for alzheimer\u2019s disease using 3d cnns",
            "venue": "In International Conference on Medical Image Computing and Computer-Assisted Intervention,",
            "year": 2022
        },
        {
            "authors": [
                "Hannah Spitzer",
                "Mathilde Ripart",
                "Kirstie Whitaker",
                "Felice D\u2019Arco",
                "Kshitij Mankad",
                "Andrew A Chen",
                "Antonio Napolitano",
                "Luca De Palma",
                "Alessandro De Benedictis",
                "Stephen Foldes"
            ],
            "title": "Interpretable surface-based detection of focal cortical dysplasias: a multi-centre epilepsy lesion detection study",
            "venue": "Brain, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Ignacio Sarasua",
                "Sebastian P\u00f6lsterl",
                "Christian Wachinger"
            ],
            "title": "Hippocampal representations for deep learning on alzheimer\u2019s disease",
            "venue": "Scientific reports,",
            "year": 2022
        },
        {
            "authors": [
                "Adam Richie-Halford",
                "Matthew Cieslak",
                "Lei Ai",
                "Sendy Caffarra",
                "Sydney Covitz",
                "Alexandre R Franco",
                "Iliana I Karipidis",
                "John Kruper",
                "Michael Milham",
                "B\u00e1rbara Avelar-Pereira"
            ],
            "title": "An open, analysisready, and quality controlled resource for pediatric brain white-matter research",
            "venue": "bioRxiv,",
            "year": 2022
        },
        {
            "authors": [
                "Jangho Kim",
                "Junhyeong Lee",
                "Seunggeun Lee"
            ],
            "title": "Investigation of genetic variants and causal biomarkers associated with brain",
            "venue": "aging. medRxiv,",
            "year": 2022
        },
        {
            "authors": [
                "Matteo Ferrante",
                "Tommaso Boccato",
                "Nicola Toschi"
            ],
            "title": "Bayesnetcnn: incorporating uncertainty in neural networks for image-based classification tasks",
            "venue": "arXiv preprint arXiv:2209.13096,",
            "year": 2022
        },
        {
            "authors": [
                "Adam Richie-Halford",
                "Matthew Cieslak",
                "Lei Ai",
                "Sendy Caffarra",
                "Sydney Covitz",
                "Alexandre R Franco",
                "Iliana I Karipidis",
                "John Kruper",
                "Michael Milham",
                "B\u00e1rbara Avelar-Pereira"
            ],
            "title": "An analysis-ready and quality controlled resource for pediatric brain white-matter research",
            "venue": "Scientific Data,",
            "year": 2022
        },
        {
            "authors": [
                "Sikun Lin",
                "Shuyun Tang",
                "Scott Grafton",
                "Ambuj Singh"
            ],
            "title": "Deep representations for time-varying brain",
            "venue": "datasets. arXiv preprint arXiv:2205.11648,",
            "year": 2022
        },
        {
            "authors": [
                "Hongchao Jiang",
                "Chunyan Miao"
            ],
            "title": "Pre-training 3d convolutional neural networks for prodromal alzheimer\u2019s disease classification",
            "venue": "In 2022 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2022
        },
        {
            "authors": [
                "Xinke Shen",
                "Xianggen Liu",
                "Xin Hu",
                "Dan Zhang",
                "Sen Song"
            ],
            "title": "Contrastive learning of subjectinvariant eeg representations for cross-subject emotion recognition",
            "venue": "IEEE Transactions on Affective Computing,",
            "year": 2022
        },
        {
            "authors": [
                "Jean Pablo Vieira de Mello",
                "Thiago M Paix\u00e3o",
                "Rodrigo Berriel",
                "Mauricio Reyes",
                "Claudine Badue",
                "Alberto F De Souza",
                "Thiago Oliveira-Santos"
            ],
            "title": "Deep learning-based type identification of volumetric mri sequences",
            "venue": "In 2020 25th International Conference on Pattern Recognition (ICPR),",
            "year": 2021
        },
        {
            "authors": [
                "Laura de la Fuente",
                "Federico Zamberlan",
                "Hern\u00e1n Bocaccio",
                "Morten Kringelbach",
                "Gustavo Deco",
                "Yonatan Sanz Perl",
                "Enzo Tagliazucchi"
            ],
            "title": "Temporal irreversibility of neural dynamics as a signature of consciousness",
            "venue": "bioRxiv,",
            "year": 2021
        },
        {
            "authors": [
                "Bowen Xin",
                "Jing Huang",
                "Yun Zhou",
                "Jie Lu",
                "Xiuying Wang"
            ],
            "title": "Interpretation on deep multimodal fusion for diagnostic classification",
            "venue": "In 2021 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2021
        },
        {
            "authors": [
                "Qiao Pan",
                "Ke Ding",
                "Dehua Chen"
            ],
            "title": "Multi-classification prediction of alzheimer\u2019s disease based on fusing multi-modal features",
            "venue": "IEEE International Conference on Data Mining (ICDM),",
            "year": 2021
        },
        {
            "authors": [
                "Yogatheesan Varatharajah",
                "Krishnakant Saboo",
                "Ravishankar Iyer",
                "Scott Przybelski",
                "Christopher Schwarz",
                "Ronald Petersen",
                "Clifford Jack",
                "Prashanthi Vemuri"
            ],
            "title": "A joint model for predicting structural and functional brain health in elderly individuals",
            "venue": "IEEE International Conference on Bioinformatics and Biomedicine (BIBM),",
            "year": 2019
        },
        {
            "authors": [
                "Chin-Fu Liu",
                "Shreyas Padhy",
                "Sandhya Ramachandran",
                "Victor X Wang",
                "Andrew Efimov",
                "Alonso Bernal",
                "Linyuan Shi",
                "Marc Vaillant",
                "J Tilak Ratnanather",
                "Andreia V Faria"
            ],
            "title": "Using deep siamese neural networks for detection of brain asymmetries associated with alzheimer\u2019s disease and mild cognitive impairment",
            "venue": "Magnetic resonance imaging,",
            "year": 2019
        },
        {
            "authors": [
                "David A Wood",
                "Sina Kafiabadi",
                "Ayisha Al Busaidi",
                "Emily Guilhem",
                "Antanas Montvila",
                "Jeremy Lynch",
                "Matthew Townend",
                "Siddharth Agarwal",
                "Asif Mazumder",
                "Gareth J Barker"
            ],
            "title": "Deep learning models for triaging hospital head mri examinations",
            "venue": "Medical Image Analysis,",
            "year": 2022
        },
        {
            "authors": [
                "Juan Manuel Mayor-Torres",
                "Sara Medina-DeVilliers",
                "Tessa Clarkson",
                "Matthew D Lerner",
                "Giuseppe Riccardi"
            ],
            "title": "Evaluation of interpretability for deep learning algorithms in eeg emotion recognition: A case study in autism",
            "venue": "arXiv preprint arXiv:2111.13208,",
            "year": 2021
        },
        {
            "authors": [
                "Christian Tinauer",
                "Stefan Heber",
                "Lukas Pirpamer",
                "Anna Damulina",
                "Reinhold Schmidt",
                "Rudolf Stollberger",
                "Stefan Ropele",
                "Christian Langkammer"
            ],
            "title": "Interpretable brain disease classification and relevance-guided deep learning",
            "year": 2021
        },
        {
            "authors": [
                "Daehyun Cho",
                "Christian Wallraven"
            ],
            "title": "Do pre-processing and augmentation help explainability? a multi-seed analysis for brain age estimation",
            "venue": "In International Workshop on Interpretability of Machine Intelligence in Medical Image Computing,",
            "year": 2022
        },
        {
            "authors": [
                "Jingjing Hu",
                "Zhao Qing",
                "Renyuan Liu",
                "Xin Zhang",
                "Pin Lv",
                "Maoxue Wang",
                "Yang Wang",
                "Kelei He",
                "Yang Gao",
                "Bing Zhang"
            ],
            "title": "Deep learning-based classification and voxel-based visualization of frontotemporal dementia and alzheimer\u2019s disease",
            "venue": "Frontiers in Neuroscience,",
            "year": 2021
        },
        {
            "authors": [
                "Esther E Bron",
                "Stefan Klein",
                "Janne M Papma",
                "Lize C Jiskoot",
                "Vikram Venkatraghavan",
                "Jara Linders",
                "Pauline Aalten",
                "Peter Paul De Deyn",
                "Geert Jan Biessels",
                "Jurgen AHR Claassen"
            ],
            "title": "Cross-cohort generalizability of deep and conventional machine learning for mri-based diagnosis and prediction of alzheimer\u2019s disease",
            "venue": "NeuroImage: Clinical,",
            "year": 2021
        },
        {
            "authors": [
                "Federica Cruciani",
                "Lorenza Brusini",
                "Mauro Zucchelli",
                "Gustavo Retuci Pinheiro",
                "Francesco Setti",
                "Ilaria Boscolo Galazzo",
                "Rachid Deriche",
                "Leticia Rittner",
                "Massimiliano Calabrese",
                "Gloria Menegaz"
            ],
            "title": "Explainable 3d-cnn for multiple sclerosis patients stratification",
            "venue": "In International Conference on Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Yu Zhang",
                "Lo\u00efc Tetrel",
                "Bertrand Thirion",
                "Pierre Bellec"
            ],
            "title": "Functional annotation of human cognitive states using deep graph convolution",
            "year": 2021
        },
        {
            "authors": [
                "Wen Shi",
                "Guohui Yan",
                "Yamin Li",
                "Haotian Li",
                "Tingting Liu",
                "Cong Sun",
                "Guangbin Wang",
                "Yi Zhang",
                "Yu Zou",
                "Dan Wu"
            ],
            "title": "Fetal brain age estimation and anomaly detection using attention-based deep ensembles with uncertainty",
            "year": 2020
        },
        {
            "authors": [
                "Xiaoxiao Wang",
                "Xiao Liang",
                "Zhoufan Jiang",
                "Benedictor A Nguchu",
                "Yawen Zhou",
                "Yanming Wang",
                "Huijuan Wang",
                "Yu Li",
                "Yuying Zhu",
                "Feng Wu"
            ],
            "title": "Decoding and mapping task states of the human brain via deep learning",
            "venue": "Human brain mapping,",
            "year": 2020
        },
        {
            "authors": [
                "Guilherme Folego",
                "Marina Weiler",
                "Raphael F Casseb",
                "Ramon Pires",
                "Anderson Rocha"
            ],
            "title": "Alzheimer\u2019s disease detection through whole-brain 3d-cnn mri",
            "venue": "Frontiers in bioengineering and biotechnology,",
            "year": 2020
        },
        {
            "authors": [
                "Sukrit Gupta",
                "Marcus Lim",
                "Jagath C Rajapakse"
            ],
            "title": "Decoding task specific and task general functional architectures of the brain",
            "venue": "Human Brain Mapping,",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Taylor",
                "Jonathan Shock",
                "Deshendran Moodley",
                "Jonathan Ipser",
                "Matthias Treder"
            ],
            "title": "Brain structural saliency over the ages",
            "venue": "arXiv preprint arXiv:2202.11690,",
            "year": 2022
        },
        {
            "authors": [
                "Ravikiran Mane",
                "Effie Chew",
                "Karen Chua",
                "Kai Keng Ang",
                "Neethu Robinson",
                "A Prasad Vinod",
                "Seong- Whan Lee",
                "Cuntai Guan"
            ],
            "title": "Fbcnet: A multi-view convolutional neural network for brain-computer interface",
            "venue": "arXiv preprint arXiv:2104.01233,",
            "year": 2021
        },
        {
            "authors": [
                "Tom\u00e1vs Pohl",
                "Marek Jakab",
                "Wanda Benesova"
            ],
            "title": "Interpretability of deep neural networks used for the diagnosis of alzheimer\u2019s disease",
            "venue": "International Journal of Imaging Systems and Technology,",
            "year": 2022
        },
        {
            "authors": [
                "AI Korda",
                "E Ventouras",
                "P Asvestas",
                "Maida Toumaian",
                "GK Matsopoulos",
                "N Smyrnis"
            ],
            "title": "Convolutional neural network propagation on electroencephalographic scalograms for detection of schizophrenia",
            "venue": "Clinical Neurophysiology,",
            "year": 2022
        },
        {
            "authors": [
                "Joshua Vic Chen",
                "Gunvant Chaudhari",
                "Christopher P Hess",
                "Orit A Glenn",
                "Leo P Sugrue",
                "Andreas M Rauschecker",
                "Yi Li"
            ],
            "title": "Deep learning to predict neonatal and infant brain age from myelination on brain",
            "venue": "mri scans. Radiology,",
            "year": 2022
        },
        {
            "authors": [
                "Juan Manuel Mayor Torres",
                "Tessa Clarkson",
                "Kathryn M Hauschild",
                "Christian C Luhmann",
                "Matthew D Lerner",
                "Giuseppe Riccardi"
            ],
            "title": "Facial emotions are accurately encoded in the neural signal of those with autism spectrum disorder: A deep learning approach",
            "venue": "Biological Psychiatry: Cognitive Neuroscience and Neuroimaging,",
            "year": 2022
        },
        {
            "authors": [
                "K Muthamil Sudar",
                "P Nagaraj",
                "S Nithisaa",
                "R Aishwarya",
                "M Aakash",
                "S Ishwarya Lakshmi"
            ],
            "title": "Alzheimer\u2019s disease analysis using explainable artificial intelligence (xai)",
            "venue": "In 2022 International Conference on Sustainable Computing and Data Communication Systems (ICSCDS),",
            "year": 2022
        },
        {
            "authors": [
                "Dohyun Kim",
                "Jungtae Lee",
                "Jangsup Moon",
                "Taesup Moon"
            ],
            "title": "Interpretable deep learning-based hippocampal sclerosis classification",
            "venue": "Epilepsia Open,",
            "year": 2022
        },
        {
            "authors": [
                "Mahmood Nazari",
                "Andreas Kluge",
                "Ivayla Apostolova",
                "Susanne Klutmann",
                "Sharok Kimiaei",
                "Michael Schroeder",
                "Ralph Buchert"
            ],
            "title": "Explainable ai to improve acceptance of convolutional neural networks for automatic classification of dopamine transporter spect in the diagnosis of clinically uncertain parkinsonian syndromes",
            "venue": "European journal of nuclear medicine and molecular imaging,",
            "year": 2022
        },
        {
            "authors": [
                "Charles A Ellis",
                "Robyn L Miller",
                "Vince D Calhoun"
            ],
            "title": "An approach for estimating explanation uncertainty in fmri dfnc classification",
            "venue": "bioRxiv,",
            "year": 2022
        },
        {
            "authors": [
                "Bonian Lu"
            ],
            "title": "Transfer learning approaches in classification of mental disorders using neuroimaging",
            "year": 2022
        },
        {
            "authors": [
                "Charles A Ellis",
                "Robyn L Miler",
                "Vince D Calhoun"
            ],
            "title": "Towards greater neuroimaging classification transparency via the integration of explainability methods and confidence estimation approaches",
            "venue": "bioRxiv,",
            "year": 2022
        },
        {
            "authors": [
                "Malte Klingenberg",
                "Didem Stark",
                "Fabian Eitel",
                "Mohamad Habes",
                "Kerstin Ritter"
            ],
            "title": "Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI, et al. Higher performance for women than men in mri-based alzheimer\u2019s disease",
            "year": 2022
        },
        {
            "authors": [
                "Alison Deatsch",
                "Matej Perovnik",
                "Mauro Nam\u00edas",
                "Maja Trovst",
                "Robert Jeraj"
            ],
            "title": "Alzheimer\u2019s Disease Neuroimaging Initiative, et al. Development of a deep learning network for alzheimer\u2019s disease classification with evaluation of imaging modality and longitudinal data",
            "venue": "Physics in Medicine & Biology,",
            "year": 2022
        },
        {
            "authors": [
                "Charles A Ellis",
                "Rongen Zhang",
                "Vince D Calhoun",
                "Darwin A Carbajal",
                "Robyn L Miller",
                "May D Wang"
            ],
            "title": "A gradient-based approach for explaining multimodal deep learning classifiers",
            "venue": "IEEE 21st International Conference on Bioinformatics and Bioengineering (BIBE),",
            "year": 2021
        },
        {
            "authors": [
                "Mahmood Nazari",
                "Andreas Kluge",
                "Ivayla Apostolova",
                "Susanne Klutmann",
                "Sharok Kimiaei",
                "Michael Schroeder",
                "Ralph Buchert"
            ],
            "title": "Data-driven identification of diagnostically useful extrastriatal signal in dopamine transporter spect using explainable ai",
            "venue": "Scientific Reports,",
            "year": 2021
        },
        {
            "authors": [
                "Ji-Seon Bang",
                "Min-Ho Lee",
                "Siamac Fazli",
                "Cuntai Guan",
                "Seong-Whan Lee"
            ],
            "title": "Spatio-spectral feature representation for motor imagery classification using convolutional neural networks",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Charles A Ellis",
                "Darwin A Carbajal",
                "Rongen Zhang",
                "Robyn L Miller",
                "Vince D Calhoun",
                "May D Wang"
            ],
            "title": "An explainable deep learning approach for multimodal electrophysiology classification",
            "venue": "bioRxiv,",
            "year": 2021
        },
        {
            "authors": [
                "Adrien Raison",
                "Pascal Bourdon",
                "Christophe Habas",
                "David Helbert"
            ],
            "title": "Explicability in resting-state fmri for gender classification",
            "venue": "Sixth International Conference on Advances in Biomedical Engineering (ICABME),",
            "year": 2021
        },
        {
            "authors": [
                "Federica Cruciani",
                "Lorenza Brusini",
                "Mauro Zucchelli",
                "G Retuci Pinheiro",
                "Francesco Setti",
                "I Boscolo Galazzo",
                "Rachid Deriche",
                "Leticia Rittner",
                "Massimiliano Calabrese",
                "Gloria Menegaz"
            ],
            "title": "Interpretable deep learning as a means for decrypting disease signature in multiple sclerosis",
            "venue": "Journal of Neural Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "Eunjin Jeon",
                "Wonjun Ko",
                "Jee Seok Yoon",
                "Heung-Il Suk"
            ],
            "title": "Mutual information-driven subjectinvariant and class-relevant deep representation learning in bci",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Fabian Eitel",
                "Jan Philipp Albrecht",
                "Martin Weygandt",
                "Friedemann Paul",
                "Kerstin Ritter"
            ],
            "title": "Patch individual filter layers in cnns to harness the spatial homogeneity of neuroimaging data",
            "venue": "Scientific reports,",
            "year": 2021
        },
        {
            "authors": [
                "Sunghee Dong",
                "Yan Jin",
                "SuJin Bak",
                "Bumchul Yoon",
                "Jichai Jeong"
            ],
            "title": "Explainable convolutional neural network to investigate age-related changes in multi-order functional",
            "venue": "connectivity. Electronics,",
            "year": 2021
        },
        {
            "authors": [
                "Jiaojiao Wang"
            ],
            "title": "Understanding 3D Convolutional Neural Networks in Alzheimer\u2019s Disease Classification",
            "venue": "PhD thesis, University of Georgia,",
            "year": 2020
        },
        {
            "authors": [
                "Md Motiur Rahman Sagar",
                "Martin Dyrba"
            ],
            "title": "Learning shape features and abstractions in 3d convolutional neural networks for detecting alzheimer\u2019s disease",
            "venue": "arXiv preprint arXiv:2009.05023,",
            "year": 2020
        },
        {
            "authors": [
                "Pamela K Douglas",
                "Farzad Vasheghani Farahani"
            ],
            "title": "On the similarity of deep learning representations across didactic and adversarial examples",
            "venue": "arXiv preprint arXiv:2002.06816,",
            "year": 2020
        },
        {
            "authors": [
                "Janzaib Masood"
            ],
            "title": "Interpretable Deep Learning for Diagnosis of Human Brain Disorders using Neuroimaging",
            "venue": "PhD thesis, Auburn University,",
            "year": 2020
        },
        {
            "authors": [
                "Taeho Jo",
                "Kwangsik Nho",
                "Shannon L Risacher",
                "Andrew J Saykin"
            ],
            "title": "Deep learning detection of informative features in tau pet for alzheimer\u2019s disease classification",
            "venue": "BMC bioinformatics,",
            "year": 2020
        },
        {
            "authors": [
                "Martin Dyrba",
                "Arjun H Pallath",
                "Stefan J Teipel"
            ],
            "title": "Validation of convolutional neural network relevance maps for revealing patterns of alzheimer\u2019s disease in mri scans: Neuroimaging/optimal neuroimaging measures for early detection",
            "venue": "Alzheimer\u2019s & Dementia,",
            "year": 2020
        },
        {
            "authors": [
                "Yijian Pang",
                "Fulei Zhang"
            ],
            "title": "Clinical brain imaging multi-class classification and interpretation\u2013cnn model and layer-wise relevance propagation analysis",
            "year": 2019
        },
        {
            "authors": [
                "Shilpa Dang",
                "Santanu Chaudhury"
            ],
            "title": "Novel relative relevance score for estimating brain connectivity from fmri data using an explainable neural network approach",
            "venue": "Journal of Neuroscience Methods,",
            "year": 2019
        },
        {
            "authors": [
                "Jin Xie",
                "Longfei Wang",
                "Paula Webster",
                "Yang Yao",
                "Jiayao Sun",
                "Shuo Wang",
                "Huihui Zhou"
            ],
            "title": "A two-stream end-to-end deep learning network for recognizing atypical visual attention in autism spectrum disorder",
            "venue": "arXiv preprint arXiv:1911.11393,",
            "year": 2019
        },
        {
            "authors": [
                "Danilo Pena",
                "Arko Barman",
                "Jessika Suescun",
                "Xiaoqian Jiang",
                "Mya C Schiess",
                "Luca"
            ],
            "title": "Giancardo, and Alzheimer\u2019s Disease Neuroimaging Initiative. Quantifying neurodegenerative progression with deepsymnet, an end-to-end data-driven approach",
            "venue": "Frontiers in neuroscience,",
            "year": 2019
        },
        {
            "authors": [
                "Athanasios Gotsopoulos",
                "Heini Saarim\u00e4ki",
                "Enrico Glerean",
                "Iiro P J\u00e4\u00e4skel\u00e4inen",
                "Mikko Sams",
                "Lauri Nummenmaa",
                "Jouko Lampinen"
            ],
            "title": "Reproducibility of importance extraction methods in neural network based fmri classification",
            "year": 2018
        },
        {
            "authors": [
                "Xiaoqian Wang",
                "Weidong Cai",
                "Dinggang Shen",
                "Heng Huang"
            ],
            "title": "Temporal correlation structure learning for mci conversion prediction",
            "venue": "In International Conference on Medical Image Computing and Computer-Assisted Intervention,",
            "year": 2018
        },
        {
            "authors": [
                "Irene Sturm",
                "Sebastian Lapuschkin",
                "Wojciech Samek",
                "Klaus-Robert M\u00fcller"
            ],
            "title": "Interpretable deep neural networks for single-trial eeg classification",
            "venue": "Journal of neuroscience methods,",
            "year": 2016
        },
        {
            "authors": [
                "Sergio E S\u00e1nchez-Hern\u00e1ndez",
                "Ricardo A Salido-Ruiz",
                "Sulema Torres-Ramos",
                "Israel Rom\u00e1n- God\u00ednez"
            ],
            "title": "Evaluation of feature selection methods for classification of epileptic seizure",
            "venue": "eeg signals. Sensors,",
            "year": 2022
        },
        {
            "authors": [
                "Angela Lombardi",
                "Domenico Diacono",
                "Nicola Amoroso",
                "Przemys\u0142aw Biecek",
                "Alfonso Monaco",
                "Loredana 103 Bellantuono",
                "Ester Pantaleo",
                "Giancarlo Logroscino",
                "Roberto De Blasi",
                "Sabina Tangaro"
            ],
            "title": "A robust framework to investigate the reliability and stability of explainable artificial intelligence markers of mild cognitive impairment and alzheimer\u2019s disease",
            "venue": "Brain informatics,",
            "year": 2022
        },
        {
            "authors": [
                "Angela Lombardi",
                "Domenico Diacono",
                "Nicola Amoroso",
                "Alfonso Monaco",
                "Sabina Tangaro",
                "Roberto Bellotti"
            ],
            "title": "Embedding explainable artificial intelligence in clinical decision support systems: The brain age prediction case study",
            "venue": "In Recent Advances in AI-enabled Automated Medical Diagnosis,",
            "year": 2022
        },
        {
            "authors": [
                "Animesh Kumar Paul",
                "Anushree Bose",
                "Sunil Vasu Kalmady",
                "Venkataram Shivakumar",
                "Vanteemar S Sreeraj",
                "Rujuta Parlikar",
                "Janardhanan C Narayanaswamy",
                "Serdar M Dursun",
                "Andrew J Greenshaw",
                "Russell Greiner"
            ],
            "title": "Superior temporal gyrus functional connectivity predicts transcranial direct current stimulation response in schizophrenia: A machine learning study",
            "venue": "Frontiers in psychiatry,",
            "year": 2022
        },
        {
            "authors": [
                "Matthias Eder",
                "Emanuel Moser",
                "Andreas Holzinger",
                "Claire Jean-Quartier",
                "Fleur Jeanquartier"
            ],
            "title": "Interpretable machine learning with brain image and survival",
            "venue": "data. BioMedInformatics,",
            "year": 2022
        },
        {
            "authors": [
                "Robiul Islam",
                "Andrey V Andreev",
                "Natalia N Shusharina",
                "Alexander E Hramov"
            ],
            "title": "Explainable machine learning methods for classification of brain states during visual perception",
            "year": 2022
        },
        {
            "authors": [
                "Mengjie Hu",
                "Yang Yu",
                "Fangping He",
                "Yujie Su",
                "Kan Zhang",
                "Xiaoyan Liu",
                "Ping Liu",
                "Ying Liu",
                "Guoping Peng",
                "Benyan Luo"
            ],
            "title": "Classification and interpretability of mild cognitive impairment based on resting-state functional magnetic resonance and ensemble learning",
            "venue": "Computational intelligence and neuroscience,",
            "year": 2022
        },
        {
            "authors": [
                "Peishan Dai",
                "Tong Xiong",
                "Xiaoyan Zhou",
                "Yilin Ou",
                "Yang Li",
                "Xiaoyan Kui",
                "Zailiang Chen",
                "Beiji Zou",
                "Weihui Li",
                "Zhongchao Huang"
            ],
            "title": "The alterations of brain functional connectivity networks in major depressive disorder detected by machine learning through multisite rs-fmri data",
            "venue": "Behavioural Brain Research,",
            "year": 2022
        },
        {
            "authors": [
                "Andreas Brink-Kjaer",
                "Eileen B Leary",
                "Haoqi Sun",
                "M Brandon Westover",
                "Katie L Stone",
                "Paul E Peppard",
                "Nancy E Lane",
                "Peggy M Cawthon",
                "Susan Redline",
                "Poul Jennum"
            ],
            "title": "Age estimation from sleep studies using deep learning predicts life expectancy",
            "venue": "NPJ digital medicine,",
            "year": 2022
        },
        {
            "authors": [
                "Michael Allwright",
                "Hamish Mundell",
                "Greg Sutherland",
                "Paul Austin",
                "Boris Guennewig"
            ],
            "title": "Machine learning analysis of the uk biobank reveals igf-1 and inflammatory biomarkers predict parkinson\u2019s disease",
            "year": 2022
        },
        {
            "authors": [
                "Chen Ran",
                "Yanwu Yang",
                "Chenfei Ye",
                "Haiyan Lv",
                "Ting Ma"
            ],
            "title": "Brain age vector: A measure of brain aging with enhanced neurodegenerative disorder specificity",
            "venue": "Human brain mapping,",
            "year": 2022
        },
        {
            "authors": [
                "Sreelakshmi Shaji",
                "Rohini Palanisamy",
                "Ramakrishnan Swaminathan"
            ],
            "title": "Explainable optimized lightgbm based differentiation of mild cognitive impairment using mr radiomic features",
            "venue": "Studies in Health Technology and Informatics,",
            "year": 2022
        },
        {
            "authors": [
                "Zhi Xu",
                "Chenjie Gao",
                "Tingting Tan",
                "Wenhao Jiang",
                "Tianyu Wang",
                "Zimu Chen",
                "Tian Shen",
                "Lei Chen",
                "104 Haiping Tang",
                "Wenji Chen"
            ],
            "title": "Combined htr1a/1b methylation and human functional connectome to recognize patients with mdd",
            "venue": "Psychiatry Research,",
            "year": 2022
        },
        {
            "authors": [
                "Byung-Hoon Kim",
                "Min-Kyeong Kim",
                "Hye-Jeong Jo",
                "Jae-Jin Kim"
            ],
            "title": "Predicting social anxiety in young adults with machine learning of resting-state brain functional radiomic features",
            "venue": "Scientific reports,",
            "year": 2022
        },
        {
            "authors": [
                "Louise Bloch",
                "Christoph M Friedrich"
            ],
            "title": "Machine learning workflow to explain black-box models for early alzheimer\u2019s disease classification evaluated for multiple datasets",
            "venue": "arXiv preprint arXiv:2205.05907,",
            "year": 2022
        },
        {
            "authors": [
                "Minji Bang",
                "Yae Won Park",
                "Jihwan Eom",
                "Sung Soo Ahn",
                "Jinna Kim",
                "Seung-Koo Lee",
                "Sang-Hyuk Lee"
            ],
            "title": "An interpretable radiomics model for the diagnosis of panic disorder with or without agoraphobia using magnetic resonance imaging",
            "venue": "Journal of Affective Disorders,",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Kannampallil",
                "Ruixuan Dai",
                "Nan Lv",
                "Lan Xiao",
                "Chenyang Lu",
                "Olusola A Ajilore",
                "Mark B Snowden",
                "Elizabeth M Venditti",
                "Leanne M Williams",
                "Emily A Kringle"
            ],
            "title": "Cross-trial prediction of depression remission using problem-solving therapy: A machine learning approach",
            "venue": "Journal of affective disorders,",
            "year": 2022
        },
        {
            "authors": [
                "Huize Pang",
                "Ziyang Yu",
                "Hongmei Yu",
                "Miao Chang",
                "Jibin Cao",
                "Yingmei Li",
                "Miaoran Guo",
                "Yu Liu",
                "Kaiqiang Cao",
                "Guoguang Fan"
            ],
            "title": "Multimodal striatal neuromarkers in distinguishing parkinsonian variant of multiple system atrophy from idiopathic parkinson\u2019s disease",
            "venue": "CNS Neuroscience & Therapeutics,",
            "year": 2022
        },
        {
            "authors": [
                "Jaime G\u00f3mez-Ram\u00edrez",
                "Miguel A Fern\u00e1ndez-Bl\u00e1zquez",
                "Javier J Gonz\u00e1lez-Rosa"
            ],
            "title": "Prediction of chronological age in healthy elderly subjects with machine learning from mri brain segmentation and cortical parcellation",
            "venue": "Brain Sciences,",
            "year": 2022
        },
        {
            "authors": [
                "Shangran Qiu",
                "Matthew I Miller",
                "Prajakta S Joshi",
                "Joyce C Lee",
                "Chonghua Xue",
                "Yunruo Ni",
                "Yuwei Wang",
                "De Anda-Duran",
                "Phillip H Hwang",
                "Justin A Cramer"
            ],
            "title": "Multimodal deep learning for alzheimer\u2019s disease dementia assessment",
            "venue": "Nature communications,",
            "year": 2022
        },
        {
            "authors": [
                "Ziv Ben-Zion",
                "Ofir Shany",
                "Roee Admon",
                "Nimrod Jackob Keynan",
                "Netanell Avisdris",
                "Shira Reznik Balter",
                "Arieh Y Shalev",
                "Israel Liberzon",
                "Talma Hendler"
            ],
            "title": "Neural responsivity to reward versus punishment shortly after trauma predicts long-term development of posttraumatic stress symptoms",
            "venue": "Biological psychiatry: cognitive neuroscience and neuroimaging,",
            "year": 2022
        },
        {
            "authors": [
                "Yingying Xie",
                "Hao Ding",
                "Xiaotong Du",
                "Chao Chai",
                "Xiaotong Wei",
                "Jie Sun",
                "Chuanjun Zhuo",
                "Lina Wang",
                "Jie Li",
                "Hongjun Tian"
            ],
            "title": "Morphometric integrated classification index: A multisite model-based, interpretable, shareable and evolvable biomarker for schizophrenia",
            "venue": "Schizophrenia Bulletin,",
            "year": 2022
        },
        {
            "authors": [
                "Riccardo Scheda",
                "Stefano Diciotti"
            ],
            "title": "Explanations of machine learning models in repeated nested cross-validation: An application in age prediction using brain complexity features",
            "venue": "Applied Sciences,",
            "year": 2022
        },
        {
            "authors": [
                "Ngoc-Huynh Ho",
                "Hyung-Jeong Yang",
                "Jahae Kim",
                "Duy-Phuong Dao",
                "Hyuk-Ro Park",
                "Sudarshan Pant"
            ],
            "title": "Predicting progression of alzheimer\u2019s disease using forward-to-backward bi-directional network with integrative imputation",
            "venue": "Neural Networks,",
            "year": 2022
        },
        {
            "authors": [
                "Constantina A Treaba",
                "Allegra Conti",
                "Eric C Klawiter",
                "Valeria T Barletta",
                "Elena Herranz",
                "Ambica Mehndiratta",
                "Andrew W Russo",
                "Jacob A Sloane",
                "Revere P Kinkel",
                "Nicola Toschi"
            ],
            "title": "Cortical and phase rim lesions on 7 t mri as markers of multiple sclerosis disease progression",
            "venue": "Brain communications,",
            "year": 2021
        },
        {
            "authors": [
                "Minji Bang",
                "Jihwan Eom",
                "Chansik An",
                "Sooyon Kim",
                "Yae Won Park",
                "Sung Soo Ahn",
                "Jinna Kim",
                "Seung-Koo Lee",
                "Sang-Hyuk Lee"
            ],
            "title": "An interpretable multiparametric radiomics model for the diagnosis of schizophrenia using magnetic resonance imaging of the corpus callosum",
            "venue": "Translational psychiatry,",
            "year": 2021
        },
        {
            "authors": [
                "Jelena Bozek",
                "Ivan Kesedzic",
                "Leonard Novosel",
                "Tomislav Bozek"
            ],
            "title": "Classification and feature analysis of the human connectome project dataset for differentiating between males and females. Automatika: vcasopis za automatiku, mjerenje, elektroniku, ravcunarstvo i komunikacije",
            "year": 2021
        },
        {
            "authors": [
                "Bum Joon Kim",
                "Su-Kyeong Jang",
                "Yong-Hwan Kim",
                "Eun-Jae Lee",
                "Jun Young Chang",
                "Sun U Kwon",
                "Jong S Kim",
                "Dong-Wha Kang"
            ],
            "title": "Diagnosis of acute central dizziness with simple clinical information using machine learning",
            "venue": "Frontiers in Neurology,",
            "year": 2021
        },
        {
            "authors": [
                "Jing Huang",
                "Bowen Xin",
                "Xiuying Wang",
                "Zhigang Qi",
                "Huiqing Dong",
                "Kuncheng Li",
                "Yun Zhou",
                "Jie Lu"
            ],
            "title": "Multi-parametric mri phenotype with trustworthy machine learning for differentiating cns demyelinating diseases",
            "venue": "Journal of translational medicine,",
            "year": 2021
        },
        {
            "authors": [
                "Johannes Breitenbach",
                "Dominik Raab",
                "Eric Fezer",
                "Daniel Sauter",
                "Hermann Baumgartl",
                "Ricardo Buettner"
            ],
            "title": "Automatic diagnosis of intellectual and developmental disorder using machine learning based on resting-state eeg recordings",
            "venue": "17th International Conference on Wireless and Mobile Computing, Networking and Communications (WiMob),",
            "year": 2021
        },
        {
            "authors": [
                "Louise Bloch",
                "Christoph M Friedrich"
            ],
            "title": "Data analysis with shapley values for automatic subject selection in alzheimer\u2019s disease data sets using interpretable machine learning",
            "venue": "Alzheimer\u2019s Research & Therapy,",
            "year": 2021
        },
        {
            "authors": [
                "Samuel O Danso",
                "Zhanhang Zeng",
                "Graciela Muniz-Terrera",
                "Craig W Ritchie"
            ],
            "title": "Developing an explainable machine learning-based personalised dementia risk prediction model: A transfer learning approach with ensemble learning algorithms",
            "venue": "Frontiers in big Data,",
            "year": 2021
        },
        {
            "authors": [
                "Louise Bloch",
                "Christoph M Friedrich"
            ],
            "title": "Comparison of automated volume extraction with freesurfer and fastsurfer for early alzheimer\u2019s disease detection with machine learning",
            "venue": "IEEE 34th International Symposium on Computer-Based Medical Systems (CBMS),",
            "year": 2021
        },
        {
            "authors": [
                "Sajila D Wickramaratne",
                "Md Shaad Mahmud"
            ],
            "title": "A deep learning based ternary task classification system using gramian angular summation field in fnirs neuroimaging data",
            "venue": "IEEE International Conference on E-health Networking, Application & Services (HEALTHCOM),",
            "year": 2020
        },
        {
            "authors": [
                "Ahmed Salih",
                "Ilaria Boscolo Galazzo",
                "Zahra Raisi-Estabragh",
                "Steffen E Petersen",
                "Polyxeni Gkontra",
                "Karim Lekadir",
                "Gloria Menegaz",
                "Petia Radeva"
            ],
            "title": "A new scheme for the assessment of the robustness of explainable methods applied to brain age estimation",
            "venue": "IEEE 34th International Symposium on Computer-Based Medical Systems (CBMS),",
            "year": 2021
        },
        {
            "authors": [
                "Natalie M Sommer",
                "Burak Kakillioglu",
                "Trevor Grant",
                "Senem Velipasalar",
                "Leanne Hirshfield"
            ],
            "title": "Classification of fnirs finger tapping data with multi-labeling and deep learning",
            "venue": "IEEE Sensors Journal,",
            "year": 2021
        },
        {
            "authors": [
                "Xiaoxiao Li",
                "Yuan Zhou",
                "Nicha C Dvornek",
                "Yufeng Gu",
                "Pamela Ventola",
                "James S Duncan"
            ],
            "title": "Efficient shapley explanation for features importance estimation under uncertainty",
            "venue": "In International Conference on Medical Image Computing and Computer-Assisted Intervention,",
            "year": 2020
        },
        {
            "authors": [
                "Mohammad SE Sendi",
                "Ji Ye Chun",
                "Vince D Calhoun"
            ],
            "title": "Visualizing functional network connectivity difference between middle adult and older subjects using an explainable machine-learning method",
            "venue": "IEEE 20th International Conference on Bioinformatics and Bioengineering (BIBE),",
            "year": 2020
        },
        {
            "authors": [
                "CH Suh",
                "WH Shim",
                "SJ Kim",
                "JH Roh",
                "J-H Lee",
                "M-J Kim",
                "S Park",
                "W Jung",
                "J Sung",
                "G-H Jahng"
            ],
            "title": "Development and validation of a deep learning\u2013based automatic brain segmentation and classification algorithm for alzheimer disease using 3d t1-weighted volumetric images",
            "venue": "American Journal of Neuroradiology,",
            "year": 2020
        },
        {
            "authors": [
                "David Ahmedt-Aristizabal",
                "Tharindu Fernando",
                "Simon Denman",
                "Jonathan Edward Robinson",
                "Sridha Sridharan",
                "Patrick J Johnston",
                "Kristin R Laurens",
                "Clinton Fookes"
            ],
            "title": "Identification of children at risk of schizophrenia via deep learning and eeg responses",
            "venue": "IEEE journal of biomedical and health informatics,",
            "year": 2020
        },
        {
            "authors": [
                "Camillo Saueressig",
                "Adam Berkley",
                "Elliot Kang",
                "Reshma Munbodh",
                "Ritambhara Singh"
            ],
            "title": "Exploring graph-based neural networks for automatic brain tumor segmentation",
            "venue": "In International Symposium: From Data to Models and Back,",
            "year": 2020
        },
        {
            "authors": [
                "Obada Al Zoubi",
                "Masaya Misaki",
                "Aki Tsuchiyagaito",
                "Vadim Zotev",
                "Evan White",
                "Martin Paulus",
                "Jerzy Bodurka",
                "Tulsa 1000 Investigators"
            ],
            "title": "Predicting sex from resting-state fmri across multiple independent acquired datasets",
            "venue": "bioRxiv,",
            "year": 2020
        },
        {
            "authors": [
                "Tiago Azevedo",
                "Luca Passamonti",
                "Pietro Li\u00f3",
                "Nicola Toschi"
            ],
            "title": "A machine learning tool for interpreting differences in cognition using brain features",
            "venue": "In IFIP International Conference on Artificial Intelligence Applications and Innovations,",
            "year": 2019
        },
        {
            "authors": [
                "Vincent Peter C Magboo",
                "Ma Magboo",
                "A Sheila"
            ],
            "title": "Important features associated with depression prediction and explainable ai",
            "venue": "In International Conference on Well-Being in the Information Society,",
            "year": 2022
        },
        {
            "authors": [
                "Sobhana Jahan",
                "Kazi Abu Taher",
                "M Shamim Kaiser",
                "Mufti Mahmud",
                "Md Sazzadur Rahman",
                "ASM Sanwar Hosen",
                "In-Ho Ra"
            ],
            "title": "Explainable ai-based alzheimer\u2019s prediction and management using multimodal data",
            "year": 2022
        },
        {
            "authors": [
                "Leonie Lampe",
                "Sebastian Niehaus",
                "Hans-J\u00fcrgen Huppertz",
                "Alberto Merola",
                "Janis Reinelt",
                "Karsten Mueller",
                "Sarah Anderl-Straub",
                "Klaus Fassbender",
                "Klaus Fliessbach",
                "Holger Jahn"
            ],
            "title": "Comparative analysis of machine learning algorithms for multi-syndrome classification of neurodegenerative syndromes",
            "venue": "Alzheimer\u2019s Research & Therapy,",
            "year": 2022
        },
        {
            "authors": [
                "Hamza Ahmed Shad",
                "Quazi Ashikur Rahman",
                "Nashita Binte Asad",
                "Atif Zawad Bakshi",
                "SM Faiaz Mursalin",
                "Md Tanzim Reza",
                "Mohammad Zavid Parvez"
            ],
            "title": "Exploring alzheimer\u2019s disease prediction 107 with xai in various neural network models",
            "venue": "IEEE Region",
            "year": 2021
        },
        {
            "authors": [
                "Mariia Sidulova",
                "Nina Nehme",
                "Chung Huyk Park"
            ],
            "title": "Towards explainable image analysis for alzheimer\u2019s disease and mild cognitive impairment diagnosis",
            "venue": "IEEE Applied Imagery Pattern Recognition Workshop (AIPR),",
            "year": 2021
        },
        {
            "authors": [
                "Jinlong Hu",
                "Lijie Cao",
                "Tenghui Li",
                "Bin Liao",
                "Shoubin Dong",
                "Ping Li"
            ],
            "title": "Interpretable learning approaches in resting-state functional connectivity analysis: the case of autism spectrum disorder",
            "venue": "Computational and Mathematical Methods in Medicine,",
            "year": 2020
        },
        {
            "authors": [
                "Lucas Alexandre Ramos",
                "Wessel E van der Steen",
                "Renan Sales Barros",
                "Charles BLM Majoie",
                "Rene van den Berg",
                "Dagmar Verbaan",
                "W Peter Vandertop",
                "I Jsbrand Andreas Jan Zijlstra",
                "AH Zwinderman",
                "Gustav J Strijkers"
            ],
            "title": "Machine learning improves prediction of delayed cerebral ischemia in patients with subarachnoid hemorrhage",
            "venue": "Journal of neurointerventional surgery,",
            "year": 2019
        },
        {
            "authors": [
                "Alex Novaes Santana",
                "Ignacio Cifre",
                "Charles Novaes De Santana",
                "Pedro Montoya"
            ],
            "title": "Using deep learning and resting-state fmri to classify chronic pain conditions",
            "venue": "Frontiers in neuroscience,",
            "year": 2019
        },
        {
            "authors": [
                "S\u00e9rgio Pereira",
                "Raphael Meier",
                "Richard McKinley",
                "Roland Wiest",
                "Victor Alves",
                "Carlos A Silva",
                "Mauricio Reyes"
            ],
            "title": "Enhancing interpretability of automatically extracted machine learning features: application to a rbm-random forest system on brain lesion segmentation",
            "venue": "Medical image analysis,",
            "year": 2018
        },
        {
            "authors": [
                "Julius Adebayo",
                "Michael Muelly",
                "Harold Abelson",
                "Been Kim"
            ],
            "title": "Post hoc explanations may be ineffective for detecting unknown spurious correlation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Sebastian Lapuschkin",
                "Stephan W\u00e4ldchen",
                "Alexander Binder",
                "Gr\u00e9goire Montavon",
                "Wojciech Samek",
                "Klaus-Robert M\u00fcller"
            ],
            "title": "Unmasking clever hans predictors and assessing what machines really learn",
            "venue": "Nature communications,",
            "year": 2019
        },
        {
            "authors": [
                "Amirata Ghorbani",
                "Abubakar Abid",
                "James Zou"
            ],
            "title": "Interpretation of neural networks is fragile",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Aaron Fisher",
                "Cynthia Rudin",
                "Francesca Dominici"
            ],
            "title": "All models are wrong, but many are useful: Learning a variable\u2019s importance by studying an entire class of prediction models simultaneously",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2019
        },
        {
            "authors": [
                "Javier Castro",
                "Daniel G\u00f3mez",
                "Juan Tejada"
            ],
            "title": "Polynomial calculation of the shapley value based on sampling",
            "venue": "Computers & Operations Research,",
            "year": 2009
        },
        {
            "authors": [
                "Pieter-Jan Kindermans",
                "Kristof T Sch\u00fctt",
                "Maximilian Alber",
                "Klaus-Robert M\u00fcller",
                "Dumitru Erhan",
                "Been Kim",
                "Sven D\u00e4hne"
            ],
            "title": "Learning how to explain neural networks: Patternnet and patternattribution",
            "venue": "arXiv preprint arXiv:1705.05598,",
            "year": 2017
        },
        {
            "authors": [
                "Yongchan Kwon",
                "James Zou"
            ],
            "title": "Weightedshap: analyzing and improving shapley based feature attributions",
            "venue": "arXiv preprint arXiv:2209.13429,",
            "year": 2022
        },
        {
            "authors": [
                "Maithra Raghu",
                "Chiyuan Zhang",
                "Jon Kleinberg",
                "Samy Bengio"
            ],
            "title": "Transfusion: Understanding 108 transfer learning for medical imaging",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Deep learning (DL) models have been popular due to their ability to learn directly from the raw data in an end-to-end paradigm, alleviating the concern of a separate error-prone feature extraction phase. Recent DL-based neuroimaging studies have also witnessed a noticeable performance advancement over traditional machine learning algorithms. But the challenges of deep learning models still exist because of the lack of transparency in these models for their successful deployment in real-world applications. In recent years, Explainable AI (XAI) has undergone a surge of developments mainly to get intuitions of how the models reached the decisions, which is essential for safety-critical domains such as healthcare, finance, and law enforcement agencies. While the interpretability domain is advancing noticeably, researchers are still unclear about what aspect of model learning a post hoc method reveals and how to validate its reliability. This paper comprehensively reviews interpretable deep learning models in the neuroimaging domain. Firstly, we summarize the current status of interpretability resources in general, focusing on the progression of methods, associated challenges, and opinions. Secondly, we discuss how multiple recent neuroimaging studies leveraged model interpretability to capture anatomical and functional brain alterations most relevant to model predictions. Finally, we discuss the limitations of the current practices and offer some valuable insights and guidance on how we can steer our future research directions to make deep learning models substantially interpretable and thus advance scientific understanding of brain disorders.\nKeywords\u2014 deep learning, interpretability, neuroimaging, brain dynamics, psychiatric disorders\n\u2217mahfuz.gsu@gmail.com \u2020vcalhoun@gsu.edu \u2021s.m.plis@gmail.com\nar X\niv :2\n30 7.\n09 61\n5v 1\n[ cs\n.L G\n] 1\n4 Ju\nContents"
        },
        {
            "heading": "1 Introduction 4",
            "text": ""
        },
        {
            "heading": "2 Related Work 5",
            "text": ""
        },
        {
            "heading": "3 Organization of the Paper 5",
            "text": ""
        },
        {
            "heading": "4 Philosophy of Scientific Explanations 6",
            "text": ""
        },
        {
            "heading": "5 What and Why Is Model Interpretability? 6",
            "text": "5.1 How to Achieve Interpretability? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 5.2 Global vs. Local Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 5.3 Looking at the Model Interpretability Problem . . . . . . . . . . . . . . . . . . . . . . . . . 8 5.4 Important Terminology in Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8"
        },
        {
            "heading": "6 Taxonomy of Interpretability Methods 10",
            "text": "6.1 Visualization Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n6.1.1 Gradient Backpropagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 6.1.2 Modified Backpropagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 6.1.3 Perturbation-Based Methods: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n6.2 Distillation Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 6.3 Intrinsic Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n6.3.1 Attention Mechanism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 6.3.2 Joint Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 6.3.3 Modular Transparency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n6.4 Counterfactual Explanations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 6.5 Influence Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21"
        },
        {
            "heading": "7 Axiomatic Properties of Attribution Methods 21",
            "text": ""
        },
        {
            "heading": "8 Evaluation Approaches 22",
            "text": "8.1 Sanity Checks for Interpretability Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 8.2 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 8.3 Metrics for Ground-truth Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 8.4 Metrics for Real Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 8.5 Criticisms of Post hoc Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26"
        },
        {
            "heading": "9 Interpretable Neuroimaging 27",
            "text": "9.1 Feature Engineering Approach to Neuroimaging . . . . . . . . . . . . . . . . . . . . . . . . 27 9.2 Deep Learning Approach to Neuroimaging . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n9.3 Transfer Learning in Neuroimaging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 9.4 Interpretability in Neuroimaging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30"
        },
        {
            "heading": "10 Review of Interpretability Methods in Neuroimaging 31",
            "text": "10.1 Backpropagation Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n10.1.1 Gradient Backpropagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 10.1.2 Modified/Relevance Backpropagation . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n10.2 Perturbation-based Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 10.3 Counterfactual . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 10.4 Distillation Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 10.5 Intrinsic Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 10.6 Feature Map Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55"
        },
        {
            "heading": "11 The Usage Trend of Interpretability Methods 57",
            "text": ""
        },
        {
            "heading": "12 Suggestions for Interpretable Models in Neuroimaging 60",
            "text": ""
        },
        {
            "heading": "13 Conclusion 65",
            "text": "References 66"
        },
        {
            "heading": "1 Introduction",
            "text": "Advancing our understanding of brain dynamics is the underpinning to uncovering the underlying neurological conditions [1\u20133]. Thus, localization and interpretation of subject-specific spatial and temporal activity may help guide our understanding of the disorder. As such, a persistent goal of Artificial Intelligence (AI) in the neuroimaging domain is leveraging magnetic resonance imaging (MRI) data to enable machines to learn the functional dynamics or anatomical alterations associated with underlying neurological disorders.\nThe current understanding of brain functions and structure reveals that the changes in different brain networks can best explain brain disorders [4, 5]. Moreover, a brain network is not necessarily spatially localized. Traditional analytical approaches attempt to find group-level differences rather than deal with individual-level decision-making. However, to receive the full translational impact of neuroimaging studies on clinical practices, clinicians must deal with each individual as a separate case. These limitations naturally encouraged people to look for an AI-led understanding of mental disorders [6\u20138]. Instead of looking into the brain regions independently, machine learning (ML) models look for undiscovered holistic patterns from the data using the advanced knowledge of applied statistics and mathematical optimization techniques [9].\nMoreover, ML can generate individual-level diagnostic and prognostic decisions. Along these lines, standard machine learning (SML) models gained a varying degree of success, and the expert-led feature extraction and selection step is almost a prerequisite for its well-functioning [10]. However, these representations heavily rely on strong assumptions and can miss essential aspects of the underlying dynamics. Unfortunately, when trained on raw data, SML models cannot perform well [11\u201313]. However, we need to go beyond existing knowledge, and learning from the raw data is essential for further advancement in mental health analysis. Specifically, direct learning from the data may reveal undiscovered and valuable patterns within the data and may bring translational value to clinical practices. It may also accelerate diagnostic and prognostic decision-making processes, eventually leading to personalized treatment plans. While SML models fail to learn from the raw data, deep learning (DL) has been very popular because it does not require prior feature selection or intermediate intervention [14\u201318]. It can learn automatically from the raw data and find discriminative and potentially useful clinical features.\nWhile the interpretability of DL models is highly desirable and may faster uncover domain-specific knowledge [19, 20], deep learning models are black-boxes [21] and the exact learning mechanism is still unknown. Introspecting DL models in post hoc manner can be unreliable because what the models actually learned depend on their architectures [22] and their intializations [23] during training. Moreover, there is a disagreement problem among different interpretability methods [24, 25] because the methods are basically heuristically inspired and investigates various aspects of the model. Moreover, there is no agreed validation method for the post hoc explanations in neuroimaging studies, hindering the widespread use of automatic discovery. While interpreting models in a faithful and useful manner is a challenging task, it is an undeniable step before applying them to generate new reliable and actionable insights to combat the disorders.\nIn this review article, we provide a comprehensive review of deep learning model interpretability for neuroimaging studies. We articulate the philosophical ground, dimensions, requirements for the\ninterpretability problem and summarize the commonly used approaches and metrics to achieve reliable model interpretability. Then, we discuss the recent developments of DL approaches to neuroimaging and show some encouraging illustrations for how various interpretability concepts have been applied for new discoveries. We complement the discussion providing a set of guidance and caveats that we think will serve as a useful guide for the future practitioners."
        },
        {
            "heading": "2 Related Work",
            "text": "There exist some reviews in the literature [26\u201329] for interpretable deep learning in neuroimaging and medical domains [30, 31]. However, they either focused on machine learning models or general medical imaging, and very few focused on deep learning interpretability in connection to neuroimaging. The complete end-to-end guideline for interpretability practices in neuroimaging is clearly lacking. We anticipate, starting from the philosophical basis, that the complete guide should provide a broader notion of interpretability, a quick introduction to the commonly used methods, and validation metrics that future studies can use. Moreover, it needs to be clarified the usage trend of these methods and their utility in clinical practices and scientific discovery. That is, we need to clarify how frequently the most prevailing methods have been utilized and the major scientific progress made along the way. Moreover, very little research [32] discussed the desiderata of interpretability framework in neuroimaging. So, there still remains a scope to make a comprehensive accumulation of the prevailing concepts focusing on aspects of deep learning performance, novel findings in interpretability research, and possible implications and connections between them in the neuroimaging domain. This review aims to provide a field guide for interpretable deep learning for neuroimaging study, especially for new aspirants in this direction of research."
        },
        {
            "heading": "3 Organization of the Paper",
            "text": "In Section 4, we discuss the philosophical views of scientific explanations. We then introduce the problem of interpretability for AI models from a holistic point of view in Section 5. In Section 6, we provide a useful taxonomy of interpretability methods and showed several illustrative neuroimaging studies using those methods. We also provide a brief introduction to all the major branches of interpretability approaches and the intuitions behind all the major methods in each branch. We discuss the desiderata of interpretability in AI and the axioms that need to be satisfied by the interpretability methods in Section 7. In Section 8, we describe the common sanity tests to justify the initial validation of the post hoc explanations. We also accumulated the formal evaluation metrics proposed in interpretability literature to provide quantitative validation of the generated explanations for synthetic and real datasets. We also complement this section with the caveats the earlier studies talked about while considering using the post hoc approaches. In Section 9, we discuss the deep learning approach for neuroimaging and the significance of interpretability in these studies. We start our discussion with the traditional feature engineering approach in Section 9.1, and then in Section 9.2, we turn our discussion to the potential of deep learning approaches for neuroimaging research.\nFor deep learning approaches, we emphasize the need for transfer learning (Section 9.3), and interpretability (Section 9.4) to support the discoveries as clinically and neuroscientifically valuable. In Section 10, we provide a detailed review of the recent neuroimaging studies that used all the major interpretability methods as depicted in Figure 2. We show some demonstrative examples of how recent neuroimaging studies are using the idea of interpretability for novel neuropsychiatric biomarker discoveries. In Section 11, we investigated the usage trend of interpretability approaches in more than 300 neuroimaging studies. We then, based on the overall findings of this review, propose useful suggestions and caveats for future interpretability practices in neuroimaging in Section 12. We finally discuss our conclusive remarks in Section 13."
        },
        {
            "heading": "4 Philosophy of Scientific Explanations",
            "text": "Hempel and Oppenheim (1948) [33] believed that explanation and prediction have the same logical structure, and hence they referred to explanations as \"deductive systematization.\" Bechtel and Abrahamsen (2005) [34] viewed explanations as a mechanistic alternative and may depart from widely accepted nomological explanations, which means a phenomenon if explained, must subsume under a law. The authors deemed explanations in life sciences as \"identifying the mechanism responsible for a given phenomenon.\" Lewis (1986) [35] viewed it as \"to explain an event is to provide some information about its causal history.\" However, Lewis did not provide any restricted notion of what information qualifies as part of the causal history. Still, there is no formal definition of \"Explainability\" or \"Interpretability\" in the field of Artificial Intelligence [36\u201338]. As many researchers indicated, the ongoing interpretability practices use only researchers\u2019 intuition that is susceptible to cognitive biases [39] and social expectations [40]. However, as de Graaf and Malle [41] hypothesized, this is not unnatural because as long as people build intentional agents, people will expect explanations from the models using the same conceptual framework people use to explain human behavior. In the current practices of \"Explainable AI,\" the communication gap between the researchers and practitioners is evident, and Miller et al. [42] describes this phenomenon as \"the inmates running the asylum.\" While we also admit that the current practices have some inherent human bias and social expectations, interpretability literature so far has been rich with different useful methods and valuable opinions that we will discuss in this paper."
        },
        {
            "heading": "5 What and Why Is Model Interpretability?",
            "text": "ML systems, generally optimized to exhibit task performance, outperform humans on different computer vision and language processing tasks. However, the deployment of these systems requires satisfying other auxiliary desiderata such as safety, nondiscrimination, justice, and providing the right to explanation [37]. The unique purpose of model interpretability is to satisfy these additional criteria.\nTraditionally, an ML system optimizes an objective function upon which it exhibits its predictive performance. However, a mere objective function does not include other desiderata of ML systems for its wide-ranging real-world scenarios. Thus, regardless of an ML system\u2019s performance, those systems are\nstill incomplete. In other words, stakeholders might seek trust, causality, transferability, informativeness, and fairness as defined in [36]. Hence, as argued in [37],interpretability or, in other words, explanations can be one of many ways to make these gaps in problem visualization more evident to us. Some scenarios Doshi-Velez and Kim include:\n\u2022 Scientific Understanding/Data Interpretation: We may want to create knowledge from an ML system.\nExplanations may be one of the ways to create knowledge from the machine\u2019s learned behavior.\n\u2022 Safety: Incorporating all the accompanying scenarios in developing an artificial agent is not feasible.\nIn that case, an explanation may flag undesirable model behavior.\n\u2022 Ethics: In problem formulation, one might not consider apriori to remove any potential bias, but the\nmodel may learn some unwanted discriminating pattern within the data.\n\u2022 Mismatched Objectives: Often, for building an agent, one may optimize for a proxy function rather\nthan the actual goal. In that case, the agent may discard all other factors that were very relevant to the ultimate goal. For example, a scientist may want to investigate different progressive stages of Alzheimer\u2019s but end up building a classifier for Alzheimer\u2019s patients from healthy controls.\n\u2022 Multi-objective Trade-offs : When an ML system has multiple competing objectives to be satisfied, it\nmay only be possible to incorporate some of them due to the unknown dynamics of their trade-offs."
        },
        {
            "heading": "5.1 How to Achieve Interpretability?",
            "text": "Interpretability in machine learning models can be achieved in different ways [43]. The first and most preferable approach is to build an inherently interpretable model, e.g., a linear one. However, these models may compromise their predictive capacity for transparent Interpretability. The second approach is to build a model that can perform predictions and simultaneously generate explanations. However, it is a very challenging task because the accepted meaning of the term \u2019interpretability \u2019still needs to be settled in the research community. Moreover, it requires both the ground-truth explanations and the labeled samples to train simultaneously for prediction and explanation generation. The third approach is to use separate explanation methods to work on top of the existing models. That is, the existing models can be any black-box model (e.g., deep learning models), and the explanation methods are responsible for generating explanations for the models. Interpretability is especially important when deep learning models are used for knowledge extraction. Regardless of good predictive performance by a DL model, it may still not be useful for discovery as the model may have only learned spurious correlations [44]. Most of the interpretability methods in the literature are designed around the third interpretability approach, frequently referred to as post hoc methods."
        },
        {
            "heading": "5.2 Global vs. Local Interpretability",
            "text": "The scope of Interpretability in machine learning is another consideration. For example, Global Interpretability deals with the overall behavior of the model, such as discovering patterns and the interrelationships among them used for predictions. Global Interpretability is useful to debug a model, specifically to diagnose if the model has any inherent bias or has learned any artifact instead of the objects of interest. As global Interpretability is very hard to obtain because it requires building a relationship among all predictions made by the model, people traditionally end up with local Interpretability that deals with explaining model behavior case-by-case basis. For example, Local Interpretability tries to explain why the image has been classified as \"cat\"/\"dog\" or why a particular loan application has been \"accepted\"/\"rejected.\"\nWhile we recommend reading some other literature reviews [45\u201351] that cover comprehensive discussion of interpretability methods, we briefly describe the key concepts, axioms, methods and metrics used in interpretable machine learning."
        },
        {
            "heading": "5.3 Looking at the Model Interpretability Problem",
            "text": "Guidotti et al. [45] divides the black-box explanation into three sub-categories: model explanation means explaining the overall logic of the model; outcome explanation means finding the correlation between individual input and corresponding decision; model inspection means explaining the behavioral change with changes in input and other parameters or explaining what parts of the model take specific micro-decisions. We provide comprehensive insights into the different aspects of the interpretability problem in Figure 1."
        },
        {
            "heading": "5.4 Important Terminology in Interpretability",
            "text": "As the field of \u201cExplainable AI (XAI)\" is growing rapidly, researchers have defined several important notions useful for the discussion. In this section, we discuss several terminologies often considered significant in interpretability literature. One important point is to note that the terms interpretability and explainability are elusive. Many studies used the terms interchangeably [38, 46, 48]. However, to define these important terminologies, we have attempted to come to a general agreement with most of the interpretability literature. We define the terms as follows:\n\u2022 Interpretability: Doshi-Velez and Kim [37] defined interpretability as the \u201cability to explain or to present\nin understandable terms to a human.\" Interpretability is a passive characteristic of a model that indicates the level the model makes sense to humans [48]. Interpretability is more about the design perspective of a model, and is often tied to the notion of transparency. For a fully interpretable model, explanations about the decisions or the decision-making process are obvious, and hence no other separate explanation tools are required.\n\u2022 Explainability: The term explainability is a broader general term compared to interpretability. Ex-\nplainability is an active characteristic of a model [48], referring to its ability to clarify the internal functions or the rationale the model is using to make decisions, usually in the case of black-box models.\nExplainability is used to specify a level of interpretability and is usually considered as a concession to the latter. In short, interpretable models are inherently explainable, but the reverse is not always true. That is, interpretability refers to \u201chow\" and \u201cwhy\" aspects of a model\u2019s decision-making process, whereas explainability only attempts to make a non-interpretable model to an explainable one, as a concession and attempts to answer only \u201cwhy\" aspect of the model\u2019s decisions.\n\u2022 Understandability: understandability is associated with the notion that if a model\u2019s behavior makes\nsense to humans without even understanding the mechanistic or algorithmic aspect of the model [52]. understandability is also referred to as intelligibility [48].\n\u2022 Comphrenhensibility: An interpretable model is comprehensible, so they imply the same aspect of a\nmodel [45]. It is the ability of how the learned knowledge of a model can be represented in human understandable form [48].\n\u2022 Transparency: The concept of transparency is related to understanding the mechanism by which the\nmodel works [36]. According to Lipton, transparency can be at different levels\u2014at the level of the entire model, the level of the individual components such as input, parameters, and calculation, and the level of the training algorithm.\n\u2022 Fidelity: fidelity of an interpretable model is a comparative assessment of its accuracy with respect to\nthe black-box model the interpretable model is trying to explain [45].\nAs a disclaimer for the remaining part of the article, we emphasize that model interpretability is more about designing inherently interpretable models and model explainability is a concession for model interpretability with the intent to clarify the functions of black-boxes. However, we took the freedom of using the more commonly used term interpretability hereafter even in the context of black-box models, where explainability is more appropriate in its true sense."
        },
        {
            "heading": "6 Taxonomy of Interpretability Methods",
            "text": "In this section, we describe different interpretability methods in the literature. We provide a taxonomy of the interpretability methods in Figure 2. We note that this taxonomy is not perfect in the traditional sense, as the categorization of interpretability methods is still evolving. While we discard some infrequent or obsolete approaches and include some emerging methods, this taxonomy is inspired mainly by Ras et al. [51].\n1. Visualization: Visualization methods focus on highlighting the discriminative regions of the input that\nmainly influenced the model\u2019s decision. This approach is prevalent for deep learning models, especially in computer vision.\n2. Distillation: Distillation methods focus on building a separate \"transparent box\" model, which is\ndirectly interpretable to extract the salient regions or crucial decision rules that guide the original model to reach its decisions. Methods under this category are usually model-agnostic. Moreover, the resulting explanations may be a set of rules or visualization of important regions, similar to visualization methods.\n3. Intrinsic: Intrinsic methods consider model interpretability during model design or training. This\napproach usually leads toward joint training for predictions and explanations or provides a more transparent model where an explanation is somewhat intuitive. A separate post hoc analysis may be required for the latter ones.\n4. Counterfactual: Counterfactual explanations [53, 54] usually do not explain the specific output.\nInstead, it explains in the form of hypothetical scenarios, potentially intending to provide algorithmic recourse. It provides a better understanding of how the decisions change over the input space and allows users more options to change the model\u2019s decision [55].\n5. Influence Functions: To generate an explanation for a prediction, influence functions [21] find the\ninfluence of the training points on the learning algorithm that leads toward this model prediction.\nTo precisely define the interpretability methods, we define an input as a vector x \u2208 Rd. We also define the model as a function F : Rd \u2192 RC , where C is the number of classes in the classification problem. Moreover, let us also assume that the mapping Fc(x) : Rd \u2192 R defines the class-specific logit, where c is the predicted class. An explanation method generates an explanation map E : Rd \u2192 Rd that maps x to a saliency map of the same shape, highlighting the important regions influencing the prediction."
        },
        {
            "heading": "6.1 Visualization Methods",
            "text": "As defined earlier, visualization methods highlight the most influencing regions of the input that drive the model\u2019s output. Generally, visualization methods for model interpretability fall under two main categories. The first category is Backpropagation Methods, also called Sensitivity Methods, and the latter category is Perturbation-Based Methods, also called Salience Methods [43]. Though other methods (e.g., LIME and SHAP) may still use visualizations to communicate explanations, we omit them from the visualization category because they require a separate interpretable model to generate explanations.\nBackpropagation methods are further classified into gradient backpropagation and modified backpropagation methods based on how backpropagation is performed during the computation of saliency maps."
        },
        {
            "heading": "6.1.1 Gradient Backpropagation",
            "text": "In gradient backpropagation, also called sensitivity methods, we measure how the output score changes with the tiny change in each feature dimension. The sensitivity methods assume this change rate indicates the importance of the corresponding input dimension.\n\u2022 Gradients (GRAD): Gradient (GRAD) [117, 118] is the gradient of the class-specific logit with\nrespect to input features x. Mathematically, e = \u2207x Fi(x), where e is the vector representing the feature importance estimate for each input variable in the sample. In fact, it determines the input features for which the least perturbation will end up with the most change in the target response. However, gradients are usually noisy indications of attribution [123, 126, 135]. The major pitfall of using gradients is that the partial derivative \u2202Fi(x)/\u2202xk is not independently related with xk but also with other input dimensions. Furthermore, the concept of saliency does not apply to the linear classifier because saliency is independent of the input for linear models.\n\u2022 Gradient \u2299 Input: Gradient \u2299 Input [128] was introduced to improve the sharpness of the attribution maps obtained through sensitivity analysis. However, Ancona et al. [136] showed that Gradient \u2299 input becomes equivalent to DeepLIFT and \u03f5-LRP, if the network has only ReLU activation functions\nand no additive biases. This point-wise multiplication was initially justified to sharpen the gradient explanations. However, it is better justified when the measure of salience is a priority over mere sensitivity [43].\n\u2022 Integrated Gradients (IG): Integrated Gradients [119] is an attribution method that satisfies\nimplementation invariance and gives one estimate per feature. IG uses the interpolation technique to integrate importance at different discrete intervals between uninformative baseline, say x\u0304 and the input x, to give an integrated estimate of feature importance. The feature importance based on integrated gradients is computed as follows:\ne = (x\u2212 x\u0304)\u00d7 k\u2211\ni=1\n\u2202Fi(x\u0304+ i k \u00d7 (x\u2212 x\u0304)) \u2202x \u00d7 1 k\n(1)\nThe ultimate estimate e depends on the value of k (number of intervals) and the choice of a suitable uninformative baseline x\u0304. IG also satisfies sensitivity-N axiom since \u2211n\ni=1R c(xi) = Fi(x)\u2212 Fi(x\u0304)\n\u2022 Smooth-Grad (SG): Smoothgrad [126, 137] expresses a feature as an averaging of N noisy estimates\nobtained when input is perturbed with some Gaussian noise \u03f5, expressed as:\ne = 1\nN N\u2211 j=1 \u2207x+\u03f5 Fi(x+ \u03f5), where \u03f5 \u223c N(0, 1) (2)\nOther variants [138] of smooth-grad, especially their squared and variance versions, exist in the literature. However, their usage is very limited in model interpretability.\n\u2022 CAM and GRAD-CAM: Zhou et al. [139] proposed Class Activation Map (CAM) to visualize the\nfocal regions using global average pooling on the last layer activations in convolutional neural networks. Subsequently, Selvaraju et al. [127] proposed a gradient-weighted class activation map called Grad-CAM and generalized the CAM computation to a broader set of networks by leveraging the gradients of the last layer activation maps. Indeed, Grad-CAM computes the gradients of the class score (logit) with respect to the last convolution layer. Let Ak be the set of feature maps of size m \u00d7 n. Grad-CAM computes \u03b1ck = 1 m\u00b7n \u2211m i \u2211n j\n\u2202yc \u2202Aki,j , the gradients of the output with respect to each feature map, and\nuse average pooling of the gradients to assign a score to the feature map. Finally, it takes the weighted combination of the feature maps followed by ReLU only, i.e., relu( \u2211\nk \u03b1 c kA k), to consider the positive\ninfluence on the class of interest. As Grad-CAM visualization is in the feature map space, Grad-CAM explanation is first upsampled to the input resolution using bilinear interpolation and then overlaid on the input image. Grad-CAM is sometimes combined with Guided backpropagation for pixel-space visualization through an element-wise product called Guided Grad-CAM. Several variants of Grad-CAM, such as GRAD-CAM++ [140] and Score-CAM [141], have been proposed to improve upon Grad-CAM.\nKapishnikov et al. also proposed two approaches [142, 143], called eXplanation with Ranked Area Integrals (XRAI) and Guided IG, that can refine the results of integrated gradients and can produce improved explanations. However, their usage in neuroimaging studies is still minimal."
        },
        {
            "heading": "6.1.2 Modified Backpropagation",
            "text": "Modified backpropagation category refers to the methods that use different forms of backpropagation other than standard backpropagation. The modification can be based on how gradients should flow backward when the ReLU layer is encountered, such as in guided backpropagation and DeConvNet methods. Another trend is to use relevance backpropagation instead of gradients, such as in layer-wise relevance propagation and deep Taylor decomposition methods.\n\u2022 Guided Backpropagation (GBP): Guided backpropagation [125] modifies the gradients during\nbackpropagation to make it consistent with ReLU activation functions. Let {f l, f l\u22121, . . . , f0} be the\ninput and output features maps of the ReLU activations during the forward pass of a DNN. Also, let {Rl, Rl\u22121, . . . , R0} be the intermediate gradients during the backward propagation. Precisely, the forward ReLU function at the intersection of l \u2212 1 and l-th layers is defined as f l = relu(f l\u22121) = max(f l\u22121, 0) and Guided backpropagation overrides the gradients of ReLU functions. The unique purpose of this modification is to allow only non-negative gradients during backpropagation. Mathematically,\nRl = 1Rl+1 > 01fl > 0R l+1 (3)\nThat is, GBP considers only positive activations with respect to ReLUs and positive gradients from the earlier step during backward propagation. Figure 3 shows example saliency maps produced using variants of Grad-CAM and Guided Backpropagation methods.\n\u2022 DeConvNet: DeConvNet [125] is another \"guided\" method but slightly differs from the Guided\nBackpropagation in that it only passes \"positive\" gradients from the upper to the lower layer when the ReLU layer is encountered. The use of DeConvNet to interpret models in neuroimaging domain is very limited.\n\u2022 Layer Relevance Propagation (\u03f5-LRP): Layer relevance propagation [124] uses the term \"relevance\"\ndenoted as r(l)i to refer to the relevance of the unit i in layer l. It starts at target neuron c in the last layer L and treats the target neuron\u2019s activation as its relevance. The relevance of all other neurons in layer L are set to 0. Subsequently, during backward propagation, it computes attributions for neurons at other layers using a recursive \u03f5-rule as described in Eq. 5. Let zij = x (l) i w (l, l+1) ij be the weighted activation of unit i in layer l onto neuron j in the next layer, bj be the additive bias for the unit j and \u03f5 be the small numerical constant to ensure stability. The final attribution for the i-th input is defined as\nRci (x) = r (1) i .\nr (L) i = Fi(x) if unit i is the target neuron0 otherwise (4) Layer relevance scores are backpropagated and distributed according to the following rule:\nr (l) i = \u2211 j zij\u2211 i\u2032 zi\u2032j + bj + \u03f5. sign( \u2211 i\u2032 zi\u2032j + bj) r (l+1) j (5)\nAncona et al. [136] showed that \u03f5-LRP is equivalent to the feature-wise product of the input and the modified partial derivative. Readers may refer to the study [135] for variants of LRP dealing with improved numerical stability.\n\u2022 DeepLIFT Rescale: DeepLIFT (Deep Learning Important FeaTures) assigns attributions to each unit\ni based on activations using original input x and baseline input x\u0304 [122]. Similar to LRP, DeepLIFT Rescale assigns attribution through backward propagation. Let z\u0304ij be the weighted activation of neuron i in layer l into neuron j in the next layer and defined as z\u0304ij = x\u0304 (l) i w (l, l+1) ij . The rule for assigning attributions during the backward pass is described in Eq. 7. The intended attribution for the i-th input is defined as Rci (x) = r (1) i . Baseline reference values are created based on a forward pass with input x\u0304.\nr (L) i = Fi(x)\u2212 Fi(x\u0304) if unit i is the target neuron0 otherwise (6) The attributions are backpropagated according to the following rule:\nr (l) i = \u2211 j zij \u2212 z\u0304ij\u2211 i\u2032 zi\u2032j \u2212 \u2211 i\u2032 z\u0304i\u2032j r (l+1) j (7)\nDeepLIFT Rescale generalizes the concept of \u03f5-LRP with no assumption of the baseline or a particular choice of non-linearity. In other words, \u03f5-LRP becomes equivalent to DeepLIFT if the baseline is 0 and only ReLU or Tanh is used in the network with no additive biases. DeepLIFT and \u03f5-LRP replace the gradient of the non-linearities with their average gradient. However, this replacement does not apply to discrete gradients. Hence the overall computed gradient of the function may not be the average gradient of the function as a whole. Due to this constraint, DeepLIFT and \u03f5-LRP do not satisfy implementation invariance. DeepLIFT was originally designed for feed-forward networks, and Ancona et al. [136] showed that DeepLIFT is a good approximation of Integrated Gradients for feed-forward networks.\n\u2022 Deep Taylor Decomposition: Montavon et al. [123] proposed another relevance backpropagation\napproach to pass relevance from the output to the input space. This backpropagation of relevance is similar to LRP but uses a different formulation using first-order Taylor expansion."
        },
        {
            "heading": "6.1.3 Perturbation-Based Methods:",
            "text": "In perturbation-based methods, also called salience methods, the marginal effect of a feature on the output score is computed relative to the same input where such a feature is absent.\n\u2022 Occlusion Sensitivity: Zeiler and Fergus (2014) [116] proposed a perturbation-based approach called\nOcclusion Sensitiveity to measure the sensitivity of the output score when some regions in the input image are occluded. This approach is also known as Box Occlusion because of using a grid or box structure during occlusion. Precisely, this method occludes different portions of the input with a grey square and expects a significant drop in classification score if the portion is strongly discriminative for the prediction the model has made. Figure 4 shows example heatmaps generated using popular post hoc methods for Inception v3 model (trained on ImageNet samples) predictions.\n\u2022 Meaningful Perturbation: Fong and Vedaldi (2017) [22] proposed a model-agnostic generalization\nof gradient-based saliency that uses input perturbations and integrates information obtained through all backpropagation. Suppose the input image be x0 and f(x) \u2208 RC . The goal is to find the smallest deletion mask m : \u039b \u2192 [ 0, 1] for which the classification score drops very significantly, i.e., fc(\u03a6(x0;m)) \u226a fc(x0), where \u03a6(x0;m) is perturbation operator. The problem of finding the minimum deletion mask is defined as the following optimization problem:\nm\u2217 = argmin m\u2208[ 0,1]\u039b \u03bb\u22251\u2212m\u22251 + fc(\u03a6(x0;m)) (8)\n\u03bb is a regularizing parameter that enforces small deletion to generate a highly informative region to explain the prediction. This optimization problem is solved using the gradient descent technique.\nGradient-based methods are fast, easy to implement, and readily applicable [119] to existing models compared to perturbation-based methods. However, gradient-based methods are extremely noisy, usually affected by high-frequency variations, and may not represent the model\u2019s decision-making process. In contrast, perturbation-based methods are directly interpretable (because it computes the marginal effect), model-agnostic, and do not require accessing the internal operations of the models.\nWhile the major advantage of perturbation-based methods is the direct computation of the marginal effect of each feature or a small subset of features, the obvious limitations are that the perturbation methods are very slow compared to gradient-based methods. Moreover, they must choose the number of input features to perturb at each iteration and the perturbation technique because the explanations depend heavily on these hyperparameters. Ideally, for realistic reasons, it is not possible to test perturbations of all possible subsets. Moreover, there is no rigorous theoretical foundation to choose from the available perturbation techniques, thus making the explanations unreliable."
        },
        {
            "heading": "6.2 Distillation Methods",
            "text": "In distillation methods, a separate explanation model, also called interpretable model, is required to explain the decision of the original model. This approach is model-agnostic, and the interpretable model does not need the internal behavior of the model. As a separate model is used to extract the essential aspects of the original model, this process is called distillation. However, similar to visualization methods, distillation methods may still produce visualization as explanations.\n\u2022 LIME: LIME [121], also called Local Interpretable Model-agnostic Explanations, is based on a surrogate\nmodel. The surrogate model is usually a linear model constructed based on different samples of the main model. It does this by sampling points around an example and evaluating models at these points. LIME generally computes attribution per sample basis. It takes a sample, perturbs multiple times based on random binary vectors, and computes output scores in the original model. It then uses the binary features (binary vectors) to train an interpretable surrogate model to produce the same outputs. Each of the coefficients in the trained surrogate linear model serves as the input feature\u2019s attribution in the input sample. Let x = hx(x\u2032) be a mapping function between \"interpretable inputs\" (x\u2032) and \"original inputs\" (x). Also, let x\u2032 \u2208 {0, 1}M , M be the number of simplified features, and \u03d5i \u2208 R. The local interpretable explanation model is defined as:\ng(x\u2032) = \u03d50 + M\u2211 i=1 \u03d5ix \u2032 i (9)\nThe explanation model g can be obtained by solving the following optimization problem:\n\u03be = argmin g \u2208G L(f, g, \u03c0x\u2032) + \u2126(g) (10)\ng(x\u2032) and f(hx(x\u2032)) are enforced to be equal. That is, L(f, g, \u03c0x\u2032) determines how unfaithful g is when it approximates f in the vicinity defined by the similarity kernel \u03c0x\u2032 . \u2126 penalizes the complexity of g and the Equation 10 can be solved using penalized linear regression. One of the major issues with LIME is robustness. LIME explanations can disagree if computed multiple times. This disagreement occurs mainly because this interpretation method is estimated with data, causing uncertainty. Moreover, the explanations can be drastically different based on kernel width and feature grouping policies.\n\u2022 SHAP: Historically, Shapley values are computed in a cooperative game theory to calculate the marginal\ncontributions of each player. The computation of this marginal effect relies on game outcomes of all possible sets of coalitions. Suppose P be a set of N players and a function f\u0302 that maps any subset S \u2286 P of players to a game score f\u0302(S). This score is obtained when the subset S of players participated in the game. The Shapley value is a way to compute the marginal contribution of each player i for the game outcome f\u0302(P )\u2014the outcome obtained when all players P participated in the game.\nRi = \u2211\nS\u2286P\\{i}\n|S|!(|P | \u2212 |S| \u2212 1)! |P |! [ f\u0302(S \u222a {i})\u2212 f\u0302(S)] (11)\nThe problem with Shapley values is that this attribution technique is computationally intractable when the number of players is large. Lundberg and Lee [120] proposed a regression-based, model-agnostic formulation of Shapley values called SHapley Additive exPlanations (SHAP). This approach is also known as Kernel SHAP and is widely used to compute SHAP explanations. As SHAP ranks the features based on their influence on the prediction function, the occurrence of overfitting is usually reflected in the provided explanation. In fact, Kernel SHAP removes the need to use heuristically chosen parameters as used in LIME to recover SHAP values. Refer to Figure 5 to see few examples of generated LIME and Kernel SHAP explanations for Resnet 50 model (trained on ImageNet) predictions.\nLIME and SHAP could also be treated as perturbation-based methods because they both perturb the original input locally to build separate interpretable models. However, as described here, the category of perturbation-based methods does not rely on a separate interpretable model. Hence, LIME and SHAP belong to a separate category for their model-agnosticism and the usage of a separate model."
        },
        {
            "heading": "6.3 Intrinsic Methods",
            "text": "Intrinsic methods focus on interpretation as part of the model design or training rather than doing a separate post hoc analysis. These methods are model-specific and are usually implemented based on different design or training perspectives. While some shallow models, such as linear models and decision trees, are directly interpretable, deep learning models are considered black boxes, and their internal functions are quite inscrutable. However, there are many doors to obtain intrinsic interpretability in DL, such as attention mechanism, joint training, and modular transparency. In this section, we briefly discuss some of the common practices used to obtain intrinsic interpretability in deep learning."
        },
        {
            "heading": "6.3.1 Attention Mechanism",
            "text": "An attention mechanism is a technique generally used in deep learning models which computes the conditional distribution over inputs leading to a vector of weights that specify the importance of different regions in\nthe input for the given context. There are several approaches [91, 92] to compute attention weights for single-modal or multi-modal tasks. The attention mechanism has been proven to improve the deep learning model\u2019s performance, and attention weights can be visualized as heatmaps to provide easy-to-understand explanations."
        },
        {
            "heading": "6.3.2 Joint Training",
            "text": "Joint training is the concept of training a model simultaneously for performance and explanations [112\u2013114]. Joint training requires a complex objective function to optimize for the additional explanation task. The additional task may provide a direct textual explanation, generate an explanation association between inputs or latent features and human-understandable concepts, or learn semantically meaningful model prototypes [144]. A very high-level view of joint training optimization can be as follows:\nargmin \u03b8\n1\nN N\u2211 i=1 \u03b1L(yn,y \u2032) + L(en, e \u2032) (12)\nThe arguments yn and y\u2032 refer to model output and output label, respectively. en and e\u2032 refer to model\nexplanation and explanation label, respectively."
        },
        {
            "heading": "6.3.3 Modular Transparency",
            "text": "Modular transparency [29] refers to a network consisting of multiple modules. The modules have pre-specified design goals and are usually black-boxes. However, the interaction among the modules is transparent. The explanation can be obtained from understanding how the model functions globally. Ba et al. [115] demonstrated a modular deep learning model constructed with attention mechanism and reinforcement learning for multiple object recognition tasks. The model was inspired by how humans perform visual sequence recognition tasks by continually moving to the next relevant locations, recognizing individual objects, and changing the internal sequence presentation."
        },
        {
            "heading": "6.4 Counterfactual Explanations",
            "text": "Counterfactual explanations, by definition, provide explanations for hypothetical scenarios. Specifically, counterfactual explanations simply ask for the smallest change required to change the model\u2019s outcome. This category of explanations is human-friendly [145] because they allow humans to choose from multiple options to change the scenarios. Wachter et al. [53] proposed a single-objective optimization method to generate a counterfactual explanation.\nL(x,x\u2032, y\u2032, \u03bb) = \u03bb \u00b7 (f\u0302(x\u2032)\u2212 y\u2032)2 + d(x,x\u2032) (13)\nThe inequality |f\u0302(x\u2032) \u2212 y\u2032| \u2264 \u03f5 determines the tolerance between the current and the counterfactual predictions. The parameter \u03bb balances the distance in prediction and the distance between original and\ncounterfactual instances. d(x,x\u2032) is the distance between the original instance x and the counterfactual x\u2032 measured as weighted Manhattan distance as defined below:\nd(x,x\u2032) = p\u2211 j=1 |xj \u2212 x\u2032j | MADj\n(14)\nwhere MADj is the median absolute deviation of feature j. Dandl et al. [55] proposed a multiobjective formulation of counterfactual explanations. This multi-objective formulation satisfies multiple requirements of counterfactual explanations. Other implementations of counterfactual explanations can be found in [54, 146]."
        },
        {
            "heading": "6.5 Influence Functions",
            "text": "Studies also proposed a data modeling approach to explaining a model prediction in terms of influence functions [21, 90]. Precisely, these methods attempt to find the representative training samples that influenced the prediction of the test sample. While this area of investigation toward explainability is still at the rudimentary level, few studies [21, 90, 147\u2013153] proposed approaches to determine the influencing training points for a particular test case. While determining influence function is yet to use in neuroimaging research as far as we know, this approach, if carefully leveraged, can lead toward many advantageous use cases, including generating counterfactual explanations [90] for different neurological disorders.\nApart from the mainstream interpretability methods, people also attempted visualizing feature maps\neither directly in the convolutional layers or in the input space via optimization techniques [154]."
        },
        {
            "heading": "7 Axiomatic Properties of Attribution Methods",
            "text": "Recent interpretability research spelled out some desirable properties of attribution methods as follows:\n\u2022 Sensitivity(a): An attribution method satisfies Sensitivity(a) [119] if for every input a and baseline\nthat differ in one feature but have different predictions, then the differing feature should be given a non-zero attribution.\n\u2022 Sensitivity(b): Suppose the function implemented by the deep network does not depend (mathemati-\ncally) on some variable. In that case, the attribution method is said to be satisfying Sensitivity(b) [119] if the attribution to that variable is always zero.\n\u2022 Linearity: Suppose two deep networks modeled by the functions f1 and f2 are linearly composed\nto form a third network that models the function a \u00d7 f1 + b \u00d7 f2, i.e., a linear combination of the two networks. Then we call an attribution method to be satisfying linearity if the attributions for a\u00d7 f1+ b\u00d7 f2 to be the weighted sum of the attributions for f1 and f2 with weights a and b respectively [119].\n\u2022 Explanation Continuity: Let Sc(x) be a continuous prediction function for the input x and class c.\nAlso, let x1 and x2 be two nearly identical points in the input space, i.e., x1 \u2248 x2 for which model responses are identical. Attribution methods, to maintain explanation continuity [52], should generate nearly identical attributions Rc(x1) and Rc(x2) i.e., Rc(x1) \u2248 Rc(x2).\n\u2022 Implementation Invariance: Let m1 and m2 be two implementations (models) Sm1(x), Sm2(x) that\ngenerate same outputs for the same input x: \u2200x : Sm1(x) = Sm2(x). An attribution method is called implementation invariant [119] if it generates identical attributions when functions Sm1(x), and Sm2(x) are in the equivalence class for the same input x. That is, \u2200(m1,m2,x, c, ) : Rc,m1(x) = Rc,m2(x)\n\u2022 Sensitivity-n: An attribution method satisfies sensitivity-n axiom [136] if the replacement of any\nsubset of features by their non-informative baseline causes the output score to drop by the sum of the attributions previously assigned to those features. Let xS = {x1, x2, . . . , xn} \u2286 x be the subset of features. Then:\nn\u2211 i=1 Rc(xi) = S(x)\u2212 S(x \\ xS) (15)\nThis sensitivity-n property is only applicable to salience methods that measure the marginal effect of input on the output. Ancona et al. [136] proved that attribution methods (based on gradients), when applied to non-linear models, cannot satisfy sensitivity-n property at least for some values of n, possibly for the reduced degrees of freedom to capture non-linear interactions.\n\u2022 Completeness or Summation to Delta: This is a variant of the sensitivity-n, also called sensitivity-\nN . It constraints the attribution methods to produce attribution that sums equal to the classification score with an assumption that non-informative baseline should produce S(x\u0304) \u2248 0. This property is denoted as: \u2211N i=1R c(xi) = S(x)\u2212 S(x\u0304)\n\u2022 Perturbation - \u03f5: This axiom proposed in [142] is a relaxed version of sensitivity-1 axiom. Suppose\n{x1, x2, . . . , xn} be the input features. For a given 0 < \u03f5 \u2264 1, if all the features except xi are fixed, and removal of xi causes the output to change by \u2206y, then the Perturbation - \u03f5 is satisfied if the attribution holds the inequality: attr(xi) \u2265 \u03f5 \u22c6\u2206y."
        },
        {
            "heading": "8 Evaluation Approaches",
            "text": ""
        },
        {
            "heading": "8.1 Sanity Checks for Interpretability Methods",
            "text": "It is generally expected that model explanation methods should be reasonably sensitive to model parameters. Moreover, the people expect that model should map data and the associated label based on the data generation mechanism relevant to the target. So, to understand if the behavior of an explanation method is reasonable or not, Adebayo et al. [155] proposed the following sanity checks: Model Randomization Test: As a model goes through an intensive training process and learns its parameters during the training process, explanations must be sensitive to the model parameters. For this\nkind of model randomization test, people use either full randomization or cascading randomization and expect to have varied explanations from the explanations generated using the original (non-randomized) model. Data Randomization Test: In this test, training labels are permuted to break the relationship between data and associated labels. A model is trained on these shuffled data and forced to memorize the labels against each training sample. As the model memorizes rather than learning the inherent logical, structural, or causal relationship between data and labels, it performs no better than a random model during inference. However, for any plausible explanation method, the post hoc explanation of this model should be substantially different from the model trained on the original training data. However, this test is extremely time-consuming because a model trained on randomized data takes a long time and customized hyperparameters to achieve reasonable convergence."
        },
        {
            "heading": "8.2 Evaluation Metrics",
            "text": "Human evaluation (qualitative) of explanation methods can be entirely wrong because it is possible to create adversarial samples [156, 157] that can fool the human eye, totally changing the model predictions. For quantitative assessment, we need to define the domain-specific desired properties of the interpretability methods formally. Moreover, we need appropriate quantitative metrics to assess the behavior of an interpretability method. When the generated attributions do not become plausible, it is hard to identify if the problem is due to the model itself or to the interpretability method that generated the attributions. In this section, we present some evaluation metrics proposed in the interpretability literature."
        },
        {
            "heading": "8.3 Metrics for Ground-truth Datasets",
            "text": "Arras et al. [158] proposed two evaluation metrics that can reliably quantify the explanation methods for the datasets that have ground truths. Relevance Mass Accuracy: This metric calculates the proportions of total attributions that reside within the relevance area.\nRelevance Mass Accuracy = Rwithin Rtotal with Rwithin = |GT |\u2211 k=1\ns.t. pk \u2208GT\nrpk and Rtotal = N\u2211 k=1 rpk (16)\nwhere rpk is the relevance score for the pixel pk. N is the total number of pixels. GT is the set of all\npixels within the relevance area (ground-truth area).\nRelevance Rank Accuracy: Let K be the number of pixels within the ground truth masks. This metric measures how many high-ranked K pixels are within the relevance area. Let Ptop K = {p1, p2, . . . , pK | rp1 > rp2 > rp3 \u00b7 \u00b7 \u00b7 > rpK} be the top K pixels sorted in descending order of their attribution values. Rank\nAccuracy is defined as follows:\nRelevance Rank Accuracy = |Ptop K \u2229 GT |\n|GT | (17)\nThe argument GT refers to the set of pixels within the ground-truth region."
        },
        {
            "heading": "8.4 Metrics for Real Datasets",
            "text": "Several studies proposed different measures, such as Remove And Retrain (ROAR) [138], RemOve And Debias (ROAD), Accuracy Information Curves, Softmax Information Curves [142], Infidelity, Sensitivity [159], to assess the quality of explanations.\nRemove and Retrain (ROAR): Hooker et al. [138] proposed another approach to evaluate the performance of an interpretability method. In this approach, samples are modified based on the post hoc explanations. In particular, the features that receive significant attributions during explanation are removed. The model is trained over the modified training data, and people expect a sharp drop in model performance because important discriminative features are absent from the training data. The method is time-consuming as it requires full retraining of the model. Another pitfall of this evaluation process is that the ROAR metric may produce erroneous evaluations when correlations among features exist and capturing only the subset of correlated features is sufficient for correct prediction [160]. However, ROAR fails to evaluate the feature relevance correctly in that scenario.\nlog-odds score: Shrikumar et al. [122] proposed a metric to evaluate the quality of explanations. This method greedily identifies the main contributing pixels to convert the original prediction c0 to some target prediction ct. That is, it removes pixels (20% of the image) based on descending ranking of Sc0 \u2212Sct . Finally, it measures the change in the log-odds score between c0 and ct for the original image and the image with pixels removed to get the prediction ct. The greater change in log-odds score implies the greater significance of the removed pixels for the original class and thus better capture the true importance. This metric is not useful for natural images and possibly meaningful for images with a strong structural association as in MNIST.\nArea Over MoRF Precision Curve: Samek et al. [135] proposed an evaluation technique for the heatmaps based on the idea of how quickly the function value f(x) (probability score) drops if the most relevant regions are perturbed. To achieve this agenda, it creates an ordered set O = (r1, r2, . . . , rL) based on the importance scores of pixels as assigned by the interpretability method. This procedure follows a region perturbation (most relevant first (MoRF)) process, where gradually, a small rectangular region m\u00d7m surrounding each important pixel location rp is removed by the uniform distribution. The quantity of interest here is termed as Area Over MoRF Perturbation Curve (AOPC).\nAOPC = 1\nL+ 1\n\u2329 L\u2211\nk=0\nf(x (0) MoRF)\u2212 f(x (k) MoRF) \u232a p(x)\nHere \u27e8.\u27e9p(x) indicates average over all samples in the dataset. The intuition is that if the ranking strongly associates with the class label, the removal will cause a steeper drop in the functional value, causing a larger AOPC.\nThough localization and saliency have different connotations, the quality of a saliency map is often measured as its localization accuracy because they overlap. For example, for a dog image, the localization box usually encapsulates the entire dog without focusing on salient details of the dog. The usage of localization in saliency evaluation is often referred to as weakly supervised localization because neither model training nor post hoc interpretability use localization information.\nSmallest Sufficient Regions (SSR): Dabkowski et al. [161] proposed a metric based on the notion of the smallest sufficient region capable of correct prediction. This metric requires maintaining the same classification and finding the smallest possible area of the image. This metric is formally defined as follows:\ns(a, p) = log(a\u0303)\u2212 log(p) (18)\na\u0303 = max(a, 0.05), where a is the proportion of the cropped image to the original image. p is the probability of the corresponding object class when the classifier classifies based on the cropped but resized image. The lower value of s(a, p) indicates a better saliency detector because it directly translates the idea of SSR \u2014less area, greater probability score. However, this metric is not suitable if the model is susceptible to the scale and aspect ratio of the object. Moreover, as this metric depends on rectangular cropping and reports results as a function of the cropped area, this approach highly penalizes if the saliency map is coherently sparse [142]. Because, in that case, it may span a larger area of the image than the map, which is locally dense, even with the same number of pixels. However, this is counterintuitive from the human vantage point. Humans tend to have sparse and coherent explanations. Moreover, this imposes a severe challenge because masking creates a sharp boundary between the masked and salient region, causing an out-of-distribution problem for the model.\nRemOve And Debias (ROAD): Rong et al. [162] proposed an evaluation strategy that overcomes the 99% computational cost of retraining to evaluate attribution methods. The authors made a useful experimental observation that existing ROAR evaluations based on MoRF (most relevant first) or LeRF (least relevant first) removal strategies are inconsistent in ranking the attribution methods. The authors attributed this inconsistency to the class information leakage through the shape of the removed pixels. To mitigate these unwanted influences, the authors proposed a Noisy Linear Imputation operator that debiases the masking effect and removes the need for additional retraining.\nPerformance Information Curve (PIC): Kapishnikov et al. proposed another perturbation-based evaluation metric [142], called Performance Information Curve to evaluate the appropriateness of an attribution method. The PIC evaluation builds a saliency-focused image. It starts with a blurred image and combines with a saliency mask thresholded, for example, at x%, to produce the saliency-focused image. The saliency-focused image is then fed into the model to assess the performance of the attribution. The accuracy/softmax score of the model is then mapped as a function of Information Level, i.e., calculated\nentropy. The entropy is a proxy measure of the information content re-introduced for evaluation. The compressed image size is an approximate proxy for the information content of an image. It normalizes the entropy of the re-introduced image by considering the proportion of the entropy from the original image. The aggregate performance measurement over all the information levels for all samples in the dataset finally generates the PIC. The PIC has two variants:\nAccuracy Information Curve (AIC): For AIC, the x-axis uses normalized entropy values and divides them into several bins. The y-axis reports the accuracy calculated over all the saliency-focused images for each bin of image information level (entropy).\nSoftmax Information Curve (SIC): The x-axis uses the same normalized entropy values for SIC. The y-axis reports median scores for the proportion of the original label\u2019s softmax score for the saliency-focused image versus the softmax for the original image."
        },
        {
            "heading": "8.5 Criticisms of Post hoc Interpretability",
            "text": "While evaluating interpretability methods, we also need to consider the criticisms or concerns people raised in the interpretability literature. The concept of interpretability is simultaneously considered essential and evasive [36, 163]. One of the obvious characteristics of interpretability is that it focuses on generating some understandable intuitions behind why a particular prediction has been made. Interpretability, however, does not care about how the model arrived at this decision. Particularly, the existing interpretability methods focus on the revelation of different aspects of the model\u2019s learned behavior [24, 25], not necessarily the way a model functions. A vast amount of studies [164\u2013178] talked about different pitfalls of post hoc interpretability methods. People raised questions about the transparency of the DL models and the incapacity of the popular interpretability methods for reliable real-world deployments. Rudin (2019) [164], for example, criticized attempts to explain black-box models. Instead, she suggested building inherently interpretable models. Rudin also thinks black-box models are not required in AI [167]. Moreover, many methods are blamed to be computationally expensive [116, 120], unstable [121], model insenstive [125, 127], noisy [117\u2013119]. Furthermore, some methods [122, 124] are criticized for not satisfying the desirable implementation invariance [119] property. In medical imaging contexts, multiple studies [164, 179, 180] reported the unreliability of saliency maps for localizing abnormalities in medical images. Moreover, we should address some ethical dilemmas as indicated by [181] because inherent human bias may violate the transparency of interpretable systems.\nWhile post hoc interpretability methods have been widely used in different applications and neuroimaging studies, we should always be aware of their usage when safety and trust are our significant concerns. For example, we must accept the explanations wisely when we want to use interpretable DL models to understand how the brain functions or what dynamics are responsible for a particular mental disorder. Generally, people use post hoc interpretability methods without any pre-condition applied to the model\u2019s design. Paez [178] argued that model transparency or model approximation is useful for objectively understanding the model.\nMoreover, it is also a necessary condition to achieve post hoc interpretability. We discuss more on this issue and provide a set of detailed insights and suggestions in Section 12 for future practitioners."
        },
        {
            "heading": "9 Interpretable Neuroimaging",
            "text": "Psychiatric disorders have strong correspondence with underlying complex brain dynamics. These everchanging dynamics supposedly reflect the progression of these disorders. Identifying the essential, interpretable, non-invasive imaging biomarkers from the dynamics can be a significant breakthrough for early diagnosis, potentially preventing its future progression with the help of new insights the model can gain from the data. As discussed earlier, DL is a powerful data-adaptive technology that can automatically learn from data [18]. As such, DL can bring breakthroughs in healthcare [23] via uncovering unforeseen and scientifically valid information about disorders. However, a strong caveat is that it is not an easy task because DL models may find different sets of hidden factors contributing to the same input-output relationship [23]. While the field of interpretability has advanced rapidly in recent years [45, 51, 182], we still need rigorous methods and validation techniques to deploy these models effectively in clinical practices and in advancing scientific understanding of the neurological disorders.\nIn the following sections, we discuss the significance of interpretability in neuroimaging studies adopting deep learning approaches. We reviewed more than 300 neuroimaging studies that considered model interpretability as their essential component. We refer to Figure 2 for a quick reference to some neuroimaging studies utilizing all the prevailing interpretability methods. We reckon that these analyses will be helpful for future neuroimaging practitioners looking for a general guideline. Additionally, we analyzed the recent usage trend of the most prevailing post hoc interpretability methods, which clearly shows their continued acceptance in the neuroimaging community. Finally, we discuss different caveats of interpretability practices and provide insights on how this specialized sub-field of AI can be used wisely and meaningfully."
        },
        {
            "heading": "9.1 Feature Engineering Approach to Neuroimaging",
            "text": "In this section, we discuss the traditional feature engineering and comparatively newer feature learning practices in neuroimaging studies.\nOne of the crucial challenges of Neuroimaging research is understanding the association between cognitive state and the underlying brain activity [13, 70]. Traditionally, people use the feature engineering approach with shallow linear interpretable models to tackle these challenges. Feature engineering or feature selection step intends to reduce the dimension of the signals while preserving useful discriminative information. Global feature-based (voxel-based) or regional feature-based approaches are commonly used in neuroimaging for feature selection [17]. Ashburner and Friston [183] summarized the advances of voxel-based morphometry (VBM), where voxel-wise parametric statistical tests are conducted to compare the smoothed gray-matter images from the two groups. Kloppel et al. [184] used normalized grey matter segment to classify AD patients from normal cohorts. Saima et al. [185] used the volume of gray matter (GM), the volume of white matter (WM), the volume of cerebrospinal fluid (CSF), the area of the left hippocampus, and the area of the\nright hippocampus to classify AD from sMRI images based on an ensemble of classifiers. Schnack et al. [186] used gray matter densities (GMD) to model SVM for schizophrenia and bipolar classification using sMRI images. Patel et al. [15] proposed a stacked autoencoder for schizophrenia classification. The autoencoder was trained in an unsupervised fashion on 116 active gray matter regions to extract region-specific features. Subsequently, the extracted features were used to train an SVM model. Dluhovs et al. [7] used three imaging features (gray matter, white matter, and modulated GM and WM tissue segments of sMRI scans to feed into SVM classifiers in a distributed setting. Xiao et al. [187] used the cortical thickness and surface area features of 68 cortical regions from sMRI images for the SVM-based classification of schizophrenia. Steele et al. [188] used mean grey matter volume and density across 13 paralimbic regions of sMRI scans in SVM based classifier to predict psychopathic traits in adolescent offenders. The regional feature-based approaches intend to summarize the whole brain signal by extracting features from some predetermined regions of interest (ROIs). For example, several studies [189, 190] divided the whole brain into multiple regions and extracted features from those regions to train machine learning models. The ROIs are predetermined based on prior neurobiological knowledge relevant to the disorders.\nRashid et al. [191] used dynamic brain connectivity from resting state fMRI for schizophrenia and bipolar patients classification and showed that dynamic FNC outperforms static FNC. Iddi et al. [192] proposed a two-stage approach for predicting AD progression. In the first stage, the authors used the joint mixed-effect model for multiple modalities such as cognitive and functional assessments, brain imaging, and biofluid assays with fixed effects for covariates like age, sex, and genetic risk. In the second stage of prediction, a random forest algorithm is used to categorize the panel of predicted continuous markers into a diagnosis of controls and stages of progression. Many other studies [193\u2013195] used functional network connectivity measured as Pearson\u2019s correlation coefficients as features for a range of classifiers. Shen et al. [193] also used locally linear embedding (LLE) to reduce the dimensionality of the feature space to demonstrate that PCA in place of LLE hardly provides separable data points. For a detailed review of feature reduction techniques, refer to [196]."
        },
        {
            "heading": "9.2 Deep Learning Approach to Neuroimaging",
            "text": "Feature engineering and shallow models suffer from several limitations: 1) the inherent interpretability of shallow models compromises the capacity to deal with high-dimensional neuroimaging data 2) it prevents the natural understanding of brain dynamics. While standard machine learning models can perform reasonably well on handcrafted features, their performance dramatically drops when trained on raw data because of their inability to learn adaptive features from the raw data [80].\nIn contrast, Deep Learning (DL) has gained significant progress in different application areas, especially for computer vision and natural language processing tasks. The primary benefit of DL is that it can independently learn from the data through varying levels of abstraction using a series of nonlinear functions. Importantly, it relieves the need to use error-prone feature engineering phase [196], which predominantly relies on some preoccupations with the data that may prevent the natural emergence of significant features.\nTo leverage the capacity of DL in neuroimaging research, researchers have started using DL to reach a new level of understanding of the association between psychiatric disorders and brain dynamics [11, 70, 197\u2013201].\nHowever, the improved performance of DL comes at the cost of intelligibility\u2014its decision-making process is quite incomprehensible to human beings. While deep learning methods can simultaneously achieve unprecedented predictive performance and potentially lead to identifying idiosyncratic brain regions associated with the disorders, the model may overfit and not generalize well to unseen subjects. Moreover, it may learn unexpected artefactual associations for its predictions. The need for explanations arises from inadequate knowledge of the data and associated data generation mechanism and poor understanding of the model\u2019s behavior during training. This lack of intelligibility prevents the widespread deployment of DL models in safety-critical domains such as healthcare, medicine, neuroscience, and self-driving cars, to name a few.\nEvidence from many recent studies reinforces the potential of deep learning toward new knowledge discovery in different domains. For example, several studies [19, 20] have demonstrated that a convolutional deep learning model, when introspected with gradients, smoothgrad, and GradCAM, might reveal crucial medical information from ECG signals. Often, interpretability may assist in identifying if the model has inherited any inherent bias from the data. For example, Young, Booth, Simpson, Dutton, and Shrapnel [202] used GradCAM and Kernel SHAP to show that produced saliency maps pass some sanity checks and can be helpful at least to diagnose potential biases in the models trained for melanoma detection. In another study, Vellido [203] pointed out the significance of interpretability and visualization in medicine and healthcare. Lucieri et al. [204] used a concept activation vector (CAV) to show that the deep learning model can encode understandable human concepts and apply the disease-relevant concepts for its predictions in a cancer classification task.\nFrom the perspective of neuroimaging applications, we must meet the two most crucial challenges to gain a broader level of acceptance of DL as a research and clinically supportive tool: 1) Neuroimaging data is inherently high-dimensional. Studies usually have a small sample size posing m \u2265 n problem, which is very susceptible to cause overfitting in deep models. 2) DL models are considered as black box models because of their multi-level non-linearity and lack of established theory behind their learning mechanism. Consequently, it is hard to establish an association between the predictive cognitive state and the underlying dynamics. In other words, the accuracy may not be representative of the quality of the features used by a model. For example, Lapuschkin et al. [205] demonstrated how a Fisher Vector model can learn to choose unintended artifacts for generating predictions. In this specific example, the model used copyright tag to predict \"horse\" as all the horse images contain the copyright tag, which turned out to be a characteristic of horses. This kind of phenomenon is entirely unexpected and must be avoided while leveraging deep learning models in medical domains."
        },
        {
            "heading": "9.3 Transfer Learning in Neuroimaging",
            "text": "One of the major concerns in neuroimaging studies is the lack of sufficient training samples [80, 206], which is hostile to the efficient training of DL models [207]. This constraint is due to the expensive data collection\nprocess in neuroimaging studies [208]. In such a scenario, transfer learning can be a convenient approach to deal with this problem, as reported in several studies [197, 200, 209\u2013211]. While adapting transfer learning in neuroimaging domain is a harder problem due to the unavailability of transferable tasks and lack of ground truth, formulating a suitable task that supports transferrable representation learning from unrelated neuroimaging datasets is essential to support studies dealing with limited training data.\nLeonardsen et al. [212] proposed a CNN model for brain age prediction and subsequently showed evidence of how a model trained to predict age can learn abstractions of the brain and hence can be useful for a series of downstream tasks. The model was selected from some architectural variants and performed well for brain age prediction. The representations as learned by the model were noticeably predictive compared to a baseline model for different unseen datasets for multiple case-control studies. The authors further studied the deviation of the predicted age from the chronicle age by correlating the brain age delta and different standard measures of MRI images. Eitel et al. [213] emphasized the significance of transfer learning by showing how learned knowledge can be transferred across diseases (AD to MS) and MRI sequences (MPRAGE to FLAIR). However, we argue that transferring knowledge across diseases can be misleading. That is, transferring knowledge from a model trained on Alzheimer\u2019s patients to a study to classify MS patients may confuse the downstream model. Instead, we should define a pretext task and apply unsupervised or self-supervised pretraining of the model on a more neutral group (e.g., healthy controls). This knowledge transfer approach, as we think, may result in more interpretable knowledge transfer [13]. Rahman et al. [13] proposed a transfer learning mechanism that uses contrastive learning to pretrain a deep learning model on publicly available healthy subjects of the Human Connectome Project (HCP). The authors showed that the self-supervised pretraining improved performance of three downstream models separately trained to classify (schizophrenia, Alzheimer\u2019s disease, and autism spectrum disorder) patients of three disorders with the diverse demographic background. In addition, the improved representations improved the post hoc interpretability of the models. Oh, et al. [80] argued in favor of deep learning-based approaches compared to the traditional way of building classical machine learning models based only on feature extraction approaches. They incorporated a transfer learning mechanism to transfer knowledge (weights) learned during AD vs. NC classification for the pMCI (progressive mild cognitive impairment) vs. sMCI (stable mild cognitive impairment) classification task. For a more detailed review of how transfer learning has been used in magnetic resonance imaging, we refer to the paper [214]."
        },
        {
            "heading": "9.4 Interpretability in Neuroimaging",
            "text": "In this section, we discuss the significance of explainability for AI models. This explainability requirement is even more pronounced for deep learning models because of their black-box nature. In particular, we first provide evidence of how deep learning approaches have been recently used for neuroimaging. We also show how interpretability has been a pivotal area of research to make these models clinically valuable tools. Next, we provide a detailed review of the contexts to which interpretability was applied in neuroimaging and discuss the findings therein.\n\"Explainable AI\"\u2014a subfield of AI has been very popular because of the recent surge in AI models and algorithms as reflected in the left panel of Figure 6. Moreover, deep learning shares a larger part of most recent AI practices. Neuroimaging community has also witnessed a similar surge in deep learning practices in recent years. As DL models are black boxes, the need to interpret the DL models has become essential to validate the models or to advance our understanding of the problem domain, as we can see in the right panel of Figure 6. For a quick reference to some neuroimaging studies using popular interpretability methods, readers are advised to refer to the Figure 2."
        },
        {
            "heading": "10 Review of Interpretability Methods in Neuroimaging",
            "text": "For the comprehensive review, we group the papers based on the interpretability methods used in those studies. As some studies used several methods in a single study, we mention them at all relevant places. The summary of the review can be accessed from Table 1.\n32\n33\nTable 1: Literature Review of Interpretable Deep Learning Research in Neuroimaging.\nAuthors, Year\nStudy Objective Dataset Modality Interpretability\nExplanation Validation\nBass et al., 2020 [66] Lesion Detection AD, Age Classification\nd0 d1 d14 T1, T2 sMRI joint training norm. cross correlation\nprevious reports\nRavi et al., 2022 [97] CN/MCI/AD 4D MRI Reconstruction\nADNI T1 sMRI modular transparency qualitative and\nquantitative assessment\nGaur et al., 2022 [64] Brain Tumor Classification MRI [228] MRI SHAP, LIME Not provided Saboo et al., 2022 [62] Cognition Prediction MCSA sMRI,\nDiffusion MRI LIME\nprior reports, exploratory analysis\nI: Interpretability Methods i0 SA-3DUCM: sensitivity analysis by 3D ultrametric contour map i1 3D-ResNet-GAP: 3D-ResNet with global average pooling layer V: Validation Methods v0 meta analysis with NeuroSynth [229] v1 CPDB: ConsensusPathDB-human S: Studies s0 AD: Alzheimer\u2019s Disease vs. NC: Normal Controls s1 seeing images of body parts, faces, places or tools s2 Multiple Sclerosis vs. NC s3 SZ: Schizophrenia vs. NC s4 ASD: Autism Spectrum Disorder vs. NC/TD (typically developing) s5 WRAT: wide range achievement test (LOW/HIGH) s6 PD: Parkinson\u2019s Disease vs. NC s7 HCM: Hypertrophic cardiomyopathy vs. NC s8 AD and other (clinical) variables s9 AD Variants (early/late/stable/progressive/amnestic) vs. NC s10 IRF: intraretinal fluid/SRF: subretinal fluid/PED: pigment epithelium detachments M: Modality m0 T1-w/FLAIR/SWI sMRI m1 complex-valued resting-state fMRI m2 SPECT DaTSCAN m3 WSI: whole slide imaging m4 Genomic Data D: Datasets d0 ADNI: Alzheimer\u2019s Disease Neuroimaging Initiative\nd1 HCP: Human Connectome Project (HCP S1200 release) d2 VIMS study d3 FBIRN: Function Biomedical Informatics Research Network d4 Open Access Series of Imaging Studies d5 ABIDE: Autism Brain Imaging Data Exchange d6 ABIDE: Autism Brain Imaging Data Exchange II d7 PNC: Philadelphia Neurodevelopmental Cohort d8 Illumina HumanHap 610 array, 500 array, Illumina Human Omni Express array d9 LIFE Adult Study [230] d10 UNM IRB: University of New Mexico Institutional Review Board d11 PPMI: Parkinson\u2019s Progression Markers Initiative Database d12 1000 Functional Connectomes Project d13 Adolescent Brain Cognitive Development (ABCD) Study d14 UKB: UK Biobank d15 International Consortium for Brain Mapping database (ICBM) d16 National Database for Autism Research d17 OpenfMRI d18 UCD-ADC: University of California, Davis Alzheimer\u2019s Disease Center Brain Bank d19 PING: Pediatric Imaging, Neurocognition and Genetics d20 AIBL: Australian Imaging, Biomarker and Lifestyle Flagship Study of Ageing d21 FHS: Framingham Heart Study d22 NACC: National Alzheimer\u2019s Coordinating Center d23 DUKE dataset [231] d24 RETOUCH dataset https://retouch.grand-challenge.org/ d25 LPBA40 dataset [232, 233] d26 MCSA: Mayo Clinic Study of Aging participants 34"
        },
        {
            "heading": "10.1 Backpropagation Methods",
            "text": ""
        },
        {
            "heading": "10.1.1 Gradient Backpropagation",
            "text": "CAM/Grad-CAM/Guided Grad-CAM Yang et al. [87] proposed three approaches for generating explanations. One of them, SA-3DUCM (sensitivity analysis by 3D ultrametric contour map), deals with sensitivity analysis of 3D-CNN via a hierarchical image segmentation approach, and the other two methods (3D-CAM, 3D-GRAD-CAM) generate explanations via visualization of network activations on a spatial map. The methods have their own constraints and complement each other. As a baseline method, the authors used occlusion using a cubic neighborhood of 7\u00d7 7\u00d7 7. However, these occlusion methods are not semantically meaningful. The neighborhood size is a hyperparameter and can drastically change the results. Moreover, this method is computationally very expensive. To address these issues, the authors used 3DUCM to produce semantically meaningful, hierarchical, and compact brain segments. Subsequently, they used the occlusion technique based on these segments rather than individual voxels. However, this addition to the baseline occlusion does not consider correlations and interaction among segments. To resolve this, they used 3D Class Activation Mapping (3D-CAM) and 3D-Grad-CAM, which still suffer from the low-resolution problem and may miss the fine details of importance score in the input space. Further analysis of heatmaps reveals that occlusion generated heatmaps fail to identify discriminative regions. SA-3DUCM and 3D-CAM are able to identify some regions that match with human expert evaluation.\nHu et al. [218] proposed an interpretable DL framework to classify subjects\u2019 cognitive ability (low/high WRAT groups) from n-back fMRI data from the PNC cohort. The proposed model can learn from multimodal fusion data and preserve the association across modalities. The authors leveraged Grad-CAM to guide convolutional collaborative learning. This study takes advantage of multimodal fusion from brain FC data and single nucleotide polymorphism (SNP) data. This study intends to extract potentially useful brain mechanisms within and between brain FC and genetics. 264 ROIs were used for brain FC data. The genetic SNP data were collected from the Illumina HumanHap 610 array, the Illumina HumanHap 500 array, and the Illumina Human Omni Express array. The results show that the classifier based on convolutional collaborative learning outperforms the traditional ML classifiers. While it has been evident that all classifiers used some hand-engineered features, the low performance of traditional classifiers might arise from the dimensionality reduction of the original hand-engineered Brain FCs and SNPs. The model identified a large number of significant FCs for the low WRAT (Wide Range Assessment Test) group. In contrast, for the high WRAT group, the model identified a smaller number of significant FCs. The authors used a hypothetical validation technique, which has little empirical significance. This study used ConsensusPathDB-human (CPDB) database as a reference to validate the identified SNPs. The authors provided probable explanations for the identified SNPs, clarifying the model\u2019s discriminative behavior.\nLin et al. [78] proposed a 3D-CNN model to classify schizophrenia patients from normal controls using spatial source phase (SSP) maps derived from complex-valued fMRI data. This study showed the superior performance of SSP maps compared to magnitude maps (MAG) extracted from magnitude-only fMRI\ndata, and spatial source magnitude maps (SSM) separated from complex-valued fMRI data. The authors used two interpretability methods, saliency maps and Grad-CAM, to separately understand the prominent and predictive regions associated with the model predictions. A snapshot of the generated explanations at the subject-level is shown in Figure 7. While CNN can be a powerful tool for feature extraction and classification, the underlying caveat was the susceptibility of model performance and associate heatmaps because they varied widely according to the number of convolutional layers used.\nZhang et al. [17] proposed a learning framework combining the residual network and self-attention\nto perform two classification tasks using sMRI images: classifying AD from NC and pMCI from sMCI. This study, in particular, showed that residual networks could learn from sMRI images compared to other variants of convolutional networks (e.g., 3D-VGGNet) and self-attention helps to upgrade the classification performance. The authors applied 3D Grad-CAM to explain individual predictions. One problem with Grad-CAM in understanding the characteristic patterns responsible for predictions is that it cannot capture the fine details in the brain space because of required upsampling. Often, people use convolution layers close to the input layer to increase the resolutions of heatmaps. However, different convolution layers learn different levels of abstraction from the data. So, in that case, explanation maps may not reflect the global behavior of the model.\nLeming et al. [223] used a diverse collection of fMRI datasets and leveraged a deep convolutional neural network for three different classification tasks\u2014ASD, gender, and resting/tasks\u2014using functional connectivity (FC). The authors showed that the deep learning model is capable of good classification when datasets are a mixture of multi-site collections. The authors used the 116-area automated anatomical labeling (AAL) parcellation template [234] and computed functional connectivity of 4\u00d7 116\u00d7 116 (4 wavelet frequency scales and 116 nodes wavelet coefficient correlation). This study showed that CAM could identify the brain\u2019s prominent spatial elements (connectome) that the models used for predictions. In contrast, activation maximization, though initially used to gain intuitions of neural network internals [235], was able to provide insights into the critical predictive features suitable for classification. However, as the variation of the accuracies of the ensemble was very large, the identified areas may not fully characterize ASD.\nGradients and Guided Backpropagation Rieke et al. [215] proposed a 3D-CNN to classify AD patients from healthy controls. The authors used four visualization methods\u2014gradients, guided backpropagation, occlusion, and brain area occlusion\u2014to generate explanations. Relevance scores from gradient-based visualization methods were more distributed across the brain, as opposed to occlusion and brain area occlusion, where relevance scores are more focused on specific regions. Distributive relevance is not feasible for occlusion-based methods because of the limited size of the patch. Hence, the authors recommend using gradient-based approaches for scenarios where distributed relevance is expected. While all four methods focused on some regions considered relevant for AD, such as the inferior and middle temporal gyrus, the distribution of relevance scores varied widely across the patients. In particular, the relevance maps for some patients focused on the temporal lobe, whereas relevance maps for others focused on larger cortical areas. Unlike LRP, as claimed in [216], the authors think that similar heatmaps as obtained for both AD and NC are reasonable because a given network should\nlook into similar regions to detect the absence or presence of the disease. To quantify the difference between visualization methods, the authors used Euclidean distance between average heatmaps of the groups (AD or HC) obtained from two visualization methods. Gradient-based methods showed a very small distance.\nOh, et al. [80] proposed a CNN-based end-to-end learning model to perform four different classification tasks classifying various stages of AD (Alzheimer\u2019s disease) from NC (normal control) and pMCI from sMCI. The study used a convolutional autoencoder to pretrain the model in an unsupervised fashion. The authors, after prediction, used the saliency method (gradients) to visualize predictive features that the models used for each classification. Analysis of the heatmaps revealed that the temporal and parietal lobes were most discriminative between AD patients and controls.\nIntegrated Gradients and Smoothgrad In a recent study, Rahman et al. [13] proposed an interpretable deep learning framework. The framework includes a pre-trainable model suitable for multiple downstream studies with limited data size. The authors also proposed how we can investigate spatio-temporal dynamics associated with mental disorders using post hoc interpretability methods (integrated gradients (IG) and smoothgrad on integrated gradients). Apart from qualitative evaluation, the framework suggested a quantitative evaluation technique, called RAR, to objectively show that identified salient regions are indeed meaningful and highly predictive. This study demonstrates the utility of IG and smoothgrad for neuroimaging interpretability.\nLevakov et al. [220] proposed an ensemble of CNNs and aggregate \"explanation maps\" to arrive at some conclusive remarks associated with brain age. The authors used smoothgrad as a post hoc interpretability method and were particularly interested in population-wise explanation rather than subjectspecific identification of anatomical brain regions. This study also used ensembles of CNN to analyze the model uncertainty behavior. Population-based map for each ensemble was produced by averaging all the volumes in the test set. To generate the global population-based map, they aggregate population-based maps generated for each CNN by taking the median value for each voxel across the ensembles. While this approach highlights important areas in the brain space, this approach is not able to comment on the direction of influence. It is impossible to determine if the regions contribute positively or negatively to brain age.\nWang et al. [81] applied Integrated Gradients (IG), LRP, and Guided Grad CAM to visualize CNN models designed for Alzheimer\u2019s classification. The authors observed that IG is the best, as revealed in the meta-analysis performed on top of all visualizations. IG heatmaps were particularly more focused on the hippocampus than Guided Grad-CAM and LRP heatmaps, consistent with well-supported biomarkers for Alzheimer\u2019s disease.\nZeineldin et al. [73] compared seven popular gradient-based explanation methods: gradients, Smoothgrad, integrated gradients, guided backpropagation (GBP), gradient-weighted class activation map (Grad-CAM), Guided Grad-CAM, and Guided Integrated Gradients for MRI image classification and segmentation tasks. For the brain glioma classification task, Guided Grad-CAM (i.e., combining GBP with GCAM) produced better localization, while Smoothgrad provided the best discriminative regions of the input. For the segmentation task, Smoothgrad was found to be the best choice because of its robustness to noise, while\nGCAM did the best visualization as it identified the most discriminative regions. Refer to Figure 8 for the heatmaps generated using the popular gradient-based methods to explain the predictions made by the brain glioma classification model."
        },
        {
            "heading": "10.1.2 Modified/Relevance Backpropagation",
            "text": "Layer-wise Relevance Propagation DeepLight [70] proposed a DL model consisting of recurrent (LSTM) and convolutional elements to analyze the whole-brain activity associated with cognitive states. Each whole-brain volume is sliced into a set of axial\nimages to feed into the convolutional and recurrent units. To generate post hoc explanations, DeepLight uses LRP (Layer-wise Relevance Propagation) [124]. The model was trained to predict four different cognitive states corresponding to four stimulus classes (seeing body parts, faces, places, or tools). The baselines used to assess the effectiveness were General Linear Model, Searchlight Analysis, and Whole-Brain Least Absolute Shrinkage Logistic Regression. The model takes each brain volume and then passes through a combination of convolutional and recurrent DL elements to predict the volume corresponding cognitive state. Along the time dimension, it produces a sequence of predictions, one for each sample time point. LSTM here is indeed used for learning spatial dependency within and across the brain slices. After each prediction for each brain volume, the LRP method is used to generate a post hoc explanation for that prediction attributing relevance to the voxel levels. LRP was used only for the correct predictions. The overall accuracy was around 68.3% on the held-out dataset. The validation or evaluation of the quality of the maps was achieved through a meta-analysis of the four cognitive states using an established cognitive state-brain association database called NeuroSynth. For relevance analysis, relevance volumes corresponding to a cognitive state were smoothened and averaged to produce a subject-level relevance map. Group-level map for each cognitive state was then produced by averaging subject-level maps relative to a cognitive state and generated for all the subjects in the test set. Figure 9 shows a comparison of group-level maps generated using the DeepLight approach and other baseline approaches. The authors used the meta-analysis with the NeuroSynth database and identified several ROIs associated with each cognitive state. For example, the upper parts of the middle and inferior temporal gyrus, the postcentral gyrus, and the right fusiform gyrus were associated with the body state, the fusiform gyrus and amygdala were associated with the face state, the parahippocampal gyrus was associated with the place state and the upper left middle and inferior temporal gyrus and the left postcentral gyrus with the tool state. Only the top 10% relevance values were considered for this comparison. While all other baseline approaches were able to identify the association of brain activity to the stimulus classes, the DeepLight model along with LRP was more accurate in finding the association of brain activity to the cognitive states, maintaining greater consistency with the meta-analysis results.\nFigure 10 shows the spatio-temporal distribution of brain activity within the first experiment block corresponding to two cognitive states: place and tool. Particularly, it demonstrates the distribution of group-level relevance values as a function of fMRI sampling time. As we can observe, while DeepLight was initially uncertain about the cognitive state, its confidence improves very quickly and the relevance maps gradually become steadily similar to the target maps from the NeuroSynth meta-analysis. However, the brain maps of the whole-brain lasso analysis showed very low similarity (F1 score) and did not vary noticeably over time, and hence they have a less meaningful association.\nEitel et al. [213] investigated the possibility of layer-wise relevance propagation (LRP) to uncover the rationale behind decisions made by 3D convolutional neural networks (CNNs) trained to diagnose multiple sclerosis (MS). The identified features revealed that CNN, in conjunction with LRP, has the potential to identify relevant imaging biomarkers, for example, individual lesions, lesion location, non-lesional white matter, or gray matter areas. These biomarkers are considered established MRI markers in MS literature.\nBohle et al. [216] used LRP to explain the decisions of a CNN model. They used a scalable brain atlas [217] and defined two metrics, \"relevance density\" and \"relevance gain,\" for objective assessment of the heatmaps. The key reason behind using LRP rather than gradient-based methods is that LRP decomposes the output in terms of contributions in the input space. As the authors mentioned, LRP has the potential to answer this question \u2014 \"what speaks for AD in this particular patient?\" where explanations using gradient-based approaches apparently address the following question: \"which change in voxels would change the outcome most?\" We argue that these two questions are not mutually exclusive. For a comparison of LRP with gradient-based methods, the authors used \"guided-backpropagation.\" While both LRP and GB were successful in localizing important regions, GB, compared to LRP, showed less contrast in importance scores between group-wise (AD vs. HCs) heatmaps. Fortunately, there are other gradient-based methods (e.g., integrated gradients [119] and smoothgrad [126] on integrated gradients) with desirable properties that future studies may consider for further investigation.\nSeveral studies have attempted to learn from different modalities. For example, Zhao et al. [107] proposed a hybrid deep learning architecture to combine sequential temporal dynamics (TCs) and functional\ndependency (FNCs). The authors used an attention module on top of C-RNN to extract temporal dynamic dependencies from TCs and used LRP to identify the most group-discriminative FNC patterns. Please note that LRP was used in a post hoc manner for the analysis of FNC patterns, not as part of the learning process.\nHofmann et al. [72] proposed ensembles of convolutional neural networks with LRP to identify which neural features contribute most to brain age. The models were acceptably accurate and could capture aging at both small and large-scale changes. Refer to Figure 11 for the visual explanations. The models\nwere also able to identify associated risk factors in case of diverging brain age. The study detected three major brain components (gray matter, white matter, and cortical spinal fluids) whose relevance scores were linearly correlated to the function of age. The authors argued in favor of ensemble models because the variability of predictions between different models, even when they have the same architecture and are trained on the same data, may arise because of the high variance and bias of individual models. Multiple studies have recommended aggregation of saliency maps generated from single base models [72, 220]. LRP, similar to other prevailing explanation methods, cannot inform us anything about the underlying biological mechanisms justifiable for the generated explanations.\nDeepLIFT, Deconvnet and Deep Taylor Decomposition Gupta et al. [103] proposed a 5-layer feed-forward neural network to perform three different classification tasks based on functional connectivity features computed for 264 anatomically and functionally diverse ROIs selected from resting-state fMRI: classifying cognitively control subjects from AD and MCI patients. Also, this study performed a harder task of classifying MCI patients from AD patients. For identifying salient regions associated with the predictions, DeepLIFT was used to generate explanations. The resulting explanations computed via DeepLIFT were evaluated via a recursive feature elimination process (10% every time) and retraining the model (5-layer feedforward network) using only the relevant subset of features. For each of the classification tasks, the retrained models achieved higher accuracy compared to the original performance, even with the reduced salient features.\nDyrba et al. [75] performed a comparison of explanations generated using popular interpretability methods for a 3D CNN model. Precisely, the study compared six interpretability methods: Deconvnet, gradient \u2299 input, Deep Taylor Decomposition, LRP, Grad-CAM, and Guided Backpropagation. The key observations from this study reveal that the modified backpropagation methods like Deep Taylor Decomposition and LRP could produce clinically useful explanations as they were focused, and aligned with the previous domain reports of the disorders. However, some of the standard backpropagation-based methods, such as Grad-CAM and Guided Backpropagation, are more scattered and did not support the domain knowledge expectedly. The obvious limitations of the study are that the model was not evaluated on an independent dataset. Also, the study does not include a quantitative validation for the generated explanations."
        },
        {
            "heading": "10.2 Perturbation-based Methods",
            "text": "Occlusion Sensitivity Abrol et al. [65] experimented with a modified deep ResNet to predict the progression to AD. While the main focus was to predict the progression from MCI class to AD class, the study also experimented with eight combinations of binary, mixed-class (based on transfer learning), and multi-class diagnostic and prognostic tasks. The authors also leveraged network occlusion sensitivity to identify the anatomical regions that were most predictive for the progression of MCI to AD. In the analysis, thirteen brain regions, including the middle temporal gyrus, cerebellum crus 1, precuneus, lingual gyrus, and calcarine, consistently emerged in the top 20 most relevant regions. As the occlusion sensitivity method considers only the output score drop due to occlusion of a defined region and does not consider connectivity among regions, the method does suffer from several limitations as pointed out by [80, 87]. The authors also projected the features from the first fully-connected layer onto a 2-dimensional space using t-SNE [236] to demonstrate the separability of the learned representations.\nIt is hypothesized that plaque morphologies can serve as a guide to understanding AD progression and associated pathophysiology. To this end, Tang et al. [224] proposed a six-layer convolutional architecture with two dense layers model trained based on whole slide images (WSIs) for the classification of amyloidbeta (A\u03b2) plaques. The authors also provided interpretations of the model decisions using deep learning\nmodel introspection techniques. As claimed in the report, the generated explanations aligned with the prior results of A\u03b2 pathology. Apart from different predictive performance estimates, the authors also investigated the interpretability of the model. This study also used two complementary model introspection methods\u2014Guided Grad-CAM and occlusion sensitivity\u2014to demonstrate that the models focused on relevant neuropathological features. As indicated, while Guided Grad-CAM is useful in identifying salient regions responsible for predictions, feature occlusion can reveal the interdependence of class-specific features. Figure 12 shows examples of how \u201cocclusion sensitivity\u201d and other competing gradient-based approaches were used to explain CNN model predictions classifying Alzheimer\u2019s disease (AD) patients from normal controls (NC).\nMeaningful Perturbation Meaningful perturbation intuitively attempts to find the important brain regions responsible for the prediction. To do this, it removes certain brain regions and expects a change in model prediction if that region was responsible for the prediction in the first place. However, determining the perturbation nature is not straightforward in the medical domain and can drastically change the input distribution and hence may not be meaningful. Uzunova et al. [134] proposed a method that generates the meaningful perturbation of the original images. As such, the method produces their closest healthy equivalent using variational autoencoders (VAE) [237]. The VAE indeed learns healthy variability of the disorder under consideration. Precisely, as shown in Eq 8, for the deletion mask m : \u039b \u2192 [ 0, 1] and each pixel u \u2208 \u039b with a value m(u), the deletion can be defined using VAE as follows:\n[ \u03a6(x0;m)] (u) = m(u)x0(u) + (1\u2212m(u))fVAE(x0) (19)\nAs VAE was trained based on only healthy subjects, the assumption here is that the VAE knows only the distribution of the healthy subjects in the latent z space. So, the reconstruction from pathological\nimages during test time will produce the nearest healthy equivalent. As shown in the patient vs. control classification of retinal OCT images, neural networks do not always learn pathological or expected features. In this case, the classifier captured the difference between two different datasets\u2014one for patients, and one for controls. Similar to previously reported results, the study reported Grad-CAM to have low resolution and Guided Backpropagation to be noisy. For the quantitative evaluation, the study used ground truth segmentation information. While no explanation method produced satisfactory explanations, VAE-based perturbation outperformed constant and blur perturbation in meaningful perturbation setups. Figure 13 shows the explanations and the use cases for this explantation technique for a classifier designed for retinal OCT images."
        },
        {
            "heading": "10.3 Counterfactual",
            "text": "Oh et al. [129] proposed an approach that combines model training, model counterfactual explanation, and model reinforcement as a unified learn-explain-reinforcement (LEAR) framework. After model training, a visual counterfactual explanation generates hypothetical abnormalities in the normal input image to be identified as a patient. The authors hypothesized that this counterfactual map generator when assisted by a diagnostic model can be a source of important generalized information about the disorder and the model can benefit from this. These counterfactual maps guide an attention-based module to refine the features\nuseful for more efficient model training and representative for the disorder in consideration. For quantitative validation, ground-truth maps were created based on the clinical diagnosis of the longitudinal samples, and normalized correlation coefficients were calculated. The counterfactual maps for the targeted labels obtained using the proposed method were compared against the counterfactual reasoning ability of popular interpretability methods, such as LRP-Z, DeepLIFT, Deep Taylor Decomposition, Integrated Gradients, Guided Backpropagation, and Grad-CAM. While only guided backpropagation, integrated gradients, and deep Taylor show some counterfactual reasoning ability, all the post hoc methods were not designed to generate counterfactual explanations. Rather, they were intended to generate explanations for the natural predictions of the model. The most interesting thing about this study is that the counterfactual maps from the control class to the MCI class and from the MCI class to the AD class add up to the counterfactual map from the control class to the AD class, indicating the disorder-specific consistency of the method. Moreover, the method is worth-investigating because the framework can be applied to any backbone diagnostic model. The LEAR framework also improves the post hoc interpretability of the model as demonstrated using CAM. However, the framework has only been evaluated on the ADNI dataset and it needs to be validated on independent datasets. Moreover, the cross-correlation measure for quantitative evaluation may be a weak measure of interpretability. Figure 14 shows the explanations based on gradient-based approaches for why the subject was diagnosed as AD by the model and the corresponding counterfactual map based on the proposed generative approach. As shown and assessed with respect to the ground-truth control (CN) map, the counterfactual explanation detects and highlights the ventricle enlargement and cortical atrophies, aligning the result with previous reports."
        },
        {
            "heading": "10.4 Distillation Methods",
            "text": "Local Interpretable Model-agnostic Explanations (LIME) Magesh et al. [61] leveraged the VGG16 network pretrained on ImageNet dataset to classify Parkinson\u2019s patients from healthy controls. The study also used LIME to explain individual predictions. The underlying reason for choosing this explanation method is unclear. Furthermore, while quantitative validation is an essential measure of the predictability of the heatmaps, the study did not conduct any experiments to validate the generated explanations objectively.\nSaboo et al. [62] developed a deep learning model for cognition prediction over five years after the baseline using clinical and imaging features and leveraged model explainability to identify certain brain structures responsible for cognitive vulnerability or cognitive resilience. Specifically, it identifies those predictive brain structures (medial temporal lobe, fornix, and corpus callosum) that best explain the heterogeneity between cognitively vulnerable and resilient subjects. To this end, the study used LIME to compute the contributions of each imaging and clinical feature toward the predicted future cognitive score. While LIME generated contributions of features matched with some prior reports, earlier literature blamed LIME for producing unstable contributions over multiple runs. Moreover, like most of the ongoing interpretability practices, this study also did not justify the rationale of using LIME.\nGaur et al. [64] proposed an explanation-driven dual-input CNN-based solution to predict the status of brain tumors using brain MRI scans. The authors adopted both SHAP and LIME to generate explanations because SHAP captures the consistency and accuracy in the explanations and LIME captures the local behavior of the model around the test example. The study reported a predictive training accuracy of 94.64% as the performance of the model, which is not the usual practice of reporting model performance. Furthermore, while the study used two post hoc interpretability methods (LIME and SHAP), validation of the generated explanations was not provided to support the study. The explainability was not used as part of the model training, and hence we argue that the model is inappropriately referred to as \"explanation-driven.\" Rather, the concept of XAI was used to analyze the model predictions in a post hoc manner, and not to enhance the model\u2019s performance.\nSHapley Additive exPlanations (SHAP) Ball et al. [225] used three different machine learning approaches to estimate brain age based on cortical development as the variations in brain age and chronological age have been linked to many psychiatric disorders. The authors also used kernel SHAP to identify which features explain errors (brain age delta) in brain age prediction. These explanations are consistent across the models and previously reported brain development regional patterns. However, no generic spatial association among individual feature estimates is found for the brain age prediction error. That is, given the similar demographics and prediction error, feature importance estimates between subjects may vary widely. Moreover, brain age delta estimates for any of the models did not associate noticeably with cognitive performance. Given the limitations of SHAP in estimating feature importance and the lack of any objective validation for these explanations, this study needs further investigation.\nLombardi et al. [56] segmented the T1-weighted brain into 68 cortical regions and 40 sub-cortical regions using atlases to extract morphological features from the scans. The authors developed a fully connected network and analyzed the feature attribution performance of two model-agnostic interpretability methods: LIME and SHAP. To assess the predictive performance and intra-consistency of the interpretability methods, the model was trained multiple times using different subsets of the training fold. SHAP, compared to LIME, provided greater intra-consistency of feature attribution scores implying that the method is comparatively robust to the training set variations. Inter-similarity analysis across the subjects reveals that SHAP values can be better partitioned into different age ranges. Furthermore, compared to LIME, feature attribution scores from SHAP were highly correlated with brain age and the features were more aligned with the previous reports of morphological association with brain age. However, the obvious limitation of this study was that the model was trained based on a smaller cohort of samples with a limited age range. Furthermore, a quantitative validation approach is required to objectively compare the performance of XAI methods. Figure 15 shows how LIME and SHAP were used to identify brain regions associated with the chronological age of the subjects."
        },
        {
            "heading": "10.5 Intrinsic Methods",
            "text": "Very few neuroimaging studies so far have considered interpretability as part of the algorithmic aspect of the model from its inception. Such models in the literature are called glass-box or transparent-box models. Biffi et al. [221] proposed a deep generative model for transparent visualization of the classification space. Some other neuroimaging studies [238, 239] considered interpretable models based on their design transparency. We discuss some studies that utilized the concept of intrinsic interpretability as part of the model design or training.\nAttention Mechanism Lian et al. [227] proposed an attention-guided unified framework that uses a convolutional neural network to work as the task-specific guidance and a multi-branch hybrid network to perform disease diagnosis. The main motivation behind this work is to extract multilevel information at interpersonal, individual, local, and global scales. The first fully convolutional network offers guidance via disease attention maps and thus assists in generating global and individual features. In the second stage, the unified framework leverages disease attention maps (DAMs) calculated using class activation maps. In the second stage, the framework passes the DAMs over different branches to capture patch-level and global-level information for actual classification. The AD classification task was designed from the scratch, whereas the MCI progression prediction model was built based upon the learned parameters of the AD classification model. Compared to the existing competing deep learning approaches, this framework offers little performance improvement. The proposed framework identified different parts of the hippocampus, frontal lobe, fusiform gyrus, amygdala, and ventricle in association with AD or MCI. The discriminative power of these regions has been supported by earlier studies conducted for Alzheimer\u2019s disease. One of the main limitations of this work is that DAMs cannot be considered as explanations for the final predictive model. Because, a smaller part of the predictive\nmodel was pre-trained earlier to generate the DAMs and subsequently guide the predictive model for the intended classification task. Moreover, DAMs as generated using class activation maps are high in semantics but low in resolution, which may not reflect the fine details of the discriminative regions in the input space.\nJin et al. [110] proposed a deep learning model combining ResNet and a 3D attention network (3DAN) to integrate the identification of discriminative regions and the classification of AD into a unified framework. The proposed model can diagnose Alzheimer\u2019s and simultaneously capture imaging biomarkers responsible for the predictions. As reported, the 3DAN provides a strong association between the model output and the clinical features of AD and MCI. The study claimed the biomarkers (attention maps) to be generalizable, reproducible, and neurobiologically plausible. The study validated these claims by demonstrating strong correlations of the attention maps between datasets, between the mean attention score and the T-map, between the attention score for the regions and the MMSE scores based on the Brainnetome Atlas, and between classification accuracy and the mean attention score of K groups of regions. The attention mechanism was able to produce generalizable and reproducible results as the findings correlated across the datasets. Figure 16 shows various aspects of the analyses for the effectiveness of attention maps in capturing significant brain regions associated with the progression of AD.\nJoint Training Zhu et al. [94] combined interpretable feature learning and dynamic graph learning modules into the Graph Convolutional Network module. These three modules are jointly optimized to provide improved diagnosis and interpretability simultaneously via learning only the essential features. This study used feature summarization and used gray matter volumes from 90 ROIs as features. Interpretability of the diagnosis was conducted based on the values of the learned weights of each region. The predictive performance of the top regions was higher compared to other feature selection methods. Moreover, the proposed method consistently identified the middle temporal gyrus right, hippocampal formation left, precuneus left, and uncus left associated with AD, which aligned with the previous reports. While this validation seems plausible, it is unclear if the underlying classification model for this validation across the feature selection methods was the same or different. Furthermore, uninterpretable parameter sensitivity is another concern, which caused a significant drop in predictive performance.\nBass et al. [66] proposed a VAE-GAN-based approach, called ICAM (Interpretable Classification via Disentangled Representations and Feature Attribution Mapping), to learn a shared class-relevant attribute latent space, that is simultaneously suitable for classification, and feature attribution. It can also inform the difference within and between classes. The authors argued that the post hoc methods are sub-optimal in finding all the discriminative regions of a class and hence not suitable for medical imaging. Instead, this work hypothesized that generative models are more useful to capture class-relevant features. As demonstrated in the results, ICAM\u2019s latent attribute space achieved greater discriminative power compared to other approaches. Furthermore, the attribution maps generated using this approach have increased correlations with the ground truth disease maps compared to other popular post hoc methods. Figure 17 shows comparative feature maps generated using ICAM, VA-GAN, and other post-hoc approaches.\nModel Transparency Qiu et al. [98] utilized multimodal inputs such as MRI and other clinical features (age, gender, and Mini-mental State Examination score) to propose an interpretable deep learning framework for Alzheimer\u2019s disease. The proposed framework improves predictive performance for disease diagnosis and identifies disease-specific neuroimaging signatures. As part of the architecture, MRI sub-volumes are passed to a fully convolutional neural network, and patient-specific probability maps of the brain are generated. As estimated by the probability maps, the high-risk voxels are passed to a fully connected network for classification. Despite site-specific distinctions among the datasets, the study was able to demonstrate performance consistency across the datasets when training was conducted based on a single cohort. The study further provided neuropathological and neurologist-level validation. For the neuropathological validation, the high-probability brain regions were closely associated with the locations and frequencies of amyloid-\u03b2 and tau pathologies. Figure 18 demonstrates how the extracted disease probability maps correspond to\nthe post-mortem findings of neuropathology examinations. For the neurologist-level validation, eleven neurologists conducted diagnoses based on the same multimodal inputs, and the average performance was compared to the model\u2019s performance. The model is transparent in the sense that it is able to directly produce disease probability maps from the scans, which is indicative of the disease-specific brain regions. While the preliminary results are promising, the model only considered the subpopulation of binary scenarios, subjects with AD and normal controls, and does not consider progressive stages. Hence, the model is still not directly applicable to the clinical decision-making process.\nRavi et al. [97] proposed a 4D-Degenerative Adversarial NeuroImage Net (4D-DANINet) for generating realistic 3D brain images over time. 4D-DANINet is a modular framework based on adversarial training and a set of spatiotemporal and biological constraints. It can generate subject-specific longitudinal scans that reflect disease stage and age. The key motivation behind this work is to provide sufficient realistic samples for model validation and efficient model training. The main components of the DANINet are as follows:\n\u2022 Conditional deep encoder: This module is a combination of an encoder that embeds each slice to a\nlatent space z and a generator that generates samples from this latent space conditioned on diagnosis and age. This module is trained based on reconstruction loss that minimizes the difference between the actual slice and the projected slice in time.\n\u2022 Discriminator networks: This module uses two discriminators Db and Dz. The Db discriminates\nbetween real and simulated brain images and the generator (G) generates realistic synthetic images to fool the Db. The encoder (E) produces embeddings from a uniform distribution to ensure smooth temporal progression. To do this, the discriminator Dz is adversarially trained with the encoder (E).\n\u2022 Biological constraints: The 4D-DANNet imposes two separate voxel-level and region-level losses Lvox\nand Lreg to capture smooth intensity changes reflecting disease progression over time.\n\u2022 Profile weight functions dynamically determine appropriate weights for the losses as required for\nefficient training.\nThe ablation study demonstrated the significance of training consistency (TC), super-resolution (SR), and transfer learning (TR) blocks as adopted in the framework to produce realistic synthetic MRI images. Figure 19 shows how different components of the proposed framework have played their respective roles in producing realistic synthetic MRI images. However, the model may produce less effective brain images due to poorly representative training sets, and cohort differences."
        },
        {
            "heading": "10.6 Feature Map Visualization",
            "text": "Biffi et al. [221] proposed a hierarchical deep generative model called ladder variational autoencoder (LVAE). LVAE learns a hierarchy of conditional latent variables to represent the population of anatomical segmentations. The latent space representation in the highest level of the hierarchy can efficiently discriminate clinical conditions. The proposed model performed two classification tasks: 1) Hypertrophic cardiomyopathy (HCM) versus healthy 3D left ventricular (LV) segmentations and 2) AD versus healthy control 3D hippocampal segmentations. The model was predictive of clinical conditions and offered suitable visualization and quantification of the anatomical shape changes associated with those clinical conditions. This study used sampling in the highest latent space to visualize the corresponding regions in the brain space. The authors further claimed that the shape changes, as evident in the visualization, agreed with the clinical literature.\nMartinez-Murcia et al. [222] used a deep CNN autoencoder for an exploratory data analysis of AD.\nThe autoencoder demonstrates links between cognitive symptoms and the underlying neurodegenerative process. The autoencoder model uses a data-driven approach to extract imaging characteristics into lowdimensional manifolds. The study further used regression analysis to show that the neurons in the manifold space correlate well with the clinical and neuropsychological test outcomes and diagnoses. Subsequently, the authors used a novel visualization approach using a linear decomposition model to show the brain regions highly influenced by each manifold coordinate, which provides additional information about the association between structural degeneration and the cognitive decline of dementia. Figure 20 shows the brain regions that affect 14th neuron (the most correlated with disease progression) in the z-space using a linear decomposition method. The regression model for different test clinical variables, such as ADAS-13, ADAS-11, Age, TAU, etc., was trained with GM maps.\nAs brain dynamics change far before the changes happen in the anatomical structure, functional neuroimaging is more powerful to understand brain disorders [226]. Parmar et al., 2020 [226] proposed a modified 3D CNN that directly works on the 4D resting-state fMRI data for a multiclass classification task to diagnose different stages of Alzheimer\u2019s disease. The network achieved very high accuracy (93%) with only 30 subjects per class. However, the data augmentation using temporal patches may prevent the\nspatio-temporal characteristics of the underlying dynamics. The study also showed the temporal features extracted from the first two convolutional layers as network activation maps. As the temporal features move from lower to higher layers, the authors reported that discriminative regions of interest gradually take a definitive structure. The obvious limitation of this study is the lack of interpretability. Moreover, the model was trained and evaluated on only one dataset."
        },
        {
            "heading": "11 The Usage Trend of Interpretability Methods",
            "text": "While the neuroimaging community has used a larger collection of interpretability methods, only a few are popular and considered important for knowledge discovery or potential clinical deployment. Several interpretability methods have often been used as experimental baselines, not for their beneficial effects in this domain. In this section, we conducted an in-depth analysis of the usage of all popular interpretability methods in neuroimaging studies. Indeed, we investigated the usage of these methods in more than 300 neuroimaging papers and observed their usage trend as shown in Table 2. As we found in our exploratory analysis, studies\nhave used the methods in the following order of frequency: 1) CAM/Grad-CAM/Grad-CAM++/Guided Grad-CAM [127, 139, 140] 2) SHapley Additive exPlanations (SHAP) [120] 3) Integrated Gradients [119] 4) Layer-wise Relevance Propagation [124] 5) Occlusion Sensitivity [116] 6) Guided Backpropagation [125] 7) Local Interpretable Model-Agnostic Explanations (LIME) [121] 8) Gradients [117, 118] 9) DeepLIFT [122] and 10) Smoothgrad [126] This usage trend also reveals that preference for \"gradients\" and \"guided backpropagation\" methods are receiving less attention because of their limitations [126, 155], while the steeper rise in the use of integrated gradients and SHAP are potentially due to their strong theoretical foundations."
        },
        {
            "heading": "12 Suggestions for Interpretable Models in Neuroimaging",
            "text": "In this section, we discuss the significant pitfalls of interpretability research in neuroimaging. One of the obvious concerns in interpretable deep learning models is that DL models are capable of learning in numerous ways for the same input-output relationship [23] and in most cases the hidden factors are not interpretable. Hence, the real challenge is to verify if the model learned from the true evidence or relied on some unintended spurious correlations [44, 541, 542] that could be entirely unknown to the humans. As such, explanations vary widely among architectures, model initializations, and interpretability methods.\nModel architecture is also a major consideration. As different neural networks may assign different regions as important for predictions, most of them would tell about different aspects of the disorder because\nof their very nature of different computations performed during training. Combining explanations from different models and further analysis of these explanations in association with medical experts may be useful in revealing undiscovered aspects of the disease. To this end, a unified framework [32] in interpretable neuroimaging research may be useful so that the findings across the studies can be directly compared to share advancement benchmarks. Based on our analysis and review, we recommend that we may focus on the following directions for useful DL-based understanding of the disorders: 1) Objective quantification of the explanation method\u2019s performance, 2) Investigation of the explanation sensitivity to interpretability parameters, 3) Understanding if the generated explanations point to any causality or underlying mechanism of the disorder, 4) Investigating the reliability of the underlying model via model debugging 5) Combining various aspects revealed using multiple approaches, multiple model initializations and model ensembles. This is important because even when models use \"true\" evidence, explanations and their relative importance may be different. So combining them in a faithful manner can reveal useful unknown insights for the disorders. As Rieke et al. [215] pointed out that different visualization methods (gradients or non-gradient approaches) vary widely, so in line with other earlier studies, we suggest investigating multiple methods instead of blindly relying on one interpretability method.\nWe also should be careful while choosing a particular interpretability method. For example, while many earlier studies used the occlusion sensitivity method to generate explanations, Yang et al., 2018 [87] pointed out several limitations of the approach. For example, this approach uses semantically meaningless neighborhoods and an unspecified way of choosing the grid size. Moreover, the method is computationally very intensive. As no backpropagation from the target score is involved during heatmap generation, this explanation is considered to be limited [80]. Also, 3D-Grad-CAM can be useful if we need to track the attention of the convolution layers, but Grad-CAM or CAM is not useful for generating explanations in the input space and hence not suitable for data interpretation. While LRP has been used extensively, it has inherent limitations. LRP cannot maintain implementation invariance as it uses modified back-propagation rules. For future interpretability practices, we leave the following suggestions for the neuroimaging community:\n1. Be aware of shortcut learning: Deep learning models are very prone to fall into the trap of\nshortcut learning because it always looks for the easiest possible solution for the problem [44]. The understanding of the influence of model architecture, training data, loss function, optimization parameters may reveal the nature of shortcuts the model may learn and thus preclude the possibility for model deployment in clinical practices or for guiding further discovery. Moreover, expert knowledge in the neuroimaging domain may help identify these undesirable behaviors.\n2. High accuracy does not necessarily indicate higher human comprehensibility: A highly\naccurate model does not necessarily mean the model is more interpretable or relies on correct hidden features. While it is always preferable for clinicians, doctors, and scientists to find the true underlying factors for a model\u2019s prediction, it is very unlikely that model would rely on the same set of features every time it starts with new initialization weights. As indicated by Hinton in [23], with the deep\nlearning models, it is not trying to identify the correct \u201chidden factors\u201d responsible for a particular diagnosis. Instead, DL models could rely on a different set of hidden factors in the data to model the relationship between input and output variables. Hence, a highly accurate model may not have intelligible interpretations for humans. Refer to Figure 3 and Figure 21 to get ideas that explanations can immensely vary for different models even when the predictions remain the same.\n3. Design strong out-of-distribution (o.o.d) tests: While the model may perform well on the i.i.d\ntest samples, it may not generalize well for the real-world datasets because of associated distribution shifts [44]. Hence, for successful deployment, we must design suitable o.o.d tests as suggested by Geirhos et al. [44]. While it may be less hard for designing those tests for some problem domains, building strong o.o.d test cases may not be so easy for neuroimaging. However, multi-cohort datasets may help mitigate this problem.\n4. Check for stability in generated explanations: Even if the model architecture, training data,\nand other hyperparameters remain same, each time the model is refitted starting from different initialization, the model may learn a very different set of features [23] which may or may not be desirable. This is indeed a very similar effect which occurs for the o.o.d cases, where a distribution-wise different test sample may end up with an untended behavior. This instability issue for the same model architecture, on the other hand, may happen during training time for different initializations.\n5. We need to be aware of the fragility of neural network interpretations: The fundamental\nproblem with the popular interpretability methods is their robustness [543]. Ghorbani et al. [543] showed that interpretations based on feature importance maps such as DeepLIFT, integrated gradients,\nand influence functions are susceptible to adversarial attacks. Put another way, a systematic perturbation of the input can lead to a very different interpretation (heatmap) because of the complexity of input feature space in deep neural networks. In neuroimaging, earlier studies, so far we are aware, usually overlooked this fragility of the interpretations, which may lead to misleading interpretations. Moreover, there is an inherent human bias to trust the model as correct and look for interpretations only based on predictive performance. While model inspection or debugging can be a hard problem in neuroimaging, it should be an essential consideration for this safety-critical domain.\n6. Lack of any guiding principle to select explanation methods: While studies have leveraged\ndifferent explanation methods for deep learning models, there is little theoretical evidence or guiding principle to choose a method for a particular study. Recently, Han et al. [25] demonstrated how different explanation methods describe different neighborhoods and thus produce different explanations. Some disagreement scenarios are common because there could be differences in the underlying aspects the methods are investigating. For example, permutation importance [544] and SHAP [120, 545] in case of model overfitting may produce very different explanations. However, some disagreement scenarios are not expected. For example, gradients and LIME should produce similar interpretations because they both focus on local neighborhoods. However, in practice, they produce very different explanations. The authors in [25] also showed how some methods cannot recover the underlying model and are entirely independent. The authors also provided valuable suggestions on choosing interpretability methods based on the nature of the data. They further suggested building an explanation method for the data for which no explanation method from the literature is considered beneficial.\n7. Post hoc methods are blamed for being insufficient: As post hoc methods heavily rely on\nthe models they are applied to, the methods can only discover the minimal discriminative parts sufficient for the prediction. For example, while LRP and GBP have been shown to be able to identify homogeneous brain regions, e.g., the hippocampus, they cannot identify heterogeneous regions, e.g., cortical folds [68, 216].\n8. Attribution normalization and polarity considerations varied widely: For the post-processing\nof different explanations, studies use an ad-hoc approach. There has yet to be an agreement on how to post-process the heatmaps. This agreement must correspond to the underlying model and the interpretability method used. This necessity of the agreement is especially applicable to gradient-based attribution methods. Studies used the sign information differently to finalize the heatmaps. As the distribution of the values in the explanation maps generated using different methods varies widely, the need for agreed upon normalization has been a open research question [75].\n9. Studies generally use an ad-hoc approach to validate explanations: For the validation\nof results, studies generally use informal and unreliable ways. Sometimes they used intuitions, hypotheses, and earlier results to justify the current attributions. These validation techniques are very susceptible and may end up with misleading conclusions. As Levakov et al. [220] indicated,\nany reasonable conclusions regarding the contributions should be made based on common parts of the maps from multiple models. Furthermore, deep learning models usually capture complex hierarchical and multivariate interactions. Localizing the brain regions should only be considered as an approximation of the significance. Even a small architectural modification can be a significant determinant of model performance and feature attribution maps, as indicated by Lin et al. [78].\n10. Validate explanations based on their predictability and expert evaluation: While RAR [13]\nand ROAR [138] evaluations of the salient regions is promising and may further enhance the trust in the significance of what the model has learned, it may still need to be guaranteed that the model did not rely on spurious correlations. The domain experts should confirm the validation of the interpretations. Equivalently the explanations must match a significant proportion of the expertextracted knowledge. We suggest complementing quantitative validation with neuro-scientifically valid explanations.\n11. Use structure-function fusion model for model diagnosis: Earlier studies, in general, inde-\npendently focused on the anatomical or functional aspects of the dynamics. However, using both modalities simultaneously and corresponding existing knowledge in each modality during explanation generation may provide rigorous validation and bring trust in the explanations.\n12. Counterfactuals may reveal the underlying biological mechanism: Wachter et al. [53] first\nintroduced counterfactual explanations to know about the hypothetical reality that could alter the model\u2019s decision. Dandi et al. [55] refined the formulation to satisfy the different practical desiderata of counterfactual explanations to make them useful in real-world applications. In the context of neuroimaging, we believe countefactual explanations may help understand the underlying biological mechanism that potentially caused the specific disorder in the first place. To our knowledge, no neuroimaging study has ever used counterfactuals to understand the model\u2019s decision-making process.\n13. Layer-wise Relevance Propagation (LRP) needs further investigation: As seen from the\ninterpretability in neuroimaging literature, LRP has been widely used, and its popularity is on an upward trend. However, the explanations produced by LRP are not reliable. Indeed, Shrikumar et al. [122] showed a strong connection between LRP and gradient\u2299 input, especially when all the activations are piecewise linear as in ReLU or Leaky ReLU. Ancona et al. [136] also showed that \u03f5-LRP is equivalent to the feature-wise product of the input and the modified partial derivative. Kindermans et al. [546] showed that DeConvNet, Guided BackProp, and LRP cannot produce the theoretically correct explanation even for a linear model\u2014the most straightforward neural network.\n14. SHAP is popular, but it should not be trusted blindly: SHAP, though very popular in the\nXAI community, has some issues. For example, SHAP assumes that the features are independent, while they are very unlikely. While features may be correlated, the algorithm may generate unrealistic observations (instances) with permutations. Moreover, no explanation method produces explanations that imply causality. SHAP indicates the importance of a feature based on the model prediction,\nnot the importance in the real world. Humans are very prone to confirmation bias. It is not very uncommon that humans tend to create narratives as a result of confirmation bias. The most important question is: Did the model learn to predict for the right reasons? This question is vital because machine learning models do not know about truths, and it only cares about correlations, and proxy or secondary or less important variables may be loosely or tightly correlated with the actual cause. They can be revealed as very important features. Moreover, Kwon and Zou [547] recently showed that SHAP is suboptimal in that it gives the same weight to all marginal contributions for a feature xi, which may potentially lead to attribution mistakes if different marginal contributions have different signal and noise. The authors further proposed a simple modification of the original SHAP, called WeightedSHAP, that estimates the weights automatically from the data.\n15. Studies generally focused only on classification and regression tasks: While many studies\nin interpretable deep learning models for general classification tasks exist, further subgrouping into patient subtypes or clustering is still a novel area. This lack of interpretability literature for clustering tasks is equally true for neuroimaging and other domains. Very few studies did projection transformation from the latent space to observe the area of influence [221, 222].\n16. Effectiveness of transfer learning in neuroimaging needs justification: what causes the\nincreased accuracy? What knowledge does it transfer? Raghu et al. [548] showed that transfer learning from natural images to medical images did help little with performance. Instead, as the authors surmised, the slight improvement may come from the over-parameterization of the standard models trained on natural images. Moreover, studies are not certain about the aspects of knowledge they are transferring from the natural image domain to the medical image domain or from one disorder area to another."
        },
        {
            "heading": "13 Conclusion",
            "text": "This article comprehensively introduces the problem of interpretability for AI models and thus offers a field guide for future AI practitioners in the neuroimaging domain. In the earlier sections, we discussed the philosophical ground, dimensions, methods, and desirable axiomatic properties of model interpretability for reliable knowledge discovery. We also provide a useful taxonomy that directly points to all the major interpretability approaches and their use cases in many neuroimaging studies. We further discuss different sanity tests and evaluation metrics required to justify the validity of the explanations generated by any post hoc method. In the later sections, we discussed how deep learning approaches have been used widely in recent neuroimaging studies. Indeed, we performed an in-depth analysis of usage trends of the most prevailing interpretability methods. We reckon that these analyses will be helpful for future neuroimaging practitioners looking for ideas of how scientists are using these approaches for novel discoveries and how model interpretability is changing the course of neuroimaging studies in recent years. Lastly, we discuss\ndifferent caveats of interpretability practices and provide insights on how this specialized sub-field of AI can be used wisely and meaningfully for better diagnosis, prognosis, and treatment of brain disorders."
        }
    ],
    "title": "Looking deeper into interpretable deep learning in neuroimaging: a comprehensive survey",
    "year": 2023
}