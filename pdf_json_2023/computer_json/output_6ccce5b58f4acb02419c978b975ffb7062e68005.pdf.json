{
    "abstractText": "In this paper, we focus on the task of 3D shape completion from partial point clouds using deep implicit functions. Existing methods seek to use voxelized basis functions or the ones from a certain family of functions (e.g., Gaussians), which leads to high computational costs or limited shape expressivity. On the contrary, our method employs adaptive local basis functions, which are learned end-to-end and not restricted in certain forms. Based on those basis functions, a local-to-local shape completion framework is presented. Our algorithm learns sparse parameterization with a small number of basis functions while preserving local geometric details during completion. Quantitative and qualitative experiments demonstrate that our method outperforms the state-of-the-art methods in shape completion, detail preservation, generalization to unseen geometries, and computational cost. Code and data are at https://github.com/yinghdb/Adaptive-Local-Basis-Functions. \u2217Corresponding author.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hui Ying"
        },
        {
            "affiliations": [],
            "name": "Tianjia Shao"
        },
        {
            "affiliations": [],
            "name": "He Wang"
        },
        {
            "affiliations": [],
            "name": "Kun Zhou"
        }
    ],
    "id": "SP:7089df6af930944d9cd937f4b9e5e33fa47bee7c",
    "references": [
        {
            "authors": [
                "Tristan Aumentado-Armstrong",
                "Stavros Tsogkas",
                "Sven Dickinson",
                "Allan D Jepson."
            ],
            "title": "Representing 3D Shapes with Probabilistic Directed Distance Fields",
            "venue": "CVPR. 19343\u201319354.",
            "year": 2022
        },
        {
            "authors": [
                "Nicolas Carion",
                "FranciscoMassa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko."
            ],
            "title": "End-to-end object detection with transformers",
            "venue": "ECCV. Springer, 213\u2013229.",
            "year": 2020
        },
        {
            "authors": [
                "Rohan Chabra",
                "Jan E Lenssen",
                "Eddy Ilg",
                "Tanner Schmidt",
                "Julian Straub",
                "Steven Lovegrove",
                "Richard Newcombe."
            ],
            "title": "Deep local shapes: Learning local sdf priors for detailed 3d reconstruction",
            "venue": "ECCV. Springer, 608\u2013625.",
            "year": 2020
        },
        {
            "authors": [
                "Angel X Chang",
                "Thomas Funkhouser",
                "Leonidas Guibas",
                "Pat Hanrahan",
                "Qixing Huang",
                "Zimo Li",
                "Silvio Savarese",
                "Manolis Savva",
                "Shuran Song",
                "Hao Su"
            ],
            "title": "Shapenet: An information-rich 3d model repository",
            "year": 2015
        },
        {
            "authors": [
                "Chao Chen",
                "Yu-Shen Liu",
                "Zhizhong Han."
            ],
            "title": "Latent partition implicit with surface codes for 3D representation",
            "venue": "arXiv preprint arXiv:2207.08631 (2022).",
            "year": 2022
        },
        {
            "authors": [
                "Weikai Chen",
                "Cheng Lin",
                "Weiyang Li",
                "Bo Yang."
            ],
            "title": "3PSDF: Three-Pole Signed Distance Function for Learning Surfaces with Arbitrary Topologies",
            "venue": "CVPR. 18522\u2013 18531.",
            "year": 2022
        },
        {
            "authors": [
                "Zhiqin Chen",
                "Andrea Tagliasacchi",
                "Thomas Funkhouser",
                "Hao Zhang."
            ],
            "title": "Neural dual contouring",
            "venue": "ACM Transactions on Graphics (TOG) 41, 4 (2022), 1\u201313.",
            "year": 2022
        },
        {
            "authors": [
                "Zhiqin Chen",
                "Hao Zhang."
            ],
            "title": "Learning implicit fields for generative shape modeling",
            "venue": "CVPR. 5939\u20135948.",
            "year": 2019
        },
        {
            "authors": [
                "Zhang Chen",
                "Yinda Zhang",
                "Kyle Genova",
                "Sean Fanello",
                "Sofien Bouaziz",
                "Christian H\u00e4ne",
                "Ruofei Du",
                "Cem Keskin",
                "Thomas Funkhouser",
                "Danhang Tang."
            ],
            "title": "Multiresolution deep implicit functions for 3d shape representation",
            "venue": "ICCV. 13087\u2013 13096.",
            "year": 2021
        },
        {
            "authors": [
                "Julian Chibane",
                "Thiemo Alldieck",
                "Gerard Pons-Moll."
            ],
            "title": "Implicit functions in feature space for 3d shape reconstruction and completion",
            "venue": "CVPR. 6970\u20136981.",
            "year": 2020
        },
        {
            "authors": [
                "Christopher B Choy",
                "Danfei Xu",
                "JunYoung Gwak",
                "Kevin Chen",
                "Silvio Savarese."
            ],
            "title": "3d-r2n2: A unified approach for single and multi-view 3d object reconstruction",
            "venue": "ECCV. Springer, 628\u2013644.",
            "year": 2016
        },
        {
            "authors": [
                "Angela Dai",
                "Angel X. Chang",
                "Manolis Savva",
                "Maciej Halber",
                "Thomas Funkhouser",
                "Matthias Nie\u00dfner."
            ],
            "title": "ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes",
            "venue": "Proc. Computer Vision and Pattern Recognition (CVPR), IEEE.",
            "year": 2017
        },
        {
            "authors": [
                "Angela Dai",
                "Charles Ruizhongtai Qi",
                "Matthias Nie\u00dfner."
            ],
            "title": "Shape completion using 3d-encoder-predictor cnns and shape synthesis",
            "venue": "CVPR. 5868\u20135877.",
            "year": 2017
        },
        {
            "authors": [
                "Yu Deng",
                "Jiaolong Yang",
                "Xin Tong."
            ],
            "title": "Deformed implicit field: Modeling 3d shapes with learned dense correspondence",
            "venue": "CVPR. 10286\u201310296.",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805 (2018).",
            "year": 2018
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition",
            "year": 2020
        },
        {
            "authors": [
                "Kyle Genova",
                "Forrester Cole",
                "Avneesh Sud",
                "Aaron Sarna",
                "Thomas Funkhouser."
            ],
            "title": "Local deep implicit functions for 3d shape",
            "venue": "CVPR. 4857\u20134866.",
            "year": 2020
        },
        {
            "authors": [
                "Kyle Genova",
                "Forrester Cole",
                "Daniel Vlasic",
                "Aaron Sarna",
                "William T Freeman",
                "Thomas Funkhouser."
            ],
            "title": "Learning shape templates with structured implicit functions",
            "venue": "ICCV. 7154\u20137164.",
            "year": 2019
        },
        {
            "authors": [
                "Xiaoguang Han",
                "Zhen Li",
                "Haibin Huang",
                "Evangelos Kalogerakis",
                "Yizhou Yu."
            ],
            "title": "High-resolution shape completion using deep neural networks for global structure and local geometry inference",
            "venue": "ICCV. 85\u201393.",
            "year": 2017
        },
        {
            "authors": [
                "Christian H\u00e4ne",
                "Shubham Tulsiani",
                "Jitendra Malik."
            ],
            "title": "Hierarchical surface prediction for 3d object reconstruction",
            "venue": "3DV. IEEE, 412\u2013420.",
            "year": 2017
        },
        {
            "authors": [
                "Amir Hertz",
                "Or Perel",
                "Raja Giryes",
                "Olga Sorkine-Hornung",
                "Daniel Cohen-Or."
            ],
            "title": "SPAGHETTI: Editing Implicit Shapes Through Part Aware Generation",
            "venue": "arXiv preprint arXiv:2201.13168 (2022).",
            "year": 2022
        },
        {
            "authors": [
                "Ka-Hei Hui",
                "Ruihui Li",
                "Jingyu Hu",
                "Chi-Wing Fu."
            ],
            "title": "Neural Template: TopologyAware Reconstruction and Disentangled Generation of 3D Meshes",
            "venue": "CVPR. 18572\u2013 18582.",
            "year": 2022
        },
        {
            "authors": [
                "Chiyu Jiang",
                "Avneesh Sud",
                "Ameesh Makadia",
                "Jingwei Huang",
                "Matthias Nie\u00dfner",
                "Thomas Funkhouser"
            ],
            "title": "Local implicit grid representations for 3d scenes",
            "year": 2020
        },
        {
            "authors": [
                "Tianyang Li",
                "Xin Wen",
                "Yu-Shen Liu",
                "Hua Su",
                "Zhizhong Han."
            ],
            "title": "Learning deep implicit functions for 3D shapes with dynamic code clouds",
            "venue": "CVPR. 12840\u201312850.",
            "year": 2022
        },
        {
            "authors": [
                "Or Litany",
                "Alex Bronstein",
                "Michael Bronstein",
                "Ameesh Makadia."
            ],
            "title": "Deformable shape completion with graph convolutional autoencoders",
            "venue": "CVPR. 1886\u20131895.",
            "year": 2018
        },
        {
            "authors": [
                "Minghua Liu",
                "Lu Sheng",
                "Sheng Yang",
                "Jing Shao",
                "Shi-Min Hu."
            ],
            "title": "Morphing and sampling network for dense point cloud completion",
            "venue": "AAAI, Vol. 34. 11596\u201311603.",
            "year": 2020
        },
        {
            "authors": [
                "Kirill Mazur",
                "Victor Lempitsky."
            ],
            "title": "Cloud transformers: A universal approach to point cloud processing tasks",
            "venue": "ICCV. 10715\u201310724.",
            "year": 2021
        },
        {
            "authors": [
                "Lars Mescheder",
                "Michael Oechsle",
                "Michael Niemeyer",
                "Sebastian Nowozin",
                "Andreas Geiger."
            ],
            "title": "Occupancy networks: Learning 3d reconstruction in function space",
            "venue": "CVPR. 4460\u20134470.",
            "year": 2019
        },
        {
            "authors": [
                "Paritosh Mittal",
                "Yen-Chi Cheng",
                "Maneesh Singh",
                "Shubham Tulsiani."
            ],
            "title": "Autosdf: Shape priors for 3d completion, reconstruction and generation",
            "venue": "CVPR. 306\u2013315.",
            "year": 2022
        },
        {
            "authors": [
                "Luca Morreale",
                "Noam Aigerman",
                "Vladimir G Kim",
                "Niloy J Mitra."
            ],
            "title": "Neural surface maps",
            "venue": "CVPR. 4639\u20134648.",
            "year": 2021
        },
        {
            "authors": [
                "Jeong Joon Park",
                "Peter Florence",
                "Julian Straub",
                "Richard Newcombe",
                "Steven Lovegrove."
            ],
            "title": "Deepsdf: Learning continuous signed distance functions for shape representation",
            "venue": "CVPR. 165\u2013174.",
            "year": 2019
        },
        {
            "authors": [
                "Niki Parmar",
                "Ashish Vaswani",
                "Jakob Uszkoreit",
                "Lukasz Kaiser",
                "Noam Shazeer",
                "Alexander Ku",
                "Dustin Tran."
            ],
            "title": "Image transformer",
            "venue": "ICML. PMLR, 4055\u20134064.",
            "year": 2018
        },
        {
            "authors": [
                "Despoina Paschalidou",
                "Angelos Katharopoulos",
                "Andreas Geiger",
                "Sanja Fidler."
            ],
            "title": "Neural parts: Learning expressive 3d shape abstractions with invertible neural networks",
            "venue": "CVPR. 3204\u20133215.",
            "year": 2021
        },
        {
            "authors": [
                "Songyou Peng",
                "Michael Niemeyer",
                "Lars Mescheder",
                "Marc Pollefeys",
                "Andreas Geiger."
            ],
            "title": "Convolutional occupancy networks",
            "venue": "ECCV. Springer, 523\u2013540.",
            "year": 2020
        },
        {
            "authors": [
                "Charles Ruizhongtai Qi",
                "Li Yi",
                "Hao Su",
                "Leonidas J Guibas."
            ],
            "title": "Pointnet++: Deep hierarchical feature learning on point sets in a metric space",
            "venue": "NIPS 30 (2017).",
            "year": 2017
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog 1,",
            "year": 2019
        },
        {
            "authors": [
                "Jason Rock",
                "Tanmay Gupta",
                "Justin Thorsen",
                "JunYoung Gwak",
                "Daeyun Shin",
                "Derek Hoiem."
            ],
            "title": "Completing 3d object shape from one depth image",
            "venue": "CVPR. 2484\u2013 2493.",
            "year": 2015
        },
        {
            "authors": [
                "Bo Sun",
                "Vladimir G Kim",
                "Noam Aigerman",
                "Qixing Huang",
                "Siddhartha Chaudhuri."
            ],
            "title": "PatchRD: Detail-Preserving Shape Completion by Learning Patch Retrieval and Deformation",
            "venue": "arXiv preprint arXiv:2207.11790 (2022).",
            "year": 2022
        },
        {
            "authors": [
                "Towaki Takikawa",
                "Joey Litalien",
                "Kangxue Yin",
                "Karsten Kreis",
                "Charles Loop",
                "Derek Nowrouzezahrai",
                "Alec Jacobson",
                "Morgan McGuire",
                "Sanja Fidler."
            ],
            "title": "Neural geometric level of detail: Real-time rendering with implicit 3D shapes",
            "venue": "CVPR. 11358\u201311367.",
            "year": 2021
        },
        {
            "authors": [
                "Maxim Tatarchenko",
                "Stephan R Richter",
                "Ren\u00e9 Ranftl",
                "Zhuwen Li",
                "Vladlen Koltun",
                "Thomas Brox."
            ],
            "title": "What do single-view 3d reconstruction networks learn",
            "venue": "CVPR. 3405\u20133414.",
            "year": 2019
        },
        {
            "authors": [
                "Edgar Tretschk",
                "Ayush Tewari",
                "Vladislav Golyanik",
                "Michael Zollh\u00f6fer",
                "Carsten Stoll",
                "Christian Theobalt."
            ],
            "title": "Patchnets: Patch-based generalizable deep implicit 3d shape representations",
            "venue": "ECCV. Springer, 293\u2013309.",
            "year": 2020
        },
        {
            "authors": [
                "Greg Turk",
                "James F O\u2019Brien"
            ],
            "title": "Modelling with implicit surfaces that interpolate",
            "venue": "TOG 21,",
            "year": 2002
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "NIPS",
            "year": 2017
        },
        {
            "authors": [
                "Zhizhong Han"
            ],
            "title": "Snowflakenet: Point cloud completion by snowflake point",
            "year": 2021
        },
        {
            "authors": [
                "Yuting Xiao",
                "Jiale Xu",
                "Shenghua Gao"
            ],
            "title": "deconvolution with skip-transformer",
            "year": 2022
        },
        {
            "authors": [
                "Wenxiu Sun"
            ],
            "title": "Grnet: Gridding residual network for dense point cloud",
            "year": 2020
        },
        {
            "authors": [
                "Hui Huang"
            ],
            "title": "Shapeformer: Transformer-based shape completion via sparse",
            "year": 2022
        },
        {
            "authors": [
                "Xumin Yu",
                "Yongming Rao",
                "Ziyi Wang",
                "Zuyan Liu",
                "Jiwen Lu",
                "Jie Zhou"
            ],
            "title": "geometry codes learning with sdf",
            "venue": "Pointr:",
            "year": 2021
        },
        {
            "authors": [
                "Zhou",
                "Connelly Barnes",
                "Jingwan Lu",
                "Jimei Yang",
                "Hao Li"
            ],
            "title": "StyleGAN for 3D Shape Generation",
            "venue": "In CGF,",
            "year": 2019
        },
        {
            "authors": [
                "Hui Ying",
                "Tianjia Shao",
                "He Wang",
                "Yin Yang",
                "Kun Zhou"
            ],
            "title": "A IMPLEMENTATION DETAILS A.1 Networks Details In Tab. 6, we show the detailed network architecture of the PointNet++ encoder",
            "venue": "[Qi et al",
            "year": 2017
        },
        {
            "authors": [
                "ShapeFormer [Yan"
            ],
            "title": "2022], and 3DILG",
            "venue": "[Zhang et al. 2022],",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "\u2217Corresponding author.\nKEYWORDS shape completion, deep implicit functions, adaptive local basis functions"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "3D Shape completion from partially scanned point clouds has been widely studied due to its importance to various applications such as automatic driving, augmented reality, and robotics. Naturally, one needs to rely on certain schemes to represent the 3D shapes we want to complete such as point clouds [Liu et al. 2020; Mazur and Lempitsky 2021; Wang et al. 2022; Xiang et al. 2021; Xie et al. 2020; Yuan et al. 2018], deformable meshes [Litany et al. 2018; Rock et al. 2015], and voxels [Choy et al. 2016; Dai et al. 2017b; Han et al. 2017; H\u00e4ne et al. 2017]. On the downside, those classic representations also exhibit several intrinsic limitations. For instance, a point cloud often needs extra post-processing; the deformable template mesh may not fit the topology of target object; while processing voxel-based shapes is much more expensive. Deep implicit functions or DIFs have recently attracted more attention, which have been proven to be highly effective for the completion of 3D ar X\niv :2\n30 7.\nobjects [Genova et al. 2020, 2019; Mescheder et al. 2019; Park et al. 2019].\nTraditionally, an implicit function is regarded as a weighted combination of multiple basis functions [Turk and O\u2019Brien 2002; Walder et al. 2006]. Those basis functions and the associated weights can be computed with respect to an individual geometry. In the context of deep learning, a DIF encodes an input observation using a latent vector z and adopts a network-based embodiment to estimate the function value \ud835\udc53 (x, z) for a given 3D query location x. Most existing DIF methods follow this modality but use different choices of implicit functions (i.e., either as a global basis function ormultiple local basis functions) andweightingmechanisms, which collectively determine the expressivity of the DIF.\nEarly DIF methods [Genova et al. 2019; Mescheder et al. 2019; Park et al. 2019] estimate a signed distance or an occupancy function utilizing a single latent code, with a global basis function. This representation is later proven to be limited in describing complex shapes [Chibane et al. 2020; Genova et al. 2020]. Therefore, researchers switched to localized basis functions for shape completion by dividing the whole shape into multiple regions and regionconditioned latent codes. One line of research is to discretize the space into a regular voxel grid and embed the local latent codes in the voxels [Chen et al. 2021; Chibane et al. 2020]. While being able to achieve the state-of-the-art results in shape completion, the required grid resolution leads to a significant growth of computational cost. Alternatively, adaptive parameterization is sought for more compact representations (LDIF) [Genova et al. 2020], which learns to decompose a shape into a collection of overlapping regions represented by 3D Gaussian basis functions. In each region a latent code is assigned to learn a residual coefficient function for the Gaussian basis function to produce details. Despite the impressive results in shape completion, as the final shape is based on a mixture of refined Gaussians, it inherently has a limited capacity to capture full details, and therefore can still miss geometric details in difficult cases (see Fig. 3 for example).\nIn this paper, we argue that the specific form of basis functions should be learnable without being restricted to a certain family of functions. Such basis functions can potentially bring multiple benefits. Since the basis functions are learnable and local, they are more likely to capture local fine details due to the data-driven nature. Because the center and the shape of basis functions are learnable, fewer basis functions can achieve equal or better representation for the same geometry compared to analytical functions. For this reason, we aim to learn arbitrarily shaped basis functions so that we can complete 3D shapes with more local details and lower computational costs.\nLearning basis functions for shape completion however is a challenging task. It is known that global DIFs can still miss local details [Chibane et al. 2020; Genova et al. 2020], and we would like to keep our learned functions local. Enforcing such locality is non-trivial during the training process. In addition, unlike learning DIFs from full observations [Chabra et al. 2020; Jiang et al. 2020; Li et al. 2022; Yao et al. 2021], we only have partial observations in shape completion tasks, which impose further difficulties. To this end, our method leverages a progressive, observed-to-unobserved process. We first encode the visible shape as a sequence of local basis functions and then use them to predict the basis functions in\nthe missing region in a sequence-to-sequence manner. This strategy tends to preserve the fine details for the visible area while learning the correlations between the visible and missing parts.\nWe address the locality problem by learning the function domain of each basis. Following the intuition that points located near a basis should be more likely to be inside its domain, we adopt the Radial Basis Function (RBF) kernels with learnable parameters to parameterize the domain. In our implementation, the RBF-based domains and DIF-based basis functions are learned jointly in an endto-end manner for a compact shape representation and preserved local details. Based on such shape formulations, we build the shape completion pipeline in two main steps. As shown in Fig. 1, the first step is to map the partial input points into a collection of local bases which encode the visible shape compactly with details. In the second step, we predict the local bases for the missing areas and refine the local bases of the visible areas by adopting Transformer encoders [Vaswani et al. 2017]. The self-attention mechanism of Transformers mimics the pairwise interaction between local bases, thus enabling the accurate sequence-to-sequence translation among visible local bases and missing local bases.\nTo summarize, our main contributions include the following aspects. First, we propose DIF-based local basis functions for effective and efficient shape representation, which can capture fine details with a small number of local basis functions. Second, we introduce a local-to-local shape completion pipeline, which is both efficient and geometry-rich. Experiments demonstrate that our method outperforms previous state-of-the-art methods by a large margin."
        },
        {
            "heading": "2 RELATEDWORK",
            "text": ""
        },
        {
            "heading": "2.1 Deep Implicit Shape Representation",
            "text": "A large amount of learning-based methods has achieved promising results using implicit shape representation. With the strength of deep learning, neural networks serve as a powerful tool to fit various implicit functions, such as signed/unsigned distance fields [Park et al. 2019; Venkatesh et al. 2020], occupancy indicator functions [Chen and Zhang 2019; Mescheder et al. 2019; Peng et al. 2020], deformation functions [Deng et al. 2021; Hui et al. 2022; Paschalidou et al. 2021] or other specifically defined implicit functions [Aumentado-Armstrong et al. 2022; Chen et al. 2022a; Morreale et al. 2021].\nPioneering works such as OccNet [Mescheder et al. 2019], IMNet [Chen and Zhang 2019], and DeepSDF [Park et al. 2019] show that many simple shapes can be represented by a latent code and the corresponding deep implicit function. However, such deep representations often fail to capture local geometries for more complex shapes. Recent works overcome the problem by focusing on the localized basis functions. Some methods divide the 3D space into voxel grids and assign each voxel with a latent code [Chabra et al. 2020; Jiang et al. 2020], while some store the latent codes in the grid points [Chen et al. 2021; Chibane et al. 2020; Peng et al. 2020] (or octree [Takikawa et al. 2021]) and interpolates them for query points within the voxel. Then the local basis functions are learned separately for each voxel and all local bases are combined for the final shape reconstruction. While state-of-the-art results can be achieved, increased resolution yields a significant growth in the number of codes, resulting in high computational costs.\nIn addition to the grid-based DIFs, some methods [Chen et al. 2022b; Genova et al. 2020, 2019; Hertz et al. 2022; Li et al. 2022; Tretschk et al. 2020; Xiao et al. 2022; Yao et al. 2021; Zhang et al. 2022] seek to formulate the local bases with irregular positions. SIF [Genova et al. 2019] decomposes a shape into a collection of overlapping regions represented by 3D Gaussian basis functions, and LDIF [Genova et al. 2020] learns adaptive weights with DIFs for further refinement. LGCL [Yao et al. 2021] samples a set of key points and divide the 3D space into local regions based on Euclidean distance, within which it learns a DIF for local shape representation. Some other methods [Chen et al. 2022b; Li et al. 2022] store the latent codes in multiple irregularly distributed key points, and the new code is interpolated from these codes. The advantage of these methods is that less computation is used to represent a complex shape. In our method, we follow this strategy but propose a novel formulation such that the DIF-based local bases are learned along with the combining weights so as to capture more details."
        },
        {
            "heading": "2.2 Shape Completion",
            "text": "Recently, neural networks have been used to predict the whole shape from partial input with the help of data priors. The shape completion methods can be classified according to the output representations, such as voxels, meshes, point clouds, and deep implicit functions. Voxel-based methods [Choy et al. 2016; Dai et al. 2017b; Han et al. 2017; H\u00e4ne et al. 2017; Sun et al. 2022] can directly generate output data thanks to 3D convolution networks, but the memory and time costs are too high when dealing with high-resolution shape grids. And mesh-based methods [Litany et al. 2018; Rock et al. 2015] are hard to handle shapes with arbitrary topology. Therefore, a mass of methods [Liu et al. 2020; Mazur and Lempitsky 2021; Wang et al. 2022; Xiang et al. 2021; Xie et al. 2020; Yuan et al. 2018] focus on performing shape completion in point clouds which do not have those problems. But usually what we want are the mesh outputs rather than the point clouds. Other popular methods recently for shape completion is using DIF [Genova et al. 2020, 2019; Mescheder et al. 2019; Park et al. 2019]. For preserving details and using the convenient 3D convolution, most methods [Chen et al. 2021; Chibane et al. 2020; Mittal et al. 2022; Yan et al. 2022; Zheng et al. 2022] employ grid-based features to process data and express the implicit function of the output shape. ShapeFormer [Yan et al. 2022], AutoSDF [Mittal et al. 2022] and SDF-StyleGAN [Zheng et al. 2022] propose to model the shape completing as a generative task which aims to generate a series of voxelized latent codes for representing the complete shape. However, due to the large amount of latent codes and the use of 3D convolution, these methods suffer the problem of high computational cost. SIF [Genova et al. 2019] and LDIF [Genova et al. 2020] perform 2D convolutions on the input partial depth map(s) to extract the features which encode the whole shape. But their Gaussian-based local bases limit their expression capacity of arbitrary shapes. Without the above issues, our shape completion method utilizes DIF-based local bases with arbitrary shapes to preserve better details and avoid the use of 3D convolution to consume lower computation."
        },
        {
            "heading": "2.3 Transformers",
            "text": "Transformer [Vaswani et al. 2017] is a powerful framework for sequence-to-sequence translation tasks, which has been proved useful in natural language processing [Devlin et al. 2018; Radford et al. 2019] and image processing [Carion et al. 2020; Dosovitskiy et al. 2020; Parmar et al. 2018]. Most recently, a number of methods [Mittal et al. 2022; Yan et al. 2022; Yu et al. 2021] model the shape completion as a sequence-to-sequence task by taking advantage of Transformers. PoinTr [Yu et al. 2021], as a point cloud completing method, uses the Transformer encoder-decoder architecture to predict point proxies for missing parts. ShapeFormer [Yan et al. 2022] and AutoSDF [Mittal et al. 2022], as implicit-function based methods, use the Transformer-based autoregressive model to predict the complete shape conditioned on the partial inputs. In our method, we utilize the Transformer encoder to model the dependencies among the visible and missing parts and predict the local bases for complete shape representation."
        },
        {
            "heading": "3 IMPLICIT FIELD FORMULATION",
            "text": "A surface can be described as an SDF and represented implicitly as {x|\ud835\udc53\ud835\udf19 (x, z) = 0}, where \ud835\udc53\ud835\udf19 (x, z) can be implemented by a neural network with learnable parameters \ud835\udf19 . Unlike previous methods which assume a global latent code z [Park et al. 2019], we represent the SDF as a weighted sum of multiple local basis functions. Each DIF \ud835\udc53\ud835\udf19 (x \u2212 \ud835\udf41\ud835\udc56 , z\ud835\udc56 ), or \ud835\udc53\ud835\udc56 (x) for simplicity, is defined with a center position \ud835\udf41\ud835\udc56 , and a latent code z\ud835\udc56 is used for expressing the local SDF. For a given query point x \u2208 R3, its final signed distance is decided by a linear combination of \ud835\udc41 DIF-based basis functions with weights \ud835\udefc\ud835\udc56 ,\n\ud835\udc60\ud835\udc51 \ud835\udc53 (x) = \u2211\ufe01\n\ud835\udc56\u2208[\ud835\udc41 ] \ud835\udefc\ud835\udc56 \ud835\udc53\ud835\udf19 (x \u2212 \ud835\udf41\ud835\udc56 , z\ud835\udc56 ),\n\ud835\udefc\ud835\udc56 = \ud835\udc54(x \u2212 \ud835\udf41\ud835\udc56 ,A\ud835\udc56 )\u2211\n\ud835\udc57\u2208[\ud835\udc41 ] \ud835\udc54(x \u2212 \ud835\udf41 \ud835\udc57 ,A\ud835\udc57 ) ,\n\ud835\udc54(x \u2212 \ud835\udf41\ud835\udc56 ,A\ud835\udc56 ) = exp (\u2212||A\ud835\udc56 (x\ud835\udc56 \u2212 \ud835\udf41\ud835\udc56 ) | |22),\n(1)\nwhere \ud835\udc54(x\u2212\ud835\udf41\ud835\udc56 ,A\ud835\udc56 ), or \ud835\udc54\ud835\udc56 (x) for simplicity, is an RBF function with learnable parameter A\ud835\udc56 . A\ud835\udc56 is a linear transform matrix, which is constructed by the product of a scaling matrix S\ud835\udc56 and a 3D rotation matrix R\ud835\udc56 . Practically, S\ud835\udc56 is mapped from a 3-dimensional vector, and R\ud835\udc56 is mapped from a 6-dimensional vector as in [Zhou et al. 2019], in which these vectors are predicted from networks directly. Eq. 1 naturally encourages sparsity through \ud835\udefc\ud835\udc56 , which increases exponentially when x is close to \ud835\udf41\ud835\udc56 , but quickly becomes damped when x moves away from \ud835\udf41\ud835\udc56 .\nFor complex shapes, one may still need to use many basis functions to capture local shape variations, and \ud835\udefc\ud835\udc56 alone is insufficient to guarantee the sparsity. To this end, we require each \ud835\udc53\ud835\udf19 only parameterizes a local neighborhood around it. As a result, only a small number of \ud835\udc53\ud835\udf19 s contribute the actual value of \ud835\udc60\ud835\udc51 \ud835\udc53 (x) for a given x. In our experiments, we found that only two nearest \ud835\udc53\ud835\udf19 s to xwill give reasonably good results. Let \ud835\udc5d and \ud835\udc5e be the indices of two largest RBFs \ud835\udc54\ud835\udc56 (x), and \ud835\udc60\ud835\udc51 \ud835\udc53 (x) becomes the linear combining the local bases with these two indices (i.e., replacing [\ud835\udc41 ] with {\ud835\udc5d, \ud835\udc5e} in Eq. 1)."
        },
        {
            "heading": "4 COMPLETION PIPELINE",
            "text": "As shown in Fig. 2, we first encode the input partial points into a series of local bases as the shape representation for the visible area (see Sec. 4.1), and then utilize the power of Transformers [Vaswani et al. 2017] to generate the whole local bases (see Sec. 4.2), which can further be optimized for better results (see Sec. 4.3)."
        },
        {
            "heading": "4.1 Compact Point Cloud Encoding",
            "text": "As shown in the orange dashed box in Fig 2, the PointNet++ encoder [Qi et al. 2017] serves to downsample and encode input points into \ud835\udc41\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61 center points with coordinates ?\u0302?\ud835\udc56 and embeddings e\u0302\ud835\udc56 . Then Multi-Layer Perceptrons (MLPs) are used to decode the embeddings into latent codes z\u0302\ud835\udc56 and domain parameters A\u0302\ud835\udc56 which together with the centers ?\u0302?\ud835\udc56 form the local bases for the shape representation.\nIn the PointNet++ encoder, key points are sampled uniformly from the input points. Such sampling is unnecessary. Ideally, regions with complex geometry should be densely sampled while regions with simple geometry should be sparsely sampled, as the domain of the local basis will be smaller for complex geometry and larger for simple geometry. Therefore, we propose an adaptive downsampling strategy based on the predicted domains of local bases after the uniform sampling in PointNet++ encoder. The detailed downsampling algorithm is shown in Alg. 1. \ud835\udc54\ud835\udc56 (\ud835\udf41 \ud835\udc57 ) can be regarded approximately as the the probability that the center of \ud835\udc57-th local basis is inside the domain of the \ud835\udc56-th local basis. So the higher \ud835\udc60 ( \ud835\udc57) implies the higher probability that the domain of \ud835\udc57-th local basis can be covered by the other local bases, so we eliminate the key point with the highest \ud835\udc60 ( \ud835\udc57) sequentially. Note that after eliminating one key point, all the other \ud835\udc60 ( \ud835\udc57) needs to be updated.\nALGORITHM 1: Domain-based Downsampling Let \ud835\udc46 = {\ud835\udc60 ( \ud835\udc57) = \u2211\ud835\udc56\u2208[\ud835\udc41\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61 ],\ud835\udc56\u2260\ud835\udc57 \ud835\udc54\ud835\udc56 (\ud835\udf41 \ud835\udc57 )} \ud835\udc57\u2208[\ud835\udc41\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61 ] ; Let \ud835\udc47 = [\ud835\udc41\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61 ] be the reserved indices; for \ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5f = 1 to \ud835\udc41\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61 \u2212 \ud835\udc41\ud835\udc63\ud835\udc56\ud835\udc60 do\nFind the max \ud835\udc60 (\ud835\udc58) in \ud835\udc46 ; Eliminate \ud835\udc58 from \ud835\udc47 , and \ud835\udc60 (\ud835\udc58) from \ud835\udc46 ; Update \ud835\udc60 ( \ud835\udc57) = \ud835\udc60 ( \ud835\udc57) \u2212 \ud835\udc54\ud835\udc58 (\ud835\udf41 \ud835\udc57 ) for \ud835\udc60 ( \ud835\udc57) \u2208 \ud835\udc47 ;\nend\nIn order to learn the compact encoding for the input partial point cloud, we perform end-to-end training for partial shape representation with the following loss:\nL\ud835\udc60\ud835\udc51 \ud835\udc53 = 1 |X| \u2211\ufe01 x,y\u2208X,Y \ud835\udefc\ud835\udc5d |\ud835\udc53\ud835\udc5d (x) \u2212 y| + \ud835\udefc\ud835\udc5e |\ud835\udc53\ud835\udc5e (x) \u2212 y|, (2)\nwhere X and Y stand for the set of query points and target signed distances, and {\ud835\udc5d, \ud835\udc5e} are as described in Sec. 3. For partial point cloud encoding, the query points X for training are sampled near the input points. With the loss function, we want each local basis function \ud835\udc53\ud835\udc56 to accurately learn the local SDF function. And at the same time, the learning of \ud835\udefc\ud835\udc56 and \ud835\udc53\ud835\udc56 is adaptive: if |\ud835\udc53\ud835\udc56 (x) \u2212 y| is smaller, the corresponding \ud835\udefc\ud835\udc56 will be learned to be larger to reduce L\ud835\udc60\ud835\udc51 \ud835\udc53 since \ud835\udefc\ud835\udc5d + \ud835\udefc\ud835\udc5e = 1. This means that if a position can be more accurately described by a local basis function, it will be more likely in the local domain of the basis."
        },
        {
            "heading": "4.2 Whole Local Bases Prediction",
            "text": "In Sec. 4.1, we get \ud835\udc41\ud835\udc63\ud835\udc56\ud835\udc60 centers ?\u0302?\ud835\udc56 and embeddings e\u0302\ud835\udc56 which can be decoded into local bases for visible shape representation. Taking these results as input, we aim to predict more centers and embeddings for complete shape representation in two steps as shown in the green and blue dashed boxes in Fig. 2.\nMissing Centers Prediction. First, the \ud835\udc41\ud835\udc63\ud835\udc56\ud835\udc60 pairs of embeddings e\u0302\ud835\udc56 and centers ?\u0302?\ud835\udc56 , as well as one learnable query embedding \u00a4e, are input into Missing Centers Transformer, and the output is then fed to the MLP to get \ud835\udc41\ud835\udc5a\ud835\udc56\ud835\udc60\ud835\udc60 center points coordinates ?\u0304?\ud835\udc56 for the missing area. We supervise the prediction of missing centers with the following chamfer distance loss:\nL\ud835\udc50\u210e\ud835\udc4e\ud835\udc5a = 1 |M| \u2211\ufe01 \ud835\udf41\u2208M min \ud835\udf42\u2208N | |\ud835\udf41 \u2212 \ud835\udf42 | |2 + 1 |N | \u2211\ufe01 \ud835\udf42\u2208N min \ud835\udf41\u2208M | |\ud835\udf42 \u2212 \ud835\udf41 | |2,\n(3) where M is the set of \ud835\udc41\ud835\udc5a\ud835\udc56\ud835\udc60\ud835\udc60 predicted missing center point coordinates, and N is the set of target missing center point coordinates. The target centers are fetched by using Furthest Point Sampling (FPS) in the surface points of the missing region.\nWhole Local Bases Prediction. Local Bases Transformer takes as input \ud835\udc41\ud835\udc63\ud835\udc56\ud835\udc60 pairs of embeddings e\u0302\ud835\udc56 and centers ?\u0302?\ud835\udc56 for visible region, as well as \ud835\udc41\ud835\udc5a\ud835\udc56\ud835\udc60\ud835\udc60 pairs of query embeddings \u00a5e and centers ?\u0304?\ud835\udc56 for missing region, to predict the final \ud835\udc41\ud835\udc50\ud835\udc5c\ud835\udc5a\ud835\udc5d embeddings e\u0303\ud835\udc56 . Then the output embeddings are sent to MLPs to get latent codes z\u0303\ud835\udc56 and domain parameters A\u0303\ud835\udc56 for complete shape representation. Note that the \ud835\udc41\ud835\udc5a\ud835\udc56\ud835\udc60\ud835\udc60 query embeddings \u00a5e share the same parameter. In addition, center offsets ?\u0303?\ud835\udc56 are predicted and added to the centers ?\u0303?\ud835\udc56 for the reason of providing optimal positions for local basis functions.\nTo learn the whole local bases, we use four loss functions to supervise the parameters of whole local bases, which areL\ud835\udc51\ud835\udc5c\ud835\udc5a\n\ud835\udc60\ud835\udc51\ud835\udc53 ,L\ud835\udc52\ud835\udc62\ud835\udc50 \ud835\udc60\ud835\udc51 \ud835\udc53 ,\nL\ud835\udc60\ud835\udc5a\ud835\udc5c\ud835\udc5c\ud835\udc61\u210e , and L\ud835\udc5f\ud835\udc52\ud835\udc54 . L\ud835\udc51\ud835\udc5c\ud835\udc5a\ud835\udc60\ud835\udc51\ud835\udc53 has the same form and function as Eq. 2. However, only using L\ud835\udc51\ud835\udc5c\ud835\udc5a\n\ud835\udc60\ud835\udc51\ud835\udc53 will cause problems in learning missing\nlocal basis functions. As with prior input embeddings, visible local bases are learned more quickly than the missing ones so that their domains soon expand to a wide range that even covers the missing coordinates, making the weights \ud835\udefc\ud835\udc56 close to 0 for missing local basis functions. We address the problem by adding the loss function L\ud835\udc52\ud835\udc62\ud835\udc50 \ud835\udc60\ud835\udc51 \ud835\udc53\n, which simply discards the use of weights. The loss function L\ud835\udc60\ud835\udc5a\ud835\udc5c\ud835\udc5c\ud835\udc61\u210e is used to make the transition between two adjacent local bases smooth, and L\ud835\udc5f\ud835\udc52\ud835\udc54 is used for keeping the coordinate offsets ?\u0303?\ud835\udc56 small. The final loss functions are shown below,\nL\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52 = L\ud835\udc51\ud835\udc5c\ud835\udc5a\ud835\udc60\ud835\udc51\ud835\udc53 + L \ud835\udc52\ud835\udc62\ud835\udc50 \ud835\udc60\ud835\udc51 \ud835\udc53 + \ud835\udf061L\ud835\udc60\ud835\udc5a\ud835\udc5c\ud835\udc5c\ud835\udc61\u210e + \ud835\udf062L\ud835\udc5f\ud835\udc52\ud835\udc54, (4)\nL\ud835\udc60\ud835\udc5a\ud835\udc5c\ud835\udc5c\ud835\udc61\u210e = 1 |X| \u2211\ufe01 x,y\u2208X,Y |\ud835\udc53\ud835\udc5d (x) \u2212 \ud835\udc53\ud835\udc5e (x) |, (5)\nL\ud835\udc52\ud835\udc62\ud835\udc50 \ud835\udc60\ud835\udc51 \ud835\udc53 = 1 |X| \u2211\ufe01 x,y\u2208X,Y |\ud835\udc53\ud835\udc58 (x) \u2212 y|, L\ud835\udc5f\ud835\udc52\ud835\udc54 = 1 \ud835\udc41\ud835\udc50\ud835\udc5c\ud835\udc5a\ud835\udc5d \u2211\ufe01 \ud835\udc56\u2208[\ud835\udc41\ud835\udc50\ud835\udc5c\ud835\udc5a\ud835\udc5d ] |\ud835\udf39\ud835\udc56 |,\n(6)\nwhere \ud835\udf061 is 0.5, and \ud835\udf062 is 0.01 in the first epoch of training and 0.0 in the other epochs. \ud835\udc58 is defined as the index of the \ud835\udf41\ud835\udc56 which is closest to the query point x in Euclidean distance. X are the mixture of uniformly sampled points and the points sampled near the complete surface as in DeepSDF [Park et al. 2019]."
        },
        {
            "heading": "4.3 Post Optimization",
            "text": "From Sec. 4.2, we get a collection of local bases defined by latent codes z\ud835\udc56 , domain parameters A\ud835\udc56 and coordinates \ud835\udf41\ud835\udc56 to represent a complete shape. The represented shapes already have good quality (see Tab. 1 and Fig. 3), but they can be further optimized. In short, we optimize z\ud835\udc56 and \ud835\udf41\ud835\udc56 to minimize the following loss, where \ud835\udf061, \ud835\udf062, \ud835\udf063, \ud835\udf064 = 1.0, 10.0, 10.0, 0.1.\nL\ud835\udc5c\ud835\udc5d\ud835\udc61 = \ud835\udf061L\ud835\udc53 \ud835\udc4e\ud835\udc50\ud835\udc52 + \ud835\udf062L\ud835\udc5d\ud835\udc5c\ud835\udc60 + \ud835\udf063L\ud835\udc4e\ud835\udc51 \ud835\udc57 + \ud835\udf064L\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc4f\ud835\udc59\ud835\udc52 , (7) L\ud835\udc53 \ud835\udc4e\ud835\udc50\ud835\udc52 is used to guarantee the predicted signed-distance values of input points X\ud835\udc56\ud835\udc5b are close to 0. L\ud835\udc5d\ud835\udc5c\ud835\udc60 is used to ensure the signed distance value of sampled points, whose signed distance value is confirmed as positive, is larger than 0. L\ud835\udc4e\ud835\udc51 \ud835\udc57 is used to make the signed distance value change smoothly between two adjacent local bases, andL\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc4f\ud835\udc59\ud835\udc52 is used to keep the optimized parameters close to the original ones. More details are in the supplementary material."
        },
        {
            "heading": "5 EXPERIMENT",
            "text": "We execute a series of experiments to evaluate our method for 3D completion. By default, we use \ud835\udc41\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61 = 128, \ud835\udc41\ud835\udc63\ud835\udc56\ud835\udc60 = 64 and \ud835\udc41\ud835\udc5a\ud835\udc56\ud835\udc60\ud835\udc60 = 32, and test with unoptimized results.\nDataset. The experiments are run on the ShapeNet dataset [Chang et al. 2015]. We preprocess the shapes to make them water-tight following the instructions from Occupancy Networks [Mescheder et al. 2019]. We first generate the 224\u00d7224 depth scans of 16 random views around the objects. The input point clouds are fetched by reprojecting the 2D pixels and sampling within the 3D points using FPS. We set the number of input points\ud835\udc40 = 2048 in all our experiments. Except in the ablation study, we use the official training splits of 8 classes as [Yuan et al. 2018] for training, and for testing, we perform two kinds of experiments. One experiment is for the trained 8 classes where we use 3000 partial inputs randomly sampled from the testing splits of the 8 classes, and the other is for the unknown classes where we use 3000 partial inputs from 5 unseen classes. As for the ablation study, we use the chair class for training and testing. More details are in the supplementary material.\nMetrics. For the evaluation we use the metrics of Intersectionover-Union (IoU) [Mescheder et al. 2019], Chamfer L2 Distance (CD) [Mescheder et al. 2019] and F-score%1 (F1) [Tatarchenko et al. 2019]. If there is no notation, we sampled 100k points on each mesh surface for the computation of CD and F1.\nBaselines. We compare our method with the state-of-the-art implicit-function based 3D completion methods: IF-Net [Chibane et al. 2020], decoder-only DeepSDF [Park et al. 2019], LDIF [Genova et al. 2020], and ShapeFormer [Yan et al. 2022]. For a fair comparison, we set the number of shape elements in LDIF to be 96 which is the same as ours. For the training of ShapeFormer, in which the latent resolution is set to 163, the scaling augmentation is used, or it will fall into overfitting quickly. We also compare our method with\nPoinTr [Yu et al. 2021] and SnowflakeNet [Xiang et al. 2021] which are 3D point cloud completion methods that also use Transformer architectures. All these baseline methods use the same training data and testing data as ours. But the input formats are a little different for some methods. The input of LDIF is a scanned depth map which is the source of our partial point cloud, and the input of IF-Net is a 128 \u00d7 128 \u00d7 128 discrete occupancy grid from the input point cloud."
        },
        {
            "heading": "5.1 Results on Trained Classes",
            "text": "As shown in Tab. 1, our method achieves much better scores than other methods. The IoU and F1 scores of our method outperform the second place by 6.2% and 6.9%. We can find that DeepSDF is very difficult to restore the shapes because of the limitation of a single\nlatent code. IF-Net, LDIF, and ShapeFormer achieve better scores but are still not comparable with ours. PoinTr-NDC and SnowflakeNetNDC obtain slightly better scores in CD than ours as the target of PoinTr and SnowflakeNet is to minimize the chamfer distance, but they have lower scores in F1. An interesting phenomenon is that after optimization, our method achieves better scores in CD but worse scores in IoU and F1. It illustrates that the initial shapes directly predicted from networks are accurate enough.\nWe also show the qualitative comparison in Fig. 3. From the results, we can find that DeepSDF fails to repair shapes when facing complex shapes. IF-Net preserves fine details for the visible part because the input points are transferred into a high-resolution voxel grid which saves the detail information in an explicit way, but it cannot avoid the generation of noises for invisible parts. LDIF, and ShapeFormer fail to recover fine details. We can see that details such as small holes and thin curves are missing in their results. It is mainly because of the low capacity of their shape basis (i.e., Gaussians and vector quantized DIF) for capturing details. Obviously, our method owns a better capacity in preserving details for shape completion. For example, in Row 1 and Row 3, our method preserves the correct holes on the chair back and the very thin line correctly. Row 2 shows the power of our method in handling complex surfaces, and it also shows the benefit of post optimization, which is flattening the uneven surface."
        },
        {
            "heading": "5.2 Results on Unseen Classes",
            "text": "The results on unseen classes can represent the generalization ability of the shape completing methods on other shapes. From Tab. 2, we can find that our method shows outstanding generalization ability in completing the shapes of unseen classes. It is mainly because of our adaptive local bases and the local-to-local translation mechanism which enables predicting missing parts locally. As local parts across different classes may share a similar shape distribution (e.g. the legs of chairs and tables), our method can share the learned local-to-local translation priors from the trained classes with the unseen classes to improve the generalization ability. Again, PoinTrNDC obtains a slightly better score in CD than ours as PoinTr aims to minimize the chamfer distance. We provide more qualitative comparisons in the supplementary materials."
        },
        {
            "heading": "5.3 Comparison with PatchRD",
            "text": "PatchRD [Sun et al. 2022] is a voxel-based completion method. While it uses 3D convolutional encoding, its output is based on retrieval rather than direct convolutional decoding. We follow PatchRD to crop out small regions to generate input data, and perform qualitative comparison with it. The results are shown in Fig. 5. Our method can produce more accurate and smoother results than PatchRD."
        },
        {
            "heading": "5.4 Different Levels of Completeness",
            "text": "In order to validate the robustness of our method, we conduct experiments on input points with different levels of completeness, as shown in Fig. 6. Our method is robust to different levels of completeness and can preserve fine details in general. But we still observe floating crossbars near the chair legs (Column 1&5 in Row 3). It may be caused by the ambiguity of crossbar existence in chairs. One interesting future work is how to avoid such ambiguities."
        },
        {
            "heading": "5.5 Results on Real Scans",
            "text": "We have investigated the completion ability of our method on real-world scans. Fig. 4 shows the results of different objects from\nScanNet [Dai et al. 2017a] and scanned by ourselves using Kinect v2. Our method achieves high-quality completion results on real scan data."
        },
        {
            "heading": "5.6 Comparison on Computational Cost",
            "text": "Tab. 3 gives the comparison of computational cost among DIF-based baseline methods and ours. Although DeepSDF achieves the fewest latent codes and FLOPs, it fails in completing complex shapes as illustrated above. Among the other methods, it can be easily found that our method has a much lower computational cost, because we use a compact shape representation which requires a small number of latent codes and avoids the use of 2D and 3D convolutions which consume a lot of computations in LDIF, IF-Net, and ShapeFormer."
        },
        {
            "heading": "5.7 Ablation Studies",
            "text": "Weight Strategies. To prove that our learnable weights \ud835\udefc\ud835\udc56 can improve the learning of DIF-based basis functions and therefore help with preserving details, we compare the completion results with different weight strategies. The comparison results can be found in Fig. 7 (right). The learnable strategy is what we used, while the soft strategy means using the same formulation as Eq. 1 but the weights \ud835\udefc\ud835\udc56 are unlearnable which means the domain parameters A\ud835\udc56 and the offsets \ud835\udf39\ud835\udc56 are fixed (we set R\ud835\udc56 to be an identity matrix, \ud835\udf48\ud835\udc56 = [500, 500, 500] and \ud835\udf39\ud835\udc56=0.). The hard strategy is more direct that \ud835\udefc\ud835\udc56 is 1 if the center \ud835\udf41\ud835\udc56 is closest to the query point x or 0 otherwise. Obviously, the learnable strategy can help provide more details.\nLocal Basis Number. In order to verify that our local bases are compact, we do an ablation study using different numbers of local bases during partial point cloud encoding. Specifically, we fix the initial local basis number (\ud835\udc41\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61 ) to 128 but vary the downsampling number (\ud835\udc41\ud835\udc63\ud835\udc56\ud835\udc60 ) for partial points encoding and shape completion. From Tab. 4, we can find that the scores keep close with different local basis numbers. When the local basis number decreases from 128 to 32, the scores only drop a little. That means our local bases can be learned adaptively to faithfully represent the shapes.\nNearest Basis Number. In our framework, the signed distance of a query point is determined by two nearest local basis functions. We have also tried to use three nearest bases, and the IoU score decreases by 0.2% and F1 score remains the same. Therefore, two nearest local bases are sufficient to express the signed distance.\nLoss Functions. We conduct ablation studies on different loss functions. As shown in Row 1 and 3 in Tab. 5, by using the smooth loss, the IoU score increases by 1.1% and F1 score increases by\n1.2%. As shown in Fig 7 (left), without L\ud835\udc52\ud835\udc62\ud835\udc50 \ud835\udc60\ud835\udc51 \ud835\udc53 , the prediction of missing parts degrades. Removing L\ud835\udc5f\ud835\udc52\ud835\udc54 makes the training unable to converge.\nDomain-based Downsampling. We validate the domain-based downsampling by replacing it with uniform downsampling. As shown in Row 2 and 3 in Tab. 5, domain-based downsampling obtains the best scores."
        },
        {
            "heading": "6 CONCLUSION AND LIMITATION",
            "text": "We have proposed a new shape completion method based on implicit function with adaptive local basis functions. These local basis functions provide an effective and efficient compact representation for complex shapes, preserving geometric details while reducing computational costs. One limitation of our method is that the whole local bases prediction does not guarantee to recover the target topology of the shape, shown in (Fig. 1 (d)) where the cross-bar is not tightly connected with one leg of the chair. This happens when the connectivity of the local bases in the missing region is different from the ground-truth. In the future, we will look into augmenting our method with a high-level graph-based representation that focuses on the global topology of the shape."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "The authors would like to thank reviewers for their insightful comments. This work was supported by NSF China (62227806), the XPLORER PRIZE, and the 100 Talents Program of Zhejiang University."
        },
        {
            "heading": "B EXPERIMENTS B.1 Comparison with PoinTr and SnowflakeNet",
            "text": "We also compare our method directly with the point-based completion method PoinTr and SnowflakeNet [Xiang et al. 2021] which also adopts a Transformer architecture. As the number of output points is fixed to be 8192 in PoinTr and SnowflakeNet, we sample the same number of points on the meshes of our method for computing CD and F1. The results are listed in Tab. 7. We can find that the PoinTr and SnowflakeNet have better scores in CD, and our method achieves better scores in F1. That means the whole point cloud generated by PoinTr and SnowflakeNet obtain lower average errors in L2 distance, but there are numbers of outliers that reduce the F1 score. Our methods can produce more stable results with fewer outliers. It can also be proven in the visualization results shown in the main paper. Notably, for both methods, there are small gaps between the scores on trained classes and unseen classes. It further proves that the generalization ability of shape completion can be improved by building local-to-local translation modules among local shape representations.\nB.2 Shape Reconstruction on Complete Input Points\nTo show the efficiency of our shape representation method, we perform shape reconstruction using complete point clouds and compared the results with other DIF-based methods. For this task, we only use the \u2018Visible Points Encoding\u2019 module and use the loss\nfunction L\ud835\udc60\ud835\udc51 \ud835\udc53 + \ud835\udf06L\ud835\udc60\ud835\udc5a\ud835\udc5c\ud835\udc5c\ud835\udc61\u210e in which \ud835\udf06 = 0.5. Here, our post optimization is not used. All the experiments are run on the chair class of ShapeNet dataset [Chang et al. 2015], and the results are shown in Tab. 8. The methods for comparison include DeepSDF [Park et al. 2019], IF-Net [Chibane et al. 2020], LDIF [Genova et al. 2020], ShapeFormer [Yan et al. 2022], and 3DILG [Zhang et al. 2022], which also adopt deep implicit functions and auto-encoder architectures. Our method achieves the best scores when using 256 local bases. Notably, our method can achieve similar scores with much fewer bases. For example, our method with 128 local bases can achieve similar scores as IFNet with 1283 resolution, and our method with 64 local bases can achieve similar scores as 3DILG with 512 latent features. Even with 32 local bases, our method outperforms LDIF and ShapeFormer in most metrics.\nB.3 More Qualitative Results We show more qualitative comparisons on the trained and unseen classes in Fig. 8 and Fig. 9. We can find that our completion results clearly outperform the ones from other methods. DeepSDF [Park et al. 2019] fails to generate reasonable shapes when meeting complex cases. LDIF [Genova et al. 2020] and ShapeFormer [Yan et al. 2022] are difficult to preserve details. Also, ShapeFormer may generate shapes that do not match the input points. PoinTr [Yu et al. 2021] may generate outlier points. IF-Net [Chibane et al. 2020] can preserve fine details for visible parts but is difficult to predict smooth results for invisible parts. In contrast, our method can predict reasonable shapes for invisible parts while preserving fine details. Fig. 9 demonstrates the strong generalization ability of our method."
        }
    ],
    "title": "Adaptive Local Basis Functions for Shape Completion",
    "year": 2023
}