{
    "abstractText": "LiDAR and camera are two critical sensors for multi-modal 3D semantic segmentation and are supposed to be fused efficiently and robustly to promise safety in various realworld scenarios. However, existing multi-modal methods face two key challenges: 1) difficulty with efficient deployment and real-time execution; and 2) drastic performance degradation under weak calibration between LiDAR and cameras. To address these challenges, we propose CPGNet-LCF, a new multimodal fusion framework extending the LiDAR-only CPGNet. CPGNet-LCF solves the first challenge by inheriting the easy deployment and real-time capabilities of CPGNet. For the second challenge, we introduce a novel weak calibration knowledge distillation strategy during training to improve the robustness against the weak calibration. CPGNet-LCF achieves stateof-the-art performance on the nuScenes and SemanticKITTI benchmarks. Remarkably, it can be easily deployed to run in 20 ms per frame on a single Tesla V100 GPU using TensorRT TF16 mode. Furthermore, we benchmark performance over four weak calibration levels, demonstrating the robustness of our proposed approach.",
    "authors": [
        {
            "affiliations": [],
            "name": "Feng Jiang"
        },
        {
            "affiliations": [],
            "name": "Chaoping Tu"
        },
        {
            "affiliations": [],
            "name": "Gang Zhang"
        },
        {
            "affiliations": [],
            "name": "Jun Li"
        },
        {
            "affiliations": [],
            "name": "Hanqing Huang"
        },
        {
            "affiliations": [],
            "name": "Junyu Lin"
        },
        {
            "affiliations": [],
            "name": "Di Feng"
        },
        {
            "affiliations": [],
            "name": "Jian Pu"
        }
    ],
    "id": "SP:3979c6fea8b722edcba5bf9652a07ed0d9b7910d",
    "references": [
        {
            "authors": [
                "Y. Guo",
                "H. Wang",
                "Q. Hu",
                "H. Liu",
                "L. Liu",
                "M. Bennamoun"
            ],
            "title": "Deep learning for 3d point clouds: A survey",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 43, no. 12, pp. 4338\u20134364, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Li",
                "H. Dai",
                "H. Han",
                "Y. Ding"
            ],
            "title": "Mseg3d: Multi-modal 3d semantic segmentation for autonomous driving",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 21 694\u201321 704.",
            "year": 2023
        },
        {
            "authors": [
                "G. Krispel",
                "M. Opitz",
                "G. Waltner",
                "H. Possegger",
                "H. Bischof"
            ],
            "title": "Fuseseg: Lidar point cloud segmentation fusing multi-modal data",
            "venue": "Proceedings of the IEEE/CVF winter conference on applications of computer vision, 2020, pp. 1874\u20131883.",
            "year": 2020
        },
        {
            "authors": [
                "L. Zhao",
                "H. Zhou",
                "X. Zhu",
                "X. Song",
                "H. Li",
                "W. Tao"
            ],
            "title": "Lif-seg: Lidar and camera image fusion for 3d lidar semantic segmentation",
            "venue": "IEEE Transactions on Multimedia, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Z. Zhuang",
                "R. Li",
                "K. Jia",
                "Q. Wang",
                "Y. Li",
                "M. Tan"
            ],
            "title": "Perceptionaware multi-sensor fusion for 3d lidar semantic segmentation",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 16 280\u201316 290.",
            "year": 2021
        },
        {
            "authors": [
                "H. Tang",
                "Z. Liu",
                "S. Zhao",
                "Y. Lin",
                "J. Lin",
                "H. Wang",
                "S. Han"
            ],
            "title": "Searching efficient 3d architectures with sparse point-voxel convolution",
            "venue": "European conference on computer vision. Springer, 2020, pp. 685\u2013 702.",
            "year": 2020
        },
        {
            "authors": [
                "X. Zhu",
                "H. Zhou",
                "T. Wang",
                "F. Hong",
                "Y. Ma",
                "W. Li",
                "H. Li",
                "D. Lin"
            ],
            "title": "Cylindrical and asymmetrical 3d convolution networks for lidar segmentation",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 9939\u20139948.",
            "year": 2021
        },
        {
            "authors": [
                "X. Yan",
                "J. Gao",
                "C. Zheng",
                "C. Zheng",
                "R. Zhang",
                "S. Cui",
                "Z. Li"
            ],
            "title": "2dpass: 2d priors assisted semantic segmentation on lidar point clouds",
            "venue": "European Conference on Computer Vision. Springer, 2022, pp. 677\u2013695.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Zhang",
                "Z. Zhou",
                "P. David",
                "X. Yue",
                "Z. Xi",
                "B. Gong",
                "H. Foroosh"
            ],
            "title": "Polarnet: An improved grid representation for online lidar point clouds semantic segmentation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 9601\u20139610.",
            "year": 2020
        },
        {
            "authors": [
                "A. Milioto",
                "I. Vizzo",
                "J. Behley",
                "C. Stachniss"
            ],
            "title": "Rangenet++: Fast and accurate lidar semantic segmentation",
            "venue": "2019 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE, 2019, pp. 4213\u20134220.",
            "year": 2019
        },
        {
            "authors": [
                "H.-X. Cheng",
                "X.-F. Han",
                "G.-Q. Xiao"
            ],
            "title": "Cenet: Toward concise and efficient lidar semantic segmentation for autonomous driving",
            "venue": "2022 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2022, pp. 01\u201306.",
            "year": 2022
        },
        {
            "authors": [
                "B. Wu",
                "X. Zhou",
                "S. Zhao",
                "X. Yue",
                "K. Keutzer"
            ],
            "title": "Squeezesegv2: Improved model structure and unsupervised domain adaptation for roadobject segmentation from a lidar point cloud",
            "venue": "2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019, pp. 4376\u20134382.",
            "year": 2019
        },
        {
            "authors": [
                "X. Li",
                "G. Zhang",
                "H. Pan",
                "Z. Wang"
            ],
            "title": "Cpgnet: Cascade point-grid fusion network for real-time lidar semantic segmentation",
            "venue": "2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022, pp. 11 117\u201311 123.",
            "year": 2022
        },
        {
            "authors": [
                "V.E. Liong",
                "T.N.T. Nguyen",
                "S. Widjaja",
                "D. Sharma",
                "Z.J. Chong"
            ],
            "title": "Amvnet: Assertion-based multi-view fusion network for lidar semantic segmentation",
            "venue": "arXiv preprint arXiv:2012.04934, 2020.",
            "year": 2012
        },
        {
            "authors": [
                "H. Qiu",
                "B. Yu",
                "D. Tao"
            ],
            "title": "Gfnet: Geometric flow network for 3d point cloud semantic segmentation",
            "venue": "arXiv preprint arXiv:2207.02605, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Fan",
                "S. Lai",
                "J. Huang",
                "X. Wei",
                "Z. Chai",
                "J. Luo",
                "X. Wei"
            ],
            "title": "Rethinking bisenet for real-time semantic segmentation",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 9716\u20139725.",
            "year": 2021
        },
        {
            "authors": [
                "H. Caesar",
                "V. Bankiti",
                "A.H. Lang",
                "S. Vora",
                "V.E. Liong",
                "Q. Xu",
                "A. Krishnan",
                "Y. Pan",
                "G. Baldan",
                "O. Beijbom"
            ],
            "title": "nuscenes: A multimodal dataset for autonomous driving",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 11 621\u201311 631.",
            "year": 2020
        },
        {
            "authors": [
                "J. Behley",
                "M. Garbade",
                "A. Milioto",
                "J. Quenzel",
                "S. Behnke",
                "C. Stachniss",
                "J. Gall"
            ],
            "title": "SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences",
            "venue": "ICCV, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H. Lu",
                "H. Shi"
            ],
            "title": "Deep learning for 3d point cloud understanding: a survey",
            "venue": "arXiv preprint arXiv:2009.08920, 2020.",
            "year": 2009
        },
        {
            "authors": [
                "C.R. Qi",
                "H. Su",
                "K. Mo",
                "L.J. Guibas"
            ],
            "title": "Pointnet: Deep learning on point sets for 3d classification and segmentation",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 652\u2013660.",
            "year": 2017
        },
        {
            "authors": [
                "H. Thomas",
                "C.R. Qi",
                "J.-E. Deschaud",
                "B. Marcotegui",
                "F. Goulette",
                "L.J. Guibas"
            ],
            "title": "Kpconv: Flexible and deformable convolution for point clouds",
            "venue": "Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 6411\u20136420.",
            "year": 2019
        },
        {
            "authors": [
                "J. Huang",
                "S. You"
            ],
            "title": "Point cloud labeling using 3d convolutional neural network",
            "venue": "2016 23rd International Conference on Pattern Recognition (ICPR). IEEE, 2016, pp. 2670\u20132675.",
            "year": 2016
        },
        {
            "authors": [
                "B. Graham"
            ],
            "title": "Sparse 3d convolutional neural networks",
            "venue": "arXiv preprint arXiv:1505.02890, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "B. Wu",
                "A. Wan",
                "X. Yue",
                "K. Keutzer"
            ],
            "title": "Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3d lidar point cloud",
            "venue": "2018 IEEE international conference on robotics and automation (ICRA). IEEE, 2018, pp. 1887\u20131893.",
            "year": 2018
        },
        {
            "authors": [
                "K. El Madawi",
                "H. Rashed",
                "A. El Sallab",
                "O. Nasr",
                "H. Kamel",
                "S. Yogamani"
            ],
            "title": "Rgb and lidar fusion based 3d semantic segmentation for autonomous driving",
            "venue": "2019 IEEE Intelligent Transportation Systems Conference (ITSC). IEEE, 2019, pp. 7\u201312.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Zhang"
            ],
            "title": "Flexible camera calibration by viewing a plane from unknown orientations",
            "venue": "Proceedings of the seventh ieee international conference on computer vision, vol. 1. Ieee, 1999, pp. 666\u2013673.",
            "year": 1999
        },
        {
            "authors": [
                "V. Fremont",
                "P. Bonnifait"
            ],
            "title": "Extrinsic calibration between a multilayer lidar and a camera",
            "venue": "2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems. IEEE, 2008, pp. 214\u2013219.",
            "year": 2008
        },
        {
            "authors": [
                "M. Berman",
                "A.R. Triki",
                "M.B. Blaschko"
            ],
            "title": "The lov\u00e1sz-softmax loss: A tractable surrogate for the optimization of the intersection-overunion measure in neural networks",
            "venue": "CVPR, 2018, pp. 4413\u20134421.",
            "year": 2018
        },
        {
            "authors": [
                "S. Hanyu",
                "W. Jiacheng",
                "W. Hao",
                "L. Fayao",
                "L. Guosheng"
            ],
            "title": "Learning spatial and temporal variations for 4d point cloud segmentation",
            "venue": "arXiv preprint arXiv:2207.04673, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "D. Ye",
                "Z. Zhou",
                "W. Chen",
                "Y. Xie",
                "Y. Wang",
                "P. Wang",
                "H. Foroosh"
            ],
            "title": "Lidarmultinet: Towards a unified multi-task network for lidar perception",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37, no. 3, 2023, pp. 3231\u20133240.",
            "year": 2023
        },
        {
            "authors": [
                "nuScenes"
            ],
            "title": "uscenes leaderboard of lidar segmentation task",
            "venue": "2023, https://www.nuscenes.org/lidar-segmentation.",
            "year": 2023
        },
        {
            "authors": [
                "K. Genova",
                "X. Yin",
                "A. Kundu",
                "C. Pantofaru",
                "F. Cole",
                "A. Sud",
                "B. Brewington",
                "B. Shucker",
                "T. Funkhouser"
            ],
            "title": "Learning 3d semantic segmentation with only 2d image supervision",
            "venue": "2021 International Conference on 3D Vision (3DV). IEEE, 2021, pp. 361\u2013372.",
            "year": 2021
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "I. Loshchilov",
                "F. Hutter"
            ],
            "title": "Sgdr: Stochastic gradient descent with warm restarts",
            "venue": "arXiv preprint arXiv:1608.03983, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "T. Cortinhal",
                "G. Tzelepis",
                "E. Erdal Aksoy"
            ],
            "title": "Salsanext: Fast, uncertainty-aware semantic segmentation of lidar point clouds",
            "venue": "International Symposium on Visual Computing. Springer, 2020, pp. 207\u2013222.",
            "year": 2020
        },
        {
            "authors": [
                "S. Vora",
                "A.H. Lang",
                "B. Helou",
                "O. Beijbom"
            ],
            "title": "Pointpainting: Sequential fusion for 3d object detection",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 4604\u20134612.",
            "year": 2020
        },
        {
            "authors": [
                "J. Wang",
                "K. Sun",
                "T. Cheng",
                "B. Jiang",
                "C. Deng",
                "Y. Zhao",
                "D. Liu",
                "Y. Mu",
                "M. Tan",
                "X. Wang"
            ],
            "title": "Deep high-resolution representation learning for visual recognition",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 43, no. 10, pp. 3349\u20133364, 2020.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014 Autonomous Driving, Semantic Segmentation, Multi-modal Fusion, Weak Calibration\nI. INTRODUCTION\nScene understanding is very important for autonomous driving, and point cloud semantic segmentation is an important phase [1]. Due to the sparsity of point clouds, the segmentation performance of distant objects is worse, which can be solved by introducing camera RGB information [2]\u2013 [5]. Multi-modal semantic segmentation can simultaneously perceive the texture and geometric information of the scene. However, existing multi-modal methods are not ready for applications since the noteworthy actual problems that may happen in real-world scenarios are under-explored. In this paper, two prerequisites are introduced that the multi-modal methods should not only be easy-deployed and run in realtime but also demonstrate strong robustness against the weak calibration between LiDAR and cameras.\nAlmost all existing multi-modal methods are built on the LiDAR backbones, resulting in the computational complexity primarily concentrated on them. Currently, sparse voxelbased backbones [6]\u2013[8] and 2D projection-based backbones [9]\u2013[12] are receiving increasing attention due to their high accuracy or fast speed. The sparse voxel-based\n\u2020 equal contribution B corresponding author, jianpu@fudan.edu.cn \u2217 zhanggang11021136@gmail.com 1ISTBI, Fudan University 2Mogo Auto Intelligence and Telematics Information Technology Co.,\nLtd 3School of Computer Science, Fudan University\nbackbones are hard to use in real-world autonomous driving systems since they utilize the time-consuming 3D sparse convolution. The 2D projection-based backbones can achieve faster inference speed by projecting the point clouds onto the 2D view, such as bird\u2019s-eye-view (BEV) and range view (RV), where the highly efficient 2D convolution network is applied. Some previous works [13]\u2013[15] further integrated the multiple 2D views for stronger representation, while keeping their efficiency. Another serious problem of the multi-modal methods [2], [3], [5] is that they do not consider the weak calibration problem that inevitably occurs in practice. Fig. 1 demonstrates how performance is affected by different weak calibration levels. Notably, at the Level 2, and 3 of weak calibration, the results are even worse than those of the LiDAR-only models. The projection deviations between the well and weak calibrations, as shown on the right side of Fig. 1, explain this performance degradation. Misalignment between LiDAR and cameras can introduce errors, guiding the multi-modal methods to learn wrong information.\nTo address the aforementioned challenges, we propose CPGNet-LCF, a LiDAR and Camera Fusion method that extends the easy-deployed CPGNet [13] as the LiDAR backbone and integrates the lightweight 2D image semantic segmentation network STDC [16] as the image backbone. Concretely, the image features are extracted by the STDC and further augment the LiDAR features by bilinear sampling according to the calibration matrices between LiDAR and cameras. Subsequently, the image-augmented LiDAR features are processed by the CPGNet to acquire the semantic segmentation results.\nThe proposed CPGNet-LCF is evaluated on nuScenes [17] and SemanticKITTI [18] LiDAR semantic segmentation\nar X\niv :2\n31 0.\n08 82\n6v 1\n[ cs\n.C V\n] 1\n3 O\nct 2\n02 3\nbenchmarks and achieves the leading results (83.2 mIoU on the nuScenes leaderboard and 67.1 mIoU on SemanticKITTI validation set). CPGNet-LCF runs 63 ms per frame with PyTorch and 20 ms per frame with TensorRT TF16 inference mode on a single NVIDIA Tesla V100 GPU, which means that the proposed method is easy-deployed and real-time. Four levels of the weak calibration evaluation benchmarks based on nuScenes [17] are established to evaluate the robustness of the model against the weak calibration. The proposed method outperforms the existing methods by a large margin while maintaining high performance even when the weak calibration level increases.\nThe main contributions of our work are as follows: \u2022 Existing multi-modal methods have difficulty in deploy-\nment, real-time execution, and robustness against weak calibration. We hope that researchers can pay more attention to the application-oriented issues. \u2022 We extend the efficient CPGNet to the LiDAR and camera fusion, dubbed CPGNet-LCF. A novel weak calibration knowledge distillation strategy is adopted to alleviate the effects of the weak calibration between LiDAR and cameras. \u2022 The proposed method achieves the SOTA results on two public datasets and shows remarkable robustness against weak calibration. Besides, it can be easily deployed on the TensorRT TF16 mode and runs 20 ms per frame."
        },
        {
            "heading": "II. RELATED WORK",
            "text": ""
        },
        {
            "heading": "A. LiDAR-Only Semantic Segmentation",
            "text": "Due to the irregular and unordered properties of point clouds, methods for processing point clouds can be divided into point-based methods, sparse voxel-based methods, and 2D projection-based methods [19].\nThe point-based methods directly process the original point cloud, which can retain more information, but requires nearest-neighbor searching to obtain surrounding information. PointNet [20] proposed a simple but effective model, which obtained global features through the stack of MLPs. KPConv [21] proposed convolution operation on points, which learned weight kernel to mimic the traditional convolutions. Sparse voxel-based methods usually convert point cloud into a dense or sparse representation. Huang et al. [22] used fully-3D convolution to extract features for each voxel and Benjamin et at. [23] further proposed sparse convolution to reduce the computational complexity. Cylinder3D [7], a classic voxel-based method, proposed to use cylindrical partition to fit the scanning mode of LiDAR. These methods are time-consuming due to the huge computation complexity and cannot be easily deployed to autonomous driving systems.\n2D projection-based methods first project the point clouds into a certain 2D view, such as bird\u2019s-eye-view (BEV) and range view (RV). Squeezeseg [24] introduced an efficient but effective backbone and RangeNet++ [10] proposed a post-processing method to solve the re-projection problem. PolarNet [9] partitioned the points into grids with polar BEV\ncoordinates and proposed ring convolution to better utilize the scanning of LiDAR. Furthermore, some works adopted a multi-view fusion approach to simultaneously benefit from BEV and RV representation. CPGNet [13] proposed a PointGrid fusion block, which fuses point, BEV and RV features in a cascade framework, which primarily consisted of 2D convolutions. In this paper, the CPGNet is adopted as the LiDAR backbone due to its effectiveness and efficiency."
        },
        {
            "heading": "B. Multi-modal Fusion Semantic Segmentation",
            "text": "The fusion of LiDAR and camera is an effective method to improve perception ability. The camera provides RGB and detailed information, while LiDAR provides more spatial and geometric information about the surrounding environment. Many previous works [2], [3], [5], [25] have made significant contributions. [3] and [25] utilized the dense intrinsic distance representation and calibration information of LiDAR sensors to establish a point correspondence relationship between two input modalities. PMF [5] proposed a LiDAR and camera fusion method based on perspective projection instead of spherical projection, which can exploit perceptual information from two modalities. MSeg3D [2] proposed a multi-modal 3D semantic segmentation model that combined intra-modal feature extraction and inter-modal feature fusion, which to some extent alleviated the modal heterogeneity and also applied the asymmetric transformations to enhance the effectiveness of data augmentation. LIF-Seg [4] predicted an offset to rectify the features of the two modalities, but the results were unstable and difficult to be widely applied. The fusion of LiDAR and camera always needs an accurate calibration matrix from LiDAR to cameras, which is the cornerstone of the above models. Weak calibration apparently has a huge influence, but few works take this problem into consideration."
        },
        {
            "heading": "III. METHOD",
            "text": "The real-world autonomous driving system needs an easydeployed, real-time, and robust multi-modal method. To achieve the first two goals, we propose a LiDAR and camera fusion method, dubbed CPGNet-LCF, which is an extendsion of the easy-deployed CPGNet [13]. For the last goal, a novel weak calibration knowledge distillation strategy is proposed.\nIn this section, we first illustrate the definitions of the semantic segmentation, calibration matrix, and weak calibration between LiDAR and cameras in Sec. III-A. Secondly, the details of the proposed CPGNet-LCF are described in Sec. III-B. Subsequently, the weak calibration knowledge distillation strategy is introduced in Sec. III-C and the loss functions are listed in Sec. III-D."
        },
        {
            "heading": "A. Preliminary",
            "text": "1) Semantic Segmentation: Given the input of point cloud P \u2208 RN\u00d7Cp , camera images C = {c1, c2, \u00b7 \u00b7 \u00b7 , cM} \u2208 RW\u00d7H\u00d73. N,M denote the number of point clouds and the number of RGB images of each frame, and Cp is the number of input channels of each LiDAR point (usually 4, including the XYZ coordinates and reflection intensity). The objective\nof 3D semantic segmentation is to predict a label for each LiDAR point, and the number of semantic categories is Ncls.\n2) Calibration Matrix: For LiDAR and camera fusion methods, the calibration matrix is used to align the point cloud 3D coordinates and image 2D pixels [26], [27]. The calibration matrices, also dubbed extrinsic matrices from LiDAR point cloud to each camera are represented as T = {T1, T2, . . . TM} \u2208 R4\u00d74, where M is the number of cameras. Each matrix is composed of rotation matrix Ri \u2208 R3\u00d73 and translation vector ti \u2208 R3\u00d71, which presents the way to convert points from the LiDAR coordinate system to the camera coordinate system. Intrinsic matrix of each camera is represented as I = {I1, I2, . . . IM} \u2208 R3\u00d74, which reflects the attributes of camera. The point cloud that has been transformed into the camera coordinate system can be projected onto the image plane through the intrinsic matrix.\nThe point cloud can be projected onto the image ci through Ti and Ii, which is the foundation for many previous works of multi-modal fusion. The formula of transformation is:\n\u03bb uv 1  = Ii [Ri ti0 1 ] x y z 1  = IiTi  x y z 1  , (1) which is an example of projecting a LiDAR point (x, y, z) to the pixel (u, v) of the i-th camera image.\n3) Weak Calibration: In real-world autonomous driving scenarios, weak calibration inevitably occurs due to loose or deformed brackets securing LiDAR and the cameras. The calibration of LiDAR to the cameras involves angles (yaw, pitch, roll) and offsets (tx, ty , tz). In practice, angles are more likely to change compared to offsets. Therefore, in this paper, only the change in angles is discussed. According to the projection relationship of Eq. 1, the weak calibration is equal to disturbing the extrinsic matrix Ti into T ei = TiEr, which can be composed of the original extrinsic matrix Ti and disturbing matrices Er = ExrE y rE z r . The disturbing matrices Exr , E y r , E z r only consider the rotation angles corresponding to the XYZ axes, respectively, where r denotes the disturbing level of the weak calibration. As\nshown in the right part of Fig. 1, the point cloud cannot be aligned with the image properly if the weak calibration occurs, which is fatal for the LiDAR and camera fusion methods. Apparently, the misaligned image features will introduce wrong information, and further degrade the performance of the multi-modal methods, even worse than that of the LiDAR-only counterpart, as shown in the left part of Fig. 1."
        },
        {
            "heading": "B. Overall Framework",
            "text": "The proposed CPGNet-LCF primarily consists of efficient and easy-deployed 2D convolution operators, which ensure the capabilities of easy deployment and fast inference speed. As shown in Fig. 2, it has three steps: 1) the image backbone is used to extract the meaningful texture features; 2) the LiDAR point features are augmented by bilinear-sampling the image features; 3) the image-augmented LiDAR features undergo the LiDAR segmentation backbone to acquire the final LiDAR semantic segmentation results. These three steps are introduced as the following.\nImages have much higher resolutions than the sparse LiDAR point clouds and can provide rich texture information, which can assist the LiDAR in recognizing faraway and small objects. Generally, the 2D convolution network is used to process the images for efficiency. STDC [16], used as the image backbone, proposes a lightweight backbone for segmentation tasks, which uses detailed guidance during training to keep more detailed information. The features F \u2208 RM\u00d7C2\u00d7H1\u00d7W1 after the image backbone are used to augment the LiDAR point features. Note that the spatial size of F is 1/8 of the input image size.\nThe image-augmented features fuse the semantic information from images and geometric information from LiDAR. Specifically, the pixel coordinates (u, v) corresponding to each LiDAR point can be obtained from the projection transformation relationship mentioned in Sec. III-A. Bilinear sampling based on the image plane coordinates (u, v) to obtain the corresponding features f iI \u2208 RN\u00d7C2 of the i-th camera is\nf iI = 1\u2211 p=0 1\u2211 q=0 \u03c9p,qFi,\u230au\u230b+p,\u230av\u230b+q, (2)\nwhere \u03c9p,q = (1 \u2212 |(\u230au\u230b+ p)|) \u00b7 (1 \u2212 |(\u230av\u230b+ q)|). In general, autonomous vehicles have more than one camera, thus obtaining features of M groups. However, the perspective field of each camera is limited, and features outside the perspective field are regarded as zeros when sampling. Concatenating along the dimensions of the camera features f1I , \u00b7 \u00b7 \u00b7 , fMI is used to obtain features fM \u2208 RN\u00d7(M\u00d7C2) for each LiDAR point. The LiDAR point cloud features obtained from PointNet [20] are fP \u2208 RN\u00d7C1 . Finally, the sampling camera features fM and LiDAR point features fP are concatenated along the channel dimension to form the image-augmented features f \u2208 RN\u00d7(C1+M\u00d7C2).\nLiDAR segmentation backbone serves as the basic part of CPGNet-LCF. After obtaining the image-augmented features, it can be considered as the point cloud features with richer information, which makes it very convenient to use existing LiDAR segmentation models. For both high accuracy and fast inference speed, a BEV and RV fusion framework CPGNet [13] is adopted as the basic backbone. CPGNet primarily consists of efficient and easy-deployed 2D convolution operations instead of the time-consuming 3D sparse convolution operations. For more details on CPGNet [13], please refer to its original paper."
        },
        {
            "heading": "C. Weak Calibration Knowledge Distillation Strategy",
            "text": "The weak calibration problem inevitably occurs in the realworld autonomous driving system, and attracts much less attention in the previous works. The definition and effect of the weak calibration have been illustrated in Sec. III-A.3. Therefore, we propose a novel weak calibration knowledge distillation strategy, which not only alleviates the effect of the weak calibration but also keeps the performance under the well calibration.\nBefore introducing the weak calibration knowledge distillation, we discuss a straightforward and simple idea, named weak calibration data augmentation. It treats the noise presented in the weak calibration matrix as a kind of data augmentation that mimics the weak calibration conditions during training. Consequently, it can help the model learn the inherent features of weak calibration and assist the multimodal methods to adapt to the weak calibration. In the experiment, it can be observed that the performance under weak calibration increases significantly but the performance under the well calibration drops. A possible reason is that this weak calibration data augmentation introduces much noise and misleads the training of the multi-modal methods.\nWeak calibration knowledge distillation strategy is proposed to address the issue of performance degradation under the well calibration mentioned above. Specifically, the model trained by the well calibration samples serves as the teacher model to guide the student model trained by weak calibration samples. It is obvious that the teacher model trained by the well calibration data, does not suffer from the performance degradation from the weak calibration data augmentation and thus has the ability to correctly guide the training of the student model. This knowledge distillation strategy can make the student model not only shows robustness against\nthe weak calibration but also keeps performance under the well calibration. Note that the teacher model and student model share the same network parameters."
        },
        {
            "heading": "D. Loss Function",
            "text": "The overall loss function has two parts: the segmentation loss Lpc and weak calibration knowledge distillation loss Lwckd.\nThe segmentation loss for point cloud Lpc follows the previous work [13] and the detailed equation is given as Lpc = Lwce + L Lov\u00e1sz . Lwce means weighted cross-entropy loss. LLov\u00e1sz is proposed by [28], which directly optimizes the intersection-over-union (IoU) score. Given the probability p\u0302 of the teacher model and the probability p of the student model, the loss is calculated by:\nLwckd = \u2212p\u0302c log(pc)\u2212 \u03bb2 Ncls\u2211 k \u0338=c N (p\u0302k) \u00b7 log(N (p\u03bbk)), (3)\nwhere c is the target category, \u03bb is the temperature of KD, and N means normalization functions. Totally, the training objective of our method is\nL = \u03bb1Lpc + \u03bb2Lwckd (4)\nwhere \u03bb1, \u03bb2 are the balanced coefficients."
        },
        {
            "heading": "IV. EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "A. Experimental Setup",
            "text": "nuScenes Dataset is a public large-scale dataset for autonomous driving, which contains 1000 scenes of 20 seconds each [17]. Specifically, 850 scenes of all scenes are used as the training and validation sets, while the remaining 150 are used as the test set. The dataset is annotated with keyframes selected at 2HZ. The point cloud of keyframes is labeled into 32 categories, including foreground categories such as Car and Bus, and background categories such as Sidewalk and Manmade. According to official requirements, we merge certain classes and only evaluate 16 classes.\nSemanticKITTI Dataset is a large dataset for LiDAR point cloud semantic segmentation, which provides annotations for all points [18]. It has 22 point cloud sequences, each collected from a different scene. As recommended, we use 00 to 10 for training, 08 for validation, and 11 to 21 for testing. However, when collecting the dataset, only two front cameras were used, and the field of view was heavily overlapped. Therefore, we follow the setting of the PMF [5], which only uses point clouds in the overlapping area of the camera and LiDAR for evaluation.\nMetric follows the previous work [2]. We use the evaluation metric of the mean intersection-over-union (mIoU) over all classes, defined as mIoU = 1Ncls \u2211Ncls c=1 TPc TPc+FPc+FNc\n, where TPc, FPc, FNc denote the number of true positive, false positives, and false negatives points of category c, respectively. The mIoU is the average of all classes.\nImplementation Details of data process, model settings, and training details are as follows. For a fair comparison with the previous SOTA method MSeg3D [2], the input\nimage size is resized to [640, 960] on the nuScenes dataset and [360, 1280] on the SemanticKITTI dataset. The range of the point cloud is set as [\u221251.2m,+51.2m] along the XY axes and [\u22125.0m,+3.0m] along the Z axis on the nuScenes dataset. The range of the point cloud is set as [\u221275.2m,+75.2m] along the XY axes and [\u22124.0m,+2.0m] along the Z axis on the SemanticKITTI dataset. On both datasets, the BEV and RV branches of the LiDAR segmentation backbone accept a 512\u00d7512 and a 64\u00d72048 tensors, respectively. Besides, the model hyperparameters follows the CPGNet [13] and the proposed CPGNet-LCF adopts a twostage version of the CPGNet.\nWe use Adam [33] as the optimizer with a base learning rate of 0.001 and weight decay of 0.001. OneCycle [34] is used as the learning rate scheduler strategy with the annealing strategy of the cosine curve. Following the convention, the data augmentation strategies, containing random flipping along the XY axes, random global scale sampled from [0.95, 1.05], random rotation around the Z axis, random Gaussian noise N (0, 0.02) are used. The balanced coefficients of the loss terms are set as \u03bb1 = 1, \u03bb2 = 1. Weak Calibration Benchmark is established to evaluate the robustness of different models affected by the calibration errors. nuScenes is considered as well calibration dataset, which has been carefully processed and validated before and after data collection [17]. The weak calibration with the corresponding disturbing matrices is illustrated in Sec. IIIA.3. r = 0, 1, 2, 3 denotes the level of the weak calibration, as shown in Tab. II. The weak calibration benchmark is built on the nuScenes validation set by randomly selecting the calibration angle noise from the predefined angle noise range\nof each level to form the disturbing matrices (Exr , E y r , E z r ). In the phases of weak calibration data augmentation and knowledge distillation, Level 3 is used during training. Note that Level 0 of the weak calibration refers to the well calibration."
        },
        {
            "heading": "B. Results on Weak Calibration Benchmark",
            "text": "Tab. III shows the results of our proposed weak calibration evaluation benchmark. LiDAR-only means that the model only has the input of LiDAR point cloud data. We compare the proposed CPGNet-LCF with the previous SOTA method MSeg3D [2], and the result shows that both MSeg3D and our proposed method CPGNet-LCF suffer from performance degradation as the calibration errors gradually increase. Even worse, at Level 3, the multi-modal CPGNet-LCF underperforms its LiDAR-only counterpart by a large performance drop(-8.3 mloU), which means that weak calibration greatly affects the performance of the multi-modal methods and deserves considerable exploration.\nWhen the weak calibration data augmentation is considered, the performance of both MSeg3D [2] and our CPGNetLCF have a significant improvement, especially +4.4 mloU of MSeg3D and +5.9 mloU of CPGNet-LCF on the Level\n3. However, the performance on well calibration (Level 0) degrades due to the misleading training process caused by the weak calibration data augmentation. Weak calibration knowledge distillation strategy can effectively solve this degradation problem under the well calibration, even improving the performance across all levels of the weak calibration. It demonstrates the effectiveness of the proposed training strategy in alleviating the weak calibration problems."
        },
        {
            "heading": "C. Comparisons With the State-of-the-Arts",
            "text": "Comparison results of the nuScenes leaderboard and SemanticKITTI validation set are shown in Tab. I and Tab. IV, respectively. CPGNet-LCF outperforms all LiDAR-only and multi-modal methods on both accuracy and inference speed by a large margin on the nuScenes leaderboard. mloU1 in Tab. IV means the setting of PMF [5], which only evaluates the performance of the overlapped areas between LiDAR and cameras. The proposed CPGNet-LCF also achieves the best mloU of 67.1. These observations prove the superiority of the proposed framework. In addition, our model also has significant advantages in deployment and speed that it runs 63 ms per frame with PyTorch and 20 ms per frame with TensorRT TF16 on a single NVIDIA Tesla V100 GPU.\nIn Tab.V, we also evaluate the impacts of different image backbones. In the proposed CPGNet-LCF framework, the STDC1 [16] outperforms the HRNet-w18 [37] on both accuracy and inference speed, demonstrating the superiority of the STDC1. Besides, with the same image backbone, HRNetw18, the proposed CPGNet-LCF outperforms the previous SOTA MSeg3D [2] on both accuracy and inference speed since the MSeg3D adopts the time-consuming 3D sparse convolution as the basic operator.\nD. Visualization\nVisualization results on the nuScenes validation set are shown in Fig. 3. It can be seen that the LiDAR-only CPGNet (a) misclassifies several LiDAR points of Manmade into Truck, while the multi-modal CPGNet-LCF (b) can make correct predictions due to the rich texture information from cameras. However, if the calibration matrices are disturbed on Level 3 (c), the predictions inferred by the same model (b) are even worse than those of the LiDAR-only CPGNet (a) since the misalignment between the LiDAR and cameras will confuse the objects with their neighbors. Surprisingly, this problem can be solved (d) by the proposed weak calibration knowledge distillation."
        },
        {
            "heading": "V. CONCLUSIONS",
            "text": "In this paper, we first point out that easy-deployed, realtime, and robust against weak calibration are significant for real-world autonomous driving systems, which are underexplored in existing multi-modal methods. The proposed LiDAR and camera fusion model, dubbed CPGNet-LCF, is an extension of the easy-deployed and real-time CPGNet. Experimental results on the two public datasets demonstrate that the proposed CPGNet-LCF achieves the new SOTA results. Meanwhile, it is easily deployed on the TensorRT TF16 mode and runs 20 ms on a single Tesla V100 GPU to meet the real-time inference requirement. Besides, the results of the multi-modal methods degrade under the weak calibration, even worse than the LiDAR-only counterparts, which is effectively alleviated by the proposed weak calibration knowledge distillation strategy. The weak calibration deserves more explorations in the future works."
        }
    ],
    "title": "Revisiting Multi-modal 3D Semantic Segmentation in Real-world Autonomous Driving",
    "year": 2023
}