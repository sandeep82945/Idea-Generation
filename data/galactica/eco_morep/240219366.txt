The growing number of cases requiring digital forensic analysis raises concerns about law enforcement’s ability to conduct investigations promptly. Consequently, this systemisation of knowledge paper delves into the potential and effectiveness of integrating Large Language Models (LLMs) into digital forensic investigation to address these challenges. A thorough literature review is undertaken, encompassing existing digital forensic models, tools, LLMs, deep learning techniques, and the utilisation of LLMs in investigations. The review identifies current challenges within existing digital forensic processes and explores both the obstacles and possibilities of incorporating LLMs. In conclusion, the study asserts that the adoption of LLMs in digital forensics, with appropriate constraints, holds the potential to enhance investigation efficiency, improve traceability, and alleviate technical and judicial barriers faced by law enforcement entities. 1  5.8 challenges for llms in digital forensics To optimise results, LLMs likely need to be trained with specific forensic data (i.e., previous case data) to achieve the best results. Given the complexity and variation of cases, it is questionable how good the training data is and whether there is sufficient data [16]. Biases present in training data can lead to skewed interpretations and unjust outcomes. Investigative findings and drawing conclusions are also heavily based on investigator experience and intuition, which are difficult to replicate using LLMs. Initially, they will only prove effective for certain subtasks within digital forensics, which raises questions about their superiority over deterministic programs in certain contexts. For instance, data may be parsed/converted using a regular piece of software or an LLM. While the former works deterministically, the latter is more general, but requires the practitioners to validate the output. This leads to the fundamental necessity for explainability (i.e., not using blackboxes) for investigations. The explainability of LLM-generated results remains a daunting challenge, as understanding the reasons for their, frequently non-deterministic, results is often elusive [70]. Maintaining the energy- and resource-intensive infrastructure required to train and deploy LLMs is a significant financial burden, making it impractical for smaller forensic laboratories to utilise such technologies. Centralised systems may be an option but require clear guidelines on how data can/may be shared and exchanged. While LLMs can serve as valuable tools to support forensic investigations, it must be recognised that they currently function best as an aid, not a substitute for human expertise [100]. There is a risk that people may place too much trust in the results generated by LLMs (over-reliance), which could lead to complacency and overlook the need for detailed human expert analysis and validation. Finally, ethical and legal considerations must also be discussed. Determining accountability in cases where LLMs produce false information or are compromised by hacking. Clarifying responsibilities between developers, users and regulators is crucial to establish a framework for accountability. If LLM-generated DF results lead to incorrect information, the responsibility may lie with the developers for ensuring the accuracy of the model and with the users for the appropriate interpretation and validation of the results. 5.9 risks of integration The integration of LLMs within the DF process comes with inherent risks – extra to those general LLM limitations outlined in Section 4.6. Notably, in the examination, analysis, and reporting phases, the utilisation of LLMs introduces the risk of producing inaccurate information, primarily due to the phenomenon of inheritance hallucinations associated with these models [70, 100]. Additionally, the biases and obscurities present in an inheritance model may significantly impact the performance of a DF-focused
LLM – potentially leading to the unacceptable generation of biased or inaccurate information within the DF process. It is also crucial to acknowledge that DF LLMs, like any complex models, are susceptible to adversarial manipulation [151]. This vulnerability poses a substantial risk in the context of sensitive domains such as DF, where the integrity of information gained is paramount. Adversarial attacks can compromise the reliability of LLM-generated outputs, potentially influencing the outcomes of various phases within the DF process. Indeed, despite incorporating human verification, outputs and reports generated by LLMs within DF applications may encounter challenges regarding acceptance within the legal systems of different countries. This highlights a significant usability risk associated with LLM-based DF applications, but one that can be carefully mitigated by limiting the technology’s deployment as a human-inthe-loop investigative aid as opposed to directly feeding into any investigative/judicial decision-making processes. 6 conclusion The convergence of LLMs with an array of technologies represents an exciting synergy. Although the utilisation of LLMs in the realm of DF is still in its nascent stages, there is evidence of their substantial potential to significantly augment the efficiency of investigations. The exploration of investment for LLMs across the entire DF process is considered, aiming to enhance the productivity and efficiency of investigations. Additionally, the integration of LLMs into current DF tools is posited to reduce user training times, as these models comprehend natural language input and provide outputs accordingly. In the dynamic landscape of LLM applications within DF, promising avenues for further exploration and advancement unfold. While the surge in LLM research is promising, it is crucial to balance enthusiasm with an awareness of existing challenges. The propensity for LLMs to produce hallucinations highlights the need for human oversight in critical decision-making processes, underscoring the irreplaceable value of human judgement, intuition and expertise. A notable limitation is the language dependency issue, as most LLMs are predominantly trained on English data, reducing their effectiveness with non-English content. Additionally, the deployment of LLMs in DF involves significant costs related to infrastructure for processing evidence. Questions also arise about the validation of task correctness and quality when automated by LLMs, as well as the legal and professional acceptance of results obtained with limited human intervention. Integrating LLMs with automated agents offers a promising path to automating DF processes, potentially allowing multiple cases to be handled concurrently for more timely and precise outcomes. This integration could significantly streamline investigations. Future research should explore the role of LLMs and AI in DF decisionmaking. It is essential to focus on validating LLM-generated outputs to ensure their scope, accuracy, reliability, and trustworthiness in investigations. Further studies comparing DF outcomes with and without LLM integration are critical, as they could highlight LLMs’ benefits and controlled applicability in DF and similar fields. In essence, while LLMs offer exciting prospects for the future of digital forensics, a balanced approach that integrates their strengths
with human oversight is essential for harnessing their full potential. Inevitably, LLM-facilitated DF processes will themselves become the focus of future investigation.