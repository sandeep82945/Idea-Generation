{
    "abstractText": "In this article, we introduce the work of the Massively Parallel Particle Hydrodynamics (MPPH) working group, part of the UK\u2019s ExCALIBUR software initiative. The aim of the group is to develop extensible software suitable for simulating complex hydrodynamics problems on exascale computing facilities using a Lagrangian particle-based approach. These methods complement mesh-based approaches, and are particularly suited to problems with a large and fluid dynamic range or that involve free surfaces. The code we are developing uses fine-grained task parallelism to achieve a good load balance, when the workload varies greatly from fluid element to element. We describe how components from astrophysics and engineering are being brought together to further develop this interesting approach, and outline some of the challenges, such as enabling task execution on graphics processing units (GPUs), that the working group will need to solve in order to extract the full potential of exascale systems. PARTICLE HYDRODYNAMICS. There are traditionally two distinct main approaches to solving hydrodynamics problems. In the Eulerian approach, a fixed mesh covers space and the equations govern how fluid moves from cell to cell. This approach is widely used and, while rectangular grids are an obvious choice, the grid structure can be adapted to trace the structure of boundaries and to refine in regions of complex flow. If the structure of the flow is constantly changing, or the flow includes a free surface, definition of the most appropriate cell structure becomes more challenging. An alternative starting point, based on a Lagrangian description of the hydrodynamics, is to represent the fluid by a large number of resolution elements (often called IT Professional Published by the IEEE Computer Society \u00a9 2019 IEEE 1 This article has been accepted for publication in Computing in Science & Engineering. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/MCSE.2021.3134604 \u00a9 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.",
    "authors": [
        {
            "affiliations": [],
            "name": "ExCALIBUR MPPH"
        },
        {
            "affiliations": [],
            "name": "R. G. Bower"
        }
    ],
    "id": "SP:e7879097f3bd5bc2235d2178addb06423352f8a7",
    "references": [
        {
            "authors": [
                "M. Shadloo",
                "G. Oger",
                "D. Le Touz\u00e9"
            ],
            "title": "Smoothed particle hydrodynamics method for fluid flows, towards industrial applications: Motivations, current state, and challenges,\u201d Computers & Fluids, vol",
            "venue": "136, p. 11\u201334,",
            "year": 2016
        },
        {
            "authors": [
                "J. Schaye"
            ],
            "title": "The EAGLE project: simulating the evolution and assembly of galaxies and their environments,",
            "venue": "MNRAS, vol. 446,",
            "year": 2015
        },
        {
            "authors": [
                "J. MacLeo"
            ],
            "title": "and T",
            "venue": "Rendall, \u201cSimulation of Aircraft Fuel Jettison Using WCSPH,\u201d in Proceedings of the 14th International SPHERIC Workshop, Jun.",
            "year": 2019
        },
        {
            "authors": [
                "H. Gotoh",
                "A. Khayyer"
            ],
            "title": "On the state-of-theart of particle methods for coastal and ocean engineering,\u201d Coastal Engineering Journal, vol",
            "venue": "60, no. 1, p. 79\u2013103,",
            "year": 2018
        },
        {
            "authors": [
                "M. J"
            ],
            "title": "Dom\u0131\u0301nguez et al., \u201cDualSPHysics: from fluid dynamics to multiphysics problems,",
            "venue": "Computational Particle Mechanics,",
            "year": 2021
        },
        {
            "authors": [
                "R. Canelas",
                "M. Brito",
                "O. Feal"
            ],
            "title": "J",
            "venue": "Dom\u0131\u0301nguez, and A. Crespo, \u201cExtending dualsphysics with a differential variational inequality: modeling fluid-mechanism interaction,\u201d Applied Ocean Research, vol. 76, pp. 88\u2013 97,",
            "year": 2018
        },
        {
            "authors": [
                "A. Tasora",
                "M. Anitescu"
            ],
            "title": "A matrix-free cone complementarity approach for solving large-scale, nonsmooth, rigid body dynamics,\u201d Computer Methods in Applied Mechanics and Engineering, vol",
            "venue": "200, no. 5-8, pp. 439\u2013 453, Jan.",
            "year": 2011
        },
        {
            "authors": [
                "P. Gonnet"
            ],
            "title": "Efficient and scalable algorithms for smoothed particle hydrodynamics on hybrid shared/distributed-memory architectures,\u201d SIAM Journal on Scientific Computing, vol",
            "venue": "37, no. 1, pp. C95\u2013C121,",
            "year": 2015
        },
        {
            "authors": [
                "P. Gonnet",
                "A.B.G. Chalk",
                "M. Schaller"
            ],
            "title": "QuickSched: Task-based parallelism with dependencies and conflicts,\u201d arXiv e-prints, p",
            "venue": "arXiv:1601.05384, Jan",
            "year": 2016
        },
        {
            "authors": [
                "J.S. Willis",
                "M. Schaller",
                "P. Gonnet",
                "R.G. Bower"
            ],
            "title": "and P",
            "venue": "W. Draper, \u201cAn Efficient SIMD Implementation of Pseudo-Verlet Lists for Neighbour Interactions in Particle-Based Codes,\u201d in Proceedings of the PASC Conference, Apr.",
            "year": 2018
        },
        {
            "authors": [
                "X. Guo",
                "B.D. Rogers",
                "S. Lind",
                "P.K. Stansby"
            ],
            "title": "New massively parallel scheme for incompressible smoothed particle hydrodynamics (isph) for highly nonlinear and distorted flow,\u201d Computer Physics Communications, vol",
            "venue": "233, pp. 16\u201328,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "PARTICLE HYDRODYNAMICS. There are traditionally two distinct main approaches to solving hydrodynamics problems. In the Eulerian approach, a fixed mesh covers space and the equations govern how fluid moves from cell to cell. This approach is widely used and, while rectangular grids are an obvious choice, the grid structure can be adapted to trace the structure of boundaries and to refine in regions of complex flow. If the structure of the flow is constantly changing, or the flow includes a free surface, definition of the most appropriate cell structure becomes more challenging. An alternative starting point, based on a Lagrangian description of the hydrodynamics, is to represent the fluid by a large number of resolution elements (often called\nIT Professional Published by the IEEE Computer Society \u00a9 2019 IEEE 1\n\u00a9 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.\nDepartment Head\nparticles) that are advected with the flow. Since the particles naturally follow the flow structure, this has important advantages. The scheme does not require a predefined grid geometry, making it very suitable for tracking flows with moving boundaries, particularly flows with free surfaces, and problems that involve the mixing of different fluids, flows with physically active elements or large dynamic range.\nIn this second category, a popular way to discretise the fluid equations is Smoothed Particle Hydrodynamics (SPH). The core principle of this method is that local values of thermodynamic properties, such as density, pressure or temperature, as well as fluxes, are calculated by averaging over neighbouring particles using a weighting kernel. This scheme was originally developed to study stellar collisions, but is widely adopted in engineering and cosmology because of its flexibility, adaptivity and natural multi-physics capability. The range of applications of the method is growing rapidly [1] and is being adopted by a rapidly growing range of commercial companies in aerospace engineering, petroleum engineering, energy companies, tyre manufacturers, and automotive companies.\nMore recently, hybrid schemes have evolved combining the Lagrangian approach with interactions based on an adaptive mesh that is based on particle positions. Nevertheless, the fundamentals of such adaptive Lagrangian element schemes are similar to SPH, requiring the tracking and identification of a variable number of neighbouring particles. Computationally, we can separate the physics of the interaction between particles (or fluid elements) from the neighbour finding and book-keeping that is required by the adaptivity of the approach.\nThe unusual interest in a common method by both the theoretical astrophysics and applied engineering communities stems from particle-based methods\u2019 intrinsic ability to adapt to scenarios where the computational domain is far from uniform by placing and moving particles only where they are needed, whether in the rolls of a tsunami wave or in the dense disk of gas where stars form. This non-uniformity of the computational workload, however, can pose immense challenges to the practitioners targeting the ever-growing evermore-distributed modern computing facilities.\nThe widespread use of SPH, its growing adoption across a wide range of science domains, and the associated HPC challenges, have made particle hydrodynamics a priority use case for the ExCALIBUR initiative. The Massively Parallel Particle Hydrodynamics (MPPH) working group1 arose from the strong synergies between applications in astrophysics and engineering. Massively parallel simulations with billion to hundreds of billions of particles have the potential for revolutionising our understanding of the Universe [2] and will empower engineering applications of unprecedented scale, ranging from the end-to-end simulation of transients (such as bird strike) in a jet engine [3], to simulation of tsunami waves over-running a series of defensive walls[4]. The widespread interest in particle hydrodynamics methods is evidenced by the popularity of the international SPHERIC 2 community and the wide diversity of industrial applications represented."
        },
        {
            "heading": "THE ORIGINS OF THE EXCALIBUR PROGRAMME",
            "text": "Although astrophysics and engineering may seem worlds apart, the numerical methods involved share strong common themes. Both center on modelling hydrodynamical flows. While the origin of the flow is very different: externally imposed in many engineering applications vs. driven by the force of gravity in astrophysics; the need to track the flow and the pressure variations generated is shared. Both applications also need to incorporate additional \u2018micro-scale\u2019 physical processes: for example, star formation and supernova explosions in astrophysics or reactive flow elements in engineering; which leads to strong variations in the computational work required per particle. There are also significant differences. Engineering applications do not usually lead to large variations in density (and hence time-step length) while astrophysics problems, particularly cosmology, lead to variations of more than 6 orders of magnitude. An example calculation is shown in Fig. 1. This makes astrophysics problems a challenging test-bed, from which new methods for industrial application will arise. The ExCALIBUR working group has emerged from\n1https://www.dur.ac.uk/icc/cosma/excalibur/mpphea/ 2SPH rEsearch and engineeRing International Community\nhttps://www.spheric-sph.org\n2 IT Professional\n\u00a9 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.\nthe engineering-astrophysics dialogue, and aims to bring together two leading codes from the astrophysics and engineering areas: SWIFT and DUALSPHYSICS.\nSWIFT (SPH With Inter-dependent Finegrained Tasking)3 is a state-of-the-art coupled gravity and SPH fluid dynamics solver that uses a novel fine-grained task parallelism approach on top of pthread and MPI to achieve impressive scaling on modern vectorized multicore and multi-node high-performance computers [5]. SWIFT \u2019s primary application is to run large, high-fidelity hydrodynamic simulations of the universe. These are complex multi-physics simulations that account for processes such as star and black hole formation, metal enrichment and complex gravitational interactions as well as the hydrodynamics of gas flows. As we write, SWIFT is carrying out some of the world\u2019s largest calculations on the DiRAC4 computing system in the UK.\nThe wide range of density contrasts and physical processes involved in such simulations present\n3www.swiftsim.com 4https://dirac.ac.uk/\na challenging problem. Challenging problems require revolutionary solutions, not just evolution of tested methods, and so SWIFT adopts a finegrained approach to task parallelism, breaking down the time integration step into a complex graph of interlinked tasks. An example is shown in Fig. 2. Each task corresponds to a set operation to be performed on a chunk of memory (here a series of particles) The tasks may be executed in any order consistent with the dependencies (e.g. compute the forces on a set of particles before integrating in time) and conflicts (no two tasks acting on the same data shall execute concurrently) expressed by the graph. As we discuss below, this approach is particularly suited to the high dynamic range and multi-physics nature of the cosmological problem.\nThe large range of densities and temperatures seen in astrophysics problems means that evolving the whole particle distribution using a single, global, time-step size is inefficient. SWIFT makes maximal use of the adaptivity and Lagrangian nature of the SPH method, computing local timesteps and hence avoiding unnecessary calculations by only updating the necessary particles.\nMay/June 2019 3\n\u00a9 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.\nDepartment Head\n4 IT Professional\n\u00a9 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.\nThis focuses computational effort on the critical regions of the simulation where flows are most complex.\nLarge calculations must be spread over multiple compute nodes because of their memory requirements. As we discuss below, an interesting development is that communication between nodes can be then handled within the task system using asynchronous MPI to exchange boundary elements. This allows communication to take place in parallel with the execution of other purely local tasks, making fabric latencies invisible. The large variations in complexity across the simulated universe mean that the division of particles between compute nodes must be determined dynamically. In SWIFT, the workload of tasks is continuously monitored and used to automatically adapt the allocation of sections of the domain to each compute node. This process is done by turning the problem into the wellunderstood question of splitting a graph, in our case, the graph of all interconnected tasks. Crucially, this guarantees that the work is split evenly, not the data and it takes into account the adaptive time-steps so that frequently updated cells are located well within the spatial boundaries of each domain. SWIFT is open-source software and includes multiple flavours of SPH as well as many example problems that can be used as the basis for development.\nDUALSPHYSICS5 is a state-of-the-art GPU enabled SPH code [6]. The current version of the code focuses on engineering applications, and the code includes modules that couple different types of flows, boundary conditions and additional physics. The code places particular emphasis on exploiting the processing power of Graphical Processing Units (GPUs). The opensource DUALSPHYSICS code has two different versions. One optimized for multi-core CPUs and another for single-GPU execution. The latter allows simulating 80 million particles at runtimes of hours/days, instead of months using multi-core CPUs.\nThe DUALSPHYSICS code has an extensive user base, with 1000s of international downloads annually, and its licensing is designed to be as flexible as possible so that it can be used in\n5https://dual.sphysics.org/\ncommercial application while remaining opensource. Part of its widespread appeal is the wide range of additional physics modules that have been integrated into the package so that it can be used to model a wide range of challenging engineering applications. The example in Fig 3 shows the use of DUALSPHYSICS in the development of a wave-driven power plant. Here the problem involves the challenging interaction of the complex motion of the floating body with the surface hydrodynamics of the incoming waves. Such applications allow designers to explore the stresses on the different parts of the structure without an expensive iterative design process.\nHowever, engineering problems often combine three factors that are not approachable by using the current single-GPU version of DUALSPHYSICS: the simulation of large domains; the adoption of high resolution in order to represent complex geometries; and long-lasting events that require extreme run times. The combination of large domains with high resolution results in hundreds of millions to billions of particles whose data volume cannot be processed in a single GPU due to its limited memory capacity. Extension of DUALSPHYSICS to multi-node simulations would open large domains of science application. On the other hand, the execution time of a calculation step grows rapidly with the number of particles and with the duration of the numerical experiment under scope: greater adaptivity would allow the computational effort to be focused on the most critical areas of the flow.\nTHE INGREDIENTS SWIFT and DUALSPHYSICS provide the ingredients that the MPPH working group is bringing together to define software for the next generation of computing hardware. The most important ones are summarised below.\nFine-grained task parallelism The cosmological science driver requires extreme levels of adaptivity in order to account for the wide range of timescales, from the expansion rate of the universe to the lifetimes of massive stars and the accretion rates of black holes. With some processes requiring a dynamic range in time-steps in excess of 104, it is critical that the codes take full advantage of the adaptivity\nMay/June 2019 5\n\u00a9 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.\nDepartment Head\navailable. The task parallelism approach in SWIFT naturally allows for a hierarchy of time-steps and spatial scales, making the solution of the underlying equations optimally efficient. Together with the wide range of physical processes involved, this level of adaptivity makes load balancing in conventional loop-based parallelism a challenging problem since the work associated with each particle or group of particles varies dramatically.\nIn the task-based approach to parallelism, the problem (integrating the Universe for a time-step) is broken down into a series of tasks as we have described earlier. In a simple SPH calculation, for example, the tasks would involve updating particle densities and pressures, calculating the net force, updating particle positions and trajectories. A task-graph links the tasks together, accounting for the dependencies and conflicts. Optimally executing the resulting network of tasks is then a separate challenge, solved by the task scheduler. A fine-grained task-based approach provides the adaptivity required to schedule work with the flexibility and concurrency that is required.\nSWIFT was initially conceived by P. Gonnet at Durham University as a demonstrator program\n[9], derived from a software originally designed for molecular dynamics problems, to illustrate a novel approach to parallelism based on the decomposition of the neighbour finding work into fine-grained tasks. This initial demonstrator showed significant promise and the subsequent development of SWIFT as a cosmological code was funded through STFC consolidated grants, the DiRAC computing network, and the INTEL Parallel Computing Centre at Durham. This funding was primarily directed at the cosmological version of the code. The ExCALIBUR MPPH working group currently funds new development of the code for exascale applications in Engineering and Astrophysics.\nSince SWIFT was designed to demonstrate task parallelism from the outset, the definition of a task can be carefully chosen and adapted for the target application in mind.\n\u2022 Firstly, particles are assigned to a hierarchical tree, each leaf node containing a target number of particles (typically 300) and a volume that is well matched to the smoothing length of the SPH kernel. This ensures that tasks interact only with their surrounding neighbour cells\n6 IT Professional\n\u00a9 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.\nlimiting the spatial extent of task dependencies. Representing each leaf cell (the smallest cell in the hierarchical tree) as a task would, however, result in a large number of tasks: sorting through the list of tasks to select the next task to run would take an excessive computational time. This is avoided by defining tasks at a higher level, themselves recursively running over the full depth of the tree. \u2022 In order to avoid excessively long and complex chains of task dependencies, \u2018ghost\u2019 tasks (i.e. tasks not performing any work) are used to aggregate similar dependencies from surrounding cells. This reduces and simplifies the time spent by the task engine, selecting the next task to run. Tasks are prioritised for execution based on the amount of dependent work: this ensures that the most demanding tasks are initiated at the start of the step. Since different task types can be interleaved this avoids waiting for long-running tasks to complete greatly freeing up opportunities for concurrent execution. The scheduling system also attempts to schedule tasks acting on the same data on the same core in order to maximise cache re-use but, within a node, task stealing (one core grabbing tasks that had been pre-assigned to another core\u2019s queue) allows optimal use of shared memory, while conflicts are avoided by the task specification. The use of fine-grained tasks ensures that the run time of individual tasks is much shorter than the overall time-step. This allows the scheduling engine greater flexibility and thus provides greater parallelism. \u2022 In production runs, the memory requirement demands that the spatial domain is distributed over several compute nodes. On exascale systems, the full calculation may be distributed over 10000s of nodes, each connected by a high speed network fabric. Although network speeds can be impressive it is essential to avoid unnecessary communication between nodes. The science problem is distributed over the nodes to both equalise the computational effort (and memory demand) and to minimise communication. This is achieved by using task runtime estimates based on earlier time-steps. The boundaries between domains are handled by replicating boundary cells across nodes.\nThis allows communication between nodes to be handled as part of the task parallelism, with the tasks being added to asynchronously send and receive data between nodes. The use of asynchronous communications avoids scheduler stalling while data is in-flight since communication delays can be overlapped with arithmetic work in other cells. An example of the execution of the task graph on a multi-node system is shown in Fig. 4.\nAsynchronous communications The task system that we use in SWIFT matches well with asynchronous communication, a key feature of the MPI standard. In contrast to conventional data exchanges that require both sides of the exchange to pause while data is shipped from one node to another, asynchronous communication allows the two sides to continue with computational work while the data is being copied between nodes across the fabric. On the sending side, data is simply flagged for transport and handed over to the MPI sub-system, while on the receiving side, the scheduler polls to see if the data has arrived and then unlocks the dependent tasks once it has. This works extremely well during time-steps that involve a large fraction of particles, effectively hiding the time taken for the MPI communication layers to transport the data.\nIn steps in which a smaller fraction of particles are updated, the situation is more challenging however. We find that some messages fail to progress between nodes, causing communication delays that may greatly exceed expected network latency. This unexpected, and unpredictable, delay is hard to hide because of the lower number of particle updates. During these steps this limits the code\u2019s efficiency. As the scale of the calculation grows (resulting in a consequent increase in the number of MPI communications simultaneously \u201cin flight\u201d) this limitation of the communication sub-system software stack (from the MPI library to the linux kernel) grows to dominate the code execution time. In-depth investigations of the communications, in part by deconstructing them into local copies plus RDMA copies over the fabric, seem to indicate that there are multiple causes but with a common pattern. The various layers will in-turn run out of buffer space or list\nMay/June 2019 7\n\u00a9 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.\nDepartment Head\n8 IT Professional\n\u00a9 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.\nof messages and will have to use precious CPU time to manage these situations before being able to hand the execution back to the application (or other layer). In parallel to this, having all the cores do intense work on the local data means that we sometimes reach the limits of the RAM bandwidth, leaving little throughput to the communication layers to move data around.\nTask execution on GPUs The computing architecture of forthcoming ExCALIBUR machines is currently unclear, but it seems likely discrete accelerator units (such as general purpose Graphical Processing Units, GPUs) will form a key part of any viable exascale machine because of their power-per-flop advantage. The DUALSPHYSICS team plays a critical role thanks to their experience of efficiently exploiting such units. Although SWIFT makes extensive use of vectorization with CPUs, exploiting GPU performance within a flexible task scheduling framework requires further development. One of the key challenges is to ensure that communications between the CPU based code and the GPU can be integrated and \u2018hidden\u2019 within the task graph in a similar way to the handling of MPI communications.\nGPU accelerators are best suited to tasks that apply the same operation over a large number of particles. For engineering simulations where SPH particles are relatively uniform with compact neighbour lists, SPH is therefore ideally suited to the streaming multiprocessors and memory hierarchy of a GPU. The SWIFT and DUALSPHYSICS codes have developed novel algorithmic approaches to make this neighbour finding operation as efficient and as vectorised as possible, which will be combined in our MPPH effort.\nThe algorithmic work of the task also needs to be sufficient that the data exchange costs can be offset. The scheduling system will need to determine which tasks can be best executed on the GPU, perhaps even deciding on the level of the cell hierarchy (and hence the number of particles per tree level) which is best suited to the GPU. This will require a greater level of \u2018intelligence\u2019 to be built into the task scheduling process. We expect to obtain the best returns in problems with only a shallow time-step hierarchy so that the number of particles actively integrated\nin each step is large, a pattern very common in the targeted engineering applications.\nSeparation of concerns From the outset, the SWIFT code has been developed from a principle of \u2018separation of concerns\u2019, making it as simple as possible for users from diverse science domains to implement new physics modules. A number of templates developed for the astrophysics application can be readily extended to new applications. There are essentially four distinct components in the code:\n\u2022 Interaction algorithms: these form the basic algorithmic work of the problem, calculating densities, pressures and forces. The nature and form of this interaction will vary with the science domain and the numerical approach to the problem. The ideal abstraction would allow the user to specify only the interaction and the particle properties involved. These interactions form the basic task set. The current version of the code is not at this stage yet with the application scientist still having to specify the task-dependency graph in C and write the interaction in C as well; no domain specific language is currently in use to help streamline that latter part. \u2022 Task graph engine: Starting from a userspecified task graph, the engine must create the relevant tasks for each step and then choose an optimal order for execution. In addition to the interaction tasks, the engine needs to include communication tasks and assign tasks to a suitable level of the cell hierarchy. Optimal execution may involve work stealing between cores, so that long-running tasks do not become a bottleneck in the execution speed. The system uses its in-built QUICKSCHED task scheduling engine [10] to carry out these tasks. \u2022 Cell hierarchy: the code gains efficiency by ordering particles into cells. The hierarchical structure limits the range of possible particle interactions and greatly increases the speed of the algorithm. As the simulation progresses, however, and particles move relative to each other, this cell structure becomes stale and needs to be refreshed. Typically small numbers of particles need to be exchanged between cells. The optimal algorithm for refreshing\nMay/June 2019 9\n\u00a9 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.\nDepartment Head\nand reconstructing the cell structure may be problem dependent. \u2022 Domain decomposition: For calculations that span multiple compute nodes, the overall task graph must be split so that the work load and memory requirement of each node is similar. At each split, additional tasks must be inserted into the network in order to handle communication and exchange of data. In order to decide on an optimal split, the code accumulates work estimates for each cell, based on previous timesteps. The mathematical problem of finding an optimal division is handled by the METIS graph partitioning library6.\nThese separation of concerns principles will also make it possible to implement a wide variety of SPH flavours (and other Lagrangian approaches) within a single structure. Indeed, it is likely that the optimal method will depend on the problem to be solved, and the appropriate level of approximation (e.g. order of convergence of the scheme). We do not expect the working group to develop these methods themselves, but rather to capitalise on state-of-the-art numerical methods developments within the members\u2019 own institutes as well as via contributions from the wider community to the open-source stack. The separation of concerns in SWIFT minimises the new coding work, and internal system knowledge, that is required to make such improvements. Application scientists need not know about the oftencomplex parallelisation concepts; they can focus their attention on the purely numerical aspects."
        },
        {
            "heading": "THE CHALLENGES",
            "text": "SWIFT scales well on the largest systems that we have available for testing. For example, large runs of a Kelvin-Helmholtz instability problem show no loss of weak-scaling performance (increasing the problem sizes as computation resource increases) up to more than 10, 000 threads. Thanks to the task parallelism and hidden communication overheads, it represents a promising approach to parallelism that can be rolled out across many science domains and applications. The approach is particularly suitable for problems that include strong spatial inhomogeneities in the workload, such as those\n6https://github.com/KarypisLab/METIS\ndriven by \u201csub-grid\u201d physics modules, and problems where surfaces are free so that a covering grid cannot be defined at the outset. In this section, we look at some of the challenges in developing the SWIFT code further. These points are the focus of the MPPH working group.\nCommunication Asynchronous communication allows processing threads to continue the execution of algorithmic work once data has been flagged for exchange. The exchange itself, however, still requires processing work to be done, for example copying data into the MPI library\u2019s communication buffers. In practice, this can lead to unexpected delays in messages being transferred between nodes, particularly in steps where the communication predominantly occurs between a small number of node pairs. This creates a problem since the delay is often unpredictable and can be long compared to the granularity of other tasks. The unpredictability of these latencies makes it difficult to hide the communication tasks with computational work. In order to fully understand the nature of the progression problem, we have developed an Remote Direct Memory Access (RDMA) implementation of the communication layer. Although this could not be easily implemented in sustainable production code destined to run on a variety of different network architectures, it makes it possible to fully control the data flow and thus to optimally identify better strategies within the MPI library.\nGPU enabled code SWIFT \u2019s initial development has targeted CPU-based systems, including the implementation of AVX512 optimised kernels [11]. Since GPU\u2019s are likely to form a key part of exascale systems, the development of a suitable strategy is critical. Linking up with DUALSPHYSICS provides the experience and expertise to write efficient kernels, building in part from the techniques developed for vectorised CPU code. The initial challenge is the development of on/off load communications as tasks that can be hidden within the arithmetic work load of both CPU and GPUs. Initial results for this are promising, showing that the overlapping of work and communication is effective. A new set of challenges then arise. Since efficient execution on the GPU requires a large particle load of similar particles, the granularity of data for the CPU and GPU will\n10 IT Professional\n\u00a9 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.\nbe different. A degree of intelligence will then be required from the scheduling engine in order to estimate whether tasks should be sent to the GPU or retained on the CPU in order to achieve the shortest possible overall execution time. This, in turn, will affect the level at which tasks should be run in the cell hierarchy.\nNew Algorithms Codes that can exploit a high level of time-step adaptivity optimally solve the problem at hand by avoiding unnecessary calculations. The difficulty, however, is that extreme levels of adaptivity can be hard to strong-scale to many core architectures, particularly as the compute node-count increases. It becomes impossible to achieve good parallel performance as the particle load in the smaller numbers of steps approaches the core count. One possible solution to this is to investigate novel ways of parallel time stepping, allowing parts of the computation to step ahead without breaking the limits imposed by strict causality. The mathematical approach to this will form part of the work of the cross-cutting panel linking multiple working groups. Implementation will be challenging, however, since the distribution of work between compute nodes will need to be adjusted to account for the dependence on the time-step hierarchy.\nSeparation of concerns SWIFT \u2019s code base has been developed with flexibility of application in mind, but in practice it requires significant knowledge of the task system to implement completely new problems. A greater level of separation of concerns is key to wider use of the finegrained task-parallel approach, so that domain scientists need only specify the task graph, and provide task kernels, leaving the implementation of the task scheduling and communication to the task subsystem. Although an important principle, achieving a clearer separation without compromising performance is challenging. In principle, specifying the problem as a task graph should allow automated construction of the tasks and their dependencies, but in practice, manual construction allows optimal control of the depth of task execution within the cell hierarchy, and the use of ghost tasks in order to aggregate dependencies is essential to avoid the code runtime becoming dominated by the task engine\u2019s own book-keeping. The issues run deeper when datastructures are considered since the optimal speed\nof MPI exchange requires data volumes to be minimised, a principle that conflicts with independence of the exchange and the kernels that will subsequently execute.\nThe exascale challenges going forward for the DUALSPHYSICS code are multiple: Firstly, of paramount importance is ensuring efficient connection of multiple GPUs, overlapping of concurrent GPU tasks across multiple GPUs and minimisation of latencies in inter-GPU communication. Secondly, is how to implement boundary conditions (BCs) in an exascale code such that developing SPH theories for BCs can be easily implemented without needing to restructure the code. For example, a boundary that extends across multiple compute accelerator devices (GPUs or otherwise) must be treated in a consistent and mathematically rigorous way which often requires multiple sweeps across the entire set of fluid and boundary particles. Finally, many of the additional physics that make SPH attractive for engineering applications are currently provided by external libraries. Including these in an exascale code is a challenge and now being examined in a closely related ExCALIBUR cross-cutting project looking at coupling (EP/W00755x/1)."
        },
        {
            "heading": "FUTURE PROSPECTS",
            "text": "In this article we have introduced the MPPH working group and outlined some novel aspects of the SWIFT and DUALSPHYSICS codes on which the working group has centered.\nSWIFT provides a fine-grained approach to task parallelism that allows the order of task execution to depend only on the requirement that dependencies are satisfied and that concurrent execution of conflicting tasks is avoided. This leads to a powerful flexibility in the order of execution that can absorb the variations in time-step and physics experienced by different parts of the flow. The task system also allows communication latencies to be hidden within the algorithmic work through the use of asynchronous MPI.\nDUALSPHYSICS demonstrates the power of exploiting GPU systems to accelerate the simulation of particle hydrodynamics. Although this is currently limited to execution on single GPU systems, the working group is exploring how the advanced GPU kernels from DUALSPHYSICS could be integrated into SWIFT \u2019s task-based\nMay/June 2019 11\n\u00a9 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.\nDepartment Head\napproach. By selecting appropriate tasks for GPU execution, a similar approach to MPI communication will allow for the overlap of task execution and data exchange. This allows the GPU to work efficiently without requiring that a complete copy of the full particle data is held in GPU memory.\nThere are many challenges to be overcome in optimising the performance of the code, and we have highlighted the issues in MPI communication progression and in the adaptations necessary to select and execute tasks on the GPU. These developments show promising progress, but need to be tempered by the desire to achieve greater separation of concerns, making it simpler to implement new algorithms within the task framework. One particular approach of interest is to combine the time-step parallelism inherent in SWIFT with a time-parallel approach to integration of causally distinct regions.\nThe MPPH working group is working closely with cross-cutting groups that span the range of ExCALIBUR application domains. We have strong ties with the parallel-in-time cross-cutting group and with the task-based parallelism crosscutting group. The latter, in particular, aims to investigate whether the flavour of task-based demonstrated in SWIFT could be of benefit to a wider range of scientific communities.\nThe current project aims at developing exascale algorithms for explicit SPH solvers of the (weakly) compressible Navier-Stokes equations. This avoids the need for a massively parallel pressure projection solver [12] for strictly incompressible flows. Implementing a linear algebra solver that scales efficiently on an exascale system will be a significant achievement in its own right and this will be considered when both the core SPH solver and linear algebra solver reached maturity on exascale systems. Similarly, extending this to the additional multiphysics and multiple physical scales that characterise engineering applications (such as hydroelastic fluidstructure interactions) is a future challenge. This will commence in a forthcoming stage of the work, initially focusing on efficient implementation of boundary conditions."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "The work of the Massively Parallel Hydrodynamics working group has been made\npossible through the award of EPSRC grant EP/V001523/1. We would like to thank all the members of the working group for their input and in helping push forward the ideas outline in this article. This work used the DiRAC@Durham facility managed by the Institute for Computational Cosmology on behalf of the STFC DiRAC HPC Facility (www. dirac.ac.uk). The equipment was funded by BEIS capital funding via STFC capital grants ST/K00042X/1, ST/P002293/1, ST/R002371/1 and ST/S002502/1, Durham University and STFC operations grant ST/R000832/1. DiRAC is part of the National e-Infrastructure. MS is supported by the Netherlands Organisation for Scientific Research (NWO) through VENI grant 639.041.749. The underlying astrophysics research is supported by the Science and Technology Facilities Council through grant ST/P000541/1. Astrophysics figures were created with the SWIFTSIMIO software package7. We thank J. Borrow, C. Correa and S. McAlpine for contributing images.\nREFERENCES 1. M. Shadloo, G. Oger, and D. Le Touze\u0301, \u201cSmoothed\nparticle hydrodynamics method for fluid flows, towards\nindustrial applications: Motivations, current state, and\nchallenges,\u201d Computers & Fluids, vol. 136, p. 11\u201334,\n2016. [Online]. Available: https://www.sciencedirect.\ncom/science/article/pii/S0045793016301773\n2. J. Schaye et al., \u201cThe EAGLE project: simulating the\nevolution and assembly of galaxies and their environ-\nments,\u201d MNRAS, vol. 446, no. 1, pp. 521\u2013554, Jan.\n2015.\n3. J. MacLeod and T. Rendall, \u201cSimulation of Aircraft Fuel\nJettison Using WCSPH,\u201d in Proceedings of the 14th\nInternational SPHERIC Workshop, Jun. 2019, pp. 204\u2013\n211.\n4. H. Gotoh and A. Khayyer, \u201cOn the state-of-the-\nart of particle methods for coastal and ocean\nengineering,\u201d Coastal Engineering Journal, vol. 60,\nno. 1, p. 79\u2013103, 2018. [Online]. Available: https:\n//doi.org/10.1080/21664250.2018.1436243\n5. M. Schaller, P. Gonnet, A. B. G. Chalk, and P. W.\nDraper, \u201cSWIFT: Using Task-Based Parallelism, Fully\nAsynchronous Communication, and Graph Partition-\nBased Domain Decomposition for Strong Scaling\non More Than 100,000 Cores,\u201d in Proceedings of\n7https://github.com/SWIFTSIM/swiftsimio\n12 IT Professional\n\u00a9 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.\nthe PASC Conference, ser. PASC 16. New York,\nUSA: ACM, 2016, pp. 2:1\u20132:10. [Online]. Available:\nhttp://doi.acm.org/10.1145/2929908.2929916\n6. J. M. Dom\u0131\u0301nguez et al., \u201cDualSPHysics: from fluid dy-\nnamics to multiphysics problems,\u201d Computational Parti-\ncle Mechanics, Mar. 2021.\n7. R. Canelas, M. Brito, O. Feal, J. Dom\u0131\u0301nguez, and\nA. Crespo, \u201cExtending dualsphysics with a differential\nvariational inequality: modeling fluid-mechanism\ninteraction,\u201d Applied Ocean Research, vol. 76, pp. 88\u2013\n97, 2018. [Online]. Available: https://www.sciencedirect.\ncom/science/article/pii/S0141118717307502\n8. A. Tasora and M. Anitescu, \u201cA matrix-free cone comple-\nmentarity approach for solving large-scale, nonsmooth,\nrigid body dynamics,\u201d Computer Methods in Applied\nMechanics and Engineering, vol. 200, no. 5-8, pp. 439\u2013\n453, Jan. 2011.\n9. P. Gonnet, \u201cEfficient and scalable algorithms\nfor smoothed particle hydrodynamics on hy-\nbrid shared/distributed-memory architectures,\u201d SIAM\nJournal on Scientific Computing, vol. 37,\nno. 1, pp. C95\u2013C121, 2015. [Online]. Available:\nhttps://doi.org/10.1137/140964266\n10. P. Gonnet, A. B. G. Chalk, and M. Schaller,\n\u201cQuickSched: Task-based parallelism with dependen-\ncies and conflicts,\u201d arXiv e-prints, p. arXiv:1601.05384,\nJan 2016.\n11. J. S. Willis, M. Schaller, P. Gonnet, R. G. Bower,\nand P. W. Draper, \u201cAn Efficient SIMD Implementation\nof Pseudo-Verlet Lists for Neighbour Interactions in\nParticle-Based Codes,\u201d in Proceedings of the PASC\nConference, Apr. 2018.\n12. X. Guo, B. D. Rogers, S. Lind, and P. K.\nStansby, \u201cNew massively parallel scheme for\nincompressible smoothed particle hydrodynamics (isph)\nfor highly nonlinear and distorted flow,\u201d Computer\nPhysics Communications, vol. 233, pp. 16\u201328,\n2018. [Online]. Available: https://www.sciencedirect.\ncom/science/article/pii/S0010465518302145\nR. G. Bower is Professor at the Institute of Computational Cosmology, Durham University in the UK. He has a track record of papers that have helped shape our understanding of the formation of galaxies and the processes that shape the observable Universe. His current focus is in developing better numerical simulation codes in order to improve the fidelity with which it is possible to simulate the Universe. He is the PI of the massively parallel particle hydrodynamics DDWG and a member of the DiRAC technical working group. Contact him at r.g.bower@durham.ac.uk\nB. Rogers is Professor at the department of Mechanical, Aeronautical and Civil Engineering, University of Manchester. His research activities include the numerical simulation and experimental investigation of free-surface flows, water waves and coastal hydrodynamics, shallow water equations and modelling natural hazzards such as flooding and tsunamis. He specialises in the use of smoothed particle hydrodynamics (SPH) and finite volume methods (FVMs) accelerated using novel low-energy hardware such as graphics processing units (GPUs) and massively parallel high-performance computing (HPC) systems. He is a founding member of the Smoothed Particle Hydrodynamics rEsearch and engineeRing International Community (SPHERIC). Contact him at Benedict.Rogers@manchester.ac.uk\nM. Schaller is lecturer in numerical cosmology jointly at the Lorentz Institute for theoretical physics and at the Leiden Observatory in the Netherlands. His research focuses on the complex interaction between gas, dark matter, and other components over the course of the evolution of the Universe. He is the lead-developer of the open-source massivelyparallel particle-based astrophysics code SWIFT currently used for multiple large collaboration projects across Europe with substantial CPU time allocations on PRACE and DiRAC systems. Contact: schaller@strw.leidenuniv.nl\nMay/June 2019 13\n\u00a9 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information."
        }
    ],
    "year": 2021
}