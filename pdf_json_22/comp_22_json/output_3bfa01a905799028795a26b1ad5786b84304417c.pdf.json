{
    "abstractText": "Consider a random sample X1, X2, . . . , Xn drawn independently from a parent distribution having cumulative distribution function (cdf) F and probability density function (pdf) f . Let the random variables X(1) \u2264 X(2) \u2264 . . . \u2264 X(n) denote the order statistics of the sample. With this notation, X(1) corresponds to the minimum value of the sample, X(n) corresponds to the maximum value of the sample, and X(n+1 2 ) (provided that n is odd) corresponds to the median of the sample. Order statistics play an important role in statistical sciences; for example, order statistics are instrumental to constructing a class of robust estimators known as L-estimators. The interested reader is referred to [1], [2] for comprehensive summaries on the application and theory of order statistics. In this paper, we study the asymptotic behavior of the distribution of the central order statistics, that is we are interested in the distribution of X(pn) for a fixed p \u2208 (0, 1) as n \u2192 \u221e. Our main contribution is the proof of a strong form of the central limit theorem (CLT) showing that the relative entropy between the Gaussian distribution and the distribution of the order statistics converges to 0 as n grows.",
    "authors": [
        {
            "affiliations": [],
            "name": "Martina Cardone"
        },
        {
            "affiliations": [],
            "name": "Alex Dytso"
        },
        {
            "affiliations": [],
            "name": "Cynthia Rush"
        }
    ],
    "id": "SP:16e3e483c15854bcc723ffab9cc2cb81a6fd059b",
    "references": [
        {
            "authors": [
                "H.A. David",
                "H.N. Nagaraja"
            ],
            "title": "Order Statistics, Third edition",
            "year": 2003
        },
        {
            "authors": [
                "P. Laplace"
            ],
            "title": "Th\u00e9orie analytique des probabilit\u00e9s, deuxi\u00e8me suppl\u00e9ment",
            "venue": "Oeuvr. comp\u0131\u0300, vol. 7, no. 2, pp. 531\u2013580, 1818.",
            "year": 1818
        },
        {
            "authors": [
                "N.V. Smirnov"
            ],
            "title": "Uber die Verteilung des allgemeinen Gliedes in der Variationsreihe",
            "venue": "Metron, vol. 12, pp. 59\u201381, 1935.",
            "year": 1935
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Limit distributions for the terms of a variational series",
            "venue": "Trudy Matematicheskogo Instituta imeni VA Steklova, vol. 25, pp. 3\u201360, 1949.",
            "year": 1949
        },
        {
            "authors": [
                "A.A. Balkema",
                "L. De Haan"
            ],
            "title": "Limit distributions for order statistics. I",
            "venue": "Theory of Probability & Its Applications, vol. 23, no. 1, pp. 77\u201392, 1978.",
            "year": 1978
        },
        {
            "authors": [
                "A.A. Balkema",
                "L. de Haan"
            ],
            "title": "Limit distributions for order statistics. II",
            "venue": "Theory of Probability & Its Applications, vol. 23, no. 2, pp. 341\u2013358, 1979.",
            "year": 1979
        },
        {
            "authors": [
                "R.-D. Reiss"
            ],
            "title": "Approximate distributions of order statistics: With applications to nonparametric statistics",
            "venue": "Springer science & business media,",
            "year": 2012
        },
        {
            "authors": [
                "M.M. Siddiqui"
            ],
            "title": "Distribution of quantiles in samples from a bivariate population",
            "venue": "J. Res. Nat. Bur. Standards B, vol. 64, pp. 145\u2013150, 1960.",
            "year": 1960
        },
        {
            "authors": [
                "L. Weiss"
            ],
            "title": "On the asymptotic joint normality of quantiles from a multivariate distribution",
            "year": 1963
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "The asymptotic distribution of order statistics",
            "venue": "Naval Research Logistics Quarterly, vol. 26, no. 3, pp. 437\u2013445, 1979.",
            "year": 1979
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "The asymptotic joint distribution of an increasing number of sample quantiles",
            "venue": "Annals of the Institute of Statistical Mathematics, vol. 21, no. 1, pp. 257\u2013263, 1969.",
            "year": 1969
        },
        {
            "authors": [
                "W. Feller"
            ],
            "title": "An Introduction to Probability Theory and its Applications",
            "year": 2008
        },
        {
            "authors": [
                "R.-D. Reiss"
            ],
            "title": "On the accuracy of the normal approximation for quantiles",
            "venue": "The Annals of Probability, pp. 741\u2013744, 1974.",
            "year": 1974
        },
        {
            "authors": [
                "S. Ikeda",
                "T. Matsunawa"
            ],
            "title": "On the uniform asymptotic joint normality of sample quantiles",
            "venue": "Annals of the Institute of Statistical Mathematics, vol. 24, no. 1, pp. 33\u201352, 1972.",
            "year": 1972
        },
        {
            "authors": [
                "M. Falk"
            ],
            "title": "A note on uniform asymptotic normality of intermediate order statistics",
            "venue": "Annals of the Institute of Statistical Mathematics, vol. 41, no. 1, pp. 19\u201329, 1989.",
            "year": 1989
        },
        {
            "authors": [
                "R.R. Bahadur"
            ],
            "title": "A note on quantiles in large samples",
            "venue": "The Annals of Mathematical Statistics, vol. 37, no. 3, pp. 577\u2013580, 1966.",
            "year": 1966
        },
        {
            "authors": [
                "M.L. Puri",
                "S.S. Ralescu"
            ],
            "title": "Limit theorems for random central order statistics",
            "venue": "Probability Theory and Extreme Value Theory. De Gruyter Mouton, 2011, pp. 154\u2013182.",
            "year": 2011
        },
        {
            "authors": [
                "Y. Ma",
                "M.G. Genton",
                "E. Parzen"
            ],
            "title": "Asymptotic properties of sample quantiles of discrete distributions",
            "venue": "Annals of the Institute of Statistical Mathematics, vol. 63, no. 2, pp. 227\u2013243, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "J.V. Linnik"
            ],
            "title": "An information-theoretic proof of the central limit theorem with Lindeberg conditions",
            "venue": "Theory of Probability & Its Applications, vol. 4, no. 3, pp. 288\u2013299, 1959.",
            "year": 1959
        },
        {
            "authors": [
                "A.R. Barron"
            ],
            "title": "Entropy and the central limit theorem",
            "venue": "The Annals of probability, pp. 336\u2013342, 1986.",
            "year": 1986
        },
        {
            "authors": [
                "S. Artstein",
                "K.M. Ball",
                "F. Barthe",
                "A. Naor"
            ],
            "title": "On the rate of convergence in the entropic central limit theorem",
            "venue": "Probability theory and related fields, vol. 129, no. 3, pp. 381\u2013390, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "O. Johnson",
                "A. Barron"
            ],
            "title": "Fisher information inequalities and the central limit theorem",
            "venue": "Probability Theory and Related Fields, vol. 129, no. 3, pp. 391\u2013409, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "M. Madiman",
                "A. Barron"
            ],
            "title": "Generalized entropy power inequalities and monotonicity properties of information",
            "venue": "IEEE Transactions on Information Theory, vol. 53, no. 7, pp. 2317\u20132329, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "S.G. Bobkov",
                "G.P. Chistyakov",
                "F. G\u00f6tze"
            ],
            "title": "Rate of convergence and Edgeworth-type expansion in the entropic central limit theorem",
            "venue": "The Annals of Probability, pp. 2479\u20132512, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "S.G. Bobkov",
                "G. Chistyakov",
                "F. G\u00f6tze"
            ],
            "title": "R\u00e9nyi divergence and the central limit theorem",
            "venue": "The Annals of Probability, vol. 47, no. 1, pp. 270\u2013323, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Baratpour",
                "J. Ahmadi",
                "N.R. Arghami"
            ],
            "title": "Some characterizations based on entropy of order statistics and record values",
            "venue": "Communications in Statistics-Theory and Methods, vol. 36, no. 1, pp. 47\u201357, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Characterizations based on R\u00e9nyi entropy of order statistics and record values",
            "venue": "Journal of Statistical Planning and Inference, vol. 138, no. 8, pp. 2544\u20132551, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "M. Abbasnejad",
                "N.R. Arghami"
            ],
            "title": "Renyi entropy properties of order statistics",
            "venue": "Communications in Statistics-Theory and Methods, vol. 40, no. 1, pp. 40\u201352, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "N. Balakrishnan",
                "F. Buono",
                "M. Longobardi"
            ],
            "title": "On cumulative entropies in terms of moments of order statistics",
            "venue": "arXiv preprint arXiv:2009.02029, 2020.",
            "year": 2009
        },
        {
            "authors": [
                "G. Zheng",
                "N. Balakrishnan",
                "S. Park"
            ],
            "title": "Fisher information in ordered data: A review",
            "venue": "Statistics and its Interface, vol. 2, pp. 101\u2013113, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "K.M. Wong",
                "S. Chen"
            ],
            "title": "The entropy of ordered sequences and order statistics",
            "venue": "IEEE Transactions on Information Theory, vol. 36, no. 2, pp. 276\u2013284, 1990.",
            "year": 1990
        },
        {
            "authors": [
                "N. Ebrahimi",
                "E.S. Soofi",
                "H. Zahedi"
            ],
            "title": "Information properties of order statistics and spacings",
            "venue": "IEEE Transactions on Information Theory, vol. 50, no. 1, pp. 177\u2013183, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "A. Dytso",
                "M. Cardone",
                "C. Rush"
            ],
            "title": "Measuring dependencies of order statistics: An information theoretic perspective",
            "venue": "2020 IEEE Information Theory Workshop (ITW). IEEE, 2021, pp. 1\u20135.",
            "year": 2020
        },
        {
            "authors": [
                "O. Rioul"
            ],
            "title": "Yet another proof of the entropy power inequality",
            "venue": "IEEE Transactions on Information Theory, vol. 63, no. 6, pp. 3595\u20133599, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "M. Abramowitz",
                "I.A. Stegun"
            ],
            "title": "Handbook of mathematical functions with formulas, graphs, and mathematical tables",
            "venue": "US Government printing office,",
            "year": 1970
        },
        {
            "authors": [
                "A. Papoulis"
            ],
            "title": "The Fourier integral and its applications",
            "venue": "Polytechnic Institute of Brooklyn, McCraw-Hill Book Company Inc., USA, ISBN: 67-048447-3, 1962.",
            "year": 1962
        },
        {
            "authors": [
                "O. Marchal",
                "J. Arbel"
            ],
            "title": "On the sub-Gaussianity of the beta and dirichlet distributions",
            "venue": "Electronic Communications in Probability, vol. 22, pp. 1\u201314, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S. Boucheron",
                "G. Lugosi",
                "P. Massart"
            ],
            "title": "Concentration inequalities: A nonasymptotic theory of independence",
            "venue": "Oxford university press,",
            "year": 2013
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n20 5.\n04 62\n1v 1\n[ cs\n.I T\n] 1\n0 M\nay 2\n02 2\n\u221a\nn) rate of convergence is established under mild conditions on the parent distribution of the sample generating the order statistics. To prove this result, ancillary results on order statistics are derived, which might be of independent interest.\nI. INTRODUCTION\nConsider a random sample X1, X2, . . . , Xn drawn independently from a parent distribution having cumulative distribution function (cdf) F and probability density function (pdf) f . Let the random variables X(1) \u2264 X(2) \u2264 . . . \u2264 X(n) denote the order statistics of the sample. With this notation, X(1) corresponds to the minimum value of the sample, X(n) corresponds to the maximum value of the sample, and X(n+12 ) (provided that n is odd) corresponds to the median of the sample. Order statistics play an important role in statistical sciences; for example, order statistics are instrumental to constructing a class of robust estimators known as L-estimators. The interested reader is referred to [1], [2] for comprehensive summaries on the application and theory of order statistics.\nIn this paper, we study the asymptotic behavior of the distribution of the central order statistics, that is we are interested in the distribution of X(pn) for a fixed p \u2208 (0, 1) as n \u2192 \u221e. Our main contribution is the proof of a strong form of the central limit theorem (CLT) showing that the relative entropy between the Gaussian distribution and the distribution of the order statistics converges to 0 as n grows.\nA. Prior Work\nThe study of the asymptotic distribution of order statistics has a long history. For example, in [3] Laplace (already in 1818!) established asymptotic normality of the sample median. The asymptotic normality of central order statistics was shown by Smirnov in [4] and, in [5], Smirnov proved a general convergence theorem establishing that a non-degenerate asymptotic distribution of X(rn) can be of only four different types, under the condition that \u221a n( rnn \u2212 p) \u2192 0 for a fixed p \u2208 (0, 1). Further, in [6], [7], Balkema and Haan established that the distribution of order statistics is dense in the space of all distributions in the following sense: there exists a parent cdf such that the limiting cdf of X(rn) is any desired cdf for\nAuthors are listed in alphabetical order.\nan appropriately chosen sequence rn. A succinct summary of these results can be found in [8, p.145].\nThe asymptotic distribution of the joint order statistics has been studied in a series of papers [9], [10], [11], [12]. Berry-Esseen type results [13] for the order statistics have been shown by Reiss in [14]. The convergence in the total variational distance has been studied in [12], [15], [16] and the law of the iterated logarithm for the order statistics has been shown in [17]. Asymptotic results for random order statistics (i.e., X(\u03bd) where \u03bd is an integer valued random variable) have been considered in [18]. All of the aforementioned results assume that the parent distribution has an absolutely continuous cdf, while the asymptotic distribution of discrete random variables has recently been considered in [19].\nThe entropic CLT for the sample mean has been first studied by Linnik in [20] and later considerably generalized by Barron in [21]. There has also been some recent activity around finding the rates of convergence and the interested reader is referred to [22], [23], [24], [25], [26] and references therein.\nInformation measures on the distribution of order statistics have also received some attention [27], [28], [29], [30], [31], [29], [32], [33]. For example, in [34] we showed that the f - divergence between the joint distribution of order statistics and the product distribution of order statistics does not depend on the parent distribution.\nB. Contributions and Outline\nWe establish an entropic version of the CLT for order statistics that ensures a strong mode of convergence in terms of relative entropy. In particular, in Section II we provide mild conditions to ensure that D(X(np)\u2016Gn,p) = O (1/ \u221a n), where Gn,p is a Gaussian random variable and D(\u00b7\u2016\u00b7) denotes the relative entropy. In order to prove the CLT, in Section III we derive some ancillary results on order statistics, which might be of independent interest. For instance, we show a rather general bound on the moments of the order statistics in terms of properties of the parent distribution. Finally, in Section IV we conclude the paper with a discussion on the necessity of the derived conditions for convergence. In particular, we show that (although mild) some of the conditions might not be necessary, whereas others (or a variation of them) do appear to be needed to ensure convergence of the relative entropy. Some of the proofs can be found in the appendices.\nC. Notation\nDeterministic quantities are denoted by lower case letters and random variables are denoted by upper case letters (e.g.,\nx,X). The differential entropy of a random variable V having pdf fV is defined and denoted by\nh(V ) = \u2212 \u222b\nR\nfV (v) log fV (v) dv. (1)\nThe relative entropy between W \u223c fW and V \u223c fV , respectively, is defined and denoted by\nD(W\u2016V ) = \u222b\nR\nfW (x) log fW (x)\nfV (x) dx. (2)\nThe inverse cdf, also known as the quantile function, of the cdf F is defined as\nF\u22121(p) = inf{x \u2208 R : p \u2264 F (x)}, p \u2208 (0, 1). (3)\nThe inverse cdf will play an important role in our analysis. In particular, we will often exploit the following well-known fact [35],\nX(k) d = F\u22121(U(k)), k \u2208 {1, . . . , n}, (4)\nwhere d = denotes equality in distribution and U(1), . . . , U(n) are the order statistics of a sample drawn independently from a parent distribution that is uniform on (0, 1). We will also frequently use the mean and variance of U(k), which can be characterized by noting that U(k) \u223c Beta(k, n+ 1 \u2212 k) [35]; hence, for k \u2208 {1, . . . , n},\nE[U(k)] = k\nn+ 1 , and Var(U(k)) = k(n+ 1\u2212 k) (n+ 1)(n+ 2)2 . (5)\nThe Lp-norm of a function f is denoted as\n\u2016f\u2016p = ( \u222b\nR\n|f(x)|p dx )\n1 p\n, p \u2208 [1,\u221e], (6)\nwhere \u2016f\u2016\u221e is understood as the essential supremum of f . Throughout the paper, we will use the following notation to describe the limiting behavior of functions as its input tends to infinity. We say that f(n) = O(g(n)) for two functions f and g if there exists a k > 0 and an n0 such that f(n) \u2264 kg(n) for all n > n0. Similarly we say that f(n) = \u0398(g(n)) if there exists a k1, k2 > 0 and an n0 such that k1g(n) \u2264 f(n) \u2264 k2g(n) for all n > n0.\nII. MAIN RESULT\nWe begin this section by reviewing the classical CLT for the order statistics due to [35], and then present our entropic CLT result in Theorem 1. In what follows, we let Gn,p \u223c N (\u00b5p, Vn,p) where (\u00b5p, Vn,p) are defined as\n\u00b5p = F \u22121(p), and Vn,p = p(1\u2212 p) n (f(F\u22121(p)))2 . (7)\nConsider a random sample X1, X2, . . . , Xn drawn independently from a parent distribution having cdf F and pdf f . The standard CLT for continuous central order statistics [35] guarantees that for a fixed p \u2208 (0, 1),\nX(np) d\u2192 Gn,p, (8)\n(where d\u2192 indicates convergence in distribution), provided that the following condition holds: t 7\u2192 f(F\u22121(t)) is continuous at the point p and f(F\u22121(p)) > 0.\nOur main result is that the following theorem, which provides a stronger mode of convergence than the classical CLT for order statistics at the expense of extra sufficient conditions.\nTheorem 1. Let X(1), . . . , X(n) be a seqeunce of order statistics generate i.i.d. according to parent cdf F with the pdf f , and let Gn,p \u223c N (\u00b5p, Vn,p) where (\u00b5p, Vn,p) are defined in (7). Fix some p \u2208 (0, 1) and assume that\n1) \u2016f\u2016m <\u221e for some m \u2208 [2,\u221e]; 2) f(F\u22121(p)) 6= 0, and f \u2032(F\u22121(t)) is continuous at t = p;\nand\n3) E[|X |r] <\u221e for some r > 0. Then,\nD(X(np)\u2016Gn,p) = O ( 1/ \u221a n ) .\nProof: We start by recalling the following well known property of the differential entropy (see for example [36]): given a differentiable and bijective function g, we have that\nh (g(V )) = h(V ) + E [log |g\u2032(V )|] . (9) Next, let \u03a6n,p and \u03c6n,p denote the cdf and the pdf of Gn,p (with mean and variance in (7)), and define a function\ng(u) = \u03a6n,p(F \u22121(u)), u \u2208 (0, 1). (10)\nThe derivative of g(u) is given by\ng\u2032(u)= d\ndu \u03a6n,p(F\n\u22121(u))= \u03c6n,p(F\n\u22121(u))\nf (F\u22121(u)) , u \u2208 (0, 1), (11)\nwhich is strictly positive. Therefore, g is bijective. Let U be the uniform random variable on (0, 1). Then,\nD(X(np)\u2016Gn,p) (a) = D ( \u03a6n,p(X(np))\u2016\u03a6n,p (Gn,p) )\n(b) = D ( \u03a6n,p(X(np))\u2016U ) (c) = \u2212h(\u03a6n,p(X(np))) (d) = \u2212h(\u03a6n,p(F\u22121(U(np))) (e) = \u2212h(U(np))\u2212 E[log |g\u2032(U(np))|], (12)\nwhere the labeled equalities follow because: (a) D(W\u2016U) = D(f(W )\u2016f(U)) for any invertible function f ; (b) FX(X) d= U for any random variable X with cdf FX ; (c) D(X\u2016U) = \u2212h(X) for any random variable X \u2208 (0, 1); (d) (4); and (e) (9) and the definition of g in (10), which is bijective.\nNow we study the terms on the right-hand side of (12). In particular, we start focusing on the term E[log |g\u2032(U(np))|]. First, notice that since \u03c6n,p(\u00b7) is the Gaussian pdf having mean and variance given in (7), we have\n\u03c6n,p(F \u22121(u)) =\nexp (\n\u2212 12Vn,p (\nF\u22121(u)\u2212 F\u22121(p) )2 )\n\u221a 2\u03c0Vn,p .\nTherefore, using (11) and the above, we obtain\nE[log |g\u2032(U(np))|]\n= E\n[\nlog\n(\n\u03c6n,p(F \u22121(U(np)))\n1\nf(F\u22121(U(np)))\n)]\n= \u22121 2 log (2\u03c0Vn,p)\u2212 1 2Vn,p E [ ( F\u22121(U(np))\u2212 F\u22121(p) )2 ]\n\u2212 E [ log ( f(F\u22121(U(np))) )]\n= \u22121 2 log\n( 2\u03c0ep(1\u2212 p) n ) + E [ log ( f(F\u22121(p))\nf(F\u22121(U(np)))\n)]\n+ 1 2 \u2212 1 2Vn,p E [ ( F\u22121(U(np))\u2212 F\u22121(p) )2 ] , (13)\nwhere the last equality follows by using the expression of Vn,p in (7). Now, combining (12) and (13), we find that\nD(X(np)\u2016Gn,p) = K1 +K2 +K3, (14a)\nwhere\nK1 = 1\n2 log\n( 2\u03c0ep(1\u2212 p) n ) \u2212 h(U(np)), (14b)\nK2 = 1\n2Vn,p E\n[\n( F\u22121(U(np))\u2212 F\u22121(p) )2 ] \u2212 1 2 , (14c)\nK3 = E\n[\nlog\n( f(F\u22121(U(np)))\nf(F\u22121(p))\n)]\n. (14d)\nIt is worth noting that K1 is independent of the parent distribution, whereas the terms K2 and K3 both depend on the parent distribution. In Section III, we will present ancillary results that show the following facts: (i) K1 = O(1/n) by Lemma 2; (ii) K2 = O(1/ \u221a n) by Corollary 5 (this requires\nAssumptions (2) and (3)); and (iii) K3 = O(1/ \u221a n) by Lemma 6 (this requires Assumptions (1) and (2)). Combing these facts together with (14) and the non-negativity property of the relative entropy, we have that\nD(X(np)\u2016Gn,p) = O(1/ \u221a n).\nThis concludes the proof of Theorem 1.\nWe conclude this section by highlighting that a variety of different distributions (e.g., uniform, Gaussian, exponential) satisfy the conditions of Theorem 1. Moreover, it is also interesting to note that the Cauchy distribution satisfies the conditions despite the fact that in this case E[|X |] = \u221e; hence, the CLT for the sample mean does not hold. This can be seen since the Cauchy pdf clearly satisfies the first two conditions and E[|X |r] <\u221e for 0 < r < 1.\nIII. ANCILLARY RESULTS\nIn this section we present auxiliary results needed for the proof of Theorem 1, some of which may be of interest on their own.\nA. Entropy of Uniform Order Statistics\nWe provide the exact expression for the differential entropy of U(k) for k \u2208 {1, . . . , n}, and an asymptotic expression for the entropy of U(pn) for p \u2208 (0, 1) as n \u2192 \u221e. The proof of Lemma 2 can be found in Appendix A.\nLemma 2. For any k \u2208 {1, 2, . . . , n},\nh(U(k)) = Tk\u22121 + Tn\u2212k \u2212 Tn \u2212Hn, (15)\nwhere for r \u2208 N, with N denoting the natural numbers,\nHr =\nr \u2211\nk=1\n1 k , and Tr = log(r!) \u2212 rHr. (16)\nMoreover, if k = pn for p \u2208 (0, 1), then\nh(U(np))\u2212 1\n2 log\n( 2\u03c0e p(1\u2212 p)\nn\n)\n=\n1 p + 1 1\u2212p \u2212 4 6n + 1 12n2 +O ( 1 n3 ) . (17)\nIt is interesting to note that in Lemma 2, the rate of\nconvergence is O(1/n2) instead of O(1/n) for p = 1/2.\nB. Bound on the Estimation Error of the p-th Quantile\nIn practice, one might desire to estimate the p-th quantile F\u22121(p) of an unknown cdf F . The order statistic X(np) based on an i.i.d. sample from the parent cdf F is a natural estimate for F\u22121(p). It is well known that this estimator is consistent as n \u2192 \u221e [35]. The next result (see Appendix B for the proof) provides an upper bound on the mean squared error of estimating F\u22121(p) with X(np).\nLemma 3. Fix some p \u2208 (0, 1), and assume a pdf f such that f(F\u22121(p)) 6= 0, and f \u2032(F\u22121(t)) is continuous at t = p. Then, for any \u01eb \u2208 ( pn+1 , p), we have that\nE\n[\n( F\u22121(U(np))\u2212 F\u22121(p) )2 ]\n\u2264 4 \u221a E [ (X(np))4 ] e\u2212(n+2)(\u01eb\u2212 p n+1) 2 + Var(U(np))\n(f(F\u22121(p))) 2\n+ Cp,\u01ebO ( 1/n 3 2 ) , (18a)\nwhere\nCp,\u01eb = max t\u2208[p\u2212\u01eb, p+\u01eb]\n\u2223 \u2223 \u2223 \u2223 f \u2032(F\u22121(t))\n(f(F\u22121(t)))3\n\u2223 \u2223 \u2223 \u2223 . (18b)\nRemark 1. Note that if the parent distribution is the uniform on (0, 1), then F\u22121(x) = x for x \u2208 (0, 1) and\nE[(F\u22121(U(np))\u2212 F\u22121(p))2] = E [ ( U(np) \u2212 p )2 ]\n= Var(U(np)) + ( E[U(np)]\u2212 p )2 = Var(U(np)) + p2\n(n+ 1)2 ,\nwhere we have used that E[U(np)] = np n+1 (see (5)) so that (\nE[U(np)]\u2212 p )2 = ( pn+1 ) 2. In other words, there exists a distribution for which the bound in Lemma 3 is asymptotically tight as n\u2192 \u221e.\nC. New Bound on the Moments of Order Statistics\nIn order to control the error in Lemma 3 we need to control the fourth moment of the order statistics. Ideally, we would like to control this in terms of the properties of the parent distribution (e.g., moments). The next result, the proof of which is in Appendix D, establishes a rather general bound on the moments of the order statistics in terms of properties of the parent distribution.\nLemma 4. Let X1, X2, . . . , Xn be an i.i.d. random sample. For any q, r > 0 and k \u2208 {1, 2, . . . , n}, we have that\nE[|X(k)|q] \u2264 Cn,k,q,r (E[|X |r]) q r ,\nwhere\nCn,k,q,r\n=\n{ \u0393(n+1)\u0393(k\u2212 qr )\u0393(n\u2212k\u2212 q r+1)\n\u0393(n\u2212 2qr +1)\u0393(k)\u0393(n\u2212k+1) if k > qr , n\u2212 k > q r \u2212 1,\n\u221e otherwise. In addition,\nlim n\u2192\u221e\nCn,pn,q,r = (p(1\u2212 p)) q r . (19)\nNotice that Lemma 4 shows that all of the moments of the order statistics exist, provided that a single moment of the parent distribution, for any order r > 0, exists. We now conclude this subsection with the following corollary, which is an immediate consequence of Lemma 3 and Lemma 4 and is used in the proof of Theorem 1 to show that K2 = O(1/ \u221a n).\nCorollary 5. Fix some p \u2208 (0, 1). Suppose that f(F\u22121(p)) 6= 0, f \u2032(F\u22121(t)) is continuous at t = p, and E[|X |r] < \u221e for some r > 0. Then, for Vn,p defined in (7),\n1\n2Vn,p E\n[\n( F\u22121(U(np))\u2212 F\u22121(p) )2 ] \u2212 1 2 = O\n(\n1\u221a n\n)\n.\nProof: We have that\n1\n2Vn,p E\n[\n( F\u22121(U(np))\u2212 F\u22121(p) )2 ] \u2212 1 2\n(a) \u2264 Var(U(np)) 2Vn,p (f(F\u22121(p))) 2 \u2212 1 2 +O\n(\n1\u221a n\n)\n(b) = n2(n+ 1\u2212 np) 2(1\u2212 p)(n+ 1)(n+ 2)2 \u2212 1 2 +O ( 1\u221a n ) \u2264 n+ 1\u2212 np 2(1\u2212 p)n \u2212 1 2 +O ( 1\u221a n )\n= 1 2(1\u2212 p)n +O ( 1\u221a n ) ,\nwhere (a) follows by using Lemma 3 and Lemma 4 since\n2 \u221a\nCn,np,4,r\nVn,p (E[|X |r]) 2r e\u2212(n+2)(\u01eb\u2212 pn+1)\n2 + Cp,\u01eb 2Vn,p O\n(\n1\nn 3 2\n)\nis O(1/ \u221a n) and (b) follows from (5) and (7).\nD. Bound on K3 in (14)\nWe now conclude this section with the following lemma, the proof of which is given in Appendix F. In particular, Lemma 6 demonstrates that K3 = O(1/ \u221a n) as long as \u2016f\u2016m has a finite norm for any m \u2265 2 including m = \u221e. Lemma 6. Fix some p \u2208 (0, 1). Choose some q, r \u2208 [1,\u221e] such that 1q + 1 r = 1 and \u01eb > 0 such that\np > \u01eb > max\n{\np n+ 1 , |(q \u2212 2)p\u2212 q + 1| q(n\u2212 1) + 2 } .\nThen,\nE\n[\nlog\n( f(F\u22121(U(np)))\nf(F\u22121(p))\n)]\n\u2264 2|log(f(F\u22121(p)))|e\u22122(n+2)(\u01eb\u2212 pn+1 ) 2\n+ C(2)\u01eb O\n(\n1\u221a n\n)\n+ 2Cq (\u2016f\u2016r+1) r+1 r n 1 2 (1\u2212 1 q )e\u22122(n+2)(\u01eb\u2212 |(q\u22122)p\u2212q+1| q(n\u22121)+2 ) 2 ,\nwhere the universal constant Cq := e 1+ 2q (\u221a 2\u03c0 ) 1 q\u22121 q\u2212 1 2q and\nC(2)\u01eb = max p\u2212\u01eb\u2264u\u2264p+\u01eb\n\u2223 \u2223 \u2223 \u2223 \u2223 f \u2032 ( F\u22121 (u) )\n(f (F\u22121 (u)))2\n\u2223 \u2223 \u2223 \u2223 \u2223 ;\nIV. DISCUSSION AND CONCLUSION\nIn this paper, we have derived an entropic version of the CLT for the order statistics that ensures a stronger mode of convergence (in terms of relative entropy) than the convergence in distribution provided by the classical CLT for continuous central order statistics [35].\nThe three sufficient conditions in Theorem 1 are fairly mild and we discuss them now in more detail. We suspect that condition 3) (or a variation of it) in Theorem 1 might be needed for convergence. To support this claim, consider the following density\nf1(x) = 2\nx log3(x) , x \u2208 (e,\u221e), (20)\nwhich is a canonical example of a pdf such that E[|X |r] = \u221e for all r > 0 (i.e., f1 does not satisfy the third condition in Theorem 1). In Appendix H, we indeed show that for this density K2 = \u221e; hence, D(X(np)\u2016Gn,p) = \u221e. Unlike condition 3), condition 1) in Theorem 1 might not be needed, as we argue next. Consider the following density,\nf2(x) = 1\nx log2(x) , x \u2208 (0, e\u22121), (21)\nwhich is a canonical example of pdf such that \u2016f2\u2016m = \u221e for all m > 1 (i.e., f2 does not satisfy the first condition in Theorem 1). Although this density does not satisfy the first condition in Theorem 1, in Appendix H we show that\nD(X(np)\u2016Gn,p) = \u0398 ( 1\nn\n)\n,\nthat is, the claim of Theorem 1 still holds. This example shows that, even if pretty mild, condition 1) in Theorem 1 might not be needed for convergence. Therefore, an interesting research direction would consist of further relaxing this condition.\nAPPENDIX A\nPROOF OF LEMMA 2\nWe first recall that by [35], the random variable U(k) is distributed as Beta(k, n+ 1\u2212 k); hence, it has pdf\nfU(k)(x) = ckx k\u22121(1\u2212 x)n\u2212k, for x \u2208 [0, 1], (22)\nwhere ck = n!\n(k\u22121)!(n\u2212k)! . Then, using the definition of\ndifferential entropy in (1), we obtain\n\u2212h(U(k)) = E [ log ( ckU k\u22121 (k) (1 \u2212 U(k))n\u2212k )]\n= log(ck) + (k \u2212 1)E[log(U(k))] + (n\u2212 k)E[log(1\u2212 U(k))].\n(23)\nNow, consider the two expectations on the right side of (23), namely E[log(U(k))] and E[log(1 \u2212 U(k))]. Notice that with U(k) \u223c Beta(k, n+1\u2212k), we also have 1\u2212U(k) \u223c Beta(n+ 1\u2212 k, k). Therefore, (see, e.g., [35]),\nE[log(U(k))] = \u03c8(k)\u2212 \u03c8(n+ 1), E[log(1\u2212 U(k))] = \u03c8(n+ 1\u2212 k)\u2212 \u03c8(n+ 1),\nwhere \u03c8(\u00b7) is the digamma function. Next, we leverage the relationship between the digamma function and the harmonic number, Hr defined in (16), namely \u03c8(k) = Hk\u22121\u2212\u03b3, where \u03b3 is the Euler-Mascheroni constant. Putting this all together in (23), we have that\n\u2212 h(U(k)) = log(ck) + (k \u2212 1)(\u03c8(k)\u2212 \u03c8(n+ 1)) + (n\u2212 k)(\u03c8(n+ 1\u2212 k)\u2212 \u03c8(n+ 1)) = log(ck) + (k \u2212 1)(Hk\u22121 \u2212Hn) + (n\u2212 k)(Hn\u2212k \u2212Hn) = log(ck) + (k \u2212 1)Hk\u22121 + (n\u2212 k)Hn\u2212k \u2212 (n\u2212 1)Hn.\nFinally, using the fact that ck = n!\n(k\u22121)!(n\u2212k)! , and the defini-\ntion of Tj = log(j!)\u2212 jHj in (16), we have that \u2212h(U(k)) = log(n!)\u2212 log((k \u2212 1)!)\u2212 log((n\u2212 k)!)\n+ (k \u2212 1)Hk\u22121 + (n\u2212 k)Hn\u2212k \u2212 (n\u2212 1)Hn = Tn \u2212 Tk\u22121 \u2212 Tn\u2212k +Hn. (24)\nThis shows (15). To show (17), we let k < n and we approximate the expression in (24) by leveraging the following expressions for the series expansion of the harmonic number and the log factorial [37],\nHk = log(k)+\u03b3+ 1 2k \u2212 1 12k2 + 1 120k4 \u2212O\n(\n1\nk6\n)\n, (25)\nlog(k!) = k log(k)\u2212 k + 1 2 log(2\u03c0k) + 1 12k\n\u2212 1 360k3 +O\n(\n1\nk5\n)\n. (26)\nCombining (25) and (26), we arrive at\nTk= 1\n2 log\n(\n2\u03c0k\ne\n)\n\u2212(1+ \u03b3)k+ 1 6k \u2212 1 90k3 +O\n(\n1\nk5\n)\n. (27)\nNow, using the expression of Tk in (27) inside (24), and collecting similar terms we find\nh(U(k)) = Tk\u22121 + Tn\u2212k \u2212 Tn \u2212Hn\n= 1\n2 log\n( 2\u03c0e(k \u2212 1)(n\u2212 k) n3 ) + 1 6 ( 1 k \u2212 1 + 1 n\u2212 k \u2212 4 n )\n\u2212 1 90\n(\n1 (k \u2212 1)3 + 1 (n\u2212 k)3 \u2212 1 n3\n)\n+ 1\n12n2\n(\n1\u2212 1 10n2\n)\n+O\n(\n1 (k \u2212 1)5 ) +O (\n1 (n\u2212 k)5 ) \u2212O ( 1 n5 ) +O ( 1 n6 ) .\nNow, letting k = np and keeping only the dominant terms in the expression above, we arrive at\nh(U(np)) = 1\n2 log\n(\n2\u03c0e ( p\u2212 1n ) (1 \u2212 p) n\n)\n+ 1\n12n2\n+ 1\n6n\n(\n1\np\u2212 1n +\n1 1\u2212 p \u2212 4 ) +O ( 1 n3 ) .\nThis concludes the proof of (17) and also of Lemma 2.\nAPPENDIX B\nPROOF OF LEMMA 3\nRecall that by assumption, pn+1 < \u01eb < p. Now, define the event\nA = {p\u2212 \u01eb \u2264 U(np) \u2264 p+ \u01eb}, (28)\nand let 1A be the indicator function of the event A and 1Ac be the indicator function of the complementary event. Now, using the law of total expectation we can write\nE\n[\n( F\u22121(U(np))\u2212 F\u22121(p) )2 ]\n= E [ ( F\u22121(U(np))\u2212 F\u22121(p) )2 1Ac ]\n+ E [ ( F\u22121(U(np))\u2212 F\u22121(p) )2 1A ] . (29)\nWe will now analyze the two expectations on the right side of (29) separately.\nFirst expectation in (29). We obtain\nE\n[\n( F\u22121(U(np))\u2212 F\u22121(p) )2 1Ac ]\n(a) \u2264 2E [ ( F\u22121(U(np)) )2 1Ac ] + 2 ( F\u22121(p) )2\nP(Ac) (b) \u2264 2 \u221a E [ (X(np))4 ] P(Ac) + 2 ( F\u22121(p) )2\nP(Ac) (c)\n\u2264 2 ( \u221a E [ (X(np))4 ] + ( F\u22121(p) )2 ) \u221a\nP(Ac) (d)\n\u2264 4 ( \u221a E [ (X(np))4 ] + ( F\u22121(p) )2 ) e\u2212(n+2)(\u01eb\u2212 p n+1 ) 2 , (30)\nwhere the labeled inequalities follow from: (a) the fact that (a \u2212 b)2 \u2264 2(a2 + b2); (b) the Cauchy-Schwarz inequality and the fact X(np) d = F\u22121(U(np)); (c) the fact that x \u2264 \u221a x for x \u2208 (0, 1); and (d) Lemma 7 below, which is proved in Appendix C.\nLemma 7. Let A = {p\u2212 \u01eb \u2264 U(np) \u2264 p+ \u01eb} for p \u2208 (0, 1) and pn+1 < \u01eb < p. Then,\nP(Ac) \u2264 2 exp ( \u22122(n+ 2) (\n\u01eb\u2212 p n+ 1\n)2 )\n.\nSecond expectation in (29). From the definition of A in (28), we know that U(np) \u2208 [p\u2212 \u01eb, p+ \u01eb]; hence, using the Taylor remainder theorem, for any t \u2208 [p\u2212 \u01eb, p+ \u01eb], where u\u0303 is some value between p and t, we have\nF\u22121(t) = F\u22121(p) + d\ndu F\u22121(u)|u=p (t\u2212 p)\n+ d2\ndu2 F\u22121(u)|u=u\u0303 (t\u2212 p)2 2\n= F\u22121(p)+g(1)(p) (t\u2212p)+ 1 2 g(2)(u\u0303) (t\u2212p)2 , (31)\nwhere in the last equality we let g(1)(p) := dduF \u22121(u)|u=p and g(2)(u\u0303) := d 2\ndu2F \u22121(u)|u=u\u0303.\nWe next provide an upper bound for g(2)(u\u0303). Define,\nCp,\u01eb = max t\u2208[p\u2212\u01eb,p+\u01eb]\n|g(2)(t)| <\u221e, (32)\nwhere the last inequality follows since\ng(2)(t) = d2\ndu2 F\u22121(u)\n\u2223 \u2223 \u2223 \u2223\nu=t\n= d\ndu\n1\nf(F\u22121(u))\n\u2223 \u2223 \u2223 \u2223\nu=t\n= \u2212 f \u2032(F\u22121(t))\n(f(F\u22121(t)))3 .\nTherefore, for sufficiently small \u01eb > 0, we have that Cp,\u01eb <\u221e if f(F\u22121(p)) 6= 0, and f \u2032(F\u22121(t)) is continuous at t = p; these two conditions are guaranteed by the assumptions of Lemma 3. Thus, plugging this into (31),\n( F\u22121(t)\u2212 F\u22121(p) )2 \u2264\n( g(1)(p) )2 (t\u2212 p)2 + C2p,\u01eb 4 (t\u2212 p)4\n+ Cp,\u01eb\n\u2223 \u2223 \u2223 g(1)(p) (t\u2212 p)3 \u2223 \u2223 \u2223 . (33)\nNow using (33),\nE\n[\n( F\u22121(U(np))\u2212 F\u22121(p) )2 1A\n]\n\u2264 ( g(1)(p) )2 E [ (U(np) \u2212 p)21A ] + C2p,\u01eb 4 E[(U(np) \u2212 p)41A]\n+ Cp,\u01ebE [ \u2223 \u2223 \u2223 g(1)(p)(U(np) \u2212 p)31A \u2223 \u2223 \u2223 ] . (34)\nNext, we individually upper bound each term on the right side of (34). First term in (34). We have\nE\n[\n( U(np) \u2212 p )2 1A\n] \u2264 E [ ( U(np) \u2212 p )2 ]\n= Var(U(np)) + p2\n(n+ 1)2 ,\n(35)\nwhere the equality follows since\nE[(U(np) \u2212 p)2] = E[(U(np) \u2212 E[U(np)] + E[U(np)]\u2212 p)2] = Var(U(np)) + ( E[U(np)]\u2212 p )2 ,\nwhere E[U(np)] = np n+1 , so\n( E[U(np)]\u2212 p )2 = ( pn+1 ) 2.\nFrom (35), the first term on the right side of (34) can be upper bounded as\n( g(1)(p) )2 E [ (U(np) \u2212 p)21A ]\n\u2264 ( g(1)(p) )2 ( Var(U(np)) + p2\n(n+ 1)2\n)\n. (36)\nSecond term in (34). We obtain\nC2p,\u01eb 4 E [ ( U(np) \u2212 p )4 1A ] \u2264 C 2 p,\u01eb\n4 E\n[\n(\nU(np) \u2212 pn\nn+ 1 +\npn n+ 1 \u2212 p )4 ]\n(a) \u2264 2C2p,\u01eb\n(\n(\npn n+ 1 \u2212 p )4 + E\n[\n(\nU(np) \u2212 pn\nn+ 1\n)4 ])\n= 2C2p,\u01eb\n(\np4\n(n+ 1)4 + E\n[\n(\nU(np) \u2212 pn\nn+ 1\n)4 ])\n(b) = 2C2p,\u01eb\n(\np4\n(n+ 1)4 +\u0398\n(\n1\nn2\n))\n, (37)\nwhere the labeled (in)equalities follow from: (a) the fact that (a+b)4 \u2264 8(a4+b4), and (b) the fact that if W \u223c Beta(\u03b1, \u03b2), then (see, for example [38])\nE[(W \u2212 E[W ])4]\n=\n(\n\u03b1\n\u03b1+\u03b2\n)4\n\u00b7 3(\u03b1 2\u03b22+2\u03b12\u03b2+\u03b1\u03b23\u22122\u03b1\u03b22+2\u03b23)\n\u03b13(\u03b1+\u03b2+1)(\u03b1+\u03b2+2)(\u03b1+\u03b2+3) ; (38)\nand, in particular, for U(np) \u223c Beta(np, n+1\u2212np), by setting \u03b1 = pn and \u03b2 = n\u2212 pn+ 1 in (38)\nE\n[\n(\nU(np) \u2212 pn\nn+ 1\n)4 ]\n= \u0398\n(\n1\nn2\n)\n.\nThird term in (34). This term can be upper bounded using Jensen\u2019s inequality (as E[|Y |3] = E[(|Y |4)3/4] \u2264 (E[|Y |4])3/4) and the same steps as we used to get the bound in (37). Indeed,\nCp,\u01ebE [\u2223 \u2223 \u2223 g(1)(p)(U(np) \u2212 p)31A \u2223 \u2223 \u2223 ]\n\u2264 Cp,\u01eb \u2223 \u2223 \u2223 g(1)(p) \u2223 \u2223 \u2223 E [ \u2223 \u2223U(np) \u2212 p \u2223 \u2223 3 1A ]\n\u2264 Cp,\u01eb \u2223 \u2223 \u2223 g(1)(p) \u2223 \u2223 \u2223 ( E [ \u2223 \u2223U(np) \u2212 p \u2223 \u2223 4 ])\n3 4\n\u2264 83/4Cp,\u01eb \u2223 \u2223 \u2223 g(1)(p) \u2223 \u2223 \u2223\n(\np3\n(n+ 1)3 +\u0398\n(\n1\nn 3 2\n))\n. (39)\nCollecting (34) together with the bounds in (36), (37), and (39),\nE\n[\n( F\u22121(U(np))\u2212 F\u22121(p) )2 1A\n]\n\u2264 ( g(1)(p) )2\nVar(U(np)) + Cp,\u01ebO\n(\n1\nn 3 2\n)\n. (40)\nTherefore, plugging (30) and (40) in (29),\nE\n[\n( F\u22121(U(np))\u2212 F\u22121(p) )2 ]\n\u2264 4 ( \u221a E [ (X(np))4 ] + ( F\u22121(p) )2 ) e\u2212(n+2)(\u01eb\u2212 p n+1 ) 2\n+ ( g(1)(p) )2\nVar(U(np)) + Cp,\u01ebO\n(\n1\nn 3 2\n)\n= 4 \u221a E [ (X(np))4 ] e\u2212(n+2)(\u01eb\u2212 p n+1 ) 2\n+ Var(U(np))\n(f(F\u22121(p))) 2 + Cp,\u01ebO\n(\n1 n 3 2\n)\n,\nwhere the last equality follows by absorbing non-dominant terms inside the big-O and recalling that g(1)(p) = [\nf(F\u22121(p)) ]\u22121 . This concludes the proof of Lemma 3.\nAPPENDIX C\nPROOF OF LEMMA 7\nUsing the definition of A in (28),\nP(Ac) = P[U(np) \u2264 p\u2212 \u01eb] + P [ U(np) \u2265 p+ \u01eb ]\n=P[U(n(1\u2212p)+1)\u22651\u2212p+\u01eb]+P [ U(np)\u2265p+\u01eb ] , (41)\nwhere in the last equality we have used the fact that U(np) d = 1 \u2212 U(n(1\u2212p)+1) where U(n(1\u2212p)+1) \u223c Beta(n + 1 \u2212 np, np) since U(np) \u223c Beta(np, n+ 1\u2212 np).\nTo upper bound P(Ac) in (41), we leverage the fact that Beta-distributed random variables U(np) and U(n(1\u2212p)+1) are \u03c30-sub-Gaussian with \u03c30 < 1 4(\u03b1+\u03b2+1) = 1 4(n+2) (see [39, Thm. 1]). Recall (see, for example, [40]), that a random variable X is sub-Gaussian with parameter \u03c30 if for every \u03bb \u2208 R,\nE[e\u03bb(X\u2212E[X])] \u2264 e\u03bb2\u03c30/2. (42)\nMoreover, if X is \u03c30-sub-Gaussian, then for all t \u2265 0,\nP(X \u2212 E[X ] \u2265 t) \u2264 e\u2212 t 2 2\u03c30 . (43)\nNow, let us consider the first term on the right side of (41). Using that E[U(n(1\u2212p)+1)] = n+1\u2212np n+1 ,\nP[U(n(1\u2212p)+1) \u2265 1\u2212 p+ \u01eb]\n= P\n[\nU(n(1\u2212p)+1) \u2212 E[U(n(1\u2212p)+1)] \u2265 \u01eb\u2212 p\nn+ 1\n]\n(a) \u2264 exp (\n\u2212 1 2\u03c30\n(\n\u01eb \u2212 p n+ 1\n)2 )\n(b) \u2264 exp ( \u22122(n+ 2) (\n\u01eb\u2212 p n+ 1\n)2 )\n, (44)\nwhere (a) follows from the bound in (43), and (b) is due to the fact that \u03c30 < 1 4(n+2) .\nSimilarly, for the second term on the right side of (41),\nusing that E[U(np)] = np n+1 , we obtain\nP [ U(np) \u2265 p+ \u01eb ] = P\n[\nU(np) \u2212 E[U(np)] \u2265 p\nn+ 1 + \u01eb\n]\n(a) \u2264 exp ( \u22122(n+ 2) ( p\nn+ 1 + \u01eb\n)2 )\n\u2264 exp ( \u22122(n+ 2)\u01eb2 ) , (45)\nwhere, again, (a) follows from the bound in (43), and the fact that \u03c30 < 1 4(n+2) .\nNow, by summing (44) and (45), we sind that P(Ac) in (41) can be upper bounded as\nP(Ac)\n\u2264 exp ( \u22122(n+ 2) (\n\u01eb\u2212 p n+ 1\n)2 )\n+ exp ( \u22122(n+ 2)\u01eb2 )\n\u2264 2 exp ( \u22122(n+ 2) (\n\u01eb\u2212 p n+ 1\n)2 )\n,\nconcluding the proof of Lemma 7.\nAPPENDIX D\nPROOF OF LEMMA 4\nRecall from (4) that X(k) d = F\u22121(U(k)), where U(k) is the order statistic of a sample of uniform random variables. Thus,\nE[|X(k)|q] = E [ |F\u22121(U(k))|q ]\n\u2264 (E[|X |r]) q r E\n[\n(\n1\nmin(U(k), 1\u2212 U(k))\n) q r ]\n, (46)\nwhere the inequality follows from Lemma 8 given below and proved in Appendix E.\nLemma 8. Let F be the cdf of the random variable X . Then, for any r > 0,\n|F\u22121(u)| \u2264 ( E[|X |r] min(u, 1\u2212 u)\n) 1 r\n, u \u2208 (0, 1). (47)\nNext, we upper bound the right side of (46) using the\nfollowing:\nE\n[\n(\n1\nmin(U(k), 1\u2212 U(k))\n) q r ] (a) \u2264 E [ (\n1\nU(k)(1\u2212 U(k))\n) q r ]\n(b)\n\u2264 Cn,k,q,r. (48) In the above, the inequalities follow from the facts that (a) min{a, b} \u2265 aba+b and (b) U(k) \u223c Beta(k, n+ 1\u2212 k); hence,\nE\n[\n(\n1\nU(k)(1\u2212 U(k))\n) q r ]\n= \u0393(n+ 1)\n\u0393(k)\u0393(n+ 1\u2212 k)\n\u222b 1\n0\ntk\u22121\u2212 q r (1 \u2212 t)n\u2212k\u2212 qr dt\n= \u0393(n+ 1)\n\u0393(k)\u0393(n+ 1\u2212 k)\n\u00b7\n\n\n\n\u0393(k\u2212 qr )\u0393(n\u2212k\u2212 q r+1)\n\u0393(n\u2212 2qr +1) k > qr and n+ 1\u2212 k > q r\n\u221e else = Cn,k,q,r.\nPlugging (48) into (46), we have shown\nE[|X(k)|q] \u2264 Cn,k,q,r (E[|X |r]) q r ,\nas desired.\nFinally, we notice that if q and r are fixed with k = np for fixed p, then as n \u2192 \u221e, we have k = np > qr and\nn + 1 \u2212 k = n(1 \u2212 p) + 1 > qr , eventually. Then, by [37, eq. 6.1.46], for a, b \u2208 R,\nlim x\u2192\u221e\n\u0393(x+ a)\n\u0393(x+ b) xb\u2212a = 1,\nwhich leads to the conclusion that\nlim n\u2192\u221e Cn,pn,q,r\n= lim n\u2192\u221e\n\u0393(n+ 1)\n\u0393 ( n\u2212 2qr + 1 )\n\u0393 ( pn\u2212 qr )\n\u0393(pn)\n\u0393 ( n(1\u2212 p)\u2212 qr + 1 )\n\u0393 (n(1\u2212 p) + 1) = lim\nn\u2192\u221e n\u2212\n2q r (pn) q r (n(1\u2212 p)) qr = (p(1\u2212 p)) q r .\nThis concludes the proof of Lemma 4.\nAPPENDIX E\nPROOF OF LEMMA 8\nFirst, recall that for a uniform random variable over [0, 1],\ndenoted as U , we have X d = F\u22121(U). Therefore,\nE[|X |r] = E[|F\u22121(U)|r] = \u222b 1\n0\n|F\u22121(t)|r dt.\nFirst, assume that 0 \u2264 F (0) \u2264 u \u2264 1. Then, \u222b 1\n0\n|F\u22121(t)|rdt\u2265 \u222b 1\nu\n|F\u22121(t)|rdt\u2265(1\u2212 u)|F\u22121(u)|r, (49)\nwhere in the last inequality we have used that F\u22121(t) is positive for t \u2265 u \u2265 F (0), implying |F\u22121(u)|r \u2264 |F\u22121(t)|r for t \u2208 [u, 1], and that F\u22121(x) is non-decreasing in x; hence, F\u22121(u) \u2264 F\u22121(t) for t \u2208 [u, 1].\nNow assume, on the other hand, 0 \u2264 u \u2264 F (0) \u2264 1. Then, \u222b 1\n0\n|F\u22121(t)|rdt \u2265 \u222b u\n0\n|F\u22121(t)|rdt \u2265 u|F\u22121(u)|r. (50)\nIn the last inequality, we have used that F\u22121(t) is negative for t \u2264 u \u2264 F (0), implying that |F\u22121(u)| \u2264 |F\u22121(t)| and that F\u22121(x) is non-decreasing in x; hence, F\u22121(t) \u2264 F\u22121(u) for t \u2208 [0, u]. Thus, |F\u22121(u)|r \u2264 |F\u22121(t)|r.\nThe bounds in (49) and (50) imply that\n|F\u22121(u)| \u2264 ( E[|X |r] u\n) 1 r\n, for u \u2208 [0, F (0)],\n|F\u22121(u)| \u2264 ( E[|X |r] 1\u2212 u\n) 1 r\n, for u \u2208 [F (0), 1].\nTaking the largest of the two bounds concludes the proof of Lemma 8.\nAPPENDIX F\nPROOF OF LEMMA 6\nChoose some q \u2208 [1,\u221e] and define an event A={p\u2212 \u01eb\u2264 U(np) \u2264 p+\u01eb}, where we assume that\np > \u01eb > max\n{\np n+ 1 , |(q \u2212 2)p\u2212 q + 1| q(n\u2212 1) + 2 } .\nNext, notice that we can write\nE [ log(f(F\u22121(U(np)))) ] = E [ log(f(F\u22121(U(np)))))1A ]\n+ E [ log(f(F\u22121(U(np))))1Ac ] . (51)\nWe now analyze each expectation on the right side of (51) separately. First expectation in (51). Using Taylor\u2019s remainder theorem we have that log(f(F\u22121(u))) = log(f(F\u22121(p))) + f \u2032(F\u22121(u\u0303))\n(f(F\u22121(u\u0303)))2 (u \u2212 p),\nwhere u\u0303 is some number between p and u. Therefore, using the definition\nC(2)\u01eb = max p\u2212\u01eb\u2264u\u2264p+\u01eb\n\u2223 \u2223 \u2223 \u2223 \u2223 f \u2032 ( F\u22121 (u) )\n(f (F\u22121 (u))) 2\n\u2223 \u2223 \u2223 \u2223 \u2223 ,\nwe find that\nE [ log(f(F\u22121(U(np))))1A ]\n\u2264 log(f(F\u22121(p)))P [A] + C(2)\u01eb E [ |U(np) \u2212 p|1A ] . (52)\nNext, we notice that\nE [ |U(np) \u2212 p|1A ]\n(a) \u2264 \u221a E [ |U(np) \u2212 p|2 ] (b) = O(1/ \u221a n), (53)\nwhere in the above, (a) follows using Cauchy-Schwarz inequality and the fact that P [A] \u2264 1 and (b) using Remark 1. Combining (52) and (53) we have the bound\nE [ log(f(F\u22121(U(np))))1A ]\n\u2264 log(f(F\u22121(p)))P [A] + C(2)\u01eb O ( 1\u221a n ) . (54)\nSecond expectation in (51). First recall that U(np) \u223c Beta(np, n+1\u2212np) and let f\u03b1,\u03b2 denote the beta distribution with parameters \u03b1 := np and \u03b2 := n + 1 \u2212 np. Then, using the bound log(x) \u2264 x, we have\nE [ log(f(F\u22121(U(np))))1Ac ] \u2264 E [ f(F\u22121(U(np)))1Ac ]\n=\n\u222b 1\n0\n1Acf\u03b1,\u03b2(u)f(F \u22121(u))du. (55)\nNext, by Ho\u0308lder\u2019s inequality, for any q, r \u2208 [1,\u221e] such that 1 q + 1 r = 1, we find\n\u222b 1\n0\n1Acf\u03b1,\u03b2(u)f(F \u22121(u))du\n\u2264 ( \u222b 1\n0\n1Acf q \u03b1,\u03b2(u)du\n) 1 q ( \u222b 1\n0\n( f(F\u22121(u)) )r du\n) 1 r\n. (56)\nNext we simplify the two terms on the right side of (56).\nFirst notice that f q\u03b1,\u03b2(u) = c\u03b1\u2217,\u03b2\u2217 cq\u03b1,\u03b2 f\u03b1\u2217,\u03b2\u2217(u) where \u03b1\u2217 = q(\u03b1 \u2212 1) + 1, \u03b2\u2217 = q(\u03b2 \u2212 1) + 1, (57) and\nci,j = \u0393(i)\u0393(j)\n\u0393(i+ j) , i \u2208 {\u03b1, \u03b1\u2217}, j \u2208 {\u03b2, \u03b2\u2217}.\nIt follows that,\n( \u222b 1\n0\n1Acf q \u03b1,\u03b2(u)du\n) 1 q\n= c\n1 q \u03b1\u2217,\u03b2\u2217 c\u03b1,\u03b2 ( \u222b 1\n0\n1Acf\u03b1\u2217,\u03b2\u2217(u)du\n) 1 q\n= c\n1 q \u03b1\u2217,\u03b2\u2217 c\u03b1,\u03b2 P 1 q (U\u03b1\u2217,\u03b2\u2217 \u2208 Ac) , (58)\nwhere in the final equality U\u03b1\u2217,\u03b2\u2217 denotes a beta random variable with parameters \u03b1\u2217 and \u03b2\u2217. For the second term on the right side of (56), we use a change of variables with x = F\u22121(u) with dx = (f(F\u22121(u)))\u22121du to get\n( \u222b 1\n0\n( f(F\u22121(u)) )r du\n) 1 r\n=\n( \u222b \u221e\n\u2212\u221e\n(f(x))r+1dx\n) 1 r\n= (\u2016f\u2016r+1) r+1 r . (59)\nTherefore, putting together (55) \u2013 (59), we have shown\nE [ log(f(F\u22121(U(np))))1Ac ]\n\u2264 c\n1 q \u03b1\u2217,\u03b2\u2217 c\u03b1,\u03b2 P 1 q (U\u03b1\u2217,\u03b2\u2217 \u2208 Ac) (\u2016f\u2016r+1) r+1 r . (60)\nWe now focus on further upper bounding the right-hand side of (60). Towards this end, we start by noting that by the definitions in (57) and the fact that \u03b1 := np and \u03b2 := n+1\u2212np, we find \u03b1\u2217+\u03b2\u2217 = q(\u03b1+\u03b2\u2212 2)+2 = q(n\u2212 1)+2, therefore\n\u03b1\u2217 \u03b1\u2217 + \u03b2\u2217 = q(pn\u2212 1) + 1 q(n\u2212 1) + 2 ,\n\u03b2\u2217 \u03b1\u2217 + \u03b2\u2217 = q(1\u2212 p)n+ 1 q(n\u2212 1) + 2 .\nMoreover, in Appendix G, it is shown that\nc 1 q\n\u03b1\u2217,\u03b2\u2217 c\u03b1,\u03b2 \u2264 Cqn 1 2 (1\u2212 1 q ). (61)\nwhere the universal constant Cq := e 1+ 2q (\u221a 2\u03c0 ) 1 q\u22121 q\u2212 1 2q .\nFinally, we note that from properties of the beta distribution,\nnamely that U\u03b1\u2217,\u03b2\u2217 d = 1\u2212 U\u03b2\u2217,\u03b1\u2217 , we find\nP(U\u03b1\u2217,\u03b2\u2217 \u2208 Ac) = P[U\u03b1\u2217,\u03b2\u2217 \u2264 p\u2212 \u01eb] + P [U\u03b1\u2217,\u03b2\u2217 \u2265 p+ \u01eb] = P[U\u03b2\u2217,\u03b1\u2217 \u2265 1\u2212 p+ \u01eb] + P [U\u03b1\u2217,\u03b2\u2217 \u2265 p+ \u01eb] , (62)\nNow focus on the first term on the right side of (62), we have\nP[U\u03b2\u2217,\u03b1\u2217 \u2265 1\u2212 p+ \u01eb]\n= P\n[\nU\u03b2\u2217,\u03b1\u2217 \u2212 \u03b2\u2217 \u03b1\u2217 + \u03b2\u2217 \u2265 1\u2212 p+ \u01eb \u2212 \u03b2 \u2217 \u03b1\u2217 + \u03b2\u2217\n]\n(a) \u2264 P [ U\u03b2\u2217,\u03b1\u2217 \u2212 \u03b2\u2217 \u03b1\u2217 + \u03b2\u2217 \u2265 \u01eb\u2212 |(q \u2212 2)p\u2212 q + 1| q(n\u2212 1) + 2 ]\n(b) \u2264 e\u22122(q(n\u22121)+3)(\u01eb\u2212 |(q\u22122)p\u2212q+1| q(n\u22121)+2 ) 2 \u2264 e\u22122(n+2)(\u01eb\u2212 |(q\u22122)p\u2212q+1| q(n\u22121)+2 ) 2 , (63)\nwhere the labeled steps follow from: (a) noting that\n1\u2212 p+ \u01eb\u2212 \u03b2 \u2217\n\u03b1\u2217 + \u03b2\u2217 = \u01eb+ (q \u2212 2)p\u2212 q + 1 q(n\u2212 1) + 2\n\u2265 \u01eb\u2212 |(q \u2212 2)p\u2212 q + 1| q(n\u2212 1) + 2 ,\nand (b) applying (43) since U\u03b2\u2217,\u03b1\u2217 is \u03c30-sub-Gaussian random variables with \u03c30 < 1 4(\u03b1\u2217+\u03b2\u2217+1) = 1 4(q(n\u22121)+3) [39, Thm. 1].\nSimilarly, since\np+ \u01eb\u2212 \u03b1 \u2217 \u03b1\u2217 + \u03b2\u2217 = \u01eb\u2212 (q \u2212 2)p\u2212 q + 1 q(n\u2212 1) + 2\n\u2265 \u01eb\u2212 |(q \u2212 2)p\u2212 q + 1| q(n\u2212 1) + 2 ;\nalong with the fact that U\u03b1\u2217,\u03b2\u2217 is \u03c30-sub-Gaussian again with \u03c30 < 1 4(\u03b1\u2217+\u03b2\u2217+1) = 1 4(q(n\u22121)+3) , we have the same bound for the second term on the right side of (62):\nP [U\u03b1\u2217,\u03b2\u2217 \u2265 p+ \u01eb]\n\u2264 P [ U\u03b1\u2217,\u03b2\u2217 \u2212 \u03b1\u2217 \u03b1\u2217 + \u03b2\u2217 \u2265 \u01eb\u2212 |(q \u2212 2)p\u2212 q + 1| q(n\u2212 1) + 2 ] \u2264 e\u22122(n+2)(\u01eb\u2212 |(q\u22122)p\u2212q+1| q(n\u22121)+2 ) 2 . (64)\nThus, we have shown in (62)-(64)\nP(U\u03b1\u2217,\u03b2\u2217 \u2208 Ac) \u2264 2e\u22122(n+2)(\u01eb\u2212 |(q\u22122)p\u2212q+1| q(n\u22121)+2 ) 2 . (65)\nNow, combining (60) with (67) and (65) we arrive at\nE [ log ( f(F\u22121(U(np))) ) 1Ac ]\n\u2264 2Cq (\u2016f\u2016r+1) r+1 r n 1 2 (1\u2212 1 q )e\u22122(n+2)(\u01eb\u2212 |(q\u22122)p\u2212q+1| q(n\u22121)+2 ) 2\n. (66)\nConsequently, combining (51), (54), and (66), we obtain\nE [ log(f(F\u22121(U(np)))) ] \u2212 log(f(F\u22121(p)))\n\u2264 |log(f(F\u22121(p)))|P[Ac] + C(2)\u01eb O ( 1\u221a n )\n+ 2Cq (\u2016f\u2016r+1) r+1 r n 1 2 (1\u2212 1 q )e\u22122(n+2)(\u01eb\u2212 |(q\u22122)p\u2212q+1| q(n\u22121)+2 ) 2\n\u2264 2|log(f(F\u22121(p)))|e\u22122(n+2)(\u01eb\u2212 pn+1 ) 2\n+ C(2)\u01eb O\n(\n1\u221a n\n)\n+ 2Cq (\u2016f\u2016r+1) r+1 r n 1 2 (1\u2212 1 q )e\u22122(n+2)(\u01eb\u2212 |(q\u22122)p\u2212q+1| q(n\u22121)+2 ) 2 ,\nwhere the last inequality follows by bounding P [Ac] using Lemma 7. This concludes the proof of Lemma 6.\nAPPENDIX G\nPROOF OF THE BOUND IN (61)\nRecall from the proof of Lemma 6 that \u03b1 := np and \u03b2 := n+1\u2212np while \u03b1\u2217 = q(\u03b1\u22121)+1, and \u03b2\u2217 = q(\u03b2\u22121)+1 for some q > 1. Thus, with reference to (61), we want to show that\n\u0393(\u03b1\u2217) 1 q\u0393(\u03b2\u2217) 1 q \u0393(\u03b1+ \u03b2)\n\u0393(\u03b1\u2217 + \u03b2\u2217) 1 q \u0393(\u03b1)\u0393(\u03b2)\n\u2264 Cqn 1 2 (1\u2212 1 q ), (67)\nwhere Cq := e 1+ 2q (\u221a 2\u03c0 ) 1 q\u22121 q\u2212 1 2q .\nFirst, using that \u0393(n+1) = n!, recall Stirling\u2019s approximation [37], which tells us that for n \u2265 1,\n\u0393(n+ 1) = \u03ban \u221a 2\u03c0nn+ 1 2 e\u2212n, (68)\nwhere 1 \u2264 e 112n+1 \u2264 \u03ban \u2264 e 112n \u2264 e. Using the bound in (68), we find \u0393(\u03b1\u2217) = \u0393(q(\u03b1\u22121)+1) \u2264 e \u221a 2\u03c0(q(\u03b1\u22121))q(\u03b1\u22121)+ 12 e\u2212q(\u03b1\u22121),\nand\n\u0393(\u03b1) \u2265 \u221a 2\u03c0(\u03b1\u2212 1)\u03b1\u2212 12 e\u2212(\u03b1\u22121);\nhence, by simplifying, we obtain\n\u0393 1 q (\u03b1\u2217)\n\u0393(\u03b1) \u2264\n( e \u221a 2\u03c0(q(\u03b1 \u2212 1))q(\u03b1\u22121)+ 12 e\u2212q(\u03b1\u22121) ) 1 q\n\u221a 2\u03c0(\u03b1\u2212 1)\u03b1\u2212 12 e\u2212(\u03b1\u22121)\n= e 1 q (\u221a 2\u03c0 ) 1 q\u22121 (\u03b1\u2212 1)\u2212 12+ 12q q\u03b1\u22121+ 12q . (69)\nWe can similarly show that\n\u0393 1 q (\u03b2\u2217)\n\u0393(\u03b2) \u2264 e 1q\n(\u221a 2\u03c0 ) 1 q\u22121 (\u03b2 \u2212 1)\u2212 12+ 12q q\u03b2\u22121+ 12q . (70)\nFinally, we agin use the bound in (68) to find\n\u0393(\u03b1 + \u03b2)\n\u0393 1 q (\u03b1\u2217 + \u03b2\u2217)\n= \u0393(n+ 1)\n\u0393 1 q (q(n\u2212 1) + 2)\n= n\u0393(n)\n(q(n\u2212 1) + 1) 1q \u0393 1q (q(n\u2212 1) + 1)\n\u2264 \u221a 2\u03c0e\u2212(n\u22122)n(n\u2212 1)n\u2212 12\n(q(n\u2212 1) + 1) 1q (\u221a 2\u03c0(q(n\u2212 1))q(n\u22121)+ 12 e\u2212q(n\u22121) ) 1 q\n= e (\u221a 2\u03c0 )1\u2212 1q n(n\u2212 1) 12\u2212 12q q\u2212(n\u22121)\u2212 12q (q(n\u2212 1) + 1)\u2212 1q\n\u2264 e (\u221a 2\u03c0 )1\u2212 1q n 3 2 (1\u2212 1 q )q\u2212(n\u22121)\u2212 3 2q . (71)\nCombining (69), (70), and (71) we arrive at\n\u0393 1 q (\u03b1\u2217)\n\u0393(\u03b1) \u00d7 \u0393\n1 q (\u03b2\u2217)\n\u0393(\u03b2) \u00d7 \u0393(\u03b1+ \u03b2)\n\u0393 1 q (\u03b1\u2217 + \u03b2\u2217)\n\u2264 e1+ 2q (\u221a 2\u03c0 ) 1 q\u22121 n 3 2 (1\u2212 1 q )(np\u2212 1)\u2212 12 (1\u2212 1q )\n(n(1 \u2212 p))\u2212 12 (1\u2212 1q )q\u2212 12q\n\u2264 e1+ 2q (\u221a 2\u03c0 ) 1 q\u22121 n 1 2 (1\u2212 1 q )q\u2212 1 2q ,\nwhich concludes the proof.\nAPPENDIX H\nEXAMPLES OF SECTION IV\nA. Density in (20)\nFor the pdf\nf1(x) = 2\nx log3(x) , x \u2208 (e,\u221e),\nthe cdf and the quantile function are given by\nF (x) = 1\u2212 1 log2(x) , x \u2208 (e,\u221e)\nF\u22121(p) = e 1\u221a 1\u2212p , p \u2208 (0, 1).\nWe note that f1 satisfies conditions 1) and 2) in Theorem 1; hence, K1 = O(1/n) by Lemma 2 and K3 = O(1/ \u221a n) by Lemma 6. However, f1 does not satisfy condition 3) in Theorem 1. As we next show, for f1 we indeed have K2 = \u221e;\nhence, D(X(np)\u2016Gn,p) = \u221e. This suggests that condition 3) (or a variation of it) in Theorem 1 might indeed be necessary for convergence. First, note that\nE\n[\n1\n(1\u2212 U(k)) m 2\n]\n= ck,n\n\u222b 1\n0\nuk\u22121(1 \u2212 u)n\u2212k (1\u2212 u)m2 du,\nwhere ck,n = \u0393(n+1)\n\u0393(k)\u0393(n+1\u2212k) ; hence,\nE\n[\n1\n(1 \u2212 U(k)) m 2\n]\n= \u221e, m 2 \u2265 n\u2212 k + 1. (72)\nNow, by using the Taylor expansion of ex, we have that\nE[X(k)] = E [ F\u22121(U(k)) ] = E [ e(1\u2212U(k)) \u22121/2 ]\n= E\n[\n\u221e \u2211\nm=0\n1\nm!\n1\n(1\u2212 U(k)) m 2\n]\n=\n\u221e \u2211\nm=0\n1\nm! E\n[\n1\n(1\u2212 U(k)) m 2\n]\n, (73)\nwhere in the last step we have used Tonelli\u2019s theorem (which holds even if the series diverges). Combining (72) and (73), we conclude that for every k there exists an m such that m2 \u2265 n\u2212 k + 1; hence, E[X(k)] = \u221e. This implies that\nE\n[\n( F\u22121(U(np))\u2212 F\u22121(p) )2 ] = E\n[\n( X(np) \u2212 e 1\u221a 1\u2212p )2 ]\n= \u221e,\nwhich leads to K2 = \u221e.\nB. Density in (21)\nFor the pdf\nf2(x) = 1\nx log2(x) , x \u2208 (0, e\u22121),\nthe cdf and the quantile function are given by\nF (x) = \u2212 1 log(x) , x \u2208 (0, e\u22121),\nF\u22121(p) = e\u2212 1 p , p \u2208 (0, 1).\nWe note that f2 satisfies conditions 2) and 3) in Theorem 1; hence, K1 = O(1/n) by Lemma 2 and K2 = O(1/ \u221a n) by Corollary 5. However, f2 does not satisfy condition 1) in Theorem 1. Nevertheless, as we next show, we can still prove the convergence of K3. We start by noting that\nf(F\u22121(p)) = p2e 1 p ;\nhence, from (14d) we obtain\nK3 = E\n[\nlog\n( f(F\u22121(U(np)))\nf(F\u22121(p))\n)]\n= 2E[logU(np)] + E\n[\n1\nU(np)\n]\n\u2212 2 log(p)\u2212 1 p\n= 2 (\u03c8(np)\u2212 \u03c8(n+ 1)) + n pn\u2212 1 \u2212 2 log(p)\u2212 1 p ,\nwhere \u03c8 is the digamma function. By noting that the digamma function can be approximated as \u03c8(x) = log(x)\u2212 12x +O( 1x2 ) and also using the fact that \u03c8(x+1) = \u03c8(x)+ 1x , we arrive at\nK3 = n+ 1\u2212 pn\u2212 p2n+ p pn(pn\u2212 1) +O ( 1 n2 ) ,\nwhich, together with K1 = O(1/n) and K2 = O(1/ \u221a n), implies D(X(np)\u2016Gn,p) = \u0398 ( 1 n ) .\nREFERENCES\n[1] C. R. Rao, C. Rao, and V. Govindaraju, Handbook of Statistics. Elsevier, 2006, vol. 17. [2] H. A. David and H. N. Nagaraja, Order Statistics, Third edition. John Wiley & Sons, 2003. [3] P. Laplace, \u201cThe\u0301orie analytique des probabilite\u0301s, deuxie\u0300me supple\u0301ment,\u201d Oeuvr. comp\u0131\u0300, vol. 7, no. 2, pp. 531\u2013580, 1818. [4] N. V. Smirnov, \u201cUber die Verteilung des allgemeinen Gliedes in der Variationsreihe,\u201d Metron, vol. 12, pp. 59\u201381, 1935. [5] \u2014\u2014, \u201cLimit distributions for the terms of a variational series,\u201d Trudy Matematicheskogo Instituta imeni VA Steklova, vol. 25, pp. 3\u201360, 1949. [6] A. A. Balkema and L. De Haan, \u201cLimit distributions for order statistics. I,\u201d Theory of Probability & Its Applications, vol. 23, no. 1, pp. 77\u201392, 1978. [7] A. A. Balkema and L. de Haan, \u201cLimit distributions for order statistics. II,\u201d Theory of Probability & Its Applications, vol. 23, no. 2, pp. 341\u2013358, 1979. [8] R.-D. Reiss, Approximate distributions of order statistics: With applications to nonparametric statistics. Springer science & business media, 2012. [9] M. M. Siddiqui, \u201cDistribution of quantiles in samples from a bivariate population,\u201d J. Res. Nat. Bur. Standards B, vol. 64, pp. 145\u2013150, 1960. [10] L. Weiss, On the asymptotic joint normality of quantiles from a multivariate distribution. Mathematics Research Center, United States Army, University of Wisconsin, 1963. [11] \u2014\u2014, \u201cThe asymptotic distribution of order statistics,\u201d Naval Research Logistics Quarterly, vol. 26, no. 3, pp. 437\u2013445, 1979. [12] \u2014\u2014, \u201cThe asymptotic joint distribution of an increasing number of sample quantiles,\u201d Annals of the Institute of Statistical Mathematics, vol. 21, no. 1, pp. 257\u2013263, 1969. [13] W. Feller, An Introduction to Probability Theory and its Applications. John Wiley & Sons, 2008, vol. 2. [14] R.-D. Reiss, \u201cOn the accuracy of the normal approximation for quantiles,\u201d The Annals of Probability, pp. 741\u2013744, 1974. [15] S. Ikeda and T. Matsunawa, \u201cOn the uniform asymptotic joint normality of sample quantiles,\u201d Annals of the Institute of Statistical Mathematics, vol. 24, no. 1, pp. 33\u201352, 1972. [16] M. Falk, \u201cA note on uniform asymptotic normality of intermediate order statistics,\u201d Annals of the Institute of Statistical Mathematics, vol. 41, no. 1, pp. 19\u201329, 1989. [17] R. R. Bahadur, \u201cA note on quantiles in large samples,\u201d The Annals of Mathematical Statistics, vol. 37, no. 3, pp. 577\u2013580, 1966. [18] M. L. Puri and S. S. Ralescu, \u201cLimit theorems for random central order statistics,\u201d in Probability Theory and Extreme Value Theory. De Gruyter Mouton, 2011, pp. 154\u2013182. [19] Y. Ma, M. G. Genton, and E. Parzen, \u201cAsymptotic properties of sample quantiles of discrete distributions,\u201d Annals of the Institute of Statistical Mathematics, vol. 63, no. 2, pp. 227\u2013243, 2011. [20] J. V. Linnik, \u201cAn information-theoretic proof of the central limit theorem with Lindeberg conditions,\u201d Theory of Probability & Its Applications, vol. 4, no. 3, pp. 288\u2013299, 1959. [21] A. R. Barron, \u201cEntropy and the central limit theorem,\u201d The Annals of probability, pp. 336\u2013342, 1986. [22] S. Artstein, K. M. Ball, F. Barthe, and A. Naor, \u201cOn the rate of convergence in the entropic central limit theorem,\u201d Probability theory and related fields, vol. 129, no. 3, pp. 381\u2013390, 2004. [23] O. Johnson and A. Barron, \u201cFisher information inequalities and the central limit theorem,\u201d Probability Theory and Related Fields, vol. 129, no. 3, pp. 391\u2013409, 2004. [24] M. Madiman and A. Barron, \u201cGeneralized entropy power inequalities and monotonicity properties of information,\u201d IEEE Transactions on Information Theory, vol. 53, no. 7, pp. 2317\u20132329, 2007.\n[25] S. G. Bobkov, G. P. Chistyakov, and F. Go\u0308tze, \u201cRate of convergence and Edgeworth-type expansion in the entropic central limit theorem,\u201d The Annals of Probability, pp. 2479\u20132512, 2013. [26] S. G. Bobkov, G. Chistyakov, and F. Go\u0308tze, \u201cRe\u0301nyi divergence and the central limit theorem,\u201d The Annals of Probability, vol. 47, no. 1, pp. 270\u2013323, 2019. [27] S. Baratpour, J. Ahmadi, and N. R. Arghami, \u201cSome characterizations based on entropy of order statistics and record values,\u201d Communications in Statistics-Theory and Methods, vol. 36, no. 1, pp. 47\u201357, 2007. [28] \u2014\u2014, \u201cCharacterizations based on Re\u0301nyi entropy of order statistics and record values,\u201d Journal of Statistical Planning and Inference, vol. 138, no. 8, pp. 2544\u20132551, 2008. [29] M. Abbasnejad and N. R. Arghami, \u201cRenyi entropy properties of order statistics,\u201d Communications in Statistics-Theory and Methods, vol. 40, no. 1, pp. 40\u201352, 2010. [30] N. Balakrishnan, F. Buono, and M. Longobardi, \u201cOn cumulative entropies in terms of moments of order statistics,\u201d arXiv preprint arXiv:2009.02029, 2020. [31] G. Zheng, N. Balakrishnan, and S. Park, \u201cFisher information in ordered data: A review,\u201d Statistics and its Interface, vol. 2, pp. 101\u2013113, 2009. [32] K. M. Wong and S. Chen, \u201cThe entropy of ordered sequences and order statistics,\u201d IEEE Transactions on Information Theory, vol. 36, no. 2, pp. 276\u2013284, 1990. [33] N. Ebrahimi, E. S. Soofi, and H. Zahedi, \u201cInformation properties of order statistics and spacings,\u201d IEEE Transactions on Information Theory, vol. 50, no. 1, pp. 177\u2013183, 2004. [34] A. Dytso, M. Cardone, and C. Rush, \u201cMeasuring dependencies of order statistics: An information theoretic perspective,\u201d in 2020 IEEE Information Theory Workshop (ITW). IEEE, 2021, pp. 1\u20135. [35] B. C. Arnold, N. Balakrishnan, and H. N. Nagaraja, A First Course in Order Statistics. Siam, 1992, vol. 54. [36] O. Rioul, \u201cYet another proof of the entropy power inequality,\u201d IEEE Transactions on Information Theory, vol. 63, no. 6, pp. 3595\u20133599, 2017. [37] M. Abramowitz and I. A. Stegun, Handbook of mathematical functions with formulas, graphs, and mathematical tables. US Government printing office, 1970, vol. 55. [38] A. Papoulis, \u201cThe Fourier integral and its applications,\u201d Polytechnic Institute of Brooklyn, McCraw-Hill Book Company Inc., USA, ISBN:\n67-048447-3, 1962. [39] O. Marchal and J. Arbel, \u201cOn the sub-Gaussianity of the beta and dirich-\nlet distributions,\u201d Electronic Communications in Probability, vol. 22, pp. 1\u201314, 2017. [40] S. Boucheron, G. Lugosi, and P. Massart, Concentration inequalities: A nonasymptotic theory of independence. Oxford university press, 2013."
        }
    ],
    "title": "Entropic CLT for Order Statistics",
    "year": 2022
}