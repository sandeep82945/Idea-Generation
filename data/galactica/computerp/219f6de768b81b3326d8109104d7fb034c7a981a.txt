Ballistic missile defense systems require accurate target recognition technology. Effective feature extraction is crucial for this purpose. The deep convolutional neural network (CNN) has proven to be an effective method for recognizing high-resolution range profiles (HRRPs) of ballistic targets. It excels in perceiving local features and extracting robust features. However, the standard CNN's fully connected manner results in high computational complexity, which is unsuitable for deployment in real-time missile defense systems with stringent performance requirements. To address the issue of computational complexity in HRRP recognition based on the standard one-dimensional CNN (1DCNN), we propose a lightweight network called group-fusion 1DCNN with layer-wise auxiliary classifiers (GFAC-1DCNN). GFAC-1DCNN employs group convolution (G-Conv) instead of standard convolution to effectively reduce model complexity. Simply using G-Conv, however, may decrease model recognition accuracy due to the lack of information flow between feature maps generated by each G-Conv. To overcome this limitation, we introduce a linear fusion layer to combine the output features of G-Convs, thereby improving recognition accuracy. Additionally, besides the main classifier at the deepest layer, we construct layer-wise auxiliary classifiers for different hierarchical features. The results from all classifiers are then fused for comprehensive target recognition. Extensive experiments demonstrate that GFAC-1DCNN with such simple and effective techniques achieves higher overall testing accuracy than state-of-the-art ballistic target HRRP recognition models, while significantly reducing model complexity. It also exhibits a higher recall rate for warhead recognition compared to other methods. Based on these compelling results, we believe this work is valuable in reducing workload and enhancing missile interception rates in missile defense systems. 1 conclusions can be drawn. 1. The Impact of Kernel Size As shown in Table 2, when the 6 models have the same 1D-Conv channel settings,
(34) ⎧⎪⎨⎪⎩ FP = TP∕(TP + FP), FR = TP∕(TP + FN), FM = 2 × FP × FR∕(FP + FR),
increasing the kernel size generally improves the recognition performance in most cases. Table 3 clearly demonstrates that increasing the kernel size also leads to an increase in the number of parameters. This suggests that increasing the kernel size enhances recognition performance but also raises computational complexity. 2. The Impact of 1D-Conv Channel Settings As shown in Table 2, increasing the channel number generally improves recognition performance for most models with the same kernel size. However, there may be a decrease in performance for some models when the channel number becomes too large. The optimal 1D-Conv channel settings for each kernel size are as follows: C5 for 1DCNN, C3 for AC-1DCNN, C6 for G-1DCNN, GAC1DCNN, GF-1DCNN, and GFAC-1DCNN. It is worth noting that increasing the channel number also results in an increase in the number of parameters, as shown in Table 3. 3. The Impact of G-Conv (1DCNN vs G-1DCNN, AC1DCNN vs GAC-1DCNN) From Table 2, it is evident that replacing the standard 1D-Conv with G-Conv results in a decrease in recognition performance, despite a significant decrease in complexity as shown in Table 3. This is due to G-Conv breaking the fully connected manner of standard 1D-Conv, which leads to a hindered information flow between channel groups. 4. The Impact of GF-Conv (G-1DCNN vs GF-1DCNN, GAC-1DCNN vs GFAC-1DCNN, 1DCNN vs GF1DCNN, AC-1DCNN vs GFAC-1DCNN) Compared to G-Conv, GF-Conv improves recognition performance by utilizing PW-Conv to fuse grouped features from G-Conv. Although adding PW-Conv after G-Conv increases computational complexity, it is still lower than standard 1D-Conv. When the 1D-Conv channel settings are C1 , C2 , C3 and C4 , GF-Conv has lower testing recognition accuracy than standard 1D-Conv. However, when the channel settings are C5 and C6 , GF-Conv significantly improves recognition performance. We believe this phenomenon is due to standard 1D-Conv being over-parameterized, while GF-Conv is somewhat underparameterized, which can be seen in Table 3. As the channel number increases, standard 1D-Conv becomes more over-parameterized, whereas GF-Conv becomes better parameterized. Consequently, the recognition performance of 1DCNN and AC-1DCNN decreases after C4 and C3 , respectively. In contrast, the recognition performance of GF-1DCNN and GFAC-1DCNN exhibits an approximately monotonically increasing trend from C1 to C6. 5. The Impact of Layer-Wise Auxiliary Classifiers (1DCNN vs AC-1DCNN, G-1DCNN vs GAC-1DCNN, GF-1DCNN vs GFAC-1DCNN) Table 2 shows that, for certain kernel sizes and 1D-Conv channel settings, the recognition
International Journal of Computational Intelligence Systems (2023) 16:190
1 3
190 Page 14 of 19
performance is improved with the inclusion of layerwise auxiliary classifiers. Specifically, the recognition of 1DCNN and GF-1DCNN is significantly enhanced with layer-wise auxiliary classifiers. However, there are still instances where the recognition performance is not improved, indicating the need for further in-depth research to enhance the effectiveness of layer-wise auxiliary classifiers in the future. Table 3 demonstrates that the introduction of auxiliary classifiers only slightly increases the computational complexity, which is applicable to missile defense systems. 6. The Impact of Combing GF-Conv and Layer-wise Auxiliary Classifiers (1DCNN vs GFAC-1DCNN) Based on the results presented in Table 2, GFAC-1DCNN exhibits higher testing accuracy compared to 1DCNN when the 1D-Conv channel settings are adjusted to C5 and C6 . Additionally, Fig. 6 depicts the testing accuracy curves of GFAC-1DCNN and 1DCNN. It is evident from Fig. 6 that GFAC-1DCNN demonstrates a faster convergence rate, reaching a higher asymptote. Moreover, GFAC1DCNN has a smaller parameter number, further affirm-
Table 4 Recognition results of different deep learning methods
Network type Metrics Recognition results of each type of target (%) Overall testing accuracy (%)
Spherical decoy Mothership Simple decoy High-imitated decoy Warhead
sDSAE [26] F P 100.00 92.24 99.85 98.21 83.59 94.56 F R
100.00 93.30 96.34 90.90 91.96 F M
100.00 92.77 98.07 94.41 87.58 Bi-LSTM [31] F
P 100.00 98.12 93.76 91.38 95.94 95.83
F R 100.00 96.86 97.65 96.86 88.40 F M
100.00 97.49 95.66 94.04 92.02 CNN1D-CA [17] F
P 100.00 99.86 97.90 95.18 91.45 96.89
F R 100.00 98.31 96.55 95.86 93.51 F M
100.00 99.08 97.22 95.52 92.47 Bi-GRU [32] F
P 100.00 97.99 94.67 94.29 93.78 96.14
F R 100.00 97.29 98.34 93.86 91.37 F M
100.00 97.64 96.47 94.07 92.56 1DCNN F
P 100.00 99.56 95.21 92.51 92.76 95.97
F R 100.00 96.86 96.40 96.11 90.85 F M
100.00 98.19 95.81 94.27 91.80 MSGF-1DCNN [35] F
P 100.00 99.56 99.01 96.04 91.68 97.11
F R 100.00 97.57 96.95 94.31 96.52 F M
100.00 98.56 97.97 95.17 94.03 AC-1DCNN F
P 100.00 98.74 99.58 94.06 93.27 97.11
F R 100.00 97.78 98.89 94.58 94.31 F M
100.00 98.26 99.23 94.32 93.78 G-1DCNN F
P 100.00 98.55 98.54 90.09 87.79 94.83
F R 100.00 94.58 94.03 94.72 90.83 F M
100.00 96.53 96.23 92.35 89.28 GAC-1DCNN F
P 100.00 98.40 97.86 94.99 83.74 94.67
F R 100.00 94.17 95.14 89.58 94.44 F M
100.00 96.24 96.48 92.21 88.77 GF-1DCNN F
P 100.00 99.15 99.44 94.11 93.91 97.31
F R 100.00 97.64 99.31 95.42 94.17 F M
100.00 98.39 99.37 94.76 94.04 GFAC-1DCNN F
P 100.00 98.60 99.31 96.58 93.42 97.48
F R 100.00 97.54 99.34 93.83 96.67 F M 100.00 98.12 99.38 95.29 95.02
1 3
ing the effectiveness and advancement of our proposed method. Figure  7 displays the receiver operating characteristic (ROC) curves and the corresponding area under curve (AUC) values of various HRRP recognition methods, with warheads as positive samples and other targets as negative samples, where FPR = TP∕(FN + TP) and TPR = TN∕(TN + FP) . Demonstrating exceptional performance specifically for warheads, GFAC-1DCNN achieves a remarkable AUC value of 0.9972. This high AUC value highlights the significant importance of GFAC-1DCNN's recognition capabilities in anti-missile missions. Based on the above experimental results, the proposed GF-Conv decreases computation complexity and enhances feature extraction by breaking away from the fully connected manner of standard 1D-Conv. Furthermore, the layerwise auxiliary classifier serves as a lightweight module to enhance recognition performance. It is noteworthy that our methods are particularly effective for over-parameterized 1DCNN models, especially those with larger channel numbers. The increase in recognition performance is more significant when the 1D-Conv channel settings are adjusted to C5 and C6. 4.4 comparison with other deep learning methods Table 4 presents the recognition results of deep neural network-based HRRP recognition methods. It is evident from
the table that each method yields varying recognition outcomes for different targets. Notably, all methods achieve 100% precision ( FP ) and recall ( FR ) for spherical decoys, which can be attributed to their distinct physical characteristics depicted in Fig. 5. The distinguishability between spherical decoys and other targets is pronounced, thereby resulting in superior recognition performance for spherical decoys. However, some methods exhibit relatively low FR (< 95%) for warhead recognition, namely Bi-LSTM, sDSAE, BiGRU, CNN1D-CA, 1DCNN, AC-1DCNN, GAC-1DCNN, GF-1DCNN, and G-1DCNN, while GFAC-1DCNN demonstrates the highest FR for warhead recognition. Figure 8 provides a detailed view of the recognition accuracy of different targets across the 11 compared models. Notably, GFAC-1DCNN exhibits excellent recognition performance for warheads and simple decoys, along with superior accuracy for all samples. However, the recognition accuracy for warheads lags behind that of other targets, demanding further research to enhance warhead recognition precision. It is worth mentioning that GFAC-1DCNN exhibits a slightly lower FP for warhead recognition compared to BiLSTM, Bi-GRU, and GF-1DCNN, potentially leading to a higher false alarm rate in anti-missile systems. Nevertheless, the higher FR can enhance missile interception rates and reduce losses. To further analyze the recognition performance of CNNbased methods in more detail, we plot the confusion matrix for each method using the test , as shown in Fig. 9. From
1 3
International Journal of Computational Intelligence Systems (2023) 16:190
1 3
Page 17 of 19 190
Fig. 9, it is evident that all CNN-based models, namely 1DCNN, AC-1DCNN, CNN1D-CA, G-1DCNN, GAC1DCNN, GF-1DCNN, MSGF-1DCNN, and GFAC-1DCNN, misclassify a significant proportion of warhead samples as high-imitated decoys. Similarly, all 8 DL methods misidentify a large number of high-imitated decoy samples as warheads. This observation aligns with the findings presented in Fig. 5, demonstrating that the physical characteristics of warheads and high-imitated decoys are remarkably similar. Consequently, distinguishing between the two proves challenging, leading to poor recognition performance in this aspect. Furthermore, it is worth noting that 1DCNN, AC-1DCNN, CNN1D-CA, G-1DCNN, GAC-1DCNN, GF-1DCNN, and MSGF-1DCNN exhibit a higher rate of misclassification for mothership and simple decoy samples as warheads. However, this limitation has been effectively addressed by our proposed method, GFAC-1DCNN, showcasing its superiority. 5 conclusions and future scope To address the issue of high computational complexity in standard 1DCNN for ballistic target HRRP recognition, this paper proposes a lightweight GFAC-1DCNN. The contributions are determined as follows. 1. The proposed GFAC-1DCNN architecture introduces G-Conv to replace standard convolutions, which significantly reduces computational complexity and model size by maximizing the group number. This enables real-time deployment feasibility for missile defense systems. 2. The linear fusion layer after G-Conv promotes information flow between groups, overcoming the limitation of lost inter-group connections that occur with naive G-Conv. This allows the aggregation of multi-group features to enhance representation learning. 3. Layer-wise auxiliary classifiers are designed to leverage hierarchical features from different network depths. Fusing their outputs improves accuracy by combining multi-level information, complementing the representation learned by the deepest classifier. Experimental results demonstrate that GFAC-1DCNN advances the SOTA in higher overall testing accuracy and recall rate for warhead targets compared to other deep learning methods while maintaining lower computational complexity. This highlights the potential of GFC-1DCNN to reduce computational burden in ballistic missile defense systems and enhance interception rates, making it highly valuable in engineering applications. However, the selection of hyper-parameters in GFAC1DCNN is currently based on rule-of-thumb methods,
which may not be suitable for real-life scenarios.