{
    "abstractText": "Grammatical Error Correction (GEC) has been broadly applied in automatic correction and proofreading system recently. However, it is still immature in Chinese GEC due to limited high-quality data from native speakers in terms of category and scale. In this paper, we present FCGEC, a fine-grained corpus to detect, identify and correct the grammatical errors. FCGEC is a human-annotated corpus with multiple references, consisting of 41,340 sentences collected mainly from multi-choice questions in public school Chinese examinations. Furthermore, we propose a Switch-Tagger-Generator (STG) baseline model to correct the grammatical errors in low-resource settings. Compared to other GEC benchmark models, experimental results illustrate that STG outperforms them on our FCGEC. However, there exists a significant gap between benchmark models and humans that encourages future models to bridge it. Our annotation corpus and codes are available at https://github.com/xlxwalex/FCGEC\u2020.",
    "authors": [
        {
            "affiliations": [],
            "name": "Lvxiaowei Xu"
        },
        {
            "affiliations": [],
            "name": "Jianwang Wu"
        },
        {
            "affiliations": [],
            "name": "Jiawei Peng"
        },
        {
            "affiliations": [],
            "name": "Jiayu Fu"
        },
        {
            "affiliations": [],
            "name": "Ming Cai"
        }
    ],
    "id": "SP:0a468ec2ae014b587674b565b3a22f0a3e5eb274",
    "references": [
        {
            "authors": [
                "Abhijeet Awasthi",
                "Sunita Sarawagi",
                "Rasna Goyal",
                "Sabyasachi Ghosh",
                "Vihari Piratla."
            ],
            "title": "Parallel iterative edit models for local sequence transduction",
            "venue": "EMNLP-IJCNLP.",
            "year": 2019
        },
        {
            "authors": [
                "Christopher Bryant",
                "Mariano Felice",
                "\u00d8istein E Andersen",
                "Ted Briscoe."
            ],
            "title": "The bea-2019 shared task on grammatical error correction",
            "venue": "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages",
            "year": 2019
        },
        {
            "authors": [
                "Christopher Bryant",
                "Hwee Tou Ng"
            ],
            "title": "How far are we from fully automatic high quality grammatical error correction",
            "venue": "In ACL,",
            "year": 2015
        },
        {
            "authors": [
                "Yiming Cui",
                "Wanxiang Che",
                "Ting Liu",
                "Bing Qin",
                "Shijin Wang",
                "Guoping Hu."
            ],
            "title": "Revisiting pre-trained models for chinese natural language processing",
            "venue": "EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Yiming Cui",
                "Wanxiang Che",
                "Ting Liu",
                "Bing Qin",
                "Ziqing Yang."
            ],
            "title": "Pre-training with whole word masking for chinese bert",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3504\u20133514.",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Dahlmeier",
                "Hwee Tou Ng."
            ],
            "title": "Better evaluation for grammatical error correction",
            "venue": "NAACL, pages 568\u2013572.",
            "year": 2012
        },
        {
            "authors": [
                "Daniel Dahlmeier",
                "Hwee Tou Ng",
                "Siew Mei Wu."
            ],
            "title": "Building a large annotated corpus of learner english: The nus corpus of learner english",
            "venue": "Proceedings of the eighth workshop on innovative use of NLP for building educational applications, pages",
            "year": 2013
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL.",
            "year": 2018
        },
        {
            "authors": [
                "Simon Flachs",
                "Oph\u00e9lie Lacroix",
                "Helen Yannakoudakis",
                "Marek Rei",
                "Anders S\u00f8gaard."
            ],
            "title": "Grammatical error correction in low error density domains: A new benchmark and analyses",
            "venue": "EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Kai Fu",
                "Jin Huang",
                "Yitao Duan."
            ],
            "title": "Youdao\u2019s winning solution to the nlpcc-2018 task 2 challenge: a neural machine translation approach to chinese grammatical error correction",
            "venue": "NLPCC, pages 341\u2013 350. Springer.",
            "year": 2018
        },
        {
            "authors": [
                "Masahiro Kaneko",
                "Masato Mita",
                "Shun Kiyono",
                "Jun Suzuki",
                "Kentaro Inui."
            ],
            "title": "Encoder-decoder models can benefit from pre-trained masked language models in grammatical error correction",
            "venue": "ACL.",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "ICLR.",
            "year": 2014
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Ves Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
            "year": 2019
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Mallinson",
                "Aliaksei Severyn",
                "Eric Malmi",
                "Guillermo Garrido."
            ],
            "title": "Felix: Flexible text editing through tagging and insertion",
            "venue": "EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Eric Malmi",
                "Sebastian Krause",
                "Sascha Rothe",
                "Daniil Mirylenka",
                "Aliaksei Severyn."
            ],
            "title": "Encode, tag, realize: High-precision text editing",
            "venue": "EMNLP.",
            "year": 2019
        },
        {
            "authors": [
                "Courtney Napoles",
                "Keisuke Sakaguchi",
                "Joel Tetreault."
            ],
            "title": "Jfleg: A fluency corpus and benchmark for grammatical error correction",
            "venue": "EACL.",
            "year": 2017
        },
        {
            "authors": [
                "Hwee Tou Ng",
                "Siew Mei Wu",
                "Ted Briscoe",
                "Christian Hadiwinoto",
                "Raymond Hendy Susanto",
                "Christopher Bryant."
            ],
            "title": "The conll-2014 shared task on grammatical error correction",
            "venue": "CoNLL: Shared Task, pages 1\u201314.",
            "year": 2014
        },
        {
            "authors": [
                "Kostiantyn Omelianchuk",
                "Vitaliy Atrasevych",
                "Artem Chernodub",
                "Oleksandr Skurzhanskyi."
            ],
            "title": "Gector\u2013grammatical error correction: tag, not rewrite",
            "venue": "BEA@ACL.",
            "year": 2020
        },
        {
            "authors": [
                "Gaoqi Rao",
                "Qi Gong",
                "Baolin Zhang",
                "Endong Xun."
            ],
            "title": "Overview of nlptea-2018 share task chinese grammatical error diagnosis",
            "venue": "Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications, pages 42\u201351.",
            "year": 2018
        },
        {
            "authors": [
                "Gaoqi Rao",
                "Erhong Yang",
                "Baolin Zhang."
            ],
            "title": "Overview of nlptea-2020 shared task for chinese grammatical error diagnosis",
            "venue": "Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications, pages 25\u201335.",
            "year": 2020
        },
        {
            "authors": [
                "Alla Rozovskaya",
                "Dan Roth."
            ],
            "title": "Grammar error correction in morphologically rich languages: The case of russian",
            "venue": "Transactions of the Association for Computational Linguistics, 7:1\u201317.",
            "year": 2019
        },
        {
            "authors": [
                "Keisuke Sakaguchi",
                "Courtney Napoles",
                "Matt Post",
                "Joel Tetreault."
            ],
            "title": "Reassessing the goals of grammatical error correction: Fluency instead of grammaticality",
            "venue": "TACL, 4:169\u2013182.",
            "year": 2016
        },
        {
            "authors": [
                "Yunfan Shao",
                "Zhichao Geng",
                "Yitao Liu",
                "Junqi Dai",
                "Fei Yang",
                "Li Zhe",
                "Hujun Bao",
                "Xipeng Qiu."
            ],
            "title": "Cpt: A pre-trained unbalanced transformer for both chinese language understanding and generation",
            "venue": "arXiv preprint arXiv:2109.05729.",
            "year": 2021
        },
        {
            "authors": [
                "Maksym Tarnavskyi",
                "Artem Chernodub",
                "Kostiantyn Omelianchuk."
            ],
            "title": "Ensembling and knowledge distilling of large sequence taggers for grammatical error correction",
            "venue": "ACL.",
            "year": 2022
        },
        {
            "authors": [
                "Viet Anh Trinh",
                "Alla Rozovskaya."
            ],
            "title": "New dataset and strong baselines for the grammatical error correction of russian",
            "venue": "Proceedings of ACLIJCNLP, pages 4103\u20134111.",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Meire Fortunato",
                "Navdeep Jaitly."
            ],
            "title": "Pointer networks",
            "venue": "Advances in neural information processing systems, 28.",
            "year": 2015
        },
        {
            "authors": [
                "Zhaohong Wan",
                "Xiaojun Wan",
                "Wenguang Wang."
            ],
            "title": "Improving grammatical error correction with data augmentation by editing latent representation",
            "venue": "COLING, pages 2202\u20132212.",
            "year": 2020
        },
        {
            "authors": [
                "Alex Wang",
                "Yada Pruksachatkun",
                "Nikita Nangia",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
            "venue": "Advances in neural information",
            "year": 2019
        },
        {
            "authors": [
                "Wei Wang",
                "Bin Bi",
                "Ming Yan",
                "Chen Wu",
                "Zuyi Bao",
                "Jiangnan Xia",
                "Liwei Peng",
                "Luo Si."
            ],
            "title": "Structbert: Incorporating language structures into pretraining for deep language understanding",
            "venue": "ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Yingying Wang",
                "Cunliang Kong",
                "Liner Yang",
                "Yijun Wang",
                "Xiaorong Lu",
                "Renfen Hu",
                "Shan He",
                "Zhenghao Liu",
                "Yun Chen",
                "Erhong Yang"
            ],
            "title": "2021a. Yaclc: A chinese learner corpus with multidimensional annotation",
            "venue": "arXiv preprint arXiv:2112.15043",
            "year": 2021
        },
        {
            "authors": [
                "Yu Wang",
                "Yuelin Wang",
                "Kai Dang",
                "Jie Liu",
                "Zhuo Liu."
            ],
            "title": "A comprehensive survey of grammatical error correction",
            "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 12(5):1\u201351.",
            "year": 2021
        },
        {
            "authors": [
                "Zheng Yuan",
                "Ted Briscoe."
            ],
            "title": "Grammatical error correction using neural machine translation",
            "venue": "NAACL-HLT, pages 380\u2013386.",
            "year": 2016
        },
        {
            "authors": [
                "Yue Zhang",
                "Zhenghua Li",
                "Zuyi Bao",
                "Jiacheng Li",
                "Bo Zhang",
                "Chen Li",
                "Fei Huang",
                "Min Zhang."
            ],
            "title": "Mucgec: a multi-reference multi-source evaluation dataset for chinese grammatical error correction",
            "venue": "NAACL-HLT.",
            "year": 2022
        },
        {
            "authors": [
                "Yuanyuan Zhao",
                "Nan Jiang",
                "Weiwei Sun",
                "Xiaojun Wan."
            ],
            "title": "Overview of the nlpcc 2018 shared task: Grammatical error correction",
            "venue": "CCF International Conference on Natural Language Processing and Chinese Computing (NLPCC), pages 439\u2013445.",
            "year": 2018
        },
        {
            "authors": [
                "Zewei Zhao",
                "Houfeng Wang."
            ],
            "title": "Maskgec: improving neural grammatical error correction via dynamic masking",
            "venue": "AAAI, pages 1226\u20131233.",
            "year": 2020
        },
        {
            "authors": [
                "Wangchunshu Zhou",
                "Tao Ge",
                "Chang Mu",
                "Ke Xu",
                "Furu Wei",
                "Ming Zhou."
            ],
            "title": "Improving grammatical error correction with machine translation pairs",
            "venue": "EMNLP. 1910",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1900 - 1918 December 7-11, 2022 \u00a92022 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "Grammatical error correction (GEC) is a complex task, aiming at detecting, identifying and correcting various grammatical errors in a given sentence. GEC has recently attracted more attention due to its ability to correct and proofread the text, which can serve a variety of industries such as education, media and publishing (Wang et al., 2021b).\nHowever, Chinese GEC (CGEC) is still confronted with the following three obstacles: (1) Lack of data. The major obstacle in CGEC is that the high-quality manually annotated data is limited compared to other languages (Dahlmeier et al., 2013; Napoles et al., 2017; Rozovskaya and Roth, 2019; Bryant et al., 2019; Flachs et al., 2020; Trinh and Rozovskaya, 2021). There are only five\n\u2217 Corresponding author. \u2020 Online evaluation site: https://codalab.lisn.upsaclay.fr/\ncompetitions/8020.\npublicly accessible datasets in CGEC: NLPCC18 (Zhao et al., 2018) , CGED (Rao et al., 2020), CTCQua, YACLC (Wang et al., 2021a) and MuCGEC (Zhang et al., 2022). (2) Data sources are nonnative speakers. The sentences in NLPCC18, CGED, YACLC and MuCGEC are all collected from Chinese as a Foreign Language (CFL) learner sources. However, massive errors from native speakers rarely arise in these sources. Therefore, the native speaker errors are more challenging with the inclusion of pragmatic data. Though CTC-Qua covers grammatical errors in native speakers, it has insufficient scale with 972 sentences. (3) Limited multiple references. For an erroneous sentence, there tends to be different correction methods. The sentences revised by the model may be correct, but different from the ground truth. This may cause unexpected performance degradation (Bryant and Ng, 2015). Besides, more references can offer various correction schemas enabling the model to accommodate more scenarios. Among CGEC, only MuCGEC and YACLC provide rich references.\nTo tackle aforementioned obstacles, we present FCGEC, a large-scale fine-grained GEC corpus with multiple references. The sentences in FCGEC are mainly collected from multi-choice questions in public school Chinese examinations. Therefore, our FCGEC is more challenging since it involves more pragmatic data in the examinations of native speakers. As for multiple references, we assign 2 to 4 annotators on each sentence, thus more references can be attained in this way. Moreover, we generate more references in the annotation process through techniques with synonym substitution.\nIn order to correct the grammatical errors, recent works are mostly based on two categories of benchmark models. Sequence-to-sequence (Seq2Seq) approaches regard GEC as a generation task that straightforward converts an erroneous sentence to the correct one (Yuan and Briscoe, 2016; Zhao and Wang, 2020; Fu et al., 2018). However, training\n1900\nsuch a generation model requires more computational resources due to the autoregressive decoder. Moreover, the generated style of Seq2Seq models is more arbitrary, which is not well applicable for GEC task. More recently, sequence-to-edit (Seq2Edit) approaches gain interests which take GEC as a token-level labeling task (Awasthi et al., 2019; Omelianchuk et al., 2020) via different edits, such as insert, delete, etc. Nevertheless, previous work falls short on altering the word order and correcting errors simultaneously with iterating.\nTo fill these gaps, we propose Switch-TaggerGenerator (STG) model as an effective baseline to correct grammatical errors in low-resource settings inspired by Mallinson et al. (2020). Our STG can be decomposed into three modules: Switch module determines the permutation of characters while Tagger module identifies the operation tags of each character in the sequence. Notably, benefiting from carefully designed compound tags, we eliminate the necessity for iteration. As for Generator module, we adopt non-autoregressive approach to fill in the characters that do not appear in the source.\nIn summary, our contributions are as follows:\n1. We present FCGEC, a large-scale fine-grained corpus with multiple references and more challenging errors for CGEC.\n2. We propose a STG model and then conduct experiments to compare with two categories of benchmark models (Seq2Seq and Seq2Edit). Experimental results illustrate that our STG model outperforms these models on FCGEC.\n3. We find a significant gap between human performance and benchmark models that encourage future models to bridge it."
        },
        {
            "heading": "2 Corpus Construction",
            "text": ""
        },
        {
            "heading": "2.1 Data Collection",
            "text": "We collect raw sentences mainly from two resources to obtain various Chinese grammatical error corpus from native speakers. (1) Public examination websites. We crawl the multi-choice grammatical error problems (More erroneous sentences than correct sentences) through public websites which contain exercises and exams designed by teachers and experts. These problems cover public school examinations for native students from elementary to high school. (2) News aggregator\nsites. To balance the quantity of erroneous sentences and correct sentences, we attain a diverse range of high quality sentences without grammatical error in news aggregator sites.\nIn total, we collect 54,026 raw sentences from these resources. After removing duplicated or incomplete sentences, there are 41,340 sentences in our FCGEC corpus. We describe in detail the data sources and data structures in Appendix A & B."
        },
        {
            "heading": "2.2 Fine-grained Data Format",
            "text": "To facilitate model for grammatical error detection and correction, we designate three-tier hierarchical levels of golden labels in FCGEC as follows:\nDetection Level. As a preliminary procedure to correcting grammatical errors, we require the binary classification of a given sentence according to whether it contains grammatical errors or not.\nIdentification Level. The labels in this level could be regarded as necessary for a multi-class categorization problem. As the examples shown in Table 1, we group grammatical errors into seven categories based on the error hierarchy. The definition of error types are as follows: Incorrect Word Collocation (IWC) is a word-level grammatical error in which the related words are combined in the improper pattern. Component Missing (CM) and Component Redundancy (CR) are also wordlevel errors that some components (e.g., subject and object) of the sentence are missing or redundant. Structure Confusion (SC) is a syntax-level grammatical error that combines two similar grammatical structures into a single incorrect one. Incorrect Word Order (IWO) covers grammatical errors in word-level and pragmatic-level. Compared to the previous work (Zhang et al., 2022), we also take into account the errors that require logic, common sense on top of syntax (e.g., recursive relationships). Illogical (ILL) and Ambiguity (AM) are pragmatic errors. The former comprises contradictory statements, while the latter includes expressions with indeterminate meanings.\nCorrection Level. In the correction level, we propose an operation-oriented paradigm to construct GEC labels instead of the error-coded or rewriting paradigms utilized in previous works (Ng et al., 2014; Sakaguchi et al., 2016). In rewriting paradigms, the annotators directly rewrite the raw sentences to the correct sentences without grammatical errors. However, it is difficult for annotators\nto rewrite in a consistent style, which leads to a drop in annotation quality. As for the error-coded paradigm, the annotators may diverge in determining the boundaries of the erroneous spans, thus raising the complexity of the procedure.\nIn contrast, the operation-oriented paradigm is on the basis of four fundamental correction operations : Insert, Delete, Modify and Switch. As an example shown in Figure 1, this paradigm is more compatible with the conventions of the annotator when correcting errors. Meanwhile, annotators only need to consider what operations are required to correct the sentences, instead of paying attention to the erroneous span (e.g., the selection of the words is left to post-processing for unified optimization). In addition, we have a large amount of correction prompts (explanations of grammatical error problems) developed by teachers and experts based on these four operations that can be utilized to accelerate annotation process."
        },
        {
            "heading": "2.3 Annotation Procedure",
            "text": "The annotators are asked to follow the given prompts to complete the three levels of labeling. Notably, we allow annotators to add unlimited references to sentences with grammatical errors based on the four operations in error correction level.\nIn order to improve annotation efficiency, we have developed a visual online tool to support the annotation procedure. In addition, we applied pattern matching and rule-based scripts to automatically convert a large amount (72.3%) of prompts into operation labels. We show the interface of our visual annotation tool in Appendix C.\nAs for annotation process, we hire 20 undergraduate students and 4 expert examiners to annotate and verify the GEC labels. We follow the annotation procedure in SuperGLUE (Wang et al., 2019a)\nthat each annotator should work on test data first. After that, they can compare their labels with the gold ones. We encourage them to discuss their mistakes, questions and standards with other annotators and experts. To attain high-quality annotation with multiple references, we duplicate the sentences in our corpus 2 to 4 times. Furthermore, it is guaranteed that the redundant sentences are annotated by different annotators. Then experts are asked to review data that the annotators could not in agreement on the labels and add reasonable references. It is worth mentioning that we search for possible synonyms of the characters generated by Insert and Modify operations in annotation. We believe that supplying more word choices to annotators can improve the multi-reference rate. Moreover, we set up a weekly communication meeting to discuss common issues in annotation and adapt the labeling criteria. In total, the entire annotation procedure lasted for more than 4 months."
        },
        {
            "heading": "2.4 Quality Control",
            "text": "To ensure the high-quality of our FCGEC, we adopt the following five criteria: (1) Each sentence is in-\nspected by two specialized annotators to correct spelling and punctuation errors before annotation. Meanwhile, they have to eliminate the incomplete sentences (due to unexpected text truncation). (2) The specialized annotators were also asked to tag the sentences from news aggregator source that might have grammatical problems while checking spelling errors. Then these potential sentences will be discussed in weekly communication meeting. (3) We ask the annotators to read our guidelines and annotate twenty test instances. Then experts check their accuracy of the annotation. The annotators that meet the accuracy (90%) could continue to label the official data. (4) We assign 2x to 4x annotators per sentence for the corpus. In case their annotations are different, the experts will determine the correct labels. After that, annotators can also learn from these mistakes to achieve selfimprovement. (5) After annotation, we unify the annotated labels under the minimal operation criteria inspired by Dahlmeier and Ng (2012) which applies fewer operations during correcting grammatical errors. More details about minimal operation algorithm is described in Appendix E."
        },
        {
            "heading": "2.5 Data Statistics and Comparison",
            "text": "We compare our corpus with other Chinese grammatical error datasets in Table 2. Moreover, the concrete statistics of FCGEC are shown in Table 3\nand Appendix D. We summarize the advantages of our FCGEC in the following three aspects:\nMultiple References. As discussed in Bryant and Ng (2015) and Zhang et al. (2022), the training and evaluation of GEC models can benefit from multiple references. In order to obtain more references, we ask the annotators to submit different reasonable operations for correcting errors. Meanwhile, we specifically provide several choices of synonyms for the generated text during annotation. We search for synonyms using both fine and coarse granularity. The fine-grained approach is to obtain synonyms from electronic dictionaries, while the coarse-grained way relies on similarity of the word vectors. It enhances the ratio of multiple references.\nMore Pragmatic Data. Pragmatic data involves errors in logic, common sense, ambiguity, etc. We increase the proportion of pragmatic data (Table 6) compared to other CGEC datasets, thus rendering the data more complex and challenging. Notably, we fix the ambiguity errors by providing references to correct them from different semantics.\nEffective Error Types. We assign more refined error types to the grammatical errors, and these types are closely related to the correction operations. As shown in Figure 2, error types are always highly relevant to particular operations (e.g., CM\nand CR rely on Insert and Delete operations respectively). We believe that error types can be utilized as auxiliary data to improve the performance of the GEC models to correct grammatical errors."
        },
        {
            "heading": "3 Benchmark Models",
            "text": "We divide the GEC task into a classification task and a correction task. The classification task involves the error detection and error type identification. We adopt the pre-trained language models (PLMs) based approaches for these tasks. As for the correction task, we propose a STG model to correct errors. Meanwhile, two categories of mainstream GEC models (Seq2Seq and Seq2Edit) are applied as benchmark models for our FCGEC."
        },
        {
            "heading": "3.1 Baselines for Classification Task",
            "text": "In error detection subtask, the model needs to determine whether a given sentence contains grammatical errors. Therefore, it is a binary classification task while the type identification subtask can be regarded as a multi-class classification task. The model should predict which of the seven error types the given erroneous sentences belong to. Note that some sentences may have multiple error types.\nRecently, PLMs are proved to be effective and achieve success in various fields, such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), BERT-WWM (Cui et al., 2021), MacBERT (Cui et al., 2020) and StructBERT (Wang et al., 2019b). Therefore, we adopt different PLMs enhanced models as the benchmark models for these classification tasks. Specifically, we treat multiple PLMs as the backbone network and apply fully-connected layers on top of it for detection and identification."
        },
        {
            "heading": "3.2 Proposed STG Model",
            "text": "To correct grammatical errors, we propose an effective benchmark model, STG, which tackles error correction in the low-resource settings. Figure 3 gives an overview of our model. STG decomposes the error correction task into three processing modules: Switch , Tagger and Generator. The Switch module determines the order of characters that appear in the output on the basis of pointer network (Vinyals et al., 2015). Our Tagger module predicts the operation tags of each character and the number of characters that need to be generated in sequence. As for the Generator module, it fills in the characters that are not present in source sentence. Notably, each module can be trained independently.\nSwitch Module. The input to Switch module for character i is the hidden representation hi \u2208 Rd from PLM, where d denotes the dimension of hidden representations. Then Switch module determines the next position index for character i based on the pointer network. We apply the self-attention to predict which possible character e(i) would be pointed to. It can be formulated as:\np (e(i)|hi) = attention ( hi, he(i) ) (1)\nThe self-attention with scaled dot-product can be computed as below:\nA = Attention (Q,K) = softmax ( QKT\u221a\nd\n)\n(2) where A is attention score matrices, Q and K are both linear projections of h. More details about our Switch module can be found in H.1.\nTagger Module. We first define five tags corresponding to the three operations (except Switch operation) as follows: the KEEP (K) tag is utilized to maintain the source character while the DEL (D) tag is assigned to remove character from source sequence. The tag of INS_t (I_t) represents the insertion of t words after the current character. The substituted character is marked as MOD (M) tag for Modify operation. As for the special case where the character is both substituted and required to insert other characters, we set the tag of MINS_t (MI_t) similar to I_t. Modification tags can perform a combination of multiple operations on a single character at the same time, thus eliminating the\nneed to correct the sentence via iterations as other edit-based methods. Limited by space, we provide some concrete examples in the Appendix H.2.\nWe take the prediction of tags and the number t of characters to be inserted or substituted for each character as classification task. Therefore, we apply two fully-connected layers to obtain tags and the number t. They can be written as:\nT = \u03c3 (Whsi + b) (3)\nwhere T denotes the tag or number t while hsi is the hidden representations of character i. And \u03c3 stands for the softmax function. W and b are the learned weights and bias.\nGenerator Module. As we can leverage the masked language modeling (MLM) task (Devlin et al., 2018) of BERT-style PLMs for generating the characters which do not appear in the source sequence, Generator module inserts or substitutes the character with a certain number t of [MASK] token according to their tags. Then it predicts which characters are suitable to fill into the masked places.\nTraining and Testing. During the training process, we utilize cross-entropy loss Lswitch, Ltag and Lgen for the three modules. The STG model can be trained in two paradigms: independent and joint. The difference between them is whether each module is trained separately and thus they cannot utilize the shared encoder. We combine the loss in joint paradigm as follows:\nL(\u03b8) = \u03bb1Lswitch + \u03bb2Ltag + \u03bb3Lgen + \u03b3\u03bb\u2225\u0398\u22252 (4) where \u03bb\u00b7 is the coupling co-efficiency that regulates the three losses.\u0398 represents all trainable parameters in STG model and \u03b3 denotes the coefficient of L2-regularization. Ltag is always larger than the other two losses, thus we generally set it one order of magnitude smaller. Furthermore, we train STG model with type identification (TTI) task to utilize auxiliary type data to improve model performance.\nAs for testing phase, we feed the erroneous sentence into each module in a pipeline fashion to correct errors. Specifically, we adopt constrained beam search to decode the sequence order."
        },
        {
            "heading": "3.3 Other Baselines for Correction Task",
            "text": "In order to present mainstream error correction models on our corpus, we take two categories of approaches as benchmark models:\nSeq2Seq Models. A portion of the works adopt Transformer-based (Vaswani et al., 2017) encoderdecoder architecture as Seq2Seq fashion for correcting grammatical errors. The neural machine translation (NMT) based method is adopted in Fu et al. (2018) to tackle CGEC. Besides, Kaneko et al. (2020) utilize BERT-fuse to incorporate BERT into an encoder-decoder model for GEC. Meanwhile, MuCGEC (Zhang et al., 2022) presents a benchmark model based on Seq2Seq architecture with the Chinese BART (Shao et al., 2021).\nSeq2Edit Models. Recent works also focus on the Seq2Edit models, which correct errors by labeling manipulations of each character. LaserTagger (Malmi et al., 2019) is applied to modify the sequence with three types of edits: insertion, deletion and substitution. PIE (Awasthi et al., 2019) presents iterative edit with custom inflection operations to correct the grammatical errors. GECToR (Omelianchuk et al., 2020) is an iterative sequence tagging framework with custom g-transformations that we adapt it to CGEC follow the efforts of Zhang et al. (2022)."
        },
        {
            "heading": "4 Experimental Results",
            "text": ""
        },
        {
            "heading": "4.1 Evaluation Metrics",
            "text": "Classification Task. We regard the error detection task and error type identification task as classification tasks. Therefore, we adopt four common metrics, i.e., Accuracy, Precision, Recall and Macro F1 score to evaluate the model performance.\nCorrection Task. As for correction task, we employ two different metrics : (1) Exact Match metric is obtained by calculating the percentage of corrected sentences for model outputs that exactly matched with the golden references. (2) The character-level edit metrics proposed by MuCGEC (Zhang et al., 2022) are utilized to compute finegrained model performance. After obtaining the optimal sequence for character-level editing, they merge consecutive edits of the same type into spanlevel for both model outputs and golden references. Then MuCGEC calculates the highest Precision, Recall and F0.5 score by comparing the edits of model outputs with each golden reference."
        },
        {
            "heading": "4.2 Experimental Settings",
            "text": "We conduct detailed experiments for fairly comparing benchmark approaches on our FCGEC. In classification tasks, we adopt officially released PLM\nparameters from HuggingFace website1. Then we fine-tune the different PLMs on our FCGEC for 4 epochs with batch size of 64. As for the correction task, we employ RoBERTa as the backbone PLM of our STG model and other benchmark models for training 100 epochs. Notably, the BART PLM (Lewis et al., 2019) utilized in Seq2Seq models is substituted by CPT (Shao et al., 2021). We set maximum t to 6 in Tagger module (It can cover 98% of the cases). In addition, we apply AdamW (Kingma and Ba, 2014) optimizer with a learning rate of 2e-5 and weight decay of 1e-2 for all tasks."
        },
        {
            "heading": "4.3 Human Evaluation Results",
            "text": "We hire 25 annotators from the crowd-sourcing platform of NetEase with a wide range of degrees and occupations. Specifically, annotators are restricted to be native speakers. We require them to annotate 10 instances for familiarization with the task requirements. Then they annotate 7,500 pieces of data (we randomly select 1,500 sentences from the test set and duplicate them 5 times), which is used to evaluate our human performance. As shown in Table 4 and 5, the error detection task is relatively easy for humans while error type identification task is harder due to the fact that it has more categories. As for correction task, it is also challenging for humans to correct grammatical errors. We further discuss the human performance based on the model performance in Section 4.4.\n1https://huggingface.co/models"
        },
        {
            "heading": "4.4 Overall Performance",
            "text": "The results of the classification tasks on FCGEC are shown in Table 4 for different PLMs from which several observations can be derived. First, the largesize variant PLMs perform better than other basesize models on the both detection and identification tasks as they can represent richer semantic information. To illustrate this observation, we roughly divide the error types into semantic and syntactic groups. We find that the average accuracy improvement for larger PLMs is significantly higher on the semantic group (7.6%) compared to the syntactic group (2.8%). Second, StructBERT-Large outperforms all PLMs on detection task while RoBERTaLarge achieves better performance on identification task, demonstrating two strong baselines at FCGEC. Moreover, there is an interesting observation on identification task that the humans have a lower performance of Recall than all PLMs, while the Precision is significantly better than them.\nIn terms of the correction task, Table 5 demonstrates the results of benchmark models on FCGEC. The overall performances of Seq2Edit-based models are better than the Seq2Seq-based models. Furthermore, our STG-series models substantially outperform them on FCGEC, which proves the effectiveness of STG architecture. Finally, there is still a significant gap comparing best-performing models with humans in all tasks. Moreover, the difficulty of the task also increases gradually on the detection, identification and correction, which are reflected on the difference of gap between models and humans."
        },
        {
            "heading": "4.5 Comparative Analysis",
            "text": "Independent training vs. Joint training. As we describe in Section 3.2, the Switch, Tagger, Generator modules in our STG can be trained flexibly either independently or jointly. In Table 5, STG with joint training (STG-Joint) brings gains of 4.17% in EM score and 4.86% in F0.5 score compared with independent training STG (STG-Indep). The results indicate that the performance of correction can be enhanced during joint training since each module of STG can share more information and complement each other under a unified optimization objective.\nInvestigate the benefit of error type identification to correction. In Figure 2, we illustrate the correlation between error types and the operations of correction. We observe a significant association among error types and operations, which motivates\nus to treat error type identification as an auxiliary task (TTI) for training STG model. As shown in Table 5, STG-Indep+TTI indicates that the three modules of STG are trained independently with the TTI task incorporated. Our STG achieves better performance after integrating the TTI task compared with STG-Indep, which demonstrates the efficient error type data can be utilized as auxiliary data to enhance model correction performance. Moreover, STG-Indep+TTI can also obtain an accuracy improvement of 1.78 points on the error type identification task compared to RoBERTa.\nFine-grained performance analysis. In Figure 4, we demonstrate the fine-grained performance based on grammatical error types for identification task with RoBERTa-Large and correction task with STG-Joint. Notably, the dark sectors in the pie chart of the identification task indicate the proportion of errors for visual comparison. First, we observe the minimum error rate on SC, indicating that the PLM is more sensitive to syntactic structure errors. Second, the PLM performs weakly in terms of word-level errors, especially CR. After analyzing the error scenarios, we discover that the PLM may easily treat CR and CM errors as IWC errors. Furthermore, the PLM fails to determine the error types at the pragmatic level (i.e., ILL and AM), which illustrates the challenge of FCGEC.\nAs for correction task, it is clear that the performance on CM and IWC is inferior. We consider this potentially due to the fact that CM and IWC always require the generation of characters, increasing the difficulty of correction. Moreover, the model encounters trouble with AM due to the inclusion of\npragmatic data such as ambiguity. It is hard for the model to distinguish the semantics in the sentences and correct it. In addition, we present more comparisons and fine-grained analyses in Appendix I.\nInfluence of the three modules in STG. The correction performance of our STG model is affected by three modules simultaneously. Thus we further investigate the impact of these modules on the STG-Joint. As shown in Figure 5, we analyze the char-level and sentence-level accuracy of each module. We ignore the Keep tag when calculating the char-level accuracy in Tagger module. Tagger-t Acc. is computed for the number t of I_t and MI_t tags. The first observation is that the performance of model is mainly constrained by Tagger module, while it fails to predict tags and number t precisely. Secondly, the performance of the Generator module illustrates that it is possible to achieve acceptable performance via only utilizing non-autoregressive approach with fine-tuning. Furthermore, despite the high performance of the Switch module, its role as the first module in the pipeline has a significant impact on the Tagger and Generator modules. Therefore a more robust performance of the Switch module is needed."
        },
        {
            "heading": "5 Related work",
            "text": "There already exists a lot of work on grammatical error correction for datasets and approaches. In terms of the dataset, most researches focus on English GEC. NUCLE (Dahlmeier et al., 2013), an early annotated corpus of GEC research, collects the erroneous sentences from students\u2019 essays in NUS. JFLEG (Napoles et al., 2017) is constructed from TOEFL exam with native sounding judgement. W&I (Bryant et al., 2019) collects the texts from non-native English students around the world in an online web platform and then manually an-\nnotates them for GEC. By contrast, the errors in LOCNESS (Bryant et al., 2019) are acquired from essays written by native English students. Unlike the previous dataset, CWEB (Flachs et al., 2020) focuses on grammatical errors in low error density domains from websites. However, the scale of the data is relatively insufficient in CGEC. NLPCC (Zhao et al., 2018), CGED (Rao et al., 2020), YACLC (Wang et al., 2021a) and MuCGEC (Zhang et al., 2022) are four publicly available non-native speaker resources for CGEC community, which encourage us to construct a high-quality CGEC corpus derived from native speakers.\nAs for the progress of GEC approaches, Seq2Seq and Seq2Edit are two mainstream approaches that achieve competitive results. Most of the work is based on Seq2Seq framework that generates the correct sentences directly (Zhou et al., 2019; Wan et al., 2020; Zhao and Wang, 2020; Kaneko et al., 2020). Furthermore, after Malmi et al. (2019) first apply the Seq2Edit approach, PIE (Awasthi et al., 2019) and GECToR (Zhang et al., 2022) are proposed to correct errors with iterating. After that, Tarnavskyi et al. (2022) employ an ensembling approach on GECToR for better performances."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we construct a large-scale corpus for Chinese grammatical error detection, identification and correction. Compared to previous CGEC corpus, our FCGEC is more complicated and challenging with pragmatic data. Furthermore, we provide multiple references so that the models can be evaluated for better performance. Furthermore, we propose a STG model to correct grammatical errors. Extensive experiments demonstrate that our STG outperforms the baselines and achieves the state-ofthe-art performance. However, experiments show that there exists a notable gap between cutting-edge models and humans. Therefore, it encourages the future GEC models to bridge the gap.\nLimitations\nThe limitations of our work can be categorized into two main aspects: our corpus and model.\nLimitations of FCGEC. In our FCGEC, a small number of sentences can be considered as different types of grammatical errors depending on correction methods. We do not provide a finer distinction between error types in this version. However, such fine-grained labels may supply more benefit in correction tasks (employing TTI as a auxiliary task).\nLimitations of STG. The major limitation of our STG is that although no iteration is required, it corrects the errors via a pipeline paradigm with each modules in inference stage, thus it takes more time in the inference stage. Moreover, we consider that better performance may be achieved if the Generator module is pre-trained with a massively parallel corpus such as Lang-8 (Zhao et al., 2018), which we do not conduct in this paper.\nEthics Statement\nLicensing Issues. FCGEC is a CGEC dataset collected from public examination websites and news aggregator sites. We collect the original grammatical error data or news data under the license of these sites or request for their permission. Meanwhile, the full attribution for original source of the data is cited in our FCGEC. In addition, we also commit not to use the corpus for commercial purposes, but only for the research studies.\nAnnotator Compensation. In our annotation procedure, we hire two categories of annotators. The first type is the annotators who annotate or examine the data for our FCGEC. We estimate that a skillful annotator requires about 1 to 2 minutes for each sentence to identify the error types and correct errors. On this basis, we pay the annotator 7 yuan (about 1 dollar) for 10 sentences. The second category is annotators from the crowd-sourcing platform of NetEase for computing human performances on FCGEC. Since they only require to do the measurement of the data, we set the compensation to 4 yuan (about 0.6 dollar) per 10 sentences."
        },
        {
            "heading": "Acknowledgements",
            "text": "This research is supported by the Science and Technology Project of Zhejiang Province (2022C01044) and the National Natural Science Foundation of China (51775496)."
        },
        {
            "heading": "A Source of Data Collection",
            "text": "For the source of public examination websites, we collect the practice exercises from KS5U (http: //5utk.ks5u.com/main.aspx), which are accessible online for public usage. As for the news aggregator sites (e.g., ZAKER, IT Home etc.), we randomly collect the titles or topic sentences from these sites. After that, we manually check and correct the sentences as we describe in Section 2.4 to ensure the quality of our corpus."
        },
        {
            "heading": "B Data Structure",
            "text": "We employ the JSON format to construct our FCGEC, as illustrated below:\n{\n\u201cid(The global id of the instance)\u201d: {\n\u201csentence\u201d: The original sentence,\n\u201cerror_flag\u201d: Whether sentence contains errors,\n\u201cerror_type\u201d: The error types of sentence,\n\u201coperation\u201d : [\n{ The operation of the first reference },\n{ The operation of the second reference },\n{ ... }],\n\u201cexternal\u201d : Additional information\n}\n}\nFor the format of four operations, we define each of the operations as follows:\n\u00bb Suppose the given sentence is \u201cA B C D E\u201d.\n1. Switch operation {\u201cSwitch\u201d:[0,2,1,3,4]} (\u201cA B C D E\u201d \u2192 \u201cA C B D E\u201d) // The values in the list indicate the order of the original\ncharacter index after the swap (Index starts from 0).\n2. Delete operation {\u201cDelete\u201d:[3]} (\u201cA B C D E\u201d \u2192 \u201cA B C E\u201d) // The characters indexed in the list will be deleted.\n3. Insert operation {\u201cInsert\u201d:[{\u201cpos\u201d:1,\u201ctag\u201d:\u201cINS_1\u201d,\u201clabel\u201d:[\u201cF\u201d]}]} (\u201cA B C D E\u201d \u2192 \u201cA B F C D E\u201d) // Insert a \u201cF\u201d after the character indexed by \u201cpos\u201d.\n4. Modify operation {\u201cModify\u201d:[{\u201cpos\u201d:2,\u201ctag\u201d:\u201cMOD_1\u201d,\u201clabel\u201d:[\u201cF\u201d]}]} (\u201cA B C D E\u201d \u2192 \u201cA B F D E\u201d) // Modify the character with index \u201cpos\u201d to \u201cF\u201d.\nC Visual Interface of Annotation Tool\nAs shown in Figure 6, we develop a visual annotation tool for the annotation process. Given an erro-\nneous sentence, annotators are able to utilize this tool for easily identifying the error types and correcting the errors via four operations, i.e., Switch, Delete, Insert and Modify. Concretely, the annotators only need to drag or click on the small blocks which represent each character at the bottom of the main form to complete the corresponding operation. In order to support multiple references of the correction task, we provide a button for adding a reference up to five. Moreover, the small form on the right side displays correction prompts from teachers and experts to inform the annotators of the correction. Our tool can automatically convert these prompts to operations via rules. In addition, the real-time correction labels are displayed on the bottom of this small form.\nFurthermore, our tool enables convenient comparison of multiple annotation operations of a sentence by different annotators, so that expert examiners can select the reasonable references. In practice, this flexible annotation tool has greatly accelerated our annotation procedure."
        },
        {
            "heading": "D More Statistics of FCGEC",
            "text": "We conduct more statistics on our corpus, including the length distribution of sentences, the proportion of error types in each category and the distribution of the number of reference numbers.\nLength distribution of sentences. We compute the distribution of sentence lengths in our corpus. As shown in Figure 7, the length of the longest sentence is 359, while the shortest only contains 9 characters. Furthermore, the average sentence length is 53.06. Based on this, we can use various types of PLMs to encode sentences from the corpus without having to deal with exceeding the length.\nDistribution of error types. In Table 6, we calculate the proportion of grammatical errors in the seven error categories on train set, validation set\nand test set. We can observe that the error types are split as closely as possible with a similar distribution. Moreover, we can notice that the pragmatic type of error (ILL, AM and part of the IWO) also accounts for a significant proportion. Therefore our corpus tends to be more challenging.\nThe distribution of references. For the erroneous sentences, we allow the annotators to correct them through a variety of references. Thus the distribution of sentences with respect to the number of references is shown in Figure 8. First, we analyze sentences with a single reference and find that most of them are due to two scenarios: (1) The sentence is short or the grammatical error is very simple and obvious. (2) Some categories of errors often have only one way to correct them, such as IWO and ILL. Second, for sentences that contain two references, the errors of SC play a significant role in them. This is due to the fact that SC errors are always corrected by removing one of the two similar grammatical structures. More references for correcting sentences often indicate the need to insert or modify characters. Furthermore, we consider that the number of references could be increased via an enhanced and more refined annotation process and\nby assigning more annotators to each sentence."
        },
        {
            "heading": "E Algorithm of Minimal Edit",
            "text": "Given a pair containing the incorrect sentence and the corrected sentence, we design an algorithm to generate the operation labels under the minimal operation criteria from such pair. The algorithm is illustrated in Algorithm 1. It is mainly utilized in the following two scenarios: (1) Automatically convert the prompts of teachers and experts into operation labels for our visual annotation tool. (2) Based on this algorithm, we can unify the operation labels after annotation procedure to ensure that fewer operations are adopted during correcting grammatical errors for quality control. Besides, this can also help us to check for mistakes and guarantee the consistency of the data in the annotation.\nIn addition, we can convert the data from other formats (i.e., rewriting and error-coded paradigm) to our operation labels through this algorithm. Thus we can apply our STG model to other datasets.\nAlgorithm 1 Attain the operation labels via minimum edit distance.\nInput: Source sentence S and target sentence T . Output: The operation labels L that convert S to T .\n1: if S == T then 2: return [{}] 3: end if 4: L = [{}] 5: tags = [\u201cCopy\u201d, \u201cModify\u201d, \u201cDelete\u201d, \u201cInsert\u201d] 6: mov = [(\u22121,\u22121), (\u22121,\u22121), (\u22121, 0), (0,\u22121)] 7: Calculate the character frequency fS of S, and fT of T . 8: if fS == fT then 9: Calculate the longest common substring s1 and sec-\nond longest one s2 between S and T . 10: Swap the positions of s1 and s2 with their indexes\npori in S. Then the swapped sentence Sswap and indexes pswap can be obtained.\n11: if Sswap == T then 12: L = [{\u201cSwitch\u201d : pswap}] 13: else 14: goto 17 15: end if 16: else 17: Calculate the Levenshtein distance and then obtain the edit path matrix Mp. 18: ops = [] 19: getOperations(Mp, len(S), len(T ), ops) 20: Merge labels with adjacent index and the same operation in ops to get L 21: end if 22: return L\n23: Function getOperations(Mp, i, j, ops) 24: if i == 0 and j == 0 then 25: return 26: end if 27: for all op such that op \u2208 tags do 28: if Mp[i][j].get(op) then 29: ops.append([i, j, op]) 30: getOperations(Mp, i+mov[0], j+mov[1], ops) 31: break 32: end if 33: end for 34: EndFunction"
        },
        {
            "heading": "F The Demographic of Humans",
            "text": "In order to evaluate human performance on our FCGEC, we employ 25 annotators from the crowdsourcing platform of NetEase. Moreover, with the aim of measuring human performance as completely as possible, we hire a diverse range of annotators with different aspects (Education, occupation, age, etc.). As shown in the Table 7, the platform of NetEase provides us with their non-private demographic information about the annotators.\nIt is worth mentioning that we ask the annotators to label more data (randomly sampling 50% of the test set and then duplicating them 5 times) compared to other work as a way to attain more precise human performance."
        },
        {
            "heading": "G Details of Pre-trained Language Models",
            "text": "We enhance the performance of the model with PLMs for both the classification and correction tasks. In order to enable better reproduction of our results, we provide the details and links to officially released pre-trained parameters in the Table 8.\nIn Seq2Seq models, we adopt CPT as the Chinese BART model. In particular, since some Chinese punctuation is missing in the vocabulary of the BART model (e.g., Chinese quotation marks), we avoid performance degradation by substituting the punctuation with their English counterparts during pre-processing stage."
        },
        {
            "heading": "H Details of Our STG Model",
            "text": "To better illustrate the details of our STG model, we present additional input samples and the processing for the three modules in this section.\nH.1 Switch Module As we describe in Appendix B, the labels of Switch indicate the order of the original character index after swapping. However, the index of the next character is predicted for each character in our Switch module. Therefore, we need to fill this gap by converting these labels. We demonstrate the differences between these two label types in Figure 9.\nIn Switch module, we utilize pointer network with self-attention mechanism to predict which character will be pointed to of each character. Furthermore, we adopt cross-entropy as the loss function to measure the margin between attention score matrix A and the golden labels for optimizing.\nH.2 Tagger Module In section 3.2, we introduce five tags (K, D, INS_t, M and MI_t) that correspond to the three operations (i.e., Delete, Insert and Modify) in Tagger module. For better understand our tags, we present some concrete examples in Figure 10.\nWith this well-designed tagging criterion, our STG model can perform arbitrary manipulations of the sequence without iteration. During the training stage, we employ two classification layers to determine the tags and the number t of INS_t and MI_t, separately. Meanwhile, The cross-entropy is also applied to compute the loss. We optimize both of the parameters in the two classification layers simultaneously by combining the loss of them.\nIn particular, we try a small trick (Note that the trick is not adopted for the results of our STG models in Table 5, for fair comparison) that can further improve the performance of the model, which is to utilize the weighted cross-entropy loss. As the majority of tags in a sentence are Keep, it is intuitive to increase the weight of other tags to solve this typical category imbalance problem. After we conduct experiment on STG-Joint with this trick, we observe a 0.72% performance improvement in Exact Match.\nH.3 Generator Module In Generator module, we exploit the features of BERT-style PLMs to predict the characters which\ndo not appear in the source sentence. The outputs of Tagger module are utilized to generate the input sequence with [MASK] tokens. We present an example of the input sequence in Figure 11.\nAfter that, the Generator module predicts the indexes of the characters in vocabulary list that should be filled in at [MASK]. Similarly, the crossentropy loss is adopted for optimizing the parameters in Generator module."
        },
        {
            "heading": "I More Comparisons and Analysis",
            "text": "To further investigate the differences in the ability of the correction models, we present more comparisons and analyses in this section.\nPerformance on four operations of correction. Table 9 illustrates another perspective of the finegrained results that we calculate the performance of correction models on different operations. More specifically, we split the entire test set into four small subsets that contain only one operation for incorrect sentences. Then we compare our STGJoint model with the best performing models in the Seq2Seq and Seq2Edit categories, respectively.\nFirst, we can observe that our STG-Joint is significantly outperforms the other two models in terms of Switch operation. This is due to the fact that we design a special Switch module to efficiently handle such operation. Secondly, the performance of STG-Joint and LaserTagger is comparable in terms of Delete operation. However, Seq2Seq model behaves relatively weakly, due to its arbitrary modifications that often tend to delete more characters. Lastly, we discover that all models have poor performances on Insert and Modify operations, indicating that the task of generating new characters is more challenging.\nOriginal performance of MuCGEC. In Section 4.4, we substitute the original backbone of the Seq2Seq and Seq2Edit models in MuCGEC (Zhang et al., 2022) for a fair comparison. We\nconduct additional experiments to demonstrate the original performance of models in MuCGEC. They employ the PLMs of Chinese-BART-Large2 and StructBERT-Large3 for Seq2Seq and Seq2Edit (GECToR) model, respectively.\nWe present the results of MuCGEC in Table 10. It is clear that the performances of models with large-size PLMs are better than those of the basesize PLMs. Specifically, Seq2Seq model is greatly improved after applying BART-Large as the backbone. Moreover, it is close to the results of our STG-series models. However, there is only a slight improvement of F0.5 for the Seq2Edit model. After we further observe the error examples of Seq2Seq and Seq2Edit on the test set, we discover that the results of Switch operation are the critical limitation for Seq2Edit model. This also illustrates the necessity of the Switch module in our STG.\nPerformances on the validation set. In order to evaluate the distribution of our split dataset, we show the results of the best performing models on the corresponding validation set for the three tasks (detection, identification and correction) in Table 11. It is reasonable to observe that the performance on the validation set is slightly better than on the test set. Therefore, we keep the data distribution on the validation set close to the test set,\n2https://huggingface.co/fnlp/bart-large-chinese 3https://github.com/alibaba/AliceMind/tree/main/\nStructBERT\nwhich facilitates the model to search for the best hyperparameters on the validation set. Thus we can obtain better performance on the test set.\nComputing resources and times. In Table 12, we show the detailed computing resources and hyperparameters for training our STG-Joint model. Meanwhile, we also record the training time consumed under these hyperparameters and devices."
        },
        {
            "heading": "J Case Study",
            "text": "In order to explore the performance of the models on the correction task, we conduct analyses on case examples. Similarly, we compare STG-Joint with the best performing models in the Seq2Seq and Seq2Edit categories (MuCGEC and LaserTagger). We present the cases in Table 13. Meanwhile, the\nEnglish version of the cases can be seen in Table 14. In particular, since the ground truth contains multiple references, we represent one of them as an illustration due to space constraints. Note that the results of the models in Table 13 are based on the multiple references.\nWe can derive several observations from these case examples. First, in the category of word order errors (IWO), both Seq2Seq and Seq2Edit models can correct the elementary errors (Example 10). However, they fail to solve the more difficult order errors (progressive relationship problem in Example 9). Since our STG model is specifically equipped with the Switch module, it is possible to correct for these errors. Secondly, in the case of error categories that require the generation of new characters (e.g. CM and IWC), more improvements are required for all models. Finally, the pragmatic errors are the most difficult to correct (especially for AM). We encourage future models to pay more attention to these types of errors."
        }
    ],
    "title": "FCGEC: Fine-Grained Corpus for Chinese Grammatical Error Correction",
    "year": 2022
}