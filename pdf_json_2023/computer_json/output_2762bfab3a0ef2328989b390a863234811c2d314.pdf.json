{
    "abstractText": "COPYRIGHT \u00a9 2023 Bodur, Nikolaus, Pr\u00e9vot and Fourtassi. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. Using video calls to study children\u2019s conversational development: The case of backchannel signaling",
    "authors": [
        {
            "affiliations": [],
            "name": "Mathieu Chollet"
        },
        {
            "affiliations": [],
            "name": "K\u00fcbra Bodur"
        },
        {
            "affiliations": [],
            "name": "Mitja Nikolaus"
        },
        {
            "affiliations": [],
            "name": "Laurent Pr\u00e9vot"
        },
        {
            "affiliations": [],
            "name": "Abdellah Fourtassi"
        }
    ],
    "id": "SP:8e401cd9e1b49239a6a8bf7988b7bfc5e5ebebb2",
    "references": [
        {
            "authors": [
                "J. Allwood",
                "L. Cerrato",
                "L. Dybkjaer",
                "K. Jokinen",
                "C. Navarretta",
                "P. Paggio"
            ],
            "title": "The MUMIN multimodal coding scheme,",
            "venue": "NorFA Yearbook,",
            "year": 2005
        },
        {
            "authors": [
                "A. Anderson",
                "H.S. Thompson",
                "E.G. Bard",
                "G. Doherty-Sneddon",
                "A. Newlands",
                "C. Sotillo"
            ],
            "title": "The HCRC map task corpus: natural dialogue for speech recognition,",
            "venue": "Proceedings of the Workshop on Human Language Technology,",
            "year": 1993
        },
        {
            "authors": [
                "A.H. Anderson",
                "M. Bader",
                "E.G. Bard",
                "E. Boyle",
                "G. Doherty",
                "S Garrod"
            ],
            "title": "The hcrc map task",
            "venue": "corpus. Lang. Speech",
            "year": 1991
        },
        {
            "authors": [
                "E. Baines",
                "C. Howe"
            ],
            "title": "Discourse topic management and discussion skills in middle childhood: the effects of age and task",
            "venue": "First Lang. 30, 508\u2013534. doi: 10.1177/0142723710370538",
            "year": 2010
        },
        {
            "authors": [
                "J.B. Bavelas",
                "L. Coates",
                "T. Johnson"
            ],
            "title": "Listeners as co-narrators",
            "venue": "J. Pers. Soc. Psychol. 79, 941. doi: 10.1037/0022-3514.79.6.941",
            "year": 2000
        },
        {
            "authors": [
                "B. Bigi"
            ],
            "title": "SPPAS: a tool for the phonetic segmentations of speech,",
            "venue": "The Eighth International Conference on Language Resources and Evaluation (Istanbul),",
            "year": 2012
        },
        {
            "authors": [
                "B. Bigi"
            ],
            "title": "SPPAS-multi-lingual approaches to the automatic annotation of speech",
            "venue": "the phonetician. J. Int. Soc. Phonetic Sci. 111, 54\u201369.",
            "year": 2015
        },
        {
            "authors": [
                "B. Bigi",
                "C. Meunier"
            ],
            "title": "Automatic segmentation of spontaneous speech",
            "venue": "Revista de Estudos da Linguagem 26, 1530. doi: 10.17851/2237-2083.26.4.1489-1530",
            "year": 2018
        },
        {
            "authors": [
                "J.E. Boland",
                "P. Fonseca",
                "I. Mermelstein",
                "M. Williamson"
            ],
            "title": "Zoom disrupts the rhythm of conversation",
            "venue": "J. Exp. Psychol. Gen. 151, 1272\u20131282. doi: 10.1037/xge0001150",
            "year": 2021
        },
        {
            "authors": [
                "A. Boudin",
                "R. Bertrand",
                "S. Rauzy",
                "M. Ochs",
                "P. Blache"
            ],
            "title": "A multimodal model for predicting conversational feedbacks,",
            "venue": "in International Conference on Text, Speech, and Dialogue (Olomouc: Springer),",
            "year": 2021
        },
        {
            "authors": [
                "S.E. Brennan",
                "H.H. Clark"
            ],
            "title": "Conceptual pacts and lexical choice in conversation",
            "venue": "J. Exp. Psychol. Learn. Mem. Cogn. 22, 1482. doi: 10.1037/02787393.22.6.1482",
            "year": 1996
        },
        {
            "authors": [
                "L.J. Brunner"
            ],
            "title": "Smiles can be backchannels",
            "venue": "J. Pers. Soc. Psychol. 37, 728. doi: 10.1037/0022-3514.37.5.728",
            "year": 1979
        },
        {
            "authors": [
                "J. Cassell",
                "A. Gill",
                "P. Tepper"
            ],
            "title": "Coordination in conversation and rapport,",
            "venue": "Proceedings of the Workshop on Embodied Language Processing (Prague: Association for Computational Linguistics),",
            "year": 2007
        },
        {
            "authors": [
                "E.V. Clark"
            ],
            "title": "Conversation and language acquisition: a pragmatic approach",
            "venue": "Lang. Learn. Dev. 14, 170\u2013185. doi: 10.1080/15475441.2017.1340843",
            "year": 2018
        },
        {
            "authors": [
                "H.H. Clark"
            ],
            "title": "Using Language",
            "venue": "New York, NY: Cambridge University Press.",
            "year": 1996
        },
        {
            "authors": [
                "C. Dideriksen",
                "R. Fusaroli",
                "K. Tyl\u00e9n",
                "M. Dingemanse",
                "M.H. Christiansen"
            ],
            "title": "Contextualizing conversational strategies: backchannel, repair and linguistic alignment in spontaneous and task-oriented conversations,",
            "venue": "Cognitive Science Society),",
            "year": 2019
        },
        {
            "authors": [
                "A.T. Dittmann"
            ],
            "title": "Developmental factors in conversational behavior",
            "venue": "J. Commun. 22, 404\u2013423. doi: 10.1111/j.1460-2466.1972.tb00165.x",
            "year": 1972
        },
        {
            "authors": [
                "A.T. Dittmann",
                "L.G. Llewellyn"
            ],
            "title": "Relationship between vocalizations and head nods as listener responses",
            "venue": "J. Pers. Soc. Psychol. 9, 79. doi: 10.1037/h0025722",
            "year": 1968
        },
        {
            "authors": [
                "B. Dorval",
                "C.O. Eckerman"
            ],
            "title": "Developmental trends in the quality of conversation achieved by small groups of acquainted peers",
            "venue": "Monogr. Soc. Res. Child Dev. 49, 1\u201372. doi: 10.2307/1165872",
            "year": 1984
        },
        {
            "authors": [
                "Y. Erel",
                "C.E. Potter",
                "S. Jaffe-Dax",
                "C. Lew-Williams",
                "A.H. Bermano"
            ],
            "title": "iCatcher: a neural network approach for automated coding of young children\u2019s eye movements",
            "venue": "Infancy 27, 765\u2013779. doi: 10.1111/infa.12468",
            "year": 2022
        },
        {
            "authors": [
                "R. Foushee",
                "D. Byrne",
                "M. Casillas",
                "S. Goldin-Meadow"
            ],
            "title": "Getting to the root of linguistic alignment: Testing the predictions of interactive alignment across developmental and biological variation in language skill,",
            "venue": "Proceedings of the 44th Annual Meeting of the Cognitive Science Society (Toronto, ON)",
            "year": 2022
        },
        {
            "authors": [
                "R. Fusaroli",
                "J. Ra\u0327czaszek-Leonardi",
                "K. Tyl\u00e9n"
            ],
            "title": "Dialog as interpersonal synergy",
            "venue": "New Ideas Psychol",
            "year": 2014
        },
        {
            "authors": [
                "R. Fusaroli",
                "E. Weed",
                "D. Fein",
                "L. Naigles"
            ],
            "title": "Caregiver linguistic alignment to autistic and typically developing children",
            "venue": "PsyArXiv [Preprint]. doi: 10.31234/osf.io/ysjec",
            "year": 2021
        },
        {
            "authors": [
                "C. Goodwin"
            ],
            "title": "Between and within: alternative sequential treatments of continuers and assessments",
            "venue": "Hum. Stud. 9, 205\u2013217. doi: 10.1007/BF00148127",
            "year": 1986
        },
        {
            "authors": [
                "C.M. Hale",
                "H. Tager-Flusberg"
            ],
            "title": "Social communication in children with autism: the relationship between theory of mind and discourse development",
            "venue": "Autism 9, 157\u2013178. doi: 10.1177/1362361305051395",
            "year": 2005
        },
        {
            "authors": [
                "V. Hazan",
                "M. Pettinato",
                "O. Tuomainen"
            ],
            "title": "kidlucid: London ucl children\u2019s clear speech in interaction database",
            "year": 2017
        },
        {
            "authors": [
                "L. Hess",
                "J. Johnston"
            ],
            "title": "Acquisition of backchannel listener responses to adequate messages",
            "venue": "Commun. Sci. Disord. Faculty Publications 11, 319\u2013335. doi: 10.1080/01638538809544706",
            "year": 1988
        },
        {
            "authors": [
                "A. Kendon"
            ],
            "title": "Some functions of gaze-direction in social interaction",
            "venue": "Acta Psychol. 26, 22\u201363. doi: 10.1016/0001-6918(67)90005-4",
            "year": 1967
        },
        {
            "authors": [
                "G. Kjellmer"
            ],
            "title": "Where do we backchannel?: on the use of mm, mhm, uh huh and such like",
            "venue": "Int. J. Corpus Linguist. 14, 81\u2013112. doi: 10.1075/ijcl.14.1.05kje",
            "year": 2009
        },
        {
            "authors": [
                "A. Krason",
                "R. Fenton",
                "R. Varley",
                "G. Vigliocco"
            ],
            "title": "The role of iconic gestures and mouth movements in face-to-face communication",
            "venue": "Psychonomic Bull. Rev. 29, 600\u2013612. doi: 10.3758/s13423-021-02009-5",
            "year": 2022
        },
        {
            "authors": [
                "K. Krippendorff",
                "Y. Mathet",
                "S. Bouvry",
                "A. Widl\u00f6cher"
            ],
            "title": "On the reliability of unitizing textual continua: Further developments",
            "venue": "Quality Quantity 50, 2347\u20132364. doi: 10.1007/s11135-015-0266-1",
            "year": 2016
        },
        {
            "authors": [
                "K. Laland",
                "A. Seed"
            ],
            "title": "Understanding human cognitive uniqueness",
            "venue": "Annu. Rev. Psychol. 72, 689\u2013716. doi: 10.1146/annurev-psych-062220-051256",
            "year": 2021
        },
        {
            "authors": [
                "A. Leung",
                "A. Tunkel",
                "D. Yurovsky"
            ],
            "title": "Parents fine-tune their speech to children\u2019s vocabulary knowledge",
            "venue": "Psychol. Sci. 32, 975\u2013984. doi: 10.1177/0956797621993104",
            "year": 2021
        },
        {
            "authors": [
                "S.C. Levinson",
                "J. Holler"
            ],
            "title": "The origin of human multimodal communication",
            "venue": "Philos. Trans. R. Soc. B Biol. Sci. 369, 20130302. doi: 10.1098/rstb.2013.0302",
            "year": 2014
        },
        {
            "authors": [
                "B.L. Long",
                "G. Kachergis",
                "K. Agrawal",
                "M.C. Frank"
            ],
            "title": "A longitudinal analysis of the social information in infants\u2019 naturalistic visual experience using automated detections",
            "venue": "Dev. Psychol. 2022, 1414. doi: 10.1037/dev0001414",
            "year": 2022
        },
        {
            "authors": [
                "B. MacWhinney"
            ],
            "title": "The CHILDES Project: Tools for Analyzing talk, Volume II: The Database",
            "venue": "Psychology Press.",
            "year": 2014
        },
        {
            "authors": [
                "B. Maroni",
                "A. Gnisci",
                "C. Pontecorvo"
            ],
            "title": "Turn-taking in classroom interactions: Overlapping, interruptions and pauses in primary school",
            "venue": "Euro. J. Psychol. Educ. 23, 59\u201376.",
            "year": 2008
        },
        {
            "authors": [
                "Y. Mathet"
            ],
            "title": "The agreement measure \u03b3 cat a complement to \u03b3 focused on categorization of a continuum",
            "venue": "Comput. Linguist. 43, 661\u2013681. doi: 10.1162/COLI_a_00296",
            "year": 2017
        },
        {
            "authors": [
                "Y. Mathet",
                "A. Widl\u00f6cher",
                "M\u00e9tivier",
                "J.-P."
            ],
            "title": "The unified and holistic method Gamma (\u03b3 ) for inter-annotator agreement measure and alignment",
            "venue": "Comput. Linguist. 41, 437\u2013479. doi: 10.1162/COLI_a_00227",
            "year": 2015
        },
        {
            "authors": [
                "C. Mazzocconi",
                "K.E. Haddad",
                "B. O\u2019Brien",
                "K. Bodur",
                "A. Fourtassi"
            ],
            "title": "Laughter mimicry in parent-child and parent-adult interaction,\u201d in International Multimodal Communication Symposium (MMSYM) (Barcelona)",
            "year": 2023
        },
        {
            "authors": [
                "C. Mazzocconi",
                "Y. Tian",
                "J. Ginzburg"
            ],
            "title": "What\u2019s your laughter doing there? a taxonomy of the pragmatic functions of laughter",
            "venue": "IEEE Trans. Affect. Comput. 13, 1302\u20131321. doi: 10.1109/TAFFC.2020.2994533",
            "year": 2020
        },
        {
            "authors": [
                "M.L. McHugh"
            ],
            "title": "Interrater reliability: the Kappa statistic",
            "venue": "Biochem. Med. 22, 276\u2013282. doi: 10.11613/BM.2012.031",
            "year": 2012
        },
        {
            "authors": [
                "T. Misiek",
                "B. Favre",
                "A. Fourtassi"
            ],
            "title": "Development of multi-level linguistic alignment in child-adult conversations,",
            "venue": "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics (Punta Cana),",
            "year": 2020
        },
        {
            "authors": [
                "T. Misiek",
                "A. Fourtassi"
            ],
            "title": "Caregivers exaggerate their lexical alignment to young children across several cultures,",
            "venue": "Proceedings of the 26th Workshop on the Semantics and Pragmatics of Dialogue (Dublin)",
            "year": 2022
        },
        {
            "authors": [
                "Morency",
                "L.-P",
                "I. de Kok",
                "J. Gratch"
            ],
            "title": "A probabilistic multimodal approach for predicting listener backchannels",
            "venue": "Auton. Agent Multi Agent Syst",
            "year": 2010
        },
        {
            "authors": [
                "S.M. Murphy",
                "D.M. Faulkner",
                "L.R. Farley"
            ],
            "title": "The behaviour of young children with social communication disorders during dyadic interaction with peers",
            "venue": "J. Abnorm. Child Psychol. 42, 277\u2013289. doi: 10.1007/s10802-013-9772-6",
            "year": 2014
        },
        {
            "authors": [
                "A. Nadig",
                "I. Lee",
                "L. Singh",
                "K. Bosshart",
                "S. Ozonoff"
            ],
            "title": "How does the topic of conversation affect verbal exchange and eye gaze? A comparison between typical development and high-functioning autism",
            "venue": "Neuropsychologia 48, 2730\u20132739. doi: 10.1016/j.neuropsychologia.2010.05.020",
            "year": 2010
        },
        {
            "authors": [
                "M. Nikolaus",
                "A. Fourtassi"
            ],
            "title": "Communicative feedback in language acquisition",
            "venue": "New Ideas Psychol. 68, 100985. doi: 10.1016/j.newideapsych.202",
            "year": 2023
        },
        {
            "authors": [
                "M. Nikolaus",
                "J. Maes",
                "J. Auguste",
                "L. Prevot",
                "A. Fourtassi"
            ],
            "title": "Large-scale study of speech acts\u2019 development using automatic labelling,",
            "venue": "Proceedings of the 43rd Annual Meeting of the Cognitive Science Society",
            "year": 2021
        },
        {
            "authors": [
                "A. \u00d6zy\u00fcrek"
            ],
            "title": "Role of gesture in language processing: Toward a unified account for production and comprehension,",
            "venue": "in Oxford Handbook of Psycholinguistics (Oxford: Oxford University",
            "year": 2018
        },
        {
            "authors": [
                "P. Paggio",
                "C. Navarretta"
            ],
            "title": "Head movements, facial expressions and feedback in conversations: empirical evidence from Danish multimodal data",
            "venue": "J. Multimodal User Interfaces 7, 29\u201337. doi: 10.1007/s12193-012-0105-9",
            "year": 2013
        },
        {
            "authors": [
                "H.W. Park",
                "M. Gelsomini",
                "J.J. Lee",
                "C. Breazeal"
            ],
            "title": "Telling stories to robots: The effect of backchanneling on a child\u2019s storytelling,\u201d in Proceedings of the 2017 ACM",
            "venue": "IEEE International Conference on Human-Robot Interaction (Singapore: IEEE), 2308\u20132314.",
            "year": 2017
        },
        {
            "authors": [
                "C. Peterson"
            ],
            "title": "The who, when and where of early narratives",
            "venue": "J. Child Lang. 17, 433\u2013455. doi: 10.1017/S0305000900013854",
            "year": 1990
        },
        {
            "authors": [
                "M.J. Pickering",
                "S. Garrod"
            ],
            "title": "Alignment as the basis for successful communication",
            "venue": "Res. Lang. Comput. 4, 203\u2013228. doi: 10.1007/s11168-0069004-0",
            "year": 2006
        },
        {
            "authors": [
                "M.J. Pickering",
                "S. Garrod"
            ],
            "title": "Understanding Dialogue: Language use and Social Interaction",
            "venue": "Cambridge: University Press.",
            "year": 2021
        },
        {
            "authors": [
                "L. Pr\u00e9vot",
                "J. Gorisch",
                "R. Bertrand",
                "E. Gorene",
                "B. Bigi"
            ],
            "title": "A sip of CoFee: a sample of interesting productions of conversational feedback,",
            "venue": "Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGdial), Prague, Czech Republic,",
            "year": 2017
        },
        {
            "authors": [
                "M. Rasenberg",
                "A. \u00d6zy\u00fcrek",
                "M. Dingemanse"
            ],
            "title": "Alignment in multimodal interaction: an integrative framework",
            "venue": "Cogn. Sci. 44, e12911. doi: 10.1111/cogs. 12911",
            "year": 2020
        },
        {
            "authors": [
                "G. Roffo",
                "Vo",
                "D.-B",
                "M. Tayarani",
                "M. Rooksby",
                "A. Sorrentino",
                "S Di Folco"
            ],
            "title": "Automating the administration and analysis of psychiatric tests: the case of attachment in school age children,",
            "venue": "Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (Glasgow),",
            "year": 2019
        },
        {
            "authors": [
                "M. Rooksby",
                "S. Di Folco",
                "M. Tayarani",
                "Vo",
                "D.-B",
                "R. Huan",
                "A Vinciarelli"
            ],
            "title": "The school attachment monitor\u2013a novel computational tool for assessment of attachment in middle childhood",
            "venue": "PLoS ONE 16,",
            "year": 2021
        },
        {
            "authors": [
                "H. Sacks",
                "E.A. Schegloff",
                "G. Jefferson"
            ],
            "title": "A simplest systematics for the organization of turn-taking for conversation",
            "venue": "Language 50, 696\u2013735. doi: 10.1353/lan.1974.0010",
            "year": 1974
        },
        {
            "authors": [
                "K. Sagae",
                "E. Davis",
                "A. Lavie",
                "B. MacWhinney",
                "S. Wintner"
            ],
            "title": "High-accuracy annotation and parsing of CHILDES transcripts,",
            "venue": "Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition (Prague),",
            "year": 2007
        },
        {
            "authors": [
                "E.A. Schegloff"
            ],
            "title": "Discourse as an interactional achievement: some uses of \u2018uh-huh\u2019 and other things that come between sentences",
            "venue": "Analyzing Discourse 71, 93.",
            "year": 1982
        },
        {
            "authors": [
                "M. Shatz",
                "R. Gelman"
            ],
            "title": "The development of communication skills: modifications in the speech of young children as a function of listener,",
            "venue": "in Monographs of the Society for Research in Child Development,",
            "year": 1973
        },
        {
            "authors": [
                "C.E. Snow"
            ],
            "title": "The development of conversation between mothers and babies",
            "venue": "J. Child Lang. 4, 1\u201322. doi: 10.1017/S0305000900000453",
            "year": 1977
        },
        {
            "authors": [
                "J. Sullivan",
                "M. Mei",
                "A. Perfors",
                "E. Wojcik",
                "M.C. Frank"
            ],
            "title": "SAYCam: a large, longitudinal audiovisual dataset recorded from the infant\u2019s perspective",
            "venue": "Open Mind 5, 20\u201329. doi: 10.1162/opmi_a_00039",
            "year": 2022
        },
        {
            "authors": [
                "L. Tickle-Degnen",
                "R. Rosenthal"
            ],
            "title": "The nature of rapport and its nonverbal correlates",
            "venue": "Psychol. Inq. 1, 285\u2013293. doi: 10.1207/s15327965pli0104_1",
            "year": 1990
        },
        {
            "authors": [
                "H. Titeux",
                "R. Riad"
            ],
            "title": "Pygamma-agreement: gamma \u03b3 measure for inter/intra-annotator agreement in Python",
            "venue": "J. Open Source Software 6, 2989. doi: 10.21105/joss.02989",
            "year": 2021
        },
        {
            "authors": [
                "M. Tomasello"
            ],
            "title": "The Cultural Origins of Human Cognition",
            "venue": "Cambridge, MA: Harvard University Press.",
            "year": 1999
        },
        {
            "authors": [
                "A.M. Turing"
            ],
            "title": "I.\u2013computing machinery and intelligence. Mind LIX 433\u2013460",
            "year": 1950
        },
        {
            "authors": [
                "K.J. Van Engen",
                "M. Baese-Berk",
                "R.E. Baker",
                "A. Choi",
                "M. Kim",
                "A.R. Bradlow"
            ],
            "title": "The wildcat corpus of native-and foreign-accented english: communicative efficiency across conversational dyads with varying language alignment profiles",
            "venue": "Lang. Speech 53, 510\u2013540. doi: 10.1177/0023830910372495",
            "year": 2010
        },
        {
            "authors": [
                "D.B. Vo",
                "S. Brewster",
                "A. Vinciarelli"
            ],
            "title": "Did the children behave? investigating the relationship between attachment condition and child computer interaction,",
            "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction (Utrecht),",
            "year": 2020
        },
        {
            "authors": [
                "A.S. Warlaumont",
                "J.A. Richards",
                "J. Gilkerson",
                "D.K. Oller"
            ],
            "title": "A social feedback loop for speech development and its reduction in autism",
            "venue": "Psychol. Sci. 25, 1314\u20131324. doi: 10.1177/0956797614531023",
            "year": 2014
        },
        {
            "authors": [
                "V.H. Yngve"
            ],
            "title": "On getting a word in edgewise,\u201d in Chicago Linguistics Society, 6th Meeting, Vol",
            "venue": "1970 (Chicago, IL), 567\u2013578.",
            "year": 1970
        }
    ],
    "sections": [
        {
            "text": "TYPE Original Research PUBLISHED 03 February 2023 DOI 10.3389/fcomp.2023.1088752"
        },
        {
            "heading": "OPEN ACCESS",
            "text": ""
        },
        {
            "heading": "EDITED BY",
            "text": "Mathieu Chollet, University of Glasgow, United Kingdom"
        },
        {
            "heading": "REVIEWED BY",
            "text": "Dong Bach Vo, \u00c9cole Nationale de l\u2019Aviation Civile (ENAC), France Patrizia Paggio, University of Copenhagen, Denmark\n*CORRESPONDENCE K\u00fcbra Bodur kubra.bodur@univ-amu.fr"
        },
        {
            "heading": "SPECIALTY SECTION",
            "text": "This article was submitted to Human-Media Interaction, a section of the journal Frontiers in Computer Science\nRECEIVED 03 November 2022 ACCEPTED 12 January 2023 PUBLISHED 03 February 2023"
        },
        {
            "heading": "CITATION",
            "text": "Bodur K, Nikolaus M, Pr\u00e9vot L and Fourtassi A (2023) Using video calls to study children\u2019s conversational development: The case of backchannel signaling. Front. Comput. Sci. 5:1088752. doi: 10.3389/fcomp.2023.1088752"
        },
        {
            "heading": "COPYRIGHT",
            "text": "\u00a9 2023 Bodur, Nikolaus, Pr\u00e9vot and Fourtassi. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.\nUsing video calls to study children\u2019s conversational development: The case of backchannel signaling\nK\u00fcbra Bodur1*, Mitja Nikolaus1,2, Laurent Pr\u00e9vot1 and Abdellah Fourtassi2\n1Aix-Marseille Universit\u00e9, CNRS, LPL, Marseille, France, 2Aix Marseille Universit\u00e9, CNRS, LIS, Marseille, France\nUnderstanding children\u2019s conversational skills is crucial for understanding their social, cognitive, and linguistic development, with important applications in health and education. To develop theories based on quantitative studies of conversational development, we need (i) data recorded in naturalistic contexts (e.g., child-caregiver dyads talking in their daily environment) where children are more likely to show much of their conversational competencies, as opposed to controlled laboratory contexts which typically involve talking to a stranger (e.g., the experimenter); (ii) data that allows for clear access to children\u2019s multimodal behavior in face-to-face conversations; and (iii) data whose acquisition method is cost-e ective with the potential of being deployed at a large scale to capture individual and cultural variability. The current work is a first step to achieving this goal. We built a corpus of video chats involving children in middle childhood (6\u201312 years old) and their caregivers using a weakly structured word-guessing game to prompt spontaneous conversation. The manual annotations of these recordings have shown a similarity in the frequency distribution of multimodal communicative signals from both children and caregivers. As a case study, we capitalize on this rich behavioral data to study how verbal and non-verbal cues contribute to the children\u2019s conversational coordination. In particular, we looked at how children learn to engage in coordinated conversations, not only as speakers but also as listeners, by analyzing children\u2019s use of backchannel signaling (e.g., verbal \u201cmh\u201d or head nods) during these conversations. Contrary to results from previous inlab studies, our use of a more spontaneous conversational setting (as well as more adequate controls) revealed that school-age children are strikingly close to adult-level mastery in many measures of backchanneling. Our work demonstrates the usefulness of recent technology in video calling for acquiring quality data that can be used for research on children\u2019s conversational development in the wild.\nKEYWORDS\nconversation, language acquisition, conversational development, cognitive development, nonverbal, backchannel\n1. Introduction\nConversation is a ubiquitous and complex social activity in our lives. Cognitive scientists consider it as a hallmark of human cognition as it relies on a sophisticated ability for coordination and shared attention (e.g., Tomasello, 1999; Laland and Seed, 2021). Prominent computer scientists have sometimes described it as the ultimate test for Artificial Intelligence (Turing, 1950). Its role in healthy development is also crucial: When conversational skills are not well developed, they can negatively impact our ability to learn from others and to maintain relationships (Hale and Tager-Flusberg, 2005; Nadig et al., 2010; Murphy et al., 2014). Thus, the scientific study of how conversational skills develop in childhood is of utmost importance to understand what makes human cognition so special, to design better (child-oriented)\nFrontiers in Computer Science 01 frontiersin.org\nconversational AI, and to allow more targeted and efficient clinical interventions (e.g., autistic individuals).\nConversation involves a variety of skills such as turntaking management, negotiating shared understanding with the interlocutor, and the ability for a coherent/contingent exchange (e.g., Sacks et al., 1974; Clark, 1996; Fusaroli et al., 2014; Pickering and Garrod, 2021; Nikolaus and Fourtassi, 2023). We know little about how these skills manifest in face-to-face conversations, mainly because conversation has two characteristics that have made it difficult to fully characterize using only traditional research methods in developmental psychology. First, conversation is inherently spontaneous, i.e., it cannot be scripted, making it crucial to study in its natural environment. Second, conversation relies on collaborative multimodal signaling, involving, e.g., gesture, eye gaze, facial expressions, as well as intonation and linguistic content (e.g., Levinson and Holler, 2014; \u00d6zy\u00fcrek, 2018; Krason et al., 2022). While controlled in-lab studies have generally fallen short of the ecological validity (that is, the first characteristic), observational studies in the wild (typically in the context of child-caregiver interaction), though very insightful, have fallen short of accounting for the multimodal dynamics, especially the role of non-verbal communicative abilities that are crucial for a coordinated conversation (e.g., Clark, 2018)."
        },
        {
            "heading": "1.1. Video calls as a scalable data acquisition method",
            "text": "Recent technological advances in Natural Language Processing and Computer Vision allow us, in theory, to go beyond the limitations of traditional research methods as they provide tools to study complex multimodal dynamics while scaling up to more naturalistic data. Nevertheless, work in this direction has been slowed down by the lack of data on child conversations that allow the study of their multimodal communicative signals. Indeed, most existing video data of child-caregiver interaction either use a third-point-view camera (CHILDES) (MacWhinney, 2014) or head-mounted cameras (Sullivan et al., 2022). Neither of these recording methods allows clear access to the interlocutor\u2019s facial expressions and head gestures.\nThe current work is a step toward filling this gap. More precisely, we introduce a data acquisition method based on online child-caregiver online video calls. Using this method, we build and manually annotate a corpus of child-caregiver multimodal conversations. While communication takes place through a computer screen, making it only an approximation of direct face-to-face conversation, the big advantage of this data acquisition method is that it provides much clearer data of facial expressions and head gestures than commonly used datasets of child-caregiver conversations. Other advantages of this setting include cost-effectiveness\u2014facilitating large-scale data collection (including across different countries)\u2014and its naturalness in terms of the recording context (as opposed to in-lab controlled studies).\nTo prompt conversations between children and their caregivers, we use a novel conversational task (a word-guessing game) that is very intuitive and weakly structured, allowing conversations to flow spontaneously. The goal is to approximate a conversational activity that children could do with their caregivers at home, eliciting as much as possible the children\u2019s natural conversational skills. Unlike other\u2014 more classic\u2014semi-structured tasks used in the study of adult-adult\nconversations (e.g., the map task Anderson et al., 1993), the wordguessing game does not require looking at a prompt (e.g., a map), thus optimizing children\u2019s non-verbal signaling behavior while interacting with the caregiver (we return to this point in the Task sub-section).\nFinally, to evaluate children\u2019s conversational skills compared to adult-level mastery, it may not be enough to consider the caregiver as the only \u201cend-state\u201d reference. The reason is that research has shown that caregivers tend to adapt to children\u2019s linguistic and conversational competencies (e.g., Snow, 1977; Misiek et al., 2020; Fusaroli et al., 2021; Leung et al., 2021; Foushee et al., 2022; Misiek and Fourtassi, 2022). Thus, in addition to child-caregiver conversations, we must study how adults behave in similar interactive situations involving other adults. We collected similar data involving the same caregiver talking either to another family member or to a non-family member, the former situation being closer to the childcaregiver social context."
        },
        {
            "heading": "1.2. Case study: Backchannel signaling",
            "text": "In addition to introducing a new data acquisition method for child-caregiver conversation, we also illustrate its usefulness in the study of face-to-face communicative development by analyzing how children compare to adults in terms of Backchannel signaling (hereafter BC). We chose BC as a case study since it is an important conversational skill that has received\u2014surprisingly\u2014little attention in the language development literature and because it relies heavily on multimodal signaling.\nBC (Yngve, 1970) is a communicative feedback that the listener provides to the speaker in a non-intrusive fashion such as short vocalizations like \u201cyeah\u201d and \u201cuh-huh,\u201d and/or nonverbal cues such as head nods and smiles. Despite not having necessarily a narrative content, BC is a crucial element in successfully coordinated conversations, signaling, e.g., attention, understanding, and agreement (or lack thereof) while allowing the speaker to make the necessary adjustments toward achieving mutual understanding or \u201ccommunicative grounding\u201d (Clark, 1996).\nWhat about the development of BC? While some studies have documented early signs of children\u2019s ability to both interpret and provide BC feedback in the preschool period (e.g., Shatz and Gelman, 1973; Peterson, 1990; Park et al., 2017), a few have pointed out that this skill continues developing well into middle childhood. For example, Dittmann (1972) analyzed conversations of 6 children between the ages of 7 and 12 in a laboratory setting where children conversed with adults and other children as well as children interacting with each other at school. The results found there to be fewer BC signals produced by young children in this age range compared to the older group (between 14 and 35 years old). Following Dittmann (1972)\u2019s study and Hess and Johnston (1988) aimed at providing a more detailed developmental account of BC behaviors in middle childhood using a task where children listened to board game instructions from an experimenter. In particular, the authors analyzed children\u2019s BC production in various speaker cues such as pauses >400 ms, the speaker\u2019s eye gaze toward the listener, and the speaker\u2019s clause boundaries. Hess and Johnston found that younger children produced less BC compared to older ones.\nWhile these previous studies (e.g., Dittmann, 1972; Hess and Johnston, 1988) provided important insight into BC development,\nFrontiers in Computer Science 02 frontiersin.org\nthey both analyzed BC when children were engaged in conversations that may not be the ideal context to elicit and characterize children\u2019s full conversational competence. Some conversations were recorded with strangers, in a laboratory, and/or with non-spontaneous predesigned scripts. We argue that children\u2019s conversational skills could have been underestimated in these previous studies because they focus on contexts that are unnatural to the child and are not similar to how they communicate spontaneously in daily life. Indeed, research has shown that context can influence the nature of conversational behavior more generally (e.g., Dideriksen et al., 2019). We hypothesize that recording children in a more natural context using methods such as ours would allow them to show more of their natural conversational skills, namely in terms of BC. More specifically, we expect children to produce more BC (relative to adults\u2019 production) compared to previous work. Furthermore, we predict that children\u2019s BC behavior would be closer to that found in the adult family dyads (than to adult non-family dyads) because it is supposed to provide a more similar social context to the childcaregiver context."
        },
        {
            "heading": "1.3. Related work and novelty of our study",
            "text": "The novelty of the current effort compared to previous work based on available child-caregiver conversational data (e.g., MacWhinney, 2014; Sullivan et al., 2022) is threefold:\n1) We aim at capturing the multimodal aspects of children\u2019s conversational skills, especially facial expressions, gaze, and head gestures which are crucial to face-to-face conversations. While much of previous work has focused on verbal or vocal signals (e.g., Snow, 1977; Warlaumont et al., 2014; Hazan et al., 2017; Clark, 2018), a comprehensive study of conversational skills requires that we also investigate how children learn to coordinate with the interlocutor using visual signals. It is worth mentioning that some previous studies have proposed procedures to collect and analyze videos of school-age children (e.g., Roffo et al., 2019; Vo et al., 2020; Rooksby et al., 2021), however, these studies do not provide a method to collect face-toface conversational data. Rather, they focus on clinical tasks such as the Manchester Child Attachment Story Task.\n2) We focus on middle childhood (i.e., 6\u201312 years old), an age range that has received little attention compared to the preschool period, although middle childhood is supposed to witness important developmental changes in conversational skills, e.g., in turn-taking management (Maroni et al., 2008), conversational grounding (e.g., backchannels, Hess and Johnston, 1988), and the ability to engage in coherent/contingent exchange (Dorval and Eckerman, 1984; Baines and Howe, 2010). Another\u2014perhaps more pragmatic\u2014reason we focus on middle childhood is that children in this period have already mastered much of formal language (e.g., phonology and syntax), allowing us to minimize the interference of language processingand production-related issues with the measurement of \u201cpure\u201d conversational skills.\n3) Regarding the case study of BC in middle childhood, the novelty of our work is that it provides a socially more natural context where children could show a more spontaneous use of their conversational skills. Indeed, in our new corpus, the children talk one-on-one with their caregivers (as opposed to a stranger/experimenter), are at home (as opposed to the lab or school),\nand converse to play a fun, easy, and natural game (word guessing) that many children are already familiar with, as opposed to scripted turns (e.g., Hess and Johnston, 1988) or complex conversational games such as the map task (e.g., Anderson et al., 1991).\nThe paper is organized into three parts: Corpus, Annotation, and the case study of BC. In the first part, we present the methodology for Corpus building. In the second, we present the coding scheme we used to annotate the video call recordings for several\u2014potentially communicative\u2014visual features as well as the inter-rater reliability for each feature. In the third, we present the results of the BC analyses comparing child-caregiver dyads to caregiver-adult dyads. Finally, we discuss the findings, impact, and limitations of this new data acquisition method."
        },
        {
            "heading": "2. The corpus",
            "text": "The Corpus consists of online video chat recordings. In what follows, we provide details about the participants, the recording setup, the task we used to elicit conversational data, and the recording procedure."
        },
        {
            "heading": "2.1. Participants",
            "text": "We recorded 20 dyads. Among these dyads, 10 involved children and their caregivers (the condition of interest) and 10 involved the same caregivers with other adults (the control condition). In the BC case study, below, we also analyzed this control condition by whether or not the caregiver and adult were family members: We had 5 family dyads and 5 non-family dyads (more detail is provided in Section 4). In total, we collected 20 conversations or N = 40 individual videos (i.e., of individual interlocutors). Each pair of videos lasted around 15 min for a total of 5 h and 49 min across both conditions. The children were 6\u201311 years old (M = 8.5, SD = 1.37). All 10 children were native French speakers and half were bilinguals. Children did not have any communicative or developmental disorders except for one child who had mild autism. One caregiver participated twice as they had two children in our sample. The caregivers were recruited among our university colleagues."
        },
        {
            "heading": "2.2. Recording setup",
            "text": "We did the recording using the online video chat system \u201cZoom\u201d (Zoom Video Communications Inc., 2021). The setup required that the caregiver and child use different devices (e.g., two laptops or a laptop and a tablet) and that they communicate from different rooms (if they record from the same house) in order to avoid issues due to echo. We also required that the caregiver wore a headset microphone during the recording for better audio quality.1\n1 If neither of the interlocutors wears a headphone, Zoom tends to\nautomatically cut the sound of one speaker when the speakers happen to talk\nover each other, which is an undesirable feature for the purpose of our data\ncollection, and in particular for the study of BC.\nFrontiers in Computer Science 03 frontiersin.org\nFIGURE 1 A snapshot of one of the recording sessions involving a child and her caregiver communicating through an online video chat system."
        },
        {
            "heading": "2.3. The task",
            "text": "The task consists in playing a word-guessing game in which one of the participants thinks of a word and the other tries to find it by asking questions. After a word has been guessed, the interlocutors alternate their roles. The participants were told that they can finish the game after around 10 min. The caregivers were provided with a list of words to use during the interaction with the child whereas the children were free to choose the words they wanted. Detailed task instructions can be found in the Appendix below.\nWe chose this weekly-structured task over free conversations in order to correct for the asymmetrical aspect of child-caregiver interaction. In fact, our piloting of free conversation has led to rather unbalanced exchanges whereby the caregiver ends up playing a dominant role in initiating and keeping the conversation alive. We also piloted other, semi-structured tasks that have been used in previous studies to elicit spontaneous exchange between adults such as the map task (Anderson et al., 1993) and variants of the \u201cspot the difference\u201d task (Van Engen et al., 2010). Nevertheless, in both of these tasks, we observed that the child tends to overly fixate on the prompt (the prompt being the map in the first task and the pair of pictures in the second) which hindered a natural multimodal signaling dynamics with the interlocutor. We ended up picking the word-guessing game as (i) it allowed for a balanced exchange between children and caregiver and (ii) it does not involve a prompt, thus, optimizing multimodal signaling behavior during the entire interaction."
        },
        {
            "heading": "2.4. Procedure",
            "text": "We recorded a three-way call involving 1) the experimenter (doing the recording), 2) the caregiver, and 3) either the child (in the condition of interest) or another adult (in the control condition). The experimenter was muted during the entire interaction and used a black profile picture in order not to distract the participants. The experimenter was able to record both interlocutors by pinning their profiles side-by-side on her local machine (Figure 1). Furthermore, the participants were instructed to hide \u201cself-view\u201d and to pin only their interlocutors. The procedure consisted of three phases (all were included in the recordings). First, the caregiver (in both conditions) explains the task to the child, then the pair does the task for around 10 min, and lastly, the caregiver initiates a free conversation with the child (or adult) to chat about how the task went (for about 5 min)."
        },
        {
            "heading": "2.5. Video call software: Zoom",
            "text": "We used Zoom for video calls since our participants were familiar with this software. It is worth mentioning that Zoom has built-in audio-enhancing features that can be problematic for the study of some conversational phenomena, especially BC. In particular, one feature consists in giving the stage to one speaker while suppressing background noise that may come from other participants\u2019 microphones. We made sure, in preliminary testing, that BC was not suppressed as \u201cbackground noise,\u201d at least in our case where there were only two active participants in the Zoom session."
        },
        {
            "heading": "3. Annotation",
            "text": "We manually annotated the entire N = 40 individual recordings in our corpus for several visual\u2014and potentially communicative\u2014 signals.2 The annotation was performed using a custom-developed template on ELAN. The annotated features were: gaze direction (looking at interlocutor vs. looking away), head movements (head nod and head shake), eyebrow movements (frowning and eyebrowraising), mouth displays (smiles and laughter), and posture change (leaning forward or backward). In what follows, we elaborate on each of these annotations followed by an explanation of the annotation methods."
        },
        {
            "heading": "3.1. Annotated features",
            "text": ""
        },
        {
            "heading": "3.1.1. Gaze direction",
            "text": "Previous studies (e.g., Kendon, 1967) suggest that eye gaze can be used to control the flow of conversation, e.g., by regulating the turntaking behavior: Speakers tend to look away from their partners when they begin talking and look back at them when they are about to finish their conversational turn. Gaze direction can also serve as a predictor of gestural feedback (e.g., Morency et al., 2010), signaling to the listener that the speaker is waiting for feedback. Thus, we annotated the occasions where the target participant was looking directly at the screen, which we took as an indication that the gaze was directed at the interlocutor."
        },
        {
            "heading": "3.1.2. Head movements",
            "text": "Head movements have various functions in human face-to-face communication such as giving feedback to\u2014and eliciting feedback from\u2014the interlocutor (Allwood et al., 2005; Paggio and Navarretta, 2013). We annotated two head movements that may play different communicative roles: head nods and head shakes."
        },
        {
            "heading": "3.1.3. Eyebrow displays",
            "text": "Eyebrow movements are among the most commonly employed facial expressions, they can communicate surprise, anger, and confusion. In our annotation scheme, we have two categories for eyebrow movements: raised (lifted eyebrows) or frown (the contraction of eyebrows and movement toward the nose).\n2 The corpus was also fully transcribed manually. Here we focus on the more\nchallenging task of coding the non-verbal signals.\nFrontiers in Computer Science 04 frontiersin.org"
        },
        {
            "heading": "3.1.4. Mouth displays",
            "text": "Smiles and laughs are common sources of backchannel communication between participants. They can, e.g., signal to the interlocutor that they are being understood (Brunner, 1979). They can also signal attention when combined with a head nod (Dittmann and Llewellyn, 1968). We annotated two categories of mouth display: smile and laughter."
        },
        {
            "heading": "3.1.5. Posture",
            "text": "Posture also plays a communicative role. In particular, leaning forward can indicate attention and/or positive feedback (Park et al., 2017) and leaning backward may occur as a turn-yielding signal (Allwood et al., 2005). We annotated posture by taking the starting point as the neutral position and any movement from there was tagged either forward or backward."
        },
        {
            "heading": "3.2. Annotation method and inter-rater reliability",
            "text": "The annotation task was structured in the following way: (1) Detecting that a target non-verbal event has occurred during the time course of the conversation; (2) Once the event has been detected, tagging its time interval (i.e., when it begins and ends) and; (3) tagging its category (e.g., smile, head nod, or leaning backward).3 The entire corpus was annotated by the first author. Additionally, a subset of 8 recordings (i.e., 20% of the data) including 4 children and 4 adults were independently annotated by another person in order to estimate inter-annotation agreement.\nEstimating the degree of agreement between annotators for timedependent events is not an easy task as it requires comparing not only agreement on the classification (whether an event is a laugh or a smile) but also agreement on the time segmentation of this event. Mathet et al. (2015) introduced a holistic measure, \u03b3 , that takes into account both classification and segmentation, accounting for some phenomena that are not well captured by other existing methods such as Krippendorff \u2019s \u03b1u (Krippendorff et al., 2016). Such phenomena include when segments overlap in time (e.g., when an interlocutor nods and smiles at the same time) which occur frequently in face-to-face data such as ours. Thus, it is the \u03b3 measure that we report in the current study using a Python implementation by Titeux and Riad (2021).\nIn a nutshell, this method estimates agreement by finding the optimal alignment in time between segments obtained from different annotators. The alignment minimizes both dissimilarity in time correspondence between the segments as well as dissimilarity in their categorization. The optimal alignment\n3 Note that similar visual events may vary in their precise communicative\nfunctions. For example, a head nod can be used either as a non-verbal\nanswer to a yes-no question or as a BC, indicating attention/understanding\nwithout necessarily signaling agreement. Such distinctions have not been made\nat this stage of this annotation (but see the case study on BC, below, for\nfurther specifications of some communicative moves). Here we annotated the\nabove-described features regardless of their precise communicative role in the\nconversation.\nis characterized by a disorder value: \u03b4. The measure \u03b3 takes into account chance by sampling N random annotations and computing their average disorder: \u03b4random. Finally, the chanceadjusted \u03b3 measure is computed as: \u03b3 = 1 \u2212 \u03b4\n\u03b4random . The\ncloser the value to 1, the stronger the agreement between the annotators.4\nWhile \u03b3 allows us to obtain a global score for the annotation. We were also interested in the finer-grained analysis of agreement comparing segmentation vs. categorization for each annotated feature. To obtain scores for categorization only, we use the \u03b3cat measure introduced by Mathet (2017) which is computed after one finds the optimal alignment using the original \u03b3 as described above. To analyze agreement in segmentation, we computed \u03b3 for each category separately, thus eliminating all ambiguities in terms of categorization and letting \u03b3 deal with agreement in segmentation only.\nRegarding agreement in our data, we found an average \u03b3av = 0.56 [0.49, 0.66] for children and an average \u03b3av = 0.65 [0.59, 0.78] for adults, where ranges correspond to the lowest and highest \u03b3 obtained in the four videos we double-annotated in each age group. How to interpret these numbers? Since \u03b3 is a new measure, it has not been thoroughly benchmarked with data similar to ours, as compared to more known measures (which, however, are less adequate for our data) such as Cohen\u2019s Kappa or Krippendorff \u2019s alpha. That said, \u03b3 is generally more conservative than other existing measures (Mathet et al., 2015). In addition, \u03b3 tends to (over-)penalize several aspects of disagreement in segmentation (see below). Thus, we can consider that the global scores obtained above reflect, overall, good agreement as can be seen in Table 1.\nTable 1 shows the detailed scores for each feature. We found that the scores for categorization were very high across all features and in both age groups, indicating that our non-verbal features are highly distinguishable from each other. The scores for segmentation, though overall quite decent (they are overall larger than 0.5 for all features), are much lower compared to categorization. Segmentation is harder because there are several ways disagreement can occur and for which it is penalized by \u03b3 . Part of this disagreement is quite relevant and should indeed be penalized such as when only one annotator detects a given event at a given time or when there is a mismatch between annotators in terms of the start and/or the end of the event. However, some aspects of disagreement need not necessarily be penalized such as when one annotator considers that an event is better characterized as a long continuous segment and the other annotator considers, instead, that it is better characterized as a sequence of smaller segments (e.g., a long continuous segment involving multiple consecutive nods vs. several consecutive but discrete segments representing, each, a single nod).\nWe found some features to be less ambiguous (to human annotators) than others, especially concerning their segmentation. For example, gaze switch, laughter, and head shake were among the least ambiguous for both children and adults. Posture change and eyebrow movement were the most ambiguous in children. Smiles and head nods had an intermediate level of ambiguity in both age groups. Finally, we noted that the agreement scores were overall\n4 N is chosen so that \u03b3 has a default precision level of 2%, we did not change\nthis default value in the current work.\nFrontiers in Computer Science 05 frontiersin.org\nhigher for adults compared to children, indicating that adults\u2019 nonverbal behavior was generally less ambiguous to our annotators than children\u2019s."
        },
        {
            "heading": "3.3. Distribution of non-verbal signals",
            "text": "Based on our annotations, we quantified non-verbal communication in child-caregiver multimodal conversation compared to the control condition of adult-caregiver conversation. Figure 2 shows the average number of target non-verbal behaviors (e.g., gaze switch, head nod, or smile) per minute in both conditions and for each speaker. We can observe that the frequency distribution is strikingly similar across all speakers. This finding suggests that non-verbal behavior data is quite balanced across children and adults. Thus, our corpus provides a good basis for future studies of how these cues are used to manage conversations and for comparison with how adults behave in similar conversational\ncontexts. The following section is an instance of how this data can be leveraged to focus on one conversational skill, i.e., backchannel signaling."
        },
        {
            "heading": "4. Case study: Backchannel signaling",
            "text": "This section provides a case study (illustrated by the examples in Table 2) of how the\u2014rather general purpose\u2014annotated corpus we presented above can be utilized to study specific conversational phenomena. We focus on BC signaling in middle childhood, providing a fresh perspective on the development of this skill compared to previous in-lab, rather controlled studies. In what follows, we present the methods we used to adapt the above-annotated corpus to the needs of this case study, then we present the results of the analyses comparing the child-caregiver dyads to adult-caregivers dyads both in terms of the rate of the listener\u2019s BC production and in terms\nFrontiers in Computer Science 06 frontiersin.org\nof the distribution of this production across the speaker\u2019s contextual cues."
        },
        {
            "heading": "4.1. Methods",
            "text": ""
        },
        {
            "heading": "4.1.1. Generic vs. specific BC",
            "text": "We adopt the distinction that Bavelas et al. (2000) has established experimentally between two types of BC: \u201cgeneric\u201d vs. \u201cspecific.\u201d5 As its name indicates, specific BC is a reaction to the content of the speaker\u2019s utterance. It might indicate the listener\u2019s agreement/disagreement, surprise, fear, etc. As for generic BC, it is performed to show that the listener is paying attention to the speaker and keeping up with the conversation without conveying narrative content (see examples in Table 2). It is important to note that, while generic BC does not target the narrative content, it does not mean that it is used randomly. Both generic and specific BCs should be timed precisely and appropriately so as to signal proper attentive listening to the speaker. Otherwise, they can be counter-productive and perceived, rather, as distracting and interrupting, even by young children (e.g., Park et al., 2017). In that sense, both specific and\n5 A roughly similar distinction has been proposed in Conversational Analysis\nliterature (Scheglo , 1982; Goodwin, 1986), using the terms \u201ccontinuers\u201d and\n\u201cassessment.\u201d\ngeneric BC are expected by interlocutors to be used in a collaborative fashion starting from their early development in childhood."
        },
        {
            "heading": "4.1.2. Family vs. non-family conditions",
            "text": "Previous work has pointed out differences in BC dynamics depending on the degree of familiarity between interlocutors (e.g., Tickle-Degnen and Rosenthal, 1990; Cassell et al., 2007), so we distinguished in the control caregiver-adult conversations between family members (5 dyads) and non-family members (5 dyads). Since interlocutors in the caregiver-adult condition are both mature speakers/users of the language, we did not separate them here based on whether they were the caregiver or adult, but based on whether they belong to family or non-family dyads. Thus, both the family and non-family conditions contain, each, N = 10 individuals, similar to the number of caregivers and children in the child-caregiver condition, making these four categories comparable in our analysis."
        },
        {
            "heading": "4.1.3. Listener\u2019s BC",
            "text": "While a multitude of signals can convey some form of specific active listening (surprise, amusement, puzzlement, etc.), here, we made this question more manageable by focusing on a subset of behaviors that can, a priori, be used for both specific BC and generic BC depending on the context of use. This subset is a good test for children\u2019s pragmatic ability to encode and decode specific vs. generic BC even when both are expressed by the same behavior. For\nFrontiers in Computer Science 07 frontiersin.org\nexample, a smile can be both an accommodating gesture (generic) or an expression of amusement (specific) depending on the context (see Table 2).\nBased on examination of the set of annotated non-verbal signals in our corpus, we found head nods and smiles to be used by children and adults both as specific and generic BC. Other nonverbal signals (e.g., head shakes, eyebrow displays, and laughs) were almost always specific, and therefore\u2014as we indicated above\u2014were not included in the analysis. Further, and in addition to non-verbal signals, BC can also be expressed verbally as short vocalizations (e.g., \u201cyeah,\u201d \u201cm-hm\u201d). We obtained these signals automatically using the SPPAS software (Bigi, 2015) which segments the audio input into Inter-Pausal Units (IPU) defined as pause-free units of speech. We operationalized a short vocalization as\u2014at least\u2014a 150 ms-long IPU. This threshold was the minimal duration of a verbal BC based on our preliminary trials and hand-checking of a few examples. The automatically detected units were then verified manually, keeping only the real instances of BC.\nA second round of annotation concerned the classification of these verbal and non-verbal signals into specific vs. generic BC. The first author annotated the entire set of these signals into specific vs. generic BC. In order to estimate inter-rater reliability, around 20% of the data were annotated independently by a second annotator. We obtained a Cohen Kappa value of \u03ba = 0.66, indicating \u201cmoderate\u201d agreement (McHugh, 2012). Table 3 shows some average statistics of Generic vs. Specific BC produced per participant in each modality. It shows that children and adults do produce both types of BC in each of the modalities we consider. Given our limited sample size and in order to optimize statistical power, all subsequent analyses were done by collapsing BC across all these three modalities."
        },
        {
            "heading": "4.1.4. Speaker\u2019s contextual cues",
            "text": "We also needed annotations of the speaker\u2019s contextual cues, i.e., the context of speaking where the listeners produce a BC. We considered two broad speaking contextual cues: a) overlap with speech, that is, when the listener produces a verbal or non-verbal BC without interrupting the speaker\u2019s ongoing voice activity, and b) during a pause, i.e., when the listener produces a BC after the speaker pauses for a minimum of 400 ms (following Hess and Johnston, 1988). In addition, we study the subset of cases where these two contexts overlap with the speaker\u2019s eye gaze, that is, when the listener produces a BC while the speaker is looking at them vs. while the speaker is looking away. The speaker\u2019s continuous segments of speech (i.e., IPUs) vs. pauses were annotated automatically using the Voice\nFIGURE 3 The rate of BC production per minute for both the child-caregiver condition and family vs. non-family in the caregiver-adult condition. Each data point represents a participant and ranges represent 95% confidence intervals.\nActivity Detection of the SPASS software (Bigi, 2012, 2015; Bigi and Meunier, 2018). As for the speaker\u2019s gaze (looking at the listener vs. looking away), it was annotated manually (see Section 3 above)."
        },
        {
            "heading": "4.2. Results",
            "text": ""
        },
        {
            "heading": "4.2.1. Rate of BC production",
            "text": "We computed the total number of specific and generic BC produced by each interlocutor, then we divided this number by the length of the conversation in each case to have a measure of the rate of production per minute. Results are shown in Figure 3. When comparing children to caregivers, we found what seems to be a developmental effect regarding the rate of production of specific vs. generic BC. More precisely, children produced specific BC at a higher rate than generic BC. This difference was larger for children than for caregivers. Statistical analysis confirmed this observation: A mixed-effect model predicting only children\u2019s rate as a function of BC type6 yielded an effect of \u03b2 = 1.47 (SE = 0.17, p < 0.001), meaning there was a difference between specific and generic BC rate in children. A second model predicting the rate of production (by both children and caregiver) as a function of both BC type and interlocutor (child or caregiver) 7 showed there to be an interaction \u03b2 = 0.81 (SE = 0.37, p < 0.05), meaning that the difference between BC types in children is larger than it is in caregivers. As can be seen in Figure 3, and although children produce slightly fewer generic BC, the developmental difference can be largely attributed to children producing more specific BC compared to caregivers.\nIn the same Figure 3, we also have the rates of production in the Adult-Adult control conditions. While these controls were meant to be contrasted with the child-caregiver dyads, here we noticed an interesting difference among them: Both BC types were higher in the non-family dyads than in the family dyads. Using a mixed-effects model predicting the rate of BC production as a function of type and family membership, 8 we found a main effect of family membership: \u03b2 = 0.81 (SE = 1.57, p = 0.01) but there was no effect of BC\n6 Specified as Rate \u223c BC_type + (1 | dyad).\n7 Specified as Rate \u223c BC_type*Interlocutor + (1 | dyad).\n8 Specified as Rate \u223c BC_type*Family + (1 | dyad).\nFrontiers in Computer Science 08 frontiersin.org\ntype nor an interaction. By comparing the child-caregiver condition to family vs. non-family of the adult-adult conditions in Figure 3, we make the following observations. Caregivers, when talking to children, produced BC at a rate roughly similar to the one used among adults in the family dyads. The same thing can be said about children: Although they tend to produce slightly more specific BC and slightly fewer generic BC than adults, these differences were small and not statistically significant. This comparison suggests that, overall, children are not less prolific in terms of BC production than adults, at least when adults converse in a family context."
        },
        {
            "heading": "4.2.2. Distribution of BC over speaker\u2019s contextual cues",
            "text": ""
        },
        {
            "heading": "4.2.2.1. Child-Caregiver condition",
            "text": "While the results shown in Figure 3 inform us about the overall rate of BC production, they do not show the speaker\u2019s contextual cues where this production occurs. As explained in the Methods subsection, we examined the nature of BC production of the listener during the speaker\u2019s speech vs. pause and in the subset of cases where these cues overlap with the speaker\u2019s gaze (Kendon, 1967; Kjellmer, 2009; Morency et al., 2010). In Figure 4 we show the rate of BC production of children and caregivers across these cues. The main observation is that children and caregivers produce BC across speaker\u2019s cues in roughly similar proportions. Another observation is that, for both interlocutors, while BC in speech almost always coincided with the speaker\u2019s gaze at the listener (though this is likely due to a floor effect), this was not the case for BC produced during pauses where part of this production did not coincide with the speaker\u2019s gaze (in other words, listeners also provided BC when the speaker paused and looked away). One minor difference between children and caregivers is the following. While their rate of BC during speech was generally low, this rate was slightly higher for children (while almost totally absent for caregivers). The origin of this small difference is unclear: it could be due to children providing fewer BC opportunities for adults to capitalize on, or to adults not providing BC despite such opportunities. Another difference is that caregivers seem to produce slightly more generic BC after pauses than children do (though this difference was not statistically significant)."
        },
        {
            "heading": "4.2.2.2. Adult-adult (control) conditions",
            "text": "Next, we examined how BC production varied across the speaker\u2019s cues between family and non-family dyads in the adultadult control conversations. The results are shown in Figure 5. Unsurprisingly, and in line with findings in Figure 3, we observe a general increase in BC production rate among non-family dyads relative to family dyads. However, this increase was\u2013interestingly\u2013 not similar across the speaker\u2019s cues. In particular, while the average BC production rate remained similar during the speaker\u2019s pauses, we observed a striking increase in generic BC produced by nonfamily listeners during the speaker\u2019s speech, going from almost zero to around 1 BC per minute. Indeed, a mixed-effects model predicting the rate of generic BC as a function of family membership showed there to be a strong difference: \u03b2 = 0.84 (SE = 0.2, p < 0.001). Another observation is that, for both family and non-family dyads, only part of the BCs co-occurred with the speaker\u2019s eye gaze toward the listener in both speaker\u2019s speech and pause, i.e., many BC occurred when the speaker was looking away. By comparing Figures 4, 5, we conclude that patterns of BC distribution (across\nspeaker\u2019s cues) of children and caregivers are not only largely similar to each other, but also similar to the patterns of BC distribution of adult-adult conversations in the family context. In fact, we observed much more differences due to family membership between adults than differences due to developmental age."
        },
        {
            "heading": "4.3. Discussion",
            "text": "This case study focused on BC in child-caregiver conversations and compared children\u2019s behavior to adult-level mastery in family and non-family contexts. While previous work (e.g., Dittmann, 1972; Hess and Johnston, 1988) found BC to be still relatively infrequent in middle childhood, here we found that children in the same age range produced BC at a similar rate as in adult-adult conversations when the adults were family members, an arguably more pertinent control condition than when adults are not family members. In the latter, the rate of production of BC was much higher.\nAn alternative interpretation of children\u2019s adult-level BC production is the fact that caregivers may provide children with more BC opportunities than what they would have received otherwise, thus \u201cscaffolding\u201d their BC production. While it is difficult to quantify exhaustively and precisely all BC opportunities, we can have an approximation by counting all pauses of at least 400 ms between two successive sound segments (or IPUs) of the same speaker. Using this rough estimate9, Figure 6 shows that, indeed, children are offered slightly more opportunities by the caregiver than the other way around. However, the number of BC opportunities is higher in adultadult conversations. Thus, the number of BC opportunities alone does not explain why children still produce BC at a similar rate as adults in the family dyads.\nRegarding the distinction between specific vs. generic BC, we found that children used verbal and non-verbal behavior to signal fewer generic BC compared to specific BC. However, this result does not necessarily mean children find generic BC harder. Indeed, the rate of generic BC was very low for both children and adults in family dyads. Findings from the non-family control condition suggest a better interpretation of this result. In this condition, adults provided a much higher rate of generic BC. We speculate that the participants used more generic BC (e.g., smiles) to establish social rapport with a stranger. In family dyads (and regardless of the interlocutor\u2019s ages), however, social rapport is already established, requiring less explicit accommodating signals (see also Tickle-Degnen and Rosenthal, 1990; Cassell et al., 2007)."
        },
        {
            "heading": "5. Conclusions",
            "text": "This paper introduced a new data acquisition method for the study of children\u2019s face-to-face conversational skills. It consists in using online video calls to record children with their caregivers at home. Compared to existing datasets of spontaneous, multimodal child-caregiver interactions in the wild (e.g., MacWhinney, 2014; Sullivan et al., 2022), our method allows much clearer access to the\n9 Note that this measure approximates BC opportunities only in the context of\nthe speaker\u2019s pauses, not opportunities for BCs that overlap with the speaker\u2019s\nspeech. Estimating the latter requires investigating finer-grained cues within\nspeech such as intonation and clause boundaries.\nFrontiers in Computer Science 09 frontiersin.org\nFIGURE 4 The rate of listener\u2019s BC production of children and caregivers across the speaker\u2019s speech and pauses (mutually exclusive). We also show the subset of cases where these cues overlap with the speaker\u2019s gaze at the listener. Each data point represents a participant and ranges represent 95% confidence intervals.\nFIGURE 5 The rate of listener\u2019s BC production in family and non-family adult dyads across the speaker\u2019s speech and pauses (mutually exclusive). We also show the subset of cases where these cues overlap with the speaker\u2019s gaze at the listener. Each data point represents a participant and ranges represent 95% confidence intervals.\nFIGURE 6 The rate of BC (received) opportunities. Opportunities were defined roughly as pauses of at least 400 ms between two successive sound segments (or IPUs) of the speaker.\ninterlocutor\u2019s facial expressions and head gestures, thus facilitating the study of conversational development. We collected an initial corpus using this data acquisition method which we hand-annotated for various multimodal communicative signals. The analysis of these annotations revealed that children in middle childhood use\nnon-verbal cues in conversation almost as frequently as adults do. In our corpus, interlocutors played a word-guessing game instead of chatting about random topics. This game was used here to elicit balanced conversational exchange in an interactive context where such a criterion is difficult to meet due to obvious social and developmental asymmetries between the interlocutors. That said, the word-guessing game remains highly flexible and useful for investigating a wide range of research questions.\nWe illustrated the usefulness of such data by focusing on a specific conversational skill: Backchannel signaling, which\u2014despite its importance for coordinated communication (Clark, 1996)\u2014has received little attention in the developmental literature. The findings from this case study have confirmed our prediction that using a new data acquisition method, which improves the naturalness of the exchange (i.e., conversation with a caregiver, at home, and using a fun/easy game), allows us to capture more of children\u2019s conversational competencies. In particular, here we found, by contrast to previous in-lab studies, that school-age children\u2019s rate and context of BC production is strikingly close to adult-level mastery.\nBackchannel signaling is only an illustration; many other aspects of conversational development can be studied with these data. In particular, ongoing research aims at characterizing children\u2019s\nFrontiers in Computer Science 10 frontiersin.org\nmultimodal alignment, a phenomenon whereby interlocutors tend to repeat each other\u2019s verbal and non-verbal behavior. Alignment is believed to be associated with the collaborative process of building mutual understanding and, thus, communicative success more generally (Brennan and Clark, 1996; Pickering and Garrod, 2006; Rasenberg et al., 2020). For example, Mazzocconi et al. (2023) used our corpus to compare laughter alignment/mimicry in child-caregiver and caregiver-adult interactions. They found that, although laughter occurrences were comparable between children and adults (as we report here in Figure 2), laughter mimicry was overall significantly less frequent in child-caregiver interactions in comparison to caregiver-adult interactions, the latter being similar to what was observed in previous studies of adult face-to-face interactions (e.g., Mazzocconi et al., 2020).\nFinally, a major advantage of the zoom-based data acquisition method is its cost-effectiveness, allowing large-scale data collection including from different cultures. In the current paper, however, our goal was not only to collect data but also to provide manual annotation for various communicative signals, a labor-intensive task. Thus, we settled on a manageable sample size. That said, progress in the automatic annotation of children\u2019s multimodal data (Sagae et al., 2007; Nikolaus et al., 2021; Rooksby et al., 2021; Erel et al., 2022; Long et al., 2022) should alleviate the constraint on large-scale data collection in future research. The current work also contributes to this effort by providing substantial hand-annotated data that can be used for the automatic models\u2019 training and/or validation."
        },
        {
            "heading": "5.1. Limitations",
            "text": "We used video calls as a way to collect face-to-face conversational data in a naturalistic context (e.g., at home instead of the lab). However, this method involves introducing a medium (i.e., a screen) that has obvious constraints and the participants may be adapting to\u2014or influenced by\u2014these constraints. For example, in the case of BC, we noticed some differences with previous work that has studied adult BC in direct face-to-face conversations in the same culture/country (i.e., France) (Pr\u00e9vot et al., 2017; Boudin et al., 2021). In particular, our rate of BC production in adults was overall lower. Besides, our number of verbal BC compared to non-verbal BC was also lower (see Table 3).10 Nevertheless, our goal is generally to compare children and adults; the constraints due to introducing a medium of communication apply equally to both children and adults, thus the comparison remains valid, albeit in this specific, mediated conversational context.\nBC behavior can also be influenced by internet issues such as time lags (Boland et al., 2021), possibly disturbing the appropriate timing and anticipation of conversational moves. That said, our preliminary testing (and, then, the full annotation of the data) has shown that, if there were lags, they must have been minimal compared to the time scale of BC dynamics. The production of BC did not seem to be disrupted nor disruptive (to the interlocutor). However, further research is required to precisely quantify the potential effect that online video call systems might have on BC and conversational coordination more generally.\n10 That said, there are other factors that may have caused these di erences,\nnamely, our use of a new elicitation task, i.e., the word-guessing game."
        },
        {
            "heading": "Author\u2019s note",
            "text": "The manuscript is based on one work presented in the \u201c2021 International Workshop on Corpora And Tools for Social skills Annotation\u201d and a second work presented (as a non-archival proceeding) in the \u201c2022 Annual Meeting of the Cognitive Science Society.\u201d"
        },
        {
            "heading": "Data availability statement",
            "text": "The original contributions presented in the study are included in the article/supplementary material, further inquiries can be directed to the corresponding author."
        },
        {
            "heading": "Ethics statement",
            "text": "The studies involving human participants were reviewed and approved by Aix-Marseille University. Written informed consent to participate in this study was provided by the participants\u2019 legal guardian/next of kin."
        },
        {
            "heading": "Author contributions",
            "text": "KB, MN, LP, and AF designed research and wrote the paper. KB performed research. KB and AF analyzed data. All authors contributed to the article and approved the submitted version."
        },
        {
            "heading": "Funding",
            "text": "This research was supported by grants ANR-16-CONV-0002 (ILCB), ANR-11-LABX-0036 (BLRI), ANR-21-CE28-0005-01 (MACOMIC), AMX-19-IET-009 (Archimedes Institute), ED356, and the Excellence Initiative of Aix-Marseille University (A*MIDEX)."
        },
        {
            "heading": "Acknowledgments",
            "text": "We would like to thank Fatima Kassim for help with the annotation. We also thank Auriane Boudin, Philippe Blache, No\u00ebl Nguyen, Roxane Bertrand, Christine Meunier, and St\u00e9phane Rauzy for useful discussion. Finally, we thank all families that have volunteered to participate in data collection."
        },
        {
            "heading": "Conflict of interest",
            "text": "The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest."
        },
        {
            "heading": "Publisher\u2019s note",
            "text": "All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher,\nFrontiers in Computer Science 11 frontiersin.org\nthe editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher."
        },
        {
            "heading": "First step: Connecting to Zoom",
            "text": "You\u2019ll need two computers to connect separately on Zoom (it is preferable for the quality of audio that they be in different rooms). Make sure to use headphones for improved audio quality (not required for your child). Follow the link sent by the experimenter in order to connect both computers to Zoom. If Zoom is not installed on your computer, you can install it by following the link. Once the installation is complete, you will be directed to a Zoom window. There you will see the experimenter who will be recording your interaction for the study.\nMake sure to expand the zoom window to full screen and to pin your video on your child\u2019s screen (and his/hers on your screen)."
        },
        {
            "heading": "Second step: The Word-Guessing Game",
            "text": "In this task, we ask you to play a \u201cGuess the Word\u201d game with your child for 10 minutes. The goal is to have an active conversation where one asks questions and the other answers them until the chosen word has been found. Try guessing as many as possible!\n- You can ask any question (not only yes/no questions). - You can give small hints if it gets too complicated.\n1. The caregiver explains the task to the child. 2. The parent starts with a word that they can choose from a list of\nwords that was provided to them prior to the recording (see the list below). 3. Once ready, indicate that you have your word so that the child could start asking questions about it. 4. The child asks his first question in order to find out the parent\u2019s first word. 5. The parent answers the questions while paying attention to not answering the questions that give away the word. 6. The pair continues interacting until the word is found. 7. Once the word is correctly guessed, it\u2019s the child\u2019s turn. 8. The child starts with a word of their choice and answers the\nquestions about the word until the parent finds it out. 9. After each correctly guessed word, it\u2019s the other participant\u2019s turn. 10. They should try to guess as many words as possible in 10 minutes. 11. At the end of 10 minutes, the parent initiates a spontaneous\nconversation with the child. They can discuss how they did in this game, what they think of it etc. (this part is also recorded as part of the data).\nList of words provided to the caregivers (only in the child-caregiver condition) \u2022 Brosse \u00e0 dents (toothbrush) \u2022 Dauphin (dolphin) \u2022 Fraise (strawberry) \u2022 Policier (police officer) \u2022 V\u00e9lo (bike) \u2022 Lune (moon) \u2022 Oreille (ear) \u2022 Anniversaire (birthday)\nFrontiers in Computer Science 14 frontiersin.org"
        }
    ],
    "title": "Using video calls to study children\u2019s conversational development: The case of backchannel signaling",
    "year": 2023
}