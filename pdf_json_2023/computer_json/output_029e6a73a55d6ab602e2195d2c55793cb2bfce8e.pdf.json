{
    "abstractText": "Large vision and language models, such as Contrastive Language-Image Pre-training (CLIP), are rapidly becoming the industry norm for matching images and texts. In order to improve its zero-shot recognition performance, current research either adds additional web-crawled imagetext pairs or designs new training losses. However, the additional costs associated with training from scratch and data collection substantially hinder their deployment. In this paper, we present HELIP, a low-cost strategy for boosting the performance of well-trained CLIP models by finetuning them with hard samples over original training data. Mixing hard examples into each batch, the well-trained CLIP model is then fine-tuned using the conventional contrastive alignment objective and a margin loss to distinguish between normal and hard negative data. HELIP is deployed in a plug-and-play fashion to existing models. On a comprehensive zero-shot and retrieval benchmark, without training the model from scratch or utilizing additional data, HELIP consistently boosts existing models to achieve leading performance. In particular, HELIP boosts ImageNet zero-shot accuracy of SLIP by 3.05 and 4.47 when pretrained on CC3M and CC12M respectively. In addition, a systematic evaluation of zero-shot and linear probing experiments across fine-grained classification datasets demonstrates a consistent performance improvement and validates the efficacy of HELIP . When pretraining on CC3M, HELIP boosts zeroshot performance of CLIP and SLIP by 8.4% and 18.6% on average respectively, and linear probe performance by 9.5% and 3.0% on average respectively. *Equally contributed to this work. \u2020Corresponding author.",
    "authors": [
        {
            "affiliations": [],
            "name": "Haonan Wang"
        },
        {
            "affiliations": [],
            "name": "Minbin Huang"
        },
        {
            "affiliations": [],
            "name": "Runhui Huang"
        },
        {
            "affiliations": [],
            "name": "Lanqing Hong"
        },
        {
            "affiliations": [],
            "name": "Hang Xu"
        },
        {
            "affiliations": [],
            "name": "Tianyang Hu"
        },
        {
            "affiliations": [],
            "name": "Xiaodan Liang"
        },
        {
            "affiliations": [],
            "name": "Zhenguo Li"
        }
    ],
    "id": "SP:6848b737be45a10a24e342bd08ccbf074cef0a5a",
    "references": [
        {
            "authors": [
                "Alberto Baldrati",
                "Marco Bertini",
                "Tiberio Uricchio",
                "Alberto Del Bimbo"
            ],
            "title": "Conditioned and composed image retrieval combining and partially fine-tuning clip-based features",
            "venue": "In Proc. of CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Tiffany Tianhui Cai",
                "Jonathan Frankle",
                "David J. Schwab",
                "Ari S. Morcos"
            ],
            "title": "Are all negatives created equal in contrastive instance discrimination",
            "venue": "ArXiv preprint,",
            "year": 2020
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In Proc. of ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Soravit Changpinyo",
                "Piyush Sharma",
                "Nan Ding",
                "Radu Soricut"
            ],
            "title": "Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts",
            "venue": "In Proc. of CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey E. Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In Proc. of ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Haoqi Fan",
                "Ross B. Girshick",
                "Kaiming He"
            ],
            "title": "Improved baselines with momentum contrastive learning",
            "venue": "ArXiv preprint,",
            "year": 2020
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Fei-Fei Li"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In Proc. of CVPR,",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "In Proc. of NAACL,",
            "year": 2019
        },
        {
            "authors": [
                "Andreas F\u00fcrst",
                "Elisabeth Rumetshofer",
                "Viet Tran",
                "Hubert Ramsauer",
                "Fei Tang",
                "Johannes Lehner",
                "David P. Kreil",
                "Michael Kopp",
                "G\u00fcnter Klambauer",
                "Angela Bitto-Nemling",
                "Sepp Hochreiter"
            ],
            "title": "CLOOB: modern hopfield networks with infoloob outperform CLIP",
            "venue": "ArXiv preprint,",
            "year": 2021
        },
        {
            "authors": [
                "Samir Yitzhak Gadre",
                "Gabriel Ilharco",
                "Alex Fang",
                "Jonathan Hayase",
                "Georgios Smyrnis",
                "Thao Nguyen",
                "Ryan Marten",
                "Mitchell Wortsman",
                "Dhruba Ghosh",
                "Jieyu Zhang"
            ],
            "title": "Datacomp: In search of the next generation of multimodal datasets",
            "venue": "ArXiv preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Shashank Goel",
                "Hritik Bansal",
                "Sumit Bhatia",
                "Ryan A Rossi",
                "Vishwa Vinay",
                "Aditya Grover"
            ],
            "title": "Cyclip: Cyclic contrastive language-image pretraining",
            "venue": "ArXiv preprint,",
            "year": 2022
        },
        {
            "authors": [
                "Junxian He",
                "Chunting Zhou",
                "Xuezhe Ma",
                "Taylor Berg- Kirkpatrick",
                "Graham Neubig"
            ],
            "title": "Towards a unified view of parameter-efficient transfer learning",
            "venue": "In Proc. of ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proc. of CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Tri Huynh",
                "Simon Kornblith",
                "Matthew R. Walter",
                "Michael Maire",
                "Maryam Khademi"
            ],
            "title": "Boosting contrastive selfsupervised learning with false negative cancellation",
            "venue": "In IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Chao Jia",
                "Yinfei Yang",
                "Ye Xia",
                "Yi-Ting Chen",
                "Zarana Parekh",
                "Hieu Pham",
                "Quoc V. Le",
                "Yun-Hsuan Sung",
                "Zhen Li",
                "Tom Duerig"
            ],
            "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "venue": "In Proc. of ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Yannis Kalantidis",
                "Mert B\u00fclent Sariyildiz",
                "No\u00e9 Pion",
                "Philippe Weinzaepfel",
                "Diane Larlus"
            ],
            "title": "Hard negative mixing for contrastive learning",
            "venue": "In Proc. of NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven C.H. Hoi"
            ],
            "title": "BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation",
            "venue": "In Proc. of ICML,",
            "year": 2022
        },
        {
            "authors": [
                "Junnan Li",
                "Ramprasaath R. Selvaraju",
                "Akhilesh Gotmare",
                "Shafiq R. Joty",
                "Caiming Xiong",
                "Steven Chu-Hong Hoi"
            ],
            "title": "Align before fuse: Vision and language representation learning with momentum distillation",
            "venue": "In Proc. of NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Yangguang Li",
                "Feng Liang",
                "Lichen Zhao",
                "Yufeng Cui",
                "Wanli Ouyang",
                "Jing Shao",
                "Fengwei Yu",
                "Junjie Yan"
            ],
            "title": "Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm",
            "venue": "In Proc. of ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge J. Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C. Lawrence Zitnick"
            ],
            "title": "Microsoft COCO: common objects in context",
            "venue": "In Proc. of ECCV,",
            "year": 2014
        },
        {
            "authors": [
                "Norman Mu",
                "Alexander Kirillov",
                "David A. Wagner",
                "Saining Xie"
            ],
            "title": "SLIP: self-supervision meets language-image pre-training",
            "venue": "In Proc. of ECCV, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Antonio Norelli",
                "Marco Fumero",
                "Valentino Maiorca",
                "Luca Moschella",
                "Emanuele Rodol\u00e0",
                "Francesco Locatello. Asif"
            ],
            "title": "Coupled data turns unimodal models to multimodal without training",
            "venue": "ArXiv preprint,",
            "year": 2022
        },
        {
            "authors": [
                "Fabian Pedregosa",
                "Ga\u00ebl Varoquaux",
                "Alexandre Gramfort",
                "Vincent Michel",
                "Bertrand Thirion",
                "Olivier Grisel",
                "Mathieu Blondel",
                "Peter Prettenhofer",
                "Ron Weiss",
                "Vincent Dubourg"
            ],
            "title": "Scikit-learn: Machine learning in python",
            "venue": "Journal of machine Learning research,",
            "year": 2011
        },
        {
            "authors": [
                "Bryan A. Plummer",
                "Liwei Wang",
                "Chris M. Cervantes",
                "Juan C. Caicedo",
                "Julia Hockenmaier",
                "Svetlana Lazebnik"
            ],
            "title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models",
            "venue": "In Proc. of ICCV,",
            "year": 2015
        },
        {
            "authors": [
                "Filip Radenovic",
                "Abhimanyu Dubey",
                "Abhishek Kadian",
                "Todor Mihaylov",
                "Simon Vandenhende",
                "Yash Patel",
                "Yi Wen",
                "Vignesh Ramanathan",
                "Dhruv Mahajan"
            ],
            "title": "Filtering, distillation, and hard negatives for vision-language pre-training",
            "venue": "CoRR, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In Proc. of ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych"
            ],
            "title": "Sentence-BERT: Sentence embeddings using Siamese BERT-networks",
            "venue": "In Proc. of EMNLP,",
            "year": 2019
        },
        {
            "authors": [
                "Joshua David Robinson",
                "Ching-Yao Chuang",
                "Suvrit Sra",
                "Stefanie Jegelka"
            ],
            "title": "Contrastive learning with hard negative samples",
            "venue": "In Proc. of ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Anshul Shah",
                "Suvrit Sra",
                "Rama Chellappa",
                "Anoop Cherian"
            ],
            "title": "Max-margin contrastive learning",
            "venue": "In Proc. of AAAI,",
            "year": 2022
        },
        {
            "authors": [
                "Piyush Sharma",
                "Nan Ding",
                "Sebastian Goodman",
                "Radu Soricut"
            ],
            "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
            "venue": "In Proc. of ACL,",
            "year": 2018
        },
        {
            "authors": [
                "Bart Thomee",
                "David A Shamma",
                "Gerald Friedland",
                "Benjamin Elizalde",
                "Karl Ni",
                "Douglas Poland",
                "Damian Borth",
                "Li-Jia Li"
            ],
            "title": "Yfcc100m: The new data in multimedia research",
            "venue": "Communications of the ACM,",
            "year": 2016
        },
        {
            "authors": [
                "Tongzhou Wang",
                "Phillip Isola"
            ],
            "title": "Understanding contrastive representation learning through alignment and uniformity on the hypersphere",
            "venue": "In Proc. of ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Bichen Wu",
                "Ruizhe Cheng",
                "Peizhao Zhang",
                "Tianren Gao",
                "Joseph E. Gonzalez",
                "Peter Vajda"
            ],
            "title": "Data efficient language-supervised zero-shot recognition with optimal transport distillation",
            "venue": "In Proc. of ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Lewei Yao",
                "Runhui Huang",
                "Lu Hou",
                "Guansong Lu",
                "Minzhe Niu",
                "Hang Xu",
                "Xiaodan Liang",
                "Zhenguo Li",
                "Xin Jiang",
                "Chunjing Xu"
            ],
            "title": "FILIP: fine-grained interactive languageimage pre-training",
            "venue": "In Proc. of ICLR,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Large vision and language models, such as Contrastive Language-Image Pre-training (CLIP), are rapidly becoming the industry norm for matching images and texts. In order to improve its zero-shot recognition performance, current research either adds additional web-crawled imagetext pairs or designs new training losses. However, the additional costs associated with training from scratch and data collection substantially hinder their deployment. In this paper, we present HELIP, a low-cost strategy for boosting the performance of well-trained CLIP models by finetuning them with hard samples over original training data. Mixing hard examples into each batch, the well-trained CLIP model is then fine-tuned using the conventional contrastive alignment objective and a margin loss to distinguish between normal and hard negative data. HELIP is deployed in a plug-and-play fashion to existing models. On a comprehensive zero-shot and retrieval benchmark, without training the model from scratch or utilizing additional data, HELIP consistently boosts existing models to achieve leading performance. In particular, HELIP boosts ImageNet zero-shot accuracy of SLIP by 3.05 and 4.47 when pretrained on CC3M and CC12M respectively. In addition, a systematic evaluation of zero-shot and linear probing experiments across fine-grained classification datasets demonstrates a consistent performance improvement and validates the efficacy of HELIP . When pretraining on CC3M, HELIP boosts zeroshot performance of CLIP and SLIP by 8.4% and 18.6% on average respectively, and linear probe performance by 9.5% and 3.0% on average respectively.\n*Equally contributed to this work. \u2020Corresponding author."
        },
        {
            "heading": "1. Introduction",
            "text": "Contrastive Language-Image Pretraining (CLIP) [27] is quickly becoming the standard for foundation models in computer vision due to its effectiveness for a variety of vision-language tasks without task-specific finetuning (i.e. zero-shot), such as image classification [19] and text and image retrieval [1]. Nevertheless, web-crawled picture-text pairs are often loosely connected, leading to multiple plausible matches, beyond the assigned ones [34]. Hence, a number of strategies [18, 19, 22, 26] have been presented to investigate appropriate matches and take advantage of the widespread supervision among the image-text pairs to improve language-image pretraining.\nVarious methods have been explored aiming to improve the vanilla contrastive language-image pretraining, which can be divided into two main categories: 1) utilizing a multitask objective to increase the effectiveness of single-modality monitoring [18, 22], and 2) employing intra/inter-modality similarity to mine hard samples and retrain with them [19, 26]. To be more specific, the multitasking methods consist of self-supervision within a single modality [18, 22], multi-view supervision across modalities [20], and the addition of extra regularization terms [11]. However, these methods require retraining the entire CLIP model, which is both inefficient and requires the tuning of newly added modules and hyperparameters. On the other hand, a number of works [19, 26] have employed intra/intermodality similarity to mine hard negative samples and upsample their importance during training. Unfortunately, due to batch size constraints, these in-batch hard negative mining methods struggle to find samples that are challenging enough to improve the contrastive learning [2]. This issue is exacerbated by the fact that loose assignment of image-text captioning pairs may result in inaccurate hard pairs. Given these issues, a natural question arises: Is it possible to enhance the performance of pre-trained CLIP models in a\n1\nar X\niv :2\n30 5.\n05 20\n8v 1\n[ cs\n.C V\n] 9\nM ay\nmore efficient and general manner without the need for additional training data?\nWe deliver a positive response to this inquiry and propose the training framework HELIP as a means to enhance CLIP models using hard samples selected from the original training sets in an efficient manner. Rather than selecting hard samples based on the intra/inter-modality similarity computed within a batch, HELIP uses information from both domains to select hard pairs as a preprocessing step to ensure that selected samples are challenging enough to serve as qualified hard negatives. In particular, the Hard Pair Mining (HPM) of HELIP firstly projects a target pair into the remaining dataset and then selects a subset that best represents the target as its hard sample set. Specifically, as seen in Figure 1, the similarity between the image (text) of the target tuple and the image (text) of the remaining tuples is calculated using a unimodal pretrained model, such as Resnet50 [13], BERT [8]. Then, HELIP selects a subset of data pairs that maximize the inner product of the textual and visual similarity vectors of the target pair. The subset serves as the hard negative samples of the target pair. Additionally, rather than finetuning CLIP models solely with the original text-image contrastive loss [27], which uniformly pushes all negative samples away from the positive one, HELIP incorporates the Hard Negative Margin Loss (HNML). As illustrated in Figure 2, the intrinsic similarity between samples should be reflected in the learned representations. Therefore, during finetuning, HELIP imposes additional geometric structure on the learned representation space by involving HNML as a regularization, which encourages the hard negatives to be closer to the positive sample than the normal negatives.\nEmpirically, by finetuning the well-trained CLIP models (i.e. CLIP, SLIP, and DECLIP) with the hard example and the hard negative margin loss, HELIP consistently boosts the CLIP checkpoints on the zero-shot classification, text-image retrieval and fine-grained linear probe benchmarks. Specifically, we finetune models well-trained on CC3M, CC12M and YFCC15M over their original training set. For zero-shot classification on Imagenet, CIFAR-10 and CIFAR-100, we observe that HELIP consistently boosts the performance of 6 pretrained models. Particularly, using HELIP to finetune SLIP models pretrained on CC3M, CC12M, and YFCC15M results in ImageNet zero-shot accuracy gains of 3.05, 4.47, and 10.14, respectively. Further, after finetuning with hard samples and hard negative margin loss, the pretrained models achieve better zero-shot and linear probe performance on 7 fine-grain image classification datasets. Specifically, the average zero-shot accuracy of CC3M pretrained CLIP and SLIP are improved from 14.45 to 15.67 (+8.4%) and from 16.96 to 20.12 (+18.6%). The average linear probe accuracy of CC3M pretrained CLIP and SLIP are improved from 53.29 to 58.34 (+9.5%) and\nfrom 64.89 to 66.81 (+3.0%). Additionally, the performance gain is also valid in terms of zero-shot retrieval, with 1.1 of R@1 on Flickr30K, and 2.2 of R@1 on COCO for SLIP-HELIP .\nOur contributions could be summarized as:\n\u2022 To our best knowledge, HELIP serves as the first plug-and-play method to further boost the well-trained CLIP models over their pretrained datasets. This is achieved without the need for additional training data or retraining the model from scratch.\n\u2022 We propose a novel hard sample selection technique for the identification of hard negative samples. Additionally, for the first time, we introduce the hard negative margin loss, an approach that considers representation distances, ensuring the successful incorporation of hard samples during finetuning.\n\u2022 Empirically, over zero-shot classification, image-text retrieval, and linear probe benchmarks, we demonstrate that HELIP is able to consistently improve CLIP model checkpoints by finetuning."
        },
        {
            "heading": "2. Related Work",
            "text": "Vision-Language Pre-training. Vision Language Pretraining (VLP) is a technique that leverages large-scale image-text datasets to learn a strong joint representation between the two modalities that can be transferred to various downstream vision-language tasks. VLP models can be generally divided into single-stream models and dualstream models. Dual-stream models [15, 20, 22, 27, 35] typically consist of two separate encoders for image and text respectively and perform cross-modality interactions on the top, are becoming more and more popular because of its flexibility of transferring pre-trained knowledge to downstream tasks. CLIP [27], uses a simple contrastive objective to learn visual features from natural language supervision and achieves remarkable zero-shot recognition performance using 400M web-crawled image-text pairs. Recent works boot the performance of CLIP by applying self-supervision within visual modal [22], additional nearest neighbor supervision [20]. These methods are actually doing data augmentations to increase data efficiency and thus bring additional computational costs.\nContrastive learning with hard negative sample. Contrastive learning learns a representation of input data that maps semantically comparable examples close together and semantically dissimilar examples far apart [5, 6, 33]. Recent works include hard negative samples into the loss function [2, 14, 16, 19, 26, 29, 30]. Empirically, it has been shown that involving hard negatives can help the model to better distinguish between similar and dissimilar examples, leading to more effective and meaningful learned\nrepresentations. Kalantidis et al. [16] presented MoCHi for Contrastive Learning with hard negative examples, inspired by Mixup [36]. MoCHi selects the most similar negative sample for each positive embedding and mixes their embeddings in feature space. Instead of augmenting the training data, another approach targeted the sampling strategy, by assigning more importance to negative examples that are similar to the query [29]. Huynh et al. [14] propose a new sampling technique that picks negative instances similar to the anchor and positive examples to filter false negatives. Motivated by SVMs, Shah et al. [30] propose the max-margin contrastive loss, which uses support vectors as hard negatives and learns representations that maximize the SVM judgment margin. Cai et al. [2] conducted an empirical investigation on the relevance of negative samples in MoCo [6] and showed that only the hardest 5% of negatives are necessary for good accuracy and that same-class negatives can harm representation learning. Language-image contrastive learning approaches mine multimodal hard negative examples using intra/inter-modality similarity [19, 26]. Li et al. [19] choose in-batch hard negative samples with image-text contrastive loss. One negative text from a mini-batch is sampled per image. Hard negative noise contrastive multimodal alignment loss by Radenovic et al. [26] up-weights the loss term for in-batch hard samples. Contrary to prior works, we design a hard sample mining method to discover similar pairs defined over the two modalities. Additionally, different from those local manners, our method is able to select samples challenging enough to improve learning."
        },
        {
            "heading": "3. Hard Sample for Visual-Language Models",
            "text": "In this section, we first define the notations and revisit CLIP for zero-shot recognition in the Preliminary Section 3.1. Then, we present the hard sample mining method (HPM), and introduce the corresponding hard negative margin loss (HNML) in Section 3.2 and 3.3 respectively."
        },
        {
            "heading": "3.1. Preliminaries",
            "text": "We consider the task of contrastive image-text pretraining. Given an image-caption dataset D = {zi}Ni=1 = {(xIi , xTi )}Ni=1 where xIi and xTi denote the image and its corresponding caption, our goal is to learn a dual encoder model \u03c6 = {\u03c6image, \u03c6text}, where \u03c6image represents the image encoder and \u03c6text denotes the text encoder. We use the shorthand Ii = \u03c6image(xIi ) and Ti = \u03c6text(x T i ) to denote the encoded representation of an image and its caption, respectively. The contrastive objective of CLIP is formulated as,\n`CLIP = \u2212 1 |B| \u2211 i\u2208B log exp (sim(Ii, Ti)/\u03c3)\u2211 j\u2208B exp (sim(Ii, Tj)/\u03c3) , (1)\nwhere sim(\u00b7, \u00b7) is the cosine similarity function, B is a batch of samples and \u03c3 is a trainable parameter controlling the temperature. Intuitively, the above formulation explicitly maximizes the distance between negative pairs of batch samples and aligns the image and text representations of the same pair."
        },
        {
            "heading": "3.2. Hard Pair Mining",
            "text": "For previous intra/inter-modality hard sample mining methods, two samples [zi, zj ] are considered as hard samples, if the cosine similarity sim(Ii, Ij) or sim(Ii, Tj) is high [19, 26]. However, due to the nature of loose assignment for web-crawled image-caption data, a high similarity indicated by intra/inter-modality doesn\u2019t always correspond to hard negatives that are difficult to tell apart. For example, a value of sim(Ii, Ij) doesn\u2019t always lead to a strong value for sim(Ii, Tj), which will be minimized in CLIP loss (Equation 1). Similarly, two pairs with high sim(Ii, Tj) value is possible to have low value on sim(Ti, Ij). The improvement of learned representations is limited [2], when a model is training with negative contrastive pairs not challenging enough.\nInspired by recent work [23] that represents image-text pairs in a common space created by single-domain encoders, we propose the Hard Pair Mining (HPM) method for contrastive learning from a data-centric perspective. The HPM method first maps each image-text pair to an interpretable space, where each dimension corresponds to the similarity of the input to a unique entry in the multimodal dataset. Then, the HPM method defines hard negative pairs as a set of pairs that, in the subspace spanned by those pairs, maximize the alignment between the visual and textual information of a target pair. By selecting hard negative pairs with the structure information of the multimodal dataset, the HPM method aims to improve the discriminative power of the learned representations in language-image contrastive learning. As shown in Figure 1, with the help\nof two pretrained unimodal models fimage and ftext, such as ResNet50 and BERT (trained with or without supervision), we encode the target pair zi to (Ii, Ti). Then, for visual and textual modality, we compute two intra-modal similarity vectors between the target pair zi and the remaining dataset Di, where Di = D \\ zi. The visual modal similarity ~SI(xIi ,Di) = [. . . , sim(Ii, Ij), . . . ]> \u2208 RN\u22121, where Ij = fimage(x I j ), x I j \u2208 (xIj , xTj ) \u2286 Di. Additionally, the textual modal similarity vector ~ST (xTi ,Di) \u2208 RN\u22121 can be computed in the same procedure. Then, we convert the hard sample mining problem to a combinational optimization problem. Specifically, the goal is to find a subset of image-text pairs, H \u2286 Di, that can greatly represent the target pair (xIi , x T i ):\nH?i = Argmax|H|=k ~SI(xIi ,H)>~ST (xTi ,H), (2)\nwhere the k \u2208 R+ is the number of selected negative samples and H?i is the hard sample set. To efficiently solve the above problem, we further simplify the problem by considering the definition of similarity vectors:\nH?i = ArgmaxH\u2286Di,|H|=k |H|\u2211 j=0 sim(Ii, Ij) \u00b7 sim(Ti, Tj)\n= Top ({ sim(Ii, Ij) \u00b7 sim(Ti, Tj) }|Di| j=1 , k ) .\n(3) Therefore, through selecting sample with top-k highest value of sim(Ii, Ij)\u00b7sim(Ti, Tj), the problem can be solved in O(N).\nIntuitively, for a dataset collecting from one certain source, the number of \u201cqualified\u201d hard negative samples (challenging enough to improve the learned representation) for one pair will be doubled, if the size of selected dataset is doubled. Thus, we explicitly assume that, for a certain target pair, the existence probability of its qualified hard samples only depends on the data source and is independent with the size of dataset. Therefore, a direct consequence is that, to select k \u201cqualified\u201d hard negative samples, only a constant number of pairs are needed from the original dataset. We introduce the approximated HPM, which avoids the time complexity of hard sampling mining over whole dataset goes to O(N2). Specifically, we change the objective into the following:\nH\u2032i = Top ({ sim(Ii, Ij) \u00b7 sim(Ti, Tj) }|D\u0303i| j=1 , k )\n(4)\nwhere Ij = fimage(xIj ), Tj = ftext(x T j ), and (x I j , x T j ) \u2208 D\u0303i is from a subset of Di, D\u0303i \u2286 Di. We denote the constant size of D\u0303i as c. Note, the selection of c depends only on the number of hard sample k, instead of the size of original training set. Therefore, the time complexity of approximated HPM is O(N). We summarize the hard pair mining algorithm in Appendix A.2."
        },
        {
            "heading": "3.3. Finetuning with hard negative margin loss",
            "text": "The image-text contrastive loss `CLIP , as illustrated in Section 3.1, aims at maximizing the distance between the positive pair and the corresponding negative one. While such an objective aligns the true image-text pairs, it poses no constraints on the overall geometry among data pairs [11]. After involving hard samples into the finetuning stage, equally maximizing the distance for normal negative pairs and hard negative pairs is an undesired way to utilize the information provided by hard negative samples. The intuition follows directly from Figure 2. In a desired representation space, the similarity between the positive sample and the hard negative sample, S1, should be greater than the similarity between the positive sample and normal negative samples, S2, S3. Therefore, to impose the additional geometric structure, we introduce the Hard Negative Margin Loss (HNML):\n`margin = 1 |B| \u2211 j\u2208B max ( 0, sim(Ii, Tj)\n\u2212 min j\u2032\u2208Hpi\n{sim(Ii, Tj\u2032)} ) ,\n(5)\nwhere Hpi is the hard negative samples for sample zi, |Hpi | = p, p \u2208 R+. Note, the HNML is memory efficient. No extra inner product computation is required. The geometric regularization is applied over the inner product matrix computed in the original CLIP loss (Equation (1)).\nThen, the well-trained model is finetuned with the following loss, where \u03b3 is the hyperparameter balancing the two losses,\n`finetune = `CLIP + \u03b3`margin. (6)\nIn all, to boost the performance of well-trained CLIP models without introducing extra data and extra parameters, we introduce the finetuning strategy which involve the hard samples prepared in the preprocessing stage into the batch composition of finetuning stage. As shown in Figure 3, for text-image pairs within the batch B, we randomly sample a subset B\u2032 as seeds. Then, for zi \u2208 B\u2032, we random select p samples from Hi and denote the subset as Hpi . The actual training batch is B = B\u222a|B \u2032| i=0 H p i . For the training pipeline, we summarize it the algorithm in Appendix A.2."
        },
        {
            "heading": "4. Experiments",
            "text": "Here we evaluate our approach across a broad range of vision and vision-language tasks."
        },
        {
            "heading": "4.1. Experimental setup",
            "text": "Training datasets. We mainly focus on finetuning welltrained models on Conceptual Captions 3M (CC3M) [31] and Conceptual Captions 12M (CC12M) [4], and a 15M subset of YFCC100M [32] collected by DECLIP [20] CC3M and CC12M are relatively small but it has been used for benchmark evaluations in many works [11, 20, 22] on language-image pretraining. with a different filtering strategy from Radford et al. [27]. It contains some additional data crawled from the Internet in addition to YFCC and is of higher quality than the subset in [27]. We refer to it as YFCC15M.\nDownstream datasets. We mainly verify the effectiveness of our methods through zero-shot image classification, linear probing of the visual representation learned by the visual encoder, and zero-shot image-text retrieval. For zero-shot classification, in addition to commonly used ImageNet [7], CIFAR10, and CIFAR100 [17], we also verify the performance on 7 fine-grained classification datasets in-\ncluding Caltech101, Food101, Sun397, Flowers102, CUB, Stanford Cars and FGVC Aircraft. For the zero-shot imagetext retrieval task, MS-COCO [21] and Flickr30K [25] are adopted.\nImplementation Details. We conduct experiments on three different architectures, ResNet-50, ViT-B/32, and ViTB/16, according to different datasets and pretrained models. Their details directly follow that of CLIP [27]. When pretraining on CC3M and CC12M using CLIP model, we adopt ResNet-50 as the backbone. We use ViT-B/16 for SLIP model on CC3M and CC12M to match the setting in [22]. While for pretraining on YFCC, we adopt ViT-B/32 in all experiments to fairly compare the results in [20]. The input resolution of the image encoder is 224 \u00d7 224 and the maximum context length of the text encoder is 77. All of our experiments are conducted on 8 V100 GPUs with a batch size of 128 for ViT-B/16 models, and a batch size of 512 for ResNet-50 models and ViT-B/32 models. The dimension of the image and text embeddings is 1024 for ResNet-50 models and 512 for ViT-B/16 and ViT-B/32 models. We set \u03b3 = 1 and p = 1 for all the expriments by default. To save GPU memory, automatic mixed-precision is used. To avoid the model from overfitting to potential harmful distribution induced by the hard sample batch, we use early stopping if there\u2019s no performance gain within 5 consecutive epochs.\nIn the preparation of hard samples, we employ the unsupervisedly pretrained vision transformer, DINO VITs8, as the image encoder [3]. As for the text encoder, we utilize the SentenceT [28] pretrained transformer, which is trained on a dataset comprising over 1 billion sentences gathered from the internet. The embedding sizes are 384 for DINO VITs8 and 768 for SentenceT. For the CC3M dataset, hard pair mining can be accomplished in 1 hour and 9 minutes using a single V100 GPU. In the case of the CC12M dataset, the complete hard pair mining process takes 9 hours and 11 minutes on 8 V100 GPUs. For the 6M and 3M approximated versions, hard pair mining requires 5 hours 3 minutes and 2 hours 18 minutes, respectively. Regarding the FYCC dataset, the full version can be processed in 17 hours and 41 minutes on 8 V100 GPUs, while the 6M and 3M approximated hard pair mining tasks take 6 hours 19 minutes and 3 hours 27 minutes, respectively."
        },
        {
            "heading": "4.2. Main results and discussion",
            "text": ""
        },
        {
            "heading": "4.2.1 Zero-shot classification",
            "text": "We compare zero-shot performances of the CLIP, SLIP, DECLIP, and those models finetuned by HELIP on CC3M, CC12M and YFCC15M. We denote the models finetuned by HELIP as CLIP-HELIP , SLIP-HELIP , and DECLIPHELIP respectively. As shown in Table 1, models further finetuned by HELIP consistently get significant improvements on the three datasets compared with their counterparts. Specifically, on CC3M, with the help of HELIP\n, zero-shot classification accuracy on ImageNet of CLIP model was improved from 19.04% to 19.86%. While SLIP model has a performance boost of over 13%, compared with its original number, achieving 26.05% accuracy on ImageNet. We additionally include two baseline methods: CYCLIP [11] and CLOOB [9] for reference. As for pretraining on CC12M, we directly adopted the checkpoints released by SLIP [22]. SLIP-HELIP outperforms its counterpart by 4.47% in zero-shot accuracy on ImageNet. Because the pretrained DECLIP doesn\u2019t have open-sourced parameters for CC3M and CC12M, we only compare DECLIP and the DECLIP-HELIP on YFCC15M. On YFCC15M, we report the performance of SLIP and DECLIP achieved with the evaluation pipeline implemented in the work [20] and denote them as SLIP\u2217 and DECLIP\u2217. We notice that both SLIP and DECLIP are boosted by HELIP. On average, SLIP and DECLIP are improved by 15.49% and 6.74% correspondingly.\nIntuitively, by involving image-text pairs that are not a match, but are difficult to differentiate from true positive pairs in contrastive learning, HELIP has the potential to improve the discriminative power of visual embedding generated by CLIP model. The discriminative ability is beneficial to classification tasks, especially for the fine-grained classification dataset. Therefore, we also evaluate zero-shot classification on 7 fine-grained classification datasets in Table 2. As can be seen from the table SLIP-HELIP boosts the zero-shot accuracy of SLIP on Caltech101 by 12.88% and 3.95% when pretraining on CC3M and CC12M respectively. Both CLIP and SLIP models witness consistent im-\nprovements with their HELIP counterparts."
        },
        {
            "heading": "4.2.2 Linear probing",
            "text": "The linear probing task trains a randomly initialized linear classifier on the feature extracted from the frozen image encoder on the downstream dataset. To accomplish this, we train the logistic regression classifier using scikit-learn\u2019s LBFGS implementation [24], with maximum 1,000 iterations on those 7 datasets. For each dataset, we search for the best regularization strength factor on the validation set over 45 logarithmically spaced steps within the range 1e-6 to 1e+5.\nExperimental results in Table 3 demonstrate that both CLIP-HELIP and SLIP-HELIP have consistent improvements over their counterparts on almost all 7 datasets. Note that on CC12M SLIP-HELIP performs marginally better on 5 out of 7 datasets. It\u2019s probably because the selfsupervision of SLIP [22] within the visual modal can be beneficial for learning fine-grained visual embedding, while SLIP-HELIP doesn\u2019t include image self-supervision during the training. In addition, we did not match the training batch size as SLIP [22] because of resource limitations. A combination of HELIP and image self-supervision and larger training batch size may be a potential direction for achieving better linear probe performance."
        },
        {
            "heading": "4.2.3 Zero-shot retrieval",
            "text": "We evaluate HELIP on zero-shot image-to-text retrieval tasks on MS-COCO [21] and Flickr30K [25]. We compare CLIP, SLIP, and their counterparts trained on CC3M and CC12M respectively in Table 4. As shown in the table, both CLIP and SLIP benefit from HELIP ."
        },
        {
            "heading": "4.3. Comparison with different hard sample mining",
            "text": "methods\nWe evaluate the efficacy of the proposed method in enhancing the discriminative capacity of learned representations by comparing its zero-shot classification performance with that of other hard sample mining strategies. As described in the Section 2, a common way to define hard pairs is through intra-modality similarity. Hence, we introduce the hard sample mining methods depending on image similarity and text similarity and denote them as IM and TM correspondingly. For a given target pair, we compute the cosine similarity between its image/text representation and those of the remaining dataset. The image and text representations are encoded using a pretrained Resnet50 and BERT, respectively. As a preprocessing step, both IM and TM methods mine hard negatives. Subsequently, we integrate the mined hard samples into the training pipeline of the CLIP+IM and CLIP+TM methods and optimize the original contrastive loss to finetune the model. Additionally, we employ the hard negative contrastive loss, HN-NCE, proposed by Radenovic et al. [26], as a baseline. HN-NCE up-samples the\nweight of hard-negatives identified by the current model. As shown in Table 5, when the CC3M pretrained CLIP model is combined with HELIP, its performance significantly outperforms other techniques.\nWe visualize the mined hard samples obtained from three\ndifferent preprocessing methods, namely, hard pair mining (HPM), image similarity mining (IM), and text similarity mining (TM), in Figure 4. The image-text pairs selected by HPM are displayed in the first row, while the second and third rows show the pairs selected by IM and TM, respectively. We observe that the captions of the hard pairs mined with image similarity are only loosely connected with the image of the target pair. For samples mined by TM, their images are even mismatched with the caption of the target pair. The fact that pairs mined by TM is easier than IM is also reflected in the Table 5, where the zero-shot per-\nformance of the CLIP+IM method consistently outperforms the CLIP+TM method across three datasets."
        },
        {
            "heading": "4.4. Effect of hard negative margin loss",
            "text": "We investigate the impact of using hard negative margin loss (HNML) on the performance of the SLIP model. Specifically, we evaluate the SLIP model, which is pretrained on the CC3M dataset, with and without finetuning using HNML. Our evaluation involves a comparison of the zero-shot classification performance of the model on ImageNet, CIFAR 100, and CIFAR 10 datasets. Our findings, which are presented in Table 6, indicate that the SLIP model performs better with HNML finetuning on ImageNet and CIFAR 100. However, without HNML, the model achieves better performance on CIFAR 10. This observation is possibly due to the HNML introducing additional discriminative power to the learned representations by utilizing the distance between classes as the cost. Consequently, for classification datasets with a larger number of sub-classes, such as ImageNet and CIFAR 100, the use of HNML during training can result in improved classification performance."
        },
        {
            "heading": "4.5. Effect of hard pair mining approximation",
            "text": "To accelerate the hard pairs mining step of HELIP, the approximated HPM is introduced. We compared the zeroshot performance of SLIP finetuned with hard samples mined by different methods and summarized the result in Table 7. For the all three settings, we keep the hyperparameter k = 500 as the previous. Besides, we set subset D\u0303i with size 3M and 6M. Correspondingly, we denote the approximated HELIP with 3M and 6M subset as HELIP-3M and HELIP-6M. As shown in Table 7, the zero-shot performance of HELIP-3M and HELIP-6M are competitive with the HPM mining hard sample globally. The result indicates that the approximated HPM is an efficient choice to mine hard samples and effective enough for contrastive learning. Besides, it indicates that approximated HPM has the potential to scale up to mine hard samples for much larger datasets for pre-training. We leave this as future work.\nBesides, we visualized the hard samples selected by those three methods. In Figure 5, the left image-text pair is the target one, and the pairs in the first row are picked by HPM. The second and third row show the image-text pairs mined by 6M approximated HPM and 3M approximated HPM. The visualization comparison show that the hard samples mined by the approximated HPM are also close enough to the target pair. We leave more visualization results in Appendix A.1."
        },
        {
            "heading": "5. Conclusion and Future Work",
            "text": "In conclusion, our experiments demonstrate that incorporating hard negative samples and a simple objective change can consistently and substantially improve the performance of language-image contrastive models through finetuning. Our proposed approach, HELIP , can be applied generically to any large-scale dataset and well-trained models for improved performance with smaller training schedules. The Hard Pair Mining (HPM) component efficiently mines qualified hard pairs, while the Hard Negative Margin Loss (HNML) effectively utilizes the supervision information provided by hard negative pairs. Our experiments show that the HELIP framework can boost pretrained VLMs on zero-shot classification and retrieval benchmarks, as well as\ndownstream linear probe tasks. Moving forward, several avenues for future research present themselves. First, we aim to explore compositionaware fine-tuning for VLMs, which could potentially enable more effective utilization of multimodal information. Moreover, we are intrigued by the prospect of combining parameter-efficient tuning [12] with HELIP potentially further enhancing performance. Another area of interest is scaling up the dataset size and examining the applicability of the scaling law to our method. We also intend to investigate how the integration of our boosting algorithm might alter the multimodal dataset curation algorithm [10]. Ultimately, we hope our work will serve as a catalyst for additional research in the fine-tuning of pre-trained, large-scale multimodal models."
        },
        {
            "heading": "Acknowledgement",
            "text": "We gratefully acknowledge the support of MindSpore, CANN (Compute Architecture for Neural Networks) and Ascend AI Processor used for this research."
        },
        {
            "heading": "A. Appendix",
            "text": "A.1. More visualization results\nWe provide more visualization results about the hard samples mined by different methods. As shown in the Figure 6, the hard samples mined by HPM are more similar to the target one (the left top). Besides, we found that, for samples with less objectives, the image and text mining method is able to deliver reasonable hard counterpart, such as the case \u201cthe harbor a small village\u201d. However, for the complex scene, only the HPM is able to provide samples challenging enough, such as the case \u201cpeople tour and enjoy the public park during summer\u201d. The web-crawled dataset include a large amount of complex cases. We think that is why training with the hard samples mined by HPM can achieve a better result.\nWe provide additional visualization results for the hard samples mined using various techniques. The hard samples mined by HPM are more comparable to the target one, as illustrated in the Figure 6 (the left top). Besides, we found that the image- and text-mode mining method can give a relatively reasonable hard counterpart for samples with less objectives, such as the case \u201cthe harbor a little settlement.\u201d. But, only the HPM is able to deliever samples that are difficult enough for the complicated scene, such as the case \u201cpeople tour and enjoy the public park throughout summer.\u201d. The dataset gathered from the web contains a large number of complex cases. We believe this is the reason training on the hard samples extracted by HPM can produce superior results.\nA.2. Algorithm\nWe summrize the hard pair mining and the training pipeline of HELIP in Algorithm 1 and 2 respectively.\nNote, the inner for loop, shown in Algorithm 1, will be\nAlgorithm 2 Hard samplE for boosting contrastive Language-Image Pretrained models (HELIP) . Require:\nHard Pair Mining algorithm, HPM() Pretrained unimodal vision model: ftext Pretrained unimodal vision model: fimage Dataset D = {(xI1, xT1 ), (xI1, xT1 ), \u00b7 \u00b7 \u00b7 , (xIN , xTN )} Pretrained contrastive language-image model {\u03c6image, \u03c6text} Hard pairs number k Hard negative margin strength \u03b3 Sampled hard negatives number p Learning ratio \u03b7 Batch size b Training iteration number E\nH \u2190 HPM(D, ftext, fimage, k) for iter \u2208 [1, E] do\nB \u2190 {z1, . . . , zb} i.i.d.\u223c Uniform(D) for zi \u2208 B do Hpi \u2190 {zi, . . . , zp}\ni.i.d.\u223c Uniform(Hi) B \u2190 B \u222aHpi\nend for Compute loss `finetune, Equation (6), with samplesB \u03c6image \u2190 \u03c6image + \u03b7 \u00b7 \u2202\u03c6image`finetune \u03c6text \u2190 \u03c6text + \u03b7 \u00b7 \u2202\u03c6text`finetune\nend for\nrepeatedly computed. To accelerate the hard pair mining and avoid unnecessary computational overhead, we compute the inner for loop firstly and save the encoded image features and text features. Besides, the outer loop is parallelized in the implementation."
        }
    ],
    "title": "Boosting Visual-Language Models by Exploiting Hard Samples",
    "year": 2023
}