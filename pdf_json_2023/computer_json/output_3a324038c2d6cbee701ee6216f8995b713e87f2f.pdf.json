{
    "abstractText": "After many researchers observed fruitfulness from the recent diffusion probabilistic model, its effectiveness in image generation is actively studied these days. In this paper, our objective is to evaluate the potential of diffusion probabilistic models for 3D human motion-related tasks. To this end, this paper presents a study of employing diffusion probabilistic models to predict future 3D human motion(s) from the previously observed motion. Based on the Human 3.6M and HumanEva-I datasets, our results show that diffusion probabilistic models are competitive for both single (deterministic) and multiple (stochastic) 3D motion prediction tasks, after finishing a single training process. In addition, we find out that diffusion probabilistic models can offer an attractive compromise, since they can strike the right balance between the likelihood and diversity of the predicted future motions. Our code is publicly available on the project website: https://sites.google. com/view/diffusion-motion-prediction.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hyemin Ahn"
        },
        {
            "affiliations": [],
            "name": "Esteve Valls Mascaro"
        },
        {
            "affiliations": [],
            "name": "Dongheui Lee"
        }
    ],
    "id": "SP:5c3b76eb88fe6a8e51f06539cbf359eb007a4ca2",
    "references": [
        {
            "authors": [
                "B. Zhou",
                "X. Tang",
                "X. Wang"
            ],
            "title": "Learning collective crowd behaviors with dynamic pedestrian-agents",
            "venue": "International Journal of Computer Vision (IJCV), vol. 111, no. 1, pp. 50\u201368, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "Y. Xu",
                "Z. Piao",
                "S. Gao"
            ],
            "title": "Encoding crowd interaction with deep neural network for pedestrian trajectory prediction",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 5275\u2013 5284.",
            "year": 2018
        },
        {
            "authors": [
                "A. Rasouli",
                "I. Kotseruba",
                "T. Kunic",
                "J.K. Tsotsos"
            ],
            "title": "Pie: A large-scale dataset and models for pedestrian intention estimation and trajectory prediction",
            "venue": "International Conference on Computer Vision (ICCV), 2019, pp. 6262\u20136271.",
            "year": 2019
        },
        {
            "authors": [
                "K. Kim",
                "Y.K. Lee",
                "H. Ahn",
                "S. Hahn",
                "S. Oh"
            ],
            "title": "Pedestrian intention prediction for autonomous driving using a multiple stakeholder perspective model",
            "venue": "International Conference on Intelligent Robots and Systems (IROS). IEEE, 2020, pp. 7957\u20137962.",
            "year": 2020
        },
        {
            "authors": [
                "J. B\u00fctepage",
                "H. Kjellstr\u00f6m",
                "D. Kragic"
            ],
            "title": "Anticipating many futures: Online human motion prediction and generation for human-robot interaction",
            "venue": "International Conference on Robotics and Automation (ICRA). IEEE, 2018, pp. 4563\u20134570.",
            "year": 2018
        },
        {
            "authors": [
                "C. Ionescu",
                "D. Papava",
                "V. Olaru",
                "C. Sminchisescu"
            ],
            "title": "Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments",
            "venue": "Transactions on Pattern Analysis and Machine Intelligence (TPAMI), vol. 36, no. 7, pp. 1325\u20131339, jul 2014.",
            "year": 2014
        },
        {
            "authors": [
                "M. Loper",
                "N. Mahmood",
                "J. Romero",
                "G. Pons-Moll",
                "M.J. Black"
            ],
            "title": "Smpl: A skinned multi-person linear model",
            "venue": "ACM transactions on graphics (TOG), vol. 34, no. 6, pp. 1\u201316, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "N. Mahmood",
                "N. Ghorbani",
                "N.F. Troje",
                "G. Pons-Moll",
                "M.J. Black"
            ],
            "title": "Amass: Archive of motion capture as surface shapes",
            "venue": "International Conference on Computer Vision (ICCV), 2019, pp. 5442\u20135451.",
            "year": 2019
        },
        {
            "authors": [
                "L. Sigal",
                "M.J. Black"
            ],
            "title": "Humaneva: Synchronized video and motion capture dataset for evaluation of articulated human motion",
            "venue": "Brown Univertsity TR, vol. 120, no. 2, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "J. Martinez",
                "M.J. Black",
                "J. Romero"
            ],
            "title": "On human motion prediction using recurrent neural networks",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 2891\u20132900.",
            "year": 2017
        },
        {
            "authors": [
                "K. Fragkiadaki",
                "S. Levine",
                "P. Felsen",
                "J. Malik"
            ],
            "title": "Recurrent network models for human dynamics",
            "venue": "Proceedings of the IEEE international conference on computer vision, 2015, pp. 4346\u20134354.",
            "year": 2015
        },
        {
            "authors": [
                "A. Jain",
                "A.R. Zamir",
                "S. Savarese",
                "A. Saxena"
            ],
            "title": "Structural-rnn: Deep learning on spatio-temporal graphs",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 5308\u20135317.",
            "year": 2016
        },
        {
            "authors": [
                "W. Mao",
                "M. Liu",
                "M. Salzmann",
                "H. Li"
            ],
            "title": "Learning trajectory dependencies for human motion prediction",
            "venue": "International Conference on Computer Vision (ICCV), 2019, pp. 9489\u20139497.",
            "year": 2019
        },
        {
            "authors": [
                "E. Aksan",
                "M. Kaufmann",
                "P. Cao",
                "O. Hilliges"
            ],
            "title": "A spatio-temporal transformer for 3d human motion prediction",
            "venue": "International Conference on 3D Vision (3DV). IEEE, 2021, pp. 565\u2013574.",
            "year": 2021
        },
        {
            "authors": [
                "E. Valls Mascaro",
                "S. Ma",
                "H. Ahn",
                "D. Lee"
            ],
            "title": "Robust human motion forcasting using transformer-based model",
            "venue": "International Conference on Intelligent Robots and Systems (IROS). IEEE, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "E. Barsoum",
                "J. Kender",
                "Z. Liu"
            ],
            "title": "Hp-gan: Probabilistic 3d human motion prediction via gan",
            "venue": "Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2018, pp. 1418\u20131427.",
            "year": 2018
        },
        {
            "authors": [
                "X. Yan",
                "A. Rastogi",
                "R. Villegas",
                "K. Sunkavalli",
                "E. Shechtman",
                "S. Hadap",
                "E. Yumer",
                "H. Lee"
            ],
            "title": "Mt-vae: Learning motion transformations to generate multimodal human dynamics",
            "venue": "European conference on computer vision (ECCV), 2018, pp. 265\u2013281.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Yuan",
                "K. Kitani"
            ],
            "title": "Dlow: Diversifying latent flows for diverse human motion prediction",
            "venue": "European Conference on Computer Vision (ECCV). Springer, 2020, pp. 346\u2013364.",
            "year": 2020
        },
        {
            "authors": [
                "W. Mao",
                "M. Liu",
                "M. Salzmann"
            ],
            "title": "Generating smooth pose sequences for diverse human motion prediction",
            "venue": "International Conference on Computer Vision (ICCV), 2021, pp. 13 309\u201313 318.",
            "year": 2021
        },
        {
            "authors": [
                "J. Ho",
                "A. Jain",
                "P. Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS), vol. 33, pp. 6840\u20136851, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Tashiro",
                "J. Song",
                "Y. Song",
                "S. Ermon"
            ],
            "title": "Csdi: Conditional scorebased diffusion models for probabilistic time series imputation",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS), vol. 34, pp. 24 804\u201324 816, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C. Saharia",
                "W. Chan",
                "S. Saxena",
                "L. Li",
                "J. Whang",
                "E. Denton",
                "S.K.S. Ghasemipour",
                "B.K. Ayan",
                "S.S. Mahdavi",
                "R.G. Lopes"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "arXiv preprint arXiv:2205.11487, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "C. Saharia",
                "W. Chan",
                "H. Chang",
                "C. Lee",
                "J. Ho",
                "T. Salimans",
                "D. Fleet",
                "M. Norouzi"
            ],
            "title": "Palette: Image-to-image diffusion models",
            "venue": "SIG- GRAPH Conference Proceedings, 2022, pp. 1\u201310.",
            "year": 2022
        },
        {
            "authors": [
                "S. Aliakbarian",
                "F. Saleh",
                "L. Petersson",
                "S. Gould",
                "M. Salzmann"
            ],
            "title": "Contextually plausible and diverse 3d human motion prediction",
            "venue": "International Conference on Computer Vision (ICCV), 2021, pp. 11 333\u201311 342.",
            "year": 2021
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.",
            "year": 1997
        },
        {
            "authors": [
                "J. Chung",
                "C. Gulcehre",
                "K. Cho",
                "Y. Bengio"
            ],
            "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
            "venue": "arXiv preprint arXiv:1412.3555, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "W. Mao",
                "M. Liu",
                "M. Salzmann"
            ],
            "title": "History repeats itself: Human motion prediction via motion attention",
            "venue": "European Conference on Computer Vision (ECCV). Springer, 2020, pp. 474\u2013489.",
            "year": 2020
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems (NeurIPS), vol. 30, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "D.P. Kingma",
                "M. Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv preprint arXiv:1312.6114, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "I. Goodfellow",
                "J. Pouget-Abadie",
                "M. Mirza",
                "B. Xu",
                "D. Warde-Farley",
                "S. Ozair",
                "A. Courville",
                "Y. Bengio"
            ],
            "title": "Generative adversarial networks",
            "venue": "Communications of the ACM, vol. 63, no. 11, pp. 139\u2013144, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "V. Popov",
                "I. Vovk",
                "V. Gogoryan",
                "T. Sadekova",
                "M. Kudinov"
            ],
            "title": "Gradtts: A diffusion probabilistic model for text-to-speech",
            "venue": "International Conference on Machine Learning. PMLR, 2021, pp. 8599\u20138608.",
            "year": 2021
        },
        {
            "authors": [
                "D. Yang",
                "J. Yu",
                "H. Wang",
                "W. Wang",
                "C. Weng",
                "Y. Zou",
                "D. Yu"
            ],
            "title": "Diffsound: Discrete diffusion model for text-to-sound generation",
            "venue": "arXiv preprint arXiv:2207.09983, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Ho",
                "T. Salimans",
                "A. Gritsenko",
                "W. Chan",
                "M. Norouzi",
                "D.J. Fleet"
            ],
            "title": "Video diffusion models",
            "venue": "arXiv preprint arXiv:2204.03458, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Kim",
                "J. Kim",
                "S. Choi"
            ],
            "title": "Flame: Free-form language-based motion synthesis & editing",
            "venue": "arXiv preprint arXiv:2209.00349, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Zhang",
                "Z. Cai",
                "L. Pan",
                "F. Hong",
                "X. Guo",
                "L. Yang",
                "Z. Liu"
            ],
            "title": "Motiondiffuse: Text-driven human motion generation with diffusion model",
            "venue": "arXiv preprint arXiv:2208.15001, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Janner",
                "Y. Du",
                "J.B. Tenenbaum",
                "S. Levine"
            ],
            "title": "Planning with diffusion for flexible behavior synthesis",
            "venue": "arXiv preprint arXiv:2205.09991, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "T. Salimans",
                "J. Ho"
            ],
            "title": "Progressive distillation for fast sampling of diffusion models",
            "venue": "International Conference on Learning Representations, 2021.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION Estimating how a human would move in the near future is an essential task for various applications such as surveillance [1], [2], autonomous driving [3], [4], and humanrobot/computer-interaction [5]. Many approaches have been proposed to solve this problem, often based on the motion capture datasets such as Human3.6M [6] or SMPL [7]-based datasets such as AMASS [8]. In this paper, we concern with a task whose goal is to predict a sequence of 3D pose skeletons in Human3.6M and HumanEva-I [9] datasets, when a previously observed 3D pose sequence is given as an input.\nExisting works on 3D skeleton motion prediction can be categorized as follows. One line of research focuses on models for deterministic motion prediction [10]\u2013[15]. These works aim at predicting a single motion that is most likely to be observed in the future. Therefore, their performance is usually evaluated based on an L2-distance between a prediction and a ground truth. Another line of research focuses on generative models for stochastic motion prediction [16]\u2013 [19]. Their performance is evaluated based on the metrics for likelihood and diversity. After generating a fixed number of prediction samples from a single observation, the likelihood is measured based on the minimum distance between the prediction samples and ground truth, and the diversity is measured based on the average distance between all pairs of prediction samples.\n1Hyemin Ahn is with Artificial Intelligence Graduate School (AIGS), Ulsan National Institute of Science and Technology (UNIST), Ulsan, Korea (e-mail: hyemin.ahn@unist.ac.kr).\n2Esteve Valls Mascaro and Dongheui Lee are with Autonomous Systems, Technische Universita\u0308t Wien (TU Wien), Vienna, Austria (e-mail: {esteve.valls.mascaro, dongheui.lee}@tuwien.ac.at).\n3Dongheui Lee is also with the Institute of Robotics and Mechatronics (DLR), German Aerospace Center, Wessling, Germany.\nHowever, we cannot judge which approach is always better than the other, since the efficiency would depend on the target application values. For instance, when one needs only the most precise sample with low latency, deterministic approaches would be better. If we say that both approaches are necessary, our next question would be whether we can propose an efficient model for both types of prediction. To answer the question, we study the possibility of using diffusion probabilistic models [20], [21] for both deterministic and stochastic 3D motion prediction tasks.\nIf we propose a diffusion probabilistic model [20], [21] as a solution, one might ask us whether this is because we are fascinated by its performance in image generation [22], [23]. Frankly speaking, yes, we initiated this study out of our curiosity \u2013 can we use diffusion probabilistic models for 3D motion prediction? Unfortunately, our experimental results show that the diffusion model cannot perfectly replace existing state-of-the-arts for both deterministic and stochastic motion prediction tasks. However, we found a glimpse of hope in diffusion models, due to their effectiveness in both prediction types after a single training procedure, and their ability to properly balance the trade-off between diversity and likelihood.\nar X\niv :2\n30 2.\n14 50\n3v 1\n[ cs\n.C V\n] 2\n8 Fe\nb 20\n23\nFigure 1 shows the example results when the diffusion models are used for both deterministic and generative motion prediction tasks. Although a diffusion model is essentially a generative model, we found that the deterministic sample with a fair performance can be obtained from the diffusion model when all randomness is excluded from its denoising process. In addition, we found out that the diffusion models can fix the flaws of several generative methods [18], which highlight the diversity of generated samples. Existing works as [18] claim that the likelihood of predicted samples is high when the minimum distance between samples and ground truth is low. Because of this, [18] can often generate the motions that are out-of-context as [24] pointed out. Compared to this, our diffusion models can generate prediction samples that are more likely to occur, so the generated motion does not diverge too much to be called out-of-context.\nThe remaining paper is constructed as follows. After representing our literature survey in Section II, Section III will explain how general diffusion models work as well as how we design ours to solve 3D motion prediction tasks. Section IV will show both qualitative and quantitative experiment results, and a related discussion will be also presented. Finally, this paper will end in Section V by mentioning limitations and future works."
        },
        {
            "heading": "II. RELATED WORK",
            "text": "A. 3D Motion Prediction\nDeterministic Models. The goal of deterministic 3D motion prediction is to minimize the distance between a predicted motion and ground truth. To solve this problem, early works relying on deep neural networks [10]\u2013[12] often employed recurrent neural networks (RNNs) [25], [26], which are still well-known for their effectiveness in processing timeseries data. Among RNN-based works, a notable model is a structure RNN (S-RNN) [12], which considers the spatio-temporal information of human motion, by manually designing the high-level spatio-temporal graph to explicitly model the human body structure (i.e., spine, arm, and leg).\nWhile S-RNN understands the human body structure based on the handcrafted network structure, there is another line of research [13], [27] that uses graph convolutional network (GCN) to overcome this manually designed spatial relationship understanding. For instance, [13] suggested a model named DCT-GCN, where discrete cosine transform (DCT) understands the temporal information of motion, and GCN learns the spatial relationship between human body joints. DCT-GCN obtains the state-of-the-art result when evaluated on Euler-angle-based mean squared error, but its best result can be obtained when the model is separately trained for each short- or long-term prediction.\nRecently, several works for deterministic motion prediction [14], [15] are based on the Transformer [28], which was originally suggested for language understanding problems. Models named Spatio-Temporal Transformer (ST-TR) [14] and 2-Channel Transformer (2CH-TR) [15], understand the spatio-temporal relationship of human motion by putting the self-attention mechanism on each pose-parameter (spatial)\nand time (temporal) dimension. After understanding each spatial and temporal information in parallel, outputs from both attention mechanisms are properly combined. The difference between ST-TR and 2CH-TR comes from when and how often the model combines spatial and temporal information.\nGenerative Models. The goal of stochastic 3D motion prediction is to build a generative model which can sample out several future motions that are likely to happen after the observed human motion. To solve this problem, early works [16], [17] employed deep generative models such as variational autoencoders (VAEs) [29] or generative adversarial networks (GANs) [30]. For instance, [17] suggested a generative model based on the conditional VAEs, and showed that VAEs can sample out several future motions that are reasonable as well as diverse. Compared to VAE, [16] showed that GANs based on the Wasserstein loss function can be effectively used in stochastic motion prediction tasks.\nWhile these works [16], [17] focused on exploring the potential of using deep generative models in stochastic motion prediction tasks, another line of works [18], [19] focused on sampling out as much as diverse motions that can contain the most plausible motion at the same time. For instance, [18] proposed to train a post-hoc model which can be attached to the pre-trained deep generative model. This post-hoc model maps a random variable to several latent vectors of the pretrained generative model. Based on the diversity-promoting prior, the post-hoc model is trained to improve the diversity between samples, which can be obtained by decoding the mapped latent vectors.\nExperiments in [18], [19] evaluate the likelihood of prediction samples based on the minimum distance between the samples and the ground truth(s). They denote the prediction samples as plausible based on the sample that is closest to the ground truth(s). However, this can make it difficult for users to choose the most plausible motion among the prediction samples, since all samples will not be distributed near the most plausible motion. For instance, if the observed motion is a human sitting down and drinking something, [18] and [19] can produce motion samples that predict the human suddenly standing up and starting discussing something with others. As [24] has pointed out, we would like to also focus on the necessity of contextually plausible and diverse motion sampling. Therefore, our paper would evaluate the likelihood of prediction also based on the mean and standard deviation of distances between the samples and ground truth."
        },
        {
            "heading": "B. Diffusion Probabilistic Models",
            "text": "Diffusion probabilistic models [20] have become a new rising star in generative models after showing excellent performance in image synthesis. Especially, its performance on text-conditioned image synthesis [22] makes researchers as well as the public in awe. Diffusion models consider two processes: a forward process that slowly destructs the data sample by gradually injecting the random noise, and a reverse process that learns how to reconstruct the data\nsample by gradually denoising the random noise. While the advantage of diffusion models can be empirically shown based on their performances, the disadvantage is the speed of their sampling process. If the reverse process includes 1000 times of denoising processes, it means that the data sample can be obtained after feed-forwarding the random noise to the denoising network for 1000 times. Of course, this disadvantage can be circumvented if the application does not require the prediction samples with low latency.\nAside from image generation tasks, nowadays researchers are suggesting to use diffusion models in various generation tasks, such as text-to-speech [31], text-to-sound [32], and video [33]. Focusing on motion-related tasks like ours, several works incorporate diffusion models in text-conditioned motion generation tasks [34], [35]. For the motion of intelligence agents, [36] suggests using diffusion models to sample out trajectories for properly solving a given task. In our paper, we use diffusion models in 3D human motion prediction tasks, but to the best of our knowledge, there is no attempt yet to use diffusion models in the 3D motion prediction task. But we believe more researchers would involve in using diffusion models to answer this question \u2013 can diffusion models be our new savior in any kind of data generation tasks?"
        },
        {
            "heading": "III. METHOD",
            "text": ""
        },
        {
            "heading": "A. Preliminaries",
            "text": "We will provide a short description of diffusion probabilistic models first. Note that our description relies on [20] and [21], which provide a basis for our work.\nDiffusion Probabilistic Model. Let x0 \u223c q(x0) denote a data point sampled from its distribution q. In order to learn p\u03b8(x0) which can model q(x0), diffusion probabilistic models consider two processes. One is a forward process which gradually deconstructs x0 by injecting a subtle Gaussian noise for K times, such that x0 can be destroyed into x1, . . . ,xK , where p(xK) = N (0, I). This process can be formulated as below, which is to follow a Markov chain q(xk|xk\u22121) for K times:\nq(x1:K |x0) = K\u220f k=1 q(xk|xk\u22121) (1)\nq(xk|xk\u22121) = N ( \u221a 1\u2212 \u03b2kxk\u22121, \u03b2kI), (2)\nwhere \u03b2k denotes a constant for a noise level. Note that xk can be sampled from x0 directly with a closed-form solution:\nxk = \u221a \u03b1kx 0 + \u221a 1\u2212 \u03b1k , \u223c N (0, I), (3)\nwhere \u03b1\u0302k = 1\u2212 \u03b2k and \u03b1k = \u220fk i=1 \u03b1\u0302i.\nAnother is a reverse process, which goal is to obtain x0 starting from xK \u223c N (0, I), by gradually denoising xK . This process can also be formulated as following a Markov chain p\u03b8(xk\u22121|xk) for K times:\np\u03b8(x 0:K) = p(xK) K\u220f k=1 p\u03b8(x k\u22121|xk), (4)\np\u03b8(x k\u22121|xk) = N ( xt\u22121;\u00b5\u03b8(x k, k), \u03c32(k)I ) , (5)\nwhere p(xK) = N (0, I). To obtain \u00b5\u03b8 and \u03c3, [20] suggests denoising diffusion probabilistic models (DDPM), which get \u03c32(k) = 1\u2212\u03b1k\u221211\u2212\u03b1k \u03b2k, parameterize \u00b5\u03b8 with \u03b8, and sample xk\u22121 \u223c p\u03b8(xk\u22121|xk) as below:\n\u00b5\u03b8(x k, k) = 1\u221a \u03b1\u0302k\n( xk \u2212 \u03b2k\u221a\n1\u2212 \u03b1k \u03b8(x\nk, k) ) . (6)\nxk\u22121 = \u00b5\u03b8(x k, k) + \u03c3(k)z, z \u223c N (0, I). (7)\nIn practice, \u03b8 is modeled with a neural network, and it learns how much to denoise from xk. To train this, [20] suggested a simplified loss function as below:\nL(\u03b8) = \u2016 \u2212 \u03b8(xk, k)\u20162\n= \u2016 \u2212 \u03b8( \u221a \u03b1tx 0 + \u221a 1\u2212 \u03b1t , k)\u20162. (8)\nIn a training process, k is randomly sampled to obtain L(\u03b8). For more details, please refer to [20] and [21].\nConditional Diffusion Model. A conditional score-based diffusion model for imputation (CSDI) [21] is proposed to solve a time-series imputation problem using diffusion models. It adds conditional information xco to eq. (4)-(5):\np\u03b8(x 0:K) = p(xK) K\u220f k=1 p\u03b8(x k\u22121|xk,xco), (9)\np\u03b8(x k\u22121|xk,xco) = N (xk\u22121;\u00b5\u03b8(xk, k|xco), \u03c32(k)I) (10)\nTo define \u00b5\u03b8(x k, k|xco), eq. (6)-(7) can be rewritten by adding xco as a condition to \u00b5\u03b8 and \u03b8. Note that \u03b8(x\nk, k|xco) is modeled with a neural network to learn how much to denoise from xk given xco. When training the network, the same loss function as eq. (8) is used, by replacing \u03b8 properly with xco as a condition."
        },
        {
            "heading": "B. Problem Formulation",
            "text": "Let pt \u2208 RD be a 3D pose vector at time t, which can be denoted with various representations such as axisangle, Euler-angle, or xyz-position. Here, D = 3n and n denotes the number of joints. A task of 3D human motion prediction can be defined as predicting future L poses, Ppre = {pT+1, . . .pT+L} \u2208 RL\u00d7D, when T poses, Pobs = {p1, . . .pT } \u2208 RT\u00d7D are observed.\nWe utilize CSDI [21] for obtaining Ppre from given Pobs. Starting from P 0pre = Ppre, our forward process can obtain P kpre as below:\nP kpre = \u221a \u03b1kP 0 pre + \u221a 1\u2212 \u03b1k , \u223c N (0, I) (11)\nFor a reverse process, we propose a denoiser network which models \u03b8(xk, k|xco) = \u03b8(P kpre, k|Pobs). This network is trained by minimizing L(\u03b8) = \u2016 \u2212 \u03b8(P kpre, k|Pobs)\u20162.\nAfter training, we can sample P 0pre by repeating below reverse process for K times, starting from PKpre \u223c N (0, I):\nP k\u22121pre = \u00b5\u03b8(P k pre, k|Pobs) + \u03c3(k)z, z \u223c N (0, I), (12)\nwhere \u00b5\u03b8(P k pre, k|Pobs) is defined with \u03b8(P kpre, k|Pobs) and properly modified version of eq. (6). After finishing training, if our denoiser network is used for deterministic prediction in the test phase, we set PKpre and z as zero-vectors, such that all randomness in eq. (12) can be ignored."
        },
        {
            "heading": "C. Transformer-based Motion Denoiser",
            "text": "Since P kpre and Pobs are time-series of human pose vectors, one can model \u03b8(P kpre, k|Pobs) with neural network architectures which can understand time-series data. For example, network architectures such as RNNs [25], [26] or Transformers [28] can be candidates. We empirically found out that the denoisers based on the Transformers that process both spatial and temporal information are most effective.\nFigure 2 shows how we design our Transformer-based denoisers in two ways. Inspired by [21], the first denoiser shown on top of the figure processes both information in series. After concatenating P kpre \u2208 RL\u00d7D and Pobs \u2208 RT\u00d7D such that input can be P kinp \u2208 R(T+L)\u00d7D, P kinp passes spatial and temporal transformer layers in series, where each layer applies self-attention to time and pose-parameter dimension. Before passing each transformer layer, positional encoding is added to the input as [28] suggests, with respect to poseparameter d \u2208 [0, D] (spatial) or time t \u2208 [0, T ] (temporal) dimension. Also, the additional learnable positional encoding that projects a diffusion step k into a vector space is added\nto the input as [21] suggests. Let P kout \u2208 R(T+L)\u00d7D denote the output which can be obtained after P kinp passing two layers. Then, the last L\u00d7D parts from P kout is obtained as \u03b8(P k pre, k|Pobs), which would be used for denoising P kpre.\nThe second denoiser shown on the bottom of Figure 2, is inspired by [14] and [15], and works in parallel to understand spatio-temporal information. After P kinp passes both spatial and temporal transformer layers in parallel, two matrices with the same size as P kinp are obtained, and concatenated into a 3rd-order tensor whose size is 2 \u00d7 (T + L) \u00d7 D. After this tensor passes 2-dimensional convolutional layer with (1 \u00d7 1)-sized kernel, the output P kout \u2208 R(T+L)\u00d7D is obtained. From P kout, \u03b8(P k pre, k|Pobs) is obtained as same as in the first denoiser. Note that we do not use encoder-decoder based structure, which encode a set of feature vectors from Pobs and decode \u03b8(P k pre, k|Pobs) from the encoded feature vectors and P kpre. We tried various denoisers of Transformer- or RNN-based encoder-decoder, but none of them turns out to be effective.\nD. Implementation Details\nOur transformer-based motion denoisers have a selfattention module with 8 multi-heads and 512-dimensional query, key, and value vectors. And each temporal or spatial transformer layer shown in Figure 2 consists of a singlelayered transformer encoder. To train denoisers, we set batch size as 512 and update parameters for 50,000 iterations with Adam optimizer of learning rate 0.0001. The diffusion step is set as k \u2208 [0, 20], with linearly scheduled noise levels \u03b2k that ranges between 0.001 (k \u2193) and 0.333 (k \u2191)."
        },
        {
            "heading": "IV. EXPERIMENT",
            "text": ""
        },
        {
            "heading": "A. Dataset and Metric",
            "text": "Dataset. We conduct our experiment for both deterministic and stochastic motion prediction tasks. For deterministic experiments, we use the Human3.6M dataset [6] and measure the Euler-angle mean square error (MSE) for evaluation as other works [12]\u2013[15] do. Here, with 25 fps, input observation has 50 frames, and output prediction has 25 frames. For stochastic experiments, we preprocess Human3.6M [6] and HumanEva-I [9] datasets into xyz-based representation as [18], [19] do. Based on that, various metrics for evaluating likelihood and diversity are measured. Here, with 50 fps, an input observation has 25 frames, output prediction has 100 frames, and the number of prediction samples is 50.\nMetrics. As mentioned above, we measure the performance of our denoiser based on the Euler-angle MSE when it is used for deterministic prediction. For stochastic prediction, we use several metrics from what [18] suggests to evaluate likelihood and diversity. But we propose more metrics such as aDE, sDE, aFDE, and sFDE to measure how the samples are distributed near the ground truth. Note that some of the below sentences describing metrics are borrowed from [18].\n(1) Average Pairwise Distance (APD): average L2 distance between pairs from N predictions x\u0302 \u2208 RL\u00d7D, which is computed as 1N(N\u22121) \u2211N i=1 \u2211N j 6=i \u2016x\u0302i\u2212 x\u0302j\u20162. This measures the diversity within N predictions. (2) minimum Displacement Error (mDE): the minimum L2 distance between all N predictions x\u0302 and ground truth x, which is computed as minx\u0302 1L\u2016x\u0302 \u2212 x\u20162. This metric was defined as ADE in [18]. (3) average Displacement Error (aDE): the average L2 distance between all N predictions x\u0302 and ground truth x, which is computed as 1NL \u2211N i=1 \u2016x\u0302i \u2212 x\u20162. (4) standard deviation of Displacement Error (sDE): the standard deviation of L2 distances between all N predictions and ground truth. (5) minimum Final Displacement Error (mFDE): the minimum L2 distance between final poses of N predictions and ground truth, which is calculated as minx\u0302 \u2016x\u0302(L) \u2212 x(L)\u20162. This metric was defined as FDE in [18]. (6) average Final Displacement Error (aFDE): the average L2 distance between final poses of N predictions and ground truth, which is calculated as 1N \u2211N i=1 \u2016x\u0302i(L) \u2212 x(L)\u20162. (7) standard deviation of Final Displacement Error (sFDE): the standard deviation of L2 distances between final poses of N predictions and ground truth."
        },
        {
            "heading": "B. Quantitative Results",
            "text": "Deterministic Prediction. Table I compares Euler-angle MSEs when our diffusion model is used for deterministic motion prediction. Here, bold fonts denote the best results among all approaches, and underlines denote the best results among our denoisers (series or parallel). It is shown that the overall performance of DCT-GCN [13] is still the best. Among our approaches, the denoiser which understands spatial and temporal information in series is better than the parallel denoiser. Although our models do not achieve state-of-the-art results, it is shown that our approaches are\nbetter in long-term prediction (1000ms) when compared with other transformer-based models [14], [15]. This is a notable result, since (1) our models are originally generative ones, and (2) our models do not require additional training for deterministic prediction since ignoring all randomness in the denoising process is all they need. Stochastic Prediction Table II shows the comparison of metrics for measuring the likelihood and diversity. Here, bold fonts denote the best result and underlines denote the second best result among all approaches. It is shown that previous works [18], [19] focusing on sample diversity best perform in APD. Also, it is shown that they are generally better in terms of mDE and mFDE. We would like to argue here that the high diversity in prediction increases the probability of having one sample closest to the ground truth. Then, how can we choose the most plausible result among predictions that are sampled to be diverse?\nThis is the same question that [24] also pointed out. So in [24], metrics for measuring the quality and context are proposed. For measuring the quality, [24] used a pretrained binary classifier which can discriminate the ground truths (real) from predictions (fake). If this classifier fails to discriminate the predicted motions as fake, a higher quality score is obtained. For measuring the context, [24] used a pre-trained model which classifies action from motion. If it estimates that the action label of prediction is as same as the observed motion, a higher context score is obtained.\nHowever, we were not able to use the same metric as [24] since its pre-trained classifiers were not openly released. Therefore, we instead propose metrics such as aDE, sDE, aFDE, and sFDE, to measure how closely the samples are distributed near the ground truth. Results show that our approaches generally perform better in terms of these new metrics, and the parallel denoiser performs better than the series one. We also present the result from VAEs [29] that were implemented by [18], to check how other nondiffusion generative models work. It is shown that the overall performances of our series/parallel denoiser in diversity and likelihood are generally better than the VAEs, especially in the HumanEva-I dataset."
        },
        {
            "heading": "C. Qualitative Results",
            "text": "Figure 3 shows two example results from our transformerbased motion denoiser. Predictions on the left of the dotted line are obtained from the motion observation labeled as \u2018smoking\u2019. It is shown that the deterministic prediction is similar to the ground truth, while the stochastic predictions show the diversity between samples. But note that still\nTABLE II DIVERSITY AND LIKELIHOOD METRICS OF STOCHASTIC MOTION PREDICTION\nHuman 3.6M [6] HumanEva-I [9] metrics APD\u2191 mDE\u2193 aDE\u2193 sDE\u2193 mFDE\u2193 aFDE\u2193 sFDE\u2193 APD\u2191 mDE\u2193 aDE\u2193 sDE\u2193 mFDE\u2193 aFDE\u2193 sFDE\u2193 DLow [18] 11.741 0.425 0.968 0.355 0.518 1.387 0.541 4.855 0.251 0.585 0.208 0.268 0.710 0.255 VAEs [18], [29] 6.852 0.460 0.720 0.139 0.557 1.025 0.243 2.299 0.265 0.426 0.083 0.299 0.562 0.137 GSPS [19] 14.757 0.389 1.206 0.623 0.496 1.554 0.729 5.825 0.233 0.655 0.206 0.244 0.763 0.268 Ours (Series) 7.587 0.527 0.764 0.132 0.669 1.093 0.228 2.746 0.257 0.383 0.065 0.260 0.490 0.130 Ours (Parallel) 6.445 0.477 0.719 0.139 0.584 1.018 0.234 1.508 0.242 0.312 0.037 0.238 0.385 0.078\nFig. 3. Deterministic (Deter.) and stochastic (Sto.) predictions from our transformer-based motion denoiser. Note that two results are given and divided based on the vertical dotted line. Predictions are obtained from observed motions labeled as \u2018smoking\u2019 (left) and \u2018walking\u2019 (right).\nthe context of \u2018smoking\u2019 looks remained in all samples. This phenomenon is also observed from the predictions on the right, which are obtained from the motion observation of \u2018walking\u2019. While its deterministic prediction resembles the ground truth, the stochastic predictions are diverse and contain the context of \u2018walking\u2019. For better visualization, please refer to our supplementary video."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "In this work, we study the potential of diffusion probabilistic models for 3D human motion prediction tasks. We propose two types of diffusion models based on the transformers, which understand the motion\u2019s spatial and temporal information in series or parallel. Since the diffusion model is originally a generative model, its main usage would be for the stochastic motion prediction task. But once it is trained, we show that it can also be used in deterministic prediction if all randomness in its denoising process is ignored.\nTo show the effectiveness of diffusion models in both deterministic and stochastic motion prediction tasks, we conduct experiments based on various metrics. Results from deterministic prediction show that the diffusion model is not superior to the state-of-the-art. But it is shown that our longterm (1000ms) prediction performance is better than other transformer-based approaches. When it comes to evaluating stochastic predictions, it is conventional to suggest metrics measuring both likelihood and diversity. However, we claim that the conventional metrics for measuring the likelihood\ndo not represent how much the samples are distributed near the plausible motion, since they measure the minimum distance between samples and ground truth. Therefore, we suggest additional metrics to measure the mean and standard deviation of that distances, and the results show that our diffusion models can properly balance the trade-off between diversity and likelihood.\nAlthough our results would provide nice answers to our first question \u2013 can we use diffusion probabilistic models for 3D motion prediction? \u2013 the most concerning disadvantage of a diffusion model is its sampling frequency. Since our diffusion model requires a K = 20 number of denoising processes to obtain prediction samples, this might occur a bit high latency. To overcome this issue, one might consider recent works for efficient sampling [37], which would be our future work, such that efficient 3D human motion prediction can be made for various real-time applications."
        },
        {
            "heading": "ACKNOWLEDGMENT",
            "text": "This work was supported by the Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.20200-01336, Artificial Intelligence Graduate School Program (UNIST)), and funded by Marie Sklodowska-Curie Action Horizon 2020 (Grant agreement No. 955778) for project \u2018Personalized Robotics as Service Oriented Applications\u2019 (PERSEO)."
        }
    ],
    "title": "Can We Use Diffusion Probabilistic Models for 3D Motion Prediction?",
    "year": 2023
}