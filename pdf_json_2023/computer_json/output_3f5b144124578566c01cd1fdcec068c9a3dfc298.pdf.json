{
    "abstractText": "Community detection is an important and powerful way to understand the latent structure of complex networks in social network analysis. This paper considers the problem of estimating community memberships of nodes in a directed network, where a node may belong to multiple communities. For such a directed network, existing models either assume that each node belongs solely to one community or ignore variation in node degree. Here, a directed degree corrected mixed membership (DiDCMM) model is proposed by considering degree heterogeneity. An efficient spectral clustering algorithm with a theoretical guarantee of consistent estimation is designed to fit DiDCMM. We apply our algorithm to a small scale of computer-generated directed networks and several real-world directed networks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Huan Qing"
        }
    ],
    "id": "SP:25d040ecd3f6e5736be0fb18500ef7d479468541",
    "references": [
        {
            "authors": [
                "S. Fortunato"
            ],
            "title": "Community detection in graphs",
            "venue": "Phys. Rep",
            "year": 2010
        },
        {
            "authors": [
                "S. Fortunato",
                "D. Hric"
            ],
            "title": "Community detection in networks: A user guide",
            "venue": "Phys. Rep",
            "year": 2016
        },
        {
            "authors": [
                "B. Karrer",
                "M.E.J. Newman"
            ],
            "title": "Stochastic blockmodels and community structure in networks",
            "venue": "Phys. Rev. E",
            "year": 2011
        },
        {
            "authors": [
                "K. Rohe",
                "S. Chatterjee",
                "B. Yu"
            ],
            "title": "Spectral clustering and the high-dimensional stochastic blockmodel",
            "venue": "Ann. Stat",
            "year": 2011
        },
        {
            "authors": [
                "Y. Zhao",
                "E. Levina",
                "J. Zhu"
            ],
            "title": "Consistency of community detection in networks under degree-corrected stochastic block models",
            "venue": "Ann. Stat",
            "year": 2012
        },
        {
            "authors": [
                "T. Qin",
                "K. Rohe"
            ],
            "title": "Regularized spectral clustering under the degree-corrected stochastic blockmodel",
            "venue": "Adv. Neural Inf. Process. Syst. 2013,",
            "year": 2013
        },
        {
            "authors": [
                "J. Jin"
            ],
            "title": "Fast community detection by SCORE",
            "venue": "Ann. Stat",
            "year": 2015
        },
        {
            "authors": [
                "J. Lei",
                "A. Rinaldo"
            ],
            "title": "Consistency of spectral clustering in stochastic block models",
            "venue": "Ann. Stat",
            "year": 2015
        },
        {
            "authors": [
                "T.T. Cai",
                "X. Li"
            ],
            "title": "Robust and computationally feasible community detection in the presence of arbitrary outlier nodes",
            "venue": "Ann. Stat",
            "year": 2015
        },
        {
            "authors": [
                "A. Joseph",
                "B. Yu"
            ],
            "title": "Impact of regularization on spectral clustering",
            "venue": "Ann. Stat. 2016,",
            "year": 2016
        },
        {
            "authors": [
                "Y. Chen",
                "X. Li",
                "J. Xu"
            ],
            "title": "Convexified modularity maximization for degree-corrected stochastic block models",
            "venue": "Ann. Stat. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "F.S. Passino",
                "N.A. Heard"
            ],
            "title": "Bayesian estimation of the latent dimension and communities in stochastic blockmodels",
            "venue": "Stat. Comput",
            "year": 2020
        },
        {
            "authors": [
                "X. Li",
                "Y. Chen",
                "J. Xu"
            ],
            "title": "Convex Relaxation Methods for Community Detection",
            "venue": "Stat. Sci",
            "year": 2021
        },
        {
            "authors": [
                "B. Jing",
                "T. Li",
                "N. Ying",
                "X. Yu"
            ],
            "title": "Community detection in sparse networks using the symmetrized Laplacian inverse matrix (SLIM)",
            "year": 2022
        },
        {
            "authors": [
                "E. Abbe"
            ],
            "title": "Community detection and stochastic block models: Recent developments",
            "venue": "J. Mach. Learn. Res. 2017,",
            "year": 2017
        },
        {
            "authors": [
                "E.M. Airoldi",
                "D.M. Blei",
                "S.E. Fienberg",
                "E.P. Xing"
            ],
            "title": "Mixed Membership Stochastic Blockmodels",
            "venue": "J. Mach. Learn. Res. 2008,",
            "year": 2008
        },
        {
            "authors": [
                "B. Ball",
                "B. Karrer",
                "M.E.J. Newman"
            ],
            "title": "Efficient and principled method for detecting communities in networks",
            "venue": "Phys. Rev. E",
            "year": 2011
        },
        {
            "authors": [
                "F. Wang",
                "T. Li",
                "X. Wang",
                "S. Zhu",
                "C. Ding"
            ],
            "title": "Community discovery using nonnegative matrix factorization",
            "venue": "Data Min. Knowl. Discov",
            "year": 2011
        },
        {
            "authors": [
                "P.K. Gopalan",
                "D.M. Blei"
            ],
            "title": "Efficient discovery of overlapping communities in massive networks",
            "venue": "Proc. Natl. Acad. Sci. USA",
            "year": 2013
        },
        {
            "authors": [
                "E. Kaufmann",
                "T. Bonald",
                "M. Lelarge"
            ],
            "title": "A spectral algorithm with additive clustering for the recovery of overlapping communities in networks",
            "venue": "Theor. Comput. Sci",
            "year": 2017
        },
        {
            "authors": [
                "M. Panov",
                "K. Slavnov",
                "R. Ushakov"
            ],
            "title": "Consistent estimation of mixed memberships with successive projections",
            "venue": "In Proceedings of the International Conference on Complex Networks and their Applications, Lyon, France,",
            "year": 2017
        },
        {
            "authors": [
                "J. Jin",
                "Z.T. Ke",
                "S. Luo"
            ],
            "title": "Mixed membership estimation for social networks",
            "venue": "arXiv 2017,",
            "year": 2017
        },
        {
            "authors": [
                "X. Mao",
                "P. Sarkar",
                "D. Chakrabarti"
            ],
            "title": "On Mixed Memberships and Symmetric Nonnegative Matrix Factorizations",
            "venue": "In Proceedings of the International Conference on Machine Learning, Sydney, Australia,",
            "year": 2017
        },
        {
            "authors": [
                "X. Mao",
                "P. Sarkar",
                "D. Chakrabarti"
            ],
            "title": "Overlapping Clustering Models, and One (class) SVM to Bind Them All",
            "venue": "In Proceedings of the Advances in Neural Information Processing Systems, Montreal, QC, Canada,",
            "year": 2018
        },
        {
            "authors": [
                "X. Mao",
                "P. Sarkar",
                "D. Chakrabarti"
            ],
            "title": "Estimating Mixed Memberships With Sharp Eigenvector Deviations",
            "venue": "J. Am. Stat. Assoc",
            "year": 2020
        },
        {
            "authors": [
                "Y. Wang",
                "Z. Bu",
                "H. Yang",
                "H.J. Li",
                "J. Cao"
            ],
            "title": "An effective and scalable overlapping community detection approach: Integrating social identity model and game theory",
            "venue": "Appl. Math. Comput",
            "year": 2021
        },
        {
            "authors": [
                "Y. Zhang",
                "E. Levina",
                "J. Zhu"
            ],
            "title": "Detecting Overlapping Communities in Networks Using Spectral Methods",
            "venue": "SIAM J. Math. Data Sci. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "K. Rohe",
                "T. Qin",
                "B. Yu"
            ],
            "title": "Co-clustering directed graphs to discover asymmetries and directional communities",
            "venue": "Proc. Natl. Acad. Sci. USA",
            "year": 2016
        },
        {
            "authors": [
                "Z. Wang",
                "Y. Liang",
                "P. Ji"
            ],
            "title": "Spectral Algorithms for Community Detection in Directed Networks",
            "venue": "J. Mach. Learn. Res. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "P. Ji",
                "J. Jin"
            ],
            "title": "Coauthorship and citation networks for statisticians",
            "venue": "Ann. Appl. Stat. 2016,",
            "year": 2016
        },
        {
            "authors": [
                "Z. Zhou",
                "A.A. Amini"
            ],
            "title": "Analysis of spectral clustering algorithms for community detection: The general bipartite setting",
            "venue": "J. Mach. Learn. Res. 2019,",
            "year": 2019
        },
        {
            "authors": [
                "S. Laenen",
                "H. Sun"
            ],
            "title": "Higher-order spectral clustering of directed graphs",
            "venue": "Adv. Neural Inf. Process. Syst. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "H. Qing",
                "J. Wang"
            ],
            "title": "Directed mixed membership stochastic blockmodel",
            "venue": "arXiv 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Y.J. Wang",
                "G.Y. Wong"
            ],
            "title": "Stochastic Blockmodels for Directed Graphs",
            "venue": "J. Am. Stat. Assoc",
            "year": 1987
        },
        {
            "authors": [
                "G. Fagiolo"
            ],
            "title": "Clustering in complex directed networks",
            "venue": "Phys. Rev. E",
            "year": 2007
        },
        {
            "authors": [
                "E.A. Leicht",
                "M.E. Newman"
            ],
            "title": "Community structure in directed networks",
            "venue": "Phys. Rev. Lett",
            "year": 2008
        },
        {
            "authors": [
                "Y. Kim",
                "S.W. Son",
                "H. Jeong"
            ],
            "title": "Finding communities in directed networks",
            "venue": "Phys. Rev. E",
            "year": 2010
        },
        {
            "authors": [
                "F.D. Malliaros",
                "M. Vazirgiannis"
            ],
            "title": "Clustering and Community Detection in Directed Networks: A Survey",
            "venue": "Phys. Rep",
            "year": 2013
        },
        {
            "authors": [
                "X. Zhang",
                "B. Lian",
                "F.L. Lewis",
                "Y. Wan",
                "D. Cheng"
            ],
            "title": "Directed Graph Clustering Algorithms, Topology, and Weak Links",
            "venue": "IEEE Trans. Syst. Man, Cybern. Syst",
            "year": 2021
        },
        {
            "authors": [
                "J. Zhang",
                "J. Wang"
            ],
            "title": "Identifiability and parameter estimation of the overlapped stochastic co-block model",
            "venue": "Stat. Comput",
            "year": 2022
        },
        {
            "authors": [
                "L. Florescu",
                "W. Perkins"
            ],
            "title": "Spectral thresholds in the bipartite stochastic block model",
            "venue": "In Proceedings of the Conference on Learning Theory. PMLR,",
            "year": 2016
        },
        {
            "authors": [
                "S. Neumann"
            ],
            "title": "Bipartite stochastic block models with tiny clusters",
            "venue": "In 32nd Conference on Neural Information Processing Systems (NIPS 2018), Montre\u0301al, QC, Canada,",
            "year": 2018
        },
        {
            "authors": [
                "M. Ndaoud",
                "S. Sigalla",
                "A.B. Tsybakov"
            ],
            "title": "Improved clustering algorithms for the bipartite stochastic block model",
            "venue": "IEEE Trans. Inf. Theory 2021,",
            "year": 1960
        },
        {
            "authors": [
                "A.V. Mantzaris"
            ],
            "title": "Uncovering nodes that spread information between communities in social networks",
            "venue": "EPJ Data Sci",
            "year": 2014
        },
        {
            "authors": [
                "F. McSherry"
            ],
            "title": "Spectral partitioning of random graphs",
            "venue": "In Proceedings of the 42nd IEEE Symposium on Foundations of Computer",
            "year": 2001
        },
        {
            "authors": [
                "L. Massouli\u00e9"
            ],
            "title": "Community detection thresholds and the weak Ramanujan property",
            "venue": "In Proceedings of the Forty-Sixth Annual ACM Symposium on Theory of Computing,",
            "year": 2014
        },
        {
            "authors": [
                "E. Mossel",
                "J. Neeman",
                "A. Sly"
            ],
            "title": "Reconstruction and estimation in the planted partition model",
            "venue": "Probab. Theory Relat. Fields",
            "year": 2015
        },
        {
            "authors": [
                "E. Abbe",
                "A.S. Bandeira",
                "G. Hall"
            ],
            "title": "Exact recovery in the stochastic block model",
            "venue": "IEEE Trans. Inf. Theory",
            "year": 2015
        },
        {
            "authors": [
                "B. Hajek",
                "Y. Wu",
                "J. Xu"
            ],
            "title": "Achieving exact cluster recovery threshold via semidefinite programming",
            "venue": "IEEE Trans. Inf. Theory 2016,",
            "year": 2016
        },
        {
            "authors": [
                "E. Mossel",
                "J. Neeman",
                "A. Sly"
            ],
            "title": "A proof of the block model threshold conjecture",
            "venue": "Combinatorica 2018,",
            "year": 2018
        },
        {
            "authors": [
                "H. Qing"
            ],
            "title": "Studying Asymmetric Structure in Directed Networks by Overlapping and Non-Overlapping Models",
            "venue": "Entropy 2022,",
            "year": 2022
        },
        {
            "authors": [
                "N. Gillis",
                "S.A. Vavasis"
            ],
            "title": "Semidefinite Programming Based Preconditioning for More Robust Near-Separable Nonnegative Matrix Factorization",
            "venue": "SIAM J. Optim",
            "year": 2015
        },
        {
            "authors": [
                "H. Qing"
            ],
            "title": "A useful criterion on studying consistent estimation in community detection",
            "venue": "Entropy 2022,",
            "year": 2022
        },
        {
            "authors": [
                "U. Von Luxburg"
            ],
            "title": "A tutorial on spectral clustering",
            "venue": "Stat. Comput",
            "year": 2007
        },
        {
            "authors": [
                "Z.T. Ke",
                "J. Jin"
            ],
            "title": "The SCORE normalization, especially for highly heterogeneous network and text data",
            "venue": "arXiv 2022,",
            "year": 2022
        },
        {
            "authors": [
                "M.E. Newman"
            ],
            "title": "Modularity and community structure in networks",
            "venue": "Proc. Natl. Acad. Sci. USA",
            "year": 2006
        },
        {
            "authors": [
                "C.C. Chang",
                "C.J. Lin"
            ],
            "title": "LIBSVM: A library for support vector machines",
            "venue": "ACM Trans. Intell. Syst. Technol. (Tist) 2011,",
            "year": 2011
        },
        {
            "authors": [
                "R. Xu",
                "D. Wunsch"
            ],
            "title": "Survey of clustering algorithms",
            "venue": "IEEE Trans. Neural Netw",
            "year": 2005
        },
        {
            "authors": [
                "W.R. Palmer",
                "T. Zheng"
            ],
            "title": "Spectral clustering for directed networks",
            "venue": "In Proceedings of the International Conference on Complex Networks and Their Applications,",
            "year": 2020
        },
        {
            "authors": [
                "H. Qing"
            ],
            "title": "Degree-corrected distribution-free model for community detection in weighted networks",
            "venue": "Sci. Rep. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "P. Erd\u00f6s",
                "A. R\u00e9nyi"
            ],
            "title": "On the evolution of random graphs. In The Structure and Dynamics of Networks",
            "year": 2011
        },
        {
            "authors": [
                "Y. Chen",
                "Y. Chi",
                "J. Fan",
                "C. Ma"
            ],
            "title": "Spectral Methods for Data Science: A Statistical Perspective",
            "venue": "Found. Trends Mach. Learn",
            "year": 2021
        },
        {
            "authors": [
                "C.E. Shannon"
            ],
            "title": "A mathematical theory of communication",
            "venue": "Bell Syst. Tech. J",
            "year": 1948
        },
        {
            "authors": [
                "K.R. \u017dalik",
                "B. \u017dalik"
            ],
            "title": "Memetic algorithm using node entropy and partition entropy for community detection",
            "venue": "in networks. Inf. Sci",
            "year": 2018
        },
        {
            "authors": [
                "A. Feutrill",
                "M. Roughan"
            ],
            "title": "A review of Shannon and differential entropy rate estimation",
            "venue": "Entropy",
            "year": 2021
        },
        {
            "authors": [
                "L.A. Adamic",
                "N. Glance"
            ],
            "title": "The political blogosphere and the 2004 US election: Divided they blog",
            "venue": "In Proceedings of the 3rd International Workshop on Link Discovery, Chicago, IL, USA,",
            "year": 2005
        },
        {
            "authors": [
                "J. Kunegis"
            ],
            "title": "Konect: The koblenz network collection",
            "venue": "In Proceedings of the 22nd International Conference on World Wide Web, Rio de Janeiro, Brazil,",
            "year": 2013
        },
        {
            "authors": [
                "H. Zhang",
                "X. Guo",
                "X. Chang"
            ],
            "title": "Randomized spectral clustering in large-scale stochastic block models",
            "venue": "J. Comput. Graph. Stat",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Citation: Qing, H. Estimating Mixed\nMemberships in Directed Networks\nby Spectral Clustering . Entropy 2023,\n25, 345. https://doi.org/10.3390/\ne25020345\nAcademic Editors: Valentino\nSantucci, Marco Baioletti and Marco\nTomassini\nReceived: 29 December 2022\nRevised: 4 February 2023\nAccepted: 10 February 2023\nPublished: 13 February 2023\nCopyright: \u00a9 2023 by the author.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: community detection; directed networks; overlapping networks; spectral clustering"
        },
        {
            "heading": "1. Introduction",
            "text": "Many real-world complex networks have community structure such that nodes within the same community (also known as cluster or module) have more links than across communities. For example, in social networks, communities can be groups of students in the same department; in co-authorship networks, a community can be formed by researchers in the same field. However, community structure for a real-world network is usually not directly observable. To process this problem, community detection, also known as graph clustering, is a popular tool for uncovering a latent community structure in a network [1,2]. For decades, many community detection methods have been proposed for non-overlapping undirected networks in which each node belongs to a single community, and the interactions between two nodes are symmetric or undirected. The stochastic block model (SBM) [3] is a popular generative model for non-overlapping undirected networks. In SBM, it is assumed that each node only belongs to one community and that nodes in the same community have the same expectation degrees. Ref. [4] proposes the classical degree corrected stochastic block model (DCSBM) which extends SBM by considering variation in node degree. In recent years, numerous algorithms have been developed to estimate node community for non-overlapping undirected networks generated from SBM and DCSBM, see [5\u201315]. For recent developments about SBM, see the wonderful review paper [16].\nHowever, in most real-world networks, a node may belong to more than one community at a time. In recent years, the problem of estimating mixed memberships for the undirected network has received a lot of attention [17\u201329], and references therein. Ref. [17] extends the SBM model from non-overlapping undirected networks to mixed membership undirected networks and designs the mixed membership stochastic block (MMSB) model. Based on the MMSB model, ref. [24] designs a model called the degree corrected mixed membership (DCMM) model by considering degree heterogeneity, where DCMM can also be seen as an extension of the non-overlapping model DCSBM, and ref. [24] also develops an efficient and provably consistent spectral algorithm. Ref. [27] presents a spectral algorithm under MMSB and establishes per-node rates for mixed memberships by sharp\nEntropy 2023, 25, 345. https://doi.org/10.3390/e25020345 https://www.mdpi.com/journal/entropy\nrow-wise eigenvector deviation. Ref. [29] proposes an overlapping continuous community assignment model (OCCAM), which is also an extension of MMSB, by considering degree heterogeneity. To fit OCCAM, ref. [29] develops a spectral algorithm requiring a relatively small fraction of mixed nodes when building theoretical frameworks. Ref. [26] finds the cone structure inherent in the normalization of the eigen-decomposition of the population adjacency matrix under DCMM and develops a spectral algorithm to hunt corners in the cone structure.\nThough the above works are encouraging and appealing, they focus on undirected networks. In reality, there exist substantial directed networks, such as citation networks, protein\u2013protein interaction networks, and the hyperlink network of websites. In recent years, a lot of works with encouraging results have been developed for directed networks. Ref. [30] proposes a stochastic co-block model (ScBM) and its extension DC-ScBM by considering degree heterogeneity to model non-overlapping directed networks, where ScBM and DC-ScBM can model directed networks whose row nodes may be different from column nodes, and the number of row communities may also be different from the number of column communities. Ref. [31] studies the theoretical guarantees for the algorithm DSCORE [32] and its variants designed under DC-ScBM. Ref. [33] studies the spectral clustering algorithms designed by a data-driven regularization of the adjacency matrix under ScBM. Ref. [34] studies higher-order spectral clustering of directed graphs by designing a nearly linear time algorithm. Based on the fact that the above works only consider non-overlapping directed networks, ref. [35] develops a directed mixed membership stochastic block model (DiMMSB), which is an extension of ScBM, and models directed networks with mixed memberships. DiMMSB can also be seen as a direct extension of MMSB from an undirected network to a directed network. Recall that DCSBM, DCMM, and DCScBM are extensions of SBM, MMSB, and ScBM by considering node degree variation, respectively, this paper aims at proposing a model as an extension of DiMMSB by considering node degree heterogeneity and building an efficient spectral algorithm to fit the proposed model. In this paper, we focus on the directed network with mixed membership. Our contributions are as follows:\n(i) We propose a novel generative model for directed networks with a mixed membership, the directed degree corrected mixed membership (DiDCMM) model. DiDCMM models a directed network with mixed memberships when row nodes have degree heterogeneities, while column nodes do not. We present the identifiability of DiDCMM under popular conditions which are also required by models modeling mixed membership networks when considering degree heterogeneity. Meanwhile, our results also show that modeling a directed network with mixed membership when considering degree heterogeneity for both row and column nodes needs nontrivial conditions. DiDCMM can be seen as an extension of the DCScBM model from a non-overlapping directed network to an overlapping directed network. DiDCMM also extends the DCMM model from an undirected network to a directed network and extends the DiMMSB model by considering node degree heterogeneity. For a detailed comparison of our DiDCMM with previous models, see Remark 2. (ii) To fit DiDCMM, we present a spectral algorithm called DiMSC, which is designed based on the investigation that there exists an ideal cone structure inherent in the normalized version of the left singular vectors and an ideal simplex structure inherent in the right singular vectors of the population adjacency matrix. We prove that our DiMSC exactly recovers the membership matrices for both row and column nodes in the oracle case under DiDCMM, and this also supports the identifiability of DiDCMM. We obtain the upper bounds of error rates for each row (and column) node and show that our method produces asymptotically consistent parameter estimations under mild conditions. Our theoretical results are consistent with classical results when DiDCMM degenerates to SBM and MMSB under mild conditions. Numerical results of simulated directed networks support our theoretical results and show that our approach outperforms its competitors. We also apply our algorithm to several real-\nworld directed networks to test the existence of highly mixed nodes and asymmetric structures between row and column communities.\nNotations. We take the following general notations in this paper. For a vector x and fixed q > 0, \u2016x\u2016q denotes its lq-norm. For a matrix M, M\u2032 denotes the transpose of the matrix M, \u2016M\u2016 denotes the spectral norm, \u2016M\u2016F denotes the Frobenius norm, and \u2016M\u20162\u2192\u221e denotes the maximum l2-norm of all the rows of M. Let rank(M) denote the rank of matrix M. Let \u03c3i(M) be the i-th largest singular value of matrix M, and \u03bbi(M) denote the i-th largest eigenvalue of the matrix M ordered by the magnitude. M(i, :) and M(:, j) denote the i-th row and the j-th column of matrix M, respectively. M(Sr, :) and M(:, Sc) denote the rows and columns in the index sets Sr and Sc of matrix M, respectively. For any matrix M, we simply use Y = max(0, M) to represent Yij = max(0, Mij) for any i, j. For any matrix M \u2208 Rm\u00d7m, let diag(M) be the m\u00d7m diagonal matrix whose i-th diagonal entry is M(i, i). Here, 1 and 0 are column vectors with all entries being ones and zeros, respectively; ei is a column vector whose i-th entry is one, while other entries are zero. C is a positive constant that may vary occasionally."
        },
        {
            "heading": "2. The Directed Degree Corrected Mixed Membership Model",
            "text": "Consider a directed network N = (Vr,Vc, E), where Vr = {1, 2, . . . , nr} is the set of row nodes, Vc = {1, 2, . . . , nc} is the set of column nodes (nr and nc indicate the number of row nodes and the number of column nodes, respectively), and E is the set of edges. Note that when Vr = Vc such that row nodes are the same as column nodes, N is a traditional directed network [31,36\u201342]; when Vr 6= Vc, N is a bipartite network (also known as a bipartite graph) [30,33,35,43\u201345]; see Figure 1 for illustrations of the topological structures for a directed network and a bipartite network. Without confusion, we also call bipartite networks directed networks occasionally in this paper.\nWe assume that the row nodes of the directed network N belong to K perceivable communities (called row communities in this paper)\nC(1)r , C (2) r , . . . , C (K) r , (1)\nand the column nodes of the directed network N belong to K perceivable communities (called column communities in this paper)\nC(1)c , C (2) c , . . . , C (K) c . (2)\nDefine an nr \u00d7 K row nodes membership matrix \u03a0r and an nc \u00d7 K column nodes membership matrix \u03a0c such that \u03a0r(i, :) is a 1\u00d7 K probability mass function (PMF) for row node i, \u03a0c(j, :) is a 1\u00d7 K PMF for column node j, and\n\u03a0r(i, k) is the weight of row node i on C(k)r , 1 \u2264 k \u2264 K, (3)\n\u03a0c(j, k) is the weight of column node j on C(k)c , 1 \u2264 k \u2264 K. (4)\nCall row node i \u2018pure\u2019 if \u03a0r(i, :) is degenerate (i.e., one entry is 1, all other K \u2212 1 entries are 0) and \u2018mixed\u2019 otherwise. The same definitions hold for column nodes. Note that mixed nodes considered in this article are not the boundary nodes introduced in [46] since boundary nodes are defined based on non-overlapping networks, while mixed nodes belong to multiple communities.\nLet A \u2208 {0, 1}nr\u00d7nc be the bi-adjacency matrix ofN such that for each entry, A(i, j) = 1 if there is a directional edge from row node i to column node j, and A(i, j) = 0 otherwise. So, the i-th row of A records how row node i sends edges, and the j-th column of A records how column node j receives edges. Let P be a K\u00d7 K matrix such that\nP(k, l) \u2265 0 for 1 \u2264 k, l \u2264 K. (5)\nNote that since we consider a directed network in this paper, P may be asymmetric. Without loss of generality, suppose that row nodes have degree heterogeneities, while column nodes do not i.e., row nodes have variation in degree, while column nodes do not. Note that in a directed network, if column nodes have degree heterogeneities while row nodes do not, to detect memberships of both row nodes and column nodes, we set the transpose of the adjacency matrix as input when applying our algorithm DiMSC. Meanwhile, in a directed network, if both row and column nodes have degree heterogeneity, to model such a directed network with mixed memberships, we need nontrivial constraints on the degree heterogeneities between row nodes and column nodes for model identifiability, for detail, see Remark 1. Let \u03b8r be an nr \u00d7 1 vector whose i-th entry is the positive degree heterogeneity of row node i. For all pairs of (i, j) with 1 \u2264 i \u2264 nr, 1 \u2264 j \u2264 nc, DiDCMM models the entries of A such that A(i, j) are independent Bernoulli random variables satisfying\nP(A(i, j) = 1) = \u03b8r(i) K\n\u2211 k=1\nK\n\u2211 l=1 \u03a0r(i, k)\u03a0c(j, l)P(k, l). (6)\nEquation (6) means that P(A(i, j) = 1) = \u03b8r(i)\u03a0r(i, :)P\u03a0\u2032c(j, :), i.e., the probability of generating a directional edge from row node i to column node j is \u03b8r(i)\u03a0r(i, :)P\u03a0\u2032c(j, :), and this probability is controlled by the degree heterogeneity parameter \u03b8r(i) of row node i, the connecting matrix P, and the memberships of nodes i and j. Equation (6) functions similarly to Equation (1.4) in [24], and both equations define the probability of generating an edge. For comparison, Equation (6) defines the probability of generating a directional edge under DiDCMM for a directed network, while Equation (1.4) in [24] defines the probability of generating an edge under DCMM for an undirected network, i.e., DiDCMM can be seen as an extension of DCMM from an undirected network to a directed network.\nIntroduce the degree heterogeneity diagonal matrix \u0398r \u2208 Rnr\u00d7nr for row nodes such that\n\u0398r(i, i) = \u03b8r(i) for 1 \u2264 i \u2264 nr. (7)\nEquation (7) uses a diagonal matrix \u0398r to contain all degree heterogeneities, and \u0398r is useful for further theoretical analysis through Equation (8).\nDefinition 1. Call model (1)\u2013(6) the directed degree corrected mixed membership (DiDCMM) model, and denote it by DiDCMMnr ,nc(K, P, \u03a0r, \u03a0c, \u0398r).\nThe following conditions are sufficient for the identifiability of DiDCMM:\n\u2022 (I1) rank(P) = K, and P has unit diagonals. \u2022 (I2) There is at least one pure node for each of the K row and K column communities.\nWhen building statistical models for a network in which nodes can belong to multiple communities, the full rank requirement of connecting matrix P and pure nodes condition are always necessary for model identifiability, see models for an undirected network such as MMSB considered in [23,27], DCMM considered in [24,26], and OCCAM considered in [26,29]. Meanwhile, if models modeling networks with mixed memberships consider degree heterogeneity, the unit diagonals requirement on connecting matrix P is also necessary for model identifiability, see the identifiability requirement of DCMM and OCCAM considered in [24,26,29]. Furthermore, based on the fact that DiDCMM, DCMM, and OCCAM can include the well-known model SBM, letting P have unit diagonals is not a serious problem since many wonderful works study a special case of SBM when P has unit diagonals and a network has K equal size clusters (this special case of SBM is also known as a planted partition model), see [12,47\u201352].\nLet \u2126 = E[A] be the expectation of the adjacency matrix A. Under DiDCMM, we have\n\u2126 = \u0398r\u03a0rP\u03a0\u2032c. (8)\nWe refer to \u2126 as the population adjacency matrix. Since rank(\u0398r) = K, rank(\u03a0r) = K, rank(\u03a0c) = K and rank(P) = K by Equation (7) and Conditions (I1) and (I2), the rank of \u2126 is K. Recall that K is the number of communities, and it is much smaller than network size. We see that \u2126 has a low dimensional structure. The form of \u2126 given in Equation (8) is powerful to build the spectral algorithm developed in this paper to fit DiDCMM. Analyzing properties of the population adjacency matrix to build a spectral algorithm fitting statistical model is a common strategy in community detection, for example, references [24,26,27,35] also use this strategy to design their algorithms fitting DCMM, MMSB, and DiDCMM.\nFor 1 \u2264 k \u2264 K, let I (k)r = {i \u2208 {1, 2, . . . , nr} : \u03a0r(i, k) = 1} and I (k) c = {j \u2208 {1, 2, . . . , nc} : \u03a0c(j, k) = 1}. By Condition (I2), I(k)r and I (k) c are nonempty for all 1 \u2264 k \u2264 K. For 1 \u2264 k \u2264 K, select one row node from I (k)r to construct the index set Ir, i.e., Ir is the indices of row nodes corresponding to K pure row nodes, one from each community, and Ic is defined similarly. W.L.O.G., let \u03a0r(Ir, :) = IK and \u03a0c(Ic, :) = IK (Lemma 2.1 [27] has a similar setting to design their spectral algorithm under MMSB.), where IK is the K\u00d7 K identity matrix. The proposition below shows that the DiDCMM model is identifiable.\nProposition 1. (Identifiability). When Conditions (I1) and (I2) hold, DiDCMM is identifiable: for eligible (P, \u03a0r, \u03a0c, \u0398r) and (P\u0303, \u03a0\u0303r, \u03a0\u0303c, \u0398\u0303r), set \u2126 = \u0398r\u03a0rP\u03a0\u2032c and \u2126\u0303 = \u0398\u0303r\u03a0\u0303r P\u0303\u03a0\u0303\u2032c. If \u2126 = \u2126\u0303, then \u0398r = \u0398\u0303r, \u03a0r = \u03a0\u0303r, \u03a0c = \u03a0\u0303c and P = P\u0303.\nRemark 1. (The reason that we do not model a directed network with mixed memberships where both row and column nodes have degree heterogeneities). Suppose both row and column nodes have degree heterogeneities in a mixed membership directed network. To model such a directed network, the probability of generating an edge from row node i to column node j is\nP(A(i, j) = 1) = \u03b8r(i)\u03b8c(j) K\n\u2211 k=1\nK\n\u2211 l=1 \u03a0r(i, k)\u03a0c(j, l)P(k, l),\nwhere \u03b8c is an nr \u00d7 1 vector whose j-th entry is the degree heterogeneity of column node j. Set \u2126 = E[A], then \u2126 = \u0398r\u03a0rP\u03a0\u2032c\u0398c, where \u0398c \u2208 Rnc\u00d7nc is a diagonal matrix whose j-th diagonal entry \u03b8c(j). Set \u2126 = U\u039bV\u2032 as the compact SVD of \u2126. Follow similar analysis as Lemma 1, we see that U = \u0398r\u03a0rBr and V = \u0398c\u03a0cBc (without causing confusion, we still use Bc here for convenience.). For model identifiability, follow similar analysis as the proof of Proposition 1, since \u2126(Ir, Ic) = \u0398r(Ir, Ir)\u03a0r(Ir, ; )P\u03a0\u2032c(Ic, :)\u0398c(Ic, Ic) = \u0398r(Ir, Ir)P\u0398c(Ic, Ic) = U(Ir, :\n)\u039bV\u2032(Ic, :), we see that \u0398r(Ir, Ir)P\u0398c(Ic, Ic) = U(Ir, :)\u039bV\u2032(Ic, :). To obtain \u0398r(Ir, Ir) and \u0398c(Ic, Ic) from U(Ir, :)\u039bV\u2032(Ic, :), when P has unit diagonals, we see that it is impossible to recover \u0398r(Ir, Ir) and \u0398c(Ic, Ic) unless we add a condition that \u0398r(Ir, Ir) = \u0398c(Ic, Ic). Now, suppose \u0398r(Ir, Ir) = \u0398c(Ic, Ic) holds and call it Condition (I3); we have \u0398r(Ir, Ir)P\u0398r(Ir, Ir) = U(Ir, :)\u039bV\u2032(Ic, :); hence, \u0398r(Ir, Ir) = \u0398c(Ic, Ic) = \u221a diag(U(Ir, :)\u039bV\u2032(Ic, :)) when P has unit diagonals. However, Condition (I3) is nontrivial since it requires \u0398r(Ir, Ir) = \u0398c(Ic, Ic), and we always prefer a directed network in which there are no connections between row nodes degree heterogeneities and column nodes degree heterogeneities. For example, when all nodes are pure in a directed network, ref. [30] models such directed network using model DC-ScBM such that \u2126 = \u0398r\u03a0rP\u03a0\u2032c\u0398c when all nodes are pure, and \u0398r and \u0398c are independent under DC-ScBM. Because Condition (I3) is nontrivial, we do not model a mixed membership directed network with all nodes having degree heterogeneities.\nFor DiDCMM\u2019s identifiability, the number of row communities should equal that of column communities when both row and column nodes may belong to more than one community. However, when only row nodes have mixed memberships while column nodes do not, the number of row communities can be lesser than that of column communities, and this is also discussed in [53]. All proofs of our theoretical results are provided in the Appendix A.1. Unless specified, we treat Conditions (I1) and (I2) as default from now on. Proposition 1 is important since it guarantees that our model DiDCMM is well-defined, and we can design efficient spectral algorithms to fit DiDCMM based on its identifiability. The reason that we do not consider degree heterogeneity for column nodes for our DiDCMM is mainly for its identifiability. As analyzed in Remark 1, considering degree heterogeneity for both row and column nodes make the model unidentifiable unless adding some nontrivial conditions on model parameters. Meanwhile, many previous statistical models in the community detection areas are identifiable, and spectral algorithms can be applied to fit them. For examples, SBM [3], DCSBM [4], MMSB [17], DCMM [24], OCCAM [29], ScBM (and DCScBM), [30], and DiMMSB [35] are identifiable. Especially, though different statistical models may have different requirements on model parameters for identifiability, the proof of identifiability enjoys a similar idea as that of Proposition 1, for instance, Proposition 1.1 [24] and Theorem 2.1 [27] build theoretical guarantees on identifiability for DCMM and MMSB, respectively.\nRemark 2. We compare our DiDCMM with some previous models in this remark.\n\u2022 When \u0398r = \u03c1I for \u03c1 > 0, Equation (8) gives \u2126 = \u03c1\u03a0rP\u03a0\u2032c and DiDCMM degenerates to DiMMSB [35], where \u03c1 is known as a sparsity parameter [9,27,35]. So, DiDCMM includes DiMMSB as a special case, and the relationship between DiDCMM and DiMMSB is similar to that between DCSBM [3,4]. Meanwhile, DiDCMM considers degree heterogeneity parameter \u0398r at the cost that DiDCMM requires P to have unit diagonals for model identifiability, while there is no such requirement for P on DiMMSB\u2019s identifiability. Note that both DiDCMM and DiMMSB are identifiable only when P is a full-rank square matrix. \u2022 When \u0398r = \u03c1I for \u03c1 > 0 and all nodes are pure, DiDCMM reduces to ScBM [30]. DiDCMM can model a directed network in which nodes enjoy overlapping memberships, while ScBM cannot. Meanwhile, DiDCMM enjoys this advantage at the cost of requiring rank(P) = K for model identifiability, while ScBM is identifiable even when P is not a square matrix, i.e., ScBM can model a directed network in which the number of row communities can be different from the number of column communities. A comparison between DiDCMM and DCScBM [30] is similar. \u2022 When \u0398r = \u03c1I and the network is undirected, DiDCMM reduces to MMSB [17]. However, DiDCMM models directed networks with mixed memberships, while MMSB only models undirected networks with mixed memberships. Again, DiDCMM enjoys its advantage at the cost of P having unit diagonals for its identifiability (not that DiDCMM allows P to be asymmetric since DiDCMM models directed networks), while MMSB is identifiable even when\nP has non-unit diagonals (note that P is symmetric under MMSB since it models undirected networks). Meanwhile, the identifiability of both DiDCMM and MMSB requires the square matrix P to have full rank.\n\u2022 When \u0398r = \u03c1I, the network is undirected and all nodes are pure, DiDCMM reduces to SBM [3]. For comparison, DiDCMM models directed networks and allows nodes to belong to multiple communities, while SBM only models undirected networks in which a node only belongs to one community. Meanwhile, DiDCMM enjoys these advantages at the cost of requiring P to be full rank with unit diagonals for its identifiability, while SBM is identifiable even when P is not full rank and P has non-unit diagonals. Note that DiDCMM allows P to be asymmetric, while P must be symmetric for SBM since DiDCMM models directed networks, while SBM models undirected networks. Comparison between DiDCMM and DCSBM [4] is similar. \u2022 Compared with DCMM introduced in [24] and OCCAM introduced in [29], DCMM, and OCCAM model undirected networks with mixed memberships, while DiDCMM models directed networks with mixed memberships. DiDCMM, DCMM, and OCCAM all consider degree heterogeneity for overlapping networks, and they are identifiable only when the full rank matrix P has unit diagonals. These three models are identifiable only when the square matrix P is full rank. Meanwhile, DiDCMM allows P to be asymmetric, while P must be symmetric for DCMM and OCCAM since DiDCMM models directed networks, while DCMM and OCCAM model undirected networks."
        },
        {
            "heading": "3. Algorithm",
            "text": "The primary goal of the proposed algorithm is to estimate the row membership matrix \u03a0r and column membership matrix \u03a0c from the observed adjacency matrix A with given K. We start by considering the ideal case when \u2126 is known, and then we extend what we learn in the ideal case to the real case.\n3.1. The Ideal Simplex (IS), the Ideal Cone (IC), and the Ideal DiMSC\nRecall that rank(\u2126) = K under Conditions (I1) and (I2), and K is much smaller than min{nr, nc}. Let \u2126 = U\u039bV\u2032 be the compact singular value decomposition of \u2126 such that U \u2208 Rnr\u00d7K, \u039b \u2208 RK\u00d7K, V \u2208 Rnc\u00d7K, U\u2032U = IK, V\u2032V = IK. The goal of the ideal case is to use U, \u039b, and V to exactly recover \u03a0r and \u03a0c. As stated in [8,24], \u03b8r is one of the major nuisances, and similar to [7], we remove the effect of \u03b8r by normalizing each row of U to have a unit l2 norm. Set U\u2217 \u2208 Rnr\u00d7K by U\u2217(i, :) = U(i,:)\u2016U(i,:)\u2016F , and let NU be the nr \u00d7 nr diagonal matrix such that NU(i, i) = 1\u2016U(i,:)\u2016F for 1 \u2264 i \u2264 nr. Then, U\u2217 can be rewritten as U\u2217 = NUU. The existences of the ideal cone (IC for short) structure inherent in U\u2217 and the ideal simplex (IS for short) structure inherent in V are guaranteed by the following lemma.\nLemma 1. (Ideal Simplex and Ideal Cone). Under DiDCMMnr ,nc(K, P, \u03a0r, \u03a0c, \u0398r), there exist a unique K\u00d7 K matrix Br and a unique K\u00d7 K matrix Bc such that \u2022 U = \u0398r\u03a0rBr, where Br = \u0398\u22121r (Ir, Ir)U(Ir, :), and U\u2217 = YU\u2217(Ir, :) where\nY = NM\u03a0r\u0398\u22121r (Ir, Ir)N\u22121U (Ir, Ir) with NM being an nr \u00d7 nr diagonal matrix whose diagonal entries are positive. Meanwhile, U\u2217(i, :) = U\u2217(i\u0304, :) if \u03a0r(i, :) = \u03a0r(i\u0304, :) for 1 \u2264 i, i\u0304 \u2264 nr.\n\u2022 V = \u03a0cBc, where Bc = V(Ic, :). Meanwhile, V(j, :) = V( j\u0304, :) if \u03a0c(j, :) = \u03a0c( j\u0304, :) for 1 \u2264 j, j\u0304 \u2264 nc.\nLemma 1 says that the rows of V form a K-simplex in RK which we call the ideal simplex (IS), with the K rows of Bc being the vertices. Such IS is also found in [24,27,35]. Lemma 1 also shows that the form of U\u2217 = YU\u2217(Ir, :) is actually the ideal cone structure mentioned in [26]. Meanwhile, we remove the influence of \u03b8r by normalizing each row of U to have a unit norm in this paper. Using the idea of the entry-wise ratio in [8] also works, where ref. [24] develops their spectral algorithms to fit DCMM using the idea of\nentry-wise ratio. Designing algorithms based on the nonnegative matrix factorization [25] to fit DiDCMM by adding some constraints on \u2126 may also work. We leave the study of using these ideas to fit DiDCMM or its submodels for our future work.\nFor column nodes (recall that column nodes have no degree heterogeneities), since Bc is full rank if V and Bc are known in advance, ideally we can exactly recover \u03a0c by setting \u03a0c = VB\u2032c(BcB\u2032c)\u22121 \u2261 VB\u22121c . For convenience, to transfer the ideal case to the real case, set Zc = VB\u22121c . Since Zc \u2261 \u03a0c, we have\n\u03a0c(j, :) = Zc(j, :) \u2016Zc(j, :)\u20161 , 1 \u2264 j \u2264 nc.\nWith given V, since it enjoys IS structure V = \u03a0cBc \u2261 \u03a0cV(Ic, :), as long as we can obtain V(Ic, :) (i.e., Bc), we can recover \u03a0c exactly. As mentioned in [24,27], for such IS, the successive projection (SP) algorithm [54] (i.e., Algorithm A2 in the Appendix E) can be applied to V with K column communities to find the column corner matrix Bc. The above analysis gives how to recover \u03a0c with given \u2126 and K under DiDCMM ideally.\nNext, we aim to recover \u03a0r from U with the given K. Since rank(U\u2217) = K, rank(U\u2217(Ir, : )) = K. As U\u2217(Ir, :) \u2208 RK\u00d7K, the inverse of U\u2217(Ir, :) exists. Therefore, Lemma 1 also gives that\nY = U\u2217U\u22121\u2217 (Ir, :). (9)\nEquation (9) holds because U\u2217 = YU\u2217(Ir, :) and U\u2217(Ir, :) is a nonsingular matrix. By Lemma 1, we know that for row nodes, their membership matrix \u03a0r appears in the expression of Y. Therefore, we aim to use Equation (9) to find the exact expression of \u03a0r using U, V, and \u039b by putting Y at the left-hand side of equality. For our next step, we aim at finding \u03a0r using Equation (9). Since Y = NM\u03a0r\u0398\u22121r (Ir, Ir)N\u22121U (Ir, Ir) by Lemma 1 and U\u2217 = NUU, using NM\u03a0r\u0398\u22121r (Ir, Ir)N\u22121U (Ir, Ir) and NUU to replace Y and U\u2217 in Equation (9), respectively, we have N\u22121U NM\u03a0r\u0398 \u22121 r (Ir, Ir)N\u22121U (Ir, Ir) = UU\u22121\u2217 (Ir, :), which gives\nN\u22121U NM\u03a0r = UU \u22121 \u2217 (Ir, :)NU(Ir, Ir)\u0398r(Ir, Ir). (10)\nFrom Equation (10), we have found the expression of \u03a0r as a function of U, U\u2217, \u0398r, NU , and Ir, where we do not move N\u22121U NM to the right-hand side of Equation (10) because it is a diagonal matrix and does not influence the expression of \u03a0r, see our next step for details. When designing a spectral algorithm in the ideal case with given \u2126 and K, we aim at recovering \u03a0r and \u03a0c by taking advantage of the singular value decomposition of \u2126. We find that though Equation (10) provides an expression for \u03a0r by \u2126\u2019s SVD, there is a term \u0398r(Ir, Ir) which relates to degree heterogeneity, and we aim at expressing \u0398r(Ir, Ir) through \u2126\u2019s SVD. By the proof of Lemma 1, we know that \u0398r(Ir, Ir) = diag(U(Ir, : )\u039bV\u2032(Ic, :)) when Condition (I1) holds. Thus, substituting diag(U(Ir, :)\u039bV\u2032(Ic, :)) for \u0398r(Ir, Ir) in Equation (10), we obtain an expression of \u03a0r such that this expression is directly related to \u2126\u2019s SVD and two index set Ir and Ic. For convenience, set J\u2217 = NU(Ir, Ir)\u0398r(Ir, Ir) \u2261 diag(U\u2217(Ir, :)\u039bV\u2032(Ic, :)), Zr = N\u22121U NM\u03a0r, Y\u2217 = UU\u22121\u2217 (Ir, :). By Equation (10), we have\nZr = Y\u2217 J\u2217 \u2261 UU\u22121\u2217 (Ir, :)diag(U\u2217(Ir, :)\u039bV\u2032(Ic, :)). (11)\nEquation (11) looks similar to Equation (7) of [55]. However, Equation (11) is related to two index sets Ir and Ic, while Equation (7) of [55] is only related to one index set because Equation (11) aims at designing a spectral algorithm for directed network generated under DiDCMM and Equation (7) of [55] aims at reviewing the generation of the SVM-cone-\nDCMMSB algorithm proposed in [26] for undirected network generated under DCMM. Meanwhile, since N\u22121U NM is an nr \u00d7 nr positive diagonal matrix, we have\n\u03a0r(i, :) = Zr(i, :) \u2016Zr(i, :)\u20161 , 1 \u2264 i \u2264 nr. (12)\nWith given \u2126 and K, we can obtain U, V; thus, the above analysis shows that once the two index sets Ir and Ic are known, we can exactly recover \u03a0r by Equations (11) and (12). Meanwhile, from Equation (10), we see that it is important to express \u0398r(Ir, Ir) as a combination of U, V, \u039b, and the two index sets Ir and Ic, where we successfully obtain an expression of \u0398r(Ir, Ir) by Condition (I1), the unit diagonal constraint on P. Otherwise, if P has no unit diagonals, we cannot obtain an expression of \u0398r(Ir, Ir) unless adding some nontrivial conditions on model parameters, just as analyzed in Remark 1. Similarly, references [24,26] also design their spectral algorithms to fit DCMM by using the unit diagonal constraint on P to obtain an expression of a sub-matrix of degree heterogeneity matrix, see Equations (6)\u2013(8) of [55] as an example. Given \u2126 and K, to recover \u03a0r in the ideal case, we need to obtain Zr by Equation (11), which means that the only difficulty is in finding the index set Ir since V(Ic, :) can be obtained by SP algorithm from the IS structure V = \u03a0cV(Ic, :). From Lemma 1, we know that U\u2217 = YU\u2217(Ir, :) forms the IC structure. In [26], their SVM-cone algorithm (i.e., Algorithm A3 in the Appendix F) can exactly obtain the row nodes corner matrix U\u2217(Ir, :) from the ideal cone U\u2217 = YU\u2217(Ir, :) as long as the Condition (U\u2217(Ir, :)U\u2032\u2217(Ir, :))\u221211 > 0 holds (see Lemma 2).\nLemma 2. Under DiDCMMnr ,nc(K, P, \u03a0r, \u03a0c, \u0398r), (U\u2217(Ir, :)U\u2032\u2217(Ir, :))\u221211 > 0 holds.\nBased on the above analysis, we are now ready to give the following four-stage algorithm which we call ideal DiMSC. Input \u2126, K. Output: \u03a0r and \u03a0c. \u2022 Let \u2126 = U\u039bV\u2032 be the compact SVD of \u2126 such that U \u2208 Rnr\u00d7K, V \u2208 Rnc\u00d7K, \u039b \u2208 RK\u00d7K, U\u2032U = I, V\u2032V = I. Let U\u2217 = NUU, where NU is an nr \u00d7 nr diagonal matrix whose i-th diagonal entry is 1\u2016U(i,:)\u2016F for 1 \u2264 i \u2264 nr. \u2022 Run the SP algorithm on V assuming that there are K column communities to obtain the column corner matrix V(Ic, :) (i.e.,Bc). Run the SVM-cone algorithm on U\u2217 assuming that there are K row communities to obtain Ir. \u2022 Set J\u2217 = diag(U\u2217(Ir, :)\u039bV\u2032(Ic, :)), Y\u2217 = UU\u22121\u2217 (Ir, :), Zr = Y\u2217 J\u2217 and Zc = VV\u22121(Ic, :). \u2022 Recover \u03a0r and \u03a0c by setting \u03a0r(i, :) =\nZr(i,:) \u2016Zr(i,:)\u20161 for 1 \u2264 i \u2264 nr, and \u03a0c(j, :) = Zc(j,:)\u2016Zc(j,:)\u20161 for 1 \u2264 j \u2264 nc.\nThe following theorem guarantees that ideal DiMSC exactly recovers nodes memberships, and this verifies the identifiability of DiDCMM in turn. Meanwhile, it should be noted that many spectral algorithms designed to fit identifiable statistical models in the community detection area can exactly recover node memberships for the ideal case. For example, the spectral clustering for K many clusters algorithm addressed in [5] under SBM, the regularized spectral clustering designed in [7] under DCSBM, the SCORE algorithm designed in [8] under DCSBM, the two algorithms designed and studied in [9] under SBM and DCSBM, the RSC-\u03c4 algorithm studied in [11] under SBM, the mixed-SCORE algorithm designed in [24] under DCMM, the DI-SIM algorithm designed in [30] under DCScBM, the D-SCORE algorithm studied in [31,32] under DCScBM, the SVM-cone-DCMMSB algorithm designed in [26] under DCMM, and the SPACL algorithm designed in [27] under MMSB can exactly recover membership matrices under respective models for the ideal case by using the population adjacency matrix to replace the adjacency matrix in the input of these algorithms. The fact that ideal cases for the above spectral algorithms can return community information also supports the identifiability of the above models.\nTheorem 1. Under DiDCMMnr ,nc(K, P, \u03a0r, \u03a0c, \u0398r), the ideal DiMSC exactly recovers the row nodes membership matrix \u03a0r and the column nodes membership matrix \u03a0c.\nTo demonstrate that U\u2217 has the ideal cone structure, we drew Panel (a) of Figure 2. The simulated data used for Panel (a) is generated from DiDCMMnr ,nc(K, P, \u03a0r, \u03a0c, \u0398r) with nr = 600, nc = 400, K = 3; each row (and column) community has 120 pure nodes. For the 240 mixed row nodes, we set \u03a0r(i, 1) = rand(1)/2, \u03a0r(i, 2) = rand(1)/2, \u03a0r(i, 3) = 1\u2212\u03a0r(j, 1)\u2212\u03a0r(j, 2), where rand(1) is any random number in (0, 1), and i is a mixed row node. For the 40 mixed column nodes, set \u03a0c(j, 1) = rand(1)/2, \u03a0c(j, 2) = rand(1)/2, \u03a0c(j, 3) = 1 \u2212 \u03a0c(j, 1) \u2212 \u03a0c(j, 2). For the degree heterogeneity parameter, set \u03b8r(i) = rand(1) for all row nodes i. The matrix P is set as\nP =  1 0.4 0.30.2 1 0.1 0.1 0.4 1 . Under such a setting, after computing \u2126 and obtaining U\u2217, V from \u2126, we can plot Figure 2. Panel (a) shows that all rows respective to mixed row nodes of U\u2217 are located at one side of the hyperplane formed by the K rows of U\u2217(Ir, :), and this phenomenon occurs since each row of U\u2217 is a scaled convex combination of the K rows of U\u2217(Ir, :) guaranteed by the IC structure U\u2217 = YU\u2217(Ir, ; ). Thus Panel (a) shows the existence of the ideal cone structure formed by U\u2217. Similarly, to demonstrate that V has the ideal simplex structure, we drew Panel (b) of Figure 2, where Panel (b) is obtained under the same setting as Panel (a). Panel (b) shows that rows respective to mixed column nodes of V are located inside of the simplex formed by the K rows of V(Ic, :), and this phenomenon occurs since each row of V is a convex linear combination of the K rows of V(Ic, :) guaranteed by the IS structure V = \u03a0cV(Ic, ; ). Thus Panel (b) shows the existence of the ideal simplex structure formed by V.\n3.2. Dimsc Algorithm\nWe now extend the ideal case to the real case. Set A\u0303 = U\u0302\u039b\u0302V\u0302\u2032 to be the top-Kdimensional SVD of A such that U\u0302 \u2208 Rnr\u00d7K, V\u0302 \u2208 Rnc\u00d7K, \u039b\u0302 \u2208 RK\u00d7K, U\u0302\u2032U\u0302 = IK, V\u0302\u2032V\u0302 = IK, and \u039b\u0302 contains the top K singular values of A. Let U\u0302\u2217 be the row-wise normalization of U\u0302 such that U\u0302 = NU\u0302U\u0302, where NU\u0302 \u2208 Rnr\u00d7nr is a diagonal matrix whose i-th diagonal entry is 1\u2016U\u0302(i,:)\u2016F . For the real case, we use J\u0302\u2217, Y\u0302\u2217, Z\u0302r, Z\u0302c, \u03a0\u0302r, \u03a0\u0302c given in Algorithm 1 to estimate\nJ\u2217, Y\u2217, Zr, Zc, \u03a0r, \u03a0c, respectively. Algorithm 1 called directed mixed simplex and cone (DiMSC for short) algorithm is a natural extension of the ideal DiMSC to the real case.\nAlgorithm 1 Directed Mixed Simplex and Cone (DiMSC) algorithm\nRequire: The adjacency matrix A \u2208 Rnr\u00d7nc of a directed network, the number of row (column) communities K. Ensure: The estimated nr\u00d7K row membership matrix \u03a0\u0302r and the estimated nc\u00d7K column membership matrix \u03a0\u0302c.\n1: Obtain A\u0303 = U\u0302\u039b\u0302V\u0302\u2032, the top-K-dimensional SVD of A. Compute U\u0302\u2217 from U\u0302. 2: Apply SP algorithm (i.e., Algorithm A2) on the rows of V\u0302 assuming there are K column\ncommunities to obtain I\u0302c, the index set returned by SP algorithm. 3: Similarly, apply SVM-cone algorithm (i.e., Algorithm A3) on the rows of U\u0302\u2217 with K row\ncommunities to obtain I\u0302r, the index set returned by SVM-cone algorithm. 4: Set J\u0302\u2217 = diag(U\u0302\u2217( I\u0302r, :)\u039b\u0302V\u0302\u2032(I\u0302c, :)), Y\u0302\u2217 = U\u0302U\u0302\u22121\u2217 (I\u0302r, :), Z\u0302r = Y\u0302\u2217 J\u0302\u2217 and Z\u0302c = V\u0302V\u0302\u22121(I\u0302c, :).\nThen, set Z\u0302r = max(0, Z\u0302r) and Z\u0302c = max(0, Z\u0302c). 5: Estimate \u03a0r(i, :) by \u03a0\u0302r(i, :) = Z\u0302r(i, :)/\u2016Z\u0302r(i, :)\u20161, 1 \u2264 i \u2264 nr and estimate \u03a0c(j, :) by\n\u03a0\u0302c(j, :) = Z\u0302c(j, :)/\u2016Z\u0302c(j, :)\u20161, 1 \u2264 j \u2264 nc.\nIn the third step, we set the negative entries of Z\u0302r as 0 by setting Z\u0302r = max(0, Z\u0302r) for the reason that weights for any row node should be nonnegative, while there may exist some negative entries of Y\u0302\u2217 J\u0302\u2217. A similar argument holds for Z\u0302c. The flowchart of DiMSC is displayed in Figure 3. Meanwhile, in community detection, researchers often use top-Kdimensional SVD of A or its variants such as Laplacian matrix or regularized Laplacian matrix to design their spectral clustering algorithms to fit identifiable statistical models such as spectral methods designed or studied in [5,7\u20139,11,24,26,27,29,31,33,35,56]. Furthermore, as discussed in [57], the SVS+ and SVS\u2217 algorithms may be used as substitutions of the SP algorithm in our DiMSC for a better estimation of \u03a0r. When applying the entry-wise normalization idea developed in [8] to deal with U, as analyzed in [24], we obtain a simplex structure, and we can use the SP algorithm (or the combinatorial vertex search and sketched vertex search approaches developed in [24]) to hunt for the corners. The above ideas suggest that we can design different spectral algorithms to fit our model DiDCMM. We leave them for our future work. In particular, in this paper, we apply the SVM-cone algorithm to hunt for the corners of the cone structure inherent in U\u2217 mainly for the theoretical convenience of the SVM-cone algorithm because ref. [26] has developed a nice theoretical framework on the performance for the SVM-cone algorithm.\n3.3. Computational Complexity\nThe computing cost of DiMSC mainly comes from SVD, SP, and SVM-cone. The computational complexity of SVD is O(max(nr, nc)min(n2r , n2c )). Since the adjacency matrix A for real-world network data sets is usually sparse, using the power method discussed in [58], the computation complexity for obtaining the top-K-dimensional SVD of A is only slightly larger than O(max(n2r , n2c )K) [8,24]. The SP algorithm step in DiMSC has a complexity of O(max(nr, nc)K2) [24]. The complexity of the one-class SVM step for SVM-cone algorithm is O(max(nr, nc)K2) [26,59]. The complexity of the K-means step for SVM-cone algorithm is O(max(nr, nc)K2) [60]. Since the number of communities K considered in this paper is much smaller than the network size, the total complexity of DiMSC is O(max(n2r , n2c )K). Results in Section 5 show that, for a computer-generated network with 15,000 nodes un-\nder SBM, DiMSC takes hundreds of seconds to process a standard personal computer (Thinkpad X1 Carbon Gen 8) using MATLAB R2021b. Meanwhile, many spectral methods developed under models SBM, DCSBM, MMSB, ScBM, DCScBM, OCCAM, DCMM, and DiMMSB for community detection also have complexity O(max(n2r , n2c )K), see spectral algorithms designed or studied in [5,7\u20139,11,24,26,27,29\u201331,33,35,61,62]. Researchers design spectral algorithms for community detection under various identifiable statistical models mainly for their convenience on building a theoretical guarantee of consistent estimation, and we also provide a theoretical guarantee on DiMSC\u2019s estimation consistency in next section."
        },
        {
            "heading": "4. Consistency Results",
            "text": "In this section, we show the consistency of our algorithm for fitting the DiDCMM by proving that the sample-based estimates \u03a0\u0302r and \u03a0\u0302c concentrate around the true mixed membership matrices \u03a0r and \u03a0c. Throughout this paper, K is a known positive integer. Set \u03b8r,max = max1\u2264i\u2264nr \u03b8r(i) and \u03b8r,min = min1\u2264i\u2264nr \u03b8r(i). Assume that\nAssumption 1. Pmaxmax(\u2016\u03b8r\u20161, \u03b8r,maxnc) \u2265 log(nr + nc).\nAssumption 1 means that the network cannot be too sparse, and it also means that we allow \u03b8r,max to go to zero with increasing numbers of row nodes and column nodes. When building theoretical guarantees on consistent estimation, controlling network sparsity is popular in the community detection area. For examples, Condition (2.9) of [8], Theorem 3.1 of [9], Condition (2.13) of [24], Assumption 3.1 of [27], and Assumption 2 of [31] all control network sparsity for their theoretical analysis. Especially, when DiDCMM reduces to SBM by letting \u0398r = \u03c1I, n = nr = nc, \u03a0r = \u03a0c, and all nodes are pure for \u03c1 > 0, Assumption 1 requires that \u03c1n log(n), which is consistent with the sparsity requirement in [8,9,24,31]. As analyzed in [55], we know that our requirement on network sparsity is optimal since it matches the sharp threshold of obtaining a connected Erd\u00f6s\u2013R\u00e9nyi (ER) random graph [63] when SBM reduces to an ER random graph by letting K = 1. For notation convenience, set v = max(\u2016U\u0302U\u0302\u2032 \u2212 UU\u2032\u20162\u2192\u221e, \u2016V\u0302V\u0302\u2032 \u2212 VV\u2032\u20162\u2192\u221e), f\u0302r = max1\u2264i\u2264nr\u2016e\u2032i(\u03a0\u0302r \u2212\u03a0rPr)\u20161, f\u0302c = max1\u2264j\u2264nc\u2016e\u2032j(\u03a0\u0302c \u2212\u03a0cPc)\u20161, and \u03c0r,min = min1\u2264k\u2264K 1\u2032\u03a0rek, where v is the row-wise singular vector deviation which can be bounded by Theorem 4.4 of [64], f\u0302r and f\u0302c measures per node clustering error of DiMSC, and \u03c0r,min measures the minimum summation of row nodes belonging to a certain row community. Increasing \u03c0r,min makes the network tend to be more balanced and vice versa. Meanwhile, row-wise singular vector deviation is important when building a theoretical guarantee of spectral methods fitting models for a network with mixed memberships, for example, refs. [24,26,27,35] also consider v when building consistent estimation for their spectral methods. The next theorem gives theoretical bounds on estimations of memberships for both row and column nodes, which is the main theoretical result for our DiMSC method.\nTheorem 2. Under DiDCMMnr ,nc(K, P, \u03a0r, \u03a0c, \u0398r), let \u03a0\u0302r and \u03a0\u0302c be obtained from Algorithm 1, when Assumption 1 holds, suppose \u03c3K(\u2126) \u2265 C \u221a \u03b8r,maxPmax(nr + nc)log(nr + nc), with probability at least 1\u2212 o((nr + nc)\u22123), we have\nf\u0302r = O( K5.5\u03b815r,maxv\u03ba4.5(\u03a0\u2032r\u03a0r)\u03ba(\u03a0c)\u03bb1.51 (\u03a0 \u2032 r\u03a0r)\n\u03b815r,min\u03c0r,min ), f\u0302c = O(vK\u03ba(\u03a0\u2032c\u03a0c)\n\u221a \u03bb1(\u03a0\u2032c\u03a0c)).\nIn Theorem 2, the Condition \u03c3K(\u2126) \u2265 C \u221a\n\u03b8r,maxPmax(nr + nc)log(nr + nc) is necessary when applying Theorem 4.4 [64] to obtain a theoretical upper bound of v. When building a theoretical guarantee on estimation consistency for spectral methods fitting models modeling network with mixed memberships, it is necessary to have a lower bound requirement on \u03c3K(\u2126), see [24,26,27,35]. Actually, this requirement matches with the con-\nsistent requirement on \u03c3K(P)\u221aPmax obtained from the theoretical upper bound of error rates for a balanced network, see Remark 4 for details. Meanwhile, similar to [7,11,30], we can design a spectral algorithm via an application of regularized Laplacian matrix to fit DiDCMM.\nThe following corollary is obtained by adding conditions on model parameters similar to Corollary 3.1 in [27], where these conditions give a directed network in which each community has the same order of size, and each node has the same order of degree, i.e., a balanced network.\nCorollary 1. Under DiDCMMnr ,nc(K, P, \u03a0r, \u03a0c, \u0398r), when conditions of Theorem 2 hold, suppose \u03bbK(\u03a0\u2032r\u03a0r) = O( nr K ), \u03bbK(\u03a0 \u2032 c\u03a0c) = O( nc K ), \u03c0r,min = O( nr K ) and K = O(1), with probability at least 1\u2212 o((nr + nc)\u22123), we have\nf\u0302r = O(( \u03b8r,max \u03b8r,min )15.5 1 \u03c3K(P)\n\u221a Pmaxlog(nr + nc)\n\u03b8r,minnc ), f\u0302c = O(( \u03b8r,max \u03b8r,min )0.5 1 \u03c3K(P)\n\u221a Pmaxlog(nr + nc)\n\u03b8r,minnr ).\nMeanwhile, \u2022 when \u03b8r,max = O(\u03c1), \u03b8r,min = O(\u03c1) (i.e., \u03b8r,min \u03b8r,max = O(1)), we have\nf\u0302r = O( 1\n\u03c3K(P)\n\u221a Pmaxlog(nr + nc)\n\u03c1nc ), f\u0302c = O( 1 \u03c3K(P)\n\u221a Pmaxlog(nr + nc)\n\u03c1nr ).\n\u2022 when nr = O(n), nc = O(n) and \u03b8r,max = O(\u03c1), \u03b8r,min = O(\u03c1), we have\nf\u0302r = O( 1\n\u03c3K(P)\n\u221a Pmaxlog(n)\n\u03c1n ), f\u0302c = O( 1 \u03c3K(P)\n\u221a Pmaxlog(n)\n\u03c1n ).\nConsider a directed mixed membership network under the settings of Corollary 1 when \u03b8r,max = O(\u03c1), \u03b8r,min = O(\u03c1) for \u03c1 > 0, to obtain consistent estimations for both row\nnodes and column nodes, by Corollary 1, \u03c3K(P)\u221aPmax should shrink slower than \u221a log(nr+nc) \u03c1min(nr ,nc) , where consistent estimation means that the theoretical upper bound of error rate goes to zero when increasing network size. Especially, when nr = O(n) and nc = O(n),\n\u03c3K(P)\u221a Pmax should shrink slower than \u221a\nlog(n) n . We further assume that P = (2\u2212 \u03b2)IK + (\u03b2\u2212 1)11 \u2032\nfor \u03b2 \u2208 [1, 2) \u222a (2, \u221e) and let P\u0303 = \u03c1P (note that for this P, we have \u03c3K(P) = |\u03b2\u2212 2| and Pmax = max(1, \u03b2\u2212 1)). So the diagonal elements for P\u0303 are \u03c1 and non-diagonal elements are \u03c1(\u03b2\u2212 1). Set pin as the diagonal entries of P\u0303, and pout as the non-diagonal entries of P\u0303, we have pin = \u03c1 , pout = \u03c1(\u03b2\u2212 1), and |pin\u2212pout|\u221amax(pin,pout) = \u221a \u03c1|\u03b2\u22122|\u221a max(1,\u03b2\u22121) = \u221a \u03c1\u03c3K(P)\u221a Pmax . Hence, for\nconsistent estimation, we see that |pin\u2212pout|\u221a max(pin,pout)\nshould shrink slower than \u221a\nlog(nr+nc) min(nr ,nc) by Corollary 1 and should shrink slower than \u221a\nlog(n) n when nr = O(n) and nc = O(n), where\nthis result is consistent with classical separation condition for a standard network with two equal-sized clusters by applying the separation condition and sharp threshold criterion developed in [55].\nRemark 3. When the network is undirected (i.e., nr = nc = n, \u03a0r = \u03a0c) with K = O(1) by setting \u03b8r(i) = \u03c1 for 1 \u2264 i \u2264 nr, DiDCMM degenerates to MMSB considered in [27], the upper bound of error rate for DiMSC is O( 1\n\u03c3K(P)\n\u221a log(n)\n\u03c1n ) when Pmax = 1. Replacing the \u0398 in [24] by \u0398 = \u221a \u03c1I, their DCMM model degenerates to MMSB. Then, their conditions in Theorem 2.2 are our Assumption 1 and \u03bbK(\u03a0\u2032\u03a0) = O( nK ), where \u03a0 = \u03a0r = \u03a0c for MMSB. When K = O(1), the error bound in Theorem 2.2 in [24] is O( 1 \u03c3K(P) \u221a log(n) \u03c1n ), which is consistent with ours.\nRemark 4. By Lemma A5 in the Appendix D, we know \u03c3K(\u2126) \u2265 \u03b8r,min\u03c3K(P)\u03c3K(\u03a0r)\u03c3K(\u03a0c). To ensure the Condition \u03c3K(\u2126) \u2265 C(\u03b8r,maxPmax(nr + nc)log(nr + nc))1/2 in Theorem 2 holds, we need\n\u03c3K(P)\u221a Pmax\n\u2265 C ( \u03b8r,max(nr + nc)log(nr + nc)\n\u03b82r,min\u03bbK(\u03a0 \u2032 r\u03a0r)\u03bbK(\u03a0\u2032c\u03a0c) )1/2. (13) When K = O(1), nr = O(n), nc = O(n), \u03bbK(\u03a0\u2032r\u03a0r) = O( nr K ), \u03bbK(\u03a0 \u2032 c\u03a0c) = O( nc K ) and \u03b8r,max = O(\u03c1), \u03b8r,min = O(\u03c1), Equation (13) gives that \u03c3K(P)\u221a\nPmax should shrink slower than\n\u221a log(n)\n\u03c1n ,\nwhich matches with the consistency requirement on \u03c3K(P)\u221aPmax of Corollary 1.\nFor convenience, we need the following definition.\nDefinition 2. Let DiDCMM(n, K, \u03a0r, \u03a0c, \u03b1in, \u03b1out) be a special case of DiMMDFnr ,nc(K, P, \u03a0r, \u03a0c, \u0398r) when \u0398r = \u03c1I, nr = nc = n, \u03bbK(\u03a0\u2032r\u03a0r) = O(n/K), \u03bbK(\u03a0\u2032c\u03a0c) = O(n/K), \u03c0r,min = O(n/K), K = O(1), and P\u0303 = \u03c1P has diagonal entries pin = \u03b1in log(n) n and non-diagonal entries pout = \u03b1out log(n)\nn .\nDiDCMM(n, K, \u03a0r, \u03a0c, \u03b1in, \u03b1out) denotes a special directed network such that row communities have nearly equal sizes since \u03bbK(\u03a0\u2032r\u03a0r) = O(n/K), and column communities also have nearly equal sizes. By Corollary 1, for consistent estimation, we need |pin\u2212pout|\u221a max(pin,pout) \u221a log(n) n under DiDCMM(n, K, \u03a0r, \u03a0c, \u03b1in, \u03b1out). Since |pin\u2212pout|\u221a max(pin,pout) =\n|\u03b1in\u2212\u03b1out| \u221a\nlog(n) n\u221a\nmax(\u03b1in,\u03b1out) , for consistent estimation, we need\n|\u03b1in \u2212 \u03b1out|\u221a max(\u03b1in, \u03b1out)\n1 (14)\nOur numerical results in Section 5 support that DiMSC can estimate memberships for both row and column nodes when the threshold |\u03b1in\u2212\u03b1out|\u221a\nmax(\u03b1in,\u03b1out) 1 holds under DiDCMM(n, K,\n\u03a0r, \u03a0c, \u03b1in, \u03b1out).\nRemark 5. When K = 2, the network is undirected (i.e., \u03a0r = \u03a0c), all nodes are pure, and each community has an equal size, DiDCMM(n, K, \u03a0r, \u03a0c, \u03b1in, \u03b1out) reduces to the SBM case such that nodes connect with probability pin within clusters and pout across clusters. This case has been well studied in recent years, see [50] and references therein. Especially, for this case, ref. [50] finds that exact recovery is possible if |\u221a\u03b1in \u2212 \u221a \u03b1out| > \u221a 2 and impossible if |\u221a\u03b1in \u2212 \u221a \u03b1out| < \u221a 2. For convenience, we use SBM(n, pin, pout) to denote this case. Our numerical results in Section 5 show that DiMSC return consistent estimation under SBM(n, pin, pout) when \u03b1in and \u03b1out are set in the impossible region of exact recovery but satisfy Equation (14).\nRemark 6. In information theory, Shannon entropy [65] quantifies the amount of information in a variable, and it is a measure of uncertainty information of a probability distribution. We use a node membership entropy (NME) derived from Shannon theory to measure the node\u2019s uncertainty about the node and all communities [66,67]. For row node i with membership \u03a0r(i, :), since \u2211Kk=1 \u03a0r(i, k) = 1 and \u03a0r(i, k) can be seen as the probability that row node i belongs to row cluster k for 1 \u2264 k \u2264 K, NME of row node i is the Shannon entropy related to \u03a0r(i, :):\nNME(i) = \u2212 K\n\u2211 k=1 \u03a0r(i, k)log(\u03a0r(i, k)). (15)\nFor column node j with membership \u03a0c(j, :), we can also obtain its NME by Equation (15). In particular, if a node belongs to each cluster with equal probability 1K , its NME is log(K) which is\nthe maximum among all NME; if a node belongs to two clusters with equal probability 12 , its NME is log(2) which is less than log(K) when K \u2265 3. Generally, we see that recovering memberships for mixed nodes is harder than for pure nodes since NME is 0 for pure nodes, while NME is larger than 0 for mixed nodes by the definition of NME."
        },
        {
            "heading": "5. Simulations",
            "text": "In this section, several experiments are conducted to investigate the performance of our DiMSC under DiDCMM. We compare our DiMSC with three model-based methods that can be thought of as special cases of our model DiDCMM. Model-based methods we compare include the DISIM algorithm proposed in [30], the DSCORE algorithm studied in [31], and the DiPCA algorithm which is obtained by using the adjacency matrix A to replace the regularized graph Laplacian matrix in the DISIM algorithm. Similar to [24,27], for simulations, we measure the errors for the inferred community membership matrices instead of simply each node. We measure the performance of DiMSC and its competitors by the mixed Hamming error rate (MHamm for short) defined below\nMHamm = max( minP\u2208SP \u2016\u03a0\u0302rP \u2212\u03a0r\u20161\nnr , minP\u2208SP \u2016\u03a0\u0302cP \u2212\u03a0c\u20161 nc ), (16)\nwhere SP is the set of K\u00d7 K permutation matrices. For all simulations in this section, unless specified, we set the parameters (nr, nc, K, P, \u03a0r, \u03a0c, \u0398r) under DiDCMM as follows: let each row community and each column community have n0 pure nodes; let all mixed row nodes (and mixed column nodes) have membership (1/K, 1/K, . . . , 1/K); for z \u2265 1, we generate the degree parameters for row nodes as below: let \u03b8\u0304r \u2208 Rnr\u00d71 such that 1/\u03b8\u0304r(i)\niid\u223c U(1, z) for 1 \u2264 i \u2264 nr, where U(1, z) denotes the uniform distribution on [1, z], and set \u03b8r = \u03c1\u03b8\u0304r, where we use \u03c1 to control the sparsity of the network; when K = 2, P is set as\nP1 = [\n1 0.1 0.2 1\n] or P2 = [ 0.8 0.1 0.2 0.9 ] ;\nwhen K = 3,\nP3 =  1 0.1 0.30.2 1 0.4 0.5 0.2 1 or P4 = 0.8 0.1 0.30.2 0.9 0.4 0.5 0.2 1 ; where P2 and P4 have non-unit diagonals, and we consider the two cases because we want to investigate DiMSC\u2019s sensitivity when P has non-unit diagonals such that P disobeys Condition (I1). After obtaining P, \u03a0r, \u03a0c, \u03b8r, similar to the five simulation steps in [8], each simulation experiment contains the following steps: (a) Let \u0398r be the nr \u00d7 nr diagonal matrix such that \u0398r(i, i) = \u03b8r(i), 1 \u2264 i \u2264 nr. Set \u2126 = \u0398r\u03a0rP\u03a0\u2032c. (b) Let W be an nr \u00d7 nc matrix such that W(i, j) are independent centered-Bernoulli with parameters \u2126(i, j). Let A\u0303 = \u2126 + W.\n(c) Set S\u0303r = {i : \u2211ncj=1 A\u0303(i, j) = 0} and S\u0303c = {j : \u2211 nr i=1 A\u0303(i, j) = 0}, i.e., S\u0303r (S\u0303c) is the set of row (column) nodes with 0 edges. Let A be the adjacency matrix obtained by removing rows respective to nodes in S\u0303r and removing columns respective to nodes in S\u0303c from A\u0303. Similarly, update \u03a0r by removing nodes in S\u0303r and update \u03a0c by removing nodes in S\u0303c.\n(d) Apply the DiMSC algorithm (and its competitors) to A. Record MHamm under investigations.\n(e) Repeat (b)\u2013(d) 50 times, and report the averaged MHamm over the 50 repetitions. Let nr,A be the number of rows of A and nc,A be the number of columns of A. In our experiments, nr,A and nc,A are usually very close to nr and nc; therefore we do not\nreport the exact values of nr,A and nc,A. After providing the above steps about how to generate A numerically under DiDCMM and how to record the error rates, now we describe our experiments in detail. We consider six experiments here. In experiments 1\u20136, we study the influence of the fraction of pure nodes, degree heterogeneity, connectivity across communities, sparsity, phase transition, and network size on performances of these methods, respectively. Experiment 1 (a): Fraction of pure nodes. Set nr = 200, nc = 300, z = 5, \u03c1 = 1 and P as P1. Let n0 range in {10, 20, 30, . . . , 100}. The numerical results are shown in Panel (a) of Figure 4. The results show that as the fraction of pure nodes increases for both row and column communities, all approaches perform better. Meanwhile, DiMSC performs best among all methods in Experiment 1 (a). Experiment 1 (b): Fraction of pure nodes. All parameters are set the same as Experiment 1 (a) except that we set P as P2 here. The numerical results are shown in Panel (b) of Figure 4. The results show that all methods perform better as n0 increases, DiMSC outperforms its competitors, and DiMSC enjoys satisfactory performance even when P has non-unit diagonals. Experiment 1 (c): Fraction of pure nodes. Set nr = 600, nc = 900, z = 5, \u03c1 = 1, and P as P3. Let n0 range in {20, 40, 60, . . . , 200}. The numerical results are shown in Panel (c) of Figure 4, and we see that all methods perform better when there are more pure nodes and our DiMSC performs best. Experiment 1 (d): Fraction of pure nodes. All parameters are set the same as Experiment 1 (c) except that we set P as P4 here. The numerical results are shown in Panel (d) of Figure 4, and the analysis is similar to that of Experiment 1 (b).\nExperiment 2 (a): Degree heterogeneity. Set nr = 200, nc = 300, n0 = 80, \u03c1 = 1, and P as P1. Let z range in {2, 3, 4, . . . , 12}. A lager z generates lesser edges. The results are displayed in Panel (a) of Figure 5. The results suggest that the error rates of DiMSC for both row and column nodes tend to increase as z increases. This phenomenon happens because decreasing degree heterogeneities for row nodes lowers the number of edges in the directed network; thus the network becomes harder to be detected for both row and column nodes. Meanwhile, DiMSC outperforms its competitors in this experiment, and it is interesting to see that the error rates of DI-SIM, DiPCA, and DSCORE are almost the same for this experiment.\nExperiment 2 (b): Degree heterogeneity. All parameters are set the same as Experiment 2 (a) except that we set P as P2 here. The results are displayed in Panel (b) of Figure 5, and we see that DiMSC performs satisfactorily when the directed network is not too sparse (i.e., a small z case) even when P has non-unit diagonals. Meanwhile, DiMSC significantly outperforms its competitors in this experiment.\nExperiment 2 (c): Degree heterogeneity. Set nr = 600, nc = 900, n0 = 150, \u03c1 = 1, and P as P3. Let z range in {2, 3, 4, . . . , 12}. The results are shown in Panel (c) of Figure 5 and can be analyzed similarly to Experiment 2 (a).\nExperiment 2 (d): Degree heterogeneity. All parameters are set the same as Experiment 2 (c) except that we set P as P4 here. The results are displayed in Panel (d) of Figure 5 and are similar to that of Experiment 2 (b).\nExperiment 2 (e): Degree heterogeneity. All parameters are set the same as Experiment 2(a) except that we set n0 = 0 (so there are no pure nodes in both row and column communities), and all mixed row nodes have two different memberships (0.9, 0.1) and (0.1, 0.9), each with nrK = 100 number of row nodes, and all mixed column nodes also have the above two memberships, each with ncK = 150 number of column nodes. Panel (e) of Figure 5 shows the results, and we see that DiMSC performs satisfactorily for a small z even for the case when there are no pure nodes for both row and column communities. Meanwhile, DiMSC performs better than its competitors when z < 7, and it perform poorer than its competitors when z \u2265 8 for this experiment. Furthermore, compared with numerical results of Experiment 2 (a), we see that DI-SIM, DiPCA, and DSCORE have better performances in Experiment 2 (e). The possible reason is the memberships (0.9, 0.1) and (0.1, 0.9) are close to (1, 0) and (0, 1) somewhat. Experiment 2 (f): Degree heterogeneity. All parameters are set the same as Experiment 2 (b) except that we set \u03a0r and \u03a0c the same as Experiment 2 (e). The results are shown in Panel (f) of Figure 5 and are similar to that of Experiment 2 (e). Experiment 2 (g): Degree heterogeneity. All parameters are set the same as Experiment 2 (c) except that we set n0 = 0, all mixed row nodes have three different memberships (0.8, 0.1, 0.1), (0.1, 0.8, 0.1), and (0.1, 0.1, 0.8), each with nrK = 200 number of row nodes, and all mixed column nodes also have the above four memberships, each with nc K = 300 number of column nodes. The results are displayed in Panel (g) of Figure 5 and are similar to that of Experiment 2 (e). Experiment 2 (h): Degree heterogeneity. All parameters are set the same as Experiment 2 (d) except that we set \u03a0r and \u03a0c the same as Experiment 2 (g). The results are shown in Panel (h) of Figure 5 and are similar to that of Experiment 2 (e).\nExperiment 3 (a): Connectivity across communities. Set nr = 200, nc = 300, n0 = 80, z = 5, \u03c1 = 1. Set\nP = [\n1 \u03b2\u2212 1 \u03b2\u2212 1 1\n] .\nand let \u03b2 range in {1, 1.2, 1.4, . . . , 4}. Decreasing |\u03b2\u2212 2| increases the hardness of detecting such directed networks. Note that P(A(i, j) = 1) = \u2126(i, j) = \u03b8r(i)\u03a0r(i, :)P\u03a0\u2032c(j, :) gives maxi,j\u2126(i, j) = \u03b8r,maxPmax should be no larger than 1. Since Pmax may be larger than one in this experiment, after obtaining \u03b8r, we need to update \u03b8r as \u03b8r/Pmax. The results are displayed in Panel (a) of Figure 6, and they support the arguments given after Corollary 1\nsuch that DiMSC performs better when |\u03b2\u2212 2| increases and vice versa. Meanwhile, our DiMSC outperforms its competitors in this experiment.\nExperiment 3 (b): Connectivity across communities. All parameters are set the same as Experiment 3 (a) except that we set\nP = [\n0.8 \u03b2\u2212 1 \u03b2\u2212 1 0.9\n] .\nThe results are displayed in Panel (b) of Figure 6, and we see that DiMSC performs better when |\u03b2\u2212 2| increases even for the case that P has non-unit diagonals.Meanwhile, our DiMSC performs better than its competitors here. Experiment 3 (c): Connectivity across communities. Set nr = 600, nc = 900, n0 = 150, z = 5, \u03c1 = 1. Set\nP =  1 \u03b2\u2212 1 \u03b2\u2212 1\u03b2\u2212 1 1 \u03b2\u2212 1 \u03b2\u2212 1 \u03b2\u2212 1 1 . and let \u03b2 range in {1, 1.2, 1.4, . . . , 4}. The results are displayed in Panel (c) of Figure 6 and can be analyzed similarly to Experiment 3 (a).\nExperiment 3 (d): Connectivity across communities. All parameters are set the same as Experiment 3(c) except that we set\nP =  0.8 \u03b2\u2212 1 \u03b2\u2212 1\u03b2\u2212 1 0.9 \u03b2\u2212 1 \u03b2\u2212 1 \u03b2\u2212 1 1 . The results are displayed in Panel (d) of Figure 6 and can be analyzed similarly to Experiment 3 (b). Experiment 3 (e): Connectivity across communities. All parameters are set the same as Experiment 3(a) except that we let \u03a0r and \u03a0c be the same as that of Experiment 2 (e) (so there are no pure nodes in both row and column communities.). Panel (e) of Figure 6 shows the results, and we see that DiMSC enjoys better performance when |\u03b2\u2212 2| increases even in the case that there are no pure nodes for both row and column communities. Meanwhile, all methods have competitive performances for this experiment, and the possible reason that DiMSC\u2019s competitors enjoy better performances here than in Experiment 3 (a) is analyzed in Experiment 2 (e).\nExperiment 3 (f): Connectivity across communities. All parameters are set the same as Experiment 3 (b) except that we set \u03a0r and \u03a0c the same as Experiment 2 (e). The results are displayed in Panel (f) of Figure 6 and can be analyzed similarly to Experiment 3 (e).\nExperiment 3 (g): Connectivity across communities. All parameters are set the same as Experiment 3 (c) except that we let \u03a0r and \u03a0c be the same as that of Experiment 2 (g) (so there are no pure nodes). Panel (g) of Figure 6 shows the results, and the analysis is similar to that of Experiment 3 (b).\nExperiment 3 (h): Connectivity across communities. All parameters are set the same as Experiment 3 (d) except that we set \u03a0r and \u03a0c the same as Experiment 2 (g). Panel (h) of Figure 6 shows the results, and the analysis is similar to that of Experiment 3 (b).\nExperiment 4 (a): Sparsity. Set nr = 200, nc = 300, n0 = 80, z = 5, and P as P1. Let \u03c1 range in {0.2, 0.3, . . . , 1}. A larger \u03c1 indicates a denser network. Panel (a) in Figure 7 displays the simulation results of this experiment. We see that DiMSC performs better as the simulated directed network becomes denser, and DiMSC significantly outperforms its competitors in this experiment.\nExperiment 4 (b): Sparsity. All parameters are set the same as Experiment 4 (a) except that P is set as P2. Panel (b) of Figure 7 shows the results, and the analysis is similar to that of Experiment 2 (b).\nExperiment 4 (c): Sparsity. Set nr = 600, nc = 900, n0 = 150, z = 5, and P as P3. Let \u03c1 range in {0.2, 0.3, . . . , 1}. Panel (c) of Figure 7 shows the results, and the analysis is similar to that of Experiment 4 (a).\nExperiment 4 (d): Sparsity. All parameters are set the same as Experiment 4 (c) except that P is set as P4. Panel (d) of Figure 7 displays the results, and the analysis is similar to that of Experiment 4 (b).\nExperiment 4 (e): Sparsity. All parameters are set the same as Experiment 4 (a) except that we let \u03a0r and \u03a0c be the same as that of Experiment 2 (e). Panel (e) of Figure 7 shows the results, and we see that DiMSC\u2019s error rates decrease for a denser directed network even when all nodes are mixed. Meanwhile, all methods enjoy similar performances in this experiment. Experiment 4 (f): Sparsity. All parameters are set the same as Experiment 4 (b) except that we set \u03a0r and \u03a0c the same as Experiment 2(e). Panel (f) of Figure 7 shows the results, and the analysis is similar to that of Experiment 4 (e).\nExperiment 4 (g): Sparsity. All parameters are set the same as Experiment 4 (c) except that we let \u03a0r and \u03a0c be the same as that of Experiment 2 (g). Panel (g) of Figure 7 shows the results, and the analysis is similar to that of Experiment 4 (e).\nExperiment 4 (h): Sparsity. All parameters are set the same as Experiment 4 (d) except that we set \u03a0r and \u03a0c the same as Experiment 2(g). Panel (h) of Figure 7 shows the results, and the analysis is similar to that of Experiment 4 (e).\nExperiment 5 (a): Phase transition. Under DiDCMM(n, K, \u03a0r, \u03a0c, \u03b1in, \u03b1out), set K = 2, n = nr = nc = 300. Let each row community have 100 pure nodes, each column community have 120 pure nodes, and all mixed nodes have membership (1/2, 1/2). Since max(pin, pout) = max(\u03b1in, \u03b1out) log(n) n \u2264 1, \u03b1in and \u03b1out should be set in (0, n log(n) ]. We let \u03b1in and \u03b1out be in the range of {2.5, 5, 7.5, . . . , 50}. Panel (a) of Figure 8 displays the results. We see that DiMSC performs satisfactorily when \u03b1in and \u03b1out satisfy Equation (14), and this means that DiMSC achieves the threshold provided in Equation (14) under DiDCMM(n, K, \u03a0r, \u03a0c, \u03b1in, \u03b1out). Experiment 5 (b): Phase transition. Under DiDCMM(n, K, \u03a0r, \u03a0c, \u03b1in, \u03b1out), set K = 3, n = nr = nc = 300. Let each row community have 60 pure nodes, each column community have 80 pure nodes, and all mixed nodes have membership (1/3, 1/3, 1/3). We also let \u03b1in and \u03b1out be in the range of {2.5, 5, 7.5, . . . , 50}. Panel (b) of Figure 8 displays the results, and the analysis is similar to that of Experiment 5 (a).\nrepresent |\u03b1in\u2212\u03b1out|\u221a\nmax(\u03b1in,\u03b1out) = 1. Panel (a): Experiment 5 (a); Panel (b): Experiment 5 (b).\nFor Experiments 1\u20135, we can conclude that DiMSC outperforms its competitors, and this supports our analysis in Remark 6 because DiMSC is designed to estimate mixed memberships, while its competitors are designed for community partition of pure nodes.\nExperiment 6: Network size. Under SBM(n, pin, pout), let \u03b1in = 2 and \u03b1out = 0.0001. On the one hand, we have \u221a \u03b1in \u2212 \u221a \u03b1out = \u221a 2\u2212 0.01 < \u221a 2, i.e., \u03b1in and \u03b1out locates in the impossible region of exact recovery introduced in [50]. On the other hand, we have \u03b1in\u2212\u03b1out\u221a\n\u03b1in > 1, i.e., \u03b1in and \u03b1out satisfy Equation (14) for DiMSC\u2019s consistent estimation. Let n range in {1000, 2000, 3000, . . . , 15000}. For each n in this experiment, we report the averaged error rate and running time of DiMSC over 10 independent repetitions. The results are shown in Figure 9. From Panel (a) of Figure 9, we see that DiMSC enjoys satisfactory performance with a small error rate for this experiment. Panels (b) of Figure 9 says that DiMSC processes computer-generated networks of up to 15,000 nodes within hundreds of seconds.\nRemark 7. For visuality, we provide some examples of different types of directed networks generated under DiDCMM in this remark. Let \u03b8r(i) = 0.9 + i 2\n9n2r for 1 \u2264 i \u2264 nr. Let each row community\nhas nr,0 pure nodes, and each column community has nc,0 pure nodes. Let all mixed nodes have membership (1/K, . . . , 1/K). For the setting of P, we set it as\nPa = [ 0.9 0.05 0.1 0.95 ] or Pb = [ 0.1 0.95 0.9 0.05 ] or Pc = [ 12 1 0 12 ] log(nr) nr or Pd = [ 0 12 12 1 ] log(nr) nr or\nPe = 12 1 00 12 0 1 0 12  log(nr)nr or Pf =  1 0 1212 0 0 0 12 1  log(nr)nr , where K = 2 when P is Pa, Pb, Pc or Pd, and K = 3 when P is Pe or Pf . Meanwhile, we can generate different types of directed networks under DiDCMM by considering the above six different settings of P, where these different types are also considered in Experiments 1\u20136, and we mainly provide the visuality for these directed networks with different structures provided in different P for this remark. Note that we allow P to have non-unit diagonals here because Condition (I1) is mainly for our theoretical buildings, and results for previous experiments show that DiMSC performs stable even when P has non-unit diagonals. We consider below eight settings.\nModel Setup 1: Set nr = 16, nr,0 = 6, nc = 16, nc,0 = 7, and P as Pa. For this setup, a directed network with 16 row nodes and 16 column nodes is generated from DiDCMM. Figure 10 shows a directed network N generated under Model Setup 1, where we also report DiMSC\u2019s error rate. Figure 10 says that there are more directed edges sent from row nodes 1\u20136 to column nodes 1\u20137 than from row nodes 7\u201312 to column nodes 1\u20137 for Pa. With given adjacency matrix A and known memberships \u03a0r and \u03a0c for this setup, readers can apply our DiMSC directly to A given in Panel (a) of Figure 10 to check the effectiveness of DiMSC. Model Setup 2: All settings are the same as Model Setup 1 except that we let P be Pb. The directed network N and its adjacency matrix are shown in Figure 11. We see that there are more directed edges sent from row nodes 1\u20136 to column nodes 10\u201316 than from row nodes 7\u201312 to column nodes 10\u201316 for Pb, which means that directed network generated using Pb and directed network from Pa has different structures. Model Setup 3: Set nr = 32, nr,0 = 14, nc = 28, nc,0 = 12, and P as Pa. For this setup, a bipartite network with 32 row nodes and 28 column nodes are generated from DiDCMM. Figure 12 shows this bipartite network and its adjacency matrix. Model Setup 4: All settings are the same as Model Setup 3 except that we let P be Pb. Figure 13 displays the results, and we see that the bipartite network from Pb also has a different structure compared with the one generated from using Pa under DiDCMM. Model Setup 5: Set nr = 100, nr,0 = 48, nc = 100, nc,0 = 45, and P as Pc. Figure 14 shows the row and column communities for a directed network generated from Setup 5 under DiDCMM, where we plot the directed network directly. Model Setup 6: All settings are the same as Model Setup 5 except that we let P be Pd. Figure 15 shows a directed network obtained from this setup, and we see that the structure of the\ndirected network from Pd in Figure 15 differs a lot from that of the directed network from Pc shown in Figure 14.\nModel Setup 7: Set nr = 100, nr,0 = 30, nc = 100, nc,0 = 32, and P as Pe. Figure 16 shows a directed network generated from this setup.\nModel Setup 8: All settings are the same as Model Setup 7 except that we let P be Pf . Figure 17 displays a directed network generated from this setup, and we see that directed networks from Pf and Pe have different structures by comparing Figures 16 and 17."
        },
        {
            "heading": "6. Application to Real-World Directed Networks",
            "text": "For the empirical directed networks considered here, row nodes are always the same as column nodes, so we let nr = nc = n. For \u03a0\u0302r, we call node i highly mixed node if 0.8 \u2265 max1\u2264k\u2264K\u03a0\u0302r(i, k), similar for \u03a0\u0302c. A highly mixed node tells us whether a node has mixed memberships and belongs to multiple communities. Let \u03c4r = |i:0.8\u2265max1\u2264k\u2264K\u03a0\u0302r(i,k)| n be the proportion of highly mixed nodes among all nodes to measure the mixability of all row communities. Define \u03c4c similar to \u03c4r. Let \u02c6\u0300r be a vector such that \u02c6\u0300r(i) = argmax1\u2264k\u2264K\u03a0\u0302r(i, k) for 1 \u2264 i \u2264 n, where we use \u02c6\u0300r(i) to denote the home base row community of node i. Define \u02c6\u0300c similar to \u02c6\u0300r. To measure the asymmetric structure of a directed network, we use\nHammrc = minP\u2208SP \u2016\u03a0\u0302cP \u2212 \u03a0\u0302r\u20161\nn ,\nwhere a large Hammrc means that the structure of row clusters differs a lot from that of column clusters. For 1 \u2264 i \u2264 n, let dr(i) = \u2211nj=1 A(i, j) be the number of edges sent by node i, dc(i) = \u2211nj=1 A(j, i) be the number of edges received by node i, where dr(i) (and dc(i)) is the out degree (in degree) of node i. Since there are many nodes with zero in degree or out degree for real-world directed network, we need the below pre-processing: for any directed network N , we let Am be its adjacency matrix for any positive integer m such that Am is connected, and every node has at least m in degree and m out degree in Am.\nWe apply DiMSC to the following real-world directed networks to discover their mixability, asymmetries, and directional communities.\nPolitical blogs: This is a directed network of hyperlinks between weblogs on US politics [68]. In this data, node means a blog, and edge means a hyperlink. This data can be downloaded from http://www-personal.umich.edu/~mejn/netdata/ (accessed on 28 August 2022). It is well-known that there are two parties, \u201cliberal\u201d and \u201cconservative\u201d, so K = 2 for this data. The are 1490 nodes in the original data. After pre-processing, A1 \u2208 {0, 1}813\u00d7813, A3 \u2208 {0, 1}495\u00d7495, A6 \u2208 {0, 1}285\u00d7285, A9 \u2208 {0, 1}158\u00d7158, where we focus on the cases when m = 1, 3, 6, 9 for this data here. Meanwhile, we use political blogs Am to denote this network when its adjacency matrix is Am, where every node has a degree at least m. Similar notations hold for other real-world directed networks used in this paper. Wikipedia links (gan): This directed network consists of the Wikilinks of Wikipedia in the Gan Chinese language (gan). In this data, node means an article, and the directed edge is a Wikilink [69]. This data can be downloaded from http://konect.cc/networks/ wikipedia_link_gan (accessed on 28 August 2022). There are 9189 nodes in the original data. After pre-processing, A1 \u2208 {0, 1}6012\u00d76012, A30 \u2208 {0, 1}820\u00d7820, A60 \u2208 {0, 1}559\u00d7569, A90 \u2208 {0, 1}240\u00d7240, where we study the cases m = 1, 30, 60, 90 for this data. The leading 20 singular values of A1, A30, A60, A90 shown in Panels (e)\u2013(h) of Figure 18 suggest K = 2 for these four adjacency matrices, where [30] also uses eigengap to estimate K. Wikipedia links (nah): This network consists of the Wikilinks of the Na\u0304huatl language (nah) [69] and can be downloaded from http://konect.cc/networks/wikipedia_link_nah/ (accessed on 28 August 2022). The original data has 10285 nodes. After pre-processing, A1 \u2208 {0, 1}6924\u00d76924, A20 \u2208 {0, 1}1057\u00d71057, A30 \u2208 {0, 1}486\u00d7486, A40 \u2208 {0, 1}136\u00d7136. Panel (i) of Figure 18 suggests K = 4 for A1, and Panels (j)\u2013(l) of Figure 18 suggest K = 2 for A20, A30, and A40. Note that it only takes around 4 seconds for DiMSC to estimate memberships of Wikipedia links (nah) A1.\nThe proportions of highly mixed nodes and Hammrc when applying DiMSC on the above real-world directed networks are reported in Table 1. For the political blogs network, small \u03c4r, \u03c4c, and Hammrc indicate that there are only a few highly mixed nodes, and the structure of row communities is similar to that of column communities, i.e., there is a slight asymmetry for this data. For Wikipedia links (gan) A1 and Wikipedia links (nah) A1, they have a large proposition of highly mixed nodes in both row and column communities, and the row communities differ a lot from column communities, suggesting heavy asymmetric structure between row and column communities for these two data. For Wikipedia links (gan) A30, A60, and Wikipedia links (nah) A20, we see that the proportion of highly mixed nodes for row (column) communities is small (large), and there is a slight asymmetric for these data. For Wikipedia links (gan) A90 and Wikipedia links (nah) A30, A40, there is no highly mixed node, and the structure of row clusters is similar to that of column clusters. For visualization, we plot the row and column communities as well as highly mixed nodes by applying DiMSC to some of these directed networks in Figures 19 and 20."
        },
        {
            "heading": "7. Discussion and Conclusions",
            "text": "In this paper, we propose a novel directed degree corrected mixed membership (DiDCMM) model. DiDCMM models a directed network with mixed memberships for row nodes with degree heterogeneities and column nodes without degree heterogeneities. DiDCMM is identifiable when the two well-used Conditions (I1) and (I2) hold. It should be mentioned that a model modeling a directed network with mixed memberships for both row and column nodes with degree heterogeneities is unidentifiable unless considering some nontrivial conditions. To fit the model, we propose a provably consistent spectral algorithm called DiMSC to infer community memberships for both row and column nodes in a directed network generated by DiDCMM. DiMSC is designed based on the SVD of the adjacency matrix, where we apply the SP algorithm to hunt for the corners in the simplex structure and the SVM-cone algorithm to hunt for the corners in the cone structure. The theoretical results of DiMSC show that it consistently recovers memberships of both row nodes and column nodes under mild conditions. Meanwhile, when DiDCMM degenerates to MMSB, our theoretical results match that of Theorem 2.2 [24] when their DCMM degenerates to MMSB under mild conditions. Experiments conducted on synthetic directed networks generated from DiDCMM verify the effectiveness and the stability of Conditions (I1) and (I2) of DiMSC. Results for real-world directed networks show that DiMSC reveals highly mixed nodes and asymmetries in the structure of row and column communities. The model DiDCMM and the algorithm DiMSC developed in this paper are useful to discover asymmetry for a directed network with mixed memberships. DiDCMM can also generate an artificially directed network with mixed memberships as a benchmark directed network for research purposes. We wish that DiDCMM and DiMSC can be widely applied in social network analysis.\nThe proposed model DiDCMM and the algorithm DiMSC can be extended in many ways. Similar to [24,57], we may obtain an ideal simplex from U using the idea of the entry-wise ratio proposed in [8]. Meanwhile, DiMSC is designed based on the SVD of the adjacency matrix, and similar to [5,7,11,30], we may design spectral algorithms based on the regularized Laplacian matrix under DiDCMM. Extending DiDCMM from an unweighted directed network to a weighted directed network with an application of the distribution-free idea introduced in [62] is one of our future research directions. The SVD step of DiMSC can be accelerated by the random projection and random sampling ideas introduced in [70] to process large-scale directed networks. Instead of simply using eigengap to find K, in our future work, it is worth focusing on estimating the number of communities in a directed network generated under ScBM (and DCScBM) [30] and DiDCMM. Ref. [46] proposes an algorithm to uncover boundary nodes that spread information between communities in undirected social networks. It is an interesting topic to extend works in [46] to directed networks generated from ScBM, DCScBM, and DiDCMM. We leave them for our future work.\nFunding: This research was funded by the Scientific research start-up fund of CUMT NO. 102520253, the High-level personal project of Jiangsu Province NO. JSSCBS20211218.\nInstitutional Review Board Statement: Not applicable.\nData Availability Statement: Not applicable.\nConflicts of Interest: The author declares no conflict of interest.\nAbbreviations The following abbreviations are used in this manuscript:\nSBM Stochastic Blockmodel DCSBM Degree Corrected Stochastic Blockmodel MMSB Mixed Membership Stochastic Blockmodel DCMM Degree Corrected Mixed Membership model OCCAM Overlapping Continuous Community Assignment model ScBM Stochastic co-Blockmodel DC-ScBM Degree Corrected Stochastic co-Blockmodel DiMMSB Directed Mixed Membership Stochastic Blockmodel DiDCMM Directed Degree Corrected Mixed Membership model SP Successive projection algorithm SVD Singular value decomposition DiMSC Directed Mixed Simplex & Cone algorithm"
        },
        {
            "heading": "Appendix A. Proof for Identifiability",
            "text": "Appendix A.1. Proof of Proposition 1\nProof. Let \u2126 = U\u039bV\u2032 be the compact singular value decomposition of \u2126. Lemma 1 gives V = \u03a0cBc \u2261 \u03a0cV(Ic, :). Since \u2126 = \u2126\u0303, V also equals to \u03a0\u0303cV(Ic, :), which gives that \u03a0c = \u03a0\u0303c.\nSince \u2126(Ir, Ic) = \u0398r(Ir, Ir)\u03a0r(Ir, :)P\u03a0\u2032c(Ic, :) = \u0398r(Ir, Ir)P = U(Ir, :)\u039bV\u2032(Ic, :) by Condition (I2), we have \u0398r(Ir, Ir)P = U(Ir, :)\u039bV\u2032(Ic, :), which gives that \u0398r(Ir, Ir) = diag(U(Ir, :)\u039bV\u2032(Ic, :)). From this step, we see that if P\u2019s diagonal entries are not ones, we cannot obtain \u0398r(Ir, Ir) = diag(U(Ir, :)\u039bV\u2032(Ic, :)) which leads to a consequence that \u0398r(Ir, Ir) does not equal to \u0398\u0303r(Ir, Ir); hence Condition (I1) is necessary by Condition (I1). Since \u2126 = \u2126\u0303, we also have \u0398\u0303r(Ir, Ir) = diag(U(Ir, :)\u039bV\u2032(Ic, :)), which gives that \u0398r(Ir, Ir) = \u0398\u0303r(Ir, Ir). Since \u0398\u0303r(Ir, Ir)P\u0303 also equals to U(Ir, :)\u039bV\u2032(Ic, :), we have P = P\u0303. Lemma 1 gives that U = \u0398r\u03a0rBr, where Br = \u0398\u22121r (Ir, Ir)U(Ir, :). Since \u2126 = \u2126\u0303, we also have U = \u0398\u0303r\u03a0\u0303r B\u0303r. Since B\u0303r = \u0398\u0303\u22121r (Ir, Ir)U(Ir, :) = \u0398\u22121r (Ir, Ir)U(Ir, :), we have B\u0303r = Br. Since U = \u0398r\u03a0rBr = \u0398\u0303r\u03a0\u0303r B\u0303r = \u0398\u0303r\u03a0\u0303rBr, we have \u0398r\u03a0r = \u0398\u0303r\u03a0\u0303r. Since each row of \u03a0r or \u03a0\u0303r is a PMF, \u0398r = \u0398\u0303r, \u03a0r = \u03a0\u0303r, and the claim follows."
        },
        {
            "heading": "Appendix B. Ideal Simplex, Ideal Cone",
            "text": "Appendix B.1. Proof of Lemma 1\nProof. First, we consider U and V. Since \u2126 = U\u039bV\u2032, we have U = \u2126V\u039b\u22121 since V\u2032V = IK. Recall that \u2126 = \u0398r\u03a0rP\u03a0\u2032c, we have U = \u0398r\u03a0rP\u03a0\u2032cV\u039b\u22121 = \u0398r\u03a0rBr, where we set Br = P\u03a0\u2032cV\u039b\u22121 and sure it is unique. Since U(Ir, :) = \u0398r(Ir, Ir)\u03a0r(Ir, :)Br = \u0398r(Ir, Ir)Br, we have Br = \u0398\u22121r (Ir, Ir)U(Ir, :). Similarly, since \u2126 = U\u039bV\u2032, we have V\u2032 = \u039b\u22121U\u2032\u2126 since U\u2032U = IK, hence V = \u2126\u2032U\u039b\u22121. Recall that \u2126 = \u0398r\u03a0rP\u03a0\u2032c, we have V = (\u0398r\u03a0rP\u03a0\u2032c)\u2032U\u039b\u22121 = \u03a0cP\u2032\u03a0\u2032r\u0398rU\u039b\u22121 = \u03a0cBc, where we set Bc = P\u2032\u03a0\u2032r\u0398rU\u039b\u22121 and sure it is unique. Since V(Ic, :) = \u03a0c(Ic, : )Bc = Bc, we have Bc = V(Ic, :). Meanwhile, for 1 \u2264 j \u2264 nc, we have V(j, :) = e\u2032j\u03a0cBc = \u03a0c(j, :)Bc. Hence, we have V(j, :) = V( j\u0304, :) as long as \u03a0c(j, :) = \u03a0c( j\u0304, :). Now, we show the ideal cone structure that appears in U\u2217. For convenience, set M = \u03a0rBr, hence U = \u0398r\u03a0rBr gives U = \u0398r M. Hence, we have U(i, :) = e\u2032iU = \u0398r(i, i)M(i, :). Therefore, U\u2217(i, :) =\nU(i,:) \u2016U(i,:)\u2016F = M(i,:)\u2016M(i,:)\u2016F , combine it with the fact that\nBr = \u0398\u22121r (Ir, Ir)U(Ir, :), we have\nU\u2217 =  1 \u2016M(1,:)\u2016F 1 \u2016M(2,:)\u2016F . . .\n1 \u2016M(nr ,:)\u2016F\n\u03a0rBr\n=  \u03a0r(1, :)/\u2016M(1, :)\u2016F \u03a0r(2, :)/\u2016M(2, :)\u2016F\n... \u03a0r(nr, :)/\u2016M(nr, :)\u2016F \u0398\u22121r (Ir, Ir)N\u22121U (Ir, Ir)U\u2217(Ir, :). Therefore, we have\nY =  \u03a0r(1, :)/\u2016M(1, :)\u2016F \u03a0r(2, :)/\u2016M(2, :)\u2016F\n... \u03a0r(nr, :)/\u2016M(nr, :)\u2016F \u0398\u22121r (Ir, Ir)N\u22121U (Ir, Ir) = NM\u03a0r\u0398\u22121r (Ir, Ir)N\u22121U (Ir, Ir), where NM is a diagonal matrix with NM(i, i) = 1\u2016M(i,:)\u2016F for 1 \u2264 i \u2264 nr. All entries of Y are nonnegative, and since we assume that each community has at least one pure node, no row of Y is 0. Then, we prove that U\u2217(i, :) = U\u2217(i\u0304, :) when \u03a0r(i, :) = \u03a0r(i\u0304, :). For 1 \u2264 i \u2264 nr, we have\nU\u2217(i, :) = e\u2032iU\u2217 = 1\n\u2016M(i, :)\u2016F e\u2032i M = 1 \u2016\u03a0r(i, :)Br\u2016F \u03a0r(i, :)Br,\nand the claim follows immediately.\nAppendix B.2. Proof of Lemma 2\nProof. Since I = U\u2032U = B\u2032r\u03a0\u2032r\u03982r \u03a0rBr = U\u2032(Ir, :)\u0398\u22121r (Ir, Ir)\u03a0\u2032r\u03982r \u03a0r\u0398\u22121(Ir, Ir)U(Ir, :) and rank(U(Ir, :)) = K (i.e., the inverse of U(Ir, :) exists), we have (U(Ir, :)U\u2032(Ir, :))\u22121 = \u0398\u22121r (Ir, Ir)\u03a0\u2032r\u03982r \u03a0r\u0398\u22121r (Ir, Ir). Since U\u2217(Ir, :) = NU(Ir, Ir)U(Ir, :), we have\n(U\u2217(Ir, :)U\u2032\u2217(Ir, :))\u22121 = N\u22121U (Ir, Ir)\u0398 \u22121(Ir, Ir)\u03a0\u2032r\u03982r \u03a0r\u0398\u22121r (Ir, Ir)N\u22121U (Ir, Ir).\nSince all entries of N\u22121U (Ir, Ir), \u03a0r, \u0398r and nonnegative and N, \u0398r are diagonal matrices, we see that all entries of (U\u2217(Ir, :)U\u2032\u2217(Ir, :))\u22121 are nonnegative, and its diagonal entries are strictly positive, hence we have (U\u2217(Ir, :)U\u2032\u2217(Ir, :))\u221211 > 0.\nAppendix B.3. Proof of Theorem 1\nProof. For column nodes, Remark A1 guarantees that SP algorithm returns Ic when the input is V with K column communities, hence ideal DiMSC recovers \u03a0c exactly. For row nodes, Remark A2 guarantees that SVM-cone algorithm returns Ir when the input is U\u2217 with K row communities, hence ideal DiMSC recovers \u03a0r exactly, and this theorem follows."
        },
        {
            "heading": "Appendix C. Equivalence Algorithm",
            "text": "In this subsection, we design one algorithm DiMSC-equivalence which returns the same estimations as DiMSC. Set U2 = UU\u2032 \u2208 Rnr\u00d7nr , U\u03022 = U\u0302U\u0302\u2032 \u2208 Rnr\u00d7nr , V2 = VV\u2032 \u2208 Rnc\u00d7nc , V\u03022 = V\u0302V\u0302\u2032 \u2208 Rnc\u00d7nc . Set U\u2217,2 \u2208 Rnr\u00d7nr as U\u2217,2(i, :) = U2(i,:)\u2016U2(i,:)\u2016F for 1 \u2264 i \u2264 nr. U\u0302\u2217,2 is defined similarly. The next lemma guarantees that V2 enjoys IS structure, and U\u2217,2 enjoys IC structure.\nLemma A1. Under DiDCMMnr ,nc(K, P, \u03a0r, \u03a0c, \u0398r), we have V2 = \u03a0cV2(Ic, :), and U\u2217,2 = YU\u2217,2(Ir, :).\nProof. By Lemma 1, we know that V = \u03a0cV(Ic, :), which gives that V2 = VV\u2032 = \u03a0cV(Ic, :)V\u2032 = \u03a0c(VV\u2032)(Ic, :) = \u03a0cV2(Ic, :). For U, since U = \u0398r\u03a0r\u0398\u22121r (Ir, Ir)U(Ir, :) by Lemma 1, we have U2 = UU\u2032 = \u0398r\u03a0r\u0398\u22121r (Ir, Ir)U(Ir, :)U\u2032 = \u0398r\u03a0r\u0398\u22121r (Ir, Ir)(UU\u2032) (Ir, :) = \u0398r\u03a0r\u0398\u22121r (Ir, Ir)U2(Ir, :). Set M2 = \u03a0r\u0398\u22121r (Ir, Ir)U2(Ir, :), we have U2 = \u0398r M2. Then, follow similar proof as Lemma 1, we have U\u2217,2 = Y2U\u2217,2(Ir, :), where Y2 = NM2 \u03a0r\u0398 \u22121 r (Ir, Ir)N\u22121U2 (Ir, Ir), and NM2 , NU2 are nr \u00d7 nr diagonal matrices whose ith diagonal entries are 1\u2016M2(i,:)\u2016F , 1 \u2016U2(i,:)\u2016F , respectively. Since \u2016U2(i, :)\u2016F = \u2016U(i, :)U\u2032\u2016F = \u2016U(i, :)\u2016F, we have NU2 = NU . Since \u2016M2(i, :)|F = \u2016\u03a0r\u0398\u22121r (Ir, Ir)U2(Ir, :)\u2016F = \u2016\u03a0r\u0398\u22121r (Ir, Ir)U(Ir, :)U\u2032\u2016F = \u2016M(i, :)\u2016F, we have NM2 = NM. Hence, Y2 \u2261 Y and the claim follows.\nSince U\u2217,2(Ir, :) \u2208 RK\u00d7nr and V2(Ic, :) \u2208 RK\u00d7nc , U\u2217,2(Ir, :) and V2(Ic, :) are singular matrix with rank K by Condition (I1), while the inverses of U\u2217,2(Ir, :)U\u2032\u2217,2(Ir, :) and V2(Ic, : )V\u20322(Ic, :) exist. Therefore, Lemma A1 gives that\nY = U\u2217,2U\u2032\u2217,2(Ir, :)(U\u2217,2(Ir, :)U\u2032\u2217,2(Ir, :))\u22121, \u03a0c = V2V\u20322(Ic, :)(V2(Ic, :)V\u20322(Ic, :))\u22121.\nSince U\u2217,2 = NUU2 and Y = NM\u03a0r\u0398\u22121r (, Ir, Ir)N\u22121U (Ir, Ir), we see that Y\u2217 also equals to U2U\u2032\u2217,2(Ir, :)(U\u2217,2(Ir, :)U\u2032\u2217,2(Ir, :))\u22121 by basic algebra.\nBased on the above analysis, we are now ready to give the ideal DiMSC-equivalence. Input \u2126. Output: \u03a0r and \u03a0c.\n\u2022 Obtain U, \u039b, V, U\u2217,2, V2 from \u2126. \u2022 Run SP algorithm on V2 with K column communities to obtain V2(Ic, :). Run SVMcone algorithm on U\u2217,2 with K row communities to obtain Ir. \u2022 Set J\u2217 = diag(U\u2217(Ir, :)\u039bV\u2032(Ic, :)), Y\u2217 = U2U\u2032\u2217,2(Ir, :)(U\u2217,2(Ir, :)U\u2032\u2217,2(Ir, :))\u22121, Zr = Y\u2217 J\u2217 and Zc = V2V\u20322(Ic, :)(V2(Ic, :)V\u20322(Ic, :))\u22121. \u2022 Recover \u03a0r and \u03a0c by setting \u03a0r(i, :) =\nZr(i,:) \u2016Zr(i,:)\u20161 for 1 \u2264 i \u2264 nr, and \u03a0c(j, :) = Zc(j,:)\u2016Zc(j,:)\u20161 for 1 \u2264 j \u2264 nc.\nFor the real case, set U\u03022 = U\u0302U\u0302\u2032, V\u03022 = V\u0302V\u0302\u2032, U\u0302\u2217,2 = NU\u0302U\u03022. We now extend the ideal case to the real one given by Algorithm A1.\nAlgorithm A1 DiMSC-equivalence\nRequire: The adjacency matrix A \u2208 Rnr\u00d7nc of a directed network, the number of row communities (column communities) K. Ensure: The estimated nr \u00d7 K row membership matrix \u03a0\u0302r,2 and the estimated nc \u00d7 K column membership matrix \u03a0\u0302c,2.\n1: Obtain A\u0303 = U\u0302\u039b\u0302V\u0302\u2032, the top-K-dimensional SVD of A. Compute U\u0302\u2217, U\u03022, V\u03022, U\u0302\u2217,2. 2: Apply SP algorithm on the rows of V\u03022 assuming there are K column communities to\nobtain I\u0302c,2, the index set returned by SP algorithm. 3: Apply SVM-cone algorithm on the rows of U\u0302\u2217,2 with K row communities to obtain I\u0302r,2, the index set returned by SVM-cone algorithm. 4: Set J\u0302\u2217,2 = diag(U\u0302\u2217( I\u0302r,2, :)\u039b\u0302V\u0302\u2032(I\u0302c,2, :)), Y\u0302\u2217,2 = U\u03022U\u0302\u2032\u2217,2(I\u0302r,2, :)(U\u0302\u2217,2(I\u0302r,2, :)U\u0302\u2032\u2217,2(I\u0302r,2, : ))\u22121, Z\u0302r,2 = Y\u0302\u2217,2 J\u0302\u2217,2 and Z\u0302c,2 = V\u03022V\u0302\u20322(I\u0302c,2, :)(V\u03022(I\u0302c,2, :)V\u0302\u20322(I\u0302c,2, :))\u22121. Then, set Z\u0302r,2 = max(0, Z\u0302r,2) and Z\u0302c,2 = max(0, Z\u0302c,2). 5: Estimate \u03a0r(i, :) by \u03a0\u0302r,2(i, :) = Z\u0302r,2(i, :)/\u2016Z\u0302r,2(i, :)\u20161, 1 \u2264 i \u2264 nr and estimate \u03a0c(j, :) by \u03a0\u0302c,2(j, :) = Z\u0302c,2(j, :)/\u2016Z\u0302c,2(j, :)\u20161, 1 \u2264 j \u2264 nc.\nLemma A2. (Equivalence). For the empirical case, we have I\u0302r,2 \u2261 I\u0302r, I\u0302c,2 \u2261 I\u0302c, U\u0302\u2217,2(I\u0302r,2, : )U\u0302\u2032\u2217,2(I\u0302r,2, :) \u2261 U\u0302\u2217(I\u0302r, :)U\u0302\u2032\u2217(I\u0302r, :), Y\u0302\u2217,2 \u2261 Y\u0302\u2217, J\u0302\u2217,2 \u2261 J\u0302\u2217, Z\u0302r,2 \u2261 Z\u0302r, Z\u0302c,2 \u2261 Z\u0302c, \u03a0\u0302r,2 \u2261 \u03a0\u0302r and \u03a0\u0302c,2 \u2261 \u03a0\u0302c.\nProof. For column nodes, Lemma 3.2 [27] gives I\u0302c = I\u0302c,2 (i.e., SP algorithm will return the same indices on both V\u0302 and V\u03022.), which gives that V\u03022V\u0302\u20322(I\u0302c,2, :) = V\u03022V\u0302\u20322(I\u0302c, : ) = V\u0302V\u0302\u2032((V\u0302V\u0302\u2032)(I\u0302c, :))\u2032 = V\u0302V\u0302\u2032(V\u0302(I\u0302c, :)V\u0302\u2032)\u2032 = V\u0302V\u0302\u2032V\u0302V\u0302\u2032(I\u0302c, :) = V\u0302V\u0302\u2032(I\u0302c, :), and V\u03022(I\u0302c,2, : )V\u0302\u20322(I\u0302c,2, :) = V\u03022(I\u0302c, :)V\u0302\u20322(I\u0302c, :) = V\u0302(I\u0302c, :)V\u0302\u2032(V\u0302(I\u0302c, :)V\u0302\u2032)\u2032 = V\u0302(I\u0302c, :)V\u0302\u2032(I\u0302c, :). Therefore, we have Z\u0302c,2 = Z\u0302c, \u03a0\u0302c,2 = \u03a0\u0302c.\nFor row nodes, Lemma G.1 [26] guarantees that I\u0302r = I\u0302r,2 (i.e., SVM-cone algorithm will return the same indices on both U\u0302\u2217 and U\u0302\u2217,2.), so immediately we have J\u0302\u2217,2 = J\u0302\u2217. Since U\u0302\u2217,2(I\u0302r,2, :) = U\u0302\u2217,2(I\u0302r, :) = NU\u0302(I\u0302r, I\u0302r)U\u03022(I\u0302r, :) = NU\u0302(I\u0302r, I\u0302r)U\u0302(I\u0302r, :)U\u0302\u2032 = U\u0302\u2217(I\u0302r, :)U\u0302\u2032, we have U\u03022U\u0302\u2032\u2217,2(I\u0302r,2, :) = U\u03022U\u0302\u2032\u2217,2(I\u0302r, :) = U\u0302U\u0302\u2032U\u0302U\u0302\u2032\u2217(I\u0302r, :) = U\u0302U\u0302\u2032\u2217(I\u0302r, :) and (U\u0302\u2217,2(I\u0302r,2, : )U\u0302\u2032\u2217,2(I\u0302r,2, :))\u22121 = (U\u0302\u2217,2(I\u0302r, :)U\u0302\u2032\u2217,2(I\u0302r, :))\u22121 = (U\u0302\u2217(I\u0302r, :)U\u0302\u2032\u2217(I\u0302r, :))\u22121, which give that Y\u0302\u2217,2 = Y\u0302\u2217, and the claim follows immediately.\nLemma A2 guarantees that the DiMSC and DiMSC-equivalence return same estimations for both row and column nodes\u2019s memberships. In this article, we introduce the DiMSC-equivalence algorithm since it is helpful to build a theoretical framework for DiMSC, see Remark A3 and A4 for detail."
        },
        {
            "heading": "Appendix D. Basic Properties of \u2126",
            "text": "Lemma A3. Under DiDCMMnr ,nc(K, P, \u03a0r, \u03a0c, \u0398r), we have\n\u03b8r,min \u03b8r,max \u221a K\u03bb1(\u03a0\u2032r\u03a0r) \u2264 \u2016U(i, :)\u2016F \u2264\n\u03b8r,max \u03b8r,min \u221a\n\u03bbK(\u03a0\u2032r\u03a0r) , 1 \u2264 i \u2264 nr,\u221a\n1 K\u03bb1(\u03a0\u2032c\u03a0c)\n\u2264 \u2016V(j, :)\u2016F \u2264 \u221a\n1 \u03bbK(\u03a0\u2032c\u03a0c) , 1 \u2264 j \u2264 nc.\nProof. Since I = U\u2032U = U\u2032(Ir, :)\u0398\u22121r (Ir, Ir)\u03a0\u2032r\u03982r \u03a0r\u0398\u22121(Ir, Ir)U(Ir, :), we have\n((\u0398\u22121r (Ir, Ir)U(Ir, :))((\u0398\u22121r (Ir, Ir)U(Ir, :))\u2032)\u22121 = \u03a0\u2032r\u03982r \u03a0r,\nwhich gives that\nmaxk\u2016e\u2032k(\u0398 \u22121 r (Ir, Ir)U(Ir, :))\u20162F = maxke\u2032k(\u0398 \u22121 r (Ir, Ir)U(Ir, :))(\u0398\u22121r (Ir, Ir)U(Ir, :))\u2032ek\n\u2264 max\u2016x\u2016F=1x \u2032(\u0398\u22121r (Ir, Ir)U(Ir, :))(\u0398\u22121r (Ir, Ir)U(Ir, :))\u2032x\n= \u03bb1((\u0398\u22121r (Ir, Ir)U(Ir, :))(\u0398\u22121r (Ir, Ir)U(Ir, :))\u2032) = 1\n\u03bbK(\u03a0\u2032r\u03982r \u03a0r) \u2264 1 \u03b82r,min\u03bbK(\u03a0 \u2032 r\u03a0r) .\nSimilarly, we have\nmink\u2016e\u2032k(\u0398 \u22121 r (Ir, Ir)U(Ir, :))\u20162F \u2265 1 \u03bb1(\u03a0\u2032r\u03982r \u03a0r) \u2265 1 \u03b82r,max\u03bb1(\u03a0\u2032r\u03a0r) .\nSince U(i, :) = e\u2032iU = e \u2032 i\u0398r\u03a0r\u0398 \u22121 r (Ir, Ir)U(Ir, :) = \u03b8r(i)\u03a0r(i, :)\u0398\u22121r (Ir, Ir)U(Ir, :) for 1 \u2264 i \u2264 nr, we have\n\u2016U(i, :)\u2016F = \u2016\u03b8r(i)\u03a0r(i, :)\u0398\u22121r (Ir, Ir)U(Ir, :)\u2016F = \u03b8r(i)\u2016\u03a0r(i, :)\u0398\u22121r (Ir, Ir)U(Ir, :)\u2016F \u2264 \u03b8r(i)maxi\u2016\u03a0r(i, :)\u2016Fmaxi\u2016e\u2032i(\u0398\u22121r (Ir, Ir)U(Ir, :))\u2016F\n\u2264 \u03b8r(i)maxi\u2016e\u2032i(\u0398\u22121r (Ir, Ir)U(Ir, :))\u2016F \u2264 \u03b8r,max \u03b8r,min \u221a \u03bbK(\u03a0\u2032r\u03a0r) .\nSimilarly, we have\n\u2016U(i, :)\u2016F \u2265 \u03b8r(i)mini\u2016\u03a0r(i, :)\u2016Fmini\u2016e\u2032i(\u0398\u22121r (Ir, Ir)U(Ir, :))\u2016F\n\u2265 \u03b8r(i)mini\u2016e\u2032i(\u0398\u22121r (Ir, Ir)U(Ir, :))\u2016F/ \u221a\nK \u2265 \u03b8r,min \u03b8r,max \u221a K\u03bb1(\u03a0\u2032r\u03a0r) .\nFor \u2016V(j, :)\u2016F, since V = \u03a0cBc, we have\nminj\u2016e\u2032jV\u20162F = minje\u2032jVV\u2032ej = minj\u03a0c(j, :)BcB\u2032c\u03a0\u2032c(j, :)\n= minj\u2016\u03a0c(j, :)\u20162F \u03a0c(j, :) \u2016\u03a0c(j, :)\u2016F BcB\u2032c \u03a0\u2032c(j, :) \u2016\u03a0c(j, :)\u2016F \u2265 minj\u2016\u03a0c(j, :)\u20162Fmin\u2016x\u2016F=1x \u2032BcB\u2032cx = minj\u2016\u03a0c(j, :)\u20162F\u03bbK(BcB\u2032c)\nBy Lemma A4 = minj\u2016\u03a0c(j, :)\u20162F \u03bb1(\u03a0\u2032c\u03a0c) \u2265 1 K\u03bb1(\u03a0\u2032c\u03a0c) .\nMeanwhile,\nmaxj\u2016e\u2032jV\u20162F = maxj\u2016\u03a0c(j, :)\u20162F \u03a0c(j, :) \u2016\u03a0c(j, :)\u2016F BcB\u2032c \u03a0\u2032c(j, :) \u2016\u03a0c(j, :)\u2016F\n\u2264 maxj\u2016\u03a0c(j, :)\u20162Fmax\u2016x\u2016F=1x \u2032BcB\u2032cx = maxj\u2016\u03a0c(j, :)\u20162F\u03bbK(BcB\u2032c)\nBy Lemma A4 = maxj\u2016\u03a0c(j, :)\u20162F \u03bbK(\u03a0\u2032c\u03a0c) \u2264 1 \u03bbK(\u03a0\u2032c\u03a0c) .\nLemma A4. Under DiDCMMnr ,nc(K, P, \u03a0r, \u03a0c, \u0398r), we have\n\u03b82r,min\u03bbK(\u03a0 \u2032 r\u03a0r)\n\u03b82r,max\u03bb1(\u03a0\u2032r\u03a0r) \u2264 \u03bbK(U\u2217(Ir, :)U\u2032\u2217(Ir, :)), \u03bb1(U\u2217(Ir, :)U\u2032\u2217(Ir, :)) \u2264 \u03b82r,maxK\u03bb1(\u03a0\u2032r\u03a0r) \u03b82r,min\u03bbK(\u03a0 \u2032 r\u03a0r) , and \u03bb1(BcB\u2032c) = 1\n\u03bbK(\u03a0\u2032c\u03a0c) , \u03bbK(BcB\u2032c) = 1 \u03bb1(\u03a0\u2032c\u03a0c) .\nProof. Recall that V = \u03a0cBc and V\u2032V = I, we have I = B\u2032c\u03a0\u2032c\u03a0cBc. As Bc is full rank, we have \u03a0\u2032c\u03a0c = (BcB\u2032c)\u22121, which gives\n\u03bb1(BcB\u2032c) = 1\n\u03bbK(\u03a0\u2032c\u03a0c) , \u03bbK(BcB\u2032c) = 1 \u03bb1(\u03a0\u2032c\u03a0c) .\nBy the proof of Lemma 2, we know that\n(U\u2217(Ir, :)U\u2032\u2217(Ir, :))\u22121 = N\u22121U (Ir, Ir)\u0398 \u22121(Ir, Ir)\u03a0\u2032r\u03982r \u03a0r\u0398\u22121r (Ir, Ir)N\u22121U (Ir, Ir),\nwhich gives that\nU\u2217(Ir, :)U\u2032\u2217(Ir, :) = NU(Ir, Ir)\u0398(Ir, Ir)(\u03a0\u2032r\u03982r \u03a0r)\u22121\u0398r(Ir, Ir)NU(Ir, Ir).\nThen, we have\n\u03bb1(U\u2217(Ir, :)U\u2032\u2217(Ir, :)) = \u03bb1(NU(Ir, Ir)\u0398(Ir, Ir)(\u03a0\u2032r\u03982r \u03a0r)\u22121\u0398r(Ir, Ir)NU(Ir, Ir)) = \u03bb1(N2U(Ir, Ir)\u03982r (Ir, Ir)(\u03a0\u2032r\u03982r \u03a0r)\u22121) \u2264 \u03bb21(NU(Ir, Ir)\u0398r(Ir, Ir))\u03bb1((\u03a0\u2032r\u03982r \u03a0r)\u22121) = \u03bb21(NU(Ir, Ir)\u0398r(Ir, Ir))/\u03bbK(\u03a0\u2032r\u03982r \u03a0r) \u2264 (maxi\u2208Ir \u03b8r(i)/\u2016U(i, :)\u2016F) 2/\u03bbK(\u03a0\u2032r\u0398 2 r \u03a0r)\n\u2264 \u03b82r,maxK\u03bb1(\u03a0\u2032r\u03a0r)\n\u03bbK(\u03a0\u2032r\u03982r \u03a0r) \u2264 \u03b82r,maxK\u03bb1(\u03a0\u2032r\u03a0r) \u03b82r,min\u03bbK(\u03a0 \u2032 r\u03a0r) .\nSimilarly, we have\n\u03bbK(U\u2217(Ir, :)U\u2032\u2217(Ir, :)) = \u03bbK(NU(Ir, Ir)\u0398(Ir, Ir)(\u03a0\u2032r\u03982r \u03a0r)\u22121\u0398r(Ir, Ir)NU(Ir, Ir)) = \u03bbK(N2U(Ir, Ir)\u03982r (Ir, Ir)(\u03a0\u2032r\u03982r \u03a0r)\u22121) \u2265 \u03bb2K(NU(Ir, Ir)\u0398r(Ir, Ir))\u03bbK((\u03a0\u2032r\u03982r \u03a0r)\u22121) = \u03bb2K(NU(Ir, Ir)\u0398r(Ir, Ir))/\u03bb1(\u03a0\u2032r\u03982r \u03a0r) \u2265 (mini\u2208Ir \u03b8r(i)/\u2016U(i, :)\u2016F) 2/\u03bb1(\u03a0\u2032r\u0398 2 r \u03a0r)\n\u2265 \u03b82r,min\u03bbK(\u03a0 \u2032 r\u03a0r)\n\u03bb1(\u03a0\u2032r\u03982r \u03a0r) \u2265\n\u03b82r,min\u03bbK(\u03a0 \u2032 r\u03a0r) \u03b82r,max\u03bb1(\u03a0\u2032r\u03a0r) .\nLemma A5. Under DiDCMMnr ,nc(K, P, \u03a0r, \u03a0c, \u0398r), we have\n\u03c3K(\u2126) \u2265 \u03b8r,min\u03c3K(P)\u03c3K(\u03a0r)\u03c3K(\u03a0c), \u03c31(\u2126) \u2264 \u03b8r,max\u03c31(P)\u03c31(\u03a0r)\u03c31(\u03a0c).\nProof. For \u03c3K(\u2126), we have\n\u03c32K(\u2126) = \u03bbK(\u2126\u2126 \u2032) = \u03bbK(\u0398r\u03a0rP\u03a0\u2032c\u03a0cP \u2032\u03a0\u2032r\u0398r) = \u03bbK(\u0398 2 r \u03a0rP\u03a0 \u2032 c\u03a0cP \u2032\u03a0\u2032r)\n\u2265 \u03b82r,min\u03bbK(\u03a0\u2032r\u03a0rP\u03a0\u2032c\u03a0cP\u2032) \u2265 \u03b82r,min\u03bbK(\u03a0\u2032r\u03a0r)\u03bbK(P\u03a0\u2032c\u03a0cP\u2032) = \u03b82r,min\u03bbK(\u03a0\u2032r\u03a0r)\u03bbK(\u03a0\u2032c\u03a0cP\u2032P) \u2265 \u03b82r,min\u03bbK(\u03a0\u2032r\u03a0r)\u03bbK(\u03a0\u2032c\u03a0c)\u03bbK(PP\u2032) = \u03b82r,min\u03c32K(\u03a0r)\u03c32K(\u03a0c)\u03c32K(P),\nwhere we have used the fact for any matrices X, Y, the nonzero eigenvalues of XY are the same as the nonzero eigenvalues of YX. Following a similar analysis, the lemma follows.\nLemma A6. Under DiDCMMnr ,nc(K, P, \u03a0r, \u03a0c, \u0398r), when Assumption 1 holds, with probability at least 1\u2212 o((nr + nc)\u22123), we have\n\u2016A\u2212\u2126\u2016 = O( \u221a\nPmaxmax(\u2016\u03b8r\u20161, \u03b8r,maxnc)log(nr + nc)).\nProof. Since the proof is similar to that of Lemma 7 [35], we omit most of the details. Let ei be an nr \u00d7 1 vector, where ei(i) = 1 and 0 elsewhere, for row nodes 1 \u2264 i \u2264 nr, and e\u0303j be an nc \u00d7 1 vector, where e\u0303j(j) = 1 and 0 elsewhere, for column nodes 1 \u2264 j \u2264 nc. Set W = \u2211n r\ni=1 \u2211 nc j=1 W(i, j)ei e\u0303 \u2032 j, where W = A\u2212\u2126. Set W(i,j) = W(i, j)ei e\u0303\u2032j, for 1 \u2264 i \u2264 nr, 1 \u2264 j \u2264 nc.\nThen, we have E(W(i,j)) = 0. For 1 \u2264 i \u2264 nr, 1 \u2264 j \u2264 nc, we have\n\u2016W(i,j)\u2016 = \u2016W(i, j)ei e\u0303\u2032j\u2016 = |A(i, j)\u2212\u2126(i, j)| \u2264 1.\nNext, we consider the variance parameter\n\u03c32 := max(\u2016 nr\n\u2211 i=1\nnc\n\u2211 j=1\nE(W(i,j)(W(i,j))\u2032)\u2016, \u2016 nr\n\u2211 i=1\nnc\n\u2211 j=1\nE((W(i,j))\u2032W(i,j))\u2016).\nSince\nE(W2(i, j)) = E((A(i, j)\u2212\u2126(i, j))2) = Var(A(i, j)),\nwhere Var(A(i, j)) denotes the variance of the Bernoulli random variable A(i, j), we have\nE(W2(i, j)) = Var(A(i, j)) = P(A(i, j) = 1)(1\u2212 P(A(i, j) = 1)) \u2264 P(A(i, j) = 1) = \u2126(i, j) = e\u2032i\u0398r\u03a0rP\u03a0\u2032c e\u0303j = \u03b8r(i)e\u2032i\u03a0rP\u03a0\u2032c e\u0303j \u2264 \u03b8r(i)Pmax.\nSince eie\u2032i is an nr \u00d7 nr diagonal matrix with (i, i)-th entry being one and other entries being zero, we have\n\u2016 nr \u2211 i=1 nc \u2211 j=1 E(W(i,j)(W(i,j))\u2032)\u2016 = \u2016 nr \u2211 i=1 nc \u2211 j=1 E(W2(i, j))eie\u2032i\u2016 = max1\u2264i\u2264nr | nc \u2211 j=1 E(W2(i, j))| \u2264 \u03b8r,maxPmaxnc.\nSimilarly, we have \u2016\u2211nri=1 \u2211 nc j=1 E((W (i,j))\u2032W(i,j))\u2016 \u2264 Pmax\u2016\u03b8r\u20161, which gives that\n\u03c32 = max(\u2016 nr \u2211 i=1 nc \u2211 j=1 E(W(i,j)(W(i,j))\u2032)\u2016, \u2016 nr \u2211 i=1 nc \u2211 j=1 E((W(i,j))\u2032W(i,j))\u2016) \u2264 Pmaxmax(\u2016\u03b8r\u20161, \u03b8r,maxnc).\nBy the rectangular version of the Bernstein inequality [71], combining with \u03c32 \u2264 Pmaxmax(\u2016\u03b8r\u20161, \u03b8r,maxnc), R = 1, d1 + d2 = nr + nc, set t = \u03b1+1+ \u221a \u03b12+20\u03b1+19\n3\n\u221a Pmaxmax(\u2016\u03b8r\u20161, \u03b8r,maxnc)log(nr + nc) for any \u03b1 > 0, we have\nP(\u2016W\u2016 \u2265 t) = P(\u2016 nr\n\u2211 i=1 nc \u2211 j=1 W(i,j)\u2016 \u2265 t) \u2264 (nr + nc)exp(\u2212 t2/2 \u03c32 + Rt3 )\n\u2264 (nr + nc)exp(\u2212 t2/2\nPmaxmax(\u2016\u03b8r\u20161, \u03b8r,maxnc) + t/3 )\n= (nr + nc)exp(\u2212(\u03b1 + 1)log(nr + nc) \u00b7 1\n18 ( \u221a \u03b1+19+ \u221a \u03b1+1)2 + 2\n\u221a \u03b1+1\u221a\n\u03b1+19+ \u221a \u03b1+1\n\u221a log(nr+nc)\nPmaxmax(\u2016\u03b8r\u20161,\u03b8r,maxnc)\n)\n\u2264 (nr + nc)exp(\u2212(\u03b1 + 1)log(nr + nc)) = 1\n(nr + nc)\u03b1 ,\nwhere we have used Assumption 1 in the last inequality. Set \u03b1 = 3, and the claim follows."
        },
        {
            "heading": "Appendix E. Proof of Consistency for DiMSC",
            "text": "Similar to [24,26,27], for our DiMSC, the main theoretical results (i.e., Theorem 2) rely on the row-wise singular vector deviation bounds for the singular eigenvectors of the adjacency matrix.\nLemma A7. (Row-wise singular vector deviation) Under DiDCMMnr ,nc(K, P, \u03a0r, \u03a0c, \u0398r), when Assumption 1 holds, suppose \u03c3K(\u2126) \u2265 C \u221a \u03b8r,max(nr + nc)log(nr + nc), with probability at least 1\u2212 o((nr + nc)\u22123), we have\nmax(\u2016U\u0302U\u0302\u2032 \u2212UU\u2032\u20162\u2192\u221e, \u2016V\u0302V\u0302\u2032 \u2212VV\u2032\u20162\u2192\u221e) = O( \u221a\nPmax\u03b8r,maxKlog(nr + nc) \u03b8r,min\u03c3K(P)\u03c3K(\u03a0r)\u03c3K(\u03a0c) ).\nProof. Let HU\u0302 = U\u0302 \u2032U, and HU\u0302 = UHU\u0302 \u03a3HU\u0302 V \u2032 HU\u0302 be the SVD decomposition of HU\u0302 with UHU\u0302 , VHU\u0302 \u2208 R nr\u00d7K, where UHU\u0302 and VHU\u0302 represent, respectively, the left and right sin-\ngular matrices of HU\u0302 . Define sgn(HU\u0302) = UHU\u0302 V \u2032 HU\u0302 ; sgn(HV\u0302) is defined similarly. Since E(A(i, j)\u2212\u2126(i, j)) = 0, E[(A(i, j)\u2212\u2126(i, j))2] \u2264 \u03b8r(i)Pmax \u2264 \u03b8r,maxPmax by the proof of Lemma A6, 1\u221a\n\u03b8r,maxPmaxmin(nr ,nc)/(\u00b5log(nr+nc)) \u2264 O(1) holds by Assumption 1, where \u00b5 is the\nincoherence parameter defined as \u00b5 = max( nr\u2016U\u2016 2 2\u2192\u221e K , nc\u2016V\u201622\u2192\u221e\nK ). By Theorem 4.4 [64], with high probability, we have below row-wise singular vector deviation\nmax(\u2016U\u0302sgn(HU\u0302)\u2212U\u20162\u2192\u221e, \u2016V\u0302sgn(HV\u0302)\u2212V\u20162\u2192\u221e)\n\u2264 C\n\u221a Pmax\u03b8r,maxK(\u03ba(\u2126) \u221a max(nr ,nc)\u00b5 min(nr ,nc) + \u221a log(nr + nc))\n\u03c3K(\u2126) \u2264 C \u221a\nPmax\u03b8r,maxKlog(nr + nc) \u03c3K(\u2126) , (A1)\nprovided that c1\u03c3K(\u2126) \u2265 \u221a \u03b8r,maxPmax(nr + nc)log(nr + nc) for some sufficiently small\nconstant c1, and here we set \u221a\nmax(nr ,nc)\u00b5 min(nr ,nc) = O(1) for convenience since this term has little\neffect on the error bounds of DiMSC, especially for the case when nrnc = O(1). Since U\u2032U = I, U\u0302\u2032U\u0302 = I, we have \u2016U\u0302U\u0302\u2032 \u2212UU\u2032\u20162\u2192\u221e \u2264 2\u2016U \u2212 U\u0302sgn(HU\u0302)\u20162\u2192\u221e by basic algebra. Now, we are ready to bound \u2016U\u0302U\u0302\u2032 \u2212UU\u2032\u20162\u2192\u221e:\n\u2016U\u0302U\u0302\u2032 \u2212UU\u2032\u20162\u2192\u221e = max1\u2264i\u2264nr\u2016e \u2032 i(UU \u2032 \u2212 U\u0302U\u0302\u2032)\u2016F \u2264 2\u2016U \u2212 U\u0302sgn(HU\u0302)\u20162\u2192\u221e \u2264 C \u221a\nPmax\u03b8r,maxKlog(nr + nc) \u03c3K(\u2126) By Lemma A5 \u2264 C\n\u221a Pmax\u03b8r,maxKlog(nr + nc) \u03b8r,min\u03c3K(P)\u03c3K(\u03a0r)\u03c3K(\u03a0c) .\nThe lemma holds by following similar proof for \u2016V\u0302V\u0302\u2032 \u2212VV\u2032\u20162\u2192\u221e.\nWhen \u0398r = \u03c1I, nr = nc, \u03a0r = \u03a0c = \u03a0, Pmax = O(1), and DiCCMM degenerates\nto MMSB, the bound in Lemma A7 is O( \u221a\nKlog(n) \u03c3K(P) \u221a \u03c1\u03bbK(\u03a0\u2032\u03a0) ). if we further assume that\n\u03bbK(\u03a0\u2032\u03a0) = O( nK ) and K = O(1), the bound is of order O( 1 \u03c3K(P) 1\u221a n\n\u221a log(n)\n\u03c1n ). Set the \u0398 in [24] as \u221a \u03c1I, their DCMM degenerates to MMSB, their assumptions are translated to our \u03bbK(\u03a0\u2032\u03a0) = O( nK ), when K = O(1), the row-wise singular vector deviation bound in the fourth bullet of Lemma 2.1 [24] is O( 1 \u03c3K(P) 1\u221a n \u221a log(n) \u03c1n ), which is consistent with ours. Meanwhile, if we further assume that \u03c3K(P) = O(1), the bound is of order 1\u221an \u221a log(n)\n\u03c1n . The next lemma is the cornerstone to characterizing the behaviors of DiMSC.\nLemma A8. Under DiDCMMnr ,nc(K, P, \u03a0r, \u03a0c, \u0398r), when conditions of Lemma A7 hold, there exist two permutation matrices Pr,Pc \u2208 RK\u00d7K such that with probability at least 1\u2212 o((nr + nc)\u22123), we have\nmax1\u2264k\u2264K\u2016e\u2032k(U\u0302\u2217,2(I\u0302r, :)\u2212P \u2032 rU\u2217,2(Ir, :))\u2016F = O(\nK3\u03b811r,maxv\u03ba3(\u03a0\u2032r\u03a0r)\u03bb1.51 (\u03a0 \u2032 r\u03a0r)\n\u03b811r,min\u03c0r,min ),\nmax1\u2264k\u2264K\u2016e\u2032k(V\u03022(I\u0302c, :)\u2212P \u2032 cV2(Ic, :))\u2016F = O(v\u03ba(\u03a0\u2032c\u03a0c)).\nProof. First, we consider column nodes. The detail of the SP algorithm is in Algorithm A2.\nAlgorithm A2 Successive Projection (SP) [54]\nRequire: Near-separable matrix Ysp = Ssp Msp + Zsp \u2208 Rm\u00d7n+ , where Ssp, Msp should satisfy Assumption 1 [54], the number r of columns to be extracted. Ensure: Set of indices K such that Y(K, :) \u2248 S (up to permutation) 1: Compute U\u0302r \u2208 Rnr\u00d7Kr and U\u0302c \u2208 Rnc\u00d7Kr from the top-Kr-dimensional SVD of A. 2: Let R = Ysp,K = {}, k = 1. 3: While R 6= 0 and k \u2264 r do 4: k\u2217 = argmaxk\u2016R(k, :)\u2016F. 5: uk = R(k\u2217, :).\n6: R\u2190 (I \u2212 uku \u2032 k\n\u2016uk\u20162F )R.\n7: K = K \u222a {k\u2217}. 8: k=k+1. 9: end while\nBased on Algorithm A2, the following theorem is Theorem 1.1 in [54].\nTheorem A1. Fix m \u2265 r and n \u2265 r. Consider a matrix Ysp = Ssp Msp + Zsp, where Ssp \u2208 Rm\u00d7r has a full column rank, Msp \u2208 Rr\u00d7n is a nonnegative matrix such that the sum of each column is at most 1, and Zsp = [Zsp,1, . . . , Zsp,n] \u2208 Rm\u00d7n. Suppose Msp has a submatrix equal to Ir. Write e \u2264 max1\u2264i\u2264n\u2016Zsp,i\u2016F. Suppose e = O(\n\u03c3min(Ssp)\u221a r\u03ba2(Ssp)\n), where \u03c3min(Ssp) and \u03ba(Ssp) are the minimum singular value and condition number of Ssp, respectively. If we apply the SP algorithm to columns of Ysp, then it outputs an index set K \u2282 {1, 2, . . . , n} such that |K| = r and max1\u2264k\u2264rminj\u2208K\u2016Ssp(:, k) \u2212 Ysp(:, j)\u2016F = O(e\u03ba2(Ssp)), where Ssp(:, k) is the k-th column of Ssp.\nLet m = K, r = K, n = nc, Ysp = V\u0302\u20322, Zsp = V\u0302 \u2032 2 \u2212V\u20322, Ssp = V\u20322(Ic, :), and Msp = \u03a0\u2032c. By\nCondition (I2), Msp has an identity submatrix IK. By Lemma A7, we have\nec = max1\u2264j\u2264nc\u2016V\u03022(j, :)\u2212V2(j, :)\u2016F = \u2016V\u03022(j, :)\u2212V2(j, :)\u20162\u2192\u221e \u2264 v.\nBy Theorem A1, there exists a permutation matrix Pc such that\nmax1\u2264k\u2264K\u2016e\u2032k(V\u03022(I\u0302c, :)\u2212P \u2032 cV2(Ic, :))\u2016F = O(ec\u03ba2(V2(Ic, :)) \u221a K) = O(v\u03ba2(V2(Ic, :))).\nSince \u03ba2(V2(Ic, :)) = \u03ba(V2(Ic, :)V\u20322(Ic, :)) = \u03ba(V(Ic, :)V\u2032(Ic, :)) = \u03ba(\u03a0\u2032c\u03a0c), where the last equality holds by Lemma A4, we have\nmax1\u2264k\u2264K\u2016e\u2032k(V\u03022(I\u0302c, :)\u2212P \u2032 cV2(Ic, :))\u2016F = O(v\u03ba(\u03a0\u2032c\u03a0c)).\nRemark A1. For the ideal case, let m = K, r = K, n = nc, Ysp = V\u2032, Zsp = V\u2032 \u2212V\u2032 \u2261 0, Ssp = V\u2032(Ic, :), and Msp = \u03a0\u2032c. Then, we have max1\u2264j\u2264nc\u2016V(j, :)\u2212 V(j, :)\u2016F = 0. By Theorem A1, SP algorithm returns Ic when the input is V assuming there are K column communities.\nNow, we consider row nodes. From Lemma 2, we see that U\u2217(Ir, :) satisfies Condition 1 in [26]. Meanwhile, since (U\u2217(Ir, :)U\u2032\u2217(Ir, :))\u221211 > 0, we have (U\u2217(Ir, :)U\u2032\u2217(Ir, :))\u221211 \u2265 \u03b71, hence U\u2217(Ir, :) satisfies Condition 2 in [26]. Now, we give a lower bound for \u03b7 to show that \u03b7 is strictly positive. By the proof of Lemma A4, we have\n(U\u2217(Ir, :)U\u2032\u2217(Ir, :))\u22121 = N\u22121U (Ir, Ir)\u0398 \u22121(Ir, Ir)\u03a0\u2032r\u03982r \u03a0r\u0398\u22121r (Ir, Ir)N\u22121U (Ir, Ir)\n\u2265 \u03b82r,min\n\u03b82r,maxN2U,max \u03a0\u2032r\u03a0r \u2265 \u03b84r,min \u03b84r,maxK\u03bb1(\u03a0\u2032r\u03a0r) \u03a0\u2032r\u03a0r,\nwhere we set NU,max = max1\u2264i\u2264nr NU(i, i), and we have used the facts that NU , \u0398r are\ndiagonal matrices, and NU,max \u2264 \u03b8r,max \u221a K\u03bb1(\u03a0\u2032r\u03a0r) \u03b8r,min by Lemma A3. Then, we have\n\u03b7 = min1\u2264k\u2264K((U\u2217(Ir, :)U\u2032\u2217(Ir, :))\u221211)(k) \u2265 \u03b84r,min\n\u03b84r,maxK\u03bb1(\u03a0\u2032r\u03a0r) min1\u2264k\u2264Ke\u2032k\u03a0 \u2032 r\u03a0r1\n= \u03b84r,min\n\u03b84r,maxK\u03bb1(\u03a0\u2032r\u03a0r) min1\u2264k\u2264Ke\u2032k\u03a0 \u2032 r1 =\n\u03b84r,min\u03c0r,min\n\u03b84r,maxK\u03bb1(\u03a0\u2032r\u03a0r) ,\ni.e., \u03b7 is strictly positive. Since U\u2217,2(Ir, :)U\u2032\u2217,2(Ir, :) \u2261 U\u2217(Ir, :)U\u2032\u2217(Ir, :), we have U\u2217,2(Ir, :) also satisfies Conditions 1 and 2 in [26]. The above analysis shows that we can directly apply Lemma F.1 of [26] since the ideal DiMSC algorithm satisfies Conditions 1 and 2 in [26], therefore there exists a permutation matrix Pr \u2208 RK\u00d7K such that\nmax1\u2264k\u2264K\u2016e\u2032k(U\u0302\u2217,2(I\u0302r, :)\u2212P \u2032 rU\u2217,2(Ir, :))\u2016F = O(\n\u221a K\u03b6er\n\u03bb1.5K (U\u2217,2(Ir, :))U\u2032\u2217,2(Ir, :) ),\nwhere \u03b6 \u2264 4K \u03b7\u03bb1.5K (U\u2217,2(Ir ,:)U\u2032\u2217,2(Ir ,:)) = O( K \u03b7\u03bb1.5K (U\u2217(Ir ,:)U\u2032\u2217(Ir ,:)) ), and er = max1\u2264i\u2264nr\u2016U\u0302\u2217,2(i, : )\u2212U\u2217,2(i, :)\u2016. Next, we bound er as below\n\u2016U\u0302\u2217,2(i, :)\u2212U\u2217,2(i, :)\u2016F = \u2016 U\u03022(i, :)\u2016U2(i, :)\u2016F \u2212U2(i, :)\u2016U\u03022(i, :)\u2016F\n\u2016U\u03022(i, :)\u2016F\u2016U2(i, :)\u2016F \u2016F \u2264 2\u2016U\u03022(i, :)\u2212U2(i, :)\u2016F \u2016U2(i, :)\u2016F\n\u2264 2\u2016U\u03022 \u2212U2\u20162\u2192\u221e\u2016U2(i, :)\u2016F \u2264 2v\u2016U2(i, :)\u2016F = 2v \u2016(UU\u2032)(i, :)\u2016F = 2v \u2016U(i, :)U\u2032\u2016F = 2v \u2016U(i, :)\u2016F\n\u2264 2v \u03b8r,max \u221a\nK\u03bb1(\u03a0\u2032r\u03a0r) \u03b8r,min ,\nwhere the last inequality holds by Lemma A3. Then, we have er = O(v \u03b8r,max \u221a K\u03bb1(\u03a0\u2032r\u03a0r) \u03b8r,min\n). Finally, by Lemma A4, we have\nmax1\u2264k\u2264K\u2016e\u2032k(U\u0302\u2217,2(I\u0302r, :)\u2212P \u2032 rU\u2217,2(Ir, :))\u2016F = O(\nK3\u03b811r,maxv\u03ba3(\u03a0\u2032r\u03a0r)\u03bb1.51 (\u03a0 \u2032 r\u03a0r)\n\u03b811r,min\u03c0r,min ).\nRemark A2. For the ideal case, when setting U\u2217 as the input of the SVM-cone algorithm assuming there are K row communities, since \u2016U\u2217 \u2212U\u2217\u20162\u2192\u221e = 0, Lemma F.1; [26] guarantees that SVMcone algorithm returns Ir exactly. Meanwhile, another view to see that the SVM-cone algorithm exactly obtains Ir when the input is U\u2217 (also U2,\u2217) is given in Appendix F, which focuses on following the three steps of SVM-cone algorithm to show that it returns Ir with input U\u2217 (also U\u2217,2), instead of simply applying Lemma F.1. [26].\nLemma A9. Under DiDCMMnr ,nc(K, P, \u03a0r, \u03a0c, \u0398r), when conditions of Lemma A7 hold, with probability at least 1\u2212 o((nr + nc)\u22123), we have\nmax1\u2264i\u2264nr\u2016e \u2032 i(Z\u0302r \u2212 ZrPr)\u2016F = O(\nK5\u03b815r,maxv\u03ba4.5(\u03a0\u2032r\u03a0r)\u03ba(\u03a0c)\u03bb1.51 (\u03a0 \u2032 r\u03a0r)\n\u03b814r,min\u03c0r,min ),\nmax1\u2264j\u2264nc\u2016e \u2032 j(Z\u0302c \u2212 ZcPc)\u2016F = O(v\u03ba(\u03a0\u2032c\u03a0c) \u221a K\u03bb1(\u03a0\u2032c\u03a0c)).\nProof. First, we consider column nodes. Recall that V(Ic, :) = Bc. For convenience, set V\u0302(I\u0302c, :) = B\u0302c, V2(Ic, :) = B2c, V\u03022(I\u0302c, :) = B\u03022c. We bound \u2016e\u2032j(Z\u0302c \u2212 ZcPc)\u2016F when the input\nis V\u0302 in the SP algorithm. Recall that Zc = max(VV\u2032(Ic, :)(V(Ic, :)V\u2032(Ic, :))\u22121, 0) \u2261 \u03a0c, for 1 \u2264 j \u2264 nc, we have\n\u2016e\u2032j(Z\u0302c \u2212 ZcPc)\u2016F = \u2016e \u2032 j(max(0, V\u0302B\u0302 \u2032 c(B\u0302c B\u0302 \u2032 c) \u22121)\u2212VB\u2032c(BcB\u2032c)\u22121Pc)\u2016F\n\u2264 \u2016e\u2032j(V\u0302B\u0302 \u2032 c(B\u0302c B\u0302 \u2032 c) \u22121 \u2212VB\u2032c(BcB\u2032c)\u22121Pc)\u2016F = \u2016e\u2032j(V\u0302 \u2212V(V \u2032V\u0302))B\u0302\u2032c(B\u0302c B\u0302 \u2032 c) \u22121 + e\u2032j(V(V \u2032V\u0302)B\u0302\u2032c(B\u0302c B\u0302 \u2032 c) \u22121 \u2212V(V\u2032V\u0302)(P \u2032c(BcB\u2032c)(B\u2032c)\u22121(V\u2032V\u0302))\u22121)\u2016F \u2264 \u2016e\u2032j(V\u0302 \u2212V(V \u2032V\u0302))B\u0302\u2032c(B\u0302c B\u0302 \u2032 c) \u22121\u2016F + \u2016e\u2032jV(V \u2032V\u0302)(B\u0302\u2032c(B\u0302c B\u0302 \u2032 c) \u22121 \u2212 (P \u2032c(BcB\u2032c)(B\u2032c)\u22121(V\u2032V\u0302))\u22121)\u2016F \u2264 \u2016e\u2032j(V\u0302 \u2212V(V \u2032V\u0302))\u2016F\u2016B\u0302\u22121c \u2016F + \u2016e\u2032jV(V \u2032V\u0302)(B\u0302\u2032c(B\u0302c B\u0302 \u2032 c) \u22121 \u2212 (P \u2032c(BcB\u2032c)(B\u2032c)\u22121(V\u2032V\u0302))\u22121)\u2016F \u2264 \u221a\nK\u2016e\u2032j(V\u0302 \u2212V(V \u2032V\u0302))\u2016F/ \u221a \u03bbK(B\u0302c B\u0302\u2032c) + \u2016e\u2032jV(V \u2032V\u0302)(B\u0302\u22121c \u2212 (P \u2032cBc(V\u2032V\u0302))\u22121)\u2016F\n= \u221a\nK\u2016e\u2032j(V\u0302V\u0302 \u2032 \u2212VV\u2032)V\u0302\u2016FO( \u221a \u03bb1(\u03a0\u2032c\u03a0c)) + \u2016e\u2032jV(V \u2032V\u0302)(B\u0302\u22121c \u2212 (P \u2032cBc(V\u2032V\u0302))\u22121)\u2016F\n\u2264 \u221a\nK\u2016e\u2032j(V\u0302V\u0302 \u2032 \u2212VV\u2032)\u2016FO( \u221a \u03bb1(\u03a0\u2032c\u03a0c)) + \u2016e\u2032jV(V \u2032V\u0302)(B\u0302\u22121c \u2212 (P \u2032cBc(V\u2032V\u0302))\u22121)\u2016F\n\u2264 \u221a KvO( \u221a\n\u03bb1(\u03a0\u2032c\u03a0c)) + \u2016e\u2032jV(V \u2032V\u0302)(B\u0302\u22121c \u2212 (P \u2032cBc(V\u2032V\u0302))\u22121)\u2016F = O(v \u221a\nK\u03bb1(\u03a0\u2032c\u03a0c)) + \u2016e\u2032jV(V \u2032V\u0302)(B\u0302\u22121c \u2212 (P \u2032cBc(V\u2032V\u0302))\u22121)\u2016F,\nwhere we have used similar idea in the proof of Lemma VII.3 in [27] such that apply O( 1\n\u03bbK(BcB\u2032c) ) to estimate 1 \u03bbK(B\u0302c B\u0302\u2032c) , then by Lemma A4, we have 1 \u03bbK(B\u0302c B\u0302\u2032c) = O(\u03bb1(\u03a0\u2032c\u03a0c)).\nNow,we aim to bound \u2016e\u2032jV(V\u2032V\u0302)(B\u0302\u22121c \u2212 (P \u2032cBc(V\u2032V\u0302))\u22121)\u2016F. For convenience, set Tc = V\u2032V\u0302, Sc = P \u2032cBcTc. We have\n\u2016e\u2032jV(V \u2032V\u0302)(B\u0302\u22121c \u2212 (P \u2032cBc(V\u2032V\u0302))\u22121)\u2016F = \u2016e\u2032jVTcS \u22121 c (Sc \u2212 B\u0302c)B\u0302\u22121c \u2016F\n\u2264 \u2016e\u2032jVTcS \u22121 c (Sc \u2212 B\u0302c)\u2016F\u2016B\u0302\u22121c \u2016F \u2264 \u2016e\u2032jVTcS \u22121 c (Sc \u2212 B\u0302c)\u2016F\n\u221a K\n|\u03bbK(B\u0302c)|\n= \u2016e\u2032jVTcS \u22121 c (Sc \u2212 B\u0302c)\u2016F\n\u221a K\u221a\n\u03bbK(B\u0302c B\u0302\u2032c) \u2264 \u2016e\u2032jVTcS \u22121 c (Sc \u2212 B\u0302c)\u2016FO(\n\u221a K\u03bb1(\u03a0\u2032c\u03a0c))\n= \u2016e\u2032jVTcT \u22121 c B \u2032 c(BcB \u2032 c) \u22121Pc(Sc \u2212 B\u0302c)\u2016FO( \u221a K\u03bb1(\u03a0\u2032c\u03a0c))\n= \u2016e\u2032jVB \u2032 c(BcB \u2032 c) \u22121Pc(Sc \u2212 B\u0302c)\u2016FO( \u221a K\u03bb1(\u03a0\u2032c\u03a0c))\n= \u2016e\u2032jZcPc(Sc \u2212 B\u0302c)\u2016FO( \u221a K\u03bb1(\u03a0\u2032c\u03a0c)) By Zc=\u03a0c \u2264 max1\u2264k\u2264K\u2016e\u2032k(Sc \u2212 B\u0302c)\u2016FO( \u221a K\u03bb1(\u03a0\u2032c\u03a0c))\n= max1\u2264k\u2264K\u2016e\u2032k(B\u0302c \u2212P \u2032 cBcV \u2032V\u0302)\u2016FO( \u221a K\u03bb1(\u03a0\u2032c\u03a0c))\n= max1\u2264k\u2264K\u2016e\u2032k(B\u0302cV\u0302 \u2032 \u2212P \u2032cBcV\u2032)V\u0302\u2016FO( \u221a K\u03bb1(\u03a0\u2032c\u03a0c))\n\u2264 max1\u2264k\u2264K\u2016e\u2032k(B\u0302cV\u0302 \u2032 \u2212P \u2032cBcV\u2032)\u2016FO( \u221a K\u03bb1(\u03a0\u2032c\u03a0c))\n= max1\u2264k\u2264K\u2016e\u2032k(B\u03022c \u2212P \u2032 cB2c)\u2016FO( \u221a K\u03bb1(\u03a0\u2032c\u03a0c)) (A2)\n= O(v\u03ba(\u03a0\u2032c\u03a0c) \u221a K\u03bb1(\u03a0\u2032c\u03a0c)).\nRemark A3. Equation (A2) supports our statement that building the theoretical framework of DiMSC benefits a lot by introducing the DiMSC-equivalence algorithm since \u2016B\u03022c \u2212P \u2032cB2c\u20162\u2192\u221e is obtained from DiMSC-equivalence (i.e., inputing V\u03022 in the SP algorithm obtains \u2016B\u03022c \u2212P \u2032cB2c\u20162\u2192\u221e).\nThen, we have\n\u2016e\u2032j(Z\u0302c \u2212 ZcPc)\u2016F \u2264 O(v \u221a K\u03bb1(\u03a0\u2032c\u03a0c)) + \u2016e\u2032jV(V \u2032V\u0302)(B\u0302\u22121c \u2212 (P \u2032cBc(V\u2032V\u0302))\u22121)\u2016F\n\u2264 O(v \u221a K\u03bb1(\u03a0\u2032c\u03a0c)) + O(v\u03ba(\u03a0\u2032c\u03a0c) \u221a K\u03bb1(\u03a0\u2032c\u03a0c)) = O(v\u03ba(\u03a0\u2032c\u03a0c) \u221a K\u03bb1(\u03a0\u2032c\u03a0c)).\nNext, we consider row nodes. For 1 \u2264 i \u2264 nr, since Zr = Y\u2217 J\u2217, Z\u0302r = Y\u0302\u2217 J\u0302\u2217, we have\n\u2016e\u2032i(Z\u0302r \u2212 ZrPr)\u2016F = \u2016e\u2032i(max(0, Y\u0302\u2217 J\u0302\u2217)\u2212Y\u2217 J\u2217Pr)\u2016F \u2264 \u2016e\u2032i(Y\u0302\u2217 J\u0302\u2217 \u2212Y\u2217 J\u2217Pr)\u2016F = \u2016e\u2032i(Y\u0302\u2217 \u2212Y\u2217Pr) J\u0302\u2217 + e\u2032iY\u2217Pr( J\u0302\u2217 \u2212P \u2032r J\u2217Pr)\u2016F \u2264 \u2016e\u2032i(Y\u0302\u2217 \u2212Y\u2217Pr)\u2016F\u2016 J\u0302\u2217\u2016F + \u2016e\u2032iY\u2217Pr\u2016F\u2016 J\u0302\u2217 \u2212P \u2032r J\u2217Pr\u2016F = \u2016e\u2032i(Y\u0302\u2217 \u2212Y\u2217Pr)\u2016F\u2016 J\u0302\u2217\u2016F + \u2016e\u2032iY\u2217\u2016F\u2016 J\u0302\u2217 \u2212P \u2032r J\u2217Pr\u2016F.\nTherefore, the bound of \u2016e\u2032i(Z\u0302r \u2212 ZrPr)\u2016F can be obtained as long as we bound \u2016e\u2032i(Y\u0302\u2217 \u2212Y\u2217Pr)\u2016F, \u2016 J\u0302\u2217\u2016F, \u2016e\u2032iY\u2217\u2016F and \u2016 J\u0302\u2217 \u2212P \u2032r J\u2217Pr\u2016F. We bound the four terms as below: \u2022 We bound \u2016e\u2032i(Y\u0302\u2217\u2212Y\u2217Pr)\u2016F first. Similar as bounding \u2016e\u2032j(Z\u0302c\u2212 ZcPc)\u2016, we set U\u2217(Ir, :\n) = BR, U\u0302\u2217(I\u0302r, :) = B\u0302R, U\u2217,2(Ir, :) = B2R, U\u0302\u2217,2(I\u0302r, :) = B\u03022R for convenience. We bound \u2016e\u2032i(Y\u0302\u2217 \u2212 Y\u2217Pr)\u2016F when the input is U\u0302\u2217 in the SVM-cone algorithm. For 1 \u2264 i \u2264 nr, we have\n\u2016e\u2032i(Y\u0302\u2217 \u2212Y\u2217Pr)\u2016F = \u2016e \u2032 i(U\u0302B\u0302 \u2032 R(B\u0302R B\u0302 \u2032 R) \u22121 \u2212UB\u2032R(BRB\u2032R)\u22121Pr)\u2016F\n= \u2016e\u2032i(U\u0302 \u2212U(U \u2032U\u0302))B\u0302\u2032R(B\u0302R B\u0302 \u2032 R) \u22121 + e\u2032i(U(U \u2032U\u0302)B\u0302\u2032R(B\u0302R B\u0302 \u2032 R) \u22121\n\u2212U(U\u2032U\u0302)(P \u2032r(BRB\u2032R)(B\u2032R)\u22121(U\u2032U\u0302))\u22121)\u2016F \u2264 \u2016e\u2032i(U\u0302 \u2212U(U \u2032U\u0302))B\u0302\u2032R(B\u0302R B\u0302 \u2032 R) \u22121\u2016F + \u2016e\u2032iU(U \u2032U\u0302)(B\u0302\u2032R(B\u0302R B\u0302 \u2032 R) \u22121\n\u2212 (P \u2032r(BRB\u2032R)(B\u2032R)\u22121(U\u2032U\u0302))\u22121)\u2016F \u2264 \u2016e\u2032i(U\u0302 \u2212U(U \u2032U\u0302))\u2016F\u2016B\u0302\u22121R \u2016F + \u2016e \u2032 iU(U \u2032U\u0302)(B\u0302\u2032R(B\u0302R B\u0302 \u2032 R) \u22121 \u2212 (P \u2032r(BRB\u2032R)(B\u2032R)\u22121(U\u2032U\u0302))\u22121)\u2016F \u2264 \u221a\nK\u2016e\u2032i(U\u0302 \u2212U(U \u2032U\u0302))\u2016F/ \u221a \u03bbK(B\u0302R B\u0302\u2032R) + \u2016e \u2032 iU(U \u2032U\u0302)(B\u0302\u22121R \u2212 (P \u2032 r BR(U\n\u2032U\u0302))\u22121)\u2016F (i) = \u221a\nK\u2016e\u2032i(U\u0302U\u0302 \u2032 \u2212UU\u2032)U\u0302\u2016FO(\n\u03b8r,max \u221a\n\u03ba(\u03a0\u2032r\u03a0r) \u03b8r,min ) + \u2016e\u2032iU(U \u2032U\u0302)(B\u0302\u22121R \u2212 (P \u2032 r BR(U \u2032U\u0302))\u22121)\u2016F\n\u2264 \u221a\nK\u2016e\u2032i(U\u0302U\u0302 \u2032 \u2212UU\u2032)\u2016FO(\n\u03b8r,max \u221a\n\u03ba(\u03a0\u2032r\u03a0r) \u03b8r,min ) + \u2016e\u2032iU(U \u2032U\u0302)(B\u0302\u22121R \u2212 (P \u2032 r BR(U \u2032U\u0302))\u22121)\u2016F\n\u2264 \u221a KvO( \u03b8r,max\n\u221a \u03ba(\u03a0\u2032r\u03a0r)\n\u03b8r,min ) + \u2016e\u2032iU(U \u2032U\u0302)(B\u0302\u22121R \u2212 (P \u2032 r BR(U \u2032U\u0302))\u22121)\u2016F\n= O(v \u03b8r,max\n\u221a K\u03ba(\u03a0\u2032r\u03a0r)\n\u03b8r,min ) + \u2016e\u2032iU(U \u2032U\u0302)(B\u0302\u22121R \u2212 (P \u2032 r BR(U \u2032U\u0302))\u22121)\u2016F,\nwhere we have used similar idea in the proof of Lemma VII.3 in [27] such that we apply O( 1\n\u03bbK(BRB\u2032R) ) to estimate 1 \u03bbK(B\u0302R B\u0302\u2032R) , hence (i) holds by Lemma A4.\nNow, we aim to bound \u2016e\u2032iU(U\u2032U\u0302)(B\u0302 \u22121 R \u2212 (P \u2032rBR(U\u2032U\u0302))\u22121)\u2016F. For convenience, set Tr = U\u2032U\u0302, Sr = P \u2032rBRTr. We have\n\u2016e\u2032iU(U\u2032U\u0302)(B\u0302\u22121R \u2212 (P \u2032 rBR(U \u2032U\u0302))\u22121)\u2016F = \u2016e\u2032iUTrS\u22121r (Sr \u2212 B\u0302R)B\u0302\u22121R \u2016F\n\u2264 \u2016e\u2032iUTrS\u22121r (Sr \u2212 B\u0302R)\u2016F\u2016B\u0302\u22121R \u2016F \u2264 \u2016e \u2032 iUTrS \u22121 r (Sr \u2212 B\u0302R)\u2016F\n\u221a K\n|\u03bbK(B\u0302R)|\n= \u2016e\u2032iUTrS\u22121r (Sr \u2212 B\u0302R)\u2016F \u221a K\u221a \u03bbK(B\u0302R B\u0302\u2032R) \u2264 \u2016e\u2032iUTrS\u22121r (Sr \u2212 B\u0302R)\u2016FO( \u03b8r,max\n\u221a K\u03ba(\u03a0\u2032r\u03a0r)\n\u03b8r,min )\n= \u2016e\u2032iUTrT\u22121r B\u2032R(BRB\u2032R)\u22121Pr(Sr \u2212 B\u0302R)\u2016FO( \u03b8r,max\n\u221a K\u03ba(\u03a0\u2032r\u03a0r)\n\u03b8r,min )\n= \u2016e\u2032iUB\u2032R(BRB\u2032R)\u22121Pr(Sr \u2212 B\u0302R)\u2016FO( \u03b8r,max\n\u221a K\u03ba(\u03a0\u2032r\u03a0r)\n\u03b8r,min )\n= \u2016e\u2032iY\u2217Pr(Sr \u2212 B\u0302R)\u2016FO( \u03b8r,max\n\u221a K\u03bb1(\u03a0\u2032r\u03a0r)\n\u03b8r,min ) \u2264 \u2016e\u2032iY\u2217\u2016F\u2016Sr \u2212 B\u0302R\u2016FO(\n\u03b8r,max \u221a\nK\u03bb1(\u03a0\u2032r\u03a0r) \u03b8r,min )\nBy Equation(A4) \u2264\n\u03b82r,max \u221a\nK\u03bb1(\u03a0\u2032r\u03a0r) \u03b82r,min\u03bbK(\u03a0 \u2032 r\u03a0r) max1\u2264k\u2264K\u2016e\u2032k(Sr \u2212 B\u0302R)\u2016FO( \u03b8r,maxK\n\u221a \u03ba(\u03a0\u2032r\u03a0r)\n\u03b8r,min )\n= max1\u2264k\u2264K\u2016e\u2032k(B\u0302R \u2212P \u2032 rBRU \u2032U\u0302)\u2016FO( \u03b83r,maxK1.5\u03ba(\u03a0\u2032r\u03a0r) \u03b83r,min \u221a \u03bbK(\u03a0\u2032r\u03a0r) )\n= max1\u2264k\u2264K\u2016e\u2032k(B\u0302RU\u0302 \u2032 \u2212P \u2032rBRU\u2032)U\u0302\u2016FO(\n\u03b83r,maxK1.5\u03ba(\u03a0\u2032r\u03a0r) \u03b83r,min \u221a \u03bbK(\u03a0\u2032r\u03a0r) )\n\u2264 max1\u2264k\u2264K\u2016e\u2032k(B\u0302RU\u0302 \u2032 \u2212P \u2032rBRU\u2032)\u2016FO(\n\u03b83r,maxK1.5\u03ba(\u03a0\u2032r\u03a0r) \u03b83r,min \u221a \u03bbK(\u03a0\u2032r\u03a0r) )\n= max1\u2264k\u2264K\u2016e\u2032k(B\u03022R \u2212P \u2032 rB2R)\u2016FO(\n\u03b83r,maxK1.5\u03ba(\u03a0\u2032r\u03a0r) \u03b83r,min \u221a \u03bbK(\u03a0\u2032r\u03a0r) ) (A3)\nBy Lemma A8 = O( K4.5\u03b814r,maxv\u03ba4.5(\u03a0\u2032r\u03a0r)\u03bb1(\u03a0\u2032r\u03a0r) \u03b814r,min\u03c0r,min ).\nRemark A4. Similar as Equation (A2), Equation (A3) supports our statement that building the theoretical framework of DiMSC benefits a lot by introducing the DiMSC-equivalence algorithm since \u2016B\u03022R \u2212P \u2032rB2R\u20162\u2192\u221e is obtained from DiMSC-equivalence (i.e., inputing U\u0302\u2217,2 in the SVM-cone algorithm obtains \u2016B\u03022R \u2212P \u2032rB2R\u20162\u2192\u221e).\nThen, we have\n\u2016e\u2032i(Y\u0302\u2217 \u2212Y\u2217Pr)\u2016F \u2264 O(v \u03b8r,max\n\u221a K\u03ba(\u03a0\u2032r\u03a0r)\n\u03b8r,min ) + \u2016e\u2032iU(U\u2032U\u0302)(B\u0302\u22121R \u2212 (P \u2032 rBRU \u2032U\u0302))\u22121)\u2016F\n\u2264 O(v \u03b8r,max \u221a\nK\u03ba(\u03a0\u2032r\u03a0r) \u03b8r,min ) + O( K4.5\u03b814r,maxv\u03ba4.5(\u03a0\u2032r\u03a0r)\u03bb1(\u03a0\u2032r\u03a0r) \u03b814r,min\u03c0r,min )\n= O( K4.5\u03b814r,maxv\u03ba4.5(\u03a0\u2032r\u03a0r)\u03bb1(\u03a0\u2032r\u03a0r)\n\u03b814r,min\u03c0r,min ).\n\u2022 for \u2016e\u2032iY\u2217\u2016F, since Y\u2217 = UU\u22121\u2217 (Ir, :), by Lemmas (A3) and (A4), we have\n\u2016e\u2032iY\u2217\u2016F \u2264 \u2016U(i, :)\u2016F\u2016U \u22121 \u2217 (Ir, :)\u2016F \u2264\n\u221a K\u2016U(i, :)\u2016F\u221a\n\u03bbK(U\u2217(Ir, :)U\u2032\u2217(Ir, :)) \u2264\n\u03b82r,max \u221a\nK\u03bb1(\u03a0\u2032r\u03a0r) \u03b82r,min\u03bbK(\u03a0 \u2032 r\u03a0r) . (A4)\n\u2022 for \u2016 J\u0302\u2217\u2016F, recall that J\u0302\u2217 = diag(U\u0302\u2217( I\u0302r, :)\u039b\u0302V\u0302\u2032(I\u0302c, :)), we have\n\u2016 J\u0302\u2217\u2016 = max1\u2264k\u2264K J\u0302\u2217(k, k) = max1\u2264k\u2264Ke\u2032kU\u0302\u2217( I\u0302r, :)\u039b\u0302V\u0302 \u2032(I\u0302c, :)ek\n= max1\u2264k\u2264K\u2016e\u2032kU\u0302\u2217( I\u0302r, :)\u039b\u0302V\u0302 \u2032(I\u0302c, :)ek\u2016 \u2264 max1\u2264k\u2264K\u2016e\u2032kU\u0302\u2217( I\u0302r, :)\u2016\u2016\u039b\u0302\u2016\u2016V\u0302 \u2032(I\u0302c, :)ek\u2016 \u2264 max1\u2264k\u2264K\u2016e\u2032kU\u0302\u2217( I\u0302r, :)\u2016F\u2016\u039b\u0302\u2016\u2016V\u0302\n\u2032(I\u0302c, :)ek\u2016 = max1\u2264k\u2264K\u2016A\u2016\u2016V\u0302\u2032(I\u0302c, :)ek\u2016 = max1\u2264k\u2264K\u2016A\u2016\u2016(e\u2032kV\u0302(I\u0302c, :))\n\u2032\u2016 = max1\u2264k\u2264K\u2016A\u2016\u2016e\u2032kV\u0302(I\u0302c, :)\u2016 \u2264 max1\u2264k\u2264K\u2016A\u2016\u2016e\u2032kV\u0302(I\u0302c, :)\u2016F \u2264 \u2016A\u2016\u2016V\u0302\u20162\u2192\u221e = \u2016A\u2016\u2016V\u0302sgn(HV\u0302)\u2212V + V\u20162\u2192\u221e \u2264 \u2016A\u2016(\u2016V\u0302sgn(HV\u0302)\u2212V\u20162\u2192\u221e + \u2016V\u20162\u2192\u221e).\nBy Lemmas (A6) and (A5), \u2016A\u2016 = \u2016A\u2212\u2126 + \u2126\u2016 \u2264 \u2016A\u2212\u2126\u2016+ \u03c31(\u2126) \u2264 \u2016A\u2212\u2126\u2016+ \u03b8r,max\u03c31(P)\u03c31(\u03a0r)\u03c31(\u03a0c) = O(\u03b8r,max\u03c31(\u03a0r)\u03c31(\u03a0c)). By Lemma (A5) and Equation (A1),\n\u2016V\u0302sgn(HV\u0302)\u2212V\u20162\u2192\u221e \u2264 C \u221a \u03b8r,maxKlog(nr+nc) \u03b8r,min\u03c3K(P)\u03c3K(\u03a0r)\u03c3K(\u03a0c) . By Lemma A3, \u2016V\u20162\u2192\u221e \u2264 \u221a 1 \u03bbK(\u03a0\u2032c\u03a0c) ,\nwhich gives \u2016V\u0302sgn(HV\u0302)\u2212 V\u20162\u2192\u221e + \u2016V\u20162\u2192\u221e = O( \u221a 1 \u03bbK(\u03a0\u2032c\u03a0c) ) (this can be seen as\nsimply using \u2016V\u20162\u2192\u221e to estimate \u2016V\u0302\u20162\u2192\u221e since \u221a\n1 \u03bbK(\u03a0\u2032c\u03a0c) is the same order as\n\u221a \u03b8r,maxKlog(nr+nc)\n\u03b8r,min\u03c3K(P)\u03c3K(\u03a0r)\u03c3K(\u03a0c) ). Then, we have \u2016 J\u0302\u2217\u2016 = O(\u03b8r,max\u03c31(\u03a0r)\u03ba(\u03a0c)), which gives that\n\u2016 J\u0302\u2217\u2016F = O(\u03b8r,max \u221a\nK\u03c31(\u03a0r)\u03ba(\u03a0c)). \u2022 for \u2016 J\u0302\u2217 \u2212 P \u2032r J\u2217Pr\u2016F, since J\u2217 = NU(Ir, Ir)\u0398r(Ir, Ir), we have \u2016J\u2217\u2016 \u2264 NU,max\u03b8r,max \u2264\n\u03b82r,max \u221a\nK\u03bb1(\u03a0\u2032r\u03a0r) \u03b8r,min , which gives that \u2016J\u2217\u2016F \u2264 \u03b82r,maxK\u03c31(\u03a0r) \u03b8r,min . Thus, we have \u2016 J\u0302\u2217\u2212P \u2032r J\u2217Pr\u2016F =\nO( \u03b8 2 r,maxK\u03c31(\u03a0r)\n\u03b8r,min ).\nCombining the above results, we have\n\u2016e\u2032i(Z\u0302r \u2212 ZrPr)\u2016F \u2264 \u2016e\u2032i(Y\u0302\u2217 \u2212Y\u2217Pr)\u2016F\u2016 J\u0302\u2217\u2016F + \u2016e\u2032iY\u2217\u2016F\u2016 J\u0302\u2217 \u2212P \u2032r J\u2217Pr\u2016F\n= O( K4.5\u03b814r,maxv\u03ba4.5(\u03a0\u2032r\u03a0r)\u03bb1(\u03a0\u2032r\u03a0r)\n\u03b814r,min\u03c0r,min )O(\u03b8r,max\n\u221a K\u03c31(\u03a0r)\u03ba(\u03a0c))\n+ \u03b82r,max\n\u221a K\u03bb1(\u03a0\u2032r\u03a0r)\n\u03b82r,min\u03bbK(\u03a0r\u03a0r) O( \u03b82r,maxK\u03c31(\u03a0r) \u03b8r,min ) = O( K5\u03b815r,maxv\u03ba4.5(\u03a0\u2032r\u03a0r)\u03ba(\u03a0c)\u03bb1.51 (\u03a0 \u2032 r\u03a0r) \u03b814r,min\u03c0r,min ).\nAppendix E.1. Proof of Theorem 2\nProof. We bound \u2016e\u2032j(\u03a0\u0302c \u2212\u03a0cPc)\u20161 first. Recall that Zc = \u03a0c, \u03a0c(j, :) = Zc(j,:) \u2016Zc(j,:)\u20161 , \u03a0\u0302c(i, : ) = Z\u0302c(j,:)\u2016Z\u0302c(j,:)\u20161 , for 1 \u2264 j \u2264 nc, since\n\u2016e\u2032j(\u03a0\u0302c \u2212\u03a0cPc)\u20161 = \u2016 e\u2032jZ\u0302c\n\u2016e\u2032jZ\u0302c\u20161 \u2212 e\u2032jZcPc \u2016e\u2032jZcPc\u20161 \u20161 = \u2016 e\u2032jZ\u0302c\u2016e\u2032jZc\u20161 \u2212 e\u2032jZcPc\u2016e\u2032jZ\u0302c\u20161 \u2016e\u2032jZ\u0302c\u20161\u2016e\u2032jZc\u20161 \u20161\n= \u2016 e\u2032jZ\u0302c\u2016e\u2032jZc\u20161 \u2212 e\u2032jZ\u0302c\u2016e\u2032jZ\u0302c\u20161 + e\u2032jZ\u0302c\u2016e\u2032jZ\u0302c\u20161 \u2212 e\u2032jZcP\u2016e\u2032jZ\u0302c\u20161\n\u2016e\u2032jZ\u0302c\u20161\u2016e\u2032jZc\u20161 \u20161\n\u2264 \u2016e\u2032jZ\u0302c\u2016e\u2032jZc\u20161 \u2212 e\u2032jZ\u0302c\u2016e\u2032jZ\u0302c\u20161\u20161 + \u2016e\u2032jZ\u0302c\u2016e\u2032jZ\u0302c\u20161 \u2212 e\u2032jZcPc\u2016e\u2032jZ\u0302c\u20161\u20161\n\u2016e\u2032jZ\u0302c\u20161\u2016e\u2032jZc\u20161\n= |\u2016e\u2032jZc\u20161 \u2212 \u2016e\u2032jZ\u0302c\u20161|+ \u2016e\u2032jZ\u0302c \u2212 e\u2032jZcPc\u20161\n\u2016e\u2032jZc\u20161 \u2264 2\u2016e\u2032j(Z\u0302c \u2212 ZcPc)\u20161 \u2016e\u2032jZc\u20161\n= 2\u2016e\u2032j(Z\u0302c \u2212 ZcPc)\u20161\n\u2016e\u2032j\u03a0c\u20161 = 2\u2016e\u2032j(Z\u0302c \u2212 ZcPc)\u20161 \u2264 2\n\u221a K\u2016e\u2032j(Z\u0302c \u2212 ZcPc)\u2016F,\nby Lemma A9, we have\n\u2016e\u2032j(\u03a0\u0302c \u2212\u03a0cPc)\u20161 = O( \u221a K\u2016e\u2032j(Z\u0302c \u2212 ZcPc)\u2016F) = O(vK\u03ba(\u03a0\u2032c\u03a0c) \u221a \u03bb1(\u03a0\u2032c\u03a0c)).\nFor row nodes 1 \u2264 i \u2264 nr, recall that Zr = Y\u2217 J\u2217 \u2261 N\u22121U NM\u03a0r, Z\u0302r = Y\u0302\u2217 J\u0302\u2217, \u03a0r(i, :) = Zr(i,:) \u2016Zr(i,:)\u20161 and \u03a0\u0302r(i, :) = Z\u0302r(i,:) \u2016Z\u0302r(i,:)\u20161 , where NM and M are defined in the proof of Lemma 1 such that U = \u0398r M \u2261 \u0398r\u03a0rBr and NM(i, i) = 1\u2016M(i,:)\u2016F , similar as the proof for column nodes, we have\n\u2016e\u2032i(\u03a0\u0302r \u2212\u03a0rPr)\u20161 \u2264 2\u2016e\u2032i(Z\u0302r \u2212 ZrPr)\u20161\n\u2016e\u2032iZr\u20161 \u2264\n2 \u221a\nK\u2016e\u2032i(Z\u0302r \u2212 ZrPr)\u2016F \u2016e\u2032iZr\u20161 .\nNow, we provide a lower bound of \u2016e\u2032iZr\u20161 as below\n\u2016e\u2032iZr\u20161 = \u2016e\u2032i N\u22121U NM\u03a0r\u20161 = \u2016N \u22121 U (i, i)e \u2032 i NM\u03a0r\u20161 = N\u22121U (i, i)\u2016NM(i, i)e \u2032 i\u03a0r\u20161 = NM(i, i) NU(i, i)\n= \u2016U(i, :)\u2016F NM(i, i) = \u2016U(i, :)\u2016F 1\n\u2016M(i, :)\u2016F = \u2016U(i, :)\u2016F 1 \u2016e\u2032i M\u2016F\n= \u2016U(i, :)\u2016F 1\n\u2016e\u2032i\u0398 \u22121 r U\u2016F\n= \u2016U(i, :)\u2016F 1\n\u2016\u0398\u22121r (i, i)e\u2032iU\u2016F = \u03b8r(i) \u2265 \u03b8r,min.\nTherefore, by Lemma A9, we have\n\u2016e\u2032i(\u03a0\u0302r \u2212\u03a0rPr)\u20161 \u2264 2 \u221a\nK\u2016e\u2032i(Z\u0302r \u2212 ZrPr)\u2016F \u2016e\u2032iZr\u20161\n\u2264 2 \u221a\nK\u2016e\u2032i(Z\u0302r \u2212 ZrPr)\u2016F \u03b8r,min\n= O( K5.5\u03b815r,maxv\u03ba4.5(\u03a0\u2032r\u03a0r)\u03ba(\u03a0c)\u03bb1.51 (\u03a0 \u2032 r\u03a0r)\n\u03b815r,min\u03c0r,min ).\nAppendix E.2. Proof of Corollary 1\nProof. Under conditions of Corollary 1, we have\n\u2016e\u2032i(\u03a0\u0302r \u2212\u03a0rPr)\u20161 = O( K5.5\u03b815r,maxv\u03ba4.5(\u03a0\u2032r\u03a0r)\u03ba(\u03a0c)\u03bb1.51 (\u03a0 \u2032 r\u03a0r)\n\u03b815r,min\u03c0r,min ) = O(\n\u03b815r,maxv \u221a\nnr \u03b815r,min ),\n\u2016e\u2032j(\u03a0\u0302c \u2212\u03a0cPc)\u20161 = O(vK\u03ba(\u03a0\u2032c\u03a0c) \u221a \u03bb1(\u03a0\u2032c\u03a0c)) = O(v \u221a nc).\nUnder conditions of Corollary 1, Lemma A7 gives v = O( \u221a\nPmax\u03b8r,maxlog(nr+nc) \u03b8r,min\u03c3K(P) \u221a nrnc\n), which gives that\n\u2016e\u2032i(\u03a0\u0302r \u2212\u03a0rPr)\u20161 = O( \u03b815r,maxv\n\u221a nr\n\u03b815r,min ) = O(\n\u03b815.5r,max \u221a\nPmaxlog(nr + nc) \u03b816r,min\u03c3K(P) \u221a nc ),\n\u2016e\u2032j(\u03a0\u0302c \u2212\u03a0cPc)\u20161 = O(v \u221a nc) = O( \u221a\nPmax\u03b8r,maxlog(nr + nc) \u03b8r,min\u03c3K(P) \u221a nr ).\nBy basic algebra, this corollary follows."
        },
        {
            "heading": "Appendix F. SVM-Cone Algorithm",
            "text": "For readers\u2019 convenience, we briefly introduce the SVM-cone algorithm given in [26] and provide another view that the SVM-cone algorithm exactly recovers \u03a0r when the input is U\u2217 (or U\u2217,2). Let S be a matrix whose rows have unit l2 norm, and S can be written as S = HSC, where H \u2208 Rn\u00d7K with nonnegative entries, no row of H is 0, and SC \u2208 RK\u00d7m corresponding to K rows of S (i.e., there exists an index set I with K entries such that SC = S(I , :)). Inferring H from S is called the ideal cone problem, i.e., Problem 1 in [26]. The ideal cone problem can be solved by applying one-class SVM to the rows of S, and the K rows of SC are the support vectors found by one-class SVM:\nmaximize b s.t. w\u2032S(i, :) \u2265 b( for i = 1, 2, . . . , n) and \u2016w\u2016F \u2264 1. (A5)\nThe solution (w, b) for the ideal cone problem when (SCS\u2032C) \u221211 > 0 is given by\nw = b\u22121 \u00b7 S\u2032C (SCS\u2032C) \u221211 1\u2032(SCS\u2032C) \u221211 , b = 1\u221a 1\u2032(SCS\u2032C) \u221211 . (A6)\nFor the empirical case, let S\u0302 \u2208 Rn\u00d7m be a matrix where all rows have unit l2 norm, infer H from S\u0302 with given K is called the empirical cone problem, i.e., Problem 2 in [26]. For the empirical cone problem, one-class SVM is applied to all rows of S\u0302 to obtain w and b\u2019s estimations w\u0302 and b\u0302. Then, apply the K-means algorithm to rows of S\u0302 that are close to the\nhyperplane into K clusters, and an estimation of the index set I can be obtained from the K clusters provided. Algorithm A3 below is the SVM-cone algorithm provided in [26].\nAlgorithm A3 SVM-cone [26]\nRequire: S\u0302 \u2208 Rn\u00d7m with rows having unit l2 norm, number of corners K, estimated distance corners from hyperplane \u03b3. Ensure: The near-corner index set I\u0302 . 1: Run one-class SVM on S\u0302(i, :) to obtain w\u0302 and b\u0302. 2: Run K-means algorithm to the set {S\u0302(i, :)|S\u0302(i, :)w\u0302 \u2264 b\u0302 + \u03b3} that are close to the hyper-\nplane into K clusters. 3: Pick one point from each cluster to obtain the near-corner set I\u0302 .\nAs suggested in [26], we can start \u03b3 = 0 and incrementally increase it until K distinct clusters are found.\nNow, turn to our DiMSC algorithm, and focus on estimating Ir with given U\u2217, U\u2217,2, and K. By Lemmas 1 and A1, we know that U\u2217 and U\u2217,2 enjoy the ideal cone structure, and Lemma 2 guarantees that one-class SVM can be applied to rows of U\u2217 and U\u2217,2. Set w1 = b\u221211 U \u2032 \u2217(Ir, :) (U\u2217(Ir ,:)U\u2032\u2217(Ir ,:))\u221211 1\u2032(U\u2217(Ir ,:)U\u2032\u2217(Ir ,:))\u22121 , b1 = 1\u221a1\u2032(U\u2217(Ir ,:)U\u2032\u2217(Ir ,:))\u221211 , and w2 = b\u221212 U \u2032 \u2217,2(Ir, : ) (U\u2217,2(Ir ,:)U\u2032\u2217,2(Ir ,:))\u221211 1\u2032(U\u2217,2(Ir ,:)U\u2032\u2217,2(Ir ,:))\u22121 , b2 = 1\u221a 1\u2032(U\u2217,2(Ir ,:)U\u2032\u2217,2(Ir ,:))\u221211 . Now that w1 and b1 are solutions of the one-class SVM in Equation (A5) by setting S = U\u2217, and w2 and b2 are solutions of the one-class SVM in Equation (A5) by setting S = U\u2217,2 . Lemma A11 says that if row node i is a pure node, we have U\u2217(i, :)w1 = b1, and this suggests that in the SVM-cone algorithm, if the input matrix is U\u2217, by setting \u03b3 = 0, we can find all pure row nodes, i.e., the set {U\u2217(i, :)|U\u2217(i, :)w1 = b1} contains all rows of U\u2217 respective to pure row nodes while including mixed row nodes. By Lemma 1, these pure row nodes belong to the K distinct row communities such that if row nodes i, i\u0304 are in the same row community, then we have U\u2217(i, :) = U\u2217(i\u0304, :), and this is the reason that we need to apply the K-means algorithm on the set obtained in Step 2 in the SVM-cone algorithm to obtain the K distinct row communities, and this is also the reason that we said the SVM-cone algorithm returns the index set I exactly when the input is U\u2217. These conclusions also hold when we set the input in the SVM-cone algorithm as U\u2217,2.\nLemma A10. Under DiDCMMnr ,nc(K, P, \u03a0r, \u03a0c, \u0398r), for 1 \u2264 i \u2264 nr, U\u2217(i, :) can be written as U\u2217(i, :) = r1(i)\u03a61(i, :)U\u2217(Ir, :), where r1(i) \u2265 1. Meanwhile, r1(i) = 1 and \u03a61(i, :) = e\u2032k if i is a pure node such that \u03a0r(i, k) = 1; r1(i) > 1 and \u03a61(i, :) 6= e\u2032k if \u03a0r(i, k) < 1 for 1 \u2264 k \u2264 K. Similarly, U\u2217,2(i, :) can be written as U\u2217,2(i, :) = r2(i)\u03a62(i, :)U\u2217,2(Ir, :), where r2(i) \u2265 1. Meanwhile, r2(i) = 1 and \u03a62(i, :) = e\u2032k if \u03a0r(i, k) = 1; r2(i) > 1 and \u03a62(i, :) 6= e \u2032 k if \u03a0r(i, k) < 1 for 1 \u2264 k \u2264 K.\nProof. Since U\u2217 = YU\u2217(Ir, :) by Lemma 1, for 1 \u2264 i \u2264 nr, we have\nU\u2217(i, :) = Y(i, :)U\u2217(Ir, :) = Y(i, :)1 Y(i, :)\nY(i, :)1 U\u2217(Ir, :) = r1(i)\u03a61(i, :)U\u2217(Ir, :),\nwhere we set r1(i) = Y(i, :)1, \u03a61(i, :) = Y(i,:)\nY(i,:)1 , and 1 is a K\u00d7 1 vector with all entries being ones.\nBy the proof of Lemma 1, Y(i, :) = \u03a0r(i,:)\u2016M(i,:)\u2016F \u0398 \u22121 r (Ir, Ir)N\u22121U (Ir, Ir), where M = \u03a0r\u0398\u22121r (Ir, Ir)U(Ir, :). For convenience, set T = \u0398\u22121r (Ir, Ir), Q = N\u22121U (Ir, Ir), and R = U(Ir, :) (such setting of T, Q, R is only for used for notation convenience for the proof of Lemma A10). On the one hand, if row node i is pure such that \u03a0r(i, k) = 1 for certain k among {1, 2, . . . , K} (i.e., \u03a0r(i, :) = ek if \u03a0r(i, k) = 1), we have M(i, :) = \u03a0r(i, :)\u0398\u22121r (Ir, Ir)U(Ir, :\n) = T(k, k)R(k, :), and \u03a0r(i, :)TQ = T(k, k)Q(k, :), which give that Y(i, :) = T(k,k)Q(k,:) \u2016T(k,k)R(k,:)\u2016F\n= Q(k,:) \u2016R(k,:)\u2016F\n. Recall that the k-th diagonal entry of N\u22121U (Ir, Ir) is \u2016[U(Ir, :)](k, :)\u2016F, i.e., Q(k, : )1 = \u2016R(k, :)\u2016F, which gives that r1(i) = Y(i, :)1 = 1 and \u03a61(i, :) = e\u2032k when \u03a0r(i, k) = 1.\nOn the other hand, if i is a mixed node, since \u2016M(i, :)\u2016F = \u2016\u03a0r(i, :)\u0398\u22121r (Ir, Ir)U(Ir, : )\u2016F = \u2016\u2211Kk=1 \u03a0r(i, k)T(k, k)R(k, :)\u2016F < \u2211 K k=1 \u03a0r(i, k)T(k, k)\u2016R(k, :)\u2016F = \u2211Kk=1 \u03a0r(i, k)T(k, k)Q(k, k), combine it with \u03a0r(i, :)TQ1 = \u2211 K k=1 \u03a0r(i, k)T(k, k)Q(k, k), so r1(i) = Y(i, :)1 = \u03a0r(i,:)TQ1 \u2016M(i,:)\u2016F > 1. The lemma follows by a similar analysis for U\u2217,2.\nLemma A11. Under DiDCMMnr ,nc(K, P, \u03a0r, \u03a0c, \u0398r), for 1 \u2264 i \u2264 nr, if row node i is a pure node such that \u03a0r(i, k) = 1 for certain k, we have\nU\u2217(i, :)w1 = b1 and U\u2217,2(i, :)w2 = b2,\nMeanwhile, if row node i is a mixed node, the above equalities do not hold.\nProof. For the claim that U\u2217(i, :)w1 = b1 holds when i is pure, by Lemma A10, when i is a pure node such that \u03a0r(i, k) = 1, U\u2217(i, :) can be written as U\u2217(i, :) = e\u2032kU\u2217(Ir, :), so U\u2217(i, :)w1 = b1 holds surely. When i is a mixed node, by Lemma A10, r1(i) > 1 and \u03a61(i, :) 6= ek for any k = 1, 2, . . . , K; hence U\u2217(i, :) 6= e\u2032kU\u2217(Ir, :) if i is mixed, which gives the result. Follow a similar analysis, we obtain the results associated with U\u2217,2, and the lemma follows.\nReferences 1. Fortunato, S. Community detection in graphs. Phys. Rep. 2010, 486, 75\u2013174. [CrossRef] 2. Fortunato, S.; Hric, D. Community detection in networks: A user guide. Phys. Rep. 2016, 659, 1\u201344. [CrossRef] 3. Holland, P.W.; Laskey, K.B.; Leinhardt, S. Stochastic blockmodels: First steps. Soc. Netw. 1983, 5, 109\u2013137. [CrossRef] 4. Karrer, B.; Newman, M.E.J. Stochastic blockmodels and community structure in networks. Phys. Rev. E 2011, 83, 16107. [CrossRef] 5. Rohe, K.; Chatterjee, S.; Yu, B. Spectral clustering and the high-dimensional stochastic blockmodel. Ann. Stat. 2011, 39, 1878\u20131915. [CrossRef] 6. Zhao, Y.; Levina, E.; Zhu, J. Consistency of community detection in networks under degree-corrected stochastic block models. Ann. Stat. 2012, 40, 2266\u20132292. [CrossRef] 7. Qin, T.; Rohe, K. Regularized spectral clustering under the degree-corrected stochastic blockmodel. Adv. Neural Inf. Process. Syst. 2013, 26, 3120\u20133128. 8. Jin, J. Fast community detection by SCORE. Ann. Stat. 2015, 43, 57\u201389. [CrossRef] 9. Lei, J.; Rinaldo, A. Consistency of spectral clustering in stochastic block models. Ann. Stat. 2015, 43, 215\u2013237. [CrossRef] 10. Cai, T.T.; Li, X. Robust and computationally feasible community detection in the presence of arbitrary outlier nodes. Ann. Stat. 2015, 43, 1027\u20131059. [CrossRef] 11. Joseph, A.; Yu, B. Impact of regularization on spectral clustering. Ann. Stat. 2016, 44, 1765\u20131791. [CrossRef] 12. Chen, Y.; Li, X.; Xu, J. Convexified modularity maximization for degree-corrected stochastic block models. Ann. Stat. 2018, 46, 1573\u20131602. [CrossRef] 13. Passino, F.S.; Heard, N.A. Bayesian estimation of the latent dimension and communities in stochastic blockmodels. Stat. Comput. 2020, 30, 1291\u20131307. [CrossRef] 14. Li, X.; Chen, Y.; Xu, J. Convex Relaxation Methods for Community Detection. Stat. Sci. 2021, 36, 2\u201315. [CrossRef] 15. Jing, B.; Li, T.; Ying, N.; Yu, X. Community detection in sparse networks using the symmetrized Laplacian inverse matrix (SLIM). Stat. Sin. 2022, 32, 1\u201322. [CrossRef] 16. Abbe, E. Community detection and stochastic block models: Recent developments. J. Mach. Learn. Res. 2017, 18, 6446\u20136531. 17. Airoldi, E.M.; Blei, D.M.; Fienberg, S.E.; Xing, E.P. Mixed Membership Stochastic Blockmodels. J. Mach. Learn. Res. 2008, 9, 1981\u20132014. 18. Ball, B.; Karrer, B.; Newman, M.E.J. Efficient and principled method for detecting communities in networks. Phys. Rev. E 2011, 84, 36103. [CrossRef] 19. Wang, F.; Li, T.; Wang, X.; Zhu, S.; Ding, C. Community discovery using nonnegative matrix factorization. Data Min. Knowl. Discov. 2011, 22, 493\u2013521. [CrossRef] 20. Gopalan, P.K.; Blei, D.M. Efficient discovery of overlapping communities in massive networks. Proc. Natl. Acad. Sci. USA 2013, 110, 14534\u201314539. [CrossRef] 21. Anandkumar, A.; Ge, R.; Hsu, D.; Kakade, S.M. A tensor approach to learning mixed membership community models. J. Mach.\nLearn. Res. 2014, 15, 2239\u20132312.\n22. Kaufmann, E.; Bonald, T.; Lelarge, M. A spectral algorithm with additive clustering for the recovery of overlapping communities in networks. Theor. Comput. Sci. 2017, 742, 3\u201326. [CrossRef] 23. Panov, M.; Slavnov, K.; Ushakov, R. Consistent estimation of mixed memberships with successive projections. In Proceedings of the International Conference on Complex Networks and their Applications, Lyon, France, 29 November\u20131 December 2017; Springer: Cham, Switzerland, 2017; pp. 53\u201364. 24. Jin, J.; Ke, Z.T.; Luo, S. Mixed membership estimation for social networks. arXiv 2017, arXiv:1708.07852. 25. Mao, X.; Sarkar, P.; Chakrabarti, D. On Mixed Memberships and Symmetric Nonnegative Matrix Factorizations. In Proceedings of the International Conference on Machine Learning, Sydney, Australia, 6\u201311 August 2017; pp. 2324\u20132333. 26. Mao, X.; Sarkar, P.; Chakrabarti, D. Overlapping Clustering Models, and One (class) SVM to Bind Them All. In Proceedings of the Advances in Neural Information Processing Systems, Montreal, QC, Canada, 3\u20138 December 2018; Volume 31, pp. 2126\u20132136. 27. Mao, X.; Sarkar, P.; Chakrabarti, D. Estimating Mixed Memberships With Sharp Eigenvector Deviations. J. Am. Stat. Assoc. 2020, 116, 1928\u20131940. [CrossRef] 28. Wang, Y.; Bu, Z.; Yang, H.; Li, H.J.; Cao, J. An effective and scalable overlapping community detection approach: Integrating social identity model and game theory. Appl. Math. Comput. 2021, 390, 125601. [CrossRef] 29. Zhang, Y.; Levina, E.; Zhu, J. Detecting Overlapping Communities in Networks Using Spectral Methods. SIAM J. Math. Data Sci. 2020, 2, 265\u2013283. [CrossRef] 30. Rohe, K.; Qin, T.; Yu, B. Co-clustering directed graphs to discover asymmetries and directional communities. Proc. Natl. Acad. Sci. USA 2016, 113, 12679\u201312684. [CrossRef] 31. Wang, Z.; Liang, Y.; Ji, P. Spectral Algorithms for Community Detection in Directed Networks. J. Mach. Learn. Res. 2020, 21, 1\u201345. 32. Ji, P.; Jin, J. Coauthorship and citation networks for statisticians. Ann. Appl. Stat. 2016, 10, 1779\u20131812. [CrossRef] 33. Zhou, Z.; Amini, A.A. Analysis of spectral clustering algorithms for community detection: The general bipartite setting. J. Mach. Learn. Res. 2019, 20, 1\u201347. 34. Laenen, S.; Sun, H. Higher-order spectral clustering of directed graphs. Adv. Neural Inf. Process. Syst. 2020, 33, 941\u2013951. 35. Qing, H.; Wang, J. Directed mixed membership stochastic blockmodel. arXiv 2021, arXiv:2101.02307. 36. Wang, Y.J.; Wong, G.Y. Stochastic Blockmodels for Directed Graphs. J. Am. Stat. Assoc. 1987, 82, 8\u201319. [CrossRef] 37. Fagiolo, G. Clustering in complex directed networks. Phys. Rev. E 2007, 76, 026107. [CrossRef] [PubMed] 38. Leicht, E.A.; Newman, M.E. Community structure in directed networks. Phys. Rev. Lett. 2008, 100, 118703. [CrossRef] [PubMed] 39. Kim, Y.; Son, S.W.; Jeong, H. Finding communities in directed networks. Phys. Rev. E 2010, 81, 016103. [CrossRef] 40. Malliaros, F.D.; Vazirgiannis, M. Clustering and Community Detection in Directed Networks: A Survey. Phys. Rep. 2013, 533, 95\u2013142. [CrossRef] 41. Zhang, X.; Lian, B.; Lewis, F.L.; Wan, Y.; Cheng, D. Directed Graph Clustering Algorithms, Topology, and Weak Links. IEEE Trans. Syst. Man, Cybern. Syst. 2021, 52, 3995\u20134009. [CrossRef] 42. Zhang, J.; Wang, J. Identifiability and parameter estimation of the overlapped stochastic co-block model. Stat. Comput. 2022, 32, 1\u201314. [CrossRef] 43. Florescu, L.; Perkins, W. Spectral thresholds in the bipartite stochastic block model. In Proceedings of the Conference on Learning Theory. PMLR, New York, NY, USA, 23\u201326 June 2016; pp. 943\u2013959. 44. Neumann, S. Bipartite stochastic block models with tiny clusters. In 32nd Conference on Neural Information Processing Systems (NIPS 2018), Montr\u00e9al, QC, Canada, 3\u20138 December 2018. 45. Ndaoud, M.; Sigalla, S.; Tsybakov, A.B. Improved clustering algorithms for the bipartite stochastic block model. IEEE Trans. Inf. Theory 2021, 68, 1960\u20131975. [CrossRef] 46. Mantzaris, A.V. Uncovering nodes that spread information between communities in social networks. EPJ Data Sci. 2014, 3, 1\u201317. [CrossRef] 47. McSherry, F. Spectral partitioning of random graphs. In Proceedings of the 42nd IEEE Symposium on Foundations of Computer Science, Newport Beach, CA, USA, 8\u201311 October 2001; pp. 529\u2013537. 48. Massouli\u00e9, L. Community detection thresholds and the weak Ramanujan property. In Proceedings of the Forty-Sixth Annual ACM Symposium on Theory of Computing, New York, NY, USA, 31 May\u20133 June 2014; pp. 694\u2013703. 49. Mossel, E.; Neeman, J.; Sly, A. Reconstruction and estimation in the planted partition model. Probab. Theory Relat. Fields 2015, 162, 431\u2013461. [CrossRef] 50. Abbe, E.; Bandeira, A.S.; Hall, G. Exact recovery in the stochastic block model. IEEE Trans. Inf. Theory 2015, 62, 471\u2013487. [CrossRef] 51. Hajek, B.; Wu, Y.; Xu, J. Achieving exact cluster recovery threshold via semidefinite programming. IEEE Trans. Inf. Theory 2016, 62, 2788\u20132797. [CrossRef] 52. Mossel, E.; Neeman, J.; Sly, A. A proof of the block model threshold conjecture. Combinatorica 2018, 38, 665\u2013708. [CrossRef] 53. Qing, H. Studying Asymmetric Structure in Directed Networks by Overlapping and Non-Overlapping Models. Entropy 2022, 24, 1216. [CrossRef] 54. Gillis, N.; Vavasis, S.A. Semidefinite Programming Based Preconditioning for More Robust Near-Separable Nonnegative Matrix Factorization. SIAM J. Optim. 2015, 25, 677\u2013698. [CrossRef] 55. Qing, H. A useful criterion on studying consistent estimation in community detection. Entropy 2022, 24, 1098. [CrossRef]\n[PubMed]\n56. Von Luxburg, U. A tutorial on spectral clustering. Stat. Comput. 2007, 17, 395\u2013416. [CrossRef] 57. Ke, Z.T.; Jin, J. The SCORE normalization, especially for highly heterogeneous network and text data. arXiv 2022, arXiv:2204.11097. 58. Newman, M.E. Modularity and community structure in networks. Proc. Natl. Acad. Sci. USA 2006, 103, 8577\u20138582. [CrossRef] [PubMed] 59. Chang, C.C.; Lin, C.J. LIBSVM: A library for support vector machines. ACM Trans. Intell. Syst. Technol. (Tist) 2011, 2, 1\u201327. [CrossRef] 60. Xu, R.; Wunsch, D. Survey of clustering algorithms. IEEE Trans. Neural Netw. 2005, 16, 645\u2013678. [CrossRef] [PubMed] 61. Palmer, W.R.; Zheng, T. Spectral clustering for directed networks. In Proceedings of the International Conference on Complex Networks and Their Applications, Madrid, Spain, 1\u20133 December 2020; Springer: Cham, Switzerland, 2020; pp. 87\u201399. 62. Qing, H. Degree-corrected distribution-free model for community detection in weighted networks. Sci. Rep. 2022, 12, 15153. [CrossRef] [PubMed] 63. Erd\u00f6s, P.; R\u00e9nyi, A. On the evolution of random graphs. In The Structure and Dynamics of Networks; Princeton University Press: Princeton, NJ, USA, 2011; pp. 38\u201382. [CrossRef] 64. Chen, Y.; Chi, Y.; Fan, J.; Ma, C. Spectral Methods for Data Science: A Statistical Perspective. Found. Trends Mach. Learn. 2021, 14, 566\u2013806. [CrossRef] 65. Shannon, C.E. A mathematical theory of communication. Bell Syst. Tech. J. 1948, 27, 379\u2013423. [CrossRef] 66. \u017dalik, K.R.; \u017dalik, B. Memetic algorithm using node entropy and partition entropy for community detection in networks. Inf. Sci. 2018, 445, 38\u201349. [CrossRef] 67. Feutrill, A.; Roughan, M. A review of Shannon and differential entropy rate estimation. Entropy 2021, 23, 1046. [CrossRef] 68. Adamic, L.A.; Glance, N. The political blogosphere and the 2004 US election: Divided they blog. In Proceedings of the 3rd International Workshop on Link Discovery, Chicago, IL, USA, 21\u201325 August 2005; pp. 36\u201343. 69. Kunegis, J. Konect: The koblenz network collection. In Proceedings of the 22nd International Conference on World Wide Web, Rio de Janeiro, Brazil, 13\u201317 May 2013; pp. 1343\u20131350. 70. Zhang, H.; Guo, X.; Chang, X. Randomized spectral clustering in large-scale stochastic block models. J. Comput. Graph. Stat. 2022, 31, 887\u2013906. [CrossRef] 71. Tropp, J.A. User-Friendly Tail Bounds for Sums of Random Matrices. Found. Comput. Math. 2012, 12, 389\u2013434. [CrossRef]\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content."
        }
    ],
    "title": "Estimating Mixed Memberships in Directed Networks by Spectral Clustering",
    "year": 2023
}