{
    "abstractText": "Micro-video background music recommendation is a complicated task where the matching degree between videos and uploader-selected background music is a major issue. However, the selection of the user-generated content (UGC) is biased caused by knowledge limitations and historical preferences among music of each uploader. In this paper, we propose a Debiased Cross-Modal (DebCM) matching model to alleviate the influence of such selection bias. Specifically, we design a teacher-student network to utilize the matching of segments of music videos, which is professional-generated content (PGC) with specialized music-matching techniques, to better alleviate the bias caused by insufficient knowledge of users. The PGC data is captured by a teacher network to guide the matching of uploader-selected UGC data of the student network by KL-based knowledge transfer. In addition, uploaders\u2019 personal preferences of music genres are identified as confounders that spuriously correlate music embeddings and background music selections, resulting in the learned recommender system to over-recommend music from the majority groups. To resolve such confounders in the UGC data of the student network, backdoor adjustment is utilized to deconfound the spurious correlation between music embeddings and prediction scores. We further utilize Monte Carlo (MC) estimator with batch-level average as the approximations to avoid integrating the entire confounder space calculated by the adjustment. Extensive experiments on the TT-150k-genre dataset demonstrate the effectiveness of the proposed method towards the selection bias. The code is publicly available on: https://github.com/jing-1/DebCM.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jing Yi"
        }
    ],
    "id": "SP:761796017c4cf6a1c30b9e7eb0494d3341367891",
    "references": [
        {
            "authors": [
                "Chien-Liang Liu",
                "Ying-Chuan Chen"
            ],
            "title": "Background music recommendation based on latent factors and moods",
            "venue": "Knowledge-Based Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Jing Yi",
                "Yaochen Zhu",
                "Jiayi Xie",
                "Zhenzhong Chen"
            ],
            "title": "Cross-modal variational auto-encoder for content-based micro-video background music recommendation",
            "venue": "IEEE Transactions on Multimedia,",
            "year": 2021
        },
        {
            "authors": [
                "Shoto Sasaki",
                "Tatsunori Hirai",
                "Hayato Ohya",
                "Shigeo Morishima"
            ],
            "title": "Affective music recommendation system based on the mood of input video",
            "venue": "In MultiMedia Modeling International Conference,",
            "year": 2015
        },
        {
            "authors": [
                "Robert E Thayer"
            ],
            "title": "The biopsychology of mood and arousal",
            "year": 1990
        },
        {
            "authors": [
                "Jiansong Chao",
                "Haofen Wang",
                "Wenlei Zhou",
                "Weinan Zhang",
                "Yong Yu"
            ],
            "title": "Tunesensor: A semantic-driven music recommendation service for digital photo albums",
            "venue": "In Proceedings of the International Semantic Web Conference,",
            "year": 2011
        },
        {
            "authors": [
                "Bochen Li",
                "Aparna Kumar"
            ],
            "title": "Query by video: Crossmodal music retrieval",
            "venue": "In International Society for Music Information Retrieval Conference,",
            "year": 2019
        },
        {
            "authors": [
                "Didac Sur\u00eds",
                "Amanda Duarte",
                "Amaia Salvador",
                "Jordi Torres",
                "Xavier Gir\u00f3-i Nieto"
            ],
            "title": "Cross-modal embeddings for video and audio retrieval",
            "venue": "In Proceedings of the European Conference on Computer Vision Workshops,",
            "year": 2018
        },
        {
            "authors": [
                "Kaiye Wang",
                "Qiyue Yin",
                "Wei Wang",
                "Shu Wu",
                "Liang Wang"
            ],
            "title": "A comprehensive survey on cross-modal retrieval",
            "venue": "arXiv preprint arXiv:1607.06215,",
            "year": 2016
        },
        {
            "authors": [
                "Lanyu Shang",
                "Zhang Daniel Yue",
                "Khan Siamul Karim",
                "Jialie Shen",
                "Dong Wang"
            ],
            "title": "CaMR: Towards connotationaware music retrieval on social media with visual inputs",
            "venue": "In IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining,",
            "year": 2020
        },
        {
            "authors": [
                "Xixuan Wu",
                "Yu Qiao",
                "Xiaogang Wang",
                "Xiaoou Tang"
            ],
            "title": "Bridging music and image via cross-modal ranking analysis",
            "venue": "IEEE Transactions on Multimedia,",
            "year": 2016
        },
        {
            "authors": [
                "Aihua Zheng",
                "Menglan Hu",
                "Bo Jiang",
                "Yan Huang",
                "Yan Yan",
                "Bin Luo"
            ],
            "title": "Adversarial-metric learning for audio-visual cross-modal matching",
            "venue": "IEEE Transactions on Multimedia,",
            "year": 2022
        },
        {
            "authors": [
                "Sami Abu-El-Haija",
                "Nisarg Kothari",
                "Joonseok Lee",
                "Paul Natsev",
                "George Toderici",
                "Balakrishnan Varadarajan",
                "Sudheendra Vijayanarasimhan"
            ],
            "title": "Youtube-8m: A largescale video classification benchmark",
            "venue": "arXiv preprint arXiv:1609.08675,",
            "year": 2016
        },
        {
            "authors": [
                "Sungeun Hong",
                "Woobin Im",
                "Hyun S Yang"
            ],
            "title": "Cbvmr: content-based video-music retrieval using soft intra-modal structure constraint",
            "venue": "In Proceedings of the ACM on International Conference on Multimedia Retrieval,",
            "year": 2018
        },
        {
            "authors": [
                "Laure Pretet",
                "Gael Richard",
                "Clement Souchier",
                "Geoffroy Peeters"
            ],
            "title": "Video-to-music recommendation using temporal alignment of segments",
            "venue": "IEEE Transactions on Multimedia,",
            "year": 2022
        },
        {
            "authors": [
                "Hao Zhang",
                "Aixin Sun",
                "Wei Jing",
                "Joey Tianyi Zhou"
            ],
            "title": "Towards debiasing temporal sentence grounding in video",
            "venue": "arXiv preprint arXiv:2111.04321,",
            "year": 2021
        },
        {
            "authors": [
                "Zhiquan Wen",
                "Guanghui Xu",
                "Mingkui Tan",
                "Qingyao Wu",
                "Qi Wu"
            ],
            "title": "Debiased visual question answering from feature and sample perspectives",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Zhenyu Huang",
                "Guocheng Niu",
                "Xiao Liu",
                "Wenbiao Ding",
                "Xinyan Xiao",
                "Hua Wu",
                "Xi Peng"
            ],
            "title": "Learning with noisy correspondence for cross-modal matching",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jiawei Chen",
                "Hande Dong",
                "Xiang Wang",
                "Fuli Feng",
                "Meng Wang",
                "Xiangnan He"
            ],
            "title": "Bias and debias in recommender system: A survey and future directions",
            "year": 2010
        },
        {
            "authors": [
                "Tobias Schnabel",
                "Adith Swaminathan",
                "Ashudeep Singh",
                "Navin Chandak",
                "Thorsten Joachims"
            ],
            "title": "Recommendations as treatments: Debiasing learning and evaluation",
            "venue": "In Proceedings of the International Conference on International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Longqi Yang",
                "Yin Cui",
                "Yuan Xuan",
                "Chenyang Wang",
                "Serge Belongie",
                "Deborah Estrin"
            ],
            "title": "Unbiased offline recommender evaluation for missing-not-at-random implicit feedback",
            "venue": "In Proceedings of the ACM Conference on Recommender Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Jae-woong Lee",
                "Seongmin Park",
                "Jongwuk Lee"
            ],
            "title": "Dual unbiased recommender learning for implicit feedback",
            "venue": "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Zhang",
                "Fuli Feng",
                "Xiangnan He",
                "Tianxin Wei",
                "Chonggang Song",
                "Guohui Ling",
                "Yongdong Zhang"
            ],
            "title": "Causal intervention for leveraging popularity bias in recommendation",
            "venue": "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2021
        },
        {
            "authors": [
                "Wenjie Wang",
                "Fuli Feng",
                "Xiangnan He",
                "Xiang Wang",
                "Tat-Seng Chua"
            ],
            "title": "Deconfounded recommendation for alleviating bias amplification",
            "venue": "In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 2021
        },
        {
            "authors": [
                "Wenjie Wang",
                "Fuli Feng",
                "Xiangnan He",
                "Hanwang Zhang",
                "Tat-Seng Chua"
            ],
            "title": "Clicks can be cheating: counterfactual recommendation for mitigating clickbait issue",
            "venue": "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2021
        },
        {
            "authors": [
                "Dugang Liu",
                "Pengxiang Cheng",
                "Zhenhua Dong",
                "Xiuqiang He",
                "Weike Pan",
                "Zhong Ming"
            ],
            "title": "A general knowledge distillation framework for counterfactual recommendation via uniform data",
            "venue": "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2020
        },
        {
            "authors": [
                "Jiawei Chen",
                "Hande Dong",
                "Yang Qiu",
                "Xiangnan He",
                "Xin Xin",
                "Liang Chen",
                "Guli Lin",
                "Keping Yang"
            ],
            "title": "Autodebias: Learning to debias for recommendation",
            "venue": "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Beutel",
                "Jilin Chen",
                "Tulsee Doshi",
                "Hai Qian",
                "Li Wei",
                "Yi Wu",
                "Lukasz Heldt",
                "Zhe Zhao",
                "Lichan Hong",
                "Ed H Chi"
            ],
            "title": "Fairness in recommendation ranking through pairwise comparisons",
            "venue": "In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
            "year": 2019
        },
        {
            "authors": [
                "Qiong Wu",
                "Yong Liu",
                "Chunyan Miao",
                "Binqiang Zhao",
                "Yin Zhao",
                "Lu Guan"
            ],
            "title": "Pd-gan: Adversarial learning for personalized diversity-promoting recommendation",
            "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Harald Steck"
            ],
            "title": "Calibrated recommendations",
            "venue": "In Proceedings of the ACM Conference on Recommender Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Madelyn Glymour",
                "Judea Pearl",
                "Nicholas P Jewell"
            ],
            "title": "Causal inference in statistics: A primer",
            "year": 2016
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "In International Conference on Learning Representations,",
            "year": 2014
        },
        {
            "authors": [
                "Shoshana Abramovich",
                "Lars-Erik Persson"
            ],
            "title": "Some new estimates of the \u2018jensen gap",
            "venue": "Journal of Inequalities and Applications,",
            "year": 2016
        },
        {
            "authors": [
                "Nicholas Metropolis",
                "S. Ulam"
            ],
            "title": "The monte carlo method",
            "venue": "Journal of the American Statistical Association,",
            "year": 1949
        },
        {
            "authors": [
                "Prasenjeet Fulzele",
                "Rajat Singh",
                "Naman Kaushik",
                "Kavita Pandey"
            ],
            "title": "A hybrid model for music genre classification using lstm and svm",
            "venue": "In Eleventh International Conference on Contemporary Computing,",
            "year": 2018
        },
        {
            "authors": [
                "Dawen Liang",
                "Laurent Charlin",
                "David M Blei"
            ],
            "title": "Causal inference for recommendation. In Causation: Foundation to Application, Workshop at UAI",
            "year": 2016
        },
        {
            "authors": [
                "Shangzhe Di",
                "Zeren Jiang",
                "Si Liu",
                "Zhaokai Wang",
                "Leyan Zhu",
                "Zexin He",
                "Hongming Liu",
                "Shuicheng Yan"
            ],
            "title": "Video background music generation with controllable music transformer",
            "venue": "In Proceedings of the 29th ACM International Conference on Multimedia,",
            "year": 2021
        },
        {
            "authors": [
                "Liangli Zhen",
                "Peng Hu",
                "Xu Wang",
                "Dezhong Peng"
            ],
            "title": "Deep supervised cross-modal retrieval",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Jiwei Wei",
                "Xing Xu",
                "Yang Yang",
                "Yanli Ji",
                "Zheng Wang",
                "Heng Tao Shen"
            ],
            "title": "Universal weighting metric learning for cross-modal matching",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Dinghan Shen",
                "Yizhe Zhang",
                "Ricardo Henao",
                "Qinliang Su",
                "Lawrence Carin"
            ],
            "title": "Deconvolutional latent-variable model for text sequence matching",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "John Duchi",
                "Elad Hazan",
                "Yoram Singer"
            ],
            "title": "Adaptive subgradient methods for online learning and stochastic optimization",
            "venue": "Journal of Machine Learning Research,",
            "year": 2011
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Recent years have witnessed the prevalence of user-generated content platforms, where micro-videos have become a good multimedia carrier for people to record and share their daily lives. Micro-video platforms have attracted billions of users. Uploaders can record or make videos and select appropriate background music by themselves. However, the huge music pool makes manual selection time-consuming and labor-intensive, so it is necessary to recommend suitable background music for micro-videos [1, 2].\nExisting work on recommending background music to videos mainly focuses on designing models to achieve the matching of video and background music. Chen et al. [1] and Sasaki et al. [3] relied on the emotion model in [4] to divide music and videos into different emotional regions for matching. Chao et al. [5] leveraged the mood tags of music and videos to calculate the relatedness graph for recommending cross-media content. Li and Kumar [6] utilized emotion tags as joint constraints to better align the embeddings of music and video modalities. However, the emotion tags were manually annotated, which takes lots of time and energy. Establishing matched video-music pairs [7] facilities content-based background music recommendation, which could train matching models using latent factors without emotion tags. Yi et al. [2] have further established a largescale video-music matching dataset from a micro-video sharing platform, which enables large amounts of UGC data to be used to train a better model.\nHowever, the matching data are collected based on videos and user-selected background music, which results in selection bias from uploaders. First, the uploader has limited knowledge stor-\nCorresponding author: Zhenzhong Chen, E-mail:zzchen@ieee.org\nage which hinders the selection of the best-matching music. Music video clips of professional-generated content could be used to inspire the matching of micro-videos and background music as illustrated in Fig. 1. For example, a soothing video which is a compilation of relaxing clips is mostly associated with light music in music videos that are composed by professional creation teams. Therefore, the co-occurrence of contents between videos and background music could inspire the recommenders to match Jazz music not rather noisy music, which may be missed by uploaders due to their limited music pool. Second, uploaders have historical preferences among different music genres, which results in different selection probabilities for different music genres. For example, as shown in Fig. 1, users who prefer Hip-hop music are exposed to more Hip-hop music and are more likely to be attracted by Hip-hop music recommended by the recommender. Such bias further causes the learned recommender to over-recommend the majority music genre, which will further intensify the exposure bias, thus resulting in the cocoon effect. Therefore, the influence of such bias amplification should be alleviated.\nIn consideration of the above biases, we design a Debiased Cross-Modal (DebCM) matching model for micro-video background music recommendations. Through the idea of knowledge distillation, we utilize a teacher network to capture the matching patterns of corresponding segments of PGC music videos, which embeds videos and music into a shared latent space with a variational encoder-decoder framework. A stable feature-learning inference model from the teacher network is further utilized to constrain and guide the learning of video and music embeddings of UGC data by the student network through KL divergence. In this way, the matching co-occurrence of PGC data could help the stable alignment of cross-modal embeddings so as to better alleviate the deviation caused by insufficient knowledge of up-\nar X\niv :2\n20 8.\n03 63\n3v 1\n[ cs\n.M M\n] 7\nA ug\n2 02\n2\nloaders. Moreover, uploaders\u2019 historical preference for different music genres in UGC data is modeled as a confounder, which not only affects the music embeddings but also influences background music selections. To resolve the confounder, backdoor adjustment is utilized, where a Monte Carlo (MC) estimator with a batch-level average method is used to calculate the approximation of backdoor adjustment. In addition, to evaluate the debiased effect, we extend the TT-150k dataset [2] with attached music genres, where skewed test sets of intervention are sampled for testing. Specifically, we sample: 1) the videos of uploaders with diverse historical interaction among music genres for the test set to verify the effect of the student network on the deconfounder of UGC data, and therefore, whether it can cope with uploaders with rich and varied preferences and recommend suitable rather than biased background music could be evaluated; 2) the videos with the highest matching degree are sampled as the test set to verify the effect of the proposed teacher-student network on debiasing, and therefore, whether the most matching background music can be recommended for the video could be evaluated.\nThe main contributions can be summarized as follows:\n\u2022 We propose to utilize the matching of PGC data to guide the learning of UGC data so as to alleviate the selection bias caused by the limited knowledge storage of uploaders. Specifically, we design a teacherstudent network with KL-based knowledge transfer to enable the PGC teacher network to constrain and guide the embedding learning of the UGC student network. Therefore, embeddings of more-matched music and videos could be aligned to better recommend more content-matching music to videos.\n\u2022 Considering that the uploaders\u2019 historical preference among music genres are confounders, which affect both the learning of music embeddings and the uploaders\u2019 choice of background music, backdoor adjustment is used to deconfound the spurious correlation. A Monte Carlo (MC) estimator with a batch-level average method is utilized to approximate the estimation of backdoor adjustment given computational efficiency and unbiasedness of MC by random sampling. Therefore, over-recommendation over the majority of historically-interacted music genres could be alleviated by the deconfounder.\n\u2022 An extended dataset TT-150k-genre has been established to evaluate the performance with intervened test sets without the requirement of unbiased matching which are inaccessible. Experiments demonstrate that good performance is achieved for debiasing with the intervened test set.\nThe remainder of this article is organized as follows. Section 2 gives a related literature review of the work and Section 3 defines the task from a causal view. In Section 4, we illustrate the proposed DebCM with details. Section 5 presents the experimental settings and analyzes the experimental results. Finally, Section 6 briefly summarizes the article."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 Audiovisual Cross-modal Matching",
            "text": "Cross-modal matching aims to match queries and targets that come from two different modalities [8], which requires measuring the similarity of different modalities. Therefore, the alignment of two modalities is the essence of the problem.\nExisting work on audiovisual cross-modal matching could be classified into emotion tag-based and content-based methods. Emotion tag-based work uses the relatedness between visual tags and music mood tags [5, 6, 9]. Moreover, emotion model [4] has also been used to match video-music pairs [1, 3]. These emotion tags for the video, however, were manually annotated from crowdsourcing, which is high in labor and time costs. Contentbased methods utilize video and music contents of matched pairs to explore the matching patterns of cross-modal contents. Canonical correlation analysis (CCA) based methods such as [10] were introduced to model the relationship between videos and music. Wu et al. [10] adopted lyrics as a middle media to connect music and image, and designed a set of lyric-based attributes for image representation. Zhang et al. [11] proposed a novel Adversarial-Metric Learning (AML) model for speakers\u2019 audio-visual matching. Suris et al. [7] further employed the visual features and audio features provided by Youtube-8M [12] to constrain the visual and audio embeddings of the same video as close as possible and predicted the corresponding label of the video. CBVMR [13] introduced a content-based retrieval model without any metadata like emotions, where inter-modal ranking-based matching loss and soft intra-modal structure loss have been proposed. Pretet et al. [14] focused on shorter segments of videos and music for training and inference, where structure-aware music segmentation and alignment of sequences have been proposed for recommendations for more professional videos. Yi et al. [2] proposed a cross-generation strategy for better aligning the latent embedding of music and micro-videos. A large-scale dataset, TT-150k, which is composed of about 3,000 music clips and 150k micro-videos, was established for evaluation. However, since the matching status between video-music pairs is extracted from uploader-selected background music for micro-videos, selection bias should be considered to better find\nthe causality of video-music matching and recommend unbiased music, which is the main focus of this article.\nSome pioneering methods have been proposed to consider the bias problem in visual-language cross-modal matching. Zhang et al. [15] proposed data debiasing and model debiasing strategies for the temporal sentence grounding in video (TSGV) task, where moment temporal distribution was considered to be the main cause of bias. Wen et al. [16] pointed out that some visual question answering (VQA) models tended to output the answer that occurs frequently in the dataset and ignored the query images, where two unimodal bias detection modules were applied to explicitly recognize and remove the negative biases but remain the positive bias. Huang et al. [17] viewed the bias problem from a perspective of noisy correspondence, which refers to mismatch paired samples. The authors proposed to divide the data into clean and noisy partitions based on the memorization effect of neural networks and then rectify the correspondence via an adaptive prediction model in a co-teaching manner."
        },
        {
            "heading": "2.2 Debiased Recommendation",
            "text": "Because the data of user behavior are observational but not experimental, there exist biases due to some factors, such as users\u2019 selection of items and the system\u2019s exposure to items. To some extent, the bias would be amplified since the recommender system would recommend more monotonous items according to the propensity in observational data [18], which further damages users\u2019 experience. Therefore, alleviating the bias in recommender systems has become a new direction in the field of recommendations. Selection bias mainly comes from Missing Not At Random (MNAR) data since users tend to rate the items they are interested in and they rarely rate the items they are not interested in. The observed scores are not representative samples of all the scores, thus resulting in selection bias. Many studies have been dedicated to solving the problem mainly based on methods as follows.\nPropensity score-based methods such as [19] are derived from statistics, which add the Inverse Propensity score (IPS) to the data for debiasing. Schnabel et al. [19] addressed selection bias using IPS, where the propensity score was assumed to be the probability of each data being observed. Lee et al. [20] considered that although click behaviors observed in the recommendation system could reflect users\u2019 preferences to some extent, the missing click data did not necessarily mean negative feedback from users (positive-unlabeled problem). The authors proposed an unbiased evaluator using IPS, where debiased methods could be properly evaluated. Lee et al. [21] proposed a dual recommender learning framework that simultaneously eliminated the bias of clicked and unclicked data. Specifically, the proposed loss function adopted two propensity weighting to effectively estimate the true positive and negative preferences from clicked and unclicked data.\nCausal graph-based methods such as [22] assume the inherent causal effect of the data and use the causal graph to depict the causal effect, which can work well if reasonable assumptions about the data-generation mechanism are made. Zhang et al. [22] proposed a popularity-bias deconfounding and adjusting model, where item popularity could directly influence interaction probability because most people have a herd mentality. Do calculus was then used to remove the influence of the parent\nnode of the confounder. Wang et al. [23] scrutinized the causeeffect factors for bias amplification, identifying the main reason lay in the confounder effect of imbalanced item distribution on user representation and prediction score. Wang et al. [24] formulated the recommendation models as a causal graph that reflected the cause-effect factors in recommendation, and addressed the clickbait issue by performing counterfactual inference on the causal graph. By estimating the click likelihood of a user in the counterfactual world, the direct effect of exposure features and the clickbait issue could be reduced.\nUniform data-based methods such as [25] utilize more reasonable experimental data (e.g., randomly investigate users\u2019 ratings on items), which could directly weaken bias. Liu et al. [25] introduced the uniform data to alleviate the bias through several knowledge distillation methods, where straight-forward samplebased, label-based, feature-based, and network structure-based methods were involved. Chen et al. [26] leveraged another (small) set of uniform data to optimize the debiasing parameters by solving the bi-level optimization problem with meta-learning.\nSome other work focuses on fairness [27], diversity [28] and calibration [29] for recommender systems, which induces different loss functions to increase the fairness and diversity of recommendation.\n3 Causal View of BackgroundMusic Recommendation\nIn this section, we formally define the task of background music recommendation from a causal perspective. For UGC data, the uploader\u2019s historical preference is identified as a confounder that spuriously correlates music embeddings and music selections. While for PGC data, professional teams make the uniform matching between videos and background music. To effectively utilize the PGC data, a Teacher-Student knowledge distillation framework could be utilized for a debiased recommendation. Moreover, a causal graph is established for UGC data, where we introduce backdoor adjustment to block uploaders\u2019 influence such that the causal effects of music contents on music selections of videos can be properly estimated."
        },
        {
            "heading": "3.1 Problem Definition",
            "text": "We assume there are uploader-video-music triplets in the form of {(u, v,m)}, where v \u2208 V, m \u2208 M and u \u2208 U. Each music m \u2208 M is associated with a music feature m from its audio clip and a music genre g. Each video v \u2208 V is associated with a visual feature v extracted from the image sequence. The mapping f : V\u00d7M\u2192 {0, 1} includes a set of triplets {(v,m, y)}, where y = f(v,m) is the indicator that depicts whether music m is matched to a video v chosen by the uploader. Besides the matching information of user-generated video-music, we introduce external information of PGC data, which include a set of matched triads {(vt,mt, yt)} from professional music video clips. Given a new video v, our goal is to retrieve a list of music candidates C(m) \u2282 M where each music m \u2208 C(m) is a potentially suitable match for the video based solely on their content features, irrespective of uploader\u2019s personal historical preference, such that selection bias that distracts users from locating suitable background music can be eliminated."
        },
        {
            "heading": "3.2 Causal Graph Modeling",
            "text": "To gain more intuition about the problem, we simplify a causal graph as illustrated in the User Deconfounder module of Fig. 2 by carefully considering the causal effects of user historical preference, music, video, and matching status. Specifically, we focus on user historical preference among music genres as the confounder, which affects both the music embeddings and the matching status. In particular: u represents the historical preference among music genres of the uploader u, zm and zv denotes the music embedding and video embedding learned from models inferred from music contents m and video contents v, y denotes the matching status of music and videos allocated by the uploader.\nThe edges in the graph describe the causal relations between variables. Specifically, u\u2192 zm denotes the user historical preference among music genres affects the exposure and modeling of user embedding, u\u2192 y represents the user historical preference affects the final selection of the matching status y, zm \u2192 y denotes the music embeddings learned from the model affects the matching status by calculating the similarity of embeddings of music and video. Owing to the co-effects of user preference among music genres between music embedding and matching status, recommender models are affected by the confounder and thus suffer from a spurious correlation between the music embedding and the prediction score.\nTo eliminate the spurious correlation caused by the confounder, experimentally, intervention on the confounder which forces uploaders\u2019 interaction distribution among music genres to be controlled could achieve the purpose. However, it is almost not feasible to collect a certain number of such experimental data, since it is necessary to control the exposure content of the recommendation system and the user\u2019s independent choice, which will greatly damage the user\u2019s experience. To enable the estimation of the causal effect from observational biased UGC data, DebCM resorts to the causal solution: backdoor adjustment [30]. According to the causal theory, since u affects both zm and y, u is a confounder between zm and y, resulting in the spurious correlation when estimating the correlation between zm and y. To dissolve the spurious correlation, we struggle to eliminate the edge between user historical preference u and music embedding zm through backdoor adjustment. The details will be illustrated in Section 4.3 of deconfounded cross-modal model for the student network."
        },
        {
            "heading": "4 METHODOLOGY",
            "text": "The overall framework of DebCM is shown in Fig. 2. Specifically, DebCM aims to acquire professional knowledge from PGC data to alleviate the selection bias caused by limited knowledge of uploaders using a teacher-student knowledge transfer. To further debias the selection bias caused by uploaders\u2019 historical preference in the UGC data, we build up a causal graph and block the spurious relation caused by the confounder through backdoor adjustment in the student network. Notations used in this article are summarized in Table 1.\nIn our framework, the teacher network captures the wellmatched PGC data, whereas the UGC data is modeled with the student network. We utilize a general distillation function based on the potentially useful knowledge from the PGC data\nto improve the learning of the biased UGC data. Specifically, we follow CMVAE [2] where a cross-modal generative process is used to model the matching of the music and videos in the teacher network. The well-modeled teacher network is then utilized as a stable inference extractor to guide the embedding of music and videos in the student network. Furthermore, the student network considers user preference among music genres as the confounder and conducts the backdoor adjustment to eliminate the impact of the confounder. The cross-modal generative process in the student network is then integrated with the user deconfounder module. The details of the proposed model are expounded in the following sections."
        },
        {
            "heading": "4.1 Overview of CMVAE",
            "text": "CMVAE [2] introduces a variational cross-generation strategy for background music recommendation for micro-videos. In this paper, we adapt the basic framework of cross-generation in CMVAE to our background music recommendation problem for better aligning the latent embeddings of music and videos and more robust learning with a deep variational manner.\nAccording to CMVAE, a shared d-dimensional Gaussian latent space Rd could be learned given the matched video-music pairs {(v,m)}, where the matching degree can be regularized to the distance between the embeddings. Cross-generation is proposed to achieve the alignment of video and music pairs where the video feature v is generated by the matched music latent embedding zm, and vice versa to the music feature m and zv. Based on the cross-generation strategy, the generation process can be formulated as:\np(v | m) = Ezm\u223cp(zm |m) [ p (v | zm) ] (1)\np(m | v) = Ezv\u223cp(zv |v) [ p (m | zv) ] . (2)\ntv tvz vLatent variable\nVariational inference is utilized to find variational posteriors of latent variables which are intractable, where the minimization of the KL-divergence is equivalent to the maximization of the Evidence Lower BOund (ELBO) [31]. The reconstruction part Lcross_recon of the ELBO is as follows:\nLcross_recon = Ezm\u223cq(zm |m)[p(v | zm)] + Ezv\u223cq(zv |v)[p(m | zv)],\n(3)\nwhich aims to reconstruct the input features with cross-modal latent variables. It is implemented by mean square error (mse) losses between the observations and reconstructed inputs.\nThe KL-divergence part LKL of the ELBO serves as a regularizer which constraints the variational posterior to be close to a standard Normal distribution N(0, Id). Therefore, only recommendation-relevant information could be encoded into the latent variable in consideration of robustness to noise. It can be formulated as:\nLKL = Ezv\u223cq(zv |v) [ KL \u2016q (zv | v) , p (zv)\u2016 ] + Ezm\u223cq(zm |m) [ KL \u2016q (zm | m) , p (zm)\u2016 ] ,\n(4)\nwhere p (zm) and p (zv) are the priors for the music and video latent variables, respectively. For Gaussian variables, the KLdivergence has a analytical solution.\nBesides the cross-generation losses, the matching loss is further defined as the expection of the generative distribution p(y | zm, zv), which could be calculated as:\nLmatching = p(y | m, v) = Ezm\u223cq(zm |m),zv\u223cq(zv |v) [ p (y | zm, zv) ] ,\n(5)\nwhere the matching probability p(y | m, v) depicts the matching degree of the given video-music pair. Dot product of the latent embeddings of video and music is used to calculate the matching degree of a video-music pair."
        },
        {
            "heading": "4.2 Cross-modal Model for Teacher Network",
            "text": "In this work, we utilize cross-generation loss, kl-divergence loss and margin-based matching loss to train our teacher network as:\nLt =Eztm\u223cq(ztm |mt)[p(v t | ztm) + KL \u2225\u2225\u2225\u2225q (ztm | mt) , p (ztm)\u2225\u2225\u2225\u2225] +Eztv\u223cq(ztv |vt)[p(m t | ztv) + KL \u2225\u2225\u2225\u2225q (ztv | vt) , p (ztv)\u2225\u2225\u2225\u2225]\n+Eztm\u223cq(ztm |mt),ztv\u223cq(ztv |vt) [ p ( yt | ztm, ztv )] ,\n(6)\nwhere a robust feature extractor of the inference network could be obtained. The validation set is excluded for selecting the best teacher network. The details of the training procedure of the teacher network are summarized in Algorithm 1 for reference."
        },
        {
            "heading": "4.3 Deconfounded Cross-modal Model for Student Network",
            "text": ""
        },
        {
            "heading": "4.3.1 Backdoor Adjustment",
            "text": "According to Pearl\u2019s theory of backdoor adjustment [30], we should control the confounder such that the backdoor path could be blocked, which is achieved by the do-calculus on the music node zm in the causal graph. do(zm) can be intuitively seen as cutting off the edge u \u2192 zm in the causal graph and blocking the effect of u on zm by controlling zm, and thus, the target of5\nDebCM could be formulated as: P(y | do(zm), zv), which could be formalized as:\nP(y | do(zm), zv)\n= \u222b Uu P(u | do(zm), zv)P(y | do(zm), zv,u)du (7a)\n= \u222b Uu P(u)P(y | zm, zv,u)du, (7b)\nwhere u denotes the historical distribution among music genres of user u, andUu represents the total sample space of u. Moreover, Eq. (7a) follows the law of total probability and Bayesian rule, and Eq. (7b) is based on the definition of do-operator in [30].\nIntuitively, user preference among music genres u has extensive possible values in a specific dataset, i.e., users have various historical distributions over music genres. In DebCM, we use an approximation approach demonstrated as follows to fulfill the estimation."
        },
        {
            "heading": "4.3.2 Backdoor Adjustment Approximation",
            "text": "We re-formalize the integral in Eq. (7b) into an expectation form as: \u222b\nUu P(u)P(y | zm, zv,u)du = EP(u)[P(y | zm, zv,u)] \u2248 P ( zm, zv,EP(u)u ) ,\n(8)\nwhere the approximation is introduced by work in [32] with a theoretical upper bound of the error. We estimate the expectation with a Monte Carlo method [33] by drawing samples from the space of user preference and calculating the expectation. For the consideration of computational efficiency and variance reduction of the Monte Carlo estimator, we utilize the average of user preference among music genres in each batch to do the approximation."
        },
        {
            "heading": "4.3.3 Backdoor Adjustment Estimator",
            "text": "To facilitate the usage of DebCM, we design the operator to instantiate backdoor adjustment. Specifically, we infer the user preference embedding zu of uploader u from user historical distribution of interacted music genre u with an Embedding operation, where the expectation is calculated by statistics of the collected user space. To further embed the user preference among music genres, we instantiate a genre embedding as Eg \u2208 Rd\u00d7Ng , where Ng is the number of music genres and d is the dimension of genre embeddings. Then, we define g \u2208 range(1,Ng) as the g-th music genre,Mu as the set of collected music of uploader u, and Genre(m) as the function that returns the music genre of music m. In this way, we could calculate the g-th music genres distribution of uploader u as ug = \u2211 m\u2208Mu I(Genre(m) = g)/|Mu|. For example, we denote u as [0.5, 0.5, 0, 0] when we have 4 music genres, with half interacted music being the first genre and half being the second genre. Finally, we calculate the user preference embedding of batch-level average zu as Eg \u00b7 \u2211 u\u2208UB uP(u) with P(u) defined as 1/|UB| andUB denotes the set of uploaders in a Batch during training, and then we concatenate it to music embedding zm to gain deconfounded music embedding z\u2032m as:\nz\u2032m = zm||zu. (9)\nThen, cross-generation strategy and margin-based matching are conducted on z\u2032m and zv to do the cross-modal alignment, where cross-generation loss, KL-divergence loss, and matching loss are calculated and added to get the loss of student network Ls."
        },
        {
            "heading": "4.4 Knowledge Transfer",
            "text": "We propose to learn a stable inference extractor of the teacher network, and then, the embedding learning process of the student network could be guided by the knowledge transferred from the teacher network. Therefore, the causal features in the PGC data modeled by the teacher network could be utilized to correct the bias from the UGC data modeled by the student network. Specifically, a feature-based distillation is leveraged to filter out the representative stable features from the teacher network, which is then used to guide a student network to mimic the feature inference of the teacher model. The teacher algorithm optimizes a deep auto-encoder model for feature modeling and guides the learning of embeddings from the student network through a KL-divergence manner, where distributions of latent variables in the student network could be guided and better aligned through the knowledge in the teacher network. The final objective Ls\u2032 of the student network is: Ls\u2032 = Ls + \u03bbCausEv KL\n\u2225\u2225\u2225ztv\u2032, zv\u2225\u2225\u2225 + \u03bbCausEm KL \u2225\u2225\u2225ztm\u2032, z\u2032m\u2225\u2225\u2225 , (10) where ztv\u2032 and zv are embedding of video from teacher and student network, ztm\u2032 and z\u2032m are embedding of music from teacher and student network of the UGC data. Here, we model each variable as a Gaussian distribution, where the KL divergence of two Gaussian variables have an analytic expression. The training steps of the student network guided by the teacher network are summarized in Algorithm 2."
        },
        {
            "heading": "4.5 Inference Strategy",
            "text": "After training, the learned recommender model by P(y | do(zm), zv) and inference model of z\u2032m and zv could be used for debiased background music recommendation. Specifically, for a newly uploaded UGC v, zv could be inferred using p (zv | v) and music embeddings in the music pool could be inferred using p (zm | m). We utilize the global average of user historical interacted distributions among music genres in the training set to calculate the averaged user embedding zu, where the deconfounded music embedding z\u2032m can be obtained by Eq. (9). Finally, the dot product is utilized to calculate the matching degree of video v and music in the music pool, the matching scores are then ranked where the top-K music is selected for recommendations.\nTo some extent, bias might be beneficial to exclude the music genres they dislike which could improve the satisfaction of recommended music for users. For example, users might only like jazz music so they don\u2019t listen to the music in other genres. In this case, a non-debiased personalized background music recommender system could learn the correlation between user preference among music genres and the choice of the background music of users, which can recommend user-preferred music for the uploaded items. However, on the other hand, users also prefer to select more suitable background music for the uploaded music to facilitate the spread and the index of the video. In this way, dynamically disentangling the effects between the user preference among music and the true appropriate matching of the music and the video is a further research area of our work.\nAlgorithm 1: TeacherN-SGD: Training TeacherN with SGD. Input: A video-music matching datasetDt = {Vt,Mt, ft}; \u0398t indicates the model parameters. Randomly initialize \u0398t. while not converged do\nRandomly sample a batch D\u0302t fromDt. Compute \u00b5tm, \u03c3 t m via variational encoders. Add the KL-divergence to the loss. Sample \u223c N (0, I) and compute ztm and ztv via the\nreparametrization trick. Add the cross reconstruction loss and matching loss to Lt. Compute the gradient of loss \u2207\u0398tLt. Update \u0398t by taking stochastic gradient steps.\nend return \u0398t Output: TeacherN model trained on datasetDt.\nAlgorithm 2: DebCM-SGD: Training DebCM with SGD. Input: A video-music matching datasetD = {u,V,M, f};\nthe video setV contains the visual feature v; the music setM contains the audio feature m and\nmusic genre g; the mapping f = {(v,m, y)} with y \u2208 {0, 1}; \u0398 indicates the model parameters. Randomly initialize \u0398. while not converged do\nRandomly sample a batch D\u0302 fromD. forall mod \u2208 {m, v} do\nCompute \u00b5mod, \u03c3mod via variational encoders. end Add the KL-divergence to the loss. Inference \u00b5tv \u2032, \u03c3tv \u2032 and \u00b5tm \u2032, \u03c3tm \u2032 from teacher network. Sample \u223c N (0, I) and compute zm and zv via the reparametrization trick. Concatenate the batch-average genre embedding zu to zm to get z\u2032m for deconfounder. Add the cross reconstruction loss and matching loss. Add KL-divergence of T-S embeddings to the loss. Compute the gradient of loss \u2207\u0398Ls\u2032. Update \u0398 by taking stochastic gradient steps.\nend return \u0398 Output: DebCM model trained on datasetD."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In this section, we conduct extensive experiments based on the established TT-150k-genre dataset to evaluate our proposed model DebCM by investigating the following research questions:\n\u2022 How does DebCM perform for the intervened micro-video background music recommendation compared with the baseline methods? Among them, non-debiased and debiased methods are included for comprehensive comparisons.\n\u2022 Since the user preference deconfounder and the PGC knowledge distillation modules are the main designs in our pro-\nposed DebCM, we investigate how do the two components contribute to the performance as ablation studies.\n\u2022 A case study is conducted to visualize whether DecRS can produce more robust and satisfying recommendations when user interest drift happens."
        },
        {
            "heading": "5.1 Dataset",
            "text": "TT-150k [2] has been established for content-based micro-video background music recommendation which composes of approximately 3,000 different background music clips associated with 150,000 micro-videos from different uploaders. We extend it by categorizing each music with a music genre using a pre-trained music genre classification model [34] where we call the extended dataset as TT-150k-genre. The pre-trained classification model is based on music genre from the GTZAN music corpus [35] with multiple layers of LSTM Neural Nets, which is then utilized to extract the music genre of each music clip in the TT-150k. Through getting the music genres, user preference among music, which is the confounder that exists in micro-video background music recommendation could be obtained by calculating her or his historical interactions among music genres. The statistics of the dataset are in Table 2.\nIntervention among Test Set: Since the dataset is generated by uploaders themselves, rather than experimental, it contains a lot of bias. Measurements on the biased test set are unlikely to be reliable. However, it is time-consuming and difficult to collect a completely unbiased, content-based music-video matching data set, and therefore, we use a non-random sampling to produce an unbiased-like test set as the intervened test set. Followed by [36], we sample two settings of the skewed test set to evaluate the effectiveness of our proposed user deconfounder module and PGC knowledge transfer network, respectively. Specifically, to evaluate the deconfounded ability of uploader\u2019s historical preference among music genres, we sample the videos with uploaders whose historical interactions among music genres are the most diverse as our skewed test set Tdiverse, where the diversity degree is calculated by the entropy of the probability among different music genres of uploader\u2019s historical interactions. In this way, the quality is assessed on videos with interest-varied uploaders, and therefore, the ability to alleviate the confounder effect which will cause the recommender to recall items with majority music genres of uploaders\u2019 historical preference could be well evaluated. On the other hand, to evaluate the effectiveness of PGC knowledge transfer, which aims to recommend the most-matching background music to the videos, we construct an intervened test set Tmatching by sampling the videos that match the background music best. The matching degree is calculated by two factors: 1) the popularity of the videos; 2) the matching scores calculated by CMT [37], where music-video rhythmic relations are utilized for referring to the matching degree of the music and the video. In this way, well-matched videos are\nsampled for evaluating the effectiveness of recommending the most-matching background music."
        },
        {
            "heading": "5.2 Methods for Comparisons",
            "text": "To demonstrate the effectiveness of the proposed DebCM, we draw comparisons with several state-of-the-art baselines, where debiased methods considering bias problems and non-debiased methods are both included. Moreover, all methods are tested with two settings: 1) using only UGC data and 2) using both UGC and PGC data. For non-debiased methods, we utilize the PGC data as the training data straightforwardly. Label-based [25] knowledge distillation is used to employ the PGC data for debiased methods because debiased methods need auxiliary genre data during training the UGC network which is not applicable for PGC data. More specifically, we have obtained a teacher model trained on PGC data, and then use it to predict all the UGC samples, where the imputed labels are combined with the original labels through a weighting parameter to train a more unbiased student model on UGC data. The baselines are listed as follows:\n\u2022 CEVAR [7]: It is a matching-based method that applies a cosine similarity loss to constrain the matched video and music embeddings to be close. A label prediction loss of the video is also attached where we dismissed it for applicable comparisons. \u2022 CMVBR [13]: CMVBR leverages a ranking-based matching\nloss to align inter-modal embeddings of music and video pairs, where an intra-modal loss is applied to constrain similarity within one modality. \u2022 DSCMR [38]: It introduces a weight-sharing strategy for the\nlast layer of the two-branch framework, and therefore, the cross-modal difference could be reduced and a matching-loss in the common latent space is utilized for matching. \u2022 UWML [39]: It is a margin-based metric learning framework\nto constrain the closeness of matched pairs to unmatched pairs. Moreover, different weights are given to different samples according to the difficulty of matching. \u2022 Dual-VAE [40]: Dual-VAE is a generative-based retrieval\nmodel, where question-to-question and answer-to-answer reconstruction loss are utilized for alignment of the matched pairs. \u2022 CMVAE [2]: It introduces a PoE fusing module to integrate\nthe visual and textual modalities in micro-videos. Moreover, a cross-generation strategy is utilized to further align the latent space of music and videos. It is the backbone of our proposed model where textual modality is dismissed for the lack of textual descriptions of PGC data.\nWe also compare our proposed debiased cross-modal background music recommender with the state-of-the-art debiased models in recommender systems:\n\u2022 IPS [20]: IPS is a statistics-based method in causal recommendation. Here we use user preference among music genres as the propensity of an uploader to down-weight the music in the majority genre group during debiased training. \u2022 DU [21]: It is an extension of IPS, which introduces a dual\nrecommender learning framework that simultaneously eliminates the bias of clicked and unclicked data.\n\u2022 Calibration [29]: It proposes a calibration metric to keep the proportion of different areas of interest in the recommendation list to be the same as user historical interests for alliterating the over-recommend phenomenon, where post-processing toward the output of the recommender is utilized to minimize the metric.\n\u2022 DecRS [23]: DecRS is a deconfounded method that considers the user preference among item categories as a confounder and utilizes backdoor adjustment to alleviate the amplification of the bias problem, where the global-average estimation of the confounder has been utilized.\nThe above methods are model-agnostic, where we use our backbone and apply the debiased module for fair comparisons."
        },
        {
            "heading": "5.3 Evaluation Metrics",
            "text": "Two standard evaluation metrics for retrieving: Recall@K and NDCG@K [39] are leveraged to evaluate the model performance, which calculates the hit-ratios of ground-truth music on top-K retrieved music. According to [2], the long-tail characteristic of the popularity of different music clips is severe, which leads to a systematic bias that favors the popular music clips when weighing the match equally for different music clips. To alleviate the problem, following [19, 2], music clips for evaluation are weighted by the inverse of their popularity levels. Therefore, the Recall@K and NDCG@K we adopt in this paper are calculated as:\nRecall@K = \u2211\nv\u2208Vte \u03bbv \u00b7 #Hits v@K (11)\nNDCGu@K = \u2211\nv\u2208Vte K\u2211 i=1 \u03bbv \u00b7 log2 2 \u00b7 (2ri \u2212 1) log2(i + 1) (12)\n\u03bbv = 1/P(v\u2192 m)\u2211\ni\u2208Vte 1/P(i\u2192 m) , (13)\nwhereVte represents the set of test videos. Recall@k calculates the hit ratio of the tested models. Normalized Discounted Cumulative Gain (NDCG) assigns different importance to different ranks as ri represents the relevance degree of the item at position i and it is assigned as binary (i.e., 0 or 1) in implicit recommendation scenario with log 2 to be a normalizer. \u03bbv denotes the weight of the video v calculated by Eq. (13). Specifically, 1/P(v\u2192 m) is assumed to re-weight the popularity bias of the music clips, which is calculated as the reciprocal of the popularity of the music clip that the target video v used as background music. Therefore, the popularity-debiased hit-ratio could be obtained with Recall@K and NDCG@K used in this paper."
        },
        {
            "heading": "5.4 Parameter Settings",
            "text": "Our DebCM model is implemented in Pytorch. The dimension of embeddings d is fixed to 128, Adam [41] is applied as the optimizer and the batch size is set to 1,024 for all models, empirically. For the proposed DebCM and all baselines, we adopt a grid search to select the hyperparameters based on evaluation metrics on the validation set. The learning rate is search in {0.0001, 0.005, 0.001, 0.05, 0.01}, the L2 normalization coefficient is tuned amongst {0.001, 0.01, 0.1, 0}, and the dropout rate is selected in {0.1, 0.2, 0.3, 0.4, 0.5}. Two fully-connected\nlayers [F \u2192 d \u2192 F] with F to be the dimension of different features are applied as the encoder and decoder for the music and video networks of the cross-modal matching model. We further select the parameters of bi-directional margin-based matching loss by searching, and specifically, the weight of the music-tovideo matching loss is set to 1, the margin of the matching loss is set to 0.05, and 40 most hard-to-match negative samples in the ranking-based matching loss are the chosen during training. By tuning, we set the weight of the L2 norm to 0.001, the weight of the reconstruction loss to 1, the dropout rate to 0.2, and the learning rate to 0.001. For the teacher distillation, we set the weight of the teacher guidance loss to be 40 by tuning."
        },
        {
            "heading": "5.5 Performance Comparison with Baselines",
            "text": "The overall comparison results are summarized in Table 3. From the table, we could see that the majority of non-debiased methods yield inferior results than debiased methods on both Tdiverse and Tmatching, where intervention is performed on the testing data. Among the non-debiased methods, generativebased methods (Dual-VAE and CMVAE) achieve improvements over matching-based methods, which demonstrates the effectiveness of generative-based models and variational-based training, which could be more robust to noise and disturbances. While on the intervened test set, the non-debiased methods that are based on strong inductive loss in an unbiased hypothesis, some relational effect, not the causal effect, could be learned that is harmful to the intervened test set so as to make the result not ideal. In addition, simply adding PGC data to UGC data for the training set cannot achieve good results. The possible reason is that PGC data is more diverse, which makes training more difficult and contains more uncertainty.\nFor debiased methods, IPS performs worse than the vanilla backbone model (CMVAE) on Tdiverse settings, which suggests that re-weighting the samples is not enough to alleviate the bias, especially when there exist not-really-matching music-video pairs caused by the selection bias of limited knowledge of uploaders. DU adds bidirectional constraint on IPS, thus gaining some additional improvement. Calibration re-ranks the matching sequence which could perform well on our skewed test set by considering the deconfounder degree and matching degree. However, the above methods that don\u2019t carefully consider user preference among music genres as confounders and truly content-matching of music-videos could not achieve splendid results. Structure causal modeling (SCM) -based approach DecRS works well for causal reasoning by establishing true causal relationships and removing harmful associations (the influence of the uploader\u2019s historical preference for music embeddings and music choice). For the use of PGC data, a label-based knowledge distillation method is utilized. It can be seen from the results that the ideal guidance cannot be obtained by using PGC data in this way, because the prediction scores of the PGC teacher network directly change the correctness of UGC data with strong labels, while in fact, the teacher model inevitably contains errors brought by model, data and other factors.\nDebCM proposed in this work further optimizes the backdoor approximation over DecRS in the student network, which replaces the global average with the batch-level average to make the model conform to the unbiased substitution of Monte Carlo sampling. From a macro point of view, the batch-level average\nintroduces more randomness into the training, so that the model can better deal with data deviation. Therefore, our proposed DebCM could outperform other methods on Tdiverse, which aims to evaluate the effectiveness of deconfounder ability caused by the uploader\u2019s historical preference. As a by-product, the deconfounder module in the student network could, on the other hand, improve true-matching between videos and music by alleviating the over-recommendation of majority groups of music. Furthermore, to employ PGC data to further eliminate the UGC selection bias caused by the uploader\u2019s personal limitation in the data, we put forward PGC training teachers to guide the UGC training of the student network. As we could see from the results of using Ds \u222a Dt on both settings of our proposed DebCM, the learning process of hidden variables in the student network is further constrained and guided by the teacher network, so as to get better performance."
        },
        {
            "heading": "5.6 Ablation Study of DebCM",
            "text": "In this section, we investigate the effectiveness of different components in DebCM. In detail, as the user preference deconfounder plays a vital role in DebCM, we change different ratios of intervention on music genre in training and test sets to investigate the effect of the deconfounder module. Moreover, we explore the effectiveness of PGC knowledge distillation by changing different weights of the PGC teacher network in the proposed model."
        },
        {
            "heading": "5.6.1 The Effectiveness of User Preference Deconfounder",
            "text": "Causal intervention on the dataset for deconfounder evaluation: To evaluate the deconfounder ability of our model using backdoor adjustment, causal intervention is utilized to control music genre distributions in the training and test set. Specifically, to conduct an intervention on the TT-150k-genre dataset, the genre distribution in training and test sets for \u201chiphop\" and \u201cjazz\" are set to be different, which are two common music gen-\nres the majority of users like both. For keeping the X : (1 \u2212 X) (X \u2208 range(0.1, 0.9)) distribution of two music genres in the training set and the 8:1:1 ratio of training, validation, and test set, we calculate the number of music of genre \u201chiphop\" and genre \u201cjazz\" in the training set, and then the ratio of two music genre in validation and test set approximates to (1 \u2212 X) : X, which exists a huge difference. We exhibit 4 different splits with X set to be 0.8, 0.7, 0.6, and 0.5, where X with a value close to extremity 1 demonstrates a stronger intervention imposed upon the dataset. Moreover, under such a setting, we split the training and test set with music clips, where the music in the training set and test set are mutually exclusive. We follow the setting of \u201cstrong generalization\u201d in [2] where stratified sampling according to music popularity is utilized.\nWe explore the performance under different ratios of music genres of the training set and test set between our proposed\nmodel and the best-performing causal reasoning baseline DecRS to verify the robustness of our model to deviations caused by users\u2019 historical preferences in data. Moreover, we could analyze whether using the strategy of batch-level average to realize the approximation of backdoor adjustment proposed in this paper is better than the global mean in DecRS. Furthermore, we could investigate the robustness of our well-designed teacher-student network to different deviation degrees.\nAs seen from the experimental results in Fig. 3, our model achieves the best performance under different music genre ratios of the test set. The comparison between DebCM and DecRS without PGC data demonstrates that the batch-level average strategy proposed in this paper introduces more randomness and conforms to the unbiased setting of Monte Carlo sampling, so as to achieve better results and show good performance for different degrees of data deviation. Furthermore, we introduce PGC data for knowledge distillation to guide students\u2019 unbiased learning of the network, which further improves the model\u2019s debiasing effect and achieves the best performance under different deviation ratios."
        },
        {
            "heading": "5.6.2 The Effectiveness of PGC Knowledge Distillation",
            "text": "The performance of PGC data-induced knowledge distillation is investigated in this section, which is one of the main designs. Specifically, we explore the impact of different weights of the PGC network to explore the control weight of distillation.\nWe explore the influence of different weights of the teacher network on the training of the student network. Specifically,\nwe control the constraints of the teacher to the student network training, and the weight of PGC loss among {5e-2, 5e-3, 5e-4, 5e-5, 5e-6, 5e-7}. With the increase of weight, the influence of the teacher network increase, thus more guidance of hidden variables of student network to be near to teacher network, so as to better remove the influence of user\u2019s personal preference effect. From the results in Fig. 4, we observe that the performance rises first and then descends with the weight of the loss of teacher network increases. It suggests that learning of hidden variables in the student network could be guided and constrained by hidden variables inferred by the teacher network, which removes the selection bias of the student network by guiding the uploader\u2019s personal matching with professional matching. However, excess constraints will limit the data fitting of the student network on its own UGC data, thereby degrading the performance of the network. Therefore, we should choose an appropriate constraint weight to reasonably use PGC data to train the appropriate teacher network and give appropriate control to achieve the best constraint and guidance for the student network."
        },
        {
            "heading": "5.7 Visualization",
            "text": "This section visualizes videos selected from test sets and displays the sorted results obtained by our model, with the best match on the left and the least match on the right. We also show the historical preference among music genres of the video\u2019s uploader to see the performance of our model when user interest drift happens. As can be seen from Fig. 5, our model can effectively reduce the effect of bias amplification of traditional recommendation systems for users whose historical preferences are concentrated in a certain genre. Instead, it recommends some background music that is more compatible with the video in the test set, which is also matched by users with interest shifts. For example, in the first video, the uploader preferred classical music in history, while the uploaded video belongs to sports videos with more intense and high rhythm, which is not suitable for classical music. Therefore, this model recommends more\nlively and bright Jazz music and some intense Hip-hop music. This makes matching more reasonable, which is achieved by deconfounding for user historical preference and learning good matches from PGC data. However, the video below the dotted line is a bad case. The video\u2019s uploader used to be more inclined to pop music, while the model learned that the video is closer to reggae and Hip-hop styles, so it recommended such songs. However, the uploader still sticks to its preference and chooses pop music. This also demonstrates the two considerations of uploaders when choosing background music: the user\u2019s own preference and the best-matching background music of the video. How to balance these two aspects is a problem worthy of further study."
        },
        {
            "heading": "6 CONCLUSIONS",
            "text": "In this work, we have proposed a Teacher-Student network where the influence of user historical preference, as a confounder, is alleviated in the student network based on causal intervention. Specifically, the backdoor adjustment strategy is well approximated, where uploader-induced confounding bias can be addressed by substituting batch-level average uploader for the true uploader with personal bias. In addition, the matching of PGC data is captured by a teacher network which is capable of guiding the matching of the UGC student network by KL-based knowledge transfer. To evaluate the debiased ability of our proposed model, we extend the background music recommendation dataset TT-150k with music genre labels where an intervened evaluation strategy is introduced accordingly. Extensive experiments demonstrate that DebCM is more robust to uploaders\u2019 selection bias than state-of-the-art strategies.\nReferences\n[1] Chien-Liang Liu and Ying-Chuan Chen. Background music recommendation based on latent factors and moods. Knowledge-Based Systems, 159:158\u2013170, 2018.\n[2] Jing Yi, Yaochen Zhu, Jiayi Xie, and Zhenzhong Chen. Cross-modal variational auto-encoder for content-based micro-video background music recommendation. IEEE Transactions on Multimedia, 2021. doi:10.1109/TMM.2021.3128254.\n[3] Shoto Sasaki, Tatsunori Hirai, Hayato Ohya, and Shigeo Morishima. Affective music recommendation system based on the mood of input video. In MultiMedia Modeling International Conference, volume 8936 of Lecture Notes in Computer Science, pages 299\u2013302, 2015. doi: 10.1007/978-3-319-14442-9\\_33.\n[4] Robert E Thayer. The biopsychology of mood and arousal. Oxford University Press, 1990.\n[5] Jiansong Chao, Haofen Wang, Wenlei Zhou, Weinan Zhang, and Yong Yu. Tunesensor: A semantic-driven music recommendation service for digital photo albums. In Proceedings of the International Semantic Web Conference, 2011.\n[6] Bochen Li and Aparna Kumar. Query by video: Crossmodal music retrieval. In International Society for Music Information Retrieval Conference, pages 604\u2013611, 2019.\n[7] Didac Sur\u00eds, Amanda Duarte, Amaia Salvador, Jordi Torres, and Xavier Gir\u00f3-i Nieto. Cross-modal embeddings for video and audio retrieval. In Proceedings of the European Conference on Computer Vision Workshops, 2018.\n[8] Kaiye Wang, Qiyue Yin, Wei Wang, Shu Wu, and Liang Wang. A comprehensive survey on cross-modal retrieval. arXiv preprint arXiv:1607.06215, 2016.\n[9] Lanyu Shang, Zhang Daniel Yue, Khan Siamul Karim, Jialie Shen, and Dong Wang. CaMR: Towards connotationaware music retrieval on social media with visual inputs. In IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, pages 425\u2013429, 2020. doi: 10.1109/ASONAM49781.2020.9381371.\n[10] Xixuan Wu, Yu Qiao, Xiaogang Wang, and Xiaoou Tang. Bridging music and image via cross-modal ranking analysis. IEEE Transactions on Multimedia, 18(7):1305\u20131318, 2016.\n[11] Aihua Zheng, Menglan Hu, Bo Jiang, Yan Huang, Yan Yan, and Bin Luo. Adversarial-metric learning for audio-visual cross-modal matching. IEEE Transactions on Multimedia, 24:338\u2013351, 2022. doi: 10.1109/TMM.2021.3050089.\n[12] Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, and Sudheendra Vijayanarasimhan. Youtube-8m: A largescale video classification benchmark. arXiv preprint arXiv:1609.08675, 2016.\n[13] Sungeun Hong, Woobin Im, and Hyun S Yang. Cbvmr: content-based video-music retrieval using soft intra-modal structure constraint. In Proceedings of the ACM on International Conference on Multimedia Retrieval, pages 353\u2013361, 2018.\n[14] Laure Pretet, Gael Richard, Clement Souchier, and Geoffroy Peeters. Video-to-music recommendation using temporal alignment of segments. IEEE Transactions on Multimedia, 2022. doi:10.1109/TMM.2022.3152598.\n[15] Hao Zhang, Aixin Sun, Wei Jing, and Joey Tianyi Zhou. Towards debiasing temporal sentence grounding in video. arXiv preprint arXiv:2111.04321, 2021.\n[16] Zhiquan Wen, Guanghui Xu, Mingkui Tan, Qingyao Wu, and Qi Wu. Debiased visual question answering from feature and sample perspectives. Advances in Neural Information Processing Systems, 34, 2021.\n[17] Zhenyu Huang, Guocheng Niu, Xiao Liu, Wenbiao Ding, Xinyan Xiao, Hua Wu, and Xi Peng. Learning with noisy correspondence for cross-modal matching. Advances in Neural Information Processing Systems, 34, 2021.\n[18] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. Bias and debias in recommender system: A survey and future directions. arXiv preprint arXiv:2010.03240, 2020.\n[19] Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, and Thorsten Joachims. Recommendations as treatments: Debiasing learning and evaluation. In Proceedings of the International Conference on International Conference on Machine Learning, pages 1670\u20131679, 2016.\n[20] Longqi Yang, Yin Cui, Yuan Xuan, Chenyang Wang, Serge Belongie, and Deborah Estrin. Unbiased offline recommender evaluation for missing-not-at-random implicit feedback. In Proceedings of the ACM Conference on Recommender Systems, pages 279\u2013287, 2018.\n[21] Jae-woong Lee, Seongmin Park, and Jongwuk Lee. Dual unbiased recommender learning for implicit feedback. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1647\u20131651, 2021.\n[22] Yang Zhang, Fuli Feng, Xiangnan He, Tianxin Wei, Chonggang Song, Guohui Ling, and Yongdong Zhang. Causal intervention for leveraging popularity bias in recommendation. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 11\u201320, 2021. doi: 10.1145/3404835. 3462875.\n[23] Wenjie Wang, Fuli Feng, Xiangnan He, Xiang Wang, and Tat-Seng Chua. Deconfounded recommendation for alleviating bias amplification. In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1717\u20131725, 2021.\n[24] Wenjie Wang, Fuli Feng, Xiangnan He, Hanwang Zhang, and Tat-Seng Chua. Clicks can be cheating: counterfactual recommendation for mitigating clickbait issue. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1288\u20131297, 2021.\n[25] Dugang Liu, Pengxiang Cheng, Zhenhua Dong, Xiuqiang He, Weike Pan, and Zhong Ming. A general knowledge distillation framework for counterfactual recommendation via uniform data. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 831\u2013840, 2020.\n[26] Jiawei Chen, Hande Dong, Yang Qiu, Xiangnan He, Xin Xin, Liang Chen, Guli Lin, and Keping Yang. Autodebias: Learning to debias for recommendation. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 21\u201330, 2021.\n[27] Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Li Wei, Yi Wu, Lukasz Heldt, Zhe Zhao, Lichan Hong, Ed H Chi, et al. Fairness in recommendation ranking through pairwise comparisons. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 2212\u20132220, 2019.\n[28] Qiong Wu, Yong Liu, Chunyan Miao, Binqiang Zhao, Yin Zhao, and Lu Guan. Pd-gan: Adversarial learning for personalized diversity-promoting recommendation. In Proceedings of the International Joint Conference on Artificial Intelligence, volume 19, pages 3870\u20133876, 2019.\n[29] Harald Steck. Calibrated recommendations. In Proceedings of the ACM Conference on Recommender Systems, page 154\u2013162, 2018.\n[30] Madelyn Glymour, Judea Pearl, and Nicholas P Jewell. Causal inference in statistics: A primer. John Wiley & Sons, 2016.\n[31] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference on Learning Representations, 2014.\n[32] Shoshana Abramovich and Lars-Erik Persson. Some new estimates of the \u2018jensen gap\u2019. Journal of Inequalities and Applications, 1:1\u20139, 2016.\n[33] Nicholas Metropolis and S. Ulam. The monte carlo method. Journal of the American Statistical Association, 44(247): 335\u2013341, 1949. ISSN 01621459. URL http://www. jstor.org/stable/2280232.\n[34] Prasenjeet Fulzele, Rajat Singh, Naman Kaushik, and Kavita Pandey. A hybrid model for music genre classification using lstm and svm. In Eleventh International Conference on Contemporary Computing, pages 1\u20133, 2018. doi: 10.1109/IC3.2018.8530557.\n[35] G. Tzanetakis. Gtzan music/speech. URL http:// marsyasweb.appspot.com/download/data_sets/.\n[36] Dawen Liang, Laurent Charlin, and David M Blei. Causal inference for recommendation. In Causation: Foundation to Application, Workshop at UAI. AUAI, 2016.\n[37] Shangzhe Di, Zeren Jiang, Si Liu, Zhaokai Wang, Leyan Zhu, Zexin He, Hongming Liu, and Shuicheng Yan. Video background music generation with controllable music transformer. In Proceedings of the 29th ACM International Conference on Multimedia, pages 2037\u20132045, 2021.\n[38] Liangli Zhen, Peng Hu, Xu Wang, and Dezhong Peng. Deep supervised cross-modal retrieval. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 10394\u201310403, 2019.\n[39] Jiwei Wei, Xing Xu, Yang Yang, Yanli Ji, Zheng Wang, and Heng Tao Shen. Universal weighting metric learning for cross-modal matching. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 13005\u201313014, 2020.\n[40] Dinghan Shen, Yizhe Zhang, Ricardo Henao, Qinliang Su, and Lawrence Carin. Deconvolutional latent-variable model for text sequence matching. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, pages 5438\u20135445, 2018.\n[41] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12 (7), 2011."
        }
    ],
    "title": "Debiased Cross-modalMatching for Content-basedMicro-video BackgroundMusic Recommendation",
    "year": 2022
}