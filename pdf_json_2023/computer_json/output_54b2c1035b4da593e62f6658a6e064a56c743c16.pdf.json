{
    "abstractText": "Clustering is at the very core of machine learning, and its applications proliferate with the increasing availability of data. However, as datasets grow, comparing clusterings with an adjustment for chance becomes computationally difficult, preventing unbiased ground-truth comparisons and solution selection. We propose FastAMI, a Monte Carlo-based method to efficiently approximate the Adjusted Mutual Information (AMI) and extend it to the Standardized Mutual Information (SMI). The approach is compared with the exact calculation and a recently developed variant of the AMI based on pairwise permutations, using both synthetic and real data. In contrast to the exact calculation our method is fast enough to enable these adjusted information-theoretic comparisons for large datasets while maintaining considerably more accurate results than the pairwise approach.",
    "authors": [
        {
            "affiliations": [],
            "name": "Kai Klede"
        },
        {
            "affiliations": [],
            "name": "Leo Schwinn"
        },
        {
            "affiliations": [],
            "name": "Dario Zanca"
        },
        {
            "affiliations": [],
            "name": "Bj\u00f6rn Eskofier"
        }
    ],
    "id": "SP:32b7efbf0643c01646ef77737663c500febdaa20",
    "references": [
        {
            "authors": [
                "C.C. Aggarwal",
                "Reddy",
                "C. K",
                "eds"
            ],
            "title": "Data Clustering: Algorithms and Applications",
            "venue": "CRC Press. ISBN 978-146-655821-2",
            "year": 2014
        },
        {
            "authors": [
                "R. Bhatia",
                "C. Davis"
            ],
            "title": "A Better Bound on the Variance",
            "venue": "Am. Math. Mon., 107(4): 353\u2013357.",
            "year": 2000
        },
        {
            "authors": [
                "P. Chunaev"
            ],
            "title": "Community detection in node-attributed social networks: A survey",
            "venue": "Comput. Sci. Rev., 37: 100286.",
            "year": 2020
        },
        {
            "authors": [
                "S. Edwards"
            ],
            "title": "Thomas M",
            "venue": "Cover and Joy A. Thomas, Elements of Information Theory (2nd ed.), John Wiley & Sons, Inc. (2006). Inf. Process. Manag., 44(1): 400\u2013401.",
            "year": 2008
        },
        {
            "authors": [
                "B. Fristedt"
            ],
            "title": "The Structure of Random Partitions of Large Integers",
            "venue": "Transactions of the American Mathematical Society, 337(2): 703\u2013735.",
            "year": 1993
        },
        {
            "authors": [
                "M Gagolewski"
            ],
            "title": "Benchmark Suite for Clustering Algorithms \u2013 Version 1. https://github.com/gagolews/ clustering benchmarks",
            "year": 2020
        },
        {
            "authors": [
                "W. H\u00f6rmann"
            ],
            "title": "A universal generator for discrete logconcave distributions",
            "venue": "Computing, 52(1): 89\u201396.",
            "year": 1994
        },
        {
            "authors": [
                "L. Hubert",
                "P. Arabie"
            ],
            "title": "Comparing Partitions",
            "venue": "Journal of Classification, 2(1): 193\u2013218.",
            "year": 1985
        },
        {
            "authors": [
                "V. Kachitvichyanukul",
                "B. Schmeiser"
            ],
            "title": "Computer Generation of Hypergeometric Random Variates",
            "venue": "Journal of Statistical Computation and Simulation, 22(2): 127\u2013145.",
            "year": 1985
        },
        {
            "authors": [
                "M.R. Karim",
                "O. Beyan",
                "A. Zappa",
                "I.G. Costa",
                "D. RebholzSchuhmann",
                "M. Cochez",
                "S. Decker"
            ],
            "title": "Deep learning-based clustering approaches for bioinformatics",
            "venue": "Briefings Bioinform., 22(1): 393\u2013415.",
            "year": 2021
        },
        {
            "authors": [
                "A. Lancichinetti",
                "S. Fortunato"
            ],
            "title": "Consensus Clustering in Complex Networks",
            "venue": "Scientific Reports, 2(1): 336.",
            "year": 2012
        },
        {
            "authors": [
                "D. Lazarenko",
                "T. Bonald"
            ],
            "title": "Pairwise Adjusted Mutual Information",
            "venue": "arXiv:2103.12641.",
            "year": 2021
        },
        {
            "authors": [
                "J. Leskovec",
                "A. Krevl"
            ],
            "title": "SNAP Datasets: Stanford Large Network Dataset Collection",
            "venue": "https://snap.stanford. edu/data/index.html. Accessed: 2022-03-22.",
            "year": 2014
        },
        {
            "authors": [
                "E. M\u00fcller",
                "S. G\u00fcnnemann",
                "I. F\u00e4rber",
                "T. Seidl"
            ],
            "title": "Discovering Multiple Clustering Solutions: Grouping Objects in Different Views of the Data",
            "venue": "Webb, G. I.; Liu, B.; Zhang, C.; Gunopulos, D.; and Wu, X., eds., ICDM 2010, The 10th IEEE International Conference on Data Mining,",
            "year": 2010
        },
        {
            "authors": [
                "X.V. Nguyen",
                "J. Epps",
                "J. Bailey"
            ],
            "title": "Information theoretic measures for clusterings comparison: is a correction for chance necessary? In Danyluk, A",
            "venue": "P.; Bottou, L.; and Littman, M. L., eds., Proceedings of the 26th Annual International Conference on Machine Learning, ICML 2009,",
            "year": 2009
        },
        {
            "authors": [
                "X.V. Nguyen",
                "J. Epps",
                "J. Bailey"
            ],
            "title": "Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance",
            "venue": "J. Mach. Learn. Res., 11: 2837\u20132854.",
            "year": 2010
        },
        {
            "authors": [
                "W.M. Patefield"
            ],
            "title": "Algorithm AS 159: An Efficient Method of Generating Random R \u00d7 C Tables with Given Row and Column Totals",
            "venue": "Journal of the Royal Statistical Society. Series C (Applied Statistics), 30(1): 91\u201397.",
            "year": 1981
        },
        {
            "authors": [
                "F. Pedregosa",
                "G. Varoquaux",
                "A. Gramfort",
                "V. Michel",
                "B. Thirion",
                "O. Grisel",
                "M. Blondel",
                "P. Prettenhofer",
                "R. Weiss",
                "V. Dubourg",
                "J. Vanderplas",
                "A. Passos",
                "D. Cournapeau",
                "M. Brucher",
                "M. Perrot",
                "E. Duchesnay"
            ],
            "title": "ScikitLearn: Machine Learning in Python",
            "venue": "Journal of Machine",
            "year": 2011
        },
        {
            "authors": [
                "S. Romano",
                "J. Bailey",
                "X.V. Nguyen",
                "K. Verspoor"
            ],
            "title": "Standardized Mutual Information for Clustering Comparisons: One Step Further in Adjustment for Chance",
            "venue": "Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June",
            "year": 2014
        },
        {
            "authors": [
                "S. Romano",
                "X.V. Nguyen",
                "J. Bailey",
                "K. Verspoor"
            ],
            "title": "Adjusting for Chance Clustering Comparison Measures",
            "venue": "J. Mach. Learn. Res., 17: 134:1\u2013134:32.",
            "year": 2016
        },
        {
            "authors": [
                "H. Shakibian",
                "N. Moghadam Charkari"
            ],
            "title": "Mutual Information Model for Link Prediction in Heterogeneous Complex Networks",
            "venue": "Scientific Reports, 7(1): 44981.",
            "year": 2017
        },
        {
            "authors": [
                "E. Stadlober"
            ],
            "title": "The ratio of uniforms approach for generating discrete random variates",
            "venue": "Journal of computational and applied mathematics, 31(1): 181\u2013189.",
            "year": 1990
        },
        {
            "authors": [
                "C.L. Staudt",
                "A. Sazonovs",
                "H. Meyerhenke"
            ],
            "title": "NetworKit: A tool suite for large-scale complex network analysis",
            "venue": "Netw. Sci., 4(4): 508\u2013530.",
            "year": 2016
        },
        {
            "authors": [
                "T. Tian",
                "J. Wan",
                "Q. Song",
                "Z. Wei"
            ],
            "title": "Clustering single-cell RNA-seq data with a model-based deep learning approach",
            "venue": "Nat. Mach. Intell., 1(4): 191\u2013198.",
            "year": 2019
        },
        {
            "authors": [
                "A.J. Walker"
            ],
            "title": "New fast method for generating discrete random numbers with arbitrary frequency distributions",
            "venue": "Electronics Letters, 10(8): 127\u2013128.",
            "year": 1974
        },
        {
            "authors": [
                "A.J. Walker"
            ],
            "title": "An Efficient Method for Generating Discrete Random Variables with General Distributions",
            "venue": "ACM Trans. Math. Softw., 3(3): 253\u2013256.",
            "year": 1977
        },
        {
            "authors": [
                "S. Wei",
                "G. Han",
                "R. Wang",
                "Y. Yang",
                "H. Zhang",
                "S. Li"
            ],
            "title": "Inductive Multi-view Multiple Clusterings",
            "venue": "2021 7th International Conference on Big Data and Information Analytics (BigDIA), 308\u2013315.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "Clustering comparison measures such as the mutual information or the Rand index are most commonly used to validate clusterings when ground truth information is available (Aggarwal and Reddy 2014). In the context of graphs, the mutual information of the neighborhoods of two nodes indicates their similarity and can predict missing links (Hoffman, Steinley, and Brusco 2015; Shakibian and Moghadam Charkari 2017). Another application is multipleclustering algorithms, which measure the mutual information or one of its variants to identify multiple qualitatively different clustering solutions for a single dataset (Mu\u0308ller et al. 2010; Wei et al. 2021). Other uses include categorical feature selection, where each feature is understood as a cluster or for the solution selection in consensus clustering (Lancichinetti and Fortunato 2012).\nA well-known problem with these clustering comparison measures is that they do not assume a constant baseline value when comparing two random clustering partitions. Instead, they tend to be larger when the number of clusters approaches the number of data points (Nguyen, Epps, and Bailey 2009), leading to a bias towards smaller clusters when used as an external validation criterion. To obtain a constant baseline, these measures have been adjusted by their expectation value under random permutations of the cluster labels\nCopyright \u00a9 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nin the Adjusted Rand index (ARI) (Hubert and Arabie 1985) and the Adjusted Mutual information (AMI) (Nguyen, Epps, and Bailey 2010).\nRomano et al. (2016) created an overview of those clustering comparison measures mentioned above and others. They analyzed the relationships between these measures and formulated a guideline for which one to use in which situation. Quoting directly from the abstract, the authors conclude that\nARI should be used when the reference clustering has large equal-sized clusters; AMI should be used when the reference clustering is unbalanced and there exist small clusters.\nHowever, it has been shown that the computational complexity of the adjusted mutual information is O(max(R,C)N), where R,C are the numbers of clusters in the clusterings to be compared and N is the number of data points (Romano et al. 2014). For large datasets with many small clusters and few bigger clusters, which is exactly the domain where the AMI should be used, its computation becomes difficult and often impractical. Many datasets originating from different domains exhibit these characteristics, such as social networks, collaboration networks, or web graphs (Leskovec and Krevl 2014). In practice, many authors resort to the nonadjusted normalized mutual information (Tian et al. 2019; Chunaev 2020; Karim et al. 2021), which is faster to compute, but does not account for random coincidences.\nAs a workaround, Lazarenko and Bonald (2021) proposed to consider only pairwise permutations in the adjustment for chance, i.e., to only adjust for clusterings where two individual samples exchanged their cluster labels. While this approach allows for faster computations (O(RC)), these pairwise permutations lead to a higher amount of shared information. In Figure 1 we show that the actual expected mutual information (EMI) is overestimated by the pairwise EMI. Consequently, the pairwise AMI systematically underestimates the AMI, and the results cannot be compared with existing values. Even the relative order of cluster similarity is affected, as shown in Table 1.\nWith FastAMI, we propose a Monte Carlo-based approach to enable AMI comparisons for large datasets with small clusters1. The presented method allows for finegrained control over its accuracy and addresses the short-\n1Code at https://github.com/mad-lab-fau/fastami-benchmark\ncomings of the pairwise approach. We prove a pessimistic upper bound to the expected runtime that is on par with the pairwise approach. We demonstrate, on synthetic and real data, that FastAMI is considerably faster in practice while producing more accurate results. FastAMI also works when both the exact and the pairwise implementation fail due to memory requirements or impractical runtimes. In the process, we identify and fix a common flaw in synthetic clustering comparison benchmarks.\nWhile the AMI is unbiased when comparing two random clusterings, it has been shown that it still exhibits a bias towards selecting larger clusters when comparing multiple random clusterings against a fixed ground truth (Romano et al. 2014). As a solution, it was suggested to account also for the variance in the Standardized Mutual Information (SMI), which, according to the authors, should be employed when N/RC < 5 and more than three clusterings are compared. However, adoption of the SMI is lagging behind the AMI, and a potential reason might be its steep computational cost O(RCN3). We extend our Monte Carlo-based approach to the SMI, making it a computationally feasible alternative for small to medium-sized datasets."
        },
        {
            "heading": "Background and Related Work",
            "text": "Let S be a set of N data points {s1, s2, . . . sN}, and two clusterings the surjections u : S \u2192 {1, 2, . . . , R} and v : S \u2192 {1, 2, . . . , C} that partition the dataset S into R and C clusters respectively. We denote with ai := |u\u22121(i)|, bj := |v\u22121(j)| the cluster sizes and nij := |u\u22121(i)\u2229 v\u22121(j)| the number of shared data points between two clusters.\nThe mutual information (Edwards 2008) of the two clusterings is given by\nI(u, v) := R\u2211 i=1 C\u2211 j=1 nij N log ( N \u00b7 nij ai \u00b7 bj ) , (1)\nwhere nij/N log (Nnij/(aibj)) is defined to be zero for nij = 0. For example, the mutual information of two clusterings that assign every point in the dataset to a single cluster is zero. In the other extreme, when every point is an individual cluster in both clusterings, the mutual information is maximal logN . These simple examples already illustrate a problem for the use as a cluster evaluation metric: The mutual information is expected to increase with the number of clusters (Figure 1), which induces a bias towards more granular clusterings.\nTherefore, the mutual information is adjusted with its expected value under random permutations of the cluster assignments, while the number and size of the clusters A = {a1, . . . aR}, B = {b1, . . . bC} is kept constant (random permutation model) (Nguyen, Epps, and Bailey 2009)\nE{I |A,B} = R\u2211 i=1 C\u2211 j=1 min(ai,bj)\u2211 nij=max(0, ai+bj\u2212N)\nnij N log ( Nnij aibj ) P{nij |ai, bj , N}. (2)\nHere P{nij |ai, bj , N} denotes the probability that nij of the ai data points in the cluster u\u22121(i) also lie in the cluster v\u22121(j) of size bj after random permutation and is given by the hypergeometric distribution.\nThe adjusted mutual information can subsequently be defined as\nAMI(u, v) := I(u, v)\u2212 E{I|A,B}\navg(H(u), H(v))\u2212 E{I|A,B} , (3)\nwhere avg(H(u), H(v)) is an upper bound to the mutual information given A,B that serves as a normalizer, with the entropy\nH(u) := R\u2211 i=1 ai N log ai N . (4)\nThe arithmetic and geometric mean, the minimum, and maximum can be used for avg (Nguyen, Epps, and Bailey 2010). In this work, we choose the arithmetic mean following the default choice of sklearn (Pedregosa et al. 2011).\nTo reduce the computational complexity of the expected mutual information, Lazarenko and Bonald (2021) restricted its computation to pairwise permutations only EMIpair(u, v) := E{I(u, \u03c3\u25e6v) |\u03c3 \u2208 SN : \u03c3 pairwise}. (5) A permutation \u03c3 \u2208 SN is pairwise when there exist i, j \u2208 {1, . . . , N} such that \u03c3(i) = j, \u03c3(j) = i, and \u03c3(t) = t \u2200t \u0338= i, j. While the original authors defined the pairwise AMI without normalization, we normalize it as in equation 3 to enable a direct comparison with the exact results.\nThe AMI has a constant baseline when comparing two clusterings directly. However, when comparing clusterings via a constant ground truth reference, the AMI is biased towards larger numbers of clusters (Romano et al. 2014). The definition of the standardized mutual information as\nSMI(u, v) := I(u, v)\u2212 E{I |A,B}\u221a\nVar{I |A,B} (6)\naccounts for that bias. The SMI indicates by how many standard deviations two clusterings deviate from each other under the hypothesis of random and independent clusterings."
        },
        {
            "heading": "Method",
            "text": "The AMI exhibits poor runtimes, mainly because the EMI is computationally expensive. The calculation of Var{I|A,B} for the SMI is even more demanding. Therefore we approximate the EMI and the variance of the mutual information via Monte Carlo sampling."
        },
        {
            "heading": "Monte Carlo Estimate of the EMI",
            "text": "We observe that the shared information of two clusters, i.e., the summands in Equation 2, is fully determined by the cluster sizes a, b, their overlap n and the total number of samples N . Hence, the expected mutual information can be reformulated in terms of the probability P{a|A} of a cluster size a in the marginals A\nE{I|A,B} =RC \u2211 a \u2211 b P{a|A}P{b|B}\n\u2211 n n N log ( Nn ab ) P{n|a, b,N}. (7)\nWe can now proceed to draw samples for a, b and n and approximate the sum through the sample mean. We can readily generate variates for a and b in constant time via Walker\u2019s method of alias (Walker 1974, 1977) after an initial setup cost of O(R logR + C logC) for determining the cluster size frequencies of A and B.\nDirectly drawing the overlap of two clusters n from the hypergeometric distribution would lead to a high occurrence of no overlap n = 0 in the case of small cluster sizes. However, these terms do not contribute to the mutual information. To exploit that sparsity, we absorb the linear term of the mutual information into the probability density function\nnP{n|a, b,N} = n ( b n )( N\u2212b a\u2212n )( N a\n) . (8) The binomial coefficients are rewritten as\nn\n( b\nn\n) =\nb! (n\u2212 1)! (b\u2212 n)! = b ( b\u2212 1 n\u2212 1 ) , (9)\nallowing us to draw variates n\u2212 1 according to the hypergeometric distribution with a\u2212 1, b\u2212 1 and N \u2212 1\nnP{n|a, b,N} =a \u00b7 b N\n( b\u22121 n\u22121 )( N\u2212b a\u2212n )( N\u22121 a\u22121\n) = a \u00b7 b N\nP{n\u2212 1|a\u2212 1, b\u2212 1, N \u2212 1}. (10)\nSeveral methods to draw hypergeometric variates in on average constant time exist (Kachitvichyanukul and Schmeiser 1985; Ho\u0308rmann 1994), here we chose a ratio of uniforms approach introduced by Stadlober (1990). The\nAlgorithm 1 The core of FastAMI is a Monte Carlo approximation to the expected mutual information. We use a numerically stable variant of Welford\u2019s online algorithm for calculating the mean and sample variance and define a stopping criterion when the desired precision is reached.\nRequire: Cluster sizes A,B, EMI precision p, minimum number of samples imin > 1 Ensure: EMI \u223c N ( E{I|A,B}, s2EMI ) where\nmin{sEMI/EMI, sEMI} \u2264 p N \u2190\u2211a\u2208A a i\u2190 0, EMI\u2190 0, M2 \u2190 0 Setup Walker random sampling for A,B while i < imin or M2 > (pmax{1,EMI})2 \u00b7 i \u00b7 (i\u22121) do i\u2190 i+ 1 Draw a, b via Walker\u2019s Alias method Draw m \u223c P{m|a \u2212 1, b \u2212 1, N \u2212 1} via Stadlober\u2019s method x\u2190 ab log [(m+ 1)N/(ab)] \u22060 \u2190 x\u2212 EMI EMI\u2190 EMI+\u22060/i \u22061 \u2190 x\u2212 EMI M2 \u2190M2 +\u22060\u22061 end while sEMI = \u221a M2/(i \u00b7 (i\u2212 1))\napproximation of Equation 7 terminates, when the relative error estimate of the EMI reaches a desired precision sEMI/EMI \u2264 p. To ensure termination when the EMI approaches zero, the algorithm switches to an absolute error criterion sEMI \u2264 p when EMI < 1.\nSummarizing all the steps above, we formulate Algorithm 1, a Monte Carlo estimate for the EMI. Samples can be drawn in constant time after an initial setup cost of O(R logR + C logC), such that in practice, the number of Monte Carlo samples governs the runtime."
        },
        {
            "heading": "Upper Bound to the Expected EMI Runtime",
            "text": "Given that samples can be drawn on average in constant time, we derive an asymptotic upper bound to the expected number of samples required for convergence of the EMI.\nTheorem 1. The expected number of samples required for Algorithm 1 with precision p to terminate on two clusterings with R and C clusters and N \u2265 3 data points is asymptotically bounded by O ( RC/p2 ) .\nProof. The absolute error of the Monte Carlo approximation sEMI for a given number of samples Nsamples is\ns2EMI = Var\n{ RC N2 ab log ( Nn ab ) |A,B } Nsamples , (11)\nin the limit of many samples, according to the central limit theorem. Conversely, the expected number of samples to reach a particular error is\nNsamples = Var\n{ RC N2 ab log ( Nn ab ) |A,B } s2EMI . (12)\nThe Bhatia-Davis inequality (Bhatia and Davis 2000) provides an upper bound to the variance when the variates are bounded m \u2264 RCN2 ab log ( N \u00b7n ab ) \u2264M\nVar\n{ RC\nN2 ab log\n( Nn\nab\n) |A,B } \u2264 (M \u2212 EMI) (EMI\u2212m) .\n(13) Equality holds when the probability mass function is concentrated on the extremes of the interval. The information contained in the overlap of two clusters is larger or equal than zero (m = 0), where equality is reached when one of the clusterings has only a single cluster a = N or b = N . For the upper bound, note that the overlap of two clusters is maximally the size of the smaller cluster n \u2264 min{a, b}. Without loss of generality, assume a \u2264 b\nab log nN ab \u2264 Nb log N b \u2264 N\n2\ne = M for N \u2265 e, (14)\nWith Equation 13 this gives the following upper bound to the expected number of required Monte Carlo samples\nNsamples \u2264 (RC \u2212 EMI)EMI\ns2EMI (15)\nThe convergence criterion in Algorithm 1 requires sEMI \u2264 p for EMI \u2264 1 and sEMI \u2264 pEMI for EMI > 1, such that\nNsamples \u2264 O ( RC\np2\n) . (16)\nThis amounts to the same asymptotic runtime as the pairwise adjusted EMI for a fixed precision p. However, the Bhatia-Davis inequality gives only a crude theoretical limit and the experiments will shed more light on the algorithm\u2019s runtime."
        },
        {
            "heading": "Monte Carlo Estimation of the Variance",
            "text": "We compare two approaches for approximating the variance of the mutual information: \u2022 Separate Monte Carlo - We split the variance into two\nterms Var{I |A,B} = E{I |A,B}2 \u2212 E{I2 |A,B}, where the first term is approximated as in Algorithm 1. We apply Equation 10 and related identities to the explicit formula for E{I2 |A,B} as given in theorem 1 in (Romano et al. 2014), and approximate it via an analogous algorithm.\n\u2022 Direct Monte Carlo - Instead of generating individual values for nij , we randomly sample the space of all contingency matrices (nij) j\u2208{1,...,C} i\u2208{1,...,R} at once with a method\ndeveloped by Patefield (1981) and observe the mutual information. The estimators for the EMI and the variance are then simply the sample mean and variance.\nThe direct Monte Carlo approach can also be applied to the EMI. However, the whole contingency matrix would be kept in memory. This is not practical for the approximation of the AMI, since memory footprint was one of the limiting factors we optimized (see Table 2)."
        },
        {
            "heading": "Experiments",
            "text": "In the previous section, we introduced a method to approximate the EMI and variance via Monte Carlo sampling. We now proceed to evaluate how quickly these methods converge. Different parameter regimes for R,C, and N are explored using synthetic data, and we demonstrate the practical use of the method on real datasets from a wide range of fields.\nA Laptop with an Intel i7-10750H with 32GB RAM was used for the synthetic experiments and the experiments on the Benchmark Suite for Clustering Algorithms. The experiments with the larger datasets from the Stanford Large Network Dataset Collection were performed on an Intel Xeon E5-2680 v4 system with 512GB RAM."
        },
        {
            "heading": "Synthetic Data",
            "text": "Previous works usually generated random clusterings by assuming a fixed number of clusters and then assigning each data point to a random cluster uniformly (Nguyen, Epps, and Bailey 2009; Romano et al. 2014). This method overemphasizes balanced clusterings, where all clusters have similar sizes. Choosing a random probability distribution instead of the uniform assignment allows more variance in the cluster sizes (Lazarenko and Bonald 2021). However, both methods do not guarantee the fixed number of clusters they were intended to generate. In the first case, exactly one cluster gets assigned zero data points with a probability of C ((C \u2212 1)/C)N and using the inclusion-exclusion principle, the probability of at least one empty cluster is\nP{\u2203i : i \u0338\u2208 Im v} = C\u2211 i=1 (\u22121)i\u22121 ( C i )( C \u2212 i C )N . (17)\nIm v denotes the image of clustering v, i.e. the set of all labels. For a fixed number of data points N , the chance of an empty cluster grows with the number of clusters, such that for example for N = 100 and C = 30 this probability is already \u2248 66.5%. This chance is even amplified when selecting the probabilities at random since cluster labels can have arbitrarily low probability.\nInstead, we propose a maximum entropy approach: Given a fixed number of data points N and fixed numbers of clusters R,C, we randomly sample the space of all possible cluster size distributions A and B that fulfill these constraints, i.e., we uniformly sample the integer partitions of R and C. We use a rejection sampling method that was outlined by Arratia and Desalvo (2016) and originally developed by Fristedt (1993). Sampling cluster size distributions is enough for the EMI, which depends only on A,B, whereas the SMI depends on two concrete clusterings. In this case, we create an ordered cluster from the marginals and shuffle it randomly, guaranteeing fixed R,C.\nEMI The computationally demanding part of computing the adjusted mutual information is obtaining the expected value of the mutual information (EMI). Therefore, we compare the methods to compute the EMI for R = C and a fixed N = 5000. We focus on R = C since the asymptotic\nruntime for the pairwise and our Monte Carlo algorithm increases the most for that configuration. Both the exact calculation and the pairwise EMI exhibit a nearly 100-fold increase in runtime, as the number of clusters increases from 500 to 4500 (Figure 2a). This slowdown is problematic since it is the regime where the AMI is preferable to the Rand Index, as stated by Romano et al. (2016). On the other hand, our Monte Carlo method performs best in the case of many clusters. The reason is that when smaller clusters dominate as R = C increases, the variance of the number of shared data points n between clusters decreases, and fewer Monte Carlo samples are required for convergence. In the case of only a few larger clusters, there are fewer cluster size values the variates a and b can assume, and hence the runtime peaks somewhere between those extremes. This empirical result confirms that the upper bound in theorem 1 overestimates the runtime.\nIn the regime of many singleton clusters, i.e., high R,C, many permutations do not affect the contingency matrix (nij) and hence the mutual information. These permutations contribute to the slowdown of the exact and the pairwise approach, but it also means that leaving them out only slightly affects the pairwise EMI and the relative error is low in that regime (Figure 2b). On the other hand, the pairwise approach has high model-dependent errors, where the method is fast. Our approach allows for tunable relative errors across the parameter range (dashed red line in Figure 2b).\nIn a second experiment, we show that the runtime of the exact and pairwise method gets even worse as the number of data points N increases (Figure 2c).\nSMI We compare the exact SMI implemented by Romano et al. (2014) with the separate and direct Monte Carlo approach for estimating the variance. We terminate the calculation when the estimated absolute or relative error estimate\nof the SMI is below the precision p = 0.1. Both approximate algorithms outperform the exact SMI, but the separate approach slows down as the number of clusters increases. In contrast, the direct approach provides a speed-up of multiple orders of magnitude across the whole parameter range (Figure 3). A possible explanation could be that the direct approach naturally incorporates the constraint that the variance is larger or equal than zero. The sample variance of the mutual information is always positive, but the difference of two separate estimators for E{I |A,B}2 and E{I2 |A,B} can also be negative. In the following we chose the direct approach for FastSMI."
        },
        {
            "heading": "Real Data",
            "text": "The previous section demonstrated how Monte Carlo approximation speeds up the EMI and SMI for random clusterings. This section shows how that speed-up translates to the practically more relevant AMI, on clusterings that arise from real datasets of various domains.\nLazarenko and Bonald (2021) used 79 datasets from the Benchmark Suite for Clustering Algorithms (Gagolewski et al. 2020) to compare the solutions of several clustering algorithms via the AMI and pairwise AMI. They used the Spearman rank correlation to measure the quality of the results. As a first benchmark, we reproduce these results in Table 1 and extend them to evaluate FastAMI. While our method has a significantly higher Spearman correlation with the exact solution than the pairwise AMI, it was slower compared to the pairwise approach. For the SMI on the other hand, 61.7% of the comparisons timed out after 20 s. FastSMI (p = 0.1) completed 99.2% of the benchmark with a high Spearman correlation with the exact results, enabling SMI comparisons for medium sized datasets. However, with an average runtime of 0.013 s per comparison, the benchmark suite is not in the domain where an approximation of the AMI is necessary due to runtime constraints.\nInstead, we propose a benchmark based on a collection of large real-world datasets, taken from the Stanford Large Network Dataset Collection (Leskovec and Krevl 2014). We select the datasets designated to community detection from that collection (See Table 2) and apply the following six methods to find clusterings (Staudt, Sazonovs, and Meyerhenke 2016): 1) Connected Components, 2) Degree Ordered Label Propagation, 3) Label Propagation, 4) Leiden, 5) Louvain, and 6) Louvain Map Equation.\nThe benchmark then consists of a pairwise comparison of the six clusterings, using AMI, pairwise AMI, and FastAMI.\nWe measure the total runtime and peak memory consumption for the 15 comparisons in each dataset and limit every individual comparison to a maximum of 2000 s and 503.6GiB of memory. If any of these limits is exceeded, we report the respective lower bound in Table 2 and omit the result from further calculations. Following Lazarenko and Bonald (2021), we report the Spearman rank correlation between the exact results and the respective approximations, excluding the cases where the exact calculation was aborted. Additionally, we report the mean absolute error where exact values are available and substitute it with the according Monte Carlo estimate otherwise.\nThe AMI is preferable to the ARI when the clusterings are imbalanced (Romano et al. 2016). In the synthetic experiments, we only fixed the number of clusters without explicitly specifying the imbalance. For the real datasets, we measure the balance using the mean normalized entropy\nBalance(U) = \u2211 uk\u2208U\nH(uk)\n|U | log | Imuk| , (18)\nwhere U denotes the set of clustering solutions, obtained via the six different methods for the same dataset. A value of one indicates that all the clusterings are perfectly balanced, i.e., every cluster i within a clustering u has the same size. A value of zero on the other hand means that all clusterings consist of a single large cluster and are thus maximally imbalanced.\nAs expected from the synthetic experiments, our method outperforms the others in terms of runtime. FastAMI also has a much lower memory footprint than the sklearn implementation and the pairwise AMI, since it does not keep the (sparse) contingency table in memory. While FastAMI does slow down for datasets with more nodes N , it does not struggle with imbalanced datasets in contrast to sklearn and the pairwise AMI. FastAMI performs comparably to the pairwise adjustment regarding the Spearman correlation. The real benefit in terms of quality is that the FastAMI results can directly be compared with the traditional AMI as demonstrated by the mean absolute error, whereas the pairwise AMI is systematically lower and somewhat harder to interpret."
        },
        {
            "heading": "Conclusion",
            "text": "This paper presents an effective and practical method for approximating the Adjusted Mutual Information. FastAMI takes advantage of the sparsity and low variance of contingency tables of clusterings, enabling AMI comparisons on datasets that were computationally inaccessible before. We analyzed the behavior of our approximation scheme based on synthetic data and demonstrated state-of-the-art performance on a suite of real datasets. In future work, one could readily parallelize Algorithm 1 to improve its performance further. We extended our algorithm to the standardized mutual information, rendering variance-adjusted clustering comparisons computationally feasible for moderately sized datasets."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was funded by the Bayerischen Verbundfo\u0308rderprogramm (BayVFP) \u2013 Fo\u0308rderlinie Digitalisierung \u2013 Fo\u0308rderbereich Informations- und Kommunikationstechnik of the Bavarian Ministry of Economic Affairs, Regional Development and Energy and supported by Bayern Innovativ \u2013 Bayerische Gesellschaft fu\u0308r Innovation und Wissenstransfer mbH."
        }
    ],
    "title": "FastAMI \u2014 a Monte Carlo Approach to the Adjustment for Chance in Clustering Comparison Metrics",
    "year": 2023
}