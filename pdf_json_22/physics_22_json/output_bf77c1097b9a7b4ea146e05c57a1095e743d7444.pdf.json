{
    "abstractText": "One of the key problems in tensor network based quantum circuit simulation is the construction of a contraction tree which minimizes the cost of the simulation, where the cost can be expressed in the number of operations as a proxy for the simulation running time. This same problem arises in a variety of application areas, such as combinatorial scientific computing, marginalization in probabilistic graphical models, and solving constraint satisfaction problems. In this paper, we reduce the computationally hard portion of this problem to one of graph linear ordering, and demonstrate how existing approaches in this area can be utilized to achieve results up to several orders of magnitude better than existing state of the art methods for the same running time. To do so, we introduce a novel polynomial time algorithm for constructing an optimal contraction tree from a given order. Furthermore, we introduce a fast and high quality linear ordering solver, and demonstrate its applicability as a heuristic for providing orderings for contraction trees. Finally, we compare our solver with competing methods for constructing contraction trees in quantum circuit simulation on a collection of randomly generated Quantum Approximate Optimization Algorithm Max Cut circuits and show that our method achieves superior results on a majority of tested quantum circuits. Reproducibility: Our source code and data are available at https://github.com/cameton/HPEC2022 ContractionTrees.",
    "authors": [
        {
            "affiliations": [],
            "name": "Cameron Ibrahim"
        },
        {
            "affiliations": [],
            "name": "Danylo Lykov"
        },
        {
            "affiliations": [],
            "name": "Zichang He"
        },
        {
            "affiliations": [],
            "name": "Yuri Alexeev"
        },
        {
            "affiliations": [],
            "name": "Ilya Safro"
        }
    ],
    "id": "SP:ceb717d747f89449e070e15d50cecf168816a785",
    "references": [
        {
            "authors": [
                "Amine Abou-Rjeili",
                "George Karypis"
            ],
            "title": "Multilevel algorithms for partitioning power-law graphs",
            "venue": "In Parallel and Distributed Processing Symposium,",
            "year": 2006
        },
        {
            "authors": [
                "Yuri Alexeev",
                "Dave Bacon",
                "Kenneth R Brown",
                "Robert Calderbank",
                "Lincoln D Carr",
                "Frederic T Chong",
                "Brian DeMarco",
                "Dirk Englund",
                "Edward Farhi",
                "Bill Fefferman"
            ],
            "title": "Quantum computer systems for scientific discovery",
            "venue": "PRX Quantum,",
            "year": 2021
        },
        {
            "authors": [
                "Lu Bai",
                "Edwin R Hancock",
                "Peng Ren"
            ],
            "title": "A jensen-shannon kernel for hypergraphs. In Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)",
            "year": 2012
        },
        {
            "authors": [
                "Jacob Biamonte",
                "Ville Bergholm"
            ],
            "title": "Tensor networks in a nutshell",
            "venue": "arXiv preprint arXiv:1708.00006,",
            "year": 2017
        },
        {
            "authors": [
                "A. Brandt",
                "D. Ron"
            ],
            "title": "Chapter 1 : Multigrid solvers and multilevel optimization strategies",
            "venue": "Multilevel Optimization and VLSICAD. Kluwer,",
            "year": 2003
        },
        {
            "authors": [
                "C\u00e9dric Chevalier",
                "Ilya Safro"
            ],
            "title": "Comparison of coarsening schemes for multilevel graph partitioning",
            "venue": "Learning and Intelligent Optimization,",
            "year": 2009
        },
        {
            "authors": [
                "Andrzej Cichocki",
                "Namgil Lee",
                "Ivan Oseledets",
                "Anh-Huy Phan",
                "Qibin Zhao",
                "Danilo P. Mandic"
            ],
            "title": "Tensor networks for dimensionality reduction and large-scale optimization: Part 1 low-rank tensor decompositions",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "T.H. Cormen",
                "C.E. Leiserson",
                "R.L. Rivest",
                "C. Stein"
            ],
            "title": "Introduction To Algorithms",
            "year": 2009
        },
        {
            "authors": [
                "Holger Dell",
                "Christian Komusiewicz",
                "Nimrod Talmon",
                "Mathias Weller"
            ],
            "title": "The PACE 2017 Parameterized Algorithms and Computational Experiments Challenge: The Second Iteration",
            "venue": "12th International Symposium on Parameterized and Exact Computation (IPEC 2017),",
            "year": 2018
        },
        {
            "authors": [
                "Jeffrey M Dudek",
                "Leonardo Duenas-Osorio",
                "Moshe Y Vardi"
            ],
            "title": "Efficient contraction of large tensor networks for weighted model counting through graph decompositions",
            "year": 1908
        },
        {
            "authors": [
                "Stavros Efthymiou",
                "Jack Hidary",
                "Stefan Leichenauer"
            ],
            "title": "Tensornetwork for machine learning",
            "venue": "arXiv preprint arXiv:1906.06329,",
            "year": 2019
        },
        {
            "authors": [
                "Edward Farhi",
                "Jeffrey Goldstone",
                "Sam Gutmann"
            ],
            "title": "A quantum approximate optimization algorithm",
            "venue": "arXiv preprint arXiv:1411.4028,",
            "year": 2014
        },
        {
            "authors": [
                "Agata Fronczak",
                "Janusz A Ho\u0142yst",
                "Maciej Jedynak",
                "Julian Sienkiewicz"
            ],
            "title": "Higher order clustering coefficients in barab\u00e1si\u2013albert networks",
            "venue": "Physica A: Statistical Mechanics and its Applications,",
            "year": 2002
        },
        {
            "authors": [
                "Johnnie Gray",
                "Stefanos Kourtis"
            ],
            "title": "Hyper-optimized tensor network contraction",
            "venue": "Quantum, 5:410,",
            "year": 2021
        },
        {
            "authors": [
                "Michael Hamann",
                "Ben Strasser"
            ],
            "title": "Graph bisection with pareto optimization",
            "venue": "ACM J. Exp. Algorithmics,",
            "year": 2018
        },
        {
            "authors": [
                "Y.F. Hu",
                "J.A. Scott"
            ],
            "title": "A multilevel algorithm for wavefront reduction",
            "venue": "SIAM J. Sci. Comput.,",
            "year": 2001
        },
        {
            "authors": [
                "Isa Inuwa-Dutse",
                "Mark Liptrott",
                "Ioannis Korkontzelos"
            ],
            "title": "A multilevel clustering technique for community detection",
            "year": 2021
        },
        {
            "authors": [
                "Stefanos Kourtis",
                "Claudio Chamon",
                "Eduardo Mucciolo",
                "Andrei Ruckenstein"
            ],
            "title": "Fast counting with tensor networks",
            "venue": "SciPost Physics,",
            "year": 2019
        },
        {
            "authors": [
                "H. Lee",
                "J. Kim",
                "S.J. Hong",
                "S. Lee"
            ],
            "title": "Processor allocation and task scheduling of matrix chain products on parallel systems",
            "venue": "IEEE Transactions on Parallel and Distributed Systems,",
            "year": 2003
        },
        {
            "authors": [
                "Xiaoyuan Liu",
                "Anthony Angone",
                "Ruslan Shaydulin",
                "Ilya Safro",
                "Yuri Alexeev",
                "Lukasz"
            ],
            "title": "Cincio. Layer VQE: A variational approach for combinatorial optimization on noisy quantum computers",
            "venue": "IEEE Transactions on Quantum Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Danylo Lykov",
                "Yuri Alexeev"
            ],
            "title": "Importance of diagonal gates in tensor network simulations",
            "venue": "IEEE Computer Society Annual Symposium on VLSI (ISVLSI),",
            "year": 2021
        },
        {
            "authors": [
                "Danylo Lykov",
                "Angela Chen",
                "Huaxuan Chen",
                "Kristopher Keipert",
                "Zheng Zhang",
                "Tom Gibbs",
                "Yuri Alexeev"
            ],
            "title": "Performance evaluation and acceleration of the qtensor quantum circuit simulator on gpus",
            "venue": "IEEE/ACM Second International Workshop on Quantum Computing Software (QCS),",
            "year": 2021
        },
        {
            "authors": [
                "Danylo Lykov",
                "Roman Schutski",
                "Alexey Galda",
                "Valerii Vinokur",
                "Yurii Alexeev"
            ],
            "title": "Tensor network quantum simulator with step-dependent parallelization",
            "venue": "arXiv preprint arXiv:2012.02430,",
            "year": 2020
        },
        {
            "authors": [
                "Igor L Markov",
                "Yaoyun Shi"
            ],
            "title": "Simulating quantum computation by contracting tensor networks",
            "venue": "SIAM Journal on Computing,",
            "year": 2008
        },
        {
            "authors": [
                "Simone Montangero",
                "Montangero",
                "Evenson"
            ],
            "title": "Introduction to Tensor Network",
            "year": 2018
        },
        {
            "authors": [
                "Bryan O\u2019Gorman"
            ],
            "title": "Parameterization of tensor network contraction",
            "venue": "arXiv preprint arXiv:1906.00013,",
            "year": 2019
        },
        {
            "authors": [
                "Elina Robeva",
                "Anna Seigal"
            ],
            "title": "Duality of graphical models and tensor networks",
            "venue": "Information and Inference: A Journal of the IMA,",
            "year": 2019
        },
        {
            "authors": [
                "Dorit Ron",
                "Ilya Safro",
                "Achi Brandt"
            ],
            "title": "Relaxation-based coarsening and multiscale graph organization",
            "venue": "Multiscale Modeling & Simulation,",
            "year": 2011
        },
        {
            "authors": [
                "Ilya Safro",
                "Dorit Ron",
                "Achi Brandt"
            ],
            "title": "Graph minimum linear arrangement by multilevel weighted edge contractions",
            "venue": "Journal of Algorithms,",
            "year": 2006
        },
        {
            "authors": [
                "Ilya Safro",
                "Dorit Ron",
                "Achi Brandt"
            ],
            "title": "Multilevel algorithms for linear ordering problems",
            "venue": "ACM J. Exp. Algorithmics,",
            "year": 2009
        },
        {
            "authors": [
                "Ilya Safro",
                "Peter Sanders",
                "Christian Schulz"
            ],
            "title": "Advanced coarsening schemes for graph partitioning",
            "venue": "ACM Journal of Experimental Algorithmics (JEA),",
            "year": 2015
        },
        {
            "authors": [
                "Ilya Safro",
                "Boris Temkin"
            ],
            "title": "Multiscale approach for the network compression-friendly ordering",
            "venue": "J. Discrete Algorithms,",
            "year": 2011
        },
        {
            "authors": [
                "Sebastian Schlag",
                "Tobias Heuer",
                "Lars Gottesb\u00fcren",
                "Yaroslav Akhremtsev",
                "Christian Schulz",
                "Peter Sanders"
            ],
            "title": "High-quality hypergraph partitioning",
            "venue": "arXiv preprint arXiv:2106.08696,",
            "year": 2021
        },
        {
            "authors": [
                "Ruslan Shaydulin",
                "Jie Chen",
                "Ilya Safro"
            ],
            "title": "Relaxation-based coarsening for multilevel hypergraph partitioning",
            "venue": "SIAM Multiscale Modeling & Simulation,",
            "year": 2019
        },
        {
            "authors": [
                "Ruslan Shaydulin",
                "Ilya Safro"
            ],
            "title": "Aggregative Coarsening for Multilevel Hypergraph Partitioning",
            "venue": "In Gianlorenzo D\u2019Angelo, editor, 17th International Symposium on Experimental Algorithms (SEA 2018),",
            "year": 2018
        },
        {
            "authors": [
                "Ben Strasser"
            ],
            "title": "Computing tree decompositions with flowcutter: Pace 2017 submission",
            "venue": "arXiv preprint arXiv:1709.08949,",
            "year": 2017
        },
        {
            "authors": [
                "Hisao Tamaki"
            ],
            "title": "Positive-instance driven dynamic programming for treewidth",
            "venue": "Journal of Combinatorial Optimization,",
            "year": 2019
        },
        {
            "authors": [
                "Hayato Ushijima-Mwesigwa",
                "Ruslan Shaydulin",
                "Christian F.A. Negre",
                "Susan M. Mniszewski",
                "Yuri Alexeev",
                "Ilya Safro"
            ],
            "title": "Multilevel combinatorial optimization across quantum architectures",
            "venue": "ACM Transactions on Quantum Computing,",
            "year": 2021
        },
        {
            "authors": [
                "C. Walshaw"
            ],
            "title": "Multilevel refinement for combinatorial optimisation problems",
            "venue": "Annals Oper. Res.,",
            "year": 2004
        },
        {
            "authors": [
                "John Watrous"
            ],
            "title": "Guest column: An introduction to quantum information and quantum circuits 1",
            "venue": "SIGACT News, 42(2):52\u201367,",
            "year": 2011
        },
        {
            "authors": [
                "Noson S Yanofsky"
            ],
            "title": "An introduction to quantum computing. In Proof, computation and agency, pages 145\u2013180",
            "year": 2011
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Contraction Tree, Tensor Network, Quantum Circuit Simulation, QAOA\nI. INTRODUCTION\nTensor networks have become a ubiquitous tool for describing high dimensional, data-sparse tensors in a space efficient manner [8], [15], [30]. In quantum circuit simulation, tensor networks can be used to model circuits with far more qubits than a more direct approach such as state vector simulation, so long as the circuit is sufficiently simple [25]. As such, tensor network based simulation methods will remain vital for the study of quantum algorithms that require a large number of qubits until sufficient advances are made with quantum hardware [25]. Moreover, even with the advanced hardware, such simulation methods will be important for such tasks as finding suitable parameters in variational quantum algorithms that utilize both quantum and classical machines [22]. These networks arise naturally in a variety of disciplines such as Combinatorial Scientific Computing [?], Probabilistic Modelling [30], Constraint Satisfaction [20], Machine Learning and Data Mining [12], Physics Modelling [28], and Quantum & Classical Circuit Simulation [15], [27].\nA tensor network is most easily conceptualized as a collection of high-order matrices called tensors which are connected by edges. These networks are often used to represent tensors which are impractical to store explicitly in memory [8]. The fundamental operation performed on these networks is called a tensor contraction, where two tensors are combined to create a third tensor. A series of tensor contractions performed on the network is called a tensor network contraction, which is used to reduce the overall size of the network in order to evaluate entries of the underlying tensor that the network represents.\nIt is useful to represent a tensor network contraction as a contraction tree, a binary tree whose leaves are tensors in the network, where each internal vertex represents the tensor contraction of its two children. In the general case, the cost of any single tensor contraction is exponential in the number of dimensions of its inputs, as is the size of its output [29]. As such, a vital part of performing a tensor network contraction is choosing a contraction tree which minimizes some cost, such as minimizing the size of any intermediary tensor or the total number of operations performed during your tensor network contraction [15]. However, it is NP-hard to construct an optimal contraction tree in the general case for these objectives [29], meaning no optimal solution can been found deterministically in polynomial time.\nOur contribution In this paper, we will show how the computationally hard portion of constructing a contraction tree can be reduced to a problem of linear ordering, and give algorithms for constructing an optimal contraction tree for a given ordering. Furthermore, we will give a useful heuristic for choosing orderings which will result in high quality contraction trees. This algorithm is being developed with integration with the QTensor quantum circuit simulation library in mind [23]\u2013[26]. In general, in order to run simulations of quantum circuits with large numbers of qubits, we must be able to find a good quality contraction order. As such, finding optimal or near optimal contraction orders is critical for the development and testing of new quantum algorithms, benchmarking and profiling upcoming quantum devices, and verification of quantum advantage and supremacy claims to name a few applications [3] Even small improvements to the quality of the contraction tree will lead to a significant acceleration of quantum simulation. We demonstrate an improvement over competitive solvers by orders of magnitude on some instances ar X iv :2\n20 9.\n02 89\n5v 1\n[ qu\nan t-\nph ]\n7 S\nep 2\n02 2\nfor a comparable running time. The paper is structured the following way. Section II provides an introduction to tensor networks and linear orderings. Section III introduces existing solvers used for constructing contraction trees for quantum circuit simulation, which we use for benchmarking the proposed algorithm. In Section IV, we present a novel polynomial time algorithm for constructing an optimal contraction tree from a given order and explain the ordering heuristic that was used. In Section V, we evaluate the developed tree structure optimization algorithm against Tamaki-2017, FlowCutter, and Cotengra on a collection of QAOA circuits on different comparison metrics. Finally, Section VI discusses possible areas for future work, while Section VII offers a conclusion."
        },
        {
            "heading": "II. BACKGROUND AND NOTATION",
            "text": ""
        },
        {
            "heading": "A. Matrix Chains and Tensor Networks",
            "text": "An m by n matrix M is a 2 dimensional array of scalar numbers from some field F (e.g. R, C). We say this matrix has two indices, i and j, where size(i) = m and size(j) = n. The size of M is size(M) = size(i) size(j).\nA matrix M1 with indices {i, j} and a matrix M2 with indices {j, k} can be combined to form a matrix M1\u2217M2 with indices {i, k} using an operation known as matrix multiplication. Note that the output has the indices of both its inputs with the shared index removed. The number of operations required to perform a matrix multiplication is size(i) size(j) size(k).\nSay we are given matrices M1,M2, . . . ,Mn\u22121,Mn with indices {i1, i2}, {i2, i3}, . . . , {in\u22121, in}, {in, in+1}. The Matrix Chain Ordering Problem aims to find the optimal parenthesization of the matrix chain expression\nM1 \u2217M2 \u2217 \u00b7 \u00b7 \u00b7 \u2217Mn\u22121 \u2217Mn\nwhich minimizes the cost of computing this product [9]. Note that this setup assumes each matrix shares indices only with its neighbors. A given parenthesization can be expressed as a binary tree with leaves labeled with matrices. For example, the binary tree in Fig. 1 corresponds to the parenthesized matrix chain\n((A \u2217B) \u2217 C) \u2217 (D \u2217 E).\nWe define tensors analogously. A tensor X is an Ndimensional array of scalar numbers from some field F with indices I = {i1, . . . , iN} (for other perspectives, see [5]).\nFor a tensor X and an index i, we say that X and i are incident to one another if i is an index of X . We define inc(X) as the set of indices incident to X and inc(i) as the set of tensors incident to i. The size of a tensor X is given by\nsize(X) = \u220f\ni\u2208inc(X)\nsize(i)\nTwo tensors X1 with indices I1 and X2 with indices I2 can similarly be combined using an operation known as tensor contraction. The output X1 \u2217 X2 will have indices I14I2, where 4 denotes symmetric difference. That is, the output\nhas indices equal to the union of the input indices, with the intersection removed. I14I2 are known as output indices, while I1 \u2229 I2 are known as the shared indices. The number of scalar operations required to perform a tensor contraction is given by\nops(X,Y ) = size(X \u2217 Y ) share(X,Y )\nwhere share(X,Y ) = \u220f i\u2208S size(i), S = inc(X) \u2229 inc(Y ).\nA set of tensors T = {X1, . . . , Xn} with indices I = \u22c3 X\u2208T inc(X)\nis known as a tensor network. Defining T (0) = T , a tensor network contraction iteratively picks X(k), Y (k) \u2208 T (k) to produce\nT (k+1) = ( T (k) \\ { X(k), Y (k) }) \u222a { X(k) \u2217 Y (k) } ,\nterminating when |T (k)| = 1. A contraction order used for this tensor network contraction can again be represented as a binary tree [29]. For example, the tree shown in Fig. 1 corresponds to the network contraction\nT (0) = {A,B,C,D,E} T (1) = {A \u2217B,C,D,E} T (2) = {(A \u2217B) \u2217 C,D,E} T (3) = {(A \u2217B) \u2217 C,D \u2217 E} T (4) = {((A \u2217B) \u2217 C) \u2217 (D \u2217 E)}.\nContraction of a tensor network is #P-Hard in the general case [29]. However, many tensor networks arising in practice may be contracted efficiently given a good quality contraction order with respect to some cost function. The costs we consider in this paper are the sum of the number of scalar operations, maximum number of scalar operations, and maximum tensor size, which are defined as follows:\u2211\nk\nops ( X(k), Y (k) ) , (1)\nmax k\nops ( X(k), Y (k) ) , max\nk size\n( X(k) \u2217 Y (k) ) . (2)\nThe problem of finding an optimal contraction order is itself NP-Hard [29].\nWhile minimizing the number of scalar operations required would seem to be more useful than minimizing the maximum number of operations for any contraction, the latter is actually useful in a parallel setting where many contractions can be performed simultaneously [21]."
        },
        {
            "heading": "B. Quantum Circuits and QAOA",
            "text": "An overview to quantum circuits and quantum computing can be found here [43]. A particular type of quantum circuit which has garnered great deal of research interest in recent years is the Quantum Approximate Optimization Algorithm ansatz, particularly those developed to target the graph theoretic MaxCut problem [13]. QAOA is a variational quantumclassical algorithm inspired by the adiabatic evolution principle. It is essentially a quantum annealer with a finite number of steps p. The quality of the final solution increases with p, as does the complexity of the circuit.\nIn quantum circuit simulation, it is possible to represent a quantum circuit as a high dimensional tensor over the complex numbers [44] or, equivalently, as a tensor network [27]. In this perspective, each quantum gate is associated with a tensor X with indices corresponding to the inputs and outputs of the gate. Two tensors in such a network will share an index only if it corresponds to a value that has been passed from one corresponding gate to the other. For the purposes of this paper, we will assume the size of every index in the network is 2.\nWith this construction, the costs described in Eq 2 are related to older literature which achieved similar estimations on the computational cost of tensor network contractions via quantities called vertex congestion, edge congestion, and treewidth [27], [29].\nIn particular, vertex congestion and treewidth are equal to the log of the maximum number of operations needed for any contraction\nmax k\nlog2 ops ( X(k), Y (k) ) , (3)\nwhile edge congestion is equal to the log of the maximum size of any intermediate tensor\nmax k\nlog2 size ( X(k) \u2217 Y (k) ) . (4)"
        },
        {
            "heading": "C. Tensor Networks as Graphs",
            "text": "An undirected graph G = (V,E) with no loops and multiedges, is a set V of vertices and a set E \u2286 ( V 2 ) of edges. The weights on the edges of the graph are denoted by the weighting function w : E \u2192 R\u22650. For an edge uv \u2208 E, w(uv) denotes its weight. The set of incident edges of a vertex v is the set of edges containing v, denoted inc(v) = {e \u2208 E | v \u2208 e}. The degree of v is the number of edges incident to v, deg(v) = |inc(v)|.\nSay we have a tensor network T with indices I where |inc(i)| = 2 for all i \u2208 I. Then T admits a representation as a weighted, undirected graph G = (V,E). Let F be the function which assigns tensors to vertices and indices to edges, such that F (i) = {F (X), F (Y )} if and only if i \u2208 inc(X)\u2229inc(Y ). The weight function w is defined as w(F (i)) = log2 size(i).\nA linear vertex ordering for a graph G = (V,E) is a bijective mapping \u03c3 : V \u2192 {1, . . . , |V |}. A linear ordering problem on a graph generally aims to find a linear ordering which minimizes a given cost function, such as the p-Sum objective, which is defined as(\u2211\nuv\u2208E w(uv)|\u03c3(u)\u2212 \u03c3(v)|p )1/p [33]. For the special case where p = 1, this problem is known\nas the Minimum Linear Arrangement Problem [32]."
        },
        {
            "heading": "D. Multilevel algorithms and refinement",
            "text": "The multilevel approach is a big class of algorithms actively used in many different areas of scientific computing, optimization, and machine learning [6]. In the context of this work, we briefly describe a version of multilevel algorithms for graph optimization problems [7], [31]. When the graph is large and a fast solution of an optimization problem is required for a specific application, it is often useful to compress the problem by (possibly nonlinearly) aggregating variables into, so called, coarse variables. This is done in a such way that a solution for each coarse variable can be effectively interpolated back to those variables that participated in the aggregation. The entire process of problem coarsening is performed gradually forming a multilevel hierarchy of coarse problems. Each next coarser problem approximates the previous finer problem from which it has been created. Thus, the number of variables at each level of this hierarchy is decreasing which allows to solve them faster that the original problem. When sufficiently small coarsest level is created, the best possible (often exact) solution is computed. The last stage of framework (called uncoarsening) is to gradually solve the problems at each level of coarseness by (1) interpolating the initial solution for the current fine level from the coarser level, and (2) refining the interpolated solution.\nIf both coarsening and uncoarsening are computed locally (i.e., their complexity is linear in the number of variables) then the entire multilevel framework becomes of linear or nearly-linear complexity assuming that the number of variables at each level is decreasing within a factor of 1.5-2.5. It is important to mention that this approach is different than such one-shot compression approaches as truncated SVD. Such problems on graphs as partitioning [2], [34], various linear orderings [18], [35], and community detection [19], [41] have benefited from the multilevel approaches to mention just few of them [42]."
        },
        {
            "heading": "III. RELATED WORK",
            "text": ""
        },
        {
            "heading": "A. FlowCutter",
            "text": "FlowCutter is used collectively to refer to an algorithm for computing small, balanced s-t cuts using Pareto optimization [17], as well as an algorithm utilizing the former to construct a tree decomposition of the input graph via recursive bisection [39], that is also a form of a multilevel algorithm.\nFlowCutter was entered into heuristic track of the PACE 2017 Parameterized Algorithms and Computational Experiments Challenge on minimizing treewidth, where it placed second as the only solver to find a solution for all test instances in the allotted 30 minutes [10], [39]. Moreover, FlowCutter is known to be useful in finding high quality tensor network contraction [11]."
        },
        {
            "heading": "B. Tamaki-2017",
            "text": "The Tamaki-2017 solver is a tree decomposition solver with components submitted to the exact and heuristic tracks for PACE 2017 [10]. On the heuristic track, this solver ranked first place, tending to achieve better results on smaller instances than the competitive approaches, while sometimes failing to find an answer in the allotted time for larger instances [39].\nThe Tamaki-2017 algorithm is based on positive-instance driven dynamic programming, and, similar to FlowCutter, operates recursively on minimal separators of the instance graph [40]."
        },
        {
            "heading": "C. Cotengra",
            "text": "Cotengra is a library for the efficient contraction of tensor networks [16]. In their seminal paper on the subject, the authors introduce a method for directly constructing contraction trees based on recursive bisection [15]. By utilizing existing hypergraph partitioning solvers such as KaHyPar [36], Cotengra is able to construct contraction trees utilizing the hypergraph structure of some inputs that arise in Quantum Circuit Simulation [15]. Cotengra also utilizes the Bayesian optimization strategy, varying the hyperparameters of the partitioning solver at various levels in order to account for a changing graph structure as the algorithm progresses [15]."
        },
        {
            "heading": "D. cuTENSOR",
            "text": "cuTENSOR is a tensor network CUDA library in development by NVidia, which aims to bring high performance tensor primatives to the GPU [1]. Because not much information about their specific method for determining contraction orders is publicly available at the time of writing, we will not be comparing to cuTENSOR at this time."
        },
        {
            "heading": "IV. CONSTRUCTING A CONTRACTION TREE",
            "text": "Our approach is founded on the observation that a parenthesization of a matrix chain expression or a contraction order for a tensor network both admit representations as binary trees with labelled leaves.\nIt is useful to introduce a more direct tensor variant of the Matrix Chain Ordering Problem, known as the Tensor Chain Ordering Problem. Given tensors X1, . . . , Xn, find an optimal parenthesization of the tensor chain expression\nX1 \u2217 \u00b7 \u00b7 \u00b7 \u2217Xn which minimizes one of the costs introduced in Eqs 1,2. Notably, indices are not restricted to immediate neighbors in the chain.\nGiven a binary tree T representing a contraction order O for a tensor network T , consider the linear ordering \u03c3 of tensors\ncorresponding to a left to right traversal of the labelled leaves of T . Then T could be considered a possible solution to the Tensor Chain Ordering Problem for the chain\n\u03c3\u22121(1) \u2217 \u00b7 \u00b7 \u00b7\u03c3\u22121(n).\nThen finding a contraction order for a tensor network can actually be reframed as finding a linear ordering of the tensors in the network, then solving the Tensor Chain Order Problem.\nmin O cost(T , O) = min \u03c3 min T cost(T , \u03c3, T ).\nWe call solving the Tensor Chain Order Problem for a fixed linear order \u03c3 tree structure optimization. Notably, an optimal contraction order of the Tensor Chain Order Problem can be found deterministically in polynomial time as demonstrated in the following section. This decouples the actual computationally Hard task of finding an optimal order of the chain, from the more tractable task of actually constructing the tree."
        },
        {
            "heading": "A. Tree Structure Optimization",
            "text": "In this section, we introduce a simple dynamic programming solution to the Tensor Chain Order Problem. Say we are given the tensor chain expression X1 \u2217 \u00b7 \u00b7 \u00b7 \u2217 Xn. Let Xi,j = Xi \u2217 Xi+1 \u2217 \u00b7 \u00b7 \u00b7 \u2217Xj for i < j.\nFrom this, we find the following recursion for computing the minimal edge congestion of any parenthesization of the tensor chain\nc(i, i) = size(Xi)\nc(i, j) = min i\u2264k<j max  log2 size(Xi,k \u2217Xk+1,j) c(i, k)\nc(k + 1, j)\n(5)\nas well as the minimal vertex congestion\nc(i, i) = size(Xi)\nc(i, j) = min i\u2264k<j max  log ops(Xi,k, Xk+1,j) c(i, k)\nc(k + 1, j)\n(6)\nIt is beneficial to avoid attempting to calculate share(Xi,k, Xk+1,j) at each iteration. Instead, we introduce the subroutine Alg. 1 which more efficiently calculates every value of shared as k is varied across the sequence. If d = maxX\u2208T |inc(X)| is the maximum degree of any tensor in T , this can be done in O(d|T |) time.\nFrom these recursions, we introduce Alg. 2, a dynamic programming algorithm for computing the minimal possible size of the largest tensor for any contraction tree given an order. We make use of pruning in order to reduce the number of needed computations, as there is no need to evaluate the second subsequence in a split if the first has already exceeded the current best result. Performance can be further improved by making use of memoization to cache previous calls. Additional gains may be achieved by parallelizing the evaluation of disjoint subproblem calls, which is an ongoing project. Utilizing these techniques, the complexity of Alg. 2 is O(|E||V |2 + d|V |3).\nAlgorithm 1 CalcShared Require: A tensor chain C = (X1, . . . , Xn), 1 \u2264 i \u2264 j \u2264 n shared\u2190 [0 | for i \u2264 k < j] (upper, lower)\u2190 ({Xi, . . . , Xk\u22121}, {Xk+1, . . . , Xj}) for i \u2264 k < j do new \u2190 sum(size, {a \u2208 inc(Xi) | inc(a)\u2229 upper 6= \u2205}) old\u2190 sum(size, {b \u2208 inc(Xi) | inc(b) \u2229 lower 6= \u2205}) shared[k]\u2190 shared[k] + new \u2212 old shared[k + 1]\u2190 shared[k]\nend for return shared\nAlgorithm 2 Tree Structure Optimization Require: A tensor chain C = (X1, . . . , Xn), 1 \u2264 i \u2264 j \u2264 n Require: x\u2295 y = x+ y or max(x, y)\nif i == j then return (size(Xi), i) end if shared\u2190 CalcShared(C, i, j) outsize\u2190 size(Xi,j) (c\u2217, t\u2217)\u2190\u221e, i for i \u2264 k < j do c\u2190 COST (outsize, shared[k]) if c > c\u2217 then\ncontinue end if (cl, l)\u2190 TreeStructureOptimization(C, i, k) c\u2190 c\u2295 cl if c > c\u2217 then\ncontinue end if (cr, r)\u2190 TreeStructureOptimization(C, k + 1, j) c\u2190 c\u2295 cr if c < c\u2217 then (c\u2217, t\u2217)\u2190 (c, (l, r))\nend if end for return (c\u2217, t\u2217)\nA variant for the classical total number of operations can be achieved simply by looking at the incremental sum of the left and right trees rather than the max.\nc(i, i) = size(Xi)\nc(i, j) = min i\u2264k<j ops(Xi,k, Xk+1,j) + c(i, k) + c(k + 1, j)\n(7)"
        },
        {
            "heading": "B. Heuristic Order Choices",
            "text": "While we have introduced algorithms for constructing an optimal contraction tree given a particular order, we must now tackle the problem of actually choosing an order. We turn to a heuristic approach for solving this problem. In particular, we select an order which minimizes the sum total of lengths of stretched edges in the graph. This is known as the Minimal\nLinear Arrangement of the vertices in the graph, and is defined as \u2211\nuv\u2208E w(uv)|\u03c3(u)\u2212 \u03c3(v)|\nThis is motivated by the property of matrix chains sharing indices only with their immediate neighbors. An order which minimizes MLA discourages long stretched edges on the chain. We have empirically observed this to be effective in reducing the congestion of the resulting contraction tree, which will be discussed further in the Results section.\nIn order to calculate an order which effectively minimizes MLA, we utilize the LinearOrdering.jl package, a Julia package for multilevel linear ordering that is currently in development by the authors. This package utilizes a volume based coarsening strategy as well as node by node minimization in order to find a high quality arrangement [32]. The package can be found at https://github.com/cameton/LinearOrdering.jl."
        },
        {
            "heading": "V. EXPERIMENTAL RESULTS",
            "text": "We evaluate tree structure optimization against existing state of the art algorithms Tamaki-2017, FlowCutter, and Cotengra on a collection of randomly generated QAOA circuits. To generate test results for tree structure optimization, we find an order for the graph with a good Minimum Linear Arrangement objective, then run tree structure optimization with the relevant cost. This is done repeatedly while varying the random seed for a set amount of time, returning the best result at the end. In most cases we observe a significant improvement in the quality of results given a comparable running time (that is usually negligible in comparison to the quantum simulation time itself).\nFor our tests, for a fixed depth p and degree d, we construct a p layer QAOA ansatz circuit targeting the Max Cut problem on a random 32 vertex d-regular graph using the QTensor library [23]. Each circuit will have a number of qubits equal to the number of vertices in the given graph. For each combination of d = 3, 4, 5 and QAOA depth p = 2, 3, 4, 5, we generate 10 random circuits to form our test set. The source code and generated circuits are available at https: //github.com/cameton/HPEC2022 ContractionTrees. In doing so, we sample a variety of circuits with a varying level of connectedness and depth, which we use as a proxy for the complexity of the circuit; such circuits are commonly used in evaluations of contraction order solvers for quantum circuits as in [15].\nA. Vertex Congestion\nWe compare the vertex congestion (Eq. 3) of the solution found by our tree structure optimizer for the test set against that found by Tamaki-2017 and FlowCutter by finding the tree decomposition of the line graph. As Cotengra has functionality to minimize edge congestion and number of FLOPS, it will be evaluated separately. For these experiments, we ran each of these optimizers for 5 seconds. Results for this test are given in Fig. 2.\nOur tree structure optimization based approach performed strictly better than Flowcutter on 96.4% of points and better than Tamaki on 90.0% of points, while never achieving a worse cost. What\u2019s more, the margin of improvement increases significantly as the vertex congestion of the circuit increases.\nAs vertex and edge congestion represent the log of the maximum size and maximum number of operations, even small improvements in congestion lead to significant gains. As such, having such a large improvement on so many points represents advantage of our solver over competitive solvers run for the same amount of time."
        },
        {
            "heading": "B. Edge Congestion and Operation Count",
            "text": "We compare the edge congestion (Eq. 4) and estimated number of operations (Eq. 1) of the solution found by our tree structure optimizer on the test set against those found by Cotengra. For these tests, Cotengra was run for 5 seconds with default settings and the KaHyPar partitioner backend. Two tests were run using Cotengra: one minimizing edge congestion and one minimizing the total number of scalar operations. Results are given in Fig. 3.\nFrom Fig. 3, we see that tree structure optimization is highly competitive with Cotengra, in every case finding a solution which is close to or better than Cotengra in terms of both estimated number of FLOPS and edge congestion.\nOur solver bests Cotengra on a full 95% of points when minimizing total number of operations, and on 84.1% of points when minimizing edge congestion. This once again demonstrates the superiority of our solver for comparatively short run times.\nC. Varying the Number of Qubits\nWe may also wish to compare these solvers as we vary the number of vertices in the input graph, by extension varying the number of qubits. To avoid redundancy, we restrict our comparison for this section to Cotengra and Total FLOPS alone. For these tests, we construct a 2 layer QAOA ansatz circuit targeting the Max Cut problem for random 3-regular graphs with 32, 64, 96 and 128 vertices. For each size, we generate 10 random circuits. Because each circuit has a\nnumber of qubits equal to the size of the input graph, we can examine circuits of a similar complexity across a variety of different qubit counts. For these tests, we ran both Cotengra and the ordering solver for 10 seconds for each input.\nThe results of this test are shown in Fig. 4. Averaging over the 32 qubit circuits, our tree structure optimization based approach produced results around 28.6% better than Cotengra, improving to 74.1% when averaging over the 64 qubit circuits. For the 96 and 128 qubit circuits, nearly every result produced by our tree structure optimization approach is several orders of magnitude better than Cotengra for the 10 second running time."
        },
        {
            "heading": "VI. FUTURE WORK",
            "text": ""
        },
        {
            "heading": "A. Width Refinement",
            "text": "The current refinement strategy used in this paper minimizes Minimum Linear Arrangement by making local changes to the order at each level of the algorithm [33]. While we have shown thus far that Minimum Linear Arrangement works as a effective heuristic for the tested circuits, we may be able to achieve further improvements through the development of refinement strategies which directly target the width of the order. That is, we would like the ability to make local changes which serve to reduce the width of the overall order, potentially achieving better results than the more general Minimum Linear Arrangement heuristic."
        },
        {
            "heading": "B. Performance & Scalability",
            "text": "The algorithms introduced here for tree structure optimization are currently not parallelized. The development of a tree structure optimization algorithm which takes advantage of the graph structure for improved performance is vital to the application of this technique to fields outside of quantum circuit simulation, such as in large scale shortest path acceleration [17].\nAlthough the multilevel algorithm component is already fast, it will be valuable to develop a strongly parallelized version of these algorithms which may be better suited for a high performance computing environment, particularly where such hardware is being used for circuit simulation already."
        },
        {
            "heading": "C. Higher Order Structures",
            "text": "Thus far we have primarily considered tensor networks representable as normal undirected graphs. In this context, we associate each index in the circuit with a corresponding edge in the graph. For some quantum circuits, a possible costsaving technique is to reconsider certain collections of indices as hyperindices, in essence a single index connecting multiple gates [24]. This introduces a hypergraph structure to our tensor network representation, a higher order generalization of undirected graphs.\nAs it stands, our work has yet to be extended to accommodate these higher order structures, something which Cotengra is currently able to handle by making use of existing hypergraph partitioning software [15]. In particular, attention needs to be paid to how the hypergraph structure may inform the coarsening portion of the ordering problem. For example, this can be done by evaluating various kernels and similarity measures on high order structures such as in [4], [14], [37], [38]. Moreover, this necessitates a tree structure optimization algorithm for higher order structures."
        },
        {
            "heading": "VII. CONCLUSION",
            "text": "Accelerating quantum circuit simulation is a critical problem not only for demonstrating quantum advantage but also for hybrid quantum-classical algorithms which require computationally heavy parameter optimization backend on the classical machine. In this paper, we presented novel algorithms for constructing an optimal contraction tree from a given order. We introduced a multilevel solver for the Minimum Linear Arrangement problem on graphs and demonstrated its applicability as a heuristic for providing orderings for contraction trees. We compared the performance of our solver against state-of-the-art industry solvers Tamaki-2017, FlowCutter, and Cotengra on a collection of randomly generated QAOA circuits. We have shown that our method achieves results that are superior by orders of magnitude on some instances to competitors when run for a comparable amount of time. There is work under way to produce higher quality orders for producing contraction trees, and on improving the speed of tree structure optimization."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) ONISQ program under Contract No. HR001120C0068. This research was partially supported by NSF, under award number 2122793. Y.A.\u2019s and D.L.\u2019s work at Argonne National Laboratory was partially supported by the U.S. Department of Energy, Office of Science, under contract DE-AC02-06CH11357."
        }
    ],
    "title": "Constructing Optimal Contraction Trees for Tensor Network Quantum Circuit Simulation",
    "year": 2022
}