{
    "abstractText": "This paper presents a substantial extension of our work published at ICLR [1]. Our ICLR work advocated for enhancing transferability in adversarial examples by incorporating a Bayesian formulation into model parameters, which effectively emulates the ensemble of infinitely many deep neural networks, while, in this paper, we introduce a novel extension by incorporating the Bayesian formulation into the model input as well, enabling the joint diversification of both the model input and model parameters. Our empirical findings demonstrate that: 1) the combination of Bayesian formulations for both the model input and model parameters yields significant improvements in transferability; 2) by introducing advanced approximations of the posterior distribution over the model input, adversarial transferability achieves further enhancement, surpassing all state-of-the-arts when attacking without model fine-tuning. Moreover, we propose a principled approach to fine-tune model parameters in such an extended Bayesian formulation. The derived optimization objective inherently encourages flat minima in the parameter space and input space. Extensive experiments demonstrate that our method achieves a new state-of-the-art on transfer-based attacks, improving the average success rate on ImageNet and CIFAR-10 by 19.14% and 2.08%, respectively, when comparing with our ICLR basic Bayesian method. We will make our code publicly",
    "authors": [
        {
            "affiliations": [],
            "name": "Qizhang Li"
        },
        {
            "affiliations": [],
            "name": "Yiwen Guo"
        },
        {
            "affiliations": [],
            "name": "Xiaochen Yang"
        },
        {
            "affiliations": [],
            "name": "Wangmeng Zuo"
        },
        {
            "affiliations": [],
            "name": "Hao Chen"
        }
    ],
    "id": "SP:489c423a59a5d6ab49b7c36ac1de42a105ebb4f4",
    "references": [
        {
            "authors": [
                "Q. Li",
                "Y. Guo",
                "W. Zuo",
                "H. Chen"
            ],
            "title": "Making substitute models more bayesian can enhance transferability of adversarial examples",
            "venue": "International Conference on Learning Representations, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "ImageNet classification with deep convolutional neural networks",
            "venue": "Communications of the ACM, vol. 60, no. 6, pp. 84\u201390, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "R. Collobert",
                "J. Weston"
            ],
            "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
            "venue": "Proceedings of the 25th international conference on Machine learning, 2008, pp. 160\u2013167.",
            "year": 2008
        },
        {
            "authors": [
                "O. Abdel-Hamid",
                "A.-r. Mohamed",
                "H. Jiang",
                "L. Deng",
                "G. Penn",
                "D. Yu"
            ],
            "title": "Convolutional neural networks for speech recognition",
            "venue": "IEEE/ACM Transactions on audio, speech, and language processing, vol. 22, no. 10, pp. 1533\u20131545, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "C. Szegedy",
                "W. Zaremba",
                "I. Sutskever",
                "J. Bruna",
                "D. Erhan",
                "I. Goodfellow",
                "R. Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "ICLR, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "N. Papernot",
                "P. McDaniel",
                "I. Goodfellow"
            ],
            "title": "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples",
            "venue": "arXiv preprint arXiv:1605.07277, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Y. Liu",
                "X. Chen",
                "C. Liu",
                "D. Song"
            ],
            "title": "Delving into transferable adversarial examples and black-box attacks",
            "venue": "ICLR, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Li",
                "S. Bai",
                "Y. Zhou",
                "C. Xie",
                "Z. Zhang",
                "A. Yuille"
            ],
            "title": "Learning transferable adversarial examples via ghost networks",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, 2020, pp. 11 458\u201311 465.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Xiong",
                "J. Lin",
                "M. Zhang",
                "J.E. Hopcroft",
                "K. He"
            ],
            "title": "Stochastic variance reduced ensemble adversarial attack for boosting the adversarial transferability",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 14 983\u201314 992.",
            "year": 2022
        },
        {
            "authors": [
                "Q. Huang",
                "I. Katsman",
                "H. He",
                "Z. Gu",
                "S. Belongie",
                "S.-N. Lim"
            ],
            "title": "Enhancing adversarial example transferability with an intermediate level attack",
            "venue": "ICCV, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Q. Li",
                "Y. Guo",
                "H. Chen"
            ],
            "title": "Yet another intermediate-leve attack",
            "venue": "ECCV, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "D. Wu",
                "Y. Wang",
                "S.-T. Xia",
                "J. Bailey",
                "X. Ma"
            ],
            "title": "Rethinking the security of skip connections in resnet-like neural networks",
            "venue": "ICLR, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Guo",
                "Q. Li",
                "H. Chen"
            ],
            "title": "Backpropagating linearly improves transferability of adversarial examples",
            "venue": "NeurIPS, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "O. Russakovsky",
                "J. Deng",
                "H. Su",
                "J. Krause",
                "S. Satheesh",
                "S. Ma",
                "Z. Huang",
                "A. Karpathy",
                "A. Khosla",
                "M. Bernstein",
                "A.C. Berg",
                "L. Fei-Fei"
            ],
            "title": "Imagenet large scale visual recognition challenge",
            "venue": "IJCV, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "A. Krizhevsky",
                "G. Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Citeseer, Tech. Rep., 2009.",
            "year": 2009
        },
        {
            "authors": [
                "I.J. Goodfellow",
                "J. Shlens",
                "C. Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "ICLR, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "A. Kurakin",
                "I. Goodfellow",
                "S. Bengio"
            ],
            "title": "Adversarial machine learning at scale",
            "venue": "ICLR, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Dong",
                "F. Liao",
                "T. Pang",
                "H. Su",
                "J. Zhu",
                "X. Hu",
                "J. Li"
            ],
            "title": "Boosting adversarial attacks with momentum",
            "venue": "CVPR, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J. Lin",
                "C. Song",
                "K. He",
                "L. Wang",
                "J.E. Hopcroft"
            ],
            "title": "Nesterov accelerated gradient and scale invariance for adversarial attacks",
            "venue": "arXiv preprint arXiv:1908.06281, 2019.",
            "year": 1908
        },
        {
            "authors": [
                "Y. Huang",
                "A.W.-K. Kong"
            ],
            "title": "Transferable adversarial attack based on integrated gradients",
            "venue": "arXiv preprint arXiv:2205.13152, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Guo",
                "Q. Li",
                "W. Zuo",
                "H. Chen"
            ],
            "title": "An intermediate-level attack framework on the basis of linear regression",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. SUBMISSION TO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, 2023 9",
            "year": 2022
        },
        {
            "authors": [
                "J. Springer",
                "M. Mitchell",
                "G. Kenyon"
            ],
            "title": "A little robustness goes a long way: Leveraging robust features for targeted transfer attacks",
            "venue": "Advances in Neural Information Processing Systems, vol. 34, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Zhu",
                "Y. Chen",
                "X. Li",
                "K. Chen",
                "Y. He",
                "X. Tian",
                "B. Zheng",
                "Y. Chen",
                "Q. Huang"
            ],
            "title": "Toward understanding and boosting adversarial transferability from a distribution perspective",
            "venue": "IEEE Transactions on Image Processing, vol. 31, pp. 6487\u20136501, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Dong",
                "T. Pang",
                "H. Su",
                "J. Zhu"
            ],
            "title": "Evading defenses to transferable adversarial examples by translation-invariant attacks",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4312\u20134321.",
            "year": 2019
        },
        {
            "authors": [
                "C. Xie",
                "Z. Zhang",
                "Y. Zhou",
                "S. Bai",
                "J. Wang",
                "Z. Ren",
                "A.L. Yuille"
            ],
            "title": "Improving transferability of adversarial examples with input diversity",
            "venue": "CVPR, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "X. Wang",
                "X. He",
                "J. Wang",
                "K. He"
            ],
            "title": "Admix: Enhancing the transferability of adversarial attacks",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 16 158\u201316 167.",
            "year": 2021
        },
        {
            "authors": [
                "M. Gubri",
                "M. Cordy",
                "M. Papadakis",
                "Y. Le Traon",
                "K. Sen"
            ],
            "title": "Efficient and transferable adversarial examples from bayesian neural networks",
            "venue": "Uncertainty in Artificial Intelligence. PMLR, 2022, pp. 738\u2013748.",
            "year": 2022
        },
        {
            "authors": [
                "M. Gubri",
                "M. Cordy",
                "M. Papadakis",
                "Y.L. Traon",
                "K. Sen"
            ],
            "title": "Lgv: Boosting adversarial example transferability from large geometric vicinity",
            "venue": "arXiv preprint arXiv:2207.13129, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "R. Johnson",
                "T. Zhang"
            ],
            "title": "Accelerating stochastic gradient descent using predictive variance reduction",
            "venue": "Advances in neural information processing systems, vol. 26, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "A. Graves"
            ],
            "title": "Practical variational inference for neural networks",
            "venue": "Advances in neural information processing systems, vol. 24, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "C. Blundell",
                "J. Cornebise",
                "K. Kavukcuoglu",
                "D. Wierstra"
            ],
            "title": "Weight uncertainty in neural network",
            "venue": "International conference on machine learning. PMLR, 2015, pp. 1613\u20131622.",
            "year": 2015
        },
        {
            "authors": [
                "D.P. Kingma",
                "T. Salimans",
                "M. Welling"
            ],
            "title": "Variational dropout and the local reparameterization trick",
            "venue": "Advances in neural information processing systems, vol. 28, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "M. Khan",
                "D. Nielsen",
                "V. Tangkaratt",
                "W. Lin",
                "Y. Gal",
                "A. Srivastava"
            ],
            "title": "Fast and scalable bayesian deep learning by weightperturbation in adam",
            "venue": "International Conference on Machine Learning. PMLR, 2018, pp. 2611\u20132620.",
            "year": 2018
        },
        {
            "authors": [
                "G. Zhang",
                "S. Sun",
                "D. Duvenaud",
                "R. Grosse"
            ],
            "title": "Noisy natural gradient as variational inference",
            "venue": "International Conference on Machine Learning. PMLR, 2018, pp. 5852\u20135861.",
            "year": 2018
        },
        {
            "authors": [
                "A. Wu",
                "S. Nowozin",
                "E. Meeds",
                "R.E. Turner",
                "J.M. Hernandez- Lobato",
                "A.L. Gaunt"
            ],
            "title": "Deterministic variational inference for robust bayesian neural networks",
            "venue": "arXiv preprint arXiv:1810.03958, 2018.",
            "year": 1810
        },
        {
            "authors": [
                "K. Osawa",
                "S. Swaroop",
                "M.E.E. Khan",
                "A. Jain",
                "R. Eschenhagen",
                "R.E. Turner",
                "R. Yokota"
            ],
            "title": "Practical deep learning with bayesian principles",
            "venue": "Advances in neural information processing systems, vol. 32, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M. Dusenberry",
                "G. Jerfel",
                "Y. Wen",
                "Y. Ma",
                "J. Snoek",
                "K. Heller",
                "B. Lakshminarayanan",
                "D. Tran"
            ],
            "title": "Efficient and scalable bayesian neural nets with rank-1 factors",
            "venue": "International conference on machine learning. PMLR, 2020, pp. 2782\u20132792.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Gal",
                "Z. Ghahramani"
            ],
            "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
            "venue": "international conference on machine learning. PMLR, 2016, pp. 1050\u20131059.",
            "year": 2016
        },
        {
            "authors": [
                "A. Kendall",
                "Y. Gal"
            ],
            "title": "What uncertainties do we need in bayesian deep learning for computer vision?",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Y. Gal",
                "J. Hron",
                "A. Kendall"
            ],
            "title": "Concrete dropout",
            "venue": "Advances in neural information processing systems, vol. 30, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J. Kirkpatrick",
                "R. Pascanu",
                "N. Rabinowitz",
                "J. Veness",
                "G. Desjardins",
                "A.A. Rusu",
                "K. Milan",
                "J. Quan",
                "T. Ramalho",
                "A. Grabska-Barwinska"
            ],
            "title": "Overcoming catastrophic forgetting in neural networks",
            "venue": "Proceedings of the national academy of sciences, vol. 114, no. 13, pp. 3521\u20133526, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "H. Ritter",
                "A. Botev",
                "D. Barber"
            ],
            "title": "A scalable laplace approximation for neural networks",
            "venue": "6th International Conference on Learning Representations, ICLR 2018-Conference Track Proceedings, vol. 6. International Conference on Representation Learning, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "D.X. Li"
            ],
            "title": "On default correlation: A copula function approach",
            "venue": "The Journal of Fixed Income, vol. 9, no. 4, pp. 43\u201354, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "S. Mandt",
                "M.D. Hoffman",
                "D.M. Blei"
            ],
            "title": "Stochastic gradient descent as approximate bayesian inference",
            "venue": "arXiv preprint arXiv:1704.04289, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "W.J. Maddox",
                "P. Izmailov",
                "T. Garipov",
                "D.P. Vetrov",
                "A.G. Wilson"
            ],
            "title": "A simple baseline for bayesian uncertainty in deep learning",
            "venue": "Advances in Neural Information Processing Systems, vol. 32, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "W. Maddox",
                "S. Tang",
                "P. Moreno",
                "A.G. Wilson",
                "A. Damianou"
            ],
            "title": "Fast adaptation with linearized neural networks",
            "venue": "International Conference on Artificial Intelligence and Statistics. PMLR, 2021, pp. 2737\u20132745.",
            "year": 2021
        },
        {
            "authors": [
                "A.G. Wilson",
                "P. Izmailov"
            ],
            "title": "Bayesian deep learning and a probabilistic perspective of generalization",
            "venue": "Advances in neural information processing systems, vol. 33, pp. 4697\u20134708, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "L. Cardelli",
                "M. Kwiatkowska",
                "L. Laurenti",
                "N. Paoletti",
                "A. Patane",
                "M. Wicker"
            ],
            "title": "Statistical guarantees for the robustness of bayesian neural networks",
            "venue": "arXiv preprint arXiv:1903.01980, 2019.",
            "year": 1903
        },
        {
            "authors": [
                "M. Wicker",
                "L. Laurenti",
                "A. Patane",
                "M. Kwiatkowska"
            ],
            "title": "Probabilistic safety for bayesian neural networks",
            "venue": "Conference on Uncertainty in Artificial Intelligence. PMLR, 2020, pp. 1198\u20131207.",
            "year": 2020
        },
        {
            "authors": [
                "X. Liu",
                "Y. Li",
                "C. Wu",
                "C.-J. Hsieh"
            ],
            "title": "Adv-bnn: Improved adversarial defense through robust bayesian neural network",
            "venue": "arXiv preprint arXiv:1810.01279, 2018.",
            "year": 1810
        },
        {
            "authors": [
                "M. Yuan",
                "M. Wicker",
                "L. Laurenti"
            ],
            "title": "Gradient-free adversarial attacks for bayesian neural networks",
            "venue": "arXiv preprint arXiv:2012.12640, 2020.",
            "year": 2012
        },
        {
            "authors": [
                "G. Carbone",
                "M. Wicker",
                "L. Laurenti",
                "A. Patane",
                "L. Bortolussi",
                "G. Sanguinetti"
            ],
            "title": "Robustness of bayesian neural networks to gradient-based attacks",
            "venue": "Advances in Neural Information Processing Systems, vol. 33, pp. 15 602\u201315 613, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Li",
                "J. Bradshaw",
                "Y. Sharma"
            ],
            "title": "Are generative classifiers more robust to adversarial attacks?",
            "venue": "in International Conference on Machine Learning. PMLR,",
            "year": 2019
        },
        {
            "authors": [
                "Z. Wang",
                "Y. Guo",
                "W. Zuo"
            ],
            "title": "Deepfake forensics via an adversarial game",
            "venue": "IEEE Transactions on Image Processing, vol. 31, pp. 3541\u20133552, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "P. Foret",
                "A. Kleiner",
                "H. Mobahi",
                "B. Neyshabur"
            ],
            "title": "Sharpnessaware minimization for efficiently improving generalization",
            "venue": "arXiv preprint arXiv:2010.01412, 2020.",
            "year": 2010
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "ICLR, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "C. Szegedy",
                "V. Vanhoucke",
                "S. Ioffe",
                "J. Shlens",
                "Z. Wojna"
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "CVPR, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "CVPR, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "G. Huang",
                "Z. Liu",
                "L. Van Der Maaten",
                "K.Q. Weinberger"
            ],
            "title": "Densely connected convolutional networks",
            "venue": "CVPR, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Z. Liu",
                "H. Mao",
                "C.-Y. Wu",
                "C. Feichtenhofer",
                "T. Darrell",
                "S. Xie"
            ],
            "title": "A convnet for the 2020s",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 11 976\u2013 11 986.",
            "year": 2022
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S. Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929, 2020.",
            "year": 2010
        },
        {
            "authors": [
                "H. Touvron",
                "M. Cord",
                "M. Douze",
                "F. Massa",
                "A. Sablayrolles",
                "H. J\u00e9gou"
            ],
            "title": "Training data-efficient image transformers & distillation through attention",
            "venue": "International Conference on Machine Learning. PMLR, 2021, pp. 10 347\u201310 357.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Liu",
                "Y. Lin",
                "Y. Cao",
                "H. Hu",
                "Y. Wei",
                "Z. Zhang",
                "S. Lin",
                "B. Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 10 012\u201310 022.",
            "year": 2021
        },
        {
            "authors": [
                "H. Bao",
                "L. Dong",
                "S. Piao",
                "F. Wei"
            ],
            "title": "Beit: Bert pre-training of image transformers",
            "venue": "International Conference on Learning Representations, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "I.O. Tolstikhin",
                "N. Houlsby",
                "A. Kolesnikov",
                "L. Beyer",
                "X. Zhai",
                "T. Unterthiner",
                "J. Yung",
                "A. Steiner",
                "D. Keysers",
                "J. Uszkoreit"
            ],
            "title": "Mlp-mixer: An all-mlp architecture for vision",
            "venue": "Advances in Neural Information Processing Systems, vol. 34, pp. 24 261\u201324 272, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "P. Izmailov",
                "D. Podoprikhin",
                "T. Garipov",
                "D. Vetrov",
                "A.G. Wilson"
            ],
            "title": "Averaging weights leads to wider optima and better generalization",
            "venue": "arXiv preprint arXiv:1803.05407, 2018.",
            "year": 1803
        },
        {
            "authors": [
                "J. Zhang",
                "W. Wu",
                "J.-t. Huang",
                "Y. Huang",
                "W. Wang",
                "Y. Su",
                "M.R. Lyu"
            ],
            "title": "Improving adversarial transferability via neuron attribution-based attacks",
            "venue": "Proceedings of the IEEE/CVF Confer- SUBMISSION TO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, 2023 10 ence on Computer Vision and Pattern Recognition, 2022, pp. 14 993\u2013 15 002.",
            "year": 2023
        },
        {
            "authors": [
                "S. Zagoruyko",
                "N. Komodakis"
            ],
            "title": "Wide residual networks",
            "venue": "BMVC, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "S. Xie",
                "R. Girshick",
                "P. Doll\u00e1r",
                "Z. Tu",
                "K. He"
            ],
            "title": "Aggregated residual transformations for deep neural networks",
            "venue": "CVPR, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "D. Han",
                "J. Kim",
                "J. Kim"
            ],
            "title": "Deep pyramidal residual networks",
            "venue": "CVPR, 2017, pp. 5927\u20135935.",
            "year": 2017
        },
        {
            "authors": [
                "X. Dong",
                "Y. Yang"
            ],
            "title": "Searching for a robust neural architecture in four gpu hours",
            "venue": "CVPR, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "R. Wightman"
            ],
            "title": "Pytorch image models",
            "venue": "https://github.com/ rwightman/pytorch-image-models, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "F. Croce",
                "M. Andriushchenko",
                "V. Sehwag",
                "E. Debenedetti",
                "N. Flammarion",
                "M. Chiang",
                "P. Mittal",
                "M. Hein"
            ],
            "title": "Robustbench: a standardized adversarial robustness benchmark",
            "venue": "arXiv preprint arXiv:2010.09670, 2020.",
            "year": 2010
        },
        {
            "authors": [
                "A. Madry",
                "A. Makelov",
                "L. Schmidt",
                "D. Tsipras",
                "A. Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "ICLR, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "E. Wong",
                "L. Rice",
                "J.Z. Kolter"
            ],
            "title": "Fast is better than free: Revisiting adversarial training",
            "venue": "arXiv preprint arXiv:2001.03994, 2020.",
            "year": 2001
        },
        {
            "authors": [
                "C. Xie",
                "M. Tan",
                "B. Gong",
                "J. Wang",
                "A.L. Yuille",
                "Q.V. Le"
            ],
            "title": "Adversarial examples improve image recognition",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 819\u2013828.",
            "year": 2020
        },
        {
            "authors": [
                "F. Croce",
                "M. Hein"
            ],
            "title": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
            "venue": "ICML, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "C. Liu",
                "Y. Dong",
                "W. Xiang",
                "X. Yang",
                "H. Su",
                "J. Zhu",
                "Y. Chen",
                "Y. He",
                "H. Xue",
                "S. Zheng"
            ],
            "title": "A comprehensive study on robustness of image classification models: Benchmarking and rethinking",
            "venue": "arXiv preprint arXiv:2302.14301, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "N.D. Singh",
                "F. Croce",
                "M. Hein"
            ],
            "title": "Revisiting adversarial training for imagenet: Architectures, training and generalization across threat models",
            "venue": "arXiv preprint arXiv:2303.01870, 2023.",
            "year": 1870
        },
        {
            "authors": [
                "Y. Bai",
                "J. Mei",
                "A. Yuille",
                "C. Xie"
            ],
            "title": "Are transformers more robust than cnns?",
            "venue": "in Thirty-Fifth Conference on Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "J. Cohen",
                "E. Rosenfeld",
                "Z. Kolter"
            ],
            "title": "Certified adversarial robustness via randomized smoothing",
            "venue": "International Conference on Machine Learning. PMLR, 2019, pp. 1310\u20131320.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Deep neural networks, adversarial examples, transferability, generalization ability.\n\u2726"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "D EEP neural networks (DNNs) have demonstrated re-markable performances in various applications, such as computer vision [2], natural language processing [3], and speech recognition [4]. Nevertheless, these models have been found to be vulnerable to adversarial examples [5], which are maliciously perturbed input data that could mislead the models into making incorrect predictions. This raises serious concerns about the safety of DNNs, particularly when applying them in security-critical domains. More severely, adversarial examples are transferable [6]. An attacker can manipulate adversarial examples on a substitute model to attack unknown victim models with different architectures and parameters. An in-depth exploration of adversarial transferability is crucial as it provides valuable insights into the nature of DNNs, enables more comprehensive evaluation of their robustness, and promotes the design of stronger defense methods and fully resistant networks.\nOver the past several years, considerable effort has been devoted to improving the transferability of adversarial examples. In particular, ensemble-based methods [7], [8], [9] attract our attention as they can be readily combined with almost all other methods, such as optimizing intermediatelevel representations [10], [11] or modifying the backpropagation computation [12], [13]. The effectiveness of intro-\n\u2022 Q. Li is with the School of Computer Science and Technology, Harbin Institute of Technology, China. E-mail: liqizhang95@gmail.com. \u2022 Y. Guo is with ByteDance AI Lab. E-mail: guoyiwen89@gmail.com. \u2022 X. Yang is with the School of Mathematics and Statistics, University of\nGlasgow, UK. E-mail: xiaochen.yang@glasgow.ac.uk. \u2022 W. Zuo is with the School of Computer Science and Technology, Harbin\nInstitute of Technology, China. E-mail: cswmzuo@gmail.com. \u2022 H. Chen is with the Department of Computer Science, University of\nCalifornia, Davis, US. Email: chen@ucdavis.edu.\nManuscript received June 1, 2023.\nducing ensemble-based attack is generally related to the number of models available in the bucket, thus we consider a statistical ensemble which consists of infinite many substitute models in some sense by introducing a Bayesian formulation. To model the randomness of models comprehensively, we introduce distributions to the model parameters and model input. Parameters of these distributions can be obtained from an off-the-shelf model, and if fine-tuning is possible, these parameters can further be optimized. Adversarial examples are then crafted by maximizing the mean prediction loss averaged across the parameters and inputs sampled from their respective distributions.\nWe evaluate our method in attacking a variety of models on ImageNet [14] and CIFAR-10 [15]. The proposed method outperforms state-of-the-arts considerably. We also show that our method can be readily integrated with existing methods for further improving the attack performance.\nWhat\u2019s new in comparison to [1]: 1) We introduce a probability measure to model input in conjunction with the parameters of the substitute models. This enables the joint diversification of both the model and input throughout the iterative process of generating adversarial examples. 2) With the incorporation of input randomness in our Bayesian formulation, we newly derive a principled optimization objective for fine-tuning that encourages flat minima in both the parameter space and the input space."
        },
        {
            "heading": "2 BACKGROUND AND RELATED WORK",
            "text": "Background on adversarial examples. Given a benign input x with label y and a substitute model fw with full knowledge of architecture and parameters w, gradientbased approaches generate adversarial examples by op-\nar X\niv :2\n30 7.\n11 33\n4v 1\n[ cs\n.L G\n] 2\n1 Ju\nl 2 02\n3\ntimizing \u2113p-bounded perturbations \u2206x to maximize the prediction loss L:\nargmax \u2225\u2206x\u2225p\u2264\u03f5 L(x+\u2206x, y,w),\nwhere \u03f5 is the perturbation budget. FGSM [16] is a simple one-step attack method to obtain adversarial examples in the p = \u221e setting:\nxadv = x+ \u03f5 \u00b7 sign(\u2207xL(x, y,w)).\nThe iterative variant of FGSM, I-FGSM [17], is capable of generating more powerful attacks:\nxadv0 = x, x adv t+1 = clip(x adv t + \u03b7 \u00b7 sign(\u2207xL(x, y,w)),\nwhere \u03b7 is the step size and the clip function ensures that the generated adversarial examples remain within the prespecified range."
        },
        {
            "heading": "2.1 Transfer-based Attacks",
            "text": "FGSM and I-FGSM require calculating gradients of the victim model. Nevertheless, in practical scenarios, the attacker may not have enough knowledge about the victim model for calculating gradients. To address this issue, many attacks rely on the transferability of adversarial examples, meaning that adversarial examples crafted for one classification model (using, for example, FGSM or I-FGSM) can often successfully attack other victim models. It is normally assumed to be able to query the victim model to annotate training samples, collect a set of samples from the same distribution as that modeled by the victim models, or collect a pre-trained substitute model that is trained to accomplish the same task as the victim models.\nTo enhance the transferability of adversarial examples, several groups of methods have been proposed. Advancements include improved optimizer and gradient computation techniques for updating xadv [10], [11], [13], [18], [19], [20], [21], innovative strategies of training and finetuning substitute models [22], [23], and two groups of methods closely related to this paper: random input augmentation [19], [24], [25], [26] and substitute model augmentation (i.e., ensemble) [7], [9], [27], [28]. For instance, in input augmentation methods, Dong et al. [24], Xie et al. [25], Lin et al. [19] introduced different sorts of transformations into the iterative update of adversarial examples. Comparing with these methods, our method introduces input diversity via a principled Bayesian formulation and for the first time takes such transformation into account during substitute model fine-tuning. For substitute model augmentation (i.e., ensemble), Liu et al. [7] proposed to generate adversarial examples on an ensemble of multiple substitute models that differ in their architectures. Additionally, Xiong et al. [9] proposed stochastic variance reduced ensemble to reduce the variance of gradients of different substitute models following the spirit of stochastic variance reduced gradient [29]. Gubri et al. [28] suggested fine-tuning with a fixed and large learning rate to collect multiple models along the training trajectory for the ensemble attack. In this paper, we consider the diversity in both the substitute models and model inputs by introducing a Bayesian approximation for achieving this."
        },
        {
            "heading": "2.2 Bayesian DNNs",
            "text": "If a deep neural network (DNN) is considered as a probabilistic model, the process of training its parameters, denoted as w, can be seen as maximum likelihood estimation or maximum a posteriori estimation (with regularization). In Bayesian deep learning, the approach involves simultaneously estimating the posterior distribution of the parameters given the data. The prediction for any new input instance is obtained by taking the expectation over this posterior distribution. Due to the large number of parameters typically involved in DNNs, optimizing Bayesian models becomes more challenging compared to shallow models. As a result, numerous studies have been conducted to address this issue, leading to the development of various scalable approximations. Effective methods utilize variational inference [30], [31], [32], [33], [34], [35], [36], [37], dropout inference [38], [39], [40], Laplace approximation [41], [42], [43], or stochastic gradient descent (SGD)-based approximation [44], [45], [46], [47]. Taking SWAG [45] as an example, which is an SGD-based approximation, it approximates the posterior using a Gaussian distribution with the stochastic weight averaging (SWA) solution as its first raw moment and the composition of a low rank matrix and a diagonal matrix as its second central moment. Our method is developed in a Bayesian spirit and we shall discuss SWAG thoroughly later in this paper. In addition to approximating the posterior over model parameters, our work also involves approximating the posterior over adversarial perturbations. This is implemented in order to introduce randomization into the model input during each iteration of iterative attacks.\nIn recent years, there has been research on studying the robustness of Bayesian DNNs. Besides exploring the probabilistic robustness and safety measures of such models [48], [49], attacks have been adapted [50], [51] to evaluate the robustness of these models in practice. While Bayesian models are often considered to be more robust [52], [53], adversarial training has also been proposed for further safeguarding them, as can be seen in the work by Liu et al. [50]. However, these studies have not specifically focused on adversarial transferability as we do in this paper."
        },
        {
            "heading": "3 BAYESIAN ATTACK FOR IMPROVED TRANSFERABILITY",
            "text": "An intuition for improving the transferability of adversarial examples suggests improving the diversity during backpropagation. As an extension of [1], this paper considers model diversity and input diversity jointly."
        },
        {
            "heading": "3.1 Generate Adversarial Examples via Bayesian Modeling",
            "text": "Bayesian learning aims to discover a distribution of likely models rather than a single deterministic model. Let D = {(xi, yi)}Ni=1 denote a training set. Bayesian inference incorporates a prior belief about the parameters w through the prior distribution p(w) and updates this belief after observing the data D using Bayes\u2019 theorem, resulting in the posterior distribution: p(w|D) \u221d p(D|w)p(w). For a new\ninput x, the predictive distribution of its class label y is given by the Bayesian model averaging , i.e.,\np(y|x,D) = \u222b w p(y|x,w)p(w|D)dw, (1)\nwhere p(y|x,w) is the conditional probability, obtained from the DNN output followed by a softmax function.\nTo perform attack on such a Bayesian model, a straightforward idea is to minimize the probability of the true class for a given input, as in [1]:\nargmin \u2225\u2206x\u2225p\u2264\u03f5\np(y|x+\u2206x,D)\n= argmin \u2225\u2206x\u2225p\u2264\u03f5 \u222b w p(y|x+\u2206x,w)p(w|D)dw.\n(2)\nAn iterative optimizer (e.g., I-FGSM) is often applied, and at the t-the iteration, it seeks \u2206\u0303x to minimize\u222b\nw p(y|x+ \u2206\u0302xt + \u2206\u0303x,w)p(w|D)dw (3)\nwhile ensuring \u2225\u2206\u0302xt + \u2206\u0303x\u2225p \u2264 \u03f5. \u2206\u0302x1 = 0, and for t > 1, \u2206\u0302xt is the sum of all perturbations accumulated over the previous t \u2212 1 iterations. Optimizing Eq. (2) or minimizing Eq. (3) can be regarded as generating adversarial examples that could succeed on a distribution of models, and it has been proved effective in our previous work [1].\nIn Eqs. (2) and (3), Bayesian model averaging is used to predict the label of a deterministic model input. Such perturbation optimization processes solely consider the diversity of models, while the diversity of model inputs is overlooked. Nevertheless, considering that different DNN models may be equipped with different pre-processing operations, given the same benign input (e.g., a clean image), x can be different for different models after their specific preprocessing steps. Moreover, even given the same x (which is the pre-processed model input), different models obtain different \u2206\u0302xt. Therefore, introducing input diversity into Eqs. (2) and (3) may also be beneficial to the transferability of generated adversarial examples.\nTo incorporate such diversity, we simply introduce some randomness to the model input by rewriting Eq. (3) as:\u222b\nw,e p(y|x+ \u2206\u0302xt + e+ \u2206\u0303x,w)p(w|D)p(e)dwde. (4)\nHere the randomness term e is added linearly to the input x and/or accumulated perturbations \u2206\u0302xt. We can also introduce more complex modifications, such as by using a Gaussian filter g(x, \u2206\u0302xt, e) with a random standard deviation [54]; yet, for simplicity, we discuss the linear case in this paper. Due to the very large number of parameters, it is intractable to perform exact inference using Eq. (4). Instead we adopt the Monte Carlo sampling to approximate the integral, where a set of M models, each parameterized by wj , are sampled from the posterior p(w|D) and S inputs are sampled from p(e). The optimization problem can then be cast to maximizing\n1\nMS M\u2211 j=1 S\u2211 k=1 L(x+ \u2206\u0302xt + ek + \u2206\u0303x, y,wj),\ns. t.wj \u223c p(w|D), ek \u223c p(e), (5)\nwhere L(\u00b7, \u00b7,wj) is a function evaluating the prediction loss of a DNN model parameterized by wj ."
        },
        {
            "heading": "3.2 Construct a Posterior without Fine-tuning",
            "text": "Given any pre-trained DNN model whose parameters are w\u0302, we can simply obtain a posterior N (w\u0302, \u03c32I) without fine-tuning by assuming it as an isotropic Gaussian. \u03c3 is a positive constant for controlling the diversity of distribution. Similarly, we can consider ek \u223c N (0, \u03c32eI). Figure 1 compares the effectiveness of generating transferable attacks using Eq. (5) and the original implementation in our ICLR paper [1]. The experiment was conducted on ImageNet with ResNet-50 used as the substitute model; experiment details are deferred to Section 4.1. Apparently, introducing either model diversity or input diversity in a Bayesian manner could outperform the baseline I-FGSM on all victim models. More significantly, a joint diversification could further enhance adversarial transferability with a 17.05% increase in the average success rate compared with considering the model diversity alone [1]."
        },
        {
            "heading": "3.3 Obtain a more Suitable Posterior via Fine-tuning",
            "text": "In this subsection, we explain the optimization procedure of the posterior when fine-tuning the Bayesian model from a pre-trained model is possible. Following prior work, we consider a threat model in which fine-tuning can be performed on datasets collected for the same task as the victim models.\nAs in Section 3.2, we assume an isotropic Gaussian posterior N (w\u0302, \u03c32I); however, the mean vector w\u0302 is now considered as a trainable parameter. Optimization of the Bayesian model, or more specifically w\u0302, can be formulated as:\nmax w\u0302\n1\nN N\u2211 i=1 Ew\u223cN (w\u0302,\u03c32I),e\u223cN (0,\u03c32eI)p(yi|xi, e,w). (6)\nBy adopting Monte Carlo sampling, it can further be reformulated as:\nmin w\u0302\n1\nNMS N\u2211 i=1 M\u2211 j=1 S\u2211 k=1 L(xi + ei,k, yi, w\u0302 +\u2206wj),\ns. t. \u2206wj \u223c N (0, \u03c32I), ei,k \u223c N (0, \u03c32eI)\n(7)\nThe computational complexity of Eq. (7) is high, thus we focus on the worst-case performance in the distributions, whose loss bounds the objective in Eq. (7) from below. The optimization problem then becomes:\nmin w\u0302 max \u2206w,{ei}\n1\nN N\u2211 i=1 L(xi + ei, yi, w\u0302 +\u2206w),\ns. t.\u2206w \u223c N (0, \u03c32I) and p(\u2206w) \u2265 \u03b5, ei \u223c N (0, \u03c32eI) and p(ei) \u2265 \u03b5e,\n(8)\nwhere \u03b5 and \u03b5e control the confidence region of the Gaussian distributions.\nBy applying the first-order Taylor approximation to the objective function in Eq. (8), we can approximate the optimal solutions \u2206w\u2217 and e\u2217i to the inner-maximization problem using \u03bb\u03b5,\u03c3\u2207w\u0302 \u2211 i L(xi, yi, w\u0302)/\u2225\u2207w\u0302 \u2211 i L(xi, yi, w\u0302)\u2225 and \u03bb\u03b5e,\u03c3e\u2207xiL(xi, yi, w\u0302)/\u2225\u2207xiL(xi, yi, w\u0302)\u2225, respectively. \u03bb\u03b5,\u03c3 and \u03bb\u03b5e,\u03c3e are computed using the quantile function of the Gaussian distributions. Thereafter, the outer gradient for solving Eq. (8) is:\n\u2207w\u0302 1\nN N\u2211 i=1 L(xi, yi, w\u0302) +Hw\u0302,w\u0302\u2206w \u2217 +Hw\u0302,xie \u2217 i , (9)\nwhich involves second-order partial derivatives in the Hessian matrix, and it can be approximately calculated using the finite difference method. That said, we use\nHw\u0302,w\u0302\u2206w \u2217 \u2248 1\n\u03b3\n( \u2207w\u0302 1\nN N\u2211 i=1 L(xi, yi, w\u0302 + \u03b3\u2206w \u2217)\n\u2212\u2207w\u0302 1\nN N\u2211 i=1 L(xi, yi, w\u0302)\n) , (10)\nwhere \u03b3 is a small positive constant. Hw\u0302,xie \u2217 i is approximated similarly as\nHw\u0302,xie \u2217 i \u2248\n1\n\u03b3\n( \u2207w\u0302 1\nN N\u2211 i=1 L(xi + \u03b3e \u2217 i , yi, w\u0302)\n\u2212\u2207w\u0302 1\nN N\u2211 i=1 L(xi, yi, w\u0302)\n) .\n(11)\nBy introducing Hw\u0302,w\u0302\u2206w \u2217 and Hw\u0302,xie \u2217 i in Eq. (9), we encourage flat minima in the parameter space and the input space, respectively. The former is known to be beneficial to the generalization ability of DNNs [55], while the later has been paid little attention during training/fine-tuning.\nTo evaluate the effectiveness of fine-tuning, an experiment was carried out on ImageNet using ResNet-50 as the substitute model, and results are shown in Figure 2. It clearly suggests that fine-tuning leads to more significant adversarial transferability. More detailed comparison results are provided in Table 1."
        },
        {
            "heading": "3.4 Improved Distribution Modeling",
            "text": "In Sections 3.2 and 3.3, we have demonstrated the superiority of adopting the Bayesian formulation for generating transferable adversarial examples. However, it should be noted that our approach relies on a relatively strong assumption that the posterior follows an isotropic distribution. Taking one step further, we remove the assumption about the covariance matrix and try to learn it from data in this subsection.\nNumerous methods have been proposed to learn covariance matrices, but in this paper we opt for SWAG [45] due to its simplicity and scalability. SWAG offers an enhanced Gaussian approximation to the distribution of w and ei (for brevity, the subscript i will be dropped in this subsection). Specifically, we adopt the SWA solution [66] as the mean of w, and decompose the covariance matrix into a diagonal term, a low-rank term, and a scaled identity term, i.e., w \u223c N (wSWA,\u03a3w), where\n\u03a3w = \u03b1(\u03a3diag +\u03a3low\u2212rank) + \u03b2I. (12)\n\u03b1 \u2265 0 represent the scaling factor of SWAG for disassociating the learning rate of the covariance [45] and \u03b2 \u2265 0 controls the covariance matrix of the isotropic Gaussian distribution. wSWA, \u03a3diag, and \u03a3low\u2212rank are obtained after fine-tuning converges, thus we keep the fine-tuning loss and fine-tuning mechanism as in Section 3.3 even with the improved distribution modeling.\nFor multi-step attacks, e.g., in I-FGSM, e can be similarly obtained from the distribution of perturbations. Unlike for wSWA and \u03a3w, it does not necessarily require model finetuning to obtain the mean vector and covariance matrix of the distribution, as they model the randomness in model input instead of in model parameters and the learning dynamics of the adversarial inputs are obtained during IFGSM. Yet, for single-step attacks like FGSM, such a distribution modeling boils down to be equivalent to the isotropic case.\nWe compare performance of methods with and without such improved distribution, in single-step and multi-step attacks, in Table 1. Approximating the covariance matrix of parameters through SWAG improves the attack success\nrates on all victim models, leading to an average increase of 8.16% (from 44.22% to 52.38%) when using single-step attacks based on FGSM. This indicates that the more general distributional assumption of model parameters aligns better with the distribution of victim parameters in practice. A similar benefit is observed when applying the improved distribution modeling to the model input, where the average success rate increases from 45.09% to 56.39% in using IFGSM. The best performance is achieved when both improved posteriors and fine-tuning are adopted. However, note that with fine-tuning, we suggest not to adopt such improved distribution modeling of model input and model parameters simultaneously, as the two distributions will not be independent under such circumstances and there will be more hyper-parameters to be tuned. Following this suggestion, in Table 1 and subsequent experiments, when fine-tuning is performed, we only adopt SWAG for the model parameters and use the isotropic formulation for the model inputs."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We evaluate the effectiveness of our method by comparing it to recent state-of-the-arts in this section."
        },
        {
            "heading": "4.1 Experimental Settings",
            "text": "To be consistent with [1], we focused on untargeted \u2113\u221e attacks to study the adversarial transferability. All experiments were conducted on the same set of ImageNet [14] models collected from the timm repository [72], i.e., ResNet50 [58], VGG-19 [56], Inception v3 [57], ResNet-152 [58], DenseNet-121 [59], ConvNeXt-B [60], ViT-B [61], DeiT-B [62], Swin-B [63], BEiT-B [64], and MLP-Mixer-B [65]. These models are well-known and encompass CNN, transformer, and MLP architectures, making the experiments more comprehensive. We randomly sampled 5000 test images that can be correctly classified by all these models from the ImageNet validation set for evaluation. Since some victim models are different from those in [1], the test images are also different. The ResNet-50 was chosen as the substitute model, same as in [1]. For the experiments conducted on CIFAR-10, we adhered to the settings established in prior work [1].\nSpecifically, we performed attacks on VGG-19 [56], WRN28-10 [68], ResNeXt-29 [69], DenseNet-BC [59], PyramidNet272 [70], and GDAS [71], employing ResNet-18 [58] as the substitute model. The test images used are the entire test set of the CIFAR-10 dataset. We ran 50 iterations with a step size of 1/255 for all the iterative attacks.\nIn this paper, we set \u03b3 to a fixed constant, which is slightly different from the approach described in [1]. In [1], \u03b3 was set to be 0.1/\u2225\u2206w\u2217\u22252, dependent on the value of \u2225\u2206w\u2217\u22252. However, for our experiments on ImageNet and CIFAR-10, we set \u03b3 to 0.1 with \u03bb\u03b5,\u03c3 = 1, and \u03b3 to 0.5 with \u03bb\u03b5,\u03c3 = 0.2, respectively. These hyper-parameters match the ones actually used in [1]. We set \u03bb\u03b5e,\u03c3e to be 1 and 0.01 for ImageNet and CIFAR-10, respectively. We used a learning rate of 0.05, an SGD optimizer with a momentum of 0.9 and a weight decay of 0.0005, a batch size of 1024 for ImageNet and 128 for CIFAR-10, and a number of epochs of 10. We fixed \u03c3 = 0.006 and 0.012 for ImageNet and CIFAR-10, respectively. The \u03c3e was set to be 0.01 and 0.05 with and without fine-tuning, respectively. Considering Eq. (12), we used \u03b1 = 1 for the posterior distribution over model parameters. For the posterior distribution over model input, we set \u03b1 = 100 and 25 for ImageNet and CIFAR-10, respectively. Due to the negligible difference in success rates observed between using a diagonal matrix and a combination of diagonal and low-rank matrices as the covariance for the SWAG posterior, we opted for simplicity and consistently employed the diagonal matrix. We also set \u03b2 = 0 in the experiments for the same reason. For our current method with fine-tuning, we used SWAG to approximate the posterior over model parameters, while we used an isotropic Gaussian distribution for the posterior over model inputs. This choice was made because modeling the posterior over model inputs becomes challenging in situations where there is a high degree of randomness in the model parameters. We set M = 5 and S = 5 unless otherwise specified.\nFor compared competitors, we followed their official implementations. For LGV [28], we sampled 5 models from the collected model set at each iteration. By doing so, the performance will be better compared with sampling only one model as in LGV\u2019s default setting. For DRA [23], we tested it on ImageNet using the ResNet-50 model provided\nby the authors. All experiments were performed on an NVIDIA V100 GPU."
        },
        {
            "heading": "4.2 Comparison with State-of-the-arts",
            "text": "We compared our method with recent state-of-the-arts in Tables 2 and 3. A variety of methods were included in the comparison, including methods that adopt advanced optimizers (MI-FGSM [18] and NI-FGSM [19]), increase input diversity (TI-FGSM [24], DI2-FGSM [25], SI-FGSM [19], and Admix [26]), use advanced gradient computations (ILA [10], SGM [12], LinBP [13], NAA [67], and ILA++ [21]), and employ substitute model fine-tuning (LGV [28]). We also evaluated our Bayesian formulation that only models the diversity in substitute model parameters [1]; for clarity, it is referred to as \u201cBasicBayesian\u201d in this paper. The performance of all compared methods was evaluated in the task of attacking 10 victim models on ImageNet and 6 victim models on CIFAR-10.\nIt can be observed from the tables that our current method outperforms all these methods. It can achieve the best average success rate on ImageNet even without finetuning. When fine-tuning is possible, the average success rate improves remarkably, achieving 75.20% on ImageNet and 85.52% on CIFAR-10."
        },
        {
            "heading": "4.3 Combination with Other Methods",
            "text": "We would also like to mention that it is possible to combine our method with other attack methods to further enhance the transferability. In Table 4, we report the attack success rate of our method, in combination with MI-FGSM, DI2FGSM, SGM, LinBP, and ILA++. It can be seen that the transferability to all victim models gets improved. The best performance is obtained when combining our current method with DI2-FGSM, achieving an average success rate of 78.73% (which shows a +34.26% gain in comparison with the original performance of DI2-FGSM)."
        },
        {
            "heading": "4.4 Attacking Defensive Models",
            "text": "It is also of interest to evaluate the transferability of adversarial examples to robust models, and we compared the performance of competitive methods in this setting in Table 5. The victim models used in this study were collected from RobustBench [73]. All these models were trained using some sorts of advanced adversarial training [74], [75], [76], and they exhibit high robust accuracy against AutoAttack [77] on the official ImageNet validation set. These models included a robust ConvNeXt-B [78], a robust Swin-B [78], and a robust ViT-B-CvSt [79]. Following the setting in [1], two models from Bai et al.\u2019s open-source repository [80], namely a robust ResNet-50-GELU and a\nrobust DeiT-S, were also adopted as the robust victim. The tested robust ConvNeXt-B, robust Swin-B, and robust ViT-BCvSt show higher prediction accuracy (i.e., 55.82%, 56.16%, and 54.66%, respectively, against AutoAttack) than that of the robust ResNet-50-GELU and robust DeiT-S (35.51% and 35.50%, respectively).\nWe still used the ResNet-50 substitute model which was trained just as normal and not robust to adversarial examples at all. From Table 5, we can observe that our newly proposed method improves the transferability of adversarial examples to these defensive models, compared with the basic Bayesian formulation in [1].\nSince the objective of adversarial training is different from that of normal training, in the sense that the distribution of input is different, we suggest increasing \u03bb\u03b5e,\u03c3e to achieve better alignment between the distributions and to further enhance transferability. When we increase \u03bb\u03b5e,\u03c3e from 1 to 5 or even 10, we indeed obtain even better results in attacking robust models on ImageNet, as will be discussed in the ablation study.\nIn addition to adversarial training, we conducted experiments on robust models obtained through randomized smoothing [81], which is widely recognized as one of the leading techniques for achieving certified robustness. When attacking such a defensive model with a ResNet-50 architecture, we observe a substantial improvement compared to [1] (26.76%\u219285.04%)."
        },
        {
            "heading": "4.5 Ablation Study",
            "text": "We conduct a series of ablation experiments to study the impact of different hyper-parameters.\nThe effect of \u03bb\u03b5,\u03c3 and \u03bb\u03b5e,\u03c3e . When adopting finetuning, we have two main hyper-parameters that have an effect, namely \u03bb\u03b5,\u03c3 and \u03bb\u03b5e,\u03c3e . We conducted an empirical study to demonstrate how the performance of our method varies with the values of these two hyper-parameters on\nImageNet. We varied \u03bb\u03b5,\u03c3 from the set {0, 0.1, 0.2, 0.5, 1, 2} and varied \u03bb\u03b5e,\u03c3e from the set {0, 0.5, 1, 5, 10} for attacking models which are the same as in Table 2 and Table 5. The average success rates of attacking these normally trained models and robust models are given in Table 6. To achieve the best performance for each \u03bb\u03b5,\u03c3 and \u03bb\u03b5e,\u03c3e , we tuned the other hyper-parameters using 500 randomly selected images from the validation set. These images show no overlap with the 5000 test images.\nFrom the table, it can be observed that increasing the values of \u03bb\u03b5,\u03c3 and \u03bb\u03b5e,\u03c3e from 0 to 0.5 enhances the transferability of adversarial examples in attacking normally trained models. However, excessively large values of these hyperparameters can lead to inferior performance. The best performance of attacking normally trained models is achieved by setting \u03bb\u03b5,\u03c3 = 0.5 and \u03bb\u03b5e,\u03c3e = 1, which results in an average success rate of 75.60%. Considering the performance on attacking robust models, it can be observed that the average success rate peaks a larger value of \u03bb\u03b5e,\u03c3e . This is partially because that reducing the prediction loss of the perturbed inputs during fine-tuning in Eq. (8) resembles performing adversarial training, and larger \u03bb\u03b5e,\u03c3e implies making the substitute model robust in a larger neighborhood of benign inputs. A recent related method, namely DRA [23], also suggests that performing regularized fine-tuning before\nattacking is beneficial to attacking defensive models, and it achieves an average success rate of 62.03% and 17.68% in attacking normally trained models and adversarially trained models, respectively. By setting \u03bb\u03b5e,\u03c3e = 5, our method has an average success rate ranging from 65.84% to 67.51% in attacking normally trained models and ranging from 18.48% to 20.47% in attacking adversarially trained models, which outperforms DRA considerably.\nThe effect of M and S. Our method is proposed based on the principle that increasing the diversity of model parameters and model input can enhance the transferability of adversarial examples. In Figure 3, we conducted experiments to evaluate the transferability of adversarial examples crafted using different choices of the number of model parameters and input noise sampled at each attack iteration, i.e., M and S, in the cases of with and without fine-tuning. The average success rates were obtained by attacking the same victim models as in Table 2. The results apparently show that sampling more substitute models and more input noise can indeed enhance the transferability of adversarial examples, just as expected."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we aim at improving the transferability of adversarial examples. We have developed a Bayesian formulation for performing attacks, which can be equivalently regarded as generating adversarial examples on a set of infinitely many substitute models with input augmentations. We also advocated possible fine-tuning and advanced posterior approximations for improving the Bayesian model. Extensive experiments have been conducted on ImageNet and\nCIFAR-10 to demonstrate the effectiveness of the proposed method in generating transferable adversarial examples. It has been shown our method outperforms recent state-ofthe-arts by large margins in attacking more than 10 DNNs, including convolutional networks and vision transformers and MLPs, as well as in attacking defensive models. We have also showcased the compatibility of our method with existing transfer-based attack methods, leading to even more powerful adversarial transferability."
        }
    ],
    "title": "Improving Transferability of Adversarial Examples via Bayesian Attacks",
    "year": 2023
}