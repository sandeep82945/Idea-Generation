{
    "abstractText": "With the construction of the modern power system, power load forecasting is significant to keep the electric Internet of Things in operation. However, it usually needs to collect massive power load data on the server and may face the problem of privacy leakage of raw data. Federated learning can enhance the privacy of the raw power load data of clients by frequently transmitting model updates. Concerning the increasing communication burden of resource-heterogeneous clients resulting from frequent communication with the server, a communication-efficient federated learning algorithm based on Compressed Model Updates and Lazy uploAd (CMULA-FL) was proposed to reduce the communication cost. CMULA-FL also integrates the error compensation strategy to improve the model utility. First, the compression operator is used to compress the transmitted model updates, of which large norms are uploaded to reduce the communication cost of each epoch and transmission frequency. Second, by measuring the error of compression and lazy upload, the error is accumulated to the next epoch to improve the model utility. Finally, based on simulation experiments on the benchmark power load data, the results show that the communication cost decreases at least 60% with controlled loss of model prediction compared with baseline. INDEX TERMS Power load forecasting, federated learning, quantization, lazy upload, error compensation.",
    "authors": [
        {
            "affiliations": [],
            "name": "ZHENGXIONG MAO"
        },
        {
            "affiliations": [],
            "name": "HUI LI"
        },
        {
            "affiliations": [],
            "name": "ZUYUAN HUANG"
        },
        {
            "affiliations": [],
            "name": "CHUANXU YANG"
        },
        {
            "affiliations": [],
            "name": "YANAN LI"
        },
        {
            "affiliations": [],
            "name": "ZIHAO ZHOU"
        }
    ],
    "id": "SP:6a64585ee8924686eec2bf23fd3c416f0f229af4",
    "references": [
        {
            "authors": [
                "Y. Saleem",
                "N. Crespi",
                "M.H. Rehmani",
                "R. Copeland"
            ],
            "title": "Internet of things-aided smart grid: technologies, architectures, applications, prototypes, and future research directions",
            "venue": "IEEE Access, vol. 7, pp. 62 962\u201363 003, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "K. Chen",
                "K. Chen",
                "Q. Wang",
                "Z. He",
                "J. Hu",
                "J. He"
            ],
            "title": "Short-term load forecasting with deep residual networks",
            "venue": "IEEE Transactions on Smart Grid, vol. 10, no. 4, pp. 3943\u20133952, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "A. Prakash",
                "A. Kumar",
                "A. Kaushal",
                "K. Namrata",
                "N. Kumar"
            ],
            "title": "Load forecasting and analysis of power scenario in bihar using time series prediction and machine learning",
            "venue": "Smart Energy and Advancement in Power Technologies. Springer, 2023, pp. 851\u2013860.",
            "year": 2023
        },
        {
            "authors": [
                "B.A.S. Oliveira",
                "A.P.D.F. Neto",
                "R.M.A. Fernandino",
                "R.F. Carvalho",
                "A.L. Fernandes",
                "F.G. Guimaraes"
            ],
            "title": "Automated monitoring of construction sites of electric power substations using deep learning",
            "venue": "IEEE Access, vol. 9, pp. 19 195\u201319 207, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Chen",
                "J. Xiang",
                "P.-O. Bagnaninchi",
                "Y. Yang"
            ],
            "title": "Mmv-net: A multiple measurement vector network for multifrequency electrical impedance tomography",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Q. Wang",
                "H. Zhang",
                "X. Li",
                "X. Duan",
                "J. Wang",
                "R. Zhang",
                "H. Zhang",
                "Y. Ma",
                "H. Wang",
                "J. Jia"
            ],
            "title": "Error-constraint deep learning scheme for electrical impedance tomography (eit)",
            "venue": "IEEE Transactions on Instrumentation and Measurement, vol. 71, pp. 1\u201311, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "B. McMahan",
                "E. Moore",
                "D. Ramage",
                "S. Hampson",
                "B.A. y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "Artificial intelligence and statistics. PMLR, 2017, pp. 1273\u20131282.",
            "year": 2017
        },
        {
            "authors": [
                "X. Ren",
                "C.-M. Yu",
                "W. Yu",
                "S. Yang",
                "X. Yang",
                "J.A. McCann",
                "S.Y. Philip"
            ],
            "title": "LoPub: high-dimensional crowdsourced data publication with local differential privacy",
            "venue": "IEEE Transactions on Information Forensics and Security, vol. 13, no. 9, pp. 2151\u20132166, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Q. Yang",
                "Y. Liu",
                "Y. Cheng",
                "Y. Kang",
                "T. Chen",
                "H. Yu"
            ],
            "title": "Federated learning",
            "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning, vol. 13, no. 3, pp. 1\u2013207, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Zhou",
                "Y. Li",
                "X. Ren",
                "S. Yang"
            ],
            "title": "Towards efficient and stable k-asynchronous federated learning with unbounded stale gradients on non-iid data",
            "venue": "IEEE Transactions on Parallel and Distributed Systems, vol. 33, no. 12, pp. 3291\u20133305, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "C. Zhao",
                "X. Sun",
                "S. Yang",
                "X. Ren",
                "P. Zhao",
                "J. McCann"
            ],
            "title": "Exploration across small silos: Federated few-shot learning on network edge",
            "venue": "IEEE Network, vol. 36, no. 1, pp. 159\u2013165, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "X. Sun",
                "S. Yang",
                "C. Zhao"
            ],
            "title": "Lightweight industrial image classifier based on federated few-shot learning",
            "venue": "IEEE Transactions on Industrial Informatics, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "C. Wu",
                "F. Wu",
                "L. Lyu",
                "Y. Huang",
                "X. Xie"
            ],
            "title": "Communication-efficient federated learning via knowledge distillation",
            "venue": "Nature communications, vol. 13, no. 1, pp. 1\u20138, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Bernstein",
                "Y.-X. Wang",
                "K. Azizzadenesheli",
                "A. Anandkumar"
            ],
            "title": "signsgd: Compressed optimisation for non-convex problems",
            "venue": "International Conference on Machine Learning. PMLR, 2018, pp. 560\u2013569.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Lin",
                "S. Han",
                "H. Mao",
                "Y. Wang",
                "W.J. Dally"
            ],
            "title": "Deep gradient compression: Reducing the communication bandwidth for distributed training",
            "venue": "arXiv preprint arXiv:1712.01887, 2017.",
            "year": 1887
        },
        {
            "authors": [
                "S.U. Stich",
                "J.-B. Cordonnier",
                "M. Jaggi"
            ],
            "title": "Sparsified sgd with memory",
            "venue": "Advances in Neural Information Processing Systems, vol. 31, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J. Sun",
                "T. Chen",
                "G.B. Giannakis",
                "Q. Yang",
                "Z. Yang"
            ],
            "title": "Lazily aggregated quantized gradient innovation for communication-efficient federated learning",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "F. Haddadpour",
                "M.M. Kamani",
                "A. Mokhtari",
                "M. Mahdavi"
            ],
            "title": "Federated learning with compression: Unified analysis and sharp guarantees",
            "venue": "International Conference on Artificial Intelligence and Statistics. PMLR, 2021, pp. 2350\u20132358.",
            "year": 2021
        },
        {
            "authors": [
                "Q. Han",
                "S. Yang",
                "X. Ren",
                "P. Zhao",
                "C. Zhao",
                "Y. Wang"
            ],
            "title": "Pcfed: Privacy-enhanced and communication-efficient federated learning for industrial iots",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 18, no. 9, pp. 6181\u20136191, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "D. Jeong",
                "C. Park",
                "Y.M. Ko"
            ],
            "title": "Short-term electric load forecasting for buildings using logistic mixture vector autoregressive model with curve registration",
            "venue": "Applied Energy, vol. 282, p. 116249, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "G.F. Savari",
                "V. Krishnasamy",
                "J. Sathik",
                "Z.M. Ali",
                "S.H.A. Aleem"
            ],
            "title": "Internet of things based real-time electric vehicle load forecasting and charging station recommendation",
            "venue": "ISA transactions, vol. 97, pp. 431\u2013 447, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Muzaffar",
                "A. Afshari"
            ],
            "title": "Short-term load forecasts using lstm networks",
            "venue": "Energy Procedia, vol. 158, pp. 2922\u20132927, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A. Ta\u00efk",
                "S. Cherkaoui"
            ],
            "title": "Electrical load forecasting using edge computing and federated learning",
            "venue": "ICC 2020-2020 IEEE international conference on communications (ICC). IEEE, 2020, pp. 1\u20136.",
            "year": 2020
        },
        {
            "authors": [
                "J. Li",
                "Y. Ren",
                "S. Fang",
                "K. Li",
                "M. Sun"
            ],
            "title": "Federated learning-based ultra-short term load forecasting in power internet of things",
            "venue": "2020 IEEE International Conference on Energy Internet (ICEI). IEEE, 2020, pp. 63\u201368.",
            "year": 2020
        },
        {
            "authors": [
                "Y. He",
                "F. Luo",
                "G. Ranzi",
                "W. Kong"
            ],
            "title": "Short-term residential load forecasting based on federated learning and load clustering",
            "venue": "2021 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm). IEEE, 2021, pp. 77\u201382.",
            "year": 2021
        },
        {
            "authors": [
                "M. Savi",
                "F. Olivadese"
            ],
            "title": "Short-term energy consumption forecasting at the edge: A federated learning approach",
            "venue": "IEEE Access, vol. 9, pp. 95 949\u201395 969, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y.M. Saputra",
                "D.T. Hoang",
                "D.N. Nguyen",
                "E. Dutkiewicz",
                "M.D. Mueck",
                "S. Srikanteswara"
            ],
            "title": "Energy demand prediction with federated learning for electric vehicle networks",
            "venue": "2019 IEEE Global Communications Conference (GLOBECOM). IEEE, 2019, pp. 1\u20136.",
            "year": 2019
        },
        {
            "authors": [
                "M.N. Fekri",
                "K. Grolinger",
                "S. Mir"
            ],
            "title": "Distributed load forecasting using smart meter data: Federated learning with recurrent neural networks",
            "venue": "International Journal of Electrical Power & Energy Systems, vol. 137, p. 107669, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "N. Gholizadeh",
                "P. Musilek"
            ],
            "title": "Federated learning with hyperparameter-based clustering for electrical load forecasting",
            "venue": "Internet of Things, vol. 17, p. 100470, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S.U. Stich",
                "J.-B. Cordonnier",
                "M. Jaggi"
            ],
            "title": "Sparsified sgd with memory",
            "venue": "Advances in Neural Information Processing Systems, vol. 31, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "T. Ryffel",
                "A. Trask",
                "M. Dahl",
                "B. Wagner",
                "J. Mancuso",
                "D. Rueckert",
                "J. Passerat-Palmbach"
            ],
            "title": "A generic framework for privacy preserving deep learning",
            "venue": "arXiv preprint arXiv:1811.04017, 2018.",
            "year": 1811
        }
    ],
    "sections": [
        {
            "text": "INDEX TERMS Power load forecasting, federated learning, quantization, lazy upload, error compensation.\nI. INTRODUCTION\nW ITH the rapid increase of electricity demand, themodern power system should have the characteristics of digital enabling, flexible opening and high efficiency [1] better to enable electric Internet of Things (IoTs). Power load forecasting is the backbone of the construction of the modern power system. Accurate prediction of short-term power load data can improve prediction ability of emergencies and guarantee the growing power demand and the reliability of the electric IoTs [2, 3]. Moreover, power load forecasting can significantly improve the economic and social benefits of electric IoTs. Therefore, it is significant to forecast power loads accurately to accelerate the construction of the modern power system.\nRecently, data-driven machine learning [4\u20136] has been\nwidely used in power load forecasting and achieves marvelous prediction accuracy. Generally, the power grid company (the server) needs to collect massive power load data from enterprises or individuals (the clients), as shown in Fig. 1. However, it will lead to the risk of privacy disclosure in the process of collecting or storing power data [7, 8]. Moreover, with the rising privacy consciousness of people and the improvement of data security law, it is difficult for the power grid company to collect and analyze the power load data from clients [9].\nFederated learning (FL) [10\u201313], proposed by Google, can effectively protect the raw power load data privacy of all clients. Concretely, in the federated settings, the server coordinates massive clients to train a shared machinelearning model by frequently transmitting model pa-\nVOLUME 10, 2022 1\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nrameters instead of collecting the raw power load data on each client. Therefore, FL significantly ensures the privacy of the raw power data of each client and realizes the efficient mining of power load data.\nHowever, frequent communication between the server and clients will result in low communication efficiency [14\u201317]. On the one hand, it leads to high communication cost to transmit high-dimension deep learning models [16, 17] and the high communication cost makes clients reluctant to take part in the federated training process. On the other hand, it is time-intensive to upload highdimension models from clients to the server frequently [14], resulting in communication bottleneck in power grid data centers. Therefore, communication cost has become one of the most important bottlenecks of FL to ensure the privacy of clients\u2019 raw data.\nTo reduce the communication cost, the existing methods mainly consider the communication frequency and the communication cost of each epoch, such as periodic averaging [7, 18], lazy upload [17], gradient or model compression [14\u201316], sampling [19]. From the perspective of communication frequency, McMahan et al. [7] adopted the method of periodic averaging to reduce the communication cost. From the perspective of the communication cost in each epoch, Bernstein et al. [14] proposed SignSGD to compress the gradient, which significantly reduced the communication cost in the uploading stage of the model. Compression operators [15, 16] can also reduce the communication cost of each epoch by uploading compressed gradients with controllable error. To bring down the communication cost comprehensively, Sun et al. [17] proposed LAQ by combining lazy upload and model updating quantization to reduce the upload frequency and transmission cost in each epoch.\nHowever, there are still three core problems. Firstly, most existing methods are not systematic and only reduce the communication cost from one dimension. Secondly, the existing methods ignore the high communication cost in the downloading stage of the model. Finally, both model compression and lazy upload strategies will\ngenerate a large bias, which significantly reduces the model utility and even diverges the model.\nTo deal with the above three problems and improve communication efficiency, we proposed a communication-efficient power load forecasting algorithm (CMULA-FL) based on model bidirectional compression and lazy upload. Moreover, CMULA-FL utilizes the error compensation strategy to diminish the impacts of generated bias. Compared with the existing power load forecasting algorithm, we summarize our main contributions below:\n1) We propose CMULA-FL, a communicationefficient FL algorithm based on bidirectional compression and lazy upload, to reduce the communication cost systematically. The proposed CMULAFL reduces the communication cost from the orthometric aspects of the transmission cost of each epoch and the transmission frequency of the model update simultaneously. 2) We propose an error compensation strategy to deal with the reduction of the model utility caused by biases from model update compression and lazy upload. The error compensation strategy improves the model utility by accumulating the model update errors of the last epoch to the next epoch. 3) We deployed massive experiments on the power load dataset to verify the efficiency of CMULAFL. The experimental results validate that bidirectional compression and lazy upload can greatly reduce the communication cost, and the fusion with error compensation strategy can ensure the model utility.\nThe remaining parts of this paper are organized as follows. We first investigate the existing work related to power load forecasting and the communication-efficient algorithm in the federated scenarios in Section II. Then, the background knowledge involved in this paper is briefly introduced in Section III. The proposed CMULAFL algorithm is introduced in detail in Section IV. In Section V, the effectiveness of the CMULA-FL algorithm and the influence of some important parameters are verified by deploying experiments and analyzing experimental results in detail. Finally, we present the conclusion in Section VI."
        },
        {
            "heading": "II. RELATED WORK",
            "text": ""
        },
        {
            "heading": "A. POWER LOAD FORECASTING",
            "text": "There are various methods to forecast power load data, such as mathematical statistics theory [20, 21] and deep learning model [22]. Jeong et al. [20] put forward the logistic mixture vector autoregressive model based on curve registration to predict the short-term power load of buildings. By integrating clustering and prediction algorithms through the expectation-maximization algorithm, it significantly achieved a better prediction"
        },
        {
            "heading": "2 VOLUME 10, 2022",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\neffect. Similarly, Savari et a. [21] proposed a real-time load prediction algorithm for electric vehicles, which fulfilled the function of power dispatching management based on charging piles recommended by the state of charge. Muzaffar and Afshari [22] proposed a LSTMbased model, which has the capabilities to mine the longterm dependencies and extract useful information from power load data.\nHowever, those centralized scenarios to forecast power load requires massive power load data from clients, and there exists a potential risk of privacy disclosure in the collection and storage process [7]. Coupled with the increase of people\u2019s privacy awareness and data security laws, data barriers are gradually forming [9]. Therefore, the fusion with FL can not only protect the privacy of the original power load data, but also coordinate amounts of clients to train the shared model.\nB. FL-BASED POWER LOAD FORECASTING In order to ensure the privacy and security of clients\u2019 raw data, FL has been widely applied in the field of power load forecasting and has achieved remarkable results. Ta\u00efk et al. [23] applied FL to short-term forecasting to protect clients\u2019 data privacy and obtained desired effectiveness. To address the problem that the load variation patterns in each region cannot be accurately grasped in time due to high communication delay, Li et al. [24] proposed an ultra FL algorithm for short-term power load forecasting. Besides, He et al. [25] adopted the federated K-means clustering algorithm to divide clients into separate clusters based on historical power load data, and learns local personalized models within each cluster to enhance the privacy security of local data. The horizontal federated LSTM algorithm [23, 26] was employed to predict the power demand of each client to enhance security.\nHowever, in the FL scenarios, each client needs to communicate frequently with the grid power data center, incurring high communication cost that significantly increase the burden on resource heterogeneous clients [14\u2013 17]. Therefore, it is crucial to design a communicationefficient power load forecasting algorithm.\nC. COMMUNICATION-EFFICIENT FL For the purpose of improving communication efficiency, the existing methods mainly consider reducing the communication frequency and communication cost of each epoch. Saputra et al. [27] proposed model sharing to reduce the communication frequency, significantly bringing down the communication overhead while safeguarding the privacy of clients\u2019 raw data. From the perspective of the high communication overhead in federated power load forecasting, Fekri et al. [28] adopted periodic averaging to reduce the communication frequency and guarantee the forecasting accuracy in resource-constrained federated scenarios. Gholizadeh and Musilek [29] found\nthat there were a few abnormal nodes in FL training, which reduced the convergence speed. Accordingly, the abnormal node detection strategy was introduced, and the abnormal nodes were removed to ameliorate the convergence speed and reduce the communication overhead. In [14], the SignSGD method was adopted to compress the gradient in the uploading phase, which significantly reduced the communication cost of each epoch.\nHowever, the approaches still face three significant problems. Firstly, the communication cost is determined by communication frequency and the communication cost in each epoch, so the existing methods lack comprehensiveness and systematicity. Secondly, the existing methods do not alleviate the communication cost in the transmission stage of the model, or only consider the cost in the uploading stage, ignoring the high communication cost in the downloading stage of the model. Finally, reducing the communication cost poses large errors and significantly reduces the model utility. To cope with these three problems, a FL-based power load forecasting algorithm based on lazy upload and bidirectional model quantization is proposed from the two dimensions of communication frequency and communication cost of each epoch. Apart from this, the error compensation strategy is utilized to improve the model utility."
        },
        {
            "heading": "III. PRELIMINARY",
            "text": "This section presents the optimization method and the basic framework of the proposed algorithm. First, Section III-A introduces the stochastic gradient descent method. Then, Section III-B describes the workflow of FL and serves it as the basic framework of the proposed algorithm."
        },
        {
            "heading": "A. STOCHASTIC OPTIMIZATION",
            "text": "In general, the objective of machine learning is to solve the following optimization problem.\nminwF(w) = 1\nN \u2211 \u03bei\u2208D f(w, \u03bei), (1)\nwhere D and f(w, \u03bei) represent the training set and the i-th loss respectively. The mini-batch stochastic gradient descent method can perform as a solver for this optimization problem and reduce the consumption of computing resources at the same time. The iterative updating rule is as follows.\nwj+1 = wj \u2212 \u03b7j |Dj| \u2211 \u03bej,i\u2208Dj \u2207f(wj, \u03bej,i), (2)\nwhereDj \u2286 D is the mini-batch data in the j-th iteration, and \u03b7j is the learning rate."
        },
        {
            "heading": "B. FEDERATED LEARNING",
            "text": "In order to ensure the clients\u2019 raw data privacy, Google proposed the FL algorithm [7]. We take a FL scenario\nVOLUME 10, 2022 3\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nwith a central server and K clients as an example to describe the system model of FL in detail. Before federated training, the data of multiple clients are not exactly overlapping or mutually exclusive. Therefore, overlapping data needs to be filtered out to ensure fairness. First, the server uses the encryption algorithm to align the data of clients and confirms the common data among clients without disclosing the raw data. Then, based on the requirements in the application scenarios, a specific portion of the clients are screened for modeling. Once the target clients are identified, the FL training process begins. The training process of FL mainly is composed of the following four steps.\n1) The server initializes some basic parameters, such as the model parameters, learning rate and so on, then broadcasts them to each client. 2) All clients train the received model separately on local data sets and calculate gradients or model parameters as intermediate results. 3) Clients meeting the upload criteria send intermediate results to the server. 4) The server receives and aggregates the gradients or model parameters. Then, the server broadcasts the updated model parameters individually to all clients.\nIterate steps 2-4 until the test loss value is less than the set threshold or a preset number of iterations is reached.\nDuring the FL training process, the raw data for each clients is always stored locally, which protects the privacy and security of the raw data. Therefore, FL greatly reduces the risk of privacy disclosure of the raw data.\nIV. CMULA-FL To lower the communication cost of FL-based power load forecasting, we proposed a communication-efficient FL algorithm based on Compressed Model Updates and Lazy uploAd (CMULA-FL). CMULA-FL can not only reduce the communication cost effectively, but also couple the error compensation strategy to ensure the model utility. The basic idea of CMULA-FL is described in Section IV-A. Based on these ideas, the framework and details of CMULA-FL algorithm are presented in Section IV-B.\nA. MAIN IDEA The communication cost of the whole federated training process is mainly determined by the transmission bits in one epoch and transmission frequency. Therefore, the proposed CMULA-FL algorithm reduces the communication cost from these two orthometric dimensions. From the perspective of the communication cost in one epoch, model update of each client will be compressed before transmitting to the server. Similarly, the aggregated\nAlgorithm 1 CMULA-FL: Server side. Input: Compression operator Q(\u00b7), number of all clients\nK. Output: Optimal model parameter w\u2217. 1: Initialize the model parameters, model update, and\nglobal error. 2: Quantize the model update and compute the error\nof quantization. 3: Broadcast the quantized model update. 4: while the preset stopping condition is not achieved\ndo 5: for i = 1\u2192 K do 6: The i-th client computes and uploads the\nquantized model update. 7: end for 8: Receive and aggregate the model updates. 9: Accumulate the error of j\u2212 1-th epoch.\n10: Quantize the model update and compute the quantization error. 11: Update the global model and broadcast the quantized model update. 12: end while\nmodel update on the server will also be compressed before broadcasting to the clients. From the aspect of communication frequency, the lazy upload strategy is adopted by only selecting part clients to upload model updates, because some clients have inconspicuous changes in model updates and make fewer contributions to the process of aggregation. Therefore, The client can upload the model update until the accumulated model changes are greater than a threshold.\nHowever, it will incur an obvious bias, which can reduce the model utility. The bias comes from compressed transmission parameters and lazy upload. With the increasing of compressed information and the lazy upload threshold, the bias will significantly reduce the model utility and even diverge the model. In order to ensure the model effectiveness, the proposed CMULA-FL algorithm adopts the error compensation to reduce the influence of errors by accumulating the errors generated in the current epoch to the next epoch."
        },
        {
            "heading": "B. DETAILED DESCRIPTION",
            "text": "Based on the above basic ideas, an efficient CMULA-FL algorithm is proposed in this paper to forecast power load data safely and efficiently. The pseudo-code of CMULA-FL algorithm is mainly composed of two parts, including the server side (Algorithm 1) and the client side (Algorithm 2). Fig. 2 shown the workflow of the proposed CMULA-FL. It mainly consists of three parts, namely the quantization of model update (\u00ae and \u00b3) and error compensation (\u00af and \u00b2) on the server and the clients, and lazy upload (\u00b0) on the clients.\nOn the server side (\u00b1-\u00b3), as shown in Algorithm 1,"
        },
        {
            "heading": "4 VOLUME 10, 2022",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nLazy\nUpload\nServer\nQuantization\nModel\nUpdate\nQuantized\nmodel updateError\n\u2026\nClient 1\nClient 2\nClient K\nAggregation\n7\n6\n1\n3 2\n2\n4\n4\n4\n2\n5Model Update\nQuantized\nmodel updateError\nModel\nUpdate\nQuantized\nmodel updateError\n8\nQuantization3\nQuantization3\n4\nQuantization\nCompensation\nError\nFIGURE 2. Framework of CMULA-FL. CMULA-FL algorithm contains three main parts: model update quantization (\u00ae, \u00b3), error compensation (\u00af, \u00b2) on both clients and the server, and lazy upload (\u00b0) on the clients.\nthe server first initializes parameters such as model and model updates (line 1). Then, the server quantizes model updates and broadcasts them to all clients (lines 2-3). After all the satisfied clients upload their model updates (lines 5-7), the server aggregates all received quantized model updates (line 8) and accumulates the error of the last epoch (line 9). In order to reduce communication costs, the server quantizes the aggregated updates and calculates quantized errors (line 10). Finally, the shared model is updated and the quantized model update is broadcast to clients (line 11). The server repeats the processes (lines 4-12) until the stopping criteria is satisfied.\nOn the client side (\u00ac - \u00b0), as shown in Algorithm 2, each client first receives the model update and updates the local model (line 1). Then, each client updates the received model on local data for E iterations (line 2). After local iteration, the variation of local model is updated. To guarantee the model utility, each client\u2019 local model update is accumulated with the error of the last epoch (line 3). The corrected model update is quantized to reduce the communication cost (line 4). Finally, the lazy upload strategy is adopted to further reduce the communication cost (lines 5-11). That is, when the model update reaches the preset threshold or the client has not uploaded the model update for tmax epoch, clients will update the quantization error and upload the local quantized model update (line 6). If the upload conditions are not satisfied, the lazy upload error is updated (line 10) .\nIn the remaining parts, we will present the three core parts of CMULA-FL in Sections IV-B1-IV-B3 and model aggregation in Section IV-B4.\nAlgorithm 2 CMULA-FL: Client side. Input: learning rate \u03b7, lazy upload parameter . Output: Quantized model update \u2206w\u0303i,j. 1: Receive the model update and update local model\nparameter w0i,j. 2: Train the model on local data for E iterations. 3: Compute the model update and accumulate the\nerror erri,j\u22121. 4: Quantize the model update \u2206w\u0303i,j = Q(\u2206w\u0304i,j). 5: if ||\u2206w\u0303i,j|| \u2265 or ti \u2265 tmax then 6: Compute the quantization error erri,j and upload\nthe model update \u2206w\u0303i,j. 7: ti \u2190 1; 8: else 9: ti \u2190 ti + 1;\n10: Update the lazy upload error erri,j. 11: end if"
        },
        {
            "heading": "1) Model Bidirectional Quantization",
            "text": "Transmission of complete model parameters requires a large amount of communication cost, which can be significantly reduced by model compression. In this paper, the transmission models of the uploading and downloading stages are compressed by the \u03b3-compression operator. The \u03b3-compression operator is defined as follows\nDefinition IV.1. (\u03b3-compression operator) If the compression operator Q satisfies\n||x\u2212Q(x)||2 \u2264 (1\u2212 \u03b3)||x||2, (3)\nthen the compression operator Q is defined as \u03b3compression operator.\nCommon compression operators include Top-k sparsity operator [30] and 1-bit quantization [31] and so\nVOLUME 10, 2022 5\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\non. However, compared with these compression operators, the multi-bits quantization method can adjust the quantization bits flexibly and control the precision of transmitted model parameters to realize the controllable error. Therefore, we adopt the multi-bits quantization.\nBy utilizing the \u03b3-compression operator to compress the model update of the clients and server, CMULA-FL can effectively reduce the communication cost effectively in the downloading and uploading process with controllable error. The compression rules for the clients and the server are as follows:\n\u2206w\u0303i,j = Q(\u2206w\u0304i,j), \u2206w\u0303j = Q(\u2206wj),\n(4)\nwhere \u2206w\u0303i,j and \u2206w\u0303j are the model updates of the i-th client and server after quantization respectively."
        },
        {
            "heading": "2) Lazy Upload",
            "text": "If the change of a model update is small, it has few impacts on the aggregated model. Therefore, skipping this redundant gradient can not only reduce the communication cost of the client, but also have few impacts on the aggregated model. Before model training, the server initializes a small norm threshold . When the norm of the model update is smaller than the set threshold , the model update can be skipped.\nIf the model update obtained by the client continues to be small, the client will make little contribution to the federated process. When the client satisfies the uploading criteria, the accumulated model update will have a large staleness, which will result in a large model deviation. Therefore, it is necessary to ensure the freshness of the model updates. Then, we can set a maximum number of local iterations to tmax empirically. If the norm of the client\u2019s model update continues to be less than the set threshold for tmax epochs, the quantized model update will be uploaded to control the staleness of the model update. By setting a smaller and tmax, the model updates will be fresher and model utility can be improved."
        },
        {
            "heading": "3) Error Compensation",
            "text": "After the compressed quantization model is updated, there is a large quantization error between the original model update and the quantization results. With the increasing epochs, the cumulative effects will significantly reduce the model utility and even diverge the model. To improve the model utility, it is indispensable to adopt the method of error compensation for both clients and the server to decrease the impacts of compression error.\nFor each client, after training the model for E times locally, the error of the last epoch can be accumulated to the current model change to alleviate the impacts of the quantization error.\nerri,j\u22121 = \u2206w\u0303i,j\u22121 \u2212Q(\u2206w\u0304i,j\u22121), \u2206w\u0304i,j = \u2206wi,j + erri,j\u22121,\n(5)\nwhere erri,j\u22121 represents the quantization error generated by the i-th client in the j\u2212 1-th epoch.\nFor the server, the local model updates of clients are aggregated first, and then the results are added to the errors of the previous epoch to decrease the impacts of compressed model update,\nerrj\u22121 = \u2206w\u0303j\u22121 \u2212Q(\u2206wj\u22121), \u2206wj = Agg(\u2206w\u0303i,j) + errj\u22121,\n(6)\nwhere errj\u22121 represents the quantization error generated by the server in the j\u2212 1-th epoch.\nApart from the quantization error, the lazy upload can also generate a large bias. If the model updates of one client continue to be small and are abandoned directly, it will incur two problems. On the one hand, frequent skipping of model updates leads to extravagant computing resources. On the other hand, the skipped gradients make the aggregated model update be biased and decrease the model utility. Therefore, when the client\u2019s local model update carries little information, the model update can be accumulated to the next epoch with the quantization error. When the client is skipped, the error is given as\nerri,j = wEi,j \u2212 w0i,j + erri,j\u22121. (7)\nwhere w0i,j and w E i,j are the models of i-th client at the beginning and end of the local training process respectively."
        },
        {
            "heading": "4) Model Aggregation",
            "text": "After the model updates on all satisfied clients reach the server side, the server aggregates the model updates and accumulates the error errj\u22121. The aggregation and accumulation rules are as follows\n\u2206wj = 1\nSj \u2211 i\u2208Sj \u2206w\u0303i,j + errj\u22121, (8)\nwhere Sj defines the set of clients who upload the model updates."
        },
        {
            "heading": "V. EXPERIMENTS",
            "text": "To validate the performance of the proposed algorithm CMULA-FL, we deployed CMULA-FL under the PySyft framework [32]. The performance is measured from the perspective of training speed and communication efficiency. Section V-A presents the experimental setup, such as three existing comparison algorithms and power load datasets. Then, detailed experimental results are shown and analyzed to demonstrate the advantages of CMULA-FL in Section V-B."
        },
        {
            "heading": "A. EXPERIMENTAL SETUP",
            "text": ""
        },
        {
            "heading": "1) Datasets and Models",
            "text": "In the experiments, we made use of a benchmark power load dataset from the Chinese Society of Power En-"
        },
        {
            "heading": "6 VOLUME 10, 2022",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\ngineering1, which records the power load data of a region from 2009 to 2015. The power load data can be converted into a matrix X. However, due to the fact of improper operation and other human factors in the process of collecting data, there exist some bad data, bringing down the model utility. Therefore, it is significant to preprocess the dataset before the training model. The data is preprocessed in three main steps, namely data standardization, generation of time-series data, generation of training data and test data. The three steps will be introduced in detail as follows.\nData standardization. The power load data has the characteristic of periodicity. Therefore, the power load data is roughly similar between two adjacent periods. In this paper, we used some basic statistical metrics and an empirical threshold to judge the similarity of data in two adjacent periods and identify bad data. The details are as follows. \u2022 Divide the power load data into different groups (N\ngroups) in days; \u2022 Compute the mean value X\u0304q and variance \u03c32q of q-th\ngroup (q = 1, . . . ,N); \u2022 Pick out the bad data based on an empirical thresh-\nold q \u2208 [1, 1.5]. The n-th data Xn,q in group q is divided into a bad data if it satisfies\n|Xn,q \u2212 X\u0304q| > 3\u03c3q q.\n\u2022 Correct the bad data based on adjacent data,\nXn,q = (Xn\u22121,q + Xn+1,q)/2.\n\u2022 Normalize the power load data. Generation of time-series data. In order to deploy LSTM to train power load data, it is significant to generate time-series data and labels. In order to ensure the model utility, we generate time-series data through a sliding window, which is set to a larger number of 12.\nGeneration of training data and test data. The training set and test set are obtained by dividing the generated time-series data in a ratio of 4:1. Then, the training power load data was randomly assigned to each client, and each client keeps a similar amount of data.\nModel. For the sake of ensuring the model utility of the power load dataset, the neural network LSTM is adopted in this paper. The LSTM consists of two layers, each of which contains an input gate, a forgetting gate and an output gate. The outputs of the first layer are the input parameters for the input gate in the second layer. The activation function for each layer set as the Softmax function.\nParameter settings. In our experiments, is set to 0.0001 empirically to ensure sixty percent of the received mode updates in each epoch can participate in the aggregation. To keep the model updates fresh, tmax is set to 10. Other default parameters are as listed in Table 1.\n1http://shumo.nedu.edu.cn."
        },
        {
            "heading": "2) Comparison Algorithms",
            "text": "The proposed CMULA-FL was compared with two communication-efficient algorithms in our experiments, including a Lazily Aggregated Quantized gradient approach (LAQ, [17]) and FL based on Periodic Averaging and Quantization (FedPAQ, [33]). FedAvg [7], one of the most classical FL algorithms, is set as the baseline. A brief introduction of the two comparison algorithms is as follows.\nLAQ. To improve communication efficiency, LAQ quantizes gradients as well as skips some quantized gradients with less information by utilizing previous gradients, which can simultaneously save communication bits and rounds without sacrificing the desired convergence guarantees.\nFedPAQ. To reduce the communication cost, FedPAQ adopts the low-precision quantizer to decrease the communication cost of one epoch. Apart from that, FedPAQ utilizes partial client participation and periodic averaging to reduce the communication frequency."
        },
        {
            "heading": "3) Metrics",
            "text": "We compared CMULA-FL with the above algorithms from the aspects of training speed and communication efficiency. The training speed is directly measured by the number of iterations before convergence which is judged based on training loss. Communication efficiency is measured by the transmission bits during the training process. We describe the metrics of training loss and transmission bits as follows.\nTraining loss. We use Mean Squared Error (MSE) as the metrics of the performance on the test data. It reflects the magnitude of the difference between the true label and the predicted value of the model and is calculated as\nMSE = 1\nm m\u2211 i=1 (yi \u2212 f(xi))2, (9)\nwhere m is the number of test data, yi and f(xi) are the true and predicted value respectively. MSE can reflect two aspects of model performance. On the one hand, the declining rate of MSE on the test dataset reflects the convergence speed. On the other hand, a smaller MSE means that the corresponding algorithm predicts more accurately.\nVOLUME 10, 2022 7\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nCommunication cost. The communication cost is measured by the bits of transmission parameters uploaded by all clients and the rule is given as\nCtot = J\u2211\nj=1 \u2211 i\u2208Sj Mnum\u2211 k=1 C(\u2206w\u0303i,j,k), (10)\nwhere C(\u2206w\u0303i,j,k) defines the bits of \u2206w\u0303i,j,k, the k-th model parameter of model \u2206w\u0303i,j.\nB. EXPERIMENTAL RESULTS AND ANALYSIS To validate the performance of CMULA-FL and explore the impacts of some important parameters on the model utility, four groups of experiments were conducted from two aspects: the communication cost and the loss value. The specific settings of each group are as follows:\n1) The first group counted up the communication cost of the model in the training phase under different algorithms and quantization bits. 2) In the second group, CMULA-FL was compared with the baseline and two comparison algorithms to verify the model utility. 3) In the third group, some ablation experiments and comparison experiments were presented to verify the function of some components of CMULA-FL. In the ablation experiments, the error compensation strategy was removed from CMULA-FL to explore the impacts of error compensation and we rename it CMULA-FL (No Compensation). In the comparison experiments, CMULA-FL was compared with FedAvg and the centralized settings with only one client (One-Client). 4) In the fourth group, various quantization bits were set to explore the influence of quantization on the model utility."
        },
        {
            "heading": "1) Impacts of Quantization Bits on Communication Cost",
            "text": "In order to explore the communication cost for different quantization bits and algorithms, we dynamically set the quantization bits for CMULA-FL and varied the algorithms to verify the effectiveness of CMULA-FL. To ensure fairness, the number of training epochs was 100 and Table 1 shows the settings of other parameters. Table 2 shows the communication cost for different algorithms and different quantization bits.\nAs can be seen from Table 2, the larger quantization bits will result in a larger communication cost for LAQ and CMULA-FL, which indicates that the bits of transmission parameters are positively correlated with the communication cost. However, the relation is not linear due to the lazy upload strategy. Table 2 also shows that CMULA-FL can significantly reduce the communication cost compared with the baseline, FedAvg. Compared with LAQ and FedPAQ, the proposed CMULAFL demonstrates a superior performance in terms of communication cost since CMULA-FL compresses the\nbidirectional transmission parameters. Apart from that, the communication cost of CMULA-FL decreases more rapidly with the diminution of quantization bits than those of LAQ due to the fact that LAQ only compresses the model during the uploading stages, whereas CMULA-FL also reduces the communication cost in the downloading stages.\nTo verify the model utility, CMULA-FL was compared with the baseline, FedAvg and two comparison algorithms. All algorithms adopted the same parameters as shown in Table 1 and the quantization bits for CMULAFL and LAQ were set to 8. Fig. 3 shows the relation between MSE and epochs for different algorithms.\nAs shown in Fig. 3, FedAvg has the smallest MSE value and the MSE of CMULA-FL is a little larger within acceptable ranges. However, LAQ and FedPAQ have larger biases and fluctuate heavily since LAQ uses stale gradients while FedPAQ ignores the error caused by model compression. It shows that CMULA-FL can reduce the communication cost without sacrificing much prediction accuracy by utilizing the error compensation strategy.\nBy combining the analysis of Table 2, Fig. 3 also shows that CMULA-FL can not only reduce the communication cost effectively by compressing bidirectional"
        },
        {
            "heading": "8 VOLUME 10, 2022",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nmodel update and uploading model update lazily, but also guarantee the model utility by utilizing error compensation strategy."
        },
        {
            "heading": "2) Impacts of Different Components on MSE",
            "text": "We deployed some ablation experiments and comparison experiments to verify the function of some components of CMULA-FL. To ensure the fairness of the comparison, the initialized model parameters were set to the same values in different scenarios and the number of epochs was 100. Table 1 shows specific parameter settings. Fig. 4 shows the relation between MSE and epochs under different settings.\nAs shown in Fig. 4, FedAvg converges the fastest, while CMULA-FL has a slightly lower convergence speed. However, compared with CMULA-FL-NoCom, the performance of CMULA-FL has been significantly improved, which shows that compression and lazy upload will significantly reduce the model utility and the impacts can be decreased by error compensation strategy.\nFig. 4 also shows that the model trained with a single client converges the slowest. It is due to the fact that all clients can jointly train the model and more data can significantly improve the model generalization. The FL scenarios are similar to aggregating all the data from clients, which enhances the data volume and thus improves the convergence speed."
        },
        {
            "heading": "3) Impacts of Quantization Bits on MSE",
            "text": "To explore the impacts of different quantization bits on CMULA-FL, we dynamically set quantization bits to observe the changes of MSE. To ensure fairness, the number of training epochs was 100, and Table 1 shows the settings of other parameters. Fig. 5 shows the trends\nof MSE corresponding to different quantization bits. As shown in Fig. 5, a larger number of quantization bits results in converging faster. If the quantization bits are set larger, the transmission model will be more accurate. Therefore, the model will be updated in a righter direction and converge faster. It shows that on the premise of sufficient communication resources, the larger quantization bits can lead to a higher prediction accuracy of the model."
        },
        {
            "heading": "VI. CONCLUSION",
            "text": "We propose communication-efficient CMULA-FL to forecast power load data on enterprises or individuals efficiently and safely to better enable electric IoTs. Based on bidirectional quantization and lazy upload, CMULAFL effectively reduces the communication cost of the federated prediction process. Apart from that, CMULA-FL also utilizes the error compensation strategy to ensure prediction accuracy and convergence speed. Firstly, the model updates of the clients and server are quantized to bring down the communication cost. CMULA-FL also adopts lazy upload to further improve communication efficiency. Secondly, the deep fusion with error compensation strategy can effectively reduce the impacts caused by quantization errors and lazy upload errors. Finally, the model effectiveness of CMULA-FL is verified by deploying massive experiments in different scenes and parameter settings. However, the proposed CMULA-FL algorithm is still imperfect and needs to be improved. For example, CMULA-FL has many hyper-parameters and it is difficult to set hyper-parameters based on experiences. In the subsequent work, the influence of hyper-parameters will be analyzed theoretically, such as the local update change threshold and time threshold. By analyzing the impacts of different parameters, it will be more convenient to deploy CMULA-FL.\nVOLUME 10, 2022 9\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nREFERENCES [1] Y. Saleem, N. Crespi, M. H. Rehmani, and R. Copeland,\n\u201cInternet of things-aided smart grid: technologies, architectures, applications, prototypes, and future research directions,\u201d IEEE Access, vol. 7, pp. 62 962\u201363 003, 2019. [2] K. Chen, K. Chen, Q. Wang, Z. He, J. Hu, and J. He, \u201cShort-term load forecasting with deep residual networks,\u201d IEEE Transactions on Smart Grid, vol. 10, no. 4, pp. 3943\u20133952, 2018. [3] A. Prakash, A. Kumar, A. Kaushal, K. Namrata, and N. Kumar, \u201cLoad forecasting and analysis of power scenario in bihar using time series prediction and machine learning,\u201d in Smart Energy and Advancement in Power Technologies. Springer, 2023, pp. 851\u2013860. [4] B. A. S. Oliveira, A. P. D. F. Neto, R. M. A. Fernandino, R. F. Carvalho, A. L. Fernandes, and F. G. Guimaraes, \u201cAutomated monitoring of construction sites of electric power substations using deep learning,\u201d IEEE Access, vol. 9, pp. 19 195\u201319 207, 2021. [5] Z. Chen, J. Xiang, P.-O. Bagnaninchi, and Y. Yang, \u201cMmv-net: A multiple measurement vector network for multifrequency electrical impedance tomography,\u201d IEEE Transactions on Neural Networks and Learning Systems, 2022. [6] Q. Wang, H. Zhang, X. Li, X. Duan, J. Wang, R. Zhang, H. Zhang, Y. Ma, H. Wang, and J. Jia, \u201cError-constraint deep learning scheme for electrical impedance tomography (eit),\u201d IEEE Transactions on Instrumentation and Measurement, vol. 71, pp. 1\u201311, 2021. [7] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, \u201cCommunication-efficient learning of deep networks from decentralized data,\u201d in Artificial intelligence and statistics. PMLR, 2017, pp. 1273\u20131282. [8] X. Ren, C.-M. Yu, W. Yu, S. Yang, X. Yang, J. A. McCann, and S. Y. Philip, \u201cLoPub: high-dimensional crowdsourced data publication with local differential privacy,\u201d IEEE Transactions on Information Forensics and Security, vol. 13, no. 9, pp. 2151\u20132166, 2018. [9] Q. Yang, Y. Liu, Y. Cheng, Y. Kang, T. Chen, and H. Yu, \u201cFederated learning,\u201d Synthesis Lectures on Artificial Intelligence and Machine Learning, vol. 13, no. 3, pp. 1\u2013207, 2019. [10] Z. Zhou, Y. Li, X. Ren, and S. Yang, \u201cTowards efficient and stable k-asynchronous federated learning with unbounded stale gradients on non-iid data,\u201d IEEE Transactions on Parallel and Distributed Systems, vol. 33, no. 12, pp. 3291\u20133305, 2022. [11] C. Zhao, X. Sun, S. Yang, X. Ren, P. Zhao, and J. McCann, \u201cExploration across small silos: Federated few-shot learning on network edge,\u201d IEEE Network, vol. 36, no. 1, pp. 159\u2013165, 2021. [12] X. Sun, S. Yang, and C. Zhao, \u201cLightweight industrial image classifier based on federated few-shot learning,\u201d IEEE Transactions on Industrial Informatics, 2022. [13] C. Wu, F. Wu, L. Lyu, Y. Huang, and X. Xie, \u201cCommunication-efficient federated learning via knowledge distillation,\u201d Nature communications, vol. 13, no. 1, pp. 1\u20138, 2022. [14] J. Bernstein, Y.-X. Wang, K. Azizzadenesheli, and A. Anandkumar, \u201csignsgd: Compressed optimisation for non-convex problems,\u201d in International Conference on Machine Learning. PMLR, 2018, pp. 560\u2013569. [15] Y. Lin, S. Han, H. Mao, Y. Wang, and W. J. Dally, \u201cDeep gradient compression: Reducing the communication bandwidth for distributed training,\u201d arXiv preprint arXiv:1712.01887, 2017.\n[16] S. U. Stich, J.-B. Cordonnier, and M. Jaggi, \u201cSparsified sgd with memory,\u201d Advances in Neural Information Processing Systems, vol. 31, 2018. [17] J. Sun, T. Chen, G. B. Giannakis, Q. Yang, and Z. Yang, \u201cLazily aggregated quantized gradient innovation for communication-efficient federated learning,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. [18] F. Haddadpour, M. M. Kamani, A. Mokhtari, and M. Mahdavi, \u201cFederated learning with compression: Unified analysis and sharp guarantees,\u201d in International Conference on Artificial Intelligence and Statistics. PMLR, 2021, pp. 2350\u20132358. [19] Q. Han, S. Yang, X. Ren, P. Zhao, C. Zhao, and Y. Wang, \u201cPcfed: Privacy-enhanced and communication-efficient federated learning for industrial iots,\u201d IEEE Transactions on Industrial Informatics, vol. 18, no. 9, pp. 6181\u20136191, 2022. [20] D. Jeong, C. Park, and Y. M. Ko, \u201cShort-term electric load forecasting for buildings using logistic mixture vector autoregressive model with curve registration,\u201d Applied Energy, vol. 282, p. 116249, 2021. [21] G. F. Savari, V. Krishnasamy, J. Sathik, Z. M. Ali, and S. H. A. Aleem, \u201cInternet of things based real-time electric vehicle load forecasting and charging station recommendation,\u201d ISA transactions, vol. 97, pp. 431\u2013 447, 2020. [22] S. Muzaffar and A. Afshari, \u201cShort-term load forecasts using lstm networks,\u201d Energy Procedia, vol. 158, pp. 2922\u20132927, 2019. [23] A. Ta\u00efk and S. Cherkaoui, \u201cElectrical load forecasting using edge computing and federated learning,\u201d in ICC 2020-2020 IEEE international conference on communications (ICC). IEEE, 2020, pp. 1\u20136. [24] J. Li, Y. Ren, S. Fang, K. Li, and M. Sun, \u201cFederated learning-based ultra-short term load forecasting in power internet of things,\u201d in 2020 IEEE International Conference on Energy Internet (ICEI). IEEE, 2020, pp. 63\u201368. [25] Y. He, F. Luo, G. Ranzi, and W. Kong, \u201cShort-term residential load forecasting based on federated learning and load clustering,\u201d in 2021 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm). IEEE, 2021, pp. 77\u201382. [26] M. Savi and F. Olivadese, \u201cShort-term energy consumption forecasting at the edge: A federated learning approach,\u201d IEEE Access, vol. 9, pp. 95 949\u201395 969, 2021. [27] Y. M. Saputra, D. T. Hoang, D. N. Nguyen, E. Dutkiewicz, M. D. Mueck, and S. Srikanteswara, \u201cEnergy demand prediction with federated learning for electric vehicle networks,\u201d in 2019 IEEE Global Communications Conference (GLOBECOM). IEEE, 2019, pp. 1\u20136. [28] M. N. Fekri, K. Grolinger, and S. Mir, \u201cDistributed load forecasting using smart meter data: Federated learning with recurrent neural networks,\u201d International Journal of Electrical Power & Energy Systems, vol. 137, p. 107669, 2022. [29] N. Gholizadeh and P. Musilek, \u201cFederated learning with hyperparameter-based clustering for electrical load forecasting,\u201d Internet of Things, vol. 17, p. 100470, 2022. [30] S. U. Stich, J.-B. Cordonnier, and M. Jaggi, \u201cSparsified sgd with memory,\u201d Advances in Neural Information Processing Systems, vol. 31, 2018. [31] M. Schl\u00fcter, M. D\u00f6rpinghaus, and G. P. Fettweis, \u201cBounds on phase, frequency, and timing synchroniza-"
        },
        {
            "heading": "10 VOLUME 10, 2022",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\ntion in fully digital receivers with 1-bit quantization and oversampling,\u201d IEEE Transactions on Communications, vol. 68, no. 10, pp. 6499\u20136513, 2020. [32] T. Ryffel, A. Trask, M. Dahl, B. Wagner, J. Mancuso, D. Rueckert, and J. Passerat-Palmbach, \u201cA generic framework for privacy preserving deep learning,\u201d arXiv preprint arXiv:1811.04017, 2018. [33] A. Reisizadeh, A. Mokhtari, H. Hassani, A. Jadbabaie, and R. Pedarsani, \u201cFedpaq: A communication-efficient federated learning method with periodic averaging and quantization,\u201d in International Conference on Artificial Intelligence and Statistics. PMLR, 2020, pp. 2021\u2013 2031.\nHUI LI was born in February 1991, male, Han nationality. His highest education is Master\u2019s degree, and he is currently working as a mid-level engineer in the Information Center of Yunnan Power Grid Co. His main research interests include application technology, data mining and analysis, data asset operations and other research work.\nZUYUAN HUANG received his M. S. degree from Yunnan University in 2021. He is currently work for the Information Center of Yunnan Power Grid Co as a manager of Data Resource Center. His main research interests are Big Data and AI.\nYUAN TIAN was born in 1989, male, Han nationality. His highest education is a master\u2019s degree, and he is currently working as a senior engineer in the Information Center of Yunnan Power Grid Co. His main research interests include Big Data and grid informatization.\nZHENGXIONG MAO was born in November 1972, male, Han nationality. His highest education is B.S, and he is currently working as a senior engineer in the Information Center of Yunnan Power Grid Co. His main research interests include power grid informatization, information operation and maintenance, information security and data mining and analysis.\nZIHAO ZHOU received his Bachelor degree from the school of mathematics and statistics at Xi\u2019an Jiaotong University of China in 2020. He is currently working towards the PhD degree in the School of Mathematics and Statistics at Xi\u2019an Jiaotong University. His research interests include federated learning and edge-cloud intelligence.\nYANAN LI received his Master and PhD degree from Henan Normal University of China in 2007 and Xi\u2019an Jiaotong University of China in 2022, respectively. He is currently working at Henan Polytechnic University. His research interests include differential privacy, machine learning, and federated learning.\nVOLUME 10, 2022 11\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
        }
    ],
    "title": "Communication-efficient Federated Learning for Power Load Forecasting in Electric IoTs",
    "year": 2023
}