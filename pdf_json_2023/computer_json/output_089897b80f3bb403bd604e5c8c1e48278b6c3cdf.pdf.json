{
    "abstractText": "Federated Learning (FL) is an innovative area of machine learning that enables different clients to collaboratively generate a shared model while preserving their data privacy. In a typical FL setting, a central model is updated by aggregating the clients\u2019 parameters of the respective artificial neural network. The aggregated parameters are then sent back to the clients. However, two main challenges are associated with the central aggregation approach. Firstly, most state-of-the-art strategies are not optimised to operate in the presence of certain types of non-iid (not independent and identically distributed) applications and datasets. Secondly, federated learning is vulnerable to various privacy and security concerns related to model inversion attacks, which can be used to access sensitive information from the training data. To address these issues, we propose a novel federated learning strategy FedNets based on ensemble learning. Instead of sharing the parameters of the clients over the network to update a single global model, our approach allows clients to have ensembles of diverse-lightweight models and collaborate by sharing ensemble members. FedNets utilises graph embedding theory to reduce the complexity of running Deep Neural Networks (DNNs) on resource-limited devices. Each Deep Neural Network (DNN) is treated as a graph, from which respective graph embeddings are generated and clustered to determine which part of the DNN should be shared with other clients. Our approach outperforms state-of-the-art FL algorithms such as Federated Averaging (Fed-Avg) and Adaptive Federated Optimisation (Fed-Yogi) in terms of accuracy; on the Federated CIFAR100 dataset (non-iid), FedNets demonstrates a remarkable 63% and 92% improvement in accuracy, respectively. Furthermore, FedNets does not compromise the client\u2019s privacy, as it is safeguarded by the design of the method. INDEX TERMS Federated Learning, Ensemble Learning, Convolutional Neural Networks, Graph Embedding, Affinity propagation, Non-IID datasets, Privacy.",
    "authors": [
        {
            "affiliations": [],
            "name": "BESHER ALHALABI"
        },
        {
            "affiliations": [],
            "name": "Shadi Basurra"
        },
        {
            "affiliations": [],
            "name": "MOHAMED MEDHAT GABER"
        }
    ],
    "id": "SP:aa11d9bf4153ae2bf6dac830db7f28ef6485f135",
    "references": [
        {
            "authors": [
                "Jeet Ghosh",
                "Gopinath Samanta",
                "Chinmay Chakraborty"
            ],
            "title": "Smart Health Care for Societies: An Insight into the Implantable and Wearable Devices for Remote Health Monitoring",
            "venue": "Green Technological Innovation for Sustainable Smart Societies,",
            "year": 2021
        },
        {
            "authors": [
                "Mehdi Mohammadi",
                "Ala Al-Fuqaha"
            ],
            "title": "Enabling Cognitive Smart Cities Using Big Data and Machine Learning: Approaches and Challenges",
            "venue": "IEEE Communications Magazine,",
            "year": 2018
        },
        {
            "authors": [
                "Andrew Kwok-Fai Lui",
                "Yin-Hei Chan",
                "Man-Fai Leung"
            ],
            "title": "Modelling of Destinations for Data-driven Pedestrian Trajectory Prediction in Public Buildings",
            "venue": "IEEE International Conference on Big Data (Big Data),",
            "year": 2021
        },
        {
            "authors": [
                "Andrew Kwok-Fai Lui",
                "Yin-Hei Chan",
                "Man-Fai Leung"
            ],
            "title": "Modelling of Pedestrian Movements near an Amenity in Walkways of Public Buildings",
            "venue": "In 2022 8th International Conference on Control, Automation and Robotics (ICCAR),",
            "year": 2022
        },
        {
            "authors": [
                "Ananda Ghosh",
                "Katarina Grolinger"
            ],
            "title": "Edge-Cloud Computing for IoT Data Analytics: Embedding Intelligence in the Edge with Deep Learning",
            "venue": "IEEE Transactions on Industrial Informatics,",
            "year": 2020
        },
        {
            "authors": [
                "Paul Voigt",
                "Axel von dem Bussche"
            ],
            "title": "The EU General Data Protection Regulation (GDPR)",
            "year": 2017
        },
        {
            "authors": [
                "Qiang Yang",
                "Yang Liu",
                "Tianjian Chen",
                "Yongxin Tong"
            ],
            "title": "Federated Machine Learning: Concept and Applications",
            "venue": "ACM Transactions on Intelligent Systems and Technology,",
            "year": 2019
        },
        {
            "authors": [
                "Qiong Wu",
                "Kaiwen He",
                "Xu Chen"
            ],
            "title": "Personalized Federated Learning for Intelligent IoT Applications: A Cloud-Edge Based Framework",
            "venue": "IEEE Open Journal of the Computer Society,",
            "year": 2020
        },
        {
            "authors": [
                "Yue Zhao",
                "Meng Li",
                "Liangzhen Lai",
                "Naveen Suda",
                "Damon Civin",
                "Vikas Chandra"
            ],
            "title": "Federated Learning with Non-IID Data",
            "venue": "Technical Report arXiv:1806.00582,",
            "year": 2018
        },
        {
            "authors": [
                "H. Brendan McMahan",
                "Daniel Ramage",
                "Kunal Talwar",
                "Li Zhang"
            ],
            "title": "Learning Differentially Private Recurrent Language Models",
            "venue": "[cs],",
            "year": 2018
        },
        {
            "authors": [
                "Wenxuan Huang",
                "Thanassis Tiropanis",
                "George Konstantinidis"
            ],
            "title": "Federated Learning-Based IoT Intrusion Detection on Non-IID Data",
            "venue": "Internet of Things, Lecture Notes in Computer Science,",
            "year": 2022
        },
        {
            "authors": [
                "Yiwei Li",
                "Shuai Wang",
                "Chong-Yung Chi",
                "Tony Q.S. Quek"
            ],
            "title": "Differentially Private Federated Clustering over Non-IID Data, January 2023",
            "year": 2023
        },
        {
            "authors": [
                "Pu Tian",
                "Weixian Liao",
                "Wei Yu",
                "Erik Blasch"
            ],
            "title": "WSCC: A Weight- Similarity-Based Client Clustering Approach for Non-IID Federated Learning",
            "venue": "IEEE Internet of Things Journal,",
            "year": 2022
        },
        {
            "authors": [
                "Jiangang Shu",
                "Tingting Yang",
                "Xinying Liao",
                "Farong Chen",
                "Yao Xiao",
                "Kan Yang",
                "Xiaohua Jia"
            ],
            "title": "Clustered Federated Multitask Learning on Non-IID Data With Enhanced Privacy",
            "venue": "IEEE Internet of Things Journal,",
            "year": 2023
        },
        {
            "authors": [
                "Mahdi Morafah",
                "Saeed Vahidian",
                "Weijia Wang",
                "Bill Lin"
            ],
            "title": "FLIS: Clustered Federated Learning via Inference Similarity for Non-IID Data Distribution, August 2022",
            "year": 2022
        },
        {
            "authors": [
                "Lin Zhang",
                "Li Shen",
                "Liang Ding",
                "Dacheng Tao",
                "Ling-Yu Duan"
            ],
            "title": "Fine- Tuning Global Model via Data-Free Knowledge Distillation for Non-IID Federated Learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Virginia Smith",
                "Chao-Kai Chiang",
                "Maziar Sanjabi",
                "Ameet S Talwalkar"
            ],
            "title": "Federated Multi-Task Learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Lei Yang",
                "Jiaming Huang",
                "Wanyu Lin",
                "Jiannong Cao"
            ],
            "title": "Personalized Federated Learning on Non-IID Data via Group-Based Meta-Learning",
            "venue": "ACM Trans. Knowl. Discov. Data,",
            "year": 2022
        },
        {
            "authors": [
                "H. Brendan McMahan",
                "Eider Moore",
                "Daniel Ramage",
                "Seth Hampson",
                "Blaise Ag\u00fcera y Arcas"
            ],
            "title": "Communication-Efficient Learning of Deep Networks from Decentralized Data",
            "venue": "[cs],",
            "year": 2017
        },
        {
            "authors": [
                "L\u00e9on Bottou"
            ],
            "title": "Large-Scale Machine Learning with Stochastic Gradient Descent",
            "venue": "Proceedings of COMPSTAT\u20192010,",
            "year": 2010
        },
        {
            "authors": [
                "Tian Li",
                "Anit Kumar Sahu",
                "Ameet Talwalkar",
                "Virginia Smith"
            ],
            "title": "Federated Learning: Challenges, Methods, and Future Directions",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2020
        },
        {
            "authors": [
                "Yiying Li",
                "Wei Zhou",
                "Huaimin Wang",
                "Haibo Mi",
                "Timothy M. Hospedales"
            ],
            "title": "FedH2L: Federated Learning with Model and Statistical Heterogeneity",
            "venue": "[cs],",
            "year": 2021
        },
        {
            "authors": [
                "Sai Praneeth Karimireddy",
                "Satyen Kale",
                "Mehryar Mohri",
                "Sashank J. Reddi",
                "Sebastian U. Stich",
                "Ananda Theertha Suresh"
            ],
            "title": "SCAFFOLD: Stochastic Controlled Averaging for Federated Learning",
            "year": 2021
        },
        {
            "authors": [
                "Tiffany Tuor",
                "Shiqiang Wang",
                "Bong Jun Ko",
                "Changchang Liu",
                "Kin K. Leung"
            ],
            "title": "Overcoming Noisy and Irrelevant Data in Federated Learning",
            "venue": "25th International Conference on Pattern Recognition (ICPR),",
            "year": 2020
        },
        {
            "authors": [
                "Moming Duan",
                "Duo Liu",
                "Xianzhang Chen",
                "Yujuan Tan",
                "Jinting Ren",
                "Lei Qiao",
                "Liang Liang"
            ],
            "title": "Astraea: Self-Balancing Federated Learning for Improving Classification Accuracy of Mobile Deep Learning Applications",
            "venue": "IEEE 37th International Conference on Computer Design (ICCD),",
            "year": 2019
        },
        {
            "authors": [
                "MyungJae Shin",
                "Chihoon Hwang",
                "Joongheon Kim",
                "Jihong Park",
                "Mehdi Bennis",
                "Seong-Lyun Kim"
            ],
            "title": "XOR Mixup: Privacy-Preserving Data Augmentation for One-Shot Federated Learning, June 2020",
            "year": 2006
        },
        {
            "authors": [
                "Tehrim Yoon",
                "Sumin Shin",
                "Sung Ju Hwang",
                "Eunho Yang"
            ],
            "title": "FEDMIX: APPROXIMATION OF MIXUP UNDER MEAN AUGMENTED FED- ERATED LEARNING",
            "year": 2021
        },
        {
            "authors": [
                "Xiaoxiao Li",
                "Meirui Jiang",
                "Xiaofei Zhang",
                "Michael Kamp",
                "Qi Dou"
            ],
            "title": "FedBN: Federated Learning on Non-IID Features via Local Batch Normalization",
            "venue": "[cs],",
            "year": 2021
        },
        {
            "authors": [
                "Jianyu Wang",
                "Qinghua Liu",
                "Hao Liang",
                "Joshi Gauri",
                "H. Vincent Poor"
            ],
            "title": "A Novel Framework for the Analysis and Design of Heterogeneous Federated Learning",
            "venue": "IEEE Transactions on Signal Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Hongyi Wang",
                "Mikhail Yurochkin",
                "Yuekai Sun",
                "Dimitris Papailiopoulos",
                "Yasaman Khazaeni"
            ],
            "title": "Federated Learning with Matched Averaging",
            "year": 2020
        },
        {
            "authors": [
                "Paul Pu Liang",
                "Terrance Liu",
                "Liu Ziyin",
                "Nicholas B. Allen",
                "Randy P. Auerbach",
                "David Brent",
                "Ruslan Salakhutdinov",
                "Louis-Philippe Morency"
            ],
            "title": "Think Locally, Act Globally: Federated Learning with Local and Global Representations",
            "year": 2020
        },
        {
            "authors": [
                "Manoj Ghuhan Arivazhagan",
                "Vinay Aggarwal",
                "Aaditya Kumar Singh",
                "Sunav Choudhary"
            ],
            "title": "Federated Learning with Personalization Layers. arXiv:1912.00818 [cs, stat",
            "year": 2019
        },
        {
            "authors": [
                "Virginia Smith",
                "Chao-Kai Chiang",
                "Maziar Sanjabi",
                "Ameet Talwalkar"
            ],
            "title": "Federated Multi-Task Learning",
            "venue": "[cs, stat],",
            "year": 2018
        },
        {
            "authors": [
                "Yiqiang Chen",
                "Xin Qin",
                "Jindong Wang",
                "Chaohui Yu",
                "Wen Gao"
            ],
            "title": "Fed- Health: A Federated Transfer Learning Framework for Wearable Healthcare",
            "venue": "IEEE Intelligent Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yansheng Wang",
                "Yongxin Tong",
                "Zimu Zhou",
                "Ziyao Ren",
                "Yi Xu",
                "Guobin Wu",
                "Weifeng Lv"
            ],
            "title": "Fed-LTD: Towards Cross-Platform Ride Hailing via Federated Learning to Dispatch",
            "venue": "In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 2022
        },
        {
            "authors": [
                "Sinno Jialin Pan",
                "Qiang Yang"
            ],
            "title": "A Survey on Transfer Learning",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2010
        },
        {
            "authors": [
                "Felix Sattler",
                "Klaus-Robert M\u00fcller",
                "Wojciech Samek"
            ],
            "title": "Clustered Federated Learning: Model-Agnostic Distributed Multi-Task Optimization under Privacy Constraints",
            "year": 2019
        },
        {
            "authors": [
                "Avishek Ghosh",
                "Jichan Chung",
                "Dong Yin",
                "Kannan Ramchandran"
            ],
            "title": "An Efficient Framework for Clustered Federated Learning",
            "year": 2020
        },
        {
            "authors": [
                "Yishay Mansour",
                "Mehryar Mohri",
                "Jae Ro",
                "Ananda Theertha Suresh"
            ],
            "title": "Three Approaches for Personalization with Applications to Federated Learning",
            "year": 2020
        },
        {
            "authors": [
                "Christopher Briggs",
                "Zhong Fan",
                "Peter Andras"
            ],
            "title": "Federated learning with hierarchical clustering of local updates to improve training on non-IID data",
            "venue": "In 2020 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2020
        },
        {
            "authors": [
                "Felix Sattler",
                "Klaus-Robert M\u00fcller",
                "Wojciech Samek"
            ],
            "title": "Clustered Federated Learning: Model-Agnostic Distributed Multitask Optimization Under Privacy Constraints",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Naichen Shi",
                "Fan Lai",
                "Raed Al Kontar",
                "Mosharaf Chowdhury"
            ],
            "title": "Fedensemble: Improving Generalization through Model Ensembling in Federated Learning, July 2021",
            "year": 2021
        },
        {
            "authors": [
                "Binghui Wang",
                "Ang Li",
                "Hai Li",
                "Yiran Chen"
            ],
            "title": "GraphFL: A Federated Learning Framework for Semi-Supervised Node Classification on Graphs, December 2020",
            "year": 2012
        },
        {
            "authors": [
                "Debora Caldarola",
                "Massimiliano Mancini",
                "Fabio Galasso",
                "Marco Ciccone",
                "Emanuele Rodola",
                "Barbara Caputo"
            ],
            "title": "Cluster-driven Graph Federated Learning over Multiple Domains",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),",
            "year": 2021
        },
        {
            "authors": [
                "Longfei Zheng",
                "Jun Zhou",
                "Chaochao Chen",
                "Bingzhe Wu",
                "Li Wang",
                "Benyu Zhang"
            ],
            "title": "ASFGNN: Automated separated-federated graph neural network",
            "venue": "Peer-to-Peer Networking and Applications,",
            "year": 2021
        },
        {
            "authors": [
                "Guangxu Mei",
                "Ziyu Guo",
                "Shijun Liu",
                "Li Pan"
            ],
            "title": "SGNN: A Graph Neural Network Based Federated Learning Approach by Hiding Structure",
            "venue": "IEEE International Conference on Big Data (Big Data),",
            "year": 2019
        },
        {
            "authors": [
                "Hangyu Zhu",
                "Jinjin Xu",
                "Shiqing Liu",
                "Yaochu Jin"
            ],
            "title": "Federated learning on non-IID data: A survey",
            "year": 2021
        },
        {
            "authors": [
                "Jakub Kone\u010dn\u00fd",
                "H. Brendan McMahan",
                "Felix X. Yu",
                "Peter Richt\u00e1rik",
                "Ananda Theertha Suresh",
                "Dave Bacon"
            ],
            "title": "Federated Learning: Strategies for Improving Communication Efficiency",
            "year": 2017
        },
        {
            "authors": [
                "Shameem Ahmed",
                "Khalid Hassan Sheikh",
                "Seyedali Mirjalili",
                "Ram Sarkar"
            ],
            "title": "Binary Simulated Normal Distribution Optimizer for feature selection: Theory and application in COVID-19 datasets",
            "venue": "Expert Systems with Applications,",
            "year": 2022
        },
        {
            "authors": [
                "Yongliang Yuan",
                "Xiaokai Mu",
                "Xiangyu Shao",
                "Jianji Ren",
                "Yong Zhao",
                "Zhenxi Wang"
            ],
            "title": "Optimization of an auto drum fashioned brake using the elite opposition-based learning and chaotic k-best gravitational search strategy based grey wolf optimizer algorithm",
            "venue": "Applied Soft Computing,",
            "year": 2022
        },
        {
            "authors": [
                "Weiguo Zhao",
                "Liying Wang",
                "Seyedali Mirjalili"
            ],
            "title": "Artificial hummingbird algorithm: A new bio-inspired optimizer with its engineering applications",
            "venue": "Computer Methods in Applied Mechanics and Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Yongliang Yuan",
                "Jianji Ren",
                "Shuo Wang",
                "Zhenxi Wang",
                "Xiaokai Mu",
                "Wu Zhao"
            ],
            "title": "Alpine skiing optimization: A new bio-inspired optimization algorithm",
            "venue": "Advances in Engineering Software,",
            "year": 2022
        },
        {
            "authors": [
                "Besher Alhalabi",
                "Mohamed Medhat Gaber",
                "Shadi Basura"
            ],
            "title": "MicroNets: A multi-phase pruning pipeline to deep ensemble learning in IoT devices",
            "venue": "Computers & Electrical Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "Junyuan Xie",
                "Ross Girshick",
                "Ali Farhadi"
            ],
            "title": "Unsupervised Deep Embedding for Clustering Analysis, May 2016",
            "year": 2016
        },
        {
            "authors": [
                "Lin Guo",
                "Qun Dai"
            ],
            "title": "Graph Clustering via Variational Graph Embedding",
            "venue": "Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yunsheng Bai",
                "Hao Ding",
                "Yang Qiao",
                "Agustin Marinovic",
                "Ken Gu",
                "Ting Chen",
                "Yizhou Sun",
                "Wei Wang"
            ],
            "title": "Unsupervised Inductive Graph-Level Representation Learning via Graph-Graph Proximity",
            "year": 2019
        },
        {
            "authors": [
                "Wei Li",
                "Andrew McCallum"
            ],
            "title": "Pachinko allocation: DAG-structured mixture models of topic correlations",
            "venue": "In Proceedings of the 23rd international conference on Machine learning - ICML",
            "year": 2006
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Identity Mappings in Deep Residual Networks",
            "venue": "[cs],",
            "year": 2016
        },
        {
            "authors": [
                "Besher Alhalabi",
                "Mohamed Medhat Gaber",
                "Shadi Basurra"
            ],
            "title": "EnSyth: A Pruning Approach to Synthesis of Deep Learning Ensembles",
            "venue": "IEEE International Conference on Systems, Man and Cybernetics (SMC),",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "INDEX TERMS Federated Learning, Ensemble Learning, Convolutional Neural Networks, Graph Embedding, Affinity propagation, Non-IID datasets, Privacy.\nI. INTRODUCTION\nW Ith the proliferation of the Internet of Things (IoT),and the launch of 5G networks, the IoT has emerged as one of the major technological advances in our lives. We can see their advances in different domains, including wearable smart health devices [1], intelligent energy networks, smart transportation [2] and smart building [3] [4]. These tiny connected devices generate massive amounts of data on the network edge, giving great opportunities to generate valuable insights and complete sophisticated machine learning (ML) tasks. The traditional approach to analysing IoT data is to transfer user data (clients) to a central cloud\nserver. Then the server completes the analysis and generates the required insights [5]. However, moving sensitive information to a remote server can pose a significant risk to data privacy and lead to breaches of data protection laws such as GDPR (General Data Protection Regulation) [6]. Federated learning is a machine learning paradigm that enables the collaborative training of a model on data that is distributed across multiple devices or data centers without the need to transfer raw data to a central server [7]. In the standard federated learning setting, each device contributes an independent and identically distributed (IID) sample of data to the model. However, most current FL strategies are\nVOLUME 4, 2016 1\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nnot optimised to handle statistical heterogeneity, such as non-iid (non-independent and identically distributed) data generated by different clients [8]. Since different clients are likely to exhibit different behaviour (distinct usage patterns), the local training samples may follow a different distribution. As a result, the local models are likely to become vastly different; hence them could reduce the accuracy and lead to slow covariance [9]. Furthermore, different studies raised some privacy concerns related to sharing weights and biases by models. Sharing the model updates during the training process make it vulnerable to penetration, potentially causing leaks of sensitive information [10]. A number of authors have considered tackling non-iid challenges using clustering [11] [12]. Clustering is a technique that can be used in non-IID federated learning to mitigate the impact of nonIID data distribution [13]. Clustering can group devices with similar data distributions together and allow local models to be trained on similar data; the resulting local models can then be combined to obtain a more accurate global model [14] [15], however performing clustering on resource-limited environments may cause performance issues.\nThis paper proposes a novel holistic approach to a federated learning strategy based on ensemble learning that improves accuracy and respects privacy at the edge end. This work is the first of its kind because it looks at federated learning from a different perspective. Unlike traditional federated learning strategies, our approach allows clients to collaborate by sharing complete models, not only weights. The proposed framework is an iterative process that begins by initialising a pool of pruned deep learning models (a global pool). These models are then randomly deployed to different clients to be trained on local datasets, taking into account any discrepancies in label distribution between the local and global datasets. Subsequently, the predictions of the models are combined using ensemble learning to achieve a better generalisation performance on the local testing sets. The next step in the process is to personalise the ensemblebased federated learning by clustering the models that exhibit similar behaviour. However, clustering DNNs on resourceconstrained devices can be computationally expensive. To address this issue, we employ graph embeddings theory to reduce the complexity of the DNNs. Each ANN can be represented as a graph, with nodes representing layers and vertices representing connections between layers. We generate embeddings of all models and then cluster them. Finally, we select representatives of the resulting embedding clusters and ask the clients to share the corresponding models to be part of the global pool, thus initiating a new iteration.\nOur key contributions could be summarised as follows: \u2022 introducing a new federated learning strategy under\nnon-iid settings; the proposed approach employs deepensemble learning to maximise the generalisation at the edge-end and provide better performance on different distributions of the clients\u2019 local data; \u2022 proposing a novel ensemble pruning technique to reduce communication overheads over the network. It aims to\nminimise the storage footprint for ensembles by applying affinity propagation clustering. The clustering is applied to the embeddings of the models, considering that a graph could represent each artificial neural network, and \u2022 establishing a new privacy approach to preserve clients\u2019 sensitive data in the applications that require running an ensemble of models.\nThe paper is organised as follows: Section II introduces related work in federated learning in non-iid settings. Section III describes the proposed approach in detail. The experimental study and the results are discussed in Section IV. Finally, the conclusion and future works are presented in Section IV-D."
        },
        {
            "heading": "II. RELATED WORK",
            "text": "Different works have been proposed to explore the high variance of federated learning algorithms in the presence of non-IID datasets (data heterogeneity) [16] [17] [18]. In the following section, we provide a deeper understanding of the effect of data heterogeneity on federated learning strategies. Subsequently, we list some popular techniques to address statistical heterogeneity in federated learning."
        },
        {
            "heading": "A. THE EFFECT OF NON-INDEPENDENTLY AND IDENTICALLY DISTRIBUTED (NON-IID DATA) IN FEDERATED LEARNING",
            "text": "FedAvg is one of the first central aggregation strategies that orchestrates the distributed federated learning process [19]. FedAvg employs SGD (Stochastic Gradient Decent) to optimise the averaged weights from the clients. However, SGD requires IID sampling of the training data sets to generate an unbiased estimate of the full gradient [20]. In real-world applications, it\u2019s unrealistic to assume that the data on edge devices is following IID distribution. Actually, dealing with non-IID datasets is a key challenge in federated learning [21]. Due to the statistical heterogeneity between the clients\u2019 datasets, the distribution of each local dataset is different from the global distribution (drift in local updates). As a result, each client\u2019s objective is inconsistent with the global optima. Furthermore, large local updates (a large number of epochs) lead to significant differences between the averaged model and global optima, leading to low accuracy in nonIID settings [22]. II-A explains the issue of FedAvg under non-iid settings. As shown in the figure above, if a client performs different local updates, then the updated global model w(t+1,0) stays close to the local minimum w1\u2217 rather than straying toward the true global minimum x\u2217."
        },
        {
            "heading": "B. APPROACHES TO DEAL WITH NON-IID DATA IN FEDERATED LEARNING",
            "text": "FedAvg suffers when the heterogeneity of the data is different between clients [23] (non-IID datasets). This happens because the distribution of each local dataset is very different from the global distribution. Different works have been"
        },
        {
            "heading": "2 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nproposed to optimise FL on non-IID datasets, we divide them into different categories and summarise the most recent ones:"
        },
        {
            "heading": "1) Data-based approaches",
            "text": "Data-based approaches could be divided into two main categories, namely data sharing and data augmentation. Data sharing is a simple, yet effective approach to tackle statistical heterogeneity in non-iid settings. [9] proposes to create a subset of the data to be globally shared between the clients, hence generalising the learning task. The experiments on the CIFAR10 dataset show that the accuracy could be increased up to 30% while globally sharing only 5% of the dataset. Similarly, [24] developed a mechanism to select a global subset of the client\u2019s data to be used in a federated learning task which is popular in some domains like the health sector. There are notable deficiencies in the data-sharing approaches. Initially, obtaining a uniformly distributed global dataset is challenging due to the server\u2019s lack of knowledge regarding data distributions among connected clients. Additionally, distributing segments of the global dataset to each client for model training goes against the fundamental motivation of privacy-preserving learning, which is a requirement for this process.\nData augmentation methods are a set of techniques to increase the number of data samples by applying different transformations. Its mainly used to mitigate the issues of using imbalanced datasets in ML applications [25]. [26] uses data augmentation to develop a self-balancing federated learning framework that outperforms FedAvg on imbalanced EMNIST and imbalanced CINIC-10 datasets. To address the issue of Non-IID data, XorMixFL framework has been introduced by [27] which applies a data augmentation technique. The basic concept in XorMixFL is that each client shares its encoded seed samples (encoded through the XOR operator) with the server for decoding. A new balanced dataset is constructed by combining the decoded samples with the base data samples on the server. Subsequently, a global model is trained on this reconstructed data, which is then downloaded to each client until the training is complete. In [28], on the other hand, the authors suggest a mean augmented method, which involves exchanging locally averaged batch data with the server. The mean data received is then combined and transferred back to each client, reducing the degree of local\ndata imbalance. In general, the use of data augmentation techniques can greatly enhance the learning performance of models trained on Non-IID data by replenishing the imbalanced local data with augmentations. Nevertheless, as mentioned above, many of these techniques require data sharing, which could potentially increase the risk of data privacy breaches."
        },
        {
            "heading": "2) Algorithm-based approaches",
            "text": "Several algorithm-based approaches were proposed in the literature as follows.\nLocal fine turning: The authors of FedProx [21], apply some modifications to FedAvg to allow partial information aggregation. This provides convergence guarantees when learning over data from non-identical distributions. [29] is another optimisation attempt to work on non-idd data. The approach uses local batch normalisation to alleviate the feature shift before averaging the clients\u2019 models. FedNova [30] is another recent framework that relies on FedAvg; it normalises and scales the local updates of the clients according to their number of local steps before updating the global model.\nPersonalisation layers: edge clients are given the option to have a set of personalised layers that will not be shared with the server. A popular approach that falls under this category is FedMA [31] which was originally designed to offer extra support for deep learning models, and it works by sharing the global model in a layer-wise manner. Furthermore, LG-FEDAVG [32] is a new federated learning framework that outperforms FedAvg in federated learning settings. In LG-FEDAVG, the shallow layers of the deep learning models are considered personalised layers, and the base layers of the networks are shared with the server. In contrast to LGFEDAVG, FedPer [33] allows the shallow layers to be shared with the aggregation server. FedPer and LG-GEDAVG have resulted into good accuracy results, and they also reduce the communication cost since shallow layers are lightweight when shared over the network with the aggregation server. In general, Personalisation Layers are a promising approach to enhance accuracy in non-iid settings.However, one major drawback is that the clients are not able to release the personalisation layers.\nMulti-task learning (MTL) methods: are inductive transfer approaches that aim to improve generalisation performance by learning multiple tasks simultaneously. [34] has developed MOCHA as a new framework that considers the issues of high communication cost, stragglers, and fault tolerance in distributed multi-task settings. MOCHA employs primal-dual optimisation to generate separate but related models for each client. However, primal-dual optimisation is unsuitable for non-convex problems and is limited to shallow networks.\nTransfer learning (Knowledge distillation): Transfer learning allows knowledge exchange between different domains to achieve higher learning rates. Following this approach, [35] has developed FedHealth as the first federated transfer learning framework for wearable health devices.\nVOLUME 4, 2016 3\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFedHealth adapts the inputs from different domains by replacing fully connected layers with an alignment layer. Similarly, [36] has developed another federated transfer learning framework for smart manufacturing with cross-domain applications (Fed-LTD). However, one of the main disadvantages of knowledge distillation is the negative transfer that can cause clients to perform worse. Negative transfer occurs when data from the source domain and the task contribute to reduced learning performance in the target domain [37]. Client clustering: In the literature, two primary types of secure data similarity evaluation methods have been introduced to address this issue. One method involves evaluating the similarity of the loss value, while the other main goal is to evaluate the similarity of model weights. The first similarity evaluation approach, as reported in [38], [39], [40], involves comparing the loss values of various cluster models. The fundamental concept behind this technique is simple: instead of creating a single global model, the server produces multiple global models and distributes all cluster models to connected clients for local empirical loss computation. Each client then updates the received cluster model with the lowest loss value and transmits it back to the server for cluster model aggregation. The second approach involves assessing the similarity of local data and clustering based on the local model weights. In [41], [42] FedAvg is employed first to train and warm up the global model, and then the model is downloaded locally to each client for local training. Then the models are sent back to the server to be clustered based on the weights. Client clustering is both necessary and justifiable to tackle non-iid challenges, and this is because merging local models trained on vastly different data can lead to negative knowledge transfer; hence, the overall performance of the shared model will decline. Furthermore, creating multiple global models instead of a single one improves the scalability and flexibility of FL systems, enabling system developers to select or combine different cluster models to suit specific tasks. Nevertheless, this method requires additional computation and communication resources for model training and testing. Ensemble learning: In Fed-ensemble [43], the authors leverage ensemble learning to bring greater generalisation power to Federated Learning (FL). Fed-ensemble utilises random permutation to update a group of models and then produces the prediction through model averaging. By doing so, Fed-ensemble is able to achieve improved performance and accuracy compared to traditional FL methods.\nGraph representation learning: most recently, graph representation has become a prominent topic in the ML community due to its wide applications. Different works have been proposed to use graph learning in FL. GraphFL [44] is specifically designed to address the challenge of non-iid using a semi-supervised node classification approach based on graphs. First, GraphFL follows the training scheme of MAML (Model Agnostic Meta-Learning) to learn a global model on a server. Then, it leverages the traditional FL methods (e.g. FedAvg) to further improve generalisation on training sets. FedCG [45] is another framework to address\nthe statistical heterogeneity in FL by means of GCN (Graph Convolutional Networks). FedCG consists of three major steps: a-identify the clusters that share the same data distributions; b-assign network components to the formed clusters; c-interaction with the GCN. [46] and [47] preserve privacy in FL using similarity-based graph neural networks. Unlike traditional FL approaches to tackle the statistical heterogeneity in non-iid settings, our approach employs graph theory and ensemble learning aiming to reduce communication overhead over the network while preserving privacy. In the proposed approach, we use deep ensemble learning to address statistical heterogeneity in real-world applications of FL. Our solution allows resource-constrained devices to run deep ensembles smoothly. Second, we adopt graph theory and, specifically, graph embeddings to develop a novel clustering mechanism."
        },
        {
            "heading": "III. METHOD",
            "text": "In this section, we describe the proposed approach by providing a summary of the entire process and then explain each step in detail.\nThe proposed framework is an iterative process; it starts by initialising a pool of pruned deep learning models (a global pool \u03b80). Then, the members of the proposed pool are randomly deployed to different clients to be trained on local datasets (assuming the label distribution of the local datasets does not match the global one). After that, the predictions of the models are combined using ensemble learning to obtain a better generalisation performance on the local testing sets. The next step in the process aims to personalise the ensemble-based federated learning by clustering the models that exhibit similar behaviour. However, clustering DNNs on devices that are resource-constrained is expensive. Thus, we employ graph embeddings theory to reduce the complexity of the DNNs. Since each ANN (Artificial Neural Network) could be represented as a graph (nodes are layers, vertices are connections of layers), we generate embeddings of all models, and then we cluster them. Finally, we choose representatives of the resulting embedding clusters and then ask the clients to share the corresponding models to be part of the global pool \u03b80 and a new iteration begins.\nThe following is a detailed description of the steps of FedNets and the overall system topology."
        },
        {
            "heading": "A. SYSTEM TOPOLOGY",
            "text": "As many federated learning strategies, FetNets follows the star network communication topology where a central cloud server is connected to a network of resource-limited devices. The server orchestrates the learning process by aggregating deep learning ensembles from all connected clients (each communication round). Figure III-A summarises the main communication topology."
        },
        {
            "heading": "B. ENSEMBLE GENERATION AND PRUNING:",
            "text": "In non-iid federated learning settings, the statistical heterogeneity of clients can vary significantly [8]. This led to a"
        },
        {
            "heading": "4 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 2. FedNets follows a star communication topology where a server connects with all the remote clients. The server orchestrates the communications in each learning round, it starts by deploying the global ensemble to the clients. Then, each client shares a few members of their ensembles with the server.\ndifferent distribution of the local data on each client [48], hence, traditional federated learning settings experience a drop in accuracy [49]. To overcome those challenges, our approach utilises deep ensemble learning to give more generalisation power at the edge end and boost performance. Typically, any pruning process has different hyperparameters and it could be a challenging task to set the optimal values of these parameters. Thus, different optimisation techniques could be used [50], [51], [52], [53]. However, due to the high complexity of DNNs, we aim to prune the models before running them at the edge end. To complete the pruning and the deployment of models, we follow a similar approach to the one described here [54]. In this work, the authors proposed a multi-phase pruning framework that enables edge devices to run deep ensemble learning without draining the resources of devices. However, our approach has two main differences: (1) we use Constant Sparsity pruning instead of weight pruning; and (2) integer quantisation is ignored as we do not run the experiments on real devices. At the end of this\nstep, we have a group of N clients; each client has a group of M different models."
        },
        {
            "heading": "C. MODEL TRAINING ON LOCAL DATASETS (NON-IIDS) UPDATE LOCAL MODELS WEIGHTS",
            "text": "As all federated learning strategies, FedNets requires the local models (ensemble members) to be trained on the datasets of each client. In the training process, the models will simply learn the good values for all the weights and the bias from the labelled examples. However, FedNets employs deep learning ensembles, thus the training accuracy will depend on all members. To calculate the accuracy of an ensemble: Let En = m1, \u00b7 \u00b7 \u00b7 ,mN be an ensemble of the deployed models mi. The prediction of the ensemble for a training/testing example (x is the data feature, y is the data label) using max-voting is the class that receives the maximum support \u03b7final(W ) from all members of the ensemble. Based on that,\nVOLUME 4, 2016 5\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nthe output of the ensemble could be defined as:\n\u03b7final(W ) = argmaxj\u2208{1,\u00b7\u00b7\u00b7 ,C} N\u2211 i=1 yi,j\nD. GRAPH CONVERSION\nGraph clustering based on embedding is a popular technique that aims to convert graph structure and node attributes into the low dimensional feature, and split similar nodes into nonoverlapping groups e [55] [56]. The purpose of this step is to cluster models with similar properties using the graph topology and node features. To convert an artificial neural network to a graph, let us have a k layer neural network, each layer has a set of weights W = w1, w2, \u00b7 \u00b7 \u00b7 , wn, the output of layer k is z = W \u00d7 x+ b.\nA graph G = (V,E) is defined as a collection of vertices V = {v1, . . . , vn} where vi \u2208 En, edges E = {eij}ni,j=1 where ei is a connection between li and li+1 assuming that k is a layer in mi, and Xi = {(w1, b1), \u00b7 \u00b7 \u00b7 , (wn, bn)} is the corresponding node vector that holds the average weights\u2211k\ni=1 avg(Wk) and biases Avg( \u2211k\ni=1(b)) in each layer k.\nThe representation of a node vi according to [57] can be calculated as :\n\u00b5ki = MLP k Wk ( (1 + \u03f5k) \u00b7 \u00b5k\u22121i + \u2211 j\u2208N(i) \u00b5k\u22121j ) (1)\nwhere \u00b5i is the representation of node vi, N(i) is the neighbourhood of node Vi, \u03f5 could be learnt by a hyperparameter or GD (gradient descent) and MLP kWk refers to multilayer perceptron for the kth GIN layer and weights Wk. After generating the node embeddings, we calculate the overall graph embeddings following the same approach described in [57]:\nhG = MLPW ( \u2225Kk=1ATT\u0398(k) (UG) ) (2)\nThe embedding of the input graph of graph g is UG \u2208 RN\u00d7D where the n\u2212 th row, un \u2208 RD. The output of this step is an array of the corresponding graph embeddings for each model Emi.\nF. CLUSTERING OF EMBEDDINGS\nAt this stage, all models mi \u2208 \u03b80 are transformed into a two-dimensional array Emi. We apply affinity clustering on Emi, and then we choose representatives from each cluster. The selection criteria will be based on the following: Let ci be one of the generated clusters, cl is the length of the cluster (number of members of the group) and acci be the precision of the corresponding model mi on a validation set V S (10% of the training set). The chosen model should satisfy the\nfollowing:{ mi \u2208 Ci where : cl > K where K = 1, \u00b7 \u00b7 \u00b7 , 20 acci > 55% on VS (3) The pseudocode of FedNets is shown in Algorithm 1.\nAlgorithm 1 FedNets pseudocode Server Initialise global pool \u03b80 for Each communication Round R, r\u2190 1 to T do\nSelect N Client for Each client i = 1, 2, .., N do\nDownload \u03b8r Clienti update and receive emi\nend for GE \u2190 AffinityPropagation(EMs) M \u2190 RepresentativeSelection(GE) Update Global Pool \u03b80 with M\nend for\nClient Update Replace local ensemble \u03b8i \u2190 \u03b8i+1 for Local Epoch e\u2190 1 to E do\n\u03b7final(W ) = argmaxj\u2208{1,\u00b7\u00b7\u00b7 ,C} N\u2211 i=1 yi,j\nend for for Each model M in \u03b8i do\nem\u2190 generatedEmbeddings(M) return em end for"
        },
        {
            "heading": "G. PRIVACY PRESERVING",
            "text": "Unlike the typical federated learning approaches, the proposed method provides a privacy-preserving-by-design approach. The sharing of a subset of the local ensemble per client makes it harder to reveal the behaviour of the users of individual clients. Formally, if the number of models making up the ensemble in a client is n, only m models are shared, where m < n. Thus, privacy increases when the value \u03c4 = mn decreases. Note that \u03c4 can be a hyperparameter when applying the proposed method. An important factor in increasing privacy is the diversity of the ensemble. The more diverse the ensemble is the more private the proposed method is. In other words, with the same value for \u03c4 , the diverse ensemble is inherently more private.\nFor example, if n = 10 and m = 3, then \u03c4 = 0.3. This is a setting that will result in high privacy. However, if \u03c4 = 0.9, then privacy may be compromised, because most of the models that make up the ensemble are shared centrally, making it possible to reproduce the data fed to the model locally. It is worth pointing out that in a typical federated learning setting, the parameters of the model are"
        },
        {
            "heading": "6 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 3. FedNets approach to convert an ANN to a graph. Each layer will be converted to a node, and the mean value of the weights/biases will be added as a feature to the node\nshared centrally (i.e. \u03c4 = 1), making the models shared by the clients vulnerable.\nIV. EXPERIMENTAL STUDY In this section, we first explain in detail our simulation environment and setup, then evaluate the precision of Fed-Nets using the ResNetV2 and CIFAR100 federated dataset. Next, we compare our results with Fed-Avg. Finally, we provide important measures related to ensemble performance, such as inference time, required training time, and the number of models per client. Those measures are essential to give an idea about the feasibility of running deep ensembles on resource-limited devices (IoT). The code used in the experiment is publicly available on GitHub1.\nA. DATA SET AND MODELS"
        },
        {
            "heading": "1) Federated CIFAR100 for simulation",
            "text": "This dataset is specially designed to simulate nonindependent and identically distributed data samples. It is derived from the original CIFAR100 dataset, and it has 50,000 training samples and 10,000 testing samples. Unlike the original dataset, the training and testing samples are partitioned across 500 and 100 clients (respectively, and no overlapping across the clients). The training clients\u2019 IDs range from 0 to 499, while the testing clients\u2019 IDs go from 0 to 99. The data partitioning part is done using PAM (Pachinko Allocation Method) [58], which is an improved version of LDA (Latent Dirichlet Allocation). This approach uses a two-stage LDA process, where each client has an associated multinomial distribution over the coarse labels of CIFAR100, and a coarse-to-fine label multinomial distribution for that coarse label over the labels under that coarse label.\n1https://github.com/besherh/FedNets"
        },
        {
            "heading": "2) ResNetV2",
            "text": "This model belongs to the Deep Residual Networks (RNNs) family that achieved breakthroughs in the deep learning community. ResNetV2 is the new version of ResNet, the main improvements are related to the arrangement of the layers in residual blocks. The Model accepts an input of shape 299 \u00d7 299 pixels; the output is the probability distribution for the predicted classes [59]."
        },
        {
            "heading": "B. SIMULATION SETUP",
            "text": "We run our simulation on a powerful workstation with multiple GPU and CPU nodes. The following are the hardware specifications:\n\u2022 CPU Nodes:5 nodes - 72 cores per node, PowerEdge R740 Server, Intel Xeon Gold 6240 2.6G. \u2022 GPU Nodes:2 nodes - 72 cores per node, PowerEdge R740 Server, Intel Xeon Gold 6240 2.6G, NVIDIA(R) Tesla(TM) T4 16GB Passive, Single Slot, Full Height GPU (2 cards per node) - 320 Turing Tensor cores and 2560 Cuda cores per card.\nAll nodes have \u2019CentOS-8.2.2004-x86_64\u2019 operating system installed. There are also different hyperparameters that control the design of FedNets, starting from initialising the global pool and ending with the deployment of the models to the clients. Table 1 summarises all hyperparameters that are used in the different processes of FedNets.\nAs shown in Table 1, there are a lot of different hyperparameters that control the process flow of FedNets. Starting from the generation of the global pool and ending with representative selection. The values of the hyperparameters are selected based on a \u201ctrial and error\u201d approach, and we only reported the values related to the presented results."
        },
        {
            "heading": "C. RESULTS AND ANALYSIS",
            "text": "We start this section by introducing some details about the baseline model and the pool of models that we use to de-\nVOLUME 4, 2016 7\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 4. FedNets Framework involves the following steps:-Step1: server deploys deep learning ensembles to the clients and clients train them locally; step2: Each client converts the local models into graphs assuming: nodes are the layers, edges are links between layers and the attributes of the nodes are mean of weights and biases of whole layers; step3: clients generate the corresponding graph embeddings and share them with the server; step4: the server to cluster the embeddings using affinity propagation and choose a representative from each cluster, then models are deployed to clients.\nploy deep learning ensembles to clients. We move next to investigate the effectiveness of FedNets on non-iid settings. We use the federated CIFAR100 dataset as a benchmarking dataset and ResNetV2 as a baseline model. Additionally, we provide accuracy comparison with state-of-the-art federated learning algorithms including Fed-Avg and Fed-Yogi. The simulation is applied to a different number of clients (two clients, five clients and ten clients) for four federated learning\nrounds, where each client has an ensemble of ten pruned models. Finally, we provide time measures related to the performance of the deep learning ensembles on the proposed virtual clients.\nThe baseline of ResNetV2 (3,575 KB) is trained on the original CIFAR100 dataset and the accuracy of the model on the testing set is 68%. We apply constant-sparsity pruning on ResNetV2 to generate a pool of 500 models. During"
        },
        {
            "heading": "8 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 5. Accuracy comparison on two clients\nFIGURE 6. Accuracy comparison on five clients .\nthe pruning process, we use different values of the pruning hyperparameters to ensure diversity between the pool\u2019s members. Diversity leads to better generalisation and provides better accuracy results at the edge end as shown in [60]. The maximum accuracy in the pruned pool against a validation set (20% of the testing set) is around 66% and the minimum is 0.05%. The pruning lead to around 37% reduction of the original baseline model size( the average size of the pruned models is 1,295 KB)."
        },
        {
            "heading": "1) Comparison with the state of the art",
            "text": "Here we compare the accuracy results of FedNets with two of the state of the federated learning strategies (FedAvg, FedYogi) on the federated CIFAR100 dataset. In the next section, we provide the simulation results for different numbers of clients (two, five, and ten clients).\nAs shown in 5 and when the number of clients is equal to two, FedNets accuracy is 93% and 86% in client1 and client2, respectively. On the other hand, the accuracy of FedAvg is 56% on client1 and 14% on client2. Similarly, the accuracy of FedYogi on client1 and client2 is 1% and 60%.\nThe results of the accuracy of FedNets on five different virtual clients are presented in figure6. From the chart, it can be seen that FedNet\u2019s accuracy is better than FedAvg and FedYogi on all clients. The minimum accuracy for FedNets is 90% while the best accuracy reached by FedNets is 80%.\nFigure7 compares the accuracy results between FedNets, FedAvg and FedYogi. Looking at figure7, it\u2019s apparent that\nFedNets is still achieving superior performance and can beat the state-of-the-art methods on all clients. We can also see that the maximum accuracy of FedNets is 100% on the client3, the maximum accuracy of FedAvg is 80% on client7 and client 8, and the best accuracy of FedYogi is 53% on client10.\nAs shown in Figures 5,6 and 7, there is a significant performance difference between FedNets and FedAvg/FedYogi. Our proposed approach provides better accuracy results on a different number of clients; the clients can share their knowledge by exchanging the members of the ensembles, and it seems to be better than the traditional approach of sharing the weights of the models."
        },
        {
            "heading": "2) Ensembles Performance",
            "text": "In the section above, we present the accuracy results for the federated CIFAR100 dataset. Now, we move to present the simulation results of the required time to complete one federated learning round inFedNets. Each round consists of three main steps: training, inferencing, and deployment. We run this simulation on CPU nodes for four federated learning rounds. The simulation results are presented in Table 2.\nOn closer inspection of Table 2, it shows that training the deep learning ensembles on the local datasets is consuming most of the time. Training time is noticeably increased when the number of clients is ten (almost 12 minutes to complete). However, the inference time of the ensembles is acceptable in most of the applications.\nIt can also be seen from the data in Table 2 that, in general, FedNets requires a relatively long period of time to complete a federated learning round (especially when the number of clients is rather considerable, like ten). However, we should accept the fact that FedNets is an ensemblebased approach that aims to maximise the generalisation and accuracy at the edge end, so it requires more time to complete\nVOLUME 4, 2016 9\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\na federated learning round. Additionally, we are running the simulation on CPU nodes only. In a real-life application, resource-limited devices could be attached to cutting-edge AI accelerators that bring the power of TPUs to the edge. We believe that this led to a significant improvement in the time complexity of FedNets as shown in [54]. Turning now to examine the size of the composed ensembles on each client after each federated learning round. This could be directly related to the effectiveness of our approach in preserving the resources of IoT devices. Table 3 displays the changes in the number of models per client (ensemble size) after each federated learning round.\nAs shown in Table 3, FedNets reduces the size of the ensemble by 50% after round 4 (assuming that each client\nstarts with ten models, as explained earlier). The simulation on both five and ten clients shows that FedNets is still able to reduce the number of models per ensemble which lead to reducing the required memory/storage required by the approach."
        },
        {
            "heading": "3) Preserving Privacy",
            "text": "As previously stated, FedNets prompts federated learning by allowing clients to share the members of the local ensembles; at the same time, FedNets respects the privacy of the clients.\nTable 4 shows summary statistics about the number of shared models per client. It is worth mentioning that the value of \u03c4 is not controlled during the simulation. However, and as explained earlier, \u03c4 could be a hyperparameter to the proposed approach which will be used to trade off accuracy with privacy by controlling the maximum number of models to be shared with the other clients. As seen in the table, when the number of models is equal to ten, FedNets tends to share a large number of models (average 9.6, \u03c4 = 0.96). This could be minimised by defining \u03c4 \u2264 0.5 sharing less than half of the models per client, forcing greater privacy. However, this may come at the cost of compromising the accuracy of the federation."
        },
        {
            "heading": "4) Discussion of the results",
            "text": "The results of FedNets as shown in Figures 5,6,7 indicate that the proposed approach is effective in providing high accuracy in non-iid settings where the distribution of class labels is vastly different among clients. This will work well for applications that cannot compromise on accuracy as the quality of the output directly impacts the reliability of the AI systems. For example, FedNets could be integrated into AI-Based Medical Diagnosis systems to offer further data privacy assurance to comply with Health Institutions\u2019 Data Protection Policies. It is possible that the results in Table 4 could be improved by adding the privacy factor \u03c4 into the list of hyperparameters introduced in Table 1. This could be useful when edge clients deal with very sensitive personal information, for example, smart wearable devices. The results obtained from Table 2 shows that FedNets could run on resource-limited environments effectively. However, the required time to complete one federated round could take a reasonably long time. The observed increase in time could be attributed to: a) using a baseline model that has not been fully optimised to run on resource-limited devices; and b) utilising deep ensemble learning to provide better generalisation in non-iid settings. Using edge-friendly models like the MobilesNet family [61] could lead to shorter training and inference time. However, the fact that FedNets requires a relatively long period to complete a federated round will still be valid. This limitation renders FedNets a less desirable choice when there are no GPUs attached to the edge devices (operating solely on CPU) and when realtime applications necessitate instantaneous inference, such as autonomous vehicles."
        },
        {
            "heading": "10 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nD. CONCLUSION AND FUTURE WORKS In this work, we introduced FedNets, the first ensemblebased federated learning strategy that provides better generalisation in non-iid settings. The key differences between this approach and other federated learning strategies are: (1) clients are running deep learning ensembles rather than having one model per client; and (2) instead of sharing the models\u2019 weights to update a single global model, which is prone to a privacy breach, our approach allows clients to share models (members of their deep learning ensembles) to compose a shared pool of outperforming models, then the pool is shared with all participating clients. The experimental results on the federated CIFAR100 dataset demonstrate that our approach outperforms the Federated Learning Averaging strategy (FedAv), and Adaptive Federated Optimisation (FedYogi). The results also show that the cost of running deep learning ensembles (inference time, ensemble size) on edge devices is feasible in a resource-limited environment.\nIn future work, we plan to test our approach on other noniid benchmarking datasets, and to discover the effectiveness of using modern CNN architectures like MobileNet and EfficientNet when applied in deep learning ensemble settings. Moreover, we plan to utilise optimisation techniques to finetune the hyperparameters of FedNets in order to achieve superior results. By leveraging these techniques, we can maximise the performance of our models and unlock their full potential.\nREFERENCES [1] Jeet Ghosh, Gopinath Samanta, and Chinmay Chakraborty. Smart Health\nCare for Societies: An Insight into the Implantable and Wearable Devices for Remote Health Monitoring. In Chinmay Chakraborty, editor, Green Technological Innovation for Sustainable Smart Societies, pages 89\u2013113. Springer International Publishing, Cham, 2021. [2] Mehdi Mohammadi and Ala Al-Fuqaha. Enabling Cognitive Smart Cities Using Big Data and Machine Learning: Approaches and Challenges. IEEE Communications Magazine, 56(2):94\u2013101, February 2018. [3] Andrew Kwok-Fai Lui, Yin-Hei Chan, and Man-Fai Leung. Modelling of Destinations for Data-driven Pedestrian Trajectory Prediction in Public Buildings. In 2021 IEEE International Conference on Big Data (Big Data), pages 1709\u20131717, Orlando, FL, USA, December 2021. IEEE. [4] Andrew Kwok-Fai Lui, Yin-Hei Chan, and Man-Fai Leung. Modelling of Pedestrian Movements near an Amenity in Walkways of Public Buildings. In 2022 8th International Conference on Control, Automation and Robotics (ICCAR), pages 394\u2013400, Xiamen, China, April 2022. IEEE.\n[5] Ananda Ghosh and Katarina Grolinger. Edge-Cloud Computing for IoT Data Analytics: Embedding Intelligence in the Edge with Deep Learning. IEEE Transactions on Industrial Informatics, pages 1\u20131, 2020. [6] Paul Voigt and Axel von dem Bussche. The EU General Data Protection Regulation (GDPR). Springer International Publishing, Cham, 2017. [7] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated Machine Learning: Concept and Applications. ACM Transactions on Intelligent Systems and Technology, 10(2):1\u201319, March 2019. [8] Qiong Wu, Kaiwen He, and Xu Chen. Personalized Federated Learning for Intelligent IoT Applications: A Cloud-Edge Based Framework. IEEE Open Journal of the Computer Society, 1:35\u201344, 2020. [9] Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated Learning with Non-IID Data. Technical Report arXiv:1806.00582, arXiv, June 2018. arXiv:1806.00582 [cs, stat] type: article. [10] H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning Differentially Private Recurrent Language Models. arXiv:1710.06963 [cs], February 2018. arXiv: 1710.06963. [11] Wenxuan Huang, Thanassis Tiropanis, and George Konstantinidis. Federated Learning-Based IoT Intrusion Detection on Non-IID Data. In Aurora Gonz\u00e1lez-Vidal, Ahmed Mohamed Abdelgawad, Essaid Sabir, S\u00e9bastien Ziegler, and Latif Ladid, editors, Internet of Things, Lecture Notes in Computer Science, pages 326\u2013337, Cham, 2022. Springer International Publishing. [12] Yiwei Li, Shuai Wang, Chong-Yung Chi, and Tony Q. S. Quek. Differentially Private Federated Clustering over Non-IID Data, January 2023. arXiv:2301.00955 [cs]. [13] Pu Tian, Weixian Liao, Wei Yu, and Erik Blasch. WSCC: A WeightSimilarity-Based Client Clustering Approach for Non-IID Federated Learning. IEEE Internet of Things Journal, 9(20):20243\u201320256, October 2022. Conference Name: IEEE Internet of Things Journal. [14] Jiangang Shu, Tingting Yang, Xinying Liao, Farong Chen, Yao Xiao, Kan Yang, and Xiaohua Jia. Clustered Federated Multitask Learning on Non-IID Data With Enhanced Privacy. IEEE Internet of Things Journal, 10(4):3453\u20133467, February 2023. Conference Name: IEEE Internet of Things Journal. [15] Mahdi Morafah, Saeed Vahidian, Weijia Wang, and Bill Lin. FLIS: Clustered Federated Learning via Inference Similarity for Non-IID Data Distribution, August 2022. arXiv:2208.09754 [cs]. [16] Lin Zhang, Li Shen, Liang Ding, Dacheng Tao, and Ling-Yu Duan. FineTuning Global Model via Data-Free Knowledge Distillation for Non-IID Federated Learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10174\u201310183, June 2022. [17] Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated Multi-Task Learning. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [18] Lei Yang, Jiaming Huang, Wanyu Lin, and Jiannong Cao. Personalized Federated Learning on Non-IID Data via Group-Based Meta-Learning. ACM Trans. Knowl. Discov. Data, August 2022. Place: New York, NY, USA Publisher: Association for Computing Machinery. [19] H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag\u00fcera y Arcas. Communication-Efficient Learning of Deep Networks from Decentralized Data. arXiv:1602.05629 [cs], February 2017. arXiv: 1602.05629. [20] L\u00e9on Bottou. Large-Scale Machine Learning with Stochastic Gradient Descent. In Yves Lechevallier and Gilbert Saporta, editors, Proceedings of COMPSTAT\u20192010, pages 177\u2013186. Physica-Verlag HD, Heidelberg, 2010. [21] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated Learning: Challenges, Methods, and Future Directions. IEEE Signal Processing Magazine, 37(3):50\u201360, May 2020. [22] Yiying Li, Wei Zhou, Huaimin Wang, Haibo Mi, and Timothy M. Hospedales. FedH2L: Federated Learning with Model and Statistical Heterogeneity. arXiv:2101.11296 [cs], January 2021. arXiv: 2101.11296. [23] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J. Reddi, Sebastian U. Stich, and Ananda Theertha Suresh. SCAFFOLD: Stochastic Controlled Averaging for Federated Learning. arXiv:1910.06378 [cs, math, stat], April 2021. arXiv: 1910.06378. [24] Tiffany Tuor, Shiqiang Wang, Bong Jun Ko, Changchang Liu, and Kin K. Leung. Overcoming Noisy and Irrelevant Data in Federated Learning. In 2020 25th International Conference on Pattern Recognition (ICPR), pages 5020\u20135027, Milan, Italy, January 2021. IEEE.\nVOLUME 4, 2016 11\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n[25] Tri Dao, Albert Gu, Alexander J Ratner, Virginia Smith, Christopher De Sa, and Christopher Re. A Kernel Theory of Modern Data Augmentation. page 10. [26] Moming Duan, Duo Liu, Xianzhang Chen, Yujuan Tan, Jinting Ren, Lei Qiao, and Liang Liang. Astraea: Self-Balancing Federated Learning for Improving Classification Accuracy of Mobile Deep Learning Applications. In 2019 IEEE 37th International Conference on Computer Design (ICCD), pages 246\u2013254, Abu Dhabi, United Arab Emirates, November 2019. IEEE. [27] MyungJae Shin, Chihoon Hwang, Joongheon Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim. XOR Mixup: Privacy-Preserving Data Augmentation for One-Shot Federated Learning, June 2020. arXiv:2006.05148 [cs, eess]. [28] Tehrim Yoon, Sumin Shin, Sung Ju Hwang, and Eunho Yang. FEDMIX: APPROXIMATION OF MIXUP UNDER MEAN AUGMENTED FEDERATED LEARNING. 2021. [29] Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. FedBN: Federated Learning on Non-IID Features via Local Batch Normalization. arXiv:2102.07623 [cs], May 2021. arXiv: 2102.07623. [30] Jianyu Wang, Qinghua Liu, Hao Liang, Joshi Gauri, and H. Vincent Poor. A Novel Framework for the Analysis and Design of Heterogeneous Federated Learning. IEEE Transactions on Signal Processing, 69:5234\u2013 5249, 2021. [31] Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federated Learning with Matched Averaging. arXiv:2002.06440 [cs, stat], February 2020. arXiv: 2002.06440. [32] Paul Pu Liang, Terrance Liu, Liu Ziyin, Nicholas B. Allen, Randy P. Auerbach, David Brent, Ruslan Salakhutdinov, and Louis-Philippe Morency. Think Locally, Act Globally: Federated Learning with Local and Global Representations. 2020. Publisher: arXiv Version Number: 3. [33] Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav Choudhary. Federated Learning with Personalization Layers. arXiv:1912.00818 [cs, stat], December 2019. arXiv: 1912.00818. [34] Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet Talwalkar. Federated Multi-Task Learning. arXiv:1705.10467 [cs, stat], February 2018. arXiv: 1705.10467. [35] Yiqiang Chen, Xin Qin, Jindong Wang, Chaohui Yu, and Wen Gao. FedHealth: A Federated Transfer Learning Framework for Wearable Healthcare. IEEE Intelligent Systems, 35(4):83\u201393, July 2020. [36] Yansheng Wang, Yongxin Tong, Zimu Zhou, Ziyao Ren, Yi Xu, Guobin Wu, and Weifeng Lv. Fed-LTD: Towards Cross-Platform Ride Hailing via Federated Learning to Dispatch. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 4079\u20134089, Washington DC USA, August 2022. ACM. [37] Sinno Jialin Pan and Qiang Yang. A Survey on Transfer Learning. IEEE Transactions on Knowledge and Data Engineering, 22(10):1345\u20131359, October 2010. [38] Felix Sattler, Klaus-Robert M\u00fcller, and Wojciech Samek. Clustered Federated Learning: Model-Agnostic Distributed Multi-Task Optimization under Privacy Constraints. arXiv:1910.01991 [cs, stat], October 2019. arXiv: 1910.01991. [39] Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An Efficient Framework for Clustered Federated Learning. arXiv:2006.04088 [cs, stat], June 2020. arXiv: 2006.04088. [40] Yishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. Three Approaches for Personalization with Applications to Federated Learning. arXiv:2002.10619 [cs, stat], July 2020. arXiv: 2002.10619. [41] Christopher Briggs, Zhong Fan, and Peter Andras. Federated learning with hierarchical clustering of local updates to improve training on non-IID data. In 2020 International Joint Conference on Neural Networks (IJCNN), pages 1\u20139, July 2020. ISSN: 2161-4407. [42] Felix Sattler, Klaus-Robert M\u00fcller, and Wojciech Samek. Clustered Federated Learning: Model-Agnostic Distributed Multitask Optimization Under Privacy Constraints. IEEE Transactions on Neural Networks and Learning Systems, 32(8):3710\u20133722, August 2021. Conference Name: IEEE Transactions on Neural Networks and Learning Systems. [43] Naichen Shi, Fan Lai, Raed Al Kontar, and Mosharaf Chowdhury. Fedensemble: Improving Generalization through Model Ensembling in Federated Learning, July 2021. arXiv:2107.10663 [cs, stat]. [44] Binghui Wang, Ang Li, Hai Li, and Yiran Chen. GraphFL: A Federated Learning Framework for Semi-Supervised Node Classification on Graphs, December 2020. arXiv:2012.04187 [cs, stat]. [45] Debora Caldarola, Massimiliano Mancini, Fabio Galasso, Marco Ciccone, Emanuele Rodola, and Barbara Caputo. Cluster-driven Graph Federated\nLearning over Multiple Domains. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 2743\u20132752, Nashville, TN, USA, June 2021. IEEE. [46] Longfei Zheng, Jun Zhou, Chaochao Chen, Bingzhe Wu, Li Wang, and Benyu Zhang. ASFGNN: Automated separated-federated graph neural network. Peer-to-Peer Networking and Applications, 14(3):1692\u20131704, May 2021. [47] Guangxu Mei, Ziyu Guo, Shijun Liu, and Li Pan. SGNN: A Graph Neural Network Based Federated Learning Approach by Hiding Structure. In 2019 IEEE International Conference on Big Data (Big Data), pages 2560\u2013 2568, Los Angeles, CA, USA, December 2019. IEEE. [48] Hangyu Zhu, Jinjin Xu, Shiqing Liu, and Yaochu Jin. Federated learning on non-IID data: A survey. Neurocomputing, 465:371\u2013390, November 2021. [49] Jakub Konec\u030cn\u00fd, H. Brendan McMahan, Felix X. Yu, Peter Richt\u00e1rik, Ananda Theertha Suresh, and Dave Bacon. Federated Learning: Strategies for Improving Communication Efficiency. arXiv:1610.05492 [cs], October 2017. arXiv: 1610.05492. [50] Shameem Ahmed, Khalid Hassan Sheikh, Seyedali Mirjalili, and Ram Sarkar. Binary Simulated Normal Distribution Optimizer for feature selection: Theory and application in COVID-19 datasets. Expert Systems with Applications, 200:116834, August 2022. [51] Yongliang Yuan, Xiaokai Mu, Xiangyu Shao, Jianji Ren, Yong Zhao, and Zhenxi Wang. Optimization of an auto drum fashioned brake using the elite opposition-based learning and chaotic k-best gravitational search strategy based grey wolf optimizer algorithm. Applied Soft Computing, 123:108947, July 2022. [52] Weiguo Zhao, Liying Wang, and Seyedali Mirjalili. Artificial hummingbird algorithm: A new bio-inspired optimizer with its engineering applications. Computer Methods in Applied Mechanics and Engineering, 388:114194, January 2022. [53] Yongliang Yuan, Jianji Ren, Shuo Wang, Zhenxi Wang, Xiaokai Mu, and Wu Zhao. Alpine skiing optimization: A new bio-inspired optimization algorithm. Advances in Engineering Software, 170:103158, August 2022. [54] Besher Alhalabi, Mohamed Medhat Gaber, and Shadi Basura. MicroNets: A multi-phase pruning pipeline to deep ensemble learning in IoT devices. Computers & Electrical Engineering, 96:107581, December 2021. [55] Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised Deep Embedding for Clustering Analysis, May 2016. arXiv:1511.06335 [cs]. [56] Lin Guo and Qun Dai. Graph Clustering via Variational Graph Embedding. Pattern Recognition, 122:108334, February 2022. [57] Yunsheng Bai, Hao Ding, Yang Qiao, Agustin Marinovic, Ken Gu, Ting Chen, Yizhou Sun, and Wei Wang. Unsupervised Inductive Graph-Level Representation Learning via Graph-Graph Proximity. arXiv:1904.01098 [cs, stat], June 2019. arXiv: 1904.01098. [58] Wei Li and Andrew McCallum. Pachinko allocation: DAG-structured mixture models of topic correlations. In Proceedings of the 23rd international conference on Machine learning - ICML \u201906, pages 577\u2013584, Pittsburgh, Pennsylvania, 2006. ACM Press. [59] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity Mappings in Deep Residual Networks. arXiv:1603.05027 [cs], July 2016. arXiv: 1603.05027. [60] Besher Alhalabi, Mohamed Medhat Gaber, and Shadi Basurra. EnSyth: A Pruning Approach to Synthesis of Deep Learning Ensembles. In 2019 IEEE International Conference on Systems, Man and Cybernetics (SMC), pages 3466\u20133473, Bari, Italy, October 2019. IEEE. [61] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. MobileNetV2: Inverted Residuals and Linear Bottlenecks. arXiv:1801.04381 [cs], January 2018. arXiv: 1801.04381."
        },
        {
            "heading": "12 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nBESHER ALHALABI is a Ph.D. researcher and Visiting Lecturer at Birmingham City University. He is also a member of the data analytics and artificial intelligence group at BCU. Besher holds a B.Sc. in software engineering and M.Sc. in business intelligence. Besher has published a few papers related to deep ensemble learning on edge devices. In addition to his academic work, Besher has extensive experience in designing and developing Business Intelligence applications.\nDR.SHADI BASURRA is an Associate Professor in Computer Science at Birmingham City University (BCU), UK, Head of the master\u2019s program in computer science, and heading the Data Analytics and AI Research Group (DAAI) at BCU. .\nShadi Basurra received his Bsc (Hons) degree in Computer Science from Exeter University, the UK, and MSc in Distributed Systems and Networks from Kent University at Canterbury, UK. He obtained his Ph.D. from the University of\nBath in collaboration with Bristol University. After completing his Ph.D., Shadi worked at Sony Corporation developing Goal Decision Systems, and then joined BCU as an academic. His recent research is related to Edge Computing, in particular, the use of lightweight machine and deep learning techniques that allow the learning and inference to occur on devices with limited resources, e.g. IoT devices, to aid rapid decision-making whilst maintaining the highest possible accuracy in Classification and Regression Analysis.\nMOHAMED GABER Mohamed is a Professor in Data Analytics at the School of Computing and Digital Technology, Birmingham City University. He is currently seconded as the Dean of the Faculty of Computer Science and Engineering, Galala University. Mohamed received his PhD from Monash University, Australia. He then held appointments with the University of Sydney, CSIRO, and Monash University, all in Australia. Prior to joining Birmingham City University, Mo-\nhamed worked for the Robert Gordon University as a Reader in Computer Science and at the University of Portsmouth as a Senior Lecturer in Computer Science, both in the UK. He has published over 200 journal articles and conference papers, co-authored 3 monograph-style books, and edited/co-edited 7 books on Artificial Intelligence. Mohamed\u2019s published work has appeared in the proceedings of prestigious conferences including ICDM and ISWC, and reputable journals including the IEEE Transactions on Neural Networks and Learning Systems, and Data Mining and Knowledge Discovery, to mention a few. His work has attracted well over eight thousand citations, with an h-index of 43. Mohamed has supervised/co-supervised to successful completion 19 PhD students. He has acted as an examiner for 39 PhD candidates. He has had numerous chairing roles serving the Artificial Intelligence community including the General co-chair of the 3rd IEEE International Conference on Data Science and Computational Intelligence (DSCI 2019) and the programme committee co-chair of the IEEE Mobile Data Management (MDM 2016), among the many other events. Professor Gaber is recognised as a Fellow of the British Higher Education Academy (HEA). In 2007, he was awarded the CSIRO teamwork award.\nVOLUME 4, 2016 13\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
        }
    ],
    "title": "FedNets: Federated Learning on Edge Devices using Ensembles of Pruned Deep Neural Networks",
    "year": 2023
}