{
    "abstractText": "The registration of synthetic aperture radar (SAR) and optical images is a meaningful but challenging multimodal task. Due to the large radiometric differences between SAR and optical images, it is difficult to obtain discriminative features only by mining local features in the traditional Siamese convolutional networks. We propose a modality-shared attention network (MSA-Net) that introduces nonlocal attention (NLA) to the partially shared two-stream network to jointly exploit local and global features. First, a modality-specific feature learning module is designed to efficiently extract shallow modality-specific features from SAR and optical images. Subsequently, a modality-shared feature learning (MShFL) module is designed to extract deep modality-shared features. The local feature extraction module and the NLA module in MShFL extract deep local and global features to enrich feature representations. Furthermore, a triplet loss function with a cross-modality similarity constraint is constructed to learn modality-shared feature representations, thereby reducing nonlinear radiometric differences between the two modalities. The MSANet is trained on a public SAR and optical dataset and tested on five pairs of SAR and optical images. In the registration results of five pairs of test SAR and optical images, the matching rate of the MSA-Net is 5% to 15% higher than that of other compared methods, and the matching errors of the matched inliers are on average reduced by about 0.28. Several ablation experiments verify the effectiveness of the partially shared network structure, the MShFL module, and the cross-modality similarity constraint. \u00a9 The Authors. Published by SPIE under a Creative Commons Attribution 4.0 International License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI. [DOI: 10.1117/1.JRS.17.036504]",
    "authors": [
        {
            "affiliations": [],
            "name": "Xin Hu"
        },
        {
            "affiliations": [],
            "name": "Yan Wu"
        },
        {
            "affiliations": [],
            "name": "Zhikang Li"
        },
        {
            "affiliations": [],
            "name": "Xiaoru Zhao"
        },
        {
            "affiliations": [],
            "name": "Xingyu Liu"
        },
        {
            "affiliations": [],
            "name": "Ming Li"
        }
    ],
    "id": "SP:f52617d5ec0a1d1b143c4201bc5d7a2910e1762c",
    "references": [
        {
            "authors": [
                "Y. Wu"
            ],
            "title": "Fusion of synthetic aperture radar and visible images based on variational multiscale image decomposition,",
            "venue": "J. Appl. Remote Sens 11(2),",
            "year": 2017
        },
        {
            "authors": [
                "A. Shakya",
                "M. Biswas",
                "M. Pal"
            ],
            "title": "CNN-based fusion and classification of SAR and Optical data,",
            "venue": "Int. J. Remote Sens. 41(22),",
            "year": 2020
        },
        {
            "authors": [
                "X. Jiang"
            ],
            "title": "Building damage detection via superpixel-based belief fusion of space-borne SAR and optical images,",
            "venue": "IEEE Sens. J. 20(4),",
            "year": 2020
        },
        {
            "authors": [
                "Z. Zhang"
            ],
            "title": "Endoscope image mosaic based on pyramid ORB,",
            "venue": "Biomed. Signal Process. Control",
            "year": 2022
        },
        {
            "authors": [
                "S. Eken"
            ],
            "title": "Resource-and content-aware, scalable stitching framework for remote sensing images,",
            "venue": "Arab. J. Geosci. 12,",
            "year": 2019
        },
        {
            "authors": [
                "S. Eken",
                "A. Sayar"
            ],
            "title": "A MapReduce-based distributed and scalable framework for stitching of satellite mosaic images,",
            "venue": "Arab. J. Geosci. 14(18),",
            "year": 2021
        },
        {
            "authors": [
                "G.N. Vivekananda",
                "R. Swathi",
                "A. Sujith"
            ],
            "title": "Multi-temporal image analysis for LULC classification and change detection,",
            "venue": "Eur. J. Remote Sens. 54(suppl",
            "year": 2021
        },
        {
            "authors": [
                "Y. Wu et al.",
                "\u201cCommonality autoencoder"
            ],
            "title": "learning common features for change detection from heterogeneous images,\u201d IEEE Trans",
            "venue": "Neural Networks Learn. Syst. 33, 4257\u20134270",
            "year": 2021
        },
        {
            "authors": [
                "X. Li"
            ],
            "title": "Collaborative attention-based heterogeneous gated fusion network for land cover classification,",
            "venue": "IEEE Trans. Geosci. Remote Sens. 59(5),",
            "year": 2020
        },
        {
            "authors": [
                "W. Kang et al.",
                "\u201cCFNet"
            ],
            "title": "a cross fusion network for joint land cover classification using optical and SAR images,\u201d IEEE J",
            "venue": "Sel. Top. Appl. Earth Obs. Remote Sens. 15, 1562\u20131574",
            "year": 2022
        },
        {
            "authors": [
                "X. Li"
            ],
            "title": "Dense adaptive grouping distillation network for multimodal land cover classification with privileged modality,",
            "venue": "IEEE Trans. Geosci. Remote Sens. 60,",
            "year": 2022
        },
        {
            "authors": [
                "X. Li",
                "L. Lei",
                "G. Kuang"
            ],
            "title": "Locality-constrained bilinear network for land cover classification using heterogeneous images,",
            "venue": "IEEE Geosci. Remote Sens. Lett. 19,",
            "year": 2022
        },
        {
            "authors": [
                "D.G. Lowe"
            ],
            "title": "Distinctive image features from scale-invariant keypoints,",
            "venue": "Int. J. Comput. Vision 60(2),",
            "year": 2004
        },
        {
            "authors": [
                "J. Fan"
            ],
            "title": "SAR image registration using phase congruency and nonlinear diffusion-based SIFT,",
            "venue": "IEEE Geosci. Remote Sens. Lett. 12(3),",
            "year": 2015
        },
        {
            "authors": [
                "F. Dellinger et al.",
                "\u201cSAR-SIFT"
            ],
            "title": "a SIFT-like algorithm for SAR images,\u201d IEEE Trans",
            "venue": "Geosci. Remote Sens. 53(1), 453\u2013466",
            "year": 2015
        },
        {
            "authors": [
                "Y. Xiang",
                "F. Wang",
                "H. You",
                "\u201cOS-SIFT"
            ],
            "title": "a robust SIFT-like algorithm for high-resolution Optical-to-SAR image registration in suburban areas,\u201d IEEE Trans",
            "venue": "Geosci. Remote Sens. 56(6), 3078\u20133090",
            "year": 2018
        },
        {
            "authors": [
                "Y. Ye"
            ],
            "title": "Robust registration of multimodal remote sensing images based on structural similarity,",
            "venue": "IEEE Trans. Geosci. Remote Sens. 55(5),",
            "year": 2017
        },
        {
            "authors": [
                "J. Fan"
            ],
            "title": "SAR and optical image registration using nonlinear diffusion and phase congruency structural descriptor,",
            "venue": "IEEE Trans. Geosci. Remote Sens. 56(9),",
            "year": 2018
        },
        {
            "authors": [
                "J. Li",
                "Q. Hu",
                "M. Ai",
                "\u201cRIFT"
            ],
            "title": "multi-modal image matching based on radiation-variation insensitive feature transform,\u201d IEEE Trans",
            "venue": "Image Process. 29, 3296\u20133310",
            "year": 2020
        },
        {
            "authors": [
                "Y. Tian",
                "B. Fan",
                "F. Wu",
                "\u201cL2-Net"
            ],
            "title": "deep learning of discriminative patch descriptor in Euclidean space,\u201d in Proc",
            "venue": "IEEE Conf. Comput. Vision and Pattern Recognit., pp. 661\u2013669",
            "year": 2017
        },
        {
            "authors": [
                "A. Mishchuk et al.",
                "\u201cWorking hard to know your neighbor\u2019s margins"
            ],
            "title": "local descriptor learning loss,\u201d in Adv",
            "venue": "Neural Inf. Process. Syst.",
            "year": 2017
        },
        {
            "authors": [
                "T. B\u00fcrgmann",
                "W. Koppe",
                "M. Schmitt"
            ],
            "title": "Matching of TerraSAR-X derived ground control points to optical image patches using deep learning,",
            "venue": "ISPRS J. Photogramm. Remote Sens. 158,",
            "year": 2019
        },
        {
            "authors": [
                "L.H. Hughes"
            ],
            "title": "Identifying corresponding patches in SAR and optical images with a pseudo-Siamese CNN,",
            "venue": "IEEE Geosci. Remote Sens. Lett. 15(5),",
            "year": 2018
        },
        {
            "authors": [
                "H. Zhang"
            ],
            "title": "Registration of multimodal remote sensing image based on deep fully convolutional neural network,",
            "venue": "IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 12(8),",
            "year": 2019
        },
        {
            "authors": [
                "L.H. Hughes"
            ],
            "title": "A deep learning framework for matching of SAR and optical imagery,",
            "venue": "ISPRS J. Photogramm. Remote Sens. 169,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Liao"
            ],
            "title": "Feature matching and position matching between optical and SAR with local deep feature descriptor,",
            "venue": "IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 15,",
            "year": 2021
        },
        {
            "authors": [
                "D. Quan"
            ],
            "title": "Deep feature correlation learning for multi-modal remote sensing image registration,",
            "venue": "IEEE Trans. Geosci. Remote Sens. 60,",
            "year": 2022
        },
        {
            "authors": [
                "H. Han",
                "C. Li",
                "X. Qiu"
            ],
            "title": "Multi-modal remote sensing image matching method based on deep learning technology,",
            "venue": "J. Phys. Conf. Ser. 2083(3),",
            "year": 2083
        },
        {
            "authors": [
                "J. Fan"
            ],
            "title": "A novel multiscale adaptive binning phase congruency feature for SAR and optical image registration,",
            "venue": "IEEE Trans. Geosci. Remote Sens. 60,",
            "year": 2022
        },
        {
            "authors": [
                "A. Dosovitskiy"
            ],
            "title": "An image is worth 16 \u00d7 16words: transformers for image recognition at scale,",
            "venue": "https:// doi.org/10.48550/arXiv.2010.11929",
            "year": 2020
        },
        {
            "authors": [
                "A. Vaswani"
            ],
            "title": "Attention is all you need,",
            "year": 2017
        },
        {
            "authors": [
                "H. Dong",
                "L. Zhang",
                "B. Zou"
            ],
            "title": "Exploring vision transformers for polarimetric SAR image classification,",
            "venue": "IEEE Trans. Geosci. Remote Sens. 60,",
            "year": 2022
        },
        {
            "authors": [
                "H. Chen",
                "Z. Qi",
                "Z. Shi"
            ],
            "title": "Remote sensing image change detection with transformers,",
            "venue": "IEEE Trans. Geosci. Remote Sens. 60,",
            "year": 2022
        },
        {
            "authors": [
                "X. Liu"
            ],
            "title": "High resolution SAR image classification using global-local network structure based on vision transformer and CNN,",
            "venue": "IEEE Geosci. Remote Sens. Lett. 19,",
            "year": 2022
        },
        {
            "authors": [
                "X. Wang"
            ],
            "title": "Non-local neural networks,",
            "venue": "Proc. IEEE Conf. Comput. Vision and Pattern Recognit.,",
            "year": 2018
        },
        {
            "authors": [
                "M. Schmitt",
                "L.H. Hughes",
                "X.X. Zhu"
            ],
            "title": "The SEN1-2 dataset for deep learning in SAR-optical data fusion,",
            "venue": "ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci.,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "\u00a9 The Authors. Published by SPIE under a Creative Commons Attribution 4.0 International License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI. [DOI: 10.1117/1.JRS.17.036504]\nKeywords: synthetic aperture radar and optical images; image registration; local feature extraction; nonlocal attention; modality-shared feature learning; crossmodality similarity constraint\nPaper 220577G received Oct. 5, 2022; revised May 21, 2023; accepted Aug. 8, 2023; published Aug. 30, 2023."
        },
        {
            "heading": "1 Introduction",
            "text": "Image registration is the process of matching two or more images of the same scene captured at different times, from different viewing angles, and using different sensors. This process achieves geometric alignment between the reference image and the image to be registered. Image registration is widely used in various fields, such as image fusion,1\u20133 image mosaic,4 image stitching,5,6 and multitemporal image change detection.7,8 With the rapid development of multisensor technology, many different modes of images can be obtained from the same scene. Synthetic\n*Address all correspondence to Yan Wu, ywu@mail.xidian.edu.cn"
        },
        {
            "heading": "Journal of Applied Remote Sensing 036504-1 Jul\u2013Sep 2023 \u2022 Vol. 17(3)",
            "text": "Downloaded From: https://www.spiedigitallibrary.org/journals/Journal-of-Applied-Remote-Sensing on 09 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use\naperture radar (SAR) and optical images are the two main ways of observing the earth. Because these two images have complementary information, they are widely used in multimodal remote image tasks.1\u20133,8\u201312 For these multimodal tasks to be accurate, the registration results of SAR and optical images are crucial.\nHowever, nonlinear radiation differences between SAR and optical images and coherent speckles in SAR images make registration an unsolved and challenging research issue. Both traditional handcrafted methods13\u201319 and deep learning-based ones have been used to address these issues.20\u201328\nThe handcrafted image registration approaches mainly include intensity-based and featurebased methods. Due to the poor similarity of intensity-based descriptors, the former is less effective in multimodal image registration. The latter can deal with nonlinear radiation differences more effectively by mining the structural features of multimodal images to obtain descriptors. As a result, feature-based approaches are increasingly being used in image registration. The gradient-based descriptor algorithms, such as scale-invariant feature transform (SIFT),13 SARSIFT,15 and OS-SIFT,16 are applied to optical image registration, SAR image registration, and optical and SAR image registration, respectively. Nonetheless, gradient-based descriptors extract less valuable information from darker images. Compared with gradients, changes in light and radiation have no effect on phase congruency. The registration algorithms based on phase congruency, such as the histogram of phase congruency (HOPC),17 phase congruency structural descriptor (PCSD),18 and radiation-invariant feature transform (RIFT),19 have been successfully employed in SAR and optical image registration. Recently, Fan et al.29 proposed a new nonlinear diffusion-based Harris-Laplace detector and a new structural descriptor based on multiscale adaptive binning phase congruency that is more robust to geometric and radiometric differences, improving the number of matching points and reducing matching errors.\nThe handcrafted methods based on gradient and phase congruency mentioned above mine modality-shared features, i.e., structural features. The handcrafted methods use a fixed feature extraction pipeline to align a small number of images. They do not have the ability to handle the resolution, scale, and rotations that occur in the alignment of a large number of SAR and optical images. With its powerful data mining capabilities, deep learning has been widely used in various tasks involving remote sensing images. In recent years, many deep-learning registration methods have been proposed. For example, L2-Net,20 HardNet,21 and Siamese fully convolutional network (SFcNet)24 based on Siamese convolutional network have been proposed for single- or multimodal image matching or registration. MatchosNet26 and CNet27 based on pseudoSiamese convolutional networks are proposed for SAR and optical image matching and registration.\nThe application of the above-mentioned Siamese or pseudo-Siamese convolutional networks to multimodal image registration has two problems. First, neither the Siamese nor pseudoSiamese networks take into account both nonlinear radiation differences and mining modality-shared features. Since it is a two-stream network with completely shared parameters, the Siamese network ignores the radiation differences between SAR and optical images. The pseudo-Siamese network is a two-stream network that does not share parameters at all, which makes it unable to fully exploit the modality-shared features. Second, the local features extracted by convolutional networks are easily affected by radiation differences and noise, resulting in low similarity of extracted features.\nFor the first problem mentioned above, we propose a partially shared dual-stream convolutional neural network (CNN) to mine shared features that help improve registration accuracy. For the second problem mentioned above, we insert nonlocal attention (NLA) into the two-stream CNNs to extract both local and global features. Local features are limited due to receptive fields, which make it difficult to extract similar features from multimodal images. Global features have a larger range of receptive areas and are less affected by modal differences than local features. Therefore, considering global correlation can improve the robustness of features to a certain extent.\nIn this paper, we propose a modality-shared attention network (MSA-Net) to construct modality-shared feature descriptors for SAR and optical image registration. The main contributions of our method are summarized as follows."
        },
        {
            "heading": "Journal of Applied Remote Sensing 036504-2 Jul\u2013Sep 2023 \u2022 Vol. 17(3)",
            "text": "Downloaded From: https://www.spiedigitallibrary.org/journals/Journal-of-Applied-Remote-Sensing on 09 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use\n(1) In MSA-Net, we built a partially shared network structure for mining the modalityspecific and modality-shared features of SAR and optical images to overcome modal differences.\n(2) In MSA-Net, a modality-specific feature learning (MSpFL) module is designed to extract shallow modality-specific features, and a modality-shared feature learning (MShFL) module is designed to extract deep modality-shared features. Specifically, the MShFL module consists of three local feature extraction (LFE) modules and three NLA modules, which can extract both local and global features.\n(3) In MSA-Net, a triplet loss function with a cross-modality similarity constraint is designed to make the MSA-Net less susceptible to modal changes. The triplet loss can encourage the MSA-Net to learn shared features between SAR and optical images.\nThe reminder of this paper is organized as follows: Section 2 briefly introduces the related work of this paper. Section 3 elaborates on the proposed registration algorithm and the architecture of MSA-Net in detail. In Sec. 4, the experimental comparisons and analysis are performed on a public SAR and optical dataset. In addition, the comparisons between five test SAR and optical image pairs are carried out. Finally, Sec. 5 summarizes this paper."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Deep Learning Registration Methods",
            "text": "Deep learning-based methods learn data-driven deep features through Siamese and pseudoSiamese networks. L2Net20 and HardNet21 are methods applied to optical image matching. The similarity between the two is that they use a Siamese network with completely shared parameters to learn local feature descriptors. The difference is that L2-Net uses all positive and negative samples in the batch to optimize the network, whereas HardNet uses hard negative samples in a batch to complete the network optimization. Due to the successful application of HardNet in optical image matching, some papers22,24 used its improved Siamese convolutional network to extract deep features and realize multimodal image matching or registration. B\u00fcrgmann et al.22 input SAR and optical image patches of larger size into a modified Siamese network based on HardNet to obtain sparse feature descriptors and use the L2 distance of feature descriptors to obtain a similarity metric. Zhang et al.24 adopted the SFcNet to learn the similarity score between two different kinds of image patches.\nHowever, the Siamese network ignores the radiometric differences between SAR and optical images and the scattered noise in SAR images. Hughes et al.23,24 explained that pseudo-Siamese networks are better suited for multimodal image matching. MatchosNet26 and CNet,27 based on pseudo-Siamese network structure, have achieved average results in SAR and optical image registration. MatchosNet is template matching, which can obtain better results only when there is no large translation or rotation transformation between the two images to be registered. However, the pseudo-Siamese network lacks the interaction between different modalities in the process of learning features, so it is more difficult to explore the shared features between different modalities. In addition, pseudo-Siamese networks increase the network parameters, which makes convergence more difficult. Therefore, we design a partially shared network to mine similar features in multimodal images while considering the differences between modalities."
        },
        {
            "heading": "2.2 Nonlocal Attention",
            "text": "Due to the limited perceptual field of CNN, it is difficult to capture the global correlations of images. Thus it does not cope well with multiple transformations and nonlinear radiometric differences in multimodal image registration. Due to its sequence (image block) modeling structure, the transformer30 based on self-attention mechanism is particularly good at capturing global interactions between token embeddings. ViT31 has successfully applied the transformer30 from natural language processing to computer vision.32\u201334 It demonstrates that transformer also has a strong ability to model spatial correlations of images. The main reason for the success of the transformer is its core component, the self-attention module that captures global information. The self-attention module computes the response at a position in a sequence (e.g., a sentence) by focusing on all positions and taking their weighted average in the embedding space. In the"
        },
        {
            "heading": "Journal of Applied Remote Sensing 036504-3 Jul\u2013Sep 2023 \u2022 Vol. 17(3)",
            "text": "Downloaded From: https://www.spiedigitallibrary.org/journals/Journal-of-Applied-Remote-Sensing on 09 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use\nregistration task, the two images to be registered may have scale, rotation, translation, and other transformations. Due to various transformations, the local information of an image changes. However, the relative position between two pixels in one image is constant, which is equivalent to the relative position of two words in a sentence in natural language. Mathematically, self-attention can be understood as a nonlocal averaging operation35 that captures long-distance dependencies. To fully exploit the features, we use a nonlocal operation similar to the self-attention module in the transformer to extract global features and CNN to extract local features."
        },
        {
            "heading": "3 Proposed Method",
            "text": "The proposed algorithm framework in this paper is depicted in Fig. 1. First, we use the phase congruency maximum moments (MMPC)19 to extract keypoints from SAR and optical images. Then we cut patches centered on the keypoints to generate SAR and optical image patches. These patches are converted into 8-bit grayscale images and then fed into MSA-Net to learn deep features. The matching points are obtained by the nearest-neighbor matching strategy. Finally, the final registration results are obtained after removing the outliers by random sample consensus (RANSAC).\nAs shown in Fig. 2, the proposed MSA-Net learns modality-shared features between the SAR and optical images to improve multimodal image registration performance. First, the MSpFL module is proposed to extract specific shallow features from SAR and optical images. Second, the MShFL module is used to extract deep local and global features shared between the two modalities. The MShFL module includes three LFE modules and three NLA modules. Third, a triplet loss function with a cross-modality similarity constraint is used to match multimodal features. The structure of MSA-Net is shown in Table 1.\nGiven a set of SAR and optical patch pairs, psi \u2286 ps, poi \u2286 po, i \u2208 1; 2; : : : ; N, where N is the number of the patches. When i \u00bc j, psi \u00bc poj are the corresponding SAR and optical image patches. Thus psi \u00bc poj are the noncorresponding SAR and optical image patches, when i \u2260 j. The goal of the MSA-Net is to make the distances between the corresponding patch pairs closer and the distances between the noncorresponding patch pairs farther. First, f\u00f0psi ; poi \u00de; i \u00bc 1; 2; : : : ; Ng are separately fed into the MSpFLSAR and MSpFLOPT modules to obtain N pairs of modality-specific feature maps f\u00f0fsi ; foi \u00de \u2208 Rc\u00d7h\u00d7w; i \u00bc 1; 2; : : : ; Ng, where h and w represent the spatial height and width of the feature map, and c is the channel dimension of each feature map. fsi and f o i represent two i\u2019th shallow modality-specific features. Second, the MShFL module extracts deep modality-shared features. Each MShFL module includes three LFE modules and three NLA modules. The LFE module is used to extract local features, and the NLA module is used to extract global features. Third, the feature maps obtained by MShFL are fed into a kernel size with 8 \u00d7 8 convolution to obtain the 128-dimensional deep feature vectors f\u00f0esi ; eoi \u00de \u2208 R128\u00d71; i \u00bc 1; 2; : : : ; Ng. Finally, the triplet loss function with a cross-"
        },
        {
            "heading": "Journal of Applied Remote Sensing 036504-4 Jul\u2013Sep 2023 \u2022 Vol. 17(3)",
            "text": "Downloaded From: https://www.spiedigitallibrary.org/journals/Journal-of-Applied-Remote-Sensing on 09 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use\nmodality similarity constraint motivates the MSA-Net to learn more similar feature representations between corresponding SAR and optical image patches."
        },
        {
            "heading": "3.1 Modality-Specific Feature Learning Module",
            "text": "Considering the nonlinear radiation differences between SAR and optical images, we design MSpFL modules that do not share parameters to extract modality-specific features, such as texture, in the shallow network. There are two reasons to propose MSpFL modules. On the one hand, since the difference between modalities is not considered, it is difficult for the Siamese network to extract effective features that are favorable for multimodal image registration. On the other hand, if a pseudo-Siamese network is used, it will increase the number of network parameters and the difficulty of convergence.\nFig. 2 Framework of the proposed MSA-Net."
        },
        {
            "heading": "Journal of Applied Remote Sensing 036504-5 Jul\u2013Sep 2023 \u2022 Vol. 17(3)",
            "text": "Downloaded From: https://www.spiedigitallibrary.org/journals/Journal-of-Applied-Remote-Sensing on 09 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use\nBased on the above analysis, this paper proposes a partially shared feature extraction network. The MSpFL module is used to extract the specific shallow features of SAR and optical images. Suppose that the SAR and optical image patch pairs are f\u00f0psi ; poi \u00de; i \u00bc 1; 2; : : : ; Ng, psi is the i\u2019th SAR image patch, and poi is the corresponding optical image patch of p s i . When they are fed into the MSpFL module, the modality-specific features learned from SAR and optical images can be expressed as follows:\nEQ-TARGET;temp:intralink-;e001;114;664fsi \u00bc MSpFLSAR\u00f0psi \u00de; foi \u00bc MSpFLOPT\u00f0poi \u00de; (1) where MSpFLSAR and MSpFLOPT represent the branches that extract modality-specific features of SAR and optical images, resepctively. fsi is the learned SAR feature of p s i , and f o i is the learned optical feature of poi . MSpFLSAR andMSpFLOPT are two networks that do not share parameters but have the same structure. As shown in Table 1,MSpFLSAR andMSpFLOPT contain two convolutional layers with a kernel size of 3 \u00d7 3 and a number of convolutional filters of 32. The strides of the two convolutional layers are 2 and 1, respectively. The first convolutional layer is followed by a batch normalization (BN) layer and a rectified linear unit (ReLU) layer, and the second convolutional layer is followed by a BN layer."
        },
        {
            "heading": "3.2 Modality-Shared Feature Learning Module",
            "text": "After extracting the shallow specific features from the two modalities, we need to further extract the shared deep features. Here we design the MShFL module to learn the shared features from SAR and optical images, which includes three LFE modules and three NLA modules:\nEQ-TARGET;temp:intralink-;e002;114;472 s i \u00bc MShFL\u00f0fsi \u00de; eoi \u00bc MShFL\u00f0foi \u00de; (2)\nwhere esi is the shared feature of f s i , e o i is the shared feature of f o i , and MShFL is the MShFL module.\n3.2.1 Local feature extract module\nWe design three LFE modules in the MShFL module to extract local features shared between SAR and optical images. Each LFE module includes a residual structure of two convolution layers with a kernel size of 3 \u00d7 3. The strides of the two convolution layers are 2 and 1, respectively. The channels of the two convolutional layers in the three modules are 32, 64, and 128. Residual connections are added to prevent the network from overfitting. The learned modalityshared local features can be represented as follows:\nEQ-TARGET;temp:intralink-;e003;114;315fsi;k\u00fe1 \u00bc LFEk\u00f0fsi;k\u00de; foi;k\u00fe1 \u00bc LFEk\u00f0foi;k\u00de; (3) where LFEk represents k\u2019th shared LFE module, k \u00bc 1, 2, 3. fsi;k and foi;k represent the SAR and optical features input to LFEk, respectively. fsi;k\u00fe1 and f o i;k\u00fe1 represent the SAR and optical features output from LFEk, respectively.\n3.2.2 Nonlocal attention module\nAlthough CNN has good perception abilities for local regions, it lacks the modeling of global information. For the registration task, the information of the local area is important, but the global information can better describe the long-distance correlation and has good robustness to changes in scale, rotation, etc. For example, in two images with the same scene, the content in the images remains the same even after a slight rotation and translation transformation. Local information cannot capture unchanged information due to its limited perceptual range, while global information can still capture relevant information due to its large perceptual region. The NLA module aims at strengthening the features of the current pixel position by aggregating the information from all other positions in the feature map.\nAs shown in Fig. 3, x \u2208 Rc\u00d7h\u00d7w is transformed into x 0 \u2208 Rc\u00d7hw by the reshape operation R. Let x 0 denotes the input feature map of the NLA module, where c is the number of channels, and hw is the number of positions in the feature map. We use three 1 \u00d7 1 convolutions that form three functions called \u03b1, g, and v, reducing the number of channels from c to c\u2215r for the input x. In our"
        },
        {
            "heading": "Journal of Applied Remote Sensing 036504-6 Jul\u2013Sep 2023 \u2022 Vol. 17(3)",
            "text": "Downloaded From: https://www.spiedigitallibrary.org/journals/Journal-of-Applied-Remote-Sensing on 09 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use\nexperiments, we set the reduction factor r to 2. We use dot-product similarity30 to define the normalized pairwise relationship between positions i and j in a feature map:\nEQ-TARGET;temp:intralink-;e004;117;603wij \u00bc f\u00f0x 0i ; x 0j\u00de C\u00f0x\u00de \u00bc hW\u03b1x 0;Wgx 0ji hw ; (4)\nwhere wij \u2208 Rhw\u00d7hw is a normalized similarity matrix, f\u00f0x 0i ; x 0j\u00de is the relationship between positions i and j, W\u03b1 and Wg are the weights of two convolution functions \u03b1 and g. C\u00f0x\u00de is a normalization factor, in this case C\u00f0x\u00de \u00bc hw.\nThen the similarity matrix wij is multiplied by the value matrix Wv to obtain the output matrix:\nEQ-TARGET;temp:intralink-;e005;117;501yi \u00bc Xhw j\u00bc1 wij\u00f0Wv \u00b7 x 0j\u00de; (5)\nwhere yi is the attention value of position i, and Wv is the weight of the convolution function v. The output of the NLA module is defined as\nEQ-TARGET;temp:intralink-;e006;117;433z 0i \u00bc Wzyi \u00fe x 0i ; (6)\nwhere Wz is a 1 \u00d7 1 convolution with the channel number of c, \u201c\u00fe\u201d in the formula indicates residual connection, z 0i represents the output of the NLA module at position i. z\n0 \u2208 Rc\u00d7hw is transformed into z \u2208 Rc\u00d7h\u00d7w by the reshape operation R. The input feature dimension of the NLA module is the same as the output feature dimension, which allows us to insert it anywhere in the network.\nThe NLA module can be regarded as a global information construction module. The specific operation is to obtain the global contextual features of the current pixel by the weighted average of all positions."
        },
        {
            "heading": "3.3 Triplet Loss with Similarity Constraint",
            "text": "In order to learn modality-shared features between the SAR and the corresponding optical patch pairs, the triple loss function is used to minimize the distances of the features of corresponding patch pairs and maximize the distances of the features of noncorresponding patch pairs. The margin m is a hyperparameter that aims to keep a certain distance between the corresponding and noncorresponding features. The triple loss Lt is calculated as follows:\nEQ-TARGET;temp:intralink-;e007;117;228Lt \u00bc 1\nNt XNt i\u00bc1 max\u00f0d\u00f0esi ; eoi \u00de \u2212 d\u00f0esi ; eoj \u00de \u00fem; 0\u00de; (7)\nwhere d\u00f0esi ; eoi \u00de \u00bc ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 2 \u2212 2esi eoi p represents the L2 pairwise distance of the corresponding fea-\ntures, d\u00f0esi ; eoj \u00de \u00bc ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 2 \u2212 2esi eoj p , i \u2260 j, represents the L2 pairwise distance of the noncorresponding features, and Nt represents the number of samples in a batch, m \u00bc 1. Negative sampling is usually required when calculating the triple loss function, which usually includes random sampling in all samples20 and hard negative sample sampling in batches.21 We adopt the latter to improve the optimization performance of the MSA-Net. Under hard negative sample mining, the triple loss is rewritten as\nFig. 3 NLAmodule extracts global features. \u201cR\u201d represents reshape operation, \u201c\u2297\u201d denotes matrix multiplication, and \u201c \u201d denotes element-wise sum."
        },
        {
            "heading": "Journal of Applied Remote Sensing 036504-7 Jul\u2013Sep 2023 \u2022 Vol. 17(3)",
            "text": "Downloaded From: https://www.spiedigitallibrary.org/journals/Journal-of-Applied-Remote-Sensing on 09 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use\nEQ-TARGET;temp:intralink-;e008;114;736Lt \u00bc 1\nNt XNt i\u00bc1 max\u00f0d\u00f0esi ; eoi \u00de \u2212 d\u00f0esi ; eohard\u00de \u00fem; 0\u00de; (8)\nwhere eohard is the hard negative sample in the minibatch of e s i .\nIn order to reduce the modal differences between corresponding patch pairs of SAR and optical images, it is necessary to add similarity constraints between them. In this way, the network can learn more compact feature representations between corresponding patches. The crossmodality similarity constraint loss function Ls is defined as\nEQ-TARGET;temp:intralink-;e009;114;639Ls \u00bc 1\nNt XNt i\u00bc1 log\u00f01\u00fe exp\u00f0d\u00f0esi ; eoi \u00de\u00de\u00de; (9)\nwhere log and exp represent a logarithmic and exponential function, respectively. Therefore, the overall loss function is expressed as\nEQ-TARGET;temp:intralink-;e010;114;573L \u00bc Lt \u00fe \u03bbLs; (10) where \u03bb is the weighting coefficient of the loss function."
        },
        {
            "heading": "4 Experiments and Discussion",
            "text": "To validate the performance of the proposed MSA-Net, the experiments are performed on a public SAR and optical image dataset SEN1-236 and five pairs of SAR and optical images. We compare our method with three handcrafted and three deep learning methods, including OS-SIFT,16 PCSD,18 RIFT,19 HardNet,21 MatchosNet,26 and CNet.27 Section 4.1 describes a data description. Section 4.2 introduces implementation details for the training stage, testing stage, and matching stage. Section 4.3 describes the evaluation criteria used to evaluate algorithm performance. Section 4.4 presents the results and analysis of the experiments. Section 4.4.1 analyzes the influence of three weight-sharing strategies on the model\u2019s performance. In Secs. 4.4.2 and 4.4.3, we verify the effectiveness of the NLA module and the cross-modality similarity constraint. In Sec. 4.4.4, we conduct a parameter sensitivity analysis to analyze the influence of parameter \u03bb on the matching accuracy. In Sec. 4.4.5, we analyze the influence of fine tuning on registration results. In Sec. 4.4.6, we compare the registration results of seven algorithms on five pairs of SAR and optical images, including qualitative analysis and quantitative evaluation. The computational times of seven methods are shown in Sec. 4.4.7."
        },
        {
            "heading": "4.1 Data Description",
            "text": "The proposed MSA-Net is trained on SEN1-2,36 which is a public SAR and optical image dataset. The SEN1-2 includes 282,384 co-registered SAR and optical images, each with a size of 256 \u00d7 256 and a resolution of 10 m. The SEN1-2 dataset contains four folders: ROIs1158_spring, ROIs1868_summer, ROIs1970_fall, and ROIs2017_winter. In order to ensure that the training samples and test samples do not overlap, we select the data in ROIs1868_ summer as the training samples and the data in ROIs1970_fall as the test samples and filter out fuzzy images without obvious ground objects, as shown in Fig. 4. We uniformly sample the images selected from SEN1-2 with a stride of 32 to obtain a patch of size 64 \u00d7 64. After rotation and zooming, we obtain 70,193 training sample pairs and 11,147 validation sample pairs. The information about five pairs of SAR and optical images used for testing is shown in Table 2."
        },
        {
            "heading": "Journal of Applied Remote Sensing 036504-8 Jul\u2013Sep 2023 \u2022 Vol. 17(3)",
            "text": "Downloaded From: https://www.spiedigitallibrary.org/journals/Journal-of-Applied-Remote-Sensing on 09 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use\nThe \u201c*\u201d in Table 2 represents unknown information. The SAR and optical images in pairs 1 to 3 are from TerraSAR-X and Google Earth, and the SAR and optical images in pairs 4 to 5 are from GaoFen-3 and Google Earth. In this paper, all experiments are conducted on a desktop computer with Windows 10, an RTX 3060 GPU, and 24 GB RAM. The MSA-Net proposed in this paper is built using the PyTorch37 deep learning framework.\n4.2 Implementation Details\n4.2.1 Training stage\nWe first train our network from scratch on 70,193 pairs of training samples in SEN1-2 and validate it on 11,147 pairs of validation samples. The batch size is 300. The SGD optimizer with an initial learning rate of 0.1 is adopted. The training epoch is 20. Next, the network is fine-tuned with the five pairs of images before testing. Specifically, we manually select four points on each SAR and optical image and use HOPC17 to get registered images. According to the stride of 16, we cut the registered image into patches with a size of 64 \u00d7 64. Some blank patches without content are removed. The reason for using a stride of 16 instead of 32 for the test image is that the number of test data is small, and reducing the stride can generate more patches. Meanwhile, reducing the stride results in more overlap between adjacent patches, leading to increased similarity between their contents. This forces the network to learn more discriminative features, so that the feature distance between different patches is as large as possible. These patches are amplified using the same data enhancement techniques as SEN1-2. The purpose of using overlapping blocks instead of nonoverlapping blocks is to make MSA-Net learn more discriminative features from certain similar samples. When one pair of images is tested, the remaining pairs of images are used to fine-tune the network. The parameters m and \u03bb in the proposed loss function are 1 and 0.1, respectively.\n4.2.2 Testing stage\nThe feature points are extracted using the MMPC.19 Next, a local patch with a size of 64 \u00d7 64 is cropped around each feature point. Then the local patch is fed into the network to obtain the 128- dimensional feature vector, that is, the proposed modality-shared feature descriptor.\n4.2.3 Matching stage\nWe match the modality-shared feature descriptors corresponding to each feature point through nearest-neighbor matching to obtain the initial matching points. The affine transformation parameters from the image to be registered (optical image) to the reference image (SAR image) are"
        },
        {
            "heading": "Journal of Applied Remote Sensing 036504-9 Jul\u2013Sep 2023 \u2022 Vol. 17(3)",
            "text": "Downloaded From: https://www.spiedigitallibrary.org/journals/Journal-of-Applied-Remote-Sensing on 09 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use\ncalculated by using the initial matching points. Finally, outliers are deleted by RANSAC to obtain the final registration result."
        },
        {
            "heading": "4.3 Evaluation Criteria",
            "text": "We use two metrics to evaluate the matching performance, including FPR95 and accuracy. The FPR95 refers to the false positive rate in which the true positive rate equals 95%, the accuracy refers to the probability that the network correctly predicts the matching label of two patches. Therefore, the smaller the FPR95 is, the higher the accuracy is, and the better the matching performance will be.\nWe quantitatively evaluate different registration methods using the number of correct matches (NCM), the ratio of correct matches (RCM), and the root-mean-square error (RMSE) as evaluation metrics:\nEQ-TARGET;temp:intralink-;e011;114;592 CM \u00bc NCM NM ; (11)\nEQ-TARGET;temp:intralink-;e012;114;547 MSE \u00bc ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1\nNCM XNCM i\u00bc1\n\u00bd\u00f0m 0i \u2212mi\u00de2 \u00fe \u00f0n 0i \u2212 ni\u00de2 vuut ; (12)\nwhere NM is the total of matches, \u00f0m 0i ; n 0i \u00de is the coordinate of the image to be registered after the transformation matrix, and \u00f0mi; ni\u00de is the coordinate of the reference image."
        },
        {
            "heading": "4.4 Experimental Results and Analysis",
            "text": "4.4.1 Influence of three weight-sharing strategies\nAs the analysis in Sec. 2.1 shows, although the Siamese network with completely shared weights can mine shared features, it may ignore modal differences. The pseudo-Siamese network with completely unshared weights can extract modality-shared features, but it does not take modalityshared features into account. Therefore, this paper discusses three weight-sharing strategies, including completely unshared, completely shared, and partially shared weights. The experiment is carried out on the SEN1-2 dataset, and the experimental results are shown in Table 3. The \u201cDist_np\u201d in Table 3 represents the L2 pairwise distance between positive samples and negative samples. The larger the distance is, the better the network\u2019s ability to distinguish between positive and negative samples will be.\nFrom Table 3, we can see that the network structure with completely shared weights achieves the lowest matching accuracy. The FPR95 is the highest and Dist_np is the lowest among the three weight-sharing strategies, which indicates that the network with completely shared weights does not cope well with differences of multimodal images. The third and fourth rows in Table 3 are the experimental results of the completely unshared weight network and the partially shared weight network, respectively. The latter achieves a higher matching accuracy, about 1.1% higher than the former, and the FPR95 of the latter is 0.1769, about 0.06 lower than the former. The Dist_np of the partially shared weight network is 0.2299, about 0.03 higher than the completely unshared weight network. Therefore, we propose a dual-stream network with partially shared weights can better mine the modality-shared features of SAR and optical images and improve matching accuracy.\nTable 3 Matching results of different weight-sharing strategies.\nWeight-sharing strategy Accuracy FPR95 Dist_np\nCompletely unshared 0.9539 0.2356 0.2035\nCompletely shared 0.9346 0.2497 0.1943\nPartially shared 0.9669 0.1769 0.2299"
        },
        {
            "heading": "Journal of Applied Remote Sensing 036504-10 Jul\u2013Sep 2023 \u2022 Vol. 17(3)",
            "text": "Downloaded From: https://www.spiedigitallibrary.org/journals/Journal-of-Applied-Remote-Sensing on 09 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use\n4.4.2 Effectiveness of the NLA module\nFigures 5(a) and 5(b) show the changing trend of FPR95 and accuracy under three network structures, which are HardNet, MSA-Net without NLA, and the proposed MSA-Net. MSA-Net without NLA represents the network structure after removing the NLA module from MSA-Net. It can be seen that the matching accuracy of MSA-Net is the highest and the FPR95 of MSA-Net is the lowest among the three models. The NLA module mines global features with a larger receptive field. This is so that MSA-Net can cope with radiation differences to a certain extent, thus improving matching performance. We can also see that MSA-Net without NLA achieves higher matching accuracy and lower FPR95 in fewer epochs compared to HardNet. The difference between the two models lies in whether the weights of the feature network are completely shared and whether residual connections are used. HardNet learns features from SAR and optical images using a network with completely shared weights and no residual connections, both of which make HardNet less effective than MSA-Net without NLA in handling multimodal data.\n4.4.3 Influence of loss function\nFigures 5(c) and 5(d) show the FPR95 and accuracy change curves of different loss functions. After adding cross-modality similarity constraints to the loss function, the matching accuracy is further improved and the FPR95 is further reduced. It proves that the combination of triplet loss and cross-modality similarity constraint loss allows the MSA-Net to further explore the similarity between SAR and optical images.\n4.4.4 Parameter sensitivity analysis\nIn this section, we analyze the influence of the weight coefficient \u03bb on matching accuracy. To observe the changing trend of accuracy in the test set of SEN1-2, we gradually increase the value of \u03bb from 0 to 1 with an interval of 0.1. Figure 6 reports the average of three experiments. When\n(a) (b) (c) (d)\nFig. 5 Changing trend of (a) FPR95 and (b) accuracy with epoch under different models. Changing trend of (c) FPR95 and (d) accuracy with epoch under different loss functions."
        },
        {
            "heading": "Journal of Applied Remote Sensing 036504-11 Jul\u2013Sep 2023 \u2022 Vol. 17(3)",
            "text": "Downloaded From: https://www.spiedigitallibrary.org/journals/Journal-of-Applied-Remote-Sensing on 09 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use\nthe value of \u03bb is between 0 and 1, the value of accuracy is between 0.978 and 0.982. It indicates that adding cross-modality similarity constraints to the loss function will not affect the stability of the network. When \u03bb is 0.1, accuracy reaches its maximum value. It can be seen that a small weight for Ls can make the network learn well. When \u03bb increases to 1, accuracy gradually decreases. We conclude that giving a larger \u03bb to cross-modal similarity constraints is not conducive to learning features of multimodal images. We should not let the network pay too much attention to learning similar features without considering that the multimodal data itself is different. So it is necessary to obtain an appropriate weight coefficient through experiments.\n4.4.5 Influence of fine-tuning on registration results\nIn Fig. 7, we show the influence of fine-tuning on the registration results of five test images, including NCM and RMSE. Figure 7(a) shows that the NCM increases significantly after fine-tuning, especially in pairs 1, 2, 4, and 5. As shown in Fig. 7(b), the RMSEs of pairs 1 and 2 decrease after fine-tuning. The RMSE of pair 3 is higher after fine-tuning. The RMSEs of pairs 4 and 5 do not change significantly before and after fine-tuning. It can be seen that registration results can be improved to some extent through fine-tuning. Even without fine-tuning, relatively good results can be obtained, indicating that the model trained with SEN1-2 data has transfer ability. Considering that different data are distributed differently, fine-tuning can improve the adaptability of the network and obtain better registration results.\n4.4.6 Comparison with the other registration methods\nWe select five pairs of SAR and optical images for evaluating seven registration methods. In pairs 1 to 3, there are not only radiation differences between SAR and optical images but also rotation and translation transformations. Pairs 4 and 5 have obvious radiation differences, noise, slight translation transformations, and no rotation transformation. Figures 8(a)\u20138(g)\u201312(a)\u201312(g) show the matching points obtained by seven methods in five pairs of SAR and optical images. Figures 8(h)\u201312(h) show the registration results of MSA-Net. We superimpose the outlines (as shown by the yellow curve) of transformed optical images on the original SAR images and use several red rectangles to highlight the registration results for certain regions.\nFrom Figs. 8(a)\u20138(c)\u201312(a)\u201312(c), we can see that the three handcrafted algorithms have achieved relatively good registration results for the five pairs of images. RIFT obtains more matching points than OS-SIFT and PCSD thanks to its use of radiation-invariant phase congruency to extract keypoints and construct descriptors. OS-SIFT has the fewest matching points among the three handcrafted algorithms because the descriptor based on gradient is worse than the one based on phase congruency in dealing with radiation differences. PCSD matches descriptors in the local window, resulting in fewer matching points obtained.\n(a) (b)\nFig. 7 Influence of fine-tuning on the registration results of five test image pairs. (a) NCM obtained by MSA-Net in five pairs of test images with or without fine-tuning. (b) RMSE obtained by MSA-Net in five pairs of test images with or without fine-tuning."
        },
        {
            "heading": "Journal of Applied Remote Sensing 036504-12 Jul\u2013Sep 2023 \u2022 Vol. 17(3)",
            "text": "Downloaded From: https://www.spiedigitallibrary.org/journals/Journal-of-Applied-Remote-Sensing on 09 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use\nAmong the four deep learning methods, MatchosNet obtains few matching points with obvious errors in pairs 1 to 3 [as shown in Figs. 8(e)\u201310(e)], but a certain number of correct matching points can be found in pairs 4 and 5 [as shown in Figs. 11(e) and 12(e)]. The main reason is that MatchosNet is a position-matching algorithm, which needs to know the approximate offset of the two images to be registered in advance. Therefore, it is often difficult for MatchosNet to find the correct number of matching points in a pair of images with rotation and translation transformations. HardNet, CNet, and MSA-Net are patch-based feature extraction networks. The difference between the three is the weight-sharing strategy of the networks. Specifically, HardNet is a Siamese network with weight sharing, CNet is a pseudo-Siamese network without weight sharing completely, and MSA-Net is a network structure with partial weight\n(a) (b) (c) (d)\n(e) (f) (g) (h)"
        },
        {
            "heading": "Journal of Applied Remote Sensing 036504-13 Jul\u2013Sep 2023 \u2022 Vol. 17(3)",
            "text": "Downloaded From: https://www.spiedigitallibrary.org/journals/Journal-of-Applied-Remote-Sensing on 09 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use\nsharing. HardNet does not consider the modal differences, and CNet does not take the mining of the modality-shared features into account, so the matching points obtained by both [as shown in Figs. 8(d) and 8(f)\u201312(d) and 12(f)] are not as much as those obtained by MSA-Net. As shown in Figs. 8(g)\u201312(g), our proposed MSA-Net achieves the highest number of matching points among the seven registration methods. MSA-Net combines two-stream CNNs and NLA to extract both local and global features and obtains features with different perceptual ranges to cope with radiation differences between SAR and optical images. In Figs. 8(h)\u201312(h), after carefully observing the registration results, we find that the outlines of the transformed optical image and the corresponding SAR images almost overlap each other. Meanwhile, the regions circled by the red rectangles are well-matched. In summary, compared with the other six methods, the proposed MSA-Net achieves the best registration results and the most matches on the five pairs of images, which can prove the effectiveness of the proposed method.\nTable 4 shows the quantitative comparison of the seven methods on five pairs of test images, including NCM, RCM, and RMSE. The higher the NCM and RCM and the lower the RMSE, the better the performance of the registration algorithm. The \u201c*\u201d in Table 4 indicates that the method is invalid for this pair of images. Even though the algorithm finds some matching points, the error of these matching points is relatively large.\nOur proposed MSA-Net obtains the highest NCM and RCM and the lowest RMSE among the seven algorithms. This is mainly due to the fact that our algorithm not only considers the radiation differences between SAR and optical images but also mines the shared features between the two modalities to improve the similarity of features. In order to achieve the above, MSA-Net\n(a) (b) (c) (d)\n(e) (f) (g) (h)"
        },
        {
            "heading": "Journal of Applied Remote Sensing 036504-14 Jul\u2013Sep 2023 \u2022 Vol. 17(3)",
            "text": "Downloaded From: https://www.spiedigitallibrary.org/journals/Journal-of-Applied-Remote-Sensing on 09 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use\nconstructs two MSpFL modules and three MShFL modules to extract local and global features and designs a loss function that considers similarity constraints. PCSD, RIFT, HardNet, and CNet all find some correct matching points in the five pairs of test images. PCSD and RIFT construct descriptors based on phase congruency, which can cope with radiation differences. HardNet and CNet are patch-based deep feature extraction networks, and the network can mine similar features of the two modalities through data learning. However, as the radiation difference increases, the matching points will decrease accordingly. PCSD, RIFT, HardNet, and CNet obtain more matching points in pairs 1 and 2 than in pairs 3 to 5. OS-SIFT and MatchosNet fail on some image pairs. OS-SIFT is a gradient-based descriptor that is susceptible to radiation differences. MatchosNet is an algorithm based on position matching, which fails in image pairs with rotation and translation transformations.\n4.4.7 Computational time of the seven methods\nThe computational time of the seven algorithms on the five pairs of images is shown in Table 5. Among the seven algorithms, RIFT has the fastest computational efficiency. This is because it"
        },
        {
            "heading": "2 NCM 40 54 65 67 17 57 78",
            "text": ""
        },
        {
            "heading": "3 NCM 21 23 25 26 16 21 29",
            "text": ""
        },
        {
            "heading": "4 NCM 10 16 47 16 22 34 69",
            "text": ""
        },
        {
            "heading": "5 NCM 18 25 48 13 27 17 62",
            "text": "Table 5 Computational time of the seven methods.\nPair OS-SIFT (s) PCSD (s) RIFT (s) HardNet (s) MatchosNet (s) CNet (s) MSA-Net (s)"
        },
        {
            "heading": "1 31.32 44.45 12.73 13.79 17.16 16.05 19.33",
            "text": ""
        },
        {
            "heading": "2 25.32 41.26 9.57 11.86 14.28 17.89 14.34",
            "text": ""
        },
        {
            "heading": "3 24.92 45.26 7.71 8.82 10.65 13.14 13.34",
            "text": ""
        },
        {
            "heading": "4 19.73 42.24 8.63 9.65 10.65 16.14 18.47",
            "text": ""
        },
        {
            "heading": "5 20.92 43.10 7.71 9.78 10.43 15.42 16.87",
            "text": ""
        },
        {
            "heading": "Journal of Applied Remote Sensing 036504-15 Jul\u2013Sep 2023 \u2022 Vol. 17(3)",
            "text": "Downloaded From: https://www.spiedigitallibrary.org/journals/Journal-of-Applied-Remote-Sensing on 09 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use\nuses the FAST detector to extract key points, the phase consistency algorithm to extract descriptors, and the ratio threshold for matching. The calculation time of OS-SIFT is much longer than that of RIFT. Because OS-SIFT extracts keypoints and constructs descriptors in the multiscale space, and it takes a certain amount of time to construct a multiscale space. PCSD takes the longest time, mainly because it uses template matching. Each keypoint calculates its similarity to all keypoints in the local window. The larger the window is, the more the keypoints are, and the longer the time will be. The computational time of the four deep learning algorithms is relatively close. HardNet takes the least time because it is a Siamese network with the least network parameters. MatchosNet and CNet are pseudo-Siamese networks with more parameters than HardNet. MSA-Net has a partially shared network structure and three NLA modules. The computational complexity of the NLA module is the square of the number of samples. So the computation time of MSA-Net is longer than that of MatchosNet and CNet."
        },
        {
            "heading": "5 Conclusion",
            "text": "This paper proposes a deep learning algorithm, called MSA-Net, for accomplishing SAR and optical image registration. The algorithm combines a dual-stream network with NLA to construct modality-shared features descriptors for SAR and optical images. In MSA-Net, the MSpFL module extracts shallow features from SAR and optical images to obtain modality-specific features, whereas the MShFL module combines the LFE module and NLA module to obtain richer deep modality-shared features, including both local and global features. A loss function that combines triplet loss with cross-modality similarity constraint is also proposed to further improve matching accuracy by constraining the network to be immune to radiometric differences. The experiments are conducted on a public dataset and five pairs of test SAR and optical images. Specifically, the validation experiments verify that the partially shared weight strategy, the NLA, and cross-modality similarity constraints in MSA-Net help to improve the matching accuracy of SAR and optical images. In addition, the comparative experimental results demonstrate that our proposed method achieves the best registration results among the seven registration methods. In future work, we would like to study more diverse attention mechanisms and feature fusion techniques to further enhance the capabilities of the network for SAR and optical image registration."
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors would like to thank all the anonymous reviewers for their valuable comments that helped to improve the quality of this paper. No potential conflicts of interest were reported by the author(s). This work was supported by the National Natural Science Foundation of China (Grant No. 62172321) and the Civil Space Thirteen Five Years Pre-Research Project (Grant No. D040114)."
        },
        {
            "heading": "Journal of Applied Remote Sensing 036504-16 Jul\u2013Sep 2023 \u2022 Vol. 17(3)",
            "text": "Downloaded From: https://www.spiedigitallibrary.org/journals/Journal-of-Applied-Remote-Sensing on 09 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use\n9. X. Li et al., \u201cCollaborative attention-based heterogeneous gated fusion network for land cover classification,\u201d IEEE Trans. Geosci. Remote Sens. 59(5), 3829\u20133845 (2020). 10. W. Kang et al., \u201cCFNet: a cross fusion network for joint land cover classification using optical and SAR images,\u201d IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 15, 1562\u20131574 (2022). 11. X. Li et al., \u201cDense adaptive grouping distillation network for multimodal land cover classification with privileged modality,\u201d IEEE Trans. Geosci. Remote Sens. 60, 1\u201314 (2022). 12. X. Li, L. Lei, and G. Kuang, \u201cLocality-constrained bilinear network for land cover classification using heterogeneous images,\u201d IEEE Geosci. Remote Sens. Lett. 19, 1\u20135 (2022). 13. D. G. Lowe, \u201cDistinctive image features from scale-invariant keypoints,\u201d Int. J. Comput. Vision 60(2), 91\u2013110 (2004). 14. J. Fan et al., \u201cSAR image registration using phase congruency and nonlinear diffusion-based SIFT,\u201d IEEE Geosci. Remote Sens. Lett. 12(3), 562\u2013566 (2015). 15. F. Dellinger et al., \u201cSAR-SIFT: a SIFT-like algorithm for SAR images,\u201d IEEE Trans. Geosci. Remote Sens. 53(1), 453\u2013466 (2015). 16. Y. Xiang, F. Wang, and H. You, \u201cOS-SIFT: a robust SIFT-like algorithm for high-resolution Optical-to-SAR image registration in suburban areas,\u201d IEEE Trans. Geosci. Remote Sens. 56(6), 3078\u20133090 (2018). 17. Y. Ye et al., \u201cRobust registration of multimodal remote sensing images based on structural similarity,\u201d IEEE Trans. Geosci. Remote Sens. 55(5), 2941\u20132958 (2017). 18. J. Fan et al., \u201cSAR and optical image registration using nonlinear diffusion and phase congruency structural descriptor,\u201d IEEE Trans. Geosci. Remote Sens. 56(9), 5368\u20135379 (2018). 19. J. Li, Q. Hu, and M. Ai, \u201cRIFT: multi-modal image matching based on radiation-variation insensitive feature transform,\u201d IEEE Trans. Image Process. 29, 3296\u20133310 (2020). 20. Y. Tian, B. Fan, and F. Wu, \u201cL2-Net: deep learning of discriminative patch descriptor in Euclidean space,\u201d in Proc. IEEE Conf. Comput. Vision and Pattern Recognit., pp. 661\u2013669 (2017). 21. A. Mishchuk et al., \u201cWorking hard to know your neighbor\u2019s margins: local descriptor learning loss,\u201d in Adv. Neural Inf. Process. Syst. (2017). 22. T. B\u00fcrgmann, W. Koppe, and M. Schmitt, \u201cMatching of TerraSAR-X derived ground control points to optical image patches using deep learning,\u201d ISPRS J. Photogramm. Remote Sens. 158, 241\u2013248 (2019). 23. L. H. Hughes et al. \u201cIdentifying corresponding patches in SAR and optical images with a pseudo-Siamese CNN,\u201d IEEE Geosci. Remote Sens. Lett. 15(5), 784\u2013788 (2018). 24. H. Zhang et al., \u201cRegistration of multimodal remote sensing image based on deep fully convolutional neural network,\u201d IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 12(8), 3028\u20133042 (2019). 25. L. H. Hughes et al., \u201cA deep learning framework for matching of SAR and optical imagery,\u201d ISPRS J. Photogramm. Remote Sens. 169, 166\u2013179 (2020). 26. Y. Liao et al., \u201cFeature matching and position matching between optical and SAR with local deep feature descriptor,\u201d IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 15, 448\u2013462 (2021). 27. D. Quan et al., \u201cDeep feature correlation learning for multi-modal remote sensing image registration,\u201d IEEE Trans. Geosci. Remote Sens. 60, 1\u201316 (2022). 28. H. Han, C. Li, and X. Qiu, \u201cMulti-modal remote sensing image matching method based on deep learning technology,\u201d J. Phys. Conf. Ser. 2083(3), 032093 (2021). 29. J. Fan et al., \u201cA novel multiscale adaptive binning phase congruency feature for SAR and optical image registration,\u201d IEEE Trans. Geosci. Remote Sens. 60, 1\u201316 (2022). 30. A. Dosovitskiy et al., \u201cAn image is worth 16 \u00d7 16words: transformers for image recognition at scale,\u201d https:// doi.org/10.48550/arXiv.2010.11929 (2020). 31. A. Vaswani et al., \u201cAttention is all you need,\u201d https://doi.org/10.48550/arXiv.1706.03762 (2017). 32. H. Dong, L. Zhang, and B. Zou, \u201cExploring vision transformers for polarimetric SAR image classification,\u201d\nIEEE Trans. Geosci. Remote Sens. 60, 1\u201315 (2022). 33. H. Chen, Z. Qi, and Z. Shi, \u201cRemote sensing image change detection with transformers,\u201d IEEE Trans.\nGeosci. Remote Sens. 60, 1\u201314 (2022). 34. X. Liu et al., \u201cHigh resolution SAR image classification using global-local network structure based on vision\ntransformer and CNN,\u201d IEEE Geosci. Remote Sens. Lett. 19, 1\u20135 (2022). 35. X. Wang et al., \u201cNon-local neural networks,\u201d in Proc. IEEE Conf. Comput. Vision and Pattern Recognit.,\npp. 7794\u20137803 (2018). 36. M. Schmitt, L. H. Hughes, and X. X. Zhu, \u201cThe SEN1-2 dataset for deep learning in SAR-optical data\nfusion,\u201d in ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., IV-1, pp. 141\u2013146 (2018). 37. A. Paszke et al., \u201cAutomatic differentiation in PyTorch,\u201d in NIPS Workshops (2017).\nXin Hu received her BE degree from Xi\u2019an University of Technology in 2019. She is currently pursuing her PhD with the Remote Sensing Image Processing and Fusion Group of the School of Electronic Engineering at Xidian University. Her main research direction is remote sensing image registration."
        },
        {
            "heading": "Journal of Applied Remote Sensing 036504-17 Jul\u2013Sep 2023 \u2022 Vol. 17(3)",
            "text": "Downloaded From: https://www.spiedigitallibrary.org/journals/Journal-of-Applied-Remote-Sensing on 09 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use\nYan Wu received her BS degree in information processing and her MA and PhD degrees in signal and information processing from Xidian University, Xi\u2019an, China, in 1987, 1998, and 2003, respectively. Since 2005, she has been a professor in the Department of Electronic Engineering, Xidian University. Her broad research interests include remote sensing image analysis and interpretation, data fusion of multisensory images, synthetic aperture radar autotarget recognition, and statistical learning theory and application.\nZhikang Li received his BS degree from the Xidian University, Xi\u2019an, China, in 2021. He is currently pursuing his PhD with the Remote Sensing Image Processing and Fusion Group, School of Electronic Engineering, Xidian University, Xi\u2019an, China. His research interests include synthetic aperture radar image analysis and feature extraction.\nXiaoru Zhao received his BS degree from the Taiyuan University of Technology, Taiyuan, China, in 2020. He is currently pursuing his MA degree with the Remote Sensing Image Processing and Fusion Group, School of Electronic Engineering, Xidian University, Xi\u2019an, China. His research interests include synthetic aperture radar image analysis and feature extraction.\nXingyu Liu received her BS degree in engineering from Xidian University, Shaanxi, China, in 2020, majoring in measurement and control technology and instrumentation program. Currently, she continues to pursue her MS degree with the Remote Sensing Image Processing and Fusion Group, Xidian University. Her main research direction is synthetic aperture radar image feature extraction and classification.\nMing Li received his BS degree in electrical engineering and his MS and PhD degrees in signal processing from Xidian University, Xi\u2019an, China, in 1987, 1990, and 2007, respectively. In 1987, he joined the Department of Electronic Engineering, Xidian University, where he is currently a professor with the National Key Laboratory of Radar Signal Processing. His research interests include adaptive signal processing, detection theory, ultrawideband, and synthetic aperture radar image processing."
        },
        {
            "heading": "Journal of Applied Remote Sensing 036504-18 Jul\u2013Sep 2023 \u2022 Vol. 17(3)",
            "text": "Downloaded From: https://www.spiedigitallibrary.org/journals/Journal-of-Applied-Remote-Sensing on 09 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use"
        }
    ],
    "title": "Synthetic aperture radar and optical image registration using local and global feature learning by modality-shared attention network",
    "year": 2024
}