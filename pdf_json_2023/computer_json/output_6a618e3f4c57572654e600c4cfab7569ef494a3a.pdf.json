{
    "abstractText": "This paper conducts fairness testing on automated pedestrian detection, a crucial but under-explored issue in autonomous driving systems. We evaluate eight widely-studied pedestrian detectors across demographic groups on large-scale real-world datasets. To enable thorough fairness testing, we provide extensive annotations for the datasets, resulting in 8,311 images with 16,070 gender labels, 20,115 age labels, and 3,513 skin tone labels. Our findings reveal significant fairness issues related to age and skin tone. The detection accuracy for adults is 19.67% higher compared to children, and there is a 7.52% accuracy disparity between light-skin and darkskin individuals. Gender, however, shows only a 1.1% difference in detection accuracy. Additionally, we investigate common scenarios explored in the literature on autonomous driving testing, and find that the bias towards dark-skin pedestrians increases significantly under scenarios of low contrast and low brightness. We publicly release the code, data, and results to support future research on fairness in autonomous driving.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xinyue Li"
        },
        {
            "affiliations": [],
            "name": "Zhenpeng Chen"
        },
        {
            "affiliations": [],
            "name": "Jie M. Zhang"
        },
        {
            "affiliations": [],
            "name": "Federica Sarro"
        },
        {
            "affiliations": [],
            "name": "Ying Zhang"
        },
        {
            "affiliations": [],
            "name": "Xuanzhe Liu"
        }
    ],
    "id": "SP:d15a225deb84d1b33caaf554aef121bed0b1ac2e",
    "references": [
        {
            "authors": [
                "Md. Al-Amin Bhuiyan",
                "Abdul Raouf Khan"
            ],
            "title": "Image quality assessment employing RMS contrast and histogram similarity",
            "venue": "Int. Arab J. Inf. Technol. 15,",
            "year": 2018
        },
        {
            "authors": [
                "Sumon Biswas",
                "Hridesh Rajan"
            ],
            "title": "Do the machine learning models on a crowd sourced platform exhibit bias? An empirical study on model fairness",
            "venue": "In Proceedings of the 28th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering,",
            "year": 2020
        },
        {
            "authors": [
                "Sumon Biswas",
                "Hridesh Rajan"
            ],
            "title": "Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline",
            "venue": "In Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (Athens, Greece) (ESEC/FSE",
            "year": 2021
        },
        {
            "authors": [
                "Martim Brandao"
            ],
            "title": "Age and gender bias in pedestrian detection algorithms",
            "venue": "ArXiv abs/1906.10490",
            "year": 2019
        },
        {
            "authors": [
                "Markus Braun",
                "Sebastian Krebs",
                "Fabian Flohr",
                "Dariu M. Gavrila"
            ],
            "title": "EuroCity Persons: A Novel Benchmark for Person Detection in Traffic Scenes",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 41,",
            "year": 2019
        },
        {
            "authors": [
                "Zhaowei Cai",
                "Nuno Vasconcelos"
            ],
            "title": "Cascade r-cnn: Delving into high quality object detection",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition",
            "year": 2018
        },
        {
            "authors": [
                "Jiale Cao",
                "Yanwei Pang",
                "Jin Xie",
                "Fahad Shahbaz Khan",
                "Ling Shao"
            ],
            "title": "From Handcrafted to Deep Features for Pedestrian Detection: A Survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 2020
        },
        {
            "authors": [
                "G. Casella",
                "R.L. Berger"
            ],
            "title": "Statistical Inference (2 ed.)",
            "year": 2007
        },
        {
            "authors": [
                "Joymallya Chakraborty",
                "Suvodeep Majumder",
                "Tim Menzies"
            ],
            "title": "Bias in machine learning software: why? how? what to do",
            "venue": "Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "Zhenpeng Chen",
                "Huihan Yao",
                "Yiling Lou",
                "Yanbin Cao",
                "Yuanqiang Liu",
                "Haoyu Wang",
                "Xuanzhe Liu"
            ],
            "title": "An empirical study on deployment faults of deep learning based mobile applications",
            "venue": "In Proceedings of the 43rd IEEE/ACM International Conference on Software Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "Zhenpeng Chen",
                "Jie M. Zhang",
                "Max Hort",
                "Federica Sarro",
                "Mark Harman"
            ],
            "title": "Fairness Testing: A Comprehensive Survey and Analysis of Trends",
            "year": 2022
        },
        {
            "authors": [
                "Zhenpeng Chen",
                "Jie M. Zhang",
                "Federica Sarro",
                "Mark Harman"
            ],
            "title": "MAAT: a novel ensemble approach to addressing fairness and performance bugs for machine learning software",
            "venue": "In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Zhenpeng Chen",
                "Jie M. Zhang",
                "Federica Sarro",
                "Mark Harman"
            ],
            "title": "A Comprehensive Empirical Study of Bias Mitigation Methods for Machine Learning Classifiers",
            "venue": "ACM Trans. Softw. Eng. Methodol. (feb 2023)",
            "year": 2023
        },
        {
            "authors": [
                "Heng-Da Cheng",
                "Xihua Jiang",
                "Ying Sun",
                "Jingli Wang"
            ],
            "title": "Color image segmentation: advances and prospects",
            "venue": "Pattern Recognit",
            "year": 2001
        },
        {
            "authors": [
                "Jacob Cohen"
            ],
            "title": "A Coefficient of Agreement for Nominal Scales",
            "venue": "Educational and Psychological Measurement",
            "year": 1960
        },
        {
            "authors": [
                "Sam Corbett-Davies",
                "Sharad Goel"
            ],
            "title": "The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning",
            "year": 2018
        },
        {
            "authors": [
                "Piotr Dollar",
                "Christian Wojek",
                "Bernt Schiele",
                "Pietro Perona"
            ],
            "title": "Pedestrian detection: A benchmark",
            "venue": "In 2009 IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2009
        },
        {
            "authors": [
                "Piotr Doll\u00e1r",
                "Christian Wojek",
                "Bernt Schiele",
                "Pietro Perona"
            ],
            "title": "Pedestrian Detection: An Evaluation of the State of the Art",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 2012
        },
        {
            "authors": [
                "Anthony Finkelstein",
                "Mark Harman",
                "S. Afshin Mansouri",
                "Jian Ren",
                "Yuanyuan Zhang"
            ],
            "title": "Fairness Analysis\u201d in Requirements Assignments",
            "venue": "In Proceedings of the 16th IEEE International Requirements Engineering Conference,",
            "year": 2008
        },
        {
            "authors": [
                "Sainyam Galhotra",
                "Yuriy Brun",
                "Alexandra Meliou"
            ],
            "title": "Fairness testing: testing software for discrimination",
            "venue": "In Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering,",
            "year": 2017
        },
        {
            "authors": [
                "Joshua Garcia",
                "Yang Feng",
                "Junjie Shen",
                "Sumaya Almanee",
                "Yuan Xia",
                "Qi Alfred Chen"
            ],
            "title": "A comprehensive study of autonomous vehicle bugs",
            "venue": "In Proceedings of the 42nd International Conference on Software Engineering,",
            "year": 2020
        },
        {
            "authors": [
                "Zheng Ge",
                "Songtao Liu",
                "Feng Wang",
                "Zeming Li",
                "Jian Sun"
            ],
            "title": "Yolox: Exceeding yolo series in 2021",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "Usman Gohar",
                "Sumon Biswas",
                "Hridesh Rajan"
            ],
            "title": "Towards Understanding Fairness and its Composition in Ensemble Machine Learning",
            "venue": "In Proceedings of the 45th IEEE/ACM International Conference on Software Engineering,",
            "year": 2023
        },
        {
            "authors": [
                "Irtiza Hasan",
                "Shengcai Liao",
                "Jinpeng Li",
                "Saad Ullah Akram",
                "Ling Shao"
            ],
            "title": "Generalizable Pedestrian Detection: The Elephant In The Room",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2020
        },
        {
            "authors": [
                "Max Hort",
                "Jie M. Zhang",
                "Federica Sarro",
                "Mark Harman"
            ],
            "title": "Fairea: A Model Behaviour Mutation Approach to Benchmarking Bias Mitigation Methods",
            "venue": "In Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (Athens, Greece) (ESEC/FSE",
            "year": 2021
        },
        {
            "authors": [
                "Vinit Jakhetiya",
                "Weisi Lin",
                "Sunil Prasad Jaiswal",
                "Ke Gu",
                "Sharath Chandra Guntuku"
            ],
            "title": "Just Noticeable Difference for natural images using RMS contrast and feed-back mechanism",
            "venue": "Neurocomputing",
            "year": 2018
        },
        {
            "authors": [
                "Hyungjung Kim",
                "Hyunsu Lee",
                "Semin Ahn",
                "Woo-Kyun Jung",
                "Sung-Hoon Ahn"
            ],
            "title": "Broken stitch detection system for industrial sewing machines using HSV color space and image processing techniques",
            "venue": "J. Comput. Des. Eng. 10,",
            "year": 2023
        },
        {
            "authors": [
                "Shunsuke Kogure",
                "Kai Watabe",
                "Ryosuke Yamada",
                "Yoshimitsu Aoki",
                "Akio Nakamura",
                "Hirokatsu Kataoka"
            ],
            "title": "Age Should Not Matter: Towards More Accurate Pedestrian Detection via Self-Training. AAAI Workshop on Artificial Intelligence with Biased or Scarce Data (AIBSD) (2022)",
            "year": 2022
        },
        {
            "authors": [
                "J Richard Landis andGaryG. Koch"
            ],
            "title": "Themeasurement of observer agreement for categorical data",
            "venue": "Biometrics 33",
            "year": 1977
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Focal loss for dense object detection",
            "venue": "In Proceedings of the IEEE international conference on computer vision",
            "year": 2017
        },
        {
            "authors": [
                "Wei Liu",
                "Dragomir Anguelov",
                "Dumitru Erhan",
                "Christian Szegedy",
                "Scott Reed",
                "Cheng-Yang Fu",
                "Alexander C Berg"
            ],
            "title": "Ssd: Single shot multibox detector",
            "venue": "In Computer Vision\u2013ECCV 2016: 14th European Conference,",
            "year": 2016
        },
        {
            "authors": [
                "Wei Liu",
                "Shengcai Liao",
                "Weidong Hu",
                "Xuezhi Liang",
                "Xiao Chen"
            ],
            "title": "Learning efficient single-stage pedestrian detectors by asymptotic localization fitting",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV)",
            "year": 2018
        },
        {
            "authors": [
                "Wei Liu",
                "Shengcai Liao",
                "Weiqiang Ren",
                "Weidong Hu",
                "Yinan Yu"
            ],
            "title": "Highlevel semantic feature detection: A new perspective for pedestrian detection",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "year": 2019
        },
        {
            "authors": [
                "Nan Niu",
                "Wentao Wang",
                "Arushi Gupta"
            ],
            "title": "Gray links in the use of requirements traceability",
            "venue": "In Proceedings of the 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,",
            "year": 2016
        },
        {
            "authors": [
                "Yuki Omori",
                "Yoshihiro Shima"
            ],
            "title": "HSV Color Space Based Lighting Detection for Brake Lamps of Daytime Vehicle Images",
            "venue": "J. Comput",
            "year": 2019
        },
        {
            "authors": [
                "Yanwei Pang",
                "Jin Xie",
                "Muhammad Haris Khan",
                "Rao Muhammad Anwer",
                "Fahad Shahbaz Khan",
                "Ling Shao"
            ],
            "title": "Mask-guided attention network for occluded pedestrian detection",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision",
            "year": 2019
        },
        {
            "authors": [
                "Kexin Pei",
                "Yinzhi Cao",
                "Junfeng Yang",
                "Suman Jana"
            ],
            "title": "DeepXplore: Automated Whitebox Testing of Deep Learning Systems",
            "venue": "Commun. ACM 62,",
            "year": 2019
        },
        {
            "authors": [
                "Eli Peli"
            ],
            "title": "Contrast in complex images",
            "venue": "J. Opt. Soc. Am. A 7,",
            "year": 1990
        },
        {
            "authors": [
                "Denis G. Pelli",
                "Peter Bex"
            ],
            "title": "Measuring contrast sensitivity",
            "venue": "Vision Research",
            "year": 2013
        },
        {
            "authors": [
                "Joseph Redmon",
                "Santosh Divvala",
                "Ross Girshick",
                "Ali Farhadi"
            ],
            "title": "You only look once: Unified, real-time object detection",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition",
            "year": 2016
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun"
            ],
            "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Xiaolin Song",
                "Kaili Zhao",
                "Wen-Sheng Chu",
                "Honggang Zhang",
                "Jun Guo"
            ],
            "title": "Progressive refinement network for occluded pedestrian detection",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Ezekiel O. Soremekun",
                "Mike Papadakis",
                "Maxime Cordy",
                "Yves Le Traon"
            ],
            "title": "Software fairness: An analysis and survey",
            "venue": "CoRR abs/2205.08809",
            "year": 2022
        },
        {
            "authors": [
                "Andrea Stocco",
                "Michael Weiss",
                "Marco Calzana",
                "Paolo Tonella"
            ],
            "title": "Misbehaviour prediction for autonomous driving systems",
            "venue": "In Proceedings of the 42nd International Conference on Software Engineering,",
            "year": 2020
        },
        {
            "authors": [
                "Shamik Sural",
                "Gang Qian",
                "Sakti Pramanik"
            ],
            "title": "Segmentation and histogram generation using theHSV color space for image retrieval",
            "venue": "In Proceedings of the 2002 International Conference on Image Processing,",
            "year": 2002
        },
        {
            "authors": [
                "Yuchi Tian",
                "Kexin Pei",
                "Suman Jana",
                "Baishakhi Ray"
            ],
            "title": "DeepTest: Automated Testing of Deep-Neural-Network-Driven Autonomous Cars",
            "venue": "In Proceedings of the 40th International Conference on Software Engineering (Gothenburg,",
            "year": 2018
        },
        {
            "authors": [
                "Alvaro Veizaga",
                "Mauricio Alf\u00e9rez",
                "Damiano Torre",
                "Mehrdad Sabetzadeh",
                "Lionel C. Briand"
            ],
            "title": "On systematically building a controlled natural language for functional requirements",
            "venue": "Empir. Softw. Eng. 26,",
            "year": 2021
        },
        {
            "authors": [
                "Jinfeng Wen",
                "Zhenpeng Chen",
                "Yi Liu",
                "Yiling Lou",
                "Yun Ma",
                "Gang Huang",
                "Xin Jin",
                "Xuanzhe Liu"
            ],
            "title": "An empirical study on challenges of application development in serverless computing",
            "venue": "In Proceedings of the 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "Michael Wick",
                "swetasudha panda",
                "Jean-Baptiste Tristan"
            ],
            "title": "Unlocking Fairness: a Trade-off Revisited",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Benjamin Wilson",
                "Judy Hoffman",
                "Jamie H. Morgenstern"
            ],
            "title": "Predictive Inequity in Object Detection",
            "venue": "ArXiv abs/1902.11097",
            "year": 2019
        },
        {
            "authors": [
                "Jianyu Wu",
                "Hao He",
                "Wenxin Xiao",
                "Kai Gao",
                "Minghui Zhou"
            ],
            "title": "Demystifying software release note issues on GitHub",
            "venue": "In Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension,",
            "year": 2022
        },
        {
            "authors": [
                "Fisher Yu",
                "Haofeng Chen",
                "Xin Wang",
                "Wenqi Xian",
                "Yingying Chen",
                "Fangchen Liu",
                "Vashisht Madhavan",
                "Trevor Darrell"
            ],
            "title": "BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2020
        },
        {
            "authors": [
                "Jie M. Zhang",
                "Mark Harman"
            ],
            "title": "Ignorance and Prejudice\" in Software Fairness",
            "venue": "IEEE/ACM 43rd International Conference on Software Engineering (ICSE)",
            "year": 2021
        },
        {
            "authors": [
                "Jie M. Zhang",
                "Mark Harman",
                "Lei Ma",
                "Yang Liu"
            ],
            "title": "Machine Learning Testing: Survey, Landscapes and Horizons",
            "venue": "IEEE Trans. Software Eng. 48,",
            "year": 2022
        },
        {
            "authors": [
                "Mengdi Zhang",
                "Jun Sun"
            ],
            "title": "Adaptive fairness improvement based on causality analysis",
            "venue": "In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Mengdi Zhang",
                "Jun Sun",
                "Jingyi Wang",
                "Bing Sun"
            ],
            "title": "TESTSGD: Interpretable testing of neural networks against subtle group discrimination",
            "venue": "ACM Transactions on Software Engineering and Methodology",
            "year": 2023
        },
        {
            "authors": [
                "Mengshi Zhang",
                "Yuqun Zhang",
                "Lingming Zhang",
                "Cong Liu",
                "Sarfraz Khurshid"
            ],
            "title": "DeepRoad: GAN-Based Metamorphic Testing and Input Validation Framework for Autonomous Driving Systems. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering (Montpellier, France) (ASE \u201918)",
            "venue": "Association for Computing Machinery,",
            "year": 2018
        },
        {
            "authors": [
                "Shanshan Zhang",
                "Rodrigo Benenson",
                "Mohamed Omran",
                "Jan Hosang",
                "Bernt Schiele"
            ],
            "title": "How Far are We from Solving Pedestrian Detection",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2016
        },
        {
            "authors": [
                "Shanshan Zhang",
                "Rodrigo Benenson",
                "Bernt Schiele"
            ],
            "title": "CityPersons: A Diverse Dataset for Pedestrian Detection",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 4457\u20134465",
            "year": 2017
        },
        {
            "authors": [
                "Husheng Zhou",
                "Wei Li",
                "Zelun Kong",
                "Junfeng Guo",
                "Yuqun Zhang",
                "Bei Yu",
                "Lingming Zhang",
                "Cong Liu"
            ],
            "title": "DeepBillboard: Systematic Physical-World Testing of Autonomous Driving Systems",
            "venue": "In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering (Seoul, South Korea) (ICSE \u201920)",
            "year": 2020
        },
        {
            "authors": [
                "Zhengxia Zou",
                "Keyan Chen",
                "Zhenwei Shi",
                "Yuhong Guo",
                "Jieping Ye"
            ],
            "title": "Object Detection in 20 Years: A Survey",
            "venue": "Proc. IEEE 111,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "KEYWORDS Group fairness, pedestrian detection, autonomous driving"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Autonomous driving systems are on track to become the predominant mode of transportation in the future [53]. However, these systems are susceptible to software bugs [26], which can potentially result in severe injuries or even fatalities for both pedestrians and passengers. The unfortunate incident in 2018 involving an autonomous vehicle from Uber serves as a stark reminder of these risks [1]. Given the safety-critical nature of autonomous driving systems, they have garnered substantial attention from the software testing community [64].\nExtensive research efforts have been devoted to the testing of autonomous driving systems. For example, Tian et al. [56] introduced DeepTest, which applies image transformation to simulate potential camera noise in autonomous driving scenarios. Zhang et al. [67] developed DeepRoad, a Generative Adversarial Network (GAN)-based approach that generates test images from real-world driving scenes. Zhou et al. [70] proposed DeepBillboard, a system\n\u2217Xinyue Li and Zhenpeng Chen are co-first authors.\nfor generating adversarial billboards to induce potential steering errors in autonomous vehicles.\nAlthough significant testing efforts have been made, to the best of our knowledge, the study of fairness testing for autonomous driving systems remains under-investigated in the literature. From a Software Engineering (SE) perspective, fairness is considered a non-functional software property, making it an important subject for testing [64]. Fairness testing, as an emerging domain within software testing, seeks to uncover fairness issues in software systems [25].\nFairness issues in autonomous driving systems, such as a higher accuracy in detecting pedestrians of white ethnicity compared to black ethnicity, can perpetuate discriminatory outcomes and unequal treatment based on race. This can result in harm to individuals belonging to marginalized groups, further exacerbating existing social inequalities. Therefore, it is crucial to prioritize fairness testing in autonomous driving systems.\nTo fill the knowledge gap, we systematically conduct fairness testing on autonomous driving systems, with a specific emphasis on the pedestrian detection component \u2014 the human-related module within such systems. Our main focus is to quantitatively assess performance disparities in current pedestrian detectors across diverse demographic groups, which are widely recognized as group fairness issues in the literature [16]. By measuring and highlighting these discrepancies, we contribute to the existing body of knowledge surrounding well-documented fairness concerns.\nIn this study, we conduct fairness testing on eight state-of-the-art Deep Learning (DL)-based pedestrian detectors that have been extensively studied in the research community. To enable this testing, we require datasets that include demographic labels (such as gender and skin tone) for pedestrians. However, commonly-used datasets for pedestrian detection often lack such labels. To address this problem, we manually enrich four widely-adopted real-world datasets with gender, age, and skin tone labels, resulting in a collection of 8,311 real-world images annotated with 16,070 gender labels, 20,115 age labels, and 3,513 skin tone labels. Using these labeled datasets, we assess the group fairness of existing pedestrian detectors and also investigate how commonly-studied scenarios in the literature of autonomous driving testing (including brightness, contrast, and weather conditions) impact the fairness of these detectors.\nOur study reveals the following findings: (1) Overall, state-ofthe-art pedestrian detectors exhibit bias across different age and\nar X\niv :2\n30 8.\n02 93\n5v 1\n[ cs\n.C Y\n] 5\nA ug\n2 02\nskin tone groups. On the four datasets examined, the undetected proportions for children surpass those for adults by an average of 19.67%. At the same time, undetected proportions for dark-skin individuals exceed those for light-skin individuals by an average of 7.52% across all datasets. (2) However, the performance of these pedestrian detectors in detecting males and females does not exhibit a large difference, with only a 1.1% gap in undetected proportions. (3) Regarding the exploration of common scenarios, detection performance for the dark-skin group decreases under low-brightness and low-contrast conditions compared to the light-skin group. For instance, the difference in undetected proportions increases from 7.14% to 9.86% between day time and night time scenarios.\nTo summarise, we make the following contributions:\n\u2022 We conduct a systematic empirical study on fairness testing of autonomous driving systems, evaluating eight widely-studied pedestrian detectors and uncovering significant fairness issues. \u2022 We augment four large-scale datasets with manually labeled demographic information, resulting in 8,311 real-world images with 16,070 gender labels, 20,115 age labels, and 3,513 skin tone labels, to facilitate future research on fairness in autonomous driving. \u2022 We provide a replication package [2] containing all the code and results from this study, facilitating replication and extension of this research."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "This study resides at the intersection of two increasingly important SE topics: software fairness and autonomous driving testing. To provide the necessary context, we begin by reviewing the background knowledge and relevant prior research in these areas."
        },
        {
            "heading": "2.1 Software Fairness",
            "text": "Fairness has gained significant attention in the SE community since its initial exploration by SE researchers in 2008 [23]. There have been various definitions of fairness in the literature. In this paper, we focus on group fairness, a concept extensively studied in software fairness research [5, 6, 12, 13, 28, 31, 63, 65, 66]. Notably, group fairness closely aligns with legal regulations on fairness [3], such as the adherence to the four-fifths rule, a cornerstone of US law [59]. Consequently, prioritizing group fairness when building software has emerged as an essential ethical duty and requirement for software engineers [12].\nIn the context of group fairness, certain personal characteristics that require protection against unfairness during decisionmaking are called sensitive attributes, also known as protected attributes [20, 64]. Well-recognized sensitive attributes include race, sex, age, pregnancy, familial status, disability status, and more [15]. These sensitive attributes typically partition a population into distinct groups: a privileged group and an unprivileged group [15]. Group fairness entails the equal treatment of these groups by the same machine learning model. However, in practice, members of unprivileged groups often experience systematic disadvantages, resulting from unfair machine learning models. For instance, in the context of a pedestrian detection task, if age is deemed a sensitive attribute, the predictive model may exhibit a bias favoring adult\ngroups over child groups. In this scenario, the adult group is considered the privileged group, while the child group becomes the unprivileged one.\nPresently, the majority of software fairness testing research primarily revolves around tabular data, as indicated by a recent survey [15]. This paper centers on fairness testing for pedestrian detection in autonomous driving systems. We specifically examine three sensitive attributes (i.e., gender, age, and skin tone) that are more recognizable in autonomous driving datasets and have been widely considered in the fairness literature [52]."
        },
        {
            "heading": "2.2 Autonomous Driving Testing",
            "text": "Autonomous driving is a hot SE research topic, and researchers have proposed various testing techniques for autonomous driving systems [46, 56, 67, 70]. These techniques mainly involve the automatic generation of artificial driving scenarios to simulate real-world conditions, enabling the evaluation of system behavior and the detection of blind spots or corner bugs across various autonomous driving scenarios.\nWhile a substantial body of knowledge focuses on assessing the robustness and correctness properties of autonomous driving systems [64], to the best of our knowledge, only few studies have explored the fairness properties, particularly in the pedestrian detection domain within autonomous driving. Pedestrian detection is a crucial process identifies pedestrians within street-level images by providing their predicted locations along with corresponding bounding boxes and confidence scores [22, 68]. Brandao [7] discussed fairness in pedestrian detection but focused solely on traditional detection techniques, which are no longer dominant in contemporary practice. Wilson et al. [60] examined skin tone bias but with limitations to one dataset. Similarly, Kogure et al. [34] only explored age bias with one detection method and a small-scale dataset, lacking a comprehensive analysis. To summarise, current fairness research in pedestrian detection of autonomous driving is still in its early stages and faces limitations in terms of diversity in pedestrian detectors, testing datasets, and sensitive attributes considered.\nTo address this knowledge gap, our paper presents the a systematic empirical study on group fairness testing in pedestrian detection. We conduct experiments using eight popular detection methods and four diverse testing datasets, encompassing different scenarios determined by a variety of factors such as brightness, contrast, and weather conditions. Through this comprehensive study, we gain insights into the potential bias and risks associated with using existing pedestrian detectors."
        },
        {
            "heading": "3 EXPERIMENTAL DESIGN",
            "text": "In this section, we introduce our research questions and experimental settings."
        },
        {
            "heading": "3.1 Research Questions",
            "text": "We aim to answer the following research questions (RQs) in this study.\nRQ1 (Overall fairness): To what extent do widely-studied pedestrian detectors exhibit unfairness concerning common sensitive attributes? This RQ explores the performance difference of widelystudied pedestrian detectors when they are applied to different demographic groups, characterized by sensitive attributes including gender, age, and skin tone. RQ2 (Fairness in different scenarios):What fairness do pedestrian detectors achieve in different brightness, contrast, and weather conditions? We further investigate the fairness of commonly-studied pedestrian detectors in different autonomous driving scenarios by considering a variety of different brightness, contrast, and weather conditions [56, 67, 70]. Correspondingly, we investigate RQ2 by answering three specific sub-questions:\nRQ2.1 What fairness do pedestrian detectors achieve under different brightness conditions?\nRQ2.2 What fairness do pedestrian detectors achieve under different contrast conditions?\nRQ2.3 What fairness do pedestrian detectors achieve under different weather conditions?\nIn the following, we introduce the pedestrian detectors (Section 3.2), benchmark datasets (Section 3.3), evaluation measures (Section 3.4), and statistical analysis (Section 3.5) that we use to address the above research questions."
        },
        {
            "heading": "3.2 Pedestrian Detection Methods",
            "text": "In recent years, DL has revolutionized pedestrian detection approaches. We focus our analysis on eight DL-based pedestrian detectors that are widely studied in the autonomous driving community [10, 71]. These detectors are pre-trained DL models that researchers and practitioners can directly use for pedestrian detection tasks. They can be classified into two categories [29]: general object detectors and pedestrian-specific detectors. Next, we briefly introduce each category and its pedestrian detectors we study. Table 1 provides an overview of these detectors (\u201cDetector\u201d shows the name of a pedestrian detector; \u201cBackbone\u201d represents the pre-trained deep neural network used for extracting features from input images; \u201cSource\u201d indicates the framework/toolkit name and its source for a given pedestrian detector). General object detectors: General detectors can detect various objects such as cars, traffic lights, and pedestrians. They have great generalization ability but lack pedestrian-specific adaptation [29]. They can be categorized into two genres [71]: two-stage and onestage detectors. Two-stage detectors propose regions before feature\nextraction and classification, achieving high accuracy but slower speed; one-stage detectors complete all operations in one step, providing faster speed but lower accuracy. Both types involve tradeoffs and are widely used for pedestrian detection. Hence, this paper selects detectors from both categories. For one-stage detectors, we adopt the widely-studied YOLOX [27] (a faster extension of the YOLO series [49]) and RetinaNet [36] (which addresses the class imbalance problem). For two-stage detectors, we employ the Faster R-CNN [50] (one of the pioneering detectors in the R-CNN family) and Cascade R-CNN [9] (which achieves higher accuracy through a cascade of multiple CNNs to refine region proposals). These detectors have been extensively used in the autonomous driving literature [8, 10, 29, 62]. Pedestrian-specific detectors: Pedestrian-specific detectors use additional pedestrian-related information to improve detection performance [29]. In this study, we investigate state-of-the-art pedestrian-specific detectors, including ALFNet [38], CSP [39], MGAN [45], and PRNet [51]. ALFNet uses progressive detection heads on SSD [37] to refine initial anchors for improved detection accuracy. CSP introduces an anchor-free approach by locating center points and scaling pedestrians. MGAN uses visible-area bounding-box information to guide attention mask generation for occluded pedestrian detection. PRNet presents a novel progressive refinement network for occluded pedestrian detection."
        },
        {
            "heading": "3.3 Benchmark Datasets",
            "text": "3.3.1 Dataset Selection. We perform our experiments on four realworld datasets that have been extensively studied by researchers to evaluate the performance of pedestrian detectors in autonomous driving [10, 29]. These datasets consist of street-level images captured by cameras mounted on autonomous vehicles, showcasing pedestrians in diverse poses, sizes, and occlusion scenarios. Table 2 presents details about these datasets, including the sensitive attributes, the number of images in each dataset, and the respective time of day when these images were captured. Next, we briefly introduce each dataset:\n\u2022 CityPersons dataset [69] stands as the most widely-studied benchmark for evaluating pedestrian detectors [10, 29, 38, 39, 45, 51]. Its test set includes 1,525 images captured across six cities, showcasing diverse weather conditions and street scenes. \u2022 EuroCityPersons dataset [8] contains 4,553 images gathered from seven European cities, encompassing both day and night time captures. The dataset can be categorized into two sets: 2,427 images captured during the day, and 2,126 images captured at night, referred to as the EuroCity-Day dataset and the EuroCity-Night dataset, respectively.\n\u2022 Berkeley Driving dataset (a.k.a., BDD100k dataset) [62] is an extensive driving dataset, including 2,233 images from 40 classes that are typical of driving scenes. These images were captured from four different cities, providing various times of the day. Notably, this dataset showcases a greater diversity of pedestrians than the CityPersons and EuroCityPersons datasets, including individuals with varied skin tones.\n3.3.2 Sensitive Attribute Labeling. As introduced in Section 2.1, we focus on three sensitive attributes (i.e., gender, age, and skin tone) of pedestrians that are recognizable in autonomous driving images and widely studied in the fairness literature [52]. To enable fairness testing, we need datasets with labels that identify these sensitive attributes of the humans depicted in the images. Among the datasets investigated herein, the only sensitive attribute already labeled is the skin tone (i.e., light-skin tone and dark-skin tone) for the BDD100k dataset. Therefore, we manually label the other sensitive attributes for each of the datasets considered in this study.\nThe labeling process involves two annotators to minimize the influence of labeling bias. We focus on images that align with the widely-adopted \u201creasonable subset\u201d principle [22], meaning that we label images containing labeled pedestrians with a height of at least 50 pixels and little to none occlusion. For such images, human annotators can label the sensitive attributes with high confidence [7]. Using the filtered datasets, the two annotators independently label the gender and age attributes for each image. For gender, we follow previous studies [7] and consider only two labels: male and female. As for age, in line with the literature [7, 34], we classify pedestrians into two labels: child and adult, based on their physical characteristics depicted in the images.\nTo ensure the reliability of the labeling procedure, both annotators independently label the age and gender attributes for each image. We use Cohen\u2019s Kappa (\ud835\udf05) [19], a widely-adopted metric for measuring inter-rater agreement, during the independent labeling process. The obtained \ud835\udf05 values for age and gender attributes in each of the four datasets are summarized in Table 3. According to the literature [35], a \ud835\udf05 value between 0.81 and 1 signifies almost perfect agreement, while a value between 0.61 and 0.8 indicates substantial agreement. In our labeling process, we achieve substantial agreement in gender labeling for the EuroCity-Day dataset and almost perfect agreement for all other tasks. This high level of agreement underscores the reliability of our labeling procedure [14, 58]. In cases where the two annotators encounter conflicts, an arbitrator is involved in the discussion to reach a consensus. This is a standard procedure in empirical SE studies [14, 58, 61]. After the labeling process, the summary of the number of labeled pedestrian instances for each dataset is presented in Table 4.\n3.3.3 Scenario Labeling. To deeply explore the fairness of pedestrian detectors across various driving scenarios (i.e., different contrast, brightness, and weather conditions), we also need images labeled with scenario information. We therefore classify the images containing labeled pedestrians (5,917 out of the total 8,311 images in all datasets) into different scenarios, to enable our analysis in RQ2. An alternative approach to achieving this purpose is via generating images with different scenarios using existing test generation techniques from the autonomous driving literature. We do not choose this approach because generated images are not real images and can suffer from unnaturalness [64]. Labeling brightness. Brightness represents the overall lightness or darkness of the image. To distinguish the brightness of the images, we adopt two methods. Firstly, we categorize the datasets into day time and night time. As seen in Table 2, images in the CityPersons and EuroCity-Day datasets belong to day time, while images in the EuroCity-Night dataset belong to night time. For the BDD100k dataset, Wilson et al. [60] have publicly provided the labels for day time and night time. We directly use the label information for study.\nSecondly, we divide the images into five brightness levels to achieve a more refined granularity. For this purpose, we employ the HSV measurement, a commonly-studied metric for evaluating the brightness of pixels in images [18, 24, 33, 43, 54]. Specifically, for each image, we utilize the OpenCV library [44] to perform a linear transformation of the RGB values of each pixel into the HSV color space format, which comprises three dimensions. Among these dimensions, one specifically represents the brightness. We calculate the mean value of the brightness dimension for all pixels in a image and use this mean value to represent the overall brightness of the image. A higher brightness value indicates a greater level of brightness. To classify the images into different brightness levels, we identify the maximum brightness value (which is 166) and the minimum brightness value (which is 10) among all images. Then, we evenly divide this brightness range into five classes, each covering an interval of 31.2 units (calculated as (166-10)/5). Each class represents a brightness level, ranging from level 1 to level 5. Higher levels represent higher brightness. Then we divide the images into different levels based on their brightness values. Labeling contrast. Contrast is the difference in brightness between objects in an image. To quantify the contrast of each image, we use the RMS contrast measurement [47], a standard measure in the computer vision literature [4, 32, 48]. To apply the RMS measurement, we first need to convert all images into the grayscale mode [47]. Then, we calculate the RMS contrast value for each image based on the converted version. A higher RMS contrast value\nindicates a greater contrast. Following a similar approach used for brightness classification, we partition the range of RMS contrast values into five classes, each class featuring equal value intervals. These classes correspond to distinct levels, labeled from level 1 to level 5, with higher levels indicating images with higher contrast. Then, we categorize the images into their respective contrast levels based on their RMS contrast values. Labeling weather conditions. Common weather conditions studied in the autonomous driving literature include rain, fog, and snow [46, 56, 67, 70]. However, our datasets rarely contain images depicting fog and snow. This is due to the fact that our datasets are collected from real-world scenarios where fog and snow are infrequently encountered. The limited samples of snowy or foggy weather pose challenges for statistical analysis. As a result, we focus on rain as the weather condition of interest. Two annotators independently classify images containing labeled pedestrians into two categories: rainy and non-rainy. During this process, 1,840 images are not annotated because the two annotators and the arbitrator all cannot accurately distinguish the weather conditions. To measure inter-rater agreement during this independent labeling, we also use Cohen\u2019s Kappa (\ud835\udf05). The obtained \ud835\udf05 value is 0.813, indicating almost perfect agreement. This high level of agreement confirms the reliability of our labeling procedure. After the scenario labeling process, the summary of the number of images under different brightness, contrast, and weather conditions is presented in Table 5."
        },
        {
            "heading": "3.4 Evaluation Measures",
            "text": "There have been well-established quantitative measures for group fairness in the literature. The most widely-adopted fairness measures include SPD (Statistical Parity Difference), EOD (Equal Opportunity Difference), and AOD (Average Odds Difference) [15, 16, 63]. SPD quantifies the difference in probabilities of favorable outcomes between unprivileged and privileged groups; EOD indicates the difference in the true-positive rate between unprivileged and privileged groups; AOD calculates the average difference between the false-positive rate and true-positive rate for unprivileged and privileged groups.\nIn the context of pedestrian detection, both SPD and EOD are measures that represent the disparity in proportions of successfully detected pedestrians between privileged and unprivileged groups. In other words, SPD and EOD also both express the difference in miss rates between privileged and unprivileged groups. Miss Rate (MR) is the most commonly-studied performance metric in pedestrian detection research [7, 34], which quantifies the proportion of undetected pedestrians. Formally, it is calculated as follows:\n\ud835\udc40\ud835\udc45 = 1 \u2212 \ud835\udc47\ud835\udc43 \ud835\udc47\ud835\udc43 + \ud835\udc39\ud835\udc41 , (1)\nwhere TP (true positives) refers to the number of successfully detected ground-truth bounding boxes, and FN (false negatives) denotes the number of undetected ground-truth bounding boxes. Pedestrian detectors generate bounding box locations and confidence scores for recognized \u201cperson\u201d instances in images. To assess whether a given ground-truth bounding box is successfully detected, the standard method in the literature is to use the Intersection over Union (IoU) metric [22]. The IoU metric quantifies the degree of overlap between the ground-truth bounding box and the detected\nbounding box. If the IoU value is greater than 50%, the ground-truth bounding box is considered successfully detected [22]. Otherwise, it is classified as undetected.\nThe calculation of AOD requires precise false-positive information, referring to instances where members of the negative class (non-pedestrians) are incorrectly classified as the positive class (pedestrians). As described in Section 3.3.2, we adhere to the standard practice in the literature, where we focus on a \u201creasonable subset\u201d of pedestrians within the images. This approach presents challenges in calculating precise false-positives for each group because the negative class (non-pedestrian) may also include instances that belong to the positive class (pedestrian). Therefore, we do not consider AOD in this study.\nIn summary, we use both SPD and EOD as fairness measures for evaluating pedestrian detectors. However, since these twomeasures yield identical values for pedestrian detection, for the remainder of the paper, we present only EOD."
        },
        {
            "heading": "3.5 Statistical Analysis",
            "text": "To assess the extent to which any observed unfairness is statistically significant (i.e., whether there is a significant difference in the miss rate between privileged and unprivileged groups) we use the twoproportion z-test [11]. This statistical test is widely adopted in SE studies [42, 57] to analyze differences between two proportions. A result is deemed significant only if the obtained \ud835\udc5d-value falls below a predetermined threshold (in our case, 0.05, a widely-accepted threshold in the literature [16, 17]). For instance, in evaluating whether there exists a difference between the miss rates for male (\ud835\udc40\ud835\udc45\ud835\udc5a\ud835\udc4e\ud835\udc59\ud835\udc52 ) and female (\ud835\udc40\ud835\udc45\ud835\udc53 \ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc59\ud835\udc52 ) individuals detected by a pedestrian detector, the null hypothesis assumes that\ud835\udc40\ud835\udc45\ud835\udc5a\ud835\udc4e\ud835\udc59\ud835\udc52 is equal to \ud835\udc40\ud835\udc45\ud835\udc53 \ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc59\ud835\udc52 . If the resulting \ud835\udc5d-value is less than 0.05, we reject the null hypothesis, indicating a significant difference between\ud835\udc40\ud835\udc45\ud835\udc5a\ud835\udc4e\ud835\udc59\ud835\udc52 and\ud835\udc40\ud835\udc45\ud835\udc53 \ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc59\ud835\udc52 ."
        },
        {
            "heading": "4 RESULTS",
            "text": "This section answers our RQs based on experimental results."
        },
        {
            "heading": "4.1 RQ1: Overall Fairness",
            "text": "RQ1 investigates the overall fairness of eight state-of-the-art pedestrian detectors with respect to gender, age, and skin tone. First, for each detector, we compute the miss rate (MR) for different demographic groups and calculate EOD based on the MR results over all the datasets that we study. We also use the two-proportion z-test to determine the significance of any observed unfairness (as described in Section 3.5). Table 6 presents the results, with significant unfairness results highlighted in shading. Next, we analyze the results for gender, age, and skin tone, respectively. Gender. As shown in Table 6, on average, the miss rate difference between female and male pedestrians is merely 1.1%, with \ud835\udc5d-value > 0.05, indicating that one cannot reject the null hypothesis and cannot assume that this difference is statistically significant.\nThen, we explore the difference across the four datasets used in our study. Figure 1 shows the miss rates for male and female pedestrians in each dataset, and significant gender unfairness is indicated by labeledmiss rate values. In the CityPersons and EuroCityDay datasets, which exclusively contain day-time data, there are"
        },
        {
            "heading": "19.67% higher miss rates for children compared to adults. Additionally, a substantial unfairness is evident concerning skin tone, with pedestrian detectors showing a miss rate 7.52% higher for the dark-skin group than the light-skin group on average.",
            "text": "no significant differences in the miss rate between females and males. However, the results on the EuroCity-Night dataset present a contrasting observation, where all detectors demonstrate a higher miss rate for females, thus illustrating bias in detecting females. In the BDD100k dataset that includes both day-time and night-time images, the performance difference is less pronounced. These observations motivate us to hypothesize that brightness conditions may influence the fairness of pedestrian detectors, which is further investigated in RQ2. Age. As observed in Table 6, the pedestrian detectors that we study exhibit large unfairness regarding age, with all detectors demonstrating significantly higher miss rates for children compared to adults (with \ud835\udc5d-value < 0.05). On average, the miss rate difference between children and adults amounts to 19.67%.\nFurthermore, we illustrate the miss rate difference across the four datasets that we use in Figure 2, with significant gender unfairness indicated by labeled miss rate values. From the figure, we observe that the miss rate of children is consistently higher than that of adults for all the datasets and all detectors. In particular, out of the total 32 results (combinations of four datasets and eight pedestrian detection models), 30 exhibit a statistically significant difference in the miss rate with \ud835\udc5d-value < 0.05. This indicates that the bias favoring adults is not specific to a particular dataset or model, highlighting a strong unfairness between adults and children.\nThe age bias can be attributed to the inherent challenge of detecting small objects in object and person detection research, owing to the limited information provided by small bounding box sizes [10, 21, 34, 68]. Given that children generally have smaller bodies compared to adults, their bounding boxes in the images also tend to be smaller. To verify this, we analyze the distribution of bounding box sizes for pedestrians detected and undetected by all pedestrian detectors, as well as the distribution of ground-truth bounding box sizes for both adults and children. The results, presented in Figure\n3, reveal a correspondence between the two distributions. Children as well as undetected pedestrians tend to have smaller bounding boxes. Skin tone. As shown in Table 6, current pedestrian detectors exhibit a notable bias towards the dark-skin group, with an average miss rate difference of 7.52% between the dark-skin and light-skin groups. Among the eight detectors examined, five detectors demonstrate higher miss rates for the dark-skin group compared to the light-skin group (with four of them being statistically significant). Furthermore, two detectors display significant miss rate disparities of 30.71% and 28.03%, signifying substantial fairness challenges related to skin tone.\nAns. to RQ1:On average, the pedestrian detectors under study exhibit no significant performance disparity across gender, but notable disparity across age and skin tone. In particular, the miss rate difference between children and adults is as high as 19.67%; the miss rate difference between dark-skin pedestrians and light-skin pedestrians is 7.52%. These observations highlight significantly higher risks for children and dark-skin pedestrians with the existing autonomous driving systems."
        },
        {
            "heading": "4.2 RQ2: Fairness in Different Scenarios",
            "text": "RQ2 investigates the fairness of state-of-the-art pedestrian detectors under different real-world autonomous driving scenarios.\n4.2.1 RQ2.1: Different brightness conditions. As introduced in Section 3.3.3, we consider brightness conditions from two perspectives. Next, we analyze the results for the two perspectives, respectively. Day time and Night time. We first evaluate the miss rates of the eight pedestrian detectors under day time and night time. The results, presented in Table 7, indicate a noticeable increase in average\nmiss rates during night time compared to day time. For example, the average miss rates for males and females at night are 36.55% and 40.05%, respectively, compared to 20.15% and 20.03% during the day. A similar pattern is observed for age and skin tone. This indicates that the transition from day time to night time influences the performance of pedestrian detectors, with statistically significant higher miss rates observed at night.\nThen, we investigate whether the performance is equally decreased for different demographic groups. Specifically, we explore the fairness change from day time to night time. Table 7 shows the results, with statistically significant unfairness (i.e., miss rate difference) emphasized in shading.\nRegarding gender, we observe a shift in the miss rate difference between males and females from day time to night time. During the day, there is only a slight 0.12% difference for males and females. However, during night time, the difference increases to -3.5%, indicating a notable change in fairness. In the night-time condition, all pedestrian detectors exhibit higher miss rates for females compared to males, with six of them showing statistically significant differences.\nFor age, the miss rate difference for children and adults increases from day time to night time, with the average difference increasing\nTable 7: RQ2.1: MR and EOD of each pedestrian detector under day time and night time. The statistically significant unfairness results are highlighted in shading. As the time shifts from day to night, the EOD between children and adults and between dark-skin and light-skin groups increases (i.e., from -22.05% to -26.63% for age and from -7.14% to -9.68% for skin tone). The female group exhibits a higher miss rate at night.\nDetectors\nGender Age Skin Tone (LS: Light Skin, DS: Dark Skin)\nDay-time Night-time Day-time Night-time Day-time Night-time\nMR Male MR Female EOD MR Male MR Female EOD MR Adult MR Child EOD MR Adult MR Child EOD MR LS MR DS EOD MR LS MR DS EOD\nYOLOX 22.71% 22.00% 0.71% 24.59% 28.97% -4.38% 24.23% 50.00% -25.77% 34.83% 76.06% -41.23% 7.33% 35.47% -28.14% 11.65% 57.76% -46.11% RetinaNet 17.18% 17.37% -0.19% 20.39% 22.90% -2.51% 18.63% 46.35% -27.72% 27.75% 69.01% -41.26% 9.75% 35.17% -25.43% 15.95% 59.48% -43.53%\nFaster RCNN 4.93% 4.86% 0.07% 6.03% 8.29% -2.26% 5.97% 26.73% -20.76% 10.04% 40.85% -30.80% 5.69% 3.58% 2.12% 9.62% 4.31% 5.31% Cascade RCNN 5.58% 5.68% -0.11% 7.57% 9.55% -1.99% 6.76% 29.62% -22.85% 11.43% 40.85% -29.41% 5.91% 3.87% 2.03% 10.89% 6.90% 3.99%\nALFNet 24.60% 24.45% 0.15% 57.33% 63.44% -6.11% 26.56% 49.62% -23.05% 65.13% 83.10% -17.97% 37.39% 38.75% -1.36% 72.41% 73.28% -0.87% CSP 26.85% 26.43% 0.42% 61.41% 67.05% -5.64% 28.69% 47.12% -18.42% 63.37% 76.06% -12.69% 56.19% 59.91% -3.72% 91.14% 92.24% -1.10% MGAN 25.57% 26.35% -0.78% 47.52% 47.75% -0.24% 27.92% 43.65% -15.74% 49.70% 69.01% -19.32% 47.35% 49.78% -2.43% 85.06% 83.62% 1.44% PRNet 33.78% 33.09% 0.69% 67.55% 72.42% -4.87% 34.82% 56.92% -22.10% 72.62% 92.96% -20.34% 54.46% 54.69% -0.23% 89.62% 86.21% 3.41%\nAverage 20.15% 20.03% 0.12% 36.55% 40.05% -3.50% 21.70% 43.75% -22.05% 41.86% 68.49% -26.63% 28.01% 35.15% -7.14% 48.29% 57.97% -9.68%\nfrom 22.05% during the day to 26.63% at night. This suggests a higher probability of children being undetected during night time conditions.\nFor skin tone, the miss rate difference between dark-skin and light-skin groups increases from day time to night time, with the average difference increasing from 7.14% at day time to 9.68% at night time. This implies increased detection difficulty for the darkskin group at night. Five brightness levels. To offer a more fine grained perspective, we partition the images into five brightness levels, as described in Section 3.3.3. After partitioning the data, we first examine the average miss rate across eight pedestrian detectors under five brightness levels, as presented in Figure 4 (with a darker line indicating a lower brightness level). Consistent with the previous findings regarding day time and night time performance, we observe that pedestrian detectors exhibit their worst performance at the darkest level 1 according to the first three figures, which indicates that all the pedestrian detectors perform worse in low brightness conditions.\nThen, we examine how the fairness changes with decreasing brightness levels for each sensitive attribute. Figure 4 illustrates the EOD changes under five brightness levels.\nFor gender, we observe a notable spike in the EOD change at level 2 and level 1, which represent the two lowest brightness levels. This indicates that as the brightness level decreases, the overall miss rate increases and pedestrian detectors tend to perform worse at detecting females.\nHowever, for age, there is no apparent pattern in EOD changes for each pedestrian detector under specific brightness levels. This may be attributed to the detectors\u2019 limitations in detecting small-scale pedestrians, which outweigh the impact of brightness, resulting in less noticeable EOD variations.\nFor skin tone, we find that different types of detectors exhibit different performance under five brightness levels. Specifically, onestage detectors (i.e., YOLOX and RetinaNet), display an increase in the miss rate difference for dark-skin individuals as the brightness decreases.\nAns. to RQ2.1: Lower brightness conditions can amplify the bias towards females, children, and dark-skin individuals in\nFigure 4: RQ2.1: Average MR of eight detectors and EOD trends under different brightness levels. The three figures above indicate that lower brightness levels lead to an increased average miss rate for each attribute. In the three figures below, explicit bias is observed in YOLOX and RetinaNet for the dark-skin group as the brightness level decreases. Furthermore, there exists an increasing EOD at brightness level"
        },
        {
            "heading": "2 and level 1, putting females at a disadvantage by making",
            "text": "them more likely to be undetected.\npedestrian detection. Specifically, for gender, we observe a shift in the miss rate difference from 0.12% during the day to 3.5% at night. For age, the miss rate difference between children and adults increases from 22.05% during the day to 26.63% at night. Additionally, for skin tone, the miss rate difference between the dark-skin and light-skin groups undergoes a change from 7.14% during the day to 9.68% at night. Moreover, the miss rate difference between the light-skin and dark-skin groups increases as the brightness level decreases, especially for onestage pedestrian detectors.\n4.2.2 RQ2.2: Different contrast levels. Following the road map of RQ2, we first compare the average miss rates of all pedestrian\ndetectors at different contrast levels, as shown in Figure 5. For skin tone, we observe that the average miss rate increases at lower contrast levels (level 1 and level 2) compared to the other three levels. However, for gender and age, the average miss rate of all pedestrian detectors does not show an obvious variation across different contrast levels.\nThen, we investigate the fairness changes, as indicated by the EOD trends in Figure 5. For gender, an increasing EOD trend is observed. As the contrast level decreases, pedestrian detectors gradually exhibit a higher miss rate difference between females and males, especially in the lowest contrast level. This observation indicates that low contrast conditions negatively impact the detection of females, thereby contributing another factor to the bias and unfairness in pedestrian detection.\nFor age, the EOD between children and adults remains large across all five contrast levels. Certain pedestrian-specific detectors such as ALFNet, CSP, and PRNet demonstrate higher miss rate differences at the highest contrast level. However, no specific pattern in the EOD across the five contrast conditions is observed. This suggests that contrast levels do not significantly impact the detection performance difference between adults and children.\nFor skin tone, as the contrast level decreases, all pedestrian detectors display greater difficulty in detecting the dark-skin group. Similarly, one-stage pedestrian detectors especially show an increased miss rate difference (with a larger EOD) between light-skin and dark-skin groups. These results demonstrate that contrast conditions also have a substantial impact on the detection of different gender and skin tone groups.\nAns. to RQ2.2: Low-contrast conditions have a negative influence on the fairness of existing pedestrian detectors regarding gender and skin tone. As the contrast level decreases, the miss rate differences between dark-skin and light-skin groups and between female and male groups increase, posing a greater challenge in detecting dark-skin and female pedestrians.\n4.2.3 RQ2.3: Different weather conditions. Table 8 presents the miss rates of the eight pedestrian detectors under non-rainy and rainy weather conditions, with statistically significant unfairness results shaded.\nIn terms of the overall detection performance of all pedestrian detectors, the miss rate for each demographic group increases in the rainy weather condition, possibly due to droplets covering the camera and disrupting the detectors. Regarding the fairness, we find that the miss rate difference increases between the dark-skin group and light-skin group (with EOD from -7.36% to -10.87%) and between males and females (with EOD from 1.63% to 3.76%), indicating a higher miss rate for the dark-skin group and males from non-rainy to rainy conditions. However, for these increases, we cannot observe a statistically significant difference. As for children and adults, the miss rates difference experiences only a slight decrease, with EOD moving from -21.95% to -20.53%. Therefore, it can be concluded that rainy weather conditions have a minor impact on the fairness of pedestrian detectors.\nAns. to RQ2.3: Rainy weather conditions have a minor impact on the fairness of pedestrian detectors. Although we observe an increase in the miss rate difference between dark-skin and light-skin groups (from 7.36% to 10.87%) and betweenmales and females (from 1.63% to 3.3%), these increases are not statistically significant."
        },
        {
            "heading": "5 THREATS TO VALIDITY",
            "text": "This section discusses the potential threats to the validity of our results. Manual labeling. The labeling process involves possible subjectivity, posing threats to the validity of the analysis. To mitigate this threat, two annotators and an arbitrator are involved in the labeling process: first each image is independently labeled by two annotators, and in cases where conflicts in labeling arise, we seek the expertise of an arbitrator to resolve such discrepancies and arrive at a consensus. The inter-rater agreement between the two annotators is high, which demonstrates the reliability of the labeling schema and procedure adopted herein. Selection of pedestrian detection models. Our study is based on eight pedestrian detectors, which may lead to possible selection bias in this study. To mitigate this threat, we select representative pedestrian detectors based on two considerations. On one hand, the selected detectors are widely-studied in pedestrian detection and autonomous driving literature [10, 62, 68, 69]. On the other hand, the selected models cover the two typical types of pedestrian detection methods (i.e., one-stage detection and two-stage detection), ensuring a comprehensive representation of the techniques used in the field.\nSelection of evaluation measures. The fairness measures that we employ may introduce potential limitations. To mitigate this concern, we consider three commonly-used fairness measures: SPD, EOD, and AOD. Upon careful discussion, we conclude that AOD may not be suitable for our study (Section 3.4), leading us to focus on SPD and EOD. We demonstrate that SPD and EOD lead to equivalent observations for pedestrian detection. Ultimately, we opt to use EOD (i.e., SPD) for our analysis, as it involves the comparison of the miss rates of different demographic groups. This ensures a consistent evaluation with the current pedestrian detection research, as miss rate is the most widely-adopted metric for measuring the performance of pedestrian detectors in the literature."
        },
        {
            "heading": "6 IMPLICATIONS",
            "text": "The findings derived from our analysis carry significant implications for researchers, software engineers, and policy makers. Implication for researchers. The significant unfairness exhibited by existing pedestrian detectors offers valuable implications for researchers to further focus their effort on fairness testing and improvement for autonomous driving systems. Our findings demonstrate that brightness and contrast conditions can significantly influence detection performance regarding skin tone and gender. Thus, to improve fairness, one practical solution is to design specific techniques to adjust contrast and brightness of input images, such as increasing brightness and contrast levels to counterbalance existing bias towards dark-skin and female pedestrians in low-brightness and low-contrast conditions. In addition, our findings highlight that one-stage detectors display significant discrimination against particularly dark-skin pedestrians. This suggests that mitigating skin tone bias requires careful consideration of detector types. For age bias, it is important to address the inherent limitations of pedestrian detectors, particularly their difficulty in accurately detecting small pedestrians. These implications underline the importance of factoring these elements into effective fairness improvement strategies. Implication for software engineers. Fairness is a critical nonfunctional requirement for software systems, but our study demonstrates the existence of significant bias in state-of-the-art pedestrian detectors. Nevertheless, there is a notable knowledge gap in detecting and addressing fairness issues in autonomous driving systems.\nThis emphasizes that it is imperative for software engineers to prioritize their efforts in this domain. Addressing these issues is not just a matter of improving autonomous driving system performance, it also help software companies avoid potential legal implications linked to anti-discrimination laws. Implication for policy makers. Autonomous driving systems play a crucial role in ensuring human safety. Our results reveal that fairness issues exist in modern pedestrian detection models. Therefore, it is essential for policy makers to enact laws and regulations that safeguard the rights of all individuals and address these concerns appropriately. Policies should also aim to address safety issues for vulnerable road users such as children and dark-skin individuals, who are, as revealed by this study, more likely to be overlooked by the current detection models."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "This paper presents the first comprehensive study on fairness testing of eight state-of-the-art pedestrian detectors, using four widelystudied testing datasets. We investigate the fairness aspects of these detectors with respect to gender, age, and skin tone. Our findings reveal significant bias in the current pedestrian detectors, particularly towards children and individuals with darker skin tones. However, we observe balanced detection performance for male and female individuals. Furthermore, we conduct an in-depth analysis of fairness in various driving scenarios. Our investigation demonstrates that the detection performance for dark-skin individuals is further reduced under low-brightness and low-contrast conditions, exacerbating the unfairness related to skin tone. As part of our contribution, we publicly release large-scale real-world pedestrian detection datasets with gender, age, and skin tone labels. These datasets aim to facilitate future fairness research in autonomous driving. Overall, this study sheds light on the fairness issues faced by existing pedestrian detectors, emphasizing the importance of addressing bias related to age and skin tone. The insights gained can pave the way for more fair and unbiased autonomous driving systems in the future.\nDATA AVAILABILITY Our GitHub repository [2] contains datasets, sensitive attribute labels, scripts, and results of this work to facilitate replication and extension."
        }
    ],
    "title": "Dark-Skin Individuals Are at More Risk on the Street: Unmasking Fairness Issues of Autonomous Driving Systems",
    "year": 2023
}