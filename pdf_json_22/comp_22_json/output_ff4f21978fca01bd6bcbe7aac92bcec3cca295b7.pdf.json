{
    "abstractText": "Although massive pre-trained vision-language models like CLIP show impressive generalization capabilities for many tasks, still it often remains necessary to fine-tune them for improved performance on specific datasets. When doing so, it is desirable that updating the model is fast and that the model does not lose its capabilities on data outside of the dataset, as is often the case with classical fine-tuning approaches. In this work we suggest a lightweight adapter, that only updates the models predictions close to seen datapoints. We demonstrate the effectiveness and speed of this relatively simple approach in the context of few-shot learning, where our results both on classes seen and unseen during training are comparable with or improve on the state of the art.",
    "authors": [
        {
            "affiliations": [],
            "name": "Moritz Ibing"
        },
        {
            "affiliations": [],
            "name": "Isaak Lim"
        },
        {
            "affiliations": [],
            "name": "Leif Kobbelt"
        }
    ],
    "id": "SP:47d45f6a2770df29f9c52c4730b7774ceb8553e8",
    "references": [
        {
            "authors": [
                "Zeynep Akata",
                "Florent Perronnin",
                "Zaid Harchaoui",
                "Cordelia Schmid"
            ],
            "title": "Label-embedding for image classification",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "Zeynep Akata",
                "Scott Reed",
                "Daniel Walter",
                "Honglak Lee",
                "Bernt Schiele"
            ],
            "title": "Evaluation of output embeddings for fine-grained image classification",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Lukas Bossard",
                "Matthieu Guillaumin",
                "Luc Van Gool"
            ],
            "title": "Food-101\u2013mining discriminative components with random forests",
            "venue": "In European conference on computer vision,",
            "year": 2014
        },
        {
            "authors": [
                "Wei-Lun Chao",
                "Soravit Changpinyo",
                "Boqing Gong",
                "Fei Sha"
            ],
            "title": "An empirical study and analysis of generalized zeroshot learning for object recognition in the wild",
            "venue": "In European conference on computer vision,",
            "year": 2016
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Mircea Cimpoi",
                "Subhransu Maji",
                "Iasonas Kokkinos",
                "Sammy Mohamed",
                "Andrea Vedaldi"
            ],
            "title": "Describing textures in the wild",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2014
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "ICLR, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Li Fei-Fei",
                "Rob Fergus",
                "Pietro Perona"
            ],
            "title": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories",
            "venue": "In 2004 conference on computer vision and pattern recognition workshop,",
            "year": 2004
        },
        {
            "authors": [
                "Andrea Frome",
                "Greg S Corrado",
                "Jon Shlens",
                "Samy Bengio",
                "Jeff Dean",
                "Marc\u2019Aurelio Ranzato",
                "Tomas Mikolov"
            ],
            "title": "Devise: A deep visual-semantic embedding model",
            "venue": "Advances in neural information processing systems,",
            "year": 2013
        },
        {
            "authors": [
                "Peng Gao",
                "Shijie Geng",
                "Renrui Zhang",
                "Teli Ma",
                "Rongyao Fang",
                "Yongfeng Zhang",
                "Hongsheng Li",
                "Yu Qiao"
            ],
            "title": "Clip-adapter: Better vision-language models with feature adapters",
            "venue": "arXiv preprint arXiv:2110.04544,",
            "year": 2021
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Mehdi Mirza",
                "Da Xiao",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "An empirical investigation of catastrophic forgetting in gradient-based neural networks",
            "venue": "arXiv preprint arXiv:1312.6211,",
            "year": 2013
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Patrick Helber",
                "Benjamin Bischke",
                "Andreas Dengel",
                "Damian Borth"
            ],
            "title": "Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification",
            "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,",
            "year": 2019
        },
        {
            "authors": [
                "Olivier Henaff"
            ],
            "title": "Data-efficient image recognition with contrastive predictive coding",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Steven Basart",
                "Norman Mu",
                "Saurav Kadavath",
                "Frank Wang",
                "Evan Dorundo",
                "Rahul Desai",
                "Tyler Zhu",
                "Samyak Parajuli",
                "Mike Guo"
            ],
            "title": "The many faces of robustness: A critical analysis of out-of-distribution generalization",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Zhao",
                "Steven Basart",
                "Jacob Steinhardt",
                "Dawn Song"
            ],
            "title": "Natural adversarial examples",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Chao Jia",
                "Yinfei Yang",
                "Ye Xia",
                "Yi-Ting Chen",
                "Zarana Parekh",
                "Hieu Pham",
                "Quoc Le",
                "Yun-Hsuan Sung",
                "Zhen Li",
                "Tom Duerig"
            ],
            "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Krause",
                "Michael Stark",
                "Jia Deng",
                "Li Fei-Fei"
            ],
            "title": "3d object representations for fine-grained categorization",
            "venue": "In Proceedings of the IEEE international conference on computer vision workshops,",
            "year": 2013
        },
        {
            "authors": [
                "Yangguang Li",
                "Feng Liang",
                "Lichen Zhao",
                "Yufeng Cui",
                "Wanli Ouyang",
                "Jing Shao",
                "Fengwei Yu",
                "Junjie Yan"
            ],
            "title": "Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Subhransu Maji",
                "Esa Rahtu",
                "Juho Kannala",
                "Matthew Blaschko",
                "Andrea Vedaldi"
            ],
            "title": "Fine-grained visual classification of aircraft",
            "venue": "arXiv preprint arXiv:1306.5151,",
            "year": 2013
        },
        {
            "authors": [
                "Maria-Elena Nilsback",
                "Andrew Zisserman"
            ],
            "title": "Automated flower classification over a large number of classes",
            "venue": "Sixth Indian Conference on Computer Vision, Graphics & Image Processing,",
            "year": 2008
        },
        {
            "authors": [
                "Omkar M Parkhi",
                "Andrea Vedaldi",
                "Andrew Zisserman",
                "CV Jawahar"
            ],
            "title": "Cats and dogs",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2012
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Benjamin Recht",
                "Rebecca Roelofs",
                "Ludwig Schmidt",
                "Vaishaal Shankar"
            ],
            "title": "Do imagenet classifiers generalize to imagenet",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Khurram Soomro",
                "Amir Roshan Zamir",
                "Mubarak Shah"
            ],
            "title": "Ucf101: A dataset of 101 human actions classes from videos in the wild",
            "venue": "arXiv preprint arXiv:1212.0402,",
            "year": 2012
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Haohan Wang",
                "Songwei Ge",
                "Zachary Lipton",
                "Eric P Xing"
            ],
            "title": "Learning robust global representations by penalizing local predictive power",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Wei Wang",
                "Vincent W Zheng",
                "Han Yu",
                "Chunyan Miao"
            ],
            "title": "A survey of zero-shot learning: Settings, methods, and applications",
            "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),",
            "year": 2019
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Gabriel Ilharco",
                "Jong Wook Kim",
                "Mike Li",
                "Simon Kornblith",
                "Rebecca Roelofs",
                "Raphael Gontijo Lopes",
                "Hannaneh Hajishirzi",
                "Ali Farhadi",
                "Hongseok Namkoong"
            ],
            "title": "Robust fine-tuning of zero-shot models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yongqin Xian",
                "Bernt Schiele",
                "Zeynep Akata"
            ],
            "title": "Zero-shot learning-the good, the bad and the ugly",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Jianxiong Xiao",
                "James Hays",
                "Krista A Ehinger",
                "Aude Oliva",
                "Antonio Torralba"
            ],
            "title": "Sun database: Large-scale scene recognition from abbey to zoo",
            "venue": "In 2010 IEEE computer society conference on computer vision and pattern recognition,",
            "year": 2010
        },
        {
            "authors": [
                "Renrui Zhang",
                "Rongyao Fang",
                "Peng Gao",
                "Wei Zhang",
                "Kunchang Li",
                "Jifeng Dai",
                "Yu Qiao",
                "Hongsheng Li"
            ],
            "title": "Tip-adapter: Training-free clip-adapter for better visionlanguage modeling",
            "venue": "arXiv preprint arXiv:2111.03930,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiyang Zhou",
                "Jingkang Yang",
                "Chen Change Loy",
                "Ziwei Liu"
            ],
            "title": "Conditional prompt learning for vision-language models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiyang Zhou",
                "Jingkang Yang",
                "Chen Change Loy",
                "Ziwei Liu"
            ],
            "title": "Learning to prompt for vision-language models",
            "venue": "International Journal of Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Beier Zhu",
                "Yulei Niu",
                "Yucheng Han",
                "Yue Wu",
                "Hanwang Zhang"
            ],
            "title": "Prompt-aligned gradient for prompt tuning",
            "venue": "arXiv preprint arXiv:2205.14865,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Much of the success of deep learning when it comes to vision tasks, such as classification, object detection or segmentation, is due to ever bigger models trained on increasingly large quantity of data.\nA popular approach to make use of immense sources of uncurated data in the form of images with textual descriptions are vision-language models [19, 25]. Here both image and description are individually mapped into a joint embedding space. This embedding is optimized, so that matching pairs are close, and the distance between all other pairs large. A model trained in this fashion can be used for zeroshot classification, as the language model can deal with every conceivable class by embedding a textual description (e.g. \u201da picture of [CLASS]\u201d).\nEven though these models can be applied to all kinds of classification tasks, their performance sometimes is suboptimal. This might be the case if used on a dataset with specific characteristics, that differ from the original training set, e.g. if the task is to recognize the action performed in an image, even though during training the model only saw generic objects. In these cases a common technique is to fine-tune the pre-trained model for the task at hand. However, updating the complete model is quite expensive\n(as the employed models are large). There are two solutions proposed in the literature to tackle this problem. One is prompt-learning [35\u201337] where the context around the class (\u201da picture of\u201d in the last example) is optimized for a specific dataset instead of hand crafted. The other option is to use adapters [11, 34], light-weight models (usually small MLPs) that modify the embedding produced by either the visual or language model (or both), thus updating the predictions without the need to update the original networks parameters.\nBoth these approaches however still have the problem, that even though the performance is improved for the specific domain and task the fine-tuning was done for, this comes at the cost of a decrease in performance on other tasks/domains compared to the original model [12].\nThe goal of this work is to reap the benefits of fine-tuning on a specific task, without losing the generalization ability of the original model. Work in this direction has already been done in the form of CoCoOp [35], where promptlearning is employed, but the context is not fine-tuned on a specific dataset, but instead a suitable context is predicted from the image to be classified. Another approach using prompt-learning is ProGrad [37] where the context update is restricted in order not to lose information from the pretraining stage. Although both methods decrease the performance loss in the zero-shot setting, they still do not reach the abilities of the original model.\nIn contrast we choose a simple method based on adapters. The idea is to only update the embedding where we actually have training data and leave it unchanged everywhere else, thus retaining the original predictions of the model, where we cannot improve on them. Furthermore, even where we have data we want to change the embedding as little as possible, to allow sensible interpolation between fine-tuned and original embedding. This approach is extremely lightweight, as we only need to tune a small amount of parameters, and back-propagating through the original model is not necessary. Still we show an improvement in performance compared to the previous state of the art.\nar X\niv :2\n21 2.\n06 55\n6v 1\n[ cs\n.C V\n] 1\n3 D\nec 2\n02 2"
        },
        {
            "heading": "2. Related Work",
            "text": "Zero-Shot Learning Zero-shot learning describes a setting, where the set of classes at training and during testing are disjoint or at least not identical [4], thus the relation between classes and images belonging to that class needs to be learned indirectly. There are numerous works of research in this area, so we instead refer to [30, 32] for an overview. A common approach to tackle this task is to relate pre-trained image and class embeddings [1, 2, 10]. This is conceptually very close to vision-language models, another popular framework that can be applied for zero-shot learning.\nVision-Language Models The term vision-language model describes networks that learn an alignment between images and text in a joint embedding space. Today these models are usually based on contrastive learning, which has been popularized for pre-training of image models [5, 13, 16] and aims to maximize the distance between similar and dissimilar instances in the embedding space. Currently one of the most popular vision-language models is CLIP [25], which we use in all our experiments. It uses a Transformer [28] as text encoder and a ResNet [14] or ViT [8] as image encoder. ALIGN [19] is a similar approach, whereas DeCLIP [21] tries to improve the training procedure in order reach the same performance with less data.\nFine-Tuning When it comes to fine-tuning a pre-trained vision-language model, there are broadly three types of approaches in the literature. It is possible to fine-tune the entire model, but afterwards interpolate between the original and updated weights, to counteract overfitting (WiSEFT [31]). Alternatively, not the model itself is trained, but only an adapter, that is applied onto the embedding space (CLIP-Adapter [11]). This is the approach we choose as well. Instead of learning this adapter, it can be extracted from the fine-tuning dataset (TIP-Adapter [34]). As this requires data for every class it is evaluated on, it is however not suitable for zero-shot learning. The last approach is called prompt engineering. Here the context of the text embedding is optimized for performance on the training set (CoOp [36]). This learning can be restricted in order not to decrease the loss of information from the pre-training stage (ProGrad [37]). Another option is to predict the (textual) context for each image (CoCoOp [35]). This mitigates overfitting on the train set and thus is better at retaining zero shot ability on unseen classes, but comes at the cost of an increased training time."
        },
        {
            "heading": "3. Method",
            "text": "Before introducing our approach in more detail, we will give a short overview on how vision-language models work in general on the example of CLIP, which is used in all our experiments."
        },
        {
            "heading": "3.1. Vision-Language Models",
            "text": "Vision-Language models consist of two networks: an image encoder fI and a text encoder fT . Their exact implementation is of no interest to us in this context. All we need to know, is that these models embed an image or a text respectively to a (normalized) feature vector of the same dimension. The cosine distance between an embedded image and text should then correspond to their similarity i.e. how well the text describes the image.\nDuring training, we are given a batch of n images X and their textual descriptions Y . We make the simplifying assumption that each text is a perfect description of the corresponding image and all other texts are completely unrelated. Thus, we want to minimize the cosine distance between embeddings of matching image/text pairs fI(xi),fT (yi) and maximize the distance between all other pairs within the batch fI(xi),fT (yj) with i 6= j.\nAnother view would be to regard the cosine distance as the likelihood, that a given text y describes the corresponding image x, or vice versa. We can compute the normalized probabilities as:\np(y|x) = exp(fI(x) T fT (y)/\u03c4)\u2211n\ni=1 exp(fI(x)T fT (yi)/\u03c4) (1)\nwhere \u03c4 is a learned temperature parameter. As we assume the embeddings to be normalized, the dot product is equivalent to the cosine similarity. The probability p(x|y) only differs in the normalization.\nIn this view it makes now sense to maximize the probability for the correct pairs, for which we can use the Cross Entropy loss. As we want to maximize the probabilities in both directions the loss is given as:\nL = \u2212 1 N N\u2211 i=1 (log(p(xi|yi)) + log(p(yi|xi)))/2 (2)\nZero-Shot Classification This approach leads to a semantically meaningful embedding of both images and text, that can be used for downstream tasks. Alternatively, we can use it directly for zero-shot classification. For this we embed a text for each class (e.g. a picture of [CLASS]) to a feature vector fT (yk). Then, to classify a given image, we embed it and compute the distance between its feature vector fI(x) and all class embeddings. The class probability for a class k is then given similarly as before as p(yk|x)."
        },
        {
            "heading": "3.2. Local Linear Updates",
            "text": "A big benefit of vision-language models is their generalization capability. As they are usually trained on immense datasets, they tend to show great zero-shot capabilities even on unseen domains. However, they are not necessarily well tuned for small specific datasets, that were not well represented in the original training data.\nAlthough we could fine-tune the networks on this specific set, this would come at the cost of the models ability to generalize and be quite expensive. Instead we add additional functions on the output of the model gI \u25e6 fI and gT \u25e6 fT respectively and optimize only those. This is much cheaper, as g has much less parameters (we will omit the subscript for both f and g if both the image and textual models are meant). Furthermore, as g is applied onto the output of f we do not even have to back-propagate through the big models. We could even precompute f on the given dataset to save further computation cost. In our case we use distinct networks for gI and gT but is possible to use the same function as well (gI = gT ) as they are applied on the same domain.\nThe training works slightly differently than before. As we now do not have unique image/text pairs, but instead images with their classes, we leave out one half of the loss, leading to the classical Cross Entropy loss:\nL = \u2212 1 N N\u2211 i=1 log(p(yi|xi)) (3)\nA similar idea was presented in CLIP Adapter [11] (Here only gI is used). This approach significantly reduces the cost of fine-tuning, but does not solve the problem of loos-\ning capabilities on the previous training dataset while overfitting on the new one.\nLocal Interpolation To lessen the overfitting WISE-FT [31] interpolates the weights between the original model and a fine-tuned version. With a similar aim in CLIP Adapter [11] the results of f and g are interpolated:\n\u03b1(g \u25e6 f) + (1\u2212 \u03b1)f (4)\nHere \u03b1 is a global parameter. It would be more sensible to localize this interpolation to the area of the feature space, where we obtained new data. Only there do we actually have information onto how to sensibly update the embeddings. As g is a global function but we only supervise it at our training samples, it is unlikely that it represents a sensible modification away from these samples. Thus, in our case \u03b1 is not a global parameter, but a function,\n\u03b1(x,D) = \u03b2 \u00b7maxd\u2208D(exp(\u2212\u03b3(1\u2212 xT d))) (5)\nwhere D is the set of datapoints we fine-tuned on and \u03b2 is a global parameter similar to how \u03b1 was defined previously. \u03b3 concentrates the focus of the interpolation mask and thus influences in what range our updated embedding should be applied. If we would let it go towards zero, we would have a global \u03b1 parameter, similarly to CLIP Adapter. On the other hand, if we let it go towards infinity, we would only update exactly the images and classes on which we fine-tuned.\nWhenever an image is close to one already seen during training, we thus use our updated features, otherwise we utilize the general knowledge of the pre-trained model. Note that we do this separately for the text and image encoder, so there are separate sets Dtext and Dimage.\nClustering This approach requires us to save the feature vectors of all datapoints seen during fine-tuning. This is not a problem, as long as the dataset used is indeed very small. If this however is not the case, we cluster the feature vectors to find sensible representatives. For this we use agglomerative clustering, where we start by regarding each datapoint as an individual cluster and then iteratively merge pairs based on the maximum distance between their members until we have reached the desired number of clusters. Each cluster needs a position, which is computed as the (normalized) mean of all its members.\nWe chose this approach, as it does not make any assumptions about cluster shapes nor does it require a sensible initialization. Furthermore, we expect the number of clusters to be in a similar order of magnitude as the number of datapoints, so we do not need many merge operations. However, as we show in the ablation (Sec.4.4), clustering only has a small effect on the performance and is mainly used to bound the memory consumption. Thus, the exact clustering algorithm is not likely to make much of a difference either.\nIdentity Regularization So far, we have restricted the region, where we change the feature space, but not the magnitude of the update, which can become arbitrarily large. It is however desirable, that the update is as small as possible, while minimizing the training loss. As we assume the original pre-trained features to already be useful, we want to stay as close to them as possible in order to retain generality. Furthermore the interpolation between f and g \u25e6 f should result in sensible embeddings, which is more likely the case, if they are close to each other. In other words, g should stay as close to the identity as possible.\nThis is easy to enforce, if we simply choose g as an affine function g = Wx + b. In this case our regularization takes the form: \u03bb(||W \u2212 I||2 + ||b||2) (6) where I is the identity matrix and \u03bb a weighting parameter.\nOf course we could choose a more complex function and regularize g to stay close to the identity at a set of sample points. However, a dense sampling of the embedding space is infeasible, and we are interested in retaining this property wherever the interpolation weight \u03b1 is non-zero.\nFurthermore, we initialize g as the identitiy function, which is trivial for affine functions, but not for non-linear MLPs."
        },
        {
            "heading": "4. Evaluation",
            "text": "For our evaluation we follow CoCoOp [35], where three problem settings are investigated:\n1. Generalization to new classes within a given dataset.\n2. Generalization to new datasets after fine-tuning.\n3. Generalization to domain-shift.\nBefore presenting the conclusions, we will introduce the used datasets, and explain the training procedure.\nDatasets Similar to CoCoOp, we follow CoOp [36] in the choice of datasets used in evaluation. To be precise we use 11 datasets that cover a wide range of tasks: ImageNet [7] and Caltech101 [9] for generic object classification, OxfordPets [24], StanfordCars [20], Flowers102 [23], Food101 [3] and FGVCAircraft for more specific object classification, [22], SUN397 [33], DTD [6], EuroSAT [15] and UCF101 [27] for a diverse set of tasks. Furthermore, to evaluate domain generalization we regard ImageNet as source and four different versions under different types of domain shift as target. The four datasets are: ImageNetV2 [26], ImageNet-Sketch [29], ImageNet-A [18] and ImageNet-R [17].\nThe set of images for few-shot training are randomly sampled for each dataset, while using the original test set for testing. For approaches that need training we average the results over three runs.\nTraining Our implementation is based on the published code of CoOp. We use the same learning rate and number of epochs as they do. Following CoCoOp we use ViT-B/16 as the vision backbone of CLIP. As ProGrad has been evaluated on a different backbone, we retrained it for a fair comparison. Note that both CoOp and CoCoOp have a context length of 4 initialized as the prompt: \u201da photo of a\u201d, whereas for CLIP Adapter and us the context is class dependent and ProGrad has a context length of 16 with a class-dependent initialization. If not stated otherwise we choose the parameters of our approach as \u03b2 = 0.5, \u03b3 = 20, \u03bb = 1e3 and the number of clusters as 512."
        },
        {
            "heading": "4.1. Base to New Generalization",
            "text": "On each dataset, the classes are split equally into a set of \u201dbase\u201d classes on which the adapter is trained and unseen \u201dnew\u201d classes, where we only evaluate. Thus, no matter how many shots are given for the training, on the new classes we will always do zero shot inference. We show results for different numbers of shots in Figure 2 and report exact values in Table 1. Here we also give the harmonic mean between the evaluation on base and new classes for an easier comparison of the approaches regarding their respective trade-offs.\nAs can be seen, on the 16 shot evaluation our approach outperforms all other methods on 8 out of 11 datasets, when regarding the harmonic mean. Here we have on average an improvement of almost 3 percentage points to CoCoOp (the next best method). Furthermore, (on average) our localized adapter beats all other methods regarding new classes independent of the number of shots and is first or second on the base classes.\nOur method is the only one that reaches the performance of CLIP when it comes to zero-shot performance on unseen classes, whereas all other methods show a drop in performance here, that usually increases with the number of shots, hinting at overfitting."
        },
        {
            "heading": "4.2. Cross-Dataset Generalization",
            "text": "In this experiment the models are fine-tuned on ImageNet and then evaluated on the other datasets, thus an improvement on ImageNet (compared to CLIP) is expected. Interestingly both CoCoOp and our approach show an improvement (on average) on the other datasets as well. Apparently the training samples of ImageNet are numerous and diverse enough to avoid overfitting and the data distribution of ImageNet is closer to the other regarded datasets than the original training set of CLIP. Although our method does not reach the results of CoCoOp on this evaluation we come very close."
        },
        {
            "heading": "4.3. Domain Generalization",
            "text": "In this last comparison the models are again fine-tuned on ImageNet and then evaluated on different versions with a clear domain shift. Here we can see a slight drop of performance between our method and prompt based approaches. This might be due to the fact, that prompt-based approaches\nonly fine-tune the input of the text encoder. As the class names and thus the text encodings are not affected by domain shift, their performance generalizes better. On the other hand we directly update the text and image embedding (and CLIP Adapter only updates the image embedding), which might be problematic as here changes caused\nby the domain-shift have a more direct effect."
        },
        {
            "heading": "4.4. Further Analysis",
            "text": "Ablation Here we discuss the effect different design choices in our approach have on the result. For this we regard the average performance achieved on the Base to New training setup, when using 16 shots (Table 4). As already mentioned, it barely makes any difference, whether we use clustering or not (\u201dno cluster\u201d), thus it is a sensible choice to limit the memory requirements. Using the global dampening parameter \u03b2 does lead to an improvement, although it is rather small (\u201dno damp\u201d).\nNot restricting the interpolation to the training sam-\nples (\u201dno mask\u201d) leaves the results on the base classes unchanged but leads to overfitting and thus a reduced performance on unseen classes. On the other hand focusing the interpolation exclusively on the training samples (\u201dDirac mask\u201d equivalent to a \u03b3 parameter of infinity) leads to the same performance as CLIP for new classes, but of course this way our method cannot improve on unseen classes either, as seen in Tables 2 and 3. Note that for numerical reasons, we did not actually implement a \u03b3 parameter of infinity, but clamped \u03b1 to zero or one, depending on some small distance threshold.\nInterestingly leaving out the identity regularization (\u201dno reg\u201d) barely has any effect on the results, whereas an initial-\nization to identity seems to be more important (\u201drand. init\u201d). We assume, that the training does not include enough update steps for effects to be seen. To substantiate this claim we report the actual distance to identity in Table 4 (for relevant experiments). Here we can see, that the distance between our adapter and the identity function does correlate with performance. Lastly we can see, that only using a single Linear Layer as an adapter without any of our additional improvements leads to significantly worse results, especially on the unseen classes, thus each of our improvements makes only a small difference individually, but together they significantly increase performance.\nTraining speed A comparison of the training speed between our approach and prompt based methods depends on both the batch size and the number of classes.\nAs we can precompute the class embeddings, the training time of our method is almost independent of their number. Prompt based approaches instead need to compute the class embeddings in every iteration. On the other hand the number of class embeddings is independent of the batch size, whereas our adapter needs to be applied to every training sample. In Figure 3 we show the timing for a single for-\nward and backward pass depending on the batch size. As can be seen our method and CLIP Adapter are consistently the fastest and the difference in their timings is negligible (it is barely possible to differentiate their lines). Gener-\nally the overhead of the computations due to the adapter barely matter, as can be discerned from the similar slope of CoOp and the adapter based methods. The number of classes influences the distance between these parallel lines, which signifies the overhead due to the computation of their embedding.\nFor CoCoOp we only have a single data point, as batches with more than one sample do not fit into memory. Thus, although for a batch size of one the training speed is similar to CoOp, in practice CoCoOp is much slower, as we cannot increase the batch size. ProGrad is consistently slower than other methods due to additional computations needed for gradient decomposition."
        },
        {
            "heading": "5. Conclusion",
            "text": "As the requirements in size, data and compute for state of the art AI models increases, it becomes more and more important to be able use available pre-trained networks for complex downstream tasks. In order to do this we need to be able to fine-tune these models in an efficient manner, preferably without loosing the generalization capability, that makes them so useful in the first place.\nWe have introduced an extremely simple approach for this task, introducing small linear updates to the embedding space, localized to the datapoints, where we fine-tune. Our model is fast to train and needs a minimal amount of extra parameters, but still reaches state of the art results both on fine-tuned and unseen classes.\nIn this work we always trained our adapter for optimal performance on a single dataset. A possible future research direction would be to generalize our approach to multiple distinct fine-tuning datasets. It would be possible to use dataset-dependent adapters and interpolation weights, but some further work would be needed to make this scalable.\nAcknowledgements This work was funded by the German Research Foundation within the Gottfried Wilhelm Leibniz programme, as well as through the project \u201dTraining the Archive\u201d in cooperation with the Ludwig Forum Aachen and the HMKV Hartware MedienKunstVerein, Dortmund. \u201dTraining the Archive\u201d is funded by the Digital Culture Programme of the Kulturstiftung des Bundes (German Federal Cultural Foundation). Funded by the Beauf-\ntragte der Bundesregierung fu\u0308r Kultur und Medien (Federal Government Commissioner for Culture and the Media). We would like to thank Dominik Bo\u0308nisch (heading the \u201dTraining the Archive\u201d project) for helpful discussions."
        }
    ],
    "title": "Localized Latent Updates for Fine-Tuning Vision-Language Models",
    "year": 2022
}