{
    "abstractText": "Distributed computing is known as an emerging and efficient technique to support various intelligent services, such as large-scale machine learning. However, privacy leakage and random delays from straggling servers pose significant challenges. To address these issues, coded computing, a promising solution that combines coding theory with distributed computing, recovers computation tasks with results from a subset of workers. In this paper, we propose the adaptive privacy-preserving coded computing (APCC) strategy, which can adaptively provide accurate or approximated results according to the form of computation functions, so as to suit diverse types of computation tasks. We prove that APCC achieves complete data privacy preservation and demonstrate its optimality in terms of encoding rate, defined as the ratio between the computation loads of tasks before and after encoding. To further alleviate the straggling effect and reduce delay, we integrate hierarchical task partitioning and task cancellation into the coding design of APCC. The corresponding partitioning problems are formulated as mixedinteger nonlinear programming (MINLP) problems with the objective of minimizing task completion delay. We propose a low-complexity maximum value descent (MVD) algorithm to optimally solve these problems. Simulation results show that APCC can reduce task completion delay by a range of 20.3% to 47.5% when compared to other state-of-the-art benchmarks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Qicheng Zeng"
        },
        {
            "affiliations": [],
            "name": "Zhaojun Nan"
        },
        {
            "affiliations": [],
            "name": "Sheng Zhou"
        }
    ],
    "id": "SP:62d9c2a24d8c56ad180b855ba42ecacb0ebc1f24",
    "references": [
        {
            "authors": [
                "J. Dean",
                "G. Corrado",
                "R. Monga",
                "K. Chen",
                "M. Devin",
                "M. Mao",
                "M. Ranzato",
                "A. Senior",
                "P. Tucker",
                "K. Yang"
            ],
            "title": "Large scale distributed deep networks",
            "venue": "Proc. Adv. neural inf. proces. syst. (NeurIPS), vol. 25, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "M. Abadi",
                "P. Barham",
                "J. Chen",
                "Z. Chen",
                "A. Davis",
                "J. Dean",
                "M. Devin",
                "S. Ghemawat",
                "G. Irving",
                "M. Isard"
            ],
            "title": "Tensorflow: a system for large-scale machine learning",
            "venue": "Proc. USENIX Symp. Oper. Syst. Des. Implement. (OSDI), 2016, pp. 265\u2013283.",
            "year": 2016
        },
        {
            "authors": [
                "G. Nguyen",
                "S. Dlugolinsky",
                "M. Bob\u00e1k",
                "V. Tran",
                "\u00c1. L\u00f3pez Garc\u0131\u0301a",
                "I. Heredia",
                "P. Mal\u0131\u0301k",
                "L. Hluch\u1ef3"
            ],
            "title": "Machine learning and deep learning frameworks and libraries for large-scale data mining: a survey",
            "venue": "Artif. Intell. Rev., vol. 52, pp. 77\u2013124, Jun. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Sun",
                "F. Zhang",
                "J. Zhao",
                "S. Zhou",
                "Z. Niu",
                "D. G\u00fcnd\u00fcz"
            ],
            "title": "Coded computation across shared heterogeneous workers with communication delay",
            "venue": "IEEE Trans. Signal Process., vol. 70, pp. 3371\u20133385, Jun. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Dean",
                "L.A. Barroso"
            ],
            "title": "The tail at scale",
            "venue": "Commun. ACM, vol. 56, no. 2, pp. 74\u201380, Feb. 2013.",
            "year": 2013
        },
        {
            "authors": [
                "R. Tandon",
                "Q. Lei",
                "A.G. Dimakis",
                "N. Karampatziakis"
            ],
            "title": "Gradient coding: Avoiding stragglers in distributed learning",
            "venue": "Proc. Int. Conf. Mach. Learn. PMLR, 2017, pp. 3368\u20133376.",
            "year": 2017
        },
        {
            "authors": [
                "K. Lee",
                "M. Lam",
                "R. Pedarsani",
                "D. Papailiopoulos",
                "K. Ramchandran"
            ],
            "title": "Speeding up distributed machine learning using codes",
            "venue": "IEEE Trans. Inf. Theory, vol. 64, no. 3, pp. 1514\u20131529, Mar. 2018.",
            "year": 2018
        },
        {
            "authors": [
                "S. Li",
                "M.A. Maddah-Ali",
                "Q. Yu",
                "A.S. Avestimehr"
            ],
            "title": "A fundamental tradeoff between computation and communication in distributed computing",
            "venue": "IEEE Trans. Inf. Theory, vol. 64, no. 1, pp. 109\u2013128, Jan. 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Q. Yu",
                "M.A. Maddah-Ali",
                "A.S. Avestimehr"
            ],
            "title": "Polynomial codes: an optimal design for high-dimensional coded matrix multiplication",
            "venue": "Proc. Adv. neural inf. proces. syst. (NeurIPS), vol. 30, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Straggler mitigation in distributed matrix multiplication: Fundamental limits and optimal coding",
            "venue": "IEEE Trans. Inf. Theory, vol. 66, no. 3, pp. 1920\u20131933, Mar. 2020.",
            "year": 1920
        },
        {
            "authors": [
                "N. Ferdinand",
                "S.C. Draper"
            ],
            "title": "Hierarchical coded computation",
            "venue": "Proc. IEEE Int. Symp. Inf. Theory (ISIT). IEEE, 2018, pp. 1620\u20131624.",
            "year": 2018
        },
        {
            "authors": [
                "A. Reisizadeh",
                "S. Prakash",
                "R. Pedarsani",
                "A.S. Avestimehr"
            ],
            "title": "Coded computation over heterogeneous clusters",
            "venue": "IEEE Trans. Inf. Theory, vol. 65, no. 7, pp. 4227\u20134242, Jul. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "W. Raghupathi",
                "V. Raghupathi"
            ],
            "title": "Big data analytics in healthcare: promise and potential",
            "venue": "Health information science and systems, vol. 2, no. 1, pp. 1\u201310, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "A. McAfee",
                "E. Brynjolfsson",
                "T.H. Davenport",
                "D. Patil",
                "D. Barton"
            ],
            "title": "Big data: the management revolution",
            "venue": "Harvard business review, vol. 90, no. 10, pp. 60\u201368, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "Q. Yu",
                "S. Li",
                "N. Raviv",
                "S.M.M. Kalan",
                "M. Soltanolkotabi",
                "S.A. Avestimehr"
            ],
            "title": "Lagrange coded computing: Optimal design for resiliency, security, and privacy",
            "venue": "Proc. Int. Conf. Artif. Intell. Stat. PMLR, 2019, pp. 1215\u20131225.",
            "year": 2019
        },
        {
            "authors": [
                "H. Yang",
                "J. Lee"
            ],
            "title": "Secure distributed computing with straggling servers using polynomial codes",
            "venue": "IEEE Trans. Inf. Forensics Secur., vol. 14, no. 1, pp. 141\u2013150, Jan. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "W.-T. Chang",
                "R. Tandon"
            ],
            "title": "On the capacity of secure distributed matrix multiplication",
            "venue": "Proc. IEEE Glob. Commun. Conf. (GLOBECOM). IEEE, 2018, pp. 1\u20136.",
            "year": 2018
        },
        {
            "authors": [
                "M. Aliasgari",
                "O. Simeone",
                "J. Kliewer"
            ],
            "title": "Private and secure distributed matrix multiplication with flexible communication load",
            "venue": "IEEE Trans. Inf. Forensics Secur., vol. 15, pp. 2722\u20132734, Feb. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Kim",
                "J. Lee"
            ],
            "title": "Private secure coded computation",
            "venue": "Proc. IEEE Int. Symp. Inf. Theory (ISIT). IEEE, 2019, pp. 1097\u20131101.",
            "year": 2019
        },
        {
            "authors": [
                "J. Kakar",
                "S. Ebadifar",
                "A. Sezgin"
            ],
            "title": "On the capacity and stragglerrobustness of distributed secure matrix multiplication",
            "venue": "IEEE Access, vol. 7, pp. 45 783\u201345 799, Mar. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H.A. Nodehi",
                "S.R.H. Najarkolaei",
                "M.A. Maddah-Ali"
            ],
            "title": "Entangled polynomial coding in limited-sharing multi-party computation",
            "venue": "Proc. IEEE Inf. Theory Workshop (ITW). IEEE, 2018, pp. 1\u20135.",
            "year": 2018
        },
        {
            "authors": [
                "Q. Yu",
                "A.S. Avestimehr"
            ],
            "title": "Entangled polynomial codes for secure, private, and batch distributed matrix multiplication: Breaking the\u201d cubic\u201d barrier",
            "venue": "Proc. IEEE Int. Symp. Inf. Theory (ISIT). IEEE, 2020, pp. 245\u2013250.",
            "year": 2020
        },
        {
            "authors": [
                "W.-T. Chang",
                "R. Tandon"
            ],
            "title": "On the upload versus download cost for secure and private matrix multiplication",
            "venue": "Proc. IEEE Inf. Theory Workshop (ITW). IEEE, 2019, pp. 1\u20135.",
            "year": 2019
        },
        {
            "authors": [
                "R.G. D\u2019Oliveira",
                "S. El Rouayheb",
                "D. Karpuk"
            ],
            "title": "Gasp codes for secure distributed matrix multiplication",
            "venue": "IEEE Trans. Inf. Theory, vol. 66, no. 7, pp. 4038\u20134050, Jul. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H. Akbari-Nodehi",
                "M.A. Maddah-Ali"
            ],
            "title": "Secure coded multi-party computation for massive matrix operations",
            "venue": "IEEE Trans. Inf. Theory, vol. 67, no. 4, pp. 2379\u20132398, Apr. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "B. Tahmasebi",
                "M.A. Maddah-Ali"
            ],
            "title": "Private function computation",
            "venue": "Proc. IEEE Int. Symp. Inf. Theory (ISIT), 2020, pp. 1118\u20131123.",
            "year": 2020
        },
        {
            "authors": [
                "T. Jahani-Nezhad",
                "M.A. Maddah-Ali"
            ],
            "title": "Berrut approximated coded computing: Straggler resistance beyond polynomial computing",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 45, no. 1, pp. 111\u2013122, Jan. 2023.",
            "year": 2023
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Codedsketch: A coding scheme for distributed computation of approximated matrix multiplication",
            "venue": "IEEE Trans. Inf. Theory, vol. 67, no. 6, pp. 4185\u20134196, Jun. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Soleymani",
                "R.E. Ali",
                "H. Mahdavifar",
                "A.S. Avestimehr"
            ],
            "title": "Approxifer: A model-agnostic approach to resilient and robust prediction serving systems",
            "venue": "Proc. AAAI Conf. Artif. Intell. (AAAI), vol. 36, no. 8, 2022, pp. 8342\u20138350.",
            "year": 2022
        },
        {
            "authors": [
                "M. Fahim",
                "V.R. Cadambe"
            ],
            "title": "Numerically stable polynomially coded computing",
            "venue": "IEEE Trans. Inf. Theory, vol. 67, no. 5, pp. 2758\u20132785, May 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Ramamoorthy",
                "L. Tang"
            ],
            "title": "Numerically stable coded matrix computations via circulant and rotation matrix embeddings",
            "venue": "IEEE Trans. Inf. Theory, vol. 68, no. 4, pp. 2684\u20132703, Apr. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "N. Charalambides",
                "H. Mahdavifar",
                "A.O. Hero"
            ],
            "title": "Numerically stable binary gradient coding",
            "venue": "Proc. IEEE Int. Symp. Inf. Theory (ISIT). IEEE, 2020, pp. 2622\u20132627.",
            "year": 2020
        },
        {
            "authors": [
                "B. Buyukates",
                "S. Ulukus"
            ],
            "title": "Timely distributed computation with stragglers",
            "venue": "IEEE Trans. Commun., vol. 68, no. 9, pp. 5273\u20135282, Sep. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "B. Has\u0131rc\u0131o\u011flu",
                "J. G\u00f3mez-Vilardeb\u00f3",
                "D. G\u00fcnd\u00fcz"
            ],
            "title": "Bivariate polynomial coding for efficient distributed matrix multiplication",
            "venue": "IEEE J. Sel. Areas Inf. Theory, vol. 2, no. 3, pp. 814\u2013829, Sep. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "E. Ozfatura",
                "S. Ulukus",
                "D. G\u00fcnd\u00fcz"
            ],
            "title": "Straggler-aware distributed learning: Communication\u2013computation latency trade-off",
            "venue": "Entropy, vol. 22, no. 5, p. 544, May 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Dutta",
                "M. Fahim",
                "F. Haddadpour",
                "H. Jeong",
                "V. Cadambe",
                "P. Grover"
            ],
            "title": "On the optimal recovery threshold of coded matrix multiplication",
            "venue": "IEEE Trans. Inf. Theory, vol. 66, no. 1, pp. 278\u2013301, Jan. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "C.-S. Yang",
                "A.S. Avestimehr"
            ],
            "title": "Coded computing for secure boolean computations",
            "venue": "IEEE J. Sel. Areas Inf. Theory, vol. 2, no. 1, pp. 326\u2013337, Mar. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "T. Tang",
                "R.E. Ali",
                "H. Hashemi",
                "T. Gangwani",
                "S. Avestimehr",
                "M. Annavaram"
            ],
            "title": "Adaptive verifiable coded computing: Towards fast, secure and private distributed machine learning",
            "venue": "Proc. IEEE Int. Parallel Distrib. Process. Symp. (IPDPS). IEEE, 2022, pp. 628\u2013638.",
            "year": 2022
        },
        {
            "authors": [
                "M. Soleymani",
                "R.E. Ali",
                "H. Mahdavifar",
                "A.S. Avestimehr"
            ],
            "title": "Listdecodable coded computing: Breaking the adversarial toleration barrier",
            "venue": "IEEE J. Sel. Areas Inf. Theory, vol. 2, no. 3, pp. 867\u2013878, Sep. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "F. Zhang",
                "Y. Sun",
                "S. Zhou"
            ],
            "title": "Coded computation over heterogeneous workers with random task arrivals",
            "venue": "IEEE Commun. Lett., vol. 25, no. 7, pp. 2338\u20132342, Jul. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "F. Wu",
                "L. Chen"
            ],
            "title": "Latency optimization for coded computation straggled by wireless transmission",
            "venue": "IEEE Wireless Commun. Lett., vol. 9, no. 7, pp. 1124\u20131128, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "N. Van Huynh",
                "D.T. Hoang",
                "D.N. Nguyen",
                "E. Dutkiewicz"
            ],
            "title": "Joint coding and scheduling optimization for distributed learning over wireless edge networks",
            "venue": "IEEE J. Sel. Areas Commun., vol. 40, no. 2, pp. 484\u2013 498, Feb. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "D. Kim",
                "H. Park",
                "J.K. Choi"
            ],
            "title": "Optimal load allocation for coded distributed computation in heterogeneous clusters",
            "venue": "IEEE Trans. Commun., vol. 69, no. 1, pp. 44\u201358, Jan. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J.-P. Berrut",
                "L.N. Trefethen"
            ],
            "title": "Barycentric lagrange interpolation",
            "venue": "SIAM review, vol. 46, no. 3, pp. 501\u2013517, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "J.-P. Berrut"
            ],
            "title": "Rational functions for guaranteed and experimentally wellconditioned global interpolation",
            "venue": "Comput. Math. Appl., vol. 15, no. 1, pp. 1\u201316, 1988.",
            "year": 1988
        },
        {
            "authors": [
                "Q. Zeng",
                "S. Zhou"
            ],
            "title": "On the capacity of privacy-preserving and straggler-robust distributed coded computing",
            "venue": "Proc. IEEE/CIC Int. Conf. Commun. China (ICCC). IEEE, 2021, pp. 664\u2013669.",
            "year": 2021
        },
        {
            "authors": [
                "E.L. Lawler",
                "D.E. Wood"
            ],
            "title": "Branch-and-bound methods: A survey",
            "venue": "Operations research, vol. 14, no. 4, pp. 699\u2013719, 1966.",
            "year": 1966
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 5.\n06 65\n4v 2\n[ cs\n.I T\n] 3\n0 O\nct 2\n02 3\nIndex Terms\u2014Coded computing, privacy preservation, hierarchical task partitioning, task cancellation, task completion delay.\nI. INTRODUCTION\nUnder the vision of \u201cInternet of Everything\u201d, intelligenceenabled applications are essential, leading to a variety of crucial computation tasks, such as training and inference of complex machine learning models based on extensive datasets [1]\u2013[3]. However, executing these computation-intensive tasks on a single device with limited computation capability and power resources presents significant challenges. To this end, distributed computing emerges as a practical solution, where a central node, referred to as master, manages task division, assignment, and result collection, while multiple distributed computing nodes, called workers, process the assigned partial computation tasks in parallel [4].\nNevertheless, while distributed computing accelerates the computation process by employing multiple workers for parallel processing, the total delay is dominated by the slowest worker, as the master must wait for all workers to complete their assigned tasks [5]. The delay of the slowest worker\nQicheng Zeng, Zhaojun Nan, and Sheng Zhou are with the Beijing National Research Center for Information Science and Technology, Department of Electronic Engineering, Tsinghua University, Beijing 100084, China (e-mail: zengqc19@mails.tsinghua.edu.cn; nzj660624@mail.tsinghua.edu.cn; sheng.zhou@tsinghua.edu.cn).\ncan exceed five times that of others, as demonstrated in the experimental results in [6]. Moreover, due to the randomness of delays, slow workers are difficult to identify in advance. To tackle this so-called straggling effect, a promising approach is to adopt coded computing [6]\u2013[12], which combines coding theory with distributed computing. By adding computational redundancies during the encoding process, this approach allows computation tasks to be completed with results from a subset of workers.\nIn coded computing, workers are tasked with processing input data and returning results, which raises privacy and security concerns. Computation tasks may involve sensitive information, such as patient medical data, customer personal information, and proprietary company data [13], [14]. It is essential to maintain the data privacy. Simultaneously, ensuring security in computation is necessary to prevent computation results from being tampered with by Byzantine (malicious) workers. Consequently, recent research in this field has aimed to develop coded computing strategies that address not only the straggling effect but also privacy and security concerns. One such approach combines additional random data insertion with prevalent coding methods like polynomial coding [15]\u2013[25]. This method enhances the robustness of the system against straggling workers while also improving privacy and security by obscuring the original data.\nIn the majority of existing studies, matrix multiplication is treated as the primary application in coded computing, and its performance has been extensively validated. However, realworld computation tasks are often more diverse than mere matrix multiplications. For instance, in a linear regression task, the iterative process of solving weights involves calculating previous weights multiplied by the quadratic power of the input data. This implies that the coded computing scheme for matrix multiplication must be executed twice in each step, and the computation becomes considerably more complex when considering other tasks, such as inference with deep neural networks.\nIn terms of extending the applicability of coded computing, one of the state-of-the-art approaches is Lagrange Coded Computing (LCC) [15]. LCC employs Lagrange polynomial interpolation to transform the input data before and after encoding into interpolation points on the computation function. This allows the recovery of desired results through the reconstruction of the interpolation function, while guaranteeing the privacy of the input data. LCC is compatible with various computation tasks, ranging from matrix multiplication to polynomial functions, and offers an optimal recovery threshold concerning the degree of polynomial functions. In [21],\n2 [25], [26], the problem of using matrix data as inputs and polynomial functions as computation tasks is also explored.\nHowever, LCC still suffers from several shortcomings [27]. First, the recovery threshold is proportional to the degree of polynomial functions, which can be prohibitively large for complex tasks. Second, Lagrange polynomial interpolation can be ill-conditioned, making it challenging to ensure the numerical stability. In [27], Berrut\u2019s Approximated Coded Computing (BACC) is proposed to address these shortcomings and further expand the scope of computation tasks to arbitrary functions. However, BACC only yields approximated computing results and does not guarantee data privacy preservation. Other related works [28]\u2013[32] also focus on approximate results while attempting to maintain the numerical stability of coded computing.\nIn the aforementioned studies, numerous aspects of coded computing have been explored; however, there is still room for improvement concerning its inherent approach to addressing the straggling effect, i.e., trading computational redundancy for reduced delay. This is because the results from straggling workers are entirely disregarded, and thus the computation resources of workers may be wasted. In [11], a hierarchical task partitioning structure is proposed, wherein divided tasks are further partitioned into multiple layers, and workers process their assigned tasks in the order of layer indices. Consequently, straggling workers can return the results of lower layers instead of none, while fast workers can reach higher layers and return more results. Similar performance improvements are achieved by multi-message communications (MMC) [33]\u2013[35], where workers are permitted to return partial results of assigned tasks in each time slot, enabling straggling workers to contribute to the system.\nEssentially, there are three ways to alleviate the straggling effect given the total number of workers. First, the recovery threshold of coded computing schemes should be minimized, as a smaller recovery threshold implies fewer workers to wait for [9], [10], [15], [16], [36]\u2013[39]. As a result, the master can recover desired computing results even with more straggling workers. Second, the computation load for each worker should be carefully designed based on its computation capability, which is formulated as optimization problems in [4], [40]\u2013 [43]. This approach narrows the gap between the delays of fast and slow workers. Third, workers should be capable of returning partial results of assigned tasks, allowing them to complete varying amounts of computation based on their computing capabilities, rather than fast workers completing all tasks while slow workers contribute virtually nothing. The third point aligns with the focus of hierarchical task partitioning structure and MMC.\nIn this work, we consider a distributed system with one master and multiple workers, and propose an adaptive privacypreserving coded computing (APCC) strategy, which primarily focuses on the applicability for diverse computation tasks, the privacy preservation of input data, and the mitigation of straggling effect. Subsequently, the hierarchical task partitioning structure is introduced into APCC, and based on this, we propose an operation called cancellation to prevent slower workers from processing completed tasks, which reduces\nresource wastes and thus improves the delay performance. Specifically, the main contributions are summarized as follows:\n\u2022 We propose the APCC framework that effectively miti-\ngates the straggling effect and fully preserves data privacy. APCC is suitable for diverse computation tasks including polynomial functions and non-polynomial functions, and can adaptively provide accurate results or approximated results with controllable error. \u2022 We rigorously prove the complete privacy preservation of\ninput data in APCC, as well as the optimality of APCC with respect to the encoding rate. The encoding rate is defined as the ratio between the computation load of tasks before and after encoding, serving as an indicator of the performance of coded computing schemes in mitigating the straggling effect. \u2022 Accounting for the randomness of task completion delay,\nwe formulate hierarchical task partitioning problems, with or without cancellation, as mixed integer nonlinear programming (MINLP) problems with the objective of minimizing task completion delay. We propose a maximum value descent (MVD) algorithm to optimally solve the problems with linear complexity. \u2022 Extensive simulations demonstrate the improvements in\ndelay performance offered by APCC when compared to other state-of-the-art coded computing benchmarks. Notably, APCC achieves a reduction in task completion delay ranging from 20.3% to 47.5% compared to LCC, LCC with MMC, and BACC. Simulations also explore the trade-off between task completion delay and the level of privacy preservation.\nThe remainder of the paper is structured as follows. Section II presents the system model. In Section III, we propose the adaptive privacy-preserving coded computing strategy, namely APCC. In Section IV, the performance of APCC is further analyzed in terms of encoding rate, privacy preservation, approximation error, numerical stability, communication costs, encoding and decoding complexity. In Section V, we proposed the MVD algorithm to address the hierarchical task partitioning optimization problem with or without cancellation. Simulation results are provided in Section VI, and conclusions are drawn in Section VII."
        },
        {
            "heading": "II. SYSTEM MODEL",
            "text": "In our distributed computing system, depicted in Fig. 1, a master and N workers collaborate to compute the function f : V \u2192 U over an equally predivided input dataset D = {Dk} K\u22121 k=0 , where Dk \u2208 V. The master aims to obtain the results {f(Dk)} K\u22121 k=0 . To achieve this, the master assigns encoded input data {D\u0303n} N\u22121 n=0 to the N workers for processing, awaiting their results. Subsequently, the master employs partial returned results for decoding, ultimately recovering {f(Dk)} K\u22121 k=0 . It is important to note that we consider the computation of {f(Dk)} K\u22121 k=0 as the entire task, with the computation of f(Dk) being a subtask. Taking into account the unreliable channels and the dynamically changing computation workload of workers, some of them may fail to return results to the master in time,\n3 Workers StragglersColluders\nMaster\n\u2026\noriginal subtasks\n\u2026\nencoded subtasks\nFig. 1. System Model.\nwhich are referred to as stragglers. It is also assumed that workers are honest but curious, meaning they will send back the correct computation results, but there could be up to L (L < N) colluding workers communicating with each other and attempting to learn information about the input data {Dk} K\u22121 k=0 . These workers are called colluders."
        },
        {
            "heading": "III. ADAPTIVE PRIVACY-PRESERVING CODED COMPUTING",
            "text": "In this section, we propose the adaptive privacy-preserving coded computing (APCC) strategy, which consists of three steps: 1) Encoding; 2) Assignment; 3) Decoding. In the encoding and assignment steps, the K equally divided input data {Dk} K\u22121 k=0 are not directly encoded, instead, they are first partitioned into r sets. Subsequently, the input data in each set are encoded into N parts, which are then assigned to N workers for computing. In the decoding step, the master utilizes the results from the first completed subtasks in each set and interpolation methods to recover the original function f , achieving the purpose of decoding. Note that the decoding step adaptively provides accurate or approximated results according to the type of function f .\nIn the following, we begin with a general description to explain how APCC works and then provide an illustrative example for accurate results case without loss of generality. Lastly, we introduce the hierarchical task partitioning structure of APCC, and the cancellation of completed subtasks based on this hierarchical structure."
        },
        {
            "heading": "A. General Description",
            "text": "In this subsection, we provide a general description of the proposed APCC strategy. As introduced in Section II, the inputs of the function f are first euqally predivided into K parts D0, D1, . . . , DK\u22121, and K corresponding subtasks f(Dk), k \u2208 [0 : K \u2212 1] are formed. The APCC strategy then follows three steps: 1) Encoding; 2) Assignment; 3) Decoding, and obtains accurate or approximated computing results of {f(Dk)} K\u22121 k=0 according to the choice of adaptive parameter w\u0303n. The pseudo-code for APCC is presented in Algorithm 1.\n1) Encoding: In the initialization step, the K subtasks are further partitioned into r sets, with set i containing Ki subtasks f(Di,j) Ki\u22121 j=0 . Here, Ki should satisfy \u2211r\u22121 i=0 Ki = K . Inspired by Barycentric polynomial interpolation [27], [44], the input\nAlgorithm 1: APCC\nInput: f,Dk, r,Ki, N, L Output: {f(Dk)} K\u22121 k=0 .\n1 1) Encoding: 2 The master partitions K subtasks into r sets, and set i\nconsists of Ki subtasks {f(Di,j)} Ki\u22121 j=0 , which satisfies \u2211r\u22121\ni=0 Ki = K; 3 for i = 0 : r \u2212 1 do 4 The master encodes {Di,j} Ki\u22121 j=0 into {D\u0303i,n} N\u22121 n=0\naccording to D\u0303i,n = gi(\u03b2n), n \u2208 [0 : N \u2212 1];\n5 2) Assignment: 6 for n = 0 : N \u2212 1 do 7 The master assigns {D\u0303i,n} r\u22121 i=0 to worker n, and\n{f(D\u0303i,n)} r\u22121 i=0 are computed in order;\n8 Workers return f(D\u0303i,n) to the master is once completing computation;\n9 3) Decoding:\n10 for i = 0 : r \u2212 1 do 11 The master performs cancellation for set i if it is\ncompleted;\n12 The master decodes for set i according to the first Ri received results; 13 if f is a polynomial function of degree d then 14 Accurate results: f(gi(x)) = ri(x);\n15 if f is an arbitrary function then 16 Approximated results: f(gi(x)) \u2248 ri(x);\n17 Set i is completed with f(Di,j) = f(gi(\u03b1i,j)), j \u2208 [1 : Ki];\ndata {Di,j} Ki\u22121 j=0 for set i is linearly encoded through function gi(x) as:\ngi(x) =\nKi\u22121\u2211\nj=0\nwi,j x\u2212\u03b1i,j\u2211Ki+L\u22121\nk=0 wi,k\nx\u2212\u03b1i,k\nDi,j\n+\nKi+L\u22121\u2211\nj=Ki\nwi,j x\u2212\u03b1i,j\u2211Ki+L\u22121\nk=0 wi,j\nx\u2212\u03b1i,k\nZi,j, (1)\nwhere {Zi,j} Ki+L\u22121 j=Ki\nare L random matrices added to preserve the privacy, each element in Zi,j follows a uniform distribution, and x \u2208 R is the encoding parameter. {\u03b1i,j} Ki+L\u22121 j=0 are distinct values selected as Chebyshev points of the first kind:\n\u03b1i,j = cos (2j + 1)\u03c0\n2(Ki + L) , j \u2208 [0 : Ki + L\u2212 1]. (2)\nwi,j is a constant related to \u03b1i,j and calculated as:\nwi,j = 1\n\u220fKi+L\u22121 k=0,k 6=j (\u03b1i,j \u2212 \u03b1i,k) , j \u2208 [0 : Ki + L\u2212 1]. (3)\nNote that the form of gi(x) ensures that\ngi(\u03b1i,j) = Di,j , j \u2208 [0 : Ki \u2212 1]. (4)\n4 The encoded input data {D\u0303i,n} N\u22121 n=0 are obtained as:\nD\u0303i,n = gi(\u03b2n), n \u2208 [0 : N \u2212 1]. (5)\n{\u03b2n} N\u22121 n=0 are selected as Chebyshev points of the second kind:\n\u03b2n = cos n\u03c0\nN \u2212 1 , n \u2208 [0 : N \u2212 1]. (6)\n2) Assignment: For set i, the encoded data D\u0303i,n = gi(\u03b2n) is assigned to worker n. Consequently, each worker obtains r encoded subtasks and executes them in the order of set indices. Once completed, the results of encoded subtasks f(D\u0303i,n) are returned to the master. In other words, after the original K subtasks are partitioned into multiple sets, each set is transformed into N encoded subtasks assigned to N workers for processing. We refer to this structure as hierarchical task partitioning.\n3) Decoding: For set i, the master decodes using function ri(x), which is constructed by interpolation [27], [44] as:\nri(x) =\nRi\u22121\u2211\nn=0\nw\u0303n x\u2212x\u0303n\u2211Ri\u22121\nm=0 w\u0303m\nx\u2212x\u0303m\nf(gi(x\u0303n)), (7)\nwhere f(gi(x\u0303n)), n \u2208 [0, Ri \u2212 1] are the first Ri received results for set i, x\u0303n are the corresponding encoding parameters that belong to a subset of {\u03b2n, n \u2208 [0, N \u2212 1]} due to the presence of stragglers, and the parameter w\u0303n is adaptive for different cases, as follows.\nCase 1: Accurate results. If f is a polynomial function of degree d, where the degree d of a polynomial function is defined as the maximum order of its monomials, the adaptive parameters w\u0303n are determined as:\nw\u0303n = 1\n\u220fRi\u22121 m=0,m 6=n(x\u0303n \u2212 x\u0303m) , n \u2208 [0 : Ri \u2212 1]. (8)\nIn this case, ri(x) is a Barycentric polynomial interpolation function [44] for f(gi(x)). The degree of gi(x) equals (Ki + L\u22121), so that f(gi(x)) remains a polynomial function, and its degree satisfies degf(gi(x)) \u2264 d(Ki + L\u2212 1). Consequently, if the number of received results Ri for set i satisfies:\nRi = d(Ki + L\u2212 1) + 1, (9)\nit implies that sufficient interpolation points have been obtained to precisely recover f(gi(x)) through ri(x), and the entire computation process is completed with\nf(Di,j) = f(gi(\u03b1i,j)) = ri(\u03b1i,j), (10)\nfor any i \u2208 [0 : r \u2212 1], j \u2208 [0 : Ki \u2212 1].\nNote that Eq.(9) means that the accurate result case of APCC has the same recovery threshold as LCC [15]. Besides, similar to LCC, when there is no need for privacy preservation which means L = 0, we can also provide an uncoded version of APCC by selecting the value of {\u03b2n} from {\u03b1i,j}. Thus, the new recovery threshold becomes:\nRi = N \u2212 \u230aN/Ki\u230b+ 1. (11)\nCase 2: Approximate results. If f is an arbitrary function,\nMaster\n12 subtasks 3 sets 3 encoded sets\n(a) Encoding\nMaster\n\u2026\n\u2026\nWorker\nWorker\nset index\n\u2026\nWorker 0\n\u2026\nset index\nMaster\n(b) Assignment and Decoding\nFig. 2. The three-step process of APCC.\nthe adaptive parameter w\u0303n is calculated as:\nw\u0303n = (\u22121) n, n \u2208 [0 : Ri \u2212 1]. (12)\nIn this case, ri(x) is a Berrut\u2019s rational interpolation function for f(gi(x)), as discussed in [27], [45]. The computed results f(gi(x\u0303n)) serve as the interpolation points of f(gi(x)), and they satisfy ri(x\u0303n) = f(gi(x\u0303n)) due to the property of Berrut\u2019s rational interpolation [45]. Therefore, the master can regard ri(x) as an approximation of f(gi(x)), which means that\nf(Di,j) = f(gi(\u03b1i,j) \u2248 ri(\u03b1i,j), (13)\nfor any i \u2208 [0 : r\u22121], j \u2208 [0 : Ki\u22121]. In addition, the approximation using ri(x) becomes more accurate as Ri increases. Thus, if the master desires more accurate computations, it simply needs to wait for more results."
        },
        {
            "heading": "B. An Illustrating Example",
            "text": "In this subsection, to show how APCC works, we present an illustrative example for the case of accurate results without loss of generality. Specifically, we consider a linear regression problem. The feature data D \u2208 Rp\u00d7q contains p data samples with q features, and the label vector is denoted by y \u2208 Rp\u00d71. The objective is to find the weighting vector w \u2208 Rq\u00d71 that minimizes the loss ||Dw \u2212 y||2. To solve this problem, the gradient descent method updates the weights iteratively along the negative gradient direction as follows:\nw(t+1) = w(t) \u2212 2\u03b7\np DT (Dw(t) \u2212 y), (14)\nwhere \u03b7 is the learning rate and t represents the iteration index. In order to apply the aforementioned update process to a distributed system with one master and N = 10 workers, for instance, the feature data D is first equally divided into K = 12 sub-matrices (D0, D1, . . . , D11) T . As w(t) is known by the workers and DTy can be precomputed by the master, the computation function (subtask) of the master in each iteration can be expressed as f(Dk) = D T k Dkw, k \u2208 [0 : 11]. After\n5 obtaining the results of the entire task {f(Dk)} 11 k=0, the gradi-\nent update is computed as DTDw = \u221111\nk=0 D T k Dkw, Dk \u2208\nR p 12\u00d7q .\nWe now illustrate how APCC can be applied in the above\nproblem, to obtain f(Dk) = D T k Dkw, k \u2208 [0 : 11].\n1) Encoding: As depicted in Fig. 2(a), since there are 12 subtasks f(Dk), k \u2208 [0 : 11], the master further partitions them into r = 3 sets before encoding the inputs, and set i (i = 0, 1, 2) contains Ki subtasks. The value selection for Ki will be formulated as an optimization problem in Section V. For instance, we assume that K0 = 5, K1 = 4, and K2 = 3, and they satisfy K0 + K1 + K2 = K = 12. After this hierarchical task partitioning, the input of the j-th subtask in set i is denoted as Di,j instead of the previous Dk, and {f(Di,j) = D T i,jDi,jw} Ki\u22121 j=0 are the Ki subtasks in this set.\nNext, the input data {Di,j} Ki\u22121 j=0 in set i are encoded into N = 10 parts {D\u0303i,n} 9 n=0 through gi(x), where x represents the encoding parameters and D\u0303i,n = gi(\u03b2n), n \u2208 [0 : 9]. Moreover, gi(x) is a polynomial function with a degree of (Ki+L\u2212 1), and its form ensures that the parameters {\u03b1i,j} satisfy gi(\u03b1i,j) = Di,j .\n2) Assignment: As Fig. 2(b) shows, for each set, the 10 encoded input data {D\u0303i,n} 9 n=0 are assigned to the 10 workers. Subsequently, each worker applies function f to compute and return the results to the master. As can be observed, the Ki original subtasks in set i are transformed into 10 subtasks performed on the 10 workers in parallel. Since there are 3 sets, each worker is assigned 3 subtasks. These subtasks are executed according to the order of sets, which implies f(D\u03030,n) is computed first, followed by f(D\u03031,n), and so on. 3) Decoding: As illustrated in Fig. 2(b), following the assignment of encoded input to workers, the master continuously awaits the subtask results from workers, and creates a decoding function ri(x) for set i. This decoding function is constructed using interpolation to recover the original function f(Di,j) = f(gi(\u03b1i,j)). Consequently, each received result, f(gi(\u03b2n)), can be regarded as an interpolation point for f(gi(x)), and ri(x) is precisely the interpolation function of f(gi(x)).\nPresently, f(Di,j) = D T i,jDi,jw is a polynomial function of degree d = 2, where the degree d of a polynomial function f is defined as the maximum order of its monomials. We have illustrated how to complete the decoding process in Subsection III.A. By setting the number of received results to Ri = d(Ki +L\u2212 1)+ 1, sufficient interpolation points are obtained to accurately recover f(gi(x)) through ri(x), i.e., f(Di,j) = f(gi(\u03b1i,j)) = ri(\u03b1i,j), for all i \u2208 [0 : 2] and j \u2208 [0 : Ki \u2212 1]."
        },
        {
            "heading": "C. Hierarchical Task Partitioning and Cancellation",
            "text": "In Fig. 2, the hierarchical task partitioning in APCC aims to maximize the utility of computing results from straggling workers. This is achieved through a well-designed structure and appropriate choice of Ki values. Although the same number of encoded subtasks are assigned to all workers, the number of successfully returned results from each worker can differ due to varying processing speeds. As a result,\nMaster\n\u2026\n\u2026 Worker\nWorker\n\u2026\n\u2026\nset index\n\u2026\n\u2026\nWorker\nindex\nWorker\n\u2026\n\u2026\nFig. 3. Hierarchical structure and the cancellation operation.\nstraggling workers may return fewer computing results than faster workers, but they can still make valuable contributions to task completion instead of being disregarded.\nFurthermore, the illustration in Fig. 2 suggests that Ki\u22121 should exceed Ki [11]. This assertion is explained as follows: The \u201ccompletion time\u201d of set i\u201d is defined as the moment when a sufficient number of encoded subtask results within set i are obtained. The overarching objective is to minimize the delay in completing the entire task, which must necessarily exceed the \u201ccompletion time\u201d of any set since the entire task remains incomplete until all r sets are recovered. Given that subtasks are executed in order of set indices, when set r is recovered, the master must have acquired results for the smaller-index sets equal to or greater than Ki. Opting for smaller values of Ki for the smaller-index sets would result in more workers experiencing straggling, a situation that should be averted. Further details are expounded in Section V.\nBased on the hierarchical structure, we propose an alternative method to further accelerate the coded computing process. As depicted in Fig. 3, the subtasks to be computed on each worker form an execution sequence. Once enough results for set i are obtained, the master can instruct workers that have not completed the computation of f(D\u0303i,n) to terminate or skip this part of the computation and proceed to compute the next subtasks f(D\u0303i+1,n) of the subsequent set. This operation, called \u201cCancellation\u201d, prevents computation resources from being wasted on completed sets. Considering the presence of nonpersistent stragglers, cancellation increases the probability of them overcoming the previous straggling effect and avoiding becoming stragglers again."
        },
        {
            "heading": "IV. PERFORMANCE ANALYSIS",
            "text": "In this section, we first define a metric called encoding rate to evaluate the efficiency performance of coded computing schemes, in terms of utilizing computation resources of workers as efficiently as possible. Then based on the optimal recovery threshold of LCC [15], we rigorously prove APCC for accurate results is also an optimal polynomial coding in terms of the encoding rate. Furthermore, an informationtheoretic guarantee to completely preserve the privacy of input data {Dk} K\u22121 k=0 in APCC is proved. Subsequently, we present an analysis of the approximation error for Case 2 of APCC, along with a discussion of numerical stability. At the end of this section, we provide a detailed analysis about the\n6 communication costs, encoding and decoding complexity for APCC, and compare it with other state-of-the-art strategies."
        },
        {
            "heading": "A. Optimality of APCC in Terms of Encoding Rate",
            "text": "To evaluate the performance of various coded computing schemes, a metric known as the encoding rate Rencode is used. This metric is defined as:\nRencode = K\nN \u2212 S , (15)\nwhere K is the number of subtasks before encoding, N is the number of subtasks after encoding (which is equivalent to the number of workers), and S represents the number of straggling workers that failed to return results before the task was completed. Similar metrics, such as those found in [17], [20], [46], have also been developed.\nBesides, since the recovery threshold, denoted by H , is defined as the minimum number of results needed to guarantee decodability, we have H = N \u2212S and thus Rencode = K/H . It is important to note that the encoding rate only applies when decodability is guaranteed.\nThe physical significance of the encoding rate is the ratio between the computation load of tasks before encoding and that required after encoding. For instance, given a task with a computation load of O(\u03b3), each subtask has a corresponding load of O( \u03b3\nK ). As (N \u2212 S) subtasks are successfully com-\npleted, the required computation load is O(\u03b3(N\u2212S) K\n). Since coded computing essentially trades computation redundancy for reduced delay to mitigate the straggling effect, it is reasonable to use this metric to evaluate the efficiency of different schemes.\nBefore proving the optimality of APCC, we present the\ndefinitions of capacity and linear coded computing schemes.\nDefinition 1. A linear coded computing scheme is one in which the encoded data is a linear combination of the original input data as follows:\nD\u0303n =\nK\u22121\u2211\nk=0\nGn,kDk + Z\u0303n, n \u2208 [0 : N \u2212 1], (16)\nwhere G \u2208 RN\u00d7k is the encoding matrix corresponding to the aforementioned encoding function gi(x), and Z\u0303n is the additive randomness.\nDefinition 2. The capacity C for a distributed computing system with one master and N workers, where the computation function used by the master is f , is defined as the supremum of the encoding rate Rencode as:\nC , supRencode, (17)\nover all feasible linear coded computing schemes that can address up to L colluders and S stragglers\nAs illustrated in Section III, APCC is a linear coded computing scheme and its hierarchical structure results in different Ki and Si for each set, with Ki and Si representing the number of subtasks before encoding and that of straggling workers, respectively. For set i, Ri represents the number of workers\nthat have successfully returned results in time, implying that the number of stragglers is Si = N \u2212 Ri. Moreover, set i is considered complete when Ri = d(Ki + L \u2212 1) + 1. Hence, the encoding rate of APCC can be calculated as:\nR [APCC] encode = Ki N \u2212 Si = N \u2212 Si \u2212 d(L \u2212 1)\u2212 1 d(N \u2212 Si) , (18)\nor the uncoded version for L = 0 :\nR [APCC] encode = Ki N \u2212 Si \u2264 N (N \u2212 Si)(Si + 1) , (19)\nwhere the equality holds when N can be divided by Ki.\nThe following theorem shows that the encoding rate of APCC achieves the capacity, thereby establishing the optimality of APCC. For the sake of clarity, we omit the set index i and focus on a specific set, without loss of generality.\nTheorem 1. For a coded computing problem (N,S, L, f), where N is the number of workers, S and L denote the number of stragglers and colluders, respectively, and f is an arbitrary polynomial function of degree d, the capacity C is given by:\nC =\n{ N\u2212S\u2212d(L\u22121)\u22121\nd(N\u2212S) , if L > 0,\nmax{N\u2212S+d\u22121 d(N\u2212S) , N (N\u2212S)(S+1)} if L = 0.\n(20)\nEssentially, the optimality of APCC in encoding rate is attributed to its identical polynomial coding structure when compared to LCC [15], despite having distinct function expressions. This observation also implies that the encoding rate of LCC can similarly achieve the capacity.\nTo prove Theorem 1, a lower bound on the capacity C is first established, which follows the encoding rate of APCC in (18) and (19). To establish the upper bound, we leverage the optimality statement of LCC, as illustrated in Theorem 1 and 2 of [15], which shows that the decodability of polynomial coded computing is guaranteed only when the following condition is met:\nN \u2265 { d(K + L\u2212 1) + 1 + S, if L > 0, min{d(K \u2212 1) + 1 + S, K(S + 1)} if L = 0.\n(21)\nTherefore, we have:\nK \u2264\n{ N\u2212S\u22121\nd \u2212 L+ 1, if L > 0,\nmax{N\u2212S+d\u22121 d , N(S+1)} if L = 0, (22)\nIneq.(22) presents the maximum number of tasks divisions permissible to ensure decodability, given the numbers of workers N , stragglers S and colluders L. The reason is that the more divisions there are, the more results are needed from workers. However, there are at most N workers, including S stragglers, to return results. Based on (22), an upper bound on the encoding rate can be derived as:\nRencode = K\nN \u2212 S\n\u2264\n{ N\u2212S\u2212d(L\u22121)\u22121\nd(N\u2212S) , if L > 0,\nmax{N\u2212S+d\u22121 d(N\u2212S) , N (N\u2212S)(S+1)} if L = 0.\n(23)\nSince the capacity C is the supremum of Rencode, it also has the same upper bound. With the lower bound provided\n7 previously, we can conclude that APCC is an optimal coded computing strategy that can reach the capacity in (20).\nTo enhance clarity, the fundamental proof for the derivation of (21) is briefly introduced in Appendix A, following the same steps as outlined in [15]. Nevertheless, we present an alternative and more concise proof for the construction of the involved multilinear function.\nPlease note that the conclusion presented in this subsection pertains only to accurate coded computing. For approximated coded computing, the use of different approximation methods can lead to varying errors, making it challenging to compare and analyze their impact on the encoding rate and capacity in a qualitative manner."
        },
        {
            "heading": "B. Guarantee of the Privacy Preservation",
            "text": "Recall that colluders are those workers who can communicate with each other and attempt to learn something about the original input data. Since the system can tolerate at most L colluders, we assume that there are L\u2032 colluders, where L\u2032 \u2264 L, and the user does not know which workers are colluding. We use the index set L = {l0, l1, . . . , lL\u2032\u22121} \u2286 {0, . . . , N \u2212 1} to denote the colluding workers, where |L| = L\u2032.\nAssuming that the input data {Di,j} Ki\u22121 j=0 are independent of each other, we denote the encoded input data sent to workers in the colluding set L for set i as:\nD\u0303i,L , (D\u0303i,l0 , D\u0303i,l1 , . . . , D\u0303i,lL\u2032\u22121). (24)\nTherefore, the information-theoretic privacy-preserving constraint can be expressed as:\nI(Di,0, Di,1, . . . , Di,Ki\u22121; D\u0303i,L) = 0, \u2200i \u2208 [0, r \u2212 1], (25)\nwhere I(\u00b7) represents the mutual information function. With the assumption of finite precision floating point arithmetic, the values of elements in the data matrices such as Di,j , D\u0303i,n, and Zi,j come from a sufficiently large field F. Given that the size of these data matrices is m\u00d7m\u2032, we have\nI(Di,0, Di,1, . . . , Di,Ki\u22121; D\u0303i,L) = H(D\u0303i,l0 , . . . , D\u0303i,lL\u2032\u22121)\n\u2212H(D\u0303i,l0 , . . . , D\u0303i,lL\u2032\u22121 |Di,0, . . . , Di,Ki\u22121) (a) = H(D\u0303i,l0 , . . . , D\u0303i,lL\u2032\u22121)\u2212H(Zi,Ki , . . . , Zi,Ki+L\u22121) (b) = H(D\u0303i,l0 , . . . , D\u0303i,lL\u2032\u22121)\u2212 Lmm \u2032 log |F| \u2264 H(D\u0303i,l0) + \u00b7 \u00b7 \u00b7+H(D\u0303i,lL\u2032\u22121)\u2212 Lmm \u2032 log |F| (c) \u2264 L\u2032mm\u2032 log |F| \u2212 Lmm\u2032 log |F| = 0, \u2200i \u2208 [0, r \u2212 1], (26)\nwhere (a) is due to the fact that all random matrices {Zi,j}\nKi+L\u22121 j=Ki are independent of the input data {Di,j} Ki\u22121 j=0 .\n(b) is because the entropy of each element in the random matrices equals log |F|, and (c) follows from the upper bound of the entropy of each element in D\u0303i,l(\u00b7) being log |F|. Since the mutual information is non-negative, it must be 0, which guarantees complete privacy preservation.\nNote that the analysis in this subsection is applicable to both accurate and approximate cases. This is because that the analysis only involves the encoding and assignment steps of APCC, and both cases require the same two initial steps.\nThe key difference between the two aforementioned cases is reflected in the decoding functions with distinct adaptive parameters w\u0303n, which correspond to Barycentric polynomial interpolation and Berrut\u2019s rational interpolation, respectively."
        },
        {
            "heading": "C. Analysis of Approximation Error for Case 2",
            "text": "According to the discussion in [27], let the interpolating objective function hi(x) = f(gi(x)) has a continuous second derivative on [\u22121, 1], and the number of received results Ri > 3, the approximation error is upper bounded as:\n||ri(x) \u2212 hi(x)|| \u2264 2(1 + \u0393) sin (N \u2212Ri + 1)\u03c0\n2(N \u2212 1) ||h\u2032\u2032i (x)||, (27)\nif Ri is even, and\n||ri(x) \u2212 hi(x)|| \u2264 2(1 + \u0393) sin (N \u2212Ri + 1)\u03c0\n2(N \u2212 1) (||h\u2032\u2032i (x)||+ ||h \u2032 i(x)||), (28)\nif Ri is odd, where \u0393 , (N\u2212Ri+1)(N\u2212Ri+3)\u03c0\n2\n4 .\nConsequently, for set i and a fixed total number of workers N , the approximation using ri(x) becomes more accurate as the number of received results Ri increases."
        },
        {
            "heading": "D. Numerical Stability",
            "text": "In coded computing, the issue of numerical stability typically arises from the decoding part, which is based on solving a system of linear equations involving a Vandermonde matrix. As previously discussed, Cases 1 and 2 of APCC employ Barycentric polynomial interpolation and Berrut\u2019s rational interpolation as decoding methods, respectively. For Case 1, Barycentric polynomial interpolation demonstrates good performance in addressing errors caused by floating-point arithmetic [44]. Regarding Case 2, it has been shown in [27] that the Lebesgue Constant of Berrut\u2019s rational interpolation grows logarithmically with the number of received results from workers, rendering it both forward and backward stable."
        },
        {
            "heading": "E. Communication Costs",
            "text": "In this subsection, we analyze the communication costs of APCC concerning the number of partitioning sets r and the number of task divisions K . For comparison, we also provide the analysis of communication costs for LCC [15]. The communication costs refers to the bit size of transmission data including encoded input data and required computing results. Given the computation function f and the original input data {Dk} K\u22121 k=0 , we assume that the communication costs to transmit {Dk} K\u22121 k=0 are O(x) for all strategies, and those to transmit {f(Dk)} K\u22121 k=0 back to the master are O(\u03c3x).\n1) APCC: Since the entire task is divided into K subtasks in APCC, and each worker (totally N workers) is assigned r subtasks, the costs for each worker are O ( xr K ) . Therefore, the total communication costs of encoded input data are O ( xrN K ) . On the other hand, the master needs to receive\u2211r\u22121\ni=0 [d(Ki + L\u2212 1) + 1] = d(K + rL \u2212 r) + r results to\n8 complete the entire task for Case 1 of APCC, so the total communication costs of feedback is O ( \u03c3x K [d(K + rL \u2212 r) + r] ) .\nFor Case 2 of APCC, the costs per feedback result are O ( \u03c3x K ) . Additionally, when considering cancellation, the master only needs to send signals to all workers at the moment when one set is finished, so the signaling overhead is relatively small compared to the communication costs of the computation data.\n2) LCC: According to [15], LCC divides the entire task into K \u2032 subtasks {f(Dk)} K\u2032\u22121 k=0 , and each worker is only assigned with one encoded subtask, so the costs for each worker in LCC are O( x K\u2032\n). Consequently, the total communication costs of encoded input data are O(xN\nK\u2032 ). In LCC, the master needs\nto receive d(K \u2032 + L \u2212 1) + 1 results to complete the entire task. Therefore, the total communication costs of feedback for LCC is O ( \u03c3x K\u2032 [d(K \u2032 + L\u2212 1) + 1] ) .\nNote that when comparing the delay performance of different coded computing strategies, we should make the computation loads for a single worker the same to ensure fairness. Assuming the computation loads of subtasks are proportional to the bit size of input data, which are exactly the communication costs for each worker, we have O (\nx K\u2032\n) = O ( xr K ) , and\nthus we can derive\nK \u2032 = K/r. (29)\nAccordingly, the communication costs of both encoded input data and feedback for the two strategies are the same. This is reasonable because, despite APCC requiring the reception of more feedback results due to multiple sets, the bit size of an individual result is smaller compared to LCC, thanks to a more granular task division where K = K \u2032r.\nIn fact, the reduction of computation delay through the hierarchical task partitioning structure in APCC does not entail increased communication costs. Rather, it is primarily attributed to its capability to enable workers to return partial results as they return every result of subtasks, thereby achieving more efficient utilization of the computing resources of the straggling workers. However, the partitioning of multiple sets in APCC does lead to increased encoding and decoding times, as elucidated in the subsequent subsection."
        },
        {
            "heading": "F. Encoding and Decoding Complexity",
            "text": "In this subsection, we provide the analysis of encoding and decoding complexity. Intuitively, APCC utilizes the hierarchical task partitioning structure to enhance delay performance. However, it does so at the cost of requiring multiple encoding and decoding operations, specifically r times for the r sets, when compared to LCC [15] and BACC [27].\nIn LCC and BACC, the encoding operations take N times, corresponding to the number of workers, while the decoding operations take K \u2032 times, equivalent to the number of task divisions. On the other hand, in the case of APCC, which features r partitioned sets, the encoding and decoding operations entail Nr and \u2211r i=0 Ki = K , respectively. When the computation loads per worker in all strategies are equal, i.e., K \u2032 = K/r, it can be deduced that the encoding and decoding operations in APCC are r times those of LCC and BACC."
        },
        {
            "heading": "V. HIERARCHICAL TASK PARTITIONING",
            "text": "In this section, the hierarchical task partitioning is formulated as an optimization problem with the objective of minimizing the task completion delay. The problem is divided into two cases for consideration: with and without cancellation. Through derivations, two mixed integer programming problems are obtained, and we propose a maximum value descent (MVD) algorithm to obtain the optimal solutions with low computational complexity. Moreover, after analysis, it is found that the MVD algorithm can be quickly executed by selecting appropriate input. Detailed explanations are provided as follows."
        },
        {
            "heading": "A. Problem Formulation",
            "text": "In the context of negligible encoding and decoding delays, with the computation delays of workers being the dominant component, the delay for a worker to complete a single subtask, denoted as T can be represented by a shifted exponential distribution [4], [7], [11], [12], [40], [41], whose cumulative distribution function (CDF) is given by:\nFT (t) = P[T \u2264 t] = { 1\u2212 e\u2212\u00b5(t\u2212a), if t \u2265 a, 0 otherwise,\n(30)\nwhere a > 0 is a parameter indicating the minimum processing time and \u00b5 > 0 is a parameter modeling the computing performance of workers. All N workers follow a uniform computation delay distribution defined in (30).\nRecall that in the hierarchical structure, the completion of a particular set is dependent on the successful receiving of a sufficient number of results from its encoded subtasks. The overall completion of the entire task is achieved only when all r sets have been completed. Notably, Hi is defined as the threshold number of successful results needed to ensure the completion of set i. Following the discussion in Section III and assuming that privacy preservation is required which means L > 0, the threshold for Case 1 of APCC can be expressed as Hi = d(Ki + L \u2212 1) + 1 according to (9). For Case 2 of APCC, the threshold Hi can be determined based on the desired approximation precision, with higher values of Hi leading to more accurate approximations.\nThe completion time of sets is defined as t , {ti, i \u2208 [0 : r \u2212 1]}, where ti denotes the time interval from the initial moment 0 of the entire task to the recovery moment of set i. The entire task is considered completed when all r sets have been recovered. Therefore, we denote the entire task completion delay as\nT [e] = max i\u2208[0:r\u22121] ti. (31)\nNote that while each worker executes the assigned subtasks in the order of set indices, the order in which these sets are recovered may not be the same. The completion time of sets is influenced not only by the set indices but also by the recovery thresholds Hi determined by Ki. Due to the randomness of delay, our objective is to minimize the entire task completion delay T [e] = maxi\u2208[0:r\u22121] ti, upon which the probability of the master recovering desired results\n9 for all sets is higher than a given threshold \u03c1s, as expressed by the following inequality:\nP[R0(t0) \u2265 H0, . . . , Rr\u22121(tr\u22121) \u2265 Hr\u22121] \u2265 \u03c1s, (32)\nwhere Ri(t) is defined as the number of returned results for set i until time t.\nHowever, to derive (32), we first need to obtain the distribution of the delay required to receive the last non-straggling result in each set and then derive their joint probability distribution, which is intractable, especially when considering the cancellation of completed sets. As a result, the problem with the constraint (32) is hard to solve.\nIn the following, we consider substituting (32) with an\nexpectation constraint (33d) and formulate the problem as:\nP1\u2212 1 : min {K} max i\u2208[0:r\u22121] ti (33a)\ns.t.\nr\u22121\u2211\ni=0\nKi = K, (33b)\nHi \u2264 N, \u2200i \u2208 [0, r \u2212 1] (33c)\nE[Ri(ti)] \u2265 Hi, \u2200i \u2208 [0, r \u2212 1] (33d) Ki, Hi \u2208 Z +, \u2200i \u2208 [0, r \u2212 1], (33e)\nwhere K , {Ki|i \u2208 [0 : r \u2212 1]} is the partitioning scheme. Constraint (33b) corresponds to the hierarchical task partitioning, and (33c) indicates that the threshold for each set should be smaller than the number of workers. In constraint (33e), Z+ represents the set of positive integers. Constraint (33d) states that the master is expected to receive sufficient results of encoded subtasks from workers to recover f(Di,j) Ki\u22121 j=0 in set i. Similar approximation approaches are also used in [4], [12], [40], [41], and the performance gap can be bounded [12].\nAs previously shown, Hi = d(Ki + L\u2212 1) + 1 for Case 1 of APCC. Additionally, the maximum of ti for all sets can be replaced with an optimization variable z by adding an extra constraint. Consequently, for Case 1 of APCC, P1\u2212 1 can be equivalently written as:\nP1\u2212 2 : min {K,z} z (34a)\ns.t. ti \u2212 z \u2264 0, \u2200i \u2208 [0, r \u2212 1], (34b)\nd(Ki + L\u2212 1) + 1\u2212 E[Ri(ti)] \u2264 0, \u2200i \u2208 [0, r \u2212 1], (34c)\nd(Ki + L\u2212 1) + 1\u2212N \u2264 0, \u2200i \u2208 [0, r \u2212 1], (34d)\nKi \u2208Z +, \u2200i \u2208 [0, r \u2212 1], (34e)\nConstraint (33b). (34f)\nFor Case 2 of APCC, one only needs to adjust constraints (34c) and (34d) according to the relationship between Ki and Hi, which does not affect the subsequent methods employed. Consequently, for the sake of convenience in expression, we will focus on Case 1 of APCC in the following parts of this section, without loss of generality."
        },
        {
            "heading": "B. APCC Without Cancellation",
            "text": "If the cancellation of completed sets is not considered, we first denote the delay of one worker to continuously complete\nAlgorithm 2: MVD\nInput: An arbitrary feasible solution: K = {Ki, i \u2208 [0, r \u2212 1]} Output: The optimal solution:\nK\u2217 = {K\u2217i , i \u2208 [0, r \u2212 1]} 1 do 2 Substitute K into the original problem and obtain\nthe following convex optimization problem.\nP : min {t,z} z s.t. (34b), (37b).\n3 Obtain the solution {t\u2217, z\u2217} to P by solving the Karush-Kuhn-Tucker (KKT) conditions. 4 Derive z\u2217 = maxi\u2208[0:r\u22121] t \u2217 i and assume\nt\u2217j = z \u2217, j \u2208 [0 : r \u2212 1] without loss of generality.\n5 Initialization: KMV D = K and zMV D = z \u2217. 6 for l = [0 : r \u2212 1], l 6= j do 7 Ktemp = K, Kj,temp = Kj \u2212 1, Kl,temp = Kl + 1. 8 Substitute Ktemp into the original problem and obtain the corresponding z\u2217temp like Step 2-3. 9 if z\u2217temp < zMV D then\n10 KMV D = Ktemp, zMV D = z \u2217 temp.\n11 K = KMV D. 12 while zMV D < z \u2217;\nResult: K\u2217 = K is the optimal solution.\nm subtasks as Tm, and derive its CDF from (30) as:\nP[Tm \u2264 t] =\n{ 1\u2212 e\u2212\u00b5( t m\n\u2212a), if t \u2265 ma, 0 otherwise.\n(35)\nSince computations on workers are independent, E[Ri(ti)] can be written as:\nE[Ri(ti)] =\nN\u22121\u2211\nn=0\nE[I{Ti+1\u2264ti}] = N \u00b7 P[Ti+1 \u2264 ti], (36)\nwhere I{x} denotes the indicator function that equals 1 if event x is true and equals 0 otherwise. P[Ti+1 \u2264 ti] is given by (35).\nSubstituting (36) into P1\u2212 2, we find (34d) is covered by (34c) and obtain the following optimization problem:\nP2\u2212 1 : min {K,z} z (37a)\ns.t. d(Ki + L\u2212 1) + 1\u2212N [1\u2212 e \u2212\u00b5( ti i+1\u2212a)] \u2264 0,\n\u2200i \u2208 [0, r \u2212 1], (37b)\nConstraints (33b), (34b), (34e). (37c)\nAs P2\u22121 shows, it is a mixed integer non-linear programming (MINLP) problem, which is usually NP-hard. Although its optimal solution can be found by the Branch and Bound (B&B) algorithm [47], the computational complexity is up to O ( (N d )r ) , which means the B&B algorithm becomes extremely time-consuming when either N or r are large.\nAccordingly, to efficiently obtain an optimal solution, we propose the maximum value descent (MVD) algorithm shown in Algorithm 2. The key idea of the MVD algorithm is to iter-\n10\natively update the input solution K = {Ki, i \u2208 [0 : r\u2212 1]} by adjusting Ki for the set that corresponds to the maximum value descent of the objective function z. In the MVD algorithm, each do-while loop can be regarded as one update, and Kj in Step 7 constantly approaches the optimal K\u2217j . Once reduced in an update, Kj will not increase because the objective function z must decrease in each update. When the updating process terminates, the optimal solution K\u2217 is exactly the obtained K in the last update. Furthermore, the MVD algorithm has a computational complexity of O ( Nr d ) , as the number of dowhile loops is determined by constraint (34d).\nBesides, the MVD algorithm can be executed quickly by selecting a sufficiently good partitioning solution as input. It should be noted that after relaxation and cancellation of the integer constraint in (34e), P2\u2212 1 can be transformed into a convex problem as follows:\nP2\u2212 2 : min {K,z} z (39a)\ns.t.Constraints (37b), (33b), (34b), (39b)\nKi > 0, \u2200i \u2208 [0, r \u2212 1]. (39c)\nand the optimal solution is given in Proposition 1.\nProposition 1. For given (N,K,L, d, r, \u00b5, a), the closed-form expressions of the optimal solution K[Prop1] and corresponding delay t[Prop1] to P2\u2212 2 are\nr\u22121\u2211\ni=0\ne\u2212\u00b5( z\u2217 i+1\u2212a) = r \u2212 d(K + rL \u2212 r) + r\nN , (40a)\nt [Prop1] i = z \u2217,K [Prop1] i =\nN d [1\u2212 e\u2212\u00b5( z\u2217 i+1\u2212a)]\u2212 1 d \u2212 L+ 1.\nProof. See Appendix B.\nDue to the convexity of P2 \u2212 2, the Euclidean distance between K[Prop1] and the optimal solution K\u2217 of P2\u2212 1 is small. Therefore, it is recommended to use a rounded result of K [Prop1] as the input for the MVD algorithm."
        },
        {
            "heading": "C. APCC With Cancellation",
            "text": "If the cancellation of completed sets is considered, a worker may be cancelled in a certain set but successfully returns results in time for the subsequent sets. For example, worker n may be a straggler for set i but completes its assigned subtask and returns the result in time for the next set (i+1) due to the cancellation. Such situations make it quite difficult to derive and analyze the expectation of Ri(t) as in the previous Section V.B, because the impact of the cancellation of the previous set on the delay of non-straggling workers in subsequent sets need to be considered. Therefore, we provide the following alternative perspective to simplify this problem.\nNote that if set i is the last completed one, the entire task is completed when the last needed result for this set is received. Thus, we define the delay of set i as T [e] i and aim to minimize maxi\u2208[0:r\u22121] E[T [e] i ]. To derive E[T [e] i ], consider that there are still N \u2212Hi + 1 = N \u2212 d(Ki + L \u2212 1) workers computing the last result for set i when other sets are finished. Once any one of these workers returns the first result, this set and the\nentire task will be completed. Accordingly, the CDF of T [e] i can be written as follows:\nP [ T\n[e] i \u2264 t ] = 1\u2212 (1\u2212 P [Ti+1 \u2264 t]) N\u2212d(Ki+L\u22121)\n=\n{ 1\u2212 e\u2212\u00b5(N\u2212d(Ki+L\u22121))( t i+1\u2212a), if t \u2265 (i+ 1)a,\n0 otherwise, (41)\nwhere Ti+1 is the delay needed to complete (i + 1) subtasks for one worker, shown previously in (35). Then we have\nE[T [e] i ] =\ni+ 1\n\u00b5[N \u2212 d(Ki + L\u2212 1)] + a(i+ 1). (42)\nBy further adding an extra optimization variable z to substitute maxi\u2208[0:r\u22121] E[T [e] i ], the optimization problem can be formulated as:\nP3\u2212 1 : min {K,z} z (43a)\ns.t. i+ 1\n\u00b5 [N \u2212 d(Ki + L\u2212 1)] + a(i + 1)\u2212 z \u2264 0, \u2200i \u2208 [0 : r \u2212 1], (43b)\nConstraints (33b), (34d), (34e). (43c)\nNote that P3\u22121 is a MINLP problem similar to P2\u22121 and has an O ( (N d )r ) computational complexity to solve if using B&B algorithm. However, after relaxation and cancelling the integer constraint in (34e), P3 \u2212 1 can also be transformed into a convex problem as:\nP3\u2212 2 : min {K,z} z (44a)\ns.t.Constraints (43b), (33b), (34d), (44b)\nKi > 0, \u2200i \u2208 [0, r \u2212 1], (44c)\nand optimal solution is given in Proposition 2.\nProposition 2. For given (N,K,L, d, r, \u00b5, a), the closed-form expression of the optimal solution K[Prop2] to P3\u2212 2 is\nr\u22121\u2211\ni=0\ni+ 1\nz\u2217 \u2212 a(i+ 1) = \u00b5 [rN \u2212 d(K + rL \u2212 r)] , (45a)\nK [Prop2] i =\nN\nd \u2212\ni + 1\nd\u00b5[z\u2217 \u2212 a(i+ 1)] \u2212 L+ 1. (45b)\nProof. See Appendix C.\nConsequently, MVD algorithm is used again to solve P3\u22121 with a computational complexity of O(Nr\nd ), and the rounded\nresult of K[Prop2] is recommended to be used as the input."
        },
        {
            "heading": "VI. SIMULATION RESULTS",
            "text": "In this section, we leverage simulation results to evaluate the performance of APCC in terms of task completion delay and compare it with other state-of-the-art coded computing strategies, including LCC [15], LCC with multi-message communications (LCC-MMC) [35], and BACC [27]. Additionally, we analyze the impact of the number of partitioned sets r and the number of colluding workers L on the task completion delay of APCC.\nIn simulations, the entire task is given, leading to a constant computation load for the entire task. In this scenario, we aim\n11\nto compare the entire task completion delay across various task division and coded computing strategies, illustrating the delay performance improvements introduced by APCC. We assume that the computation delay T0 of a single worker to complete the entire task follows a shifted exponential distribution, which is modeled as:\nP[T0 \u2264 t] = { 1\u2212 e\u2212\u00b50(t\u2212a0), if t \u2265 a0, 0 otherwise,\n(46)\nthen the computation delay T of a single worker to complete one subtask follows:\nP[T \u2264 t] =\n{ 1\u2212 e\u2212\u00b50(Kt\u2212a0), if t \u2265 a0\nK ,\n0 otherwise, (47)\nwhere K denotes the task division number, which may vary depending on the chosen coded computing strategies. The parameter a0 is set to 0.5 seconds, and \u00b50 is set as 1\n10a0 .\nIn APCC, {Ki} r\u22121 i=0 correspond to the number of subtasks in each set before encoding, and their value are obtained by MVD algorithm. Then, 5\u00d7 104 Monte Carlo realizations are run to obtain the average completion delay of the entire task, and the simulation codes are shared here1. Note that by comparing (47) with (30), we have \u00b5 = K\u00b50 and a = a0 K , and can further derive the distribution of Tm in (35). The benchmarks involved in this section are as follows:\n1) APCC: APCC is our proposed coded computing strategy in this paper. It first divides the entire task into K subtasks and then partitions them into r sets with different sizes. The number of subtasks in set i, i \u2208 [0, r \u2212 1] is denoted as Ki, which satisfies \u2211r\u22121 i=0 Ki = K . After that, each set is encoded into N subtasks assigned to the N workers. Consequently, each worker is assigned r subtasks. For Case 1 of APCC, the set i is recovered when the master has received d(Ki + L \u2212 1) + 1 results, and the entire task is completed when all sets are recovered.\n2) LCC: LCC proposed in [15] divides the entire task into K \u2032 subtasks and then encodes them into N subtasks assigned to N workers. Each worker in LCC is assigned one subtasks. Therefore, the entire task is completed when the master has received d(K \u2032 +L\u2212 1)+ 1 results. L = 0 means the absence of a requirement for privacy preservation. We assume that the number of workers N is greater than dK \u2032\u2212 1 to facilitate our analysis. Consequently, when L = 0, the recovery threshold is defined as d(K \u2032\u22121)+1 instead of N\u2212\u230aN/K \u2032\u230b+1 according to [15].\n3) LCC-MMC: MMC proposed in [35] is an another approach to utilize the computing results of straggling workers except for the hierarchical structure. It also achieves partial returning of results from workers through a more granular task division. Specifically, LCC-MMC divides the entire task into KLM subtasks and then encodes them into Nr subtasks. Each worker in LCC-MMC is assigned r subtasks and the entire task is completed when the master has received d(KLM \u2212 1) + 1 results. However, LCC-MMC can not preserve the privacy of input data because multiple encoded data from the same encoding function is sent to a worker, which is different from the case of APCC where r subtasks assigned to the\n1code link: https://github.com/Zemiser/APCC\nsame worker are generated by r different encoding functions {gi(x)} r\u22121 i=0 .\n4) BACC: The BACC strategy, as introduced in [27], offers approximate results with improved precision achievable by increasing the number of return results from workers. It shares a task division structure identical to LCC, partitioning the task into K \u2032 subtasks and then further encoding them into N subtasks. Each worker in BACC is assigned one such subtask.\nTo ensure fairness, and all strategies employ an identical number of workers and distribute an equivalent computation loads for a single worker. Assuming that the computation loads of the entire task are O(\u03b3), then each subtask f(Dk) in APCC has a computation load of O( \u03b3\nK ), and the computation\nloads of each worker in APCC are O(\u03b3r K ) because there are r partitioned sets. Similarly, we can derive that the computation loads of each worker in LCC, BACC and LCC-MMC are O( \u03b3 K\u2032 ), O( \u03b3 K\u2032 ) and O( \u03b3r KLM\n), respectively. In order to ensure that each worker in these schemes performs an identical fraction of the entire task as APCC, we have\nK \u2032 = KLM/r = K/r. (48)\nDue to the different applicability of various coded computing strategies, we will first conduct a comprehensive analysis and comparison of APCC alongside other strategies within the following three scenarios: 1) Accurate results with L colluding workers (L > 0); 2) Accurate results without colluding workers (L = 0); 3) Approximated results. Finally, we study the impact of the parameters r and L on the delay performance of APCC."
        },
        {
            "heading": "A. Accurate Results With L Colluding Workers (L > 0)",
            "text": "In this scenario, we consider the following three benchmarks: LCC, APCC without cancellation and APCC with cancellation. For fair comparison, the computation load of workers should be set the same, so we have K \u2032 = K/r. As shown in Fig. 4, the average completion delay of the entire task {f(Dk)} K\u22121 k=0 first decreases and then increases with the task division number K , indicating the existence of an optimal division that minimizes the delay. This trade-off arises from balancing the computation load of each worker and the minimum number of workers needed to recover {f(Dk)} K\u22121 k=0 .\n12\nOn the one hand, as the division number decreases, the computation load of each subtask increases, which leads to longer computation delays for each worker due to the increased workload. Although the number of workers waiting for results decreases, the increase in load negates this advantage. On the other hand, while the division number approaches the maximum, as illustrated in the inequality (22), the number of workers that the master needs to wait for approaches N , making the straggling effect a bottleneck for performance and increasing the delay. The zigzag fluctuations in the curve are mainly due to the integer values of the partitioning numbers.\nNote that the primary metric for evaluating different schemes in our study is the minimum task completion delay under different division numbers, as depicted in Fig. 4. This is because the division number K \u2032 = K r corresponds to the division of computation function inputs, which is typically a high-dimensional matrix. As such, K \u2032 can be adjusted flexibly in most cases. Therefore, the minimum achieved task completion delay is the main focus of our analysis.\nFig. 5 compares APCC and LCC in terms of the minimum task completion delay. In these benchmarks, \u2019Brute-Force\u2019 refers to a partitioning strategy derived from an exhaustive search across all possible values of {Ki}. Due to the highly complex traversal search, the brute-force results are only provided for scenarios with a smaller number of sets (r = 4). Fig. 5 illustrates that both APCC with and without cancellation yield sufficient reductions in task completion delay compared to LCC. For instance, when N = 100, L = 10, d = 2, r = 16, and the partitioning strategy obtained from MVD algorithm is utilized, APCC with and without cancellation achieve delay reductions of 41.4% and 47.5%, respectively, compared to LCC. Moreover, the comparison with the \u2019Brute-Force\u2019 benchmarks show that the partitioning strategy {Ki} obtained through the MVD algorithm is near-optimal."
        },
        {
            "heading": "B. Accurate Results Without Colluding Workers (L = 0)",
            "text": "In this scenario, we evaluate four benchmarks: LCC, LCCMMC, APCC with and without cancellation. Among these, only LCC does not consider partial results from straggling workers. Similar to Subsection IV.A, we set K \u2032 = KLM = K/r, with KLM representing the task division number for LCC-MMC.\nIn Fig. 6, both LCC-MMC and APCC effectively reduce task completion delay compared to LCC. Specifically, when r is large enough, APCC with cancellation closely approaches the performance of LCC-MMC. This similarity arises because in both APCC and LCC-MMC, the master utilizes nearly all computing results from workers when divided subtasks are sufficiently small. Fig. 6 also illustrates that when privacy is not a concern, MMC is a viable method to reduce the delay in coded computing.\nComparing to Fig. 5, we observe that the absence of colluding workers limits the potential for delay optimization. For instance, with parameters N = 100, L = 0, d = 2, and r = 16, APCC with cancellation achieves only a 20.3% delay reduction compared to LCC."
        },
        {
            "heading": "C. Approximated Results",
            "text": "In this subsection, we compare the task completion delay of BACC and the case 2 of APCC, which both can provide approximated results with fewer workers than the recovery threshold. To ensure uniform worker computation load, we also set K \u2032 = K/r, as in our previous analysis. Furthermore, since BACC shares an identical task division structure with LCC, we employ a smaller recovery threshold of the same form as LCC to evaluate its delay performance. For instance, when the recovery threshold d(K \u2032 +L\u2212 1)+ 1 exceeds N , a reduced uniform recovery threshold d2 (K\n\u2032+L\u2212 1)+1 below N can be employed for both BACC and APCC.\n13\nAs shown in Fig. 7, the hierarchical task partitioning and the cancellation of completed sets in APCC yield sufficient delay performance improvement. Compared to BACC, the proposed MVD algorithm for APCC achieves up to 42.9% delay reduction. Note that in this scenario, both APCC and BACC can obtain approximated results with fewer returned results, while LCC for accurate computation fails to work when K \u2032 is larger than 20 in the two cases of Fig. 7, as the recovery threshold of LCC needs to be larger than d(K \u2032 + L\u2212 1) + 1.\nD. Impact of r and L on the Performance of APCC\nThe impact of the hierarchical partitioning number of sets r on the task completion delay of APCC is illustrated in Fig. 8(a). It is observed that a larger number of sets r results in a smaller computation delay, which is consistent with the results shown in previous figures. The reduction in delay can be attributed to the fact that a larger r implies a smaller computation load for each subtask in the hierarchical structure, and the difference in computation load between fast and slow workers can be described more precisely. Consequently, the proposed MVD algorithm can better utilize the computing results of straggling workers to reduce delay. Furthermore, Fig. 8(a) indicates that the benefit of increasing r has a boundary effect, which corresponds to the upper bound of benefit brought by the granularity refinement of task divisions.\nRecall that L denotes the maximum number of colluding workers that a coded computing scheme can tolerate. The value of L can serve as an indirect indicator of the level of privacy preservation offered by the scheme. Specifically, a larger value of L corresponds to more stringent privacy protection and a higher tolerance for colluders. It is demonstrated in Section IV.B that complete data privacy can be achieved as long as the number of colluders remains below L.\nFig. 8(b) illustrates the impact of the number of colluding workers L on the trade-off between delay and privacy preservation. It is worth noting that, for a fixed K \u2032, increasing the value of L leads to a larger recovery threshold H for the original subtasks, which results in a longer task completion delay. Moreover, as demonstrated in (22), choosing a larger value of L restricts the maximum number of task divisions. Consequently, the range of K \u2032 values corresponding to the plotted curves in Fig. 8(b) varies with L."
        },
        {
            "heading": "VII. CONCLUSION",
            "text": "In this paper, we have investigated a distributed computing system that consists of one master and multiple workers. We have first proposed an adaptive privacy-preserving coded computing (APCC) strategy, which is suitable for diverse task scenarios and computation functions. APCC adaptively provides accurate or approximated results with controllable error according to the form of computation functions, and the computation process keeps numerically stable. We have rigorously proved the optimality of APCC in terms of encoding rate. The complete privacy preservation of input data has also been proved.\nWe have further provided a low-complexity maximum value descent (MVD) algorithm to optimally solve the hierarchical task partitioning problem in APCC, with and without considering cancellation, aiming at minimizing task completion delay. The cancellation is our proposed operation aiming to further accelerate computation by timely cancelling the completed tasks. Extensive simulations have demonstrated that APCC outperforms the state-of-the-art coded computing strategies by a range of 20.3% to 42.9% in terms of task completion delay."
        },
        {
            "heading": "APPENDIX A PROOF OF THE INEQUALITY (21)",
            "text": "In this appendix, the proof for the optimal recovery threshold of LCC [15] to guarantee decodablity is briefly introduced to enhance the clarity of the inequality (21). Additionally, we provide an alternative and more concise proof for the construction of the involved multilinear function.\nTo simplify the proof, a weakened result under the condition of multilinearity is first derived. After that, in order to extend to the case of a general polynomial function, a construction of multilinear functions based on polynomial functions is provided. The definition of the multilinear function is as follows:\nDefinition 3. For a multilinear function f(D1, D2, . . .Dd) defined on V with degree d, D1, D2, . . . Dd are its d input variables, and f is linear with respect to each variable.\nUnder the assumption of the multilinearity of f , the optimal recovery threshold is provided in Lemma 1 of [15] as:\nLemma 1. [15] Consider an (N,S, L, f) coded computing problem, where N is the number of workers, S,L is the maximum number of stragglers and colluding workers that can be tolerated, respectively. f is a multilinear function, the degree of f is d, and the number of the equally divided input\n14\ndata is K . The optimal recovery threshold for linear coded computing schemes, denoted by H\u2217, is defined as:\nH\u2217 , { d(K + L\u2212 1) + 1, if L > 0, min{d(K \u2212 1) + 1, N \u2212 \u230aN/K\u230b+ 1} if L = 0.\n(49)\nIn order to generalize to the case of polynomial functions, a construction method of multilinear functions is given in Lemma 4 of [15] as follows:\nLemma 4. [15] For a general polynomial function f with degree d, f \u2032 is a d-variable multilinear polynomial function constructed based on f and satisfies:\nf \u2032(D1, D2, . . . , Dd) = \u2211\nT \u2286[1:d]\n[(\u22121)|T |f( \u2211\nk\u2208T\nDk)], (50)\nwhere f \u2032 is linear with respect to each input variable, T is a subset of the set [1 : d] and the degree of f \u2032 also equals d.\nThough Lemma 4 has been proved in [15], here we provide an alternative and more concise proof. In order to prove Lemma 4, we need to prove the order of each variable in f \u2032 is at most 1 due to the multilinearity of f \u2032. Therefore, if the coefficients of higher-order terms in the multilinear polynomial function f \u2032 equal 0, the proof is completed.\nFor any j \u2208 [1 : d], we use h(Dj) to denote a general higher-order term in f \u2032. In h(Dj), the order of Dj is greater than 1, and h(Dj) consists of {Dj, Dj1 , Dj2 , . . . , Djm} through multiplication. Also, the number of subsets T that have {j, j1, j2, . . . , jm} is 2 (d\u2212m\u22121), and the constant coefficients of h(Dj) are the same for these different T in the calculation result of f( \u2211 k\u2208T Dk). We only need to consider the impact of (\u22121)|T |.\nNote that for i \u2208 [0 : d\u2212m\u2212 1], there are\n( i\nd\u2212m\u2212 1\n)\nsubsets T that meet above conditions and include extra i variables except for {j, j1, j2, . . . , jm}. Consequently, any coefficient of h(Dj), denoted by Coeffj , can be obtained as:\nCoeffj = \u2211\n{j,j1,j2,...,jm}\u2286T ,T \u2286[1:d]\n(\u22121)|T |,\n=\nd\u2212m\u22121\u2211\ni=0\n( d\u2212m\u2212 1\ni\n) (\u22121)m+1+i,\n= (\u22121)m+1 \u00b7\nd\u2212m\u22121\u2211\ni=0\n[( d\u2212m\u2212 1\ni\n) 1d\u2212m\u22121\u2212i(\u22121)i ] ,\n= (\u22121)m+1 \u00b7 (1 \u2212 1)d\u2212m\u22121 = 0, (51)\nwhich completes the proof of Lemma 4.\nBased on above two lemmas, Lemma 1 can be extended to the case of general polynomial [15]. Moreover, the actual number of results returned by workers equals (N \u2212S), which must be larger than the recovery threshold. Consequently, to guarantee the decodability for general polynomial coded computing, N \u2212 S \u2265 H\u2217 should hold, and thus the formula (21) is derived."
        },
        {
            "heading": "APPENDIX B PROOF OF PROPOSITION 1",
            "text": "The Lagrangian of P2\u2212 2 is given by:\nL(K, t, z, \u03bb, {\u03b1i}, {\u03b2i})\n=z + \u03bb\n( r\u22121\u2211\ni=0\nKi \u2212K\n) + r\u22121\u2211\ni=0\n\u03b1i(ti \u2212 z)\n+ r\u22121\u2211\ni=0\n\u03b2i [ d(Ki + L\u2212 1) + 1\u2212N(1\u2212 e \u2212\u00b5( ti i+1\u2212a)) ] , (52)\nwhere \u03bb, {\u03b1i} and {\u03b2i} are the Lagrange multipliers associated with (33b), (34b) and (37b), respectively.\nThe partial derivatives of L(K, t, z, \u03bb, {\u03b1i}, {\u03b2i}) can be derived as:\n\u2202L\n\u2202Ki = \u03bb+ d\u03b2i,\n\u2202L \u2202z = 1\u2212\nr\u22121\u2211\ni=0\n\u03b1i, (53a)\n\u2202L \u2202ti = \u03b1i \u2212 \u03b2iN\u00b5 i+ 1 e\u2212\u00b5( ti i+1\u2212a). (53b)\nThe Karush-Kuhn-Tucker (KKT) conditions are written as:\n\u2202L\n\u2202H\u2217i = 0,\n\u2202L \u2202t\u2217i = 0, \u2202L \u2202z\u2217 = 0, (54a)\n\u03b2\u2217i [ d(K\u2217i + L\u2212 1) + 1\u2212N(1\u2212 e \u2212\u00b5( t\u2217 i i+1\u2212a)) ] = 0, (54b)\n\u03b1\u2217i (t \u2217 i \u2212 z \u2217) = 0, (54c) \u03b1\u2217i , \u03b2 \u2217 i \u2265 0,K \u2217 i > 0, \u2200i \u2208 [0 : r \u2212 1]. (54d)\nby solving the KKT conditions, the optimal solution to P2\u22122 is obtained, as shown in Proposition 1."
        },
        {
            "heading": "APPENDIX C PROOF OF PROPOSITION 2",
            "text": "Similarly, as the constraints of P3\u2212 2 are convex, problem P3\u2212 2 is a convex optimization problem. The Lagrangian of P3\u2212 2 is given by:\nL(K, z, \u03bb, {\u03b1i}) = z + \u03bb\n( r\u22121\u2211\ni=0\nKi \u2212K\n)\n+\nr\u22121\u2211\ni=0\n\u03b1i\n[ i+ 1\n\u00b5(N \u2212 d(Ki + L\u2212 1)) + a(i+ 1)\u2212 z\n] . (55)\nFor the convex problems, the optimal solution {K\u2217, z\u2217} must satisfy the KKT conditions. By solving\n\u2202L\n\u2202K\u2217i = \u03bb\u2217 +\n\u03b1\u2217i d(i + 1)\n\u00b5(N \u2212 d(K\u2217i + L\u2212 1)) 2 = 0, (56a)\n\u2202L \u2202z\u2217 = 1\u2212\nr\u22121\u2211\ni=0\n\u03b1\u2217i = 0, \u03b1 \u2217 i \u2265 0, (56b)\n\u03b1\u2217i\n[ i+ 1\n\u00b5(N \u2212 d(K\u2217i + L\u2212 1)) + a(i+ 1)\u2212 z\u2217\n] = 0, (56c)\n(N \u2212 1)/d\u2212 L+ 1 \u2265 K\u2217i > 0, \u2200i \u2208 [0 : r \u2212 1] (56d)\nthe optimal solution to P3\u2212 2 is obtained.\n15"
        }
    ],
    "title": "Adaptive Privacy-Preserving Coded Computing With Hierarchical Task Partitioning",
    "year": 2023
}