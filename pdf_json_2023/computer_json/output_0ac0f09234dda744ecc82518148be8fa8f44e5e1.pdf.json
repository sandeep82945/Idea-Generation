{
    "abstractText": "Studies involving both randomized experiments as well as observational data typically involve time-to-event outcomes such as time-to-failure, death or onset of an adverse condition. Such outcomes are typically subject to censoring due to loss of follow-up and established statistical practice involves comparing treatment efficacy in terms of hazard ratios between the treated and control groups. In this paper we propose a statistical approach to recovering sparse phenogroups (or subtypes) that demonstrate differential treatment effects as compared to the study population. Our approach involves modelling the data as a mixture while enforcing parameter shrinkage through structured sparsity regularization. We propose a novel inference procedure for the proposed model and demonstrate its efficacy in recovering sparse phenotypes across large landmark real world clinical studies in cardiovascular health.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chirag Nagpal"
        },
        {
            "affiliations": [],
            "name": "Vedant Sanil"
        },
        {
            "affiliations": [],
            "name": "Artur Dubrawski"
        },
        {
            "affiliations": [],
            "name": "NAGPAL SANIL DUBRAWSKI"
        }
    ],
    "id": "SP:4d9b3b0114e81d15bfe4596e70e57da6fdd02fda",
    "references": [
        {
            "authors": [
                "Ahmed M Alaa",
                "Mihaela Van Der Schaar"
            ],
            "title": "Bayesian inference of individualized treatment effects using multi-task gaussian processes",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "David A Binder"
            ],
            "title": "Fitting cox\u2019s proportional hazards models from survey",
            "venue": "data. Biometrika,",
            "year": 1992
        },
        {
            "authors": [
                "Norman E Breslow"
            ],
            "title": "Contribution to discussion of paper by dr cox",
            "venue": "J. Roy. Statist. Soc., Ser. B,",
            "year": 1972
        },
        {
            "authors": [
                "Michael Bretthauer",
                "Magnus L\u00f8berg",
                "Paulina Wieszczy",
                "Mette Kalager",
                "Louise Emilsson",
                "Kjetil Garborg",
                "Maciej Rupinski",
                "Evelien Dekker",
                "Manon Spaander",
                "Marek Bugajski"
            ],
            "title": "Effect of colonoscopy screening on risks of colorectal cancer and related death",
            "year": 2022
        },
        {
            "authors": [
                "John B Buse",
                "ACCORD Study Group"
            ],
            "title": "Action to control cardiovascular risk in diabetes (accord) trial: design and methods",
            "venue": "The American journal of cardiology,",
            "year": 2007
        },
        {
            "authors": [
                "Paidamoyo Chapfuwa",
                "Serge Assaad",
                "Shuxi Zeng",
                "Michael J Pencina",
                "Lawrence Carin",
                "Ricardo Henao"
            ],
            "title": "Enabling counterfactual survival analysis with balanced representations",
            "venue": "In Proceedings of the Conference on Health, Inference, and Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Yi-Hau Chen"
            ],
            "title": "Weighted breslow-type and maximum likelihood estimation in semiparametric transformation models",
            "year": 2009
        },
        {
            "authors": [
                "David R Cox"
            ],
            "title": "Regression models and life-tables",
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological),",
            "year": 1972
        },
        {
            "authors": [
                "Jonathan Crabb\u00e9",
                "Alicia Curth",
                "Ioana Bica",
                "Mihaela van der Schaar"
            ],
            "title": "Benchmarking heterogeneous treatment effect models through the lens of interpretability",
            "venue": "arXiv preprint arXiv:2206.08363,",
            "year": 2022
        },
        {
            "authors": [
                "Alicia Curth",
                "Changhee Lee",
                "Mihaela van der Schaar"
            ],
            "title": "Survite: Learning heterogeneous treatment effects from time-to-event data",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Elise Dusseldorp",
                "Iven Mechelen"
            ],
            "title": "Qualitative interaction trees: A tool to identify qualitative treatmentsubgroup interactions",
            "venue": "Statistics in medicine,",
            "year": 2014
        },
        {
            "authors": [
                "Jared C Foster",
                "Jeremy MG Taylor",
                "Stephen J Ruberg"
            ],
            "title": "Subgroup identification from randomized clinical trial data",
            "venue": "Statistics in medicine,",
            "year": 2011
        },
        {
            "authors": [
                "Jerome Friedman",
                "Trevor Hastie",
                "Rob Tibshirani"
            ],
            "title": "glmnet: Lasso and elastic-net regularized generalized linear models",
            "venue": "R package version,",
            "year": 2009
        },
        {
            "authors": [
                "Jerome Friedman",
                "Trevor Hastie",
                "Robert Tibshirani"
            ],
            "title": "A note on the group lasso and a sparse group lasso",
            "venue": "arXiv preprint arXiv:1001.0736,",
            "year": 2010
        },
        {
            "authors": [
                "Curt D Furberg"
            ],
            "title": "Major outcomes in high-risk hypertensive patients randomized to angiotensin-converting enzyme inhibitor or calcium channel blocker vs diuretic: the antihypertensive and lipid-lowering treatment to prevent heart attack trial (allhat)",
            "venue": "Journal of the American Medical Association,",
            "year": 2002
        },
        {
            "authors": [
                "David C Goff Jr.",
                "Donald M Lloyd-Jones",
                "Glen Bennett",
                "Sean Coady",
                "Ralph B D\u2019agostino",
                "Raymond Gibbons",
                "Philip Greenland",
                "Daniel T Lackland",
                "Daniel Levy",
                "Christopher J O\u2019donnell"
            ],
            "title": "acc/aha guideline on the assessment of cardiovascular risk: a report of the american college of cardiology/american heart association task force on practice guidelines",
            "venue": "suppl 2):S49\u2013S73,",
            "year": 2013
        },
        {
            "authors": [
                "William Herrington",
                "Ben Lacey",
                "Paul Sherliker",
                "Jane Armitage",
                "Sarah Lewington"
            ],
            "title": "Epidemiology of atherosclerosis and the potential to reduce the global burden of atherothrombotic disease",
            "venue": "Circulation research,",
            "year": 2016
        },
        {
            "authors": [
                "Fredrik D Johansson",
                "Uri Shalit",
                "Nathan Kallus",
                "David Sontag"
            ],
            "title": "Generalization bounds and representation learning for estimation of potential outcomes and causal effects",
            "venue": "arXiv preprint arXiv:2001.07426,",
            "year": 2020
        },
        {
            "authors": [
                "Victoria Kehl",
                "Kurt Ulm"
            ],
            "title": "Responder identification in clinical trials with censored data",
            "venue": "Computational Statistics & Data Analysis,",
            "year": 2006
        },
        {
            "authors": [
                "Kwonsang Lee",
                "Falco J Bargagli-Stoffi",
                "Francesca Dominici"
            ],
            "title": "Causal rule ensemble: Interpretable inference of heterogeneous treatment effects",
            "venue": "arXiv preprint arXiv:2009.09036,",
            "year": 2020
        },
        {
            "authors": [
                "DY Lin"
            ],
            "title": "On the breslow estimator",
            "venue": "Lifetime data analysis,",
            "year": 2007
        },
        {
            "authors": [
                "Ilya Lipkovich",
                "Alex Dmitrienko",
                "Jonathan Denne",
                "Gregory Enas"
            ],
            "title": "Subgroup identification based on differential effect search (sides) \u2013 a recursive partitioning method for establishing response to treatment in patient subpopulations",
            "venue": "Statistics in medicine, 30:2601\u201321,",
            "year": 2011
        },
        {
            "authors": [
                "Christos Louizos",
                "Uri Shalit",
                "Joris M Mooij",
                "David Sontag",
                "Richard Zemel",
                "Max Welling"
            ],
            "title": "Causal effect inference with deep latent-variable models",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "David J Maron",
                "Judith S Hochman",
                "Sean M O\u2019Brien",
                "Harmony R Reynolds",
                "William E Boden",
                "Gregg W Stone",
                "Sripal Bangalore",
                "John A Spertus",
                "Daniel B Mark",
                "Karen P Alexander"
            ],
            "title": "International study of comparative health effectiveness with medical and invasive approaches (ischemia) trial: rationale and design",
            "venue": "American heart journal,",
            "year": 2018
        },
        {
            "authors": [
                "Marco Morucci",
                "Vittorio Orlandi",
                "Sudeepa Roy",
                "Cynthia Rudin",
                "Alexander Volfovsky"
            ],
            "title": "Adaptive hyperbox matching for interpretable individualized treatment effect estimation",
            "venue": "In Conference on Uncertainty in Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Chirag Nagpal",
                "Dennis Wei",
                "Bhanukiran Vinzamuri",
                "Monica Shekhar",
                "Sara E Berger",
                "Subhro Das",
                "Kush R Varshney"
            ],
            "title": "Interpretable subgroup discovery in treatment effect estimation with application to opioid prescribing guidelines",
            "venue": "In Proceedings of the ACM Conference on Health, Inference, and Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Chirag Nagpal",
                "Steve Yadlowsky",
                "Negar Rostamzadeh",
                "Katherine Heller"
            ],
            "title": "Deep cox mixtures for survival regression",
            "venue": "In Machine Learning for Healthcare Conference,",
            "year": 2021
        },
        {
            "authors": [
                "Chirag Nagpal",
                "Mononito Goswami",
                "Keith Dufendach",
                "Artur Dubrawski"
            ],
            "title": "Counterfactual phenotyping with censored time-to-events",
            "venue": "In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD",
            "year": 2022
        },
        {
            "authors": [
                "Chirag Nagpal",
                "Willa Potosnak",
                "Artur Dubrawski"
            ],
            "title": "auton-survival: an open-source package for regression, counterfactual estimation, evaluation and phenotyping with censored time-to-event data",
            "venue": "In Proceedings of the 7th Machine Learning for Healthcare Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Uri Shalit",
                "Fredrik D Johansson",
                "David Sontag"
            ],
            "title": "Estimating individual treatment effect: generalization bounds and algorithms",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Xiaogang Su",
                "Chih-Ling Tsai",
                "Hansheng Wang",
                "David Nickerson",
                "Bogong Li"
            ],
            "title": "Subgroup analysis via recursive partitioning",
            "venue": "Journal of Machine Learning Research, 10:141\u2013158,",
            "year": 2009
        },
        {
            "authors": [
                "Stefan Wager",
                "Susan Athey"
            ],
            "title": "Estimation and inference of heterogeneous treatment effects using random forests",
            "venue": "Journal of the American Statistical Association,",
            "year": 2018
        },
        {
            "authors": [
                "Tong Wang",
                "Cynthia Rudin"
            ],
            "title": "Causal rule sets for identifying subgroups with enhanced treatment effects",
            "venue": "INFORMS Journal on Computing,",
            "year": 2022
        },
        {
            "authors": [
                "Han Wu",
                "Sarah Tan",
                "Weiwei Li",
                "Mia Garrard",
                "Adam Obeng",
                "Drew Dimmery",
                "Shaun Singh",
                "Hanson Wang",
                "Daniel Jiang",
                "Eytan Bakshy"
            ],
            "title": "Interpretable personalized experimentation",
            "venue": "In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 2022
        },
        {
            "authors": [
                "Yizhe Xu",
                "Nikolaos Ignatiadis",
                "Erik Sverdrup",
                "Scott Fleming",
                "Stefan Wager",
                "Nigam Shah"
            ],
            "title": "Treatment heterogeneity with survival outcomes",
            "venue": "arXiv preprint arXiv:2207.07758,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Data driven decision making across multiple disciplines including healthcare, epidemiology, econometrics and prognostics often involves establishing efficacy of an intervention when outcomes are measured in terms of the time to an adverse event, such as death, failure or onset of a critical condition. Typically the analysis of such studies involves assigning a patient population to two or more different treatment arms often called the \u2018treated\u2019 (or \u2018exposed\u2019) group and the \u2018control\u2019 (or \u2018placebo\u2019) group and observing whether the populations experience an adverse event (for instance death or onset of a disease) over the study period at a rate that is higher (or lower) than for the control group. Efficacy of a treatment is thus established by comparing the relative difference in the rate of event incidence between the two arms called the hazard ratio. However, not all individuals benefit equally from an intervention. Thus, very often potentially beneficial interventions are discarded even though there might exist individuals who benefit, as the population level estimates of treatment efficacy are inconclusive.\nIn this paper we assume that patient responses to an intervention are typically heterogeneous and there exists patient subgroups that are unaffected by (or worse, harmed) by the intervention being assessed. The ability to discover or phenotype these patients is thus clinically useful as it would allow for more precise clinical decision making by identifying individuals that actually benefit from the intervention being assessed.\nTowards this end, we propose Sparse Cox Subgrouping, (SCS) a latent variable approach to model patient subgroups that demonstrate heterogeneous effects to an intervention. As opposed to existing literature in modelling heterogeneous treatment effects with censored time-to-event outcomes our approach involves structured regularization of the covariates that assign individuals to\n\u00a9 C. Nagpal, V. Sanil & A. Dubrawski.\nar X\niv :2\n30 2.\n12 50\n4v 1\n[ st\nat .M\nE ]\n2 4\nFe b\n20 23\nsubgroups leading to parsimonious models resulting in phenogroups that are interpretable. We release a python implementation of the proposed SCS approach as part of the auton-survival package (Nagpal et al., 2022b) for survival analysis:\nhttps://autonlab.github.io/auton-survival/"
        },
        {
            "heading": "2. Related Work",
            "text": "Large studies especially in clinical medicine and epidemiology involve outcomes that are time-toevents such as death, or an adverse clinical condition like stroke or cancer. Treatment efficacy is typically estimated by comparing event rates between the treated and control arms using the Proportional Hazards (Cox, 1972) model and its extensions.\nIdentification of subgroups in such scenarios has been the subject of a large amount of traditional statistical literature. Large number of such approaches involve estimation of the factual and counterfactual outcomes using separate regression models (T-learner) followed by regressing the difference between these estimated potential outcomes. Within this category of approaches, Lipkovich et al. (2011) propose the subgroup identification based on differential effect search (SIDES) algorithm, Su et al. (2009) propose a recursive partitioning method for subgroup discovery, Dusseldorp and Mechelen (2014) propose the qualitative interaction trees (QUINT) algorithm, and Foster et al. (2011) propose the virtual twins (VT) method for subgroup discovery involving decision tree ensembles. We include a parametric version of such an approach as a competing baseline.\nIdentification of heterogeneous treatment effects (HTE) is also of growing interest to the machine learning community with multiple approaches involving deep neural networks with balanced representations (Shalit et al., 2017; Johansson et al., 2020), generative models Louizos et al. (2017) as well as Non-Parametric methods involving random-forests (Wager and Athey, 2018) and Gaussian Processes (Alaa and Van Der Schaar, 2017). There is a growing interest in estimating HTEs from an interpretable and trustworthy standpoint (Lee et al., 2020; Nagpal et al., 2020; Morucci et al., 2020; Wu et al., 2022; Crabbe\u0301 et al., 2022). Wang and Rudin (2022) propose a sampling based approach to discovering interpretable rule sets demonstrating HTEs.\nHowever large part of this work has focused extensively on outcomes that are binary or continuous. The estimation of HTEs in the presence of censored time-to-events has been limited. Xu et al. (2022) explore the problem and describe standard approaches to estimate treatment effect heterogeneity with survival outcomes. They also describe challenges associated with existing risk models when assessing treatment effect heterogeneity in the case of cardiovascular health.\nThere has been some initial attempt to use neural network for causal inference with censored time-to-event outcomes. Curth et al. (2021) propose a discrete time method along with regularization to match the treated and control representations. Chapfuwa et al. (2021)\u2019s approach is related and involves the use of normalizing flows to estimate the potential time-to-event distributions under treatment and control. While our contributions are similar to Nagpal et al. (2022a), in that we assume treatment effect heterogeneity through a latent variable model, our contribution differs in that 1) Our approach is free of the expensive Monte-Carlo sampling procedure and 2) Our generalized EM inference procedure allows us to naturally incorporate structured sparsity regularization, which helps recovers phenogroups that are parsimonious in the features they recover that define subgroups.\nSurvival and time-to-event outcomes occur pre-eminently in areas of cardiovascular health. One such area is reducing combined risk of adverse outcomes from atherosclerotic disease1 (Herrington et al., 2016; Furberg et al., 2002; Group, 2009; Buse et al., 2007) The ability of recovering groups with differential benefits to interventions can thus lead to improved patient care through framing of optimal clinical guidelines."
        },
        {
            "heading": "3. Proposed Model: Sparse Cox Subgrouping",
            "text": "Notation As is standard in survival analysis, we assume that we either observe the true time-toevent or the time of censoring U = min{T,C} indicated by the censoring indicator defined as \u2206 = 1{T < C}. We thus work with a dataset of right censored observations in the form of 4- tuples, D = {(xi, \u03b4i,ui,ai)}ni=1, where ui \u2208 R+ is the time-to-event or censoring as indicated by \u03b4i \u2208 {0, 1}, ai \u2208 {0, 1} is the indicator of treatment assignment, and xi are individual covariates that confound the treatment assignment and the outcome.\nAssumption 1 (Independent Censoring) The time-to-event T and the censoring distribution C are independent conditional on the covariates X and the intervention A.\nModel Consider a maximum likelihood approach to model the data D the set of parameters \u2126. Under Assumption 1 the likelihood of the data D can be given as,\nL(\u2126;D) \u221d |D|\u220f\ni=1\n\u03bb(ui|X = xi, A = ai)\u03b4iS(ui|X = xi, A = ai), (1)\n1. A class of related clinical conditions from increasing deposits of plaque in the arteries, leading to Stroke, Myorcardial Infarction and other Coronary Heart Diseases.\nhere \u03bb(t) = lim \u2206t\u21920\nP(t\u2264T<t+\u2206t|T\u2265t) \u2206t is the hazard and S(t) = P(T > t) is the survival rate.\nAssumption 2 (PH) The distribution of the time-to-event T conditional on the covariates and the treatment assignment obeys proportional hazards.\nFrom Assumption 2 (Proportional Hazards), an individual with covariates (X = x) under intervention (A = a) under a Cox model with parameters \u03b2 and treatment effect \u03c9 is given as\n\u03bb(t|A = a, X = x) = \u03bb0(t) exp ( \u03b2>x+ \u03c9 \u00b7 a ) , (2)\nHere, \u03bb0(\u00b7) is an infinite dimensional parameter known as the base survival rate. In practice in the Cox\u2019s model the base survival rate is a nuisance parameter and is estimated non-parametrically. In order to model the heterogeneity of treatment response. We will now introduce a latent variable Z \u2208 {0, 1,\u22121} that mediates treatment response to the model,\n\u03bb(t|A = a, X = x, Z = k) = \u03bb0(t) exp(\u03b2>x) exp(\u03c9)ka,\nand, P(Z = k|X = x) = exp(\u03b8 > k x)\u2211\nj exp(\u03b8 > j x)\n. (3)\nHere, \u03c9 \u2208 R is the treatment effect, and \u03b8 \u2208 Rk\u00d7d is the set of parameters that mediate assignment to the latent group Z conditioned on the confounding features x. Note that the above choice of parameterization naturally enforces the requirements from the model as in Figure 1. Consider the following scenarios,\nCase 1: The study population consists of two sub-strata ie. Z \u2208 {0,+1}, that are benefit and are unaffected by treatment respectively.\nCase 2: The study population consists of three sub-strata ie. Z \u2208 {0,+1,\u22121}, that benefit, are harmed or unaffected by treatment respectively.\nFollowing from Equations 1 & 3, the complete likelihood of the data D under this model is,\nL(\u2126;D) = |D|\u220f\ni=1\n\u2211\nk\u2208Z\n( \u03bb0(ui)h(x,a,k) )\u03b4i S0(ui) h(x,a,k)P(Z = k|X = xi)\nwhere, lnh(x,a,k) = \u03b2>x+ k \u00b7 a \u00b7w and lnS0(\u00b7) = \u2212\u039b0(\u00b7), (4)\nNote that \u039b0(\u00b7) = \u222b t\n0 \u03bb0(\u00b7) is the infinite dimensional cumulative hazard and is inferred when learning the model. We will notate the set of all learnable parameters as \u2126 = {\u03b8,\u03b2,w,\u039b0}. Shrinkage In retrospective analysis to recover treatment effect heterogeneity a natural requirement is parsimony of the recovered subgroups in terms of the covariates to promote model interpretability. Such parsimony can be naturally enforced through appropriate shrinkage on the coefficients that promote sparsity. We want to recover phenogroups that are \u2018sparse\u2019 in \u03b8. We enforce sparsity in the parameters of the latent Z gating function via a group `1 (Lasso) penalty. The final loss function to be optimized including the group sparsity regularization term is,\nL(\u2126;D)+ \u00b7 R(\u03b8) where,R(\u03b8) = \u2211\nd\n\u221a\u2211\nk\u2208Z\n( \u03b8kd )2\nand > 0 is the strength of the shrinkage parameter. (5)\nIdentifiability Further, to ensure identifiability we restrict the gating parameters for the (Z = 0) to be 0. Thus \u03b81 = 0.\nInference We will present a variation of the Expectation Maximization algorithm to infer the parameters in Equation 3. Our approach differs from Nagpal et al. (2022a, 2021) in that it does not require storchastic Monte-Carlo sampling. Further, our generalized EM inference allows for incorporation of the structured sparsity in the M-Step.\nA Semi-Parametric Q(\u00b7) Note that the likelihood in Equation 3 is semi-parametric and consists of parametric components and the infinite dimensional base hazard \u039b(\u00b7). We define the Q(\u00b7) as:\nQ(\u2126;D) = n\u2211\ni=1\n\u2211 k\u2208Z \u03b3ki ( lnp\u03b8(Z = k|X = xi) + lnpw,\u03b2,\u039b(T |Z = k,X = xi) ) +R(\u03b8)\nThe E-Step Requires computation of the posteriors counts \u03b3 := p(Z = k|T,X = x, A = a).\nResult 1 (Posterior Counts) The posterior counts \u03b3 for the latent Z are estimated as,\n\u03b3k = P\u0302(Z = k|X = x, A = a,u)\n= P(u|Z = k, X = x, A = a)P(Z = k|X = x)\u2211 k P(u|Z = k, X = x, A = a)P(Z = k|X = x) = h(x,a,k)\u03b4iS\u03020(u)\nh(x,a,k) exp(\u03b8>k x)\u2211 j\u2208Z h(x,a, j) \u03b4iS\u03020(u)h(x,a,j) exp(\u03b8>j x) . (6)\nFor a full discussion on derivation of the Q(\u00b7) and the posterior counts please refer to Appendix B\nThe M-Step Involves maximizing the Q(\u00b7) function. Rewriting the Q(\u00b7) as a sum of two terms,\nQ(\u2126) = n\u2211\ni=1\n\u2211 k\u2208Z \u03b3ki lnpw,\u03b2,\u039b0(T |Z = k,X = xi, A = ai) \ufe38 \ufe37\ufe37 \ufe38 A(w,\u03b2,\u039b0) + n\u2211 i=1 \u2211 k\u2208Z \u03b3ki lnp\u03b8(Z = k|X = xi) +R(\u03b8) \ufe38 \ufe37\ufe37 \ufe38 B(\u03b8)\nResult 2 (Weighted Cox model) The term A can be rewritten as a weighted Cox model and thus optimized using the corresponding \u2018partial likelihood\u2019,\nUpdates for {\u03b2,\u03c9}: The partial-likelihood, PL(\u00b7) under sampling weights (Binder, 1992) is\nPL(\u2126;D) = n\u2211\ni=1,\u03b4i=1\n\u2211 k\u2208Z \u03b3ki ( lnhk(xi,ai,k)\u2212 ln\n\u2211\nj\u2208RiskSet(ui)\n\u2211 k\u2208Z \u03b3kj hk(xj ,aj ,k)\n)] (7)\nHere RiskSet(\u00b7) is the \u2018risk set\u2019 or the set of all individuals who haven\u2019t experienced the event till the corresponding time, i.e. RiskSet(t) := {i : ui > t}. PL(\u00b7) is convex in {\u03b2,\u03c9} and we update these with a gradient step.\nUpdates for \u039b0: The base hazard \u039b0 are updated using a weighted Breslow\u2019s estimate (Breslow, 1972; Lin, 2007) assuming the posterior counts \u03b3 to be sampling weights (Chen, 2009),\n\u039b\u03020(t) + =\nn\u2211\ni=1\n\u2211 k\u2208Z 1{ui < t} \u03b3ki \u00b7 \u03b4i\u2211 j\u2208RiskSet(ui) \u2211 k\u2208Z \u03b3kj hk(xj ,aj ,k) (8)\nTerm B is a function of the gating parameters \u03b8 that determine the latent assignment Z along with sparsity regularization. We update B using a Proximal Gradient update as is the case with Iterative Soft Thresholding (ISTA) for group sparse `1 regression.\nUpdates for \u03b8: The proximal update for \u03b8 including the group regularization (Friedman et al., 2010) termR(\u00b7) is,\n\u03b8\u0302+ = prox\u03b7\n( \u03b8 \u2212 d\nd\u03b8 B(\u03b8)\n) , where prox\u03b7 (y) = y\n||y||2 max{0, ||y||2 \u2212 \u03b7 }. (9)\nAll together the inference procedure is described in Algorithm 1.\nAlgorithm 1: Parameter Learning for SCS with a Generalized EM Input : Training set, D = {(xi, ui, ai, \u03b4i)ni=1}; maximum EM iterations, B, step size \u03b7\nwhile <not converged> do for b \u2208 {1, 2, ..., B} do\nE-STEP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\u03b3ki = h(x,a,k)\u03b4i S\u03020(u)h(x,a,k) exp(\u03b8>k x)\u2211 j\u2208Z h(x,a,j) \u03b4i S\u03020(u)h(x,a,j) exp(\u03b8>j x) . Compute posterior counts (Equation 6).\nM-STEP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\u03b2\u0302+ \u2190 \u03b2\u0302 \u2212 \u03b7 \u00b7 \u2207\u03b2PL(\u03b2,w;D)\nw\u0302+ \u2190 w\u0302 \u2212 \u03b7 \u00b7 \u2207wPL(\u03b2,w;D) . Gradient descent update.\n\u039b\u03020(t) + \u2190\u2211ni=1 \u2211 k\u2208Z 1{ui < t} \u03b3ki \u00b7\u03b4i\u2211 j\u2208RiskSet(ui) \u2211 k\u2208Z \u03b3kj hk(xj ,aj ,k) .Breslow (1972)\u2019s estimator. \u03b8\u0302+ \u2190 \u03b8\u0302 \u2212 \u03b7 \u00b7 \u2207\u03b8B(\u03b8) . Update \u03b8 with gradient of Q\u0302.\n\u03b8\u0302+ \u2190 prox \u03b7(\u03b8\u0302) . Proximal update.\nend end\nReturn: learnt parameters \u2126;"
        },
        {
            "heading": "4. Experiments",
            "text": "In this section we describe the experiments conducted to benchmark the performance of SCS against alternative models for heterogenous treatment effect estimation on multiple studies including a synthetic dataset and multiple large landmark clinical trials for cardiovascular diseases."
        },
        {
            "heading": "4.1. Simulation",
            "text": "In this section we first describe the performance of the proposed Sparse Cox Subgrouping approach on a synthetic dataset designed to demonstrate heterogeneous treatment effects. We randomly assign individuals to the treated or control group. The latent variable Z is drawn from a uniform categorical distribution that determines the subgroup,\nA \u223c Bernoulli(1/2), Z \u223c Categorical(1/3) Conditioned on Z we sample X1:2 \u223c Normal(\u00b5z,\u03c3z) as in Figure 2 that determine the conditional Hazard Ratios HR(k), and randomly sample noisy covariates X3:6 \u223c Uniform(\u22121, 1) . The true\ntime-to-event T and censoring times C are then sampled as,\nT |(X = x, A = a, Z = k) \u223c Gompertz(\u03b2 = 1, \u03b7 = 0.25 \u00b7 HR(k)a), C|T \u223c Uniform(0, T ) Finally we sample the censoring indicator \u2206 \u223c Bernoulli(0.8) and set the observed time-to-event,\nU = T if \u2206 = 1, else we set U = C.\nFigure 2 presents the ROC curves for SCS\u2019s ability to identify the groups with enhanced and diminished treatment effects respectively. In Figure 3 we present Kaplan-Meier estimators of the Time-to-Event distributions conditioned on the predicted Z by SCS. Clearly, SCS is able to identify the phenogroups corresponding to differential benefits."
        },
        {
            "heading": "4.2. Recovering subgroups demonstrating Heterogeneous Treatment Effects from Landmark studies of Cardiovascular Health",
            "text": "Antihypertensive and Lipid-Lowering Treatment to Prevent Heart Attack (Furberg et al., 2002) The ALLHAT study was a large randomized experiment conducted to assess the efficacy of multiple classes of blood pressure lowering medicines for patients with hypertension in reducing risk of adverse cardiovascular conditions. We considered a subset of patients from the original ALLHAT study who were randomized to receive either Amlodipine (a calcium channel blocker) or Lisinopril (an Angiotensin-converting enzyme inhibitor). Overall, Amlodipine was found to be more efficacious than Lisinopril in reducing combined risk of cardio-vascular disease.\nBypass Angioplasty Revascularization Investigation in Type II Diabetes (Group, 2009)\nDiabetic patients have been traditionally known to be at higher risk of cardiovascular disease however appropriate intervention for diabetics with ischemic heart disease between surgical coronary revascularization or management with medical therapy is widely debated. The BARI2D was a large landmark experiment conducted to assess efficacy between these two possible medical interventions. Overall BARI2D was inconclusive in establishing the appropriate therapy between Coronary Revascularization or medical management for patients with Type-II Diabetes.\nFigure 4 presents the event-free survival rates as well as the summary statistics for the studies. In our experiments, we included a large set of confounders collected at baseline visit of the patients which we utilize to train the proposed model. A full list of these features are in Appendix A."
        },
        {
            "heading": "4.3. Baselines",
            "text": "Cox PH with `1 Regularized Treatment Interaction (COX-INT)\nWe include treatment effect heterogeneity via interaction terms that model the time-to-event distribution using a proportional hazards model as in Kehl and Ulm (2006). Thus,\n\u03bb(t|X = x, A = a) = \u03bb0(t) exp ( \u03b2>x+ a \u00b7 \u03b8>x ) (10)\nThe interaction effects \u03b8 are regularized with a lasso penalty in order to recover a sparse phenotyping rule defined as G(x) = \u03b8>x.\nBinary Classifier with `1 Regularized Treatment Interaction (BIN-INT)\nInstead of modelling the time-to-event distribution we directly model the thresholded survival outcomes Y = 1{T < t} at a five year time horizon using a log-linear parameterization with a logit link function. As compared to COX-INT, this model ignores the data-points that were right-censored prior to the thresholded time-to-event, however it is not sensitive to the strong assumption of Proportional Hazards.\nE[T > t|X = x, A = a] = \u03c3(\u03b2>x+ \u03b20 + a \u00b7 \u03b8>x), and, \u03c3(\u00b7) is the logistic link function. (11)\nCox PH T-Learner with `1 Regularized Logistic Regression (COX-TLR)\nWe train two separate Cox Regression models on the treated and control arms (T-Learner) to estimate the potential outcomes under treatment (A = 1) and control (A = 0). Motivated from the \u2018Virtual Twins\u2019 approach as in Foster et al. (2011), a logistic regression with an `1 penalty is trained to estimate if the risk of the potential outcome under treatment is higher than under control. This logistic regression is then employed as the phenotyping function G(\u00b7) and is given as,\nG(x) = E[1{f1(x, t) > f0(x, t)}|X = x] where, fa(x, t) = P(T > t|do(A = a), X = x). (12)\nThe above models involving sparse `1 regularization were trained with the glmnet (Friedman et al., 2009) package in R.\nThe ACC/AHA Long term Atheroscleoratic Cardiovascular Risk Estimate 2\nThe American College of Cardiology and the American Heart Association model for estimation of risk of Atheroscleratic disease risk (Goff Jr et al., 2014) involves pooling data from multiple observational cohorts of patients followed by modelling the 10-year risk of an adverse cardiovascular condition including death from coronary heart disease, Non-Fatal Myocardial Infarction or Non-fatal Stroke. While the risk model was originally developed to assess factual risk in the observational sense, in practice it is also employed to assess risk when making counterfactual decisions."
        },
        {
            "heading": "4.4. Results and Discussion",
            "text": "Protocol We compare the performance of SCS and the corresponding competing methods in recovery of subgroups with enhanced (or diminished treatment effects). For each of these studies we stratify the study population into equal sized sets for training and validation while persevering the proportion of individuals that were assigned to treatment and experienced the outcome in the follow up period. The models were trained on the training set and validated on the held-out test set. For each of the methods we experiment with models that do not enforce any sparsity ( = 0) as well as tune the level of sparsity to recover phenotyping functions that involve 5 and 10 features. The subgroup size are varied by controlling the threshold at which the individual is assigned to a group. Finally, the treatment effect is compared in terms of Hazard Ratios, Risk Differences as well as Restricted Mean Survival Time over a 5 Year event period.\nResults We present the results of SCS versus the baselines in terms of Hazard Ratios on the ALLHAT and BARI2D datasets in Figures 5 and 6. In the case of ALLHAT, SCS consistently recovered phenogroups with more pronounced (or diminished) treatment effects. On external validation on the heldout dataset, we found a subgroup of patients that had similar outcomes whether assigned to Lisinopril or Amlodipine, whereas the other subgroup clearly identified patients that were harmed with Lisinopril. The group harmed with Lisinopril had higher Diastolic BP. On the other hand, patients with Lower kidney function did not seem to benefit from Amlodipine.\nIn the case of BARI2D, SCS recovered phenogroups that were both harmed as well as benefitted from just medical therapy without revascularization. The patients who were harmed from Medical therapy were typically older, on the other hand the patients who benefitted primarily included patients who were otherwise assigned to receive PCI instead of CABG revascularization, suggesting PCI to be harmful for diabetic patients.\nTables 3 and 4 present the features that were selected by the proposed model for the studies. Additionally, we also report tabulated results involving metrics like risk difference and restricted mean survival time in the Appendix C."
        },
        {
            "heading": "5. Concluding Remarks",
            "text": "We presented Sparse Cox Subgrouping (SCS) a latent variable approach to recover subgroups of patients that respond differentially to an intervention in the presence of censored time-to-event outcomes. As compared to alternative approaches to learning parsimonious hypotheses in such settings,\n2. https://tools.acc.org/ascvd-risk-estimator-plus/\nour proposed model recovered hypotheses with more pronounced treatment effects which we validated on multiple studies for cardiovascular health.\nWhile powerful in its ability to recover parsimonious subgroups there exists limitations in SCS in its current form. The model is sensitive to proportional hazards and may be ill-specified when the proportional hazards assumptions are violated as is evident in many real world clinical studies (Maron et al., 2018; Bretthauer et al., 2022). Another limitation is that SCS in its current form looks at only a single endpoint (typically death, or a composite of multiple adverse outcome). In practice however real world studies typically involve multiple end-points. We envision that extensions of SCS would allow patient subgrouping across multiple endpoints, leading to discovery of actionable sub-populations that similarly benefit from the intervention under assessment."
        },
        {
            "heading": "Appendix A. Additional Details on the ALLHAT and BARI 2D Case Studies",
            "text": "Tables 1 and 2 represent additional confounding variables found in the ALLHAT and BARI2D trials respectively.\nName Description ETHNIC Ethnicity\nSEX Sex of Participant ESTROGEN Estrogen supplementation BLMEDS Antihypertensive treatment MISTROKE History of Stroke HXCABG History of coronary artery bypass STDEPR Prior ST depression/T-wave inversion OASCVD Other atherosclerotic cardiovascular disease DIABETES Prior history of Diabetes HDLLT35 HDL cholesterol <35mg/dl; 2x in past 5 years LVHECG LVH by ECG in past 2 years WALL25 LVH by ECG in past 2 years\nLCHD History of CHD at baseline CURSMOKE Current smoking status. ASPIRIN Aspirin use\nLLT Lipid-lowering trial AGE Age upon entry\nBLWGT Weight upon entry BLHGT Height upon entry BLBMI Body Mass Index upon entry\nBV2SBP Baseline SBP BV2DBP Baseline DBP APOTAS Baseline serum potassium BLGFR Baseline est glomerular filtration rate ACHOL Total Cholesterol AHDL Baseline HDL Cholesterol AFGLUC Baseline fasting serum glucose\nTable 1: List of confounding variables used for experiments involving the ALLHAT dataset.\nName Description hxmi History of MI age Age upon entry\ndbp stand Standing diastolic BP sbp stand Standing systolic BP\nsex Sex asp Aspirin use\nsmkcat Cigarette smoking category betab Beta blocker use\nccb Calcium blocker use hxhtn History of hypertension requiring tx\ninsulin Insulin use weight Weight (kg) upon entry\nbmi BMI upon entry qabn Abnormal Q-Wave trig Triglycerides (mg/dl) upon entry\ndmdur Duration of diabetes mellitus ablvef Left ventricular ejection fraction <50%\nrace Race priorrev Prior revascularization\nhxcva Cerebrovascular accident screat Serum creatinine (mg/dl)\nhmg Statin hxhypo History of hypoglycemic episode hba1c Hemoglobin A1c(%)\npriorstent Prior stent spotass Serum Potassium(mEq/L)\nhispanic Hispanic ethnicity tchol Total Cholesterol\nhdl HDL Cholesterol insul circ Circulating insulin (IU/ml)\ntzd Thiazolidinedione ldl LDL Cholesterol tabn Abnormal T-waves nsgn Nonsublingual nitrate sulf Sulfonylurea hxchf Histoty of congestive heart failure req tx\narb Angiotensin receptor blocker acr Urine albumin/creatinine ratio mg/g diur Diuretic apa Anti-platelet\nhxchl Hypercholesterolemia req tx acei ACE inhibitor\nabilow Low ABI (<= 0.9) biguanide Biguanide\nstabn Abnormal ST depression\nTable 2: List of confounding variables used for experiments involving the BARI2D dataset.\nBARI2D"
        },
        {
            "heading": "Appendix B. Derivation of the Inference Algorithm",
            "text": "Censored Instances: Note that in the case of the censored instances we will condition on the thresholded survival (T > u). The the posterior counts thus reduce to:\n\u03b3k = P(Z = k|X = x, A = a, T > u)\n= P(T > t|Z = k, X = x, A = a)p(Z = k|X = x)\u2211 k P(T > t|Z = k, X = x, A = a)P(Z = k|X = x)\n(13)\nHere, P(T > t|Z = k, X = x, A = a) = exp ( \u2212\u039b(t) )h(x,a,k)\nUncensored Instances The posteriors are \u03b3k = p\u03b8(Z = k|X = x, T = u),\nPosteriors for the uncensored data are more involved and involve the base hazard \u03bb0(\u00b7). Posteriors for uncensored data are independent of the base hazard function, \u03bb0(\u00b7) as,\n\u03b3k = \u03bb0(u)hk(x,a)S0(ui)\nhk(x,a)\n\u2211 k \u03bb0(u)hk(x,a)S0(u)hk(x,a) =\nhk(x,a)S0(ui) hk(x,a)\n\u2211 k hk(x,a)S0(ui)hk(x,a)\nCombining Equations 13 and 14 we arrive at the following estimate for the posterior counts\n\u03b3k = P\u0302(Z = k|X = x, A = a,u)\n= P(u|Z = k, X = x, A = a)P(Z = k|X = x)\u2211 k P(u|Z = k, X = x, A = a)P(Z = k|X = x) = h(x,a,k)\u03b4iS\u03020(u)\nh(x,a,k) exp(\u03b8>k x)\u2211 j\u2208Z h(x,a, j) \u03b4iS\u03020(u)h(x,a,j) exp(\u03b8>j x) . (14)"
        },
        {
            "heading": "Appendix C. Additional Results",
            "text": "Figures 7, 8, 9 present tabulated metrics on ALLHAT with Hazard Ratio, Risk Difference and Restricted Mean Survival Time respectively. Figures 10, 11, 12 present tabulated metrics BARI2D with Hazard Ratio, Risk Difference and Restricted Mean Survival Time metrics respectively."
        }
    ],
    "title": "Recovering Sparse and Interpretable Subgroups with Heterogeneous Treatment Effects with Censored Time-to-Event Outcomes",
    "year": 2023
}