{
    "abstractText": "Software plays a central role in our life nowadays. We use it almost anywhere, at any time, and for everything: to browse the Internet, to check our emails, and even to access critical services such as health monitoring and banking. Hence, its reliability and general quality is critical. As software increases in complexity, developers spend more time fixing bugs or making code work rather than designing or writing new code. Thus, improving software understandability and maintainability would translate into an economic relief over the total cost of a project. Different cognitive complexity measures have been proposed to quantify the understandability of a piece of code and, therefore, its maintainability. However, the cognitive complexity metric provided by SonarSource and integrated in SonarCloud and SonarQube is quickly spreading in the software industry due to the popularity of these well-known static code tools for evaluating software quality. Despite SonarQube suggests to keep method\u2019s cognitive complexity no greater than 15, reducing method\u2019s complexity is challenging for a human programmer and there are no approaches to assist developers on this task. We model the cognitive complexity reduction of a method as an optimization problem where the search space contains all sequences of Extract Method refactoring opportunities. We then propose a novel approach that searches for feasible code extractions allowing developers to apply them, all in an automated way. This will allow software developers to make informed decisions while reducing the complexity of their code. We evaluated our approach over 10 open-source software projects and was able to fix 78% of the 1,050 existing cognitive complexity issues reported by SonarQube. We finally discuss the limitations of the proposed approach and provide interesting findings and guidelines for developers. INDEX TERMS Software quality, software maintenance, optimization, cognitive complexity.",
    "authors": [
        {
            "affiliations": [],
            "name": "RUB\u00c9N SABORIDO"
        }
    ],
    "id": "SP:ce5278324ef7da063cd90ae38fbbed1728b49407",
    "references": [
        {
            "authors": [
                "D. Coleman",
                "D. Ash",
                "B. Lowther",
                "P. Oman"
            ],
            "title": "Using metrics to evaluate software system maintainability",
            "venue": "Computer, vol. 27, no. 8, pp. 44\u201349, Aug. 1994.",
            "year": 1994
        },
        {
            "authors": [
                "D.S. Kushwaha",
                "A.K. Misra"
            ],
            "title": "Cognitive complexity metrics and its impact on software reliability based on cognitive software development model",
            "venue": "ACM SIGSOFT Softw. Eng. Notes, vol. 31, no. 2, pp. 1\u20136, Mar. 2006, doi: 10.1145/1118537.1118544.",
            "year": 2006
        },
        {
            "authors": [
                "S.T. Rabani",
                "K. Maheswaran"
            ],
            "title": "Software cognitive complexity metrics for OO design: A survey",
            "venue": "Int. J. Sci. Res. Sci., Eng. Technol., vol. 3, pp. 692\u2013698, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "T. Jayalath",
                "S. Thelijjagoda"
            ],
            "title": "A modified cognitive complexity metric to improve the readability of object-oriented software",
            "venue": "Proc. Int. Res. Conf. Smart Comput. Syst. Eng. (SCSE), Sep. 2020, pp. 37\u201344.",
            "year": 2020
        },
        {
            "authors": [
                "T. McCabe"
            ],
            "title": "A complexity measure",
            "venue": "IEEE Trans. Softw. Eng., vol. SE-2, no. 4, pp. 308\u2013320, Dec. 1976.",
            "year": 1976
        },
        {
            "authors": [
                "G.A. Campbell"
            ],
            "title": "Cognitive complexity: An overview and evaluation",
            "venue": "Proc. Int. Conf. Tech. Debt, May 2018, pp. 57\u201358, doi: 10.1145/3194164.3194186.",
            "year": 2018
        },
        {
            "authors": [
                "M. Mu\u00f1oz Bar\u00f3n",
                "M. Wyrich",
                "S. Wagner"
            ],
            "title": "An empirical validation of cognitive complexity as a measure of source code understandability",
            "venue": "Proc. 14th ACM/IEEE Int. Symp. Empirical Softw. Eng. Meas. (ESEM), Oct. 2020, pp. 1\u201312, doi: 10.1145/3382494.3410636.",
            "year": 2020
        },
        {
            "authors": [
                "D. Silva",
                "N. Tsantalis",
                "M.T. Valente"
            ],
            "title": "Why we refactor? Confessions of GitHub contributors",
            "venue": "Proc. 24th ACM SIGSOFT Int. Symp. Found. Softw. Eng., Nov. 2016, pp. 858\u2013870, doi: 10.1145/2950290.2950305.",
            "year": 2016
        },
        {
            "authors": [
                "N. Tsantalis",
                "A. Chatzigeorgiou"
            ],
            "title": "Identification of extract method refactoring opportunities for the decomposition of methods",
            "venue": "J. Syst. Softw., vol. 84, no. 10, pp. 1757\u20131782, Oct. 2011, doi: 10.1016/ j.jss.2011.05.016.",
            "year": 2011
        },
        {
            "authors": [
                "N. Tsantalis",
                "V. Guana",
                "E. Stroulia",
                "A. Hindle"
            ],
            "title": "A multidimensional empirical study on refactoring activity",
            "venue": "Proc. Conf. Center Adv. Stud. Collaborative Res. (CASCON). Armonk, NY, USA: IBM Corp., 2013, pp. 132\u2013146.",
            "year": 2013
        },
        {
            "authors": [
                "A. Hora",
                "R. Robbes"
            ],
            "title": "Characteristics of method extractions in java: A large scale empirical study",
            "venue": "Empirical Softw. Eng., vol. 25, no. 3, pp. 1798\u20131833, May 2020, doi: 10.1007/s10664-020-09809-8.",
            "year": 1833
        },
        {
            "authors": [
                "R. Haas",
                "B. Hummel"
            ],
            "title": "Deriving extract method refactoring suggestions for long methods",
            "venue": "Software Quality. The Future of Systemsand Software Development, D.Winkler, S. Biffl, and J. Bergsmann, Eds. Cham, Switzerland: Springer, 2016, pp. 144\u2013155.",
            "year": 2016
        },
        {
            "authors": [
                "J. Hubert"
            ],
            "title": "Implementation of an automatic extract method refactoring",
            "venue": "M.S. thesis, Fac. Comput. Sci., Elect. Eng. Inf. Technol., Univ. Stuttgart, Stuttgart, Germany, Germany, Apr. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M.R. Woodward",
                "M.A. Hennell",
                "D. Hedley"
            ],
            "title": "A measure of control flow complexity in program text",
            "venue": "IEEE Trans. Softw. Eng., vol. SE-5, no. 1, pp. 45\u201350, Jan. 1979.",
            "year": 1979
        },
        {
            "authors": [
                "C.R. Douce",
                "P.J. Layzell",
                "J. Buckley"
            ],
            "title": "Spatial measures of software complexity",
            "venue": "Proc. PPIG. Delft, The Netherlands: Psychology of Programming Interest Group, 1999, p. 6.",
            "year": 1999
        },
        {
            "authors": [
                "J. Shao",
                "Y. Wang"
            ],
            "title": "A new measure of software complexity based on cognitive weights",
            "venue": "Proc. CCECE Can. Conf. Electr. Comput. Eng. Toward Caring Humane Technol., vol. 2, May 2003, pp. 1333\u20131338.",
            "year": 2003
        },
        {
            "authors": [
                "S. Misra"
            ],
            "title": "Modified cognitive complexity measure",
            "venue": "Computer and Information Sciences ISCIS 2006, A. Levi, E. Sava\u015f, H. Yenig\u00fcn, S. Balc\u0131soy, and Y. Sayg\u0131n, Eds. Berlin, Germany: Springer, 2006, pp. 1050\u20131059.",
            "year": 2006
        },
        {
            "authors": [
                "S. Misra"
            ],
            "title": "A complexity measure based on cognitive weights",
            "venue": "Int. J. Theor. Appl. Comput. Sci., vol. 1, no. 1, pp. 1\u201310, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "S. Misra",
                "A.K. Misra"
            ],
            "title": "Evaluation and comparison of cognitive complexity measure",
            "venue": "ACM SIGSOFT Softw. Eng. Notes, vol. 32, no. 2, pp. 1\u20135, Mar. 2007, doi: 10.1145/1234741.1234761.",
            "year": 2007
        },
        {
            "authors": [
                "S. Misra"
            ],
            "title": "Cognitive program complexity measure",
            "venue": "Proc. 6th IEEE Int. Conf. Cognit. Informat., Aug. 2007, pp. 120\u2013125.",
            "year": 2007
        },
        {
            "authors": [
                "S. Misra"
            ],
            "title": "An object oriented complexity metric based on cognitive weights",
            "venue": "Proc. 6th IEEE Int. Conf. Cognit. Informat., Aug. 2007, pp. 134\u2013139.",
            "year": 2007
        },
        {
            "authors": [
                "S. Misra",
                "I. Akman"
            ],
            "title": "A model for measuring cognitive complexity of software",
            "venue": "Knowledge-Based Intelligent Information and Engineering Systems, I. Lovrek, R. J. Howlett, and L. C. Jain, Eds. Berlin, Germany: Springer, 2008, pp. 879\u2013886.",
            "year": 2008
        },
        {
            "authors": [
                "S. Misra",
                "I. Akman"
            ],
            "title": "A new complexity metric based on cognitive informatics",
            "venue": "Rough Sets and Knowledge Technology, G. Wang, T. Li, J. W. Grzymala-Busse, D. Miao, A. Skowron, and Y. Yao, Eds. Berlin, Germany: Springer, 2008, pp. 620\u2013627.",
            "year": 2008
        },
        {
            "authors": [
                "S. Misra",
                "I. Akman",
                "M. Koyuncu"
            ],
            "title": "An inheritance complexity metric for object-oriented code: A cognitive approach",
            "venue": "Sadhana, vol. 36, no. 3, p. 317, Jul. 2011, doi: 10.1007/s12046-011-0028-2.",
            "year": 2011
        },
        {
            "authors": [
                "S. Misra",
                "M. Koyuncu",
                "M. Crasso",
                "C. Mateos",
                "A. Zunino"
            ],
            "title": "A suite of cognitive complexity metrics",
            "venue": "Computational Science and its Applications ICCSA 2012, B. Murgante, O. Gervasi, S. Misra, N. Nedjah, A. M. A. C. Rocha, D. Taniar, and B. O. Apduhan, Eds. Berlin, Germany: Springer, 2012, pp. 234\u2013247.",
            "year": 2012
        },
        {
            "authors": [
                "S. Misra",
                "I. Akman",
                "R. Colomo-Palacios"
            ],
            "title": "Framework for evaluation and validation of software complexity measures",
            "venue": "IET Softw., vol. 6, no. 4, pp. 323\u2013334, Aug. 2012. [Online]. Available: https://digitallibrary.theiet.org/content/journals/10.1049/iet-sen.201%1.0206",
            "year": 2012
        },
        {
            "authors": [
                "D.R. Wijendra",
                "K.P. Hewagamage"
            ],
            "title": "Automated tool for the calculation of cognitive complexity of a software",
            "venue": "Proc. 2nd Int. Conf. Sci. Inf. Technol. (ICSITech), Oct. 2016, pp. 163\u2013168.",
            "year": 2016
        },
        {
            "authors": [
                "M. Crasso",
                "C. Mateos",
                "A. Zunino",
                "S. Misra",
                "P. Polvor\u00edn"
            ],
            "title": "Assessing cognitive complexity in java-based object-oriented systems: Metrics and tool support",
            "venue": "Comput. Inform., vol. 35, no. 3, pp. 497\u2013527, 2016. [Online]. Available: http://www.cai.sk/ojs/index.php/cai/article/view/1747",
            "year": 2016
        },
        {
            "authors": [
                "S. Misra",
                "A. Adewumi",
                "R. Damasevicius",
                "R. Maskeliunas"
            ],
            "title": "Analysis of existing software cognitive complexity measures",
            "venue": "Int. J. Secure Softw. Eng., vol. 8, no. 4, pp. 51\u201371, Oct. 2017, doi: 10.4018/IJSSE.2017100103.",
            "year": 2017
        },
        {
            "authors": [
                "S. Misra",
                "A. Adewumi",
                "L. Fernandez-Sanz",
                "R. Damasevicius"
            ],
            "title": "A suite of object oriented cognitive complexity metrics",
            "venue": "IEEE Access, vol. 6, pp. 8782\u20138796, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J.C. Cherniavsky",
                "C.H. Smith"
            ],
            "title": "On Weyuker\u2019s axioms for software complexity measures",
            "venue": "IEEE Trans. Softw. Eng., vol. 17, no. 6, pp. 636\u2013638, Jun. 1991, doi: 10.1109/32.87287. VOLUME 10, 2022 11655 R. Saborido et al.: Automatizing Software Cognitive Complexity Reduction",
            "year": 1991
        },
        {
            "authors": [
                "L. Kaur",
                "A. Mishra"
            ],
            "title": "Cognitive complexity as a quantifier of version to version java-based source code change: An empirical probe",
            "venue": "Inf. Softw. Technol., vol. 106, pp. 31\u201348, Feb. 2019. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0950584918301903",
            "year": 2019
        },
        {
            "authors": [
                "B.S. Alqadi"
            ],
            "title": "The relationship between cognitive complexity and the probability of defects",
            "venue": "Proc. IEEE Int. Conf. Softw. Maintenance Evol. (ICSME), Sep. 2019, pp. 600\u2013604.",
            "year": 2019
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "U. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Proc. 31st Int. Conf. Neural Inf. Process. Syst. (NIPS). Red Hook, NY, USA: Curran Associates Inc., 2017, pp. 6000\u20136010.",
            "year": 2017
        },
        {
            "authors": [
                "R.K. Yin"
            ],
            "title": "Case Study Research: Design Methods, 3rd ed",
            "year": 2002
        },
        {
            "authors": [
                "E.J. Weyuker"
            ],
            "title": "Evaluating software complexity measures",
            "venue": "IEEE Trans. Softw. Eng., vol. SE-14, no. 9, pp. 1357\u20131365, Sep. 1988. [Online]. Available: http://ieeexplore.ieee.org/document/6178/",
            "year": 1988
        },
        {
            "authors": [
                "C. Kaner",
                "W.P. Bond"
            ],
            "title": "Software engineering metrics: What do they measure and how do we know",
            "venue": "Proc. Int. Softw. Metrics Symp., Washington, DC, USA: IEEE Computer Society Press, 2004, pp. 1\u201312.",
            "year": 2004
        },
        {
            "authors": [
                "L.C. Briand",
                "S. Morasca",
                "V.R. Basili"
            ],
            "title": "Property-based software engineering measurement",
            "venue": "IEEE Trans. Softw. Eng., vol. 22, no. 1, pp. 68\u201386, Jan. 1996.",
            "year": 1996
        }
    ],
    "sections": [
        {
            "text": "INDEX TERMS Software quality, software maintenance, optimization, cognitive complexity.\nI. INTRODUCTION Most of the cost during software development is due to its maintenance [1], [2]. In complex software systems the time spent for validation could even be longer than the development time. Previous studies showed that debugging errors could be up to the 50% of the total cost of software projects [3]. This is due to the fact that software maintenance tasks are usually performed by hand instead of using automatic approaches.\nSoftware metrics provide a quantitative basis for the development and validation of models of software development process. Information gained from metrics can be used in\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Porfirio Tramontana .\nmanaging the development process in order to improve the reliability and quality of software products [4]. Cognitive informatics plays an important role in understanding the fundamental characteristics of software and cognitive complexity metrics are a good indicator for this [5]. A number of such measures have been proposed in the literature. However, there is no single metric which has the capability of measuring the complexity of a program based on multiple objectoriented concepts [6]. One of the most popular software metrics is the Cyclomatic Complexity, proposed by Thomas McCabe in 1976 [7]. This metric quantifies the control flow complexity of a piece of code and it has been extensively used in Object Oriented Programming (OOP) to compute the minimum number of test cases to cover a method. However, this metric is not adequate to quantify the understandability\n11642 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ VOLUME 10, 2022\nand maintainability of a code and, therefore, its cognitive complexity. Recently, a novel cognitive complexity metric has been proposed and integrated in the well-known static code tools SonarCloud1 and SonarQube,2 an open-source service and platform, respectively, for continuous inspection of code quality, which are extensively used by developers and software factories today. This cognitive complexity metric, which we refer to as SonarSource Cognitive Complexity (SSCC), has been defined as a measure of how hard the control flow of a method is to understand and maintain [8]. It breaks from the practice of using mathematical models to assess software maintainability. It starts from the precedents set by Cyclomatic Complexity, but uses human judgment to assess how structures should be counted and to decide what should be added to the model as a whole. The SSCC is given by a positive number which is increased every time a control flow sentence appear. Their nested levels also contribute to the SSCC of a method. Note that SonarQube suggests to keep methods\u2019 cognitive complexity no greater than 15, although this threshold can be set by the user to a different value.\nAs an introductory example, let us compare the sumOfPrimes and getWords methods, both shown in Fig. 1. Although they have equal Cyclomatic Complexity (4), it is intuitively obvious that the control flow of sumOfPrimes is more difficult to understand than that of getWords. The sumOfPrimemethod has a more complex control flow than that ofgetWords, mainly due to the nested loop and the continue statement. Thus, the cognitive effort required by developers to understand andmaintain both codes is not the same, being sumOfPrimes much harder (SSCC = 7) than getWords (SSCC = 1).\nTherefore, the cognitive complexity metric integrated in SonarQube yields method complexity scores which strike programmers as fairer relative assessments of understandability than have been available with previous models [8]. This assessment of understandability is also valid at the class level, just aggregating methods\u2019 cognitive complexity. Despite the fact that SSCC correlates with source code understandability\n1https://www.sonarcloud.org/ 2https://www.sonarqube.org/\nin a meaningful way [9], software developers lack support to reduce the cognitive complexity of their code to a given threshold.\nThere are different refactoring operations to handle different tasks. However, Extract Method is the most versatile refactoring operation serving 11 different purposes [10]. In addition, identification of Extract Method refactoring opportunities for the decomposition of methods can be performed in an automatic way [11]. Due to its many uses [12], Extract Method has been recognized as the \u2018\u2018Swiss army knife of refactorings\u2019\u2019 [10], [13]. It has also been recently used to reduce the complexity of code [14], [15] as we do in this paper. The main differences with already existing approaches are the following: they performed a more limited experimental validation, they do not impose any threshold for the cognitive complexity of themethods, and,most important, they are not able to generate sequences of feasible extractions. For example, in the project Knowage-Core, one of the projects analyzed as part of the case of study in this paper, there are more than 100 methods which require a sequence of code extractions to reduce their SSCC. However, previous approaches are not able to reduce the SSCC of those methods in a single execution.\nWemodel the reduction of the SSCC to a given threshold as an optimization problem. The search space contains all feasible sequences of Extract Method refactoring opportunities. An optimal solution is one which reduces SSCC to the chosen threshold while minimizing the number of method extractions.3 Note that the new extracted methods must be below the threshold too. We here propose an approach to reduce the SSCC of software projects in an automated way. We finally implement the proposed approach as a software tool for Java code and we apply it over 10 open-source software projects to reduce their SSCC. The developed tool will be available as an open-source project in a public repository.4 To the best of our knowledge, the SSCC metric has not been properly validated as a cognitive complexity measure. We additionally perform a theoretical validation of this metric, which we include as an appendix of this paper.\nWe thus make the following contributions:\n\u2022 Modeling the SSCC reduction to a given threshold as an optimization problem. \u2022 Providing a software tool to reduce the SSCC of Java projects in an automated way. \u2022 Validating the proposed approach over 10 real world open-source applications. \u2022 Defining best practices to improve software readability andmaintainability while benefiting the SSCC reduction task. \u2022 Performing a theoretical validation of the SSCC metric.\nThe remainder of this paper is organized as follows. Section II discusses related work. Section III formulates the\n3We minimize the number of method extractions to reduce the number of modifications in the original code.\n4https://github.com/rsain/SoftwareCognitiveComplexityReducer\nVOLUME 10, 2022 11643\nSSCC reduction as an optimization problem. Section IV, introduces our approach for reducing the SSCC to a given threshold. In Section V we present the case of study and summarize the experimental setting for evaluating our proposal. Section VI provides the results of our experiments. Section VII discusses the limitations of our approach, reports interesting findings, and identifies open research gaps. Section VIII discusses the threats to the validity of our work. Finally, Section IX presents conclusions and future work.\nII. RELATED WORK Probably the oldest and most intuitively obvious notion of software complexity is the number of statements in the program, or the statement count. However, a large number of software complexity measures have been proposed in the past. In the 70s, the number of program statements,McCabe\u2019s cyclomatic number [7], Halstead\u2019s programming effort [16], and the Knot measure [17] were the most frequently cited measures. In the 90s, Douce et al. introduced a set of metrics that help in calculating the complexity of a given system or program code based on the object-oriented concepts such as the object and class [18]. All those metrics were based on the spatial abilities which measure the complexity by calculating the distances between the program elements in the code.\nIn 2003, Shao and Wang proposed cognitive complexity as a new measure of the cognitive and psychological complexity of software by examining the cognitive weights of basic control structures (BCS) of software. Based on this approach a new concept of Cognitive Functional Size (CFS) of software was developed [19]. Cognitive weights are degree of difficulty or relative time and effort required for comprehending a given piece of software. In 2006, Misra et al. proposed the modification in CFS measure by taking into account the total occurrence of operators and operands and all internal BCS [20]. The same year, Misra proposed the CognitiveWeight ComplexityMeasure (CWCM) complexity measure which is also based on cognitive weights [21]. Then, Kushwaha and Misra framed different cognitive complexity metrics with the goal of aiding in increasing the reliability of software product during the development lifecycle [4].\nIn 2007, Misra S. and Misra A. K. compared cognitive complexity measures in terms of nine properties [22]. Then, Misra proposed an improved cognitive complexity measure named Cognitive Program Complexity Measure (CPCM) which establishes a relation between total number of inputs and outputs, cognitive weights, and cognitive complexity [23]. The same year, Misra proposed an object-oriented complexity metric which calculates the complexity of a class at method level [24]. Later, in 2008, Misra et al. proposed a metric that considers internal attributes which directly affect the complexity of software: number of lines, total occurrence of operators and operands, number of control structures, and function calls (coupling) [25]. The same year, Misra and Akman proposed a new complexity metric based on cognitive informatics for object-oriented code covering cognitive\ncomplexity of the system, method complexity, and complexity due to inheritance together [26].\nFew years later, in 2011, Misra et al. proposed a cognitive complexity metric for evaluating design of object-oriented code. The proposed metric is based on the inheritance feature of the object-oriented systems. It calculates the complexity at method level considering internal structure of methods, and also considers inheritance to calculate the complexity of class hierarchies [27]. In 2012,Misra et al. proposed a suite of cognitive metrics for evaluating complexity of object-oriented codes [28]. All the metrics are critically examined through theoretical and empirical validation processes. The same year, Misra et al. also proposed a framework for the evaluation and validation of software complexity measure. This framework is designed to analyse whether or not software metric qualifies as a measure from different perspectives [29].\nIn 2016, Haas and Hummel addressed the problem of finding the most appropriate refactoring candidate for long methods written in Java. The approach determines valid refactoring candidates and ranks them using a scoring function that aims to improve readability and reduce code complexity [14]. Later that year, Wijendra and Hewagamage proposed a cognitive complexity metric which determines the amount of information inside the software through cognitive weights and the way of information scattering in terms of Lines of Code (LOC) [30]. In this paper, authors also analyzed how the proposed cognitive complexity calculation can be automated. The same year, Crasso et al. presented a software metric to assess cognitive complexity in objectoriented systems developed in the Java language [31]. The proposed metric is based on a characterization of basic control structures present in Java systems. Authors also provided several algorithms to compute the metric and introduced their materialization in the Eclipse IDE. Finally, the applicability of the tool was shown by illustrating the metric in the context of 10 real world Java projects.\nIn 2017, Rabani and Maheswaran discussed and analyzed classical and modern metrics of software cognitive complexity [5]. The same year, Misra et al. identified the features and advantages of the existing software cognitive complexity metrics [32]. They also performed a comparative analysis based on some selected criteria. The results showed that there is a similar trend in the output obtained from the different measures when they are applied to different examples.\nIn 2018,Misra et al. presented an updated suite of cognitive complexity metrics that can be used to evaluate objectoriented software projects [33]. The metrics suite was evaluated theoretically using measurement theory and Weyuker\u2019s properties and practically using Kaner\u2019s framework [34]. The same year, SonarSource introduced cognitive complexity as a new metric for measuring the understandability of any given piece of code [8]. This paper investigated developers\u2019 reaction to the introduction of cognitive complexity in the static code analysis tool service SonarCloud. In an analysis of 22 opensource projects, they assessed whether a development team \u2018accepted\u2019 the proposed metric based on whether they fixed\n11644 VOLUME 10, 2022\ncode areas of high cognitive complexity as reported by the tool. They found that the metric had a 77% acceptance rate among developers.\nIn 2019, Kaur andMishra conducted an experimental analysis in which the software developer\u2019s level of difficulty in comprehending the software (the cognitive complexity) was theoretically computed and empirically evaluated for estimating its relevance to actual software change [35]. This study validated a cognitive complexity metric as a noteworthy measure of version to version source code change. Also in 2019, Alqadi proposed novel metrics to compute the cognitive complexity of code slices [36]. Empirical investigation into how cognitive complexity correlates with defects in the version histories of three open-source systems was performed. The results showed that the increase of cognitive complexity significantly increases the number of defects in 93% of the cases. The same year, Hubert proposed an approach to fully automate the extract method refactoring task ranking refactoring opportunities according to a scoring function which takes into account software cognitive complexity [15].\nRecently, in 2020, Jayalath and Thelijjagoda proposed a new metric to evaluate the complexity of object-oriented programs based on the influence of previous object-oriented metrics and some disregarded factors in calculating the complexity [6]. The same year, Mu\u00f1oz Bar\u00f3n et al. conducted a systematic literature search to obtain data sets from studies which measured code understandability and found that cognitive complexity integrated in the well-known static code analysis tool service SonarCloud positively correlates with comprehension time and subjective ratings of understandability [9].\nAlthough existing approaches can indirectly reduce software cognitive complexity, they are not able to automatically reduce methods cognitive complexity to a given threshold. Our proposal is novel because we (i) model the software cognitive complexity reduction to a given threshold as an optimization problem and (ii) provide a tool that generates and applies a sequence of feasible extractions to reduce the SonarSource Cognitive Complexity of Java projects.\nIII. PROBLEM DEFINITION AND MOTIVATION SonarCloud and SonarQube compute cognitive complexity of a method as the sum of two components that we call the inherent component and the nesting component. The inherent component depends on the presence of certain control flow structures and complex expressions (like decisions combining several conditional expressions). When a control flow structure or complex expression is found it contributes +1 to the inherent component. The nesting component depends on the depth that a certain control flow structure is in the code with respect to the root node (e.g. a method declaration). This depth is the contribution to the nesting component. Let si and ei be the start and end offset (in characters) of the ith sequence of sentences of amethod in its source file.We consider that ith sequence is nested in the jth sequence, denoted with i \u2192 j, when [si, ei] \u2282 [sj, ej]. We say that the ith sequence is in\nconflict with the jth sequence, denoted with i= j, when i and j are not nested one in the other and [si, ei] \u2229 [sj, ej] 6= \u2205.\nThe SSCC at method level can be reduced to a threshold applying Extract Method refactorings: extracting as a new method in the same class sequences of sentences (i.e., lines of code). However, this task is not straightforward for software developers due to the following reasons:\n1) The number of different Extract Method refactoring opportunities l is bounded by (n 2 ) = n\u00b7(n\u22121) 2 , where n\nis the number of sentences of the method.5\n2) Two code extractions cannot be applied simultaneously if they are in conflict. 3) Extract method refactoring opportunities are not applicable when they introduce compilation errors or the SSCC of the extracted code cannot be reduced to the threshold. 4) More than one Extract Method refactoring could be required to reduce the SSCC of a method.\nBased on the previous, we define the method cognitive complexity reduction task as an optimization problem which asks \u2018\u2018What is the optimal sequence of extract-method refactoring to apply in order to reduce the SSCC of the original method to/below a given threshold?\u2019\u2019. Thus, a solution to this problem is a sequence of code extractions which is bounded by 2l (all possible combinations of Extract Method refactorings).\nA. CHALLENGES OF REDUCING SOFTWARE COGNITIVE COMPLEXITY Fig. 2 shows a running example to illustrate the difficulties developers face when reducing the SSCC of a method. Note that some code has been replaced by \u2018\u2018. . . \u2019\u2019 due to space limitations, but the whole code is accesible in the following URL.6 This method has SSCC 46 and SonarQube suggests to reduce it to 15 in order to improve the understandability and maintainability of the method. As shown, there are 37 statements and the upper bound of Extract Method refactoring opportunities is (37 2 ) = 666. However, we have checked computationally that there are only 28 applicable code extractions. After analyzing the method, a developer who faces this cognitive complexity reduction task could realize that the optimal solution is a sequence of three Extract Method refactorings. Therefore, one would need to evaluate all possible sequences of one, two, and three Extract Method operations totaling (28 1 ) + (28 2 ) + (28 3 ) = 28 + 378 + 3, 276 = 3, 682 solutions. Although this number of solutions is much smaller than the theoretical upper bound of all possible sequences of extractions explained in Section III (which is 228 \u2248 268 million solutions), it is still unmanageable for developers without an automated approach.\n5Combination of n sentences taken two at a time without repetition. Note that those two sentences determine the beginning and ending of a code extraction.\n6https://github.com/KnowageLabs/Knowage-Server/blob/master/ knowage-core/src/main/java/it/eng/knowage/api/dossier/DocumentExecutio nWorkForDoc.java\nVOLUME 10, 2022 11645\nIV. COGNITIVE COMPLEXITY REDUCER APPROACH We propose a SSCC reducer approach consisting in a solver method implementing an automatic algorithm that takes as input the path to the software project to process and the cognitive complexity threshold (\u03c4 ). Then, for each method with SSCC greater than \u03c4 , it searches for sequences of applicable Extract Method refactoring operations. Finally, it shows the changes to perform to each method and apply them all at once in an automated way.\nIn order to search for Extract Method refactoring opportunities in a method, our approach generates its corresponding Abstract Syntax Tree (AST). Second, it parses the AST and annotates different properties7 in each node: its contribution to the SSCC of the method, the accumulated value of the inherent component (\u03b9), the accumulated value of the nesting component (\u03bd), the number of elements contributing to the nesting component of the SSCC of the node (\u00b5), and its absolute nesting level (\u03bb). Note that \u03bb is 0 when no nesting exists in the target piece of software. Third, the approach processes the annotated AST to compute the list of consecutive sentences contributing to the SSCC of the method. This is done to obtain Extract Method refactoring opportunities. Although sentences contributing to the SSCC must be part of code extractions, it is also necessary to consider single statements even if they do not contribute to the value of this metric. The inclusion of statements of this kind could suppose that the extraction is feasible or not. For example when several arithmetic operations are needed to compute a result, if all operations are not included in the extraction, the refactoring probably is not possible because only one variable could be returned. Once the approach identifies Extract Method refactoring opportunities, it checks if the extractions are applicable. This is done with the help of refactoring tools which are able to check pre-conditions, post-conditions, and apply the corresponding operation over the source code.\nA. COGNITIVE COMPLEXITY REDUCER TOOL IMPLEMENTATION We propose a Java cognitive complexity reducer tool as an Eclipse application. The goal is to provide the necessary means for generating an Eclipse product that can be run from the operating system command-line as a standalone executable, without the need for opening Eclipse for running. This is particularly useful if, for instance, one needs to integrate it in their current development workflow (e.g., using continuous integration). We got this idea from the jDeodorant project,8 an Eclipse plug-in that detects design problems in Java software and recommends appropriate refactorings to resolve them.\nThe developed tool takes as input (i) a SonarQube server URL, (ii) the path to the software project to process, (iii) the cognitive complexity threshold (\u03c4 ), and (iv) a stopping criteria (a number of Extract Method refactoring evaluations). Then, it runs SonarQube to perform an analysis of the project and get all existing cognitive complexity issues. Finally, for eachmethodwith SSCC greater than \u03c4 , it searches for Extract Method refactoring opportunities. In order to do this, the tool first generates and processes the AST associated to themethod declaration as explained in the previous section. Then, it enumerates sequences of applicable Extract Method refactorings while the given stopping criteria is not met. The tool uses the Extract Method refactoring operation provided\n7These are used to compute the SSCC of extracted methods. 8https://github.com/tsantalis/JDeodorant\n11646 VOLUME 10, 2022\nby the Java Development Toolkit (JDT) of Eclipse to test the feasibility of code extractions programmatically. Finally, the tool chooses the best sequence of method extractions found during the search: the one that reduces the SSCC to (or below) the threshold and minimizes the number of method extractions. If wished, the tool applies the required code extractions in an automated way by using the Extract Method refactoring provided by JDT.\nThe tool internally generates for each method under processing what we name the conflicts graph. A conflicts graph is a directed graph where vertices are applicable extractions and edges represent nested sequences of statements (i.e., if an edge targets j from i, then i\u2192 j). The tool labels vertices in the conflicts graph as [si, ei](CCi, \u03b9i, \u03bdi, \u00b5i, \u03bbi), where si and ei refer to the start and end offset (in characters in the source file) of the ith extraction. Red edges connect conflict vertices in the conflicts graph (i.e., i = j). Note that two vertices in conflict cannot be both selected for extraction in the same sequence. The root in a conflicts graph is a special vertex representing the whole body of the method. Conflicts graphs are used when searching for applicable Extract Method refactorings and to compute the impact of code extractions when reducing the SSCC of a method. Fig. 3 shows the conflicts graph of the running example whose source code is shown in Fig. 2. As shown, there are 28 extractable nodes plus the root node which is located in the left lower corner. In addition, there are 35 black edges that represent nested sequences of statements and 44 red edges that represent 22 pairs of nodes in conflict.\nV. CASE STUDY In this section we describe the study we conduct to evaluate the proposed approach when reducing the SSCC of 10 open-source projects. Next, we detail the objects of study. Then, we report the experimental setup used to conduct the experiments.\nA. OBJECTS OF STUDY We used the GitHub REST API to create calls to get repositories from GitHub satisfying two conditions: Java applications using Apache Maven as software project management. We choose Maven as software management because it eases the execution of SonarQube analysis via a regular Maven goal. We ended up selecting a diverse set of 10 open-source projects: two popular frameworks for multi-objective optimization, five platform components to accelerate the development of smart solutions, and three popular open-source projects with more than 10,000 stars and forked more than 900 times. Table 1 shows these projects and some software metric values. In order to ease the replication of the study, for each software project we also show its abbreviated commit hash in GitHub.\nThe simplest open-source project is QueryExecution: it contains 53 methods and six classes, summing up 1,013 lines of code. Despite the low number of methods in comparison to other open-source projects, 6 over 53 (11%) of the methods\nof QueryExecution have SSCC greater than 15 (the default threshold). Although this project looks simple, and, therefore, easy to maintain, reducing the SSCC of these six methods is not straight forward. For instance, for themethodgetDBIds SonarQube suggests to reduce its SSCC from 41 to 15. However, there are several refactoring opportunities that can be applied to get this done. Conversely, Knowage-core is the most complex project in our case study: it contains 6,967 methods and 1,093 classes, summing up 149,137 lines of code. Even for a senior developer, maintaining this ecosystem is complicated and prone to errors. SonarQube reports 558 cognitive complexity issues for this project, i.e., 8% of the methods in the project have SSCC greater than 15. Reducing the SSCC of these 558 methods would be time consuming and prone to errors when done manually.\nWe validate the proposed cognitive complexity reduction tool over the 10 open-source projects shown in Table 1. In total, these projects have 1,050 cognitive complexity issues. The goal of the study is to validate if the proposed approach is able to reduce the number of cognitive complexity issues existing on these projects. In addition, we want to uncover howmany extractions are needed, howmany lines of codes are extracted, and how many parameters new extracted methods have when reducing the SSCC of methods.\nB. ALGORITHMS We use an exhaustive search as resolution technique because it is conceptually simple and effective. It generates possible sequences of code extractions and assures the optimal one when all combinations can be generated. We want to keep the resolution technique simple to focus more on the problem and not in the resolution process.\nThe algorithm generates an exhaustive list of refactoring candidates first. To get this list, the source code is transformed into a block structure which contains structural information. After that, the algorithm starts to enumerate all possible code extractions in a recursive way with the help of a stack structure. The way the elements are introduced in the stack determines two variants of the algorithm: Exhaustive SearchLong Sequences First (ES-LSF) and Exhaustive Search-Short Sequences First (ES-SSF). The former is aimed at exploring as many consecutive statements as possible in a single extraction first. In contrast, the latter is aimed at exploring short sequences of statements first. We propose these two different\nVOLUME 10, 2022 11647\nways of exploring the search space because we set a number of evaluations as stopping criteria. If no stop condition is set, both variants must return an optimal solution.\nC. EXPERIMENTAL SETUP We conducted the experiments in a laptop Dell XPS 15 9560 with 4 \u00d7 Intelr Core i7-7700HQ CPU @ 2.80GHz and 16 GiB of RAM, running the operating system Windows 10 Pro. We used SonarQube version 7.2 and the Eclipse IDE version 2020-06 (4.16.0). We set the cognitive complexity threshold to the default value proposed by SonarQube (\u03c4 = 15). AST processing and Extract Method refactorings were performed through JDT version 3.16.0. All graph generation in our tool has been developed using the jGraphT library, a Java library of graph theory data structures and algorithms.9 In order to check if the observed differences in the results of ES-LSF and ES-SSF are statistically significant, we applied the non-parametric Mann\u2013 Whitney\u2013Wilcoxon test with a confidence level of 95% (p-value < 0.05).\nVI. RESULTS Table 2 reports the number of cognitive complexity issues, the number (and percentage) of cognitive complexity issues fixed, and the number and percentage of cognitive complexity issues that keep unfixed, respectively, for the projects under study.\nAs shown, the proposed approach is able to fix, on average, 78% of the cognitive complexity issues on these projects. Therefore, our approach is able to fix most cognitive complexity issues in most of the projects under study. However, 288 methods out of 1,050 (27%) has no solution since there are not applicable Extract Method refactorings. The reason is that most of these methods use multiple return statements\n9https://jgrapht.org/\nand loops containing multiple break or continue statements. These kind of statements prevent the extraction of any piece of code contributing to the SSCC of the method.\nTable 3 summarizes the combined results of our experiments. The first column shows the name of the different variants of the exhaustive algorithm implemented in our tool. The second column indicates whether found solutions are feasible or not. Note that a solution is a sequence of Extract Method refactorings. We define a solution as feasible when the original method and all the new extracted methods have a SSCC no greater than \u03c4 . In other case, the solution is unfeasible. Note that the best solution is that one which minimizes the number of method extractions. The remaining columns show some aggregated function values (min, max, mean, standard deviation, and sum) for different metrics. Next, there are four blocks of metrics. The first one (columns 4-11) is devoted to SSCC related metrics: iniCC is the initial cognitive complexity of the original methods (always above the threshold), extrac is the number of Extract Method refactorings proposed by the best solution, reducCC is the reduction on the cognitive complexity of the original methods, minReduc is the minimum reduction for a single extraction, maxReduc is the maximum reduction for a single extraction,\n11648 VOLUME 10, 2022\navgReduc is the mean (average) reduction considering all extractions of the best solution, totalReduc is the sum of the reductions of all extractions of the best solution, and finalCC is the SSCC of the original methods after applying the sequence of Extract Method refactorings determined by the best solution. The second block (columns 12-15) shows LOCmetrics, the third block (columns 16-19) provides information about the parameters involved in the extracted methods, and, finally, the last block shows the execution time in milliseconds.\nES-LSF and ES-SSF found 291 and 289 unfeasible solutions, respectively (i.e., they were able to reduce the SSCC of those methods but above \u03c4 ). Interestingly, those algorithms found 759 and 761 feasible solutions for existing cognitive complexity issues, respectively. The slight difference between the two algorithms is due to the way they explore the search space.\nNext, we focus on feasible solutions. Existing methods in the source code require, on average, more than one code extraction to reduce its SSCC. These code extractions reduce the SSCC of original methods below \u03c4 and extract around 15-20 lines of code from their original location into new methods. ES-LSF prioritizes during the search long code extractions while ES-SSF prefers short ones. Based on this, we expect that, on average, solutions found by ES-LSF extract portions of code with higher SSCC, more lines of code, and need more parameters for the new extracted methods, than solutions found by the ES-SSF algorithm. This is supported by statistical differences as shown in Table 4.\nFig. 4 shows boxplots of different metrics for feasible solutions. In Fig. 4(a) we can appreciate that there exist solutions with a high number of extractions for the ES-SSF algorithm. In contrast, the ES-LSF algorithm obtains solutions with lower number of code extractions. As commented previously,\nVOLUME 10, 2022 11649\nthis is due to the way these algorithms explore the search space. Fig. 4(b) compares the final SSCC of the original method after the cognitive complexity reduction. This boxplot confirms that ES-LSF tends to reduce SSCC more than ES-SSF. In fact, differences are significant between these two algorithms for this metric. Figs. 4(c), 4(d), and 4(e) can be interpreted all together as the behaviour of the presented approaches slightly affect them in the same way. The results confirm our expectations again. In fact, differences are significant between the two algorithms for avgReduc and avgLOC metrics. Finally, Fig. 4(f) shows the execution time in seconds. Both algorithms took almost 20 hours to process all methods with cognitive complexity issues on the 10 software projects under study. Despite the execution time looks similar, there are some outlier solutions in the case of ES-SSF that take longer time to meet the stopping criteria. The outlier near 800 seconds of execution time represent unique method which ES-SSF is able to obtain a solution but ES-LSF is not. Both algorithms took, on average, less than 70 seconds to reduce methods cognitive complexity.\nIn order to analyze the overall performance of the proposed approach to automatize the cognitive complexity reduction task, Table 5 shows aggregated metrics (min, max, mean, and standard deviation) for each project.\nAs shown, all projects, excepting Fiware-Commons, require, on average, more than one code extraction to reduce the SSCC of their methods to 15. However, five projects (CyberCaptor, FastJson, Jmetal, Knowage-core, and MOEA) required five or more code extractions to reduce the SSCC of some methods. In general, code extractions reduced, on average, SSCC by 12 units. Nevertheless, some code extractions reduced methods SSCC up to 72 units."
        },
        {
            "heading": "1) ANALYZING NOT SO TYPICAL SOLUTIONS",
            "text": "Next, we explain not so typical solutions obtained in this experiment which can be detected analyzing the results shown in Tables 3 and 5.\n\u2022 Short code extractions. There are 10 solutions where an extraction of only one line of code is enough to reduce\nTABLE 5. Statistics for the cognitive complexity reduction task performed over the 10 projects under study.\nthe SSCC of the method. A extraction of this kind is the following:\nBoolean b = Boolean . va lueOf ( s t r ? t r u e : f a l s e )\nThe ternary operator is a part of Java\u2019s conditional statements. As the name ternary suggests, it is the only operator in Java consisting of three operands. The ternary operator can be thought of as a simplified version of the if-else statement with a value to be returned. This kind of statement contributes to the SSCC of a method with its inherent component (\u03b9 = 1) plus the nesting component, which depends on the nesting level of the statement in the source code. Therefore, when extracting this kind of one-line statement the cognitive complexity of a method might be reduced by at least one. \u2022 Too many code extractions. There is a solution provided by the ES-SSF that suggests 30 extractions. This happens in the decodeDispatchContext method in the SchedulerUtilities class of the Knowagecore project. This method has 154 lines of code and its SSCC is 86. It has 37 if statements at the same level of nesting. This results in too many extractions when applying the ES-SSF algorithm. Note that increasing the number of evaluations, the number of extraction\n11650 VOLUME 10, 2022\ndecreases. For example, using 100,000 and one million evaluations, ES-SSF performs 28 and 25 extractions, respectively. \u2022 Many parameters in extracted methods. There is a solution that extracts a new method with 15 parameters. This happens in the manageRecursiveSection method in the HierarchyMasterService class of the Knowage-core project. The reason is that the original method has 16 parameters. The best solution for this case is a single extraction of 32 lines of code which reduces the SSCC of the original method in 19. However, most original parameters are needed in the extracted method."
        },
        {
            "heading": "2) COMING BACK TO THE RUNNING EXAMPLE",
            "text": "Followingwith the running example introduced in Section III, Fig. 5 shows the addParametersToServiceUrl method after cognitive complexity reduction. The SSCC of this method has been reduced from 46 to 8 after applying three Extract Method refactoring operations. The number of lines of code has also reduced from 65 to 24. Note that, although three Extract Method operations are applied, just one appears in the code (line 16). The reason is that the other two method extractions are called from the extracted method extraction1. These two additional method extractions are required to reduce the SSCC of the first extracted method to \u03c4 .\nVII. DISCUSSION A number of cognitive complexity metrics have been proposed in the literature measuring software cognitive complexity in different ways. However, software complexity has gained popularity last years due to the usage of SonarCloud and SonarQube as service and platform, respectively, for continuous inspection of code quality. For this reason we used the cognitive complexity measure provided by these well-known static code tools in this work, which we referred to SSCC. In order to search for feasible refactoring opportunities to\nreduce cognitive complexity of a method, the proposed approach generates its Abstract Syntax Tree (AST) and annotates in their nodes information about the contribution to the cognitive complexity of the method. Thus, other cognitive complexity measures which take into account the presence of control flow structures in source code for their computation could be integrated in our tool: it would just require to adapt the way the approach gets the list of existing cognitive complexity issues in a project and the computation of the properties annotated in the nodes of the AST of the methods. However, this is out of the scope of the paper: our main contribution is that software cognitive complexity reduction can be modeled as an optimization problem and automatizing SSCC reduction is feasible, which is empirically proven through our experiments.\nConcerning the resolution process, a block of consecutive statements can be extracted if pre-conditions and postconditions are met and the extraction generates compilable code. The more blocks of consecutive statements that are extractable, the more possibilities the cognitive complexity can be reduced. However, developers do not know which sequences of statements are extractable but also the impact of any code extraction on code cognitive complexity. Therefore, developers cannot make informed decisions concerning the cognitive complexity of a method when maintaining its code. The resolution techniques proposed here have a number of advantages. Among them, it stands out that they achieve optimal solutions quickly for most of the methods analyzed (78%) without using any heuristic or randomized operator. In contrast, they require a high number of evaluations to find feasible solutions in methods with high number of extractable blocks of code. As the number of evaluations increases, the better the solution obtained. However, the minimum number of evaluations required to find optimal solutions is unknown beforehand. If execution time is a constraint when reducing the cognitive complexity of a method, search-based software engineering techniques could be applied instead of the exhaustive algorithms used in this paper (note that our tool is algorithm independent when solving the cognitive complexity reduction problem). Nevertheless, the proposed approach and the implemented resolution techniques took, on average, less than 70 seconds to process each method of the 10 software projects in our case study. It seems reasonable to integrate software cognitive complexity reduction in the development workflow (e.g., using continuous integration). Thus, the proposed approach could be automatically run at night or after developers commit new changes to software repositories.\nIn this work we decided to minimize the number of Extract Method refactoring operations when reducing the SSCC of a method. Therefore, all applicable sequences of extractions with minimum size are optimal. However, not all these sequences have the same characteristics and they vary in most metrics studied in this paper: extracted lines of code, extracted SSCC, number of parameters, final SSCC in the original method, and many others. It is possible to\nVOLUME 10, 2022 11651\nmodel the cognitive complexity reduction problem as a multiobjective or many-objective optimization problem. In that case different techniques can be applied to optimize several of these metrics at the same time. In addition, multiple criteria decision making could be applied allowing developers to decide which solutions seemmost appropriate for them based on their preferences.\nInterestingly, we found that some coding practices could hinder the cognitive complexity reduction task. This usually happenswhenmethods containmultiplereturn statements. Having too many return statements in a method decreases the method\u2019s essential understandability because the flow of execution is broken each time a return statement is encountered. This makes it harder to read and understand the logic of the method but could also prevent the extraction of the code.10 Consequently, the use of multiple return statements in a method might make an instance of the cognitive complexity reduction problem unsolvable. This also holds for loops containing multiple break or continue statements, which also breaks the execution flow. Therefore, restricting the number of break and continue statements in a loop is done in the interest of good structured programming.11\nThe use of multiple return, break, and continue statements in a method could hinder the cognitive complexity reduction task or even make it unsolvable. An aspect that is out of the scope of this article is the choice of the name for the new extracted methods. The name of newmethods can influence the understanding of the resulting source code. Therefore, this is an important aspect we plan to address in the near future. Creating a dictionary with keywords in the original method and using natural language processing techniques with Transformers [37] could be a good starting point to handle this fact.\nVIII. THREATS TO VALIDITY This section discusses all threats that might have an impact on the validity of our study following common guidelines for empirical studies [38].\nThreats to internal validity concern factors that could have influenced our results. A possible threat to internal validity is that we set a stopping criteria of 10,000 evaluations. This stop condition might have influenced our results because the algorithms, in some cases, stop before all possible sequences of extractions are explored. However, in order to alleviate this issue, we have presented two completely different ways of exploring the search space. Another aspect that can influence the results is the choice of the cognitive complexity metric and the used threshold. A number of cognitive complexity measures have been proposed in the literature. However, there is no single metric which has the capability of measuring the complexity of a program based on multiple object-oriented concepts [6]. We used the cognitive complexity metric integrated in the well-known static code tools SonarCloud and\n10https://rules.sonarsource.com/java/RSPEC-1142?search=return 11https://rules.sonarsource.com/java/RSPEC-135?search=break\nSonarQube because (i) this metric positively correlates with source code understandability [9] and has a 77% acceptance rate among developers [8], (ii) it is accessible via SonarQube API REST, and (iii) a well-defined cognitive complexity threshold is suggested for it.\nThreats to construct validity concern relationship between theory and observation and the extent to which the measures represent real values. In our study all the experiments were run in the same computer and the metrics we collected are all consistent when analyzing the original and the resulting source codes.\nThreats to external validity concern the generalization of our findings. To reduce external validity threats we selected a diverse set of 10 open-source projects for our case of study. Aggregating all projects, we processed 1,050 methods with SSCC greater than 15. This high number of existing issues guarantees that we have analyzed very diverse methods in terms of complexity and size. Thus, we guess our findings can be generalized to other software projects.\nThreats to conclusion validity concern the relationship between experimentation and outcome. We compared the results of two different variants of an exhaustive algorithm and performed a Mann-Whitney-Wilcoxon test to determine the statistical significance of the results. In addition, a large number of methods were analyzed and the algorithms had enough evaluations to find feasible solutions for most of the methods of the software projects under study.\nIX. CONCLUSION We formulated the reduction of software cognitive complexity provided by SonarCloud and SonarQube, to a given threshold, as an optimization problem. We then proposed an approach to automatically reduce the cognitive complexity of methods in software projects to the chosen threshold using sequences of Extract Method refactorings. We conducted some experiments in 10 open-source software projects analyzing more than 1,000 methods with a cognitive complexity greater than the default threshold suggested by SonarQube (15). Our automated approach was able to reduce the cognitive complexity to or below the threshold in 78% of those methods.\nWe found that statements that brake the execution flow of programs could prevent the extraction of code and, therefore, make a particular instance of the cognitive complexity reduction problem unsolvable. With the aim of helping to alleviate this issue, we propose as future work a semantically equivalent code transformation that increases the number of extractable blocks of code in a method by reducing the number of return, break, and continue statements. This transformation will indirectly improve the readability and maintainability of the code, but it will also benefit the cognitive complexity reduction task.\nAlthough our approach was able to reduce the cognitive complexity for most methods, we cannot assure that no solution exist for the rest of the methods. The reason is that the cost of exploring all possible sequences of extractions might\n11652 VOLUME 10, 2022\nbe unaffordable. However, we think that it is preferable to maintain the simplicity of the approach to emphasize the benefits of providing an automated tool. As future work we want to study the NP-hardness of the modeled cognitive complexity reduction problem. If we prove this, a different procedure (like an ad-hoc metaheuristic) could be included in our approach to solve the problem. Last but not least, we plan to validate our approach on software developers in order to get their feedback and analyze the way of including our approach as part of the continuous integration practice.\nAPPENDIX In the field of theoretical validation, a number of researchers have proposed different criteria to which software measures should adhere. Weyuker established a formal list of nine properties in order to estimate the accuracy of software metrics [39]. It has been used to evaluate numerous existing software metrics. Next we evaluate the cognitive complexity metric provided by SonarCloud and SonarQube (which we refer to as SSCC) against Weyuker\u2019s properties and validate it against measurement theory, as suggested in the framework proposed by Misra et al. [28]. Then, we perform a practical validation with Kaner\u2019s framework [40]. We finally end up with a comparative analysis and conclusion of the validation.\nA. WEYUKER\u2019S PROPERTIES In the following, P, Q, and R are methods of a class. With |P| and (P;Q) we refer to the SSCC of method P and the composition of P and Q methods, respectively.\n1) PROPERTY 1. (\u2203P)(\u2203Q)(|P| 6= |Q|), WHERE P AND Q ARE TWO DISPARATE METHODS By definition of the measure, usually different methods have different SSCC values. Hence, this property holds for this measure."
        },
        {
            "heading": "2) PROPERTY 2: LET c BE A NON-NEGATIVE NUMBER THEN THERE ARE ONLY FINITELY MANY METHODS OF COMPLEXITY c",
            "text": "All projects have finite number of classes and methods, and all methods have a finite number of statements. Because SSCC of a method depends on its statements then there are only finitely many methods that will be equal to the measure c. The SSCC metric thus holds for this property."
        },
        {
            "heading": "3) PROPERTY 3: THERE ARE DISTINCT METHODS P AND Q",
            "text": "SUCH THAT |P| = |Q| This property says that there can be multiple methods containing the same SSCC value. Two methods without control flow structures will have equal complexity (0). Hence this property is satisfied by this measure.\n4) PROPERTY 4: (\u2203P)(\u2203Q)(P \u2261 Q AND |P| 6= |Q|) This property states that even though two methods compute the same function, it is the details of the implementation that determine the methods complexity. Even though the\nfunctionalities of two methods are equal, their complexity depends on the number of control flow structures and their nesting level on the code. Because of that the SSCC measure holds this property.\n5) PROPERTY 5: (\u2200P)(\u2200Q)(|P| \u2264 |P;Q| AND |Q| \u2264 |P;Q|) This property states that the complexity values of two methods should be less than or equal to the complexity of the composition of the two methods. The SSCC measure mainly depends on the presence of control flow structures and complex expressions which determine the inherent component complexity of a method. Thus, the complexity value of the combination of two methods should be greater than or equal to the complexity value of these two methods. Hence this property is satisfied by this measure.\n6) PROPERTY 6: (\u2203P)(\u2203Q)(\u2203R)(|P| = |Q|) AND (|P;R| 6= |Q;R|) This property states that if there are two methods P and Q with same SSCC andwhen they are separately combined with the same third method R, yields a method of different SSCC. For any two methods P and Q, any combination of them with another method R will produce new methods with similar SSCC. Therefore, this measure does not satisfy this property."
        },
        {
            "heading": "7) PROPERTY 7: THERE ARE METHODS P AND Q SUCH THAT Q IS FORMED BY PERMUTING THE ORDER OF THE",
            "text": "STATEMENTS OF P AND (|P| 6= |Q|) Changing the order of the statements in a method, without changing the functionality of the method, will not change its complexity value. Therefore, this measure does not satisfy this property.\n8) PROPERTY 8: IF P IS RENAMING OF Q, THEN |P| = |Q| Renaming of a method does not impact its SSCC. As a consequence, this property is satisfied by this measure.\n9) PROPERTY 9: (\u2203P)(\u2203Q)(|P| + |Q| < |P;Q|) This property states that the addition of complexities of two separate methods is lower than the complexity of a method which is created by joining those two separate methods. The SSCC of the combinedmethod never reduces. Because of this situation this condition is not fulfilled by this metric. However, the modified version of this property, (\u2203P)(\u2203Q)(|P| + |Q| \u2264 |P;Q|) [33], is satisfied. Table 6 gives a summary of the evaluation process through Weyuker\u2019s properties. The satisfied properties are marked. As shown, and according to the above explanation, the SSCC measure satisfies all the properties of Weyuker\u2019s framework excluding 7th and 9th. Thus, we suggest that the SSCC measure establishes as a well-structured one.\nB. MEASUREMENT THEORY Here we validate the SSCC measure against measurement theory using the Briand et al. framework [41]. We do our\nVOLUME 10, 2022 11653\nassessment providing the basic definitions and desirable properties that make up the framework. Definition (Representation of Systems and Modules): \u2018\u2018A system S is represented as a pair < E,R >, where E represents the set of elements of S, and R is a binary relation on E (R \u2286 E\u00d7E) representing the relationships between S\u2019s elements.\u2019\u2019.\nFor the SSCC metric, E can be defined as a method containing a set of code statements and R as the set of inherent components: complex expressions and control flows from one statement to another. Definition (Complexity): \u2018\u2018The complexity of a system S is a function Complexity (S) that is characterized by the following five properties: non-negativity, null value, symmetry, module monotonicity, and disjoint module additive.\u2019\u2019. \u2022 Non-negative: \u2018\u2018The complexity of a system S =< E,R > is non-negative if Complexity(S) \u2265 0.\u2019\u2019. SSCC values are always positive, this property is thus satisfied by this measure.\n\u2022 Null value: \u2018\u2018The complexity of a system S =< E,R > is null if R is empty.\u2019\u2019. If a method does not contain inherent components (certain control flow structures or complex expressions), then it will have null (0) complexity. Thus, this property is satisfied by the SSCC metric. \u2022 Symmetry: \u2018\u2018The complexity of a system S =< E,R > does not depend on the convention chosen to represent the relationships between its elements.\u2019\u2019. There is no effect on the complexity values of the SSCC metric by changing the order or representation because the contribution of control flow structures and its nesting level to the overall complexity of a method cannot depend on the order or way of representation. Therefore, this property is satisfied by the SSCC measure. \u2022 Module monotonicity: \u2018\u2018The complexity of a system S =< E,R > is no less than the sum of the complexities of any two of its modules with no relationships in common.\u2019\u2019. For the SSCC metric, a module can be defined as a code segment in a method. For this property, if any method is partitioned into two methods, the sum of the complexity values of its partitioned methods will never be greater than the one of the joined method. Therefore, this property holds for the metric. \u2022 DisjointModuleAdditivity: \u2018\u2018The complexity of a system S =< E,R > composed of two disjoint modules m1 and m2 is equal to the sum of the complexities of the two modules.\u2019\u2019. The SSCC value of the method obtained by concatenating m1 and m2 is equal to the sum of their calculated complexity values. Thus, if two independent methods\nare combined into a single method then the complexity of the individual methods will be combined. Therefore, this property is satisfied by the SSCC measure.\nBy fulfilling these properties, one may say that the SSCC metric is on the ratio scale, which is the most desirable property of complexity measures from the point of view of measurement theory [33].\nC. PRACTICAL VALIDATION WITH KANER\u2019S FRAMEWORK In addition to the theoretical validation using Weyukers\u2019 properties and measurement theory, the framework given by Kaner [40] can also be adopted for evaluation of the SSCCmetric. This approach is more practical than the formal approach of Weyukers\u2019 properties and measurement theory. The framework is based on providing answers to the following points:"
        },
        {
            "heading": "1) PURPOSE OF THE MEASURE",
            "text": "The purpose of the measure is to evaluate the complexity of methods in object-oriented programming languages."
        },
        {
            "heading": "2) SCOPE OF THE MEASURE",
            "text": "Object-orientation is widely adopted nowadays in the development of software, from open-source to proprietary software. The SSCCmeasure can be used within and across these projects."
        },
        {
            "heading": "3) IDENTIFIED ATTRIBUTE TO MEASURE",
            "text": "The SSCC metric is defined as a measure of how hard the control flow of a method is to understand and maintain. Thus, The identified attributes that the SSCCmeasure addresses are understandability and maintainability."
        },
        {
            "heading": "4) NATURAL SCALE OF THE ATTRIBUTE",
            "text": "The natural scales of the attributes cannot be defined, since it is subjective and requires the development of a common view about them [33]."
        },
        {
            "heading": "5) NATURAL VARIABILITY OF THE ATTRIBUTE",
            "text": "Natural variability of the attributes can also not be defined because of their subjective nature. It is possible that one can develop a sound approach to handle such attribute, but it may not be complete because other factors also exist that can affect the attribute\u2019s variability [33]."
        },
        {
            "heading": "6) DEFINITION OF METRIC",
            "text": "The SSCC metric has been formally defined by Campbell [42] and briefly introduced in Section I."
        },
        {
            "heading": "7) MEASURING INSTRUMENT TO PERFORM THE MEASUREMENT",
            "text": "The SSCC measure was computed by SonarQube."
        },
        {
            "heading": "8) NATURAL SCALE FOR THE METRIC",
            "text": "The SSCC measure is on the ratio scale, as mentioned earlier in this section.\n11654 VOLUME 10, 2022"
        },
        {
            "heading": "9) RELATIONSHIP BETWEEN THE ATTRIBUTE AND THE",
            "text": "METRIC VALUE The SSCCmetric contributes to determining the overall complexity of methods and classes in object-oriented programming languages. Higher SSCC values translate into code harder to understand and maintain."
        },
        {
            "heading": "10) NATURAL FORESEEABLE SIDE EFFECTS OF USING THE INSTRUMENT",
            "text": "There are no side effects of using SonarQube to measure the SSCC of software projects because the computation of the metric is automatically performed by it. In addition to the previous, SonarQube is an open-source tool and its source code is available in public repositories.\nD. COMPARATIVE ANALYSIS AND CONCLUSION OF THEORETICAL VALIDATION The SSCC measure satisfied seven out of the nine Weyuker\u2019s properties. Although these results were convincing enough, we turned to the measurement theory. Measurement theory has five properties all of which were satisfied by the SSCC metric. This shown that this measure is additive and on the ratio scale. Finally, the Kaner\u2019s framework was used to prove the usefulness of the SSCC measure after asking practical questions.\nREFERENCES [1] D. Coleman, D. Ash, B. Lowther, and P. Oman, \u2018\u2018Using metrics to evaluate\nsoftware system maintainability,\u2019\u2019 Computer, vol. 27, no. 8, pp. 44\u201349, Aug. 1994. [2] G. J. Myers, C. Sandler, and T. Badgett, The Art of Software Testing, 3rd ed. Hoboken, NJ, USA: Wiley, 2011. [3] UNDO. (2017). Increasing Software Development Productivity With Reversible Debugging. [Online]. Available: https://undo.io/media/ uploads/files/Undo_ReversibleDebugging_Whitepap%er.pdf [4] D. S. Kushwaha and A. K. Misra, \u2018\u2018Cognitive complexity metrics and its impact on software reliability based on cognitive software development model,\u2019\u2019 ACM SIGSOFT Softw. Eng. Notes, vol. 31, no. 2, pp. 1\u20136, Mar. 2006, doi: 10.1145/1118537.1118544. [5] S. T. Rabani and K. Maheswaran, \u2018\u2018Software cognitive complexity metrics for OO design: A survey,\u2019\u2019 Int. J. Sci. Res. Sci., Eng. Technol., vol. 3, pp. 692\u2013698, 2017. [6] T. Jayalath and S. Thelijjagoda, \u2018\u2018A modified cognitive complexity metric to improve the readability of object-oriented software,\u2019\u2019 in Proc. Int. Res. Conf. Smart Comput. Syst. Eng. (SCSE), Sep. 2020, pp. 37\u201344. [7] T. McCabe, \u2018\u2018A complexity measure,\u2019\u2019 IEEE Trans. Softw. Eng., vol. SE-2, no. 4, pp. 308\u2013320, Dec. 1976. [8] G. A. Campbell, \u2018\u2018Cognitive complexity: An overview and evaluation,\u2019\u2019 in Proc. Int. Conf. Tech. Debt, May 2018, pp. 57\u201358, doi: 10.1145/3194164.3194186. [9] M. Mu\u00f1oz Bar\u00f3n, M. Wyrich, and S. Wagner, \u2018\u2018An empirical validation of cognitive complexity as a measure of source code understandability,\u2019\u2019 in Proc. 14th ACM/IEEE Int. Symp. Empirical Softw. Eng. Meas. (ESEM), Oct. 2020, pp. 1\u201312, doi: 10.1145/3382494.3410636. [10] D. Silva, N. Tsantalis, and M. T. Valente, \u2018\u2018Why we refactor? Confessions of GitHub contributors,\u2019\u2019 in Proc. 24th ACM SIGSOFT Int. Symp. Found. Softw. Eng., Nov. 2016, pp. 858\u2013870, doi: 10.1145/2950290.2950305. [11] N. Tsantalis and A. Chatzigeorgiou, \u2018\u2018Identification of extract method refactoring opportunities for the decomposition of methods,\u2019\u2019 J. Syst. Softw., vol. 84, no. 10, pp. 1757\u20131782, Oct. 2011, doi: 10.1016/ j.jss.2011.05.016. [12] N. Tsantalis, V. Guana, E. Stroulia, and A. Hindle, \u2018\u2018A multidimensional empirical study on refactoring activity,\u2019\u2019 in Proc. Conf. Center Adv. Stud. Collaborative Res. (CASCON). Armonk, NY, USA: IBM Corp., 2013, pp. 132\u2013146.\n[13] A. Hora and R. Robbes, \u2018\u2018Characteristics of method extractions in java: A large scale empirical study,\u2019\u2019 Empirical Softw. Eng., vol. 25, no. 3, pp. 1798\u20131833, May 2020, doi: 10.1007/s10664-020-09809-8. [14] R. Haas and B. Hummel, \u2018\u2018Deriving extract method refactoring suggestions for long methods,\u2019\u2019 in Software Quality. The Future of Systemsand Software Development, D.Winkler, S. Biffl, and J. Bergsmann, Eds. Cham, Switzerland: Springer, 2016, pp. 144\u2013155. [15] J. Hubert, \u2018\u2018Implementation of an automatic extract method refactoring,\u2019\u2019 M.S. thesis, Fac. Comput. Sci., Elect. Eng. Inf. Technol., Univ. Stuttgart, Stuttgart, Germany, Germany, Apr. 2019. [16] M. H. Halstead, Elements of Software Science, M. H. Halstead, Ed. New York, NY, USA: Elsevier, 1977. [17] M. R. Woodward, M. A. Hennell, and D. Hedley, \u2018\u2018A measure of control flow complexity in program text,\u2019\u2019 IEEE Trans. Softw. Eng., vol. SE-5, no. 1, pp. 45\u201350, Jan. 1979. [18] C. R. Douce, P. J. Layzell, and J. Buckley, \u2018\u2018Spatial measures of software complexity,\u2019\u2019 in Proc. PPIG. Delft, The Netherlands: Psychology of Programming Interest Group, 1999, p. 6. [19] J. Shao and Y. Wang, \u2018\u2018A new measure of software complexity based on cognitive weights,\u2019\u2019 in Proc. CCECE Can. Conf. Electr. Comput. Eng. Toward Caring Humane Technol., vol. 2, May 2003, pp. 1333\u20131338. [20] S. Misra, \u2018\u2018Modified cognitive complexity measure,\u2019\u2019 in Computer and Information Sciences ISCIS 2006, A. Levi, E. Sava\u015f, H. Yenig\u00fcn, S. Balc\u0131soy, and Y. Sayg\u0131n, Eds. Berlin, Germany: Springer, 2006, pp. 1050\u20131059. [21] S. Misra, \u2018\u2018A complexity measure based on cognitive weights,\u2019\u2019 Int. J. Theor. Appl. Comput. Sci., vol. 1, no. 1, pp. 1\u201310, 2006. [22] S. Misra and A. K. Misra, \u2018\u2018Evaluation and comparison of cognitive complexity measure,\u2019\u2019 ACM SIGSOFT Softw. Eng. Notes, vol. 32, no. 2, pp. 1\u20135, Mar. 2007, doi: 10.1145/1234741.1234761. [23] S. Misra, \u2018\u2018Cognitive program complexity measure,\u2019\u2019 in Proc. 6th IEEE Int. Conf. Cognit. Informat., Aug. 2007, pp. 120\u2013125. [24] S. Misra, \u2018\u2018An object oriented complexity metric based on cognitive weights,\u2019\u2019 in Proc. 6th IEEE Int. Conf. Cognit. Informat., Aug. 2007, pp. 134\u2013139. [25] S. Misra and I. Akman, \u2018\u2018A model for measuring cognitive complexity of software,\u2019\u2019 in Knowledge-Based Intelligent Information and Engineering Systems, I. Lovrek, R. J. Howlett, and L. C. Jain, Eds. Berlin, Germany: Springer, 2008, pp. 879\u2013886. [26] S. Misra and I. Akman, \u2018\u2018A new complexity metric based on cognitive informatics,\u2019\u2019 in Rough Sets and Knowledge Technology, G. Wang, T. Li, J. W. Grzymala-Busse, D. Miao, A. Skowron, and Y. Yao, Eds. Berlin, Germany: Springer, 2008, pp. 620\u2013627. [27] S. Misra, I. Akman, and M. Koyuncu, \u2018\u2018An inheritance complexity metric for object-oriented code: A cognitive approach,\u2019\u2019 Sadhana, vol. 36, no. 3, p. 317, Jul. 2011, doi: 10.1007/s12046-011-0028-2. [28] S. Misra, M. Koyuncu, M. Crasso, C. Mateos, and A. Zunino, \u2018\u2018A suite of cognitive complexity metrics,\u2019\u2019 in Computational Science and its Applications ICCSA 2012, B. Murgante, O. Gervasi, S. Misra, N. Nedjah, A. M. A. C. Rocha, D. Taniar, and B. O. Apduhan, Eds. Berlin, Germany: Springer, 2012, pp. 234\u2013247. [29] S. Misra, I. Akman, and R. Colomo-Palacios, \u2018\u2018Framework for evaluation and validation of software complexity measures,\u2019\u2019 IET Softw., vol. 6, no. 4, pp. 323\u2013334, Aug. 2012. [Online]. Available: https://digitallibrary.theiet.org/content/journals/10.1049/iet-sen.201%1.0206 [30] D. R. Wijendra and K. P. Hewagamage, \u2018\u2018Automated tool for the calculation of cognitive complexity of a software,\u2019\u2019 in Proc. 2nd Int. Conf. Sci. Inf. Technol. (ICSITech), Oct. 2016, pp. 163\u2013168. [31] M. Crasso, C. Mateos, A. Zunino, S. Misra, and P. Polvor\u00edn, \u2018\u2018Assessing cognitive complexity in java-based object-oriented systems: Metrics and tool support,\u2019\u2019 Comput. Inform., vol. 35, no. 3, pp. 497\u2013527, 2016. [Online]. Available: http://www.cai.sk/ojs/index.php/cai/article/view/1747 [32] S. Misra, A. Adewumi, R. Damasevicius, and R. Maskeliunas, \u2018\u2018Analysis of existing software cognitive complexity measures,\u2019\u2019 Int. J. Secure Softw. Eng., vol. 8, no. 4, pp. 51\u201371, Oct. 2017, doi: 10.4018/IJSSE.2017100103. [33] S. Misra, A. Adewumi, L. Fernandez-Sanz, and R. Damasevicius, \u2018\u2018A suite of object oriented cognitive complexity metrics,\u2019\u2019 IEEE Access, vol. 6, pp. 8782\u20138796, 2018. [34] J. C. Cherniavsky and C. H. Smith, \u2018\u2018On Weyuker\u2019s axioms for software complexity measures,\u2019\u2019 IEEE Trans. Softw. Eng., vol. 17, no. 6, pp. 636\u2013638, Jun. 1991, doi: 10.1109/32.87287.\nVOLUME 10, 2022 11655\n[35] L. Kaur and A. Mishra, \u2018\u2018Cognitive complexity as a quantifier of version to version java-based source code change: An empirical probe,\u2019\u2019 Inf. Softw. Technol., vol. 106, pp. 31\u201348, Feb. 2019. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0950584918301903 [36] B. S. Alqadi, \u2018\u2018The relationship between cognitive complexity and the probability of defects,\u2019\u2019 in Proc. IEEE Int. Conf. Softw. Maintenance Evol. (ICSME), Sep. 2019, pp. 600\u2013604. [37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, U. Kaiser, and I. Polosukhin, \u2018\u2018Attention is all you need,\u2019\u2019 in Proc. 31st Int. Conf. Neural Inf. Process. Syst. (NIPS). Red Hook, NY, USA: Curran Associates Inc., 2017, pp. 6000\u20136010. [38] R. K. Yin, Case Study Research: Design Methods, 3rd ed. Newbury Park, CA, USA: Sage, 2002. [39] E. J. Weyuker, \u2018\u2018Evaluating software complexity measures,\u2019\u2019 IEEE Trans. Softw. Eng., vol. SE-14, no. 9, pp. 1357\u20131365, Sep. 1988. [Online]. Available: http://ieeexplore.ieee.org/document/6178/ [40] C. Kaner and W. P. Bond, \u2018\u2018Software engineering metrics: What do they measure and how do we know,\u2019\u2019 in Proc. Int. Softw. Metrics Symp., Washington, DC, USA: IEEE Computer Society Press, 2004, pp. 1\u201312. [41] L. C. Briand, S. Morasca, and V. R. Basili, \u2018\u2018Property-based software engineering measurement,\u2019\u2019 IEEE Trans. Softw. Eng., vol. 22, no. 1, pp. 68\u201386, Jan. 1996. [42] G. A. Campbell, \u2018\u2018Cognitive complexity\u2014A new way of measuring understandability,\u2019\u2019 SonarSource, Geneva, Switzerland, White Paper, 2017. [Online]. Available: https://www.sonarsource.com/docs/ CognitiveComplexity.pdf\nRUB\u00c9N SABORIDO received the B.S. degree in computer engineering and the M.S. degree in software engineering and artificial intelligence from the University of M\u00e1laga, Spain, and the Ph.D. degree in computer engineering from PolytechniqueMontr\u00e9al, Canada, in 2017. He worked three years as a Researcher Assistant at the University of M\u00e1laga. He is currently a Researcher at the Networking and Emerging Optimization Group, Department of Computer Science, University of\nM\u00e1laga. In 2018, he held a postdoctoral fellowship at Concordia University, Canada, where he worked on search-based software engineering for the Internet of Things (IoT). He was awarded a Juan de la Cierva Grant, in 2020, funded by the Spanish State Research Agency, and ranked in the top five of the candidates in the information and communications technologies area. His research interest includes search-based software engineering. He is also interested in the use of metaheuristics to solve multidisciplinary real-world problems of interest for our society and computer science. He has published several papers in ISI indexed journals (such as EMSE, IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, Evolutionary Computation, and Swarm and Evolutionary Computation) and conference papers in IEEE ICPC, MCDM, IEEE SANER, and ACM ESEC/FSE. He also acts as a reviewer of prestigious international journals (ISI/JCR). His Ph.D. thesis was nominated for best thesis award. He has co-organized the International Conference onMultiple Criteria DecisionMaking, in 2013. He is on the organizing committee of the 1st, 2nd, 3rd, and 4th International Workshop on Software Engineering Research and Practices for the Internet of Things (SERP4IoT), co-located with ICSE 2019, ICSE 2020, ICSE 2021, and ICSE 2022, respectively. He has been on the program committee of the Real World Applications (RWA) track of the Genetic and Evolutionary Computation Conference (GECCO), since 2016.\nJAVIER FERRER received the bachelor\u2019s degree in computer science and technical computer science, the master\u2019s degree in software engineering and artificial intelligence from the University of M\u00e1laga, and the Ph.D. degree in computer engineering from the University of M\u00e1laga, in 2016.\nHe obtained a national FPI (Training of Research Staff) fellowship. He also holds postgraduate certificate from the University of M\u00e1laga (Pedagogical Aptitude Certificate). His research\ninterests include optimization techniques, particularly with the use of artificial intelligence techniques applied to the fields of software engineering and, recently, of smart cities. His research works have materialized in publications in international journals (ten), book chapters (three), and international and national conference proceedings (28). Some of these works have been developed in collaboration with other research groups in countries, such as Germany, Austria, and Mexico. He has been a member of the research team in four national projects, three regional project, four project funded by the University of M\u00e1laga, and ten contracts with companies and institutions for transference of knowledge (OTRI). He also acts as a reviewer of prestigious international journals (ISI/JCR). His H-index is currently 12 with 503 cites to his works.\nFRANCISCO CHICANO received the degree in physics from the National Distance Education University and the Ph.D. degree in computer science from the University of M\u00e1laga. Since 2008, he has been with the Department of Languages and Computing Sciences, University of M\u00e1laga. His research interests include the application of search techniques to software engineering problems and the use of theoretical results to efficiently solve combinatorial optimization problems. He has also\nbeen the program chair and the editor-in-chief in international events. He is in the Editorial Board of Evolutionary Computation journal, Engineering Applications of Artificial Intelligence, Journal of Systems and Software, ACM Transactions on Evolutionary Learning and Optimization, andMathematical Problems in Engineering.\nENRIQUE ALBA received the degree in engineering and the Ph.D. degree in computer science from the University ofM\u00e1laga, Spain, in 1992 and 1999, respectively. He works as a Full Professor with the University of M\u00e1laga with varied teaching duties, such as data communications, distributed programming, software quality, and also evolutionary algorithms, bases for R+D+i and smart cities, both at graduate and master/doctoral programs. He leads an international team of researchers in\nthe field of complex optimization/learning with applications in smart cities, bioinformatics, software engineering, telecoms, and others. In addition to the organization of international events (ACM GECCO, IEEE IPDPS-NIDISC, IEEE MSWiM, IEEE DS-RT, and smart-CT), he has offered dozens postgraduate courses, more than 70 seminars in international institutions, and has directed many research projects (nine with national funds, seven in Europe, and numerous bilateral actions). Also, he has directed 12 projects for innovation in companies (OPTIMI, Tartessos, ACERINOX, ARELANCE, TUO, INDRA, AOP, VATIA, EMERGIA, SECMOTIC, ArcelorMittal, ACTECO, CETEM, and EUROSOTERRADOS) and has worked as an invited Professor at INRIA, Luxembourg, Ostrava, Scotland, Japan, Argentina, Cuba, Uruguay, and Mexico. He is an editor in several international journals and book series of Springer-Verlag andWiley, as well as he often reviews articles for more than 30 impact journals. He is included in the list of most prolific DBLP authors, and has published 130 articles in journals indexed by ISI, 11 books, and hundreds of communications to scientific conferences. He is included in the top five most relevant researchers in informatics in Spain (according to ISI), and is the most influent researcher of UMA in engineering (webometrics), with 14 awards to his professional activities. His H-index is 62, with more than 18,000 cites to his work.\n11656 VOLUME 10, 2022"
        }
    ],
    "title": "Automatizing Software Cognitive Complexity Reduction",
    "year": 2022
}