{
    "abstractText": "Undersampled MRI reconstruction is crucial for accelerating clinical scanning procedures. Recent deep learning methods for MRI reconstruction adopt CNN or ViT as backbone, which lack in utilizing the complementary properties of CNN and ViT. In this paper, we propose DuDoRNeXt, whose backbone hybridizes CNN and ViT in an domainspecific, intra-stage way. Besides our hybrid vertical layout design, we introduce domain-specific modules for dual-domain reconstruction, namely image-domain parallel local detail enhancement and k-space global initialization. We evaluate different conventions of MRI reconstruction including image-domain, k-space-domain, and dual-domain reconstruction with a reference protocol on the IXI dataset and an in-house multicontrast dataset. DuDoRNeXt achieves significant improvements over competing deep learning methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ziqi Gao"
        },
        {
            "affiliations": [],
            "name": "Kevin Zhou"
        }
    ],
    "id": "SP:2a5fe23956e964ec0f48622151eed73150fec36c",
    "references": [
        {
            "authors": [
                "E.Z. Chen",
                "T. Chen",
                "S. Sun"
            ],
            "title": "Mri image reconstruction via learning optimization using neural odes",
            "venue": "MICCAI 2020",
            "year": 2020
        },
        {
            "authors": [
                "M. Ding",
                "B. Xiao",
                "N. Codella",
                "P. Luo",
                "J. Wang",
                "L. Yuan"
            ],
            "title": "Davit: Dual attention vision transformers",
            "venue": "ECCV",
            "year": 2022
        },
        {
            "authors": [
                "P.L.K. Ding",
                "Z. Li",
                "Y. Zhou",
                "B. Li"
            ],
            "title": "Deep residual dense u-net for resolution enhancement in accelerated mri acquisition",
            "venue": "Medical Imaging 2019: Image Processing. vol. 10949, pp. 110\u2013117. SPIE",
            "year": 2019
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "ICLR",
            "year": 2021
        },
        {
            "authors": [
                "T. Eo",
                "Y. Jun",
                "T. Kim",
                "J. Jang",
                "H.J. Lee",
                "D. Hwang"
            ],
            "title": "Kiki-net: cross-domain convolutional neural networks for reconstructing undersampled magnetic resonance images",
            "venue": "Magnetic resonance in medicine 80(5), 2188\u20132201",
            "year": 2018
        },
        {
            "authors": [
                "C.M. Feng",
                "Y. Yan",
                "G. Chen",
                "Y. Xu",
                "Y. Hu",
                "L. Shao",
                "H. Fu"
            ],
            "title": "Multi-modal transformer for accelerated mr imaging",
            "venue": "IEEE Transactions on Medical Imaging",
            "year": 2022
        },
        {
            "authors": [
                "C.M. Feng",
                "Y. Yan",
                "H. Fu",
                "L. Chen",
                "Y. Xu"
            ],
            "title": "Task transformer network for joint mri reconstruction and super-resolution",
            "venue": "MICCAI",
            "year": 2021
        },
        {
            "authors": [
                "P. Guo",
                "Y. Mei",
                "J. Zhou",
                "S. Jiang",
                "V.M. Patel"
            ],
            "title": "Reconformer: Accelerated mri reconstruction using recurrent transformer",
            "venue": "arXiv preprint arXiv:2201.09376",
            "year": 2022
        },
        {
            "authors": [
                "P. Guo",
                "J.M.J. Valanarasu",
                "P. Wang",
                "J. Zhou",
                "S. Jiang",
                "V.M. Patel"
            ],
            "title": "Over-andunder complete convolutional rnn for mri reconstruction",
            "venue": "MICCAI",
            "year": 2021
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "CVPR",
            "year": 2016
        },
        {
            "authors": [
                "J. Hu",
                "L. Shen",
                "G. Sun"
            ],
            "title": "Squeeze-and-excitation networks",
            "venue": "CVPR",
            "year": 2018
        },
        {
            "authors": [
                "G. Huang",
                "Z. Liu",
                "L. Van Der Maaten",
                "K.Q. Weinberger"
            ],
            "title": "Densely connected convolutional networks",
            "venue": "CVPR",
            "year": 2017
        },
        {
            "authors": [
                "J. Huang",
                "Y. Fang",
                "Y. Wu",
                "H. Wu",
                "Z. Gao",
                "Y. Li",
                "J. Del Ser",
                "J. Xia",
                "G. Yang"
            ],
            "title": "Swin transformer for fast mri",
            "venue": "Neurocomputing 493, 281\u2013304",
            "year": 2022
        },
        {
            "authors": [
                "K.H. Jin",
                "M.T. McCann",
                "E. Froustey",
                "M. Unser"
            ],
            "title": "Deep convolutional neural network for inverse problems in imaging",
            "venue": "IEEE Transactions on Image Processing 26(9), 4509\u20134522",
            "year": 2017
        },
        {
            "authors": [
                "D. Lee",
                "J. Yoo",
                "S. Tak",
                "J.C. Ye"
            ],
            "title": "Deep residual learning for accelerated mri using magnitude and phase networks",
            "venue": "IEEE Transactions on Biomedical Engineering 65(9), 1985\u20131995",
            "year": 2018
        },
        {
            "authors": [
                "J. Liang",
                "J. Cao",
                "G. Sun",
                "K. Zhang",
                "L. Van Gool",
                "R. Timofte"
            ],
            "title": "Swinir: Image restoration using swin transformer",
            "venue": "WACV",
            "year": 2021
        },
        {
            "authors": [
                "Z. Liu",
                "Y. Lin",
                "Y. Cao",
                "H. Hu",
                "Y. Wei",
                "Z. Zhang",
                "S. Lin",
                "B. Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "ICCV",
            "year": 2021
        },
        {
            "authors": [
                "Z. Liu",
                "H. Mao",
                "C.Y. Wu",
                "C. Feichtenhofer",
                "T. Darrell",
                "S. Xie"
            ],
            "title": "A convnet for the 2020s",
            "venue": "CVPR",
            "year": 2022
        },
        {
            "authors": [
                "J. Lyu",
                "B. Sui",
                "C. Wang",
                "Y. Tian",
                "Q. Dou",
                "J. Qin"
            ],
            "title": "Dudocaf: Dual-domain crossattention fusion with recurrent transformer for fast multi-contrast mr imaging",
            "venue": "MICCAI. pp. 474\u2013484. Springer",
            "year": 2022
        },
        {
            "authors": [
                "N. Park",
                "S. Kim"
            ],
            "title": "How do vision transformers work",
            "year": 2022
        },
        {
            "authors": [
                "C. Peng",
                "W.A. Lin",
                "R. Chellappa",
                "S.K. Zhou"
            ],
            "title": "Towards multi-sequence mr image recovery from undersampled k-space data",
            "venue": "Medical Imaging with Deep Learning. pp. 614\u2013623. PMLR",
            "year": 2020
        },
        {
            "authors": [
                "C. Qin",
                "J. Schlemper",
                "J. Caballero",
                "A.N. Price",
                "J.V. Hajnal",
                "D. Rueckert"
            ],
            "title": "Convolutional recurrent neural networks for dynamic mr image reconstruction",
            "venue": "IEEE transactions on medical imaging 38(1), 280\u2013290",
            "year": 2018
        },
        {
            "authors": [
                "T.M. Quan",
                "T. Nguyen-Duc",
                "W.K. Jeong"
            ],
            "title": "Compressed sensing mri reconstruction using a generative adversarial network with a cyclic loss",
            "venue": "IEEE transactions on medical imaging 37(6), 1488\u20131497",
            "year": 2018
        },
        {
            "authors": [
                "O. Ronneberger",
                "P. Fischer",
                "T. Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "MICCAI. Springer",
            "year": 2015
        },
        {
            "authors": [
                "J. Schlemper",
                "J. Caballero",
                "J.V. Hajnal",
                "A. Price",
                "D. Rueckert"
            ],
            "title": "A deep cascade of convolutional neural networks for mr image reconstruction",
            "venue": "IPMI",
            "year": 2017
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "L. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "NeurIPS",
            "year": 2017
        },
        {
            "authors": [
                "S. Wang",
                "Z. Su",
                "L. Ying",
                "X. Peng",
                "S. Zhu",
                "F. Liang",
                "D. Feng",
                "D. Liang"
            ],
            "title": "Accelerating magnetic resonance imaging via deep learning",
            "venue": "ISBI",
            "year": 2016
        },
        {
            "authors": [
                "Z. Wang",
                "Y. Bai",
                "Y. Zhou",
                "C. Xie"
            ],
            "title": "Can cnns be more robust than transformers",
            "year": 2023
        },
        {
            "authors": [
                "L. Xiang",
                "Y. Chen",
                "W. Chang",
                "Y. Zhan",
                "W. Lin",
                "Q. Wang",
                "D. Shen"
            ],
            "title": "Ultra-fast t2-weighted mr reconstruction using complementary t1-weighted information",
            "venue": "MICCAI 2018",
            "year": 2018
        },
        {
            "authors": [
                "Y. Xu",
                "Q. Zhang",
                "J. Zhang",
                "D. Tao"
            ],
            "title": "Vitae: Vision transformer advanced by exploring intrinsic inductive bias",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "G. Yang",
                "S. Yu",
                "H. Dong",
                "G. Slabaugh",
                "P.L. Dragotti",
                "X. Ye",
                "F. Liu",
                "S. Arridge",
                "J. Keegan",
                "Y Guo"
            ],
            "title": "Dagan: deep de-aliasing generative adversarial networks for fast compressed sensing mri reconstruction",
            "venue": "IEEE transactions on medical imaging 37(6), 1310\u20131321",
            "year": 2017
        },
        {
            "authors": [
                "Q. Zhang",
                "Y. Xu",
                "J. Zhang",
                "D. Tao"
            ],
            "title": "Vitaev2: Vision transformer advanced by exploring inductive bias for image recognition and beyond",
            "venue": "International Journal of Computer Vision pp. 1\u201322",
            "year": 2023
        },
        {
            "authors": [
                "B. Zhou",
                "N. Dey",
                "J. Schlemper",
                "S.S.M. Salehi",
                "C. Liu",
                "J.S. Duncan",
                "M. Sofka"
            ],
            "title": "Dsformer: a dual-domain self-supervised transformer for accelerated multi-contrast mri reconstruction",
            "venue": "WACV",
            "year": 2023
        },
        {
            "authors": [
                "B. Zhou",
                "S.K. Zhou"
            ],
            "title": "Dudornet: learning a dual-domain recurrent network for fast mri reconstruction with deep t1 prior",
            "venue": "CVPR",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Keywords: Dual-domain MRI reconstruction \u00b7 Vision Transformer \u00b7 Neural network \u00b7 Hybrid model"
        },
        {
            "heading": "1 Introduction",
            "text": "Magnetic resonance imaging (MRI) is an non-invasive and flexible imaging modality widely used in clinical practice. A tricky problem in utilizing MRI is that complete K-space measurements lead to unbearable acquisition time while fewer measurements lead to aliasing and blurring in the image. Undersampled MRI reconstruction aims to reconstruct the high-quality, clean MRI image from its lowquality, aliased counterpart. Previously, Compressed Sensing (CS) and Parallel Imaging (PI) accelerate MRI reconstruction for 2-3 times. Since the revolutionary work[27], convolutional neural networks (CNN) have become the primary workhorse for undersampled MRI reconstruction.\nMany CNN-based methods [31,14,23] focus on elaborate architecture designs such as residual learning [10] and dense connections[12] stemming from UNet [3] or other existing baseline methods. Customization of conventional CNNs further benefited MRI reconstruction, including K-space data consistency (DC) [25,22], dual-domain recurrent learning [5,34], over-complete representation [9]. Recently, Transformer [26,4] has been considered as an alternative to CNN. Its Multihead Self-Attention (MSA) mechanism captures long-range interactions among\nar X\niv :2\n30 3.\n10 61\n1v 1\n[ ee\nss .I\nV ]\n1 9\nM ar\ncontexts globally or inside local windows [17]. As the success of Transformer is now indisputable in computer vision, Transformer has shown great potential for undersampled MRI reconstruction as well [13,33,8,7,19].\nYet performant, ViTs have not fully substituted CNNs as ViTs require a larger amount of training data due to a low inductive bias and have longer training schedules [30,32]. Furthermore, CNNs and ViTs have different emphases. Fig. 1 prevails their distinctive emphases on MRI reconstruction. DuDoRNet is a CNN model while Dual-SwinIR mostly consists of Swin Transformer blocks. DuDoRNet can extract small, isolated features better (the red one) while DualSwinIR generates sharper results for large structural details (the green one). Finally, DuDoRNet generates (sometimes wrong) details of higher frequency, which are seen especially in soft tissues; while Dual-SwinIR generates a smoother image. In recent works in decomposing Transformer from the basic theory [20] to empirical network design [18,28], a potential direction for modernizing deep learning models arises: hybridizing CNNs and ViTs. While several work [30,32,20] in computer vision show the effectiveness of hybrid structures, there is no research systematically studying a hybrid model for MRI reconstruction.\nIn our study, (1) we systematically study designing hybrid models for MRI reconstruction and propose a vertical layout design of hybrid MRI reconstruction models under a computational constraint; (2) we propose DuDoRNeXt with several domain-specific modules for MRI reconstruction, including imagedomain parallel local detail enhancement and k-space global initiation; and (3) we test our model on multiple settings of MRI reconstruction including guided by a reference protocol. The public IXI-dataset and in-house multi-contrast MRI\ndataset are used for evaluation and ablation study, respectively. The results show that our models exceed baseline comparison methods in all settings."
        },
        {
            "heading": "2 Method",
            "text": ""
        },
        {
            "heading": "2.1 Undersampled MRI Reconstruction",
            "text": "Let ku \u2208 Cmn and kf \u2208 Cmn be the undersampled and fully-sampled k-space signal respectively; iu \u2208 Cmn, if \u2208 Cmn, and ir \u2208 Cmn be the undersampled, fully-sampled and reconstructed image signal respectively; M \u2208 Rmn be the binary k-space mask for acceleration.\nThe undersampled MRI reconstruction can be formulated as an image recovery problem [27,9,1,15,13] with K-space DC as a regularisation term[25]:\narg min \u03b8i\n( \u2016if \u2212 Pi (iu; \u03b8i)\u201622 +\u03bb \u2016ku \u2212M F (Pi (iu; \u03b8i))\u2016 2 2 ) , (1)\nwhere F is 2D discrete Fourier Transform and the approximation function Px(\u00b7; \u03b8x) is used to predict a reconstructed signal xr given its parameter \u03b8x and any undersampled input. A few works [34,5,19] leverage the relationship between image and k-space domain using Fourier Transform pairs (F ,F\u22121) and transform MRI reconstruction into a multivariable optimization problem described as\narg min \u03b8i,\u03b8k\n( \u2016kf \u2212 Pk (F (Pi (iu; \u03b8i)) ; \u03b8k)\u201622 + \u2225\u2225if \u2212 Pi (F\u22121 (Pk (ku; \u03b8k)) ; \u03b8i)\u2225\u222522 + \u03bb\n\u2225\u2225ku \u2212M F (Pi (F\u22121 (Pk (ku; \u03b8k)) ; \u03b8i))\u2225\u222522) , (2)\nand solve it using a dual-domain recurrent learning strategy [34]. Recently, a growing number of works [21,29,6,34,33,19] utilize a fast-to-acquire fully-sampled auxiliary MRI protocol to guide the reconstruction of a slow protocol."
        },
        {
            "heading": "2.2 DuDoRNeXt: Towards Hybridizing CNNs and ViTs",
            "text": "Design motivation. The design of DuDoRNeXt is motivated by two findings: (i) ViTs benefit dual domain reconstruction with larger receptive fields and (ii) vertical layout design works better when hybridizing CNNs and ViTs in an intra-stage manner. The details behind these motivations are elaborated in supplementary materials. Architecture description. Compared with DuDoRNet[34], we improve dual domain recurrent blocks using an intra-stage CNN-ViT hybrid strategy and customize global and local structure based on domain-specific properties. Our model is illustrated in Figure 2, including domain-specific Shallow Feature Extraction (X-SFE), Global Feature Refinement(GFR) and 4-stage domain-specific hybrid\nbuilding blocks (X-BB) as backbone. Global residual learning and global feature fusion are preserved in our model. The overall pipeline goes as follow:\nF\u22121 = Conv 3(xu), F C 0 = Conv 3(F\u22121). (3)\nwhere Convk denotes a convolution operation with kernel size k \u2217 k and F\u22121 denotes the first extracted feature used for global residual learning. FC0 denotes the intermediate extracted feature by convolution. The second extracted feature F0 is the input of X-Si. For image domain, F0 = F C 0 . For K-space, a Global Initiation Module (K-GLIM) is introduced based on the idea of a sketchy K-space initiation: F0 = PK\u2212GLIM (F C 0 ) = PK\u2212GLIM (Conv\n3(F\u22121)). Global Feature Fusion (GFF) fuses features from X-BB\u2019s and is input for global residual learning:\nFi = PX\u2212Si(Fi \u2212 1), xr = PGFR(F\u22121 + PGFF (concat(F1, F2, F3, F4))), (4)\nwhere GFF consists of 1x1 and 3x3 convolutions and GFR consists of two 3x3 convolutions, creating the refined reconstructed image/K-space xr. X-BB: Domain-specific hybrid building block. The design of X-BB is shown in the lower right part of Figure 2. It is constructed by vertically stacking Dilated Residual Dense Block (DRDB) and Spatial Local Transformer Block followed by a Hybrid Local Feature Fusion (HLFF).\nFollowing DuDoRNet, we use DRDB for effective local feature extraction, while our DRDB composes a smaller feature pyramid using one less atrous convolution layers with dilation rate of 1, 2, and 4, expressed by: FCi = PDRDB(Fi\u22121), where FCi denotes convolutional features in the ith stage. Then, a Spatial Local Transformer Block is used to aggregate convolution features FCi in a large local window, whose main part is Windowed Multi-head Self-Attention (W-MSA):\nFH \u2032\ni = W \u2212MSA(LN(WinEmb(FCi , w))) +WinEmb(FCi , w), (5)\nFHi = WinUnemb(FFN(LN(F H\u2032 i ) + F H\u2032 i ), (6)\nwhere w is window size and set to 16 in our model. FHi \u2019s theoretical receptive field is 31, forming a larger and more dynamic scale pyramid than DuDoRNet. Then, convolution features FCi and hybrid features F H i is concatenated to Hybrid Local Feature Fusion (HLFF), adaptively fusing hybrid features with convolution and Squeeze-and-Excitation [11] layers. The final result is acquired using a local residual learning to the output of HLFF by adding the residual connection of X-BB\u2019s input:\nFi = PHLFF (concat(F C i , F H i )) + Fi\u22121, (7)\nand FCi has k0 + k \u2217 (l\u2212 1) feature maps, where k0, k and l denotes the number of channels in the first layer, growth rate and current layer number.\nDirectly feeding convolution features FCi to Transformer Block without tuning hidden dimension largely increase computation. To address this issue, we propose a domain-specific transition layer (X-TL) to compress c feature maps into b\u03b8cc using 1x1 convolution, where \u03b8 \u2208 (0, 1] differs for image and Kspace. In practice, we adopt (\u03b8I , \u03b8K) = (1, 2 3 ) since we observe a great reduction in performance after adding Transition Layer for image recovery network; while K-space recovery network benefits from K-TL with wider convolution layers, leading an richer representation of K-space feature in each convolutional layer. This corresponds to the result in DenseNet[12], where image-domain DenseNet\u2019s convolution layers can be very narrow. I-PLDE: Image parallel local detail enhancement. We notice that the vertical hybrid layout design leads to unsatisfactory performance in detail recovery. A possible reason is that MSAs at the end of each stage act as spatial smoothing and aggregation[20], thus neglecting details unavoidably. To this end, we propose the I-PLDE module, a parallel branch emphasizing local detail on the top of the vertical hybrid design, inspired by the \u201cdivide-and-conquer\u201d idea in [30,32].I-PLDE consists of a 1x1 convolution to match hidden dimension with its parallel branch, three stacked depth-wise convolution layers and an window embedding operation. SiLU is used for non-linear activation following the convention in [30,32]. The output of I-PLDE FCEi is added after W-MSA for preserving details. By including I-PLDE, the expression (5) is corrected into:\nFH \u2032\ni = W \u2212MSA(LN(WinEmb(FCi , w))) +WinEmb(FCi , w) + FCEi . (8)\nK-GLIM: K-space global initiation. Missing K-space recovery is an interpolation problem. MSAs of larger receptive fields are placed at the end of each stage, making it difficult for earlier layers\u2019 interpolation. To this end, we propose K-space global initialization module (K-GLIM) placing at the beginning of DuDoRNeXt. K-GLIM is applied on the transformed features of two convolution layers in SFE, providing channel-wise interaction and a global view. The main part of K-GLIM is a Channel-wise Multi-head Self-attention (C-MSA). Instead of performing pixel-level or patch-level attention in a conventional spatial attention way, C-MSA is performed on the transpose of pixel-level tokens. Similar\nto MSA, C-MSA is an extension of Self-Attention (C-SA) in which h times SA operations are run. C-SA can be expressed as zcj = \u2211 i Softmax ( QT K\u221a\nd ) i V Ti,j ,\nand its computational complexity is O(6(hw)C2), linear to (hw). C-MSA naturally captures global information and interactions for visual recognition tasks and complements (windowed-)spatial attention[2]. Compared with channel-wise convolution, it is data-specific and fine-grained."
        },
        {
            "heading": "3 Experiment",
            "text": ""
        },
        {
            "heading": "3.1 Settings and Results",
            "text": "Dataset and training. Our evaluation is carried out on the Multi-Contrast IXI dataset1. We use all 575 subjects with paired T2-PD and uniformly sample 14 slices from each subject volume. We split the dataset patient-wise into training, validation and testing set with a ratio of 7 : 1 : 2, corresponding to 5628 training images, 812 validation images, and 1610 test images each protocol. Images are edge-cropped from 256x256 to 224x224. Code is written in Pytorch and experiments are performed using an an NVIDIA GeForce RTX 3090 with 24GB memory. As for the rest, we follow the same experiment settings in DuDoRNet[34] including loss function. As a result, our experiments are run under the best setup of DuDoRNet. It is conceivable that more extensive hyper-parameter searches may further improve the performance of our hybrid model. Performance evaluation. We compare our methods with other baseline deep learning methods in three conventions of MRI reconstruction: image-domain[24,34,6,9,13], dual-domain[24,34,33] and reference-protocol-guided dual-domain reconstruction [29,6,34,33,19]. All reference-guided methods are self-implemented besides DuDoRNet and examined without considering multi-modal fusion modules for controlled backbone comparisons. We further unite these methods considering their backbones. [6,29] adopt Dense-Unet; [33,19,13] share similar Swin-Transformer backbones derived from SwinIR [16]. [33] proposed k-space filling using the reference protocol for de-aliasing initially in self-supervised reconstruction, yet it does not improve the model\u2019s performance when fully-supervised.\nAll models besides UNet [24] have \u2248 1M parameters by only tuning hidden dimensions. UNet has 2M parameters. All models are recurred twice and a DC is added at the end of each recurrent block. Recurrent time of local residual blocks in OUCR [9] is set to 5, complying with their default setting; yet DCs at the end of their local recurrent structure are discarded while those at the end of their global recurrent structure are preserved for fairness. All models are trained for 100 epochs and the hyperparameters of each method are tuned on the validation set with test data held-out for final evaluation. We consider Cartesian sampling pattern with acceleration rate ranging from 4 to 8; center sampling fraction Racs is set to 0.125. Peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) are used as the quantitative evaluation metrics. Results on undersampled MRI reconstruction. In Table 1, we demon-\n1 https://brain-development.org/ixi-dataset/, CC BY-SA 3.0 license\nstrated PD reconstruction evaluations using \u00d74, \u00d76, \u00d78 acceleration in three common approaches of MRI reconstruction: image-domain, dual-domain, and T2-guided dual-domain MRI reconstruction. The best results under the same setting and acceleration rate are colored with red. Our method achieves the best performances under all settings and acceleration rates. Notice that only the center region in K-space is sampled with \u00d78 acceleration, an extreme case to the disadvantage of K-space recovery networks. Witnessing the performance drops of other dual-domain models compared with their image-domain counterparts, DuDoRNeXt maintains its superior performance.\nResults on \u00d75, \u00d77 and qualitative comparisons with similar performances are provided in supplementary materials. Fig. 1 visualizes the reconstructed images from different models, with zoom-in comparisons."
        },
        {
            "heading": "3.2 Ablation Study",
            "text": "To isolate various components for our hybrid model, we carry out ablation study using an in-house MRI dataset with 20 patients with pre-aligned T1 and T2 under the setting of reference-guided (T1) dual-domain reconstruction. All the experiment settings are the same with those above. Hybrid strategy. Firstly, we evaluate our hybrid strategy from two perspectives: reconstruction performance and runtime. To evaluate hybrid strategy only, we compare our model without domain-specific design module with naive interstage hybrid models. The evaluation is summarized in Figure 4. Hybridizing CNN and ViT does improve performance even with a naive strategy while our hybrid strategy achieves the best in performance-runtime tradeoff.\nDual-domain hybrid structure and modules. Next, we verify our domainspecific designs. We evaluate five main components of our model, namely hybrid vertical design(HVL), HLFF, K-GLIM, I-PLDE, X-TL. For each component, we apply it on image-domain and dual-domain subsequently. If there is an performance drop in either way, we apply it on K-space only to observe the performance. The reason for this design is that we weigh the utility of image reconstruction network and the synergy between image and K-space reconstruction networks over K-space reconstruction. For K-GLIM and I-LDE, we try applying it to the other domain, both leading to performance drops. This demonstrates our design is domain-specific. For X-TL, \u03b8x is set to 1 by default, corresponding to the best choice for \u03b8i. As a result, only \u03b8k is modified in the last row. By gradually changing DuDoRNet to ours, our method also provides a possible hybridizing strategy for current CNN models. The effect of recurrent time is similar to DuDoRNet[34] and results are included in supplementary materials."
        },
        {
            "heading": "4 Conclusion",
            "text": "We propose a CNN-ViT hybrid model DuDoRNeXt for MRI reconstruction. By introducing domain-specific, intra-stage hybrid designs, DuDoRNeXt surpasses popular deep learning methods in three common settings of MRI reconstruction. The improvement over DuDoRNet provides a possible direction for improving current CNN or ViT models since we consider both effectiveness and efficiency. In the future, we will further validate our hybridizing strategy by transforming other baseline models. Future work also includes extending DuDoRNeXt from single-coil to multi-coil reconstruction under different sampling patterns."
        }
    ],
    "title": "DuDoRNeXt: A hybrid model for dual-domain undersampled MRI reconstruction",
    "year": 2023
}