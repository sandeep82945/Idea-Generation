{
    "abstractText": "Recently, theoretical analyses of deep neural networks have broadly focused on two directions: 1) Providing insight into neural network training by SGD in the limit of infinite hidden-layer width and infinitesimally small learning rate (also known as gradient flow) via the Neural Tangent Kernel (NTK), and 2) Globally optimizing the regularized training objective via cone-constrained convex reformulations of ReLU networks. The latter research direction also yielded an alternative formulation of the ReLU network, called a gated ReLU network, that is globally optimizable via efficient unconstrained convex programs. In this work, we interpret the convex program for this gated ReLU network as a Multiple Kernel Learning (MKL) model with a weighted data masking feature map and establish a connection to the NTK. Specifically, we show that for a particular choice of mask weights that do not depend on the learning targets, this kernel is equivalent to the NTK of the gated ReLU network on the training data. A consequence of this lack of dependence on the targets is that the NTK cannot perform better than the optimal MKL kernel on the training set. By using iterative reweighting, we improve the weights induced by the NTK to obtain the optimal MKL kernel which is equivalent to the solution of the exact convex reformulation of the gated ReLU network. We also provide several numerical simulations corroborating our theory. Additionally, we provide an analysis of the prediction error of the resulting optimal kernel via consistency results for the group lasso.",
    "authors": [
        {
            "affiliations": [],
            "name": "Rajat Vadiraj Dwaraknath"
        },
        {
            "affiliations": [],
            "name": "Tolga Ergen"
        },
        {
            "affiliations": [],
            "name": "Mert Pilanci"
        }
    ],
    "id": "SP:19cca6e63e72b1bd1057da2378b38d554d407be9",
    "references": [
        {
            "authors": [
                "Arthur Jacot",
                "Franck Gabriel",
                "Cl\u00e9ment Hongler"
            ],
            "title": "Neural tangent kernel: Convergence and generalization in neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Lenaic Chizat",
                "Francis Bach"
            ],
            "title": "A note on lazy training in supervised differentiable programming",
            "venue": "arXiv preprint arXiv:1812.07956,",
            "year": 2018
        },
        {
            "authors": [
                "Stanislav Fort",
                "Gintare Karolina Dziugaite",
                "Mansheej Paul",
                "Sepideh Kharaghani",
                "Daniel M Roy",
                "Surya Ganguli"
            ],
            "title": "Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Boris Hanin",
                "Mihai Nica"
            ],
            "title": "Finite depth and width corrections to the neural tangent kernel",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Sanjeev Arora",
                "Simon S Du",
                "Wei Hu",
                "Zhiyuan Li",
                "Russ R Salakhutdinov",
                "Ruosong Wang"
            ],
            "title": "On exact computation with an infinitely wide neural net",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Tolga Ergen",
                "Mert Pilanci"
            ],
            "title": "Convex geometry and duality of over-parameterized neural networks",
            "venue": "Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Mert Pilanci",
                "Tolga Ergen"
            ],
            "title": "Neural networks are convex regularizers: Exact polynomial-time convex optimization formulations for two-layer networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Aaron Mishkin",
                "Arda Sahiner",
                "Mert Pilanci"
            ],
            "title": "Fast convex optimization for two-layer relu networks: Equivalent model classes and cone decompositions",
            "venue": "arXiv preprint arXiv:2202.01331,",
            "year": 2022
        },
        {
            "authors": [
                "Arda Sahiner",
                "Tolga Ergen",
                "John M. Pauly",
                "Mert Pilanci"
            ],
            "title": "Vector-output relu neural network problems are copositive programs: Convex analysis of two layer networks and polynomial-time algorithms",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Tolga Ergen",
                "Mert Pilanci"
            ],
            "title": "Convex geometry of two-layer relu networks: Implicit autoencoding and interpretable models",
            "venue": "Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Tolga Ergen",
                "Mert Pilanci"
            ],
            "title": "Convex duality and cutting plane methods for over-parameterized neural networks",
            "venue": "In OPT-ML workshop,",
            "year": 2019
        },
        {
            "authors": [
                "Tolga Ergen",
                "Mert Pilanci"
            ],
            "title": "Convex programs for global optimization of convolutional neural networks in polynomial-time",
            "venue": "In OPT-ML workshop,",
            "year": 2020
        },
        {
            "authors": [
                "Tolga Ergen",
                "Mert Pilanci"
            ],
            "title": "Revealing the structure of deep neural networks via convex duality",
            "venue": "Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Yifei Wang",
                "Tolga Ergen",
                "Mert Pilanci"
            ],
            "title": "Parallel deep neural networks have zero duality gap",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "T. Ergen",
                "M. Pilanci"
            ],
            "title": "Convex optimization for shallow neural networks",
            "venue": "57th Annual Allerton Conference on Communication, Control, and Computing (Allerton),",
            "year": 2019
        },
        {
            "authors": [
                "Shai Shalev-Shwartz",
                "Ohad Shamir",
                "Shaked Shammah"
            ],
            "title": "Failures of gradient-based deep learning",
            "venue": "arXiv preprint arXiv:1703.07950,",
            "year": 2017
        },
        {
            "authors": [
                "Itay Safran",
                "Ohad Shamir"
            ],
            "title": "Spurious local minima are common in two-layer relu neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Rong Ge",
                "Jason D. Lee",
                "Tengyu Ma"
            ],
            "title": "Learning one-hidden-layer neural networks with landscape design, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Yifei Wang",
                "Jonathan Lacotte",
                "Mert Pilanci"
            ],
            "title": "The hidden convex optimization landscape of regularized two-layer reLU networks: an exact characterization of optimal solutions",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Ming Yuan",
                "Yi Lin"
            ],
            "title": "Model selection and estimation in regression with grouped variables",
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
            "year": 2006
        },
        {
            "authors": [
                "Lei Tan",
                "Shutong Wu",
                "Xiaolin Huang"
            ],
            "title": "Weighted neural tangent kernel: A generalized and improved network-induced kernel",
            "venue": "arXiv preprint arXiv:2103.11558,",
            "year": 2021
        },
        {
            "authors": [
                "Ziwei Ji",
                "Matus Telgarsky"
            ],
            "title": "Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow relu networks",
            "year": 1909
        },
        {
            "authors": [
                "Lenaic Chizat"
            ],
            "title": "Sparse optimization on measures with over-parameterized gradient descent",
            "venue": "Mathematical Programming,",
            "year": 2022
        },
        {
            "authors": [
                "Song Mei",
                "Andrea Montanari",
                "Phan-Minh Nguyen"
            ],
            "title": "A mean field view of the landscape of two-layer neural networks",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2018
        },
        {
            "authors": [
                "Rajat Vadiraj Dwaraknath"
            ],
            "title": "Understanding the neural tangent kernel",
            "venue": "https:// rajatvd.github.io/NTK/,",
            "year": 2019
        },
        {
            "authors": [
                "Simon Du",
                "Wei Hu"
            ],
            "title": "Ultra-wide deep nets and the neural tangent kernel (ntk)",
            "venue": "https://blog.ml.cmu.edu/2019/10/03/ultra-wide-deep-nets-and-theneural-tangent-kernel-ntk/,",
            "year": 2019
        },
        {
            "authors": [
                "Francis R Bach",
                "Gert RG Lanckriet",
                "Michael I Jordan"
            ],
            "title": "Multiple kernel learning, conic duality, and the smo algorithm",
            "venue": "In Proceedings of the twenty-first international conference on Machine learning,",
            "year": 2004
        },
        {
            "authors": [
                "Francis R Bach"
            ],
            "title": "The \u201ceta-trick\u201d reloaded: multiple kernel learning, 2019",
            "year": 2022
        },
        {
            "authors": [
                "Francis R Bach"
            ],
            "title": "Consistency of the group lasso and multiple kernel learning",
            "venue": "Journal of Machine Learning Research,",
            "year": 2008
        },
        {
            "authors": [
                "Ingrid Daubechies",
                "Ronald DeVore",
                "Massimo Fornasier",
                "C Sinan G \u00fcnt\u00fcrk"
            ],
            "title": "Iteratively reweighted least squares minimization for sparse recovery",
            "venue": "Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences,",
            "year": 2010
        },
        {
            "authors": [
                "Francis Bach",
                "Rodolphe Jenatton",
                "Julien Mairal",
                "Guillaume Obozinski"
            ],
            "title": "Optimization with sparsity-inducing penalties",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2012
        },
        {
            "authors": [
                "Julien Mairal",
                "Francis Bach",
                "Jean Ponce"
            ],
            "title": "Sparse modeling for image and vision processing",
            "venue": "arXiv preprint arXiv:1411.3230,",
            "year": 2014
        },
        {
            "authors": [
                "Francis R Bach"
            ],
            "title": "The \u201ceta-trick\u201d or the effectiveness of reweighted least-squares",
            "venue": "https://francisbach.com/the-%CE%B7-trick-or-the-effectiveness-ofreweighted-least-squares/,",
            "year": 2019
        },
        {
            "authors": [
                "Han Liu",
                "Jian Zhang"
            ],
            "title": "Estimation consistency of the group lasso and its applications",
            "venue": "In Artificial Intelligence and Statistics,",
            "year": 2009
        },
        {
            "authors": [
                "Peter B\u00fchlmann",
                "Sara van de Geer"
            ],
            "title": "Theory for the lasso. Statistics for High-Dimensional Data: Methods",
            "venue": "Theory and Applications,",
            "year": 2011
        },
        {
            "authors": [
                "Kurt Hornik",
                "Maxwell Stinchcombe",
                "Halbert White"
            ],
            "title": "Multilayer feedforward networks are universal approximators",
            "venue": "Neural networks,",
            "year": 1989
        },
        {
            "authors": [
                "Dario Azzimonti",
                "David Ginsbourger"
            ],
            "title": "Estimating orthant probabilities of high-dimensional gaussian vectors with an application to set estimation",
            "venue": "Journal of Computational and Graphical Statistics,",
            "year": 2018
        },
        {
            "authors": [
                "Peter Craig"
            ],
            "title": "A new reconstruction of multivariate normal orthant probabilities",
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
            "year": 2008
        },
        {
            "authors": [
                "Christopher C Paige",
                "Michael A Saunders"
            ],
            "title": "Lsqr: An algorithm for sparse linear equations and sparse least squares",
            "venue": "ACM Transactions on Mathematical Software (TOMS),",
            "year": 1982
        },
        {
            "authors": [
                "David Chin-Lung Fong",
                "Michael Saunders"
            ],
            "title": "Lsmr: An iterative algorithm for sparse leastsquares problems",
            "venue": "SIAM Journal on Scientific Computing,",
            "year": 2011
        },
        {
            "authors": [
                "Manuel Fern\u00e1ndez-Delgado",
                "Eva Cernadas",
                "Sen\u00e9n Barro",
                "Dinani Amorim"
            ],
            "title": "Do we need hundreds of classifiers to solve real world classification problems",
            "venue": "Journal of Machine Learning Research,",
            "year": 2014
        },
        {
            "authors": [
                "Song Mei",
                "Theodor Misiakiewicz",
                "Andrea Montanari"
            ],
            "title": "Generalization error of random feature and kernel methods: Hypercontractivity and kernel matrix concentration",
            "venue": "Applied and Computational Harmonic Analysis,",
            "year": 2022
        },
        {
            "authors": [
                "Corinna Cortes",
                "Mehryar Mohri",
                "Afshin Rostamizadeh"
            ],
            "title": "Generalization bounds for learning kernels",
            "venue": "International Conference on Machine Learning,",
            "year": 2010
        },
        {
            "authors": [
                "Tolga Ergen",
                "Halil I. Gulluk",
                "Jonathan Lacotte",
                "Mert Pilanci"
            ],
            "title": "Globally optimal training of neural networks with threshold activation functions",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Tolga Ergen",
                "Mert Pilanci"
            ],
            "title": "Implicit convex regularizers of cnn architectures: Convex optimization of two- and three-layer networks in polynomial time",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Arda Sahiner",
                "Tolga Ergen",
                "Batu Ozturkler",
                "Burak Bartan",
                "John M. Pauly",
                "Morteza Mardani",
                "Mert Pilanci"
            ],
            "title": "Hidden convexity of wasserstein GANs: Interpretable generative models with closed-form solutions",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Tolga Ergen",
                "Arda Sahiner",
                "Batu Ozturkler",
                "John M. Pauly",
                "Morteza Mardani",
                "Mert Pilanci"
            ],
            "title": "Demystifying batch normalization in reLU networks: Equivalent convex optimization models and implicit regularization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Vikul Gupta",
                "Burak Bartan",
                "Tolga Ergen",
                "Mert Pilanci"
            ],
            "title": "Convex neural autoregressive models: Towards tractable, expressive, and theoretically-backed models for sequential forecasting and generation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2021
        },
        {
            "authors": [
                "Tolga Ergen",
                "Behnam Neyshabur",
                "Harsh Mehta"
            ],
            "title": "Convexifying transformers: Improving optimization and understanding of transformer networks",
            "year": 2022
        },
        {
            "authors": [
                "Arda Sahiner",
                "Tolga Ergen",
                "Batu Ozturkler",
                "John Pauly",
                "Morteza Mardani",
                "Mert Pilanci"
            ],
            "title": "Unraveling attention via convex duality: Analysis and interpretations of vision transformers",
            "venue": "Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Kaare Brandt Petersen",
                "Michael Syskind Pedersen"
            ],
            "title": "The matrix cookbook",
            "venue": "Technical University of Denmark,",
            "year": 2008
        },
        {
            "authors": [
                "Erling D Andersen",
                "Knud D Andersen"
            ],
            "title": "The mosek interior point optimizer for linear programming: an implementation of the homogeneous algorithm",
            "venue": "In High performance optimization,",
            "year": 2000
        },
        {
            "authors": [
                "Tolga Ergen",
                "Mert Pilanci"
            ],
            "title": "Global optimality beyond two layers: Training deep relu networks via convex programs",
            "venue": "Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Neural Networks (NNs) have become popular in various machine learning applications due to their remarkable modeling capabilities and generalization performance. However, their highly nonlinear and non-convex structure precludes an effective theoretical analysis. Therefore, developing theoretical tools to understand the fundamental mechanisms behind neural networks is still an active research topic. To tackle this problem, [1] studied the training dynamics of neural networks trained with Stochastic Gradient Descent (SGD) in a regime where each layer has infinitely many neurons and SGD uses an infinitesimally small learning rate, i.e., gradient flow. Thus, they related the training dynamics of neural networks to the training dynamics of a fixed kernel called the Neural Tangent Kernel (NTK). However, [2] showed that neurons barely move from their initial values in this regime so that neural networks fail to learn useful features from the training data. This is in contrast to their finite width counterparts, which are able to learn predictive features in practice [3]. Moreover, [4, 5] provided further theoretical and empirical evidence to show that existing kernel approaches are not able to explain the remarkable performance of finite width networks. Therefore, although NTK and\n\u2217Institute for Computational and Mathematical Engineering\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\nar X\niv :2\n30 9.\n15 09\n6v 1\n[ cs\n.L G\n] 2\n6 Se\np 20\nsimilar kernel based approaches enable theoretical analysis unlike standard finite width networks, they fail to explain the effectiveness of finite width neural networks that are employed in practice.\nRecently a series of papers [6\u201315] introduced an analytic framework to analyze finite width neural networks by leveraging certain convex duality arguments. Particularly, they showed that the standard regularized non-convex training problem can be equivalently cast as a finite dimensional convex program. This convex approach has two major advantages over standard non-convex training: (1) Since the training objective is convex, one can find globally optimal parameters of the network efficiently and reliably unlike standard nonconvex training which can get stuck at a local minimum, and (2) As we show in this work, a class of convex reformulations can be interpreted as an instance of Multiple Kernel Learning (MKL) [16] which allows us to characterize the corresponding finite width networks by a learned data-dependent kernel that can be iteratively computed. This is in contrast to the infinite-width kernel characterization in which the NTK stays constant throughout training.\nNotation and Preliminaries. In the paper, we use lowercase and uppercase bold letters to denote vectors and matrices respectively. We also use subscripts to denote a certain column or element. We denote the identity matrix of size k \u00d7 k as Ik. To denote the set {1, 2, . . . , n}, we use [n]. We also use \u2225\u00b7\u2225p and \u2225\u00b7\u2225F to represent the standard \u2113p and Frobenius norms. Additionally, we denote 0-1 valued indicator function and ReLU activation as 1 {x \u2265 0} and (x)+ := max{x, 0}, respectively. In this paper, we focus on analyzing the regularized training problem of ReLU networks. Particularly, we consider a two-layer ReLU network with m neurons whose output function is defined as follows\nf(x,\u03b8) := m\u2211 j=1 ( xTw (1) j ) + w (2) j = m\u2211 j=1 ( 1 { xTw (1) j \u2265 0 } xTw (1) j ) w (2) j . (1)\nwhere w(1)j \u2208 Rd and w (2) j are the j th hidden and output layer weights, respectively and \u03b8 := {(w(1)j , w (2) j )}mj=1 represents all trainable parameters. Given a training data matrix X \u2208 Rn\u00d7d and a target vector y \u2208 Rn, we minimize the following weight decay regularized training objective\nmin W(1),w(2) \u2225\u2225\u2225\u2225\u2225\u2225 m\u2211 j=1 ( Xw (1) j ) + w (2) j \u2212 y \u2225\u2225\u2225\u2225\u2225\u2225 2\n2\n+ \u03bb m\u2211 j=1 (\u2225\u2225\u2225w(1)j \u2225\u2225\u22252 2 + |w(2)j | 2 ) , (2)\nwhere \u03bb > 0 is the regularization coefficient. We also use f(X,\u03b8) = \u2211m\nj=1\n( Xw\n(1) j ) + w (2) j for\nconvenience. We discuss extensions to generic loss and deeper architectures in appendix."
        },
        {
            "heading": "Our Contributions.",
            "text": "\u2022 In section 4, we show that the convex formulation of the Gated ReLU network is equivalent to Multiple Kernel Learning with a specific set of Masking Kernels. (Theorem 4.3).\n\u2022 In section 5, we connect this formulation to the Neural Tangent Kernel by showing that the NTK is a specific weighted combination of masking kernels (Theorem 5.1).\n\u2022 In Corollary 5.2, we show that on the training set, the NTK is suboptimal when compared to the optimal kernel learned by MKL, which is equivalent to the model learnt by our convex Gated ReLU program.\n\u2022 In section 6, we also derive bounds on the prediction error of this optimal kernel and specify how to choose the regularization parameter \u03bb (Theorem 6.1)."
        },
        {
            "heading": "2 Convex Optimization and the NTK",
            "text": "Here, we briefly review the literature on convex training and the NTK theory of neural networks."
        },
        {
            "heading": "2.1 Convex Programs for ReLU Networks",
            "text": "Even though the network in (1) has only two layers, previous studies show that (2) is a challenging optimization problem due to the non-convexity of the objective function. Thus, local search heuristics such as SGD might fail to globally optimize the training objective [17\u201320]. To eliminate the issues\nassociated with the inherent non-convexity, [7] introduced an exact convex reformulation of (2) as the following constrained optimization problem\nmin wi,w\u2032i \u2225\u2225\u2225\u2225\u2225 p\u2211\ni=1\nDiX(wi \u2212w\u2032i)\u2212 y \u2225\u2225\u2225\u2225\u2225 2\n2\n+ \u03bb p\u2211 i=1 (\u2225wi\u22252 + \u2225w \u2032 i\u22252) s.t. (2Di \u2212 I)Xwi \u2265 0 (2Di \u2212 I)Xw\u2032i \u2265 0 ,\u2200i, (3)\nwhere Di \u2208 DX are n\u00d7 n binary masking diagonal matrices given by p := |DX| and DX := { diag (1 {Xu \u2265 0}) : u \u2208 Rd } . (4)\nThe mask setDX can be interpreted as the set of all possible ways to separate the training data X by a hyperplane passing through the origin. With these masks, we can characterize a single ReLU activated neuron on the training data as follows: (Xwi)+ = diag (1 {Xwi \u2265 0})Xwi = DiXwi provided that (2Di \u2212 In)Xwi \u2265 0. Therefore, by enforcing these cone constraints in (3), we maintain the masking property of the ReLU activation and parameterize the neural network as a linear function of the weights, thus make the learning problem convex. We refer the reader to [6] for more details.\nAlthough (3) is convex and therefore eliminates the drawbacks associated with the non-convexity of (2), it might still be computationally complex to solve. Precisely, a worst-case upper-bound on the number of variables is O(nr), where r = rank(X) \u2264 min{n, d}. Although this is still significantly better than brute-force search over 2mn ReLU patterns, it could still be exponential in the dimension d. To mitigate this issue, [8] proposed a relaxation of (1) called the gated ReLU network\nfG (x,\u03b8) := m\u2211 j=1 ( 1 { xTgj \u2265 0 } xTw (1) j ) w (2) j , (5)\nwhere G := {gj}mj=1 is a set of gate vectors that are also optimized throughout training. Then, the corresponding non-convex learning problem is as follows\nmin W(1),w(2),G \u2225\u2225\u2225\u2225\u2225\u2225 m\u2211 j=1 diag (1 {Xgj \u2265 0})Xw(1)j w (2) j \u2212 y \u2225\u2225\u2225\u2225\u2225\u2225 2\n2\n+ \u03bb m\u2211 j=1 (\u2225\u2225\u2225w(1)j \u2225\u2225\u22252 2 + |w(2)j | 2 ) . (6)\nBy performing this relaxation, we decouple the dependence between the indicator function and the linear term in the exact ReLU network (1). To express the equivalent convex optimization problem corresponding to the gated ReLU network, we introduce the notion of complete gate sets. Definition 2.1. A gate set G is complete with respect to a dataset X if the corresponding set of hyperplane arrangements covers all possible arrangement patterns for X defined in (4), i.e.,\n{diag (1 {Xgj \u2265 0}) : gj \u2208 G} = DX. Additionally, G is minimally complete if |G| = |DX| = p.\nNow, with this relaxation, [8] showed that the optimal value for (6) can always be achieved by choosing G to be a complete gate set. Therefore, by working only with complete gate sets, we can modify (6) to only optimize over the network parameters W(1) and w(2)\nmin W(1),w(2) \u2225\u2225\u2225\u2225\u2225\u2225 m\u2211 j=1 diag (1 {Xgj \u2265 0})Xw(1)j w (2) j \u2212 y \u2225\u2225\u2225\u2225\u2225\u2225 2\n2\n+ \u03bb m\u2211 j=1 (\u2225\u2225\u2225w(1)j \u2225\u2225\u22252 2 + |w(2)j | 2 ) . (7)\nAdditionally, [21] also showed that we can set m = p without loss of generality. Then, [8] showed that the equivalent convex optimization problem for (6) and also (7) in the complete gate set setting is\nmin wi \u2225\u2225\u2225\u2225\u2225 p\u2211\ni=1\nDiXwi \u2212 y \u2225\u2225\u2225\u2225\u2225 2\n2\n+ \u03bb p\u2211 i=1 \u2225wi\u22252 . (8)\nNotice that (8) is a least squares problem with group lasso regularization [22]. Therefore, this relaxation for (3) can be efficiently optimized via convex optimization solvers. Furthermore, [8] proved that after solving the relaxed problem (8), one can construct an equivalent ReLU network from a gated ReLU network via a convex optimization based procedure called cone decomposition. We discuss the computational complexity of this approach in Section E of the supplementary material."
        },
        {
            "heading": "2.2 The Neural Tangent Kernel",
            "text": "Previous works [1, 2, 5, 23, 24] characterized the training dynamics of SGD with infinitesimally small learning rates on neural networks in the infinite-width limit, i.e., as m \u2192 \u221e, via the NTK. [25, 26] also present analyses in this regime via a mean-field approach. In this section, we provide a brief overview of this theory and refer the reader to [1, 27, 28] for more details. The main idea behind the NTK theory is to approximate the neural network model function f (x,\u03b8) by linearizing it with respect to the parameters \u03b8 around its initialization \u03b80:\nf\u0302 (x,\u03b8) \u2248 f (x,\u03b80) +\u2207\u03b8f (x,\u03b80)T (\u03b8 \u2212 \u03b80) .\nThe authors of [1] show that if f is a neural network (with appropriately scaled output), and parameters \u03b8 initialized as i.i.d standard Gaussians, the linearization f\u0302 better approximates f in the infinite-width limit. We can interpret the linearized model f\u0302 as a kernel method with a feature map given by \u03d5 (x) = \u2207\u03b8f (x,\u03b80). The corresponding kernel induced by this feature map is termed as the NTK. Note that this is a random kernel since it depends on the random initialization of the parameters denoted as \u03b80. The main result of [1] in the simplified case of two-layer neural networks is that, in the infinite-width limit this kernel approaches a fixed deterministic limit given by H ( x,x\u2032 ) := E [ \u2207\u03b8\u0302f ( x, \u03b8\u03020\n)T\u2207\u03b8\u0302f(x\u2032, \u03b8\u03020)], where \u03b8\u0302 corresponds to the parameters of a single neuron. Furthermore, [1] show that in this infinite limit, SGD with an infinitesimally small learning rate is equivalent to performing kernel regression with the fixed NTK.\nTo link the convex formulation (8) with NTK theory, we first present a scaled version of the gated ReLU network in (5) as follows\nf\u0303G (x,\u03b8) := 1\u221a 2m m\u2211 j=1 ( 1 { xTgj \u2265 0 } xTw (1) j ) w (2) j . (9)\nIn the next lemma, we provide the infinite width NTK of the scaled gated ReLU network in (9).\nLemma 2.2. 2 The infinite width NTK of the gated ReLU network (9) with i.i.d gates sampled as gj \u223c N (0, Id) and randomly initialized parameters as w(1)j \u223c N (0, Id) and w (2) j \u223c N (0, 1) is\nH ( x,x\u2032 ) := 1\n2\u03c0\n( \u03c0 \u2212 arccos ( xTx\u2032\n\u2225x\u22252 \u2225x\u2032\u22252\n)) xTx\u2032. (10)\nAdditionally, we introduce a reparameterization of the standard ReLU network (1) with \u03b8 :={( w\n(+) j ,w (\u2212) j )}m j=1 , with w(+)j , w (\u2212) j \u2208 Rd which can still represent all the functions that (1)\ncan\nfr(x,\u03b8) := 1\u221a 2m m\u2211 j=1 ( xTw (+) j ) + \u2212 ( xTw (\u2212) j ) + . (11)\nLemma 2.3. The gated ReLU network (9) and the reparameterized ReLU network (11) have the same infinite width NTK given by Lemma 2.2.\nNext, we present an equivalence between the gated ReLU network and the MKL model [16, 29]."
        },
        {
            "heading": "3 Multiple Kernel Learning and Group Lasso",
            "text": "The Multiple Kernel Learning (MKL) model [16, 29] is an extension of the standard kernel method that learns an optimal data-dependent kernel as a convex combination of a set of fixed kernels and then performs regression with this learned kernel. We provide a brief overview of the MKL setting based on the exposition in [30]. Consider a set of p kernels given by corresponding feature maps \u03d5i : Rd \u2192 Rdi . Given n training samples X \u2208 Rn\u00d7d with targets y \u2208 Rn, we define the feature matrices on this data by stacking the feature vectors as \u03a6i := [\u03d5i(x1)T ; . . . ;\u03d5i(xn)T ] \u2208 Rn\u00d7di . Then, the corresponding n\u00d7 n kernel matrices are given by Ki := \u03a6i\u03a6Ti . A convex combination of these kernels can be written as K (\u03b7) := \u2211p i=1 \u03b7iKi where \u03b7 \u2208 \u2206p := {\u03b7 : 1T\u03b7 = 1, \u03b7 \u2265 0} is a set of weights in the unit simplex. By noticing that the feature map corresponding to K (\u03b7)\n2All the proofs and derivations are presented in the supplementary material.\nis obtained by taking a weighted concatenation of \u03d5i with weights \u221a \u03b7i, we can write the MKL optimization problem in terms of the feature matrices as\nmin \u03b7\u2208\u2206p,vi\u2208Rdi \u2225\u2225\u2225\u2225\u2225 p\u2211\ni=1\n\u221a \u03b7i\u03a6ivi \u2212 y \u2225\u2225\u2225\u2225\u2225 2\n2\n+ \u03bb\u0302 p\u2211 i=1 \u2225vi\u222522 , (12)\nwhere \u03bb\u0302 > 0 is a regularization coefficient. For a set of fixed weights \u03b7, the optimal objective value of the kernel regression problem over v is proportional to yT ( \u2211p i=1 \u03b7iKi + \u03bb\u0302In)\n\u22121y up to constant factors [16]. Thus, the MKL problem can be equivalently written as the following problem\nmin \u03b7\u2208\u2206p\nyT ( K (\u03b7) + \u03bb\u0302In )\u22121 y. (13)\nIn this formulation, we can interpret MKL as finding the optimal data-dependent kernel that can be expressed as a convex combination of the fixed kernels given by Ki. In the next section, we link this kernel learning formulation with the convex group lasso problem in (8)."
        },
        {
            "heading": "3.1 Equivalence to Group Lasso",
            "text": "We first show that the MKL problem in (12) can be equivalently stated as a group lasso problem.\nLemma 3.1 ([16, 29]). The MKL problem (12) is equivalent to the following kernel regression problem using a uniform combination of the fixed kernels with squared group lasso regularization where the groups are given by parameters corresponding to each feature map\nmin wi\u2208Rdi \u2225\u2225\u2225\u2225\u2225 p\u2211\ni=1\n\u03a6iwi \u2212 y \u2225\u2225\u2225\u2225\u2225 2\n2\n+ \u03bb\u0302\n( p\u2211\ni=1\n\u2225wi\u22252\n)2 . (14)\nWe now present a short derivation of this equivalence. Using the variational formulation of the squared group \u21131-norm [31] (\np\u2211 i=1 \u2225wi\u22252\n)2 = min\n\u03b7\u2208\u2206p p\u2211 i=1 \u2225wi\u222522 \u03b7i ,\nwe can rewrite the group lasso problem (14) as a joint minimization problem over both the parameters w and regularization weights \u03b7 as follows\nmin \u03b7\u2208\u2206p min wi\u2208Rdi \u2225\u2225\u2225\u2225\u2225 p\u2211\ni=1\n\u03a6iwi \u2212 y \u2225\u2225\u2225\u2225\u2225 2\n2\n+ \u03bb\u0302 p\u2211 i=1 \u2225wi\u222522 \u03b7i .\nFinally, with a change of variables given by vi = wi/ \u221a \u03b7i, we recover the MKL problem (12). We note that the MKL problem (12) is also equivalent to the following standard group lasso problem\nmin wi\u2208Rdi \u2225\u2225\u2225\u2225\u2225 p\u2211\ni=1\n\u03a6iwi \u2212 y \u2225\u2225\u2225\u2225\u2225 2\n2\n+ \u03bb p\u2211 i=1 \u2225wi\u22252 . (15)\nThis is due to the fact that squared and standard group lasso problems have the same regularization paths [31], so (14) and (15) are equivalent when \u03bb\u0302 = \u03bb\u2211p\ni=1\u2225w\u2217i \u22252 , where w\u2217 is the solution to (14)."
        },
        {
            "heading": "3.2 Solving Group Lasso by Iterative Reweighting",
            "text": "Previously, we used a variational formulation of the squared group \u21131-norm to show equivalences to MKL. Now, we present the Iteratively Reweighted Least Squares (IRLS) algorithm [32\u201335] to solve the group lasso problem (15) using the following variational formulation of the group \u21131-norm [34]\np\u2211 i=1 \u2225wi\u22252 = min \u03b7\u2208Rp+ 1 2 p\u2211 i=1 ( \u2225wi\u222522 \u03b7i + \u03b7i ) .\nBased on this, we rewrite the group lasso problem (15) as the following minimization problem\nmin \u03b7\u2208Rp+ min wi\u2208Rdi\n\u2225\u2225\u2225\u2225\u2225 p\u2211\ni=1\n\u03a6iwi \u2212 y \u2225\u2225\u2225\u2225\u2225 2\n2\n+ \u03bb\n2 p\u2211 i=1 ( \u2225wi\u222522 \u03b7i + \u03b7i ) .\nSince the objective is jointly convex in (\u03b7,w), it can be solved using alternating minimization [33]. Particularly, note that the inner minimization problem in wi\u2019s is simply a \u21132 regularized least squares problem with different regularization strengths for each group and this can be solved in closed form\nmin wi\u2208Rdi \u2225\u2225\u2225\u2225\u2225 p\u2211\ni=1\n\u03a6iwi \u2212 y \u2225\u2225\u2225\u2225\u2225 2\n2\n+ \u03bb p\u2211 i=1 \u2225wi\u222522 \u03b7i . (16)\nThe outer problem in \u03b7 is also directly solved by setting \u03b7i = \u2225wi\u22252 [34]. To avoid convergence issues and instability around \u03b7i = 0, we approximate the reweighting by adding a small positive constant \u03f5. We use this procedure to solve the group lasso formulation of the gated ReLU network (8) by setting \u03a6i = DiX. A detailed description is provided Algorithm 1. For further details regarding convergence, we refer the reader to [32\u201334].\nAlgorithm 1 Iteratively Reweighted Least Squares (IRLS) for gated ReLU and ReLU networks 1: Set iteration count k \u2190 0 2: Initialize weights \u03b7(0)i 3: Set \u03a6i := DiX, \u2200Di \u2208 DX 4: while not converged and k \u2264 max iteration count do 5: Solve the weighted \u21132 regularized least squares problem:{\nw (k) i } i = argmin\n{wi}i\n\u2225\u2225\u2225\u2225\u2225 p\u2211\ni=1\n\u03a6iwi \u2212 y \u2225\u2225\u2225\u2225\u2225 2\n2\n+ \u03bb p\u2211 i=1 \u2225wi\u222522 \u03b7 (k) i\n6: Update the weights: \u03b7(k+1)i = \u221a\u2225\u2225\u2225w(k)i \u2225\u2225\u2225\n2 + \u03f5\n7: Increment iteration count: k \u2190 k + 1 8: end while 9: Optional: Convert the gated ReLU network to a ReLU network (see Section E for details)"
        },
        {
            "heading": "4 Gated ReLU as MKL with Masking Kernels",
            "text": "Motivated by the MKL interpretation of group lasso, we return to the convex reformulation (8) of the gated ReLU network. Notice that this problem has the same structure as the MKL equivalent group lasso problem (15) with a specific set of feature maps that we define below. Definition 4.1. The masking feature maps \u03d5j : Rd \u2192 Rd generated by a fixed set of gates G are defined as \u03d5j(x) = 1 { xTgj \u2265 0 } x.\nThese feature maps can be interpreted as simply passing the input unchanged if it lies in the positive halfspace of the corresponding gate vector gj , i.e., xTgj \u2265 0, and returning zero if the input does not lie in this halfspace. Since diag (1 {Xgj \u2265 0}) \u2208 DX, \u2200 gj \u2208 G holds for an arbitrary gate set G, we can conveniently express the corresponding feature matrices of these masking feature maps on the data X in terms of fixed diagonal data masks as \u03a6j = DjX, where Dj = diag (1 {Xgj \u2265 0}). Similarly, the corresponding masking kernel matrices take the form Kj = DjXXTDj . Note that for an arbitrary set of gates, the generated masking feature matrices on X may not cover the entire set of possible masks DX. Additionally, multiple gate vectors can result in identical masks if diag (1 {Xgi \u2265 0}) = diag (1 {Xgj \u2265 0}) for i \u0338= j leading to degenerate feature matrices. However, if we work with minimally complete gate sets as defined in Definition 2.1, we can rectify these issues. Lemma 4.2. For a minimally complete gate set G defined in Definition 2.1, we can uniquely associate a gate vector gi to each data mask Di \u2208 DX such that Di = diag (1 {Xgi \u2265 0}) ,\u2200i \u2208 [p].\nConsequently, for minimally complete gate sets G, the generated masking feature matrices \u2200i \u2208 [p] can be expressed as \u03a6i = DiX and the masking kernel matrices take the form Ki = DiXXTDi. In the context of the gated ReLU problem (7), since G is complete, we can replace it with a minimally complete subset of G without loss of generality since [21] showed that increasing m beyond p cannot reduce the value of the regularized training objective in (7). We are now ready to combine the MKL-group lasso equivalence with the convex reformulation of the gated ReLU network to present the following characterization of the nonconvex gated ReLU learning problem. Theorem 4.3. The non-convex gated ReLU problem (7) with a minimally complete gate set G is equivalent to performing multiple kernel learning (12) with the masking feature maps generated by G\nmin \u03b7\u2208\u2206p,vi\u2208Rd \u2225\u2225\u2225\u2225\u2225 p\u2211\ni=1\n\u221a \u03b7iDiXvi \u2212 y \u2225\u2225\u2225\u2225\u2225 2\n2\n+ \u03bb\u0302 p\u2211 i=1 \u2225vi\u222522 .\nThis theorem implies that the gated ReLU network finds the optimal combination of linear models restricted to the different masked datasets DiX generated by the gates. By optimizing with all possible data maskings, we obtain the best possible gated ReLU network. From the kernel perspective, we have characterized the problem of finding an optimal finite width gated ReLU network as learning a data-dependent kernel and then performing kernel regression. This is in contrast to the NTK theory where the training of an infinite width network by gradient flow is characterized by regression with a constant kernel that is not learned from data. We further explore this connection below."
        },
        {
            "heading": "5 NTK as a Weighted Masking Kernel",
            "text": "We now connect the NTK of a gated ReLU network with the masking kernels generated by its gates. Theorem 5.1. Let KG (\u03b7\u0303) \u2208 Rn\u00d7n be the weighted masking kernel obtained by taking a convex combination of the masking feature maps generated by a minimally complete gate set G with weights given by \u03b7\u0303i = P[diag (1 {Xh \u2265 0}) = Di] where h \u223c N (0, Id) and let H \u2208 Rn\u00d7n be the infinite width NTK of the gated ReLU network (10) evaluated on the training data, i.e., the ijth entry of H is defined as Hij := H(xi,xj). Then, KG (\u03b7\u0303) = H.\nA rough sketch of the proof of this theorem is to express the matrix H as an expectation of indicator random variables using the definition of the NTK [1]. Then, by conditioning on the event that these indicators equal the masks Di, we can express the NTK as a convex combination of the masking kernels Ki. The weights end up being precisely the probabilities that are described in Theorem 5.1. A detailed proof is provided in the supplementary material.\nThis theorem implies that the outputs of the gated ReLU network obtained via (16) with regularization weights \u03b7\u0303 on the training data is identical to that of kernel ridge regression with the NTK. Corollary 5.2. Let w\u0303 be the solution to (16) with feature matrices \u03a6i = DiX and regularization weights \u03b7\u0303i = P[diag (1 {Xh \u2265 0}) = Di] where h \u223c N (0, Id) and let y\u0303 = H(H+ \u03bbIn)\u22121y be the outputs of kernel ridge regression with the NTK on the training data. Then, \u2211p i=1 DiXw\u0303i = y\u0303.\nSince G is minimally complete, the weights in Theorem 5.1 satisfy \u2211p\ni=1 \u03b7\u0303i = 1. In other words, \u03b7\u0303 \u2208 \u2206p. Therefore, we can interpret Theorem 5.1 as follows \u2013 the NTK evaluated on the training data lies in the convex hull of all possible masking kernels of the training data. So H lies in the feasible set of kernels for MKL using these masking kernels. By Theorem 4.3, we can find the optimal kernel in this set by solving the group lasso problem (8) of the gated ReLU network. Therefore, we can interpret solving (8) as fixing the NTK by learning an improved data-dependent kernel. Remark 5.3 (Suboptimality of NTK). Note that the weights \u03b7\u0303 do depend on the training data X, but do not depend on the target labels y. Since MKL learns the optimal kernel using both X and y, the NTK still cannot perform better than the optimal MKL kernel on the training set. Thus, we fix the NTK."
        },
        {
            "heading": "6 Analysis of Prediction Error",
            "text": "In this section, we present an analysis of the in-sample prediction error for the gated ReLU network given in (5) along the lines of existing consistency results [36, 37] for the group lasso problem (8).\nWe assume that the data is generated by a noisy ReLU neural network model y = f(X,\u03b8\u2217) + \u03f5 where f is the ReLU network defined in (1) with true parameters \u03b8\u2217 and the noise is distributed as \u03f5 \u223c N ( 0, \u03c32In ) . By the seminal universal approximation theorem of [38], this model is able to capture a broad class of ground-truth functions by using a ReLU network with enough neurons. We can transform \u03b8\u2217 to the weights w\u2217 in the convex ReLU model and write f(X,\u03b8\u2217) = \u2211p i=1 DiXw \u2217 i . We denote by w\u0302 the solution of the group lasso problem (8) for the gated ReLU network with an additional 1n factor on the loss to simplify derivations,\nw\u0302 = argmin wi\u2208Rd\n1\nn \u2225\u2225\u2225\u2225\u2225 p\u2211\ni=1\nDiXwi \u2212 y \u2225\u2225\u2225\u2225\u2225 2\n2\n+ \u03bb p\u2211 i=1 \u2225wi\u22252 . (17)\nWe now present our main theorem which bounds the prediction error of the gated ReLU network obtained from the solution w\u0302 below.\nTheorem 6.1 (Prediction Error of Gated ReLU). For some t > 0, let the regularization parameter in (17) be \u03bb = t\u03c3\u2225X\u2225F /n. Then, with probability at least 1\u2212 2e\u2212t 2/8, we have\n1\nn \u2225\u2225\u2225fG(X, \u03b8\u0302)\u2212 f (X,\u03b8\u2217)\u2225\u2225\u22252 2 \u2264 2\u03bb p\u2211 i=1 \u2225w\u2217i \u22252\nwhere fG ( X, \u03b8\u0302 ) = \u2211p i=1 DiXw\u0302i are the predictions of the gated ReLU network obtained from w\u0302.\nThe proof closely follows the analysis of the regular lasso problem presented in [37], but we extend it to the specific case of the group lasso problem corresponding to the gated ReLU network and leverage the masking structure of the lifted data matrix to obtain simplified bounds."
        },
        {
            "heading": "7 Experiments",
            "text": "Here, we empirically corroborate our theoretical results via experiments on several datasets.3\n3We provide additional details in Section B.\n1D datasets. For the 1D experiments in Figure 1, we add a second data dimension with value equal to 1 for all data points to simulate a bias term in the first layer of the gated ReLU network. Also, in this case we can enumerate all 2n data masks Di directly. We use the cone decomposition procedure described in [8] to obtain a ReLU network from the gated ReLU network obtained by solving (8). We also train a 100 neuron ReLU network using gradient descent (GD) and compare the learned output functions in the right plot in Figure 1. Exact details can be found in the supplementary material.\nStudent-teacher setting. We generate the training data by sampling X \u223c N (0, In) and computing the targets y using a fixed, randomly initialized gated ReLU teacher network. In Figure 2, n = 10, d = 5, and we use a teacher network with width m = 10, with gates and parameters randomly drawn from independent standard multivariate Gaussians. To solve the convex formulation (8), we estimate DX by randomly sampling unique hyperplane arrangements."
        },
        {
            "heading": "Computing the NTK weights \u03b7\u0303.",
            "text": "The weights induced by the NTK are given as in Theorem 5.1 by \u03b7\u0303i = P[diag (1 {Xh \u2265 0}) = Di] where h \u223c N (0, Id) is a standard multivariate Gaussian vector. These probabilities can be interpreted either as the orthant probabilities of the multivariate Gaussians given by (2Di \u2212 In)Xh or as the solid angle of the cones given by{ u \u2208 Rd : (2Di \u2212 In)Xu \u2265 0 } . Closed form expressions exist for d = 2, 3, and [39, 40] present approximating schemes for higher dimensions. We calculate these weights exactly for the 1D example presented in Figure 1, and estimate them using Monte Carlo sampling for the student-teacher example in Figure 2.\nFixing the NTK weights by IRLS. We solve (16) with \u03a6i = DiX and regularization weights given by the NTK weights \u03b7\u0303 to obtain the solution w\u0303. We use efficient least squares solvers from [41, 42]. By Theorem 5.1, this corresponds to choosing the weights \u03b7 in the MKL problem (13) such that the resulting kernel matrix is equal to the NTK matrix H. We find the exact solution of the group lasso problem (8) using CVXPY. Comparing the optimal value of the group lasso problem (8) with the objective value of w\u0303 (given by the green line) in Figures 1 and 2, we observe that the NTK weighted solution is sub-optimal. This means that H is not the optimal kernel that would be learnt by MKL (which is expected since H has no dependence on the targets y). By applying IRLS initialized with the NTK weights, we fix the NTK and find the weights of the optimal MKL kernel. In Figures 1 and 2, we observe that IRLS converges to the solution of the group lasso problem (8) and fixes the NTK.\nUCI datasets. We compare the regularied NTK with our IRLS algorithm (Algorithm 1) on the UCI ML Repository datasets. We follow the procedure described in [43] for n \u2264 1000 to extract and standardize the datasets. We observe that our method achieves higher (or the same) test accuracy for 26 out of 33 datasets (see Table 1 for details) while the NTK achieves higher (or the same) test accuracy for 14 datasets which empirically supports our main claim that the IRLS procedure fixes the NTK. Details of the experiments can be found in Section B of the supplementary material."
        },
        {
            "heading": "8 Discussion and Limitations",
            "text": "In this work, we explored the connection between finite-width theories of neural networks given by convex reformulations and infinite-width theories of neural networks given by the NTK. To bridge these theories, we first interpreted the group lasso convex formulation of the gated ReLU network as a multiple kernel learning model using the masking kernels generated by its gates. Then, we linked this MKL model with the NTK of the gated ReLU network evaluated on the training data. Specifically, we showed that the NTK is equivalent to the weighted masking kernel with weights that depend only on the input data X and not on the targets y. We contrast this with the MKL interpretation of the gated ReLU network which learns the optimal data-dependent kernel using both X and y. Therefore, the NTK cannot perform better than the optimal MKL kernel. To fix the NTK, we improve the weights induced by it using the iteratively reweighted least squares (IRLS) scheme to obtain the optimal solution of the group lasso formulation of the gated ReLU network. We corroborated our theoretical results by empirically running IRLS on toy datasets.\nWhile our theory is able to link the optimization properties of the NTK with those of finite width networks via the MKL characterization of group lasso, we do not derive explicit generalization results on the test set. Applying existing generalization theory for kernel methods [44, 45] to the MKL interpretation of the convex reformulation could be a promising direction for future work.\nFinally, although we studied fully connected networks in this paper, our approach can be directly extended to various neural network architectures, e.g., threshold/binary networks [46], convolution networks [47], generative adversarial networks [48], NNs with batch normalization [49], autoregressive models [50], and Transformers [51, 52]."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported in part by the National Science Foundation (NSF) CAREER Award under Grant CCF-2236829, Grant DMS-2134248 and Grant ECCS-2037304; in part by the U.S. Army Research Office Early Career Award under Grant W911NF-21-1-0242; in part by the Stanford Precourt Institute; and in part by the ACCESS\u2014AI Chip Center for Emerging Smart Systems through InnoHK, Hong Kong, SAR."
        },
        {
            "heading": "Supplementary Material",
            "text": ""
        },
        {
            "heading": "Table of Contents",
            "text": ""
        },
        {
            "heading": "A Proofs and Theoretical Results 14",
            "text": "A.1 Proof of Lemma 2.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 A.2 Proof of Lemma 2.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 A.3 Proof of Lemma 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 A.4 Proof of Theorem 4.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 A.5 Proof of Theorem 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 A.6 Proof of Corollary 5.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 A.7 Proof of Theorem 6.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17"
        },
        {
            "heading": "B Experimental Details 18",
            "text": ""
        },
        {
            "heading": "C Generic Loss Functions 19",
            "text": ""
        },
        {
            "heading": "D Extensions to Deeper Networks 20",
            "text": ""
        },
        {
            "heading": "E Cone Decomposition 20",
            "text": "E.1 Computational Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21"
        },
        {
            "heading": "A Proofs and Theoretical Results",
            "text": ""
        },
        {
            "heading": "A.1 Proof of Lemma 2.2",
            "text": "The gradient feature map of (9) is\n\u2207\u03b8 f\u0303G (x,\u03b8) = 1\u221a 2m  1 { xTg1 \u2265 0 } xTw (1) 1 ... 1 { xTgm \u2265 0 } xTw (1) m 1 { xTg1 \u2265 0 } w (2) 1 x\n... 1 { xTgm \u2265 0 } w (2) m x\n ."
        },
        {
            "heading": "The finite width random NTK given by Hm,\u03b8,G (x,x\u2032) := \u2207\u03b8 f\u0303G (x,\u03b8)T \u2207\u03b8 f\u0303G (x\u2032,\u03b8) is",
            "text": "Hm,\u03b8,G (x,x \u2032) =\n1\n2m m\u2211 j=1 ( 1 { xTgj \u2265 0 } 1 { x\u2032Tgj \u2265 0 }( xTw (1) j x \u2032Tw (1) j + ( w (2) j )2 xTx\u2032 )) .\nBy the Law of Large numbers since gj ,w (1) j , w (2) j are iid, Hm,\u03b8,G (x,x \u2032) converges in probability to the expectation of the quantity in the sum as m\u2192\u221e. We denote this infinite limit by H (x,x\u2032). Therefore,\nH (x,x\u2032) = 1 2 E [ 1 { xTg \u2265 0 } 1 { x\u2032Tg \u2265 0 }( xTw(1)x\u2032Tw(1) + ( w(2) )2 xTx\u2032 )] ,\nwhere g \u223c N (0, Id), w(1) \u223c N (0, Id), w(2) \u223c N (0, 1). Using E [( w(2) )2] = 1,\nE [ xTw(1)x\u2032Tw(1) ] = xTx\u2032 and the linearity of expectation, we get\nH (x,x\u2032) = xTx\u2032 \u00b7 E [ 1 { xTg \u2265 0 } 1 { x\u2032Tg \u2265 0 }] . (18)\nTo evaluate the expectation of the product of the indicator random variables, we condition on one of the indicators being equal to 1.\nE [ 1 { xTg \u2265 0 } 1 { x\u2032Tg \u2265 0 }] = E [ 1 { xTg \u2265 0 } | 1 { x\u2032Tg \u2265 0 } = 1 ] \u00b7 P [ 1 { x\u2032Tg \u2265 0 } = 1 ]\n= 1 2 \u00b7 E [ 1 { xTg \u2265 0 } | 1 { x\u2032Tg \u2265 0 } = 1 ] .\nThe conditional expectation is the probability of the event that xTg \u2265 0 given that x\u2032Tg \u2265 0. This can be geometrically seen as the ratio of \u03c0 minus the angle between x and x\u2032 to \u03c0. So, we get\nE [ 1 { xTg \u2265 0 } 1 { x\u2032Tg \u2265 0 }] = 1\n2 \u00b7 1 \u03c0\n( \u03c0 \u2212 arccos ( xTx\u2032\n\u2225x\u22252 \u2225x\u2032\u22252\n)) .\nCombining this with (18) yields the desired result."
        },
        {
            "heading": "A.2 Proof of Lemma 2.3",
            "text": "The gradient feature map of (11) is\n\u2207\u03b8fr (x,\u03b8) = 1\u221a 2m  1 { xTw (+) 1 \u2265 0 } x ... 1 { xTw (+) m \u2265 0 } x \u22121 { xTw (\u2212) 1 \u2265 0 } x\n... \u22121 { xTw (\u2212) m \u2265 0 } x\n .\nThe finite width random NTK given by Hrm,\u03b8 (x,x \u2032) := \u2207\u03b8fr (x,\u03b8)T \u2207\u03b8fr (x\u2032,\u03b8) is\nHrm,\u03b8 (x,x \u2032) =\n1\n2m m\u2211 j=1 ( 1 { xTw (+) j \u2265 0 } 1 { x\u2032Tw (+) j \u2265 0 } xTx\u2032+\n1 { xTw\n(\u2212) j \u2265 0\n} 1 { x\u2032Tw\n(\u2212) j \u2265 0\n} xTx\u2032 ) .\nBy the Law of Large numbers since w(+)j ,w (\u2212) j are iid, H r m,\u03b8 (x,x \u2032) converges in probability to the expectation of the quantity in the sum as m \u2192 \u221e. We denote this infinite limit by Hr (x,x\u2032). Therefore,\nHr (x,x\u2032) = 1 2 E [ 1 { xTw(+) \u2265 0 } 1 { x\u2032Tw(+) \u2265 0 } +\n1 { xTw(\u2212) \u2265 0 } 1 { x\u2032Tw(\u2212) \u2265 0 }] xTx\u2032,\nwhere w(+) \u223c N (0, Id) and w(\u2212) \u223c N (0, Id). Since w(+) and w(\u2212) are i.i.d., we can combine the expectations to get\nHr (x,x\u2032) = E [ 1 { xTw(+) \u2265 0 } 1 { x\u2032Tw(+) \u2265 0 }] xTx\u2032.\nComparing to (18), we get the desired result."
        },
        {
            "heading": "A.3 Proof of Lemma 4.2",
            "text": "Since G is complete, \u2200 i \u2208 [p] ,\u2203 gi \u2208 G such that diag (1 {Xgi \u2265 0}) = Di. Additionally, these gates gi are unique since diag (1 {Xgi \u2265 0}) \u0338= diag (1 {Xgj \u2265 0}) =\u21d2 gi \u0338= gj . Therefore, there is a subset of gates G of size p that are uniquely associated to each mask Di. Since G is also minimally complete, this subset must be G and this proves the lemma."
        },
        {
            "heading": "A.4 Proof of Theorem 4.3",
            "text": "By Lemma 3.1, the MKL problem given by\nmin \u03b7\u2208\u2206p,vi\u2208Rd \u2225\u2225\u2225\u2225\u2225 p\u2211\ni=1\n\u221a \u03b7iDiXvi \u2212 y \u2225\u2225\u2225\u2225\u2225 2\n2\n+ \u03bb\u0302 p\u2211 i=1 \u2225vi\u222522 .\nis equivalent to the group lasso problem (14). Additionally, (14) is also equivalent to the standard group lasso (15) since they have the same regularization paths. Since G is minimally complete, (7) (which is equivalent to (6) in the minimally complete gate set setting) is equivalent to the group lasso problem (15) and this proves the theorem."
        },
        {
            "heading": "A.5 Proof of Theorem 5.1",
            "text": "The weighted masking kernel is given by\nKG (\u03b7\u0303) = p\u2211 i=1 \u03b7\u0303iDiXX TDi. (19)\nNow, the NTK matrix H can be written as (using (18)),\nH = Eg\u223cN (0,Id) [ diag (1 {Xg \u2265 0})XXTdiag (1 {Xg \u2265 0}) ] .\nWe can condition on the events that diag (1 {Xg \u2265 0}) = Di \u2200 i \u2208 [p]. SinceDX covers all possible data maskings Di, we know that \u2211p i=1 P [diag (1 {Xg \u2265 0}) = Di] = 1. Therefore,\nH = p\u2211 i=1 Eg\u223cN (0,Id) [ DiXX TDi | diag (1 {Xg \u2265 0}) = Di ] P [diag (1 {Xg \u2265 0}) = Di]\n= p\u2211 i=1 DiXX TDiP [diag (1 {Xg \u2265 0}) = Di] .\nPlugging in \u03b7\u0303i = P [diag (1 {Xg \u2265 0}) = Di] and comparing to (19), we get that KG (\u03b7\u0303) = H and this proves Theorem 5.1."
        },
        {
            "heading": "A.6 Proof of Corollary 5.2",
            "text": "First, we define a single combined masked data matrix X\u0303 \u2208 Rn\u00d7pd as follows\nX\u0303 := [D1X D2X\u00b7 \u00b7 \u00b7DpX] .\nWe also define a diagonal regularization weighting matrix R \u2208 Rpd\u00d7pd as follows"
        },
        {
            "heading": "R := diag ([\u03b7\u030311d \u03b7\u030321d \u00b7 \u00b7 \u00b7 \u03b7\u0303p1d]) ,",
            "text": "where 1d is a d-dimensional all-1s row vector. Now, we can rewrite (16) with \u03a6i = DiX and regularization weights \u03b7\u0303 as\nmin w\u2208Rpd \u2225\u2225\u2225X\u0303w \u2212 y\u2225\u2225\u22252 2 + \u03bb \u2225\u2225\u2225R\u2212 12w\u2225\u2225\u22252 2 . (20)\nSetting the gradient of the convex objective in (20) to 0, we get the following equation for the solution w\u0303,\n2X\u0303T ( X\u0303w\u0303 \u2212 y ) + 2\u03bbR\u22121w\u0303 = 0.\nSolving for w\u0303, we get\nw\u0303 = ( X\u0303T X\u0303+ \u03bbR\u22121 )\u22121 X\u0303Ty.\nBy applying the matrix inversion lemma [53], we can rewrite this as w\u0303 = RX\u0303T ( X\u0303RX\u0303T + \u03bbIn )\u22121 y.\nNow, we notice that p\u2211\ni=1\nDiXw\u0303i = X\u0303w\u0303 = X\u0303RX\u0303 T ( X\u0303RX\u0303T + \u03bbIn )\u22121 y. (21)\nAdditionally, observe that X\u0303RX\u0303T = \u2211p\ni=1 \u03b7\u0303iDiXXDi = H where the last equality is by Theorem 5.1. Plugging this into (21), we get\np\u2211 i=1 DiXw\u0303i = H (H+ \u03bbIn) \u22121 y\nand this proves Corollory 5.2."
        },
        {
            "heading": "A.7 Proof of Theorem 6.1",
            "text": "We begin with the optimality of w\u0302,\n1\nn \u2225\u2225\u2225\u2225\u2225 p\u2211\ni=1\nDiXw\u0302i \u2212 y \u2225\u2225\u2225\u2225\u2225 2\n2\n+ \u03bb p\u2211 i=1 \u2225w\u0302i\u22252 \u2264 1 n \u2225\u2225\u2225\u2225\u2225 p\u2211 i=1 DiXw \u2217 i \u2212 y \u2225\u2225\u2225\u2225\u2225 2\n2\n+ \u03bb p\u2211 i=1 \u2225w\u2217i \u22252 .\nBy plugging in the model y = \u2211p\ni=1 DiXw \u2217 i + \u03f5, we simplify this to\n1\nn \u2225\u2225\u2225\u2225\u2225 p\u2211\ni=1\nDiX (w\u0302i \u2212w\u2217i )\u2212 \u03f5 \u2225\u2225\u2225\u2225\u2225 2\n2\n+ \u03bb p\u2211 i=1 \u2225w\u0302i\u22252 \u2264 1 n \u2225\u03f5\u222522 + \u03bb p\u2211 i=1 \u2225w\u2217i \u22252 .\nWe expand the square on the LHS and obtain the following bound on the prediction error\n1\nn \u2225\u2225\u2225\u2225\u2225 p\u2211\ni=1\nDiX (w\u0302i \u2212w\u2217i ) \u2225\u2225\u2225\u2225\u2225 2\n2\n\u2264 2 n p\u2211 i=1 \u03f5TDiX (w\u0302i \u2212w\u2217i ) + \u03bb p\u2211 i=1 (\u2225w\u2217i \u22252 \u2212 \u2225w\u0302i\u22252)\n\u2264 2 n p\u2211 i=1 \u2225\u2225\u03f5TDiX\u2225\u22252 \u2225w\u0302i \u2212w\u2217i \u22252 + \u03bb p\u2211 i=1 (\u2225w\u2217i \u22252 \u2212 \u2225w\u0302i\u22252) ,\nwhere we used Cauchy-Schwarz inequality in the second step. Now, since Di are diagonal matrices with either 0 or 1 on the diagonals, we can immediately say that \u2225\u2225\u03f5TDiX\u2225\u22252 \u2264 \u2225\u2225\u03f5TX\u2225\u22252 for all i = 1, . . . , p. As a consequence, we get\n1\nn \u2225\u2225\u2225\u2225\u2225 p\u2211\ni=1\nDiX (w\u0302i \u2212w\u2217i ) \u2225\u2225\u2225\u2225\u2225 2\n2\n\u2264 2 n \u2225\u2225\u03f5TX\u2225\u2225 2 p\u2211 i=1 \u2225w\u0302i \u2212w\u2217i \u22252 + \u03bb p\u2211 i=1 (\u2225w\u2217i \u22252 \u2212 \u2225w\u0302i\u22252)\n\u2264 ( 2\nn\n\u2225\u2225\u03f5TX\u2225\u2225 2 + \u03bb ) p\u2211 i=1 \u2225w\u2217i \u22252 + ( 2 n \u2225\u2225\u03f5TX\u2225\u2225 2 \u2212 \u03bb ) p\u2211 i=1 \u2225w\u0302i\u22252 ,\nwhere we used the triangle inequality in the second step. Now, if \u03bb \u2265 2n \u2225\u2225\u03f5TX\u2225\u2225 2 , we can get rid of\nthe second term, namely ( 2 n \u2225\u2225\u03f5TX\u2225\u2225 2 \u2212 \u03bb )\u2211p i=1 \u2225w\u0302i\u22252 and obtain a bound on the prediction error that does not depend on w\u0302. To this end, notice that \u03f5TX is a multivariate Gaussian with mean 0 and covariance \u03c32XTX. We now present a standard concentration result for the norms of Gaussians in the lemma below,\nLemma A.1. Let z be a d-dimensional multivariate Gaussian with mean 0 and covariance matrix \u03a3. We have\nP [\u2225z\u22252 \u2264 z] \u2265 1\u2212 2 exp ( \u2212z2\n2tr (\u03a3)\n) .\nApplying Lemma A.1 to \u03f5TX, we get\nP [\u2225\u2225\u03f5TX\u2225\u2225 2 \u2264 z ] \u2265 1\u2212 2e \u2212z2 2\u03c32tr(XT X) .\nPlugging in z = n2\u03bb where \u03bb = t\u03c3 \u221a tr(XTX) n as in the statement of theorem, we get\nP [ 2\nn\n\u2225\u2225\u03f5TX\u2225\u2225 2 \u2264 \u03bb ] \u2265 1\u2212 2e \u2212t2 8 .\nTherefore, by choosing \u03bb = t\u03c3 \u221a tr(XTX)\nn , we have that 2 n \u2225\u2225\u03f5TX\u2225\u2225 2 \u2264 \u03bb with probability at least\n1\u2212 2e\u2212t2/8. Consequently, we have\n1\nn \u2225\u2225\u2225\u2225\u2225 p\u2211\ni=1\nDiX (w\u0302i \u2212w\u2217i ) \u2225\u2225\u2225\u2225\u2225 2\n2\n\u2264 2\u03bb p\u2211\ni=1\n\u2225w\u2217i \u22252\nwith probability at least 1\u2212 2e\u2212t2/8, which completes the proof."
        },
        {
            "heading": "A.7.1 Proof of Lemma A.1",
            "text": "Since the norm of Gaussian vector does not change by applying a rotation, we know that \u2225\u2225\u039b1/2u\u2225\u2225 2 and \u2225z\u22252 have the same distribution where \u039b is the matrix of eigenvalues from the spectral decomposition \u03a3 = Q\u039bQT and u \u223c N (0, Id). Now we apply a standard Chernoff bound argument. We have for some s > 0,\nP [\u2225\u2225\u2225\u039b1/2u\u2225\u2225\u2225 2 > z ] \u2264 P [\u2225\u2225\u2225\u039b1/2u\u2225\u2225\u2225 1 > z ]\n\u2264 P [ es\u2225\u039b 1/2u\u2225 1 > esz ] \u2264 e\u2212sz\u03a0di=1E [ es \u221a \u03bbi|ui|\n] \u2264 e\u2212sz\u03a0di=12es 2\u03bbi/2.\nBy optimizing the bound over s, we get\nP [\u2225\u2225\u2225\u039b1/2u\u2225\u2225\u2225 2 > z ] \u2264 2 exp ( \u2212z2 2 \u2211d\ni=1 \u03bbi\n) \u2264 2 exp ( \u2212z2\n2tr (\u03a3)\n) .\nFinally, using P [\u2225z\u22252 \u2264 z] = 1\u2212 P [\u2225z\u22252 > z] gives the result."
        },
        {
            "heading": "B Experimental Details",
            "text": "Hyperplane Arrangements for 1D example. In the 1D example in Figure 1, the data matrix X is in Rn\u00d72, i.e. the dimension d = 2. This is because we set the second component of each input to be exactly equal to 1 to simulate a bias in the neural network. Though the datapoints lie in R2, they have some structure which allows us to enumerate all hyperplane arrangements DX easily. Specifically, the datapoints all lie on the line y = 1 in the xy plane, so the hyperplanes that separate them always separate them into two contiguous segments. Therefore, by sorting the datapoints according to their first component, we can directly enumerate all 2n hyperplane arrangements.\nNonconvex ReLU network. For the black dashed line in Figure 1, we trained a two layer ReLU network with 100 hidden neurons using the standard weight decay regularized objective with gradient descent using a learning rate of 0.01 for 200000 epochs.\nConvex program solver specifications. For directly solving the group lasso problem (8), we used CVXPY with the MOSEK solver [54]. For the \u21132 regularized least squares problems (16), we used iterative methods with an efficient matrix-vector product implementation to avoid explicitly constructing the large combined masked data matrix X\u0303. Specifically, we used the LSQR and LSMR methods as described in [41, 42]. The experiments were run locally on a MacBook Air 2020 version with the Apple M1 chip.\nUCI Machine Learning Repository. Here we present details for the experiments on several real datasets from the UCI Machine Learning Repository [55]. We follow the procedure described in [43] for n \u2264 1000 to extract and standardize the datasets. For these experiments, we use the 75% \u2212 25% ratio for the training and test set splits. We use the squared loss and tune the regularization parameter \u03bb for both NTK and our approach by performing a grid search over the set {10\u22125, 10\u22124, 10\u22123, 10\u22122, 10\u22121, 100, 101}. As shown in Table 1, our method achieves higher (or the same) test accuracy for 26 datasets out 33 datasets while standard NTK achieves higher (or the same) test accuracy for 14."
        },
        {
            "heading": "C Generic Loss Functions",
            "text": "Here, we prove that the analysis with squared loss presented in the main paper can be extended to arbitrary convex loss function. Let us consider the following regularized two-layer ReLU network training problem with an arbitrary convex loss function L (\u00b7, \u00b7)\nmin W(1),w(2)\nL  m\u2211 j=1 ( Xw (1) j ) + w (2) j ,y + \u03bb m\u2211 j=1 (\u2225\u2225\u2225w(1)j \u2225\u2225\u22252 2 + |w(2)|2 ) . (22)\nThen, as shown in [7], the corresponding gated convex reformulation of (22) is\nP \u2217 := min wi L\n( p\u2211\ni=1\nDiXwi \u2212 y ) + \u03bb\np\u2211 i=1 \u2225wi\u22252 . (23)\nIn order to realize the emergence of kernel characterization in the case of arbitrary loss function, we use Lagrange duality. Particularly, since (23) is a convex optimization problem and Slater\u2019s conditions holds as proven in [7, 8], we equivalently represent (22) as the following dual problem\nP \u2217 = max z \u2212L\u2217 (z,y) s.t. \u2225\u2225zTDiX\u2225\u22252 \u2264 \u03bb,\u2200i \u2208 [p] (24) where L (z,y) is the Fenchel conjugate of L (\u00b7,y) defined as [56]\nL\u2217 (z,y) := max v vT z\u2212 L (v,y) .\nWe then form the Lagrangian for (24) as follows\nL(z, \u03b1i) = \u2212L\u2217 (z,y) + p\u2211\ni=1\n\u03b1i ( \u03bb\u2212 \u2225\u2225zTDiX\u2225\u22252) = \u2212L\u2217 (z,y) +\np\u2211 i=1 \u03b1i ( \u03bb\u2212 zTDiXXTDiz ) = \u2212L\u2217 (z,y) +\np\u2211 i=1 \u03b1i ( \u03bb\u2212 zTKiz ) ,\nwhere \u03b1i \u2265 0 is the Lagrange multiplier for the inequality constraint. Based on the representation above, the dual problem in (24) can be written as\nP \u2217 = max z min \u03b1i\u22650\n\u2212L\u2217 (v,y) + p\u2211\ni=1\n\u03b1i ( \u03bb\u2212 zTKiz ) . (25)\nThus, similar to (12), the problem in (25) can be globally optimized with alternating minimization [33, 35]."
        },
        {
            "heading": "D Extensions to Deeper Networks",
            "text": "In this section, we generalize the kernel characterization of two-layer ReLU networks presented in the main paper to deeper architectures. We first note that most prior works on convex reformulation of ReLU networks studied only two-layer ReLU networks, e.g., [6\u201310], since the non-convex interaction of multiple nonlinear layers hinders the convex analysis of deeper networks. However, recent followup studies [13, 57, 58] showed that the analysis can be extended to arbitrarily deep networks. Based on this recent results, below we briefly explain how one can extend our kernel characterization to three-layer networks. By following the convex reformulations in [57, 58], we have the following gated ReLU training problem for three-layer network\nmin wij \u2225\u2225\u2225\u2225\u2225\u2225 p1\u2211 i=1 p2\u2211 j=1 D (1) i D (2) j Xwij \u2212 y \u2225\u2225\u2225\u2225\u2225\u2225 2\n2\n+ \u03bb p1\u2211 i=1 p2\u2211 j=1 \u2225wij\u22252 , (26)\nwhere the hyperplane arrangements for the first and second ReLU layers, i.e., denoted as D(1)i \u2208 D (1) X and D(2)j \u2208 D (2) X , are defined as follows\nD(1)X := { diag (1 {Xu \u2265 0}) : u \u2208 Rd } and p1 := \u2223\u2223\u2223D(1)X \u2223\u2223\u2223 (27) D(2)X := { diag ( 1 {( XU(1) ) + u(2) \u2265 0 }) : U(1) \u2208 Rd\u00d7m1 , u(2) \u2208 Rm1 } and p2 :=\n\u2223\u2223\u2223D(2)X \u2223\u2223\u2223 . Then, following the derivations in Section 4, we can conveniently express the corresponding feature matrices of these masking feature maps on the data X in terms of fixed diagonal data masks as \u03a6ij = D (1) i D (2) j X. Similarly, the corresponding masking kernel matrices take the form Kij = D (1) i D (2) j XX TD (1) i D (2) j . The same interpretation also extends to arbitrarily deep networks and yields a more complicated feature map due to the interaction of multiple hyperplane arrangement matrices, e.g., D(1)i D (2) j D (3) k . . ."
        },
        {
            "heading": "E Cone Decomposition",
            "text": "Here, we briefly review the cone decomposition concept introduced by [8]. We first note that (3) is the exact convex reformulation of the standard non-convex regularized training problem in (2). However, as mentioned in the main paper, the number of constraints in (3) can be prohibitively large, precisely can be O(nd), and consequently prevents solving the exact convex optimization problem especially for large scale datasets. To remedy this issue, [8] proposed the unconstrained formulation in (8), but, this relaxation corresponds to another neural network architecture called gated ReLU network in (5). Although this relaxation breaks dependence between the arrangements and preactivations of a ReLU activation, [8] proved that once the gated ReLU network training problem in (8) training problem is solved, one can construct a solution for the ReLU network model in (1). This procedure is named as cone decomposition and we provide details regarding the implementation steps below.\nLet us assume that we solve the gated ReLU network training problem in (8) and denote the optimal solution as {w\u2217i } p i=1. Then, since each DiXw \u2217 i can have both negative and positive entries (due to the absence of nonnegativity constraint ), we need to decompose DiXw\u2217i into two separate cones to maintain nonnegativity property of a ReLU activation. We formulate this decomposition step as the following convex optimization problem\nmin vi,ui\n\u2225vi\u22252 + \u2225ui\u22252 s.t. DiXw \u2217 i = DiX(vi \u2212 ui), (2Di \u2212 In)Xvi \u2265 0 (2Di \u2212 In)Xui \u2265 0 . (28)\nAfter solving the convex optimization problem in (28) for all i \u2208 [p]. We declare the optimal solutions, {v\u2217i ,u\u2217i } p i=1 as a solution to the exact ReLU model in (1). Thus, instead of directly solving the convex reformulation with extremely large number of constraint in (3), we first solve the relaxed gated ReLU problem in (8) and then map the solutions to the proper cones to enforce the nonnegativity property of the ReLU activation."
        },
        {
            "heading": "E.1 Computational Complexity",
            "text": "Although [8] reduce the computational complexity by eliminating the constraints in the convex optimization problem in (3), the number of variables, denotes as p, can still be exponentially large in feature dimension d. However, there are multiple ways to remedy this issue.\n\u2022 We can change the architecture. Particularly, we can replace fully connected networks with convolutional networks. Then, since CNNs operate on the patch matrices {Xb}Bb=1 instead of the full data matrix X, where Xb \u2208 Rn\u00d7h and h denotes the filter size, even when the data matrix is full rank, i.e., r = min(n, d), the number of hyperplane arrangements p is upperbounded as p \u2264 O(nrc), where rc := maxb rank(Xb) \u2264 h\u226a min(n, d). For instance, let us consider a CNN with 3\u00d7 3 filters, then rc \u2264 9 independent of n, d. As a consequence, weight sharing structure in CNNs dramatically limits the number of possible hyperplane arrangements and avoids exponential complexity. This also explains efficiency and remarkable generalization performance of CNNs in practice.\n\u2022 We can also use a sampling based approach where one can randomly sample a tiny subset of all possible hyperplane arrangements and then solve the convex program with this subset. Thus, although the resulting approach is not exact, the training complexity will not be exponential in d anymore. The experimental results in Appendix C show that this approximation in fact works extremely well, specifically resulting in models that outperform the NTK in 26/33 UCI datasets as detailed in Table 1."
        }
    ],
    "title": "Fixing the NTK: From Neural Network Linearizations to Exact Convex Programs",
    "year": 2023
}