{
    "abstractText": "We introduce a new approach for generating sequences of implied volatility (IV) surfaces across multiple assets that is faithful to historical prices. We do so using a combination of functional data analysis and neural stochastic differential equations (SDEs) combined with a probability integral transform penalty to reduce model misspecification. We demonstrate that learning the joint dynamics of IV surfaces and prices produces market scenarios that are consistent with historical features and lie within the sub-manifold of surfaces that are essentially free of static arbitrage. Finally, we demonstrate that delta hedging using the simulated surfaces generates profit and loss (P&L) distributions that are consistent with realised P&Ls.",
    "authors": [
        {
            "affiliations": [],
            "name": "Vedant Choudhary"
        },
        {
            "affiliations": [],
            "name": "Maxime Bergeron"
        }
    ],
    "id": "SP:efec78ef0e20837a7f312e5b5525ca854f54f810",
    "references": [
        {
            "authors": [
                "Damien Ackerer",
                "Natasa Tagasovska",
                "Thibault Vatter"
            ],
            "title": "Deep smoothing of the implied volatility surface",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "John E Angus"
            ],
            "title": "The probability integral transform and related results",
            "venue": "SIAM review,",
            "year": 1994
        },
        {
            "authors": [
                "Michal Benko",
                "Wolfgang H\u00e4rdle",
                "Alois Kneip"
            ],
            "title": "Common functional principal components",
            "venue": "The Annals of Statistics,",
            "year": 2009
        },
        {
            "authors": [
                "Maxime Bergeron",
                "Nicholas Fung",
                "John Hull",
                "Zissis Poulos",
                "Andreas Veneris"
            ],
            "title": "Variational autoencoders: A hands-off approach to volatility",
            "venue": "The Journal of Financial Data Science,",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Alexandre Bloch",
                "Arthur B\u00f6\u00f6k"
            ],
            "title": "Deep learning based dynamic implied volatility surface",
            "venue": "Available at SSRN 3952842,",
            "year": 2021
        },
        {
            "authors": [
                "Hans Buehler",
                "Lukas Gonon",
                "Josef Teichmann",
                "Ben Wood"
            ],
            "title": "Deep hedging",
            "venue": "Quantitative Finance,",
            "year": 2019
        },
        {
            "authors": [
                "Jay Cao",
                "Jacky Chen",
                "John Hull",
                "Zissis Poulos"
            ],
            "title": "Deep hedging of derivatives using reinforcement learning",
            "venue": "The Journal of Financial Data Science,",
            "year": 2021
        },
        {
            "authors": [
                "Rene Carmona",
                "Yi Ma",
                "Sergey Nadtochiy"
            ],
            "title": "Simulation of implied volatility surfaces via tangent l\u00e9vy models",
            "venue": "SIAM Journal on Financial Mathematics,",
            "year": 2017
        },
        {
            "authors": [
                "Ricky TQ Chen",
                "Yulia Rubanova",
                "Jesse Bettencourt",
                "David K Duvenaud"
            ],
            "title": "Neural ordinary differential equations",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Kyunghyun Cho",
                "Bart Van Merri\u00ebnboer",
                "Caglar Gulcehre",
                "Dzmitry Bahdanau",
                "Fethi Bougares",
                "Holger Schwenk",
                "Yoshua Bengio"
            ],
            "title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation",
            "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2014
        },
        {
            "authors": [
                "Anthony Coache",
                "Sebastian Jaimungal"
            ],
            "title": "Reinforcement learning with dynamic convex risk measures",
            "venue": "arXiv preprint arXiv:2112.13414,",
            "year": 2021
        },
        {
            "authors": [
                "Anthony Coache",
                "Sebastian Jaimungal",
                "\u00c1lvaro Cartea"
            ],
            "title": "Conditionally elicitable dynamic risk measures for deep reinforcement learning",
            "venue": "arXiv preprint arXiv:2206.14666,",
            "year": 2022
        },
        {
            "authors": [
                "Samuel N Cohen",
                "Christoph Reisinger",
                "Sheng Wang"
            ],
            "title": "Arbitrage-free neural-sde market models",
            "venue": "arXiv preprint arXiv:2105.11053,",
            "year": 2021
        },
        {
            "authors": [
                "Rama Cont",
                "Jos\u00e9 Da Fonseca"
            ],
            "title": "Dynamics of implied volatility surfaces",
            "venue": "Quantitative finance,",
            "year": 2002
        },
        {
            "authors": [
                "Rama Cont",
                "Milena Vuleti\u0107"
            ],
            "title": "Simulation of arbitrage-free implied volatility surfaces",
            "venue": "Available at SSRN,",
            "year": 2022
        },
        {
            "authors": [
                "Rama Cont",
                "Jose da Fonseca",
                "Valdo Durrleman"
            ],
            "title": "Stochastic models of implied volatility surfaces",
            "venue": "Economic Notes,",
            "year": 2002
        },
        {
            "authors": [
                "Christa Cuchiero",
                "Wahid Khosrawi",
                "Josef Teichmann"
            ],
            "title": "A generative adversarial network approach to calibration of local stochastic volatility models",
            "year": 2020
        },
        {
            "authors": [
                "Francis X. Diebold",
                "Todd A. Gunther",
                "Anthony S. Tay"
            ],
            "title": "Evaluating density forecasts with applications to financial risk management",
            "venue": "International Economic Review,",
            "year": 1998
        },
        {
            "authors": [
                "Valdo Durrleman"
            ],
            "title": "Implied volatility: Market models",
            "venue": "Encyclopedia of Quantitative Finance,",
            "year": 2010
        },
        {
            "authors": [
                "Matthias R Fengler"
            ],
            "title": "Arbitrage-free smoothing of the implied volatility surface",
            "venue": "Quantitative Finance,",
            "year": 2009
        },
        {
            "authors": [
                "Matthias R Fengler",
                "Qihua Wang"
            ],
            "title": "Fitting the smile revisited: A least squares kernel estimator for the implied volatility surface",
            "venue": "Technical report,",
            "year": 2003
        },
        {
            "authors": [
                "Matthias R Fengler",
                "Wolfgang K H\u00e4rdle",
                "Christophe Villa"
            ],
            "title": "The dynamics of implied volatilities: A common principal components approach",
            "venue": "Review of Derivatives Research,",
            "year": 2003
        },
        {
            "authors": [
                "Pascal Francois",
                "R\u00e9mi Galarneau-Vincent",
                "Genevi\u00e8ve Gauthier",
                "Fr\u00e9d\u00e9ric Godin"
            ],
            "title": "Joint dynamics for the underlying asset and its implied volatility surface: A new methodology for option risk management",
            "venue": "Available at SSRN 4319972,",
            "year": 2023
        },
        {
            "authors": [
                "Jim Gatheral"
            ],
            "title": "A parsimonious arbitrage-free implied volatility parameterization with application to the valuation of volatility derivatives",
            "venue": "Presentation at Global Derivatives & Risk Management,",
            "year": 2004
        },
        {
            "authors": [
                "Jim Gatheral",
                "Antoine Jacquier"
            ],
            "title": "Arbitrage-free svi volatility surfaces",
            "venue": "Quantitative Finance,",
            "year": 2014
        },
        {
            "authors": [
                "Patryk Gierjatowicz",
                "Marc Sabate-Vidales",
                "David \u0160\u01d0ska",
                "Lukasz"
            ],
            "title": "Szpruch, and \u017dan \u017duri\u010d. Robust pricing and hedging via neural sdes",
            "venue": "arXiv preprint arXiv:2007.04154,",
            "year": 2020
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial networks",
            "venue": "Communications of the ACM,",
            "year": 2020
        },
        {
            "authors": [
                "Patrick S Hagan",
                "Deep Kumar",
                "Andrew S Lesniewski",
                "Diana E Woodward"
            ],
            "title": "Managing smile risk",
            "venue": "The Best of Wilmott,",
            "year": 2002
        },
        {
            "authors": [
                "Kari Karhunen"
            ],
            "title": "Under lineare methoden in der wahr scheinlichkeitsrechnung",
            "venue": "Annales Academiae Scientiarun Fennicae Series A1: Mathematia Physica,",
            "year": 1947
        },
        {
            "authors": [
                "Patrick Kidger",
                "James Morrill",
                "James Foster",
                "Terry Lyons"
            ],
            "title": "Neural controlled differential equations for irregular time series",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Brian Ning",
                "Sebastian Jaimungal",
                "Xiaorong Zhang",
                "Maxime Bergeron"
            ],
            "title": "Arbitragefree implied volatility surface generation with variational autoencoders",
            "venue": "arXiv preprint arXiv:2108.04941,",
            "year": 2021
        },
        {
            "authors": [
                "Greg Orosi"
            ],
            "title": "Empirical performance of a spline-based implied volatility surface",
            "venue": "Journal of Derivatives & Hedge Funds,",
            "year": 2012
        },
        {
            "authors": [
                "Michael Roper"
            ],
            "title": "Arbitrage free implied volatility",
            "venue": "surfaces. preprint,",
            "year": 2010
        },
        {
            "authors": [
                "Han Lin Shang",
                "Fearghal Kearney"
            ],
            "title": "Dynamic functional time-series forecasts of foreign exchange implied volatility surfaces",
            "venue": "International Journal of Forecasting,",
            "year": 2022
        },
        {
            "authors": [
                "Xingjian Shi",
                "Zhourong Chen",
                "Hao Wang",
                "Dit-Yan Yeung",
                "Wai-Kin Wong",
                "Wangchun Woo"
            ],
            "title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "BW Silverman"
            ],
            "title": "Density estimation for statistics and data analysis",
            "venue": "Monographs on Statistics and Applied Probability,",
            "year": 1986
        },
        {
            "authors": [
                "Ingo Steinwart",
                "Clint Scovel"
            ],
            "title": "Mercer\u2019s theorem on general domains: On the interaction between measures, kernels, and rkhss",
            "venue": "Constructive Approximation,",
            "year": 2012
        },
        {
            "authors": [
                "Belinda Tzen",
                "Maxim Raginsky"
            ],
            "title": "Neural stochastic differential equations: Deep latent gaussian models in the diffusion limit",
            "venue": "arXiv preprint arXiv:1905.09883,",
            "year": 2019
        },
        {
            "authors": [
                "Tina Ruiwen Wang",
                "Jithin Pradeep",
                "Jerry Zikun Chen"
            ],
            "title": "Objective driven portfolio construction using reinforcement learning",
            "venue": "In Proceedings of the Third ACM International Conference on AI in Finance,",
            "year": 2022
        },
        {
            "authors": [
                "Magnus Wiese",
                "Lianjun Bai",
                "Ben Wood",
                "Hans Buehler"
            ],
            "title": "Deep hedging: learning to simulate equity option markets",
            "venue": "arXiv preprint arXiv:1911.01700,",
            "year": 2019
        },
        {
            "authors": [
                "Yaxiong Zeng",
                "Diego Klabjan"
            ],
            "title": "Online adaptive machine learning based algorithm for implied volatility surface modeling",
            "venue": "Knowledge-Based Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yijun Zhao",
                "Shengjian Xu",
                "Jacek Ossowski"
            ],
            "title": "Deep learning meets statistical arbitrage: An application of long short-term memory networks to algorithmic trading",
            "venue": "The Journal of Financial Data Science,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Keywords: generative models, neural SDEs, functional data analysis, implied volatility"
        },
        {
            "heading": "1. Introduction",
            "text": "It is well known that implied volatility (IV) surfaces exhibit skews/smiles and term structures that are inconsistent with the na\u0308\u0131ve assumptions of Black-Scholes. Moreover, IV surfaces evolve over time as the underlying asset price fluctuates and traders incorporate market information into option prices. These surface fluctuations pose challenges to modeling. Furthermore, IV surfaces must be free of static arbitrage, which imposes constraints on the shape of the IV surfaces (Gatheral and Jacquier, 2014; Durrleman, 2010; Roper, 2010), and adds to the complexity of modeling them.\nLeaving aside the modeling of the evolution dynamics, even the problem of interpolating arbitrage-free IV surfaces is not straightforward. Markets have option price quotes for only a finite (and often limited) number of times to expiry and strikes, and extending and interpolating the surfaces to a continuum is nontrivial. In the extant literature, there are a variety of approaches to fit continuous IV surfaces to observed discrete data, including spline inter-\n\u2217. A github repository that generates dynamical IV surfaces may be found at https://github.com/ vedantch/FuNVol.\nar X\niv :2\n30 3.\n00 85\n9v 4\n[ q-\nfi n.\nC P]\npolation (Fengler, 2009; Orosi, 2012), parametric models such as surface SVI (Gatheral and Jacquier, 2014) and SABR (Hagan et al., 2002), and non-parametric approaches (Cont and Da Fonseca, 2002; Fengler and Wang, 2003; Fengler et al., 2003). These approaches, while useful, have some limitations. For instance, they may impose overly strong assumptions on the shape of IV surfaces, consequently, good fits to historical data may be difficult and the resulting fits may introduce arbitrage. To avoid some of these limitations, researchers are adopting machine learning techniques to model IV surfaces, for example: support vector machines (Zeng and Klabjan, 2019) and variational autoencoders (Ning et al., 2021; Bergeron et al., 2022). To prevent static arbitrage in the IV surfaces, Ackerer et al. (2020) penalize the arbitrage constraints whereas Zheng et al. (2021) impose hard constraints on the neural network architecture. The advantage of these deep learning approaches is that they replicate market data better, especially when data is sparse. Additionally, they are able to embed financial constraints on IV surfaces through appropriate activation functions, neural network architectures, and loss functions.\nShifting from a static perspective to a dynamic one, Cont and Da Fonseca (2002) study IV surfaces through the lens of the Karhunen-Loe\u0301ve decomposition of their daily variations. Cont et al. (2002) build on this to jointly model the evolution of the IV surface and the underlying. Bloch and Bo\u0308o\u0308k (2021) model IV surface dynamics by introducing stochastic evolution of the parameters of the SVI model (Gatheral, 2004). They use deep learning (with architecture comprising convolutional LSTMs (Shi et al., 2015)) to capture the spatiotemporal relationship between the strikes and expiries. Shang and Kearney (2022) employ dynamic functional principal component analysis (FPCA) to forecast IV smiles (across deltas for a fixed maturity) in foreign exchange (FX) markets, using ARIMA to forecast the functional principal component coefficients (FPCCs). These works focus on one-step ahead point forecasts of IV smiles or surfaces, and are not full generative models.\nIn contrast with these works, we develop a conditional generative model that captures the historical temporal dynamics of the IV surfaces and allows us to simulate sequences of IV surfaces over multiple steps into the future. Furthermore, our proposed modeling paradigm allows us to incorporate (and simulate) other market observables such as asset prices, interest rates, trade volume, and VIX. The framework is very flexible and has many applications. For example, the generative model may be used to simulate a large number of sequences of IV surfaces, and any associated market data, which can then be fed as state inputs to train data-hungry reinforcement learning (RL) algorithms for hedging (Buehler et al., 2019; Cao et al., 2021), portfolio allocation (Coache and Jaimungal, 2021; Coache et al., 2022; Wang et al., 2022), statistical arbitrage strategies Zhao et al. (2022), and so on. We demonstrate the performance of our model by generating sequences of IV surfaces and corresponding asset prices for four popular equity tickers.\nOur model does not assume a parametric form for surfaces such as SSVI or SABR. Instead, we use functional data analysis (Ramsay and Silverman, 2005) in the spirit of Cont and Da Fonseca (2002) (who, however, model changes in log-IV surfaces) to provide a direct functional representation of IV surfaces and then model their evolution with nonMarkovian neural SDEs (Kidger et al., 2020; Chen et al., 2018; Tzen and Raginsky, 2019; Gierjatowicz et al., 2020; Cuchiero et al., 2020). Given the IVs on a discrete grid of option deltas and maturities, we project the data onto a set of Legendre basis functions to generate a complete surface. The Legendre basis functions provide good surface fits and have\nthe added advantage of being orthogonal which simplifies the task of Functional Principal Component Analysis (FPCA). Our modeling approach, however, is agnostic to the choice of basis functions. As a result, the main sources of variation in the surfaces (across all assets) are represented by the Functional Principal Components (FPCs). Each IV surface is then projected onto a lower dimensional set of FPCs, providing us with a time series of Functional Principal Component Coefficients (FPCCs) corresponding to IV surface evolution. In particular, we obtain a low dimensional model-agnostic characterization of complete IV surfaces.\nThe temporal dynamics of the FPCCs are captured by a neural SDE whose drift and diffusion are parametrized by neural networks comprising Gated Recurrent Units (GRUs) (Cho et al., 2014) coupled to feed forward (FF) layers. The choice of GRUs was driven by their ability to learn long range dependencies, with potentially fewer parameters than long short term memory (LSTM) layers. The time series of other relevant market data can also be incorporated into the neural SDE at this stage to simulate consistent market scenarios. Modeling the time series using a data-driven approach, like neural SDEs, rather than a parametric model like ARIMA or GARCH has the added advantage of reducing model misspecification. We impose a novel probability integral transform (PIT) penalty to reduce model misspecification even further, which to the best of our knowledge is the first of its kind. Static arbitrage constraints can be embedded into the neural SDE loss function, however, we demonstrate that the generated surfaces are mostly arbitrage-free. Specifically, we show using static arbitrage metrics that the generated surfaces have less overall static arbitrage than the market data which is used for training. Hence, we do not explicitly impose penalties for static arbitrage. A flowchart of the entire process may be viewed as in Figure 1. Our main contribution is the development of an approach that consistently models the evolution of multiple assets\u2019 IV surfaces, together with other relevant market data, in a model-agnostic manner.\nThe paradigm of generative modeling using deep learning has gained immense traction for the past decade since the seminal work of Goodfellow et al. (2020) on Generative Adversarial Networks (GANs). The aim of generative modeling is to implicitly learn the distribution of the observed data by generating samples from the target distribution. Training these models requires huge amounts of data, which while easily available for visual or audio data, is not readily available when it comes to time series data. This data scarcity is especially true in a financial context where just one realization of a stochastic process is observed, and the non-stationarity of market data means only a small subset of it is relevant for the modeling exercise in the present. The lack of consensus regarding reliable test metrics to assess the quality of generated time series data presents another challenge.\nGenerative models that specifically focus on IV surfaces have garnered considerable attention lately. Carmona et al. (2017) propose an arbitrage-free Monte-Carlo simulator for future paths of IV surfaces that is consistent with historical data based on tangent Le\u0301vy models. Wiese et al. (2019) use GANs to simulate prices of equity index options on a discrete grid of strike and maturity by working with the discrete local volatilities instead of IVs to avoid complex arbitrage constraints. Cuchiero et al. (2020) develop a GAN-based model for calibrating, on a fixed calendar date, a local stochastic volatility model parametrised by neural networks. Cohen et al. (2021) develop a factor-based model for European options in which they extract latent market factors from finitely many option prices directly rather\nthan from IVs as we do. The factor dynamics are modeled via neural SDEs, with the drift and diffusion neural networks having hard constraints to prevent static arbitrage. Cont and Vuletic\u0301 (2022) propose VolGAN that learns the one-step ahead joint conditional distribution of the changes in the log IV surfaces, the underlying return conditional on the realized volatility, the IV surface of the previous day, and the price returns for the past two days. It, however, generates the IVs on a fixed moneyness-maturity grid rather than modeling the entire surface. They also propose a second approach based on FPCA of the changes of the log IV surfaces and model the coefficients by Ornstein-Uhlenbeck processes, identical to what was done in Cont and Da Fonseca (2002), but now including a down-sampling method for reducing static arbitrage. Francois et al. (2023) model the joint dynamics of the IV surfaces and its underlying using a five-factor parametric representation of the IV surfaces. The underlying returns and the factor coefficients are modeled using GARCH-type models, with a Gaussian copula capturing the dependence structure. Our work contrasts with the extant literature, as we take a model-agnostic approach and use functional data analysis and neural-SDEs to characterize the dynamics of the entire surface together with other market factors.\nThe remainder of the paper is structured as follows. In Section 2, we discuss the projection of market data on a set of Legendre basis functions followed by a FPCA of the data. The training of a neural SDE model on the FPCCs is presented in Section 3. Section 4 deals with the metrics to quantify the amount of arbitrage in the generated surfaces in contrast to the observed ones. Finally, the results, together with a delta hedging exercise, are presented in Section 5."
        },
        {
            "heading": "2. Functional Data Projection",
            "text": "Vanilla financial options are liquid for only a limited collection of strikes (or deltas) and maturities. The pricing of exotic options and dynamic hedging, however requires the entire volatility surface. In the extant literature, there exists several methods for obtaining an IV surface from discrete values, including parametric and non-parametric techniques. Parametric approaches attempt to fit the discrete data points using a parametric model, which can then be used to interpolate and extrapolate to the whole IV surface. Common non-parametric approaches include using Nadaraya-Watson kernel estimators and FPCA (Shang and Kearney, 2022; Benko et al., 2009). Here, we model the whole IV surface using two-dimensional normalized Legendre polynomials as basis functions followed by FPCA to reduce the dimensionality. The basis functions in our case are product of Legendre polynomials parameterized by delta and maturity which enables us to obtain entire surfaces in these parameters rather than just IV smiles requiring further interpolation. Using FPCs as factors to model the IV surfaces enables us to capture the most important co-movements of IV across maturities and deltas simultaneously."
        },
        {
            "heading": "2.1 Basis Function Projection",
            "text": "We denote the sequence of historical IV surfaces by {\u03c3t}t\u2208T , where T := {1, . . . , T} denotes the collection of historical dates on which we have data. On each date t \u2208 T , a given IV surface is represented by \u03c3t := (\u03c3t(\u03c4i,\u2206i))i\u2208It where It := {i, . . . , It} \u2013 we may often use the more compact notation \u03c3t,i := \u03c3t(\u03c4i,\u2206i). Thus, (\u03c4i,\u2206i)i\u2208It represents the collection of\npairs of time to maturity and delta that IV data is available on day t. In principle, the (\u03c4,\u2206) pairs do not need to lie on a grid, nor do they need to be the same collection of pairs across all days. In practice, however, it may be convenient to have a fixed grid through time.\nOur first goal is to obtain a faithful representation of the IV surfaces across all days using a functional basis in (\u03c4,\u2206) space. The orthogonality of the basis is a desirable property as it simplifies the task of FPCA which we perform later. This is one of the advantages of choosing Legendre polynomials as basis over, e.g., B-splines. In our implementations, we use normalized Legendre polynomials to generate the basis. Recall that one representation of (normalized) Legendre polynomials of order-m is Lm(x) := \u221a 2m+1 2 1 2mm! dm dxm (x 2 \u2212 1)m. To this end, we denote the set of basis functions by (\u03d5b)b\u2208B with B := {1, . . . , B}, where \u03d5b : X \u2192 R and X := [\u22121, 1] \u00d7 [\u22121, 1]. Specifically, our set of basis functions (\u03d5b)b\u2208B is given by {Li(\u03c4)Lj(\u2206) : 0 \u2264 i + j \u2264 no; i, j \u2265 0} where B is an enumeration of the set {0 \u2264 i + j \u2264 no; i, j \u2265 0}. We find that no = 4 provides good surface fits to the data and hence obtain B = 12(no + 1)(no + 2) = 15 basis functions. In other words, we take all products of the first no order Legendre polynomials in \u03c4 and \u2206 such that the powers sum to at most no and where one Legendre polynomial applies to \u03c4 and the other to \u2206.\nWhile the (\u03c4,\u2206) pairs from data do not lie in X and \u03c3 are all positive, we first preprocess the data, as described in detail in 5.1, to map (\u03c4,\u2206) into X and \u03c3 into R. We\ndenote the transformed data as \u03c3\u0303, \u03c4\u0303 , and \u2206\u0303. With the hope that no confusion shall arise, we omit the tilde notation in this subsection from this point so that \u03c3t, \u03c4 and \u2206 refer to \u03c3\u0303t, \u03c4\u0303 and \u2206\u0303 respectively.\nThe normalized Legendre polynomials are orthonormal in the interval [\u22121, 1] in the following sense: \u222b 1\n\u22121 Ln(x)Lm(x)dx = \u03b4nm , \u2200n,m (1)\nwhere \u03b4nm is the Kronecker delta \u2013 which equals zero unless n = m, in which case it equals one. The orthonormality of the Legendre polynomials on [\u22121, 1] is inherited by our choice of basis functions on the space X . In particular we have that\u222b 1\n\u22121 \u222b 1 \u22121 \u03d5n(x, y)\u03d5m(x, y) dx dy = \u03b4nm , \u2200n,m \u2208 B . (2)\nGiven IV data, (\u03c3t)t\u2208T , for each day t \u2208 T , we project its transformed version (\u03c3\u0303t)t\u2208T onto the chosen basis to generate a continuous surface \u03c3\u030ct : X \u2192 R as follows\n\u03c3\u030ct(x, y) = \u2211 k\u2208B at,k \u03d5k(x, y) , (x, y) \u2208 X (3)\nwhere the sequence of coefficients ((at,k)k\u2208B)t\u2208T are estimated by minimizing the leastsquare errors\nat = argmin \u03b1t \u2211 i\u2208It\n( \u03c3t,i \u2212\n\u2211 k\u2208B \u03b1t,k \u03d5k(\u03c4i,\u2206i)\n)2 . (4)\nWe assume that |It| > |B| so that the regression is well posed. When working with multiple assets, the above optimization is carried out separately for each asset (albeit using the same basis functions), which leads to a distinct sequence of estimated coefficients for each asset ((at,k)k\u2208B)t\u2208T .\nFigure 2 shows a scatter plot of all pairs of the estimated coefficients for the four sets of IV surfaces that we analyse. The figure illustrates that there is a high degree of dependence between the coefficients, and this motivates us to perform a dimensional reduction on the functional regression by making use of FPCA. For a review of FPCA we refer to Ramsay and Silverman (2005). In the next subsections we provide some of the key results needed to develop the FPCs."
        },
        {
            "heading": "2.2 Obtaining functional principal components",
            "text": "We next summarize some of the key results needed to construct FPCs. Consider a compact set S \u2282 Rm and let K : S \u00d7 S \u2192 R be a symmetric function. Define the corresponding kernel operator acting on square-integrable functions f \u2208 L2(S) as K : L2(S) \u2192 L2(S) given by\n(Kf)(u) = \u222b S K(u, v) f(v) dv (5)\nThe kernel operator is said to be positive semi-definite if \u2200f \u2208 L2(S), we have\u222b S\u00d7S K(u, v) f(u) f(v) du dv \u2265 0 (6)\nTheorem 1 (Mercer\u2019s theorem (Steinwart and Scovel, 2012)) For symmetric functions K(u, v) that are continuous on S \u00d7 S and corresponding kernel operators K that are positive semi-definite, there exists an orthonormal sequence of continuous functions {\u03c8i \u2208 L2(S), i = 1, 2, . . .} which are eigenfunctions of K and eigenvalues {\u03bbi \u2265 0, i = 1, 2, . . .} such that \u2200x \u2208 S, (K\u03c8i)(u) = \u03bbi\u03c8i(u). Moreover, the function K can be written in terms of the eigenpair as\nK(u, v) = \u221e\u2211 i=1 \u03bbi \u03c8i(u) \u03c8i(v) (7)\nAs the eigenfunctions are orthonormal, we further have\u222b S \u03c8i(u) \u03c8j(u) du = \u03b4ij , (8) and hence \u221e\u2211 i=1 \u03bbi = \u222b S K(u, u) du <\u221e . (9)\nTheorem 2 (Karhunen-Loe\u0301ve expansion (Karhunen, 1947)) Consider a stochastic process X : S \u00d7 \u2126 \u2192 R that is mean-square continuous with X \u2208 L2(S \u00d7 \u2126). Then, the eigenfunctions \u03c8i of the kernel operator K associated with the covariance function K of the process X act as a basis such that \u2200u \u2208 S, the process can be represented as\nX(u)\u2212 E[X(u)] = \u221e\u2211 i=1 \u03bei \u03c8i(u) \u2208 L2(\u2126) , (10)\nwhere the coefficients \u03bei are random variables given by\n\u03bei = \u222b X X(u) \u03c8i(u) du . (11)\nsatisfying (i) E[\u03bei] = 0 and (ii) E[\u03bei\u03bej ] = \u03b4ij\u03bbi where \u03bbi is the eigenvalue corresponding to \u03c8i. The infinite series representation of X(u) converges uniformly on X with respect to the L2 norm.\nIn practice, the centered process X is approximated using a dimensionally reduced basis consisting of M eigenfunctions corresponding to the M largest eigenvalues, i.e.,\nX\u0302(u) = M\u2211 i=1 \u03bei \u03c8\u0302i(u) (12)\nand \u03c8\u0302 denotes the estimated eigenfunctions.\nLemma 3 The covariance function of process X given by K(u, v) = Cov(X(u), X(v)) is continuous on S if and only if the process X is mean-square continuous, i.e.\nlim \u03f5\u21920\nE[(X(u+ \u03f5)\u2212X(u))2] = 0 . (13)\nThe above lemma allows us to use the Karhunen-Loe\u0301ve expansion under the assumptions in Mercer\u2019s theorem. We next show how one estimates eigenfunctions and eigenvalues for a process X given solely observations of process {X1, . . . , XT } when \u2126 = T . Further details on how we link this process to implied volatility surfaces in our case may be found in Section 2.3. For this, we assume the data so that both the process and its observations are mean-centered. If they are not, we apply a shifting by their corresponding means. First, we project the centered observations onto the chosen basis functions and express the estimated eigenfunctions \u03c8\u0302 as a linear combination of the basis functions, as follows\nXt(u) = B\u2211\nk=1\nat,k \u03d5k(u) and \u03c8\u0302m(u) = B\u2211\nk=1\ncm,k \u03d5k(u) . (14)\nDenoting the matrix of time series of the coefficients by A, so that Atk = at,k, the vector of basis functions by \u03a6 = (\u03d51, . . . , \u03d5B), and the vector of the basis coefficients for the mth eigenfunction by cm = (cm,1, . . . , cm,B), (14) can be written in matrix notation as X(u) = A\u03a6(u) and \u03c8\u0302m(u) = \u03a6(u) \u22bacm. The empirical estimate of the covariance function K, denotes K\u0302, is given by the sample covariance of the observations\nK\u0302(u, v) = 1T \u03a6(u) \u22baA\u22baA\u03a6(v) . (15)\nUsing (5), we get (K\u03c8\u0302m)(u) = \u222b S [ 1 T \u03a6(u) \u22baA\u22baA\u03a6(v) ] \u03a6(v)\u22bacm dv = 1 T \u03a6(u) \u22baA\u22baAWcm , (16)\nwhere W = \u222b S \u03a6(v)\u03a6(v) \u22ba dv is the basis weight matrix. Hence, we obtain the eigenproblem\n1 T \u03a6(u) \u22baA\u22baAWcm = \u03bbm\u03a6(u) \u22bacm , \u2200u \u2208 X . (17)\nThe eigenproblem can be solved by pre-multiplying (17) by W 1/2 and integrating over the set S to get the finite dimensional problem(\n1 TW 1/2A\u22baAW 1/2 ) dm = \u03bbmdm , (18)\nwhere cm = W \u2212 1\n2dm. Working with orthonormal basis functions like normalized Legendre polynomials has the benefit that the basis weight matrix W is identity. The eigenproblem in such a scenario reduces to PCA on the matrix of coefficients A. Once we obtain the solution to the eigenproblem in (18), we can express the eigenfunctions in terms of the basis functions."
        },
        {
            "heading": "2.3 Projecting onto functional principal components",
            "text": "In our application setting, X = \u03c3, S = X = [\u22121, 1]\u00d7 [\u22121, 1] corresponding to the grid range of (\u03c4\u0303 , \u2206\u0303) and \u2126 = T . During the functional principal component analysis, we treat our implied volatility as a random field indexed by the tuple (\u03c4,\u2206) \u2208 S, and each observation of the IV surface on a given day t is treated as a realization of this random surface. In this section, when we speak of the IV surfaces we mean the transformed surfaces. With the hope that no confusion shall arise, we omit the tilde notation so that \u03c3t here means \u03c3\u0303t. Similarly, we refer to (\u03c4\u0303 , \u2206\u0303) simply by (\u03c4,\u2206). We choose the kernel function K to be the covariance function of IV, i.e. K(u, v) = Cov(\u03c3(u),\u03c3(v)) for (u, v) \u2208 X \u00d7X , which may be estimated by the sample covariance as\nK\u0302(u, v) = 1T T\u2211 t=1 (\u03c3t(u)\u2212 \u03c3(u)) (\u03c3t(v)\u2212 \u03c3(v)) where \u03c3(u) = 1T T\u2211 t=1 \u03c3t(u) . (19)\nThe above formulation of the sample covariance and sample mean is for a single asset. It can be easily extended to the multi-asset case by having an additional summation running over the number of assets.\nThe sample covariance function is continuous with the corresponding kernel being positive semi-definite thus ensuring the applicability of Mercer\u2019s theorem. Note that here we\ntreat the IV as a stochastic process indexed by maturity and delta, rather than time. As the assumptions of Mercer\u2019s theorem are satisfied by K\u0302(u, v), \u03c3(u) is mean-square continuous and hence the Karhunen-Loe\u0301ve expansion applies.\nWe thus employ the Karhunen-Loe\u0301ve expansion to express the IV process in terms of the B eigenfunctions obtained as in Section 2.2, and explicitly by solving the eigenproblem in (18). We do this using IVs from all assets after some transformations described in detail in Section 5.1. The resulting first M = 8 (eight) eigenfunctions are shown in Figure 3. These eigenfunctions explain approximately 99.5% of the variability of the surfaces, and their shape aligns well with the expected characteristics of the IV surfaces, namely term structure, skew, and convexity. Further details on the interpretation of the FPC surfaces can be found in Section 5.2.\nTo reduce the dimension of the data, we next project the data onto this reduced set of M eigenfunctions and express the surfaces as\n\u03c3\u0302t(\u03c4,\u2206) = M\u2211\nm=1\nbt,m \u03c8m(\u03c4,\u2206) . (20)\nThe sequence of coefficients ((bt,m)) are obtained by minimizing the sum of squared errors as in (4), but with the original basis functions replaced with the first M eigenfunctions. Once again, the multi-asset case will have a distinct sequence of coefficients corresponding to each asset. We do not add any additional penalization, such as penalties associated\nwith butterfly spread or calendar spread arbitrage, however, as we show in Section 5.4, our generator produces surfaces that are as free of arbitrage as the original surfaces.\nUsing this procedure, we obtain FPCCs as a time series (bt)t\u2208T , bt \u2208 RM . In the next section, we proceed to model the time series of these FPCCs using neural SDEs. In this manner, we are able to construct dynamical surfaces from a low-dimensional set of coefficients, but in a manner that encodes path dependency."
        },
        {
            "heading": "3. Neural stochastic differential equations",
            "text": "In this section, we describe how we model the time series of FPCCs. Our key assumption on the dynamics is that FPCCs are driven by a neural SDE where the drift and diffusion terms do not necessarily have a Markovian structure, i.e., they may depend on the entire history of the coefficients, and furthermore, their values are obtained as outputs of neural networks. Consider an M -dimensional adapted Brownian motion W = (Wt)t\u22650 defined on the probability space (\u2126,F , {Ft}t\u22650,P), with {Ft}t\u22650 the natural filtration generated by the Brownian motion. We assume that the FPCCs (bt)t\u22650 satisfy the SDE\ndbt = \u03bdt dt+ \u03b7t dWt , (21)\nwhere \u03bdt \u2208 RM and \u03b7t \u2208 RM\u00d7M are Ft-measurable and Lipschitz, and correspond to the drift and diffusion respectively. We do not, however, assume any specific parametric form for the drift and diffusion processes but rather learn them via a neural network. The structure of the neural network given in Figure 4 ensures that the drift and diffusion are Lipschitz continuous, as it is essentially a composition of linear transformations and activation functions with Lipschitz constant 1, albeit computing its Lipschitz constant may be challenging. Moreover, the initial condition is fixed, and hence we can guarantee the existence and uniqueness of the solution to the SDE.\nAt this stage, it is possible to incorporate other time series data, such as equity prices, trading volume, open interest, or any other features the modeler wishes to enhance the data with. This has the added advantage of being able to model the dependence between equity prices, their option\u2019s IVs, and other features. For our implementations, we restricted to including only equity prices for simplicity. Moreover, in the multi-asset case, we can concatenate the FPCCs and corresponding market data of each asset to obtain a higherdimensional time series. We can model this concatenated time series via the neural SDE to capture the correlations across the different assets, as we later show in our results."
        },
        {
            "heading": "3.1 Neural Architecture",
            "text": "The SDE in (21) may be approximated using an Euler discretization by introducing a time grid {0, \u2206t, 2\u2206t, . . . } and approximating\nbt+\u2206t \u2212 bt = \u03bdt \u2206t+ \u03b7t (Wt+\u2206t \u2212Wt) . (22)\nAs a consequence, we have that\nbt+\u2206t|Ft \u223c N (bt + \u03bdt \u2206t , \u03b7t \u03b7 \u22ba t\u2206t) . (23)\nFor computational efficiency, we assume the drift and diffusion coefficients depend on the last L lags of the discrete process and are given as outputs of neural networks with parameters \u03b8 and \u03b3, respectively. Both neural networks have the structure shown in Figure 4, which consists of multiple gated recurrent unit (GRU) layers (Cho et al. Cho et al. (2014)) coupled to a feed forward (FF) output layer. Rather than modeling the diffusion directly, the \u03b3 network models its Cholesky decomposition. We do so, by reshaping the output of the \u03b3 network to a lower triangular matrix denoted Lt and, to ensure positive definiteness of the resulting covariance matrix, set \u03b7t \u03b7 \u22ba t = LtL \u22ba t +\u03b5 I, where I is the identity matrix and 0 < \u03b5\u226a 1.\nWe opt to use GRU layers to learn the path dependency of the FPCCs drift and volatility due to their ability to learn long range dependencies, as well as their ability to tackle the vanishing gradients problem by the use of reset and update gates.\nWe perform a neural architecture search with a variety of alternative architectures for the \u03b8 and \u03b3 neural networks that model the conditional drift and diffusion. For example, instead of having two distinct networks, we experimented with an architecture whereby the outputs of a single three layered GRU were mapped through (a) one single FF neural net, and (b) two seperate FF neural networks to generate the conditional drifts and diffusions. Both of these architectures under-performed the seperate GRU models and FF layers for drift and diffusion described above. A plausible explanation for this observation is that the non-Markovian features learnt by the hidden states in the GRU are qualitatively different for the conditional drift and conditional diffusion."
        },
        {
            "heading": "3.2 Training",
            "text": "To enhance the stability of the model, we learn the parameters \u03b8 and \u03b3 in an iterative manner, and the entire process consists of three steps.\nIn the first step, we estimate \u03b8\u0302 by minimizing the mean squared error (MSE) of the observed time series and its estimated conditional mean. The optimization problem at this stage is given by (here, and in the sequel, the subscript of the FPCCs denote the time count index and not time itself)\n\u03b8\u0302 = argmin \u03b8\n1\nT \u2212 L T\u2211 t=L+1 (bt \u2212 bt\u22121 \u2212 \u03bdt\u22121\u2206t)\u22ba(bt \u2212 bt\u22121 \u2212 \u03bdt\u22121\u2206t) . (24)\nIn the second step, we learn the parameters \u03b3\u0302 of the diffusion neural network while keeping \u03b8\u0302 fixed at the minima above. In this step, we aim to maximize the log-likelihood of the observed time series.\nTo help aid in avoiding model misspecification, we regularize by including an additional penalty in the objective function. In the absence of model misspecification, the sequence of marginal PITs (Angus, 1994) has a standard uniform distribution. Hence, we add a penalty corresponding to deviations of the marginal PIT from being uniform across all the features. For completeness, we next recall what the PIT of a sequence of random variables are, followed by a proposition which relates the distribution of the sequence of PITs to the misspecification of the data generating process. Finally, we will define the PIT penalty we work with.\nDefinition 4 Given a sequence of univariate random variables {Xt}t=1,...,T with Xt having the cumulative distribution Ft, the probability integral transform is the sequence of random variables {Ut}t=1,...,T given by Ut = Ft(Xt).\nProposition 5 (Diebold et al. (1998)) Suppose the true data generating process of univariate {Xt}t=1,...,T is governed by the sequence of distributions {Gt(Xt|Xt\u22121, . . . , X1)}t=1,...,T and let the learnt conditional distribution forecasts from the generative model be given by {Ft(Xt|Xt\u22121, . . . , X1)}t=1,...,T . In the absence of any model misspecification of the data generating process, in which case the sequence of conditional distributions {Ft}t=1,...,T and {Gt}t=1,...,T coincide, the sequence of probability integral transforms of {Xt}t=1,...,T with respect to {Ft}t=1,...,T are i.i.d. Uniform(0,1).\nThus, to reduce model misspecification of our generative model, we impose an additional penalty on the sequence of PITs for each feature of the time series. In the case of the time series of FPCCs, we have that bt|Ft \u223c N (\u00b5t\u22121,\u03a3t\u22121) where \u00b5t := bt + \u03bdt\u2206t and \u03a3t := (LtL \u22ba t + \u03f5I)\u2206t. For the time series of FPCCs, we obtain a sequence of PITs (U i t )t\u2208T ,i\u2208M\ngiven by U it = \u03a6 ( bit\u2212(\u00b5t\u22121)i (\u03a3t\u22121)ii ) , where \u03a6 is the CDF of the standard normal distribution.\nDefinition 6 The PIT penalty corresponding to a sequence (bt)t\u2208T is given by\nPIT (b) = M\u2211 i=1 \u222b 1 0 (\u03c1i(u)\u2212 1)2du , (25)\nwhere, for each i \u2208M, \u03c1i is the Kernel density estimator of (U it )t\u2208T using the Gaussian kernel with 10% of Silverman\u2019s estimate (Silverman, 1986) as the bandwidth.\nUsing the estimated value of \u03b8\u0302 in the first step, the optimization problem in the second step is given by\n\u03b3\u0302 = argmin \u03b3\n( \u2212\nT\u2211 t=L+1 log \u03d5(bt;\u00b5t\u22121,\u03a3t\u22121) + \u03b1 PIT (b)\n) , (26)\nwhere \u03d5(\u00b7;\u00b5,\u03a3) is the density of a multivariate normal with mean \u00b5 and covariance \u03a3, and \u03b1 is a hyperparameter \u2013 we discuss how we set it in Section 5.3.\nIn the third step of our optimization procedure, we jointly optimize both \u03b8 and \u03b3 using the same objective function as in the second step:\n\u03b8\u0302, \u03b3\u0302 = argmin \u03b8,\u03b3\n( \u2212\nT\u2211 t=L+1 log \u03d5(bt;\u00b5t\u22121,\u03a3t\u22121) + \u03b1 \u2032 PIT (b)\n) , (27)\nbut with a potentially different value for the hyperparameter \u03b1\u2032."
        },
        {
            "heading": "3.3 Generating Surfaces",
            "text": "Once the neural SDE model is trained, we run a forward pass through it using the time series of FPCCs for the last L lags to obtain the mean and covariance estimates for the FPCC at the next time step. We then recursively apply the one-step transition\nbt+1|Ft \u223c N (bt + \u03bdt\u2206t , \u03a3t) , (28)\nby sampling from a multivariate normal, and where \u03bdt and \u03a3t are provided by the neural net architecture discussed previously, and use the last L lags of b as inputs. Algorithm 1 summarizes the approach.\nAlgorithm 1 Pseudocode to generate surfaces in the future\nInput: FPCCs for the last L lags bt=t0\u2212L+1:t0 , number of future time steps n, learnt neural SDE models \u03b8\u0302 and \u03b3\u0302, learnt FPCs {\u03c8i}i\u2208M, \u03b5 = 10\u22123 Output: IV surface (\u03c3t)t=t0+1:t0+n 1: for t = t0 : t0 + n\u2212 1 do 2: \u03bdt, Lt \u2190 bt\u2212L:t using \u03b8\u0302 and \u03b3\u0302 networks 3: bt+1 \u223c N (bt + \u03bdt\u2206t, (LtL\u22bat + \u03b5I)\u2206t) 4: store IV surface \u03c3t+1 = \u2211 i\u2208M bt+1,i \u03c8i\n5: end for"
        },
        {
            "heading": "4. Quantifying Arbitrage",
            "text": "The lack of consensus for assessing the quality of generated samples for time series data in the extant literature makes assessing the quality of generated surfaces challenge. For a model to be useful it needs to be able to replicate the data in the market while remaining\narbitrage free. While our trained models have PITs that are very close to uniform, which is one measure of goodness of fit from an academic viewpoint, from a traders perspective, it is important to quantify the amount of arbitrage there is in generated surfaces. To this end, in this section, we present metrics for quantifying the amount of arbitrage observed in both the historical data and in the generated surfaces, and we demonstrate that our generated surfaces are in line with historical data in Section 5.4. Thus, we provide both the academic and practitioner\u2019s evaluation of the model.\nSpecifically, we focus on butterfly and calendar spread metrics which quantify the presence/absence of static arbitrage and are measures that are often employed in practice for making trading decisions. For this purpose, we use the conditions presented by Gatheral and Jacquier (2014) (see Lemma 2.1 and 2.2). We then compare the distribution of these metrics for observed and synthetic data over a 30-day period following the training period. To obtain the distribution of the arbitrage metrics for synthetic data, we use 100,000 independent paths with IV generated on the same delta-maturity grid as the observed data.\nAlthough we model the IV as a function of option deltas, Gatheral\u2019s conditions on the absence of arbitrage are given in terms of the total implied variance and the log-moneyness m := log KS (where K is the option\u2019s strike and S the underlying spot price). Given an option\u2019s delta, time to maturity and IV, we obtain its log-moneyness as follows (assuming zero interest rates):\nm = \u2212\u03a6\u22121(\u03b4)\u03c3 \u221a \u03c4 + \u03c32\u03c4\n2 . (29)\nThe total implied variance of an option with log-moneyness m and time to expiry \u03c4 is defined as w(m, \u03c4) := \u03c32(m, \u03c4)\u03c4 where \u03c3(m, \u03c4) is the IV of the option.\nDefinition 7 An IV surface is free of calendar spread arbitrage if\n\u2202\u03c4w(m, \u03c4) \u2265 0, \u2200m \u2208 R, \u03c4 > 0 . (30)\nCorrespondingly, we define the calendar spread metrics for a given IV surface by \u2202\u03c4w(m, \u03c4). Given the IV surface \u03c3t on day t, the calendar spread metrics are given by {CSi,j}i=1,...,ne\u22121;j=1,...,nd where\nCSi,j = w(mj , \u03c4i)\u2212 w(mj , \u03c4i\u22121)\n\u03c4i \u2212 \u03c4i\u22121 , (31)\nand {mj}j=1,...,nd and {\u03c4i}i=1,...,ne are a fixed grid of points on which we estimate the calendar spread.\nDefinition 8 A slice of the IV surface w(m) (we drop the \u03c4 for simplicity of notation) obtained by fixing maturity \u03c4 is free of butterfly spread arbitrage if\ng\u03c4 (m) :=\n( 1\u2212 m\u2202mw(m)\n2w(m)\n)2 \u2212 (\u2202mw(m)) 2\n4\n( 1\nw(m) +\n1\n4\n) + \u22022mmw(m)\n2 \u2265 0, \u2200m \u2208 R (32)\nCorrespondingly, we denote the butterfly spread metrics for a given IV \u03c3t on day t by {BSi,j}i=1,...,ne;j=1,...,nd\u22122 where BSi,j = g\u03c4i(mj) for {mj}j=1,...,nd and {\u03c4i}i=1,...,ne a given fixed grid of points. The partial derivatives in (32) are estimated by central differencing."
        },
        {
            "heading": "5. Results",
            "text": "In this section, we illustrate our methodology to generate synthetic IV surfaces for equity options. In addition to generating IV surfaces, we also generate price paths for the assets. Moreover, we generate synthetic samples for four (4) equities simultaneously. This has the added benefit that each generated scenario is consistent across all four equities. The data used for these purposes along with the relevant transformations are discussed in Section 5.1. Section 5.2 deals with the functional projection data to obtain the FPCCs, and Section 5.3 discusses the neural SDE model for FPCCs. Section 5.4 presents samples of the generated surfaces, as well as price paths, along with the metrics that quantify any arbitrage opportunities found in the generated surfaces. Finally, in Section 5.5 we conduct a simple delta-hedging exercise as an additional validation of the quality of the generated data."
        },
        {
            "heading": "5.1 Data and Transformations",
            "text": "We source IV data for American call options on Amazon (AMZN), Intel (INTC), IBM (IBM) and Tesla (TSLA) and the corresponding asset price for the time period July 8, 2010 to December 31, 2021 from Option Metrics via Wharton Research Data Services [31] (WRDS). Data is retained for just those days where all four (4) equities are available, totaling to 2,893 observations. For each ticker, we have IV data on a grid of 17 deltas in the range of 0.1 to 0.9 at intervals of 0.05, and 11 maturities ranging from 10 calendar days to 2 years. The volatility values on this grid are obtained by interpolating actual values, which is done internally in the Option Metrics data, and not by ourselves. Hence, there is no guarantee of the IV surfaces being arbitrage-free. Given that the observed values are on a discrete grid, we first project this data onto the orthonormal Legendre polynomial basis functions to obtain entire surfaces and then perform FPCA to reduce the dimensionality of the problem, as described in Sections 2.1 and 2.2. The range of values for delta and maturity, however, are not in [\u22121, 1], but rather in [0, 1]\u00d7 [0, 2]. The Legendre polynomials are not orthogonal in this range. Therefore, we perform a series of transformations to render them amenable to analysis.\nFirstly, the deltas are transformed as \u2206 7\u2192 \u2206\u0303 := 2\u2206 \u2212 1 so that they lie in the range [\u22121, 1]. The option maturity are transformed as \u03c4 7\u2192 \u03c4\u0303 := \u221a \u03c4 to prevent clustering of observed data near the short maturity axis in the transformed space. This leads to more robust fits of the IV surface towards the longer expiry. This transformation is followed by scaling and shifting the transformed values for them to lie in [\u22121, 1] so that all together \u03c4 7\u2192 \u03c4\u0303 := 2( \u221a \u03c4/\u03c4ne) \u2212 1, where \u03c4ne = 2 is the longest time to expiry in the data. To ensure the positivity of the generated IV surfaces, we perform the transformation \u03c3 7\u2192 \u03c3\u0303 := c0 + c1 log(e\n\u03c3 \u2212 1), where c0,1 are constants chosen such that only 10% of the transformed data lies outside the range [\u22121, 1], i.e., P(\u03c3\u0303 < \u22121) = P(\u03c3\u0303 > 1) = 0.1. The constants c0,1 vary across equities, as the various equities have varying levels of IV. For a particular equity e, denote the qth empirical quantile of {log(e\u03c3t,i \u2212 1)}t\u2208T ,i\u2208I by Qeq, then\nce0 = \u2212 Qe0.9 +Q e 0.1\nQe0.9 \u2212Qe0.1 and ce1 =\n2\nQe0.9 \u2212Qe0.1 . (33)\nFor equity prices, we perform a similar linear transform to ensure the transformed data lie with 90% probability in the range [\u22121, 1], i.e., S 7\u2192 S\u0303 := c0 + c1 S, where c0,1 are\nobtained as above, but using the empirical quantiles of {St}t\u2208T . As with the original prices, the transformed prices have an increasing trend over time, therefore we detrend them by regressing \u03b2e0 + \u03b2 e 1 t and subtracting the regression from S\u0303t.\nThere is a two-fold rationale behind the set transformations on IV surfaces and equity prices. First, the IV transformations ensure that the generated IV surfaces (after inverting the transformations) are always positive. Second, the observed data has vastly different ranges for IV and prices, and it also differs across different equities. The linear transformations normalize the data to enhance the learning of the neural SDE."
        },
        {
            "heading": "5.2 Projecting data onto surfaces",
            "text": "The transformed data is first projected onto orthonormalized Legendre polynomials following the methodology in Section 2.1. Figure 2 shows the pairwise scatter plots of the time series of coefficients {at,k} and suggests there is a high degree of correlation across many pairs. Therefore, we perform FPCA to obtain a collection of functional principal component surfaces to remove this correlation. It is important to note that the FPCs we obtain are common across all assets. Once the FPCs are obtained following Section 2.2, we choose the M largest principal components that explain at least 99.5% of the variability in the data, for the given data. We find that M = 8 FPCs explain more than 99.5% of the variability and they are presented in Figure 3.\nThe shape of the resulting FPCs align well with the expected characteristics of the IV surfaces, namely term structure, skew, and convexity. The first two FPCs highlight the term structure of the IV, with the IV decreasing with increasing time to maturity, the decrease happening at a faster rate in the second FPC. The volatility skew is reflected in the third FPC most prominently in addition to the first two. The fourth FPC appears to induce skew in one direction for the short term and the opposite in the long term \u2013 in others it induces twists and skew reversals in the surface. The fifth FPC mostly accounts for inducing convexity for short term maturities. A combination of all these FPCs enables us to capture the statistical properties of the IV surfaces observed empirically.\nNext, we project the transformed data onto the reduced set of FPCs as per Section 2.3 to obtain the estimated time series of FPCCs ((bt,i)i\u2208M)t\u2208T . Figure 5 shows the pairwise scatter plots of these coefficients, and in comparison to Figure 2, we see that there is little to zero pairwise correlation among these new sets of coefficients \u2013 hence, as expected, the FPCs represent orthogonal degrees of freedom of the surface dynamics. To measure the efficiency of the FPCs in compressing the full IV surface, we compare the RMSE of daily IV surface fits when using the set of Legendre basis functions vs the FPCs. The median RMSE when using the Legendre basis functions vs the FPCs are 0.006, 0.0076 for AMZN, 0.0063, 0.0072 for IBM, 0.0079, 0.0092 for INTC and 0.0082, 0.0103 for TSLA respectively. Figure 14 in the appendix shows the evolution of the RMSE over time for all the assets. The figures indicate that the surface fits are good across time with no period having significantly worse fits when compared to others. Moreover, to have a closer look at the distribution of the RMSE values and how much they differ when compressing or not, we also present histograms of the RMSE in Figure 15 in the appendix. The figure indicates that the RMSE when using FPCs is right shifted very slightly compared with the full basis, again affirming the fact\nthat the FPCs provide good fits with the benefits of dimension reduction outweighing the slight increase in RMSE."
        },
        {
            "heading": "5.3 Training the neural SDEs",
            "text": "In this section we describe how the neural SDE is trained. First, we concatenate the time series of the estimated FPCCs and equity prices for all assets, resulting in a 36-dimensional time series. The input time series is normalized feature-wise by subtracting the median followed by division with the inter-quartile range. We next model the transformed time\nseries using the neural SDEs described in Section 3. In the sequel, we make a slight abuse of notation and refer to the transformed values by bt as well. We use the first ninety percent of the available data (2,603 observations from July 8, 2010 to November 5, 2020) to train the \u03b8 and \u03b3 networks. We conduct training in the three phases as detailed in Section 3.2, and use the AdamW optimizer. In the first step, we perform 700,000 iterations to minimize the mean squared error (24) and obtain an initial estimate \u03b8\u0302 for the \u03b8 network. The model with the least error is retained and used in the second step where we minimize the sum of the negative log-likelihood and PIT penalty (26), with a hyperparameter of \u03b1 = 1, by varying only the \u03b3 network to obtain an estimate \u03b3\u0302. We use 100,000 iterations at this stage. Finally, we minimize the loss (27), with a hyperparameter of \u03b1\u2032 = 100, over both networks to obtain the final estimates for the \u03b8 and \u03b3 networks.\nThe hyperparameter \u03b1\u2032 is selected as the order of magnitude of the absolute ratio of log-likelihood and the PIT penalty at the end of stage two. With this choice, we give equal weights to both the objectives, and the log-likelihood and the PIT penalty converged individually. An alternative we considered was updating \u03b1 dynamically every 10,000 iterations, with its value being set as the ratio of the log-likelihood and the PIT penalty. This resulted in a less stable training regimen and we opted for the static value mentioned above.\nThe various losses as training progresses are shown in Figure 6. All losses converged, with the exception of the MSE. The kink in the MSE where it starts increasing occurs at the onset of the third stage of training where both \u03b8 and \u03b3 are optimized simultaneously to minimize the sum of negative log-likelihood and the PIT penalty. In this stage, MSE is not explicitly being minimized, hence there is no need for it to converge. Nonetheless, we show the evolution of this loss to assess how far the training of both networks worsens the mean-predictive error.\n0 2 b 1\nAMZN\n0\n2\n4\n6 IBM\n0\n2\n4\nINTC\n0\n1\n2\n3\nTSLA\n2\n0\n2\n4\nb 2\n2\n0\n2\n2\n0\n2\n2.5\n0.0\n2.5\n2\n0\n2\nb 3\n2\n0\n2\n2\n0\n2\n2\n0\n2\n0\n2\nb 4\n2\n0\n2\n2\n0\n2\n0\n2\n2\n0\n2\n4\nb 5\n2\n0\n2\n2\n0\n2\n4\n2\n0\n2\n0\n2\nb 6\n2\n0\n2\n4\n0.0\n2.5\n5.0\n2\n0\n2\n2\n0\n2\nb 7\n2\n0\n2\n4\n2\n0\n2\n4\n2\n0\n1\n0\n1\nb 8\n2\n0\n2\n2\n0\n2\n2\n0\n2400 2450 2500 2550 2600\n0\n2\nS\n2400 2450 2500 2550 2600\n2\n0\n2400 2450 2500 2550 2600\n1\n0\n1\n2\n2400 2450 2500 2550 2600 1\n0\n1\nDay of training data\nFigure 7: Plot showing the observed transformed FPCCs and prices (in red), its predicted mean (in black) and associated 95% confidence bands (in green) for the last 200 days of training data from the trained model. The first 8 rows correspond to the transformed FPCCs whereas the last row represents the transformed prices. Each column corresponds to one of the equities.\nWe retain the model that achieves the minimum loss during stage three of the training for synthetic surface generation. Figure 7 shows the predicted mean and 95% confidence bands of the time series of FPCCs and equity prices for all the equities during the last 200 days of training data. The figure suggests that the neural SDE learns the dynamics of the evolution of the time series with the predicted mean closely tracking the observed values and the observed values lying within the confidence bands. The distribution of the PITs of each feature of the time series in Figure 8 are very close to U(0, 1), indicating little to no model misspecification. The one common observation across the PITs is that there are two small spikes at the very extreme left and right (near 0 and 1). These small spikes indicate that the predictive model induced by the neural SDE has just slightly too little weight in the tails compared to the data. One reason for this may is that we use Brownian motions to drive the neural SDE. It may be possible to improve on this by adding in a jump component, such as one driven by a Poisson random measure with compensator given by the output of yet a third neural network. However, as these tail deviations are insignificant, our model fit is acceptable for applications.\nTo further demonstrate the impact that the PIT penalty has on reducing model misspecification, we also present the distribution of the PITs when the PIT penalty is not part of the training objective in Figure 16. The PITs in this case are not as close in distribution to U(0, 1) compared to when the PIT was part of the objective function. This is further evidenced by the Kolmogorov-Smirnov test on the PITs. When the PIT penalty is minimized, the null hypothesis is rejected for none of the 36 features in the time series at both the 1% and the 5% significance level whereas in the other case, it is rejected 13 and 25 times respectively. This clearly indicates that including the PIT penalty does indeed reduce model misspecification significantly. Further testing to illustrate that the learnt neural SDE has indeed captured the correlation structure across features as well as the long-range dependencies in the time series are presented in Appendix A."
        },
        {
            "heading": "5.4 Simulated Surfaces",
            "text": "The trained neural SDE model may be used to generate a sequence of FPCCs and prices over multiple consecutive days. These FPCCs then induce IV surfaces through the representation in (20), and, hence, the simulation provides us with a distribution over sequences of surfaces. First, however, we must invert the transformations from Section 5.1. For our experiments, we generate 100,000 independent paths of bte+1:te+30, where te is the end of the training period, and use them to reconstruct surfaces and price paths.\nFigure 9 shows 100 randomly selected (from 100,000) price paths over the 30 days along with the price quantiles (red). By comparing the quantiles with the observed prices (black), the figure suggests that the generated paths are reasonable and can account for scenarios where a drastic upturn or downturn is observed in the market \u2013 e.g., the case of TSLA where the 99th quantile closely approximates the actual prices day 15 onwards. The synthetic data is therefore robust for downstream use as it is able to capture extreme scenarios well.\nFigure 10 shows the evolution of the IV surfaces, for the four assets, over a 5 day window for one of the randomly chosen scenarios. The figure illustrates that the generated surfaces show typical dynamics seen in the training data and, as we simultaneously generate all IV surfaces across all assets, we see some dependence across the assets. Providing confidence intervals across surfaces, and through time, and comparing with the out-of-sample surfaces, akin to what we show for prices in Figure 9, is not feasible. Instead, next, we provide a comparison of the calendar and butterfly spread metrics introduced in Section 4 from our simulated surfaces and compare to the in- and out-of-sample surfaces.\nThe distribution of the arbitrage metrics for training, test, and synthetic data are shown in Figures 11 and 12. The distribution of the butterfly spread metrics overlap to a great extent, however, the training data has slightly heavier tails. The calendar spread metrics for the generated data lie closer to the test data than the training data, and overlaps almost completely for IBM and INTC. The generated (and test) data are further to the right from zero compared to the training data. This supports the conclusion that the generated surfaces has no more static arbitrage than the training data used to learn the model. As the generated data are simulated from the end of the training period, the local information at\nthe end of this time frame embedded in the hidden states of the GRUs allows our simulated surfaces to capture the shift in the arbitrage metrics from training to test data.\n0.25 0.00 0.25 0.500\n10\n20\n30\n40\nDe ns\nity\nAMZN Synthetic Test Training\n0.25 0.00 0.25 0.500\n7\n14\n21\n28\nIBM\n0.25 0.00 0.25 0.500\n5\n10\n15\n20\nINTC\n0.5 0.0 0.5 1.00\n2\n4\n6\n8 TSLA\nCalendar spread metrics\nFigure 12: Distribution of the calendar spread metrics for the training data between July 8, 2010 to November 5, 2020 (Days 1-2603), test data between November 6 to December 18, 2020 (Days 2604-2633) and the 100,000 paths of the generated data for 30 days starting November 6, 2020.\nTo further quantify arbitrage, we show in Tables 1 and 2 the quantiles of the butterfly and calendar spread metrics for the training data and synthetic data (over 100,000 scenarios) for all four equities. The generated data has positive butterfly spread metrics except for IBM and INTC at the 0.001 quantile. This is in line with the training data, which also has negative metrics for in these extreme cases. While for IBM and INTC the training data has negative metrics at the 0.01 quantile, the synthetic data has positive metric values at these levels which indicates that the generated surfaces are free of static-arbitrage. The results are even more encouraging when considering the calendar spread metrics where our simulated surfaces have positive metrics at all quantile levels across all equities. This extends even to\nthe 0.001 quantile level where, in the training data, all equities except AMZN have negative calendar spread metrics.\nTo further quantify arbitrage across an entire surface, we introduce a summary statistic that equals one if any point on a surface has a negative arbitrage metric and zero otherwise. The sum of this summary statistic over all test days and training days are reported in the last two columns of Tables 3 and 4. To compare with our simulator, we generate the summary statistics along every simulated path of thirty days, and sum them. The first seven\ncolumns of Tables 3 and 4 reports the quantiles of this total summary statistics across the simulation scenarios. Thus, for e.g., looking at Table 3, for IBM at the 95% quantile there are six out of thirty days that have at least one point on the IV surface that contains a butterfly spread arbitrage, for the test data there are three days in the same period, while in the training data that number is 1,457 days out of 2,603 days. Note that to compute the metrics using historical data, we use finite difference approximations for computing the derivatives that appear in (30) and (32). From our simulated data, we could use a much finer grid, or in principle derive semi-analytical formulae for the same metrics, in which case our metrics will improve even further."
        },
        {
            "heading": "5.5 Delta Hedging",
            "text": "To further ensure the accuracy, and faithfulness to historical patterns, of the generated IV surfaces and asset prices, we conduct a delta hedging exercise. This involves implementing a self-financing strategy using the underlying asset and an interest-free bank account. On Day 0, the last day of our training data (November 5, 2020), we sell an at-the-money (ATM) European call option expiring in \u03c4h0 = 30 days with a strike denoted K\nh. At day t, the option\u2019s time to expiry is now \u03c4ht := \u03c4 h 0 \u2212 t, the underlying asset price is St, the price of the option is Pt, and the corresponding IV is \u2206t. We implement a hedge that equals to the prevailing delta in the market, which is the solution to the non-linear equation\n\u2206ht = \u03a6  log(St/K) + (\u03c3t(\u2206ht , \u03c4ht )2/2)\u03c4ht \u03c3t(\u2206ht , \u03c4 h t ) \u221a \u03c4ht  (34) where \u03c3t(\u00b7, \u00b7) is the simulated IV surface at time t. This delta represents the number of units of asset that the hedge strategy holds at time t.\nTo be delta neutral, we hedge our position at day 0 by having a long position of \u2206h0 in the underlying asset, where \u2206ht represents the Black-Scholes delta of the sold European call at day t. Here we make two simplifying assumptions: (i) zero-interest rates, an assumption we make throughout the paper, and (ii) zero transaction costs with an ability to purchase fractional units of the asset. As a result, the bank account has a balance of B0 = P0 \u2212 \u2206h0S0. Subsequently, we rebalance our position every day to stay delta neutral by buying (\u2206ht \u2212\u2206ht\u22121) units of the underlying so that the bank account at the end of day t is given by Bt = Bt\u22121 \u2212 (\u2206ht \u2212\u2206ht\u22121)St. Finally, at day \u03c4h0 we settle our position by paying the option payoff (S\u03c4h0\n\u2212 K)+ as well as liquidating our position in the underlying leaving us with a P&L of B\u03c4h0 \u22121 \u2212 (S\u03c4h0 \u2212K)+ +\u2206 h \u03c4h0 \u22121 S\u03c4h0 .\nWe execute the delta hedging procedure on four equities using 10,000 simulated paths and the actual path. The distribution of the P&L across the synthetic paths and the observed data is depicted in Figure 13. To establish a reference point for the P&L, we report the initial price of the at-the-money (ATM) option and the asset on Day 0, as well as\nother features such as IV and Black-Scholes delta in Table 5. Figure 13 indicates that the P&L distribution is centered around zero and the P&L generated from the realized path lands in the bulk of this distribution. Additionally, considering the ATM option\u2019s price and the asset price scale, we deem the range of P&L appears to be reasonable."
        },
        {
            "heading": "6. Conclusions",
            "text": "In this paper we developed a combined functional data projection approach, coupled with neural SDEs, to model dynamical surfaces. The approach allows us to faithfully represent the historical dynamics of IV surfaces across multiple assets simultaneously. Our generative model is also able to produce essentially arbitrage free surfaces, even though the training data contains arbitrage and no additional penalties have been added for deviations from the arbitrage free submanifold of surfaces. We are able to generate IV surfaces simultaneously for multiple assets as well as asset prices themselves. If the user wishes, they may add any additional features into the neural SDE modeling to enhance the surface generation. Another avenue for future work is to experiment with the neural architecture for the drift and diffusion by considering, for instance, forward-backward GRU layers, or replace them entirely with long short term memory (LSTM) layers, attention networks, and so on. There are a multitude of potential applications of the resulting surface generator, including but not limited to using the surfaces to generate hedging strategies for path-dependent financial options, such as auto-callables, portfolio allocation problems where options are part of the portfolio, and/or obtaining statistical arbitrage strategies with options."
        },
        {
            "heading": "Acknowledgments",
            "text": "S.J. acknowledges the support of the Natural Sciences & Engineering Research council of Canada through NSERC Alliance [ALLRP 550308 - 20] and the Data Science Institute, University of Toronto. The authors would also like to thank Ivan Sergienko, Brian Ning, and Nicholas Fung, John Hull, Zisis Poulos, and Jacky Chen for their comments on earlier versions of this work.\nDisclosure statement\nThe authors report there are no competing interests to declare."
        },
        {
            "heading": "Appendix A. Additional testing",
            "text": "In this section, we provide additional tests to demonstrate the goodness of fit of the IV surfaces using FPCs and to conclude that the learnt neural SDE has indeed captured the correlation structure across features as well as the long-range dependencies present in the observed time series.\n0 1000 20000.00\n0.01\n0.02\n0.03\n0.04 AMZN\nBasis FPC\n0 1000 2000 0.00\n0.02\n0.04\n0.06\nIBM\n0 1000 2000 0.00\n0.05\n0.10\nINTC\n0 1000 20000.00\n0.01\n0.02\n0.03\n0.04\nTSLA\nEvolution of RMSE over time\nDay of training data\nRo ot\nm ea\nn sq\nua re\ne rro\nr\n0.00 0.01 0.02 0.03 0.040.0\n0.1\n0.2\n0.3\nAMZN\nFPC Basis\n0.00 0.02 0.04 0.060.0\n0.1\n0.2\n0.3\nIBM\n0.00 0.05 0.100.00\n0.05\n0.10\n0.15\n0.20\nINTC\n0.00 0.01 0.02 0.03 0.040.0\n0.1\n0.2\nTSLA\nHistogram of RMSE\nRoot mean square error\nPr op\nor tio\nn\nmisspecification in the correlation structure, we consider the sequence of PITs of the sum of pairwise features. Given that our time series has 36 features, we consider all 630 \u2018features\u2019 obtained by the pair-wise sum of features. The learnt (conditional) onestep distribution of these \u2018features\u2019 is also Gaussian with variance incorporating the correlation between the two raw features involved. Considering the sequence of PITs of these augmented \u2018features\u2019, we then check if they are uniformly distributed via the Kolmogorov-Smirnov test. It is important to highlight that the objective function consists of the PIT penalty only on the univariate PITs, and not on the PITs of the augmented \u2018features\u2019.\nWhen the PIT penalty is minimized, the null hypothesis is rejected 44 and 195 out of 630 times at the 1% and 5% significance level respectively. Note that these rejections are not caused just due to the correlation structure not being captured well, but may also be caused due to the learnt mean and variance haveing slight variations from the true ones. On the other hand, if we do not include the PIT penalty in the minimization problem, the null hypothesis is rejected 284 and 506 times at the 1% and 5% significance level respectively. This again indicates that including the PIT penalty enables to better capture the correlation structure over time.\nMoreover, we assess if the time-varying correlation structure is reflected in the synthetic scenarios as we would like them to be inclusive of all observed scenarios in the past. To this end, we consider the correlation between features over windows of length 10 and 30 in the training period in a rolling window fashion, sliding the window one day at a time. If we have T observations, this would give us T \u2212 9 and T \u2212 29 values for the correlations between a pair. Given that ours is a conditional generative model, it is sensible to look at the correlations over small windows as these are the conditioning paths we consider. We now compare the kernel density estimate of these correlations with that of the correlations across the generated scenarios over a 30 day period. Given that there are 630 pairwise correlations, we present the results only for pairwise correlations between the first FPC and equity price for the same asset, and for the prices across assets. The results are presented in Figure 18 and 19. It is evidently clear that there is a larger density of negative correlations between the first FPC and the asset price in the observed data, something which is also reflected in the generated paths. Similarly, there is a higher positive correlation between the prices of distinct assets, once again captured by our generative model. In fact, the density plots for the correlations overlap significantly for both synthetic and observed data of different window lengths, and the model generates scenarios that are consistent with historical data \u2013 both in terms of the values and the proportion of the correlation.\nThis further provides evidence of our model learning the correlation structure really well."
        }
    ],
    "title": "FuNVol: A Multi-Asset Implied Volatility Market Simulator using Functional Principal Components and Neural SDEs",
    "year": 2023
}