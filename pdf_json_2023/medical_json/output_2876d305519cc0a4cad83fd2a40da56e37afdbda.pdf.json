{
    "abstractText": "The study of emotions through the analysis of the induced physiological responses gained increasing interest in the past decades. Emotion-related studies usually employ films or video clips, but these stimuli do not give the possibility to properly separate and assess the emotional content provided by sight or hearing in terms of physiological responses. In this study we have devised an experimental protocol to elicit emotions by using, separately and jointly, pictures and sounds from the widely used International Affective Pictures System and International Affective Digital Sounds databases. We processed galvanic skin response, electrocardiogram, blood volume pulse, pupillary signal and electroencephalogram from 21 subjects to extract both autonomic and central nervous system indices to assess physiological responses in relation to three types of stimulation: auditory, visual, and auditory/visual. Results show a higher galvanic skin response to sounds compared to images. Electrocardiogram and blood volume pulse show different trends between auditory and visual stimuli. The electroencephalographic signal reveals a greater attention paid by the subjects when listening to sounds compared to watching images. In conclusion, these results suggest that emotional responses increase during auditory stimulation at both central and peripheral levels, demonstrating the importance of sounds for emotion recognition experiments and also opening the possibility toward the extension of auditory stimuli in other fields of psychophysiology. Clinical and Translational Impact StatementThese findings corroborate auditory stimuli\u2019s importance in eliciting emotions, supporting their use in studying affective responses, e.g., mood disorder diagnosis, human-machine interaction, and emotional perception in pathology. Keywords\u2014Biomedical signal processing, Emotion elicitation, International Affective Digital Sounds (IADS), International Affective Pictures System (IAPS), physiological responses",
    "authors": [
        {
            "affiliations": [],
            "name": "Edoardo M. Polo"
        },
        {
            "affiliations": [],
            "name": "Andrea Farabbi"
        },
        {
            "affiliations": [],
            "name": "Maximiliano Mollura"
        },
        {
            "affiliations": [],
            "name": "Alessia Paglialonga"
        },
        {
            "affiliations": [],
            "name": "Luca Mainardi"
        },
        {
            "affiliations": [],
            "name": "Riccardo Barbieri"
        }
    ],
    "id": "SP:74c99f6e1da1212a2932a6ab709f0e5851c6133d",
    "references": [
        {
            "authors": [
                "I.B. Mauss",
                "M.D. Robinson"
            ],
            "title": "Measures of emotion: A review",
            "venue": "Cognition and emotion,",
            "year": 2009
        },
        {
            "authors": [
                "J. Sorinas",
                "J.M. Ferr\u00e1ndez",
                "E. Fernandez"
            ],
            "title": "Brain and body emotional responses: Multimodal approximation for valence classification",
            "venue": "Sensors, 20(1),",
            "year": 2020
        },
        {
            "authors": [
                "A.M.P.R. Kleinginna"
            ],
            "title": "Kleinginna. A categorized list of motivation definitions, with a suggestion for a consensual definition",
            "venue": "Motivation and emotion,",
            "year": 1981
        },
        {
            "authors": [
                "R. Cowie",
                "E. Douglas-Cowie",
                "N. Tsapatsoulis",
                "G.mVotsis",
                "S. Kollias",
                "W. Fellenz",
                "J.G. Taylor"
            ],
            "title": "Emotion recognition in human-computer interaction",
            "venue": "IEEE Signal processing magazine,",
            "year": 2001
        },
        {
            "authors": [
                "G. Valenza",
                "M. Nardelli",
                "A. Lanata",
                "C. Gentili",
                "G. Bertschy",
                "R. Paradiso",
                "E.P Scilingo"
            ],
            "title": "Wearable monitoring for mood recognition in bipolar disorder based on history-dependent long-term heart rate variability analysis",
            "venue": "IEEE Journal of Biomedical and Health Informatics,",
            "year": 2013
        },
        {
            "authors": [
                "P. Ekman"
            ],
            "title": "An argument for basic emotions",
            "venue": "Cognition & emotion,",
            "year": 1992
        },
        {
            "authors": [
                "J.A. Russell"
            ],
            "title": "A circumplex model of affect",
            "venue": "Journal of personality and social psychology,",
            "year": 1980
        },
        {
            "authors": [
                "C. Saarni"
            ],
            "title": "The development of emotional competence",
            "venue": "Guilford press,",
            "year": 1999
        },
        {
            "authors": [
                "P.J. Lang",
                "M.M. Bradley",
                "B.N. Cuthbert"
            ],
            "title": "International affective picture system (IAPS): Affective ratings of pictures and instruction manual, NIMH, Center for the Study of Emotion & Attention",
            "year": 2005
        },
        {
            "authors": [
                "Bradley",
                "Margaret M",
                "Peter J. Lang"
            ],
            "title": "The International Affective Digitized Sounds (; IADS-2): Affective ratings of sounds and instruction manual",
            "venue": "Technical Rep. B-3,",
            "year": 2007
        },
        {
            "authors": [
                "J. Anttonen",
                "V. Surakka"
            ],
            "title": "Emotions and heart rate while sitting on a chair",
            "venue": "In Proceedings of the SIGCHI conference on Human factors in computing systems (pp. 491-499),",
            "year": 2005
        },
        {
            "authors": [
                "S. Koelstra",
                "C. Muhl",
                "M. Soleymani",
                "J.S. Lee",
                "A. Yazdani",
                "T. Ebrahimi",
                "I. Patras"
            ],
            "title": "Deap: A database for emotion analysis; using physiological signals",
            "venue": "IEEE transactions on affective computing,",
            "year": 2011
        },
        {
            "authors": [
                "M. Soleymani",
                "J. Lichtenauer",
                "T. Pun",
                "M. Pantic"
            ],
            "title": "A multimodal database for affect recognition and implicit tagging",
            "venue": "IEEE transactions on affective computing,",
            "year": 2011
        },
        {
            "authors": [
                "M. Khateeb",
                "S.M. Anwar",
                "M. Alnowami"
            ],
            "title": "Multi-domain feature fusion for emotion classification using DEAP dataset",
            "venue": "Ieee Access,",
            "year": 2021
        },
        {
            "authors": [
                "M.B.H. Wiem",
                "Z. Lachiri"
            ],
            "title": "Emotion classification in arousal valence model using MAHNOB-HCI database",
            "venue": "International Journal of Advanced Computer Science and Applications,",
            "year": 2017
        },
        {
            "authors": [
                "L. Ivonin",
                "H.M. Chang",
                "W. Chen",
                "M. Rauterberg"
            ],
            "title": "Unconscious emotions: quantifying and logging something we are not aware of",
            "venue": "Personal and ubiquitous computing,",
            "year": 2013
        },
        {
            "authors": [
                "E.M. Polo",
                "A. Farabbi",
                "M. Mollura",
                "R. Barbieri",
                "A. Paglialonga",
                "L. Mainardi"
            ],
            "title": "Analysis of the skin conductance and pupil signals for evaluation of emotional elicitation by images and sounds",
            "venue": "In 2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC) (pp. 1968-1971)",
            "year": 1971
        },
        {
            "authors": [
                "A. Farabbi",
                "E.M. Polo",
                "R. Barbieri",
                "L. Mainardi"
            ],
            "title": "Comparison of different emotion stimulation modalities: an EEG signal analysis",
            "venue": "44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC) (pp. 3710-3713)",
            "year": 2022
        },
        {
            "authors": [
                "B. Laeng",
                "T. Endestad"
            ],
            "title": "Bright illusions reduce the eye\u2019s pupil",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2012
        },
        {
            "authors": [
                "G. Valenza",
                "L. Citi",
                "A. Lanat\u00e1",
                "E.P. Scilingo",
                "R. Barbieri"
            ],
            "title": "Revealing realtime emotional responses: a personalized assessment based on heartbeat dynamics",
            "venue": "Scientific reports,",
            "year": 2014
        },
        {
            "authors": [
                "M. Nardelli",
                "G. Valenza",
                "A. Greco",
                "A. Lanata",
                "E.P. Scilingo"
            ],
            "title": "Recognizing emotions induced by affective sounds through heart rate variability",
            "venue": "IEEE Transactions on Affective Computing,",
            "year": 2015
        },
        {
            "authors": [
                "M.M. Bradley",
                "P.J. Lang"
            ],
            "title": "Measuring emotion: the self-assessment manikin and the semantic differential",
            "venue": "Journal of behavior therapy and experimental psychiatry,",
            "year": 1994
        },
        {
            "authors": [
                "J. Bakker",
                "M. Pechenizkiy",
                "N. Sidorova"
            ],
            "title": "What\u2019s your current stress level? Detection of stress patterns from GSR sensor data",
            "venue": "IEEE 11th international conference on data mining workshops (pp. 573-580)",
            "year": 2011
        },
        {
            "authors": [
                "H. Sedghamiz"
            ],
            "title": "Matlab implementation of Pan Tompkins ECG QRS detector",
            "venue": "Code Available at the File Exchange Site of MathWorks,",
            "year": 2014
        },
        {
            "authors": [
                "R. Barbieri",
                "E.C. Matten",
                "A.A. Alabi",
                "E.N. Brown"
            ],
            "title": "A point-process model of human heartbeat intervals: new definitions of heart rate and heart rate variability",
            "venue": "American Journal of Physiology-Heart and Circulatory Physiology,",
            "year": 2005
        },
        {
            "authors": [
                "A. Priyanka",
                "G. Bharti",
                "M. Suresh"
            ],
            "title": "Introduction to EEG- and Speech- Based Emotion Recognition",
            "venue": "Chapter",
            "year": 2016
        },
        {
            "authors": [
                "C.M. C\u00f3mez"
            ],
            "title": "Frequency analysis of the EEG during spatial selective attention",
            "venue": "International Journal of Neuroscience",
            "year": 1998
        },
        {
            "authors": [
                "G. Tusman",
                "C.M. Acosta",
                "S. Pulletz",
                "S.H. B\u00f6hm",
                "A. Scandurra",
                "J.M. Arca",
                "F.S. ... Sipmann"
            ],
            "title": "Photoplethysmographic characterization of vascular tone mediated changes in arterial pressure: an observational study",
            "venue": "Journal of clinical monitoring and computing,",
            "year": 2019
        },
        {
            "authors": [
                "R. Sassi",
                "S. Cerutti",
                "F. Lombardi",
                "M. Malik",
                "H.V. Huikuri",
                "C.K. Peng",
                "R. ... Macfadyen"
            ],
            "title": "Advances in heart rate variability signal analysis: joint position statement by the e-Cardiology ESC Working Group and the European Heart Rhythm Association co-endorsed by the Asia Pacific Heart Rhythm Society",
            "venue": "Ep Europace,",
            "year": 2015
        },
        {
            "authors": [
                "J.I. Lacey"
            ],
            "title": "The visceral level; Situational determinants and behavioral correlates of autonomic response patterns",
            "venue": "Expression of the emotions in man,",
            "year": 1963
        },
        {
            "authors": [
                "P.J. Lacey"
            ],
            "title": "Some autonomic-central nervous system inter-relations",
            "venue": "Physiological correlates of emotion,",
            "year": 1970
        },
        {
            "authors": [
                "A.A. Jayaswal"
            ],
            "title": "Comparison between auditory and visual simple reaction times and its relationship with gender in 1st year MBBS students of Jawaharlal Nehru Medical College, Bhagalpur, Bihar",
            "venue": "Int. J. Med. Res. Rev,",
            "year": 2016
        },
        {
            "authors": [
                "C. Bloomer",
                "C. Hitt",
                "D. Olson",
                "C. Wruck"
            ],
            "title": "Stress responses due to application of audio or visual stimuli",
            "year": 2014
        },
        {
            "authors": [
                "S. Nakakoga",
                "H. Higashi",
                "J. Muramatsu",
                "S. Nakauchi",
                "T. Minami"
            ],
            "title": "Asymmetrical characteristics of emotional responses to pictures and sounds: Evidence from pupillometry",
            "venue": "PloS one,",
            "year": 2020
        },
        {
            "authors": [
                "J. Carney"
            ],
            "title": "Culture and mood disorders: the effect of abstraction in image, narrative and film on depression and anxiety",
            "venue": "Medical Humanities,",
            "year": 2020
        },
        {
            "authors": [
                "T. Narimoto",
                "N. Matsuura"
            ],
            "title": "Retention of Visual Images: Children With ADHD",
            "venue": "The Japanese Journal of Educational Psychology,",
            "year": 2023
        },
        {
            "authors": [
                "L. Lee"
            ],
            "title": "Investigating the impact of music activities incorporating Soundbeam Technology on children with multiple disabilities",
            "venue": "Journal of the European Teacher Education Network,",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "Keywords\u2014Biomedical signal processing, Emotion elicitation, International Affective Digital Sounds (IADS), International Affective Pictures System (IAPS), physiological responses\nI. INTRODUCTION\nEmotions are defined as a physiological mental state which involves people\u2019s reactions after the onset of internal or external stimuli. As such, they are strongly affected by subjective experience [1]. Emotions are fundamental in our life since they make us survive and reproduce through adaptation to the environment [2]. They are generated by a very complex aggregate of neuronal and hormonal interactions which strongly influence decision-making processes [3]. Emotionrelated studies are gathering much interest in our society since they can be extended in many fields such as the new horizons of Human-Computer and Human-Robot interaction [4] but also in more clinical applications as the monitoring of depression states, anxiety, stress, chronic anger and mood disorders. Thus, devising methodologies to properly elicit and recognize emotions can largely help in assessing and quantifying diseases which influence human health and wellbeing.\nDate of submission: Feb 15, 2023 This work was supported in part by project \u201cFit4MedRob - Fit for Medical Robotics\u201d, Piano Nazionale Complementare (PNC), D.D. n. 931, June 6, 2022: Avviso per la concessione di finanziamenti destinati ad iniziative di ricerca per tecnologie e percorsi innovativi in ambito sanitario e assistenziale, code PNC0000007.\nEdoardo M. Polo is with Politecnico di Milano, Italy and with DIAG, Sapienza University of Rome, Italy (e-mail: edoardomaria.polo@polimi.it).\nAndrea Farabbi, Maximiliano Mollura, Luca Mainardi and Riccardo Barbieri are with Politecnico di Milano, Milano, Italy (e-mail: andrea.farabbi@polimi.it; maximiliano.mollura@polimi.it; luca.mainardi@polimi.it; riccardo.barbieri@polimi.it).\nAlessia Paglialonga is with Cnr-Istituto di Elettronica e di Ingegneria dell\u2019Informazione e delle Telecomunicazioni (CNR-IEIIT), Milano, Italy (corresponding author e-mail: alessia.paglialonga@cnr.it). A great interest is rising on the study of physiological signals and how they change in response to different elicited emotions. This interest is mainly due to the fact that physiological signals are linked above all to the autonomic nervous system (ANS) and can be easily acquired by wearable devices [5]. Moreover, physiological reactions are objective in nature and are not influenced by subjective judgment. In the literature there are many models which classify emotions and can be divided in two principal categories: categorical and dimensional. In the first case emotions are described with a limited number of innate and universal categories, easily distinguishable between each other (e.g. anger, fear, joy, surprise, sadness) [6] while in the second case each emotion can be explained as the linear combination of specific and independent dimensions. The most used dimensional model in literature is the Russel\u2019s Circumplex model [7] which identifies two principal dimensions: arousal, which represents the level of excitation that an emotion can elicit and valence, which is the level of pleasantness/unpleasantness carried by the emotion itself. Difficulties in describing emotions as categorical have led several researchers to make use of dimensional models since emotions are not discrete entities but are more blurred experiences which often overlap with each other [8]. Most studies in the field of affective computing use pictures as stimuli, predominantly the International Affective Pictures System (IAPS) [9]. These pictures are widely used for experiments about emotions as they possess valence and arousal values obtained over the years as an average given by the labeling of thousands of people and for this reason are well suited stimuli for emotion recognition, for example using\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
        },
        {
            "heading": "IEEE JOURNAL OF TRANSLATIONAL ENGINEERING IN HEALTH AND MEDICINE 2",
            "text": "computational models. Very few studies use instead auditory stimuli from the International Affective Digital Sounds (IADS) [10]. The IADS is homologue to the IAPS database, but it contains standardized sounds in valence and arousal levels. Although a soundtrack in video-clips is a part of the stimulus itself, it is not easy to understand what kind of stimulation such as audio, visual or audio-visual is more effective in eliciting emotions. To the best of our knowledge, almost no study used the combination of audio and video stimuli [11]. Moreover, there are more recent studies in the literature which created emotion databases by acquiring physiological signals and using films or video-clips as stimuli [12]-[13]. From the creation of these databases, other studies arised with the purpose of achieving highest possible levels of accuracy in discriminating the different emotions elicited through machine learning models [14]-[15]. However, in most of these studies little importance is given to the interpretation of the features extracted from the signals and, also, limited evidence is provided to better understand which signals are most effective to assess the relation between emotions and physiology. In [11], which is one of the very few studies using both IAPS and IADS separately and together, authors found a general heart rate (HR) deceleration in response to emotional stimulation. The observed HR deceleration was independent from the stimulation modality and more emphasized in response to negative stimuli as compared with responses to positive and neutral stimuli. In [16], which uses IAPS and IADS separately, results showed that heart rate responses are unique for every category of the emotional stimuli with an higher rate of heart rate deceleration when listening to sounds only. In our research, we have introduced an innovative approach to elicit emotions. We utilized the well-established IAPS and IADS datasets, both individually and in combination, while simultaneously monitoring various physiological signals. This study\u2019s novelty lies in its use of these validated datasets to compare different emotional stimulation methods. While most studies primarily focus on visual stimuli, incorporating sound is rare, and even rarer is the use of both modalities with these datasets. Furthermore, in this study we tracked six different physiological signals. Our objective is to examine the physiological patterns associated with various stimulation methods and evaluate their effectiveness. This study places particular emphasis on analyzing responses triggered by auditory stimuli to quantify their influence on emotional responses and to explore their potential in diverse psycho-physiological contexts. Preliminary findings of this research were presented in [17]-[18]."
        },
        {
            "heading": "II. METHODS AND PROCEDURES",
            "text": ""
        },
        {
            "heading": "A. Study Design and Data",
            "text": "Our experimental protocol includes 13 female and 8 male (age: 26.18 \u00b1 1.47 years) volunteers. The experiments were performed at SpinLab of Politecnico of Milano after all subjects signed an inform consent approved by the Politecnico di Milano Reasearch Ethical Committee (Opinion n. 29/2021). During the experiment, the electrocardiogram (ECG), blood volume pulse (BVP), galvanic skin response (GSR), pupillary signal (PUPIL) and electroencephalogram (EEG) were\nacquired. ECG, BVP and GSR were acquired by Procomp Infiniti device with a sampling frequency fixed at 256 for GSR and 2048 Hz for ECG and BVP. PUPIL was acquired by Tobii Pro X2 Compact eye-tracker with a sampling frequency of 60 Hz. The EEG signal was acquired with the DSI 24 headset composed by 21 dry electrodes located at: Fp1, Fp2, Fz, F3, F4, F7, F8, Cz, C3, C4, T7/T3, T8/T4, Pz, P3, P4, P7/T5, P8/T6, O1, O2, A1, A2. The electrodes location follows the international 10-20 system. The headset acquired data with a sampling rate of 300 Hz with and A/D converter at 16 bits. Participants were seated on a comfortable chair during the experiment. To minimize behavioral heterogeneity, participants were asked to avoid coffee and smoke for two hours prior to the experiment. During experiments one experimenter was always present in the room to manage possible problems with the acquisitions and prevent any pronounced movement of the subjects or detachments of sensors. All subjects were tested with the pure tone audiometry examination using a clinical audiometer (Amplaid 177+, Amplifon with TDH49 headphones) to ensure that their hearing thresholds were in the normal hearing range (pure tone average thresholds at 0.5, 1, 2, and 4 kHz < 20 dB HL). The acoustic stimulation was performed by using headphones (UXD CT887) and subjects with vision problems were able to use glasses. All participants were asked if they had any pre-existing medical conditions and/or mental disorders, and if so, they were excluded from the study. Regarding visual aspects, especially the pupillary signal, we adhered to established guidelines [19]. To reduce external light interference, we darkened the lab windows and used consistent artificial lighting during recording. Screen brightness remained constant for all participants. For the auditory component, participants were exposed to a sound before the test and were guided to choose a comfortable volume level. The protocol consists of three randomized phases interspersed with 2 minutes rest: 1) IAPS-only 2) IADS-only 3) IAPS + IADS This protocol extends an existing protocol in which IAPS and IADS have been used as stimuli separately on different subjects [20][21]. Each of the above phase includes four sessions: after an initial resting period of 5 minutes while subjects look at a grey screen, four sessions at increasing arousal levels are alternated with neutral sessions of 90 seconds each. Every arousing session includes six visual and/or acoustic stimuli, lasting 15 seconds each, having the first half with low valence and the second half with high valence. The experiment lasts 47 minutes. The rationale is to slowly increase the arousal maintaining a median neutral valence in each session. Neutral sessions deliver low arousal stimuli with neutral valence which serve as a baseline interlude of rest between two emotional sessions.\nIn IAPS+IADS, pictures are delivered with simultaneous sounds chosen to have a semantic match (e.g. picture of a dog growling and the sound of a dog barking). In IAPS+IADS, pictures and sounds belonging to the neutral sessions of the IAPS-only and IADS-only are delivered together. Figure 1 shows the experimental protocol. Arousal and valence levels\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
        },
        {
            "heading": "IEEE JOURNAL OF TRANSLATIONAL ENGINEERING IN HEALTH AND MEDICINE 3",
            "text": "are set according to IAPS and IADS scores as reported in Table I. In order to compare IAPS and IADS labeling with the subjective one, immediately after the end of the protocol, participants were asked to perform a self-assessment of the stimuli seen and heard during the experiment. Each stimulus was quickly reproduced and subjects had to select arousal and valence levels using self-assessment manikins according to their own emotional reaction. Self-assessment manikins are a pictorial and non-verbal questionnaire to evaluate emotions [22] in which low valence is represented by a sad manikin increasingly smiling moving towards higher valence levels while arousal is painted used manikins showing relaxation for lower levels and excitement for higher levels. The average subjective ratings of values and arousal were aligned with the valence and arousal values shown in Table I, suggesting that the stimuli here used were effective on both negative and positive valence dimension and on the increasing arousal dimension."
        },
        {
            "heading": "B. Data processing and Feature Extraction",
            "text": "In the following subsections the processing and the feature extraction steps are explained for each signal.\n1) GSR: GSR was filtered at 2 Hz with a zero-phase low pass Butterworth of 4th order and it was then downsampled at 5 Hz. In order to separate the phasic component from the entire signal, a median filter was applied. Specifically, each sample amplitude was replaced by the median amplitude of the surrounding samples in a time window of +/- 4 seconds centered on the current sample [23]. The resulting median signal was then subtracted from the filtered one and the resulting phasic signal was used to extract GSR peaks linked to the eccrine glands\u2019 spikes. GSR peaks were computed as local\nmaxima on the filtered signal between each onset (amplitude > 0.01 \u00b5S) and offset (0 \u00b5S < amplitude) occurrences on the phasic signal. Given the relative slowness of the signal and having to consider short time windows to be analyzed, we focused only on time features. In particular we extracted:\n\u2022 the number of GSR peaks (GSR n peaks) \u2022 the amplitude of GSR peaks (GSR Amp peaks) \u2022 the average rise and recovery time of GSR peaks (AVGSR\nrise time, AVGSR recovery time) \u2022 GSR average (AVGSR) and standard deviation (s.d.)\n(SDGSR) \u2022 the maximum signed amplitude between two consecutive\nmaxima (MAXGSR sign amp) computed on the signal filtered with a Butterworth band-pass filter of the 2th order from 0.5 to 1 Hz \u2022 the average (AVGSR der), s.d. (SDGSR der) and maximum of the first derivative (MAXGSR der) of the signal \u2022 the envelope of the phasic component computed as the average of its integral (AVenv)\n2) ECG: A 4th order zero-phase low-pass Butterworth antialiasing filter with a cut-off frequency at 125 Hz was applied to the ECG signal in order to remove high frequency noise coming both from the acquisition system and eventual subjects\u2019 movements. Subsequently, the signal was downsampled at 250 Hz and R-peak locations were extracted by means of a Pan-Tompkins based algorithm [24]. All R-peaks locations were then checked and any mistake in the annotation was manually corrected through a in-house software. In order to deal with non-stationarities of physiological responses and with the intrinsic non-stationary nature of the emotion stimuli as well as the short duration of the stimuli, we applied a Point Process framework to model the inter-beat interval (RR) series. Given the intrinsic Point Process nature of the RR intervals relying on the R-wave events this model well fits as a framework for modeling the heartbeat [25]. More specifically, the inter-beat-interval series was modelled according to the following history-dependent inverse gaussian distribution:\np(t) =\n( \u03b8p+1\n2\u03c0(t\u2212Rk)3\n) 1 2\nexp ( \u2212\u03b8p+1(t\u2212Rk \u2212 \u00b5RR)\n2\u00b52RR(t\u2212Rk) ) (1)\nwhose expected value is estimated with an autoregressive (AR) model as follows:\n\u00b5RR = \u03b80 + p\u2211 i=1 \u03b8i(t)RRk\u2212i (2)\nwhere RRk represents the k-th RR interval closer to time t, and \u03b80:p and \u03b8p+1 parameters represent the AR model parameters and the shape parameter of the inverse gaussian, which are continuously estimated through local maximum likelihood estimation according to [25]. AR coefficients were then used to compute a time-varying spectral analyses of the RR interval series. In this way we were able to track very fast stimulus-response changes. From the resulting time-varying estimation we were able to extract averaged features in each emotion time window such as:\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
        },
        {
            "heading": "IEEE JOURNAL OF TRANSLATIONAL ENGINEERING IN HEALTH AND MEDICINE 4",
            "text": "\u2022 the modelled RR series (\u00b5RR) and the variability of the inverse gaussian distribution (\u03c32) \u2022 the RR power spectral density of the modelled RR series in very low (RR VLF) [<0.04 Hz], low (RR LF) [0.04-0.15 Hz] and high (RR HF) [0.15-0.5 Hz] frequency ranges and the sympatho-vagal balance index (RR LF/HF) \u2022 the normalized power spectral density of the modelled RR series in low (RR LFn) and high (RR HFn) frequency ranges and the total power spectral density of the modelled RR series (RR TOT)\nFigure 2 shows for the first phase (720 seconds) of \u2019subject1\u2019 the instantaneous tracking of the heart rate variability (HRV) indices extracted.\n3) BVP: BVP was filtered by a 4th order zero-phase lowpass Butterworth anti-aliasing filter with a cut-off frequency at 25 Hz and then it was downsampled at 250 Hz. Locations and related amplitude values of systolic, diastolic and onset fiducial points were extracted starting from the R-peaks annotation on the ECG signal. More specifically, the maximum and minimum BVP values between two consecutive R-peaks were considered as systoles and diastoles, respectively. Onset values were found by looking at each inflection point between each systolic and diastolic location. All locations and relative amplitudes of the three fiducials points were checked to be sure of the their correctness and eventual mistakes were manually corrected. As for the ECG signal, the signal processing was done considering the entire signal, then fiducial points\u2019 locations and feature extraction took place in shorter windows. From BVP and its link with ECG, two features were extracted in the emotion time windows:\n\u2022 The average amplitude difference between each systolic and the corresponding diastolic value, computed and referred as a index of the average volume pulse (VP) \u2022 The average pulse arrival time (PAT), computed as the average time difference between each onset value on the\nBVP signal and the corresponding R-peak on the ECG For the PAT computation onsets were used with respect to diastoles or systoles since they reveal to be more stable points given the lower indecision on the correct position in the noisiest signal sections.\n4) EEG: The EEG signal was processed in order to extract the relevant features. As the signal acquired by the electrode Pz was found to be corrupted, it was replaced by an interpolation obtained by the signals recorded at the nearest electrodes. All signals were re-referenced through Common-Average Referencing (CAR) in order to remove common noise and filtered in the main brain waves oscillation frequencies as defined in [26]: \u03b4(1\u2212 3Hz), \u03b8(4\u2212 7Hz), \u03b1(8\u2212 12Hz), \u03b2(16\u2212 38Hz). It was decided to not include the \u03b3 waves (> 38Hz) since literature suggests that no emotion-related brain activity happens at this frequencies and also to remove muscular artifacts manifesting at higher frequencies. Regarding EEG, the following features were extracted in each emotion session:\n\u2022 the power spectral density in the frontal and parietal regions for each frequency band (\u03b4(1\u22123Hz) F, \u03b4(1\u22123Hz) P, \u03b8(4 \u2212 7Hz) F, \u03b8(4 \u2212 7Hz) P, \u03b1(8 \u2212 12Hz) F, \u03b1(8\u2212 12Hz) F, \u03b2(16\u2212 38Hz) F, \u03b2(16\u2212 38Hz) P) \u2022 the ratio of the power spectral densities \u03b2/\u03b8 in the frontal and parietal regions (\u03b2/\u03b8 F, \u03b2/\u03b8 P) in order to assess the attention level of the subjects during the trials.\nAll power spectral densities were computed using the Welchmethod. Concerning attention related features, the ratio \u03b2/\u03b8 is supposed to increase during attentive states[27].\n5) PUPIL: All samples were checked and values lower than 2 mm or higher than 8 mm were replaced by NaN and treated as blinks since considered out of the physiological range [2- 8 mm]. At first, for each time instant if only one eye had a blink then the sample value of the other eye was replaced to the missing sample. Secondly, where both eyes had blinks, a linear interpolation of the signal was carried out. Once all samples were within the physiological range, the signal was\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
        },
        {
            "heading": "IEEE JOURNAL OF TRANSLATIONAL ENGINEERING IN HEALTH AND MEDICINE 5",
            "text": "filtered by a 4th order zero-phase low-pass Butterworth antialiasing filter with a cut-off frequency at 5 Hz and then it was downsampled at 10 Hz. The pupillary diameter was then computed as the average of the amplitude of both eyes. Both time and frequency features were then extracted on the processed signal in the emotion time windows. In particular, from PUPIL we extracted:\n\u2022 the average of the diameter (AVD) and its s.d. (SDD) \u2022 the power spectral density of the diameter in low [0.05-\n0.15 Hz] (DLF), high [0.15-0.45 Hz] (DHF), very high [0.45-1.5 Hz] (DVHF) frequencies ranges and the balance index (DLF/DHF) \u2022 the normalized power spectral density of the diameter in low (DLFn) and high (DHFn) frequencies ranges.\nFeatures in the frequency domain were obtained by computing the Welch\u2019s periodogram of the detrended signal by using overlapped segments of 300 samples windowed with a Hamming window."
        },
        {
            "heading": "C. Statistical analysis",
            "text": "Two different comparisons were performed among the three different phases in the two emotional dimensions of valence and arousal. The aim was to assess how the body reacted according to different stimulation modalities such as only the\nview of images, only the listening to sounds or more complex stimuli given by the interaction of the two. For the arousal dimension, we compared features in each arousal session of one phase with the same arousal session of the other two phases. The same was done for the valence dimension: features computed during low and high valence of the same arousal session were compared among the three different phases. Specifically, Shapiro-Wilk test was performed for each feature values in low, high valence and in each entire arousal session to assess the normality of the data. Then, since always at least one variable was not normal distributed, the non parametric and pair-wise Friedman\u2019s test was performed. In particular, if a comparison was significant, a multcompare comparison was then performed to assess pairwise differences. In all comparisons Tukey\u2019s correction was applied."
        },
        {
            "heading": "III. RESULTS",
            "text": "Table II and Table III summarize all the relevant features found in the comparison among the three phases related to different stimulation modalities for the arousal and valence dimensions, respectively. In the followings, results are shown divided by signal and for emotional dimension, in terms of arousal and valence. Figure 4 displays four features related to GSR, ECG, BVP, and PUPIL signals, which exhibited greater significance in\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
        },
        {
            "heading": "IEEE JOURNAL OF TRANSLATIONAL ENGINEERING IN HEALTH AND MEDICINE 6",
            "text": "delineating differences during the same arousal session across the three types of stimulation. These distinctions, as observed in the image, are presented in more detail below within the context of their respective signals. GSR Arousal: From Table II it is possible to notice that GSR Amp peaks are significantly higher in IADS-only in A1 and A3 with respect to IAPS-only and the same behavior is detectable for Aenv which is always significantly greater in IADS-only than in IAPS-only in A1 and A2. GSR n peaks, moreover, are significantly higher not only in IADS-only but also in IAPS+IADS with respect to IAPS-only during A1. A general lower activity is observed for GSR in the phase involving only the sight of pictures. GSR Valence: Table III shows how significantly higher GSR Amp peaks, GSR n peaks, Aenv and AVGSR rise time are observed during high valence stimuli of IADS-only with respect to IAPS-only in A1. GSR Amp peaks are higher for high valence stimuli also in A2 during IADS-only with respect to IAPS-only. Overall, high valence stimuli, linked to positive emotions, seem to elicit a higher GSR physiological response. ECG Arousal: Table II shows how \u00b5RR has a general higher trend in IADS-only with respect to the other two phases. In all arousal sessions, median values are higher in IADSonly. We can state that a deceleration of the heartbeat is therefore observed when subjects listen to sounds alone. A lower sympathetic activity for IADS-only is also visible if we look at the time-varying spectral indices. RR LFn shows significantly lower values during IADS-only with respect to IAPS+IADS in A4. According to the same two phases and the same arousal session a significant opposite behavior is observed for RR HFn. Also RR LF/HF is significantly lower in IADS-only with respect to the other two phases in A4 (see Figure 4). No statistically significant difference is observed in the valence dimension. BVP Arousal: from the ECG-BVP interaction the computed PAT is significantly lower in IADS-only with respect to IAPSonly in A3. The same significant behavior is observed between IADS-only and IAPS+IADS in A2 and A4 (see Figure 4 and Table II). No statistically significant difference was found for VP as shown in Table II. However, this feature shows a general trend of low values in IADS-only with respect to the other two phases. Despite deceleration of the heartbeat, the blood propagation time from the heart to the periphery narrows and the blood volume decreases during the beats related to IADSonly. BVP Valence: From Table III it is possible to notice significantly lower PAT values in IADS-only during high valence stimuli in A2 and A4 with respect to IAPS+IADS and in A3 with respect to IAPS-only. Significantly lower VP values are observed in IADS-only during high valence stimuli in A1 with respect to IAPS+IADS. In general we observe lower values during IADS-only in all arousal sessions with respect to the other two phases and this behavior seem to be attributed to high valence stimulation. EEG arousal: The average activation during all arousal sessions on the scalp in \u03b4 has been reported in Figure 3. We can qualitatively notice that for all types of stimulation the most activated areas are the frontal and central, with a more pro-\nnounced activation in IADS-only. The power spectral density computed in \u03b4, however, showed no statistically significant differences between IAPS-only, IADS-only and IAPS+IADS. Nevertheless, the main significant differences have been highlighted for the attention features extracted (see Table II). In particular, it is worth to note that IADS-only resulted always in higher attention levels (in both frontal and parietal areas), beside the forth arousal level where no difference can be assessed. In this regard, Figure 5 displays distributions of the Attention index \u03b2/\u03b8 in the frontal and parietal lobes for the three types of stimulation. It is evident that in the IADS-only phase, this index tends to be significantly higher compared to the IAPS+IADS phase, with a clear upward trend also when compared to the IAPS-only phase. EEG valence: Low valence stimulation resulted in different responses depending on the area analyzed (see Table III). For the frontal area IADS-only stimulation resulted in significantly higher attention levels during the first and second arousal sessions, while in the parietal areas it was higher than the other two types of stimulations only in the first arousal and higher of IADS+IAPS only in the second and third arousal sessions. Similar results have been obtained during high valence stimuli. In the frontal area IADS-only is significantly higher than the other two phases during A1, A2 and A3, while for the parietal in A1 it showed significantly higher attention levels only if compared to IAPS+IADS stimulation and in A2 and A3 the attention level in IADS-only is significantly higher than the other two phases. PUPIL Arousal: AVD is significantly lower in each arousal session during IADS-only with respect to the other two phases. No statistically significant difference is observed between IAPS-only and IAPS+IADS in any of the arousal sessions. Diameter values are not reported in Table II since the comparison with IADS-only was not fair since during listening to sounds only subjects were looking at a gray screen so as expected the pupillary excursion is minor in this phase than in the other two. SSD is significantly lower during IADS-only than in the other two phases in A3. With regard to frequency features, DVHF is significantly higher in IAPS+IADS with respect to IAPSonly in A4 (Figure 4). In particular this feature, not very much investigated in the literature, seems to have a general increase at increasing arousal. PUPIL Valence: During high valence stimulation Table III shows a significantly higher DHF during IAPS-only with respect to IADS-only in A3. Always in A3, DLF is significantly higher during IAPS-only with respect IAPS+IADS. For DLFn, DHFn and DLF/DHF, though, even if no statistically significant difference is observed, we observe an agreement\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
        },
        {
            "heading": "IEEE JOURNAL OF TRANSLATIONAL ENGINEERING IN HEALTH AND MEDICINE 7",
            "text": "with higher excitation during IADS-only in A3: higher values for DLFn and DLF/DHF and lower values for DHFn. On the other hand, for low valence stimuli we can observe significantly higher DLF during IAPS-only with respect to both other phases in A4. Of note, the same behavior is not observed for DLFn and DLF/DHF in A4. Indeed, even if no statistically significant difference is found for DLFn and DLF/DHF, both features show an increase during IADS-only with respect to the other two phases, whereas the opposite trend is observed in both DHF and DHFn, i.e. lower values during IADS-only. Finally, pupillary diameter s.d. is found to be significantly higher during IAPS-only with respect to the other two phases in A4. Figure 6 shows the main results obtained in the study. In particular, both 2D and 3D boxplots were created by joining all feature values of the four arousal sessions of each phase (mean \u00b1 standard error). Figure 6 displays features that best explain the physiological behavior linked to the different stimulations. The attention index increases when transitioning from IAPS+IADS to IAPS-only and further attains higher values during the IADS-only phase, which is entirely distinct from the IAPS+IADS phase where the stimulus becomes more complex by involving both visual and auditory senses. It is also important to observe that from a cardiovascular perspective, at the peripheral vessel level, the IADS-only phase is associated with lower values of VP, which are linked to an increase in blood pressure [28]. This is, however, connected to a more centrally located lower RR LF/HF index derived from the ECG, indicating reduced sympathetic activity. From the perspective of eccrine glands, lower activity is observed during the IAPS-only phase."
        },
        {
            "heading": "IV. DISCUSSION",
            "text": "Overall, our results from the analysis of physiological signals such as GSR, ECG, BVP, PUPIL and EEG, demonstrate that sounds tend to elicit higher emotional responses than visual stimuli, either alone or in combination with sounds.\nA unique aspect is the use of the Point process framework to analyze ECG data, which is rarely used for HRV analysis in this context. Unlike traditional methods used in shorter time windows, this framework adheres to established guidelines for HRV analysis, allowing real-time extraction of HRV indices [29]. By looking at the results from GSR, we can speculate that higher sympathetic activity is elicited when listening to sounds compared to watching images. Looking at the arousal dimension, indeed, GSR Amp peaks and Avenv (which are related to the spikes of the eccrine glands and thus linked to sympathetic activation), result to be higher during IADS-only with respect to IAPS-only in two arousal sessions. Moreover, GSR n peaks results higher in IADS-only and IAPS+IADS with respect to IAPS-only in the first arousal session. By looking at the valence dimension, it seems that this higher excitation during the listening to sounds belongs to high valence stimuli, i.e.,\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
        },
        {
            "heading": "IEEE JOURNAL OF TRANSLATIONAL ENGINEERING IN HEALTH AND MEDICINE 8",
            "text": "stimuli that carry a positive emotional content. For what concerns the ECG signal, it shows an opposite behavior with respect to GSR. Even if the average modeled RR series did not show any significant differences among the three stimulation modalities in any of the arousal sessions, there is an evident heartbeat deceleration during IADS-only. This behaviour is reflected also in the spectral indices: the normalized power spectral density in the low- and in the high-frequency ranges are significantly lower and higher, respectively, during IADSonly with respect to IAPS+IADS. Also the ratio between RR LF/HF, which is more related to the sympathetic branch of the autonomic nervous system is significantly higher in IAPS-only and IAPS+IADS with respect to IADS-only in the most arousing session A4. Conversely, by looking at VP from the BVP signal and from PAT derived from both ECG and BVP, is evident how the cardiovascular system compensates this heartbeat deceleration by increasing the blood pressure. By looking at Table II, indeed, we can see how VP has lower values during IADS-only with respect to the other two phases. The amplitude modulation of the BVP signal represents, indeed, the volume of blood on the periphery and lower amplitudes are therefore linked to a greater peripheral blood pressure, which is associated with vasoconstriction. This effect is especially visible during low valence stimuli linked to negative emotions. By looking at Table II and Table III we can see how this compensation is reflected also in PAT which has lower values, especially at high valence. We can state that there is therefore an acceleration of the pressure wave since less time is spent by the blood to reach the periphery from the heart. By looking at the literature, this behavior could be referred to the attention theory. The Lacey\u2019s theory of attention, indeed, has proved that the response of an organism to a task which involves attention is linked to a heart beat deceleration often accompanied by an increased GSR [30]. According to Laceys\u2019 theory [31], this deceleration occurs due to the acceptance of environmental stimuli which require attention such as, for instance, perception of a visual or auditory stimulus. In terms of auditory involvement, emotional sounds are processed more rapidly and directly than images, which involve more complex visual processing. This quicker engagement of the auditory system may result in heightened sympathetic and attentive activation [32]. Although few studies directly compare visual and auditory emotional stimuli, the literature offers diverse insights. In [33], findings suggest that audio and\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
        },
        {
            "heading": "IEEE JOURNAL OF TRANSLATIONAL ENGINEERING IN HEALTH AND MEDICINE 9",
            "text": "visual stimuli trigger comparable stress responses. Conversely, [34] emphasizes asymmetrical emotional responses to pictures and sounds, where pupillary dilation is more associated with arousal in images and valence in sounds. Moreover, one of the few study on emotion elicitation which makes use of IAPS and IADS [16] found that auditory stimuli led to a higher speed of heart rate deceleration in comparison to visual stimuli as in our case. According also to the feedback provided by some of the subjects tested, sounds alone in the absence of a related image required them to focus and use their past memories and experiences to understand the meaning of the sound, thus eliciting stronger emotions. This assertion is confirmed by the response to emotional stimuli given by the central nervous system (i.e., EEG recordings). EEG did not show statistically significant differences between stimulation modalities in terms of brain activity in the brain areas and frequency bands usually related to emotion processing. Nevertheless, the activation along the scalp shows a better localized and higher response during IADS-only, suggesting that this type of stimulation may better evoke the expected emotion. Moreover, in most cases, it has been found a higher attention level of the subjects when stimulated with IADS-only. Regarding the pupillary signal, results are more difficult to compare and interpret since subjects were looking at a grey screen during IADS-only. Of note, the pupillary diameter is lower in all the arousal sessions for IADS-only with respect to the other two phases. The standard deviation of the diameter is lower during IADS-only as well. In general, features linked to pupillary amplitude seem to be not very representative since no statistically significant difference is observed between IAPS-only and IAPS+IADS, so sounds do not seem to influence the pupil diameter. About the frequency content of the pupillary signal, it seems that higher power spectral density is observed during IAPS-only both in low and high frequency ranges with respect to the other two phases in some arousal sessions. However, by looking at the normalized power spectral densities, even if not significantly, it seems that IADS-only is the one associated with greater values of DLFn and lower values of DHFn for the most arousing sessions (e.g. A3 and A4), thus emphasizing a greater sympathetic control also on the pupil when listening to sounds only. Moreover, statistically significant differences are observed in the very high frequency range in which higher differences are observed during IAPS+IADS compared to IAPS-only. Since this frequency range may not be associated with the autonomic nervous system, this point should be investigated further. Figure 6 provides an intuitive graphical summary of the main results obtained. Specifically, IADS-only is associated with sympathetic cardiovascular deactivation, which is compensated by an increase of the peripheral blood pressure. Moreover, increased GSR peak amplitudes in IADSonly are observed, especially with respect to IAPS-only. This behavior is explained by Lacey\u2019s theory of attention and reflected at the central level by looking at the EEG where the attention index turned out to be greater when listening to sounds. It therefore seems that when listening to sounds subjects pay more attention and are more stimulated. These results are encouraging and support the use of a multimodal signal processing approach to investigate physiological responses. While EEG predictably offered the most significant results, less invasive signals such as GSR, BVP, and ECG still provided valuable insights into physiological patterns. Monitoring central and peripheral signals improved our grasp of physiology, aligning with Lacey\u2019s theory that ECG and GSR alone can capture attention changes without more cumbersome EEG procedures. In any case, the main focus of our study was about understanding the effects and impact of different stimulation modalities, rather than investigate signals reliability. From a clinical perspective, our finding that emotional sounds elicit stronger physiological responses than visual and audiovisual stimuli holds promise for clinical applications. For example, proper sound protocols could enhance mood disorder treatments that are often only based on visual clues [35], potentially leading to improved emotional regulation with a specific focus on a personalized set of sound stimuli. An approach based on personalized sound stimulation can also be particularly relevant in neurofeedback therapy, especially for conditions like Attention Deficit Hyperactivity Disorder, where attention and self-control are of major concerns [36]. Moreover, individuals with disabilities, including those on the autism spectrum, may benefit from sound-based therapies, as suggested by studies combining music activities with sound therapy [37]. The implications of these findings extend beyond clinical practice. For example, the use of sound stimuli could bring new opportunities in neuromarketing, where the emotional impact of sounds could be leveraged to design personalized, more effective marketing strategies."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "We here present a new acquisition protocol in order to compare three different stimulation modalities to convey emotional content through images, sounds and a combination of them. In particular, we focus on the acquisition of ECG, BVP, GSR, PUPIL, and EEG to understand which stimulation is the most effective. By extracting features linked to both the autonomic and central nervous systems we found how sounds are able to stimulate more at a physiological level. From the GSR signal it is evident how the phase of only sounds is more linked to a sympathetic activation than the other two phases. On the other hand, at the cardiovascular level there is a deceleration of the heartbeat, which is accompanied by a compensation given by a minor blood volume amplitude at the peripheral level, and a greater speed of blood from the heart to the periphery. At the central level, the relevant result is that a greater attention index is observed while listening to sounds. We can conclude by saying that sounds seem to stimulate the subjects more, placing greater importance on this type of stimulation where very little has been investigated in the literature. Overall, we conclude that sounds convey emotional content and reveal to be very effective in eliciting specific emotions, thus confirming their potential effectiveness in different fields of psychophysiology. The main limitation of this study is that acquisitions were conducted in a laboratory, where subjects may be little induced to feel emotions being not in a natural environment. Another limitation of the study is the absence of a detailed assessment of the participants\u2019\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
        },
        {
            "heading": "IEEE JOURNAL OF TRANSLATIONAL ENGINEERING IN HEALTH AND MEDICINE 10",
            "text": "mental health and emotional state, which could have allowed to identify the potential presence of response outliers and support a more precise interpretation of individual results. Future research is needed to further validate the findings of this study through an in-depth analysis of possible confounding variables in a larger sample of participants. Moreover, it will be important to identify strategies for more personalized stimulation, for example by optimizing sound stimuli on an individualized basis and by addressing the possible influence of the individual emotional state. Finally, it would be important to create more immersive protocols, such as in virtual reality environments, that can provide an even more effective and emotion-targeting stimulation."
        }
    ],
    "title": "Comparative assessment of physiological responses to emotional elicitation by auditory and visual stimuli",
    "year": 2023
}