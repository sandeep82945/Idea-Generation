{
    "abstractText": "Solving the camera-to-robot pose is a fundamental requirement for vision-based robot control, and is a process that takes considerable effort and cares to make accurate. Traditional approaches require modification of the robot via markers, and subsequent deep learning approaches enabled markerless feature extraction. Mainstream deep learning methods only use synthetic data and rely on Domain Randomization to fill the sim-to-real gap, because acquiring the 3D annotation is labor-intensive. In this work, we go beyond the limitation of 3D annotations for real-world data. We propose an end-to-end pose estimation framework that is capable of online camera-to-robot calibration and a self-supervised training method to scale the training to unlabeled real-world data. Our framework combines deep learning and geometric vision for solving the robot pose, and the pipeline is fully differentiable. To train the Camerato-Robot Pose Estimation Network (CtRNet), we leverage foreground segmentation and differentiable rendering for image-level self-supervision. The pose prediction is visualized through a renderer and the image loss with the input image is back-propagated to train the neural network. Our experimental results on two public real datasets confirm the effectiveness of our approach over existing works. We also integrate our framework into a visual servoing system to demonstrate the promise of real-time precise robot pose estimation for automation tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jingpei Lu"
        },
        {
            "affiliations": [],
            "name": "Florian Richter"
        },
        {
            "affiliations": [],
            "name": "Michael C. Yip"
        }
    ],
    "id": "SP:7eb83fd38961b99878de8a2fb424b6df361c8a66",
    "references": [
        {
            "authors": [
                "Jeannette Bohg",
                "Javier Romero",
                "Alexander Herzog",
                "Stefan Schaal"
            ],
            "title": "Robot arm pose estimation through pixel-wise part classification",
            "venue": "In 2014 IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2014
        },
        {
            "authors": [
                "Joao Borrego",
                "Rui Figueiredo",
                "Atabak Dehban",
                "Plinio Moreno",
                "Alexandre Bernardino",
                "Jos\u00e9 Santos-Victor"
            ],
            "title": "A generic visual perception domain randomisation framework for gazebo",
            "venue": "IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC),",
            "year": 2018
        },
        {
            "authors": [
                "Konstantinos Bousmalis",
                "Alex Irpan",
                "Paul Wohlhart",
                "Yunfei Bai",
                "Matthew Kelcey",
                "Mrinal Kalakrishnan",
                "Laura Downs",
                "Julian Ibarz",
                "Peter Pastor",
                "Kurt Konolige"
            ],
            "title": "Using simulation and domain adaptation to improve efficiency of deep robotic grasping",
            "venue": "IEEE international conference on robotics and automation (ICRA),",
            "year": 2018
        },
        {
            "authors": [
                "Fran\u00e7ois Chaumette",
                "Seth Hutchinson",
                "Peter Corke"
            ],
            "title": "Visual servoing",
            "venue": "In Springer Handbook of Robotics,",
            "year": 2016
        },
        {
            "authors": [
                "Bo Chen",
                "Alvaro Parra",
                "Jiewei Cao",
                "Nan Li",
                "Tat-Jun Chin"
            ],
            "title": "End-to-end learnable geometric vision by backpropagating pnp optimization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Liang-Chieh Chen",
                "George Papandreou",
                "Florian Schroff",
                "Hartwig Adam"
            ],
            "title": "Rethinking atrous convolution for semantic image segmentation",
            "venue": "arXiv preprint arXiv:1706.05587,",
            "year": 2017
        },
        {
            "authors": [
                "Yuhua Chen",
                "Wen Li",
                "Christos Sakaridis",
                "Dengxin Dai",
                "Luc Van Gool"
            ],
            "title": "Domain adaptive faster r-cnn for object detection in the wild",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Jacques Denavit",
                "Richard S"
            ],
            "title": "Hartenberg. A kinematic notation for lower-pair mechanisms based on matrices",
            "year": 1955
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Karthik Desingh",
                "Shiyang Lu",
                "Anthony Opipari",
                "Odest Chadwicke Jenkins"
            ],
            "title": "Factored pose estimation of articulated objects using efficient nonparametric belief propagation",
            "venue": "In 2019 International Conference on Robotics and Automation (ICRA),",
            "year": 2019
        },
        {
            "authors": [
                "Irene Fassi",
                "Giovanni Legnani"
            ],
            "title": "Hand to sensor calibration: A geometrical interpretation of the matrix equation ax= xb",
            "venue": "Journal of Robotic Systems,",
            "year": 2005
        },
        {
            "authors": [
                "Chelsea Finn",
                "Xin Yu Tan",
                "Yan Duan",
                "Trevor Darrell",
                "Sergey Levine",
                "Pieter Abbeel"
            ],
            "title": "Deep spatial autoencoders for visuomotor learning",
            "venue": "In 2016 IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2016
        },
        {
            "authors": [
                "Xiao-Shan Gao",
                "Xiao-Rong Hou",
                "Jianliang Tang",
                "Hang-Fei Cheng"
            ],
            "title": "Complete solution classification for the perspective-three-point problem",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2003
        },
        {
            "authors": [
                "Sergio Garrido-Jurado",
                "Rafael Mu\u00f1oz-Salinas",
                "Francisco Jos\u00e9 Madrid-Cuevas",
                "Manuel Jes\u00fas Mar\u0131\u0301n- Jim\u00e9nez"
            ],
            "title": "Automatic generation and detection of highly reliable fiducial markers under occlusion",
            "venue": "Pattern Recognition,",
            "year": 2014
        },
        {
            "authors": [
                "Abhishek Gupta",
                "Coline Devin",
                "YuXuan Liu",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Learning invariant feature spaces to transfer skills with reinforcement learning",
            "venue": "arXiv preprint arXiv:1703.02949,",
            "year": 2017
        },
        {
            "authors": [
                "Ran Hao",
                "Orhan \u00d6zg\u00fcner",
                "M Cenk \u00c7avu\u015fo\u011flu"
            ],
            "title": "Visionbased surgical tool pose estimation for the da vinci\u00ae robotic surgical system",
            "venue": "In 2018 IEEE/RSJ international conference on intelligent robots and systems (IROS),",
            "year": 2018
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Stefan Hinterstoisser",
                "Vincent Lepetit",
                "Paul Wohlhart",
                "Kurt Konolige"
            ],
            "title": "On pre-trained image features and synthetic images for deep learning",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV) Workshops,",
            "year": 2018
        },
        {
            "authors": [
                "Judy Hoffman",
                "Eric Tzeng",
                "Taesung Park",
                "Jun-Yan Zhu",
                "Phillip Isola",
                "Kate Saenko",
                "Alexei Efros",
                "Trevor Darrell"
            ],
            "title": "Cycada: Cycle-consistent adversarial domain adaptation",
            "venue": "In International conference on machine learning,",
            "year": 1998
        },
        {
            "authors": [
                "Radu Horaud",
                "Fadi Dornaika"
            ],
            "title": "Hand-eye calibration",
            "venue": "The international journal of robotics research,",
            "year": 1995
        },
        {
            "authors": [
                "D\u00e1niel Horv\u00e1th",
                "G\u00e1bor Erd\u0151s",
                "Zolt\u00e1n Istenes",
                "Tom\u00e1\u0161 Horv\u00e1th",
                "S\u00e1ndor F\u00f6ldi"
            ],
            "title": "Object detection using sim2real domain randomization for robotic applications",
            "venue": "IEEE Transactions on Robotics,",
            "year": 2022
        },
        {
            "authors": [
                "Jarmo Ilonen",
                "Ville Kyrki"
            ],
            "title": "Robust robot-camera calibration",
            "venue": "In 2011 15th International Conference on Advanced Robotics (ICAR),",
            "year": 2011
        },
        {
            "authors": [
                "Stephen James",
                "Paul Wohlhart",
                "Mrinal Kalakrishnan",
                "Dmitry Kalashnikov",
                "Alex Irpan",
                "Julian Ibarz",
                "Sergey Levine",
                "Raia Hadsell",
                "Konstantinos Bousmalis"
            ],
            "title": "Sim-to-real via simto-sim: Data-efficient robotic grasping via randomizedto-canonical adaptation networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Hiroharu Kato",
                "Yoshitaka Ushiku",
                "Tatsuya Harada"
            ],
            "title": "Neural 3d mesh renderer",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Steven George Krantz",
                "Harold R Parks"
            ],
            "title": "The implicit function theorem: history, theory, and applications",
            "venue": "Springer Science & Business Media,",
            "year": 2002
        },
        {
            "authors": [
                "Yann Labb\u00e9",
                "Justin Carpentier",
                "Mathieu Aubry",
                "Josef Sivic"
            ],
            "title": "Single-view robot pose and joint angle estimation via render & compare",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Jens Lambrecht",
                "Philipp Grosenick",
                "Marvin Meusel"
            ],
            "title": "Optimizing keypoint-based single-shot camera-to-robot pose estimation through shape segmentation",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2021
        },
        {
            "authors": [
                "Jens Lambrecht",
                "Linh K\u00e4stner"
            ],
            "title": "Towards the usage of synthetic data for marker-less pose estimation of articulated robots in rgb images",
            "venue": "In 2019 19th International Conference on Advanced Robotics (ICAR),",
            "year": 2019
        },
        {
            "authors": [
                "Timothy E Lee",
                "Jonathan Tremblay",
                "Thang To",
                "Jia Cheng",
                "Terry Mosier",
                "Oliver Kroemer",
                "Dieter Fox",
                "Stan Birchfield"
            ],
            "title": "Camera-to-robot pose estimation from a single image",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2020
        },
        {
            "authors": [
                "Vincent Lepetit",
                "Francesc Moreno-Noguer",
                "Pascal Fua"
            ],
            "title": "Epnp: An accurate o (n) solution to the pnp problem",
            "venue": "International journal of computer vision,",
            "year": 2009
        },
        {
            "authors": [
                "Yang Li",
                "Florian Richter",
                "Jingpei Lu",
                "Emily K Funk",
                "Ryan K Orosco",
                "Jianke Zhu",
                "Michael C Yip"
            ],
            "title": "Super: A surgical perception framework for endoscopic tissue manipulation with surgical robotics",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2020
        },
        {
            "authors": [
                "Yi Li",
                "Gu Wang",
                "Xiangyang Ji",
                "Yu Xiang",
                "Dieter Fox"
            ],
            "title": "Deepim: Deep iterative matching for 6d pose estimation",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Shichen Liu",
                "Tianye Li",
                "Weikai Chen",
                "Hao Li"
            ],
            "title": "Soft rasterizer: A differentiable renderer for image-based 3d reasoning",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Jingpei Lu",
                "Florian Richter",
                "Michael C Yip"
            ],
            "title": "Pose estimation for robot manipulators via keypoint optimization and sim-to-real transfer",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2022
        },
        {
            "authors": [
                "Jeffrey Mahler",
                "Matthew Matl",
                "Vishal Satish",
                "Michael Danielczuk",
                "Bill DeRose",
                "Stephen McKinley",
                "Ken Goldberg"
            ],
            "title": "Learning ambidextrous robot grasping policies",
            "venue": "Science Robotics,",
            "year": 2019
        },
        {
            "authors": [
                "Fabian Manhardt",
                "Wadim Kehl",
                "Nassir Navab",
                "Federico Tombari"
            ],
            "title": "Deep model-based 6d pose refinement in rgb",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Frank Michel",
                "Alexander Krull",
                "Eric Brachmann",
                "Michael Ying Yang",
                "Stefan Gumhold",
                "Carsten Rother"
            ],
            "title": "Pose estimation of kinematic chain instances via object coordinate regression",
            "venue": "In BMVC,",
            "year": 2015
        },
        {
            "authors": [
                "Edwin Olson"
            ],
            "title": "Apriltag: A robust and flexible visual fiducial system",
            "venue": "IEEE international conference on robotics and automation,",
            "year": 2011
        },
        {
            "authors": [
                "Frank C Park",
                "Bryan J Martin"
            ],
            "title": "Robot sensor calibration: solving AX= XB on the Euclidean group",
            "venue": "IEEE Transactions on Robotics and Automation,",
            "year": 1994
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Soumith Chintala",
                "Gregory Chanan",
                "Edward Yang",
                "Zachary DeVito",
                "Zeming Lin",
                "Alban Desmaison",
                "Luca Antiga",
                "Adam Lerer"
            ],
            "title": "Automatic differentiation in pytorch",
            "year": 2017
        },
        {
            "authors": [
                "Karl Pauwels",
                "Leonardo Rubio",
                "Eduardo Ros"
            ],
            "title": "Real-time model-based articulated object pose detection and tracking with variable rigidity constraints",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2014
        },
        {
            "authors": [
                "Nikhila Ravi",
                "Jeremy Reizenstein",
                "David Novotny",
                "Taylor Gordon",
                "Wan-Yen Lo",
                "Justin Johnson",
                "Georgia Gkioxari"
            ],
            "title": "Accelerating 3d deep learning with pytorch3d",
            "venue": "arXiv preprint arXiv:2007.08501,",
            "year": 2020
        },
        {
            "authors": [
                "Florian Richter",
                "Jingpei Lu",
                "Ryan K Orosco",
                "Michael C Yip"
            ],
            "title": "Robotic tool tracking under partially visible kinematic chain: A unified approach",
            "venue": "IEEE Transactions on Robotics,",
            "year": 2021
        },
        {
            "authors": [
                "Swami Sankaranarayanan",
                "Yogesh Balaji",
                "Arpit Jain",
                "Ser Nam Lim",
                "Rama Chellappa"
            ],
            "title": "Learning from synthetic data: Addressing domain shift for semantic segmentation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Tanner Schmidt",
                "Richard A Newcombe",
                "Dieter Fox"
            ],
            "title": "Dart: Dense articulated real-time tracking",
            "venue": "In Robotics: Science and Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Martin Sundermeyer",
                "Zoltan-Csaba Marton",
                "Maximilian Durner",
                "Manuel Brucker",
                "Rudolph Triebel"
            ],
            "title": "Implicit 3d orientation learning for 6d object detection from rgb images",
            "venue": "In Proceedings of the european conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Josh Tobin",
                "Lukas Biewald",
                "Rocky Duan",
                "Marcin Andrychowicz",
                "Ankur Handa",
                "Vikash Kumar",
                "Bob McGrew",
                "Alex Ray",
                "Jonas Schneider",
                "Peter Welinder"
            ],
            "title": "Domain randomization and generative models for robotic grasping",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2018
        },
        {
            "authors": [
                "Josh Tobin",
                "Rachel Fong",
                "Alex Ray",
                "Jonas Schneider",
                "Wojciech Zaremba",
                "Pieter Abbeel"
            ],
            "title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "venue": "IEEE/RSJ international conference on intelligent robots and systems (IROS),",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Tremblay",
                "Aayush Prakash",
                "David Acuna",
                "Mark Brophy",
                "Varun Jampani",
                "Cem Anil",
                "Thang To",
                "Eric Cameracci",
                "Shaad Boochoon",
                "Stan Birchfield"
            ],
            "title": "Training deep networks with synthetic data: Bridging the reality gap by domain randomization",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition workshops,",
            "year": 2018
        },
        {
            "authors": [
                "Jonathan Tremblay",
                "Thang To",
                "Balakumar Sundaralingam",
                "Yu Xiang",
                "Dieter Fox",
                "Stan Birchfield"
            ],
            "title": "Deep object pose estimation for semantic robotic grasping of household objects",
            "venue": "arXiv preprint arXiv:1809.10790,",
            "year": 2018
        },
        {
            "authors": [
                "Mei Wang",
                "Weihong Deng"
            ],
            "title": "Deep visual domain adaptation: A survey",
            "venue": "Neurocomputing, 312:135\u2013153,",
            "year": 2018
        },
        {
            "authors": [
                "Felix Widmaier",
                "Daniel Kappler",
                "Stefan Schaal",
                "Jeannette Bohg"
            ],
            "title": "Robot arm pose estimation by pixel-wise regression of joint angles",
            "venue": "In 2016 IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2016
        },
        {
            "authors": [
                "Yu Xiang",
                "Tanner Schmidt",
                "Venkatraman Narayanan",
                "Dieter Fox"
            ],
            "title": "Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes",
            "venue": "arXiv preprint arXiv:1711.00199,",
            "year": 2017
        },
        {
            "authors": [
                "Wenshuai Zhao",
                "Jorge Pe\u00f1a Queralta",
                "Tomi Westerlund"
            ],
            "title": "Sim-to-real transfer in deep reinforcement learning for robotics: a survey",
            "venue": "IEEE Symposium Series on Computational Intelligence (SSCI),",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "The majority of modern robotic automation utilizes cameras for rich sensory information about the environment to infer tasks to be completed and provide feedback for closed-loop control. The leading paradigm for converting the valuable environment information to the robot\u2019s frame of reference for manipulation is position-based visual servoing (PBVS) [4]. At a high level, PBVS converts 3D environmental information inferred from the visual data (e.g.\nthe pose of an object to be grasped) and transforms it to the robot coordinate frame where all the robot geometry is known (e.g. kinematics) using the camera-to-robot pose. Examples of robotic automation using the PBVS range from bin sorting [35] to tissue manipulation in surgery [31].\nCalibrating camera-to-robot pose typically requires a significant amount of care and effort. Traditionally, the camera-to-robot pose is calibrated with externally attached fiducial markers (e.g. Aruco Marker [14], AprilTag [38]). The 2D location of the marker can be extracted from the image and the corresponding 3D location on the robot can be calculated with forward kinematics. Given a set 2D-3D correspondence, the camera-to-robot pose can be solved using Perspective-n-Point (PnP) methods [13, 30]. The procedure usually requires multiple runs with different robot configurations and once calibrated, the robot base and the camera are assumed static. The incapability of online calibration limits the potential applications for vision-based robot control in the real world, where minor bumps or simply shifting due to repetitive use will cause calibrations to be thrown off,\nar X\niv :2\n30 2.\n14 33\n2v 2\n[ cs\n.C V\n] 2\n1 M\nar 2\n02 3\nnot to mention real-world environmental factors like vibration, humidity, and temperature, are non-constant. Having flexibility on the camera and robot is more desirable so that the robot can interact with an unstructured environment.\nDeep learning, known as the current state-of-the-art approach for image feature extraction, brings promising ways for markerless camera-to-robot calibration. Current approaches to robot pose estimation are mainly classified into two categories: keypoint-based methods [27\u201329,34,43] and rendering-based methods [16,26]. Keypoint-based methods are the most popular approach for pose estimation because of the fast inference speed. However, the performance is limited to the accuracy of the keypoint detector which is often trained in simulation such that the proposed methods can generalize across different robotic designs. Therefore, the performance is ultimately hampered by the sim-to-real gap, which is a long-standing challenge in computer vision and robotics [55].\nRendering-based methods can achieve better performance by using the shape of the entire robot as observation, which provides dense correspondence for pose estimation. The approaches in this category usually employ an iterative refinement process and require a reasonable initialization for the optimization loop to converge [32]. Due to the nature that iteratively render and compare is time- and energyconsuming, rendering-based methods are more suitable for offline estimation where the robot and camera are held stationary. In more dynamic scenarios, such as a mobile robot, the slow computation time make the rendering-based methods impracticable to use.\nIn this work, we propose CtRNet, an end-to-end framework for robot pose estimation which, at inference, uses keypoints for the fast inference speed and leverages the high performance of rendering-based methods for training to overcome the sim-to-real gap previous keypoint-based methods faced. Our framework contains a segmentation module to generate a binary mask of the robot and keypoint detection module which extracts point features for pose estimation. Since segmenting the robot from the background is a simpler task than estimating the robot pose and localizing point features on robot body parts, we leverage foreground segmentation to provide supervision for the pose estimation. Toward this direction, we first pretrained the network on synthetic data, which should have acquired essential knowledge about segmenting the robot. Then, a self-supervised training pipeline is proposed to transfer our model to the real world without manual labels. We connect the pose estimation to foreground segmentation with a differentiable renderer [24,33]. The renderer generates a robot silhouette image of the estimated pose and directly compares it to the segmentation result. Since the entire framework is differentiable, the parameters of the neural network can be optimized by back-propagating the image loss.\nContributions. Our main contribution is the novel framework for image-based robot pose estimation together with a scalable self-training pipeline that utilizes unlimited real-world data to further improve the performance without any manual annotations. Since the keypoint detector is trained with image-level supervision, we effectively encompass the benefits from both keypoint-based and rendering-based methods, where previous methods were divided. As illustrated in the Fig. 1, our method maintains high inference speed while matching the performance of the rendering-based methods. Moreover, we integrate the CtRNet into a robotic system for PBVS and demonstrate the effectiveness on real-time robot pose estimation."
        },
        {
            "heading": "2. Related Works",
            "text": ""
        },
        {
            "heading": "2.1. Camera-to-Robot Pose Estimation",
            "text": "The classical way to calibrate the camera-to-robot pose is to attach the fiducial markers [14, 38] to known locations along the robot kinematic chain. The marker is detected in the image frame and their 3D position in the robot base frame can be calculated with forward kinematics. With the geometrical constraints, the robot pose can be then derived by solving an optimization problem [11, 20, 22, 39].\nEarly works on markerless articulated pose tracking utilize a depth camera for 3D observation [10, 37, 41, 45]. For a high degree-of-freedom articulated robot, Bohg et al. [1] proposed a pose estimation method by first classifying the pixels in depth image to robot parts, and then a voting scheme is applied to estimate the robot pose relative to the camera. This method is further improved in [52] by directly training a Random Forest to regress joint angles instead of part label. However, these methods are not suitable for our scenario where only single RGB image is available.\nMore recently, as deep learning becomes popular in feature extraction, many works have been employing deep neural networks for robot pose estimation. Instead of using markers, a neural network is utilized for keypoint detection, and the robot pose is estimated through an optimizer (e.g. PnP solver) [28, 29, 34, 56]. To further improve the performance, the segmentation mask and edges are utilized to refine the robot pose [16,27]. Labbe\u0301 et al. [26] also introduces the render&compare method to estimate the robot pose by matching the robot shape. These methods mainly rely on synthetically generated data for training and hope the network can generalize to the real world by increasing the variance in data generation. Our method explicitly deals with the sim-to-real transfer by directly training on real-world data with self-supervision."
        },
        {
            "heading": "2.2. Domain Adaptation for Sim-to-Real Transfer",
            "text": "In computer vision and robotics, Domain Randomization (DR) [48] is the most widely used method for sim-\nto-real transfer due to its simplicity and effectiveness. The idea is to randomize some simulation parameters (e.g. camera position, lighting, background, etc.) and hope that the randomization captures the distribution of the real-world data. This technique has been applied to object detection and grasping [2, 18, 21, 47, 49], and pose estimation [26, 28, 29, 34, 36, 46, 50, 56]. The randomization is usually tuned empirically hence it is not efficient.\nAnother popular technique for domain transfer is Domain Adaptation (DA), which is to find the feature spaces that share a similar distribution between the source and target domains [51]. This technique has shown recent success in computer vision [7, 19, 44] and robotic applications [3, 15, 23]. In this work, instead of finding the latent space and modeling the distribution between the simulation and the real world, we perform sim-to-real transfer by directly training on the real-world data via a self-training pipeline."
        },
        {
            "heading": "3. Methods",
            "text": "In this paper, we introduce an end-to-end framework for robot pose estimation and a scalable training pipeline to improve pose estimation accuracy on real-world data without the need for any manual annotation. We first explain the self-supervised training pipeline for sim-to-real transfer in Sec. 3.1 given a pretrained CtRNet on synthetic data which both segments the robot and estimates its pose from images. Then, we detail the camera-to-robot pose estimation network in Sec. 3.2 which utilizes a keypoint detector and a PnP solver to estimate the pose of the robot from image data in real-time."
        },
        {
            "heading": "3.1. Self-supervised Training for Sim-to-Real Transfer",
            "text": "The most effective way to adapt the neural network to the real world is directly training the network on real sensor data. We propose a self-supervised training pipeline for sim-to-real transfer to facilitate the training without 3D annotations. To conduct the self-supervised training, we employ foreground segmentation to generate a mask of the robot, fseg , alongside the pose estimation, fpose. Given an input RGB image from the physical world, I, and the robot joint angles, q, fpose estimates the robot pose which is then transformed to a silhouette image through a differentiable renderer. Our self-supervised objective is to optimize neural network parameters by minimizing the difference between the rendered silhouette image and the mask image. We formulate the optimization problem as:\n\u03b8bb, \u03b8kp, \u03b8seg = arg min \u03b8bb,\u03b8kp,\u03b8seg\nL[fseg(I|\u03b8bb, \u03b8seg),\nR(fpose(I|q, \u03b8bb, \u03b8kp)|K)] (1)\nwhere \u03b8bb, \u03b8kp, \u03b8seg denote the parameters of the backbone, keypoint, and segmentation layers of the neural network. R is the differentiable renderer with camera parameters K, and L(.) is the objective loss function capturing the image difference.\nWe pretrained CtRNet\u2019s parameters which makes up fseg and fpose, with synthetic data where the keypoint and segmentation labels are obtained freely (details in Supplementary Materials). During the self-training phase, where CtRNet learns with real data, the objective loss in (1) captures the difference between the segmentation result and the rendered image. The loss is iteratively back-propagated to, \u0398, where each iteration fseg and fpose take turns learning from each other to overcome the sim-to-real gap.\nOverview. The overview of the self-supervised training pipeline is shown in the Fig. 2. The segmentation module, fseg , simply takes in a robot image and outputs its mask. The pose estimation module, fpose, consists of a keypoint detector and a PnP solver to estimate the robot pose using the 2D-3D point correspondence, as shown in Fig. 3. Given the input robot image and joint angles, our camerato-robot pose estimation network outputs a robot mask and the robot pose with respect to the camera frame. Mathematically, these functions are denoted as\nM = fseg(I|\u03b8bb, \u03b8seg) Tcb = fpose(I|q, \u03b8bb, \u03b8kp) (2)\nwhere M is the robot mask and Tcb \u2208 SE(3) is the 6-DOF robot pose. Finally, the self-supervised objective loss in (1) is realized through a differentiable renderer, R, which generates a silhouette image of the robot given its pose, Tcb.\nDifferentiable Rendering. To render the robot silhouette image, we utilize the PyTorch3D differentiable render [42]. We initialize a perspective camera with intrinsic parameters K and a silhouette renderer, which does not apply any lighting nor shading, is constructed with a rasterizer and a shader. The rasterizer applies the fast rasterization method [42] which selects the k nearest mesh triangles that effects each pixel and weights their influence according to the distance along the z-axis. Finally, the SoftSilhouetteShader is applied to compute pixel values of the rendered image using the sigmoid blending method [33].\nWe construct the ready-to-render robot mesh by connecting the CAD model for each robot body part using its forward kinematics and transforming them to the camera frame with the estimated robot pose Tcb from fpose. Let v\nn \u2208 R3 be a mesh vertex on the n-th robot link. Each vertex is transformed to the camera frame, hence ready-to-render, by\nvc = TcbT b n(q)v n (3)\nwhere \u00b7 represents the homogeneous representation of a point (e.g. v = [v, 1]T ), and Tbn(q) is the coordinate frame transformation obtained from the forward kinematics [8].\nObjective loss function. The objective loss in (1) is iteratively minimized where fseg and fpose take turns supervising each other on real data to overcome the sim-to-real gap faced by keypoint detection networks. To optimize fpose, the L2 image loss is used since the segmentation network\u2019s accuracy, within the context of estimating robot poses, has been shown to effectively transfer from simulation to the real world [26]. Mathematically the loss is expressed as\nLmask = H\u2211 i=1 W\u2211 j=1 (S(i, j)\u2212M(i, j))2 (4)\nwhere H and W is the height and width of the image, and S is the rendered silhouette image.\nAlthough the pretrained robot segmentation, fseg , already performs well on real-world datasets, it is still desirable to refine it through self-supervised training to better extract fine details of corners and boundaries. To prevent the foreground segmentation layers from receiving noisy training signals, we apply the weighted Binary Cross Entropy Loss so that the high-quality rendering image can be used to further refine the foreground segmentation:\nLseg = \u2212 w\nH \u2217W H\u2211 i=1 W\u2211 j=1 [M(i, j) log S(i, j)\n+ (1\u2212M(i, j)) log(1\u2212 S(i, j))]. (5)\nwhere w is the weight for the given training sample. For PnP solvers, the optimal solution should minimize the point reprojection error. Therefore, we assign the weight for each training sample according to the reprojection error:\nw = exp (\u2212sO(o,p,K,Tcb)) (6)\nwhere s is a scaling constant, O is the reprojection loss in the PnP solver (explained in Sec. 3.2), {oi|oi \u2208 R2}ni=1 and {pi|pi \u2208 R3}ni=1 are the 2D-3D keypoints inputted into the PnP solver. The exponential function is applied to the weight such that training samples with poor PnP convergence are weighted exponentially lower than good PnP convergence thereby stabilizing the training."
        },
        {
            "heading": "3.2. Camera-to-Robot Pose Estimation Network",
            "text": "The overview of the proposed Camera-to-Robot Pose Estimation Network, CtRNet, is shown in Fig. 3. Given an\ninput RGB image, we employ ResNet50 [17] as the backbone network to extract the latent features. The latent features are then passed through the Atrous Spatial Pyramid Pooling layers [6] to form the segmentation mask of input resolution. The keypoint detector, sharing the backbone network with the foreground segmentation, upsamples the feature maps through transposed convolutional layers and forms the heatmaps with n channels. Then, we apply the spatial softmax operator [12] on the heatmaps, which computes the expected 2D location of the points of maximal activation for each channel and results in a set of keypoints [o1, ...,on] for all n channels. For simplicity, we define the set of keypoints at each joint location of the robot. Given the joint angles, the corresponding 3D keypoint location pi can be calculated with robot forward kinematics:\npi = T b i (q)t, for i = 1, ..., n (7)\nwhere t = [0, 0, 0]. With the 2D and 3D corresponding keypoints, we can then apply a PnP solver [30] to estimate the robot pose with respect to the camera frame.\nBack-propagation for PnP Solver. A PnP solver is usually self-contained and not differentiable as the gradient with respect to the input cannot be derived explicitly. Inspired by [5], the implicit function theorem [25] is applied to obtain the gradient through implicit differentiation. Let the PnP solver be denoted as followed in the form of a non-linear function g:\nTc\u2217b = g(o,p,K) (8)\nwhere Tc\u2217b is output pose from the PnP solver. In order to back-propagate through the PnP solver for training the keypoint detector, we are interested in finding the gradient of the output pose Tc\u2217b with respect to the input 2D points o. Note that, the objective of the PnP solver is to minimize the reprojection error, such that:\nTc\u2217b = arg min Tcb O(o,p,K,Tcb) (9)\nwith\nO(Tcb,p,K,T c b) = n\u2211 i=1 ||oi \u2212 \u03c0(pi|Tcb,K)||22 (10)\n= n\u2211 i=1 ||ri||22 (11)\nwhere \u03c0(.) is the projection operator. Since the optimal solution Tc\u2217b is a local minimum for the objective function O(o,p,Tcb,K), a stationary constraint of the optimization process can be constructed by taking the first order derivative of the objective function with respect to Tcb:\n\u2202O\n\u2202Tcb (o,p,K,Tcb)|Tcb=Tc\u2217b = 0. (12)\nFollowing [5], we construct a constrain function F to employ the implicit function theorem:\nF (o,p,K,Tcb) = \u2202O\n\u2202Tcb (o,p,K,Tc\u2217b ) = 0. (13)\nSubstituting the Eq. (10) and Eq. (11) to Eq. (13), we can derive the constraint function as:\nF (o,p,K,Tcb) = n\u2211 i=1 \u2202||ri||22 \u2202Tcb\n(14)\n= \u22122 n\u2211 i=1 rTi \u2202\u03c0 \u2202Tcb (pi|Tc\u2217b ,K). (15)\nFinally, we back-propagate through the PnP solver with the implicit differentiation. The gradient of the output pose with respect to the input 2D points is the Jacobian matrix:\n\u2202g \u2202o (o,p,K)\n= \u2212 ( \u2202F\n\u2202Tcb (o,p,K,Tcb)\n)\u22121( \u2202F\n\u2202o (o,p,K,Tcb)\n) .\n(16)"
        },
        {
            "heading": "4. Experiments",
            "text": "We first evaluate our method on two public real-world datasets for robot pose estimation and compare it against several state-of-the-art image-based robot pose estimation algorithms. We then conduct an ablation study on the pretraining procedure and explore how the number of pretraining samples could affect the performance of the selfsupervised training. Finally, we integrate the camera-torobot pose estimation framework into a visual servoing system to demonstrate the effectiveness of our method on real robot applications."
        },
        {
            "heading": "4.1. Datasets and Evaluation Metrics",
            "text": "DREAM-real Dataset. The DREAM-real dataset [29] is a real-world robot dataset collected with 3 different cameras: Azure Kinect (AK), XBOX 360 Kinect (XK), and RealSense (RS). This dataset contains around 50K RGB images of Franka Emika Panda arm and is recorded at (640 \u00d7 480) resolution. The ground-truth camera-to-robot pose is provided for every image frame. The accuracy is evaluated with average distance (ADD) metric [54],\nADD = 1\nn n\u2211 i=1 ||T\u0303cbpi \u2212Tcbpi||2 (17)\nwhere T\u0303cb indicates the ground-truth camera-to-robot pose. We also report the area-under-the-curve (AUC) value, which integrates the percentage of ADD over different\nthresholds. A higher AUC value indicates more predictions with less error.\nBaxter Dataset. The Baxter dataset [34] contains 100 RGB images of the left arm of Rethink Baxter collected with Azure Kinect camera at (2048\u00d71526) resolution. The 2D and 3D ground-truth end-effector position with respect to the camera frame is provided. We evaluate the performance with the ADD metric for the end-effector. We also evaluate the end-effector reprojection error using the percentage of correct keypoints (PCK) metric [34]."
        },
        {
            "heading": "4.2. Implementation details",
            "text": "The entire pipeline is implemented in PyTorch [40]. We initialize the backbone network with ImageNet [9] pretrained weights, and we train separate networks for different robots. The number of keypoints n is set to the number of robot links and the keypoints are defined at the robot joint locations. The neural network is pretrained on synthetic data for foreground segmentation and keypoint detection for 1000 epochs with 1e-5 learning rate. We reduce the learning rate by a factor of 10 once learning stagnates for 5 epochs. The Adam optimizer is applied to optimize the network parameters with the momentum set to 0.9. For selfsupervised training on real-world data, we run the training for 500 epochs with 1e-6 learning rate. The same learning rate decay strategy and Adam optimizer is applied here similar to the pretraining. To make the training more stable, we clip the gradient of the network parameters at 10. The scaling factor in Eq. (6) is set to 0.1 for DREAM-real dataset and 0.01 for Baxter dataset, mainly accounting for the difference in resolution."
        },
        {
            "heading": "4.3. Robot Pose Estimation on Real-world Datasets",
            "text": "Evaluation on DREAM-real Dataset. The proposed CtRNet is trained at (320\u00d7240) resolution and evaluated at the original resolution by scaling up the keypoints by a factor of 2. Some qualitative results for foreground segmentation and pose estimation are shown in the Fig. 4a. We compared our method with the state-of-the-art keypoint-based method DREAM [29] and the rendering-based method RoboPose [26]. The results for DREAM and RoboPose are compiled from the implementation provided by [26]. In Tab. 1, we report the AUC and mean ADD results on DREAM-real dataset with 3 different camera settings and the overall results combining all the test samples. Our method has a significantly better performance compared to the method in the same category and achieves comparable performance with the rendering-based method. We outperform DREAM on all settings and outperform RoboPose on the majority of the dataset. Overall on DREAMreal dataset, we achieve higher AUC (+17.378 compared to DREAM, +5.868 compared to RoboPose), and lower error compared to DREAM (-17.457).\nEvaluation on Baxter Dataset. For the Baxter dataset, we trained the CtRNet at (640 \u00d7 480) resolution and evaluate at the original resolution, and Fig. 4b shows some of the qualitative results. We compared our method with several keypoint-based methods (Aruco Marker [14], DREAM [29], Optimized Keypoints [34]). We also implemented Differentiable Rendering for robot pose estimation, where the robot masks are generated with the pretrained foreground segmentation. The 2D PCK results and 3D ADD results are reported in Tab. 2. Our method out-\nperforms all other methods on both 2D and 3D evaluations. For 2D evaluation, we achieve 93.94 AUC for PCK with an average reprojection error of 11.62 pixels. For 3D evaluation, we achieve 83.93 AUC for ADD with an average ADD of 63.81mm. Notably, 99 percent of our estimation has less than 50 pixel reprojection error, which is less than 2 percent of the image resolution, and 88 percent of our estimation has less than 100mm distance error when localizing the end-effector."
        },
        {
            "heading": "4.4. Ablation Study",
            "text": "We study how the number of pretraining samples affects the convergence and performance of the self-supervised training empirically on the Baxter dataset. We pretrain the neural network with different numbers of synthetic data samples Npretrain = {500, 1000, 2000, 4000, 8000}, and examine the convergence of the self-supervised training process. Fig. 5 shows the plot of self-training loss (Lmask + Lseg) vs. the number of epochs for networks pretrianed with different number of synthetic data. We observe that doubling the size of pretraining dataset significantly improves the convergence of the self-training process at the beginning. However, the improvement gets smaller as the pretrainig size increase. For the Baxter dataset, the improvement saturates after having more than 2000 pretraining samples. Continuing double the training size results in very marginal improvement. Noted that the Baxter dataset captures 20 different robot poses from a fixed camera position. The required number of pretraining samples might vary according to the complexity of the environment.\nWe further evaluate the resulting neural networks with the ground-truth labels on the Baxter dataset. We report the mean ADD and AUC ADD for the pose estimation in Tab. 3. The result verifies our observation on the convergence analysis. Having more pretraining samples improves the performance of pose estimation at the beginning, but the improvement stagnates after having more than 2000 pretraining samples."
        },
        {
            "heading": "4.5. Visual Servoing Experiment",
            "text": "We integrate the proposed CtRNet into a robotic system for position-based visual servoing (PBVS) with eye-to-hand\nconfiguration. We conduct the experiment on a Baxter robot and the details of the PBVS are described in the Supplementary Materials. The PBVS is purely based on RGB images from a single camera and the goal is to control the robot end-effector reaching a target pose defined in the camera frame. Specifically, we first set a target pose with respect to the camera frame. The target pose is then transformed into the robot base frame through the estimated camera-to-robot transformation. The robot controller calculates the desired robot configuration with inverse kinematics and a control law is applied to move the robot end-effector toward the target pose.\nFor comparison, we also implemented DREAM [29] and a Differentiable Renderer for PBVS. For DREAM, the pretrained model for Baxter is applied. For Differentiable Renderer, we use the foreground segmentation of CtRNet to generate a robot mask. The optimizing loop for the renderer takes the last estimation as initialization and performs 10 updates at each callback to ensure convergence and maintain 1Hz loop rate. In the experiment, we randomly set the target pose and the position of the camera, and the robotic system applies PBVS to reach the target pose from an arbitrary initialization, as shown in the Fig. 6. We ran the experiment for 10 trails with different robot pose estimation methods, and the translational (Euclidean distance) and rotational errors (Euler angles) of the end-effector are reported in Tab. 4. The experimental results show that our proposed method significantly improves the stability and accuracy of the PBVS, achieving 0.002m averaged translational error and 0.002rad rotational error on the end-effector.\nWe also plot the end-effector distance-to-goal over time for a selected trail in Fig. 7. In this selected trial, the system could not converge with DREAM because the poor robot pose estimation confuses the controller by giving the wrong target pose in the robot base frame, which is unreachable.\nWith the differentiable renderer, the servoing system takes more than 10 seconds to converge and oscillate due to the low loop rate. With our proposed CtRNet, the servoing system converges much faster (\u2264 5 seconds), thanks to the fast and robust robot pose estimation. We show more qualitative results in the Supplementary Materials."
        },
        {
            "heading": "5. Conclusion",
            "text": "We present the CtRNet, an end-to-end image-based robot pose estimation framework, and a self-supervised training pipeline that utilizes unlabelled real-world data for simto-real transfer. The CtRNet, using a keypoint detector for pose estimation while employing a rendering method for training, achieves state-of-the-art performance on robot pose estimation while maintaining high-speed inference. The Fig. 1 illustrates the advantages of CtRNet over existing methods, where the AUC values are normalized across two evaluation datasets by taking DREAM and CtRNet as references. We further experiment with different robot pose estimation methods by applying them to PBVS, which demonstrates CtRNet\u2019s fast and accurate robot pose estimation enabling stability when using single-frame robot pose estimation for feedback. Therefore, CtRNet supports real-time markerless camera-to-robot pose estimation which has been utilized for surgical robotic manipulation [43] and mobile robot manipulators [53]. For future work, we would like to extend our method to more robots and explore vision-based control in an unstructured environment."
        },
        {
            "heading": "6. Supplementary Materials",
            "text": ""
        },
        {
            "heading": "6.1. Generate Synthetic Training Data",
            "text": "Setting up a pipeline for generating robot masks and keypoints can be complicated or simple depending on the choice of the simulator. However, an interface for acquiring the robot pose with respect to a camera frame must exist for every robot simulator. Therefore, instead of generating ground-truth labels for the robot mask and keypoints directly from a simulator, we only save the robot pose and configuration pair for each synthetic image, and the labels for the robot mask and keypoint are generated on-the-fly during the training process.\nGiven the ground-truth robot pose with respect to the camera frame T\u0303cb and robot configuration q, the keypoint labels o\u0303i can be generated through the projection operation:\no\u0303i = \u03c0(pi|T\u0303cb,K) (18)\nwhere K is the camera intrinsic matrix and pi is calculated using robot forward kinematics with known robot joint angles q as Eq. (7). For generating the robot mask, we apply a silhouette renderer R which is described in Section 3.1. Given the ground-truth robot pose, the robot mask is generated as:\nS\u0303 = R(T\u0303cb|K) (19)\nwhere S\u0303 is the ground-truth label for robot mask. For generating the synthetic images, similar to Domain Randomization [48], we also randomize a few settings of the robot simulator so that the generated samples can have some varieties. Specifically, we randomize the following aspects:\n\u2022 Robot joint configuration.\n\u2022 The camera position, which applies the look-at method so that the robot is always in the viewing frustum.\n\u2022 The number, position, and intensity of the scene lights.\n\u2022 The position of the virtual objects.\n\u2022 The background of the images.\n\u2022 The color of robot mesh.\nWe also perform image augmentation with additive white Gaussian noise. The synthetic data for the Baxter is generated using CoppeliaSim1. For the Panda, the synthetic dataset provided from [29] is used for training.\n1https://www.coppeliarobotics.com/"
        },
        {
            "heading": "6.2. Details on Position-based Visual Servoing",
            "text": "The diagram of the position-based visual servoing system is shown in the Fig. 8. In our experimental setup, the camera is not necessarily stationary and the goal pose, Tcg , defined in the camera frame, is also changing over time. Therefore, for each control loop, we need to compute the camera-to-robot pose Tcb to update the goal pose in the robot base frame:\nTbg = [T c b] \u22121Tcg. (20)\nGiven the current joint angle reading and image, CtRNet outputs the camera-to-robot pose. The goal pose is a predefined trajectory in the camera frame. For each loop, we take the camera-to-robot pose estimation to transform the goal end-effector pose from the camera frame to the robot base frame, and the goal joint configuration is computed via inverse kinematics. Then, a joint controller is employed to minimize the joint configuration error by taking a step toward the goal joint configuration. The camera was running at 30Hz and the joint controller was running at 120Hz. All of the communication between sub-modules was done using the Robot Operating System (ROS), and everything ran on a single computer with an Intel\u00ae Core\u2122 i9 Processor and NVIDIA\u2019s GeForce RTX 3090.\nFor qualitative analysis, we provide 2 experiment setups. At first, we fixed the goal pose at the camera center with a depth of one meter. In the second experiment, we set the goal pose following a circle centered at [0, 0, 1], with a radius of 0.15 meters, in the camera frame. For both experiments, we manually moved the camera, and the results are presented in the supplementary videos."
        },
        {
            "heading": "6.3. Evaluation on Foreground Segmentation",
            "text": "In this section, we evaluate the performance of foreground segmentation before and after the self-supervised training of the CtRNet. As the real-world datasets do not have segmentation labels, we evaluate the segmentation performance qualitatively on the DREAM-real dataset. The qualitative results are shown in Fig. 9, where we show sample segmentation masks before and after the self-supervised training, together with the original RGB images. Notably, the robot mask exhibits enhanced quality after selfsupervised training, preserving fine details of corners and boundaries. We believe the high-quality segmentation mask is the key to achieving SOTA performance on robot pose estimation, which provides close-to-ground-truth supervision for the keypoint detector."
        },
        {
            "heading": "6.4. Segmentation Accuracy Impact on Pose Estimation",
            "text": "The performance of the robot pose estimation relies on the accuracy of the foreground segmentation, as the seg-\nmentation mask offers image-level guidance during the selfsupervised training phase. In this section, we investigate the influence of segmentation accuracy on the performance of robot pose estimation. To analyze this, we carry out experiments using the Baxter dataset by training the neural network with and without the segmentation loss Lseg . We use different numbers of synthetic training samples to pre-train the neural network as described in Section 4.4. Throughout the self-supervised training phase, only the mask loss Lmask is utilized to train the keypoint detector, while re-\nfraining from fine-tuning the segmentation layers with segmentation loss Lseg .\nWe report the mean ADD and AUC ADD for robot pose estimation in Tab. 5, and include the results obtained with segmentation loss Lseg for comparative purposes. We observed that training with segmentation loss consistently yields better performance by a considerable margin, given enough pertrained samples. We also discovered a similar trend as in Section 4.4: having more pretraining samples improves the performance but the improvement is sat-\nurated after having more than 4000 training samples. However, when limited to a low number of pretraining samples (\u2264 500), CtRNet performs better without using segmentation loss because the segmentation layers are not affected by the large numbers of inaccurately detected keypoints."
        },
        {
            "heading": "6.5. Limitation",
            "text": "The keypiont detection can only detect the points in the image frame. In some cases, part of the robot body is out of the camera frustum and hence does not appear in the image frame. This will result in false positives in keypoint detection, which undermines the performance of the robot pose estimation. However, the false positives in keypoint usually result in a pose that has a large reprojection error. Therefore, the foreground segmentation would not affect by these bad samples as the weights are close to zero, according to Eq. (6). During the inference, we can also use the reprojection error to indicate the confidence of the pose estimation."
        }
    ],
    "title": "Markerless Camera-to-Robot Pose Estimation via Self-supervised Sim-to-Real Transfer",
    "year": 2023
}