{
    "abstractText": "We propose a novel approach for ASR N-best hypothesis rescoring with graph-based label propagation by leveraging cross-utterance acoustic similarity. In contrast to conventional neural language model (LM) based ASR rescoring/reranking models, our approach focuses on acoustic information and conducts the rescoring collaboratively among utterances, instead of individually. Experiments on the VCTK dataset demonstrate that our approach consistently improves ASR performance, as well as fairness across speaker groups with different accents. Our approach provides a low-cost solution for mitigating the majoritarian bias of ASR systems, without the need to train new domainor accent-specific models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Srinath Tankasala"
        },
        {
            "affiliations": [],
            "name": "Long Chen"
        },
        {
            "affiliations": [],
            "name": "Andreas Stolcke"
        },
        {
            "affiliations": [],
            "name": "Anirudh Raju"
        },
        {
            "affiliations": [],
            "name": "Qianli Deng"
        },
        {
            "affiliations": [],
            "name": "Chander Chandak"
        },
        {
            "affiliations": [],
            "name": "Aparna Khare"
        },
        {
            "affiliations": [],
            "name": "Roland Maas"
        },
        {
            "affiliations": [],
            "name": "Venkatesh Ravichandran"
        }
    ],
    "id": "SP:5d044cf9841b7726f98b4d7570bf9e76f0dea15c",
    "references": [
        {
            "authors": [
                "Richard Schwartz",
                "Steve Austin"
            ],
            "title": "A comparison of several approximate algorithms for finding multiple (n-best) sentence hypotheses",
            "venue": "Proc. IEEE ICASSP, 1991, pp. 701\u2013704.",
            "year": 1991
        },
        {
            "authors": [
                "Alex Graves"
            ],
            "title": "Sequence transduction with recurrent neural networks",
            "venue": "Proc. ICML, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "Alex Graves",
                "Abdel-rahman Mohamed",
                "Geoffrey Hinton"
            ],
            "title": "Speech recognition with deep recurrent neural networks",
            "venue": "Proc. IEEE ICASSP, 2013, pp. 6645\u20136649.",
            "year": 2013
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Martin Karafi\u00e1t",
                "Luk\u00e1\u0161s Burget",
                "Jan Honza \u010cernock\u00fd",
                "Sanjeev Khudanpur"
            ],
            "title": "Recurrent neural network based language model",
            "venue": "Proc. Interspeech, 2010, pp. 1045\u2013 1048.",
            "year": 2010
        },
        {
            "authors": [
                "Vassil Panayotov",
                "Guoguo Chen",
                "Daniel Povey",
                "Sanjeev Khudanpur"
            ],
            "title": "LibriSpeech: An ASR corpus based on public domain audio books",
            "venue": "Proc. IEEE ICASSP, 2015, pp. 5206\u2013 5210.",
            "year": 2015
        },
        {
            "authors": [
                "Mei Wang",
                "Weihong Deng",
                "Jiani Hu",
                "Xunqiang Tao",
                "Yaohai Huang"
            ],
            "title": "Racial faces in the wild: Reducing racial bias by information maximization adaptation network",
            "venue": "Proc. ICCV, 2019, pp. 692\u2013702.",
            "year": 2019
        },
        {
            "authors": [
                "Alex Beutel",
                "Jilin Chen",
                "Tulsee Doshi",
                "Hai Qian",
                "Li Wei",
                "Yi Wu",
                "Lukasz Heldt",
                "Zhe Zhao",
                "Lichan Hong",
                "Ed H. Chi",
                "Cristos Goodrow"
            ],
            "title": "Fairness in recommendation ranking through pairwise comparisons",
            "venue": "Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Min., 2019, pp. 2212\u20132220.",
            "year": 2019
        },
        {
            "authors": [
                "Pranav Dheram",
                "Murugesan Ramakrishnan",
                "Anirudh Raju",
                "I- Fan Chen",
                "Brian King",
                "Katherine Powell",
                "Melissa Saboowala",
                "Karan Shetty",
                "Andreas Stolcke"
            ],
            "title": "Toward fairness in speech recognition: Discovery and mitigation of performance disparities",
            "venue": "Proc. Interspeech, 2022, pp. 1268\u20131272.",
            "year": 2022
        },
        {
            "authors": [
                "Hua Shen",
                "Yuguang Yang",
                "Guoli Sun",
                "Ryan Langman",
                "Eunjung Han",
                "Jasha Droppo",
                "Andreas Stolcke"
            ],
            "title": "Improving fairness in speaker verification via group-adapted fusion network",
            "venue": "Proc. IEEE ICASSP, 2022, pp. 7707\u20137081.",
            "year": 2022
        },
        {
            "authors": [
                "Dengyong Zhou",
                "Olivier Bousquet",
                "Thomas Lal",
                "Jason Weston",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Learning with local and global consistency",
            "venue": "Proc. NIPS, 2003, pp. 321\u2013328.",
            "year": 2003
        },
        {
            "authors": [
                "Yan Wang",
                "Rongrong Ji",
                "Shih-Fu Chang"
            ],
            "title": "Label propagation from ImageNet to 3D point clouds",
            "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 3135\u20133142.",
            "year": 2013
        },
        {
            "authors": [
                "Bin Liu",
                "Zhirong Wu",
                "Han Hu",
                "Stephen Lin"
            ],
            "title": "Deep metric transfer for label propagation with limited annotated data",
            "venue": "Proc. IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), 2019, pp. 1317\u20131326.",
            "year": 2019
        },
        {
            "authors": [
                "Amarnag Subramanya",
                "Slav Petrov",
                "Fernando Pereira"
            ],
            "title": "Efficient graph-based semi-supervised learning of structured tagging models",
            "venue": "Proc. EMNLP, 2010, p. 167\u2013176.",
            "year": 2010
        },
        {
            "authors": [
                "Long Chen",
                "Venkatesh Ravichandran",
                "Andreas Stolcke"
            ],
            "title": "Graph-based label propagation for semi-supervised speaker identification",
            "venue": "Proc. Interspeech, 2021, pp. 4588\u20134592.",
            "year": 2021
        },
        {
            "authors": [
                "Long Chen",
                "Yixiong Meng",
                "Venkatesh Ravichandran",
                "Andreas Stolcke"
            ],
            "title": "Graph-based multi-view fusion and local adaptation: Mitigating within-household confusability for speaker identification",
            "venue": "Proc. Interspeech, 2022, pp. 4805\u20134809.",
            "year": 2022
        },
        {
            "authors": [
                "Mohammad Shokoohi-Yekta",
                "Bing Hu",
                "Hongxia Jin",
                "Jun Wang",
                "Eamonn Keogh"
            ],
            "title": "Generalizing DTW to the multidimensional case requires an adaptive approach",
            "venue": "Data mining and knowledge discovery, vol. 31, no. 1, pp. 1\u201331, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Shih-Hsuan Chiu",
                "Tien-Hong Lo",
                "Fu-An Chao",
                "Berlin Chen"
            ],
            "title": "Cross-utterance reranking models with BERT and graph convolutional networks for conversational speech recognition",
            "venue": "Proc. Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, 2021, pp. 1104\u20131110, also arXiv:2106.06922.",
            "year": 2021
        },
        {
            "authors": [
                "Hengguan Huang",
                "Fuzhao Xue",
                "Hao Wang",
                "Ye Wang"
            ],
            "title": "Deep graph random process for relational-thinking-based speech recognition",
            "venue": "Proc. ICML, 2020, vol. 119, pp. 4531\u2013 4541.",
            "year": 2020
        },
        {
            "authors": [
                "Geoffrey Zweig"
            ],
            "title": "New methods for the analysis of repeated utterances",
            "venue": "Proc. Interspeech, 2009, pp. 2791\u20132794.",
            "year": 2009
        },
        {
            "authors": [
                "William Chan",
                "Navdeep Jaitly",
                "Quoc Le",
                "Oriol Vinyals"
            ],
            "title": "Listen, Attend and Spell: A neural network for large vocabulary conversational speech recognition",
            "venue": "Proc. IEEE ICASSP, 2016, pp. 4960\u20134964.",
            "year": 2016
        },
        {
            "authors": [
                "Ankur Gandhe",
                "Ariya Rastrow"
            ],
            "title": "Audio-attention discriminative language model for ASR rescoring",
            "venue": "Proc. IEEE ICASSP, 2020, pp. 7944\u20137948.",
            "year": 2020
        },
        {
            "authors": [
                "Tara N. Sainath",
                "Ruoming Pang",
                "David Rybach",
                "Yanzhang He",
                "Rohit Prabhavalkar",
                "Wei Li",
                "Mirk\u00f3 Visonta",
                "Qiao Liang",
                "Trevor Strohman",
                "Yonghui Wu"
            ],
            "title": "Two-pass end-to-end speech recognition",
            "venue": "Proc. Interspeech, 2019, pp. 2773\u20132777.",
            "year": 2019
        },
        {
            "authors": [
                "Junichi Yamagishi",
                "Christophe Veaux",
                "Kirsten MacDonald"
            ],
            "title": "CSTR VCTK Corpus: English multi-speaker corpus for CSTR Voice Cloning Toolkit (version 0.92)",
            "venue": "University of Edinburgh. The Centre for Speech Technology Research (CSTR), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Taku Kudo",
                "John Richardson"
            ],
            "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
            "venue": "Proc. EMNLP, 2018, pp. 66\u201371.",
            "year": 2018
        },
        {
            "authors": [
                "N. Br\u00fcummer",
                "J. du Preez"
            ],
            "title": "Application-independent evaluation of speaker detection",
            "venue": "Computer Speech and Language, vol. 20, pp. 230\u2013275, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "Martin Ester",
                "Hans-Peter Kriegel",
                "J\u00f6rg Sander",
                "Xiaowei Xu"
            ],
            "title": "A density-based algorithm for discovering clusters in large spatial databases with noise",
            "venue": "Proc. KDD, 1996, p. 226\u2013231.",
            "year": 1996
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014 automatic speech recognition, hypothesis rescoring, graph-based learning, label propagation, cross-utterance"
        },
        {
            "heading": "1. INTRODUCTION",
            "text": "AI virtual assistants are used widely today, allowing customers to access a large variety of services and experiences by voice. Automatic speech recognition (ASR), which converts spoken utterances into textual form, is key to enable this human-machine interaction.\nIn a conventional ASR system, a two-pass system is employed where the first pass produces N-best hypotheses [1], and the second pass rescores/reranks them to produce the final ASR hypothesis. Conventionally, an end-to-end deep neural acoustic model, such as a recurrent neural network transducer (RNN-T) [2, 3], is used in the first pass, while a language model (LM) [4] trained on a large text dataset is employed for the rescoring stage. However, these conventional components face several challenges. First, the first-pass deep neural acoustic model is typically trained with datasets such as LibriSpeech [5] to optimize an average loss over all training samples, which usually introduces a majoritarian bias and leads to worse ASR performance for underrepresented groups (such as nonnative or regional accents, idiosyncratic pronunciations, or special domains). This fairness concern has been widely discussed in a variety of machine learning domains such as face recognition [6], recommendation systems [7], as well as ASR [8] and speaker recognition [9]. As only textual information is available to the LM, this majoritarian bias introduced due to acoustic factors, such as accents, cannot be fully addressed with LM-based rescoring. Second, the conventional rescoring system only considers a single utterance during rescoring. While some LM approaches take context into account, no acoustic information beyond the current utterance is used, thereby making it impossible to take advantage of acoustic patterns at the domain, household, or user level.\nWe propose an ASR rescoring method in which multiple utterances effectively collaborate in deciding the most likely hypotheses\n\u2020Equal contribution. This first author was an intern at Amazon.\nby leveraging cross-utterance acoustic similarity. Graph-based label propagation (graph-LP) [10] has been widely used in fields like computer vision [11, 12] and natural language processing [13], and has recently been applied to speech classification tasks such as speaker identification (SID) [14, 15]. The intuition behind graph-LP applied to speech utterance classification is to exploit pairwise similarities to ensure a consistent overall labeling of utterances. In the case of SID, this can be used to extend a partial speaker labeling of utterances to an unlabeled set, based on speaker embedding similarity. Similarly, for ASR we should be able to obtain evidence about the correctness of hypotheses by comparing utterances acoustically. If two utterances sound similar then they should have similar hypotheses, and conversely, if they sound dissimilar, their hypotheses should be too. This could help especially in the case of idiosyncratic pronunciations or accents. If two utterances contain a low-frequency phrase in their hypotheses or any word with a nonstandard and therefore low-scoring pronunciation, and they share an acoustically similar segment, then the correctness of those hypotheses is mutually consistent and therefore more likely. However, unlike for standard classification problems, directly applying graph-LP to the ASR task is nontrivial, as the label space for ASR is infinite, consisting of all strings over the vocabulary.\nTo make the problem tractable, we limit ourselves to a finite set of labels, i.e., N-best hypotheses for each utterance, and take their union across utterances as the label set. We create graphs with utterances as the nodes, and utterance-utterance similarities as the edge weights. We introduce a distance metric based on dynamic time warping (DTW) [16] to measure the utterance-utterance similarity, and apply graph-LP to predict the overall best hypotheses. We demonstrate that this approach can improve the ASR model performance and fairness, without tuning embedding or training any domain- or accent-specific adapted models. To the best of our knowledge, this is the first work utilizing utterance-utterance acoustic similarity to carry out cross-utterance ASR hypothesis rescoring.\nIn contrast to other recent work that considers the crossutterance information for ASR rescoring [17, 18, 19], which utilizes context/semantic information and assumes utterances to be from the same dialog, our approach utilizes acoustic information and can be applied to utterances from disparate contexts. There has been prior work on ASR rescoring that uses acoustic information [20, 21, 22], but these approaches deal with utterances individually. In contrast, our approach focuses on utterance-utterance acoustic similarity and uses it for joint rescoring. Our method is not replacing the existing LM-based rescoring systems (which can be applied prior to crossutterance rescoring), but provides an alternative and low-cost solution for leveraging non-local acoustic information in rescoring. Our method would most naturally be employed in offline processing of speech utterance collections, e.g., for teacher label creation in semisupervised learning.\nar X\niv :2\n30 3.\n15 13\n2v 1\n[ ee\nss .A\nS] 2\n7 M\nar 2\n02 3"
        },
        {
            "heading": "2.1. Problem setup",
            "text": "In large speech datasets, including those from AI virtual assistants, it is common for groups of utterances to have some or all of their words in common; we call these overlapped utterances and transcripts, respectively. Our goal is to take advantage of this overlap in joint rescoring of such utterance sets, using graph-LP. Given a dataset with multiple groups of overlapped utterances, we build a graph for each group, with utterances (u1, u2, . . . , uM ) corresponding to the graph nodes. For a tractable graph-LP solution, we need a label set for the graph nodes that is finite. Therefore, we use an existing ASR model to generateN -best hypotheses, giving us a total of M \u00b7 N hypotheses. Since the utterances in the graph are similar, these hypotheses may be redundant. We create a hypothesis index set H = {1, 2, 3, . . . , C} referring to the unique hypotheses across the M utterances. Each utterance ui will have a label vector yi \u2208 RC , indicating the likelihood of the possible hypotheses. The label set Y\u0302 = {y1, . . . , yM} \u2282 RC is initialized based on the ASR model\u2019s predicted confidence in each hypothesis. Let X = {x1, . . . , xM} be the acoustic embeddings of the utterances and Y\u0302 (0) = {y01 , . . . , y0M} be the initial labels of the utterances. The goal is to improve (rescore) Y\u0302 based on Y\u0302 (0) and X ."
        },
        {
            "heading": "2.2. Utterance-utterance distance modeling",
            "text": "The utterance-utterance distance metric is the key for graph-LP [14, 15]. Our goal is to improve the performance for any given ASR system without tuning or retraining the embeddings. We employ an RNN-T model to generate both utterance embeddings and hypotheses. Frame-wise outputs from the RNN-T encoder are used as the utterance embeddings. In order to model the aggregated distance over all frames, we compute the distance between two sets of frame-level embeddings, xi \u2208 RT1\u00d7D and xj \u2208 RT2\u00d7D , by using a dependent dynamic time warping (d-DTW) distance [16] with length normalization:\nd-DTWnorm(xi, xj) = min(p,q)\u2208P\n\u221a\u2211 (p,q) d(xip, xjq) 2\nmax(len(xi), len(xj)) (1)\nwhere, (p, q), p \u2208 [1, T1], q \u2208 [1, T2], is the warping path that matches time indices in xi to time indices in xj . d(xip, xjq) is the frame-wise distance function between the D-dimensional vectors xip and xjq; we use Euclidean distance in our experiments. max(len(xi), len(xj)) is the length normalization term, with len(\u00b7) giving the number of frames in an utterance. We also tested other embeddings and metrics, such as traditional DTW distance (with and without length normalization). However, the metric defined above was found to be suited best as our graph edge function, as it had a high correlation with the Levenshtein distance between the corresponding utterance transcripts, as discussed in Section 3.3."
        },
        {
            "heading": "2.3. Graph construction",
            "text": "We create a fully connected graph for each group of utterances with similar audio transcripts. For each graph, a graph node represents an utterance, and an edge connecting two nodes represents the acoustic distance of the corresponding utterances, using d-DTW. In development, we tried applying a soft radial basis function kernel to the distances as the edge weight function, similar to [14]. However, we found that binarizing the edge weights to 0/1 values gave better results. Specifically, we threshold the distances between utterances.\nThe final affinity matrix W with edge weights between nodes i, j, is defined as:\nWij = { 1 if d-DTWnorm(xi, xj) < \u0398 0 if d-DTWnorm(xi, xj) \u2265 \u0398\n(2)\nwhere d-DTWnorm(xi, xj) is the normalized dependent DTW distance and \u0398 is the threshold to determine if two utterances are close enough in the embedding space. We optimize \u0398 on a development set."
        },
        {
            "heading": "2.4. Label propagation",
            "text": "Label propagation (LP) is a transductive graph-based semisupervised learning (graph-SSL) approach where labels are propagated from \u201clabeled\u201d nodes to unlabeled nodes. LP tries to find a joint labelling Y\u0302 \u2217 for all graph nodes such that (a) Y\u0302 \u2217 is close to Y\u0302 (0); and (b) the labels are smooth over the graph, i.e., they do not differ drastically between neighbours. This is typically done by optimizing the following objective function:\nY\u0302 \u2217 = argmin f ||f \u2212 Y ||22 + \u03bb \u00b7 trace(fTLsymf) (3)\nwhere Y is the input of known labels, f is the labeling solution and \u03bb is a regularization hyperparameter. Lsym is the symmetric normalized Laplacian graph matrix, i.e., Lsym = I\u2212\u2206\u22121/2W\u2206\u22121/2, where \u2206 is the degree diagonal matrix with \u2206ii = \u2211M j=1Wij . To solve Equation (3), an iterative algorithm by Zhou et al. [10] is used, as follows:\nAlgorithm 1 Label propagation 1: Compute the affinity matrix W if i 6= j & Wii = 0; 2: Compute matrix S = \u2206\u22121/2W\u2206\u22121/2\n3: Initialize Y\u0302 (0) with each row (Y\u0302 (0))i = yi, where yi is a soft label vector for utterance i (see Section 2.5) 4: Iterate Y\u0302 (t+1) = \u03b1SY\u0302 (t) + (1 \u2212 \u03b1)Y\u0302 (0) until convergence, where \u03b1 \u2208 (0, 1)\n5: Label each point ui with yi = argmax j\u2264C\nY\u0302 (\u221e) ij"
        },
        {
            "heading": "2.5. Graph-LP for cross-utterance ASR rescoring",
            "text": "Graph-LP relies on an initial label matrix Y\u0302 (0). Typically in graphSSL work [14, 15], ground truth or \u201clabeled\u201d samples are included in the graph with hard (i.e., one-hot) initialized labels, to serve as the \u201cseeds\u201d for propagating information to unlabeled samples. In our scenario, there is no ground truth. Instead, we initialize the label vector for all utterances with soft labels over the hypothesis set H. To do this we use the log likelihood scores of hypotheses as computed by the RNN-T model. Assume for a given utterance ui the model predicts the hypotheses {h1, . . . , hB} with log likelihoods {s1, . . . , sB}, where B is the beam size (B \u2265 N ). For each hypothesis k, we compute the score pk as\npk = softmax(sk) = esj\u2211B\nk=1 e sk\n(4)\nThese probabilities pk corresponding to the top N hypotheses are used as the soft labels yi \u2208 RC for utterance ui, such that yi > 0, ||yi||1 \u2264 1. We generate Y\u0302 (0) by computing yi for all utterances ui in the graph. Algorithm 1 in Section 2.4 is then applied with Y\u0302 (0) as initialization."
        },
        {
            "heading": "3.1. Datasets",
            "text": "We use the LibriSpeech [5] training dataset to train the ASR RNN-T model for embedding and hypothesis generation. Evaluation is based on the VCTK [23] dataset. We further divide the VCTK utterances into development and test sets with a ratio of 1:2. The development set is used for metric and hyperparameter selection, while the test set is used for reporting ASR performance. LibriSpeech is commonly used for ASR tasks in the literature, with the majority of the speech coming from American English speakers reading audio books. The VCTK dataset is a popular dataset for accent studies, with English sentences sourced from newspapers read-out by speakers from 13 English-speaking regions. We chose these two datasets since they are mismatched in both domain and accents. We did not tune or adapt the ASR model to the VCTK data, to evaluate the efficacy of our proposed approach in improving the ASR model trained on outof-domain data."
        },
        {
            "heading": "3.2. Baseline and embedding generation model",
            "text": "The baseline RNN-T ASR model uses a six-layer LSTM encoder with a hidden dimension of 1024, and a transcription network with two 1024-dimensional LSTM layers. We use a sentence-piece model [24] to generate output targets for the ASR model. The model was trained on the LibriSpeech dataset and has a word error rate (WER) of 6.05% and 15.43% on LibriSpeech-Clean and LibriSpeech-Other test sets, respectively. We evaluate the model on the VCTK dataset and use that as the baseline for comparing with the proposed graphLP method, using both WER and sentence error rate (SER). Additionally, we focus on overall model performance as well as performance on different accent groups to test whether the proposed method can improve model performance and fairness.\nThe baseline RNN-T model is also used to generate the inputs for the graph-LP algorithm. The embeddings computed by the final RNN-T encoder layer are used for utterance-utterance distance calculation, as described in Section 2.2."
        },
        {
            "heading": "3.3. Metric selection for utterance-utterance distance",
            "text": "A good utterance-utterance distance function used for graph-LP needs to satisfy the following property: For any pair of utterances i, j in the graph, the distance in the embedding space should reflect the distant between the corresponding ground-truth transcripts, e.g., embedding distance should be highly correlated with the Levenshtein distance between transcripts.\nThe above property ensures that the distance function serves the ASR task, rather than measuring similarity along other dimensions, such as speaker ID or acoustic environment. To quantify this property, we borrow the concept of equal error rate (EER) used for metric learning and verification tasks [25]. We create trials of utterance pairs from the development set with 10,000 positive and 50,000 negative pairs, where positive/negative pairs correspond to utterances having the same/different ground-truth transcripts. The utteranceutterance distance is calculated for each pair. We then find the threshold at which false accept rate (FAR) and false reject rate (FRR) are equalized, giving us EER = FAR = FRR. We also use t-SNE plots to visualize utterance similarities.\nWe consider two groups of candidate methods for the utteranceutterance distance function, as well as variants with and without length normalization:\n\u2022 Euclidean distance between the last frame embeddings emitted by the RNN-T audio encoder\n\u2022 Traditional DTW or dependent DTW (d-DTW ) distance between RNN-T audio encoder embeddings of all frames\nThe rationale behind this choice is as follows: (1) for RNN-based models, the last frame embedding from the output layer in principle could encapsulate the information of the whole audio; (2) the DTWbased distance function evaluates the time-warped distance between a given pair of sequences, intuitively reflecting the accumulated distance over all frames; (3) length normalization allows more consistent distance thresholding across different utterance lengths.\nUsing the above candidate distance functions, we compute the EER for all the methods. The EER value is used to select the utterance-utterance distance metrics. The evaluation results for the distance functions described above are reported in Section 4.2."
        },
        {
            "heading": "3.4. Graph-LP experiments",
            "text": "As described in Section 2.3, we aim to construct graphs by pooling utterances with similar transcripts. However, given that ASR is the task, we do not have prior access to the ground truth transcripts for the test utterances. Instead, we pool the utterances based on their baseline ASR hypotheses. To make the label propagation method scalable, we only group utterances with similar hypotheses into one graph. First, the tf-idf embeddings of all utterances are generated using the ASR 1-best hypotheses. We then use the DBSCAN algorithm [26] to identify utterance clusters and build a graph from all the utterances in one cluster. Ideally, we want the sizes of generated clusters to be within a suitable range. Too many utterances would result in large graphs with many nodes having low hypothesis overlap. If the cluster size is too small, there may be not be enough information added by considering multiple utterances in joint rescoring. We tuned the parameters of the DBSCAN algorithm on the development set to maximize the number of clusters with sizes in the range 4 to 800. Utterances that cannot be clustered are not included in graphLP and their hypotheses are left unchanged.\nGiven an utterance cluster, we select the top N = 3 hypotheses for each utterance to construct the label setH. The label confidences are calculated using the method described in Section 2.5 to generate the initial labelings Y\u0302 (0). In the graph, we want label information to flow strongly between similar utterances. Hence, we calculate Wij using Equation (2). To reduce computation further, we remove connections between nodes where the minimum word edit distance between the top 3 hypotheses is > 4. Graph-LP is then applied to generate the final labels for all the nodes. We allow label sharing, i.e., the final label for an utterance can be outside its initial N -best hypothesis set, potentially improving results."
        },
        {
            "heading": "4. RESULTS",
            "text": ""
        },
        {
            "heading": "4.1. Baseline model results",
            "text": "Performance of the baseline RNN-T model on the VCTK dataset is shown in Table 1. We show word error rates by speaker accents. Here, WER-5best is the oracle WER of the 5-best hypotheses. We can see a significant difference between performance on American/Canadian compared to English, Scottish, and other regional accents, attributable to the LibriSpeech training dataset consisting mainly of American English speech. There is also a significant performance gap between the 1-best WER and 5-best WER (13.98% \u2192 7.89%), showing the potential for improvement with hypothesis rescoring."
        },
        {
            "heading": "4.2. EER and metric selection results",
            "text": "Figure 1 visualizes a sample of VCTK utterances using t-SNE, based on various distance metrics. It clearly shows the clustering of similar utterances in the label space when using d-DTW distance based on all frame embeddings. We also observe that clusters that have more transcript overlap are closer together (e.g., blue versus red samples). This is not the case when using a distances based only on the last-frame RNN-T encoder embeddings, for which no clustering of utterances with identical audio transcripts is observed. From the visualization, we infer that the last-frame embedding distance is an unsuitable metric for constructing our graphs. Table 2 shows the EERs for same-ground-truth classification with several alternative distance metrics, with and without length normalization. Length-normalized d-DTW achieves the lowest EER; it is used in all graph-LP results reported here. The devtest-optimized distance threshold in Equation (2) is \u0398 = 1.5, leaving about 47% of edges remaining."
        },
        {
            "heading": "4.3. Graph-LP rescoring results",
            "text": "Table 3 shows results for graph-LP-based cross-utterance rescoring as described in Section 3.4. We observe significant improvements in WER across cluster sizes, with an overall improvement of 43.5% for WER and 40.5% for SER, respectively. Clusters of larger size seem to show a bigger performance gain. This is consistent with\nthe notion that the more related utterances are involved in graphLP, the more additional information can be aggregated, compared to single-utterance recognition. Moreover, as shown in Table 4, sharing labels (hypotheses) among all utterances in the same cluster gives a substantial benefit, reducing WER by 35.5% and SER by 28.8%. Label sharing effectively recovers plausible hypotheses left out of the original N-best lists, and graph-LP allows evaluating them even though they do not have a likelihood based on the first-pass ASR.\nTable 5 shows ASR results for different accent groups. (Note that Tables 3 and 5 are based on the test set only, rather than all of VCTK as in Table 1.) WER and SER across all accent groups are improved. Moreover, accent groups other than American/Canadian show larger improvements, leading to a much smaller gap between the high and low performance groups. These results demonstrate that the proposed approach is effective at mitigating the majoritarian bias of the original ASR system, improving both accuracy and fairness."
        },
        {
            "heading": "5. CONCLUSIONS",
            "text": "We have proposed a cross-utterance ASR hypothesis rescoring approach based on graph-based label propagation (graph-LP). Our approach improves ASR performance by leveraging (1) cross-utterance information, especially acoustic similarity, modeled by a DTWbased distance metric and (2) joint cross-utterance rescoring enabled by graph-LP and a shared hypothesis set among utterances. The approach is designed to help ASR systems adapt to idiosyncratic pronunciations, accents, or out-of-domain content. Experiments on the VCTK dataset demonstrate that the proposed approach consistently improves overall error rates, as well as for speaker groups with specific accents. Our method is well-suited to offline ASR settings, without requiring adaptation or fine-tuning of the baseline model.\n[1] Richard Schwartz and Steve Austin, \u201cA comparison of several approximate algorithms for finding multiple (n-best) sentence hypotheses,\u201d in Proc. IEEE ICASSP, 1991, pp. 701\u2013704.\n[2] Alex Graves, \u201cSequence transduction with recurrent neural networks,\u201d in Proc. ICML, 2012.\n[3] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in Proc. IEEE ICASSP, 2013, pp. 6645\u20136649.\n[4] Tomas Mikolov, Martin Karafia\u0301t, Luka\u0301s\u030cs Burget, Jan Honza C\u030cernocky\u0301, and Sanjeev Khudanpur, \u201cRecurrent neural network based language model,\u201d in Proc. Interspeech, 2010, pp. 1045\u2013 1048.\n[5] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur, \u201cLibriSpeech: An ASR corpus based on public domain audio books,\u201d in Proc. IEEE ICASSP, 2015, pp. 5206\u2013 5210.\n[6] Mei Wang, Weihong Deng, Jiani Hu, Xunqiang Tao, and Yaohai Huang, \u201cRacial faces in the wild: Reducing racial bias by information maximization adaptation network,\u201d in Proc. ICCV, 2019, pp. 692\u2013702.\n[7] Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Li Wei, Yi Wu, Lukasz Heldt, Zhe Zhao, Lichan Hong, Ed H. Chi, and Cristos Goodrow, \u201cFairness in recommendation ranking through pairwise comparisons,\u201d in Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Min., 2019, pp. 2212\u20132220.\n[8] Pranav Dheram, Murugesan Ramakrishnan, Anirudh Raju, IFan Chen, Brian King, Katherine Powell, Melissa Saboowala, Karan Shetty, and Andreas Stolcke, \u201cToward fairness in speech recognition: Discovery and mitigation of performance disparities,\u201d in Proc. Interspeech, 2022, pp. 1268\u20131272.\n[9] Hua Shen, Yuguang Yang, Guoli Sun, Ryan Langman, Eunjung Han, Jasha Droppo, and Andreas Stolcke, \u201cImproving fairness in speaker verification via group-adapted fusion network,\u201d in Proc. IEEE ICASSP, 2022, pp. 7707\u20137081.\n[10] Dengyong Zhou, Olivier Bousquet, Thomas Lal, Jason Weston, and Bernhard Scho\u0308lkopf, \u201cLearning with local and global consistency,\u201d in Proc. NIPS, 2003, pp. 321\u2013328.\n[11] Yan Wang, Rongrong Ji, and Shih-Fu Chang, \u201cLabel propagation from ImageNet to 3D point clouds,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 3135\u20133142.\n[12] Bin Liu, Zhirong Wu, Han Hu, and Stephen Lin, \u201cDeep metric transfer for label propagation with limited annotated data,\u201d in Proc. IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), 2019, pp. 1317\u20131326.\n[13] Amarnag Subramanya, Slav Petrov, and Fernando Pereira, \u201cEfficient graph-based semi-supervised learning of structured tagging models,\u201d in Proc. EMNLP, 2010, p. 167\u2013176.\n[14] Long Chen, Venkatesh Ravichandran, and Andreas Stolcke, \u201cGraph-based label propagation for semi-supervised speaker identification,\u201d in Proc. Interspeech, 2021, pp. 4588\u20134592.\n[15] Long Chen, Yixiong Meng, Venkatesh Ravichandran, and Andreas Stolcke, \u201cGraph-based multi-view fusion and local adaptation: Mitigating within-household confusability for speaker identification,\u201d in Proc. Interspeech, 2022, pp. 4805\u20134809.\n[16] Mohammad Shokoohi-Yekta, Bing Hu, Hongxia Jin, Jun Wang, and Eamonn Keogh, \u201cGeneralizing DTW to the multidimensional case requires an adaptive approach,\u201d Data mining and knowledge discovery, vol. 31, no. 1, pp. 1\u201331, 2017.\n[17] Shih-Hsuan Chiu, Tien-Hong Lo, Fu-An Chao, and Berlin Chen, \u201cCross-utterance reranking models with BERT and graph convolutional networks for conversational speech recognition,\u201d in Proc. Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, 2021, pp. 1104\u20131110, also arXiv:2106.06922.\n[18] Hengguan Huang, Fuzhao Xue, Hao Wang, and Ye Wang, \u201cDeep graph random process for relational-thinking-based speech recognition,\u201d in Proc. ICML, 2020, vol. 119, pp. 4531\u2013 4541.\n[19] Geoffrey Zweig, \u201cNew methods for the analysis of repeated utterances,\u201d in Proc. Interspeech, 2009, pp. 2791\u20132794.\n[20] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals, \u201cListen, Attend and Spell: A neural network for large vocabulary conversational speech recognition,\u201d in Proc. IEEE ICASSP, 2016, pp. 4960\u20134964.\n[21] Ankur Gandhe and Ariya Rastrow, \u201cAudio-attention discriminative language model for ASR rescoring,\u201d in Proc. IEEE ICASSP, 2020, pp. 7944\u20137948.\n[22] Tara N. Sainath, Ruoming Pang, David Rybach, Yanzhang He, Rohit Prabhavalkar, Wei Li, Mirko\u0301 Visonta, Qiao Liang, Trevor Strohman, and Yonghui Wu, \u201cTwo-pass end-to-end speech recognition,\u201d in Proc. Interspeech, 2019, pp. 2773\u20132777.\n[23] Junichi Yamagishi, Christophe Veaux, and Kirsten MacDonald, \u201cCSTR VCTK Corpus: English multi-speaker corpus for CSTR Voice Cloning Toolkit (version 0.92),\u201d University of Edinburgh. The Centre for Speech Technology Research (CSTR), 2019.\n[24] Taku Kudo and John Richardson, \u201cSentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,\u201d in Proc. EMNLP, 2018, pp. 66\u201371.\n[25] N. Bru\u0308ummer and J. du Preez, \u201cApplication-independent evaluation of speaker detection,\u201d Computer Speech and Language, vol. 20, pp. 230\u2013275, 2006.\n[26] Martin Ester, Hans-Peter Kriegel, Jo\u0308rg Sander, and Xiaowei Xu, \u201cA density-based algorithm for discovering clusters in large spatial databases with noise,\u201d in Proc. KDD, 1996, p. 226\u2013231."
        }
    ],
    "title": "CROSS-UTTERANCE ASR RESCORING WITH GRAPH-BASED LABEL PROPAGATION",
    "year": 2023
}