{
    "abstractText": "Games are usually created incrementally, requiring repeated testing of the same scenarios, which is a tedious and error-prone task for game developers. Therefore, we aim to alleviate this game testing process by encapsulating it into a game called Playtest, which transforms the tiring testing process into a competitive game with a purpose. Playtest automates the generation of valuable test cases based on player actions, without the players even realising it. We envision the use of Playtest to crowdsource the task of testing games by giving players access to the respective games through our tool in the playtesting phases during the development process.",
    "authors": [
        {
            "affiliations": [],
            "name": "Patric Feldmeier"
        },
        {
            "affiliations": [],
            "name": "Philipp Straubinger"
        },
        {
            "affiliations": [],
            "name": "Gordon Fraser"
        }
    ],
    "id": "SP:5170a0b921073290b21e0b211af121682059d9ca",
    "references": [
        {
            "authors": [],
            "title": "Video Game Automated Testing Approaches: An Assessment Framework",
            "venue": "IEEE Transactions on Games (2020)",
            "year": 2020
        },
        {
            "authors": [
                "Saeed Amiri-Chimeh",
                "Hassan Haghighi",
                "Mojtaba Vahidi-Asl",
                "Kamyar Setayesh- Ghajar",
                "Farshad Gholami-Ghavamabad"
            ],
            "title": "Rings: A Game with a Purpose for Test Data Generation",
            "venue": "Interact. Comput. 30,",
            "year": 2018
        },
        {
            "authors": [
                "Carlos Futino Barreto",
                "C\u00e9sar Fran\u00e7a"
            ],
            "title": "Gamification in Software Engineering: A literature Review",
            "venue": "In 14th IEEE/ACM International Workshop on Cooperative and Human Aspects of Software Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "Daniel de Paula Porto",
                "Gabriela Martins de Jesus",
                "Fabiano Cutigi Ferrari"
            ],
            "title": "Initiatives and challenges of using gamification in software engineering: A Systematic Mapping",
            "venue": "J. Syst. Softw",
            "year": 2021
        },
        {
            "authors": [
                "Adina Deiner",
                "Patric Feldmeier",
                "Gordon Fraser",
                "Sebastian Schweikl",
                "Wengran Wang"
            ],
            "title": "Automated Test Generation for Scratch Programs",
            "venue": "Empirical Software Engineering 28,",
            "year": 2023
        },
        {
            "authors": [],
            "title": "Methods for Game User Research: Studying Player Behavior to Enhance Game Design",
            "venue": "IEEE Computer Graphics Applications 33,",
            "year": 2013
        },
        {
            "authors": [
                "Sebastian Deterding",
                "Dan Dixon",
                "Rilla Khaled",
                "Lennart E. Nacke"
            ],
            "title": "From game design elements to gamefulness: defining \"gamification",
            "venue": "In Proceedings of the 15th International Academic MindTrek Conference: Envisioning Future Media Environments,",
            "year": 2011
        },
        {
            "authors": [
                "Patric Feldmeier",
                "Gordon Fraser"
            ],
            "title": "Learning by Viewing: Generating Test Inputs for Games by Integrating Human Gameplay Traces in Neuroevolution",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Patric Feldmeier",
                "Gordon Fraser"
            ],
            "title": "Neuroevolution-Based Generation of Tests and Oracles for Games",
            "venue": "In Proceedings of the International Conference on Automated Software Engineering (ASE\u201922). ACM,",
            "year": 2023
        },
        {
            "authors": [
                "Jeff Howe"
            ],
            "title": "The rise of crowdsourcing",
            "venue": "Wired magazine 14,",
            "year": 2006
        },
        {
            "authors": [
                "Ethan C. Jackson",
                "Mark Daley"
            ],
            "title": "Novelty Search for Deep Reinforcement Learning Policy Network Weights by Action Sequence Edit Metric Distance",
            "venue": "In Proceedings of the Genetic and Evolutionary Computation Conference Companion (GECCO\u201919)",
            "year": 2019
        },
        {
            "authors": [
                "Jinhan Kim",
                "Robert Feldt",
                "Shin Yoo"
            ],
            "title": "Guiding Deep Learning System Testing Using Surprise Adequacy",
            "venue": "In Proceedings of the 41st International Conference on Software Engineering",
            "year": 2019
        },
        {
            "authors": [
                "Vladimir Levenshtein"
            ],
            "title": "Binary Codes Capable of Correcting Deletions, Insertions, and Reversals",
            "year": 1966
        },
        {
            "authors": [
                "Dayi Lin",
                "Cor-Paul Bezemer",
                "Ahmed E. Hassan"
            ],
            "title": "An Empirical Study of Early Access Games on the Steam Platform",
            "venue": "Empirical Software Engineering 23,",
            "year": 2018
        },
        {
            "authors": [
                "John Maloney",
                "Mitchel Resnick",
                "Natalie Rusk",
                "Brian Silverman",
                "Evelyn Eastmond"
            ],
            "title": "The Scratch Programming Language and Environment",
            "venue": "Transactions on Computing Education 10,",
            "year": 2010
        },
        {
            "authors": [
                "Ke Mao",
                "Licia Capra",
                "Mark Harman",
                "Yue Jia"
            ],
            "title": "A Survey of The Use of Crowdsourcing in Software Engineering",
            "venue": "Journal of Systems and Software",
            "year": 2017
        },
        {
            "authors": [
                "Pejman Mirza-Babaei",
                "Naeem Moosajee",
                "Brandon Drenikow"
            ],
            "title": "Playtesting for Indie Studios",
            "venue": "In Proceedings of the 20th International Academic Mindtrek Conference (AcademicMindtrek\u201916). ACM, 366\u2013374",
            "year": 2016
        },
        {
            "authors": [
                "Sharmin Moosavi",
                "Hassan Haghighi",
                "Hasti Sahabi",
                "Farzam Vatanzade",
                "Mojtaba Vahidi-Asl"
            ],
            "title": "Greenify: A Game with the Purpose of Test Data Generation for Unit Testing",
            "venue": "In Fundamentals of Software Engineering - 8th International Conference, FSEN 2019,",
            "year": 2019
        },
        {
            "authors": [
                "A Jefferson Offutt",
                "Ammei Lee",
                "Gregg Rothermel",
                "Roland H Untch",
                "Christian Zapf"
            ],
            "title": "An Experimental Determination of Sufficient Mutant Operators",
            "venue": "Transactions on Software Engineering and Methodology (TOSEM) 5,",
            "year": 1996
        },
        {
            "authors": [
                "Erik Pasternak",
                "Rachel Fenichel",
                "Andrew N Marshall"
            ],
            "title": "Tips for Creating a Block Language with Blockly",
            "venue": "In Blocks and Beyond Workshop (B&B). IEEE,",
            "year": 2017
        },
        {
            "authors": [
                "Cristiano Politowski",
                "Fabio Petrillo",
                "Yann-Ga\u00ebl Gu\u00e9h\u00e9neuc"
            ],
            "title": "A Survey of Video Game Testing",
            "venue": "In Proceedings of the IEEE/ACM International Conference on Automation of Software Test (AST\u201921)",
            "year": 2021
        },
        {
            "authors": [
                "Kenneth O Stanley",
                "Risto Miikkulainen"
            ],
            "title": "Evolving Neural Networks Through Augmenting Topologies",
            "venue": "Evolutionary Computation 10,",
            "year": 2002
        },
        {
            "authors": [
                "Klaas-Jan Stol",
                "Mario Schaarschmidt",
                "Shelly Goldblit"
            ],
            "title": "Gamification in software engineering: the mediating role of developer engagement and job satisfaction",
            "venue": "Empir. Softw. Eng. 27,",
            "year": 2022
        },
        {
            "authors": [
                "Kathryn T. Stolee",
                "Sebastian Elbaum"
            ],
            "title": "Exploring the Use of Crowdsourcing to Support Empirical Studies in Software Engineering",
            "venue": "In Proceedings of the International Symposium on Empirical Software Engineering and Measurement (ESEM\u201910)",
            "year": 2010
        },
        {
            "authors": [
                "Philipp Straubinger",
                "Laura Caspari",
                "Gordon Fraser"
            ],
            "title": "Code Critters: A Block-Based Testing Game",
            "venue": "In IEEE International Conference on Software Testing, Verification and Validation,",
            "year": 2023
        },
        {
            "authors": [
                "Luis Von Ahn",
                "Laura Dabbish"
            ],
            "title": "ESP: Labeling Images with a Computer Game",
            "venue": "In AAAI spring symposium: Knowledge collection from volunteer contributors,",
            "year": 2005
        },
        {
            "authors": [
                "Luis von Ahn",
                "Laura Dabbish"
            ],
            "title": "Designing games with a purpose",
            "venue": "Commun. ACM 51,",
            "year": 2008
        }
    ],
    "sections": [
        {
            "text": "CCS CONCEPTS \u2022 Software and its engineering\u2192 Software testing and debugging; \u2022 Applied computing\u2192 Computer games.\nKEYWORDS Gamification, Game Testing, Games with a Purpose\nACM Reference Format: Patric Feldmeier, Philipp Straubinger, and Gordon Fraser. 2023. PlayTest: A Gamified Test Generator for Games. In Proceedings of the 2nd International Workshop on Gamification in Software Development, Verification, and Validation (Gamify \u201923), December 4, 2023, San Francisco, CA, USA. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3617553.3617884"
        },
        {
            "heading": "1 MOTIVATION",
            "text": "Since its inception, the video game industry has continuously grown, reaching an approximated record revenue of $365.60 billion in 2023 and an expected annual growth rate of 6.52% from 2023 to 2027 [25]. To stand a chance in this emerging market, developers must ensure great gaming experiences by minimising the presence of bugs using extensive testing procedures. Due to the high degree of randomisation inherent to most games, testing games using conventional static test cases consisting of fixed test inputs and test oracles is challenging because these tests are not suited to adapt to changes in program behaviour. Thus, game companies nowadays spend enormous amounts of human resources on testing games manually [2, 23], resulting in the tedious and error-prone task of testing the same game scenarios over and over again.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Gamify \u201923, December 4, 2023, San Francisco, CA, USA \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0373-7/23/12. . . $15.00 https://doi.org/10.1145/3617553.3617884\nPreviously, neuroevolution was used to tackle the problem of testing games by generating dynamic test cases capable of adapting to changes in program behaviour [11]. These dynamic test cases consist of neural networks trained to generate test inputs that are able to reach targeted program scenarios reliably, regardless of the encountered and often randomised program behaviour. For instance, Neatest [11] employs the Neat [24] algorithm to generate dynamic test cases by simultaneously optimising the architecture and weights of neural networks. However, due to the co-evolutionary approach, generating dynamic tests may involve unreasonable long training durations. Thus, the training speed of Neatest has been improved by optimising weights via backpropagation using human gameplay traces as a ground truth dataset [10]. Intentional recording of these traces results in incomplete ground truth data, as players typically aim to win in games, leading to many game traces involving mastery and a lack of traces capturing poor play. As test oracles, Neatest uses the Surprise Adequacy [14] metric that measures how surprised networks are from an encountered program scenario. While this allows Neatest to distinguish between randomised and buggy program behaviour, it also restricts the tool to be only applicable as a regression testing approach.\nWe aim to collect a wide range of gameplay traces and test oracles by hiding the tedious process of recording human traces and generating meaningful assertions behind gamification elements [4, 9]. Gamification has proven to be effective in motivating individuals to complete tedious tasks and improving overall outcomes [6, 26]. Specifically, we propose the concept of Playtest, a game with a purpose [30], where human gameplay traces and traditional static test cases are generated while players interact with a multiplayer version of games to test. Previously, games with a purpose were successfully employed in various domains, such as image labelling [29], protein structure prediction [5], and test data generation [3, 20].\nAs depicted in Fig. 1, Playtest consists of two modules, Play and Test, which separate the game aspect presented to the player from the actual purpose of generating valuable tests from extracted gameplay traces and assertions. This separation ensures that players enjoy interacting with Playtest by hiding the test generation\nar X\niv :2\n31 0.\n19 40\n2v 1\n[ cs\n.S E\n] 3\n0 O\nct 2\n02 3\nprocess from the player. The game encapsulated in the Play module is designed as a player-vs-player game, where two players compete to identify as many program mutants as possible. To achieve this, players actively play the game under test, generating various gameplay traces to explore different program scenarios. Based on these recorded playthroughs, players have to create assertions to defend themselves against automatically generated mutants of the game. In order to ensure accessibility for all players, regardless of their programming knowledge, we facilitate the creation of these assertions by utilising a block-based programming approach [17, 22, 28].\nThe Test module operates behind the scenes and is responsible for generating static and dynamic tests. To this end, Playtest extracts the players\u2019 recorded gameplay traces and assertions through crowdsourcing [18, 27], which refers to the practice of collecting information or input for a task from a large group of individuals, often through online platforms [12]. Since Playtest incentivises players to play the underlying game in many different ways to detect lots of program mutants, we expect the resulting game traces to be better suited to train a wide range of differently behaving dynamic tests than traces that were recorded intentionally. Furthermore, we improve Neatest\u2019s test oracle by incorporating human-made assertions, which removes the limitation of regression testing and improves bug detection. Please note that even though static tests are often inferior to dynamic tests, we nevertheless synthesise them since they may prove beneficial in games that are not randomised.\nWe intend to apply Playtest in a crowdsourcing manner to facilitate the testing procedure of games during the game development process. By giving players access to games infused with Playtest during early access [16] and playtest [8, 19] phases, players can engage with single-player games in novel ways while also assisting developers with human gameplay traces. Our contributions include the proposal of the Playtest concept, a novel game with a purpose that gamifies the generation of valuable tests for games through crowdsourcing. Moreover, we outline the planned evaluation of Playtest, which involves the individual assessment of the Play and Test module. Finally, based on the obtained human game traces and assertions, we plan to advance the field of automatic test generation for games by generating dynamic tests trained to mimic a wide range of player behaviours, enabling them to reach more diverse program scenarios and detect more bugs."
        },
        {
            "heading": "2 PLAYTEST",
            "text": "Playtest is a game with a purpose since it hides the purpose of generating tests for games behind engaging gameplay. As shown in Fig. 1, Playtest consists of the two name-giving modules, Play and Test, which host the game logic and the underlying purpose of generating tests, respectively.\nThe Play module involves two phases in which two players compete against each other to survive for as long as possible by detecting mutated program versions. In the Planning Phase (Section 2.1), both players generate game traces by playing the game for a limited amount of time. Based on the resulting traces, each player earns action points, which serve as a currency for strategic updates and for purchasing assertions to detect mutated program versions. After a timer has run out or both players have agreed that\nthey do not intend to perform further actions, the Planning Phase ends and transitions to the Execution Phase.\nDuring the Execution Phase (Section 2.2), Playtest evaluates the players\u2019 actions by executing the saved gameplay traces together with the placed assertions on several automatically generated program mutants. If the placed assertions fail to detect a mutant, the players receive a penalty in the form of a reduction in their overall life points. After assessing the placed assertions, the players are encouraged to examine undetected mutants and adjust their strategy for the subsequent Planning Phase. These two phases continue to alternate until one player\u2019s life points reach zero, at which point the surviving player with remaining life points is deemed the winner. Additionally, after each Execution Phase, the players are given slightly more time for their playthroughs during the next Planning Phase to allow the discovery of advanced program statements and provide new opportunities for placing assertions.\nThe Test module in Playtest is responsible for generating both conventional static and adaptive dynamic test cases (Section 2.3). This module operates in the background, continuously recording the gameplay traces and the corresponding assertions placed by the players. These recorded gameplay traces serve as inputs for generating conventional static test cases. Additionally, the collected traces can be used to synthesise dynamic test cases that are resilient to randomised program behaviour [10, 11].\nPlaytest operates based on two fundamental principles: Abstracting the Purpose and Correlating Success with Valuable Test Cases. The first principle ensures that the players are unaware of the underlying purpose of generating tests for games. The game itself should be enjoyable, and the motivation to play should not solely stem from the purpose of generating tests. This principle is essential to maintain a long-term engagement with the game, as playing it solely for its test generation purpose may not be motivating enough. The second principle, Correlating Success with Valuable Test Cases, states that successful gameplay should result in valuable static and dynamic test cases. However, in line with the Abstracting the Purpose principle, the players should never be burdened directly with the tedious task of creating test cases.\nIn the following sections, the different modules and phases of Playtestwill be further explained using the open-source SuperTux1 game as an example. However, our approach generalises to any other game, regardless of the game genre."
        },
        {
            "heading": "2.1 Planning Phase",
            "text": "During the Planning Phase, players execute actions to defend themselves against generated mutants using various strategic options available from the user interface shown in Fig. 2. The clock indicates how much time both players have left until the Planning Phase transitions into the Execution Phase. Below the clock resides the player\u2019s avatar , together with two bars representing the player\u2019s remaining life points and action points, respectively.\nBeneath the avatar resides a game clip on which the players can click once during each Planning Phase in order to record a single playthrough by playing the game for a limited duration. This duration in seconds is defined by the number next to the hourglass symbol depicted in the players\u2019 attributes bar below the\n1August 2023: https://www.supertux.org\ngame clip. The duration for a single playthrough increases after each gameplay cycle, involving one Planning and one Execution Phase. The recorded playthroughs may be accessed at any time by clicking on the robot icons. These recorded playthroughs serve two main purposes in the gameplay. First, Playtest extracts all performed actions during the playthroughs and uses them to reproduce the execution trace of the underlying playthrough. Based on these execution traces, players can place assertions in order to defend themselves against program mutations. Thus, players can only validate areas of the program they have captured during their recorded playthrough. For example, testing if the game ends when the player touches an enemy requires creating a gameplay trace that demonstrates this behaviour, while testing advanced program states requires meaningful gameplay. Second, in addition to receiving a default amount of action points after each Execution Phase, players also earn additional action points based on the total program coverage achieved across all gameplay traces. The design of recorded playthroughs follows the Correlating Success with Valuable Test Cases principle as they encourage diverse gameplay, which leads to test inputs that can evaluate different aspects of the game. Among other things, earned action points may be used to purchase assertions needed to detect program mutations.\nPlayers can access the user interface shown in Fig. 3 by clicking on one of the robot symbols . This interface allows players to place assertions during their recorded gameplay. Additionally, a time-lapse bar located at the bottom of the game clip enables players to fast-forward or rewind their recorded gameplay. This feature allows players to analyse their execution traces and place assertions at specific points in time. In addition to setting time-sensitive assertions, players can also place global assertions that must be satisfied during the entire program execution.\nFig. 3 depicts three assertions that were already generated by the player. While the first two assertions test whether the game stops\nif the player (Penguin) touches a bomb or falls down a hole (\ud835\udc66 < 0), the last assertion validates whether the players\u2019 score increases after collecting coins. The process of generating assertions aligns with the principle of Abstracting the Purpose since we use a block-based programming approach similar to Code Critters [28] to retain players from having to mingle with code. In this approach, essential aspects of the game are represented by differently coloured blocks.\n\u2022 Grey: Pre-defined block constructs governing the remaining statement structure, such as If ... Then. \u2022 Darkblue: Actors of the game like the player. \u2022 Lightblue: Attributes of preceding actors, such as their position in x and y coordinates. \u2022 Green: Operators linking two operands, such as universal operators (<) or pre-defined operators (touching). \u2022 Purple: Values as numbers or strings that may be inserted by the player in a text field.\n\u2022 Orange: Outcomes that must be satisfied after Then blocks. Players implement assertions by combining If conditions with desired outcomes using a toolbox of blocks similar to the Scratch [17] programming environment. Block arguments may be changed by clicking on the triangle icon, which opens a drop-down menu offering various choices for the respective block. For instance, players can select other actors discovered during the playthrough via the menu of an actor block. In order to encourage players to create effective assertions, grey block constructs required for generating new assertions must be purchased using precious action points.\nBesides purchasing assertion constructs, action points may also be spent to improve various player attributes. These attributes are depicted below the game clip in Fig. 2 and include:\n\u2022 Attack Power : Increases damage the enemy receives for every survived program mutant. \u2022 Armour : Reduces incoming damage of every survived program mutant. \u2022 Playthrough Time : Increases available time for generating gameplay traces. \u2022 Number of Mutants : Increases the number of mutants the enemy has to defend.\nThe Planning Phase is the central platform for player interaction, in which players have to strategically decide whether to purchase more block constructs for assertions or use action points to improve their attributes. Once the timer for the Planning Phase runs out, players move on to the Execution Phase, where they can observe whether their strategy was successful or not."
        },
        {
            "heading": "2.2 Execution Phase",
            "text": "The Execution Phase starts by generating multiple mutants based on the Number of Mutants attribute of the competing player. This is done using a traditional set of mutation operators [21]. To ensure fairness, the order of generated mutants is always the same for both players. However, to keep players engaged, the mutants are randomly generated across multiple rounds of Playtest. Each human execution trace and assertion pair created in the Planning Phase is then executed on all generatedmutants. For every surviving mutant, the player\u2019s life points are reduced as a penalty.\nAfterwards, players are presented with a summary of survived and killed mutants, as shown in Fig. 4. This summary displays all generated mutants of the current round. Detected mutants are marked with a green check mark together with the assertion that revealed them, while non-detected mutants are marked with a red cross . By clicking on the corresponding game clips, players can analyse the execution of the generated mutants using the same fast-forward and rewind function as during the placement of assertions. This feature allows players to identify differences in the game behaviour and learn how to generate an assertion that reveals the mutant in the next Planning Phase. To further assist players in detecting mutants, Playtest depicts a bomb icon within the time laps slider whenever mutated code has been executed."
        },
        {
            "heading": "2.3 Synthesising Static and Dynamic Tests",
            "text": "As depicted in the Test module of Fig. 1, Playtest continuously gathers gameplay traces and assertions players create during each Planning Phase. These two sources of information are then utilised\nto generate both static and adaptive dynamic tests that can effectively handle randomised program behaviour. Static tests are generated by extracting executed actions from the players\u2019 gameplay traces and combining them with test oracles derived from created assertions. Dynamic tests, in the form of neural networks, are optimised using backpropagation on a ground truth dataset of human gameplay traces to train networks capable of reaching specific program statements regardless of random program behaviour [10, 11]. Previous research has demonstrated that static assertions are vulnerable to randomised behaviour because they are not capable of adapting to changes in program behaviour, which results in flaky test behaviour and numerous false-positive test outcomes [7]. We address this issue by combining dynamic test networks with humanmade assertions and surprise adequacy-based test oracles capable of adapting to randomised program behaviour [11, 14].\nTo avoid an explosion of the number of extracted tests, we implement the guiding principle of Correlating Success with Valuable Test Cases and only gather traces and assertions from players who have emerged victorious against their competitors. In a crowdsourcing approach [18, 27], we then search for similar gameplay traces across multiple games of Playtest using the Levenshtein Distance [13, 15] as a similarity metric. Since even a tiny change in block-based assertions can significantly impact the testing outcome, player-generated assertions are compared on a block-by-block basis and deemed nonsimilar if at least one non-matching block pair is encountered. We anticipate gathering numerous valuable gameplay traces and assertions through keeping players engaged with Playtest by adhering to the Abstracting the Purpose principle. Due to Correlating Success with Valuable Test Cases principle, the gameplay traces of successful players can then be transformed into valuable tests for games."
        },
        {
            "heading": "3 FUTUREWORK",
            "text": "In the future, our goal is to implement Playtest as an online playerversus-player game, using the SuperTux game as an example application. We will evaluate Playtest by assessing our adherence to the two guiding principles: Abstracting the Purpose and Correlating Success with Valuable Test Cases. To achieve this, we will release Playtest as a free-to-play game on various gaming platforms like itch.io [1] and analyse key metrics such as the number of players reached, the number of downloads, and the feedback received in order to determine whether players enjoy playing Playtest. Similar to previous research [7, 11], we plan to measure the effectiveness of Playtest in generating useful test suites by evaluating the proportion of detected mutants after executing the synthesised tests on a suite of generated program mutants. If our evaluation of Playtest on the example game leads to positive results, we plan to package Playtest as a library with a user-friendly API, enabling an easy integration of our tool into any game environment. Finally, we envision applying Playtest to other UI-intensive domains requiring extensive user interaction, such as testing Android applications."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work is supported by FR 2955/3-1, \u201cTENDER-BLOCK: Testing, Debugging, and Repairing Block-based Programs\u201d and FR 2955/2-1, \u201cQuestWare: Gamifying the Quest for Software Tests\u201d. The authors are responsible for this publication\u2019s content."
        }
    ],
    "title": "PlayTest: A Gamified Test Generator for Games",
    "year": 2023
}