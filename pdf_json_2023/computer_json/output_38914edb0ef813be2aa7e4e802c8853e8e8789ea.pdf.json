{
    "abstractText": "Recent advances in machine learning offer new ways to represent and study scholarly works and the space of knowledge. Graph and text embeddings provide a convenient vector representation of scholarly works based on citations and text. Yet, it is unclear whether their representations are consistent or provide different views of the structure of science. Here, we compare graph and text embedding by testing their ability to capture the hierarchical structure of the Physics and Astronomy Classification Scheme (PACS) of papers published by the American Physical Society (APS). We also provide a qualitative comparison of the overall structure of the graph and text embeddings for reference. We find that neural network-based methods outperform traditional methods and graph embedding methods such as node2vec are better than other methods at capturing the PACS structure. Our results call for further investigations into how different contexts of scientific papers are captured by different methods, and how we can combine and leverage such information in an interpretable manner. 1 ar X iv :2 30 8. 15 70 6v 1 [ cs .S I] 3 0 A ug 2 02 3",
    "authors": [
        {
            "affiliations": [],
            "name": "Isabel Constantino"
        },
        {
            "affiliations": [],
            "name": "Sadamori Kojaku"
        },
        {
            "affiliations": [],
            "name": "Santo Fortunato"
        },
        {
            "affiliations": [],
            "name": "Yong-Yeol Ahn"
        }
    ],
    "id": "SP:23c82b1c1e6d3e3654a19cdc1ddad166c313f66b",
    "references": [
        {
            "authors": [
                "M. Belkin",
                "P. Niyogi"
            ],
            "title": "Laplacian Eigenmaps for Dimensionality Reduction and Data Rep",
            "year": 2003
        },
        {
            "authors": [
                "D.M. Blei"
            ],
            "title": "Probabilistic topic models",
            "venue": "IJCNLP),",
            "year": 2012
        },
        {
            "authors": [
                "K.W. Boyack"
            ],
            "title": "Design and Update of a Classification System: The UCSD Map",
            "year": 2012
        },
        {
            "authors": [
                "K.W. Boyack",
                "R. Klavans",
                "K. B\u00f6rner"
            ],
            "title": "Mapping the backbone of science",
            "year": 2005
        },
        {
            "authors": [
                "M. Chinazzi",
                "B. Gon\u00e7alves",
                "Q. Zhang",
                "A. Vespignani"
            ],
            "title": "Mapping the physics research",
            "year": 2019
        },
        {
            "authors": [
                "W. Conover"
            ],
            "title": "Practical nonparametric statistics",
            "venue": "Social Studies of Science,",
            "year": 1999
        },
        {
            "authors": [
                "D. N19-1423 Endres",
                "J. Schindelin"
            ],
            "title": "A new metric for probability distributions",
            "venue": "IEEE Transactions",
            "year": 2003
        },
        {
            "authors": [
                "W. Gu",
                "A. Tandon",
                "Ahn",
                "Y.-Y",
                "F. Radicchi"
            ],
            "title": "Mining - KDD",
            "year": 2021
        },
        {
            "authors": [
                "M. Douze",
                "H. J\u00e9gou"
            ],
            "title": "Billion-scale similarity search with GPUs",
            "year": 2019
        },
        {
            "authors": [
                "S. Kojaku",
                "J. Yoon",
                "I. Constantino",
                "Ahn",
                "Y.-Y"
            ],
            "title": "Residual2Vec: Debiasing graph",
            "year": 2021
        },
        {
            "authors": [
                "D. Systems. Kozlowski",
                "J. Dusdal",
                "J. Pang",
                "A. Zilian"
            ],
            "title": "Semantic and relational spaces in science",
            "year": 2021
        },
        {
            "authors": [
                "S. Kullback",
                "R.A. Leibler"
            ],
            "title": "On Information and Sufficiency",
            "venue": "The Annals of Mathematical Statistics,",
            "year": 1951
        },
        {
            "authors": [
                "Q. Le",
                "T. Mikolov"
            ],
            "title": "Distributed Representations of Sentences and Documents",
            "venue": "International Conference on Machine Learning,",
            "year": 2014
        },
        {
            "authors": [
                "E. Leahey",
                "J. Moody"
            ],
            "title": "Sociological Innovation through Subfield Integration: Social Currents",
            "year": 2014
        },
        {
            "authors": [
                "L. Leydesdorff",
                "I. Rafols"
            ],
            "title": "Indicators of the interdisciplinarity of journals: Diversity, centrality, and citations",
            "venue": "Journal of Informetrics,",
            "year": 2011
        },
        {
            "authors": [
                "L. Leydesdorff",
                "C.S. Wagner",
                "L. Bornmann"
            ],
            "title": "Interdisciplinarity as diversity in citation patterns among journals: Rao-Stirling diversity, relative variety, and the Gini coefficient",
            "venue": "Journal of Informetrics,",
            "year": 2019
        },
        {
            "authors": [
                "D. Liben-Nowell",
                "J. Kleinberg"
            ],
            "title": "The link-prediction problem for social networks",
            "venue": "Journal of the American Society for Information Science and Technology,",
            "year": 2007
        },
        {
            "authors": [
                "Y. Lin",
                "J.A. Evans",
                "L. Wu"
            ],
            "title": "New directions in science emerge from disconnection and discord",
            "venue": "Journal of Informetrics,",
            "year": 2022
        },
        {
            "authors": [
                "D. Liu",
                "T. Eliassi-Rad"
            ],
            "title": "STABLE: Identifying and Mitigating Instability in Embeddings of the Degenerate Core",
            "venue": "SIAM International Conference on Data Mining",
            "year": 2023
        },
        {
            "authors": [
                "T. Mikolov",
                "K. Chen",
                "G. Corrado",
                "J. Dean"
            ],
            "title": "Efficient estimation of word representations in vector space",
            "year": 2013
        },
        {
            "authors": [
                "T. Mikolov",
                "I. Sutskever",
                "K. Chen",
                "G.S. Corrado",
                "J. Dean"
            ],
            "title": "Distributed representations of words and phrases and their compositionality",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2013
        },
        {
            "authors": [
                "H. Peng",
                "Q. Ke",
                "C. Budak",
                "D.M. Romero",
                "Ahn",
                "Y.-Y"
            ],
            "title": "Neural embeddings of scholarly periodicals reveal complex disciplinary organizations",
            "venue": "Science Advances,",
            "year": 2021
        },
        {
            "authors": [
                "I. Rafols",
                "M. Meyer"
            ],
            "title": "Diversity and network coherence as indicators of interdisciplinarity",
            "year": 2010
        },
        {
            "authors": [
                "N. s11192-009-0041-y Reimers",
                "I. Gurevych"
            ],
            "title": "Sentence-BERT: Sentence embeddings using Siamese BERT",
            "year": 2019
        },
        {
            "authors": [
                "M. Rosen-Zvi",
                "T. Griffiths",
                "M. Steyvers",
                "P. Smyth"
            ],
            "title": "The author-topic model for authors",
            "venue": "IJCNLP),",
            "year": 2004
        },
        {
            "authors": [
                "N. 487\u2013494. Shibata",
                "Y. Kajikawa",
                "I. Sakata"
            ],
            "title": "Link prediction in citation networks",
            "year": 2012
        },
        {
            "authors": [
                "M. Steel",
                "W. Hordijk",
                "S.A. Kauffman"
            ],
            "title": "Dynamics of a birth\u2013death process based on",
            "year": 2020
        },
        {
            "authors": [
                "B. 1016/j.jtbi.2020.110187 Uzzi",
                "S. Mukherjee",
                "M. Stringer",
                "B. Jones"
            ],
            "title": "Atypical Combinations and Scientific",
            "year": 2013
        },
        {
            "authors": [
                "T. Le Scao",
                "S. Gugger",
                "A. . . Rush"
            ],
            "title": "Transformers: State-of-the-art natural language",
            "year": 2020
        },
        {
            "authors": [
                "L. Wu",
                "D. Wang",
                "J.A. Evans"
            ],
            "title": "Large teams develop and small teams disrupt science and technology",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Recent advances in machine learning offer new ways to represent and study scholarly works and the space of knowledge. Graph and text embeddings provide a convenient vector representation of scholarly works based on citations and text. Yet, it is unclear whether their representations are consistent or provide different views of the structure of science. Here, we compare graph and text embedding by testing their ability to capture the hierarchical structure of the Physics and Astronomy Classification Scheme (PACS) of papers published by the American Physical Society (APS). We also provide a qualitative comparison of the overall structure of the graph and text embeddings for reference. We find that neural network-based methods outperform traditional methods and graph embedding methods such as node2vec are better than other methods at capturing the PACS structure. Our results call for further investigations into how different contexts of scientific papers are captured by different methods, and how we can combine and leverage such information in an interpretable manner.\nar X\niv :2\n30 8.\n15 70"
        },
        {
            "heading": "1 Introduction",
            "text": "New discoveries and innovations build upon existing knowledge, often by combining previous knowledge together (Kauffman, 1984; Weitzman, 1998; Steel et al., 2020). Subsequently, new knowledge can be used in many different domains in the future. In this sense, each publication can be largely characterized by its references and citations (i.e. the knowledge upon which the publication builds, and how future publications use its knowledge), and its content. These two sources of information have thus been used to investigate how the scientific enterprise discovers new knowledge.\nPrevious studies have analyzed the references of both patents and scientific publications to understand the relationship between the ideas referenced by inventors and scientists, and the impact of their work. A study by Fleming (2001) uses patent data to compare the effects of combining novel versus familiar ideas, finding that new combinations and new components result in less useful or less successful patents (i.e. patents receive fewer citations). However, this also results in greater variability in future success. The riskiness of exploring new or otherwise unusual combinations of ideas has also been explored in scientific research. Papers that reference both atypical and conventional combinations of ideas are more likely to be highly cited, as well as more \u201cdisruptive\u201d (Uzzi et al., 2013; Wu et al., 2019; Lin et al., 2022). That is, such papers are more likely to be cited in future work than the papers they referenced or built on. Furthermore, research citing novel combinations of ideas are more likely to experience delayed recognition, but are also more likely to be cited by other fields (Wang et al., 2017). In these studies, atypicality and novelty are measured in terms of the number of cocitations between pairs of referenced journals. Atypicality and novelty can then be framed as the \u201cdistance\u201d between reference pairs: ideas are more distant if they are less likely to be cited together, and combining distant ideas are more likely to result in risky but high-impact research.\nPrevious studies have also used the keywords and titles of publications to study the interrelatedness and integration of scientific fields over time. Courtial and Law (1989) proposed a co-word analysis of keywords in artificial intelligence papers by creating a keyword-keyword matrix where each entry corresponds to the number of papers that contain both of these keywords. They then show how clusters in this keyword co-occurrence matrix correspond to various subdomains in AI,\nand how they change over time. In another study by Leahey and Moody (2014), combinations of publication keywords within the same paper are used to measure subfield integration in sociological research. They find that papers that span multiple domains, especially unique combinations of domains, are cited more frequently.\nScience mapping integrates various efforts to quantify the \u201cspace\u201d of knowledge, by obtaining a spatial representation of journals, fields, and papers, where distance and similarity can be quantified. For instance, Boyack, Klavans, and Bo\u0308rner developed a map of scientific journals and disciplines, where similar (i.e. in terms of cocitations and/or co-references) journals are closer to one another (Boyack et al., 2005; Bo\u0308rner et al., 2012; Boyack & Klavans, 2014). On the other hand, the distance between referenced disciplines has been integrated into measures of the interdisciplinarity of research, such as the Rao-Stirling index (Rafols & Meyer, 2010; Leydesdorff & Rafols, 2011; Leydesdorff et al., 2019). These studies in mapping science and evaluating the distance between ideas are crucial in understanding how scientists collectively create, develop, and combine ideas in the context of the \u201cspace\u201d of knowledge.\nThe main caveat in using existing disciplinary categorizations of scientific work, such as the Web of Science subject categories of journals, is that they tend to be rigid and change slowly over time, even when science rapidly develops. In an effort to overcome this limitation, previous studies have applied methods in topic modeling to analyze trends and \u201chot topics\u201d in science. Specifically, latent Dirichlet allocation (LDA) is used to learn representations of publications as probability distributions over scientific topics, and representations of topics as probability distributions over words (Blei et al., 2003; Griffiths & Steyvers, 2004; Blei, 2012). LDA has also been used to learn representations of authors\u2019 body of work as probability distributions over topics (Rosen-Zvi et al., 2004). However, one challenge with LDA and other topic modeling methods is the need to specify the number of topics in the corpus, which is not always known, or may change over time. Moreover, topic modeling approaches documents as \u201cbags of words\u201d; it ignores the order and context of words.\nBy contrast, representative learning (embedding) methods take into account the context within which words or entities appear, and it represents them into continuous space without the need to use categorization (whether provided or learned from the data). These methods have been shown to be capable at meaningfully representing the space of scientific knowledge. For instance, in a\nstudy by Chinazzi et al. (2019), papers published in American Physical Society (APS) journals are represented as \u201cbags of topics\u201d. The embeddings are then used to map the various specialties of physics research, and study the production of physics research by various countries. In another study by Peng et al. (2021), journals in the Web of Science (WOS) are represented using their citation network, which are then used to learn analogy graphs of journals, and visualize the \u201cspectra\u201d of soft versus hard science.\nDespite the increasing use of embedding methods in the study of scientific production and innovation, relatively little attention has been paid to the systematic comparison of two primary embedding approaches\u2014content and graph embedding. While previous studies (Zhang et al., 2019; Kozlowski et al., 2021) have explored the use of various graph and text embedding methods to represent scientific publications, the methods were evaluated separately. Here, we compare popular graph and text embedding methods in terms of how well the resulting embeddings capture the disciplinary structure of physics. We focus on the field of physics, particularly papers published in the American Physical Society (APS) (\u201cPhysical Review Journals\u201d, n.d.), which, until recently, has used a hierarchical classification system called the Physics and Astronomy Classification Scheme (PACS). We then conduct a series of experiments to evaluate the embeddings\u2019 ability to capture the hierarchical structure of PACS."
        },
        {
            "heading": "2 Data and Methods",
            "text": ""
        },
        {
            "heading": "2.1 Data",
            "text": "The APS provides a citation network of all articles published in its journals, as well as papers\u2019 metadata on authors, affiliations, titles, publication dates, etc. We consider the citation network of all peer-reviewed scientific papers, reviews, and letters in all APS journals published up to the year 2010. We remove articles such as announcements, comments and replies, errata, and retractions. The resulting citation network consists of 452,096 papers and 4,931,143 citations. We note that this includes papers published before 1979, which do not have PACS codes. These are included in all embeddings; however, they are not considered in the evaluation.\nFor the content embedding, although it would be ideal to use the full text of papers, this is usually not available to researchers. Therefore, we consider the titles and abstracts of the papers in the citation network. Because the data provided by the APS does not contain abstract information, we match entries in the APS to entries in the Web of Science using exact DOI matching. Among the 358,478 papers with exact DOI matches to the Web of Science up to 2010, 159,375 have abstracts. We limit the preprocessing of text data to removing HTML and MathML tags, as well as removing extraneous whitespace."
        },
        {
            "heading": "2.2 Embedding",
            "text": ""
        },
        {
            "heading": "2.2.1 Graph embedding",
            "text": "We embed the citation network of the 452K papers by using graph embedding. Graph embedding places one paper in a space in relation to other papers. There are two types of relations, i.e., references (out-going citations) and citations (in-coming citations), which provides semantically different views of a paper. The references of a paper imply the knowledge relevant to the content of the paper, while the citations to a paper imply how the paper is utilized by the future papers. We incorporate both aspects of relations into consideration by ignoring the edge directionality and producing the graph embeddings using the undirected citation network.\nA traditional family of graph embedding methods is based on matrix factorization, with one of the most popular being Laplacian Eigenmap (Belkin & Niyogi, 2003). Laplacian Eigenmap is a spectral embedding method, which obtains an m-dimensional embedding by concatenating the second to the (m + 1)-th largest eigenvectors of the symmetric normalized Laplacian L =\nI\u2212D\u2212 12AD 12 , where I is the diagonal identity matrix, D is the diagonal degree matrix Dii = \u2211 j Aij, and A is the graph adjacency matrix.\nThe second graph embedding method is node2vec (Grover & Leskovec, 2016). node2vec is a direct application of the skip-gram negative sampling method (SGNS) word2vec (Mikolov, Chen, et al., 2013; Mikolov, Sutskever, et al., 2013), a word embedding model that produces an embedding of words from given sentences. node2vec is adapted to network embedding by using a random walk, where each sentence is composed of nodes visited by a random walk. node2vec allows the random\nwalker to preferentially visit the previous nodes, or nodes that are not adjacent to previous nodes. This bias is controlled by hyperparameters p and q. A value of p = 1 and q = 1 is equivalent to a uniform random walk. For this study, we use p = 1 and q = 1, similar to previous studies evaluating graph embeddings (Gu et al., 2021; Dehghan-Kooshkghazi et al., 2022; Liu & Eliassi-Rad, 2023).\nFinally, the third graph embedding method is residual2vec (Kojaku et al., 2021). residual2vec, also a random walk-based embedding method, uses the bias removal mechanism of the negative sampling method used in node2vec to remove any prescribed bias from the embedding. Here, we consider several structural biases present in the citation network. First, older papers are more likely to have a higher degree (i.e. more citations) due to the time it has to accumulate citations. Second, papers tend to cite more recent references. To address these biases, we use the configuration model as a null model for residual2vec. In other words, we remove the impact of each paper\u2019s degree from the embedding. For all graph embeddings, we use dimension d = 128, given that most real-world networks do not require a large number of dimensions, and that the embedding tends not to suffer much from overparametrization of the number of dimensions (Gu et al., 2021). Other parameters we use are: walk length l = 80, walks per node r = 10, context window size k = 10, and random walk restart probability s = 0.01."
        },
        {
            "heading": "2.2.2 Text embedding",
            "text": "We use three popular text embedding methods: doc2vec (Le & Mikolov, 2014), SciBERT (Beltagy et al., 2019), and Sentence-BERT (Reimers & Gurevych, 2019). doc2vec learns document representations by training them to predict words in the document, similar to how word2vec learns word representations by predicting words within its context. We specifically use the Paragraph VectorDistributed Memory (PV-DM) model of doc2vec, where the paragraph (or document) vector serves as a \u201cmemory\u201d that provides a broader context. For doc2vec, we use dimension d = 128, window size 5, and minimum word instance count 5.\nThe second text embedding method is SciBERT (Beltagy et al., 2019), an extension of BERT (Devlin et al., 2019). BERT is a pretrained deep neural language model that is trained using a bidirectional transformer network, and can be fine-tuned for tasks such as classification and question answering. SciBERT is a BERT model finetuned on a dataset of computer science and biomedical\nscience papers. We use the allenai/scibert scivocab uncased model hosted on the Huggingface Transformers library (Wolf et al., 2020). As BERT generates representations for word pieces or punctuation marks (also known as tokens), some extra processing is required to obtain representations of \u201csentences\u201d or documents. The processing is as follows (Reimers & Gurevych, 2019): (1) For each text input or document, obtain the special CLS token; (2) for each CLS token, obtain the vectors from the last 4 hidden layers of the neural network; and (3) sum up the 4 layers.\nThe last text embedding method is Sentence-BERT (Reimers & Gurevych, 2019). SentenceBERT extends on BERT using a Siamese network architecture to obtain sentence embeddings that can be directly compared using measures such as cosine similarity. The pretrained model used is sentence-transformers/paraphrase-mpnet-base-v2, also hosted by Huggingface. As SentenceBERT already results in an embedding for each piece of text, no further processing is required. We note that both SciBERT and Sentence-BERT support a maximum of 512 word tokens; any input beyond 512 tokens is ignored."
        },
        {
            "heading": "2.3 Evaluation",
            "text": "What makes a good embedding? There are a multitude of ways in which the parameters of each embedding method can be tuned, and many other ways to judge \u201csimilarity\u201d or distance\u201d between two embedded entities. Because we are interested in how each of the two main families of embedding methods can encode the hierarchical disciplinary structure of physics, we evaluate the embeddings based on how well they reflect an existing hierarchical categorization method, the Physics and Astronomy Classification Scheme (PACS) (\u201cPhysics and Astronomy Classification Scheme (PACS)\u201d, 2008). We summarize the embedding and evaluation framework in Figure 1."
        },
        {
            "heading": "2.3.1 Classification",
            "text": "We first evaluate the embeddings\u2019 ability to capture the general clusters of physics research, by running a k-nearest neighbor (KNN) classification task to predict the level 1 (L1) PACS code, that is, its general physics classification. KNN is a supervised machine learning algorithm where data is labeled according to the labels of its k nearest neighbors, as calculated by a distance metric. We\nData and Methods APS articles (1893\u20132010)\nEmbed citation network\nLaplacian Eigenmap node2vec\nresidual2vec\nEmbed titles, abstracts\ndoc2vec SciBERT\nSentence-BERT\nEvaluation\nLink prediction k-NN classification\nPACS code \u201cspread\u201d Pairwise embedding\ndistance\nFigure 1: Embedding and evaluation framework for APS papers.\nfirst take the citation network consisting of papers published on or before the year 2010. For each paper, we take all full PACS codes, and take the first digit of each PACS code to get the L1 PACS code. We then take 80% of the papers to be used as a training set, and take note of their L1 PACS codes. The remaining 20% of papers, or the testing set, are labeled with the L1 PACS code of the majority of its k nearest neighbors, as measured by the cosine distance between the paper vectors in the embedding space. For this study, we implement KNN using the Faiss library (Johnson et al., 2019), which provides a faster calculation of distance in large, high-dimensional datasets. We repeat for the values k \u2208 {2, 4, 8, 16, 32, 64, 128}. The resulting classification is then evaluated using the micro-F1 score."
        },
        {
            "heading": "2.3.2 Evaluating hierarchical disciplinary structure",
            "text": "We then evaluate whether the embeddings can capture the hierarchical structure of PACS. We do this in two ways. First, we check whether papers of more specific subcategories are closer to one another, compared to papers of general subcategories. Intuitively, sets of papers of a same general\ntopic \u201ccover\u201d more material than sets of papers of a more specific topic. We consider each PACS code at each level. At the highest level, there are 10 PACS codes (00\u201390) each corresponding to a general physics category such as particle physics, nuclear physics, etc. Each succeeding level then corresponds to a more specific subtopic. Below is an example of a six-digit PACS code: 61.30.Jf. The first level, 6, or 60, corresponds to \u201cthe structural, mechanical, and thermal properties of condensed matter\u201d. The second level, 61, then represents \u201ccrystallography\u201d and \u201csolid or liquid structures\u201d. The third level, 61.30, refers to \u201cliquid crystals\u201d. Finally, the fourth or lowest level, 61.30.Jf, refers to \u201cdefects in liquid crystals\u201d. We want to check whether each succeeding PACS level has smaller and smaller variability among its constituent papers. We do this using a measure inspired by the radius of gyration (ROG), or the root mean square distance of an object\u2019s parts to its center of mass. In our case, we measure the root mean square cosine distance of each vector, to the centroid or average of these vectors:\nROGC =\n\u221a 1\nN \u2211 x\u2208C ( 1 \u2212 cos(x, x\u0304) )2 (1)\nwhere C is the PACS code of interest, x a vector representation of a paper in C, x\u0304 the centroid of all vectors in C, and cos(a,b) the cosine similarity between vectors a and b. We calculate the ROG of each PACS code at each of the 4 levels. We then get the distribution of ROG values at each level. To confirm whether each finer level has smaller or larger ROG values, we perform a one-sample Wilcoxon signed-rank text (Wilcoxon, 1945; Conover, 1999) to test the null hypothesis that the median ROG distribution of a PACS level is greater than or equal to the median ROG distribution of the next lower PACS level.\nSecond, we evaluate whether pairs of papers of the same PACS code are more likely to be closer to one another in the embedding space, compared to papers of different PACS codes. We sample 15,000 pairs of papers in each of the following categories: (1) random, (2) different discipline or PACS code, (3) same discipline (L1 PACS code), and (4) same subdiscipline (L2 PACS code). We then get the distribution of cosine distance between the pairs of papers in each subcategory. We compare these distributions in two ways. First, we compute the Jensen-Shannon divergence between each of the cosine distance distributions. The Jensen-Shannon (JS) divergence is a similarity measure for two\ndistributions (Endres & Schindelin, 2003). Unlike the popular KL divergence (Kullback & Leibler, 1951), it is symmetric and has a finite range of 0 to 1. However, The Jensen-Shannon divergence does not indicate direction; that is, whether the cosine distance of same-discipline pairs is the same of that of random pairs. Hence, we also conduct one-sample t-tests to compare the distributions. In particular, we test the null hypothesis that the mean distance between random pairs is less than or equal to than the mean distance between same-discipline pairs."
        },
        {
            "heading": "2.3.3 Link prediction",
            "text": "Finally, we evaluate whether the embeddings are useful for predicting links between papers and references, specifically, whether a paper cites or references a previous paper (Adamic & Adar, 2003; Liben-Nowell & Kleinberg, 2007; Shibata et al., 2012). We first take the citation network of APS papers published up to the year 2010. We then take the largest connected component of this citation network, then sample 50% or approximately 149,000 edges from papers published in 2010 (i.e. edges corresponding to citations coming from papers in 2010). This is the \u201cpositive\u201d example set. We then sampled the same number of paper pairs (such that one of the papers in each pair is from 2010) that have no edge between them, to serve as the \u201cnegative\u201d examples. The \u201cpositive\u201d examples or edges are removed from the citation network, and the resulting network is embedded using each of the embedding methods as described in section 2.2. Then, for each embedding, we calculate the cosine similarity of the positive and negative examples. In this case, positive examples, or pairs of papers linked by citation, should have a high cosine similarity, while negative examples should have low cosine similarity. The results are evaluated using the ROC curve, used in binary classification: the ROC curve plots the false positive rate against the true positive rate, denoting how the true positive rate changes as the false positive rate threshold is increased. The area under the ROC curve (AUC) is then calculated to summarize the results. The AUC ranges from 0 to 1, where 1 represents a perfect classification, and an AUC of 0.5 represents a random classification."
        },
        {
            "heading": "3 Results",
            "text": "Figure 2 shows UMAP projections of each embedding, using a sample of 10,000 papers. Each point is colored according to its level 1 PACS code. Here, Sentence-BERT\u2014both title and abstract embeddings\u2014node2vec, and residual2vec provide paper-level embeddings that follow the general clustering structure of research published in APS. The Laplacian Eigenmap embedding also somewhat exhibits a general cluster structure, although it also presents many overlaps among the clusters.\nFigure 3 shows the results for the k-nearest neighbor classification. Among the text embeddings, Sentence-BERT has the highest micro-F1 score of approximately 0.75, across multiple values of k. All text embeddings performed worse than the tested graph embeddings, which all resulted in micro-F1 scores of approximately 0.76 to 0.81. In Laplacian Eigenmap, higher values of k result in lower micro-F1 scores. Finally, all embeddings except node2vec performed worse than a baseline citation network-based classification which predicts a paper\u2019s level 1 PACS code using a majority vote among its references, although this may indicate a tendency for authors to self-assign PACS codes similar to those of their references.\nFigure 4 shows the distributions of ROG at each PACS level, and Table 1 shows the results of the one-sample Wilcoxon test for the distributions. For all tested graph embedding methods, as well as in Sentence-BERT, the median ROG at each level is consistently lower than the median ROG of the preceding level. Hence, for these embeddings, the more specific the subdiscipline of physics, the closer together its papers are in the embedding space. Meanwhile, for doc2vec and SciBERT, the median ROG of the highest PACS level is lower than or equal to the median ROG of the second PACS level. This may be explained by the \u201cscale\u201d of these text embeddings, where all points are relatively close to one another, thereby resulting in very small differences in ROG across the PACS levels. By contrast, in the graph embeddings, the points are more spread out across the embedding space, which results in more dramatic decreases in ROG as the PACS level increases. This is especially apparent in node2vec and residual2vec.\nFigure 5 shows the distributions of sampled paper pairs, Figure 6 shows the Jensen-Shannon (JS) distance matrices for each embedding, and Table 2 shows the results of the t-test to compare these embeddings\u2019 distance distributions. In the representations generated using the graph embeddings and Sentence-BERT, the mean cosine distance of random paper pairs is consistently greater than that of pairs with the same L1 or L2 PACS code. In addition, the JS distances between the distributions in the node2vec and residual2vec embeddings are increasing. That is, the JS distance between random pairs and pairs with different PACS codes is less than the JS distance between pairs with the same L1 and L2 PACS codes. However, on doc2vec embeddings on titles, we fail to reject the hypothesis that the mean cosine distance of random pairs is not equal to that of random pairs. Moreover, there are no clear differences in Jensen-Shannon distance among the cosine distance distributions of random versus other paper pairs. Meanwhile, on abstracts, although there are also no visible differences in Jensen-Shannon distance, we do reject the hypothesis that the mean cosine distance of random pairs is not equal to that of random pairs.\nrand diff same_1 same_2\nra nd di ff\nsa m\ne_ 1\nsa m\ne_ 2\n0 0.045 0.046 0.048\n0.045 0 0.046 0.048\n0.046 0.046 0 0.049\n0.048 0.048 0.049 0\nLaplacian Eigenmap, Undirected Graph\n0.00\n0.01\n0.02\n0.03\n0.04\nrand diff same_1 same_2\nra nd di ff\nsa m\ne_ 1\nsa m\ne_ 2\n0 0.088 0.11 0.12\n0.088 0 0.1 0.12\n0.11 0.1 0 0.13\n0.12 0.12 0.13 0\nnode2vec, Undirected Graph\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\nrand diff same_1 same_2\nra nd di ff\nsa m\ne_ 1\nsa m\ne_ 2\n0 0.092 0.13 0.15\n0.092 0 0.12 0.14\n0.13 0.12 0 0.17\n0.15 0.14 0.17 0\nresidual2vec, Undirected Graph\n0.00\n0.03\n0.06\n0.09\n0.12\n0.15\nrand diff same_1 same_2\nra nd di ff\nsa m\ne_ 1\nsa m\ne_ 2\n0 0.2 0.2 0.2\n0.2 0 0.2 0.2\n0.2 0.2 0 0.2\n0.2 0.2 0.2 0\ndoc2vec, Title\n0.00\n0.04\n0.08\n0.12\n0.16\n0.20\nrand diff same_1 same_2\nra nd di ff\nsa m\ne_ 1\nsa m\ne_ 2\n0 0.39 0.4 0.39\n0.39 0 0.4 0.4\n0.4 0.4 0 0.4\n0.39 0.4 0.4 0\nSciBERT, Title\n0.00\n0.08\n0.16\n0.24\n0.32\n0.40\nrand diff same_1 same_2 ra nd di ff sa m e_ 1 sa m e_ 2 0 0.075 0.078 0.081 0.075 0 0.077 0.08 0.078 0.077 0 0.084 0.081 0.08 0.084 0\nSentence-BERT, Title\n0.000\n0.015\n0.030\n0.045\n0.060\n0.075\ndoc2vec, Abstract\nSciBERT, Abstract\nSentence-BERT, Abstract\nDe ns\nity\nFigure 7 shows the ROC curves and AUC for the link prediction experiments. The graph embedding methods have the highest performance for link prediction, as measured by the area under the ROC curve (AUC). Sentence-BERT performs best among the text embedding methods, but it does not perform as well as the random-walk based embedding methods node2vec and residual2vec."
        },
        {
            "heading": "4 Discussion",
            "text": "We have found that while graph embedding methods use the rich information stored in the citation network structure and generally perform better, Sentence-BERT\u2014whether it is trained on titles or abstracts\u2014has a remarkable performance. Our results suggest that the disciplinary structure of physics may be better encoded with graph embedding rather than content embedding. In other words, titles\u2014which are often the only \u201ccontent\u201d information available for many papers\u2014may not fully capture the content of the paper. Abstracts are able to encode the disciplinary structure better than titles, although abstracts tend to be more limited in availability. Meanwhile, citations are more informative and provide more accurate information about the \u201clocation\u201d of a paper. In the future,\nwe may explore further the variety of information captured by different embedding methods, as well as test embedding methods that use both text data and citation information. Embedding text and citation data provides many exciting opportunities to study how scientific knowledge is referenced, created, and shared.\nData and Code Availability\nThe code used in the analysis is available at https://github.com/sabsconstantino/emb-comp-aps. The American Physical Society dataset is available upon request at https://journals.aps.org/datasets. Abstract data from the Web of Science is proprietary and cannot be made available.\nAuthor Contributions\nIsabel Constantino\u2014Conceptualization, Data curation, Methodology, Software, Visualization, Writing\u2014 original draft, Writing\u2014review and editing. Sadamori Kojaku\u2014Methodology, Software, Writing\u2014 original draft, Writing\u2014review and editing. Santo Fortunato\u2014Conceptualization, Funding acquisition, Methodology, Writing\u2014review and editing. Yong-Yeol Ahn\u2014Conceptualization, Funding acquisition, Methodology, Supervision, Writing\u2014original draft, Writing\u2014review and editing.\nFunding Information\nI.C., S.K., and Y.Y.A. are supported by the Air Force Office of Scientific Research under award number FA9550-19-1-0391. S.F. is supported by the National Science Foundation under award number 1927418, the Air Force Office of Scientific Research under award number FA9550-19-10354, and the National Institute of General Medical Sciences of the National Institutes of Health under award number R35GM146974."
        }
    ],
    "title": "Representing the disciplinary structure of physics: a comparative evaluation of graph and text embedding methods",
    "year": 2023
}