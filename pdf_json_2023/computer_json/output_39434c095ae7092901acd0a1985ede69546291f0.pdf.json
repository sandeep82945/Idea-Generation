{
    "abstractText": "The YOLOv5 network architecture prioritizes speed and efficiency, but this may limit its ability to capture intricate details of complex objects. To solve the problems of insufficient feature extraction ability and incomplete feature fusion in the YOLOv5 single-stage detection network, we propose a YOLOv5 algorithm based on an improved bidirectional feature pyramid network (BiFPN). The FPN is modified into BiFPN with recursive feature fusion, bidirectional connections, and alignment of multi-scale features. It can utilize different levels of feature information to improve the ability of feature expression, thereby improving model detection accuracy and efficiency. This method incorporates CBAM attention mechanism, which can adaptively adjust the feature map while considering both channel and spatial features, thus more comprehensively expressing information in the feature map. For small targets, due to their small size, they often have high similarity with background and unclear features that make it difficult for traditional loss functions to optimize accurately. Using SIoU as a loss function can improve the recognition rate for small object. The experimental results show that our strategy significantly improves the performance of YOLOv5 on the NEU-DET dataset, reaching 77.5% mAP at 92 Frame Per Second(FPS), which is 3.1% higher than unimproved YOLOv5 network and 19.6% higher than SSD algorithm. Moreover, F1 score has also been improved while showing strong generalization ability on GC-DET dataset as well. Its detection speed is also superior to other algorithms indicating that this improved algorithm can quickly and efficiently detect surface defects in steel materials while significantly improving detection performance. INDEX TERMS BiFPN, Channel attention mechanism, Defect detection, Spatial attention mechanism",
    "authors": [
        {
            "affiliations": [],
            "name": "HAITAO XIN"
        },
        {
            "affiliations": [],
            "name": "KAI ZHANG"
        }
    ],
    "id": "SP:b863a9ade48982102ec7c09cd82bc975427d3369",
    "references": [
        {
            "authors": [
                "X. Kou",
                "S. Liu",
                "K. Cheng",
                "Y. Qian"
            ],
            "title": "Development of a YOLO- V3-based model for detecting defects on steel strip surface",
            "venue": "Measurement, vol. 182, 2021, doi: 10.1016/j.measurement.2021.109454.",
            "year": 2021
        },
        {
            "authors": [
                "V. Sharma",
                "R.N. Mir"
            ],
            "title": "A comprehensive and systematic look up into deep learning based object detection techniques: A review",
            "venue": "Computer Science Review, vol. 38, 2020, doi: 10.1016/j.cosrev.2020.100301.",
            "year": 2020
        },
        {
            "authors": [
                "D. Bhatt"
            ],
            "title": "CNN variants for computer vision: history, architecture, application, challenges and future scope",
            "venue": "Electronics, vol. 10, no. 20, p. 2470, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R. Girshick",
                "J. Donahue",
                "T. Darrell",
                "J. Malik"
            ],
            "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2014, pp. 580-587.",
            "year": 2014
        },
        {
            "authors": [
                "R. Girshick"
            ],
            "title": "Fast r-cnn",
            "venue": "Proceedings of the IEEE international conference on computer vision, 2015, pp. 1440-1448.",
            "year": 2015
        },
        {
            "authors": [
                "S. Ren",
                "K. He",
                "R. Girshick",
                "J. Sun"
            ],
            "title": "Faster r-cnn: Towards realtime object detection with region proposal networks",
            "venue": "Advances in neural information processing systems, vol. 28, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "K. He",
                "G. Gkioxari",
                "P. Doll\u00e1r",
                "R. Girshick"
            ],
            "title": "Mask r-cnn",
            "venue": "Proceedings of the IEEE international conference on computer vision, 2017, pp. 2961-2969.",
            "year": 2017
        },
        {
            "authors": [
                "W. Liu"
            ],
            "title": "Ssd: Single shot multibox detector",
            "venue": "Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11\u201314, 2016, Proceedings, Part I 14, 2016: Springer, pp. 21-37.",
            "year": 2016
        },
        {
            "authors": [
                "Y. Xie",
                "W. Hu",
                "S. Xie",
                "L. He"
            ],
            "title": "Surface Defect Detection Algorithm Based on Feature-Enhanced YOLO",
            "venue": "Cognitive Computation, vol. 15, no. 2, pp. 565-579, 2023/03/01 2023, doi: 10.1007/s12559- 022-10061-z.",
            "year": 2023
        },
        {
            "authors": [
                "Y. Xu",
                "K. Zhang",
                "L. Wang"
            ],
            "title": "Metal Surface Defect Detection Using Modified YOLO",
            "venue": "Algorithms, vol. 14, no. 9, 2021, doi: 10.3390/a14090257.",
            "year": 2021
        },
        {
            "authors": [
                "X. Tao",
                "D. Zhang",
                "W. Ma",
                "X. Liu",
                "D. Xu"
            ],
            "title": "Automatic Metallic Surface Defect Detection and Recognition with Convolutional Neural Networks",
            "venue": "Applied Sciences, vol. 8, no. 9, 2018, doi: 10.3390/app8091575.",
            "year": 2018
        },
        {
            "authors": [
                "I. Katsamenis"
            ],
            "title": "TraCon: A novel dataset for real-time traffic cones detection using deep learning",
            "venue": "Novel & Intelligent Digital Systems: Proceedings of the 2nd International Conference (NiDS 2022), 2022: Springer, pp. 382-391.",
            "year": 2022
        },
        {
            "authors": [
                "S. Lin",
                "P. Hui"
            ],
            "title": "Where's your focus: Personalized attention",
            "venue": "arXiv preprint arXiv:1802.07931, 2018.",
            "year": 1802
        },
        {
            "authors": [
                "Y. Zhu",
                "W.Q. Yan"
            ],
            "title": "Traffic sign recognition based on deep learning",
            "venue": "Multimedia Tools and Applications, vol. 81, no. 13, pp. 17779-17791, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Zhou",
                "T. Wu",
                "K. Sun",
                "C. Zhang"
            ],
            "title": "Towards high accuracy pedestrian detection on edge gpus",
            "venue": "Sensors, vol. 22, no. 16, p. 5980, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "W. Li",
                "K. Liu",
                "L. Zhang",
                "F. Cheng"
            ],
            "title": "Object detection based on an adaptive attention mechanism",
            "venue": "Scientific Reports, vol. 10, no. 1, p. 11307, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Chen",
                "Y. Chen",
                "W. Li",
                "G. Ning",
                "M. Tong",
                "A. Hilton"
            ],
            "title": "Channel and spatial attention based deep object co-segmentation",
            "venue": "Knowledge-Based Systems, vol. 211, p. 106550, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "X. Zhu",
                "D. Cheng",
                "Z. Zhang",
                "S. Lin",
                "J. Dai"
            ],
            "title": "An empirical study of spatial attention mechanisms in deep networks",
            "venue": "Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 6688-6697.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Yu"
            ],
            "title": "Teat detection of dairy cows based on deep learning neural network FS-YOLOv4 model",
            "venue": "Computers and Electronics in Agriculture, vol. 200, p. 107224, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Liu",
                "L. Qi",
                "H. Qin",
                "J. Shi",
                "J. Jia"
            ],
            "title": "Path aggregation network for instance segmentation",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 8759-8768.",
            "year": 2018
        },
        {
            "authors": [
                "Y. He",
                "K. Song",
                "Q. Meng",
                "Y. Yan"
            ],
            "title": "An End-to-End Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features",
            "venue": "IEEE Transactions on Instrumentation and Measurement, vol. 69, no. 4, pp. 1493-1504, 2020, doi: 10.1109/tim.2019.2915404.",
            "year": 2020
        },
        {
            "authors": [
                "R. Solovyev",
                "W. Wang",
                "T. Gabruseva"
            ],
            "title": "Weighted boxes fusion: Ensembling boxes from different object detection models",
            "venue": "Image and Vision Computing, vol. 107, p. 104117, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "L. Tychsen-Smith",
                "L. Petersson"
            ],
            "title": "Improving object localization with fitness nms and bounded iou loss",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 6877-6885.",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "VOLUME XX, 2023 1\nability to capture intricate details of complex objects. To solve the problems of insufficient feature extraction ability and incomplete feature fusion in the YOLOv5 single-stage detection network, we propose a YOLOv5 algorithm based on an improved bidirectional feature pyramid network (BiFPN). The FPN is modified into BiFPN with recursive feature fusion, bidirectional connections, and alignment of multi-scale features. It can utilize different levels of feature information to improve the ability of feature expression, thereby improving model detection accuracy and efficiency. This method incorporates CBAM attention mechanism, which can adaptively adjust the feature map while considering both channel and spatial features, thus more comprehensively expressing information in the feature map. For small targets, due to their small size, they often have high similarity with background and unclear features that make it difficult for traditional loss functions to optimize accurately. Using SIoU as a loss function can improve the recognition rate for small object. The experimental results show that our strategy significantly improves the performance of YOLOv5 on the NEU-DET dataset, reaching 77.5% mAP at 92 Frame Per Second(FPS), which is 3.1% higher than unimproved YOLOv5 network and 19.6% higher than SSD algorithm. Moreover, F1 score has also been improved while showing strong generalization ability on GC-DET dataset as well. Its detection speed is also superior to other algorithms indicating that this improved algorithm can quickly and efficiently detect surface defects in steel materials while significantly improving detection performance.\nINDEX TERMS BiFPN, Channel attention mechanism, Defect detection, Spatial attention mechanism\nI. INTRODUCTION In recent years, with the development of deep learning and computer vision, object detection has made remarkable progress and has been widely used in various fields[1]. As an important application of object detection, steel surface defect detection has attracted more and more attention from researchers and practitioners because of its practical significance in ensuring the quality and safety of steel products[2]. However, it is inevitable that steel will have surface defects such as cracks and scratches during the production process. Traditional inspection methods rely on manual labor, which involves a lot of repetitive activities that are prone to human error. It is difficult to avoid missed or incorrect inspections due to the subjectivity of workers. Existing defect detection methods still face challenges in terms of accuracy, speed, and robustness.\nObject detection is an important branch of computer vision,\nwhich aims to detect specific objects from images or videos, localize and classify them. As the mainstream technology of deep learning, CNN (convolutional neural network, CNN) is widely used in the field of computer vision[3]. The current mainstream object detection technology is divided into single-stage detection and two-stage detection. The main difference between the two is whether to use region candidate blocks. The single-stage algorithm first generates a region candidate box, then classifies and locates it, and divides it into two stages, the most representative of which are R-CNN[4], Fast R-CNN[5], Faster R-CNN[6], Mask R - CNN[7], etc., single-stage detection does not produce region candidate blocks. Typical single-stage detection methods are SSD[8] and YOLO series algorithm. Compared with two\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nH.Xin, K.Zhang: Surface Defect Detection with Channel-Spatial Attention Modules and Bi-Directional Feature Pyramid\n2 VOLUME XX, 2023\nstage object detection, the speed of single-stage detection is significantly improved. Furthermore, the results obtained through these methods also fulfill practical requirements.\nIn recent years, many researchers have done a lot of\nresearch and experiments on steel surface defect detection.\nXie[9] proposed an improved feature pyramid network to enhance the spatial location correlation of the multi-scale detection layer and a novel regression prediction frame with a statistical-based k-means++ algorithm to improve the quality of the initial anchor points and accelerate the convergence of the model. Fast and accurate end-to-end detection was achieved.\nXu[10] et al. proposed an enhanced technique for surface defect detection, which combines the shallow features of Darknet-53 with the deep features of a neural network to generate new scaled feature layers using the YOLOv3 network structure. In addition, the K-Means++ algorithm is used to reduce the sensitivity to the initial clustering center, analyze anchor frame size information and select the best anchor frame for more accurate localization. In addition, Tao[11] and his team have developed an automated system for detecting metal defects in industrial environments. In their paper, they discuss the use of a dual approach to precisely locate and classify defects in the input image. Their system uses a novel cascaded self-encoder architecture for defect segmentation and localization. The defective regions in the segmentation result are then classified into specific classes using a compact CNN. They tested their system using an industrial dataset to detect metal defects under different conditions. The results show that their method meets the performance criteria for metal defect detection.\nTo address the complexity of surface defects, Dong[12] proposed a new method called Pyramid Feature Fusion and Global Contextual Attention Network. The method extracts multi-scale features from the backbone network and fuses them into five resolutions using a pyramid feature fusion module with efficient dense jump connections. The global contextual attention module is then applied to the fused feature map, enabling valid information to be propagated from the low-resolution to the high-resolution fused feature map. A boundary refinement module is added to improve the defect boundaries and improve the prediction results. The final prediction result is a fusion of five resolution fused feature maps. By optimizing the model, detection accuracy is improved. This approach addresses the complexity of surface defects and provides a new solution for pixel-level detection of surface defects.\nSignificant progress has been made in object detection technology based on deep learning, among which Yolov5 is an effective object detection algorithm[13]. While these methods have had some success in detecting defects on metal surfaces, the complex network structure and intensive computation make them also have some potential drawbacks:\n(a) Algorithm complexity: some of the methods use\ncomplex network structures and algorithms, leading to increased difficulty in implementation and understanding, and may require more training and learning costs for non-specialists.\n(b) Training data requirements: These methods often\nrequire large amounts of marker data for training, and acquiring high-quality marker data can be a time-consuming and expensive task, especially in industrial environments where real-world metal defect data is available.\n(c) Parameter tuning and optimization: Some methods\nrely on a large number of parameters and hyperparameters that need to be carefully tuned and optimized to achieve optimum performance. This can be time-consuming and computationally resource-intensive.\n(d) Scenario-specific limitations: The performance of\neach method may perform well in a particular industrial scenario, but may not be well adapted in other scenarios. Therefore, it needs to be tailored and optimized for different scenarios.\n(e) Detection accuracy and robustness: Although\nthese methods have achieved some improvements in accuracy, there are still certain problems of false and missed detections. Especially in complex industrial environments, they may be interfered with by factors such as light and noise, which may have an impact on the detection results.\nYOLOv5 provides efficient parameters, resulting in faster training and inference times. It also exhibits high detection accuracy and achieves state-of-the-art results. However, the similarity between steel surface defects and steel is such that the feature extraction capability needs further enhancement. And CBAM and BiFPN can improve these shortcomings, so ours proposes an improved YOLOv5 steel surface defect detection method based on BiFPN (bi-directional feature pyramid network):\n(a) This method utilizes the advantages of BiFPN\nrecursive feature fusion, bidirectional connections, efficiency, and alignment of multi-scale features, which can more fully utilize feature information at different levels and improve the ability of feature expression. This bidirectional flow helps to alleviate the loss of information and gradients that can occur in traditional FPN, leading to more robust feature representations.\n(b) Meanwhile, incorporating the CBAM attention\nmodule can fully comprehensively express information in the feature map.\nAt the same time, in order to enhance the detection effect of small targets, ours use SIoU as the loss function, it better adapts to object detection tasks with various scales and shapes, has good versatility and scalability, and can also\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nH.Xin, K.Zhang: Surface Defect Detection with Channel-Spatial Attention Modules and Bi-Directional Feature Pyramid\nVOLUME XX, 2023 3\naccelerate convergence and improve the robustness of objects during training. This paper will introduce the architecture of\nthis method in detail and verify its effectiveness through experiments."
        },
        {
            "heading": "II. RELATED WORK",
            "text": ""
        },
        {
            "heading": "A. CBAM Attention",
            "text": "The YOLOv5 network structure can be divided into two main parts: one is the feature extraction layer, which is used to extract the features of the image; the other is the detection layer, which is used for object detection based on the results of feature extraction[14]\nThe feature extraction layer is divided into four parts:\ninput, backbone, neck, and head.\nThe input to YOLOv5 is a simple and effective model that takes an image of any size and converts it to an input of a specific size. It uses a technique called multiscale prediction that automatically selects the appropriate resolution based on the image size. In addition, YOLOv5 also adopts a new data enhancement method, through various transformations such as rotation, flipping, scaling and noise injection, the model can be exposed to more diverse visual changes[15]. This enhancement process helps to improve the model's ability to generalize and extract meaningful features, and can effectively expand the dataset, thereby helping the model learn more features. Ultimately improving the performance of object detection tasks.\nThe backbone part of YOLOv5 uses the CSPDarknet53 network, which is a deep separable convolutional neural network consisting of 53 layers. Its first 7 layers are a separable convolutional block, where each convolutional block contains three convolutional layers, including a 1x1 convolutional layer[16]. This reduces computation and keeps feature maps high resolution. In addition, CSPDarknet53 also contains three residual blocks, which are used to extract highorder features in the feature map. Finally, there is an output layer in CSPDarknet53, which maps the feature maps onto the output. YOLOv5 uses these feature maps to recognize objects and estimate their frames. But CSPDarknet53 is used as a standalone backbone, so it has certain limitations. It lacks specific mechanisms for effective feature fusion, attention, and contextual understanding, which can result in\nreduced detection accuracy, especially in complex scenarios. Additionally, its fixed architecture limits adaptability to different datasets and detection requirements. Furthermore, the computational demands of CSPDarknet53 may hinder its practical usability in resource-constrained environments. To overcome these limitations, it is often necessary to integrate additional components or optimize the network architecture. In order to improve the detection accuracy, the CBAM (Convolutional Block Attention Module) attention mechanism is used to enhance the feature representation and improve the detection accuracy.\nThe CBAM attention mechanism is an attention mechanism for computer vision tasks, which improves the performance of the model by weighting the channels and spatial dimensions of the image. It consists of two key components: channel attention and spatial attention[17].\nThe channel attention module is mainly used to weight the input channel feature maps. It uses global average pooling to obtain a global channel feature representation and then passes through a layer of a fully connected network to generate channel attention weights. These weights are used to adjust the importance of each channel so that the model can automatically learn the correlation and importance between different channels, so as to make better use of channel information.\nThe spatial attention module is mainly used to weight the input spatial feature maps. It extracts spatial information at different scales by using channel-by-channel max pooling and average pooling and then passes through a fully connected network to generate spatial attention weights. These weights are used to adjust the importance of each spatial location, so that the model can adaptively focus on the details of different locations, thereby improving the model's perception of spatial changes[18, 19].\nBy combining channel attention and spatial attention, the CBAM attention mechanism can adaptively adjust the importance of different channels and spatial positions, thereby improving the model's ability to express image\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nH.Xin, K.Zhang: Surface Defect Detection with Channel-Spatial Attention Modules and Bi-Directional Feature Pyramid\n4 VOLUME XX, 2023\nfeatures. This enables the model to better capture the key information in the image, improving the accuracy and robustness of the task[20]. The specific structure of CBAM is shown in Fig. 1.\nAfter the introduction of CBAM, YOLOv5 added a CBAM module at the end of each residual unit of the basic backbone network and also added a CBAM module after the SPP (Spatial Pyramid Pooling) layer of the middle layer. This modification enables the model to better capture the dependencies between features and improve feature\nrepresentation and detection accuracy. And the detection head of YOLOv5 also adds a CBAM module, so that the model can adaptively learn the dependencies between features of different scales, and further improve the detection accuracy. In short, by introducing the CBAM attention mechanism, YOLOv5 improves the basic backbone network, intermediate layer, and detection head of the network, enabling the model to better capture the dependencies between features, and improve feature representation capabilities and detection accuracy."
        },
        {
            "heading": "B. BiFPN",
            "text": "The neck's primary function is to enhance object detection\naccuracy by fusing and aggregating features.\nThe neck in YOLOv5 helps improve the representation of the model by merging features from multiple layers with different spatial resolutions. However, the effectiveness of the neck depends heavily on the capability of the backbone network, which may impair its performance if it is not sufficiently capable. Also, necks face challenges in handling extreme scale variations in images and have difficulty adapting to the coexistence of objects of very different sizes. To address these shortcomings, we introduced BiFPN as a new Neck component, which has higher detection performance and fewer parameters than the previous FPN (Feature Pyramid Network) and PAN (Path Aggregation Network)[21]. Compared with FPN, BiFPN introduces a bidirectional feature propagation mechanism to better fuse and propagate feature information. The core idea of BiFPN is to propagate features between different levels through topdown and bottom-up paths. Specifically, BiFPN first constructs a top-down path that enhances the low-resolution feature maps to match the resolution of high-resolution features through upsampling operations. Then, it constructs a bottom-up path to fuse high-resolution features with adjacent low-resolution features through a feature fusion operation.\nThis enables BiFPN to efficiently propagate and fuse features at different scales. The advantage of BiFPN is its ability to handle multi-scale feature information and provide a richer semantic representation. Through the bi-directional feature propagation mechanism, BiFPN can make full use of the semantic information of the underlying and top-level features to improve the accuracy and robustness of object detection and semantic segmentation. It is particularly effective in dealing with small or large objects and capturing their details and contextual information more efficiently. The structure of FPN, the structure of PANet, and the structure of BiFPN are compared as shown in Fig. 2.\nIn YOLOv5, the head is composed of several layers that process the features extracted from the backbone network and produce the bounding box predictions along with the associated class probabilities. The convolutional layers in the head further refine the features extracted from the backbone network by applying spatial filtering and feature mapping. These layers are responsible for capturing more detailed and contextual information about the objects in the image. The fully connected layers, take the output of the convolutional layers and perform the necessary computations to generate the final bounding box predictions.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nH.Xin, K.Zhang: Surface Defect Detection with Channel-Spatial Attention Modules and Bi-Directional Feature Pyramid\nVOLUME XX, 2023 5\nThese computations involve adjusting the predicted bounding box coordinates based on anchor boxes, applying activation functions to produce class probabilities, and handling any required post-processing steps. The output layers of the head provide the final predictions in the form of bounding box coordinates, class probabilities, and confidence scores. These predictions are then used for object detection and localization tasks. The YOLOv5 architecture allows for flexibility in configuring the head based on specific requirements or variations of the YOLO framework. Different YOLOv5 versions, such as YOLOv5s, YOLOv5m, YOLOv5l, and YOLOv5x, have variations in the head\narchitecture, such as the number and size of the layers, to\naccommodate different trade-offs between accuracy and computational efficiency. Overall, the head of YOLOv5 plays a crucial role in generating accurate object detections by processing the extracted features and producing the final bounding box predictions and class probabilities."
        },
        {
            "heading": "C. Loss function",
            "text": "The loss function used in YOLOv5 is CIoU, which is an improved version of the traditional IoU (Intersection over Union) metric, and this experiment uses SIoU\u2014a new method that is further improved on the basis of CIoU. Compared with CIoU, SIoU is more stable when dealing with small objects, so in some tasks that need to deal with small objects, SIoU can often achieve better performance. Regarding the IoU loss function, the direction between the real frame and the predicted frame is not considered, resulting in slow convergence. For this, SIoU introduces the vector angle between the real frame and the predicted frame and redefines the relevant loss function. The principle of SIoU is:"
        },
        {
            "heading": "1) ANGLE LOSS",
            "text": "\ud835\udeec = 1 \u2212 2 \u2217 \ud835\udc60\ud835\udc56\ud835\udc5b2 (\ud835\udc4e\ud835\udc5f\ud835\udc50\ud835\udc60\ud835\udc56\ud835\udc5b(\ud835\udc65) \u2212 \ud835\udf0b\n4 ) (1)\nWhere Ch is the height difference between the center point of the real frame and the predicted frame, and \u03c3 is the distance between the center point of the real frame and the predicted frame.\n\ud835\udc65 = \ud835\udc36\u210e\n\ud835\udf0e = \ud835\udc60\ud835\udc56\ud835\udc5b(\ud835\udefc) (2)\n\ud835\udf0e = \u221a(\ud835\udc4f\ud835\udc50\ud835\udc65 \ud835\udc54\ud835\udc61 \u2212 \ud835\udc4f\ud835\udc50\ud835\udc65) 2 + (\ud835\udc4f\ud835\udc50\ud835\udc66 \ud835\udc54\ud835\udc61 \u2212 \ud835\udc4f\ud835\udc50\ud835\udc66) 2\n(3)\n\ud835\udc50\u210e = \ud835\udc5a\ud835\udc4e\ud835\udc65 (\ud835\udc4f\ud835\udc50\ud835\udc66 \ud835\udc54\ud835\udc61 , \ud835\udc4f\ud835\udc50\ud835\udc66) \u2212 \ud835\udc5a\ud835\udc56\ud835\udc5b (\ud835\udc4f\ud835\udc50\ud835\udc66 \ud835\udc54\ud835\udc61 , \ud835\udc4f\ud835\udc50\ud835\udc66) (4)\n(\ud835\udc4f\ud835\udc50\ud835\udc65 \ud835\udc54\ud835\udc61 , \ud835\udc4f\ud835\udc50\ud835\udc66 \ud835\udc54\ud835\udc61 ) is the center coordinate of the real frame; \uff08\ud835\udc4f\ud835\udc50\ud835\udc65\uff0c \ud835\udc4f\ud835\udc50\ud835\udc66\uff09is the center coordinate of the predicted frame, it can be noticed that when \u03b1 is \u03a0\n2 or 0, the angle loss is 0. In the\ntraining process, if \u03b1< \u03a0\n4 , then minimize \u03b1, otherwise\nminimize \u03b2."
        },
        {
            "heading": "2) DISTANCE LOSS",
            "text": "\ud835\udee5 = \u2211 (1 \u2212 \u2147\u2212\ud835\udf0c)\ud835\udc61 = 2 \u2212 \u2147 \u2212\ud835\udf0c \u2212 \u2147\u2212\ud835\udf0c (5)\n\ud835\udee5 = \u2211 (1 \u2212 \u2147\u2212 \ud835\udefe\ud835\udf0c\ud835\udc61)\ud835\udc61=\ud835\udc65,\ud835\udc66 = 2 \u2212 \u2147 \u2212 \ud835\udefe\ud835\udf0c\ud835\udc65 \u2212 \u2147\u2212 \ud835\udefe\ud835\udf0c\ud835\udc66 (6)\nIn which,\n\ud835\udf0c\ud835\udc65 = ( \ud835\udc4f\ud835\udc50\ud835\udc65\n\ud835\udc54\ud835\udc61 \u2212\ud835\udc4f\ud835\udc50\ud835\udc65\n\ud835\udc50\ud835\udc64 )\n2\n\ud835\udf0c\ud835\udc66 = ( \ud835\udc4f\ud835\udc50\ud835\udc66\n\ud835\udc54\ud835\udc61 \u2212\ud835\udc4f\ud835\udc50\ud835\udc66\n\ud835\udc50\u210e )\n2\n\u03b3 = 2 \u2212 \ud835\udeec (7)"
        },
        {
            "heading": "3) SHAPE LOSS",
            "text": "\ud835\udefa = \u2211 (1 \u2212 \u2147\u2212\ud835\udc64\ud835\udc61)\ud835\udf03 \ud835\udc61=\ud835\udc64,\u210e = (1 \u2212 \u2147\u2212\ud835\udc64\ud835\udc64 )\ud835\udf03 + (1 \u2212 \u2147\u2212\ud835\udc64\u210e )\ud835\udf03 (8)\n\ud835\udc64\ud835\udc64 = |\ud835\udc64\u2212\ud835\udc64\ud835\udc54\ud835\udc61|\n\ud835\udc5a\ud835\udc4e\ud835\udc65(\ud835\udc64,\ud835\udc64\ud835\udc54\ud835\udc61) , \ud835\udc64\u210e =\n|\u210e\u2212\u210e\ud835\udc54\ud835\udc61|\n\ud835\udc5a\ud835\udc4e\ud835\udc65(\u210e,\u210e\ud835\udc54\ud835\udc61) (9)\n4) IOU LOSS\n\ud835\udc3c\ud835\udc5c\ud835\udc48 = \ud835\udc3c\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc60\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\n\ud835\udc48\ud835\udc5b\ud835\udc56\ud835\udc5c\ud835\udc5b (10)\n\ud835\udc46\ud835\udc3c\ud835\udc5c\ud835\udc48 = 1 \u2212 \ud835\udc3c\ud835\udc5c\ud835\udc48 + \ud835\udee5+\ud835\udefa\n2 (11)\nSIoU does not consider the overlapping degree when matching object boxes, but only considers the distance between object boxes, so it has better stability when dealing with the matching between large objects and small objects. The improved loss function is more reasonable in design because it is defined on the distance between object boxes instead of the overlap, which makes it easier to optimize during training.\nThe structural changes of YOLOv5 after introducing the\nCBAM attention mechanism are shown in Fig. 5.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nH.Xin, K.Zhang: Surface Defect Detection with Channel-Spatial Attention Modules and Bi-Directional Feature Pyramid\n6 VOLUME XX, 2023"
        },
        {
            "heading": "III. Experimental Results and Analysis",
            "text": ""
        },
        {
            "heading": "A. DATASET",
            "text": "This experiment uses the steel surface defect data set of Northeastern University, which is an open and large-scale data set for surface defect detection, released by the research institute of Song Kechen's team[22] at Northeastern University. The dataset contains 1800 images of 6 defect types from steel surfaces defect, including crazing(CR), inclusion(IN), patches(PA), pitted_surface(PS), rolled-in scape(RS), and scratches(SC). The dataset contains highresolution raw images and corresponding annotation information for training and evaluating the performance of surface defect detection algorithms. This dataset has high diversity and complexity and is one of the important datasets in the field of surface defect detection.\nThrough the Python algorithm, 1260 images were randomly selected without repetition from 1800 pictures as the training set, 360 images were used for the validation set, and 180 images were used for the test set. The training set, validation set, and test set are 7:2:1, and the ratio of the number of labels in each category of the training set is shown in Fig.7.\nThe experimental configuration used in the experiment is\nshown in Table 1.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nH.Xin, K.Zhang: Surface Defect Detection with Channel-Spatial Attention Modules and Bi-Directional Feature Pyramid\nVOLUME XX, 2023 7\nTABLE 1. Experiment configuration\nExperiment configuration Version\nHardware accelerator Tesla T4\nTorch 1.13\nCUDA 11.6 Python 3.9"
        },
        {
            "heading": "B. OBJECTIVE EVALUATION OF RESULTS",
            "text": "Mean average precision (mAP) is one of the commonly used evaluation indicators in object detection, which is used to measure the detection accuracy of the model. Accuracy, that is the degree of overlap between the detection frame and the real object (the overlapping area divided by the union area), the commonly used calculation method is Intersection over Union (IoU)[23]. Then, for multiple objects in a test image, mAP is the average precision of all objects, which is calculated as follows: for each object category, the predicted boxes are sorted according to their confidence from high to low; then according to IoU The threshold setting, this paper uses the threshold of IoU of 0.5 as the evaluation index of the whole model (Map@0.5)[24], and matches the prediction frame with the real object, for each prediction frame, calculate its IoU value with the matched real object, if the IoU is greater than the set threshold, it is considered to be a correct detection; calculate the precision-recall curve (PR curve), and calculate the area under the curve, that is, AP (average precision); average the AP for all categories, That is mAP. The formula for calculating mAP is\uff0812\uff09.\n\ud835\udc5a\ud835\udc34\ud835\udc43 = 1\n\ud835\udc5b\ud835\udc56 \u2211 \ud835\udc34\ud835\udc43\ud835\udc56 \ud835\udc5b\ud835\udc56 \ud835\udc56=1\n(12)\nWhere n is the number of a category; APi is the detection accuracy of category i.\nIn addition, F1-score is also widely used in evaluating the detection results of models. In the object detection task, the F1 score can be used to evaluate the balance of the model's detection accuracy and recall rate for the objects. The precision and recall of each detection result can be calculated by comparing the degree of overlap between the detected objects and the ground truth. Then, the harmonic mean of precision and recall are calculated as the F1 score to comprehensively measure the performance of the model in object detection. For object detection tasks, precision represents the proportion of detected objects that are true objects, while recall represents the proportion of objects that the model can correctly detect to all actual objects. The F1 score combines these two metrics, enabling the model to detect objects accurately while capturing all objects as well as possible. By using the F1 score to evaluate the results of object detection, a comprehensive performance indicator can be obtained. A higher F1 score indicates that the model has achieved a better balance of accuracy and recall in the object detection task, while a lower F1 score it may mean that the model is deficient in one aspect and needs further improvement. Therefore, in the object detection task, the F1 score is an important evaluation index. It can\ncomprehensively consider the accuracy and recall rate of the model, provide a comprehensive evaluation of the model detection results, and have guiding significance for improving and optimizing the object detection model."
        },
        {
            "heading": "C. EXPERIMENTAL RESULTS",
            "text": "From the comparison of the above experimental results, it can be seen that the overall mAP @0.5 of the improved YOLOv5 has increased by 3.1%, Almost every item is better than before no improvement, especially the crazy class has increased by a full 10%. Unfortunately, overfitting can negatively impact the results when it comes to patch. This is an area that we will continue to investigate. Figures (a), (b), (c), (d), (e), (f), and (g) in Fig. 11 show the detection results of six types of defects, namely crazing, inclusion, patches, pitted surface, rolled-in scale, and scratches. The locations of the detected defects are marked with bounding boxes. It can be seen from the detection and label, in the original crazing defect, most of the methods did not detect the crazing defect, only the improved method detected it, and we proposed to completely detect the defect. In the roll-in scale defect, the detection results of various methods are quite different, but our improved method is closer to the real frame, which fully shows that the improved YOLOv5 network is more effective for defect detection. It is worth mentioning that our FPS reached 92 frames per second, while the original was 85 FPS.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nH.Xin, K.Zhang: Surface Defect Detection with Channel-Spatial Attention Modules and Bi-Directional Feature Pyramid\n\u2160\n(Original)\n\u2161\n\uff08SSD\uff09\n\u2162\n(CBAM)\n\u2163\n(YOLOv5)\n\u2164\n(BiFPN)\n\u2165\n(SIoU)\n\u2166\n(CBAM,BiFPN)\n\u2167\n(CBAM, SIoU)\n\u2168\n(BiFPN, SIoU)\n\u2169\n(Ours)\nH.Xin, K.Zhang: Surface Defect Detection with Channel-Spatial Attention Modules and Bi-Directional Feature Pyramid\nVOLUME XX, 2023 9\n\u216a\n(Label)\n(a) (b) (c) (d) (e) (f)\nFIGURE 9. (Continued.) The experimental results of various methods, the methods used are in brackets, and the last row is the dataset label visualization."
        },
        {
            "heading": "D. COMPARATIVE EXPERIMENT",
            "text": "In order to compare the effect of the improved part with other mainstream algorithms. The data set is divided according to the same standard, and Faster R-CNN algorithm,\nSSD algorithm and YOLO algorithm are used for comparative experiments. The results are shown in Table 2, it can be seen that the mAP of the SSD algorithm is only 54.5%, indicating that there is still a certain gap between the SSD algorithm and the improved algorithm, and the improved method is very effective, better than other mainstream algorithms, and improves the detection of surface defects by the algorithm performance. And the F1 score of the improved algorithm is also the best.\nTo further demonstrate the generalization of the improved YOLOv5, we conducted similar experiments on another publicly available surface defect detection dataset called GCDET (3000 images). Using the same experimental setup as before, our results showed as Fig.12-13, an improvement in detecting most types of defects with a 2.2% increase in mAP for the improved model. Notably, rolled_pit exhibited a more significant improvement with an increase of 8.2%. However,\nit is possible that the image preprocessing process for improving the model does not adequately cater to the inclusion category. This can lead to a loss of important details and a decrease in results specifically for this category. This experiment indicates that our proposed improved model not only achieved better performance on NEU-DET dataset but also demonstrated significant improvements on GC-DET dataset, highlighting its strong generalization ability."
        },
        {
            "heading": "E. ABLATION EXPERIMENTS",
            "text": "In order to explore the gain effect of each scheme in the improved method on the model, YOLOv5 was used as the basic model, and the ablation experiment was carried out on the data set. When other conditions remained unchanged, the following statistical data were obtained. It can be seen from the following Table 3, add The CBAM attention module and the network model using the BiFPN bidirectional feature pyramid increased mAP by 2.5% on the same data set, indicating that these improved methods can effectively improve the performance of the object detection algorithm, especially when dealing with surface defect scenes. The CBAM attention module can adaptively adjust the feature map and comprehensively express the information in the feature map, thereby improving the ability of feature extraction. The BiFPN can use different levels of feature information to improve the ability of feature expression, improve the accuracy and robustness of the detection algorithm, and thus improve the detection accuracy and efficiency of the model. SIoU is more stable when dealing with small objects and can speed up the convergence of the model. Although it can provide some gains, the gains it brings are limited.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nH.Xin, K.Zhang: Surface Defect Detection with Channel-Spatial Attention Modules and Bi-Directional Feature Pyramid\n10 VOLUME XX, 2023\nTABLE 3. Ablation experiment results\nCBAM BiFPN SIoU AP\nmAP CR IN PA PS RS SC\n0.413 0.871 0.959 0.797 0.527 0.896 0.744\n 0.439 0.874 0.927 0.826 0.542 0.922 0.755\n 0.481 0.882 0.913 0.827 0.532 0.931 0.761\n 0.412 0.868 0.942 0.793 0.548 0.925 0.748\n  0.503 0.893 0.915 0.834 0.547 0.922 0.769\n  0.417 0.879 0.949 0.800 0.567 0.898 0.752\n  0.494 0.888 0.945 0.795 0.539 0.941 0.767\n   0.513 0.903 0.918 0.829 0.545 0.942 0.775\n*The optimal data is bold;  indicates that the improved method is used; Please refer to page 6 for a list of abbreviations and their meanings."
        },
        {
            "heading": "IV. CONCLUSION",
            "text": "In summary, the improved YOLOv5 algorithm, incorporating the CBAM attention mechanism module and BiFPN, achieves promising results in detecting steel surface defects. It enables mAP to reach 77.5% at 92 FPS at almost the same cost, helping to reduce the rate of missed and false detections. This improvement addresses issues such as insufficient feature extraction capability. The CBAM attention mechanism enhances the network's focus on important features by learning attention weights from feature maps of different spatial dimensions, thereby improving model performance. Additionally, BiFPN efficiently extracts feature information at various scales to enhance object detection performance. However, there is still potential for further improvements in this project. Exploring different feature fusion methods and enhancing loss functions could be beneficial. Furthermore, testing the algorithms on larger and more diverse datasets would provide valuable insights. Moreover, it is worth investigating whether this proposed algorithm can be applied to other industrial inspection applications beyond steel surface defect detection. As research and development progress in this field, steel surface defect detection holds increasing promise."
        },
        {
            "heading": "ACKNOWLEDGMENT",
            "text": "We would like to express our sincere gratitude to all those who have contributed to this project.\nFirst and foremost, we would like to thank our supervisor for his guidance, encouragement, and support throughout the entire research process. His expertise and valuable feedback have been invaluable to our success.\nWe would like to acknowledge the various research institutions and organizations that provided us with the necessary resources, including access to equipment, software, and datasets, that were instrumental in the success of our project.\nFinally, we would like to thank our friends and family for their unwavering support and encouragement during this challenging process. Their patience, understanding, and\nencouragement were essential in helping us to stay motivated and focused on our research goals.\nWithout the contributions and support of these individuals and organizations, this project would not have been possible. We are deeply grateful for their assistance and look forward to continuing our work in this field."
        }
    ],
    "title": "Surface Defect Detection with Channel-Spatial Attention Modules and Bi-Directional Feature Pyramid",
    "year": 2023
}