{
    "abstractText": "Deep learning models have demonstrated great potential in medical imaging, but their development is limited by the expensive, large volume of annotated data required. Active learning (AL) addresses this by training a model on a subset of the most informative data samples without compromising performance. We compared different AL strategies and propose a framework that minimizes the amount of data needed for state-of-the-art performance. 638 multi-institutional brain tumor MRI images were used to train a 3D U-net model and compare AL strategies. We investigated uncertainty sampling, annotation redundancy restriction, and initial dataset selection techniques. Uncertainty estimation techniques including Bayesian estimation with dropout, bootstrapping, and margins sampling were compared to random query. Strategies to avoid annotation redundancy by removing similar images within the to-be-annotated subset were considered as well. We determined the minimum amount of data necessary to achieve similar performance to the model trained on the full dataset (\u03b1 = 0.1). A variance-based selection strategy using radiomics to identify the initial training dataset is also proposed. Bayesian approximation with dropout at training and testing showed similar results to that of the full data model with less than 20% of the training data (p=0.293) compared to random query achieving similar performance at 56.5% of the training data (p=0.814). Annotation redundancy restriction techniques achieved state-of-the-art performance at approximately 40%-50% of the training data. Radiomics dataset initialization had higher Dice with initial dataset sizes of 20 and 80 images, but improvements were not significant. In conclusion, we investigated various AL strategies with dropout uncertainty estimation achieving state-of-the-art performance with the least annotated data.",
    "authors": [
        {
            "affiliations": [],
            "name": "Daniel D Kim"
        },
        {
            "affiliations": [],
            "name": "Rajat S Chandra"
        },
        {
            "affiliations": [],
            "name": "Jian Peng"
        },
        {
            "affiliations": [],
            "name": "Jing Wu"
        },
        {
            "affiliations": [],
            "name": "Xue Feng"
        },
        {
            "affiliations": [],
            "name": "Michael Atalay"
        },
        {
            "affiliations": [],
            "name": "Chetan Bettegowda"
        },
        {
            "affiliations": [],
            "name": "Craig Jones"
        },
        {
            "affiliations": [],
            "name": "Haris Sair"
        },
        {
            "affiliations": [],
            "name": "Wei-hua Liao"
        },
        {
            "affiliations": [],
            "name": "Chengzhang Zhu"
        },
        {
            "affiliations": [],
            "name": "Beiji Zou"
        },
        {
            "affiliations": [],
            "name": "Li Yang"
        },
        {
            "affiliations": [],
            "name": "Anahita Fathi Kazerooni"
        },
        {
            "affiliations": [],
            "name": "Ali Nabavizadeh"
        },
        {
            "affiliations": [],
            "name": "Harrison X Bai"
        },
        {
            "affiliations": [],
            "name": "Zhicheng Jiao"
        }
    ],
    "id": "SP:4869d929e1ac6e2afa5d0009dc9432a382546287",
    "references": [
        {
            "authors": [
                "S Bakas",
                "M Reyes",
                "A Jakab"
            ],
            "title": "Identifying the Best Machine Learning Algorithms for Brain Tumor Segmentation, Progression Assessment, and Overall Survival Prediction in the BRATS Challenge. 2018:arXiv:1811.02629",
            "venue": "Accessed November",
            "year": 2018
        },
        {
            "authors": [
                "S Budd",
                "EC Robinson",
                "B. Kainz"
            ],
            "title": "A survey on active learning and human-in-the-loop deep learning for medical image analysis",
            "venue": "Med Image Anal. Jul 2021;71:102062",
            "year": 2021
        },
        {
            "authors": [
                "K Wang",
                "D Zhang",
                "Y Li",
                "R Zhang",
                "L. Lin"
            ],
            "title": "Cost-Effective Active Learning for Deep Image Classification. 2017:arXiv:1701.03551",
            "venue": "Accessed January",
            "year": 2017
        },
        {
            "authors": [
                "L Yang",
                "Y Zhang",
                "J Chen",
                "S Zhang",
                "DZ. Chen"
            ],
            "title": "Suggestive Annotation: A Deep Active Learning Framework for Biomedical Image Segmentation. 2017:arXiv:1706.04737",
            "venue": "Accessed June",
            "year": 2017
        },
        {
            "authors": [
                "Y Gal",
                "Z. Ghahramani"
            ],
            "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
            "venue": "2015:arXiv:1506.02142. Accessed June",
            "year": 2015
        },
        {
            "authors": [
                "Y Gal",
                "R Islam",
                "Z. Ghahramani"
            ],
            "title": "Deep Bayesian Active Learning with Image Data. 2017:arXiv:1703.02910",
            "venue": "Accessed March",
            "year": 2017
        },
        {
            "authors": [
                "A Smailagic",
                "HY Noh",
                "P Costa"
            ],
            "title": "MedAL: Deep Active Learning Sampling Method for Medical Image Analysis. 2018:arXiv:1809.09287",
            "venue": "Accessed September",
            "year": 2018
        },
        {
            "authors": [
                "ST Kim",
                "F Mushtaq",
                "N. Navab"
            ],
            "title": "Confident Coreset for Active Learning in Medical Image Analysis",
            "year": 2004
        },
        {
            "authors": [
                "KE Warren",
                "G Vezina",
                "TY Poussaint"
            ],
            "title": "Response assessment in medulloblastoma and leptomeningeal seeding tumors: recommendations from the Response Assessment in Pediatric Neuro-Oncology committee",
            "venue": "Neuro Oncol. Jan",
            "year": 2018
        },
        {
            "authors": [
                "D Sharma",
                "Z Shanis",
                "CK Reddy",
                "S Gerber",
                "A. Enquobahrie"
            ],
            "title": "Active Learning Technique for Multimodal Brain Tumor Segmentation Using Limited Labeled Images",
            "year": 2019
        },
        {
            "authors": [
                "J Wang",
                "Y Yan",
                "Y Zhang",
                "G Cao",
                "M Yang",
                "MK. Ng"
            ],
            "title": "Deep Reinforcement Active Learning for Medical Image Classification",
            "year": 2020
        },
        {
            "authors": [
                "H Li",
                "Z. Yin"
            ],
            "title": "Attention, Suggestion and Annotation: A Deep Active Learning Framework for Biomedical Image Segmentation",
            "year": 2020
        },
        {
            "authors": [
                "O Ronneberger",
                "P Fischer",
                "T. Brox"
            ],
            "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
            "venue": "2015:arXiv:1505.04597. Accessed May",
            "year": 2015
        },
        {
            "authors": [
                "J Peng",
                "DD Kim",
                "JB Patel"
            ],
            "title": "Deep Learning-Based Automatic Tumor Burden Assessment of Pediatric High-Grade Gliomas, Medulloblastomas, and Other Leptomeningeal Seeding Tumors",
            "venue": "Neuro Oncol. Jun",
            "year": 2021
        },
        {
            "authors": [
                "B. Settles"
            ],
            "title": "Active learning literature survey",
            "year": 2009
        },
        {
            "authors": [
                "JJM van Griethuysen",
                "A Fedorov",
                "C Parmar"
            ],
            "title": "Computational Radiomics System to Decode the Radiographic Phenotype",
            "venue": "Cancer Res. Nov",
            "year": 2017
        },
        {
            "authors": [
                "Y Li",
                "J Chen",
                "X Xie",
                "K Ma",
                "Y. Zheng"
            ],
            "title": "Self-Loop Uncertainty: A Novel Pseudo-Label for Semi-Supervised Medical Image Segmentation. 2020:arXiv:2007.09854",
            "venue": "Accessed July",
            "year": 2020
        },
        {
            "authors": [
                "F Last",
                "T Klein",
                "M Ravanbakhsh",
                "M Nabi",
                "K Batmanghelich",
                "V. Tresp"
            ],
            "title": "Human-Machine Collaboration for Medical Image Segmentation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "year": 2020
        },
        {
            "authors": [
                "L Venturini",
                "AT Papageorghiou",
                "JA Noble",
                "AIL. Namburete"
            ],
            "title": "Uncertainty Estimates as Data Selection Criteria to Boost Omni-Supervised Learning",
            "venue": "23rd International Conference,",
            "year": 2020
        },
        {
            "authors": [
                "D Mahapatra",
                "B Bozorgtabar",
                "J-P Thiran",
                "M. Reyes"
            ],
            "title": "Efficient Active Learning for Image Classification and Segmentation Using a Sample Selection and Conditional Generative Adversarial Network",
            "year": 2018
        },
        {
            "authors": [
                "H Wang",
                "Y Rivenson",
                "Y Jin"
            ],
            "title": "Deep learning enables cross-modality super-resolution in fluorescence microscopy",
            "venue": "Nature Methods",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "Active Learning in Brain Tumor Segmentation with Uncertainty Sampling, Annotation",
            "text": ""
        },
        {
            "heading": "Redundancy Restriction, and Data Initialization",
            "text": "Running Title: Active Learning in Brain Tumor Imaging\nDaniel D Kim1,2,\u2020, Rajat S Chandra3,\u2020, Jian Peng4,\u2020, Jing Wu4, Xue Feng5, Michael Atalay1,2, Chetan Bettegowda6, Craig Jones6,7, Haris Sair6, Wei-hua Liao4, Chengzhang Zhu8, Beiji Zou9, Li Yang4, Anahita Fathi Kazerooni10,11, Ali Nabavizadeh10,12, Harrison X Bai6, and Zhicheng Jiao1,2\n1 Warren Alpert Medical School of Brown University, Providence RI, USA 2 Department of Diagnostic Imaging, Rhode Island Hospital, Providence, RI, USA 3 Perelman School of Medicine at the University of Pennsylvania, Philadelphia, PA, USA 4 Second Xiangya Hospital of Central South University, Changsha, Hunan, China 5 Biomedical Engineering, University of Virginia, Charlottesville, VA, USA 6 Department of Radiology, Johns Hopkins University, Baltimore, MD, US 7 Department of Computer Science, Johns Hopkins University, Baltimore, MD, US 8 College of Literature and Journalism, Central South University, Changsha, China 9 School of Computer Science and Engineering, Central South University, Changsha, China 10 Center for Data-Driven Discovery in Biomedicine (D3b), Children\u2019s Hospital of Philadelphia,\nPhiladelphia, PA, USA 11 Department of Neurosurgery, Perelman School of Medicine, University of Pennsylvania,\nPhiladelphia, PA, USA 12 Department of Radiology, Perelman School of Medicine, University of Pennsylvania,\nPhiladelphia, PA, USA\n\u2020DDK, RSC, and JP contributed equally to this work and share co-first authorship"
        },
        {
            "heading": "Corresponding Author:",
            "text": "Zhicheng Jiao, Department of Radiology, Brown University\n593 Eddy St, Rhode Island Hospital, Department of Diagnostic Imaging, Providence, RI, 02903, USA zhicheng_jiao@brown.edu"
        },
        {
            "heading": "ABSTRACT",
            "text": "Deep learning models have demonstrated great potential in medical imaging, but their development is limited by the expensive, large volume of annotated data required. Active learning (AL) addresses this by training a model on a subset of the most informative data samples without compromising performance. We compared different AL strategies and propose a framework that minimizes the amount of data needed for state-of-the-art performance. 638 multi-institutional brain tumor MRI images were used to train a 3D U-net model and compare AL strategies. We investigated uncertainty sampling, annotation redundancy restriction, and initial dataset selection techniques. Uncertainty estimation techniques including Bayesian estimation with dropout, bootstrapping, and margins sampling were compared to random query. Strategies to avoid annotation redundancy by removing similar images within the to-be-annotated subset were considered as well. We determined the minimum amount of data necessary to achieve similar performance to the model trained on the full dataset (\u03b1 = 0.1). A variance-based selection strategy using radiomics to identify the initial training dataset is also proposed. Bayesian approximation with dropout at training and testing showed similar results to that of the full data model with less than 20% of the training data (p=0.293) compared to random query achieving similar performance at 56.5% of the training data (p=0.814). Annotation redundancy restriction techniques achieved state-of-the-art performance at approximately 40%-50% of the training data. Radiomics dataset initialization had higher Dice with initial dataset sizes of 20 and 80 images, but improvements were not significant. In conclusion, we investigated various AL strategies with dropout uncertainty estimation achieving state-of-the-art performance with the least annotated data."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Deep learning in medical imaging has made significant progress, achieving near or superior performance to that of human expert annotators.1 Despite the strides in performance, these models are limited by requiring substantial training data and annotations, which are expensive and timeconsuming to produce.2\nActive learning (AL) is a strategy that identifies a subset of unannotated data that would\nbe most informative so that a model can be trained on a subset of annotated samples without compromising performance. Models are built iteratively until acceptable performance is achieved.\nAL strategies generally have two approaches: (a) calculating uncertainty and annotating\nthe most uncertain or difficult images or (b) grouping images based on similarity and selecting a subset from each similarity group to identify a representative cohort.2 To identify uncertain images, Wang et. al uses a preliminary model to predict on unannotated data and assigns images with the smallest probability of the most probable class as most uncertain.3 An ensemble approach that identifies images with the most disagreement among models can also be used.4 Bayesian neural networks have alternatively been proposed to use one model to generate a probability distribution instead of a single probability, and wider distributions are attributed to higher uncertainty.5,6 To reduce annotation redundancy, Yang et. al compares the output from convolutional neural networks, which are ultimately high-level feature vectors, to assess the similarity of unannotated images and identify a representative set of images to annotate.3 Similarly, traditional computer vision techniques have also been used for feature extraction.7 Kim et. al combines both uncertainty and representativeness techniques when selecting data to annotate for skin lesion classification and segmentation.8\nMany AL strategies, including the ones above, focus on 2D imaging, classification tasks,\nor non-medical imaging.2-4,6,7 However, application of validated techniques onto 3D medical imaging, such as magnetic resonance imaging (MRI) or computerized tomography (CT), is not straightforward. Some medical imaging tasks have an additional complexity in that they focus on a small region of interest (ROI). Prognosis of brain cancer for example focuses on contrastenhancing tumor, which is much smaller than the whole brain.9 This characteristic is exacerbated in 3D imaging as uncertainty calculations need to focus on a small portion of voxels of interest, making them sensitive to noise from the substantial background. Sharma et. al demonstrates remarkable success here by combining least confidence uncertainty estimation and\nrepresentativeness to create a high performing model using less than 15% of the 2018 Brain Tumor Segmentation (BraTS) dataset.10 Other works have pursued active learning in 3D medical imaging by incorporating reinforcement learning rather than traditional uncertainty and representative strategies.11,12\nIn this paper, we contribute further to AL in 3D medical imaging with a pilot study by\ncomparing multiple uncertainty and representative techniques and evaluating their individual contribution in reducing the annotation burden on real-world, multi-institutional clinical brain tumor MRI data."
        },
        {
            "heading": "2 MATERIALS AND METHODS",
            "text": "2.1 Neural Network Architecture A 5-layered 3D U-net neural network architecture was used.13 Models used both contrast-enhanced T1-weighted (T1ce) and T2-weighted (T2) sequences to segment the contrast enhancing region. Two patches of size 128 \u00d7 128 \u00d7 128 biased 25% to the ROI from each image were used for model training. Data augmentation included scaling, rotation, and flipping transformations. Models optimized a soft Dice loss function on the validation set until there was no improvement for 50 epochs. During validation and testing, the full image was inputted. All models were trained using a 16 GB NVIDIA V100 Tensor Core graphical processing unit (GPU). Detailed hyperparameter settings are available in the published study on the full data model.14\n2.2 Active Learning Algorithm\nOur proposed methods to improve AL for 3D image segmentation consists of three major components: (1) uncertainty sampling, (2) annotation redundancy restriction, and (3) initial dataset selection (Figure 1).\nUncertainty sampling first uses a model trained on a smaller subset of data to predict\nsegmentations on unannotated data. Uncertainty is calculated from the predicted segmentations, and k images with the highest uncertainty are chosen for annotation. Uncertainty scores estimate the model\u2019s confidence on data that was not included in training. Three different uncertainty estimating techniques are outlined below.\nThe first technique involves bootstrapping. At each AL iteration, n bootstrapped datasets\nare generated by sampling with replacement and used to train a separate model. Higher variance in predictions across models suggests ensemble disagreement and higher uncertainty.4 The uncertainty score is the mean of the variance map of all of the probability maps returned from each bootstrapped model.\nNext, we discuss margins sampling. One model is trained on the current batch of annotated\ndata. Each voxel-probability within a probability map \ud835\udc5d! for image \ud835\udc56 can range from values from [0,1]. Voxel-probabilities closer to 0.5 are associated with higher uncertainty.15 Uncertainty score \ud835\udc62! calculation is described in Equation 1.\n\ud835\udc62! = \u2212(\ud835\udc5a\ud835\udc52\ud835\udc4e\ud835\udc5b( |\ud835\udc5d! \u2212 0.5| )) ( 1 )\nFinally, Bayesian models return a probability distribution rather than a single probability.\nThese models can be approximated by generating \ud835\udc5b predictions from a model that includes a dropout layer.5 Images with a wider distributions have higher uncertainty. A dropout layer is added to the last decoding convolutional layer in the 3D U-net model. Performance was compared when dropout was enabled during both training and testing versus only at testing. To localize uncertainty estimation to ROIs, the uncertainty score \ud835\udc62! was calculated by taking the mean of the top 0.1% variances of the probability maps, as shown in Equation 2 where \ud835\udc5d!! represents the \ud835\udc57th probability map returned by the model for image \ud835\udc56.\n\ud835\udc63\ud835\udc4e\ud835\udc5f! = var5p#! , \u22ef , p#\"9\n\ud835\udc61\ud835\udc5c\ud835\udc5d! = highest 0.1% of values in var#\nu# = \ud835\udc5a\ud835\udc52\ud835\udc4e\ud835\udc5b(\ud835\udc61\ud835\udc5c\ud835\udc5d!) ( 2 )\nWhile uncertainty sampling identifies images unfamiliar to the current model, annotation\nredundancy restriction prevents annotation of images similar to one another. The first annotation redundancy restriction method selects the most representative uncertain images. Consider a subset of \ud835\udc58 uncertain, unannotated images. If there are \ud835\udc57 images, where \ud835\udc57 < \ud835\udc58, similar to one another, then annotating only some of \ud835\udc57 images may be sufficient. To evaluate image similarity, we can compare the high-level features between two images. 3D U-net has both an encoder and decoder arm.13 The encoder arm uses multiple convolutional layers in series to generate an array of highlevel features. We modified Yang et al.\u2019s approach of using cosine similarity to compare arrays of high level features for 3D images.4 The encoder arm returns an 4D array of size (x, y, z, 512), where x, y, z are variable to the size of the input image. The 4D array is then flattened to a 1D array of size 512 by taking the mean across other axes. We can then measure the similarity between two images by calculating the similarity score \ud835\udc60\ud835\udc60(\ud835\udc3c! , \ud835\udc3c\") = \ud835\udc50\ud835\udc5c\ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc52_\ud835\udc60\ud835\udc56\ud835\udc5a\ud835\udc56\ud835\udc59\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc66(\u210e! , \u210e\") where \u210e! are flattened high-level features of image \ud835\udc56 and \u210e\" are that of image \ud835\udc57. Next, we use the maximum set cover approach to approximate a subset \ud835\udc46# that most represents the entire set of uncertain, unannotated images \ud835\udc46$ (Supplementary Algorithm 1).4 \ud835\udc46# begins as an empty set and is iteratively built to maximize the representativeness of \ud835\udc46# for \ud835\udc46$ . To quantify how much an image \ud835\udc65 \u2208 \ud835\udc46$ is already represented within \ud835\udc46# , we calculate \ud835\udc53(\ud835\udc46#, \ud835\udc65) = \ud835\udc5a\ud835\udc4e\ud835\udc65%\"\u2208'#\ud835\udc60\ud835\udc60(\ud835\udc65! , \ud835\udc65) . Next, we calculate \ud835\udc39(\ud835\udc46#, \ud835\udc46$) = \u2211 \ud835\udc53(\ud835\udc46#, \ud835\udc65!)%\"\u2208'$ to measure how much \ud835\udc46# represents \ud835\udc46$ . By iteratively adding the image from \ud835\udc46$ that maximizes \ud835\udc39(\ud835\udc46#, \ud835\udc46$) until \ud835\udc46# is of size \ud835\udc5a , we can estimate a subset \ud835\udc46# most representative for \ud835\udc46$.\nThe second redundancy restriction method selects uncertain images most non-similar to\nalready annotated data. Consider a subset of \ud835\udc58 uncertain, unannotated images where there are \ud835\udc57 images, where \ud835\udc57 < \ud835\udc58, similar to the images already annotated. Then, annotating images from \ud835\udc46\" may not be informative to the model. In order to get a subset \ud835\udc46# \u2282 \ud835\udc46$ that is most non-similar to the set of already annotated images \ud835\udc46( and images already selected to be annotated, we iteratively build \ud835\udc46# by comparing each image in \ud835\udc46$ to the images in \ud835\udc46( and those already in \ud835\udc46#, and add the one least similar to these images to \ud835\udc46# from \ud835\udc46$ (Supplementary Algorithm 2).\nThe final investigated AL technique determined the optimal images to initially annotate as\nopposed to random initialization. We used radiomics feature extraction to build an initial training set with the most diverse features. Radiomic feature extraction was performed with PyRadiomics,\nan open-source Python package for extracting radiomic features from medical images.16 The skullstripped brain region was used as the mask to focus feature extraction. First-order features, including mean, median, entropy, and energy, were extracted to build a feature array per image. Each feature array was normalized across all feature arrays. An initial dataset was then obtained by iteratively selecting the subset of images that maximizes the minimum Euclidean distance between normalized feature arrays similarly to Supplementary Algorithm 1.\n2.3 Experiments We retrospectively collected T2 and T1ce imaging sequences and clinical data from pediatric patients with intracranial leptomeningeal seeding brain tumors who were admitted to 4 large academic hospitals in Hunan Province, China from January 2011 to December 2018 and to the Children\u2019s Hospital of Philadelphia (CHOP) from January 2005 to December 2019. Exclusion criteria included patients above 18 years old, patients with missing pathological reports or image sequences, or if images were collected after any tumor-reducing treatment. The institutional review boards of all involved institutions approved this study, and the requirement for informed consent was waived.\nManual segmentation of contrast-enhancing tumor was performed by a neuro-oncologist\n(J.P.) with 7 years of experience, using the Level Tracing and Threshold tools in 3D Slicer (v.4.10). The MR acquisition parameters are shown in Supplementary Figures 1 and 2. 20% of the data was partitioned as the testing set. At each AL iteration, 20% of the annotated training data was used as the validation set. Images were skull stripped, resampled to isotropic resolution, and co-registered to the same anatomical template. For experimentation purposes, all images were proactively annotated for the contrast-enhancing lesion. However, we assign the images unannotated and annotated states based on the images selected by the AL algorithm. Only images with annotated states were used for model training.\nExperiment 1 compared different uncertainty sampling techniques. We initialized a\nrandom training set of 40 images and trained a preliminary model. We then annotated 50 of the most uncertain, unannotated images (~10% of entire dataset) returned by the uncertainty sampling technique and retrained the model. Model performance was evaluated at each AL iteration.\nExperiment 2 compared different annotation redundancy restriction techniques after\nuncertainty sampling. We initialized a random training set of 40 images and trained a preliminary\nmodel. Of the 100 most uncertain images, we annotated 50 of the most representative images to add to the training data. Because we were interested when the performance of the model is not different to that of the full data model, we use \u03b1 = 0.1 for Experiment 1 and 2.\nExperiment 3 compares initial dataset selection using radiomics features of the brain to\nrandom initialization. To generate a distribution of model performance trained on random dataset initialization, a model is trained and evaluated on n randomly selected images. This is repeated 10 times. Then, a model is trained on a dataset of size n determined by radiomics and compared in performance to the distribution of random initializations. As we are determining if the methods are statistically different, \u03b1 = 0.05 is used."
        },
        {
            "heading": "3 RESULTS",
            "text": "T2 and T1ce sequences with contrast enhancing tumor segmentation were available for 683 brain\nMRIs from 683 patients (85 Hunan, 598 CHOP). 39 patients were excluded due to skull stripping failure, and 6 were excluded due to co-registration failure. Characteristics for the remaining 638 patients can be found in Table 1. Each model took approximately 1-5 hours to train depending on the AL iteration or size of the annotated training data. The pre-trained models and the AL framework is publicly accessible at https://github.com/naddan27/ActiveLearning/.\nFigure 2 compares the mean and median Dice scores of the uncertainty estimation\ntechniques at different percentages of the training data from Experiment 1. Performance of the full data model are taken from its previously published study.14 Both bootstrapping and Bayesian approximation using dropout at training and testing (dropout traintest) consistently outperformed\nFigure 2. Dice scores of uncertainty techniques at different percentages of training data. Horizontal dashed line is performance with full training data. Error bars represent the standard error.\nrandom query. In contrast, margins and Bayesian approximation using dropout only at testing (dropout test) performed worse than random query. For bootstrapping, there was no significant difference in model performance trained with 27.3% (p = 0.890) and 27.3% (p = 0.874) of the data versus trained with the full data for mean and median Dice, respectively. For dropout traintest, there was no significant difference in model performance trained with 17.5% (p = 0.293) and 17.5% (p = 0.108) of the data versus trained with the full data for mean and median Dice, respectively.\nIn Experiment 2 comparing annotation redundancy restriction techniques, dropout traintest\nwas used to identify the uncertain, unannotated images before annotation redundancy restriction due to its performance in Experiment 1 and smaller computational burden than bootstrapping. Table 2 shows the effect of adding an annotation redundancy restriction technique on top of uncertainty sampling versus just uncertainty sampling alone. While both redundancy restriction techniques achieved similar performance to that of the full model before random query, both redundancy restriction techniques were not able to outperform AL strategies that solely used uncertainty sampling. Figure 3 shows examples of predicted segmentations by models that were built using uncertainty and annotation redundancy restriction AL strategies.\nIn Experiment 3 evaluating for a radiomics based initial dataset selection, radiomics dataset\ninitialization had higher Dice at n=20 and 80 than random query, but improvements were not significant. Full comparisons are shown in Table 3."
        },
        {
            "heading": "4 DISCUSSION",
            "text": "We compared multiple uncertainty and representative techniques and evaluated their individual and synergistic performance in reducing the number of annotations needed on real-world, multiinstitutional clinical data. We show that an AL framework using Bayesian approximation with dropout at training and testing can achieve state-of-the-art performance with less than 20% of the\nFigure 3. Examples of predicted (blue) vs expert (red) segmentations at 17.5% of training data with random query, dropout traintest, bootstrapping, redundancy representative, and redundancy non-similar in order. Dice scores are shown for each\nrespective image.\ntraining data. Comparatively, AL with only random query achieved full training data model performance at 56.5% of the training data.\nWe compared 4 different uncertainty estimation techniques to random query: bootstrapping,\nmargins, dropout traintest, and dropout test. While bootstrapping did reduce training data by 70%, its computational demand can be prohibitive. We were therefore interested in using dropout as an alternative. The primary concern with dropout was that it would not be able to generate distinct enough predictions to generate a reliable uncertainty score.2,6 Furthermore, the prediction variability, which is generally concentrated at the ROI, would be diluted by the substantial number of background voxels in 3D imaging. To address this concern, we presented a dropout strategy that focuses on regions of high disagreement within the image to estimate uncertainty. With this strategy, we show that dropout is generalizable to AL in 3D segmentation tasks and in fact superior to bootstrapping. We also attempted removing dropout training stabilization by implementing dropout only at testing to force more diverse predictions. However, model prediction instability had a stronger negative effect in model performance than the possible positive effects of having diverse predictions for uncertainty estimation as demonstrated by the consistently lower than random query performance even with larger percentages of the training data. Results also show that uncertainty estimation may require calibration given the significant amount of noise contributed by the background voxels in 3D imaging. This may explain margins sampling performing worse than random query in our paper despite other studies showing better performance on 2D imaging.2\nWe were also interested in reducing annotation redundancy. While these techniques were\nable to achieve similar performance to that of the model trained on the full data with approximately 40%-50% of the training data, they were not able to outperform AL strategies that only incorporate uncertainty. Adding a redundancy restriction strategy can bias training away from uncertain images. Future projects may add a way to prioritize more uncertain images within the representative cohort. Uncertain images can be clustered based on similarity, where each cluster is assigned an overall uncertainty. While only a subset of images from each cluster are selected for annotation, training can be biased toward the uncertain images by artificially increasing images from uncertain clusters with data transformations or generative adversarial networks.17-21\nLastly, we evaluate using radiomics features of the brain to construct the initial dataset. At\nvarious initial dataset sizes, the performance of the radiomics strategy was not statistically different\nfrom random query, demonstrating that selecting an initial dataset based on high level radiomic features of the brain do not translate to selecting images with diverse tumor characteristics. This is understandable given that radiomic features are very dependent on acquisition parameters and heterogeneity in age groups and histologies. An alternative strategy may be to only consider voxels above a specific intensity within the brain mask to vaguely localize the ROI, though this only applies to tasks that segment contrast-enhancing ROI. This AL strategy may be more applicable for models that accept multiple organ systems or imaging modalities to initialize a dataset balanced in all imaging modalities and organ systems of interest.\nThe major strength of our study is its demonstration of the specific reduction in annotation\nburden of each AL technique when applied by itself or when combined with multiple techniques. Recent studies have used combined uncertainty and representative approaches,8,10 but as demonstrated in our uncertainty experiments, AL frameworks are sensitive to calibration when applied to real-world, clinical imaging. By doing a detailed analysis of each technique on annotation burden reduction, our study can guide future studies that combine AL techniques. Furthermore, given the calibration sensitivity and need to optimize hyperparameters at each iteration, our study suggests that AL frameworks may benefit from an adaptative strategy as AL iterations are added. Wang et. al incorporates reinforcement learning with Markov models to create an adaptive AL framework for example,11 and our study can be used to understand the adaptive strategies returned by reinforcement learning strategies in future studies.\nOur study does have limitations. First, we purposefully used the hyperparameters\noptimized for the full data model on all AL models to address hyperparameter confounding bias, and therefore, results at each iteration may be lower than if they were trained with hyperparameters optimized for each iteration. We assumed performance would be similarly affected for each iteration. Additionally, we only focus on the top 0.1% of variances for the dropout methods when comparing the uncertainty techniques, while the bootstrapping strategy incorporates all variances, making it more sensitive to background voxel noise. Bootstrapping was tested before the incorporation of focusing on the highest variance voxels, and therefore, bootstrapping was not repeated with this technique given that dropout strategy was able to perform similarly to bootstrapping with a significantly lower computational demand. Furthermore, we randomly split the annotated data into the training and validation set at each AL iteration rather than having a consistent validation set at each iteration. Experiments were designed as such with the thought that\nalready deployed AL models should continuously look for more informative samples to add to the training data as time passes and more imaging is available. However, this may bias and overfit models to the training data, hindering model performance at larger AL iterations. Lastly, our generalizability of AL techniques onto real-world, clinical data uses a single dataset and single task of brain tumor segmentation. Further studies with different datasets and tasks are needed before AL strategies can be confidently applied onto real-world data."
        },
        {
            "heading": "5 CONCLUSIONS",
            "text": "In conclusion, we assess multiple AL methods and demonstrate their applicability onto real-world clinical 3D brain tumor segmentation. We compare various AL uncertainty estimation as well as annotation redundancy restriction and initial dataset selection strategies, finding that a dropout uncertainty estimation framework is optimal."
        },
        {
            "heading": "FIGURE LEGEND",
            "text": "Figure 1. Workflow of the full active learning framework\nFigure 2. Dice scores of uncertainty techniques at different percentages of training data. Horizontal dashed line is performance with full training data. Error bars represent the standard error.\nFigure 3. Examples of predicted (blue) vs expert (red) segmentations at 17.5% of training data with random query, dropout traintest, bootstrapping, redundancy representative, and redundancy non-similar in order. Dice scores are shown for each respective image."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "Authors declare no acknowledgements. All participating parties gave significant contributions for project development, data availability, annotation, and interpretation and are listed as authors."
        },
        {
            "heading": "FUNDING",
            "text": "This project was supported by Alpert Medical School Summer Assistantship award to DDK. This work was supported by National Science Foundation of Hunan Province, China (2022JJ30762), International Science and Technology Innovation Joint Base of Machine Vision and Medical Image Processing in Hunan Providence, China (2021CB1013), and the 111 project (B18059) to CZ. This work was supported by Hunan Province Key Areas Research and Development Program, China (2022SK2054) to BZ. This work was supported by Huxiang High-level Talent Gathering\nProject-Innovation Talent, China (2021RC5003) to WL. This project was supported by the National Cancer Institute (NCI) of the National Institutes of Health under Award Number R03CA235202 to HXB. This work was supported by the Natural Science Foundation of China (81971696 to LY), Natural Science Foundation of Hunan Province (2022JJ30861 to LY), and Sheng Hua Yu Ying Project of Central South University to LY."
        },
        {
            "heading": "CONFLICTS OF INTEREST",
            "text": "Authors declare no conflict of interest."
        },
        {
            "heading": "DATA AVAILABILITY",
            "text": "Data used is not publicly available for patient confidentiality. Interested parties may contact authors regarding questions on data."
        },
        {
            "heading": "6 REFERENCES",
            "text": "1. Bakas S, Reyes M, Jakab A, et al. Identifying the Best Machine Learning Algorithms for Brain Tumor Segmentation, Progression Assessment, and Overall Survival Prediction in the BRATS Challenge. 2018:arXiv:1811.02629. Accessed November 01, 2018. https://ui.adsabs.harvard.edu/abs/2018arXiv181102629B 2. Budd S, Robinson EC, Kainz B. A survey on active learning and human-in-the-loop deep learning for medical image analysis. Med Image Anal. Jul 2021;71:102062. doi:10.1016/j.media.2021.102062 3. Wang K, Zhang D, Li Y, Zhang R, Lin L. Cost-Effective Active Learning for Deep Image Classification. 2017:arXiv:1701.03551. Accessed January 01, 2017. https://ui.adsabs.harvard.edu/abs/2017arXiv170103551W 4. Yang L, Zhang Y, Chen J, Zhang S, Chen DZ. Suggestive Annotation: A Deep Active Learning Framework for Biomedical Image Segmentation. 2017:arXiv:1706.04737. Accessed June 01, 2017. https://ui.adsabs.harvard.edu/abs/2017arXiv170604737Y 5. Gal Y, Ghahramani Z. Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. 2015:arXiv:1506.02142. Accessed June 01, 2015. https://ui.adsabs.harvard.edu/abs/2015arXiv150602142G 6. Gal Y, Islam R, Ghahramani Z. Deep Bayesian Active Learning with Image Data. 2017:arXiv:1703.02910. Accessed March 01, 2017. https://ui.adsabs.harvard.edu/abs/2017arXiv170302910G 7. Smailagic A, Noh HY, Costa P, et al. MedAL: Deep Active Learning Sampling Method for Medical Image Analysis. 2018:arXiv:1809.09287. Accessed September 01, 2018. https://ui.adsabs.harvard.edu/abs/2018arXiv180909287S 8. Kim ST, Mushtaq F, Navab N. Confident Coreset for Active Learning in Medical Image Analysis. arXiv preprint arXiv:200402200. 2020; 9. Warren KE, Vezina G, Poussaint TY, et al. Response assessment in medulloblastoma and leptomeningeal seeding tumors: recommendations from the Response Assessment in Pediatric Neuro-Oncology committee. Neuro Oncol. Jan 10 2018;20(1):13-23. doi:10.1093/neuonc/nox087 10. Sharma D, Shanis Z, Reddy CK, Gerber S, Enquobahrie A. Active Learning Technique for Multimodal Brain Tumor Segmentation Using Limited Labeled Images. Springer International Publishing; 2019:148-156. 11. Wang J, Yan Y, Zhang Y, Cao G, Yang M, Ng MK. Deep Reinforcement Active Learning for Medical Image Classification. Springer International Publishing; 2020:33-42. 12. Li H, Yin Z. Attention, Suggestion and Annotation: A Deep Active Learning Framework for Biomedical Image Segmentation. Springer International Publishing; 2020:3-13. 13. Ronneberger O, Fischer P, Brox T. U-Net: Convolutional Networks for Biomedical Image Segmentation. 2015:arXiv:1505.04597. Accessed May 01, 2015. https://ui.adsabs.harvard.edu/abs/2015arXiv150504597R\n14. Peng J, Kim DD, Patel JB, et al. Deep Learning-Based Automatic Tumor Burden Assessment of Pediatric High-Grade Gliomas, Medulloblastomas, and Other Leptomeningeal Seeding Tumors. Neuro Oncol. Jun 26 2021;doi:10.1093/neuonc/noab151 15. Settles B. Active learning literature survey. 2009; 16. van Griethuysen JJM, Fedorov A, Parmar C, et al. Computational Radiomics System to Decode the Radiographic Phenotype. Cancer Res. Nov 1 2017;77(21):e104-e107. doi:10.1158/0008-5472.Can-17-0339 17. Li Y, Chen J, Xie X, Ma K, Zheng Y. Self-Loop Uncertainty: A Novel Pseudo-Label for Semi-Supervised Medical Image Segmentation. 2020:arXiv:2007.09854. Accessed July 01, 2020. https://ui.adsabs.harvard.edu/abs/2020arXiv200709854L 18. Last F, Klein T, Ravanbakhsh M, Nabi M, Batmanghelich K, Tresp V. Human-Machine Collaboration for Medical Image Segmentation. ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2020:1040-1044. 19. Venturini L, Papageorghiou AT, Noble JA, Namburete AIL. Uncertainty Estimates as Data Selection Criteria to Boost Omni-Supervised Learning. presented at: Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI 2020: 23rd International Conference, Lima, Peru, October 4\u20138, 2020, Proceedings, Part I; 2020; Lima, Peru. https://doi.org/10.1007/978-3-030-59710-8_67 20. Mahapatra D, Bozorgtabar B, Thiran J-P, Reyes M. Efficient Active Learning for Image Classification and Segmentation Using a Sample Selection and Conditional Generative Adversarial Network. Springer International Publishing; 2018:580-588. 21. Wang H, Rivenson Y, Jin Y, et al. Deep learning enables cross-modality super-resolution in fluorescence microscopy. Nature Methods. 2019/01/01 2019;16(1):103-110. doi:10.1038/s41592-018-0239-0\nSupplementary Figure 1. Magnetic field strength and slice thickness of (A) CHOP for T2 and T1contrast enhanced images\nSupplementary Figure 2. (A) Echo time for T2 and T1-contrast enhanced MRI images of CHOP; (B) Repetition time for T2 and T1-contrast enhanced MRI images of CHOP\nSupplemental Algorithm 1 Representative subset within uncertain, unannotated images"
        },
        {
            "heading": "1: Initialize Sm \u2190{}",
            "text": "2: Initialize Sk \u2190 set of k unannotated images with highest uncertainty"
        },
        {
            "heading": "3: Move random image from Sk to Sm",
            "text": "4: while size(Sm) < m do\n5: Set Fmax \u2190 0, xmax \u2190 none 6: for all images xi \u2208 Sk do\n7: Temporarily move xi from Sk to Sm\nA\nB\n8: Calculate F(Sm,Sk)\n9: if F(Sm,Sk) > Fmax then\n10: Fmax \u2190 F(Sm,Sk)\n11: xmax \u2190 xi\n12: end if\n13: Move xi back from Sm to Sk\n14: end for\n15: Move xmax from Sk to Sm\n16: end while\nSupplemental Algorithm 2 Subset of uncertain unannotated images non-similar to already\nannotated images"
        },
        {
            "heading": "1: Initialize Sm \u2190{}",
            "text": "2: Initialize Sk \u2190 set of k unannotated images with highest uncertainty"
        },
        {
            "heading": "3: Move random image from Sk to Sm",
            "text": "4: while size(Sm) < m do\n5: Set ss_summin \u2190 +\u221e, xmin \u2190 none 6: for all images xi \u2208 Sk do"
        },
        {
            "heading": "7: similarity_sum \u2190 0",
            "text": "8: for all images xj \u2208 (Sa \u222a Sm) do\n9: similarity_sum += ss(xi, xj)\n10: end for\n11: if similarity_sum < ss_summin then\n12: ss_summin \u2190 similarity_sum\n13: xmin \u2190 xi\n14: end if\n14: end for\n15: Move xmin from Sk to Sm\n16: end while"
        }
    ],
    "title": "Active Learning in Brain Tumor Segmentation with Uncertainty Sampling, Annotation Redundancy Restriction, and Data Initialization Running Title: Active Learning in Brain Tumor Imaging",
    "year": 2023
}