{
    "abstractText": "Many real-life signal-based applications use the Tucker decomposition of a high dimensional/order tensor. A well-known problem with the Tucker model is that its number of entries increases exponentially with its order, a phenomenon known as the \u201ccurse of the dimensionality\u201d. The Higher-Order Orthogonal Iteration (HOOI) and Higher-Order Singular Value Decomposition (HOSVD) are known as the gold standard for computing the range span of the factor matrices of a Tucker Decomposition but also suffer from the curse. In this paper, we propose a new methodology with a similar estimation accuracy as the HOSVD with non-exploding computational and storage costs. If the noise-free data follows a Tucker decomposition, the corresponding Tensor Train (TT) decomposition takes a remarkable specific structure. More precisely, we prove that for a Q-order Tucker tensor, the corresponding TT decomposition is constituted by Q \u2212 3 3-order TT-core tensors that follow a Constrained Canonical Polyadic Decomposition. Using this new formulation and the coupling property between neighboring TTcores, we propose a JIRAFE-type scheme for the Tucker decomposition, called TRIDENT. Our numerical simulations show that the proposed method offers a drastically reduced complexity compared to the HOSVD and HOOI while outperforming the Fast Multilinear Projection (FMP) method in terms of estimation accuracy.",
    "authors": [
        {
            "affiliations": [],
            "name": "Maxence Giraud"
        },
        {
            "affiliations": [],
            "name": "Vincent Itier"
        },
        {
            "affiliations": [],
            "name": "Remy Boyer"
        },
        {
            "affiliations": [],
            "name": "Yassine Zniyed"
        },
        {
            "affiliations": [],
            "name": "Andr\u00e9 de Almeida"
        },
        {
            "affiliations": [],
            "name": "R\u00e9my Boyer"
        },
        {
            "affiliations": [],
            "name": "L.F. de Almeida"
        }
    ],
    "id": "SP:a687f50c8c3e0e95b39d5dd42c76b5b77c4b8f1d",
    "references": [
        {
            "authors": [
                "N.D. Sidiropoulos",
                "L. De Lathauwer",
                "X. Fu",
                "K. Huang",
                "E.E. Papalexakis",
                "C. Faloutsos"
            ],
            "title": "Tensor decomposition for signal processing and machine learning",
            "venue": "IEEE Transactions on Signal Processing, vol. 65, no. 13, pp. 3551\u20133582, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Zniyed",
                "R. Boyer",
                "A.L.F. de Almeida",
                "G. Favier"
            ],
            "title": "Tensor-train modeling for mimo-ofdm tensor coding-and-forwarding relay systems",
            "venue": "2019 27th European Signal Processing Conference (EUSIPCO), 2019, pp. 1\u20135.",
            "year": 2019
        },
        {
            "authors": [
                "A. Cichocki",
                "D. Mandic",
                "L. De Lathauwer",
                "G. Zhou",
                "Q. Zhao",
                "C. Caiafa",
                "H.A. PHAN"
            ],
            "title": "Tensor decompositions for signal processing applications: From two-way to multiway component analysis",
            "venue": "IEEE Signal Processing Magazine, vol. 32, no. 2, pp. 145\u2013163, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "A. Cichocki",
                "D. Mandic",
                "L. De Lathauwer",
                "G. Zhou",
                "Q. Zhao",
                "C. Caiafa",
                "H.A. Phan"
            ],
            "title": "Tensor decompositions for signal processing applications: From two-way to multiway component analysis",
            "venue": "IEEE Signal Processing Magazine, vol. 32, no. 2, pp. 145\u2013163, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "A.L.F. de Almeida",
                "G. Favier",
                "J.C.M. Mota"
            ],
            "title": "A constrained factor decomposition with application to mimo antenna systems",
            "venue": "IEEE Transactions on Signal Processing, vol. 56, no. 6, pp. 2429\u20132442, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "G.T. de Ara\u00fajo",
                "A.L.F. de Almeida",
                "R. Boyer"
            ],
            "title": "Channel estimation for intelligent reflecting surface assisted mimo systems: A tensor modeling approach",
            "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 15, no. 3, pp. 789\u2013802, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "N. Sidiropoulos",
                "R. Bro",
                "G. Giannakis"
            ],
            "title": "Parallel factor analysis in sensor array processing",
            "venue": "IEEE Transactions on Signal Processing, vol. 48, no. 8, pp. 2377\u20132388, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "H. Zheng",
                "C. Zhou",
                "Z. Shi",
                "Y. Gu",
                "Y.D. Zhang"
            ],
            "title": "Coarray tensor direction-of-arrival estimation",
            "venue": "IEEE Transactions on Signal Processing, vol. 71, pp. 1128\u20131142, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "H. Zheng",
                "C. Zhou",
                "Z. Shi",
                "A.L.F. de Almeida"
            ],
            "title": "Subttd: Doa estimation via sub-nyquist tensor train decomposition",
            "venue": "IEEE Signal Processing Letters, vol. 29, pp. 1978\u20131982, 2022.",
            "year": 1978
        },
        {
            "authors": [
                "C.I. Kanatsoulis",
                "X. Fu",
                "N.D. Sidiropoulos",
                "W.-K. Ma"
            ],
            "title": "Hyperspectral super-resolution: Combining low rank tensor and matrix structure",
            "venue": "2018 25th IEEE International Conference on Image Processing (ICIP). IEEE, 2018, pp. 3318\u20133322.",
            "year": 2018
        },
        {
            "authors": [
                "R.A. Borsoi",
                "C. Pr\u00e9vost",
                "K. Usevich",
                "D. Brie",
                "J.C.M. Bermudez",
                "C. Richard"
            ],
            "title": "Coupled tensor decomposition for hyperspectral and multispectral image fusion with inter-image variability",
            "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 15, no. 3, pp. 702\u2013717, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Signoretto",
                "L. De Lathauwer",
                "J.A. Suykens"
            ],
            "title": "A kernel-based framework to tensorial data analysis",
            "venue": "Neural networks, vol. 24, no. 8, pp. 861\u2013874, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "Y. Panagakis",
                "J. Kossaifi",
                "G.G. Chrysos",
                "J. Oldfield",
                "M.A. Nicolaou",
                "A. Anandkumar",
                "S. Zafeiriou"
            ],
            "title": "Tensor methods in computer vision and deep learning",
            "venue": "Proceedings of the IEEE, vol. 109, no. 5, pp. 863\u2013 890, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "F.L. Hitchcock"
            ],
            "title": "The expression of a tensor or a polyadic as a sum of products",
            "venue": "Journal of Mathematics and Physics, vol. 6, no. 1-4, pp. 164\u2013189, 1927.",
            "year": 1927
        },
        {
            "authors": [
                "L.R. Tucker"
            ],
            "title": "Some mathematical notes on three-mode factor analysis",
            "venue": "Psychometrika, vol. 31, no. 3, pp. 279\u2013311, 1966.",
            "year": 1966
        },
        {
            "authors": [
                "T.G. Kolda",
                "B.W. Bader"
            ],
            "title": "Tensor decompositions and applications",
            "venue": "SIAM review, vol. 51, no. 3, pp. 455\u2013500, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "N. Vervliet",
                "O. Debals",
                "L. Sorber",
                "L. De Lathauwer"
            ],
            "title": "Breaking the curse of dimensionality using decompositions of incomplete tensors: Tensor-based scientific computing in big data analysis",
            "venue": "IEEE Signal Processing Magazine, vol. 31, no. 5, pp. 71\u201379, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "A.-H. Phan",
                "A. Cichocki",
                "A. Uschmajew",
                "P. Tichavsk\u00fd",
                "G. Luta",
                "D.P. Mandic"
            ],
            "title": "Tensor networks for latent variable analysis: Novel algorithms for tensor train approximation",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 31, no. 11, pp. 4622\u20134636, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Cichocki",
                "N. Lee",
                "I.V. Oseledets",
                "A.-H. Phan",
                "Q. Zhao",
                "D. Mandic"
            ],
            "title": "Low-Rank Tensor Networks for Dimensionality Reduction and Large-Scale Optimization Problems: Perspectives and Challenges PART 1",
            "venue": "vol. 9, no. 4-5, pp. 249\u2013429."
        },
        {
            "authors": [
                "I.V. Oseledets"
            ],
            "title": "Tensor-train decomposition",
            "venue": "SIAM Journal on Scientific Computing, vol. 33, no. 5, pp. 2295\u20132317, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "Y. Liu"
            ],
            "title": "Tensors for Data Processing. Theory, Methods, and Applications",
            "year": 2022
        },
        {
            "authors": [
                "Y. Liu",
                "J. Liu",
                "Z. Long",
                "C. Zhu"
            ],
            "title": "Tensor computation for data analysis",
            "year": 2022
        },
        {
            "authors": [
                "A. Cichocki"
            ],
            "title": "Era of big data processing: A new approach via tensor networks and tensor decompositions",
            "venue": "arXiv preprint arXiv:1403.2048, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "Y. Zniyed",
                "R. Boyer",
                "A.L. de Almeida",
                "G. Favier"
            ],
            "title": "High-order tensor estimation via trains of coupled third-order CP and Tucker decompositions",
            "venue": "Linear Algebra and its Applications, vol. 588, pp. 304\u2013 337, Mar. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "R.A. Harshman"
            ],
            "title": "Foundations of the PARAFAC procedure: models and conditions for an \u201cexplanatory\u201d multi-modal factor analysis",
            "venue": "UCLA Working Papers in Phonetics, vol. 16, no. 1, p. pages, 1970.",
            "year": 1970
        },
        {
            "authors": [
                "A.L. de Almeida",
                "G. Favier",
                "J.C.M. Mota"
            ],
            "title": "PARAFAC-based unified tensor modeling for wireless communication systems with application to blind multiuser equalization",
            "venue": "Signal Processing, vol. 87, no. 2, pp. 337\u2013351, Feb. 2007.",
            "year": 2007
        },
        {
            "authors": [
                "L.D. Lathauwer",
                "B.D. Moor",
                "J. Vandewalle"
            ],
            "title": "On the Best Rank- 1 and Rank-(R1,R2,. . .,RN) Approximation of Higher-Order Tensors",
            "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 21, no. 4, pp. 1324\u20131342, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "L. De Lathauwer",
                "B. De Moor",
                "J. Vandewalle"
            ],
            "title": "A multilinear singular value decomposition",
            "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 21, no. 4, pp. 1253\u20131278, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "Y. Zniyed",
                "R. Boyer",
                "A.L.F. de Almeida",
                "G. Favier"
            ],
            "title": "A tt-based hierarchical framework for decomposing high-order tensors",
            "venue": "SIAM Journal on Scientific Computing, vol. 42, no. 2, pp. A822\u2013A848, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A.-H. Phan",
                "P. Tichavsk\u00fd",
                "A. Cichocki"
            ],
            "title": "Fast alternating LS algorithms for high order candecomp/parafac tensor factorizations",
            "venue": "IEEE Transactions on Signal Processing, vol. 61, no. 19, pp. 4834\u20134846, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "C. Battaglino",
                "G. Ballard",
                "T.G. Kolda"
            ],
            "title": "A practical randomized CP tensor decomposition",
            "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 39, no. 2, pp. 876\u2013901, 2018.",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Tensor, Tucker decomposition, Constrained CPD, Tensor Train, Multilinear Algebra\nI. INTRODUCTION\nLow-rank tensor decompositions are increasingly used to solve a variety of difficult problems with multilinear data, and this in various areas such as signal processing and telecommunications [1], [2], [3], [4], [5], [6], [7], [8], [9], hyperspectral image reconstruction [10], [11] and machine learning [1], [12], [13]. A popular generalization of the Singular Value Decomposition (SVD) [14] to tensors is the Tucker Decomposition [15], [16], [17]. Large-scale tensors suffer from the \u201ccurse of dimensionality\u201d [18] as complexity and storage costs scale exponentially with the order of the tensor. Recently, tensor networks [4], [19], [20] such as the Tensor Train (TT) decomposition [21] have been introduced\nThis work has been partially supported by the ANR-20-THIA-0013 program \u201cAI PhD@Lille\u201d and the CornelIA (CO-construction RespoNsable Et durable d\u2019une Intelligence Artificielle) CPER project.\n\u2217 Univ. Lille, UMR 9189 CRIStAL, F-59000 Lille, France \u2020 IMT Nord Europe, Institut Mines-Te\u0301le\u0301com, Centre for Digital Systems, F59000 Lille, France \u2021 Univ. Lille, CNRS, Centrale Lille, Institut Mines-Te\u0301le\u0301com, UMR 9189 CRIStAL, F-59000 Lille, France \u00a7 Univ. Toulon, Aix Marseille Universite\u0301, CNRS, LIS, UMR 7020, France \u00b6 Department of Teleinformatics Engineering, Federal University of Fortaleza, Brazil Corresponding author: maxence.giraud@univ-lille.fr\nto mitigate the curse of dimensionality. The TT decomposition represents a tensor as a cascade of lower-order tensors and has been exploited in several recent works such as signal processing applications [22], [23], [24]. This format is very popular due to its low storage compactness and has a good trade-off between optimality (with the TT-SVD algorithm) and numerical stability from an algorithmic point of view. In this paper, we propose to use the TT decomposition formulation of the Tucker decomposition [25] to efficiently estimate the factor matrices. We go further than [25] by proving that the TT-cores follow a Constrained Canonical Polyadic Decomposition. This theoretical result suggests us a new algorithm called TRIDENT (Tucker Decomposition based on a Tensor Train of Coupled and constraint CP cores). Notations: In this paper, we represent vectors, matrices, and higher-order tensors by lower case bold letters (x), upper case bold letters (X) and upper case calligraphic letters (X ) respectively. To indicate an entry, we write xi1,...,in . The transpose, the transpose of the inverse, and the Moore-Penrose inverse of the matrix X are respectively denoted by XT, X\u2212T and X\u2020. The nth unfolding of a tensor X is written as X (n). The identity tensor is written as If,d where d is the order and f is the size of the dimensions. The symbols \u25e6 and \u2297 denote respectively the outer product between vectors and the Kronecker multiplication between matrices respectively. The Khatri-Rao product written as \u2299 is defined as the columnwise Kronecker product [17]. We write the n-mode product and the ( m n ) -mode product respectively as \u00d7n and \u00d7mn [17]."
        },
        {
            "heading": "II. BACKGROUND",
            "text": "In this section, we provide a few definitions and properties that will be useful in the formulation of the proposed method. Definition 1: The Tucker decomposition of a Q-order tensor X of dimensions I1\u00d7 ...\u00d7IQ with multilinear ranks {T1, ..., TQ} is defined as [16]\nX = C \u00d71 F1 \u00d72 F2 \u00d73 \u00b7 \u00b7 \u00b7 \u00d7Q FQ, (1) where C is a Q-order tensor of size T1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 TQ, and Fq is a factor matrix of size Iq \u00d7 Tq , for 1 \u2264 q \u2264 Q. The CP decomposition [15], [26] expression can be interpreted as a Tucker decomposition with the core tensor being the identity tensor and the multilinear ranks all equal to a single value called the canonical rank [17].\nSince the CP format is a special case of the Tucker format, we can rewrite the Tucker decomposition as a CPD. A useful property of such refactoring is given below.\n2 G1 G2 Gq\u0304 GQ\u22121 GQR1 R2 Rq\u0304\u22121 Rq\u0304+1 RQ\u22122 RQ\u22121\nFq\u0304\nSVD\n{\u00b7}\nLS\n{\u00b7, \u00b7, \u00b7}\nTri-ALS\n{\u00b7, \u00b7} Bi-ALS\n{\u00b7}\nLS\n{\u00b7, \u00b7, \u00b7}\nTri-ALS\n{\u00b7, \u00b7} Bi-ALS\nFig. 1: Big picture of the TRIDENT algorithm. Starting from G2 and GQ\u22121 we can estimate iteratively the Tucker factors and their associated latent variables thanks to the coupling property of the TT-cores.\nProperty 1: Defining Y = reshape(Ic; (a, b, c)) with ab = c, as a reshaping of the identity matrix into a 3-order tensor, we have [27]\nY = I3,ab \u00d71 \u03a6a,b \u00d72 \u03a8a,b (2)\nwith \u03a8a,b = Ia \u2297 1Tb \u2208 Ra\u00d7ab and \u03a6a,b = 1Ta \u2297 Ib \u2208 Rb\u00d7ab.\nDefinition 2: The Tensor-Train decomposition of a tensor X with TT-ranks {R1, ..., RQ\u22121} is defined as [21]\nX = G1 \u00d712 G2 \u00d713 G3 \u00d714 \u00b7 \u00b7 \u00b7 \u00d71Q GQ (3)\nwhere Gj of dimensions Rj\u22121 \u00d7 Ij\u22121 \u00d7Rj , for 1 \u2264 q \u2264 Q, are called the TT-cores, with R0 = RQ = 1."
        },
        {
            "heading": "III. NEW LINK BETWEEN THE TUCKER AND THE TENSOR-TRAIN DECOMPOSITION",
            "text": ""
        },
        {
            "heading": "A. Established link",
            "text": "The proposed method relies on the relationship between the Tucker decomposition and the TT established recently in [25], where a high-order tensor estimation problem is solved via trains of coupled third-order CP and Tucker tensors.\nTheorem 1: Consider a tensor X of dimensions I1\u00d7\u00b7 \u00b7 \u00b7\u00d7IQ expressed in a Tucker format with multilinear ranks {T1, ..., TQ}. The set of TT-cores {Gq} associated with the TT decomposition of X are given by [25]\nG1 = F1M \u22121 1 ,\nGq = Tq \u00d71 Mq\u22121 \u00d72 Fq \u00d73 M\u2212Tq (1 < q < q\u0304), Gq\u0304 = Cq\u0304 \u00d71 Mq\u0304\u22121 \u00d72 Fq\u0304 \u00d73 M\u2212Tq\u0304 , Gq = T\u0304q \u00d71 Mq\u22121 \u00d72 Fq \u00d73 M\u2212Tq (q\u0304 < q < Q),\nGQ = MQ\u22121F \u2212T Q .\n(4)\nThe TT-ranks associated with the TT-cores are defined as Rq = min (\u220fq i=1 Ti, \u220fQ i=q+1 Ti ) , with q\u0304 being the smallest\nq that verifies \u220fq i=1 Ti < \u220fQ\ni=q+1 Ti. The factor matrices are denoted by Fq and have dimensions Iq\u00d7Tq . The matrices Mq are change-of-basis matrices of dimensions Rq\u00d7Rq which are latent variables shared between neighboring TT-cores. Finally, Tq and T\u0304q are proper reshapings of the identity matrix to match the dimensions of the related factors in Eq. (5). This relationship leads to the FMP algorithm [25], which allows estimating the Tucker factors using the TT decomposition."
        },
        {
            "heading": "B. The coupled constrained CP formulation",
            "text": "In order to estimate all the Tucker factors using the equivalence between Tucker and TT decomposition, we propose to reformulate this equivalence using Property 1. The tensors Tq and T\u0304q from Eq. (4) are reshapings of the identity matrix defined as\nTq = reshape(IRq ; (Rq\u22121, Tq, Rq)) (1 < q < q\u0304), T\u0304q = reshape(IRq ; (Rq\u22121, Tq, Rq)) (q\u0304 < q < Q). (5)\nUsing Theorem 1 and Eq. (5), we can decompose the TTcores in Eq. (4) as constrained CPDs, the structure of which are defined by the following lemma.\nLemma 1: Using the constrained CP formulation of Property 1, we can rewrite the second and fourth equations (for 1 < q < q\u0304 and q\u0304 < q < Q, respectively) of the TT-cores defined in Theorem 1 as\nGq = I3,Rq \u00d71 Mq\u22121\u03a8Rq\u22121,Tq \u00d72 Fq\u03a6Rq\u22121,Tq \u00d73 M\u2212Tq Gq = I3,Rq \u00d71 Mq\u22121 \u00d72 Fq\u03a6Rq,Tq \u00d73 M\u2212Tq \u03a8Rq,Tq .\nProof. Using Property 1, we can rewrite Tq and T\u0304q as Tq = I3,Rq \u00d71 \u03a6Rq\u22121,Tq \u00d72 \u03a8Rq\u22121,Tq , T\u0304q = I3,Rq \u00d72 \u03a6Rq,Tq \u00d73 \u03a8Rq,Tq .\n(6)\nWe can obtain the above two expressions for Gq (for 1 < q < q\u0304 and q\u0304 < q < Q, respectively) by substituting Eq. (6) into Eq. (4) using the fact that the order of the mode-n multiplication is irrelevant."
        },
        {
            "heading": "IV. ESTIMATION SCHEME",
            "text": ""
        },
        {
            "heading": "A. TRIDENT algorithm",
            "text": "In this section, we introduce a new algorithm which efficently solve the problem of interest\nmin F1,...,FQ,M1,...,MQ\u22121\n\u2225G1 \u2212 F1M\u221211 \u22252F\n+ q\u0304\u22121\u2211 q=2 \u2225Gq \u2212 I3,Rq \u00d71 Mq\u22121\u03a8Rq\u22121,Tq \u00d72 Fq\u03a6Rq\u22121,Tq \u00d73 M\u2212Tq \u22252F\n+ \u2225Gq\u0304 \u2212 Cq\u0304 \u00d71 Mq\u0304\u22121 \u00d72 Fq\u0304 \u00d73 M\u2212Tq\u0304 \u22252F\n+ Q\u22121\u2211 q=q\u0304+1 \u2225Gq \u2212 I3,Rq \u00d71 Mq\u22121 \u00d72 Fq\u03a6Rq,Tq \u00d73 M\u2212Tq \u03a8Rq,Tq\u22252F\n+ \u2225GQ \u2212MQ\u22121F\u2212TQ \u22252F . An overview of the algorithm is depicted in Figure 1.\nUsing the formulation of the equivalence between the TT and Tucker decomposition and incorporating Lemma 1, the\n3 G1 G2 G3 R1 R2\n{M1,F2,M2}\nTri-ALS\n{F3,M3} Bi-ALS {F1} LS\nFig. 2: Zoomed picture of the TRIDENT algorithm for the first three TT-cores.\nTucker factor matrices are estimated after the TT-SVD step. We first estimate both F2 and FQ\u22121 as well as the latent variables associated with those factors using an alternating least squares method (ALS) operating on the corresponding CCPD-decomposed TT-cores. Then, by propagating iteratively the latent matrices, we estimate the remaining factors using either a Least Squares (LS) method or an ALS. For 1 < q < q\u0304, the matrix factors of the TT-cores are found via a constrained Tri-ALS estimation scheme, each iteration being composed of the following estimation steps\nMq\u22121 = 1\nTq G(1)q\n(( M\u2212Tq \u2299 Fq\u03a6Rq\u22121,Tq )T)\u2020 \u03a8TRq\u22121,Tq ,\nFq = 1\nRq\u22121 G(2)q\n(( M\u2212Tq \u2299Mq\u22121\u03a8Rq\u22121,Tq )T)\u2020 \u03a6TRq\u22121,Tq ,\nM\u2212Tq = G (3) q (( Fq\u03a6Rq\u22121,Tq \u2299Mq\u22121\u03a8Rq\u22121,Tq )T)\u2020 ,\n(7) where we have used the fact that \u03a8a,b\u03a8Ta,b = bIa and \u03a6a,b\u03a6 T a,b = aIb. Similarly, for q\u0304 < q < Q, we get\nMq\u22121 = G (1) q (( M\u2212Tq \u03a8Rq,Tq \u2299 Fq\u03a6Rq,Tq )T)\u2020 ,\nFq = 1\nRq+1 G(2)q\n(( M\u2212Tq \u03a8Rq,Tq \u2299Mq\u22121 )T)\u2020 \u03a6TRq,Tq ,\nM\u2212Tq = 1\nTq G(3)q\n(( Fq\u03a6Rq,Tq \u2299Mq\u22121 )T)\u2020 \u03a8TRq,Tq .\n(8)\nApplying this Tri-ALS estimation scheme to the TT-cores G2 and GQ\u22121 allows us to estimate the associated Tucker factors and latent matrices. We can then propagate these latent matrices using the coupled LS criterion to perform a BiALS estimation on other cores (following the Tri-ALS scheme described above but skipping the propagated change-of-basis matrix). The propagation of the latent matrices is illustrated in Figure 2. As we do not have a theoretical convergence guarantee for ALS, we use the criterion \u03bb = \u2225\u27e8F\u0302i\u27e9\u2212\u27e8F\u0302i\u22121\u27e9\u2225F \u2225\u27e8F\u0302i\u22121\u27e9\u2225F where i is the current iteration and \u27e8\u00b7\u27e9 the spanned subspace, to declare convergence. The value of \u03bb is set experimentally to optimize the trade-off between the number of iterations and the reconstruction error. Finally, a SVD is applied to the second unfolding of Gq\u0304\u00d71M\u0302\u22121q\u0304\u22121\u00d73M\u0302q\u0304 , and compute the left singular vectors as the Tucker factor F\u0302q\u0304 . A summary of the complete estimation scheme is provided in Algorithm 1."
        },
        {
            "heading": "B. Computational Complexity",
            "text": "The computational complexity of our algorithm is directly related to that of the TT-SVD, which is of order O(TIQ) [25]\nAlgorithm 1: TRIDENT input : Q-order tensor X and multilinear ranks\n{T1, ..., TQ} output: Factor matrices Fi and core tensor C of the\nTucker decomposition\n1 Compute q\u0304 and the TT-ranks using the multilinear ranks and Theorem 1. 2 Compute the TT decomposition: {G1,G2, ...,GQ\u22121,GQ} = TT-SVD(X ) 3 Compute the factors using the ALS described in Eq. (7) and (8):\n{M\u03021, F\u03022, M\u03022} = Tri-ALS(G2) {M\u0302Q\u22122, F\u0302Q\u22121, M\u0302Q\u22121} = Tri-ALS(GQ\u22121)\nfor 3 \u2264 q \u2264 q\u0304 \u2212 1 do {F\u0302q, M\u0302q} = Bi-ALS(Gq, M\u0302q\u22121) end for Q\u2212 1 \u2265 q \u2265 q\u0304 + 1 do\n{F\u0302q, M\u0302q} = Bi-ALS(Gq, M\u0302q\u22121) end F\u03021 = G1M\u03021 F\u0302Q = G \u2020 QM\u0302Q\u22121\n4 Compute Fq\u0304 as the left singular bases of the second unfolding of Gq\u0304 \u00d71 M\u0302\u22121q\u0304\u22121 \u00d73 M\u0302q\u0304 . 5 (Optional) Compute the core tensor C = X \u00d71 F\u20201 \u00d72 ...\u00d7q\u0304 FTq\u0304 \u00d7q\u0304+1 ...\u00d7Q F\u2020Q;\nwith T the largest multilinear rank, I and Q represent respectively the largest dimension and the order of the original tensor. As the order of the tensor increases the complexity of the ALS becomes insignificant compared to that of the TTSVD and thus both FMP and TRIDENT have similar computational complexities. On the other hand, the computational complexities of the HOOI and the HOSVD scale linearly by an additional factor Q, resulting in O(QTIQ +NiterQTQ+1) and O(QTIQ), respectively."
        },
        {
            "heading": "C. Remarks on uniqueness",
            "text": "For 1 < q < q\u0304, the model considered in Lemma 1 does not enjoy the essential uniqueness property of the CPD. By adopting the analysis made in [27], we can estimate the factors of the constrained CPD up to the ambiguity relationship TTMq = ( TMq\u22121 \u2297TFq )\u22121 , so that F\u0302q = FqTFq , M\u0302q = MqTMq , M\u0302q\u22121 = Mq\u22121TMq\u22121 . Indeed, the Tucker model has no essential uniqueness but only uniqueness up to a change-of-basis matrix, which is the case in the considered model. Let F be a generic Tucker factor and \u03a6 its associated contraint matrix, we define F\u0303 = F\u0302\u03a6T . We have F\u0303\u03a6 = RFTF . The projector on F\u0303 is given by\nF\u0303\u03a6(F\u0303\u03a6)\u2020 = R (FTF ) 1\nR (FTF )\n\u2020 = FTFT \u22121 F F \u2020 = FF\u2020."
        },
        {
            "heading": "V. NUMERICAL SIMULATIONS",
            "text": "In order to assess the performances of the TRIDENT algorithm, we compare it with the HOOI [28], HOSVD [29]\n4\nand the FMP [25] for three different experiments. These experiments are carried out for Q-order tensors, where Q ranges from 4 to 9. We look at how closely the algorithm is able to reconstruct the original tensor. In our experiments, we generate the original tensor as X = T /\u2225T \u2225F +\u03c3B/\u2225B\u2225F , where T is the reconstructed tensor from a single realization of a zero-mean Gaussian distribution of the Tucker factors and B is an Additive White Gaussian Noise (AWGN). We consider hypercubic tensors with dimensions equal to 8 and multilinear ranks all equal to 2. The results are averaged over 300 independent Monte Carlo runs. We point out that similar experiments have been carried out for several dimensions (ranging from 6 to 10) and ranks (ranging from 2 to 4) and we observed the same conclusions (the results are not displayed in this paper due to space limitations). The code 1 was written in a non-optimized way to ensure a fair comparison. Reconstruction experiment: We first compare the reconstruction error for different SNRs, defined as SNR=\u221210 log10(\u03c32). To measure the reconstruction error, we consider the normalized reconstruction error defined as \u2225X \u2212 X\u0302\u2225F /\u2225X\u2225F . The results are shown in Figure 3 considering an 8-order tensor. We can observe that the proposed method achieves similar results to the HOOI and HOSVD while performing better than the competing FMP method at higher SNRs (over 30dB). Time complexity: In a second experiment, we verify our computational complexity analysis by evaluating the time complexity with an increasing tensor order. The results are averaged over 300 runs with ranks and dimensions as stated at the beginning of Section V. As shown in Table I, the proposed method asymptotically converges to the results of FMP, while the time complexities of HOOI and HOSVD grow much faster. These results consider an SNR=30dB, but the impact of the noise on the time complexity is only significant for lowerorder tensors (6-order or less). For higher-order tensors, the time complexity of the TT-SVD is the main limiting factor. For a 9-order tensor, we observe a 6-fold difference between the HOSVD and TRIDENT. When comparing HOOI and TRIDENT, an 8-fold is observed, which is consistent with our analysis considering the non-dominating terms.\n1https://gitlab.univ-lille.fr/maxence.giraud/trident\nConvergence of ALS: We finally investigate the convergence of the ALS stage of our algorithm. More specifically the number of steps until convergence depends on the SNR for both the Tri-ALS and all the Bi-ALS. For these experiments, we used the same conditions as stated at the beginning of section V, as well as an 8-order tensor for the original tensor. The stopping criterion for the ALS is set to \u03bb = 10\u22126 which was experimentally verified as the best choice considering the tradeoff between reconstruction error and execution time. The results are reported in Figure 4. We can see that the convergence is achieved within tens of iterations (or even faster) while exhibiting a small variance. We observe a SNR higher than 20dB the convergence is achieved within less than 20 iterations, while for a SNR higher than 5dB, 40 iterations are enough for convergence. Moreover, the variance of the convergence speed (measured in terms of the number of iterations) yields a confident result."
        },
        {
            "heading": "VI. CONCLUSION",
            "text": "This paper proposed a new JIRAFE-like algorithm to compute the Tucker decomposition with low storage and computational costs. The curse of dimensionality can be mitigated by the proposed TRIDENT estimator, which is based on a new algebraic equivalence between the Tucker model and a structured Tensor Train (TT) decomposition. In particular, we prove that for a Q-order tensor, Q \u2212 3 TT-cores follow a Constrained CPD model. Our numerical simulations show that the TRIDENT estimator has similar estimation accuracy for most of the SNR range as the HOOI and HOSVD methods with considerably lower time complexity. Perspectives include an adaptation of the proposed algorithm by replacing the TTSVD with the TT-HSVD to obtain a faster implementation [30] as well as resorting to enhancements to the iterative part of the algorithm using the approaches proposed in [31], [32].\n5"
        }
    ],
    "title": "Tucker Decomposition Based on a Tensor Train of Coupled and Constrained CP Cores",
    "year": 2023
}