{
    "abstractText": "To fully evaluate the overall performance of different NLP models in a given domain, many evaluation benchmarks are proposed, such as GLUE, SuperGLUE and CLUE. The field of natural language understanding has traditionally focused on benchmarks for various tasks in languages such as Chinese, English, and multilingual, however, there has been a lack of attention given to the area of classical Chinese, also known as \"wen yan wen (\u6587\u8a00\u6587)\", which has a rich history spanning thousands of years and holds significant cultural and academic value. For the prosperity of the NLP community, in this paper, we introduce the WYWEB evaluation benchmark, which consists of nine NLP tasks in classical Chinese, implementing sentence classification, sequence labeling, reading comprehension, and machine translation. We evaluate the existing pre-trained language models, which are all struggling with this benchmark. We also introduce a number of supplementary datasets and additional tools to help facilitate further progress on classical Chinese NLU. The github repository is https://github.com/baudzhou/WYWEB.",
    "authors": [
        {
            "affiliations": [],
            "name": "Bo Zhou"
        },
        {
            "affiliations": [],
            "name": "Qianglong Chen"
        },
        {
            "affiliations": [],
            "name": "Tianyu Wang"
        },
        {
            "affiliations": [],
            "name": "Xiaomi Zhong"
        },
        {
            "affiliations": [],
            "name": "Yin Zhang"
        }
    ],
    "id": "SP:41fd325029b10eee20d0ea54d015d1742a556167",
    "references": [
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Ernie Chang",
                "Yow-Ting Shiue",
                "Hui-Syuan Yeh",
                "Vera Demberg."
            ],
            "title": "Time-aware ancient chinese text translation and inference",
            "venue": "arXiv preprint arXiv:2107.03179.",
            "year": 2021
        },
        {
            "authors": [
                "Alexis Conneau",
                "Douwe Kiela."
            ],
            "title": "Senteval: An evaluation toolkit for universal sentence representations",
            "venue": "arXiv preprint arXiv:1803.05449.",
            "year": 2018
        },
        {
            "authors": [
                "Yiming Cui",
                "Wanxiang Che",
                "Ting Liu",
                "Bing Qin",
                "Shijin Wang",
                "Guoping Hu."
            ],
            "title": "Revisiting pretrained models for chinese natural language processing",
            "venue": "arXiv preprint arXiv:2004.13922.",
            "year": 2020
        },
        {
            "authors": [
                "Yiming Cui",
                "Wanxiang Che",
                "Ting Liu",
                "Bing Qin",
                "Ziqing Yang."
            ],
            "title": "Pre-training with whole word masking for chinese bert",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3504\u20133514.",
            "year": 2021
        },
        {
            "authors": [
                "Yiming Cui",
                "Wanxiang Che",
                "Ting Liu",
                "Bing Qin",
                "Ziqing Yang"
            ],
            "title": "2021b. Pre-training with whole word masking for chinese bert",
            "year": 2021
        },
        {
            "authors": [
                "Yiming Cui",
                "Ting Liu",
                "Wanxiang Che",
                "Li Xiao",
                "Zhipeng Chen",
                "Wentao Ma",
                "Shijin Wang",
                "Guoping Hu."
            ],
            "title": "A span-extraction dataset for chinese machine reading comprehension",
            "venue": "arXiv preprint arXiv:1810.07366.",
            "year": 2018
        },
        {
            "authors": [
                "Yiming Cui",
                "Ting Liu",
                "Ziqing Yang",
                "Zhipeng Chen",
                "Wentao Ma",
                "Wanxiang Che",
                "Shijin Wang",
                "Guoping Hu."
            ],
            "title": "A sentence cloze dataset for Chinese machine reading comprehension",
            "venue": "Proceedings of the 28th International Conference on Computational",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Li Dong",
                "Nan Yang",
                "Wenhui Wang",
                "Furu Wei",
                "Xiaodong Liu",
                "Yu Wang",
                "Jianfeng Gao",
                "Ming Zhou",
                "Hsiao-Wuen Hon."
            ],
            "title": "Unified language model pre-training for natural language understanding and generation",
            "venue": "Advances in Neural Information Process-",
            "year": 2019
        },
        {
            "authors": [
                "Xingyi Duan",
                "Baoxin Wang",
                "Ziyue Wang",
                "Wentao Ma",
                "Yiming Cui",
                "Dayong Wu",
                "Shijin Wang",
                "Ting Liu",
                "Tianxiang Huo",
                "Zhen Hu"
            ],
            "title": "Cjrc: A reliable human-annotated benchmark dataset for chinese judicial reading comprehension",
            "year": 2019
        },
        {
            "authors": [
                "Pengcheng He",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "Deberta: Decoding-enhanced bert with disentangled attention",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Zhu Yuchen Hu Renfen",
                "Li Shen."
            ],
            "title": "Knowledge representation and sentence segmentation of ancient chinese based on deep language models",
            "venue": "JOURNAL OF CHINESE INFORMATION PROCESSING, 35(4):8\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Jishi Jin"
            ],
            "title": "A brief critical summuary of the chinese language education in rok",
            "venue": "DongJiang Journal,",
            "year": 2004
        },
        {
            "authors": [
                "Yasuoka Koichi",
                "Wittern Christian",
                "Morioka Tomohiko",
                "Ikeda Takumi",
                "Yamazaki Naoki",
                "Nikaido Yoshihiro",
                "Suzuki Shingo",
                "Moro Shigeki",
                "Fujita Kazunori."
            ],
            "title": "Designing universal dependencies for classical chinese and its application",
            "venue": "Journal of Information",
            "year": 2022
        },
        {
            "authors": [
                "Guokun Lai",
                "Qizhe Xie",
                "Hanxiao Liu",
                "Yiming Yang",
                "Eduard Hovy."
            ],
            "title": "Race: Large-scale reading comprehension dataset from examinations",
            "venue": "arXiv preprint arXiv:1704.04683.",
            "year": 2017
        },
        {
            "authors": [
                "Zhenzhong Lan",
                "Mingda Chen",
                "Sebastian Goodman",
                "Kevin Gimpel",
                "Piyush Sharma",
                "Radu Soricut."
            ],
            "title": "Albert: A lite bert for self-supervised learning of language representations",
            "venue": "arXiv preprint arXiv:1909.11942.",
            "year": 2019
        },
        {
            "authors": [
                "Guangjie Li",
                "Xiaomei Gao",
                "Xiulan Cui."
            ],
            "title": "\u6c49 \u8bed\u53d1\u5c55\u53f2\u7814\u7a76",
            "venue": "\u9ed1\u9f99\u6c5f\u5927\u5b66\u51fa\u7248\u793e.",
            "year": 2013
        },
        {
            "authors": [
                "Guoxin Li."
            ],
            "title": "The development and task of chinese ancient book resources digitization",
            "venue": "Journal of Academic Libraries, (1):21\u201326.",
            "year": 2002
        },
        {
            "authors": [
                "Wenhao Li",
                "Fanchao Qi",
                "Maosong Sun",
                "Xiaoyuan Yi",
                "Jiarui Zhang."
            ],
            "title": "Ccpm: A chinese classical poetry matching dataset",
            "venue": "arXiv preprint arXiv:2106.01979.",
            "year": 2021
        },
        {
            "authors": [
                "Jian Liu",
                "Guangshun Cao",
                "Fuxiang Wu."
            ],
            "title": "Several factors that induce lexical-grammaticalization in chinese",
            "venue": "Chinese language, (3):161\u2013169.",
            "year": 1995
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Bai",
                "Soumith Chintala."
            ],
            "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
            "venue": "Curran Associates Inc., Red Hook, NY, USA.",
            "year": 2019
        },
        {
            "authors": [
                "Nguyen Xuan Phong",
                "Vu Hong Van"
            ],
            "title": "Taoism in vietnam during the northern colonial period and some notes when studying taoism in vietnam",
            "venue": "Journal of Natural Remedies,",
            "year": 2020
        },
        {
            "authors": [
                "Fanchao Qi",
                "Yanhui Yang",
                "Jing Yi",
                "Zhili Cheng",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "QuoteR: A benchmark of quote recommendation for writing",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2022
        },
        {
            "authors": [
                "Jianglei Qi."
            ],
            "title": "Strategies for the development of a platform for ancient text knowledge service",
            "venue": "Chinese Editors Journal, (2):60\u201365.",
            "year": 2022
        },
        {
            "authors": [
                "Xipeng Qiu",
                "Tianxiang Sun",
                "Yige Xu",
                "Yunfan Shao",
                "Ning Dai",
                "Xuanjing Huang."
            ],
            "title": "Pre-trained models for natural language processing: A survey",
            "venue": "Science China Technological Sciences, 63(10):1872\u2013 1897.",
            "year": 2020
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "arXiv preprint arXiv:1910.10683.",
            "year": 2019
        },
        {
            "authors": [
                "Yizhan Shao",
                "Tong Shao",
                "Minghao Wang",
                "Peng Wang",
                "Jie Gao."
            ],
            "title": "A sentiment and style controllable approach for chinese poetry generation",
            "venue": "Proceedings of the 30th ACM International Conference on Information & Knowledge Management, CIKM \u201921,",
            "year": 2021
        },
        {
            "authors": [
                "Kai Sun",
                "Dian Yu",
                "Dong Yu",
                "Claire Cardie."
            ],
            "title": "Probing prior knowledge needed in challenging chinese machine reading comprehension",
            "venue": "arXiv preprint arXiv:1904.09679.",
            "year": 2019
        },
        {
            "authors": [
                "Alex Wang",
                "Yada Pruksachatkun",
                "Nikita Nangia",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
            "venue": "Advances in Neural Information",
            "year": 2019
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R. Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Dongbo Wang",
                "Chang Liu",
                "Zihe Zhu",
                "Jiangfeng Liu",
                "Haotian Hu",
                "Si Shen",
                "Bin Li."
            ],
            "title": "Sikubert \u4e0esikuroberta:\u9762\u5411\u6570\u5b57\u4eba\u6587\u7684\u300a\u56db\u5e93\u5168\u4e66\u300b\u9884 \u8bad\u7ec3\u6a21\u578b\u6784\u5efa\u53ca\u5e94\u7528\u7814\u7a76",
            "venue": "Library Tribune.",
            "year": 2021
        },
        {
            "authors": [
                "Li Wang."
            ],
            "title": "\u6c49\u8bed\u53f2\u7a3f",
            "venue": "\u4e2d\u534e\u4e66\u5c40.",
            "year": 2004
        },
        {
            "authors": [
                "Wei Wang",
                "Bin Bi",
                "Ming Yan",
                "Chen Wu",
                "Zuyi Bao",
                "Jiangnan Xia",
                "Liwei Peng",
                "Luo Si."
            ],
            "title": "Structbert: Incorporating language structures into pretraining for deep language understanding",
            "venue": "arXiv preprint arXiv:1908.04577.",
            "year": 2019
        },
        {
            "authors": [
                "Xiaoqiang Wang",
                "Bang Liu",
                "Fangli Xu",
                "Bo Long",
                "Siliang Tang",
                "Lingfei Wu."
            ],
            "title": "Feeding what you need by understanding what you learned",
            "venue": "arXiv preprint arXiv:2203.02753.",
            "year": 2022
        },
        {
            "authors": [
                "Junqiu Wei",
                "Xiaozhe Ren",
                "Xiaoguang Li",
                "Wenyong Huang",
                "Yi Liao",
                "Yasheng Wang",
                "Jiashu Lin",
                "Xin Jiang",
                "Xiao Chen",
                "Qun Liu."
            ],
            "title": "Nezha: Neural contextualized representation for chinese language understanding",
            "venue": "arXiv preprint arXiv:1909.00204.",
            "year": 2019
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz"
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "year": 2020
        },
        {
            "authors": [
                "Beilei Xiang",
                "Changbing Yang",
                "Yu Li",
                "Alex Warstadt",
                "Katharina Kann."
            ],
            "title": "CLiMP: A benchmark for Chinese language model evaluation",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main",
            "year": 2021
        },
        {
            "authors": [
                "Yiwen Zhang",
                "He Zhou",
                "Shaoweihua Liu",
                "Zhe Zhao",
                "Qipeng Zhao",
                "Cong Yue",
                "Xinrui Zhang",
                "Zhengliang Yang",
                "Kyle Richardson",
                "Zhenzhong Lan."
            ],
            "title": "CLUE: A Chinese language understanding evaluation benchmark",
            "venue": "Proceedings of the 28th Inter-",
            "year": 2020
        },
        {
            "authors": [
                "Qiuhan Xu."
            ],
            "title": "Chinese characters in japan",
            "venue": "\u4e2d\u56fd\u6587 \u5316\u7814\u7a76, pages 135\u2013139+6.",
            "year": 1995
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime Carbonell",
                "Russ R Salakhutdinov",
                "Quoc V Le."
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "venue": "Advances in neural information processing systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Zinong Yang",
                "Ke-jia Chen",
                "Jingqiang Chen."
            ],
            "title": "Guwen-unilm: Machine translation between ancient and modern chinese based on pre-trained models",
            "venue": "CCF International Conference on Natural Language Processing and Chinese Computing, pages 116\u2013128.",
            "year": 2021
        },
        {
            "authors": [
                "Fanchao Qi",
                "Junwei Bao",
                "Jinran Nie"
            ],
            "title": "Cuge: A chinese language understanding and generation evaluation benchmark",
            "venue": "arXiv preprint arXiv:2112.13610",
            "year": 2021
        },
        {
            "authors": [
                "Shaofei Ye",
                "Zhiyong Tian."
            ],
            "title": "On the origins of vietnamese ancient history",
            "venue": "Southeast Asian and South Asian Studies, (2):83\u201389.",
            "year": 2013
        },
        {
            "authors": [
                "Jingsong Yu",
                "Yi Wei",
                "Yongwei Zhang."
            ],
            "title": "Automatic ancient chinese texts segmentation based on bert",
            "venue": "JOURNAL OF CHINESE INFORMATION PROCESSING, 33(11):57\u201363.",
            "year": 2021
        },
        {
            "authors": [
                "JIANG Yuying"
            ],
            "title": "A study on the readability of reading test texts in chinese proficiency test(hsk)",
            "year": 2020
        },
        {
            "authors": [
                "Ningyu Zhang",
                "Mosha Chen",
                "Zhen Bi",
                "Xiaozhuan Liang",
                "Lei Li",
                "Xin Shang",
                "Kangping Yin",
                "Chuanqi Tan",
                "Jian Xu",
                "Fei Huang"
            ],
            "title": "Cblue: A chinese biomedical language understanding evaluation benchmark. arXiv preprint arXiv:2106.08087",
            "year": 2021
        },
        {
            "authors": [
                "Chujie Zheng",
                "Minlie Huang",
                "Aixin Sun."
            ],
            "title": "Chid: A large-scale chinese idiom dataset for cloze test",
            "venue": "arXiv preprint arXiv:1906.01265.",
            "year": 2019
        },
        {
            "authors": [
                "Bin Zhou."
            ],
            "title": "A comprehensive study on the history presented in a series of biographies written in chinese in japan",
            "venue": "Journal of Historiography, pages 98\u2013104.",
            "year": 2009
        },
        {
            "authors": [
                "Sergey Zinin",
                "Yang Xu."
            ],
            "title": "Corpus of Chinese dynastic histories: Gender analysis over two millennia",
            "venue": "Proceedings of the 12th Language Resources and Evaluation Conference, pages 785\u2013793, Marseille, France. European Language Resources Association.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "For the prosperity of the NLP community, in this paper, we introduce the WYWEB evaluation benchmark, which consists of nine NLP tasks in classical Chinese, implementing sentence classification, sequence labeling, reading comprehension, and machine translation. We evaluate the existing pre-trained language models, which are all struggling with this benchmark. We also introduce a number of supplementary datasets and additional tools to help facilitate further progress on classical Chinese NLU. The github repository is https://github.com/baudzhou/WYWEB."
        },
        {
            "heading": "1 Introduction",
            "text": "Classical Chinese, as a written form of the Chinese language, had been widely used in the Confucian cultural circle, including China, Japan, Korea, Vietnam, etc (Ye and Tian, 2013; Phong1 and Van2, 2020; Xu, 1995; Zhou, 2009; Jin, 2004). As far as we know, there are about 400 million words, 3 million ancient articles have been passed down, covering literature, art, history, philosophy, etc, half of which are of great value (Yin et al., 2018). However, in recent centuries, the use of classical Chinese has been gradually phased out and replaced by modern languages, resulting in increasing difficulty in comprehending it. Therefore, it is necessary to\n\u2217Corresponding Author: Yin Zhang.\nintroduce efficient NLP technology to process, understand, and research such literature.\nWhile pre-trained language models such as BERT (Devlin et al., 2019) and BERT-like models (Yang et al., 2019; Dong et al., 2019; Lan et al., 2019; Liu et al., 2019; He et al., 2020; Raffel et al., 2019; Wang et al., 2019c) have shown remarkable performance on English NLP benchmarks, including GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a), there are also many efforts (Cui et al., 2020a; Wei et al., 2019; Cui et al., 2021a) in Chinese NLP community, achieving significant improvement on modern Chinese NLP benchmark (Xu et al., 2020; Cui et al., 2018; Duan et al., 2019; Cui et al., 2020b).\nHowever, since classical Chinese differs from modern Chinese in writing and grammar, these benchmarks can not be applied well to the studies in the classical Chinese domain. In order to better adapt to the understanding of classical Chinese, many tasks and datasets are required to be redesigned, such as sequence labeling and sentence pair similarity. Meanwhile, due to the performance of the model being closely related to the pre-training corpus (Qiu et al., 2020), such as scale, language, domain, etc., the existing language models pre-trained on modern Chinese corpus can not adapt well to classical Chinese tasks.\nConsidering that previous studies (Wang et al., 2021; Yang et al., 2021; Koichi et al., 2022) for classical Chinese have typically evaluated models on few or different NLU tasks, it is difficult to compare the performances of these models. To facilitate such research in classical Chinese, it is necessary to design a standard classical Chinese NLP evaluation benchmark.\nIn this paper, we introduce WYWEB (Wen Yan Wen Evaluation Benchmark), which will be open, and continually developed as much as we can. To evaluate the performance of the models of classical Chinese language representation, we create and\nar X\niv :2\n30 5.\n14 15\n0v 1\n[ cs\n.C L\n] 2\n3 M\nay 2\n02 3\nrefine nine tasks for different aspects of language understanding.\nSpecifically, considering the importance of breaks and pauses in sentences for the comprehension of classical Chinese, for sequence labeling, we design two novel tasks, including punctuation PUNC and named entity recognition GLNER, to evaluate word separation capability of pre-trained language models. For sentence classification, we design three novel tasks, including text category classification GJC, written time classification TLC, and emotion classification of poems task FSPC. For reading comprehension, we create a multiple choice task named WYWRC. And on the other hand, the assessment of the natural language comprehension capability of a model through previous reading comprehension tasks is challenged by the extensive use of rare vocabulary and idiomatic expressions in classical Chinese. Therefore, we design a novel reading comprehension task, IRC, from the exam papers and idiom dictionary. Meanwhile, since machine translation of classical Chinese is also a problem of great concern, we design a novel WYWMT task to study this topic. In addition, considering that some tokens in the classic Chinese language have the functions of prepositions, conjunctions and auxiliaries, and the same token has different meanings in different sentences, we design a new task, Xuci, for token comparison. More details of these tasks are described in Section 4 and Appendix A. And we describe the principles we used to design tasks and the process of data collection in Section 3.\nFurthermore, to better understand the challenges provided by WYWEB, we build a baseline for each task and evaluate several pre-trained models released by the community. The experimental results demonstrate that current state-of-the-art methods are struggling with these tasks, which suggests that those tasks in WYWEB can constitute a useful testbed for developing and comparing NLP systems for classical Chinese.\nThe contributions of our work are summarized as follows:\n\u2022 We propose and establish a novel benchmark for classical Chinese natural language understanding via redesigning, creating and collecting nine classical Chinese NLP tasks.\n\u2022 To validate the challenge of this benchmark on existing pre-trained model models, we con-\nduct a series of experiments with several baselines. Experimental results demonstrate these baselines are struggling with these novel tasks in classical Chinese.\n\u2022 Finally, we build an online leaderboard and provide an evaluation tool set for further exploration, which will be publicly accessible as soon as possible."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Benchmarks for Pre-trained Language Model",
            "text": "With the rise of the pre-training language model, pre-training a model on large corpus and finetuning them on downstream tasks becomes a general practice in the NLP community. To evaluate the ability of pre-trained language models in NLP tasks, several benchmarks are proposed for NLU tasks, such as SentEval (Conneau and Kiela, 2018), GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a), making existing models more comparable. For Chinese NLU, CLUE (Xu et al., 2020) benchmark is proposed with more than 10 tasks, including most NLP problems. To evaluate the ability of pre-trained language models in both natural language understanding and generation, CUGE (Yao et al., 2021) is proposed, which is designed as a hierarchical framework via a multilevel scoring strategy. Meanwhile, to evaluate whether language models can learn a linguistic phenomenon of Chinese, Xiang et al. (2021) develops CLiMP which covers 9 major Mandarin linguistic phenomena. QuoteR (Qi et al., 2022) is designed for the evaluation of quote recommendation methods. Moreover, CBLUE (Zhang et al., 2021) is a biomedical language understanding benchmark for Chinese, which mainly focuses on information extraction.\nHowever, compared to modern Chinese, there has been a lack of sufficient datasets and benchmarks for classical Chinese. CCLUE 1 provides 5 NLU tasks for classical Chinese, including punctuation, NER, classification, sentiment recognition and retrieval between classical and modern Chinese. Unfortunately, this project was not finished and is no longer maintained. Therefore, a new carefully designed benchmark for classical Chinese is very crucial for current studies.\n1https://cclue.top/"
        },
        {
            "heading": "2.2 Corpus Datasets for Classical Chinese",
            "text": "The largest classical corpus dataset available is Daizhige (\u6b86\u77e5\u9601)2, which contains about 3.3 billion tokens of classical Chinese literature, making classical Chinese corpus not low-resource. Most of pre-training related works use this dataset for model training. Ancient Chinese Corpus (ACC) 3 dataset contains the word segmented, POS-tagged data of Zuozhuan (an ancient Chinese history classical book). This dataset is widely used in ancient Chinese studies. Recently, Zinin and Xu (2020) introduces an open source corpus of Twenty-Four Histories and some other ancient books. Meanwhile, FSPC (Shao et al., 2021) and CCMP (Li et al., 2021) are proposed for ancient poem understanding. While CUGE (Yao et al., 2021) uses CCMP as a sub-task for classical poetry matching, in this work, we apply the FSPC dataset for poetry emotion recognition."
        },
        {
            "heading": "2.3 Pre-trained Models for Classical Chinese",
            "text": "In Classical Chinese pre-trained language models, SikuBERT and SikuRoBERTa (Wang et al., 2021) are pre-trained BERT/RoBERTa model on the Si Ku Quan Shu (Complete library in the Four Branches of Literature) corpus, and evaluated on 4 tasks, which are built from ACC dataset. Meanwhile, based on RoBERTa model, GuwenBERT 4 is pre-trained on Daizhige corpus with continuous training method and is evaluated on several NLU tasks. Other works (Hu Renfen, 2021; Yu et al., 2021; Yang et al., 2021) also evaluate their models on different few NLP tasks.However, since these models are not consistent in their evaluation tasks,\n2https://github.com/garychowcmu/daizhigev20 3https://catalog.ldc.upenn.edu/docs/LDC2017T14/ 4https://github.com/ethan-yt/guwenbert\nto compare the natural language understanding ability of different pre-trained models in classical Chinese, a standard evaluation benchmark is required."
        },
        {
            "heading": "3 WYWEB Overview",
            "text": "In this section, we introduce the principles and methods we applied during the construction process of WYWEB, and describe a brief overview of tasks in Table 1. Firstly, we describe the process of task design and the principles we follow. Then, we introduce the data selection principles in Section 3.2. Finally, we provide a description of the leaderboard and evaluation toolkit."
        },
        {
            "heading": "3.1 Task Design Principles",
            "text": "In this work, to assure that the benchmark can evaluate most aspects of pre-trained models and language phenomenons, we design evaluation tasks following best practices of other NLP benchmarks (Xu et al., 2020; Yao et al., 2021; Wang et al., 2019a,b) and suggestions from experts.\nFollowing the principles of Xu et al. (2020), firstly, these tasks should vary in most aspects of NLP, including text classification, reading comprehension and machine translation, etc.\nSecondly, these tasks should be well-defined in the academic community and easily processed for corpus collection.\nThirdly, they should be challenging but solvable. Finally, these tasks should be useful for followup studies and representative of classical Chinese natural language understanding tasks.\nWith the study of thousands of Chinese exam papers and requirements from academia and applications, we construct the evaluation tasks, covering most of the regular NLP tasks.\nIn addition to the regular tasks, we designed\nseveral tasks specifically for classical Chinese, i.e., punctuation of sentences without punctuation marks, comparison of confusing words and written period classification. These tasks will be introduced in Section 4 and Appendix A."
        },
        {
            "heading": "3.2 Corpora Selection",
            "text": "Diversity Over Time Since classical Chinese has a very long history and evolves over time, when designing tasks, we should choose texts that cover as many periods as possible. It is supposed that it is not reasonable enough to treat an isolated article as an independent task.\nDiversity Over Style The stylistic theory is an important part of Chinese traditional literary theory. Since there are great differences between different styles, such as prose, parallel prose, poetry and so on, we believe that the benchmark should cover as many styles as possible.\nFor instance, Wang et al. (2021) evaluate their model on ACC corpus which is built on Zuo Zhuan. However, Zuo Zhuan was written by Zuo Qiuming in East Zhou Dynasty, so the text features of Zuo Zhuan are relatively simple and unable to test the models for a variety of linguistic phenomenons of classical Chinese. Therefore, in this work, we refine datasets like this and combine them into welldefined datasets to build uniform sample sets.\nTo evaluate an NLU ability for classical Chinese, it is natural to handle classical Chinese text as it is. However, dealing with raw classical Chinese text without sentence segmentation is extremely difficult, which leads to the task becoming unsolvable. Therefore, besides the PUNC task itself handling sentence segmentation of classical Chinese text, segmented texts are adopted for other tasks.\nWhen selecting candidate corpora, we apply rules such as (1) having refined punctuation marks; (2) having more than 4 words in classification tasks; (3) being originally simplified Chinese character style preferred."
        },
        {
            "heading": "3.3 Data Collection and Ethical Concerns",
            "text": "We collect data from as many channels as possible, i.e, open source projects, public websites, competition data and education institutions. Since classical Chinese sources are all works from many years ago, people could use the corpora for free. For other texts, if the copyright issue is concerned, we have been granted to use the data in this work. For example, GLNER is data from a competition for\nclassical Chinese, we contact the owner by email and finally get licensed.\nClassical Chinese was officially and commonly used as a written language before recent times in East Asia, but now, modern languages have taken its place in these countries. In China, people learn classical Chinese at school but rarely use it in everyday life except for some poems and idioms. We could hardly collect any publicly available NLP datasets compared to modern Chinese. As a result, we design and create most of WYWEB datasets by ourselves."
        },
        {
            "heading": "3.4 Annotation",
            "text": "Annotation Process For different tasks, we apply different annotation processes. For PUNC, TLC and GJC tasks, the process is \"data collecting, extracting, proofreading and final review\". Specifically, since the data source is very important, these tasks are created using high quality and authoritative documents. Then paragraphs are sampled from every document with a certain small proportion, avoiding information leakage as much as possible. The annotators double-check every sample to ensure correctness of the computer work. Finally, domain experts review the whole work to get the result dataset.\nFor other tasks extracted from examination papers, the process is different because they need much more human workload. Given the papers (part of them are in PDF format or images), annotators copy or type the questions and do proofreading to assure the quality. Some new samples are also created by annotators to enrich the task. After the annotation work, the datasets are also sent to domain experts to carry out a final review.\nAnnotators This is a community-based project, and most of the annotators and reviewers are volunteer students and scholars who are interested in classical Chinese or natural language processing. The authors take on the remaining annotation tasks.\nIt takes us a long time to create this benchmark, so the annotators vary during the process. When selecting annotators, we make sure they have got a good language score in national college entrance examination and are familiar with classical Chinese. Some rules annotators following are: (1) dropping out confusing sentences; (2) double-checking rarely used words and dropping out sentences with uncertain rarely used words; (3) removing unnecessary symbols except specified punctuation marks.\nTo ensure the quality of the datasets, we asked some domain experts to do the final review.\nFor instance, Buddhist scriptures in classical Chinese are kind of important documents. As Buddhist scriptures are generally collated and proofread by Buddhist believers, the correctness of these texts is relatively reliable. We apply many Buddhist scripture texts in our tasks, hence the need for a review of scholars from the Buddhist Academy.\nQuality checks for WYWMT Translating classical Chinese sentences to modern Chinese is challenging. We follow Guzm\u00e1n et al. (2019) to filter texts collected from the internet. In addition, because classical Chinese sentences are usually short, the limitation of sample length is set to 5 to 200 characters."
        },
        {
            "heading": "3.5 Toolkit and Leaderboard",
            "text": "For the evaluation toolkit, we provide scripts implemented using PyTorch (Paszke et al., 2019) and transformers (Wolf et al., 2020), which can help the followers evaluate their models easily. Otherwise, they can upload their models to Hugging Face Model Hub 5 and contact us for the evaluated results. This toolkit is also released on WYWEB repository. Furthermore, we provide a leaderboard for the community to present the performance of each model. The leaderboard includes a general list and a sub list of each task. The results will be updated soon after the submission of models. As shown in Section E, we provide some details and screen-shots of the leaderboard."
        },
        {
            "heading": "4 Tasks",
            "text": "In this section, we describe tasks and datasets designed for specific aspects of classical Chinese NLP. These datasets, except GLNER and FSPC, are firstly created by ourselves. More details are shown in Appendix A."
        },
        {
            "heading": "4.1 Sequence Classification Tasks",
            "text": "GJC This task aims to work on the problem of ancient book classification which has been discussed since ancient times. We select a proportion of text from each category of the Daizhige project and divide them into the training set and evaluation set, where each sample is a selected paragraph from an article or book, ranging from a few dozen to hundreds of characters in length. More details are shown in Appendix A.5.\n5https://huggingface.co/\nTLC This task is to identify the written time of ancient books. We create the TLC dataset where each sample has a coarse-grained label (period) and a fine-grained label (Dynasty) forming a hierarchical structure. Similar to GJC task, each sample is a paragraph selected from ancient literature. More details are shown in Appendix A.4.\nFSPC FSPC (Fine-grained Sentiment Poetry Corpus) is an emotion recognition task for ancient rhythmic poetry, created by THUAIPoet (\u4e5d\u6b4c) group (Shao et al., 2021). Sentiments are annotated into 5 classes, i.e. negative, implicit negative, neutral, implicit positive, and positive. THUAIPoet designs a reasonable annotation mechanism to ensure annotations follow similar standards during the work process. See Appendix A.6 for details."
        },
        {
            "heading": "4.2 Sequence Labeling Tasks",
            "text": "PUNC This task is designed to perform text segmentation, i.e., adding punctuation marks to continuous ancient Chinese texts. The dataset uses some fairly authoritative texts, including historical records and Buddhist scriptures, to construct the samples. In order to reduce the complexity of the research, only a few commonly used punctuation marks are used. Each sample consists of the original text and its corresponding label sequence. Furthermore, the samples are paragraphs containing several sentences. See Appendix A.2 for details.\nGLNER GLNER is a named entity recognition task created by GULIAN (2020). Texts of the dataset are selected from ancient books and some other relevant literature. There are two kinds of entities in this dataset, i.e., classical book name and other which includes human name, location name, etc. Since the entity category is of coarse grain size, it is expected to implement new labeling work to refine this dataset in the future. See Appendix A.3 for details."
        },
        {
            "heading": "4.3 Sentence Pair Tasks",
            "text": "XuCi This task is designed to determine whether two function words in a sentence pair have the same meaning and usage. The words to be compared in the samples generally consist of one or two single characters. Each sample includes fields such as the pair of sentences, the position of the function words in the sentence, and a label indicating whether they are the same or not (True or False). See Appendix A.7 for details."
        },
        {
            "heading": "4.4 Reading Comprehension Tasks",
            "text": "WYWRC Similar to the RACE dataset (Lai et al., 2017; Sun et al., 2019), this task involves providing a classical Chinese paragraph, a question, and selecting the best answer from among four options. This task is quite challenging, as the model must possess a proficient understanding of both modern and classical Chinese. For analysis purpose, we separate the samples into 10 types according to their questions and answers. More details are shown in Appendix A.1.\nIRC Considering idiom comprehension is a very important part of classical Chinese learning, to evaluate the idiom comprehension ability of the model, we design and collect the IRC dataset. In IRC, given an idiom and its origin (most are in classical Chinese), the model is required to select the best explanation from four options. See Appendix A.8 for details."
        },
        {
            "heading": "4.5 Sequence to Sequence Tasks",
            "text": "WYWMT Machine translation of classical Chinese is a problem of great concern. This task is used to evaluate whether pre-trained models can effectively improve the performance of machine translation models for classical Chinese. Due to the limited number of samples, we only use WYWMT dataset for evaluation rather than fine-tuning. Furthermore, we separate this task from others and make a stand-alone leaderboard. See Appendix A.9 for details."
        },
        {
            "heading": "5 Baselines",
            "text": ""
        },
        {
            "heading": "5.1 Baseline Implementation",
            "text": "Sequence Labeling We get hidden states from the last layer of the model encoder, and pass them to a classifier to get sequence labels. See Figure 1.\nSentence Classification We get pooled output of model encoder, i.e., hidden state of [CLS] token, and pass it to a classifier to get sequence labels. See Figure 2.\nReading Comprehension We encode each option concatenated with paragraph-question, pass the hidden states to a shared classifier to get prediction score and choose the best as final answer. See Figure 3.\nToken Similarity Similar to sentence classification, we encode sentence pairs and get the hidden state of the corresponding token, then we use {u ; v; |u - v|} to represent the similarity score, where we mark the vectors as u and v. See Figure 4.\nMachine Translation We implement this task as sentence pair with a prefix attention mask to adapt BERT-like models. To save inference time cost, the sequence output of the target sentence is greedy decoded. Note that this implementation is untypical for the sequence-to-sequence models, and is just used to reflect the capabilities of the model itself.\nAll the experiments are implemented using PyTorch (Paszke et al., 2019)."
        },
        {
            "heading": "5.2 Pre-trained Models",
            "text": "GuwenBERT GuwenBERT has three versions, including GuwenBERT-base, GuwenBERT-large, GuwenBERT-fs-base. While GuwenBERT-base and GuwenBERT-large are trained based on RoBERTa-wwm-ext (Cui et al., 2021b), a modern Chinese pre-trained model, and then continue trained on classical Chinese corpus, GuwenBERTfs-base is trained purely on classical Chinese corpus.\nRoBERTa-classical-chinese RoBERTaclassical-chinese has two versions, including RoBERTa-classical-chinese-base-char (RoBERTaCCBC), RoBERTa-classical-chinese-large-char (RoBERTa-CCLC). This is a RoBERTa model pre-trained on classical Chinese texts, derived from GuwenBERT-base. Character-embeddings are enhanced into traditional/simplified characters (Koichi et al., 2022).\nSikuBERT, SikuRoBERTa These models are pre-trained on the verified high-quality \u201cSiku Quanshu\u201d (Wang et al., 2021). Note that these two models are pre-trained on traditional Chinese. In the fine-tuning stage, we convert simplified Chinese corpus into traditional Chinese.\nDeBERTa-base Based on the structure of DeBERTa (He et al., 2020), we pre-trained the model on DaiZhiGe corpus from scratch.\nRoBERTa-wwm-ext This model is trained with BERT (RoBERTa) structure (Cui et al., 2021b) and whole word masking.\nNote that there are not as many pre-training models of classical Chinese as modern Chinese. We collect all models of classical Chinese accessible to evaluate and take them as baselines. More details of these models can be found in Appendix C."
        },
        {
            "heading": "5.3 Experiment Setting",
            "text": "For the evaluation, we fine-tune the pre-trained models mentioned above. For each task, we train 3 runs, and the model with the best development score is used for testing. When the learning rate decreases to a specified small value or the performance does not improve for 5 evaluations, the training is stopped. More details of hyper-parameters are shown in Appendix D."
        },
        {
            "heading": "5.4 Human Performance",
            "text": "For all tasks, we evaluate human performance following the principle of SuperGLUE: extract 30 samples in the training phase, and then sample 100 items from the test set in the testing phase. We collect test results from three annotators and calculate the human performance. The annotators are all college students majoring in ancient Chinese. The results of human performance are shown in Table 2 and Table 3. More details are shown in Appendix B."
        },
        {
            "heading": "5.5 Benchmark Results & Analysis",
            "text": "As shown in Table 2, we present the performance of existing baseline models in classical Chinese NLU tasks. Since evaluation metrics of sequenceto-sequence tasks are different from NLU tasks, as shown in Table 3, we evaluate each model on WYWMT task independently with several metrics, including BLEU, chrF2, TER and ROUGE.\nFrom the results, it can be seen that some regular patterns, i.e. \"the bigger (model scale and batch size), the better\"; \"the more (data and train steps), the better\" appear as described in other experiments.\nDeBERTa-base (He et al., 2020) performs best on this benchmark showing that the model structure and training strategy are both effective. Note\nthat this model is pre-trained just according to the default settings of DeBERTa V2 English version without the convolution layer and purely on classical Chinese. Meanwhile, some techniques that have obvious effects in Chinese are not used, such as Whole Word Masking (Cui et al., 2021b), etc. All models pre-trained on classical Chinese get better scores than chinese-roberta-wwm-ext (Cui et al., 2021b) pre-trained on modern Chinese corpus. Similarly, models trained on both classical Chinese and modern Chinese perform better on tasks involving both scripts, such as WYWRC, IRC and GLNER.\nFor FSPC task, composed of ancient Chinese rhythmic poems, SikuRoBERTa (Wang et al., 2021) performs the best, which is pre-trained with a high-quality classical Chinese corpus of Si Ku Quan Shu rather than on Daizhige.\nSince poems are different from general texts, the models could learn a better representation of ancient words on Si Ku Quan Shu instead of on Daizhige. The two large models yield similar scores to DeBERTa-base but much better than other smaller ones, however, the parameter size of the large model is 3 times larger than that of DeBERTabase.\nOn WYWMT task, GuwenBERT-base achieves the best score with its pre-training strategy, which initializes the parameters of the transformer model from a pre-trained model and trains the model via freezing encoder layers to translate modern Chinese knowledge to classical and updates all parameters of the model. With the strategy, the model could learn a good representation of both modern and classical Chinese and achieve the best perfor-\nmance on translation tasks. Compared with human performance, all the models have a big gap with the artificial results, especially on tasks WYWRC, XuCi, and IRC, which require a lot of implicit knowledge.\nOne limitation of our evaluation is, the models we collected are all BERT or RoBERTa style and are lacking some variety. Furthermore, the models we evaluated maybe not achieve the best score in this baseline due to differences among them. However, they are fine-tuned with similar hyperparameters, so that the results are comparable as expected."
        },
        {
            "heading": "5.6 Task Probing",
            "text": "According to scores in Table 2, we choose the most challenging task, WYWRC for further exploration and analysis. As shown in details of Table 4 in Appendix A.1, this task has a variety of questions, which is different from traditional machine reading comprehension tasks, and is more difficult for existing models.\nFollowing previous work of Wang et al. (2022), we assess WYWRC dataset for the 4-dimensional MRC capabilities, which are word reading, sentence reading, word understanding and sentence understanding. To adapt to classical Chinese, we make some adjustments to the metrics and establish necessary dictionaries. The main adjustment is oriented towards readability, as Chinese and English have significant differences. Instead of the Flesch-Kincaid index, which could not be suitable for Chinese, we adopt the readability index for Chinese issued by (Yuying, 2020). The final evaluation results are shown in Figure 5. As when the capability-specific value score increases, the\ndifficulty of the problem that the model is understanding also increases and the correlation between sentence understanding and model performance is higher (Wang et al., 2022), we can see that the proportion of high scores in the WYWRC\u2019s sentence understanding (v4) category is relatively large, thus poses a huge challenge to the pre-trained models.\nIn Table 5, we probe the accuracy of the test set to find that there are significant differences in scores between the different types of samples. Since the number of type 8 is small and it is a really hard question requiring extra knowledge, it is not very meaningful to discuss type 8.\nIt can be seen that Type 2 is the most difficult of the categories with a score of 17.9. In this type of question, given a list of characteristics or actions of the protagonist in a passage, the test-taker is required to select one option that either conforms to or does not conform to the given information. Some of the questions require the test-taker to have some reasoning ability.\nType 5 is the easiest category of problems to solve, with an average score of 66.5. This type of problem involves segmenting sentences with contextual information, similar to the PUNC task, with the difference being that the four options are confusing to each other. Nevertheless, because the problem is relatively simple and does not require much knowledge, it is easy for the models to handle.\nType 0 is the category with the largest number of samples, but its score is not high, indicating a considerable level of challenge. This type of problem involves several statements about an article, and the task is to identify the correct or incorrect option. Solving this type of problem requires not\nonly understanding the content of the article, but also sometimes additional knowledge support and even reasoning. Therefore, this type of problem not only requires a good pre-trained model, but may also requires better fine-tuning methods and the injection of more knowledge to achieve better accuracy.\nAll models have a significant gap compared to human scores, mainly due to the additional knowledge and reasoning abilities of the testers. This suggests that while current natural language processing models have made significant progress in understanding and handling language, there is still a long way to go before they can approach the level of human language comprehension and reasoning."
        },
        {
            "heading": "6 Conclusions and Future Work",
            "text": "In this paper, we introduce a NLP benchmark for classical Chinese, which contains nine NLP tasks and datasets respectively to help researchers to evaluate and analyze NLP models. Also, we build a toolkit for reproduction and a leaderboard online for the community.\nThe study of ancient Chinese is a highly specialized subject, so the professionalism of this benchmark may need to be further improved. On the other hand, there is a big gap between the performance of the classical Chinese models on this benchmark with other leader-boards. Better models are needed to handle more linguistic features of classical Chinese. Furthermore, to resolve traditional and simplified character issue, traditional style tasks are meaningful to researchers. We consider it as a future work of the community.\nClassical Chinese is a treasure of the entire human cultural history. We contribute this work with the hope of helping the entire community to be more prosperous. Our work will be an open, community-driven project which improves with the advancement of technology."
        },
        {
            "heading": "7 Limitations",
            "text": "In this work, we contribute an evaluation benchmark for classical Chinese NLP tasks. We did our best to create as comprehensive a well-defined task set as possible, something no one has done before. However, our work has several limitations due to lacking expertise knowledge and data.\nWhen designing the tasks, we got a lot of inspiration from the middle school Chinese test paper. thousands of test papers are collected in order to ex-\ntract data for NLP tasks. During the work process, we learn that it is difficult to extract a sufficient number of questions of a single type. The main difficulty is due to the variety of questions on the test papers and the mixture of the language of classical and modern Chinese. Finally, we create Xuci task, WYWRC task and IRC task from the test papers and related literature but failed to create solvable natural language inference tasks.\nWhen working on some datasets which have less corpus, i.e, the Xuci task, we find it very difficult to calibrate existing samples or create new ones, resulting a small dataset size.\nMeanwhile, the category rule we followed in the GJC task is not certified by authoritative experts, so this method is not completely reliable if viewed by experts of classical Chinese.\nIn this work, tasks for more aspects of grammar phenomenon are lacking. It\u2019s expected that more classical Chinese experts and researchers join this work in the future to solve the above problems.\nOn the other hand, we lack a diagnostic dataset compared to other benchmarks. This is because similar data (NLI corpus generally) are even more difficult to retrieve. However, this benchmark works for NLP researchers even though the diagnostic dataset is missing. This issue is also expected to be solved in future work."
        },
        {
            "heading": "8 Impact Statement",
            "text": "This work aims to help in enhancing the capabilities of pre-trained models for classical Chinese language basic infrastructure. Classical Chinese is a wealth of all humanity, with a great influence worldwide. We hope to help to improve the prosperity of the classical Chinese NLP community and better mine this spiritual wealth.\nOur data sources, including raw classical Chinese text and related modern text, mainly come from official and authoritative releases, so there are generally not many ethical concerns. As mentioned above, when using copyrighted texts, we have got their permission. In terms of data sets, we expect to objectively and comprehensively reflect the language characteristics of classical Chinese as much as possible, so we also try to be as general as possible when sampling, without deliberately doing content filtering.\nBias and Race Concern Classical Chinese was born in ancient times when people entered the patriarchal society, and it is inevitable to be some\ngender bias. Since such content is not common, we believe that there will not be many ethical issues. Additionally, one of the most important categories in ancient texts - historical books - records a large number of wars between the central government and surrounding minority ethnic groups, so there are derogatory and insulting terms among different ethnic groups. But fusions of different ethnic groups have been more often in the history books and no contents so-called racial discrimination in the current society exist.\nEnergy Cost Practical pre-training language model requires large amounts of computation, so the cost and efficiency of such models should be taken into account(Brown et al., 2020). To achieve a better score on the leaderboard, many tries of pre-training may be required hence a large amount of energy consumption. So models with better efficiency are preferred for environment-friendly reasons. For instance, in this work, the DeBERTa-base model scores better than the large models which have three times more parameters."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank everyone who contributes dataset to this project. We are also grateful to the annotators and scholars who have spent much of their time and effort helping with the creation of this benchmark. This work was supported by Zhejiang Provincial Natural Science Foundation of China under Grant No. LZ23F020009, the NSFC under Grant No. 62072399, Chinese Knowledge Center for Engineering Sciences and Technology, MoE Engineering Research Center of Digital Library. Thanks to Hangzhou AI Computing Center for the computing resource. Special thanks to the following companies and organizations: Hangzhou Xiaoniao AI Co., Ltd., Hangzhou Bolazhe Co., Ltd. Gulian (Beijing) media tech. Co., Ltd."
        },
        {
            "heading": "A Data Details and Annotation",
            "text": "In this section, we use a \"[SEP]\" mark to denote separation between two parts of a sample. And we try to translate the classical sentence to English to make it easier to understand. It should be noted that the English translations of the following texts are machine-translated, which are not very accurate."
        },
        {
            "heading": "A.1 WYWRC",
            "text": ""
        },
        {
            "heading": "A.1.1 Details",
            "text": "As previously mentioned, reading comprehension is a crucial aspect of classical Chinese learning, and is tested annually in the college entrance exam. With the assistance of middle school teachers, we have collected thousands of examples from examination papers. This dataset is in JSON format. Statistics are show in Figure 21 Figure 22 and Table 10. All the samples are separated into 10 types according to the difference between the questions which are shown in Table 4. Furthermore, as shown in Figure 5, we carry out a competency assessment of MRC capabilities to probe the challenge of this task in many fine-grained metrics."
        },
        {
            "heading": "A.1.2 Annotation",
            "text": "\u2022 Prepossess test papers, including OCR, layout\nparser and then copy all reading comprehension problems;\n\u2022 Annotate, including filter the sub-problems for which suitable for machine learning and correct misspelling, etc.;\n\u2022 Cross double-check between annotators;\n\u2022 Final review by experts."
        },
        {
            "heading": "A.2 PUNC",
            "text": ""
        },
        {
            "heading": "A.2.1 Details",
            "text": "This task is designed for text punctuation. Since there are not any punctuation marks in traditional Chinese literature, discriminatory of sentence punctuation is important for reading ancient books. Even though ancient Chinese researchers have made great efforts in the proofreading and sorting out of ancient books, there are still a large number of ancient books without punctuation waiting to\nbe solved (Qi, 2022; Li, 2002). So that punctuation task is useful for classical Chinese researchers. Therefore, all related works evaluate their models mainly on this task.\nTo make sure the time distribution of the corpus as uniform as possible, we select history books as source data for this task including \u4e8c\u5341\u56db\u53f2(the Twenty-Four Histories),\u6625\u79cb(The Spring and Autumn Annals), \u6218\u56fd\u7b56(Strategies of the Warring States Period) and so on. The corpus contains historical books from the Zhou Dynasty to the Republic of China, which cover nearly three thousand years (1046 BC to 1927). All of the books are concatenated and shuffled by paragraph, then sampled by a reasonable rate and finally split into task datasets.\nThis dataset is in sequence pair TSV format. Every sample is a pair of source text and label sequence as shown following. We choose eight punctuation marks as prediction target in this dataset. Statistics are show in Figure 10 Figure 11 and Table 6.\n\u58ec\u620c\u8bcf\u5b9a\u79d1\u4e3e\u6d41\u5bd3\u4eba\u540d\u989d\u8499\u53e4\u8272\u76ee\u5357 \u4eba\u5404\u5341\u4e94\u540d\u6c49\u4eba\u4e8c\u5341\u540d OO\uff0cOOOOOOOO\uff0cO\u3001O\u3001OOOOO\uff0c OOOO\u3002 On the 24th, an imperial edict was issued to establish the quota of expatriates in the imperial examination, 15 for Mongolians, 15 for colored-eyes and 20 for Han Chinese.\n\u8c22\u8087\u300a\u5317\u6cb3\u7eaa\u300b\u516b\u5377\u300a\u7eaa\u4f59\u300b\u56db\u5377\u9664 \u575b\u897f\u90ca\u574e\u5176\u51fb\u9f13\u767e\u7075\u81f3\u6b62\u7ed3\u4f5c\u4e3b\nOOOOOOOOOO\uff0cOOOOO|OOO\uff0cOOO \u3002OOO\uff0cOOO\u3002 Xie Zhaozhe wrote eight volumes of Beihe Ji and four volumes of Ji Yu. At the altar in the western suburbs, playing drums, hundreds of gods stopped here and became the leader of the alliance."
        },
        {
            "heading": "A.2.2 Annotation",
            "text": "\u2022 Extract corpora from the Twenty-Four Histo-\nries and some other history books;\n\u2022 Filter out low quality samples, for example,\ntoo short or punctuation too few;\n\u2022 Sample from the result corpora with a specific ratio;\n\u2022 Annotator check;\n\u2022 Final review by experts."
        },
        {
            "heading": "A.3 GLNER",
            "text": ""
        },
        {
            "heading": "A.3.1 Details",
            "text": "This dataset is in JSON format as shown below. Every sample consists two keys which are \"text\" and \"label\", and every label is represented as start index, end index and category style. Statistics are show in Figure 12 Figure 13 and Table 7.\n{ \"text\": \"\u8c22\u7edb \u4e09\u6708\u620a\u620c\uff0c\u77e5\u793c\u4eea \u9662\u3001\u5175\u90e8\u5458\u5916\u3001\u77e5\u5236\u8bf0\u8c22\u7edb\u77e5\u9093\u5dde\u3002 \u5341\u4e00\u6708\u5df1\u9149\uff0c\u5352\u3002\u6b27\u6587\u3002\u957f\u7f16\uff1a\u7edb\u6309 \u53ec\u4fe1\u81e3\u6545\u8ff9\uff0c\u8ddd\u57ce\u4e09\u91cc\uff0c\u58c5\u6e4d\u6c34\uff0c\u6ce8 \u94b3\u5e90\u9642\uff0c\u6e89\u7530\uff0c\u8bf7\u590d\u4fee\u4e4b\u3002\u53ef\uff0c\u7f62\u5dde \u4eba\u5c81\u5f79\u3002\", \"label\": [[0, 2, \"other\"], [21, 23, \"other\"], [24, 26, \"other\"], [35, 36, \"other\"], [38, 40, \"bookname\"], [41, 42, \"other\"], [43, 46, \"other\"], [59, 62, \"other\"]] }\n{ \"text\": \"\u516d\u6708\u5df1\u672a\uff0c\u90d1\u5c45\u4e2d\u7b49\u4e0a\u54f2\u5b97 \u5fa1\u96c6\u3002\u58ec\u620c\uff0c\u666f\u7075\u5bab\u5efa\u79a7\u7956\u6bbf\u5ba4\u3002\u590d \u5e7f\u3001\u60e0\u3001\u5eb7\u3001\u8d3a\u5dde\u65e7\u94f8\u5939\u9521\u94b1\u76d1\u3002\u8f9b \u672a\uff0c\u6e56\u5357\u8def\u63d0\u70b9\u5211\u72f1\u9648\u4e49\u592b\u594f\u90b5\u9633\u53bf \u8d3c\u5e73\u3002\", \"label\": [[5, 8, \"other\"], [10, 14, \"bookname\"], [18, 21, \"other\"], [22, 24,\n0 100 200 300 400 500 0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\nSample Length\nC ou\nnt"
        },
        {
            "heading": "A.4 TLC",
            "text": ""
        },
        {
            "heading": "A.4.1 Details",
            "text": "Since ancient books have been handed down over a period of more than 2,000 years, it is a very meaningful and challenging task to identify the writing time of ancient books according to the characteristics of the text. Chang et al. (2021) propose that identifying written time of literature is help-\nful for understanding works. Being classified according to the period, ancient Chinese is generally divided into ancient (Pre-Qin and Han Dynasty), mid-ancient (Jin Dynasty to Song Dynasty) and late-ancient (Yuan, Ming, Qing Dynasty) (Wang, 2004). Furthermore, in the process of the development of classical Chinese, each dynasty has its own unique characteristics (Li et al., 2013). In such background, we collect about 300 ancient books and famous articles which have exact time of writing, and sample a reasonable number of paragraphs from the texts. This dataset is in TSV format as shown below. The three segments of a sample are Period label, Dynasty label and source text respectively. Statistics are show in Figure 14, Figure 15 Figure 16 and Table 8.\n\u8fd1\u53e4 [SEP]\u5143\u660e [SEP]\u4e3b\u6cbb\u98ce\u75f9\uff0c\u7b4b \u9aa8\u4e0d\u4ec1\uff0c\u529f\u4e0e\u8102\u540c\u3002\u8865\u865a\u7fb8\u3002 Indications of wind arthralgia, numbness of the muscles and bones, its power is the same as fat. Make up for weakness. \u4e2d\u53e4 [SEP]\u9b4f\u664b\u5357\u5317\u671d [SEP]\u4e1c\u89c2\u6c49 \u8bb0\u66f0\uff1a\u7f8c\u4ec0\u957f\u5de9\u4fbf\u3002\u7136\u66f4\u76d6\u5176\u79cd\u4e5f\u3002 \u5c1a\u4e66\u66f0\uff1a\u6b7c\u53a5\u6e20\u9b41\u3002\u65e2\u5df2\u88ad\u800c\u9986\u5176 \u53bf\u3002\u5de6\u6c0f\u4f20\u66f0\uff1a\u51e1\u5e08\u8f7b\u66f0\u88ad\u3002\u675c\u9884 \u66f0\uff1a\u63a9\u5176\u4e0d\u5907\u3002\u5b50\u4ee5\u7707\u5c14\u4e4b\u8eab\uff0c\u4ecb \u4e4e\u91cd\u56f4\u4e4b\u91cc\uff1b\u7387\u5be1\u5f31\u4e4b\u4f17\uff0c\u636e\u5341\u96c9\u4e4b \u57ce\u3002 Dongguan Han Ji said: Gong Bian, the leader of the Qiang people. But it is another cover. The book of Shang said: Destroy the head of the thief. Has attacked Fianxian and stayed in a hotel. Zuo\u2019s biography said: \"Any army with light baggage is called\u88ad.\" Du Yu said: Attacking who is unprepared. With a small body, you are in the center of the encirclement, leading the weak, and defending the city of ten feet. \u8fdc\u53e4 [SEP]\u5148\u79e6 [SEP]\u9f50\u664f\u6853\u5b50\u5352\uff0c \u664f\u5a74\u7c97\u65a9\uff0c\u82f4\u3001\u5e26\u3001\u6756\uff0c\u83c5\u5c66\uff0c\u98df \u9b3b\uff0c\u5c45\u501a\u5e90\uff0c\u5bdd\u82eb\u3001\u6795\u8349\u3002 When father died, Yan Ying wore coarse cloth mourning clothes, made filial piety clothes, belts and walking sticks of coarse linen, wore shoes made of thatch, ate thatch, ate thatch, lived in a leaning hut, and slept on a straw mattress.\n50 100 150 0\n2k\n4k\n6k\n8k\n10k\nSample Length\nC ou\nnt"
        },
        {
            "heading": "A.4.2 Annotation",
            "text": "\u2022 Annotator check;\n\u2022 Final review by experts."
        },
        {
            "heading": "A.5 GJC",
            "text": ""
        },
        {
            "heading": "A.5.1 Details",
            "text": "The Si Ku Quan Shu had formed a classification method of four parts of Jing, Shi, Zi, Ji (Confucian classics, historical records, philosophical writings, and miscellaneous works), and 40 categories. This is the authoritative method till now. The largest classical Chinese corpus dataset Daizhige extends the method to 10 collections. Since this corpus is actually the basis of most of classical Chinese NLP research, we apply this method to design our text classification task following CCLUE. This dataset is in text\u2013category format as shown below. Statistics are show in Figure 17 Figure 18 and Table 9.\n\u7136\u5219\u4e16\u6240\u8c13\u96c5\u4e50\u8005\uff0c\u672a\u5fc5\u5982\u53e4\uff0c\u800c\u6559 \u574a\u6240\u594f\uff0c\u5c82\u5c3d\u4e3a\u6deb\u58f0\u54c9\uff1f\u201d [SEP] \u827a \u85cf However, the elegant music in the society is not necessarily the same as in ancient times, but is the music played by Jiaofang all debauched music? \u6709\u4e27\u5fc5\u6c42\u7267\u5e08\u6b93\uff0c\u72ec\u81ea\u5165\u623f\u628a\u95e8 \u63a9\u3002[SEP]\u5b50\u85cf If there is a funeral, you must find the priest to be buried, entering the room alone and close the door. \u201c\u68a6\u5e7b\u7a7a\u82b1\uff0c\u4f55\u52b3\u628a\u6349\uff1f\u5f97\u5931\u662f\u975e\uff0c \u4e00\u65f6\u653e\u5374\u3002\u201d [SEP]\u4f5b\u85cf \"Dreaming of empty flowers, how to take the handle? Do not care about the right and wrong, and put it back at once. \"\" \u7fb2\uff0c\u4e43\u5929\u7687\u4f0f\u7fb2\u6c0f\u4e5f\u3002\u9f50\u9a71\uff0c\u5373\u5e76 \u9a7e\u3002\u5143\u59cb\uff0c\u4e07\u6709\u4e07\u65e0\u4e4b\u7956\u53f7\u3002\u6bd4\u80a9\uff0c \u5e76\u7acb\u4e4b\u4e49\u3002\u662f\u8db3\u4e0a\u6587\u6bd4\u55bb\u4e5f\u3002\u5b66\u8005\u614e \u6bcb\u4f4f\u76f8\uff0c\u662f\u5373\u821c\u4f55\u4eba\u4e5f\uff0c\u4e88\u4f55\u4eba\u4e5f\u4e91 \u5c14\u3002[SEP]\u9053\u85cf Xi is also called the Emperor Fuxi. \u201c\u9f50 \u9a71\u201d means the two marched side by side. \u201c\u5143\u59cb\u201d, the ancestor of all things and nothing. \"\u6bd4\u80a9\" means to stand side by\n0 50 100 150 200 250 0\n5k\n10k\n15k\n20k\n25k\n30k\n35k\n40k\nSample Length\nC ou\nnt"
        },
        {
            "heading": "A.5.2 Annotation",
            "text": "\u2022 Final review by experts."
        },
        {
            "heading": "A.6 FSPC",
            "text": "This dataset is in JSON format as shown below. The sentiment labels are of five specifications which shift from negative to positive. Statistics are show in Figure 19 Figure 20.\n{ \"poet\": \"\u8303\u4ef2\u6df9\", \"poem\": \"\u9759\u6620\u5bd2\u6797\u665a\u672a\u82b3|\u4eba\u4eba\u6b32\u770b \u5bff\u9633\u5986|\u7389\u989c\u987b\u508d\u97f6\u6625\u7b11|\u83ab\u6597\u4e25\u98ce\u4e0e \u6076\u971c\", Quietly reflecting the cold, the forest is late and not fragrant | Everyone wants to see Shouyang makeup | Jade face must be close to spring smile | Do not fight with strong wind and cold frost. \"dynasty\": \"\u5b8b\", \"sentiments\": { \"holistic\": \"implicit positive\", \"line1\": \"implicit positive\", \"line2\": \"neutral\", \"line3\": \"implicit positive\", \"line4\": \"neutral\" }, \"title\": \"\u548c\u63d0\u5211\u8d75\u5b66\u58eb\u63a2\u6885\u4e09\u7edd\" }, { \"poet\": \"\u738b\u7ef4\", \"poem\": \"\u72ec\u5728\u5f02\u4e61\u4e3a\u5f02\u5ba2|\u6bcf\u9022\u4f73\u8282 \u500d\u601d\u4eb2|\u9065\u77e5\u5144\u5f1f\u767b\u9ad8\u5904|\u904d\u63d2\u8331\u8438\u5c11 \u4e00\u4eba\", Being alone and a stranger in a foreign land | I miss my relatives every time during the festival | I know my brothers climb a high place from afar | Everyone plant cornel all over the place but me \"dynasty\": \"\u5510\", \"sentiments\": { \"holistic\": \"implicit negative\", \"line1\": \"implicit negative\", \"line2\": \"implicit negative\", \"line3\": \"neutral\", \"line4\": \"implicit negative\" },\nA.7 Xuci"
        },
        {
            "heading": "A.7.1 Details",
            "text": "Function words (Xu ci in Chinese) have no real meaning and generally cannot be used as a single\n24 26 28 30 32 0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\nSample Length\nC ou\nnt\nFigure 20: Statistic of Sample Length of FSPC dataset. Note that this dataset only contains five-character quatrains and seven-character quatrains.\nsentence element (Liu et al., 1995). They are very important in classical Chinese but easily confused. Relevant topics are part of the basic knowledge for Chinese students which appears in the college entrance exam every year. We collect sentence pairs with function words from examination papers with help of middle school teachers to construct this dataset.\nThis dataset is in TSV format. Statistics are show in Figure 21 Figure 22 and Table 10.\n\u4f7f\u592b\u90aa\u6c61\u4e4b\u6c14\u65e0\u7531\u5f97\u63a5\u7109\u3002[SEP]\u590d \u9a7e\u8a00\u516e\u7109\u6c42\u3002[SEP] 10, 10 [SEP] 4, 4 [SEP] f so that there is no way for those evil and filthy atmospheres to reach them. [SEP]What am I driving for? \u4e0a\u5b98\u4ee4\u6c11\u9001\u725b\u7f8a\u4e4b\u9655\u897f\u3002[SEP] \u4e45 \u4e4b\uff0c\u4e3e\u4e8e\u671d\u3002[SEP] 7, 7 [SEP] 1, 1 [SEP] f The superior commander sent cattle and sheep to Shaanxi. [SEP] After a long"
        },
        {
            "heading": "A.7.2 Annotation",
            "text": "correct misspelling, etc.;\n\u2022 Cross double-check between annotators;\n\u2022 Final review by experts.\nA.8 IRC"
        },
        {
            "heading": "A.8.1 Details",
            "text": "Idiom is one of the major features of Chinese culture. Most of the idioms are long-standing fixed phrases, derived from ancient classics or writings, historical stories, and oral stories. For idiom comprehension, there are other tasks (Zheng et al., 2019) ready. However, they are mainly aiming to test modern Chinese texts with idioms. To focus on classical Chinese, we implement this task\u3002This dataset is in JSON format. Every sample consist of four fields which are \"idiom\", \"options\", \"label\" and \"origin\". The ground truth \"label\" is best fit of the four options. Statistics are show in Figure 23 Figure 24 and Table 11.\n{ \"idiom\": \"\u773c\u53bb\u7709\u6765\", eye to eyebrow \"options\": [ \"\u706b\u70e7\u5230\u7709\u6bdb\u3002\u6bd4\u55bb\u4e8b\u5230\u773c\u524d\uff0c\u975e\u5e38 \u6025\u8feb\u3002\", The fire burned to the eyebrows. The metaphor is very urgent. \"\u5f62\u5bb9\u4e8b\u60c5\u5df2\u5230\u773c\u524d\uff0c\u60c5\u52bf\u5341\u5206\u7d27 \u8feb\u3002\", Describe the matter has come to the front, the situation is very urgent. \"\u539f\u6307\u773c\u524d\u89c1\u5230\u7684\u3002\u540e\u5f62\u5bb9\u7528\u7709\u773c\u4f20 \u60c5\u3002\", It meant what was seen. After describing the use of eyebrows teasing. \"\u5f62\u5bb9\u7709\u773c\u542b\u60c5\u793a\u610f\u7684\u795e\u6001\u3002\" Describe the expression of the eyebrows showing affection. ], \"label\": 2, \"origin\": \"\u843d\u65e5\u82cd\u832b\uff0c\u98ce \u624d\u5b9a\uff0c\u7247\u5e06\u65e0\u529b\u3002\u8fd8\u8bb0\u5f97\u7709\u6765\u773c\u53bb\uff0c \u6c34\u5149\u5c71\u8272\u3002\" The setting sun is vast, the wind is fixed, and the sails are weak. I still remember the frowning, the water and the mountains. },"
        },
        {
            "heading": "A.8.2 Annotation",
            "text": "\u2022 Prepossess test papers, including OCR, layout\nparser and then copy all idiom comprehension problems;\n\u2022 Study the idiom dictionary to extract idiom comprehension problems;\n\u2022 Annotate, including filter the sub-problems for which suitable for machine learning and correct misspelling, etc.;\n\u2022 Cross double-check between annotators;\n\u2022 Final review by experts."
        },
        {
            "heading": "A.9 WYWMT",
            "text": ""
        },
        {
            "heading": "A.9.1 Details",
            "text": "Classical Chinese is a very concise written language, so it\u2019s not easy for everyone to understand.\nScholars often translate classical Chinese into modern Chinese with notations to make it easier for people to read. We consider it as an in-language translation or rewriting task because the source and target could share the same vocabulary and some semantic features.\nThis dataset is filtered and calibrated from hundreds of translated classical Chinese books collected from multiple channels. Since allusions and quotations appear frequently in classical Chinese, and these references may have a time span of thousands of years, it\u2019s not easy to construct a very well-established dataset by ourselves.\nThis dataset is in sentence pair TSV format. Samples are represented as \"source\" and \"reference\" segment which are separated by \"tab\". Statistics are show in Figure 25 and Table 12.\n\u5171\u56db\u91cc\uff0c\u53c8\u8d8a\u4e00\u5188\u810a\u800c\u4e0b\uff0c\u5176\u810a\u9ad8\u4e0d \u53ca\u9ad8\u4e95\u4e4b\u534a\uff0c\u800c\u5b9e\u4e3a\u897f\u5317\u6765\u8fc7\u810a\u4ee5\u8d8b \u6e05\u79c0\u8005\u4e5f\u3002[SEP]\u5171\u56db\u91cc\uff0c\u53c8\u8d8a\u4e00\u9053 \u5188\u810a\u540e\u4e0b\u8d70\uff0c\u8fd9\u4e2a\u5188\u810a\u9ad8\u5904\u4e0d\u5230\u9ad8\u4e95 \u7684\u4e00\u534a\uff0c\u4f46\u5b9e\u9645\u4e0a\u662f\u4ece\u897f\u5317\u524d\u6765\u8d8b\u5411 \u6e05\u79c0\u5c71\u7684\u5ef6\u4f38\u800c\u8fc7\u7684\u5c71\u810a\u3002 After a total of four miles, we went down after another ridge. This ridge was less than half of the height of Gaojing, but it was actually a ridge extending from the northwest towards Qingxiu Mountain. \u8bfb\u6027\u7406\u4e66\u65f6\uff0c\u5219\u6742\u4ee5\u8bd7\u6587\u5404\u96c6\uff0c\u4ee5\u6b67 \u5176\u8d8b\u3002[SEP]\u5728\u8bfb\u6027\u7406\u4e66\u7684\u65f6\u5019\uff0c\u53c8 \u63ba\u6742\u5199\u8bd7\u6587\uff0c\u8d70\u4e86\u5c94\u8def\u3002 When I read books about ethics, I mixed it with writing poetry, so I went to a wrong road.\n0 100 200 300 400 500 0\n100k\n200k\n300k\n400k\nModern Chinese Classical Chinese\nSample Length\nC ou\nnt\nFigure 25: Statistic of Classical Sample Length of WYWMT dataset."
        },
        {
            "heading": "A.9.2 Annotation",
            "text": "\u2022 Crawl classical Chinese articles with modern\ntranslation from all channels, note that these articles are open and free for everyone;\n\u2022 Text align;\n\u2022 Split long paragraphs into shorter sentences;\n\u2022 Filter low quality examples;\n\u2022 Final review by experts."
        },
        {
            "heading": "B Details of human evaluation",
            "text": "To obtain a more reliable evaluation of model performance, we chose students majoring in Classical Chinese to provide an upper bound score. This sets a higher value for the benchmark and helps researchers improve their models. The students were given access only to the train-set during the test, without any additional tools. For different tasks, set of the test examples is different. See Table 13."
        },
        {
            "heading": "C Details of Models Evaluated",
            "text": "In this section, we present the details of pretrained language models we used, including guwenbert-base, guwenbert-large, guwenbertbase-fs, roberta-classical-chinese-base-char, roberta-classical-chinese-large-char, SikuBERT, SikuRoBERTa, DeBERTa-base and RoBERTawwm-ext. As shown in 14, the masking, scale, corpus, vocabulary and parameter initialization are different in each pre-trained language model."
        },
        {
            "heading": "D Hyper-parameters for fine-tuning",
            "text": "As shown in Table 15, we present the hyperparameters applied in fine-tuning. For different scale of pre-trained language model, we set different learning rates. In large scale, we set learning rates with 5e-6, 8e-6, 9e-16 and 1e-5. In base scale, we set learning rates from 1e-5 to 5e-5. We set warmup to 0.1, maximum epochs to 10. For Adam, we set \u03f5 to 1e-6, \u03b21 and \u03b22 to 0.9 and 0.999 respectively. Meanwhile, we use linear for LR decay and set weight decay to 0.01."
        },
        {
            "heading": "E Leader-board",
            "text": "Following other benchmark leaderboard, this leaderboard is designed containing an overall list and several sub-lists. Metrics of the tasks are listed in Table 1. Since most of the tasks are common tasks, we did not design more metrics for them. For the sequence-to-sequence task of WYWMT, due to the shortcomings of various indicators, we calculate several metrics for the prediction results to make that clearer (3)."
        }
    ],
    "title": "WYWEB: A NLP Evaluation Benchmark For Classical Chinese",
    "year": 2023
}