{
    "abstractText": "Many word-level adversarial attack approaches for textual data have been proposed in recent studies. However, due to the massive search space consisting of combinations of candidate words, the existing approaches face the problem of preserving the semantics of texts when crafting adversarial counterparts. In this paper, we develop a novel attack strategy to find adversarial texts with high similarity to the original texts while introducing minimal perturbation. The rationale is that we expect the adversarial texts with small perturbation can better preserve the semantic meaning of original texts. Experiments show that, compared with state-of-the-art attack approaches, our approach achieves higher success rates and lower perturbation rates in four benchmark datasets1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xingyi Zhao"
        },
        {
            "affiliations": [],
            "name": "Lu Zhang"
        },
        {
            "affiliations": [],
            "name": "Depeng Xu"
        },
        {
            "affiliations": [],
            "name": "Shuhan Yuan"
        }
    ],
    "id": "SP:8ebdd4cb014037cb12e2ceec922caaeed1b39175",
    "references": [
        {
            "authors": [
                "Moustafa Alzantot",
                "Yash Sharma",
                "Ahmed Elgohary",
                "Bo-Jhang Ho",
                "Mani Srivastava",
                "Kai-Wei Chang."
            ],
            "title": "Generating natural language adversarial examples",
            "venue": "arXiv preprint arXiv:1804.07998.",
            "year": 2018
        },
        {
            "authors": [
                "Zhendong Dong",
                "Qiang Dong."
            ],
            "title": "Hownet and the computation of meaning (with Cd-rom)",
            "venue": "World Scientific.",
            "year": 2006
        },
        {
            "authors": [
                "Christiane Fellbaum."
            ],
            "title": "Wordnet",
            "venue": "Theory and applications of ontology: computer applications, pages 231\u2013243. Springer.",
            "year": 2010
        },
        {
            "authors": [
                "Ji Gao",
                "Jack Lanchantin",
                "Mary Lou Soffa",
                "Yanjun Qi."
            ],
            "title": "Black-box generation of adversarial text sequences to evade deep learning classifiers",
            "venue": "2018 IEEE Security and Privacy Workshops (SPW), pages 50\u201356. IEEE.",
            "year": 2018
        },
        {
            "authors": [
                "Di Jin",
                "Zhijing Jin",
                "Joey Tianyi Zhou",
                "Peter Szolovits."
            ],
            "title": "Is bert really robust? a strong baseline for natural language attack on text classification and entailment",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 34, pages",
            "year": 2020
        },
        {
            "authors": [
                "Dianqi Li",
                "Yizhe Zhang",
                "Hao Peng",
                "Liqun Chen",
                "Chris Brockett",
                "Ming-Ting Sun",
                "Bill Dolan."
            ],
            "title": "Contextualized perturbation for textual adversarial attack",
            "venue": "arXiv preprint arXiv:2009.07502.",
            "year": 2020
        },
        {
            "authors": [
                "Jinfeng Li",
                "Shouling Ji",
                "Tianyu Du",
                "Bo Li",
                "Ting Wang."
            ],
            "title": "Textbugger: Generating adversarial text against real-world applications",
            "venue": "arXiv preprint arXiv:1812.05271.",
            "year": 2018
        },
        {
            "authors": [
                "Linyang Li",
                "Ruotian Ma",
                "Qipeng Guo",
                "Xiangyang Xue",
                "Xipeng Qiu."
            ],
            "title": "Bert-attack: Adversarial attack against bert using bert",
            "venue": "arXiv preprint arXiv:2004.09984.",
            "year": 2020
        },
        {
            "authors": [
                "Zongyi Li",
                "Jianhan Xu",
                "Jiehang Zeng",
                "Linyang Li",
                "Xiaoqing Zheng",
                "Qi Zhang",
                "Kai-Wei Chang",
                "ChoJui Hsieh."
            ],
            "title": "Searching for an effective defender: Benchmarking defense against adversarial word substitution",
            "venue": "arXiv preprint arXiv:2108.12777.",
            "year": 2021
        },
        {
            "authors": [
                "Andrew L. Maas",
                "Raymond E. Daly",
                "Peter T. Pham",
                "Dan Huang",
                "Andrew Y. Ng",
                "Christopher Potts."
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu-",
            "year": 2011
        },
        {
            "authors": [
                "John X Morris",
                "Eli Lifland",
                "Jin Yong Yoo",
                "Jake Grigsby",
                "Di Jin",
                "Yanjun Qi."
            ],
            "title": "Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp",
            "venue": "arXiv preprint arXiv:2005.05909.",
            "year": 2020
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee."
            ],
            "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
            "venue": "arXiv preprint cs/0506075.",
            "year": 2005
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher D Manning."
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543.",
            "year": 2014
        },
        {
            "authors": [
                "Shuhuai Ren",
                "Yihe Deng",
                "Kun He",
                "Wanxiang Che."
            ],
            "title": "Generating natural language adversarial examples through probability weighted word saliency",
            "venue": "Proceedings of the 57th annual meeting of the association for computational linguistics, pages 1085\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D Manning",
                "Andrew Y Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 conference on",
            "year": 2013
        },
        {
            "authors": [
                "Puyudi Yang",
                "Jianbo Chen",
                "Cho-Jui Hsieh",
                "Jane-Ling Wang",
                "Michael I Jordan."
            ],
            "title": "Greedy attack and gumbel attack: Generating adversarial examples for discrete data",
            "venue": "J. Mach. Learn. Res., 21(43):1\u201336.",
            "year": 2020
        },
        {
            "authors": [
                "Yuan Zang",
                "Fanchao Qi",
                "Chenghao Yang",
                "Zhiyuan Liu",
                "Meng Zhang",
                "Qun Liu",
                "Maosong Sun."
            ],
            "title": "Word-level textual adversarial attacking as combinatorial optimization",
            "venue": "arXiv preprint arXiv:1910.12196.",
            "year": 2019
        },
        {
            "authors": [
                "Wei Emma Zhang",
                "Quan Z Sheng",
                "Ahoud Alhazmi",
                "Chenliang Li"
            ],
            "title": "Adversarial attacks on",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in neural information processing systems, 28.",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recent studies have demonstrated that deep learning based natural language processing (NLP) models are vulnerable to adversarial attacks(Li et al., 2018; Gao et al., 2018; Yang et al., 2020). Various textual adversarial attack strategies have been proposed including character-level, word-level, and sentence-level attacks (Zhang et al., 2020; Morris et al., 2020). In this paper, we focus on wordlevel attacks on text classifiers, especially the word substitution-based attacks, which are shown to be both effective and efficient (Alzantot et al., 2018; Ren et al., 2019; Zang et al., 2019; Jin et al., 2020; Li et al., 2020b,a).\nA successful adversarial attack should satisfy three requirements: 1) can fool the neural network models to make the wrong prediction; 2) the modification is slight and imperceptible to human judgments; and 3) can find the adversarial example in a reasonable run time. However, existing approaches\n1The code is available at https://github.com/ xingyizhao/TAMPERS\nsuffer from various limitations. For example, the greedy algorithm-based attack approaches (Ren et al., 2019; Jin et al., 2020) usually find a word in each step that can maximize the change of neural network outputs and do not explicitly consider the requirement of semantic preservation. On the other hand, the combinatorial optimization-based approaches, such as genetic algorithm (Alzantot et al., 2018; Zang et al., 2019), define fitness functions to control the semantic quality of adversarial texts, but due to the huge search space, they usually need a large amount of time to find a solution, especially for a long text.\nIn this work, we propose Textual Adversarial attack with Minimal PErturbation in a Reduced Search Space (TAMPERS) to craft high quality adversarial texts.\nThe major motivation is that, by substituting a vulnerable word with its synonyms or sememes, we expect that the adversarial text with small perturbation can have large semantic preservation. We model the word-level attacks as a combinatorial optimization problem (Zang et al., 2019). We then develop a two-step heuristic for solving this problem. In the first step search space reduction, we first build candidate lists for all the content words by combining synonyms from WordNet (Fellbaum, 2010) and sememes from HowNet (Dong and Dong, 2006) similar to other word level\nar X\niv :2\n21 1.\n06 57\n1v 1\n[ cs\n.C L\n] 1\n2 N\nov 2\n02 2\nattack strategies (Ren et al., 2019; Zang et al., 2019). To avoid searching in a huge search space, instead of searching through all content words to generate adversarial texts, we reduce the search space by finding a small set of vulnerable words that are sufficient to fool the neural network using a greedy algorithm. Then, in the second step iterative search, we further minimize the perturbation by iteratively restoring the substituted words back to the original words while keeping the classifier making the wrong prediction. Each time we restore a word, we adopt the genetic algorithm (GA) to search adversarial texts over the remaining substituted words. In this way, we expect to find an adversarial example with the least perturbation on the original text.\nWe have conducted experiments on four benchmark datasets. The results show that our approach can successfully fool the fine-tuned BERT model with high success attack rates and low perturbation rates. Table 1 shows an example of adversarial texts generated by different attack approaches. We can notice that TAMPERS has a much lower perturbation rate compared with PWWS, TextFooler, and BERT-Attack."
        },
        {
            "heading": "2 Methodology",
            "text": "Given any text X = {w1, . . . , wn, . . . , wN} with N words and the true label Y , consider a pretrained classifier that maps X to Y , i.e., F : X \u2192 Y . We assume the soft-label black-box setting where adversaries can query the classifier for classification probabilities on the given sample, but have no access to the model structures, parameters as well as training data. Our goal is to craft an adversarial sample X\u2217 such that F (X\u2217) 6= Y with minimal word substitutions.\nWe propose a novel framework called Textual Adversarial attack with Minimal PErturbation in a Reduced search Space (TAMPERS), which consists of two steps. The search space reduction step is to find vulnerable words with the goal of reducing the search space, while the iterative search step is to further minimize the perturbation so that the semantics of the original text can be well-preserved. The overview of TAMPERS is shown in Figure 1. Below we describe each step in detail."
        },
        {
            "heading": "2.1 Search Space Reduction",
            "text": "In order to formalize the search space to craft the adversarial texts, the first step is to find the vulnera-\nble words in the text X = {w1, . . . , wn, . . . , wN}. Some related studies (Jin et al., 2020; Li et al., 2020b) define the importance score of a content word wn based on the probability change of the classifier with and without the word wn (word deletion). Differently, we derive the importance score of the word wn by replacing the word with another word in a candidate list that leads to the maximum probability change of the classifier (word derivative). The motivation of our approach is to approximate the computation of the gradient of the classifier output on the search space.\nSpecifically, given a content word wn, we first find its synonyms from WordNet and sememes from HowNet with the same part of speech. Then, the substitution candidate list Cn for the word wn consists of the top-z synonyms and sememes that are similar to wn based on the GloVe embeddings (Pennington et al., 2014). After getting the candidate list Cn, we use each word w\u2032n \u2208 Cn to substitute wn to get a perturbed text X \u2032n. The set of perturbed texts formed by all words in the candidate list is denoted by X \u2032n. Then, we compute the probability change of the classifier and define the importance score S(wn) as the maximum probability change across X \u2032n:\nS(wn) = max X\u2032n\u2208X \u2032n\n{P (Ytrue|X)\u2212 P (Ytrue|X \u2032n)}.\nWe denote the candidate word that achieves the score S(wn) as w\u2217n.\nNext, we rank all content words according to the importance score in descending order and adopt a greedy algorithm to generate the adversarial text,\ndenoted as X\u2217init. The greedy algorithm substitutes the content words one by one according to a descending order of their importance scores until the perturbed text can fool the classifier. In this step, we always substitute each word wn with the candidate word w\u2217n that is computed above. After this step, we get a small set of vulnerable words L = {w\u2217(1), . . . , w \u2217 (k), . . . , w \u2217 (K)} as the initial substitution words that is sufficient to fool the classifier, where w\u2217(k) is the (k)-th vulnerable word by importance score. The pseudo-code of the search space reduction is shown in Algorithm 1 in the appendix."
        },
        {
            "heading": "2.2 Iterative Search",
            "text": "Step 1 could significantly reduce the search space and produce an initial solution X\u2217init that already has a small perturbation. However, since the first step is in principle a one-step greedy strategy, we would like to further minimize the perturbation in order to better preserve the semantic integrity. To this end, we develop a novel iterative search approach, where we iteratively restore the substituted words back to the original words according to the importance scores while keeping the classifier getting fooled. Each time after a word is restored, we reduce the list L accordingly and adopt GA to search for new substitutions for the perturbed words in the reduced L. Since the search space of the GA is limited to L, TAMPERS is much more efficient than previous GA-based approaches (e.g., Alzantot et al., 2018).\nSymbolically, we start from the least vulnerable word w\u2217(K) in L and restore it to the original word w(K). Then, we run the GA over L \\ w\u2217(K) in order to find an adversarial text that can still fool the classifier. If we successfully find a solution, we then continue to restore the second least vulnerable word w\u2217(K\u22121) to w(K\u22121), reduce the list L, and run the GA to find the adversarial text. We repeat this process until the GA cannot find a solution in the reduced search space. Finally, the algorithm returns the last successful solution as the output, which is the adversarial text with the minimal perturbation among those that have ever been found by the algorithm. The pseudo-code of the algorithm is shown in Algorithm 2 in the appendix.\nThe details of the genetic algorithm including initialization, selection, crossover, and mutation are described as follows\nStep 1. Initialization. This step is to generate the first generation of genetic algorithm G0\nwith M samples (population size). Given the current set of vulnerable words L, we randomly choose a candidate from C for each vulnerable word in L and generate M texts, denoted as G0 = {X01 , . . . , X0m, . . . , X0M}.\nStep 2. Selection. In the g-th generation Gg, we first sort the texts based on the fitness score h(Xgm), defined as\nh(Xgm) = { P (Ytrue|X)\u2212 P (Ytrue|Xgm), if F (Xgm) = Ytrue 1, if F (Xgm) 6= Ytrue\nThen, we select texts with the top 20% highest fitness score in the current generation as elitism and directly send to the next generation Gg+1.\nStep 3. Crossover. For the texts in the g-th generation Gg with the top 50% highest fitness score, we first randomly select two texts, denoted as Xgp and X g q . Then, for the positions of vulnerable words in the text, we randomly sample a word from either Xgp or X g q at each position to generate a new text. We repeat the above two steps to generate the rest 80% of texts in the next generation Gg+1.\nStep 4. Mutation. During the process of crossover, there is a small probability that we randomly select a new word from the candidate list instead of the word from Xgp or X g q .\nAfter initializing the first generation G0 in Step 1 and repeating Steps 2 to 4 multiple iterations, if we are able to find an adversarial text giving current vulnerable words L, it means compared with the adversarial text X\u2217init with a perturbation rate K/N , we generate new adversarial text with a perturbation rate (K \u2212 1)/N ."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Experimental Setup",
            "text": "Datasets and Victim Model. We evaluate TAMPERS on four text classification datasets, including IMDB (Maas et al., 2011), Yelp (Zhang et al., 2015), MR (Pang and Lee, 2005), and SST2 (Socher et al., 2013). Following Jin et al., 2020 and Li et al., 2020b, we report the average outcome of 5 runs where in each run we randomly sample 1000 texts from each dataset. The victim model is the fine-tuned BERTbase. Baselines. We choose three word-level adversarial attack models as baselines: 1) TextFooler adopts counter-fitting word embeddings and the greedy search to generate adversarial texts (Jin et al., 2020); 2) PWWS is a synonym-based attack method using the greedy algorithm (Ren et al.,\n2019); 3) BERT-Attack uses a mask language model to predict candidate words and applies the greedy algorithm to craft adversarial texts (Li et al., 2020b). Evaluation Metrics. Following TextDefender (Li et al., 2021), we adopt various evaluation metrics to evaluate the quality of adversarial texts, including original accuracy, attacked accuracy, attack success rate, average perturbation rate, and semantic similarity. Following TextFooler and BERT-Attack, we adopt Universal Sentence Encoder (Cer et al., 2018) to evaluate the semantic similarity between the original text and its adversarial counterpart. Implementation Details. We constrain the size of candidates set Cn as 50 for each content word. For the initialized adversarial texts X\u2217init with more than one perturbed word, we apply iterative search to find better adversarial examples with fewer perturbation. In GA, we set the population size M = 10 and maximal generation T = 100."
        },
        {
            "heading": "3.2 Experimental Results",
            "text": "Attack Results. As shown in Table 2, TAMPERS successfully attacks the fine-tune BERTbase model on four datasets. In general, TAMPERS has the smallest perturbation rate and highest success attack rate among all baselines. As expected, the performance of TAMPERS is better on long texts.\nFor the IMDB and Yelp datasets that mainly contain long texts, TAMPERS significantly reduces the perturbation rates compared with baselines and still achieves the highest attack success rate. On the MR and SST-2 datasets that mainly contain short texts, the improvement of TAMPERS is less significant but it can still reduce the perturbation rates by substituting around 2 words while maintaining high attack success rates. Runtime Comparison. We compare the runtime of TAMPERS with that of Genetic Attack (Alzantot et al., 2018) and Particle Swarm Optimization (PSO) (Zang et al., 2019), both of which are also combinatorial optimization-based approaches. The results on the IMDB dataset are shown in Table 3. We observe similar results on other datasets which are included in the appendix. As can be seen, TAMPERS is much faster and can craft an adversarial text in a reasonable time frame compared with Genetic Attack and PSO, thanks to the search space reduction step that significantly improves the efficiency of the algorithm."
        },
        {
            "heading": "4 Conclusions",
            "text": "In this work, we developed a novel word-level attack framework, called TAMPERS, that adopts a two-step approach with the goal of minimizing the perturbation. The first step reduces the search space and finds an initial adversarial text with the reduced perturbation. The second step then further minimizes the perturbation by iteratively restoring the substituted words back to the original words while keeping the classifier getting fooled. Experimental results show that TAMPERS generally achieves\nminimal perturbation in crafting adversarial examples while keeping a high success attack rate.\nLimitations\nOne limitation of the proposed framework is that in the iterative search step, each time we restore a vulnerable word w\u2217(k) to the original word w(k), GA starts from the scratch to find an adversarial example, which requires a lot of queries to the victim models. In the future, we would like to study how to improve query efficiency while maintaining the high success rate and low perturbation rate."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work was supported in part by NSF 2103829."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Pseudo-code\nThe pseudo-code of Search Space Reduction and Iterative Search is given in Algorithms 1 and 2, respectively.\nAlgorithm 1: Search Space Reduction Input :Text X; Label Y ; Classifier F Output :Adversarial example X\u2217init\n1 for each content word wn in X do 2 Get a substitution candidates set Cn for wn; 3 Compute the importance score S(wn) and get the candidate word w\u2217n; 4 end 5 Build a list S of content words wn \u2208 X in a\ndescending order based on S(wn); 6 L = list(); 7 X\u2217 \u2190 {w1, ..., wn, ...wN}; 8 for each word wn in S do 9 L.append(wn);\n10 X\u2217.replace(wn, w \u2217 n); 11 if F (X\u2217) 6= Y then 12 X\u2217init \u2190 X\u2217; 13 return X\u2217init and L; 14 end 15 end 16 return None;\nAlgorithm 2: Iterative Search Input :Text X; Label Y ; Classifier F ; Adversarial\ntext X\u2217init; Vulnerable word list L with size K; Substitution candidate sets Q for w \u2208 L\nOutput :Adversarial example X\u2217 1 X\u2217 \u2190 X\u2217init; 2 for k = K to 1 do 3 L = L \\ w(k); 4 Initialize the first generation G0; 5 for g = 1 to T do 6 Generate texts in Gg based on Selection, Crossover and Mutation; 7 Gg \u2190 sort(Gg) ; 8 if F (Gg[0]) 6= Y then 9 X \u2032 \u2190 Gg[0];\n10 break; 11 end 12 end 13 if F (Gg[0]) == Y then 14 break; 15 end 16 X\u2217 \u2190 X \u2032; 17 end 18 return X\u2217;\nA.2 Implementation Details\nFor TextFooler and PWWS, we follow the implementations in TextAttack (Morris et al., 2020), while for BERT-Attack, we follow the implementations in TextDefender (Li et al., 2021). We use the\nfine-tuned victim models for all the datasets shared by TextAttack 2. All experiments are run on AMD Ryzen Threadripper 3960X 24-core Processor and NVIDIA GeForce RTX 3090.\nA.3 Experimental Results Due to the high time-consuming of Genetic Attack (Alzantot et al., 2018) and PSO (Zang et al., 2019) algorithms, especially on long texts, we compare Genetic Attack and PSO with TAMPERS on MR and SST-2 by randomly selecting 250 correctly predicted texts. Table 4 shows the experimental results. Overall, compared with Genetic Attack and PSO, TAMPERS can still achieve the lowest perturbation rate with a high success attack rate. Meanwhile, TAMPERS is much faster than Genetic Attack and PSO even in datasets with short texts, which shows the advantage of the search space reduction step proposed in our framework.\nA.4 Case Study We show the cases of adversarial examples generated by different approaches on IMDB and Yelp in Tables 5 and 6, respectively.\n2https://huggingface.co/textattack"
        }
    ],
    "title": "Generating Textual Adversaries with Minimal Perturbation",
    "year": 2022
}