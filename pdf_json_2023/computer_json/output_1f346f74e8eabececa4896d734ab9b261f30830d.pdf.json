{
    "abstractText": "Transfer learning has recently become the dominant paradigm of machine learning. Pretrained models fine-tuned for downstream tasks achieve better performance with fewer labelled examples. Nonetheless, it remains unclear how to develop models that specialise towards multiple tasks without incurring negative interference and that generalise systematically to non-identically distributed tasks. Modular deep learning has emerged as a promising solution to these challenges. In this framework, units of computation are often implemented as autonomous parameter-efficient modules. Information is conditionally routed to a subset of modules and subsequently aggregated. These properties enable positive transfer and systematic generalisation by separating computation from routing and updating modules locally. We offer a survey of modular architectures, providing a unified view over several threads of research that evolved independently in the scientific literature. Moreover, we explore various additional purposes of modularity, including scaling language models, causal inference and discovery, programme simulation, and hierarchical reinforcement learning. Finally, we report various concrete applications where modularity has been successfully deployed such as cross-lingual and cross-modal knowledge transfer. More information on modular deep learning is available at www.modulardeeplearning.com/.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jonas Pfeiffer"
        },
        {
            "affiliations": [],
            "name": "Sebastian Ruder"
        },
        {
            "affiliations": [],
            "name": "Edoardo M. Ponti"
        }
    ],
    "id": "SP:8ff393c11fec2f3e6c1bc88307fcb513b17640a2",
    "references": [
        {
            "authors": [
                "Eunwoo Kim",
                "Songhwai Oh"
            ],
            "title": "Deep elastic networks with model selection for multi-task",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Hayase",
                "Siddhartha S. Srinivasa"
            ],
            "title": "Git re-basin: Merging models",
            "year": 2019
        },
        {
            "authors": [
                "Brock",
                "Aida Nematzadeh",
                "Sahand Sharifzadeh",
                "Mikolaj Binkowski",
                "Ricardo Barreira",
                "Oriol Vinyals",
                "Andrew Zisserman",
                "Karen Simonyan"
            ],
            "title": "Flamingo: a visual language model for few-shot learning",
            "venue": "CoRR, abs/2204.14198,",
            "year": 2022
        },
        {
            "authors": [
                "Ferran Alet",
                "Tom\u00e1s Lozano-P\u00e9rez",
                "Leslie Pack Kaelbling"
            ],
            "title": "Modular meta-learning",
            "venue": "In 2nd Annual Conference on Robot Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Rahaf Aljundi",
                "Punarjay Chakravarty",
                "Tinne Tuytelaars"
            ],
            "title": "Expert gate: Lifelong learning with a network of experts",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Andreas",
                "Marcus Rohrbach",
                "Trevor Darrell",
                "Dan Klein"
            ],
            "title": "Learning to compose neural networks for question answering",
            "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2016
        },
        {
            "authors": [
                "Jacob Andreas",
                "Marcus Rohrbach",
                "Trevor Darrell",
                "Dan Klein"
            ],
            "title": "Neural module networks",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Jacob Andreas",
                "Dan Klein",
                "Sergey Levine"
            ],
            "title": "Modular multitask reinforcement learning with policy sketches",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning, ICML 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Alan Ansell",
                "Edoardo Maria Ponti",
                "Jonas Pfeiffer",
                "Sebastian Ruder",
                "Goran Glava\u0161",
                "Ivan Vuli\u0107",
                "Anna Korhonen"
            ],
            "title": "MAD-G: Multilingual adapter generation for efficient cross-lingual transfer",
            "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Alan Ansell",
                "Edoardo Ponti",
                "Anna Korhonen",
                "Ivan Vuli\u0107"
            ],
            "title": "Composable sparse fine-tuning for cross-lingual transfer",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Sajid Anwar",
                "Kyuyeon Hwang",
                "Wonyong Sung"
            ],
            "title": "Structured pruning of deep convolutional neural networks",
            "venue": "ACM Journal on Emerging Technologies in Computing Systems (JETC),",
            "year": 2017
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Shruti Bhosale",
                "Naman Goyal",
                "Todor Mihaylov",
                "Myle Ott",
                "Sam Shleifer",
                "Xi Victoria Lin",
                "Jingfei Du",
                "Srinivasan Iyer",
                "Ramakanth Pasunuru",
                "Giridharan Anantharaman",
                "Xian Li",
                "Shuohui Chen",
                "Halil Akin",
                "Mandeep Baines",
                "Louis Martin",
                "Xing Zhou",
                "Punit Singh Koura",
                "Brian O\u2019Horo",
                "Jeffrey Wang",
                "Luke Zettlemoyer",
                "Mona Diab",
                "Zornitsa Kozareva",
                "Veselin Stoyanov"
            ],
            "title": "Efficient large scale language modeling with mixtures of experts",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural",
            "year": 2022
        },
        {
            "authors": [
                "Akari Asai",
                "Mohammadreza Salehi",
                "Matthew Peters",
                "Hannaneh Hajishirzi"
            ],
            "title": "ATTEMPT: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Bernard J. Baars"
            ],
            "title": "Global workspace theory of consciousness: toward a cognitive neuroscience of human experience",
            "venue": "Progress in Brain Research,",
            "year": 2005
        },
        {
            "authors": [
                "Arun Babu",
                "Changhan Wang",
                "Andros Tjandra",
                "Kushal Lakhotia",
                "Qiantong Xu",
                "Naman Goyal",
                "Kritika Singh",
                "Patrick von Platen",
                "Yatharth Saraf",
                "Juan Pino",
                "Alexei Baevski",
                "Alexis Conneau",
                "Michael Auli"
            ],
            "title": "XLS-R: self-supervised cross-lingual speech representation learning at scale",
            "venue": "In Interspeech 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Pierre-Luc Bacon",
                "Jean Harb",
                "Doina Precup"
            ],
            "title": "The option-critic architecture",
            "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Alexei Baevski",
                "Yuhao Zhou",
                "Abdelrahman Mohamed",
                "Michael Auli"
            ],
            "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
            "venue": "In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio"
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "Ye Bai",
                "Jie Li",
                "Wenjing Han",
                "Hao Ni",
                "Kaituo Xu",
                "Zhuo Zhang",
                "Cheng Yi",
                "Xiaorui Wang"
            ],
            "title": "Parameterefficient conformers via sharing sparsely-gated experts for end-to-end speech recognition",
            "venue": "In Interspeech 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Mahsa Baktashmotlagh",
                "Mehrtash Tafazzoli Harandi",
                "Brian C. Lovell",
                "Mathieu Salzmann"
            ],
            "title": "Unsupervised domain adaptation by domain invariant projection",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 2013
        },
        {
            "authors": [
                "Carliss Young Baldwin",
                "Kim B. Clark"
            ],
            "title": "Design rules: The power of modularity",
            "year": 2000
        },
        {
            "authors": [
                "Dana H Ballard"
            ],
            "title": "Cortical connections and parallel processing: Structure and function",
            "venue": "Behavioral and Brain Sciences,",
            "year": 1986
        },
        {
            "authors": [
                "Ankur Bapna",
                "Orhan Firat"
            ],
            "title": "Simple, scalable adaptation for neural machine translation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
            "year": 2019
        },
        {
            "authors": [
                "Christos Baziotis",
                "Mikel Artetxe",
                "James Cross",
                "Shruti Bhosale"
            ],
            "title": "Multilingual machine translation with hyper-adapters",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Elad Ben Zaken",
                "Yoav Goldberg",
                "Shauli Ravfogel"
            ],
            "title": "BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Emmanuel Bengio",
                "Pierre-Luc Bacon",
                "Joelle Pineau",
                "Doina Precup"
            ],
            "title": "Conditional computation in neural networks for faster models",
            "venue": "CoRR, abs/1511.06297,",
            "year": 2015
        },
        {
            "authors": [
                "Yoshua Bengio",
                "Nicholas L\u00e9onard",
                "Aaron C. Courville"
            ],
            "title": "Estimating or propagating gradients through stochastic neurons for conditional computation",
            "venue": "CoRR, abs/1308.3432,",
            "year": 2013
        },
        {
            "authors": [
                "Yoshua Bengio",
                "Tristan Deleu",
                "Nasim Rahaman",
                "Nan Rosemary Ke",
                "S\u00e9bastien Lachapelle",
                "Olexa Bilaniuk",
                "Anirudh Goyal",
                "Christopher J. Pal"
            ],
            "title": "A meta-transfer objective for learning to disentangle causal mechanisms",
            "venue": "In 8th International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Rodrigo Ferreira Berriel",
                "St\u00e9phane Lathuili\u00e8re",
                "Moin Nabi",
                "Tassilo Klein",
                "Thiago Oliveira-Santos",
                "Nicu Sebe",
                "Elisa Ricci"
            ],
            "title": "Budget-aware adapters for multi-domain learning",
            "venue": "IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Luca Bertinetto",
                "Jo\u00e3o F. Henriques",
                "Jack Valmadre",
                "Philip H.S. Torr",
                "Andrea Vedaldi"
            ],
            "title": "Learning feed-forward one-shot learners",
            "venue": "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems",
            "year": 2016
        },
        {
            "authors": [
                "Fadi Biadsy",
                "Youzheng Chen",
                "Xia Zhang",
                "Oleg Rybakov",
                "Andrew Rosenberg",
                "Pedro J. Moreno"
            ],
            "title": "A scalable model specialization framework for training and inference using submodels and its application to speech model personalization",
            "venue": "In Interspeech 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Hakan Bilen",
                "Andrea Vedaldi"
            ],
            "title": "Universal representations: The missing link between faces, text, planktons, and cat breeds",
            "venue": "CoRR, abs/1701.07275,",
            "year": 2017
        },
        {
            "authors": [
                "Olivier Bodenreider"
            ],
            "title": "The unified medical language system (UMLS): integrating biomedical terminology",
            "venue": "Nucleic Acids Research,",
            "year": 2004
        },
        {
            "authors": [
                "Grady Booch",
                "Robert A. Maksimchuk",
                "Michael W. Engle",
                "Bobbi J. Young",
                "Jim Conallen",
                "Kelli A. Houston"
            ],
            "title": "Object-oriented analysis and design with applications, third edition",
            "venue": "ACM SIGSOFT Software Engineering Notes,",
            "year": 2008
        },
        {
            "authors": [
                "Konstantinos Bousmalis",
                "George Trigeorgis",
                "Nathan Silberman",
                "Dilip Krishnan",
                "Dumitru Erhan"
            ],
            "title": "Domain separation networks",
            "venue": "Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems",
            "year": 2016
        },
        {
            "authors": [
                "Felix J.S. Bragman",
                "Ryutaro Tanno",
                "S\u00e9bastien Ourselin",
                "Daniel C. Alexander",
                "Manuel Jorge Cardoso"
            ],
            "title": "Stochastic filter groups for multi-task cnns: Learning specialist and generalist convolution kernels",
            "venue": "IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Christopher Hesse",
                "Mark Chen",
                "Eric Sigler",
                "Mateusz Litwin",
                "Scott Gray",
                "Benjamin Chess",
                "Jack Clark",
                "Christopher Berner",
                "Sam McCandlish",
                "Alec Radford",
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners",
            "venue": "In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Emanuele Bugliarello",
                "Fangyu Liu",
                "Jonas Pfeiffer",
                "Siva Reddy",
                "Desmond Elliott",
                "Edoardo Maria Ponti",
                "Ivan Vuli\u0107"
            ],
            "title": "IGLUE: A benchmark for transfer learning across modalities, tasks, and languages",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Lucas Caccia",
                "Edoardo Ponti",
                "Lucas Liu",
                "Matheus Pereira",
                "Nicolas Le Roux",
                "Alessandro Sordoni"
            ],
            "title": "Multi-head adapter routing for data-efficient fine-tuning, 2022",
            "venue": "URL https://arxiv.org/abs/2211.03831",
            "year": 2022
        },
        {
            "authors": [
                "Han Cai",
                "Chuang Gan",
                "Ligeng Zhu",
                "Song Han"
            ],
            "title": "Tinytl: Reduce memory, not parameters for efficient on-device learning",
            "venue": "In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Rich Caruana"
            ],
            "title": "Multitask learning",
            "venue": "Machine Learning,",
            "year": 1997
        },
        {
            "authors": [
                "Ignacio Cases",
                "Clemens Rosenbaum",
                "Matthew Riemer",
                "Atticus Geiger",
                "Tim Klinger",
                "Alex Tamkin",
                "Olivia Li",
                "Sandhini Agarwal",
                "Joshua D. Greene",
                "Dan Jurafsky",
                "Christopher Potts",
                "Lauri Karttunen"
            ],
            "title": "Recursive routing networks: Learning to compose modules for language understanding",
            "venue": "In Proceedings of the",
            "year": 2019
        },
        {
            "authors": [
                "Stephen Casper",
                "Shlomi Hod",
                "Daniel Filan",
                "Cody Wild",
                "Andrew Critch",
                "Stuart Russell"
            ],
            "title": "Graphical clusterability and local specialization in deep neural networks",
            "venue": "In ICLR 2022 Workshop on PAIR2Struct,",
            "year": 2022
        },
        {
            "authors": [
                "Michael Chang",
                "Abhishek Gupta",
                "Sergey Levine",
                "Thomas L. Griffiths"
            ],
            "title": "Automatically composing representation transformations as a means for generalization",
            "venue": "In 7th International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Tianlong Chen",
                "Jonathan Frankle",
                "Shiyu Chang",
                "Sijia Liu",
                "Yang Zhang",
                "Zhangyang Wang",
                "Michael Carbin"
            ],
            "title": "The lottery ticket hypothesis for pre-trained BERT networks",
            "venue": "In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Tianlong Chen",
                "Yu Cheng",
                "Zhe Gan",
                "Lu Yuan",
                "Lei Zhang",
                "Zhangyang Wang"
            ],
            "title": "Chasing sparsity in vision transformers: An end-to-end exploration",
            "venue": "In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Ting Chen",
                "Mario Lu\u010di\u0107",
                "Neil Houlsby",
                "Sylvain Gelly"
            ],
            "title": "On self modulation for generative adversarial networks",
            "venue": "In 7th International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Xilun Chen",
                "Claire Cardie"
            ],
            "title": "Multinomial adversarial networks for multi-domain text classification",
            "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association",
            "year": 2018
        },
        {
            "authors": [
                "Zih-Ching Chen",
                "Chin-Lun Fu",
                "Chih-Ying Liu",
                "Shang-Wen (Daniel) Li",
                "Hung-yi Lee"
            ],
            "title": "Exploring efficienttuning methods in self-supervised speech models",
            "venue": "In IEEE Spoken Language Technology Workshop,",
            "year": 2022
        },
        {
            "authors": [
                "Rochelle Choenni",
                "Dan Garrette",
                "Ekaterina Shutova"
            ],
            "title": "Data-efficient cross-lingual transfer with languagespecific subnetworks",
            "venue": "CoRR, abs/2211.00106,",
            "year": 2022
        },
        {
            "authors": [
                "Alexandra Chronopoulou",
                "Matthew Peters",
                "Jesse Dodge"
            ],
            "title": "Efficient hierarchical domain adaptation for pretrained language models",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Alexandra Chronopoulou",
                "Dario Stojanovski",
                "Alexander Fraser"
            ],
            "title": "Language-family adapters for multilingual neural machine translation",
            "venue": "CoRR, abs/2209.15236,",
            "year": 2022
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Unsupervised crosslingual representation learning at scale",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Asa Cooper Stickland",
                "Alexandre Berard",
                "Vassilina Nikoulina"
            ],
            "title": "Multilingual domain adaptation for NMT: Decoupling language and domain information with adapters",
            "venue": "In Proceedings of the Sixth Conference on Machine Translation, pp. 578\u2013598,",
            "year": 2021
        },
        {
            "authors": [
                "Guzm\u00e1n",
                "Philipp Koehn",
                "Alexandre Mourachko",
                "Christophe Ropers",
                "Safiyyah Saleem",
                "Holger Schwenk",
                "Jeff Wang"
            ],
            "title": "No language left behind: Scaling human-centered machine translation",
            "venue": "CoRR, abs/2207.04672,",
            "year": 2022
        },
        {
            "authors": [
                "Michael Crawshaw"
            ],
            "title": "Multi-task learning with deep neural networks: A survey",
            "venue": "CoRR, abs/2009.09796,",
            "year": 2020
        },
        {
            "authors": [
                "R\u00f3bert Csord\u00e1s",
                "Sjoerd van Steenkiste",
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Are neural nets modular? inspecting functional modularity through differentiable weight masks",
            "venue": "In 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Abhishek Das",
                "Georgia Gkioxari",
                "Stefan Lee",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Neural modular control for embodied question answering",
            "venue": "In 2nd Annual Conference on Robot Learning, CoRL 2018, Zu\u0308rich, Switzerland,",
            "year": 2018
        },
        {
            "authors": [
                "Peter Dayan",
                "Geoffrey E. Hinton"
            ],
            "title": "Feudal reinforcement learning. In Advances in Neural Information Processing Systems 5, [NIPS Conference, Denver, Colorado, USA, November 30 ",
            "venue": "December 3,",
            "year": 1992
        },
        {
            "authors": [
                "Harm de Vries",
                "Florian Strub",
                "J\u00e9r\u00e9mie Mary",
                "Hugo Larochelle",
                "Olivier Pietquin",
                "Aaron C. Courville"
            ],
            "title": "Modulating early visual processing by language",
            "venue": "In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Coline Devin",
                "Abhishek Gupta",
                "Trevor Darrell",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Learning modular neural network policies for multi-task and multi-robot transfer",
            "venue": "IEEE International Conference on Robotics and Automation,",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "year": 2019
        },
        {
            "authors": [
                "Aniket Didolkar",
                "Anirudh Goyal",
                "Nan Rosemary Ke",
                "Charles Blundell",
                "Philippe Beaudoin",
                "Nicolas Heess",
                "Michael Mozer",
                "Yoshua Bengio"
            ],
            "title": "Neural production systems",
            "venue": "In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Thomas G. Dietterich"
            ],
            "title": "Hierarchical reinforcement learning with the MAXQ value function decomposition",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2000
        },
        {
            "authors": [
                "Katharina Dobs",
                "Julio Martinez",
                "Alexander JE Kell",
                "Nancy Kanwisher"
            ],
            "title": "Brain-like functional specialization emerges spontaneously in deep neural networks",
            "venue": "Science Advances,",
            "year": 2022
        },
        {
            "authors": [
                "Shachar Don-Yehiya",
                "Elad Venezian",
                "Colin Raffel",
                "Noam Slonim",
                "Yoav Katz",
                "Leshem Choshen"
            ],
            "title": "Cold fusion: Collaborative descent for distributed multitask finetuning",
            "venue": "arXiv prerint,",
            "year": 2022
        },
        {
            "authors": [
                "Jeff Donahue",
                "Yangqing Jia",
                "Oriol Vinyals",
                "Judy Hoffman",
                "Ning Zhang",
                "Eric Tzeng",
                "Trevor Darrell"
            ],
            "title": "Decaf: A deep convolutional activation feature for generic visual recognition",
            "venue": "In Proceedings of the 31th International Conference on Machine Learning,",
            "year": 2014
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Felix Draxler",
                "Kambis Veschgini",
                "Manfred Salmhofer",
                "Fred A. Hamprecht"
            ],
            "title": "Essentially no barriers in neural network energy landscape",
            "venue": "In Proceedings of the 35th International Conference on Machine Learning, ICML 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Dheeru Dua",
                "Shruti Bhosale",
                "Vedanuj Goswami",
                "James Cross",
                "Mike Lewis",
                "Angela Fan"
            ],
            "title": "Tricks for training sparse translation models",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Steven Vander Eeckt",
                "Hugo Van hamme"
            ],
            "title": "Using adapters to overcome catastrophic forgetting in end-toend automatic speech recognition",
            "venue": "CoRR, abs/203.16082,",
            "year": 2022
        },
        {
            "authors": [
                "David Eigen",
                "Marc\u2019Aurelio Ranzato",
                "Ilya Sutskever"
            ],
            "title": "Learning factored representations in a deep mixture of experts",
            "venue": "In 2nd International Conference on Learning Representations,",
            "year": 2014
        },
        {
            "authors": [
                "Fahim Faisal",
                "Antonios Anastasopoulos"
            ],
            "title": "Phylogeny-inspired adaptation of multilingual models to new languages",
            "venue": "In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Angela Fan",
                "Shruti Bhosale",
                "Holger Schwenk",
                "Zhiyi Ma",
                "Ahmed El-Kishky",
                "Siddharth Goyal",
                "Mandeep Baines",
                "Onur Celebi",
                "Guillaume Wenzek",
                "Vishrav Chaudhary",
                "Naman Goyal",
                "Tom Birch",
                "Vitaliy Liptchinsky",
                "Sergey Edunov",
                "Michael Auli",
                "Armand Joulin"
            ],
            "title": "Beyond english-centric multilingual machine translation",
            "venue": "Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Ruchao Fan",
                "Abeer Alwan"
            ],
            "title": "DRAFT: A novel framework to reduce domain shifting in self-supervised learning and its application to children\u2019s ASR",
            "venue": "Interspeech 2022,",
            "year": 2022
        },
        {
            "authors": [
                "William Fedus",
                "Barret Zoph",
                "Noam Shazeer"
            ],
            "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
            "venue": "CoRR, abs/2101.03961,",
            "year": 2021
        },
        {
            "authors": [
                "William Fedus",
                "Jeff Dean",
                "Barret Zoph"
            ],
            "title": "A review of sparse expert models in deep learning",
            "venue": "CoRR, abs/2209.01667,",
            "year": 2022
        },
        {
            "authors": [
                "Chrisantha Fernando",
                "Dylan Banarse",
                "Charles Blundell",
                "Yori Zwols",
                "David Ha",
                "Andrei A. Rusu",
                "Alexander Pritzel",
                "Daan Wierstra"
            ],
            "title": "Pathnet: Evolution channels gradient descent in super neural networks",
            "venue": "CoRR, abs/1701.08734,",
            "year": 2017
        },
        {
            "authors": [
                "Carlos Florensa",
                "Yan Duan",
                "Pieter Abbeel"
            ],
            "title": "Stochastic neural networks for hierarchical reinforcement learning",
            "venue": "In 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Jerry A. Fodor"
            ],
            "title": "The modularity of Mind",
            "year": 1983
        },
        {
            "authors": [
                "Negar Foroutan",
                "Mohammadreza Banaei",
                "R\u00e9mi Lebret",
                "Antoine Bosselut",
                "Karl Aberer"
            ],
            "title": "Discovering language-neutral sub-networks in multilingual language models",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Michael Carbin"
            ],
            "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
            "venue": "In 7th International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Gintare Karolina Dziugaite",
                "Daniel M. Roy",
                "Michael Carbin"
            ],
            "title": "Linear mode connectivity and the lottery ticket hypothesis",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "C. Daniel Freeman",
                "Joan Bruna"
            ],
            "title": "Topology and geometry of half-rectified network optimization",
            "venue": "In 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Robert M. French"
            ],
            "title": "Catastrophic forgetting in connectionist networks",
            "venue": "Trends in Cognitive Sciences,",
            "year": 1999
        },
        {
            "authors": [
                "Yaroslav Ganin",
                "Evgeniya Ustinova",
                "Hana Ajakan",
                "Pascal Germain",
                "Hugo Larochelle",
                "Fran\u00e7ois Laviolette",
                "Mario Marchand",
                "Victor S. Lempitsky"
            ],
            "title": "Domain-adversarial training of neural networks",
            "venue": "Journal of Machine Learning Research,",
            "year": 2016
        },
        {
            "authors": [
                "Peng Gao",
                "Shijie Geng",
                "Renrui Zhang",
                "Teli Ma",
                "Rongyao Fang",
                "Yongfeng Zhang",
                "Hongsheng Li",
                "Yu Qiao"
            ],
            "title": "CLIP-Adapter: Better vision-language models with feature adapters",
            "venue": "CoRR, abs/2110.04544,",
            "year": 2021
        },
        {
            "authors": [
                "Tianyu Gao",
                "Adam Fisch",
                "Danqi Chen"
            ],
            "title": "Making pre-trained language models better few-shot learners",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2021
        },
        {
            "authors": [
                "Yuan Gao",
                "Jiayi Ma",
                "Mingbo Zhao",
                "Wei Liu",
                "Alan L. Yuille"
            ],
            "title": "NDDR-CNN: layerwise feature fusing in multi-task cnns by neural discriminative dimensionality reduction",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Timur Garipov",
                "Pavel Izmailov",
                "Dmitrii Podoprikhin",
                "Dmitry P. Vetrov",
                "Andrew Gordon Wilson"
            ],
            "title": "Loss surfaces, mode connectivity, and fast ensembling of dnns",
            "venue": "In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems",
            "year": 2018
        },
        {
            "authors": [
                "Neeraj Gaur",
                "Brian Farris",
                "Parisa Haghani",
                "Isabel Leal",
                "Pedro J. Moreno",
                "Manasa Prasad",
                "Bhuvana Ramabhadran",
                "Yun Zhu"
            ],
            "title": "Mixture of informed experts for multilingual speech recognition",
            "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Tomas Geffner",
                "Javier Antoran",
                "Adam Foster",
                "Wenbo Gong",
                "Chao Ma",
                "Emre Kiciman",
                "Amit Sharma",
                "Angus Lamb",
                "Martin Kukla",
                "Nick Pawlowski",
                "Miltiadis Allamanis",
                "Cheng Zhang"
            ],
            "title": "Deep end-toend causal inference",
            "venue": "NeurIPS",
            "year": 2022
        },
        {
            "authors": [
                "Andrea Gesmundo"
            ],
            "title": "A multi-agent framework for the asynchronous and collaborative extension of multitask ML systems",
            "venue": "CoRR, abs/2209.14745,",
            "year": 2022
        },
        {
            "authors": [
                "Andrea Gesmundo",
                "Jeff Dean"
            ],
            "title": "An evolutionary approach to dynamic introduction of tasks in largescale multitask learning systems. CoRR, abs/2205.12755, 2022a",
            "venue": "doi: 10.48550/arXiv.2205.12755. URL https://doi.org/10.48550/arXiv.2205.12755",
            "year": 2022
        },
        {
            "authors": [
                "Andrea Gesmundo",
                "Jeff Dean"
            ],
            "title": "munet: Evolving pretrained deep neural networks into scalable autotuning multitask systems",
            "venue": "CoRR, abs/2205.10937,",
            "year": 2022
        },
        {
            "authors": [
                "Anirudh Goyal",
                "Alex Lamb",
                "Jordan Hoffmann",
                "Shagun Sodhani",
                "Sergey Levine",
                "Yoshua Bengio",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Recurrent independent mechanisms",
            "venue": "In 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Graves",
                "Santiago Fern\u00e1ndez",
                "Faustino J. Gomez",
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
            "venue": "Machine Learning, Proceedings of the Twenty-Third International Conference (ICML",
            "year": 2006
        },
        {
            "authors": [
                "Alex Graves",
                "Greg Wayne",
                "Malcolm Reynolds",
                "Tim Harley",
                "Ivo Danihelka",
                "Agnieszka Grabska-Barwinska",
                "Sergio Gomez Colmenarejo",
                "Edward Grefenstette",
                "Tiago Ramalho",
                "John P. Agapiou",
                "Adri\u00e0 Puigdom\u00e8nech Badia",
                "Karl Moritz Hermann",
                "Yori Zwols",
                "Georg Ostrovski",
                "Adam Cain",
                "Helen King",
                "Christopher Summerfield",
                "Phil Blunsom",
                "Koray Kavukcuoglu",
                "Demis Hassabis"
            ],
            "title": "Hybrid computing using a neural network with dynamic external memory",
            "venue": "doi: 10.1038/nature20101. URL",
            "year": 2016
        },
        {
            "authors": [
                "Klaus Greff",
                "Sjoerd van Steenkiste",
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "On the binding problem in artificial neural networks",
            "venue": "CoRR, abs/2012.05208,",
            "year": 2020
        },
        {
            "authors": [
                "Thomas L Griffiths",
                "Zoubin Ghahramani"
            ],
            "title": "The indian buffet process: An introduction and review",
            "venue": "Journal of Machine Learning Research,",
            "year": 2011
        },
        {
            "authors": [
                "Demi Guo",
                "Alexander Rush",
                "Yoon Kim"
            ],
            "title": "Parameter-efficient transfer learning with diff pruning",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4884\u20134896,",
            "year": 2021
        },
        {
            "authors": [
                "Jiang Guo",
                "Darsh Shah",
                "Regina Barzilay"
            ],
            "title": "Multi-source domain adaptation with mixture of experts",
            "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Shashank Gupta",
                "Subhabrata Mukherjee",
                "Krishan Subudhi",
                "Eduardo Gonzalez",
                "Damien Jose",
                "Ahmed Hassan Awadallah",
                "Jianfeng Gao"
            ],
            "title": "Sparsely activated mixture-of-experts are robust multi-task learners",
            "venue": "CoRR, abs/2204.07689,",
            "year": 2022
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Mike Lewis",
                "Ari Holtzman",
                "Noah A. Smith",
                "Luke Zettlemoyer"
            ],
            "title": "DEMix layers: Disentangling domains for modular language modeling",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Karen Hambardzumyan",
                "Hrant Khachatrian",
                "Jonathan May"
            ],
            "title": "WARP: Word-level Adversarial ReProgramming",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2021
        },
        {
            "authors": [
                "John B. Hampshire",
                "Alex Waibel"
            ],
            "title": "The meta-pi network: Building distributed knowledge representations for robust multisource pattern recognition",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 1992
        },
        {
            "authors": [
                "Song Han",
                "Jeff Pool",
                "John Tran",
                "William J. Dally"
            ],
            "title": "Learning both weights and connections for efficient neural network",
            "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems",
            "year": 2015
        },
        {
            "authors": [
                "Song Han",
                "Jeff Pool",
                "Sharan Narang",
                "Huizi Mao",
                "Enhao Gong",
                "Shijian Tang",
                "Erich Elsen",
                "Peter Vajda",
                "Manohar Paluri",
                "John Tran",
                "Bryan Catanzaro",
                "William J. Dally"
            ],
            "title": "DSD: dense-sparse-dense training for deep neural networks",
            "venue": "In 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Wenjuan Han",
                "Bo Pang",
                "Ying Nian Wu"
            ],
            "title": "Robust transfer learning with pretrained language models through adapters",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),",
            "year": 2021
        },
        {
            "authors": [
                "Hussein Hazimeh",
                "Zhe Zhao",
                "Aakanksha Chowdhery",
                "Maheswaran Sathiamoorthy",
                "Yihua Chen",
                "Rahul Mazumder",
                "Lichan Hong",
                "Ed H. Chi"
            ],
            "title": "Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning",
            "venue": "In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Junxian He",
                "Chunting Zhou",
                "Xuezhe Ma",
                "Taylor Berg-Kirkpatrick",
                "Graham Neubig"
            ],
            "title": "Towards a unified view of parameter-efficient transfer learning",
            "venue": "In The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Identity mappings in deep residual networks. In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October",
            "venue": "Proceedings, Part IV,",
            "year": 2016
        },
        {
            "authors": [
                "Ruidan He",
                "Linlin Liu",
                "Hai Ye",
                "Qingyu Tan",
                "Bosheng Ding",
                "Liying Cheng",
                "Jiawei Low",
                "Lidong Bing",
                "Luo Si"
            ],
            "title": "On the effectiveness of adapter-based tuning for pretrained language model adaptation",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2021
        },
        {
            "authors": [
                "Xuehai He",
                "Chunyuan Li",
                "Pengchuan Zhang",
                "Jianwei Yang",
                "Xin Eric Wang"
            ],
            "title": "Parameter-efficient finetuning for vision transformers",
            "venue": "CoRR, abs/2203.16329,",
            "year": 2022
        },
        {
            "authors": [
                "Yun He",
                "Huaixiu Steven Zheng",
                "Yi Tay",
                "Jai Prakash Gupta",
                "Yu Du",
                "Vamsi Aribandi",
                "Zhe Zhao",
                "YaGuang Li",
                "Zhao Chen",
                "Donald Metzler",
                "Heng-Tze Cheng",
                "Ed H. Chi"
            ],
            "title": "Hyperprompt: Prompt-based taskconditioning of transformers",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Nicolas Heess",
                "Gregory Wayne",
                "Yuval Tassa",
                "Timothy P. Lillicrap",
                "Martin A. Riedmiller",
                "David Silver"
            ],
            "title": "Learning and transfer of modulated locomotor",
            "venue": "controllers. CoRR,",
            "year": 2016
        },
        {
            "authors": [
                "Wenxin Hou",
                "Han Zhu",
                "Yidong Wang",
                "Jindong Wang",
                "Tao Qin",
                "Renjun Xu",
                "Takahiro Shinozaki"
            ],
            "title": "Exploiting adapters for cross-lingual low-resource speech recognition",
            "venue": "IEEE ACM Transactions on Audio Speech Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin de Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly"
            ],
            "title": "Parameter-efficient transfer learning for NLP",
            "venue": "In Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Jeremy Howard",
                "Sebastian Ruder"
            ],
            "title": "Universal language model fine-tuning for text classification",
            "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2018
        },
        {
            "authors": [
                "Cheng-Ping Hsieh",
                "Subhankar Ghosh",
                "Boris Ginsburg"
            ],
            "title": "Adapter-based extension of multi-speaker textto-speech model for new speakers",
            "venue": "CoRR, abs/2211.00585,",
            "year": 2022
        },
        {
            "authors": [
                "Edward J. Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "In The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Junjie Hu",
                "Sebastian Ruder",
                "Aditya Siddhant",
                "Graham Neubig",
                "Orhan Firat",
                "Melvin Johnson"
            ],
            "title": "XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Wenlong Huang",
                "Pieter Abbeel",
                "Deepak Pathak",
                "Igor Mordatch"
            ],
            "title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "venue": "In Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Dieuwke Hupkes",
                "Verna Dankers",
                "Mathijs Mul",
                "Elia Bruni"
            ],
            "title": "Compositionality decomposed: How do neural networks generalise",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2020
        },
        {
            "authors": [
                "Gabriel Ilharco",
                "Marco T\u00falio Ribeiro",
                "Mitchell Wortsman",
                "Suchin Gururangan",
                "Ludwig Schmidt",
                "Hannaneh Hajishirzi",
                "Ali Farhadi"
            ],
            "title": "Editing models with task",
            "venue": "arithmetic. CoRR,",
            "year": 2022
        },
        {
            "authors": [
                "Robert A. Jacobs",
                "Michael I. Jordan",
                "Andrew G. Barto"
            ],
            "title": "Task decomposition through competition in a modular connectionist architecture: The what and where vision tasks",
            "venue": "Cognitive Science,",
            "year": 1991
        },
        {
            "authors": [
                "Robert A. Jacobs",
                "Michael I. Jordan",
                "Steven J. Nowlan",
                "Geoffrey E. Hinton"
            ],
            "title": "Adaptive mixtures of local experts",
            "venue": "Neural Computation,",
            "year": 1991
        },
        {
            "authors": [
                "Eric Jang",
                "Shixiang Gu",
                "Ben Poole"
            ],
            "title": "Categorical reparameterization with gumbel-softmax",
            "venue": "In 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Adri\u00e1n Javaloy",
                "Isabel Valera"
            ],
            "title": "Rotograd: Gradient homogenization in multitask learning",
            "venue": "In The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Yiding Jiang",
                "Shixiang Gu",
                "Kevin Murphy",
                "Chelsea Finn"
            ],
            "title": "Language as an abstraction for hierarchical deep reinforcement learning",
            "venue": "In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Michael I. Jordan",
                "Robert A. Jacobs"
            ],
            "title": "Hierarchical mixtures of experts and the EM algorithm",
            "venue": "Neural Computation,",
            "year": 1994
        },
        {
            "authors": [
                "Anjuli Kannan",
                "Arindrima Datta",
                "Tara N. Sainath",
                "Eugene Weinstein",
                "Bhuvana Ramabhadran",
                "Yonghui Wu",
                "Ankur Bapna",
                "Zhifeng Chen",
                "Seungji Lee"
            ],
            "title": "Large-scale multilingual speech recognition with a streaming end-to-end model",
            "venue": "In Proceedings of Interspeech 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Rabeeh Karimi Mahabadi",
                "Sebastian Ruder",
                "Mostafa Dehghani",
                "James Henderson"
            ],
            "title": "Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2021
        },
        {
            "authors": [
                "Nadav Kashtan",
                "Uri Alon"
            ],
            "title": "Spontaneous evolution of modularity and network motifs",
            "venue": "Proceedings of the National Academy of Sciences (PNAS),",
            "year": 2005
        },
        {
            "authors": [
                "Nan Rosemary Ke",
                "Olexa Bilaniuk",
                "Anirudh Goyal",
                "Stefan Bauer",
                "Hugo Larochelle",
                "Chris Pal",
                "Yoshua Bengio"
            ],
            "title": "Learning neural causal models from unknown interventions",
            "venue": "URL http://arxiv.org/abs/1910.01075",
            "year": 1910
        },
        {
            "authors": [
                "Nan Rosemary Ke",
                "Aniket Didolkar",
                "Sarthak Mittal",
                "Anirudh Goyal",
                "Guillaume Lajoie",
                "Stefan Bauer",
                "Danilo Jimenez Rezende",
                "Michael Mozer",
                "Yoshua Bengio",
                "Chris Pal"
            ],
            "title": "Systematic evaluation of causal discovery in visual model based reinforcement learning",
            "venue": "In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Zixuan Ke",
                "Hu Xu",
                "Bing Liu"
            ],
            "title": "Adapting BERT for continual learning of a sequence of aspect sentiment classification tasks",
            "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Simran Khanuja",
                "Melvin Johnson",
                "Partha Talukdar"
            ],
            "title": "MergeDistill: Merging language models using pre-trained distillation",
            "venue": "pp. 2874\u20132887,",
            "year": 2021
        },
        {
            "authors": [
                "Young-Bum Kim",
                "Karl Stratos",
                "Dongchan Kim"
            ],
            "title": "Adversarial adaptation of synthetic or stale data. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "year": 2017
        },
        {
            "authors": [
                "Scott Kirkpatrick",
                "C. Daniel Gelatt Jr.",
                "Mario P. Vecchi"
            ],
            "title": "Optimization by simulated annealing",
            "venue": "Science, 220(4598):671\u2013680,",
            "year": 1983
        },
        {
            "authors": [
                "Louis Kirsch",
                "Julius Kunze",
                "David Barber"
            ],
            "title": "Modular networks: Learning to decompose neural computation",
            "venue": "In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems",
            "year": 2018
        },
        {
            "authors": [
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Honglak Lee",
                "Geoffrey E. Hinton"
            ],
            "title": "Similarity of neural network representations revisited",
            "venue": "In Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Sneha Kudugunta",
                "Yanping Huang",
                "Ankur Bapna",
                "Maxim Krikun",
                "Dmitry Lepikhin",
                "Minh-Thang Luong",
                "Orhan Firat"
            ],
            "title": "Beyond distillation: Task-level mixture-of-experts for efficient inference",
            "venue": "In Findings of the Association for Computational Linguistics: EMNLP",
            "year": 2021
        },
        {
            "authors": [
                "Tejas D. Kulkarni",
                "Karthik Narasimhan",
                "Ardavan Saeedi",
                "Josh Tenenbaum"
            ],
            "title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation",
            "venue": "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems",
            "year": 2016
        },
        {
            "authors": [
                "Ken\u2019ichi Kumatani",
                "Robert Gmyr",
                "Felipe Cruz Salinas",
                "Linquan Liu",
                "Wei Zuo",
                "Devang Patel",
                "Eric Sun",
                "Yu Shi"
            ],
            "title": "Building a great multi-lingual teacher with sparsely-gated mixture of experts for speech recognition",
            "venue": "CoRR, abs/2112.05820,",
            "year": 2021
        },
        {
            "authors": [
                "Brenden M. Lake",
                "Marco Baroni"
            ],
            "title": "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks",
            "venue": "In Proceedings of the 35th International Conference on Machine Learning, ICML 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Adrian Lancucki"
            ],
            "title": "Fastpitch: Parallel text-to-speech with pitch prediction",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Richard D. Lange",
                "David S. Rolnick",
                "Konrad P. Kording"
            ],
            "title": "Clustering units in neural networks: upstream vs downstream information",
            "venue": "CoRR, abs/2203.11815,",
            "year": 2022
        },
        {
            "authors": [
                "Anne Lauscher",
                "Olga Majewska",
                "Leonardo F.R. Ribeiro",
                "Iryna Gurevych",
                "Nikolai Rozanov",
                "Goran Glava\u0161"
            ],
            "title": "Common sense or world knowledge? investigating adapter-based knowledge injection into pretrained transformers. In Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures",
            "year": 2020
        },
        {
            "authors": [
                "Anne Lauscher",
                "Tobias Lueken",
                "Goran Glava\u0161"
            ],
            "title": "Sustainable modular debiasing of language models. In Findings of the Association for Computational Linguistics: EMNLP",
            "venue": "Punta Cana, Dominican Republic,",
            "year": 2021
        },
        {
            "authors": [
                "Hang Le",
                "Juan Pino",
                "Changhan Wang",
                "Jiatao Gu",
                "Didier Schwab",
                "Laurent Besacier"
            ],
            "title": "Lightweight adapter tuning for multilingual speech translation",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),",
            "year": 2021
        },
        {
            "authors": [
                "Quoc Viet Le",
                "Tam\u00e1s Sarl\u00f3s",
                "Alexander Johannes Smola"
            ],
            "title": "Fastfood: Approximate kernel expansions in loglinear time",
            "venue": "CoRR, abs/1408.3060,",
            "year": 2014
        },
        {
            "authors": [
                "Dmitry Lepikhin",
                "HyoukJoong Lee",
                "Yuanzhong Xu",
                "Dehao Chen",
                "Orhan Firat",
                "Yanping Huang",
                "Maxim Krikun",
                "Noam Shazeer",
                "Zhifeng Chen"
            ],
            "title": "Gshard: Scaling giant models with conditional computation and automatic sharding",
            "venue": "In 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant"
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Mike Lewis",
                "Shruti Bhosale",
                "Tim Dettmers",
                "Naman Goyal",
                "Luke Zettlemoyer"
            ],
            "title": "BASE layers: Simplifying training of large, sparse models",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Belinda Li",
                "Jane Yu",
                "Madian Khabsa",
                "Luke Zettlemoyer",
                "Alon Halevy",
                "Jacob Andreas"
            ],
            "title": "Quantifying adaptability in pre-trained language models with 500 tasks",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Chunyuan Li",
                "Heerad Farkhoor",
                "Rosanne Liu",
                "Jason Yosinski"
            ],
            "title": "Measuring the intrinsic dimension of objective landscapes",
            "venue": "In 6th International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Junnan Li",
                "Ramprasaath R. Selvaraju",
                "Akhilesh Gotmare",
                "Shafiq R. Joty",
                "Caiming Xiong",
                "Steven ChuHong Hoi"
            ],
            "title": "Align before fuse: Vision and language representation learning with momentum distillation",
            "venue": "In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Margaret Li",
                "Suchin Gururangan",
                "Tim Dettmers",
                "Mike Lewis",
                "Tim Althoff",
                "Noah A. Smith",
                "Luke Zettlemoyer"
            ],
            "title": "Branch-train-merge: Embarrassingly parallel training of expert language models. CoRR, abs/2208.03306",
            "venue": "2022b. doi: 10.48550/arXiv.2208.03306. URL https://doi.org/10.48550/arXiv.2208",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang"
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
            "venue": "pp. 4582\u20134597,",
            "year": 2021
        },
        {
            "authors": [
                "Jianze Liang",
                "Chengqi Zhao",
                "Mingxuan Wang",
                "Xipeng Qiu",
                "Lei Li"
            ],
            "title": "Finding sparse structures for domain specific neural machine translation",
            "venue": "In Thirty-Fifth AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Zehui Lin",
                "Liwei Wu",
                "Mingxuan Wang",
                "Lei Li"
            ],
            "title": "Learning language specific sub-network for multilingual machine translation",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2021
        },
        {
            "authors": [
                "Robert Litschko",
                "Ivan Vuli\u0107",
                "Goran Glava\u0161"
            ],
            "title": "Parameter-efficient neural reranking for cross-lingual and multilingual retrieval",
            "venue": "In Proceedings of the 29th International Conference on Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Chen Liu",
                "Jonas Pfeiffer",
                "Anna Korhonen",
                "Ivan Vuli\u0107",
                "Iryna Gurevych"
            ],
            "title": "Delving deeper into cross-lingual visual question answering",
            "venue": "CoRR, abs/2202.07630,",
            "year": 2022
        },
        {
            "authors": [
                "Fangyu Liu",
                "Emanuele Bugliarello",
                "Edoardo Maria Ponti",
                "Siva Reddy",
                "Nigel Collier",
                "Desmond Elliott"
            ],
            "title": "Visually grounded reasoning across languages and cultures",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 1865
        },
        {
            "authors": [
                "Haokun Liu",
                "Derek Tam",
                "Mohammed Muqeeth",
                "Jay Mohta",
                "Tenghao Huang",
                "Mohit Bansal",
                "Colin Raffel"
            ],
            "title": "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning",
            "venue": "CoRR, abs/2205.05638,",
            "year": 2022
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig"
            ],
            "title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "ACM Computing Surveys,",
            "year": 2023
        },
        {
            "authors": [
                "Xiao Liu",
                "Kaixuan Ji",
                "Yicheng Fu",
                "Weng Tam",
                "Zhengxiao Du",
                "Zhilin Yang",
                "Jie Tang"
            ],
            "title": "P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
            "year": 2022
        },
        {
            "authors": [
                "Xiaodong Liu",
                "Pengcheng He",
                "Weizhu Chen",
                "Jianfeng Gao"
            ],
            "title": "Multi-task deep neural networks for natural language understanding",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Yinhan Liu",
                "Jiatao Gu",
                "Naman Goyal",
                "Xian Li",
                "Sergey Edunov",
                "Marjan Ghazvininejad",
                "Mike Lewis",
                "Luke Zettlemoyer"
            ],
            "title": "Multilingual denoising pre-training for neural machine translation",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Francesco Locatello",
                "Dirk Weissenborn",
                "Thomas Unterthiner",
                "Aravindh Mahendran",
                "Georg Heigold",
                "Jakob Uszkoreit",
                "Alexey Dosovitskiy",
                "Thomas Kipf"
            ],
            "title": "Object-centric learning with slot attention",
            "venue": "In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Qiuhao Lu",
                "Dejing Dou",
                "Thien Huu Nguyen"
            ],
            "title": "Parameter-efficient domain knowledge integration from multiple sources for biomedical pre-trained language models",
            "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Yao Lu",
                "Max Bartolo",
                "Alastair Moore",
                "Sebastian Riedel",
                "Pontus Stenetorp"
            ],
            "title": "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Yongxi Lu",
                "Abhishek Kumar",
                "Shuangfei Zhai",
                "Yu Cheng",
                "Tara Javidi",
                "Rog\u00e9rio Schmidt Feris"
            ],
            "title": "Fullyadaptive feature sharing in multi-task networks with applications in person attribute classification",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Jiaqi Ma",
                "Zhe Zhao",
                "Xinyang Yi",
                "Jilin Chen",
                "Lichan Hong",
                "Ed H. Chi"
            ],
            "title": "Modeling task relationships in multitask learning with multi-gate mixture-of-experts",
            "venue": "In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Chris J. Maddison",
                "Andriy Mnih",
                "Yee Whye Teh"
            ],
            "title": "The concrete distribution: A continuous relaxation of discrete random variables",
            "venue": "In 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Rabeeh Karimi Mahabadi",
                "James Henderson",
                "Sebastian Ruder"
            ],
            "title": "Compacter: Efficient lowrank hypercomplex adapter layers. In Advances in Neural Information Processing Systems",
            "venue": "Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Adyasha Maharana",
                "Darryl Hannan",
                "Mohit Bansal"
            ],
            "title": "Storydall-e: Adapting pretrained text-to-image transformers for story continuation",
            "venue": "In Computer Vision - ECCV 2022 - 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Olga Majewska",
                "Ivan Vuli\u0107",
                "Goran Glava\u0161",
                "Edoardo Maria Ponti",
                "Anna Korhonen"
            ],
            "title": "Verb knowledge injection for multilingual event processing",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2021
        },
        {
            "authors": [
                "Arun Mallya",
                "Svetlana Lazebnik"
            ],
            "title": "Packnet: Adding multiple tasks to a single network by iterative pruning",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Arun Mallya",
                "Dillon Davis",
                "Svetlana Lazebnik"
            ],
            "title": "Piggyback: Adapting a single network to multiple tasks by learning to mask weights",
            "venue": "In Computer Vision - ECCV 2018 - 15th European Conference,",
            "year": 2018
        },
        {
            "authors": [
                "Michael Matena",
                "Colin Raffel"
            ],
            "title": "Merging models with fisher-weighted averaging",
            "venue": "CoRR, abs/2111.09832,",
            "year": 2021
        },
        {
            "authors": [
                "Michael McCloskey",
                "Neal J. Cohen"
            ],
            "title": "Catastrophic interference in connectionist networks: The sequential learning problem",
            "venue": "In Psychology of Learning and Motivation,",
            "year": 1989
        },
        {
            "authors": [
                "Rahul Mehta"
            ],
            "title": "Sparse transfer learning via winning lottery tickets",
            "venue": "CoRR, abs/1905.07785,",
            "year": 2019
        },
        {
            "authors": [
                "Elliot Meyerson",
                "Risto Miikkulainen"
            ],
            "title": "Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer Ordering",
            "venue": "In Proceedings of ICLR",
            "year": 2018
        },
        {
            "authors": [
                "Richard Meyes",
                "Constantin Waubert de Puiseau",
                "Andres Posada-Moreno",
                "Tobias Meisen"
            ],
            "title": "Under the hood of neural networks: Characterizing learned representations by functional neuron populations and network ablations",
            "venue": "CoRR, abs/2004.01254,",
            "year": 2020
        },
        {
            "authors": [
                "Paul Michel",
                "Omer Levy",
                "Graham Neubig"
            ],
            "title": "Are sixteen heads really better than one? In Advances in Neural Information Processing Systems",
            "venue": "Annual Conference on Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Tom\u00e1s Mikolov",
                "Ilya Sutskever",
                "Kai Chen",
                "Gregory S. Corrado",
                "Jeffrey Dean"
            ],
            "title": "Distributed representations of words and phrases and their compositionality",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2013
        },
        {
            "authors": [
                "Swaroop Mishra",
                "Daniel Khashabi",
                "Chitta Baral",
                "Hannaneh Hajishirzi"
            ],
            "title": "Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "year": 2022
        },
        {
            "authors": [
                "Ishan Misra",
                "Abhinav Shrivastava",
                "Abhinav Gupta",
                "Martial Hebert"
            ],
            "title": "Cross-stitch networks for multi-task learning",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Sarthak Mittal",
                "Alex Lamb",
                "Anirudh Goyal",
                "Vikram Voleti",
                "Murray Shanahan",
                "Guillaume Lajoie",
                "Michael Mozer",
                "Yoshua Bengio"
            ],
            "title": "Learning to combine top-down and bottom-up signals in recurrent neural networks with attention over modules",
            "venue": "Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Sarthak Mittal",
                "Yoshua Bengio",
                "Guillaume Lajoie"
            ],
            "title": "Is a modular architecture enough",
            "venue": "CoRR, abs/2206.02713,",
            "year": 2022
        },
        {
            "authors": [
                "Pavlo Molchanov",
                "Stephen Tyree",
                "Tero Karras",
                "Timo Aila",
                "Jan Kautz"
            ],
            "title": "Pruning convolutional neural networks for resource efficient inference",
            "venue": "In 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Nobuyuki Morioka",
                "Heiga Zen",
                "Nanxin Chen",
                "Yu Zhang",
                "Yifan Ding"
            ],
            "title": "Residual adapters for few-shot text-to-speech speaker adaptation",
            "venue": "CoRR, abs/2210.15868,",
            "year": 2022
        },
        {
            "authors": [
                "Mohammed Muqeeth",
                "Haokun Liu",
                "Colin Raffel"
            ],
            "title": "Models with Conditional Computation Learn Suboptimal Solutions",
            "venue": "arXiv preprint,",
            "year": 2022
        },
        {
            "authors": [
                "Ofir Nachum",
                "Shixiang Gu",
                "Honglak Lee",
                "Sergey Levine"
            ],
            "title": "Data-efficient hierarchical reinforcement learning",
            "venue": "In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems",
            "year": 2018
        },
        {
            "authors": [
                "Vaishnavh Nagarajan",
                "J. Zico Kolter"
            ],
            "title": "Uniform convergence may be unable to explain generalization in deep learning",
            "venue": "In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Nihal V Nayak",
                "Peilin Yu",
                "Stephen H Bach"
            ],
            "title": "Learning to compose soft prompts for compositional zero-shot learning",
            "venue": "arXiv preprint arXiv:2204.03574,",
            "year": 2022
        },
        {
            "authors": [
                "Renato Negrinho",
                "Matthew Gormley",
                "Geoffrey J Gordon",
                "Darshan Patil",
                "Nghia Le",
                "Daniel Ferreira"
            ],
            "title": "Towards modular and programmable architecture search",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Alejandro Newell",
                "Lu Jiang",
                "Chong Wang",
                "Li-Jia Li",
                "Jia Deng"
            ],
            "title": "Feature partitioning for efficient multi-task architectures",
            "venue": "CoRR, abs/1908.04339,",
            "year": 2019
        },
        {
            "authors": [
                "Behnam Neyshabur",
                "Hanie Sedghi",
                "Chiyuan Zhang"
            ],
            "title": "What is being transferred in transfer learning",
            "venue": "In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Oleksiy Ostapenko",
                "Pau Rodr\u00edguez",
                "Massimo Caccia",
                "Laurent Charlin"
            ],
            "title": "Continual learning via local module composition",
            "venue": "In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Junting Pan",
                "Ziyi Lin",
                "Xiatian Zhu",
                "Jing Shao",
                "Hongsheng Li"
            ],
            "title": "ST-Adapter: Parameter-efficient image-to-video transfer learning for action recognition",
            "venue": "CoRR, abs/2206.13559,",
            "year": 2022
        },
        {
            "authors": [
                "Pinelopi Papalampidi",
                "Mirella Lapata"
            ],
            "title": "Hierarchical3d adapters for long video-to-text summarization",
            "venue": "CoRR, abs/2210.04829,",
            "year": 2022
        },
        {
            "authors": [
                "Giambattista Parascandolo",
                "Niki Kilbertus",
                "Mateo Rojas-Carulla",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Learning independent causal mechanisms",
            "venue": "In Proceedings of the 35th International Conference on Machine Learning, ICML 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Marinela Parovi\u0107",
                "Goran Glava\u0161",
                "Ivan Vuli\u0107",
                "Anna Korhonen"
            ],
            "title": "BAD-X: Bilingual adapters improve zero-shot cross-lingual transfer",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Ethan Perez",
                "Florian Strub",
                "Harm de Vries",
                "Vincent Dumoulin",
                "Aaron C. Courville"
            ],
            "title": "Film: Visual reasoning with a general conditioning layer",
            "venue": "In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Andreas R\u00fcckl\u00e9",
                "Clifton Poth",
                "Aishwarya Kamath",
                "Ivan Vuli\u0107",
                "Sebastian Ruder",
                "Kyunghyun Cho",
                "Iryna Gurevych"
            ],
            "title": "AdapterHub: A framework for adapting transformers",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,",
            "year": 2020
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Ivan Vuli\u0107",
                "Iryna Gurevych",
                "Sebastian Ruder"
            ],
            "title": "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Aishwarya Kamath",
                "Andreas R\u00fcckl\u00e9",
                "Kyunghyun Cho",
                "Iryna Gurevych"
            ],
            "title": "AdapterFusion: Non-destructive task composition for transfer learning",
            "venue": "In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Ivan Vuli\u0107",
                "Iryna Gurevych",
                "Sebastian Ruder"
            ],
            "title": "UNKs everywhere: Adapting multilingual language models to new scripts",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Gregor Geigle",
                "Aishwarya Kamath",
                "Jan-Martin Steitz",
                "Stefan Roth",
                "Ivan Vuli\u0107",
                "Iryna Gurevych"
            ],
            "title": "xGQA: Cross-lingual visual question answering. In Findings of the Association for Computational Linguistics: ACL 2022",
            "venue": "Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.196. URL https://aclanthology.org/2022.findings-acl",
            "year": 2022
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Naman Goyal",
                "Xi Lin",
                "Xian Li",
                "James Cross",
                "Sebastian Riedel",
                "Mikel Artetxe"
            ],
            "title": "Lifting the curse of multilinguality by pre-training modular transformers",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "MinhQuang Pham",
                "Josep Maria Crego",
                "Fran\u00e7ois Yvon"
            ],
            "title": "Revisiting multi-domain machine translation",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Jerin Philip",
                "Alexandre Berard",
                "Matthias Gall\u00e9",
                "Laurent Besacier"
            ],
            "title": "Monolingual adapters for zero-shot neural machine translation",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4465\u20134470,",
            "year": 2020
        },
        {
            "authors": [
                "Thomas Pierrot",
                "Guillaume Ligner",
                "Scott E. Reed",
                "Olivier Sigaud",
                "Nicolas Perrin",
                "Alexandre Laterre",
                "David Kas",
                "Karim Beguir",
                "Nando de Freitas"
            ],
            "title": "Learning compositional neural programs with recursive tree search and planning",
            "venue": "In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Pilault",
                "Amine Elhattami",
                "Christopher J. Pal"
            ],
            "title": "Conditionally adaptive multi-task learning: Improving transfer learning in NLP using fewer parameters & less data",
            "venue": "In 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Emmanouil Antonios Platanios",
                "Mrinmaya Sachan",
                "Graham Neubig",
                "Tom Mitchell"
            ],
            "title": "Contextual parameter generation for universal neural machine translation",
            "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Edoardo M. Ponti"
            ],
            "title": "Inductive Bias and Modular Design for Sample-Efficient Neural Language Learning",
            "venue": "PhD thesis, University of Cambridge,",
            "year": 2021
        },
        {
            "authors": [
                "Edoardo M. Ponti",
                "Ivan Vuli\u0107",
                "Ryan Cotterell",
                "Marinela Parovic",
                "Roi Reichart",
                "Anna Korhonen"
            ],
            "title": "Parameter space factorization for zero-shot learning across tasks and languages",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Edoardo M. Ponti",
                "Alessandro Sordoni",
                "Siva Reddy"
            ],
            "title": "Combining modular skills in multitask learning",
            "venue": "CoRR, abs/2202.13914,",
            "year": 2022
        },
        {
            "authors": [
                "Edoardo Maria Ponti",
                "Goran Glava\u0161",
                "Olga Majewska",
                "Qianchu Liu",
                "Ivan Vuli\u0107",
                "Anna Korhonen"
            ],
            "title": "XCOPA: A multilingual dataset for causal commonsense reasoning",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2362\u20132376,",
            "year": 2020
        },
        {
            "authors": [
                "Sai Prasanna",
                "Anna Rogers",
                "Anna Rumshisky"
            ],
            "title": "When BERT Plays the Lottery, All Tickets Are Winning",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 3208\u20133229,",
            "year": 2020
        },
        {
            "authors": [
                "Doina Precup"
            ],
            "title": "Temporal Abstraction in Reinforcement Learning",
            "venue": "PhD thesis, University of Massachusetts Amherst,",
            "year": 2000
        },
        {
            "authors": [
                "Nasim Rahaman",
                "Muhammad Waleed Gondal",
                "Shruti Joshi",
                "Peter V. Gehler",
                "Yoshua Bengio",
                "Francesco Locatello",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Dynamic inference with neural interpreters",
            "venue": "In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Samyam Rajbhandari",
                "Conglong Li",
                "Zhewei Yao",
                "Minjia Zhang",
                "Reza Yazdani Aminabadi",
                "Ammar Ahmad Awan",
                "Jeff Rasley",
                "Yuxiong He"
            ],
            "title": "Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation AI scale",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Janarthanan Rajendran",
                "Aravind S. Lakshminarayanan",
                "Mitesh M. Khapra",
                "P. Prasanna",
                "Balaraman Ravindran"
            ],
            "title": "Attend, adapt and transfer: Attentive deep architecture for adaptive transfer from multiple sources in the same domain",
            "venue": "In 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Sylvestre-Alvise Rebuffi",
                "Hakan Bilen",
                "Andrea Vedaldi"
            ],
            "title": "Learning multiple visual domains with residual adapters",
            "venue": "In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Sylvestre-Alvise Rebuffi",
                "Hakan Bilen",
                "Andrea Vedaldi"
            ],
            "title": "Efficient parametrization of multi-domain deep neural networks",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Scott E. Reed",
                "Nando de Freitas"
            ],
            "title": "Neural programmer-interpreters",
            "venue": "In 4th International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "Carlos Riquelme",
                "Joan Puigcerver",
                "Basil Mustafa",
                "Maxim Neumann",
                "Rodolphe Jenatton",
                "Andr\u00e9 Susano Pinto",
                "Daniel Keysers",
                "Neil Houlsby"
            ],
            "title": "Scaling vision with sparse mixture of experts",
            "venue": "In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Stephen Roller",
                "Sainbayar Sukhbaatar",
                "Arthur Szlam",
                "Jason Weston"
            ],
            "title": "Hash layers for large sparse models",
            "venue": "In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Clemens Rosenbaum",
                "Tim Klinger",
                "Matthew Riemer"
            ],
            "title": "Routing networks: Adaptive selection of non-linear functions for multi-task learning",
            "venue": "In 6th International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Clemens Rosenbaum",
                "Ignacio Cases",
                "Matthew Riemer",
                "Tim Klinger"
            ],
            "title": "Routing networks and the challenges of modular and compositional computation",
            "venue": "CoRR, abs/1904.12774,",
            "year": 2019
        },
        {
            "authors": [
                "Andreas R\u00fcckl\u00e9",
                "Gregor Geigle",
                "Max Glockner",
                "Tilman Beck",
                "Jonas Pfeiffer",
                "Nils Reimers",
                "Iryna Gurevych"
            ],
            "title": "AdapterDrop: On the efficiency of adapters in transformers",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Sebastian Ruder"
            ],
            "title": "An overview of multi-task learning in deep neural networks",
            "venue": "CoRR, abs/1706.05098,",
            "year": 2017
        },
        {
            "authors": [
                "Sebastian Ruder",
                "Joachim Bingel",
                "Isabelle Augenstein",
                "Anders S\u00f8gaard"
            ],
            "title": "Latent multi-task architecture learning",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 4822\u20134829,",
            "year": 2019
        },
        {
            "authors": [
                "Sebastian Ruder",
                "Matthew E. Peters",
                "Swabha Swayamdipta",
                "Thomas Wolf"
            ],
            "title": "Transfer learning in natural language processing",
            "venue": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials,",
            "year": 2019
        },
        {
            "authors": [
                "Sebastian Ruder",
                "Noah Constant",
                "Jan Botha",
                "Aditya Siddhant",
                "Orhan Firat",
                "Jinlan Fu",
                "Pengfei Liu",
                "Junjie Hu",
                "Dan Garrette",
                "Graham Neubig",
                "Melvin Johnson"
            ],
            "title": "XTREME-R: Towards more challenging and nuanced multilingual evaluation",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Victor Sanh",
                "Thomas Wolf",
                "Sebastian Ruder"
            ],
            "title": "A hierarchical multi-task approach for learning embeddings from semantic tasks",
            "venue": "In The Thirty-Third AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Victor Sanh",
                "Thomas Wolf",
                "Alexander M. Rush"
            ],
            "title": "Movement pruning: Adaptive sparsity by fine-tuning",
            "venue": "In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Santilli",
                "Thibault F\u00e9vry",
                "Jason Alan Fries",
                "Ryan Teehan",
                "Stella Biderman",
                "Leo Gao",
                "Tali Bers",
                "Thomas Wolf",
                "Alexander M. Rush"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "venue": "In The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Kanthashree Mysore Sathyendra",
                "Thejaswi Muniyappa",
                "Feng-Ju Chang",
                "Jing Liu",
                "Jinru Su",
                "Grant P. Strimel",
                "Athanasios Mouchtaris",
                "Siegfried Kunzmann"
            ],
            "title": "Contextual adapters for personalized speech recognition in neural transducers",
            "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Danielle Saunders"
            ],
            "title": "Domain adaptation and multi-domain adaptation for neural machine translation: A survey",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2022
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze"
            ],
            "title": "Exploiting cloze-questions for few-shot text classification and natural language inference",
            "venue": "In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze"
            ],
            "title": "It\u2019s not just size that matters: Small language models are also few-shot learners",
            "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Timo Schick",
                "Sahana Udupa",
                "Hinrich Sch\u00fctze"
            ],
            "title": "Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in NLP",
            "venue": "Trans. Assoc. Comput. Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Bernhard Sch\u00f6lkopf",
                "Dominik Janzing",
                "Jonas Peters",
                "Eleni Sgouritsa",
                "Kun Zhang",
                "Joris M. Mooij"
            ],
            "title": "On causal and anticausal learning",
            "venue": "In Proceedings of the 29th International Conference on Machine Learning,",
            "year": 2012
        },
        {
            "authors": [
                "Bernhard Sch\u00f6lkopf",
                "Francesco Locatello",
                "Stefan Bauer",
                "Nan Rosemary Ke",
                "Nal Kalchbrenner",
                "Anirudh Goyal",
                "Yoshua Bengio"
            ],
            "title": "Toward causal representation learning",
            "venue": "Proceedings of the IEEE,",
            "year": 2021
        },
        {
            "authors": [
                "Karin Kipper Schuler"
            ],
            "title": "VerbNet: A broad-coverage, comprehensive verb lexicon",
            "venue": "PhD thesis, University of Pennsylvania,",
            "year": 2005
        },
        {
            "authors": [
                "Noam Shazeer",
                "Azalia Mirhoseini",
                "Krzysztof Maziarz",
                "Andy Davis",
                "Quoc V. Le",
                "Geoffrey E. Hinton",
                "Jeff Dean"
            ],
            "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
            "venue": "In 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Hava T. Siegelmann",
                "Eduardo D. Sontag"
            ],
            "title": "On the computational power of neural nets",
            "venue": "Journal of Computer and System Sciences,",
            "year": 1995
        },
        {
            "authors": [
                "Anders S\u00f8gaard",
                "Yoav Goldberg"
            ],
            "title": "Deep multi-task learning with low level tasks supervised at lower layers. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
            "year": 2016
        },
        {
            "authors": [
                "Robyn Speer",
                "Joshua Chin",
                "Catherine Havasi"
            ],
            "title": "Conceptnet 5.5: An open multilingual graph of general knowledge",
            "venue": "In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Karolina Stanczak",
                "Edoardo Ponti",
                "Lucas Torroba Hennigen",
                "Ryan Cotterell",
                "Isabelle Augenstein"
            ],
            "title": "Same neurons, different languages: Probing morphosyntax in multilingual pre-trained models",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Asa Cooper Stickland",
                "Iain Murray"
            ],
            "title": "BERT and PALs: Projected attention layers for efficient adaptation in multi-task learning",
            "venue": "In Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Gjorgji Strezoski",
                "Nanne van Noord",
                "Marcel Worring"
            ],
            "title": "Many task learning with task routing",
            "venue": "October 27 - November",
            "year": 2019
        },
        {
            "authors": [
                "Emma Strubell",
                "Ananya Ganesh",
                "Andrew McCallum"
            ],
            "title": "Energy and policy considerations for deep learning in NLP",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Tianxiang Sun",
                "Yunfan Shao",
                "Xiaonan Li",
                "Pengfei Liu",
                "Hang Yan",
                "Xipeng Qiu",
                "Xuanjing Huang"
            ],
            "title": "Learning sparse sharing architectures for multiple tasks",
            "venue": "In The Thirty-Fourth AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Tianxiang Sun",
                "Zhengfu He",
                "Hong Qian",
                "Xuanjing Huang",
                "Xipeng Qiu"
            ],
            "title": "BBTv2: pure black-box optimization can be comparable to gradient descent for few-shot learning",
            "venue": "CoRR, abs/2205.11200,",
            "year": 2022
        },
        {
            "authors": [
                "Ximeng Sun",
                "Rameswar Panda",
                "Rog\u00e9rio Feris",
                "Kate Saenko"
            ],
            "title": "AdaShare: Learning what to share for efficient deep multi-task learning",
            "venue": "In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Yi-Lin Sung",
                "Varun Nair",
                "Colin Raffel"
            ],
            "title": "Training neural networks with fixed sparse masks",
            "venue": "In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Yi-Lin Sung",
                "Jaemin Cho",
                "Mohit Bansal"
            ],
            "title": "VL-ADAPTER: parameter-efficient transfer learning for visionand-language tasks",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Richard S. Sutton"
            ],
            "title": "Two problems with back propagation and other steepest descent learning procedures for networks",
            "venue": "In Proceedings of the Eighth Annual Conference of the Cognitive Science Society,",
            "year": 1986
        },
        {
            "authors": [
                "Richard S. Sutton",
                "Doina Precup",
                "Satinder Singh"
            ],
            "title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning",
            "venue": "Artificial Intelligence,",
            "year": 1999
        },
        {
            "authors": [
                "Pawel Swietojanski",
                "Jinyu Li",
                "Steve Renals"
            ],
            "title": "Learning hidden unit contributions for unsupervised acoustic model adaptation",
            "venue": "IEEE ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2016
        },
        {
            "authors": [
                "Bethan Thomas",
                "Samuel Kessler",
                "Salah Karout"
            ],
            "title": "Efficient adapter transfer of self-supervised speech models for automatic speech recognition",
            "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Brian Thompson",
                "Huda Khayrallah",
                "Antonios Anastasopoulos",
                "Arya D. McCarthy",
                "Kevin Duh",
                "Rebecca Marvin",
                "Paul McNamee",
                "Jeremy Gwinnup",
                "Tim Anderson",
                "Philipp Koehn"
            ],
            "title": "Freezing subnetworks to analyze domain adaptation in neural machine translation",
            "venue": "In Proceedings of the Third Conference on Machine Translation: Research Papers,",
            "year": 2018
        },
        {
            "authors": [
                "Katrin Tomanek",
                "Vicky Zayats",
                "Dirk Padfield",
                "Kara Vaillancourt",
                "Fadi Biadsy"
            ],
            "title": "Residual adapters for parameter-efficient ASR adaptation to atypical and accented speech",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Karl Ulrich"
            ],
            "title": "The role of product architecture in the manufacturing firm",
            "venue": "Research Policy,",
            "year": 1995
        },
        {
            "authors": [
                "Ahmet \u00dcst\u00fcn",
                "Arianna Bisazza",
                "Gosse Bouma",
                "Gertjan van Noord"
            ],
            "title": "UDapter: Language adaptation for truly Universal Dependency parsing",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2302\u20132315,",
            "year": 2020
        },
        {
            "authors": [
                "Ahmet \u00dcst\u00fcn",
                "Alexandre Berard",
                "Laurent Besacier",
                "Matthias Gall\u00e9"
            ],
            "title": "Multilingual unsupervised neural machine translation with denoising adapters",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Ahmet \u00dcst\u00fcn",
                "Arianna Bisazza",
                "Gosse Bouma",
                "Gertjan van Noord",
                "Sebastian Ruder"
            ],
            "title": "Hyper-X: A unified hypernetwork for multi-task multilingual transfer",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Gido M. van de Ven",
                "Andreas S. Tolias"
            ],
            "title": "Three scenarios for continual learning",
            "venue": "CoRR, abs/1904.07734,",
            "year": 2019
        },
        {
            "authors": [
                "Simon Vandenhende",
                "Stamatios Georgoulis",
                "Luc Van Gool",
                "Bert De Brabandere"
            ],
            "title": "Branched multi-task networks: Deciding what layers to share",
            "venue": "British Machine Vision Conference",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017,",
            "year": 2017
        },
        {
            "authors": [
                "David Vilar"
            ],
            "title": "Learning hidden unit contribution for adapting neural machine translation models. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
            "year": 2018
        },
        {
            "authors": [
                "Elena Voita",
                "David Talbot",
                "Fedor Moiseev",
                "Rico Sennrich",
                "Ivan Titov"
            ],
            "title": "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "John von Neumann"
            ],
            "title": "First draft of a report on the EDVAC",
            "venue": "Technical report,",
            "year": 1945
        },
        {
            "authors": [
                "Johannes von Oswald",
                "Christian Henning",
                "Jo\u00e3o Sacramento",
                "Benjamin F. Grewe"
            ],
            "title": "Continual learning with hypernetworks",
            "venue": "In 8th International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Tu Vu",
                "Aditya Barua",
                "Brian Lester",
                "Daniel Cer",
                "Mohit Iyyer",
                "Noah Constant"
            ],
            "title": "Overcoming catastrophic forgetting in zero-shot cross-lingual generation",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Tu Vu",
                "Aditya Barua",
                "Brian Lester",
                "Daniel Cer",
                "Mohit Iyyer",
                "Noah Constant"
            ],
            "title": "Overcoming catastrophic forgetting in zero-shot cross-lingual generation",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Tu Vu",
                "Brian Lester",
                "Noah Constant",
                "Rami Al-Rfou",
                "Daniel Cer"
            ],
            "title": "SPoT: Better frozen model adaptation through soft prompt transfer",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "G\u00fcnter P. Wagner",
                "Jason Mezey",
                "Raffaele Calabretta"
            ],
            "title": "Natural selection and the origin of modules. In Werner Callebaut and Diego Rasskin-Gutman (eds.), Modularity: Understanding the Development and Evolution of Complex",
            "venue": "Natural Systems,",
            "year": 2005
        },
        {
            "authors": [
                "Alex Wang",
                "Yada Pruksachatkun",
                "Nikita Nangia",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R. Bowman"
            ],
            "title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
            "venue": "In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Pu Wang",
                "Hugo Van hamme"
            ],
            "title": "Bottleneck low-rank transformers for low-resource spoken language understanding",
            "venue": "In Proceedings of Interspeech 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Ruize Wang",
                "Duyu Tang",
                "Nan Duan",
                "Zhongyu Wei",
                "Xuanjing Huang",
                "Jianshu Ji",
                "Guihong Cao",
                "Daxin Jiang",
                "Ming Zhou"
            ],
            "title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters",
            "venue": "In Findings of the Association for Computational Linguistics: ACL-IJCNLP",
            "year": 2021
        },
        {
            "authors": [
                "Yaqing Wang",
                "Sahaj Agarwal",
                "Subhabrata Mukherjee",
                "Xiaodong Liu",
                "Jing Gao",
                "Ahmed Hassan Awadallah",
                "Jianfeng Gao"
            ],
            "title": "AdaMix: Mixture-of-adaptations for parameter-efficient model tuning",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Zirui Wang",
                "Zachary C. Lipton",
                "Yulia Tsvetkov"
            ],
            "title": "On negative interference in multilingual models: Findings and a meta-learning treatment",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4438\u20134450,",
            "year": 2020
        },
        {
            "authors": [
                "Zirui Wang",
                "Yulia Tsvetkov",
                "Orhan Firat",
                "Yuan Cao"
            ],
            "title": "Gradient vaccine: Investigating and improving multi-task optimization in massively multilingual models",
            "venue": "In 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Chihiro Watanabe"
            ],
            "title": "Interpreting layered neural networks via hierarchical modular representation",
            "venue": "Neural Information Processing - 26th International Conference,",
            "year": 2019
        },
        {
            "authors": [
                "Albert Webson",
                "Ellie Pavlick"
            ],
            "title": "Do prompt-based models really understand the meaning of their prompts",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V Le"
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Ronald J. Williams"
            ],
            "title": "Toward a theory of reinforcement-learning connectionist systems",
            "venue": "Technical Report NU-CCS-88-3, Northeastern University,",
            "year": 1988
        },
        {
            "authors": [
                "Ronald J. Williams"
            ],
            "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
            "venue": "Machine Learning,",
            "year": 1992
        },
        {
            "authors": [
                "Genta Indra Winata",
                "Samuel Cahyawijaya",
                "Zhaojiang Lin",
                "Zihan Liu",
                "Pascale Fung"
            ],
            "title": "Lightweight and efficient end-to-end speech recognition using low-rank transformer",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "Remi Louf",
                "Morgan Funtowicz",
                "Joe Davison",
                "Sam Shleifer",
                "Patrick von Platen",
                "Clara Ma",
                "Yacine Jernite",
                "Julien Plu",
                "Canwen Xu",
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush"
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,",
            "year": 2020
        },
        {
            "authors": [
                "Online",
                "October"
            ],
            "title": "Association for Computational Linguistics",
            "venue": "doi: 10.18653/v1/2020.emnlp-demos.6. URL https://aclanthology.org/2020.emnlp-demos.6.",
            "year": 2020
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Vivek Ramanujan",
                "Rosanne Liu",
                "Aniruddha Kembhavi",
                "Mohammad Rastegari",
                "Jason Yosinski",
                "Ali Farhadi"
            ],
            "title": "Supermasks in superposition",
            "venue": "In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Gabriel Ilharco",
                "Samir Ya Gadre",
                "Rebecca Roelofs",
                "Raphael Gontijo Lopes",
                "Ari S. Morcos",
                "Hongseok Namkoong",
                "Ali Farhadi",
                "Yair Carmon",
                "Simon Kornblith",
                "Ludwig Schmidt"
            ],
            "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
            "venue": "International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Teng Xi",
                "Yifan Sun",
                "Deli Yu",
                "Bi Li",
                "Nan Peng",
                "Gang Zhang",
                "Xinyu Zhang",
                "Zhigang Wang",
                "Jinwen Chen",
                "Jian Wang",
                "Lufei Liu",
                "Haocheng Feng",
                "Junyu Han",
                "Jingtuo Liu",
                "Errui Ding",
                "Jingdong Wang"
            ],
            "title": "UFO: unified feature optimization",
            "venue": "Computer Vision - ECCV 2022 - 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "An Yang",
                "Junyang Lin",
                "Rui Men",
                "Chang Zhou",
                "Le Jiang",
                "Xianyan Jia",
                "Ang Wang",
                "Jie Zhang",
                "Jiamang Wang",
                "Yong Li",
                "Di Zhang",
                "Wei Lin",
                "Lin Qu",
                "Jingren Zhou",
                "Hongxia Yang"
            ],
            "title": "Exploring sparse expert models and beyond",
            "venue": "CoRR, abs/2105.15082,",
            "year": 2021
        },
        {
            "authors": [
                "Guangyu Robert Yang",
                "Madhura R. Joglekar",
                "H. Francis Song",
                "William T. Newsome",
                "Xiao-Jing Wang"
            ],
            "title": "Task representations in neural networks trained to perform many cognitive tasks",
            "venue": "Nature Neuroscience,",
            "year": 2019
        },
        {
            "authors": [
                "Yongxin Yang",
                "Timothy M. Hospedales"
            ],
            "title": "Deep multi-task representation learning: A tensor factorisation approach",
            "venue": "In 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Qinyuan Ye",
                "Bill Yuchen Lin",
                "Xiang Ren"
            ],
            "title": "CrossFit: A few-shot learning challenge for cross-task generalization in NLP",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Zhao You",
                "Shulin Feng",
                "Dan Su",
                "Dong Yu"
            ],
            "title": "Speechmoe2: Mixture-of-experts model with improved routing",
            "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Haonan Yu",
                "Sergey Edunov",
                "Yuandong Tian",
                "Ari S. Morcos"
            ],
            "title": "Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP",
            "venue": "In 8th International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Fan Zhang",
                "Duyu Tang",
                "Yong Dai",
                "Cong Zhou",
                "Shuangzhi Wu",
                "Shuming Shi"
            ],
            "title": "SkillNet-NLU: A Sparsely Activated Model for General-Purpose Natural Language Understanding",
            "venue": "CoRR, abs/2203.03312,",
            "year": 2022
        },
        {
            "authors": [
                "Rongsheng Zhang",
                "Yinhe Zheng",
                "Xiaoxi Mao",
                "Minlie Huang"
            ],
            "title": "Unsupervised domain adaptation with adapter",
            "venue": "CoRR, abs/2111.00667,",
            "year": 2021
        },
        {
            "authors": [
                "Zhanpeng Zhang",
                "Ping Luo",
                "Chen Change Loy",
                "Xiaoou Tang"
            ],
            "title": "Facial landmark detection by deep multi-task learning",
            "venue": "In Computer Vision - ECCV 2014 - 13th European Conference,",
            "year": 2014
        },
        {
            "authors": [
                "Zhengyan Zhang",
                "Zhiyuan Zeng",
                "Yankai Lin",
                "Chaojun Xiao",
                "Xu Han",
                "Zhiyuan Liu",
                "Maosong Sun",
                "Jie Zhou"
            ],
            "title": "Emergent modularity in pre-trained transformers, 2022b. URL https://openreview.net/forum? id=XHuQacT6sa6",
            "year": 2022
        },
        {
            "authors": [
                "Mengjie Zhao",
                "Tao Lin",
                "Fei Mi",
                "Martin Jaggi",
                "Hinrich Sch\u00fctze"
            ],
            "title": "Masking as an efficient alternative to finetuning for pretrained language models",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2226\u20132241,",
            "year": 2020
        },
        {
            "authors": [
                "Zihao Zhao",
                "Eric Wallace",
                "Shi Feng",
                "Dan Klein",
                "Sameer Singh"
            ],
            "title": "Calibrate before use: Improving few-shot performance of language models",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Tao Zhong",
                "Zhixiang Chi",
                "Li Gu",
                "Yang Wang",
                "Yuanhao Yu",
                "Jin Tang"
            ],
            "title": "Meta-dmoe: Adapting to domain shift by meta-distillation from mixture-of-experts",
            "venue": "CoRR, abs/2210.03885,",
            "year": 2022
        },
        {
            "authors": [
                "Zexuan Zhong",
                "Dan Friedman",
                "Danqi Chen"
            ],
            "title": "Factual probing is [MASK]: Learning vs. learning to recall",
            "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5017\u20135033,",
            "year": 2021
        },
        {
            "authors": [
                "Han Zhou",
                "Xingchen Wan",
                "Ivan Vuli\u0107",
                "Anna Korhonen"
            ],
            "title": "AutoPEFT: automatic configuration search for parameter-efficient fine-tuning",
            "venue": "CoRR, abs/2301.12132,",
            "year": 2023
        },
        {
            "authors": [
                "Hattie Zhou",
                "Janice Lan",
                "Rosanne Liu",
                "Jason Yosinski"
            ],
            "title": "Deconstructing lottery tickets: Zeros, signs, and the supermask",
            "venue": "In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Kaiyang Zhou",
                "Jingkang Yang",
                "Chen Change Loy",
                "Ziwei Liu"
            ],
            "title": "Conditional prompt learning for visionlanguage models",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yanqi Zhou",
                "Tao Lei",
                "Hanxiao Liu",
                "Nan Du",
                "Yanping Huang",
                "Vincent Y. Zhao",
                "Andrew M. Dai",
                "Zhifeng Chen",
                "Quoc Le",
                "James Laudon"
            ],
            "title": "Mixture-of-experts with expert choice",
            "venue": "routing. CoRR,",
            "year": 2022
        },
        {
            "authors": [
                "Yunzheng Zhu",
                "Ruchao Fan",
                "Abeer Alwan"
            ],
            "title": "Towards better meta-initialization with task augmentation for kindergarten-aged speech recognition",
            "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Barret Zoph",
                "Irwan Bello",
                "Sameer Kumar",
                "Nan Du",
                "Yanping Huang",
                "Jeff Dean",
                "Noam Shazeer",
                "William Fedus"
            ],
            "title": "ST-MoE: designing stable and transferable sparse expert models",
            "year": 2022
        },
        {
            "authors": [
                "Simiao Zuo",
                "Xiaodong Liu",
                "Jian Jiao",
                "Young Jin Kim",
                "Hany Hassan",
                "Ruofei Zhang",
                "Jianfeng Gao",
                "Tuo Zhao"
            ],
            "title": "Taming sparsely activated transformer with stochastic experts",
            "venue": "In The Tenth International Conference on Learning Representations,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Transfer learning has recently become the dominant paradigm of machine learning. Pretrained models fine-tuned for downstream tasks achieve better performance with fewer labelled examples. Nonetheless, it remains unclear how to develop models that specialise towards multiple tasks without incurring negative interference and that generalise systematically to non-identically distributed tasks. Modular deep learning has emerged as a promising solution to these challenges. In this framework, units of computation are often implemented as autonomous parameter-efficient modules. Information is conditionally routed to a subset of modules and subsequently aggregated. These properties enable positive transfer and systematic generalisation by separating computation from routing and updating modules locally. We offer a survey of modular architectures, providing a unified view over several threads of research that evolved independently in the scientific literature. Moreover, we explore various additional purposes of modularity, including scaling language models, causal inference and discovery, programme simulation, and hierarchical reinforcement learning. Finally, we report various concrete applications where modularity has been successfully deployed such as cross-lingual and cross-modal knowledge transfer. More information on modular deep learning is available at www.modulardeeplearning.com/."
        },
        {
            "heading": "1 Introduction and Motivation",
            "text": "Transfer learning has recently become pervasive in machine learning technology, such as in natural language processing (Ruder et al., 2019b; Brown et al., 2020), computer vision (Dosovitskiy et al., 2021), and reinforcement learning (Reed et al., 2022), among other areas. In its most successful incarnation, transfer learning consists of pre-training a model on vast amounts of raw data in a self-supervised fashion and subsequently fine-tuning it for new tasks based on a small number of labelled examples. Despite its success, this paradigm for transfer learning suffers from a series of limitations in various settings. Firstly, in multi-task fine-tuning, the learning signals from different tasks may negatively interfere with each other (McCloskey & Cohen, 1989). Similarly, in continuous learning, adapting to new examples can result in catastrophic forgetting of knowledge acquired from previous examples (Sutton, 1986; French, 1999).1 Secondly, in settings where the training and evaluation distributions are not identical, these models fail in generalising systematically (Lake & Baroni, 2018; Hupkes et al., 2020). This makes models brittle and inaccurate and hampers their deployment in real-world applications, where distribution shifts are common.\n\u2217Authors contributed equally. 1These phenomena have also been referred to as spatial and temporal \u2018crosstalk\u2019 (Jacobs et al., 1991b).\nar X\niv :2\n30 2.\n11 52\n9v 1\n[ cs\n.L G\n] 2\n2 Fe\nIn contrast, many biological and artificial systems do not suffer from these weaknesses by virtue of their modularity (Fodor, 1983; Ballard, 1986), defined as the correspondence between strongly interconnected components of a system (i.e., modules) and the functions they perform (Baldwin & Clark, 2000; Ulrich, 1995). In other words, each module is specialised for a unique purpose, for which it is reused consistently. In animal brains, this favours evolvability, the ability to adapt quickly to new environments, and resilience to environment perturbations (Wagner et al., 2005) because it makes rewiring connections easier than in monolithic, entangled networks (Kashtan & Alon, 2005). Artificial systems, such as programming languages and computer hardware, are similarly designed in a modular fashion (Booch et al., 2008; Baldwin & Clark, 2000) because this modular design favours consistency, ease of adaptation, and interpretability.\nTo what extent, then, do \u2018vanilla\u2019 neural networks display the desirable property of being modular? In principle, given their fully connected nature, they could develop such a structure as a by-product of optimising a loss for a downstream task. Recent structural analyses based on hierarchical clustering of neurons revealed that vanilla neural networks can indeed learn such a modular pattern (Watanabe, 2019; Casper et al., 2022; Foroutan et al., 2022). Favourable conditions for the emergence of modularity include multi-task learning (Dobs et al., 2022) and regularisation through dropout (Lange et al., 2022). In particular, from a structural perspective, populations of neurons may activate jointly in response to specific features of the input or the output classes,2 resulting in similar changes in model performance when ablated (Meyes et al., 2020). From a functional perspective, multi-task learning may lead to segregated, specialised sub-networks (Yang et al., 2019; Dobs et al., 2022). On the other hand, Csord\u00e1s et al. (2021) revealed that a given sub-network does not tend to be re-used for similar sub-tasks nor to be combined with others to express more complex functions.\n2Lange et al. (2022) found that clusters identified through downstream (output) information do not match with the clusters identified through upstream (input) information. They attribute this phenomenon to their different roles, namely disentanglement of the input structure and composition of the output structure, respectively.\nIn fact, in many cases, the performance of a model on simple tasks requiring a certain skill and composite tasks requiring a combination thereof is entirely uncorrelated (Li et al., 2022a).\nFor this reason, previous work explored the idea of designing neural networks that are explicitly modular (Jacobs et al., 1991a; Rosenbaum et al., 2018; Ponti, 2021; Mittal et al., 2022). This has the goal of achieving not only functional specialisation (Zhang et al., 2022b), but also re-usability and composability. In particular, these methods involve identifying 1) modules in a neural network that can be updated locally and asynchronously, without affecting the rest of the parameters; 2) a routing function that chooses a subset of modules for each example or task; and 3) an aggregation function that aggregates the outputs of the active modules. Each of these three ingredients can be manually specified or learned. We provide several case studies of different configurations of these components in Figure 1.\nThe main advantages of modular neural architectures are positive transfer, compositionality, and parameter efficiency. Firstly, modularity encourages positive transfer by encoding similar functions with the same module. At the same time, it prevents interference and forgetting by allocating distinct functions to different dedicated modules (Jacobs et al., 1991b). For instance, massively multilingual Transformer-based models in NLP are known to suffer from a \u2018curse of multilinguality\u2019 (Conneau et al., 2020) due to the conflicting information that the gradient from each language-specific loss carries (Wang et al., 2021b). A possible solution is augmenting these entangled, fully shared models with specialised modules responsible for individual languages (Pfeiffer et al., 2020b; 2022b). More generally, as the range of tasks modelled jointly by a single model becomes increasingly diverse, modularity may be instrumental in the advent of general-purpose, multi-modal agents that encompass vision, language, and action (Reed et al., 2022).\nSecondly, modules representing different skills (at the task level) or features (at the example level) can be composed together and updated locally, without affecting the rest of the network. These two properties are crucial in two main settings, which correspond to different aspects of systematic generalisation: one is the ability to re-compose, i.e. zero-shot transfer to tasks consisting of new combinations of learned skills, or examples consisting of new combinations of observed features (Hupkes et al., 2020). For instance, while modules for the Guaran\u00ed language and for dependency parsing can only be trained separately due to the lack of annotated data for dependency parsing in Guaran\u00ed, they can be composed to perform inference on this unobserved task\u2013language combination (Pfeiffer et al., 2020b). Similarly, in hierarchical reinforcement learning, an agent can follow different sequences of modular policies known as options in tasks requiring the completion of similar sub-goals in different orders (Sutton et al., 1999; Precup, 2000). The other aspect of systematic generalisation is robustness. In fact, if modules are taken to correspond to independent and reusable physical mechanisms (Sch\u00f6lkopf et al., 2012), local shifts in their distributions require updating only the parameters accounting for the affected skills or features (Goyal et al., 2021; Sch\u00f6lkopf et al., 2021), while the rest of the model remains invariant to the change. In practice, the ability to perform local updates facilitates sample efficiency, as fewer examples are necessary to adapt models to new tasks (Bengio et al., 2020; Ponti et al., 2022).\nThirdly, an additional advantage of modular neural architectures is parameter and time efficiency. In this framework, fine-tuning a model on a specific task only requires storing a modular adapter rather than a separate copy of the entire (typically large) model. What is more, modules can be added or removed on-the-fly in an incremental manner, adjusting the model capacity according to the task complexity. This ability is known as conditional computation (Bengio et al., 2015). Finally, modularity enables language models to scale to larger numbers of parameters while retaining the same time complexity, by selecting only a small set of experts per example (Shazeer et al., 2017; Fedus et al., 2021).\nAs the main contribution of this survey, we offer a unified view of modular deep learning, illustrating how many families of methods can be defined along four key dimensions: 1) how they implement modules, which constitute the minimum unit of computation; 2) how they select active modules through a routing function; 3) how module outputs are aggregated; and 4) how the modules are trained with the rest of the model.\nFor module implementation, we discuss sparse subnetworks (Hu et al., 2022; Ansell et al., 2022), adapter layers (Rebuffi et al., 2018; Pfeiffer et al., 2020b), and prefix tuning (Li & Liang, 2021), among others. These methods have been proven as an effective way to adapt large pre-trained models, achieving better performance and sample efficiency than alternative strategies such as in-context learning (Liu et al., 2022b), which may be\nbrittle (Lu et al., 2022). In fact, modules can also take the form of human-engineered prompts, where the model is provided with input\u2013output examples (Brown et al., 2020) or task instructions (Wei et al., 2022). While many module implementations share the same underlying functional form (He et al., 2021), they offer different trade-offs between efficiency and performance.\nWe then discuss how routing functions control the flow of information to the modules: in fixed routing, module allocation is manually defined when expert knowledge is available(Hampshire & Waibel, 1992; Rajendran et al., 2017, inter alia). In learned routing, a parameterised routing function is inferred during training. This, however, poses a series of challenges, such as training instability, module collapse, and overfitting (Rosenbaum et al., 2019). Orthogonally, we also distinguish between hard and soft routing. In hard routing, only a subset of modules is activated (Rosenbaum et al., 2018; Ponti et al., 2022; Fernando et al., 2017, inter alia). In soft routing, all modules are aggregated according to continuous scores (Jacobs et al., 1991b; Jordan & Jacobs, 1994). While soft routing is amenable to vanilla gradient descent, it is highly inefficient. On the other hand, hard routing requires approximate inference but facilitates conditional computation and module specialisation. When multiple modules are selected, several aggregation strategies are possible. For instance, these can be based on interpolating the parameters of active modules (Ansell et al., 2022) or an attention mechanism over the module outputs (Pfeiffer et al., 2021a). Alternative methods include input prompt concatenation (Vu et al., 2022b) and function composition (Andreas et al., 2016b).\nFinally, modules can be trained jointly with the rest of the base model in multi-task learning (Caruana, 1997; Ruder, 2017), added sequentially in classic continual learning (Rusu et al., 2016), or integrated post-hoc into an already pre-trained and frozen model (Rebuffi et al., 2017; Houlsby et al., 2019). The last scenario is most common with current state-of-the-art models, which are trained as dense, fully shared models and may be \u2018modularised\u2019 after pre-training.\nCrucially, this taxonomy reveals unexpected connections between several independent threads of research, including aggregation functions and mode connectivity (Frankle et al., 2020), routing and hypernetworks (Ha et al., 2017), among others. We further illustrate a series of applications of modular networks in transfer learning across different areas such as natural language processing, computer vision, and speech processing. In addition, we show how modularity plays an important role in causal inference and discovery, programme simulation, and hierarchical reinforcement learning. We hope that our overview will spark future research on modular deep learning in areas that may benefit from it such as community-driven efforts to develop and maintain machine learning technology. This survey, as well as related talks and projects, is available at https://www.modulardeeplearning.com/."
        },
        {
            "heading": "2 Modular Deep Learning",
            "text": "This survey focuses on modular deep learning: namely, on models composed of modules. These are autonomous computation functions that, depending on their architecture and purpose, are variously referred to as adapters (Rebuffi et al., 2017; Pfeiffer et al., 2020a), options (Sutton et al., 1999; Precup, 2000), or experts (Jacobs et al., 1991a; Jordan & Jacobs, 1994). Crucially, these modules are distinguished from a routing function, which controls the information flow to the modules. Finally, an aggregation function aggregates their outputs. Modules can be optionally combined with fully shared (thus, non-modular) parameters as part of the same neural architecture. In order to provide a unified view of the landscape of modular deep learning, we create a taxonomy of four dimensions of variation: computation, routing, aggregation, and training. These dimensions are mutually independent; hence, many methods can be interpreted as different combinations of these dimensions, listed in \u00a7 2.1. Concurrently, we provide a unified, consistent notation in \u00a7 2.2, which helps illuminate the relationship among such methods."
        },
        {
            "heading": "2.1 Taxonomy",
            "text": "1) Computation function: How is each module implemented? (\u00a7 3) A module may consist of any component of a neural architecture, such as multiple copies of a model (Jacobs et al., 1991a) or one of its layers (Fedus et al., 2021). Alternatively, as it is common in transfer learning, modules can be combined with a function parameterised by fully shared pre-trained weights. In this case, we distinguish between modification of\nparameters (parameter composition), concatenation with input features (input composition), and function composition by stacking neural modules.\n2) Routing function: How are active modules selected? (\u00a7 4) Under fixed routing, we categorise approaches where the routing function is fixed. This assumes that the specialisation of each module, as well as the combination of modules required for each task, is known a priori. In learned routing, the parameters of the routing mechanism are learned during training. In this case, routing is soft if all modules are ranked through a continuous score, or hard if each module is given a binary score (active or inactive).\n3) Aggregation function: How are the outputs of the active modules aggregated? (\u00a7 5) We differentiate between methods that compose the outputs of the active modules deterministically (e.g., based on a weighted average) from those where the aggregation function is implemented as a learnable neural network that depends on the output of all modules.\n4) Training setting: How are the modules trained? (\u00a7 6) Some methods, such as MoEs, train the modules (and possibly the routing function) jointly with the shared weights of a randomly initialised model. As an alternative, transfer learning approaches introduce modules post-hoc after pre-training weights and adapt them during fine-tuning. In continuous learning settings, instead, new modules may be introduced iteratively for every new task in a sequence."
        },
        {
            "heading": "2.2 Notation",
            "text": "More formally, let a neural network f\u03b8 : X \u2192 Y be decomposed into a graph of sub-functions. In the simplest case, this graph is a linear chain f\u03b81 \u25e6 f\u03b82 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 f\u03b8l , where \u25e6 stands for function composition. These sub-functions refer to the model\u2019s l layers, each with unique indexed parameters \u03b8i, i = 1, . . . , l.3 In turn, these can be further decomposed recursively into a graph of their constituent sub-functions: for instance, a Transformer layer (Vaswani et al., 2017) includes linear mappings for the query, key, value, and output, as well as a non-linear feed-forward network, and residual connections. We further denote the values of the parameters at initialisation as \u03b80, and the parameters after training are denoted as \u03b8?.\nAny i-th sub-function with input x can be modified by a module with parameters \u03c6 from the inventory Mi in the following different ways:\n1. parameter composition: f \u2032i(x) = f\u03b8i\u2295\u03c6(x), where \u2295 stands for an operation that composes the original parameters with the module parameters, such as element-wise addition. An example is low-rank (Hu et al., 2022) or sparse (Ansell et al., 2022) adapters.\n3We abuse notation by treating indexing over functions, fi, as identical to indexing over the parameters of a function, f\u03b8i . In this survey, both are used interchangeably.\n2. input composition: f \u2032i(x) = f\u03b8i([\u03c6,x]), where [\u00b7, \u00b7] stands for concatenation. An example is prefix tuning Li & Liang (2021).\n3. function composition: f \u2032i(x) = f\u03c6 \u25e6 f\u03b8i(x), where the outputs of the first function is fed into the second function. An example are adapter layers (Rebuffi et al., 2017).\nFor each i-th sub-function, multiple modules from an inventory Mi = f\u03c61 , . . . , f\u03c6|M| can be selected through a routing function r(\u00b7), which returns a score \u03b1j for each module f\u03c6j conditioned on the data itself, such as a language token or a visual region x or the full input x, or metadata such as the task identity t \u2208 T . Note that \u03b1 can be fixed a priori through expert knowledge or learned through an appropriate parameterisation r\u03c1(\u00b7), where \u03c1 refers to (learnable) parameters of the routing function. Often, the routing function takes special forms:\n1. In hard routing, \u03b1 \u2208 {0, 1}|M | is a discrete binary vector. If these parameters are learned, inference usually relies on score function estimators, stochastic re-parameterisation, or evolutionary algorithms.\n2. In soft routing, \u03b1 \u2208 [0, 1]|M | is a continuous probability distribution, such that \u2211 j \u03b1j = 1.\n3. Finally, \u03b1 \u2208 R|M | can be an unnormalised score vector. This is the case in linear hypernetworks (Ha et al., 2017), where \u03b1 is usually interpreted as a task embedding and the row-wise stacked module parameters \u03a6 = [\u03c61, . . . ,\u03c6|M |] act as a parameter generator.\nFinally, the output of each module is combined through an aggregation function g(\u00b7).4 The aggregation function usually takes two possible forms. One consists of a deterministic operation based on the routing scores (e.g., weighted averaging of module parameters or outputs). The other consists of a learnable neural network, such as an attention mechanism between the modules\u2019 inputs and outputs (Pfeiffer et al., 2021a). When we put the computation function, routing function, and aggregation function together, we obtain the general recipe for a modular function, illustrated in Algorithm 1.\nAlgorithm 1: Forward pass of a modular function 1 Inputs: example x, task t 2 \u03b1\u2190 r\u03c1(x, t) // Routing 3 H \u2190 {} 4 for \u03c6j \u2208Mi do 5 hj \u2190 f(x;\u03b8i,\u03c6j) // Computation 6 H \u2190 H \u222a hj 7 y \u2190 g\u03b3(\u03b1, H) // Aggregation\nGiven shared parameters \u03b8i for the i-th sub-function and a corresponding inventory of modules Mi, we first sample a task t, and an input x. The routing scores \u03b1 are obtained from the routing function r(\u00b7). We now compute the hidden representation hj of each module \u03c6j and aggregate them with the function g(\u00b7) into the output y. We elaborate on the settings for training these different components in \u00a7 6. We provide an overview of representative computation, routing, and aggregation functions in Table 2."
        },
        {
            "heading": "3 Computation Function",
            "text": "The computation function determines the design of a module. Various module architectures have been proposed such as MLP layers (Rosenbaum et al., 2018; Kirsch et al., 2018; Chang et al., 2019), independent RNNs (Goyal et al., 2021), independent CNNs (Parascandolo et al., 2018), or special-purpose architectures (Andreas et al., 2016b). However, in transfer learning, modules are most often integrated into a base architecture whose parameters are fully shared. We identify three core methods to merge a single module with\n4To avoid clutter in terminology, throughout this work we use the term composition to refer to the merger of computation functions (\u00a7 3), and the term aggregation to refer to different approaches of combining the outputs of different modules (\u00a7 5).\nthe corresponding sub-function: parameter composition, input composition, and function composition. While all three methods instantiate modules differently, we demonstrate how they can be seen in a unified view in \u00a7 3.5. We provide example illustrations of the three computation functions (in addition to a hypernetwork) as part of a Transformer architecture in Figure 2 and provide a high-level overview of their trade-offs in Table 3, which we further discuss in the respective sections.5"
        },
        {
            "heading": "3.1 Parameter Composition",
            "text": "Parameter composition methods augment the function fW of a base model with weights W \u2208 Ro\u00d7i with module parameters \u03a6 \u2208 Ro\u00d7i, where i is the input dimensionality, and o is the output dimensionality. In particular, the module inventory consists of a set of sparse or low-rank weights to ensure that the modules are parameter-efficient. Therefore, the resulting function is parameterised as f\u03b8\u2295\u03c6i , where \u2295 stands for element-wise addition.\nSparse Subnetworks Sparsity is a common inductive bias based on the assumptions (i) that only a small number of parameters of an over-parameterised model are relevant for a particular task, and that (ii) similar tasks share similar sub-networks. This is the case, for instance, for language subnetworks in multilingual language models (Stanczak et al., 2022; Foroutan et al., 2022).\nThe most widespread method to induce sparsity is pruning. This can be interpreted as the application of a binary mask b \u2208 {0, 1}|\u03b8| that selectively keeps or removes each connection in a model with trained parameters \u03b8?: f \u2032 = f\u03b8? b. The merger of \u03b8 and b results in a sparse subnetwork, but the corresponding model parameters usually remain dense for hardware and software reasons.6 After training, the trained weights are sorted based on a criterion and a fraction (bottom-k) of the weights are set to zero. Examples of\n5The comparison is mainly meant as a high-level guideline. Individual methods may have different trade-offs and mitigate certain weaknesses indicated in the table.\n6In fact, sparse linear algebra operations on graphic processing units remain highly inefficient, if available at all. Examples include the sparse tensor classes in Pytorch: https://pytorch.org/docs/stable/sparse.html\ncriteria include magnitude after convergence (Han et al., 2017) and change of magnitude between initialisation and convergence (Frankle & Carbin, 2019).\nAs pruning generally leads to a loss in performance due to the change in network connections, the non-pruned weights are typically re-wound to their initialisation value and re-trained. In practice, rather than pruning all weights in a single run, iterative pruning is carried out (Han et al., 2015; Frankle & Carbin, 2019) over multiple stages. The models pruned in this fashion often retain\u2014if not surpass \u2014the performance of the original dense model. The existence of a subnetwork with such property in any given randomly initialised model is known as the Lottery Ticket Hypothesis (LTH; Frankle & Carbin, 2019; Chen et al., 2020). These \u2018winning tickets\u2019 have also been shown to exist in RL and NLP (Yu et al., 2020), as well as in computer vision (Frankle et al., 2020). Subnetworks achieve above-random performance even when kept fixed at their random initialisation (Zhou et al., 2019; Wortsman et al., 2020; Zhao et al., 2020), so f \u2032 = f\u03b80 b. In this case, they are known as supermasks.\nWinning tickets also occur in pre-trained models, such as language models (Chen et al., 2020; Prasanna et al., 2020). These often outperform tickets from randomly initialised models (Prasanna et al., 2020) and are less sensitive to specific hyper-parameter choices (Sun et al., 2020a). Magnitude pruning, which relies on zeroth-order information (the absolute value of a weight), is sub-optimal in this setting as fine-tuned weights typically stay close to their pre-trained values. Thus, magnitude pruning selects a similar set of weights for pruning regardless of the downstream task. Pruning based on first-order (gradient-based) information better captures the task-specific relevance of each weight (Molchanov et al., 2017). For instance, movement pruning (Sanh et al., 2020) learns the mask b jointly with the parameters \u03b8. As the mask is a discrete binary variable, they rely on straight-through estimators (Bengio et al., 2013). Alternatively, b can be first learned as a real-valued mask and then binarised via a thresholding function (Mallya et al., 2018).\nIn addition to pruning, sparsification techniques can be employed for adaptation. In particular, a sparse module \u03c6 can be merged with pre-trained parameters \u03b8. For instance, in Sparse Fine-Tuning (SFT; Ansell et al., 2022) the LTH is re-purposed such that, instead of zeroing out weights with the lowest change in magnitude, they are simply frozen. Thus, only a subset of weights is fine-tuned.7 The difference between these and the original pre-trained model results in a sparse module \u03c6 where \u03c6i = 0 if bi = 0, which can be plugged in and out of the model as f \u2032\u03b8 = f\u03b8\u2295\u03c6. Diff pruning (Guo et al., 2021) instead obtains a sparse adapter by fine-tuning a dense difference vector \u03c6 regularised to be sparse with a differentiable approximation to the L0-norm penalty. Sung et al. (2021) induce a fixed sparse mask by selecting the top-k weights ranked according to (a diagonal approximation of) their Fisher information. This second-order information reveals the impact of the change of a parameter on the model predictions. Thus,\nbj = { 1 if j \u2208 top-k 1n \u2211n i=1 Ey\u223cf\u03b8? (y|xi) (\u2207\u03b8 log f\u03b8?(y | xi)) 2\n0 otherwise (1)\nBeyond the sparsification of individual weights, sparse model adaptation can also be structured. In this case, only a group of model sub-functions is fine-tuned, while the rest of the parameters remain frozen. The most common setting is for such a group to correspond to a subset of layers, e.g. the last one (Donahue et al., 2014). Groups can also relate to more fine-grained parts of the model. For instance, a group consisting of a model\u2019s bias parameters is a practical choice as this removes the need to store the model\u2019s intermediate activations (Cai et al., 2020; Ben Zaken et al., 2022). At the level of parameter tensors, some methods prune filters in CNNs (Anwar et al., 2017; Newell et al., 2019) or attention heads in pre-trained Transformers (Voita et al., 2019; Michel et al., 2019). In structured diff pruning, members of a group are encouraged to share the same mask value (Guo et al., 2021).\nLow-Rank Modules Similar to sparsity, another efficient solution is for the module parameters \u03c6i to lie in a low-dimensional subspace. Li et al. (2018) show that models can be optimised in a low-dimensional, randomly oriented subspace rather than the full parameter space. In this setting, the module parameters \u03c6 \u2208 Rd are low-dimensional compared to the model parameters \u03b8 \u2208 RD and d D. A random matrix M \u2208 Rd\u00d7D can\n7This is typically implemented by masking the gradient based on the binary mask b \u2207\u03b8L(f\u03b8 ,D) where L is a loss function and D is a dataset (Ansell et al., 2022).\nbe used to project from d to D: f \u2032\u03b8 = f\u03b8+\u03c6M. An efficient way to compute M is via the Fastfood transform (Le et al., 2014), which factorises M as random linear matrices. Specifically, M = HG\u03a0HB consists of a Hadamard matrix H, a random diagonal matrix with independent standard normal entries G, a random diagonal matrix with equal probability \u00b11 entries B, and a random permutation matrix \u03a0. Li et al. (2018) refer to the minimum d that achieves within 90% of the full-parameter model performance as the intrinsic dimensionality of a given task. Aghajanyan et al. (2021) investigate the intrinsic dimensionality of various NLP tasks with different pre-trained models. They observe that it decreases during pre-training and that larger models have lower values.\nHowever, storing the random matrices results in a substantial memory overhead and is slow to train (Mahabadi et al., 2021). If the weight matrix W \u2208 Ro\u00d7i is small enough, we can directly compose it into low-rank matrices W = \u03bbBA where A \u2208 Rk\u00d7i and B \u2208 Ro\u00d7k, where i is the input dimensionality, o is the output dimensionality, k is the rank of the matrix, and \u03bb is a scaling hyper-parameter. To save space, the factorisation may be only applied to certain groups of parameters G. In LoRA (Hu et al., 2022), this group corresponds to the linear projections in the self-attention mechanisms of each Transformer layer: f \u2032j = f\u03b8j+vec(BjAj)\u2200f \u2032j \u2208 G.\nOverall, parameter composition methods (both sparse and low-rank) are very parameter-efficient and often require updating less than 0.5% of a model\u2019s parameters (Guo et al., 2021). At inference time, they keep the model size constant or even reduce it, if the resulting model is sparse. Sparse modules, however, increase the time complexity of optimisation as they typically require multiple iterations of re-training. Finally, state-of-the-art parameter composition methods, e.g., LoRA (Hu et al., 2022) and SFT (Ansell et al., 2022) achieve strong performance in zero-shot and few-shot transfer."
        },
        {
            "heading": "3.2 Input Composition",
            "text": "Input composition methods augment a function\u2019s input x by concatenating it with a parameter vector \u03c6i: f \u2032i(x) = f\u03b8i([\u03c6i,x]). The most common strategy is to augment the input fed to the model\u2019s first layer f1. In a prompting setup with auto-regressive language models (Brown et al., 2020) or encoders (Schick & Sch\u00fctze, 2021a;b), the input prompt p consists of (optional) instructions and (optional) in-context examples that have been converted to natural language. From a different perspective, the task-specific text prompt, when encoded using the model\u2019s embedding layer Emb(\u00b7), corresponds to modular parameters \u03c6 that elicit the desired behaviour (Gao et al., 2021b; Liu et al., 2023): Emb(p) = \u03c6. However, models are ostensibly sensitive to the formulation of the prompt as well as to the set and order of the (few-shot) examples (Zhao et al., 2021; Lu et al., 2022; Webson & Pavlick, 2022).\nInstead, a continuous prompt vector \u03c6 can be learned directly (Lester et al., 2021; Liu et al., 2021b; Zhong et al., 2021; Hambardzumyan et al., 2021). However, if \u03c6 is only concatenated with the first layer\u2019s input, the model has limited capacity to adapt to a specific task. As a result, such continuous (also called soft) prompts perform poorly at smaller model sizes and on some harder tasks (Mahabadi et al., 2021; Liu et al., 2022c). To mitigate this, initialisation via multi-task learning has been proposed (Vu et al., 2022c). As an alternative, module vectors \u03c6i can be learned for each layer of the model (Figure 2b; Li & Liang, 2021; Liu et al., 2022c).\nWhile this increases the number of parameters, it increases the modules\u2019 capacity to adapt to a given task. In practice, module parameters in the form of prefix vectors \u03c6i = P ik,P iv \u2208 Rl\u00d7d are prepended to the keys and values of every multi-head attention layer. Attention is defined as fi(x) = Attn(xW iq ,CW ik,CW iv) where Wq,Wk,Wv \u2208 Rd\u00d7dh are the projections that produce the queries, keys, and values, and C \u2208 Rm\u00d7d is a sequence of context vectors. Multi-layer prompt tuning thus takes the following form:\nf \u2032i(x) = Attn(xW iq , [P ik,CW ik], [P iv ,CW iv ]). (2)\nIn summary, input composition is exceptionally parameter-efficient as it only adds a very small number of parameters. However, these parameters extend a model\u2019s context window, which makes them less efficient during training and inference. Prompt tuning methods also require large models to achieve decent performance."
        },
        {
            "heading": "3.3 Function Composition",
            "text": "While parameter composition deals with individual weights and input composition methods act only on a function\u2019s input, function composition methods augment the model with new task-specific sub-functions (see Figure 2c): f \u2032i(x) = f\u03c6i \u25e6 f\u03b8i(x) = f\u03c6i(f\u03b8i(x)), where \u25e6 stands for function composition.\nParameter Sharing Models in multi-task learning traditionally consist of shared layers f\u03b8 stacked under task-specific modules f\u03c6 (Ruder, 2017). Conversely, given models for tasks t and s expressed as a composition of functions f t\u03c61 \u25e6 . . . \u25e6 f t \u03c6l and fs\u03c61 \u25e6 . . . \u25e6 f s \u03c6l , respectively, a multi-task architecture can also be obtained by tying sets of parameters between the models: f t\u03c6i = f s \u03c6i \u2200i \u2208 G where the group G contains the set of shared layer indices.8 Many multi-task neural architectures can be characterised in terms of their definition of G, which determines which modules are task-specific and which ones are shared. This is the case, for instance, of \u2018shared trunk\u2019 approaches in computer vision (Zhang et al., 2014; Ma et al., 2018) and approaches with supervision at different layers in NLP (S\u00f8gaard & Goldberg, 2016; Sanh et al., 2019; Liu et al., 2019).\nSome approaches learn finer-grained interactions between pairs of modules. Misra et al. (2016) propose the cross-stitch unit, which linearly combines the inputs at every layer9: (x\u0303t, x\u0303s) = W [xt,xs] where[\nx\u0303tij x\u0303s ij\n] = [ \u03b1tt \u03b1ts\n\u03b1st \u03b1ss ] [xtij xs ij ]\nand \u03b1 \u2208 R. Sluice networks (Ruder et al., 2019a) extend cross-stitch units to multiple modules per layer and additionally employ a soft selection of the skip connections from all layers at the output layer l:\nx\u0303t> = \u03b2t1\u00b7 \u00b7 \u00b7 \u03b2tl > [xt1> , . . . , xtl>] and \u03b2 \u2208 R. On the other hand, Gao et al. (2019) fuse features from multiple tasks through a 1x1 convolution. Bragman et al. (2019) employ variational inference to assign filters in a CNN to task-specific or shared roles.\nRather than learning which modules should be shared among which tasks, which is a combinatorially large problem, Lu et al. (2017) and Vandenhende et al. (2020) start with a fully shared model and then dynamically widen it during training, by cloning function f\u03b8i into new modules f\u03c6i,1 , . . . , f\u03c6i,k shared among a smaller subset of tasks, in top-down order across layers. More information on parameter-sharing strategies in multi-task learning can be found in relevant surveys (Ruder, 2017; Crawshaw, 2020).\nAdapter Layers As an alternative to parameter sharing, a new task-specific learnable function f\u03c6i can be composed with an (often frozen) shared function f\u03b8i . As the main purpose of such modules is adapting a pre-trained model to new tasks, they are also simply known as \u2018adapter layers\u2019. We provide examples of different adapter layers in Figure 3.\n8In this view, there is no clear differentiation between model parameters \u03b8 and module parameters \u03c6. 9We omit the layer index n to simplify the presentation.\nThe adapter\u2019s design and composition with the pre-trained model are often modality-specific. In computer vision, the adapter typically consists of a 1\u00d71 convolution, i.e., f\u03c6i(x) = F \u2217x where F is a bank of 1\u00d71 filters and \u2217 is the convolution operation (Rebuffi et al., 2017). The module is then inserted between the convolutional blocks of a pre-trained model, such as a ResNet (He et al., 2016). In NLP, a bottleneck architecture has become popular which consists of a down- and up-projection, coupled with an intermediate activation function \u03c3: f\u03c6i(x) = W d(\u03c3(W ux)) where W d \u2208 Rdx\u00d7k and WU \u2208 Rk\u00d7dx , dx is the dimensionality of the input (typically the hidden dimension), and k is the bottleneck dimension. \u03c3 is commonly a non-linearity such as a ReLU unit (Figure 3a; Houlsby et al., 2019; Pfeiffer et al., 2020b). In a Transformer model, adapters are placed both after the multi-head attention and the feed-forward layer (Houlsby et al., 2019), just after the multi-head attention (Bapna & Firat, 2019), or just after the feed-forward layer (Pfeiffer et al., 2020b).\nOther variants for \u03c3 such as the identity function, standard multi-head attention, and multi-head attention with shared projection matrices have also been explored (Stickland & Murray, 2019). Mahabadi et al. (2021) propose Compacter, a hyper-complex, low-rank adapter that reparameterises W in the adapter as: W = \u2211n i=1Ai \u2297 Bi where Ai \u2208 Rn\u00d7n is shared across layers (n is a hyper-parameter), Bi \u2208 R k n\u00d7 d n is parameterised as a low-rank matrix Bi = sit>i (r is the rank of Bi), and \u2297 is the Kronecker product.\nAdapters can be routed sequentially or in parallel. Sequential adapters, are inserted between existing functions: f \u2032i(x) = f\u03c6i(f\u03b8i(x)) (Rebuffi et al., 2017; Houlsby et al., 2019). Parallel adapters are applied in parallel to a model pretrained function: f \u2032i(x) = x+ f\u03b8i(x) + f\u03c6i(x) (Figure 3b; Rebuffi et al., 2018; Stickland & Murray, 2019; He et al., 2022a). Moreover, adapters involve two residual connections: between the output of f\u03b8i and the output of f\u03c6i , which is further added to x and normalised. Adapters have been shown to lead to increased sample efficiency, flatter minima, and more robustness to hyper-parameter choices compared to standard model fine-tuning (Karimi Mahabadi et al., 2021; He et al., 2021; Han et al., 2021).\nRescaling The output representations can also be directly transformed via element-wise multiplication with a vector of learned parameters: f \u2032i(x) = f\u03b8i(x) \u03c6. Crucially, this is equivalent to stacking the original function f\u03b8i with a linear transformation W = I\u03c6. Such task-specific rescaling is typically applied to batch\nnormalisation parameters in computer vision (Bilen & Vedaldi, 2017) and to layer normalisation parameters in NLP (Houlsby et al., 2019).\nThe adapter (IA)3 (Figure 3c; Liu et al., 2022b) multiplies learned vectors with the keys and values in self-attention blocks and the intermediate activations in position-wise feedforward networks in the Transformer. Rescaling activations favours dimensions that are important for a given task. Multiplication with a binary mask is a special case of rescaling that incorporates sparsity: Strezoski et al. (2019) multiply a task-specific random binary mask b with a function\u2019s input x at every layer.\nOverall, standard function composition methods such as adapter layers typically require more parameters as the new function depends on a model\u2019s input size and hidden size. While they do not require storing the gradients of the frozen parameters, they increase the number of operations at training and inference time. State-of-the-art function composition methods match or outperform standard fine-tuning."
        },
        {
            "heading": "3.4 Hypernetworks",
            "text": "In the above-mentioned adapters, different modules \u03c61, . . . ,\u03c6|M | correspond to disjoint sets of parameters. However, the modules may benefit from sharing information. Rather than learning \u03c6i directly, a (small) neural network W , known as a hypernetwork, can generate the module parameters instead, conditioned on an embedding \u03b1 (Ha et al., 2017; Platanios et al., 2018). Thus, \u03c6 = W\u03b1. As a result, the modules are \u2018entangled\u2019, which violates the strong definition of modularity that postulates that modules are autonomous (Goyal et al., 2021). In fact, in hypernetworks, computation and routing are inseparably intertwined. In fact, foreshadowing our discussion in \u00a7 4.2.4, the embedding \u03b1 can also be interpreted as unnormalised, learned routing scores for each task. In turn, the parameter generator weight would correspond to a set of modules stacked column-wise: W = [\u03c61, . . . ,\u03c6|M |].\nHypernetworks can also be conditioned on inputs x (Figure 2d). For instance, in conditional batch normalisation (de Vries et al., 2017), rescaling parameters are generated based on a representation of the model input obtained via an LSTM. Feature-wise linear modulation (FiLM; Perez et al., 2018) generates an element-wise affine transformation that is applied to image features, conditioned on the linguistic input of the model, for text-and-vision tasks. In self-modulation for Generative Adversarial Networks (Chen et al., 2019), the affine transformation is applied to hidden representations of the generator conditioned on the noise sample. Bertinetto et al. (2016) conditions the parameter generator on individual examples, in order to perform one-shot learning.\nHypernetworks have been used to generate a diverse set of module parameters, including classifier heads (Ponti et al., 2021), continuous prompts (He et al., 2022c), and adapter layers (\u00dcst\u00fcn et al., 2020; Ansell et al., 2021; Karimi Mahabadi et al., 2021), most commonly conditioned on task (Karimi Mahabadi et al., 2021) or language embeddings (\u00dcst\u00fcn et al., 2020; Baziotis et al., 2022). Such task or language embeddings \u03b1 can themselves be learned directly from random initialisations or fixed as the typological features of a language (\u00dcst\u00fcn et al., 2020; Ansell et al., 2021). This is a strategy to integrate side (or metadata) information about the relationship among languages. Other examples of side information, such as the example label y, can be integrated into the hypernetwork input embedding via bi-linear interaction (Chen et al., 2019).\nNevertheless, even the smallest possible module generator network is a linear projection W \u2208 Rd\u03c6\u00d7d\u03b1 . To make the hypernetwork more parameter-efficient, it can be shared across layers by conditioning it on the module position in the neural architecture, in addition to the task index (Karimi Mahabadi et al., 2021). In general, the hypernet can be conditioned on multiple (concatenated) embeddings: e.g., one corresponding to the task index and another to the language index. This allows the hypernetwork to generalise systematically to new task\u2013language combinations at inference time. In particular, the hypernet can either generate a single module from all the embeddings (Ponti et al., 2021) or separate modules (Ansell et al., 2021; \u00dcst\u00fcn et al., 2022). In turn, the embedding combination chosen for any example is a form of hard routing (cf. \u00a7 4.2.2)."
        },
        {
            "heading": "3.5 Unifying Parameter, Input, and Function Composition",
            "text": "While the above methods may seem different, they all covertly share a similar functional form. He et al. (2022a) cast LoRA (Hu et al., 2022), prefix tuning (Li & Liang, 2021), and bottleneck adapters (Houlsby\net al., 2019), representative methods of the three composition functions, into the same framework. We extend their framework to cover parameter composition, input composition, and function composition in general. Specifically, all modular computation functions can be reduced to function composition: the output of the function f\u03b8i of a model is added to a new term that depends on a learned function f\u03c6: f \u2032i(x) = f\u03b8(x)+f\u03c6i(x).\nFor function composition methods, this form is the most natural. In the case of parallel adapters, for instance, f \u2032i(x) = f\u03b8i(x) + f\u03c6i(x) where f\u03b8i(x) may be a multi-head attention module f\u03b8i(x) = MHA(C,x) = [head1, . . . ,headh]Wo, with headj = Attn(xW jq ,CW j k ,CW j v ), and f\u03c6i(x) = W d(\u03c3(W ux)). In this setting, \u03b8i and \u03c6i are independent and must only agree regarding the dimensionality of their inputs and outputs.\nFor parameter composition methods, which modify the parameters directly, the dimensionality of the module parameters \u03c6 should match exactly the original parameters \u03b8i. For instance, if we apply the module to a linear projection, then they should consist of weight matrices \u03b8i = Wi \u2208 Rdx\u00d7k and \u03c6i = Vi \u2208 Rdx\u00d7k, respectively. Because of linearity:\nf \u2032i(x) = f\u03b8i\u2295\u03c6(x) = fW+V (x) = (W + V )x = Wx+ V x = f\u03b8i(x) + f\u03c6i(x)\nFor instance, in the case of LoRA (Hu et al., 2022), V = \u03bbBiAi. In the case of sparse adapters (Ansell et al., 2022), V is a sparse matrix.\nFor input composition methods, with the form f \u2032i(x) = f\u03b8i([\u03c6i,x]), the equivalence is derived as follows. Prefix tuning (Li & Liang, 2021) generalises other continuous prompt methods by concatenating prefix vectors \u03c6i = P ik,P iv \u2208 Rl\u00d7d to the keys and values of self-attention. He et al. (2022a) show that prefix tuning can be expressed in the following way:\nf \u2032i(x) = Attn(xW iq , [P ik,CW ik], [P iv ,CW iv ]) = (1\u2212 \u03bb(x))f\u03b8i(x) + \u03bb(x) softmax(xWqP>k )Pv\nwhere \u03bb(x) is a scalar that represents the sum of normalised attention weights on the prefixes and f\u03b8i(x) is the attention module in a Transformer. If we set, f\u03c6i(x) = softmax(xWqP>k )Pv, then we obtain a function composition (1 \u2212 \u03bb(x))f\u03b8i(x) + \u03bb(x)f\u03c6i(x) that incorporates a weighted addition. On the contrary, note that for function and parameter composition, the sum is unweighted.\nOverall, despite their conceptual differences, most modular approaches are similar in their functional form and can be expressed as function composition. In practice, the way different methods are realised, however, leads to different trade-offs, which we illustrate in Table 3. Recent empirical studies (Mahabadi et al., 2021; He et al., 2022a; Liu et al., 2022b) provide further evidence for the strengths and weaknesses of different methods. For instance, prompt tuning (Vu et al., 2022a) underperforms other methods due to limited capacity while intrinsic dimensionality (Aghajanyan et al., 2021) uses a very small number of parameters but leads to a large memory footprint and poor performance. Fine-tuning only biases (Ben Zaken et al., 2022) has a small memory footprint but achieves lower performance. Finally, function composition methods such as adapter layers (Pfeiffer et al., 2021a) and compacter layers (Mahabadi et al., 2021), achieve the best performance, but add more parameters. (IA)3 (Liu et al., 2022b) mitigates this by composing a lightweight linear diagonal weight. Modular deep learning architectures, however, have many other differences beyond their choice of computation function. In the following sections, we discuss the routing, aggregation, and training settings for the modules presented so far."
        },
        {
            "heading": "4 Routing Function",
            "text": "In \u00a7 3, we described how to compose a sub-function fi with shared weights \u03b8 with a single module function with weights \u03c6. However, in a modular neural architecture, multiple modules are available from an inventory M = \u03c61, . . . ,\u03c6|M |. A decision-making process is required to determine which modules are active, conditioned on the model input or auxiliary metadata. This process is implemented through a routing function r(\u00b7) that assigns a score \u03b1i to each module from the inventory M . These scores determine which subset of modules is active, i.e. contributes to the computation. We provide an overview of different routing methods in Figure 4.\nWhen metadata such as expert knowledge about sub-tasks (or skills) involved in a task is available, r(\u00b7) can be designed as a fixed function, that is, each routing decision can be made a priori (Figure 4a). For instance,\nwhen using a language model to generate dialogue in Swahili, a task module for dialogue generation and a language module for Swahili can be selected. When no such prior information is available\u2014for instance when modelling heterogeneous unlabelled data\u2014routing of a given example needs to be learned (Figures 4b-4c). In this case, the routing function can be conditioned on the current example x.10\nUnfortunately, learned routing is crucially under-constrained, as multiple possible ways of decomposing tasks into sub-tasks are reasonable (Jacobs et al., 1991a). In addition, it presents a series of unique challenges (see \u00a7 4.2.1). In an empirical study on synthetic data, Mittal et al. (2022) found that learned routing is sub-optimal compared to fixed routing, as it tends to under-utilise modules and to specialise them to a lesser degree. This behaviour is exacerbated as the number of tasks in the data grows. In real-world applications, Muqeeth et al. (2022) report similar results; however, Ponti et al. (2022) find that learned routing may surpass expert module selection even in settings where tasks are procedurally constructed to require certain skills, such as instruction following in simulated environments.\nLearning-to-route can roughly be split into hard routing and soft routing (Rosenbaum et al., 2019). Hard routing methods learn a binary selection of modules, similarly to the fixed routing scheme, where only a subset of modules is selected for each decision-making step (Figure 4b). Inference for hard routing systems typically builds on score function estimators (Williams, 1988; 1992) or stochastic re-parameterisation (Jang et al., 2017). On the other hand, soft routing methods learn a probability distribution over modules (Figure 4c; Jacobs et al., 1991a). While soft selection is more easily amenable to end-to-end learning via gradient descent, hard selection may lead to a sparse architectural design, owing to the fact that inactive modules are not part of the forward and backward computation graph. This reduces time complexity while augmenting the model capacity (Bengio et al., 2013)."
        },
        {
            "heading": "4.1 Fixed Routing",
            "text": "Making the routing decision a priori\u2014i.e. when we utilise metadata (e.g. task identity t) to make the discrete routing decisions before training\u2014is referred to as fixed routing (Figure 4a). Here the routing function r(\u00b7) simplifies to a selection of a subset of modules K \u2286M for the examples with certain metadata:\nr(\u03c6i) = {\n1 if i \u2208 K 0 otherwise\n(3)\nThis function defines a binary matrix A \u2208 {0, 1}|T |\u00d7|M |, where the number of rows corresponds to possible tasks and the number of columns corresponds to the size of the module inventory.\n10Alternative non-parametric routing strategies include random routing (Zuo et al., 2022; Wang et al., 2022) or routing based on hash functions (Roller et al., 2021).\nOne simple example of fixed routing in multi-task learning is when all parameters, except the final classification layer, are shared among all tasks (Ruder, 2017). Independently from the task identity, the examples are passed through the same network until after the penultimate layer. The penultimate layer\u2019s representations are then routed to their respective final classification layer according to the task identity. This boils down to setting |K| = 1, with the additional constraint that tasks cannot share modules, which results in the allocation matrix being an identity matrix, A = I.\nWhile not immediately apparent, methods that adapt pre-trained models towards individual tasks (Rebuffi et al., 2017; 2018; Houlsby et al., 2019; Bapna & Firat, 2019; Li & Liang, 2021; Liu et al., 2022b; Hu et al., 2022; Ansell et al., 2022; Ben Zaken et al., 2022, inter alia)\u2013as discussed in \u00a7 3\u2013deterministically route representations through the newly introduced module f\u03c6. Given that the pre-trained weights are frozen and modules trained on different tasks can be added or removed, the components become modular even if they are developed asynchronously and independently of each other (Pfeiffer et al., 2021a). In a sense, community-based hubs of pre-trained adapters such as AdapterHub (Pfeiffer et al., 2020a) can be considered as ever-evolving multi-task models, the development of whose components has been distributed throughout the community.11 Moreover, since newly introduced weights are encapsulated between frozen (shared) weights, adapted representations of intermediate layers are implicitly aligned as they are passed as input to the same frozen components.\nHampshire & Waibel (1992) were possibly among the first to train independent experts for a series of sub-tasks known a priori. In this case, the (fixed-size) subset of experts K associated with each task t is assumed as given, resulting in the rows of A being k-way vectors. In cross-lingual transfer, any problem can be decomposed into a task and language variety. Fixed routing can select separate language and task components, and facilitate generalisation to new, unobserved combinations of tasks and languages at inference time (Pfeiffer et al., 2020b; Ponti et al., 2021; \u00dcst\u00fcn et al., 2022). In this case, |K| = 2. Similarly, in reinforcement learning, Heess et al. (2016) and Devin et al. (2017) design a modular policy that is composed of a robot-specific module and a task-specific module, which are instantiated as separate neural networks. Composing these modules enables generalisation to unseen robot\u2013task combinations.\nBeyond task identity, routing can be performed based on other metadata such as language, domain, or modality information. Pfeiffer et al. (2022b) add adapters for each language to a multilingual language model during pre-training on unlabelled text. Fan et al. (2021) route deterministically for multilingual machine translation according to the language family: as a consequence, all languages in a family share the same expert. In a similar vein, Gururangan et al. (2022) add domain-specific adapters to language models, deterministically routing based on the text source domain. This concept was further extended by Li et al. (2022b), who proposed the branch\u2013train\u2013merge method: copies of the same model are trained on different domains and then averaged. Finally, modality can also inform fixed routing, such as in vision-and-language models (Pfeiffer et al., 2022a). This allows for adapting the encoders of different modality streams."
        },
        {
            "heading": "4.2 Learned Routing",
            "text": "When the routing function r(\u00b7) is not known in advance, it can be implemented as a learnable neural network with parameters \u03c1. In input, it receives the example representation x or metadata such as the task t. In output, it returns routing scores \u03b1. Usually, r\u03c1 is a linear projection or a Multi-Layer Perceptron. While the former represents a less expressive family of functions, the latter may collapse into ignoring the input features. Note that learning the routing function also implies that the specialisation of each module is unknown. Thus, modules are not trained on different sets of examples; rather, they are all trained jointly with the routing function."
        },
        {
            "heading": "4.2.1 Challenges of Learned Routing",
            "text": "Learned routing introduces a number of challenges, including training instability, module collapse (Kirsch et al., 2018), and overfitting. These were first systematically described by Rosenbaum et al. (2019), and we\n11Alternatively, combining entire models stored in model repositories via distillation (Khanuja et al., 2021) or averaging (Matena & Raffel, 2021) can also help avoid negative interference (Don-Yehiya et al., 2022); however, this is usually less efficient and subject to limitations such as those discussed later in \u00a7 5.\nfollow a similar taxonomy. In general, they identify two root causes for all these challenges: first, the need to balance between exploration and exploitation (Sutton, 1986). More specifically, routing must find the optimal trade-off between allocating information to the most suitable modules versus under-explored modules. Second, routing must share modules across examples or tasks in such a way as to reap the benefits of positive transfer while avoiding negative interference. We elaborate on the individual challenges below.\nTraining Instability emerges especially in the early phases of training; at this point, modules are randomly initialised and have no clear functional specialisation. Thus, the router cannot make any principled decision in selecting modules. On the other hand, modules do not start specialising until they are consistently routed to different subsets of tasks or examples.\nCurriculum learning can mitigate this challenge to some extent (Chang et al., 2019), as simpler tasks require simpler sets of skills. However, this assumes that information about task complexity is available and that the data can be ordered accordingly. As an alternative, the router parameters can be trained with a different learning rate than the module parameters, either lower (Rosenbaum et al., 2018) or higher (Ponti et al., 2022). These create two different dynamics: either the necessary skills for a task are determined after specialisation, or the relationship among tasks is figured out first and modules are updated accordingly.\nModule Collapse describes scenarios where only a small number of modules (in the extreme case, one) from the available inventory are selected. This leaves the remaining modules untrained and negatively impacts their overall diversity. Often, this results from excessively favouring exploitation over exploration, which leads to sub-optimal results. To amend this, Ahn et al. (2019) use -greedy routing for initial exploration of all modules and afterwards switch to learned routing. Other strategies to avoid module collapse include auxiliary losses for load balancing (Shazeer et al., 2017; Fedus et al., 2021) and intrinsic rewards that encourage diversity in module selection (Cases et al., 2019). The choice of information that conditions the router also plays an important role: metadata, e.g. text genre (Cases et al., 2019) or task identity (Kudugunta et al., 2021), make routing more robust than individual examples. The diversity of training tasks also facilitates diversity in routing selections (Chang et al., 2019; Caccia et al., 2022). Dua et al. (2022) warms up the sampling temperature over training, in order to over-sample domains with fewer examples in unbalanced distributions.\nOverfitting to noise is a risk faced by deep modular networks due to their ability to model subsets of examples independently (Rosenbaum et al., 2019). For instance, routing at the token level was shown to lead to performance drops in out-of-domain generalisation for MoEs (Artetxe et al., 2022). For a similar reason, gains in pre-training do not always translate into gains in fine-tuning for MoEs (Fedus et al., 2021). Increased robustness can be achieved by routing conditioned on metadata if available (Chang et al., 2019; Cases et al., 2019; Kudugunta et al., 2021). In addition, strategies that favour the combinatorial behaviour of modules yield superior generalisation (Chang et al., 2019; Ponti et al., 2022)."
        },
        {
            "heading": "4.2.2 Hard Learned Routing",
            "text": "A model may learn how to select modules through hard routing. This implies that the choice of whether a module is active or excluded from the computation graph is binary. Discrete decisions are not amenable to be learned through vanilla gradient descent: since small perturbations of parameters do not affect the selection of modules, the gradient of the loss with respect to the routing parameters is zero. Thus, various methods, including reinforcement learning, evolutionary algorithms, and stochastic re-parameterisation, have been proposed for inference. These are discussed separately below.\nOn the other hand, hard routing is more efficient than soft routing in terms of time and space complexity. In addition, binary selection implies that parameter updates are localised to a subset of modules. This reflects the intuition that the shifts in distribution of the variables in an environment are similarly local (Parascandolo et al., 2018; Goyal et al., 2021). Since the inactive module parameters are not affected, they remain invariant with respect to the distribution shift. On top of this, this type of routing may result in variable-size sets of active modules. This allocates model capacity according to task complexity, which follows the principle of conditional computation (Bengio et al., 2015). In fact, it is fair to assume that the skills required for complex tasks are a superset of those of simpler tasks. For instance, dialogue modelling requires (among others) intent detection, slot filling, and conditional response generation.\nReinforcement Learning In Routing Networks (Rosenbaum et al., 2018), Modular Networks (Kirsch et al., 2018), and the Compositional Recursive Learner (CRL; Chang et al., 2019), a router network is trained through reinforcement learning. Specifically, Routing Networks rely on multi-agent RL (MARL), Modular Networks rely on the score function estimator (REINFORCE), whereas the CRL relies on Proximal Policy Optimisation (PPO). Commonly, this family of methods alternate between a score function estimator for the routing parameters \u03c1 and SGD for module parameters {\u03c61, . . . ,\u03c6|M |}. For a vanilla score function estimator, where routing is conditioned on the input example and m \u2208M , the update takes the form:\n\u2207\u03c1 Ex,y p(y | x,\u03b8,\u03c61, . . . ,\u03c6|M |,\u03c1) \u2248 1 n n\u2211 i=0 [ p(yi | xi,\u03b8,\u03c6m)\u2207\u03c1 log p(m | xi)] (4)\nUnder this lens, routing becomes a policy \u03c0(m | x). If applied layer-wise, each hidden representation at a given layer 1 \u2265 t \u2264 l constitutes a state ht \u2208 H. The routing policy determines the action, i.e. the selection of a module index m. In particular, this assumes that the inventory M is shared across layers.12 In turn, applying the transformation of the corresponding module on the input is equivalent to a transition function \u03c0 : H \u2192 H, which returns the next layer\u2019s hidden state ht+1. The loss function at the top layer corresponds to a (delayed) negative reward, i.e. L(\u00b7) = \u2212R.13 Crucially, in this setting the transition functions are non-stationary, as the module parameters are amenable to change. Because modules are applied sequentially based on the policy, the number of steps of computation in the model can vary when a special halting action is available.\nEvolutionary Algorithms Alternatively, routing can be learned via a genetic algorithm. In PathNet (Fernando et al., 2017), the loss function indicates the fitness of a configuration of active modules K \u2286M . For each task, two configurations are selected at random and trained until a stopping criterion is met. The one incurring the lower loss on a validation set overwrites the other. This copy, in turn, receives a random mutation, and then the procedure is repeated. In \u00b5Net (Gesmundo & Dean, 2022a;b), mutations involve cloning, insertion, and removal of layers. The fitness criteria include not only performance but also parameter efficiency. This approach has been extended to a multi-task setting where multiple agents update different modules asynchronously (Gesmundo, 2022). However, as is common for evolutionary algorithms, this search is brute-force and thus highly inefficient.\nStochastic Re-parametrisation Hard routing can also be performed via a continuous relaxation of the discrete latent variable \u03b1 determining the module allocation. Several stochastic re-parameterisations such as Gumbel-Softmax (Jang et al., 2017) or the Concrete distribution (Maddison et al., 2017) have been proposed for this purpose. Compared to the score function estimator, stochastic re-parameterisations are biased but have lower variance. Moreover, they are differentiable, which makes a hard router trainable in an end-to-end fashion. For instance, AdaShare (Sun et al., 2020b) uses Gumbel-Sigmoid to learn a binary vector for each task that indicates whether a model layer should be included in the forward pass or skipped entirely. This may be interpreted as choosing between a parameterised module and an identity function at each layer.\nStochastic re-parameterisation also allows for selecting module subsets of varying sizes for each layer. In Neural Interpreters (Rahaman et al., 2021), this is based on a threshold. Each module is associated with a \u2018signature vector\u2019. The dot product between this vector and the output of an unnormalised routing function (\u2018type inference\u2019) conditioned on a token determines a score. If this surpasses a certain threshold, then the module is allowed to access the given token. As an alternative, variable-size module routing can be achieved by learning a soft clustering (a.k.a. soft partition) of modules (Ponti et al., 2022; Caccia et al., 2022). Thus, each entry \u03b1ij , which represents the routing of the j-th module to the i-th task, is constructed as follows:\n\u03b1i,j = sigmoid [\nlog sigmoid(\u03b1\u0302i,j)u(1\u2212 sigmoid(\u03b1\u0302i,j)) (1\u2212 u)\n1/\u03c4 ]\nu \u223c Uniform(0, 1). (5)\n12This encourages module re-usage at different layers. 13Intrinsic rewards can be added, for instance favouring diversity in the module selection across time steps (Rosenbaum et al.,\n2018).\nwhere \u03b1\u0302ij represents the unnormalised routing score. This latent variable also admits priors such as the Indian Buffet Process (Griffiths & Ghahramani, 2011) to encourage both diversification and sharing of module subsets across tasks (Ponti et al., 2022). Caccia et al. (2022) extend this framework to multi-head routing, where different modules can be allocated to contiguous subsets of dimensions of the layer\u2019s input and output. While this just requires as many copies of \u03b1 as the number of subsets of dimension, it provides higher expressivity to the routing function.\nTop-k Selection Finally, hard selection can rely on top-k selection from (possibly unnormalised) scores \u03b1 over modules. In the case of Independent Causal Mechanisms (Parascandolo et al., 2018), \u03b1 is given by a discriminator that scores the outputs of a generator, and k = 1. In the case of Recurrent Independent Mechanisms (Goyal et al., 2021), the scores are derived from attention between modules and the input, and k > 1. These methods are grounded on the assumption that the competition among modules to be activated facilitates their specialisation (see \u00a7 8.3 for more details)."
        },
        {
            "heading": "4.2.3 Soft Learned Routing",
            "text": "Mixture of Experts To sidestep discrete selections of modules, several works propose soft routing methods, where all modules are selected and aggregated according to a weighted combination, i.e. a mixture of experts (MoE; Jacobs et al., 1991b; Jordan & Jacobs, 1994).14 Here, the router learns a probability distribution over the available modules, i.e. p(M) = r\u03c1(\u00b7). Hence, routing and aggregation take place as:\nf \u2032i(x) = \u2211 \u03c6j\u2208M r(\u03c6j) f(x;\u03b8i,\u03c6j) (6)\nIn contrast to the discrete selection of hard routing methods, this setup is easily trained end-to-end via gradient descent. A number of works (Eigen et al., 2014; Meyerson & Miikkulainen, 2018; Wortsman et al., 2020, inter alia) train a continuous weighting (i.e. a mixture) of all modules; however, this limits the degree of modularity as parameter updates are not local; instead, they always affect all modules. Additionally, activating all modules for each example significantly increases the computational cost for each forward and backward pass through the network. To circumvent this, Shazeer et al. (2017) and Lepikhin et al. (2021) only route to the top-k of |M | modules, where 1 < k < |M |. The output representations of the k active modules are averaged according to the respective routing weights, whose sum is re-normalised to 1. Thus, top-k MoEs stand between hard routing, as only a subset of modules is active, and soft routing, as their average is weighted by the routing scores. In practice, a layer performs the following computation:\nf \u2032i(x) = \u2211\n\u03c6j\u2208 topk[r(\u03c6)]\nr(\u03c6j)\u2211k 1 r(\u03c6) f(x;\u03b8i,\u03c6j) (7)\nFedus et al. (2021) and Clark et al. (2022) demonstrate that even top-1 routing can achieve competitive results for language modelling.\nToken-Level Routing MoEs have recently undergone a revival as part of the efforts to scale Transformers. In particular, MoE Transformers route to a subset of Feed-Forward Network (FFN) modules per layer instead of a single FFN. The focus of these works is on computationally efficient training of very large models. This is achieved by splitting the input tokens across different (hardware) accelerators. The MoE routing algorithm is therefore required to (ideally) uniformly distribute the tokens of all the examples in an input batch across all accelerators, i.e. to load balance computation across \u201cexperts\u201d. The dominating routing strategy is for each token to choose the top-k experts (Shazeer et al., 2017; Lepikhin et al., 2021; Fedus et al., 2021; Clark et al., 2022; Yang et al., 2021; Dua et al., 2022; Hazimeh et al., 2021; Rajbhandari et al., 2022; Riquelme et al., 2021; Du et al., 2022; Zoph et al., 2022). Alternative approaches let each expert choose the top-k tokens (You et al., 2022; Zhou et al., 2022b) or globally determine the best routing path (Lewis et al., 2021).15\n14In the following sections we use the term \u201cexpert\u201d and \u201cmodule\u201d interchangeably to reflect common practice in the body of research on MoEs.\n15For more details on load balancing methods we refer to Fedus et al. (2022), Chapter 4.\nHowever, since routing is conditioned on the token level, and the load balancing restriction limits the system from routing an entire example to a single module, the system potentially has to relearn similar concepts in multiple modules. Hence, load balancing hinders the router from selecting the single best module for longer (e.g., repetitive) sequences. This is investigated further by Lewis et al. (2021), who find that sparse models route syntactically and semantically similar words (in contrast to sentences or phrases) to the same modules. This sheds light on the limited expressiveness of modules which are learned on the token-level. Since scaling is the main focus of these works, their goals are orthogonal to modular approaches centred on parameter efficiency, transfer\u2013interference trade-offs, and combinatorial generalisation.\nExample-Level Routing Nevertheless, one could imagine obtaining the best of both worlds by hybridising sparse MoE Transformers models with deterministic or learned routing strategies from \u00a7 4.1 and \u00a7 4.2.2. Instead of routing each individual token separately, all tokens of a single example can be routed to the same experts. Kudugunta et al. (2021) experiment with two versions of example-level routing for machine translation: In sentence-level routing, they average pool over the token embeddings, and condition the router on the resulting representation. In task-level routing, a task embedding is trained, based on which the router learns the distribution over modules. In a similar vein, Gupta et al. (2022) and Xi et al. (2022) implement task-level routing across modular experts to improve the amount of knowledge sharing during multi-task learning in NLP and computer vision, respectively.\nSince task identity (or other metadata) is not always given, especially in continual learning, it can be inferred through an auxiliary model. Van de Ven & Tolias (2019) refer to this scenario as \u2018class-incremental learning\u2019. For instance, the current task can be identified based on the lowest predictive uncertainty or an auxiliary task classifier (von Oswald et al., 2020). In these cases, routing can depend on the predicted task identity."
        },
        {
            "heading": "4.2.4 Hypernetworks",
            "text": "In addition to hard and soft routing, hypernetworks (Ha et al., 2017), as introduced in \u00a7 3.4, can be considered a third kind of routing, with unnormalised routing scores. More formally, the parameters \u03b8t \u2208 Rd for a task t can be generated by a linear function \u03a6\u03b1t. The task embedding \u03b1t \u2208 R|M | can be interpreted as the output of a task-level routing function with unnormalised scores over |M | modules. In turn, the generator \u03a6 \u2208 Rd\u00d7|M | can be considered a matrix of module parameters stacked column-wise, where each module has d parameters. Thus, the generated parameters \u03b8t is a linear combination of the columns of the linear generator. This is also reminiscent of tensor factorisation models where parameters are factorised into shared tensors and task-specific tensors (Yang & Hospedales, 2017), which in hypernetworks correspond to the generator and the task embedding, respectively. However, hypernetworks learn both sets of parameters jointly rather than obtaining them from a factorisation of task-specific networks a posteriori."
        },
        {
            "heading": "4.3 Level of Routing",
            "text": "Another aspect of designing a routing function is its level of granularity. Routing can select modules globally for the entire network, make different allocation decisions per layer, or even hierarchically select sub-routers. This last method is also referred to as \u2018dispatched routing\u2019 by Rosenbaum et al. (2018). A naive version of global routing (Figure 5b) assumes that a single routing configuration is shared across layers. Allowing for different decisions per layer (Figure 5a) is more challenging as the space of potential architectures grows exponentially as |M |l, where l is the number of layers or sub-functions of the network. In fact, to compute the posterior over parameters, one would need to marginalise over every possible configuration of A = [\u03b11, . . . , \u03b1l]. Kirsch et al. (2018) resort to Expectation Maximisation to make it tractable. Instead, per-layer routing (Figure 5c) assumes conditional independence among decisions, thus facilitating scaling. Crucially, routing scores are sometimes employed not only to select a subset of modules but also to aggregate their outputs. This second purpose is addressed in more depth in \u00a7 5.\nMost methods assume that routing decisions occur in a sequence, whose length is bounded or unbounded. This is the case where the output of every layer is fed into the next. However, routing may also involve defining both the selection of modules and their order of composition (i.e., the model architecture). For instance, in Neural Module Networks (NMNs; Andreas et al., 2016b; 2017), the routing function consists of a parser that takes in a query and produces a dependency tree. This is post-processed and transformed into a tree graph where nodes are modules and directed edges control the flow of the information, i.e. route the output(s) of a subset of modules as input to another module. in Modular Meta Learning, Alet et al. (2018) alternate between sampling compositional graphs using simulated annealing (Kirkpatrick et al., 1983) and performing a step of gradient descent on the network parameters for a set of meta-training tasks."
        },
        {
            "heading": "5 Aggregation Function",
            "text": "While in the previous section on routing we have covered the topic of how to select different modules during training, we will now focus on how we can aggregate these functions in order to combine the respective information. It is important to emphasise that, for the majority of current approaches, routing and aggregation are inseparable; that is, the selection and aggregation of modules are performed simultaneously. On the other hand, the strategies for aggregating functions in this section are reminiscent of the taxonomy previously discussed for computation functions (see \u00a73); while in the latter we looked into the composition of shared components with modules, in this section we provide insights into the composition of multiple modules. This is often required when modules are recombined for zero-shot transfer or task-level generalisation (for more details on these applications, see \u00a7 7).\nIn particular, for a subset of active modules K \u2286Mi the aggregation of modular components can (similarly) be realised on the parameter level f \u2032i(x) = f\u03c61\u2295\u00b7\u00b7\u00b7\u2295\u03c6|K|(x), input level f \u2032i(x) = f\u03b8i([\u03c61, . . . ,\u03c6|K|,x]), as well as function level f \u2032i(x) = f\u03c61 \u25e6 ... \u25e6 f\u03c6|K|(x). In addition, we cover output level aggregation f \u2032i(x) = f\u03b8i(x)\u2295 f\u03c61(x)\u2295 \u00b7 \u00b7 \u00b7 \u2295 f\u03c6|K|(x). Crucially, this differs from parameter aggregation if f is non-linear. We discuss these different strategies in the following sections."
        },
        {
            "heading": "5.1 Parameter Aggregation",
            "text": "Mode Connectivity A natural strategy to aggregate information from multiple modules is interpolating their weights. However, given that neural architectures differ, and that hidden representations might not necessarily be equivalent (e.g. under invariance to invertible linear transformations) even if the model architectures are the same (Kornblith et al., 2019), naively aggregating module weights may have catastrophic consequences. However, recent work on linear mode connectivity (Frankle et al., 2020) suggests that under certain conditions, it is in fact possible to interpolate between multiple models, which has positive ramifications for modular aggregation methods. To understand these conditions, we first provide a brief introduction to the constraints under which parameter aggregation is permissible.\nThe phenomenon where the minima found by two networks are connected by a path of non-increasing error, has been the subject of research for many years (Freeman & Bruna, 2017; Draxler et al., 2018; Garipov\net al., 2018; Nagarajan & Kolter, 2019). However, most works demonstrate that mode paths are in fact not linear. While Nagarajan & Kolter (2019) find linear paths between networks, their experimental setup requires initialising models with the same set of weights. Frankle et al. (2020) and Neyshabur et al. (2020) demonstrate that this linear mode connectivity phenomenon is closely linked to the Lottery Ticket Hypothesis (Frankle & Carbin, 2019), which suggests that only a small subset of randomly initialised weights are the main drivers for the final performance of a model\u2014the so-called winning tickets (see \u00a7 3.1). When interpolating between models trained on different tasks but initialised with the same set of weights, the models tend to stay in the same loss basin, indicated by the lack of a sudden increase in loss when interpolating the weights. Consequently, it appears that the flatness of the basin of the loss landscape translates to better generalisation capabilities of a model. On the other hand, Ainsworth et al. (2022) argue that the success of such interpolation is strongly connected to the inherent bias of the optimiser being used, and not the neural network architecture itself.\nWeight Interpolation Building on the findings of interpolating the weights of models, Ansell et al. (2022) propose Lottery Ticket Sparse Fine-Tuning (LT-SFT), described in \u00a7 3.1. In particular, they identify language-, and task-specific sub-networks \u03c6l and \u03c6t. These can be aggregated by simply adding them to the base model, i.e. \u03b8\u2032 = \u03b80 + \u03c6l + \u03c6t. Instead of identifying task adaptations on subsets of model parameters, Ilharco et al. (2022) propose to edit entire models with further arithmetic operations. For example, tasks can include toxic language generation and general language modelling. By performing the arithmetic negation operation \u03b8\u2032 = \u03b80 + (\u03c6general \u2212 \u03c6toxic), their new model f\u03b8\u2032(x) generates less toxic text. This idea was influenced by the word analogy task (i.e., \u2018word arithmetics\u2019) (Mikolov et al., 2013).16\nRather than sparse adapters, Asai et al. (2022) aggregate parameters of soft prompts learned via prefix tuning (\u00a7 3.2). In order to generalise to new tasks, (frozen) modules from past tasks and a learnable module created for the new task are interpolated according to the weights of an attention mechanism between the modules and the input."
        },
        {
            "heading": "5.2 Representation Aggregation",
            "text": "Closely related to parameter aggregation, representation aggregation consists of interpolating the outputs of individual modules. Crucially, both operations are equivalent if the functions are linear: (\u03b1i\u03a6i + \u03b1j\u03a6j)x = \u03b1i\u03a6ix+ \u03b1j\u03a6jx. However, this does not hold true for non-linear functions, e.g. if the module is an adapter layer (Houlsby et al., 2019) or a feed-forward component of a Transformer layer (Fedus et al., 2021).\nWeighted Representation Averaging At the i-th sub-function of the model, where multiple modules \u03c6 \u2208 Mi exist, the representations are passed through the (active) modules, outputting |Ki| (latent) representations h1, . . . ,h|Ki|. One way of performing aggregation is to learn the weights \u03b1 to interpolate over the hidden representations:\nf \u2032i(x) = |Ki|\u2211 j \u03b1jhj (8)\nwith \u03b1j being a module-specific scalar weighting.\nThis aggregation is equivalent to Equation (6) when interpreting each weight \u03b1j \u2208 [0, 1] as the output of a soft router, i.e. \u03b1j = r(\u03c6j). Consequently, all soft-learned routing approaches (e.g. MoE) that do not perform top-1 routing (see \u00a7 4.2.3) also determine how to aggregate the representations of different modules.\nAs an extension to the traditional MoE aggregation/routing function, Ma et al. (2018) propose to learn one aggregation function per task t in a multi-task setup. Gururangan et al. (2022) pre-train modular components for different textual domains d \u2208 D. When utilising the pre-trained modules on unseen data, they weight the output representations hd of the respective domain modules \u03c6d according to the posterior distribution over the input examples, i.e. \u03b1 = p(D | x):\n16vec(\u2018King\u2019)\u2212 vec(\u2018Man\u2019) + vec(\u2018Woman\u2019) \u2248 vec(\u2018Queen\u2019), with vec(\u00b7) denoting word embeddings of the respective words.\nf \u2032i(x) = \u2211 d\u2208D p(d | x) f\u03c6d(x) (9)\nThis posterior is inferred through the Bayes rule. This does not require any auxiliary model, and only relies on the original d-conditioned language model. In fixed routing, module representations are often averaged without weighting (Zhang et al., 2022a; Chronopoulou et al., 2022a). Similarly, in hard routing methods, the representations of all active modules are averaged, such as in Polytropon (Ponti et al., 2022), or summed, as in PathNet (Fernando et al., 2017).17\nOne disadvantage of simply learning gating parameters is that the weights do not depend on the hidden representations. Thus, they do not take into account their information content. This issue is tackled by attention-based aggregation functions.\nAttention-Based Representation Aggregation Instead of inferring the weighting before a module has performed its transformation on the latent representation, the aggregation decision can take place afterwards. This allows for identifying whether or not the information added by the respective module is ancillary to the target task. In AdapterFusion, Pfeiffer et al. (2021a) propose an attention mechanism (Bahdanau et al., 2015) between the stacked hidden representations Hi produced by the modules and their input x:\nfi(x) = Attn(xQi,HiKi,HiVi) (10)\nwhere Q,K,V \u2208 Rd\u00d7h are the projections that produce the queries, keys, and values, and x is the input representation to each of the modules (i.e., the output representation of the previous layer). Hi \u2208 R|M |\u00d7d is a matrix consisting of row-wise stacking of the output representations h1, . . . ,h|Mi| of each module. In other words, the input of each module is interpreted as the query and the output of each module is interpreted as the value and key. The attention mechanism thus learns to attend over the module representations and weigh them according to their relevance for the current task.\nInstead of aggregating module outputs into a single representation, Recurrent Independent Mechanisms (Goyal et al., 2021) concatenate the outputs of the top-k active modules. However, in between the application of recurrent computation functions, they exploit an attention mechanism over hidden representations to enable sparse communication among modules.\nOne major disadvantage of both weighted and attention-based representation averaging, is that\u2014when used in combination with soft routing\u2014they require a full forward pass through all modules, even if they contribute only marginally to the final aggregation. Thus, they incur significant increases in time and space complexity. While this can be mitigated by pruning (i.e., dropping) some modules during inference (R\u00fcckl\u00e9 et al., 2021), latency still remains an issue for scalability. Thus, top-k hard routing offers a more efficient solution for both weighted averaging (Shazeer et al., 2017; Lepikhin et al., 2021) and attention-based aggregation (Goyal et al., 2021)."
        },
        {
            "heading": "5.3 Input Aggregation",
            "text": "Input aggregation lends itself naturally to adapters such as prompts or prefix tuning (Brown et al., 2020; Lester et al., 2021; Li & Liang, 2021, see \u00a7 3.2). In prompting, we have a set of instructions or few-shot examples \u03c61, . . . ,\u03c6|K|. Given that the nature of prompting is to prepend the prompts to the input, aggregating the respective modules boils down to concatenating all prompts. That is, providing the model with multiple instructions, or with multiple examples (i.e. few-shot in-context learning) is a version of module input aggregation f \u20321(x) = f\u03b81([\u03c61, . . . ,\u03c6|K|,x]). This concept also extends to prefix-tuning, where we can simply concatenate all prefixes at every layer: f \u2032i(x) = f\u03b8i([\u03c61i , . . . ,\u03c6 |K| i ,x]).\nIn the context of prompting, Schick et al. (2021) leverage input aggregation by concatenating multiple textual descriptions of undesired behaviours of a language model to generate toxic text for model debiasing. In\n17Note that the latter strategy leads to high variance in the norms of hidden representations if the router can select variable-size subsets of modules.\nthe context of prompt tuning, Vu et al. (2022b) learn separate task and language soft prompts that are recombined for zero-shot cross-lingual transfer in summarization. Nayak et al. (2022) compose soft prompts of attributes and objects in visual tasks to generalise to new classes. Note, however, that soft prompts can be aggregated with methods different from concatenation, such as attention-based parameter interpolation (Asai et al., 2022).\nHypernetworks Similarly to soft prompts, hypernetworks may aggregate information from different embeddings by combining them in the input to the parameter generator. For instance, in (Ponti et al., 2021) task and language embeddings are concatenated in the input when training a multilingual multitask architecture where the encoder is fully shared and the hypernetwork generates the classifier head. By recombining embeddings appropriately, this method allows for inferring the parameters of unseen task\u2013language combinations. Similar combinations of embeddings have been used to generate adapters in multilingual (\u00dcst\u00fcn et al., 2020) and multi-task settings (Karimi Mahabadi et al., 2021; Pilault et al., 2021). Embeddings may also represent the position of the generated parameters in the neural architecture (Ansell et al., 2021; \u00dcst\u00fcn et al., 2022)."
        },
        {
            "heading": "5.4 Function Aggregation",
            "text": "Finally, aggregation can be achieved on the function level; f \u2032i(x) = f\u03c61 \u25e6 f\u03c62(x). Different aggregation methods infer either a sequence or a (tree) structure that determines the order of the aggregation.\nSequential Aggregation By performing a forward pass through multiple modules, where the input to the next module is the output of the previous one, the respective hidden representations are sequentially transformed: f \u2032i(x) = f\u03c61(f\u03c62(. . . (f\u03c6|M|(x)))).\nThis form of information aggregation is often chosen in conjunction with fixed routing, as discussed in \u00a7 4.1, given that the routing order is determined by the role of each module (e.g. language and task adapters). Pfeiffer et al. (2020b; 2021b) propose a two-stage setup where language-specific components are disentangled from task-specific components, in order to perform zero-shot cross-lingual transfer. First, language (adapter) modules f\u03c6ls and f\u03c6lt are trained on monolingual unlabelled data for the source language s and the target language t, respectively. Then, in the second stage, the language component f\u03c6ls is inserted but frozen, and a new (adapter) module is added for a task f\u03c6t and trained on annotated data for the source language: f\u03c6t(f\u03c6ls (x)). Since this effectively disentangles language from task information, this also enables zero-shot inference on the target language t without annotated data. In particular, f\u03c6ls is substituted with f\u03c6lt , thereby hierarchically aggregating the information from the respective modular components: f\u03c6t(f\u03c6lt (x)). Similarly, Stickland et al. (2021) performs function composition of a language module f\u03c6l and a domain module f\u03c6d for multilingual multi-domain machine translation. For more examples, see \u00a7 7.1.\nHierarchical Aggregation Alternatively, when global routers jointly determine the selection of modules and the model architecture, the order of function composition follows the structure of a tree. For instance, Neural Module Networks (Andreas et al., 2016b) leverage a semantic parse to infer a graphical structure for module aggregation. While all leaf nodes find objects by identifying regions of an image through attention, intermediate nodes either transform or combine these representations (depending on the arity of the node). The root then predicts the label by describing or measuring the attended objects."
        },
        {
            "heading": "6 Training Setting",
            "text": "Finally, we explore the training settings for modular architectures. We can identify three main strategies in the literature: 1) all modules are jointly trained for multi-task learning; 2) modules are introduced at different stages during continual learning; and 3) in transfer learning, modules are added post-hoc after pre-training, often as a way to fine-tune the model in a parameter-efficient fashion. However, it is important to note that these strategies do not necessarily rule out each other, as they can all be realised in combination."
        },
        {
            "heading": "6.1 Joint Multitask Learning",
            "text": "In joint multi-task learning, there are two main settings. Firstly, task-specific parameterised components can be integrated into shared neural network architectures as a means to mitigate catastrophic forgetting or negative interference (McCloskey & Cohen, 1989; French, 1999) and as a way to scale the model size efficiently (Kudugunta et al., 2021). In these scenarios, modules are often optimised on individual tasks via fixed routing and specialise accordingly (Hampshire & Waibel, 1992; Rajendran et al., 2017, inter alia; see \u00a7 4.1 for more details). As an alternative, the architecture can be fully modular, sharing only the parameters for learned routing (Jacobs et al., 1991b;a; Rosenbaum et al., 2018; Kirsch et al., 2018; Chang et al., 2019, inter alia; see \u00a7 4.2.3 for more details).\nJoint training can also be performed before post-hoc training: a shared base model can be pre-trained on multiple tasks as a warm-up before creating task-specific sparse subnetworks (Sun et al., 2020a) or as a way to provide a useful initialisation for modular parameters (Vu et al., 2022c). Dua et al. (2022) converts a dense language model pre-trained on text data into an MoE by decomposing the learned feed-forward layers. Pfeiffer et al. (2022b) add language-specific layers during multilingual pre-training of a language model. This prepares the model to be extended to more languages post-hoc; when new languages become available, a new (randomly initialised) learnable layer can be added to the inventory of modules, whereas the shared parameters remain untouched."
        },
        {
            "heading": "6.2 Continual Learning",
            "text": "In a similar vein to countering negative interference in multi-task learning, continual learning\u2014that is, continuously integrating new data into the model\u2014often aims at mitigating catastrophic forgetting (i.e., the knowledge learned at early stages of training should not get overwritten by updates to the model later on).\nSimilar to the multi-task learning approaches discussed in \u00a7 6.1, new layers can be continuously introduced within the network which are only updated on the new data, keeping the others untouched. In methods like Progressive Networks (Rusu et al., 2016), PathNet (Fernando et al., 2017), and PackNet (Mallya & Lazebnik, 2018) when the model is trained on a new task, the parameters of the previous tasks are frozen; however, for new tasks, new modules may be learned, which connect to the existing set of modules. Often, the decision of inserting new modules at a given stage is made dynamically based on outlier detection (Ostapenko et al., 2021). Progressive Networks (Rusu et al., 2016), on the other hand, scale the model capacity linearly with the number of tasks. Aljundi et al. (2017) train separate experts for every task and route new examples based on the distribution of the reconstruction errors of task-specific auto-encoders.\nInstead of adding new parameters to the model, other works in the continual learning landscape identify subnetworks for different tasks. For instance, some works identify subnetworks of the model, which have not been used by previous tasks. Consequently, updating these parts of the model will have little effect on the previously learned knowledge (Javaloy & Valera, 2022). Similarly, \u2018supermasks\u2019 (\u00a73.1; Wortsman et al., 2020), which learn a binary mask over a randomly initialised model, enable the extension to a potentially vast number of tasks during continual learning. Supermasks of previous tasks can be also linearly combined as a way to generalise to new tasks."
        },
        {
            "heading": "6.3 Parameter-efficient Transfer Learning",
            "text": "Recently, transfer learning has become the dominating strategy for state-of-the-art results on most tasks. Auxiliary self-supervised objectives are utilised to pre-train models on a large amount of data. Subsequently, the model\u2019s weights are fine-tuned on the target tasks (Howard & Ruder, 2018; Devlin et al., 2019). Updating a small set of parameters of these large models has been demonstrated to perform equally well as full model fine-tuning, leading to the emergence of parameter-efficient fine-tuning strategies.\nMost methods discussed in \u00a7 3 that are applied to large pre-trained models can be considered as post-hoc adaptation. Modularity can be achieved through parameter composition (\u00a7 3.1) using sparse subnetworks (Mehta, 2019; Chen et al., 2020; Donahue et al., 2014; Cai et al., 2020; Ben Zaken et al., 2022; Guo et al., 2021), or low-rank adapters (Li et al., 2018; Hu et al., 2022), input composition (\u00a7 3.2) by augmenting\nthe function\u2019s input (Brown et al., 2020; Li & Liang, 2021), and function composition (\u00a7 3.3) through adapter layers (Rebuffi et al., 2017; Houlsby et al., 2019) and rescaling (Liu et al., 2022b). Additionally, hypernetworks can be used to generate the parameters of any of the above-mentioned types of modules (\u00a7 3.4). Essentially, all of these methods are tightly connected as they share the same functional form (\u00a7 3.5)."
        },
        {
            "heading": "7 Applications in Transfer Learning",
            "text": "Most applications of modular deep learning revolve around transfer learning. In particular, the two main purposes are: 1) parameter-efficient fine-tuning (\u00a7 7.1), which achieves superior efficiency, prevents negative interference, and enables zero-shot transfer; and 2) zero/few-shot generalisation to new tasks (\u00a7 7.2). In what follows, we provide a quick overview of transfer learning applications of modular deep learning. For the in-depth discussions and illustrations of the key concepts, we will first focus on applications in NLP, and then draw direct analogies with other deep learning areas such as speech processing, computer vision, and multi-modal (representation) learning. In \u00a7 8, we will explore additional purposes of modular deep learning, including hierarchical reinforcement learning, programme simulation, and causal inference."
        },
        {
            "heading": "7.1 Parameter-Efficient Fine-tuning",
            "text": "Regardless of the application area, one of the principal uses of modules has been to boost parameter efficiency and decrease model storage requirements of fine-tuning, eschewing so-called full model fine-tuning which requires storing a separate copy of the full model per task (Howard & Ruder, 2018; Devlin et al., 2019), see \u00a76.3. In the simplest formulation, all task-specific updates are pushed to the parameters of the lightweight modules, while the parameters of the large base model are kept frozen throughout task fine-tuning. The modules then store task-specific knowledge that can be composed with the \u2018general-purpose\u2019 knowledge of the base model to adapt it to the task at hand. In NLP, this led to a number of research papers that introduced diverse modular architectures, as surveyed in \u00a7 3 and \u00a7 6. A typical evaluation protocol is fine-tuning a type of module on the popular GLUE and SuperGLUE benchmarks (Wang et al., 2019), comparing against full model fine-tuning or alternative modular architectures. The results usually corroborate either of two main goals: (i) improving performance with the same parameter budget versus (ii) maintaining performance with a smaller parameter budget (Mahabadi et al., 2021; Zhou et al., 2023). In addition, modular adaptation has further benefits: first, it prevents negative interference between tasks (Bapna & Firat, 2019). Second, it allows for combining adapters to enable zero-shot transfer (Pfeiffer et al., 2020b)."
        },
        {
            "heading": "7.1.1 Machine Translation",
            "text": "In the seminal work of Bapna & Firat (2019), bilingual (i.e., language-pair) adapters (see \u00a73.3) were used to adapt a massively multilingual NMT model (spanning 103 languages) to a particular source\u2013target translation direction. One benefit of such bilingual adapters is their ability to \u2018skew\u2019 the multilingual model to the language pair at hand without losing the benefits of massively multilingual training for low-resource languages. Another positive effect of bilingual adapters concerns recovering the MT performance also for high-resource languages. High-resource languages might typically suffer from performance deterioration due to the particular interference phenomenon known as the \u2018curse of multilinguality\u2019 (Conneau et al., 2020; Wang et al., 2020): when (too) many languages compete for the fixed parameter budget of the model, the model\u2019s expressiveness and representation power deteriorates for all languages. The use of modules extends the parameter budget to recover the detrimental effects of multilingual inference through dedicated (i.e., modular) bilingual adaptation. Their work also demonstrates the superior performance of a multilingual model specialised towards a particular language pair over merely training a bilingual NMT model for the same pair from scratch.\nHowever, fine-tuning bilingual adapters (or more generally, modules) for each translation direction assumes parallel data for all language pairs and requires n(n\u2212 1) modules to cater for all possible language pairs (one dedicated module in the encoder and another module in the decoder). Therefore, follow-up work (Philip et al., 2020; \u00dcst\u00fcn et al., 2021) aimed to learn monolingual (i.e., language-specific) adapters. Again assuming standard encoder-decoder architectures for MT such as mBART (Liu et al., 2020), this design requires only 2n modules in total. Besides improving parameter efficiency, this also bypasses the critical dependency on parallel data for all language pairs and allows for learning from monolingual data. Crucially, this design\nalso enables translation to or from languages without parallel data, in a fully unsupervised way, and even to/from languages unseen by the base pre-trained encoder-decoder model. Put simply, when translating from language ls to lt, only the encoder adapters for ls plus the decoder adapters for lt are activated: the model is able to translate from ls to lt without seeing a single parallel ls to lt sentence. This application in the field of NMT exemplifies the power of modular design: available components, which were previously learned locally and asynchronously, can be recombined in novel ways to generalise systematically to unseen applications (i.e., in this particular case, to unseen translation directions). This is one of the main goals of modular deep learning (\u00a7 1).\nThe separation into dedicated language-specific modules mitigates interference and catastrophic forgetting; however, it also hinders any positive transfer between modules of similar languages. The positive transfer can be achieved through the use of hypernetworks (see \u00a73.4): Baziotis et al. (2022) learn to generate monolingual language-specific adapters for NMT. In fact, sharing the parameter generator takes advantage of language similarities (Platanios et al., 2018). As discussed in more detail later in \u00a77.1.2, similar ideas of combining the modular design with hypernetworks have also been applied earlier and beyond NMT, e.g., for task fine-tuning with adapters in monolingual multi-task setups (Karimi Mahabadi et al., 2021) and for cross-lingual transfer in single-task (Ansell et al., 2021) as well as in multi-task setups (Ponti et al., 2021; \u00dcst\u00fcn et al., 2022).\nThe curse of multilinguality and catastrophic interference in multilingual MT models have also been tackled through sparse sub-networks (see \u00a7 3.1). Lin et al. (2021) extract sparse sub-networks for specific language pairs from a trained multilingual MT model via pruning. Subnetworks are then trained separately in order to specialise towards the particular translation direction. In fact, there exist dedicated small sub-networks (which can be obtained via standard masking) that store language pair-specific knowledge within the large network, where such knowledge should not interfere with other language pair-specific sub-networks (Dua et al., 2022). The same high-level idea has also been applied to domain adaptation of bilingual MT systems: e.g., Liang et al. (2021) show that it is possible to learn domain-specific sub-networks when fine-tuning the MT system on new domains, where a single large network (i.e., the full neural MT system) comprises multiple disjoint domain-specific sub-networks specialised to particular domains.\nAnother approach that leverages modularity for an increased language-specific capacity in MT is mixtureof-experts. Each expert is typically dedicated to a particular language or translation direction (Kudugunta et al., 2021; Costa-juss\u00e0 et al., 2022). To maintain feasible decoding time, the procedure works as follows: (i) during training, mix the inputs from different translation directions in the same batch, in order to learn the routing network and encourage positive transfer among related tasks; (ii) at inference time, different translation directions are decoded separately, and only the corresponding subset for elevant experts is loaded."
        },
        {
            "heading": "7.1.2 Cross-Lingual Transfer",
            "text": "NMT focuses on translation as a single task and modularity was exploited mainly to carve language-specific and/or domain-specific modules that can support multilingual and multi-domain systems, respectively. In more general cross-lingual transfer setups, the aim is to transfer large models (Devlin et al., 2019; Conneau et al., 2020) fine-tuned for any task (e.g., sequence labelling tasks such as NER, text classification tasks such as NLI, sentiment analysis or intent detection for dialogue systems) on one or more source languages (where such task annotations exist) to one or more target languages (Hu et al., 2020; Ruder et al., 2021). Ideally, the transfer should be achieved without fine-tuning the full large model (Hu et al., 2020), which either results in catastrophic forgetting and negative interference, or requires the creation of separate model copies for each task.\nThe idea of training language modules thus largely follows what already outlined for MT in \u00a77.1.1, with the addition of another set of dedicated modules that aim to capture task-related knowledge: task modules. Such language modules and task modules can then be combined to 1) favour zero-shot cross-lingual transfer for particular source-target directions (Pfeiffer et al., 2020b; Ansell et al., 2021; 2022; Parovi\u0107 et al., 2022); 2) provide extra capacity to low-resource languages under-represented (or even not covered) in the large multilingual models such as mBERT or XLM-R (Pfeiffer et al., 2021b; 2022b; Ponti et al., 2020; Faisal & Anastasopoulos, 2022), independently from task knowledge; and 3) enable handling unseen language\u2013task or even language\u2013domain\u2013task configurations (Ponti et al., 2021; Stickland et al., 2021).\nAs an example of zero-shot cross-lingual transfer, the original MAD-X framework (Pfeiffer et al., 2020b, Figure 1a) relies on bottleneck adapters to implement language and task modules: In particular: 1) Language modules are inserted into each layer of the original neural model and are fine-tuned on (unsupervised) data of the particular language (e.g., via Masked Language Modelling) while the weights of the original model are kept fixed. 2) After obtaining language modules, task modules are stacked on top of the source language module(s) and are fine-tuned relying on the task objective and task-annotated data in the source language(s), while both the original model and language modules are kept fixed. 3) At inference, source language module(s) are replaced with the desired target language module while retaining the task module: this enables zero-shot task inference in the target language.\nRecent work has introduced a spectrum of variations and enhancements to this core idea. For instance, inspired by the bilingual \u2018translation direction\u2019 adapters for NMT systems (\u00a77.1.1), Parovi\u0107 et al. (2022) learn bilingual adapters instead of single language adapters to boost transfer for a particular language pair. Faisal & Anastasopoulos (2022) and Chronopoulou et al. (2022b) learn language family adapters to reduce data sparsity for low-resource languages and capitalise on language similarity and cross-language sharing. Stickland et al. (2021) decouple language and domain knowledge into dedicated modules (see also \u00a77.1.3 later). Further, Ansell et al. (2022) implement dedicated modules as sparse sub-networks, the so-called language and task masks, which can be composed with the base model via parameter composition. Following the analogy between language-specific and bilingual adapters, instead of learning separate language and task subnetworks, Foroutan et al. (2022) learn dedicated task\u2013language sub-networks, demonstrating the variance in the extracted subnetworks across different task\u2013language combinations. The use of such language sub-networks as language modules, even without dedicated task modules, improves cross-lingual transfer for dependency parsing when used within a meta-learning setup (Choenni et al., 2022). Litschko et al. (2022) compare sparse sub-networks and bottleneck adapters for transferring ranking functions for information retrieval tasks across languages and find them both superior to full model fine-tuning.\nFinally, a body of work again focuses on \u2018contextually generating\u2019 the modules via hypernetworks, aiming to increase efficiency and benefit from connections between different languages and tasks. A representative example is the Hyper-X framework (\u00dcst\u00fcn et al., 2022) provided in Figure 6, where the module parameter generation is conditioned on the (disentangled) task and language, and additionally on the index of the Transformer layer where the generated module is inserted. Each task and language are parameterised via separate embeddings, which enables adaptation to any task\u2013 language combination, where these embeddings are low-dimensional\nvectors which are learned together with the parameters of the hypernetwork (see Figure 6 again). The framework thus leverages supervision and positive transfer from both multiple tasks and languages. Hyper-X can be seen as a more general variant of a series of precursors backed by the idea of contextual generation: Ponti et al. (2021) condition the hypernetwork on both task and language embeddings but generates only the model\u2019s classifier head. Other methods generate modules but condition the hypernetwork only on tasks in a monolingual setup (Karimi Mahabadi et al., 2021) or only on languages in a cross-lingual transfer setup (\u00dcst\u00fcn et al., 2020; Ansell et al., 2021)."
        },
        {
            "heading": "7.1.3 Domain Adaptation",
            "text": "As already hinted at in \u00a77.1.1 and \u00a77.1.2, dealing with different domains adds another tier to the modular design: domain-specific knowledge might be captured within dedicated domain modules.18 This can again be accomplished through similar modular architectures as with language and task adapters. For instance, it is possible to inject domain-specific knowledge into (bottleneck) adapters (Zhang et al., 2021; Chronopoulou et al., 2022a) or to extract sparse domain-specific or task-specific sub-networks (Thompson et al., 2018; Ke et al., 2021b) for multi-domain and multi-task learning. Mixture-of-experts also enable multi-domain joint learning as well as domain adaptation (Guo et al., 2018; Zhong et al., 2022). Similar strategies have also been used in multi-domain and cross-domain speech processing and computer vision applications (see \u00a77.1.5 and \u00a77.1.6 later).\nIn domain adaptation, it is common to combine both shared parameters and domain modules that are learned jointly (Bousmalis et al., 2016). Beyond this standard setting, many approaches employ additional regularisation terms. The most common are 1) a domain-adversarial loss on the shared parameters in order to encourage them to be domain-invariant (Ganin et al., 2016; Chen & Cardie, 2018); 2) an orthogonality constraint on the domain modules to ensure that they capture different information (Baktashmotlagh et al., 2013; Kim et al., 2017); and 3) similarity constraints that bring representations of similar modules close together (Bousmalis et al., 2016)."
        },
        {
            "heading": "7.1.4 Knowledge Injection",
            "text": "Naturally, dedicated modules can also be assigned to inject and store external knowledge (e.g., from manually curated external knowledge bases), which can then interact with language, domain, or task knowledge. This idea has been explored with diverse external knowledge sources. For instance, Lauscher et al. (2020) aimed at complementing the distributional knowledge of large language models with conceptual and commonsense knowledge from ConceptNet (Speer et al., 2017). The external knowledge was captured within dedicated bottleneck adapters: they were fine-tuned via language modelling on synthetically created sentences from random walks over the ConceptNet graph structures. Majewska et al. (2021) stored verb-related knowledge from VerbNet (Schuler, 2005), a human-created verb classification repository, into bottleneck adapters, and demonstrated its usefulness in a range of tasks that require understanding of verb semantics. Along similar lines, Wang et al. (2021a) offered a generalisation of these approaches where different knowledge sources (e.g., Wikipedia, WikiData) are mapped to different dedicated adapters, which can be aggregated according to the task at hand. The same idea has been explored by Lu et al. (2021) in the biomedical domain, where the main knowledge sources were the UMLS Metathesaurus graph (Bodenreider, 2004) and biomedical Wikipedia articles. Lu et al. (2021) also introduce another component, the so-called knowledge controller, which can be seen as a standard attention-based function aggregator from \u00a75.4.\nFinally, Lauscher et al. (2021) learned bottleneck adapters without manually curated external data, with the focus on model debiasing: the debiasing adapters were fine-tuned via standard language modelling on a counterfactually augmented corpus."
        },
        {
            "heading": "7.1.5 Speech Processing",
            "text": "The use of modular deep learning for speech processing applications closely matches the ideas already exposed for NLP tasks. The landscape of the possible modular designs is exactly the same, where the only crucial differences are (i) the choice of the underlying large model, and (ii) the corresponding objective functions used to inject the specialised knowledge into the modules. For instance, the typical choice of the base model for automatic speech recognition (ASR) applications is one from the wav2vec family (Baevski et al., 2020; Babu et al., 2022), while the ASR-oriented objective function is the standard Connectionist Temporal Classification (CTC) loss (Graves et al., 2006). The high-level modular structure remains the same, as illustrated in Figure 7 with an example from Thomas et al. (2022), which utilises standard bottleneck adapters.\n18For instance, disentangling domain and language information yields benefits for NMT and cross-lingual transfer applications (Vilar, 2018; Cooper Stickland et al., 2021; Pham et al., 2021; Saunders, 2022).\nWhile in theory a large variety of possible modular configurations from \u00a7 3-\u00a7 6 can be applied to diverse speech processing tasks, the majority of current work in the area has indeed focused on the use of bottleneck (sequentially placed) adapters for ASR in monolingual and multilingual contexts. Before that, the concept of modularity can be traced to the work of Swietojanski et al. (2016), where the model re-weights hidden units using small amounts of unsupervised data to better adapt to a particular speaker or an environment. More recently, bottleneck adapters have been used to perform ASR adaptation to atypical and accented speech (Tomanek et al., 2021), unseen speakers with limited adaptation data (Wang & Van hamme, 2022; Eeckt & Van hamme, 2022; Chen et al., 2023), new domains and manners of speaking (e.g., children\u2019s speech) (Fan & Alwan, 2022; Zhu et al., 2022), or to perform further model customisation to specific speakers (Biadsy et al., 2022; Sathyendra et al., 2022) and for multilingual learning (Kannan et al., 2019; Hou et al., 2022). A notable exception, not resorting to adapter layers, is the method of (Winata et al., 2020) which aims to learn low-rank modules (\u00a7 3.1), akin to the idea of LoRA (Hu et al., 2022), for end-to-end ASR.\nMulti-task (where the term \u2018task\u2019 in this context can e.g. refer to different languages, domains, speakers, or accents) ASR setups have also witnessed the usage of mixture-of-experts, closely following the basic ideas already discussed for NMT (\u00a77.1.1) where different languages are assigned their dedicated modules through fixed routing. For instance, in speech processing, MoEs have been applied to multilingual ASR and cross-lingual ASR transfer (Bai et al., 2022; Gaur et al., 2021; Kumatani et al., 2021), while You et al. (2022) propose MoE for ASR with learned routing.\nBeyond ASR, bottleneck adapters have also been used for speech translation (Le et al., 2021). Most recently, modular adapter-based approaches have been applied to text-to-speech methods (TTS) (Hsieh et al., 2022; Morioka et al., 2022), aiming to extend standard large multi-speaker TTS models such as FastPitch (Lancucki, 2021) to new speakers without compromising the TTS quality for the seen speakers. From a high-level perspective, one can see a direct analogy of this goal to the objectives in the MT literature of extending multilingual MT systems to unseen languages without compromising seen languages (see \u00a77.1.1 again)."
        },
        {
            "heading": "7.1.6 Computer Vision and Cross-Modal Learning",
            "text": "In computer vision, similar to NLP and speech processing (\u00a77.1.5), dedicated modules are again used to enable parameter-efficient fine-tuning across multiple tasks and domains (Rusu et al., 2016; Rebuffi et al., 2018; Berriel et al., 2019; He et al., 2022b, among others). The core difference, again, is the choice of the actual neural architecture for the underlying model as well as for the modules: e.g., residual adapters (Rebuffi et al., 2017) consisted of simple 1\u00d7 1 convolutions combined with the base ResNet neural model (He et al., 2016) while other work learned task-specific convolutional filters (Newell et al., 2019; Bragman et al., 2019). More recent work aims to exploit modular architectures from NLP (e.g., sequential or parallel adapters, LoRA, prefix tuning) with pretrained Vision Transformer (ViT) architectures (Dosovitskiy et al., 2021): e.g., He et al. (2022b) run a comparative empirical analysis of various modular architectures for vision tasks, while Chen et al. (2021) rely on sparse sub-networks.\nModular design lends itself naturally to cross-modal and multi-modal applications, where different modalities may be captured by modality-specific parameters and routing can also be modality-conditioned. For instance, in multilingual vision-and-language (V&L) settings, it is possible to conduct inference in languages that lack labelled task examples. In fact, language knowledge is again disentangled from the task and modality knowledge, and the knowledge for different input modality streams can be captured in ded-\nicated modules. This idea has been heavily explored in recent work in multi-modal multi-task scenarios, both in monolingual (Sung et al., 2022) and multilingual contexts (Bugliarello et al., 2022; Pfeiffer et al., 2022a), for tasks such as image captioning (Zhou et al., 2022a; Gao et al., 2021a), text-to-image generation (Maharana et al., 2022), visual question answering (Liu et al., 2022a; Sung et al., 2022), visual reasoning (Liu et al., 2021a), etc. For instance, Flamingo (Alayrac et al., 2022) uses frozen pretrained vision and language models, and only trains adapter layers to handle sequences of arbitrarily interleaved visual and textual data. It is trained with a sequence modelling objective on Web-scale data (Li et al., 2021) and displays impressive zero-shot and few-shot capabilities. Pfeiffer et al. (2022a) use adapter modules to equip multilingual text-only models with the ability to also process the visual modality, as well as to equip monolingual multi-modal models to deal with input from multiple languages. Papalampidi & Lapata (2022) rely on hierarchical adapters (akin to hierarchical representation aggregation discussed in \u00a7 5) for the task of summarising long videos into textual descriptions. Pan et al. (2022) demonstrate that modular design also helps in image-to-video transfer tasks: they use adapter modules to equip a large image-based model without temporal knowledge with the ability to reason about dynamic video content.\nWe note that in this survey, we aim to list some exemplary applications and draw parallels between different yet similar application areas such as NLP, speech processing, and computer vision. While we acknowledge that there exists a wealth of other work in these areas, we have no pretence of exhaustiveness."
        },
        {
            "heading": "7.1.7 Comparison and Design Principles",
            "text": "While a full-fledged comprehensive empirical study of the plethora of modular architectures across various application tasks and areas is still lacking, there exist initiatives such as the publicly available AdapterHub platform (Pfeiffer et al., 2020a): it provides (re)implementations of representative modular NLP architectures, within a unified framework tied to HuggingFace Transformers (Wolf et al., 2020). Among others, AdapterHub includes representatives of each computation method in \u00a7 3: LoRA (Hu et al., 2022) (i.e., low-rank parameter composition), prefix tuning of Li & Liang (2021) (input composition) and a number of bottleneck adapter configurations (function composition). The existence of AdapterHub delineates another crucial advantage of modularity: reusability of existing, already fine-tuned modules which can be (re)combined with the large neural models. In short, any practitioner can share or reuse a module specialised for a particular purpose (e.g., capturing specific task or language knowledge) with the community, facilitating community-wide sharing and thus avoiding time- and energy-costly repetitions of the same fine-tuning procedure.19 As discussed in \u00a7 4, one can observe initiatives such as AdapterHub as continuously updating community-distributed multi-task models.\nThe discussion in this section also points to a more general principle: different end-goals even within the same end-application (e.g., NMT, cross-lingual transfer, domain adaptation) require rethinking the actual modular design, and the desired level and nature of modularity. For instance, if the goal in NMT (or cross-lingual transfer) is to boost performance for a particular translation or transfer direction, it might be useful to trade off some modularity for a better final performance by replacing language-specific monolingual modules with bilingual modules (Bapna & Firat, 2019; Parovi\u0107 et al., 2022). On the other hand, if the goal is to enable zero-shot or few-shot translation or transfer, the design with monolingual modules might be a better choice. In another example, if the focus is on MT or transfer for a particular low-resource language, the model designer should enable positive transfer to that language by \u2018opening\u2019 the flow of information from a module storing knowledge on high-resource languages similar to the target language if such languages exist (e.g., from Spanish to Galician) (\u00dcst\u00fcn et al., 2021), or by learning modules for families or groups of similar languages (Chronopoulou et al., 2022b; Faisal & Anastasopoulos, 2022). Analogously, related domains can also be grouped and hierarchically organised to enable positive transfer for domain adaptation (Chronopoulou et al., 2022a).\nOther practical desiderata may also influence the selection of the actual modular design. If the final task performance is paramount, larger modules might be preferred, e.g., in order to offer enough extra capacity to store the wealth of language-specific information (Ansell et al., 2022). However, if model compactness is paramount, the criterion for choosing a specific design is instead the trade-off between efficiency (in terms of\n19The (concept of) reusability enabled by the modular design also positively impacts energy consumption (Strubell et al., 2019), making an important leap towards Green(er) AI (Schwartz et al., 2020).\nparameters and/or train and test time) and task performance; the optimisation of this trade-off has been the focus of recent research (R\u00fcckl\u00e9 et al., 2021; Mahabadi et al., 2021; Karimi Mahabadi et al., 2021; Sun et al., 2022). In another example, if time efficiency during inference is a crucial requirement (e.g., real-time ASR in dialogue systems, low latency for information search systems) parameter composition methods such as sparse subnetworks or low-rank composition methods may be preferred over function composition methods as the latter increase the number of computations required during the forward pass, (see Table 3). In yet another example, if storage requirements are a critical constraint, one cannot resort to huge mixture-of-expert models where billions of parameters must be stored (Lepikhin et al., 2021)."
        },
        {
            "heading": "7.2 Task Generalisation",
            "text": "The diverse applications of modular deep learning covered so far almost exclusively focus on learning modules associated with (arguably) well-formed and interpretable \u2018units of knowledge\u2019 such as languages, tasks, domains, dialects, accents, and speakers. However, modularity might also be achieved when such units are unknown. This relies on jointly learning arbitrarily sized inventories of so-called latent skills and a learned routing function (\u00a7 4.2). Since such skills are learned end-to-end on a mixture of data from multiple tasks, they are often not straightforwardly interpretable. On the other hand, since arbitrary subsets of skills can be combined and each skill can be updated locally, these modular neural architectures are ideal for systematic generalisation to new tasks (Zhang et al., 2022a; Ponti et al., 2022).\nIn fact, another fundamental application in transfer learning is achieving zero-shot or few-shot generalisation to new tasks, where test examples are not i.i.d. with respect to training examples. The general experimental setup involves disjoint sets of training and test tasks. A model is pre-trained through multi-task learning on training tasks and then adapted to each new test task based on zero or few data points. Common examples of evaluation benchmarks for this setting include CrossFit (Ye et al., 2021), the T0 task suite (Sanh et al., 2022), or Natural Instructions (Mishra et al., 2022). While a common strategy to tackle this problem is instruction tuning (Sanh et al., 2022; Wei et al., 2022), where models are fine-tuned prepending the instructions for each task, modular deep learning has emerged as a strong contender (Alet et al., 2018; Kudugunta et al., 2021; Ponti et al., 2022)."
        },
        {
            "heading": "8 Other Purposes of Modularity",
            "text": "In addition to scaling large models (for instance, through MoEs, as discussed in \u00a7 4.2.3) and facilitating transfer learning, which we covered in \u00a7 7, modularity serves multiple additional purposes. In particular, we devote this section to a cursory view of modularity for i) hierarchical reinforcement learning (\u00a7 8.1); ii) neural programme simulation (\u00a7 8.2); iii) neural causal inference (\u00a7 8.3). While most of these applications predate the advent of neural networks, (modular) deep learning expands the scope and potential of these lines of research for a series of reasons. First, it holds promise to induce the relevant latent structures (such as options, programmes, or causal graphs, respectively) in an end-to-end fashion. Second, it marries these traditional problems with the ability to jointly model low-level perception, such as vision and language."
        },
        {
            "heading": "8.1 Hierarchical Reinforcement Learning",
            "text": "The goal of reinforcement learning is to learn a policy, which predicts the next action of an agent based on past observation(s) from the environment, that maximises the return, i.e. the sum of future discounted rewards. However, many tasks span extremely dilated temporal horizons or provide only highly sparse or delayed rewards. In these cases, it becomes helpful to model intermediate abstractions between high-level goal specifications and low-level actions and observations (Sutton et al., 1999; Precup, 2000). This facilitates the planning abilities of the agent as well as their sample efficiency. In fact, the above-mentioned intermediate abstractions, known as options or skills, consist of sub-policies that are transferable across tasks.\nMore formally, each reinforcement learning task is a Markov Decision Process (MDP) consisting of states S and actions A, a transition function p : S \u00d7A\u00d7 S \u2192 [0, 1] and a reward function r : S \u00d7A \u2192 R. We aim to learn a policy \u03c0 : S \u00d7A \u2192 [0, 1]. We also define a value function as the expected (discounted) return from a given state s as V\u03c0(s) = E\u03c0[ \u2211\u221e t=0 \u03b3\ntrt+1 | s0 = s], as well as a Q function from a state s and an action a as Q\u03c0(s, a) = E\u03c0[ \u2211\u221e t=0 \u03b3\ntrt+1 | s0 = s, a0 = a]. Following Sutton et al. (1999) and Precup (2000), each option \u03c9 \u2208 \u2126 is defined as a tuple (I\u03c9, \u03c0\u03c9, \u03b2\u03c9), where I\u03c9 \u2286 S is the initiation set, \u03c0\u03c9 : S \u00d7 \u2126 \u2192 [0, 1] the option-specific policy, and \u03b2\u03c9 : S \u2192 [0, 1] is the termination function. For simplicity, many works assume that \u2200s \u2208 S,\u2200\u03c9 \u2208 \u2126, s \u2208 I\u03c9: in other words, all options are available at every state. Augmenting a task with options transforms it into a Semi-MDP, with corresponding functions V\u2126(\u03c9) and Q\u2126(s, \u03c9).\nLearning options involves a series of challenges (Jiang et al., 2019). Firstly, it is not trivial to specialise sub-policies towards distinct behaviours. This shortcoming is common to many modular architectures with learned routing (Mittal et al., 2022, see \u00a7 4.2). Not only this, the problem of hard learned routing has often been cast in a reinforcement learning framework (\u00a7 4.2.2). Secondly, one must define the space where the actions of the high-level policy, which are latent variables, lie. In practice, one could treat them as a discrete, unordered set. In this case, a module from an inventory is chosen for a certain amount of time steps. However, alternative methods operate in structured spaces such as language, which is more transferable and scalable due to its combinatorial nature. Thirdly, training multiple options dilates the training time and requires collecting an appropriate amount of experiences for each of them. Fourthly, if trained jointly, options change simultaneously with the master policy, which is a source of non-stationarity. As a consequence, previous experiences for the master policy become invalid if the options have been updated in the meantime. Again, this is reminiscent of the challenges of learned routing exposed in \u00a7 4.2.\nThe simplest solution to circumvent end-to-end joint learning of the master policy and options is to provide separate supervision to both (Sutton et al., 1999; Dayan & Hinton, 1992). However, this may require extensive annotation, which is often not available. Thus, an alternative method is defining sub-goals, i.e. states an agent should reach as a stepping stone towards the high-level goal Dietterich (2000). Nevertheless, this similarly fails to scale due to the exponentially growing number of combinations of sub-goals some tasks may entail. Moreover, this does not eschew the need to train individual sub-policies for each sub-goal. A partial remedy is offered by hindsight learning, where an off-policy correction is introduced (Nachum et al., 2018). Specifically, the original target sub-goal of the current option is substituted with the one maximising the probability of the past sequence of low-level actions. Similarly, the master policy can be trained in hindsight through the currently predicted sequence of high-level sub-goals. Overall, relabelling past experiences significantly improves the model\u2019s sample efficiency.\nA more radical solution to the challenge of scalability is jointly training both the master policy and options in an end-to-end fashion. To this end, Bacon et al. (2017) put forth a new architecture, the option\u2013critic, that discovers options from data, without supervision for the intermediate abstractions. This architecture is trained based on policy gradient theorems Bacon et al. (2017) derive for options. Moreover, they augment the set of actions A available to each policy \u03c0\u03c9 with a special end-of-policy action eop instead of explicitly modelling \u03b2\u03c9. Intuitively, formulating the execution as call-and-return, a master policy \u03c0\u2126 determines the active option \u03c9, whose policy \u03c0\u03c9 is followed until the eop action is chosen. At this point, control returns to the master policy to choose the next option, and so on until termination. A downside of this method is that it is unstable and often diverges to degenerate solutions (Jiang et al., 2019). Thus, several inductive biases have been proposed to correct it. A popular method is leveraging intrinsic rewards: an auxiliary loss diversifies options by maximising the mutual information between each option and the next state conditioned on the current state (Florensa et al., 2017; Kulkarni et al., 2016).\nAn orthogonal question revolves around the ideal space for the option variables. In fact, compared to a discrete, unordered inventory of (possibly hard-coded) options, language affords more flexibility (Andreas et al., 2017; Jiang et al., 2019) as it solves many of the above-mentioned challenges. In fact, all sub-policies can be implemented through a single model conditioned on the linguistic label of the current option. This not only allows options to borrow statistical strength from each other but also makes options reusable in new tasks. Moreover, the nature of language (through its infinite use of finite means) is suitable to capture the extremely complex combination of sub-goals of many reinforcement learning tasks. Note that linguistic options can be interpreted as a generalisation of sub-goals, as every instruction implicitly corresponds to a subset of states (Jiang et al., 2019).\nIn practice, to learn linguistic options, Andreas et al. (2017) assumes that \u2018sketches\u2019 of options are provided for supervision (see Figure 8). To induce them, subsequent methods rely instead on synthetic experiences through relabelling (Jiang et al., 2019), or restricted vocabularies and syntax such as predicate\u2013argument pairs (Das et al., 2018). Recently, the master policy has been frequently implemented as a large language model. Since these are pre-trained on text, they already contain world knowledge that can serve as a powerful inductive bias for grounded learning. For instance, Huang et al. (2022) use frozen language models to generate options through prompting in a zero-shot fashion."
        },
        {
            "heading": "8.2 Programme Simulation",
            "text": "Another distinct purpose of modular architectures is to model programmes, as a means to induce them from data or to simulate symbolic algorithms. The simplest (and least expressive) family of programmes are Finite State Automata (FSA). These receive a neural implementation in the Compositional Recursive Learner (CRL; Chang et al., 2019), similarly to Routing Networks (Rosenbaum et al., 2018) and Modular Networks (Kirsch et al., 2018). In these neural architectures, a loose equivalence can be drawn as follows: transformations induced by modules are transition functions (arcs in the graph), input and output representations are the states (nodes in the graph), and the input is the starting state. A memoryless routing function selects the transition based on the current state. Thus, the programme graph is constructed dynamically. The final states are defined as those reached after the router selects a special end-of-computation action.\nOn the other hand, a programme graph can be constructed globally based on the task description before processing the data. In particular, Neural Module Networks (NMNs; Andreas et al., 2016b;a) rely on an off-the-shelf semantic parser (and custom rules) to map a query in natural language into a tree graph. The nodes of this graph are learnable modules characterised by: 1) their types of input (raw image features and/or attention scores) and output (attention scores or labels); and 2) the particular instances of a type, indicated as an argument in the form of a natural language string. For instance, the module find[cat] takes an image and returns attention scores over the regions that contain cats. Compositionality is achieved by sharing weights across modules with the same type or instance. NMNs have been further extended to be amenable to end-to-end training without the aid of an external parser (Hu et al., 2017). In this case, the mapping from queries to programme graphs is learned by imitating expert demonstrations while the module parameters are learned based on the downstream loss of visual question answering.\nIn addition to the routing function and computation functions, a model can be extended with an external memory. In fact, these three mirror the fundamental components of a computer architecture: elementary operations, logical flow control, and a random-access memory that can be read and written to (von Neumann, 1945; Graves et al., 2014). While (appropriately wired) recurrent neural networks have been shown to be Turing-complete (Siegelmann & Sontag, 1995), separating the three functions into distinct components provides an inductive bias to simulate the workflow or a computer programme. Neural Turing Machines (NTMs; Graves et al., 2014) introduced a fully differentiable read\u2013write memory matrix that interfaces with the main recurrent network through an attentional mechanism. In particular, this memory can be addressed both based on content (i.e., the match between its entries and the current input) and based on location, in order to store and retrieve temporally ordered information in contiguous entries. NTMs were further extended into the Differentiable Neural Computer (DNCs; Graves et al., 2016, Figure 9), which amended some of the limitations of NMTs, such as avoiding interference in the memory, freeing up previously written locations, and storing temporally ordered sequences in non-contiguous chunks. Another family of memory-augmented methods include the Neural Programmer Interpreter (NPI; Reed & de Freitas, 2016). This model is trained with full supervision from execution traces or through reinforcement learning (Pierrot et al., 2019). In particular, a core recurrent network receives information from a programme module, as well as representations from the environment module. In its output, it produces the index for the next sub-programme and its arguments (as well as a special termination symbol).\nFinally, a recent thread of research focused on simulating the behaviour of symbolic algorithms with vanilla (non-modular) neural networks. An example is neural algorithmic reasoning (Veli\u010dkovi\u0107 & Blundell, 2021). First, a processor network is trained to emulate the output of a symbolic programme (e.g., Dijkstra\u2019s algorithm for shortest paths) that operates on abstract representations (e.g., weighted graphs). Second, encoder and decoder networks can be trained to operate on sensory real-world data while matching the input\u2013output types expected by the processor network.\nAmong the main applications for programme simulations are settings where sub-problems are shared, such as multi-task or curriculum learning. By distilling the most common functionalities into modules, these can be reused to generalise compositionally to new sequences of sub-tasks. Another application is compositional reasoning, such as (visual) question answering (Andreas et al., 2016b;a). In general, external memory is useful for reasoning over complex data structures, such as graphs (Graves et al., 2014; 2016; Reed & de Freitas,\n2016). Finally, neural models can emulate symbolic algorithms to extend their capabilities to operate on sensory real-world data."
        },
        {
            "heading": "8.3 Causal Discovery and Inference",
            "text": "Modularity in the design of a model may be assumed to reflect the modularity in the (physical) mechanisms of the world. In fact, a crucial assumption in causal inference (Sch\u00f6lkopf et al., 2012) is that such mechanisms underlying data generation are independent, as they do not influence each other, and reusable, as they may play a role in multiple distributions. Consequently, if one of the mechanisms, which defines a conditional distribution in the model graph, changes\u2014possibly because of an intervention\u2014the other modules remain invariant. If a machine learning model mirrors this modular structure, it is better suited to generalise in a sample-efficient way to new tasks: in fact, local distribution shifts require updating only the corresponding module parameters, which in turn results in faster adaptation (Bengio et al., 2020; Mittal et al., 2022).\nThe key challenge for this problem is how to specialise each module towards a specific mechanism based uniquely on observational data, especially when the number and nature of the mechanisms are unknown. Competition among the modules through top-k routing (see \u00a7 4.2.2) is a common feature of many proposed solutions.20 Parascandolo et al. (2018) show how to invert causal independent mechanisms through a modular neural architecture, given data from the original distribution and an unlabelled mixture of their transformations (see Figure 10). Their model consists of a mixture of experts and an adversarial discriminator, which enforces that the inverted transformation lies in the support of the original distribution. Another architecture relying on module competition and capable of modelling sequential data is Recurrent Independent Mechanisms (RIMs; Goyal et al., 2021). Here, the modules are recurrent networks with separate parameters, each representing a different transition dynamics. However, their states are not entirely independent, as active modules are allowed to communicate through attention. This reflects a second assumption, namely that the dependencies among variables are highly sparse (Mittal et al., 2022). Attention can also serve to direct the flow of bottom-up and top-down information (Mittal et al., 2020).\n20In addition to causal inference, this strategy is also inspired by the global workspace theory (Baars, 2005). This theory postulates specialised modules compete to update a shared workspace, and the resulting communication bottleneck creates a crucial inductive bias in human cognition.\nAnother challenge of neural causal discovery is jointly inducing abstract latent variables (such as objects or entities) from low-level perception (e.g., pixels of an image) while simultaneously learning the causal graph underlying such variables, which determines how they interact (Ke et al., 2021a). The lacklustre abilities of vanilla neural models to understand the compositional properties of symbolic building blocks, i.e. their \u2018binding problem\u2019, arguably explains their current shortfalls in systematic generalisation (Greff et al., 2020). Object-centric learning holds promise to mitigate these limitations. For instance, it can be facilitated by slot attention, which is a fully differentiable and iterative attention mechanism that interfaces between perceptual representations and slots, a set of unordered placeholder variables (Locatello et al., 2020). (Didolkar et al., 2021) propose Neural Production Systems, where rule templates can be bound to specific entities present in the working memory, in order to update their representations. In particular, rules are MLP modules and the matching with entities (triggering updates) is parameterised by attention.\nCrucially, observational data alone is often21 insufficient to learn structural causal models as they may not be identifiable (Pearl, 2009). Hence the necessity to augment observation with interventions and counterfactuals. These allow for answering questions about cause\u2013effect relationships rather than mere correlations. In real-world scenarios, however, the nature and number of interventions are unknown Ke et al. (2021a). In this setting, there is no formal guarantee that causal discovery succeeds. Yet, Ke et al. (2019) finds that DAG discovery on interventional data based on continuous optimisation recovers causal graphs reliably. In particular, modular architectures surpass both vanilla models and graph neural networks (Ke et al., 2021a). Recently, Geffner et al. (2022) perform causal inference in a deep non-linear additive noise structural equation model, based on autoregressive flows. Variational inference is used to learn a posterior over causal graphs. The learned functions can be further used to estimate conditional average treatment effects based on simulations.\nThe main purpose of these deep modular methods is causal inference and discovery, which has applications in several branches of medicine and economics (Geffner et al., 2022). In addition, these methods are particularly relevant in grounded settings, where the distribution of the observations from the environment changes as the agent learns better policies (Goyal et al., 2021). Moreover, causal discovery can be combined with model-based RL methods to learn a self-supervised model of the environment, i.e. its variables and their causal dependencies, from trajectories of observations, actions, and rewards. This allows for simulating the potential outcomes of a policy before execution and thus estimating better value functions, which dramatically improves sample efficiency in agents (Ke et al., 2021a). Another common application of this family of modular neural architectures is out-of-distribution generalisation: for instance, zero-shot transfer to images of different sizes or sequences of different lengths (Goyal et al., 2021).\n21Unless specific assumptions are made about the data generating process, such as linear but non-Gaussian data."
        },
        {
            "heading": "9 Conclusions",
            "text": "\u2022 Modularity is defined as the functional specialisation of the components of a system. \u2022 Specialised sub-networks may emerge in vanilla neural modules (from multitask training or\nregularisation), but they are seldom reused and recombined. \u2022 Deep modular architectures rest on the separation between computation functions on the one\nhand and routing and aggregation functions on the other. \u2022 Computation functions may consist of any neural module. Modules may modify the original\nparameters, be concatenated to the input, or composed with the original function. \u2022 All composition strategies are equivalent to summing the original output with a term depending\non the new module. In practice, however, they offer different trade-offs between efficiency (in time and space, during training and inference) and performance.\n\u2022 Routing controls the flow of information, i.e., module selection. In fixed routing, it is determined a priori based on expert knowledge. When this is not available, routing parameters are learned.\n\u2022 Learned routing is challenging because of training instability, module collapse, and overfitting. Thus, learned routing often underperforms fixed routing.\n\u2022 Routing can be conditioned on (parts of) the input or metadata such as task identity. Routing can take place at different levels, such as globally for the whole model or layer-wise.\n\u2022 Soft routing assigns every module a continuous score and performs a weighted combination of their outputs. It is amenable to being learned via gradient descent but is highly inefficient.\n\u2022 Hard routing activates only a subset of modules via top-1, top-k, or variable-size selection. It is learned via reinforcement learning, evolutionary algorithms, or stochastic re-parameterisation. It corresponds to the principles of conditional computation and information bottleneck in cognition.\n\u2022 Hypernetworks can be interpreted as combining unnormalised routing (task embedding) with modules (generator). They can in turn generate parameters of other modules.\n\u2022 If routing selects multiple modules, these must be aggregated via a function. \u2022 Module parameters or outputs can be interpolated for aggregation, according to scores from\nthe routing function, an attention mechanism, or via simple averaging. \u2022 Alternatively, aggregation may involve composing the module functions, either sequentially or\nbased on a tree graph obtained from global routing. \u2022 The applications include parameter-efficient fine-tuning in NLP, computer vision, and speech\nprocessing. These rely on the same types of modules and fixed routing. In addition to increased efficiency, this prevents negative interference and enables zero-shot transfer.\n\u2022 Modularity also serves the purpose of generalising to new tasks systematically, by recombining modules and locally updating them.\n\u2022 Modular deep learning transcends the confines of private research: it enables community-driven sharing, expanding, reusing, and updating of the modules.\n\u2022 In hierarchical reinforcement learning, modular options serve as abstractions between task goals and low-level actions and observations. They facilitate planning in long-horizon and sparse-reward tasks and increase sample efficiency due to transferability.\n\u2022 In programme induction, the components of deep models can mirror a computer architecture: modules are elementary operations and routing is logical flow control. These are often augmented by an external read\u2013write memory. Modules can also simulate symbolic algorithms.\n\u2022 In causal discovery and inference, modules may be taken to correspond to physical mechanisms that are independent and reusable.\n\u2022 Modular deep learning empowers these traditional applications by learning abstractions (options, programmes, causal graphs) end-to-end from perceptual stimuli."
        },
        {
            "heading": "9.1 Future Work",
            "text": "While recently modularity has attracted increasing attention in research, there remain many interesting open research questions along the axes of modularity introduced in this survey. We provide an overview of some of these directions for future work.\nCombination of Different Types of Computation Functions Existing computation functions (see \u00a7 3) are mostly associated with a single category: parameter composition, input composition, or function composition. There are a few exceptions such as compacter (Mahabadi et al., 2021)\u2014low-rank adapters\u2014 which combine multiple types. In general, techniques from parameter composition that incorporate sparsity, a low-rank or structural constraint are agnostic of the form of the module. In practice, this should enable more efficient learning and aggregation.\nLearned Module Structure Most modules used in current works share the same architecture, which is reused across different settings. Depending on the skill or knowledge that should be learned, a module may need to be structured differently and might require access to another component or other type of data. In the extreme, a model may require a special-purpose architecture to be able to perform a specific capability (Andreas et al., 2016b). As modules are more widely used, they may benefit from being learned in a more flexible manner, perhaps incorporating ideas from neural architecture search (Negrinho et al., 2019) in a module-specific space of architecture primitives.\nStandardising Modularity Evaluation Depending on the dimension studied, modular approaches may be evaluated based on a variety of factors including downstream performance, memory footprint, number of parameters, latency, and compositional generalisation. In order to make progress on modular models in general, evaluation should be standardised. Current evaluation is additionally mainly based on existing datasets that are re-purposed to enable modular evaluation such as by framing them in a zero-shot transfer setting. Future work on modularity evaluation should design forward-looking evaluation benchmarks that are designed to test the capabilities of the next generation of modular models such as assessing the composition of skills and acquisition of new types of reasoning abilities.\nNature of Modular Representations While modular representations have been aggregated and composed, it remains mostly unclear how the inductive bias of a computation function influences the modular representation that is learned. In addition, it remains unclear how computation functions differ on a representation level. Beyond the computation function, it is also unclear how the other dimensions of our taxonomy, i.e., the routing function, the aggregation function, and the training setting influence the nature of the modular representations.\nHierarchical Modularity Current approaches mostly do not differentiate between high-level and low-level skills and how they relate to each other. It might also be possible to designate particular parts of the model or dedicated modules to capture a set of specialised skills or options, and clearly distinguish between other (sets of) skills. At fine-tuning, even more specialised sub-modules could be learned focused only on the previously designated modules. One example might be learning fine-grained specialised subnetworks over larger subnetworks of the original model, offering gradual module specialisation.\nLearned Routing for Pre-training Fixed routing (see \u00a7 4) is the most common strategy to disentangle knowledge into modular parts of the model. However, fixed routing limits the usability of the proposed methods as they cannot be used on data, which lacks the metadata needed for fixed routing; for instance, when training on heterogeneous data, metadata such as domain information often does not exist. While learned routing methods do not require this metadata to perform routing a priori, they suffer from training difficulties (as discussed in \u00a7 4.2). This opens up research directions that enable modular pre-training with learned routing, which would make modular models applicable to a broader set of data.\nBenchmarking Routing Methods Existing studies mainly evaluate routing methods based on performance but do not take into account how different routing strategies influence modular representations. In\norder to make progress on better routing methods, benchmarks and metrics are necessary that compare routing mechanisms from a modularity perspective across different settings.\nStructured and Sparse Aggregation Current aggregation methods (see \u00a7 5) combine the information from multiple modular components by applying arithmetic operations such as addition and subtraction across all parameters, which likely includes parameters that should not be modified. Structured or sparse aggregation methods could focus on aggregating information within salient subnetworks or parameter groups, which might make aggregation more efficient and improve out-of-distribution generalisation.\nLearned Aggregation Methods Most aggregation methods are based on arithmetic operations. Depending on the nature of the modular information, it may be useful to (non-)linearly transform the representations. More complex domain-specific aggregation methods can be learned in conjunction with the modular representations to enable better generalisation to new settings.\nMerging Modular Models In recent work, merging models trained with different settings has led to improved performance (Wortsman et al., 2022, inter alia). Rather than requiring separate training runs of a model, a multi-task model can alternatively be trained with modular components that are designed to be merged at a later stage. This potentially allows for an architecture, which can be computationally efficiently trained while covering many modalities.\nExtensible Multi-task Models Most approaches in multi-task learning have focused on training dense models, with a key limitation being that models cannot easily be extended to new settings. Focusing on training multi-task models with modular components ensures that the baseline models are much easier to adapt and extend to new settings. Given the trend of pre-training larger and larger models from scratch, modularising parts of such models and developing modular methods that can be shared across different architectures and model sizes may lead to more sustainable model development."
        },
        {
            "heading": "Acknowledgements",
            "text": "Ivan Vuli\u0107 is supported by a personal Royal Society University Research Fellowship (no 221137; 2022-).\nWe are grateful to Colin Raffel for his comments and suggestions, which have greatly improved the manuscript. We thank Andrea Gesmundo for feedback on a draft of this paper. We are thankful to Kyunghyun Cho and Alessandro Sordoni for stimulating discussions."
        }
    ],
    "title": "Modular Deep Learning",
    "year": 2023
}