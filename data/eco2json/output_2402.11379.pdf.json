{
    "abstractText": "We propose an indirect inference strategy for estimating heterogeneous-agent business cycle models with micro data. At its heart is a first-order vector autoregression that is grounded in linear filtering theory as the cross-section grows large. The result is a fast, simple and robust algorithm for computing an approximate likelihood that can be easily paired with standard classical or Bayesian methods. Importantly, our method is compatible with the popular sequence-space solution method, unlike existing state-of-the-art approaches. We test-drive our method by estimating a canonical HANK model with shocks in both the aggregate and cross-section. Not only do simulation results demonstrate the appeal of our method, they also emphasize the important information contained in the entire micro-level distribution over and above simple moments.",
    "authors": [
        {
            "affiliations": [],
            "name": "Man Chon"
        },
        {
            "affiliations": [],
            "name": "Yatheesan J. Selvakumar"
        }
    ],
    "id": "SP:d855e3963615d15c188aff0057719dcecd36a3ac",
    "references": [
        {
            "authors": [
                "S. AHN",
                "G. KAPLAN",
                "T.B. MOLL"
            ],
            "title": "WINBERRY, AND C",
            "venue": "WOLF",
            "year": 2017
        },
        {
            "authors": [
                "AN S"
            ],
            "title": "Bayesian analysis of DSGE models,",
            "venue": "SCHORFHEIDE",
            "year": 2007
        },
        {
            "authors": [
                "T.W. ANDERSON"
            ],
            "title": "Estimating Linear Restrictions on Regression Coefficients for Multivariate Normal Distributions,",
            "venue": "The Annals of Mathematical Statistics,",
            "year": 1951
        },
        {
            "authors": [
                "T.W. ANDERSON",
                "H. RUBIN"
            ],
            "title": "Estimation of the parameters of a single equation in a complete system of stochastic equations,",
            "venue": "The Annals of Mathematical Statistics,",
            "year": 1949
        },
        {
            "authors": [
                "S.B. ARUOBA",
                "F.X. DIEBOLD",
                "J. NALEWAIK",
                "F. SCHORFHEIDE",
                "D. SONG"
            ],
            "title": "Improving GDP measurement: A measurement-error perspective,",
            "venue": "Journal of Econometrics,",
            "year": 2016
        },
        {
            "authors": [
                "Y.F. ATCHAD\u00c9",
                "J. S"
            ],
            "title": "On adaptive markov chain monte carlo algorithms,",
            "venue": "ROSENTHAL",
            "year": 2005
        },
        {
            "authors": [
                "A. AUCLERT",
                "B. BARD\u00d3CZY",
                "M. ROGNLIE"
            ],
            "title": "STRAUB (2021a): \u201cUsing the sequence-space Jacobian to solve and estimate heterogeneous-agent",
            "venue": "models,\u201d Econometrica,",
            "year": 2021
        },
        {
            "authors": [
                "BAI J",
                "NG S"
            ],
            "title": "Determining the number of factors in approximate factor models,",
            "year": 2002
        },
        {
            "authors": [
                "C. BAYER",
                "B. BORN"
            ],
            "title": "Shocks, Frictions, and Inequality in U.S. Business Cycles,",
            "venue": "LUETTICKE",
            "year": 2024
        },
        {
            "authors": [
                "C. BAYER"
            ],
            "title": "Solving heterogeneous agent models in discrete time with many idiosyncratic states by perturbation methods,",
            "venue": "LUETTICKE",
            "year": 2020
        },
        {
            "authors": [
                "T. BOPPART",
                "P. KRUSELL"
            ],
            "title": "Exploiting MIT shocks in heterogeneous-agent economies: the impulse response as a numerical derivative,",
            "venue": "MITMAN",
            "year": 2018
        },
        {
            "authors": [
                "S.L. BRUNTON",
                "J. N"
            ],
            "title": "Data-Driven Science and Engineering: Machine Learnings, Dynamical Systems, and Control, second edition",
            "venue": "KUTZ",
            "year": 2022
        },
        {
            "authors": [
                "G. CHAMBERLAIN"
            ],
            "title": "Arbitrage, factor structure, and mean-variance analysis on large asset markets,",
            "venue": "ROTHSCHILD",
            "year": 1982
        },
        {
            "authors": [
                "L.J. CHRISTIANO",
                "R. J"
            ],
            "title": "Maximum likelihood in the frequency domain: the importance of time-to-plan,",
            "venue": "VIGFUSSON",
            "year": 2003
        },
        {
            "authors": [
                "C.C. DROVANDI",
                "A.N. PETTITT",
                "A. LEE"
            ],
            "title": "Bayesian Indirect Inference Using a Parametric Auxiliary Model,",
            "venue": "Statistical Science,",
            "year": 2015
        },
        {
            "authors": [
                "L. HANSEN"
            ],
            "title": "Exact linear rational expectations models: specification and estimation,",
            "venue": "SARGENT",
            "year": 1981
        },
        {
            "authors": [
                "A. KHAN",
                "J.K. THOMAS"
            ],
            "title": "Idiosyncratic shocks and the role of nonconvexities in plant and aggregate investment dynamics,",
            "year": 2008
        },
        {
            "authors": [
                "D. KRUEGER",
                "K. MITMAN"
            ],
            "title": "Macroeconomics and household heterogeneity,",
            "venue": "PERRI",
            "year": 2016
        },
        {
            "authors": [
                "P. KRUSELL",
                "JR A.A. SMITH"
            ],
            "title": "Income and wealth heterogeneity in the macroeconomy,",
            "venue": "Journal of political Economy,",
            "year": 1998
        },
        {
            "authors": [
                "LIU L"
            ],
            "title": "Full-information estimation of heterogeneous agent models using macro and micro data,",
            "venue": "PLAGBORG-M\u00d8LLER",
            "year": 2023
        },
        {
            "authors": [
                "M. PLAGBORG-M\u00d8LLER"
            ],
            "title": "Bayesian inference on structural impulse response functions,",
            "venue": "Quantitative Economics,",
            "year": 2019
        },
        {
            "authors": [
                "M. REITER"
            ],
            "title": "Solving heterogeneous-agent models by projection and perturbation,",
            "venue": "Journal of Economic Dynamics & Control,",
            "year": 2009
        },
        {
            "authors": [
                "J.J. ROTEMBERG"
            ],
            "title": "Sticky prices in the United States,",
            "venue": "Journal of Political Economy,",
            "year": 1982
        },
        {
            "authors": [
                "F. SMETS"
            ],
            "title": "Shocks and frictions in US business cycles: A Bayesian DSGE approach,",
            "venue": "WOUTERS",
            "year": 2007
        },
        {
            "authors": [
                "JR A.A. SMITH"
            ],
            "title": "Estimating nonlinear time-series models using simulated vector autoregressions,",
            "venue": "Journal of Applied Econometrics,",
            "year": 1993
        },
        {
            "authors": [
                "J.H. STOCK",
                "M. W"
            ],
            "title": "Forecasting Using Principal Components from a Large Number of Predictors,",
            "venue": "WATSON",
            "year": 2002
        },
        {
            "authors": [
                "T. WINBERRY"
            ],
            "title": "A method for solving and estimating heterogeneous agent macro models,",
            "venue": "Quantitative Economics,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Keywords: indirect inference, singular value decomposition, dynamic mode decomposition, heterogeneous-agent models\nJEL Classification Numbers: C13, C32, E1\n*We thank Corina Boar, Simon Gilchrist, Virgiliu Midrigan, and Tom Sargent for their helpful comments.\nar X\niv :2\n40 2.\n11 37\n9v 1\n[ ec\non .G\nN ]\n1 7"
        },
        {
            "heading": "1 Introduction",
            "text": "Over the past decade, the tremendous progress in developing models featuring rich heterogeneity with aggregate shocks has coincided with the proliferation of innovative and novel datasets at the household or individual level. These micro data were originally used to calibrate model parameters by matching cross-sectional moments of relevant variables (e.g. asset holdings, marginal propensity to consume, amongst others) at the model\u2019s stationary equilibrium. Subsequent advancements in computational methods opened the door to formal parameter estimation using aggregate timeseries data.1 Until recently however, the ability to leverage similar informational content in the entire distributions of the micro data to discipline the dynamics of these models remained elusive.2\nThis paper contributes to that literature by proposing an indirect inference strategy with a simple, fast and effective algorithm for approximating the model-implied likelihood of repeated cross-sections of micro data. The algorithm can easily be paired with a maximization routine for maximum-likelihood estimation, or any MCMC posterior sampling algorithm for Bayesian estimation. One differentiating feature of our strategy is its compatibility with sequence-space solution (Auclert et al. 2021a, Boppart et al. 2018), since most available methods for estimating models with micro-data require a solution in state-space form.3\nThe method relies on two key assumptions. The first is that the number of units (e.g households or individuals) in the repeated cross-sections are large. In other words, our data must be high dimensional. This a natural property of micro-data. The second is that the dynamics of the heterogeneous-agent model is approximately low rank \u2013 that the dynamics of the high-dimensional vector of observables can be well-approximated by relatively few factors. This appears to be the case in even the more complex heterogeneous-agent models of today. Under these two assumptions, indirect inference consists of treating a Dynamic Factor Model (DFM) as auxiliary to the intractable heterogeneous agent model of interest. Even so, the assumed high-dimensional nature of the data may render the DFM itself intractable. By leveraging and extending insights in Sargent\n1For example, Bayer, Born, and Luetticke (2024) estimate a medium-scale HANK model in state space using a Bayesian approach. Auclert, Rognlie, and Straub (2020) estimate a HANK model in sequence space by matching the impulse response to identified monetary policy shocks. Both approaches use only aggregate time-series data or time-series of cross-sectional moments.\n2To our best knowledge, the only available method is the full-information approach developed by Liu and Plagborg-M\u00f8ller (2023). 3Examples of state-space solution methods include Reiter (2009), Winberry (2018), Bayer and Luetticke (2020), and Ahn, Kaplan,\nMoll, Winberry, and Wolf (2017) for continuous-time models\nand Selvakumar (2023), we show that the likelihood implied by a reduced-rank first-order vector autoregression in the observables is an unbiased estimate for that of the DFM. This result paves the way to approximate the likelihood of the DFM and therefore the model of interest. We implement that computation using the Dynamic Mode Decomposition, a workhorse tool in the fluid dynamics literature, which provides a consistent estimate of the reduced-rank first-order VAR coefficients.4 Figure 1 summarizes the key idea of our indirect inference strategy.\nExisting methods To our best knowledge, there exist two ways to estimate a model solved with sequence-space methods. The first is to directly use the moving average representation, as employed by Auclert et al. (2021a). This approach is primarily suited for situations in which the observables are either aggregate data or only a few moments of the cross-sectional distribution. Using the entire distribution renders it infeasible. To see why, note that the MA representation approach requires stacking the full MT \u00d7 1 observation vector, where M is the number of units in the cross-section and T the number of periods in time. The associated covariance matrix is therefore of dimension MT \u00d7MT . For a conservative M = 300 and T = 120, the dimensions of the covariance matrix are 36, 000\u00d7 36, 0005.\nThe second is the use of Whittle likelihood approximation in frequency domain, as in Hansen and Sargent (1981) and Christiano and Vigfusson (2003). While estimation is feasible, its quality of the approximation relies on large T , which is unrealistic for existing micro-datasets. In contrast, our method relies on large M , which seems to us a far more satisfiable requirement.\n4See Brunton et al. (2015) and Brunton and Kutz (2022) for a reader\u2019s guide. 5For further discussion, see Auclert et al. (2021b) pp 28, footnote 31.\nApplication We illustrate our methodology by considering a small-scale HANK model that features both aggregate and cross-sectional shocks. We solve the model using the sequence-space Jacobian method of Auclert et al. (2021a) and demonstrate the finite-sample properties of our method via Monte-Carlo simulation. We compare finite-sample properties from our method (MicroDMD) with that of an estimation using only aggregate data (Agg). We find that MicroDMD has superior finite-sample properites over Agg, demonstrating the value of information contained in micro data6. We also implement an estimation using aggregate data plus a few cross-sectional moments (Agg+). Though the finite-sample properties improve over Agg, MicroDMD appears superior yet again, suggesting the existence of additional informational content beyond simple cross-sectional moments of the micro-data.\nFinally, we compare the results of MicroDMD compared to an estimation using Whittle Likelihood MicroFD. Again, we document that MicroDMD has more favorable finite-sample properties than MicroFD. One reason for this is that consistency in MicroFD requires large T .\nThe rest of the paper is structured as follows. In Section 2, we lay out our estimation framework and formally justify our method. In Section 3, we illustrate our method with a small-scale HANK model and compare its performance with other conventional approaches. Section 4 discusses the robustness of our method and extends our method for Bayesian inference. Section 5 concludes with suggestions for future research."
        },
        {
            "heading": "2 Estimation framework",
            "text": "Consider a fully-specified structural general equilibrium model with heterogeneous agents and aggregate shocks, called M. Let yi,t \u2208 R denote the observable (e.g. consumption) of individual i at time t. Moreover, let yt \u2208 RM\u00d71 denote a vector of individuals\u2019 consumption at time t.\nAssumption 1. We make the following assumptions on the available micro-data{yt}Tt=1\n1. The observations are repeated cross-sections of individuals from a given sampling scheme\n2. The observation vector yt is high dimensional (i.e M is large)\n6This finding underscores the insights of Liu and Plagborg-M\u00f8ller (2023)\nAssumption 2. The heterogeneous-agent model M is approximately low rank (N )\nBoth sets of assumptions are crucial to the theoretical results that justify our algorithm. The first condition in Assumption 1 requires that we sample individuals from the same states over time. Practically speaking, we have in mind a dataset where individuals are grouped and binned by their state variables implied by M. It is important to our theory that we do not have any \u2019gaps\u201d in the dataset, i.e. that for a given grid of states, there is always at least one unit in each grid point. Note that the grid need not cover the whole state-space of the model, we only require that were it to be included in our dataset, there are no \u201dmissing datapoints\u201d in the time-series. We discuss extensions to this data requirement in Section 4.\nThe second condition in Assumption 1 requires that we essentially sample \u201denough\u201d individuals. In our simulated example below, we set M = 300, which we believe is also realistic in empirical settings.\nThe first condition in Assumption 2 implies that micro-data generated by M is approximately low rank This assumption may seem a priori, since one of the main benefits of heterogeneous-agent models is that they do not aggregate, providing rich insights into the effects of heterogeneity on macroeconomic outcomes and vice versa. Crucially however, we do not require that the model is low rank, but only that it is approximately low rank.\nThis condition is easily verifiable for any model. For example, consider a simulated data matrix Y\u0303 = [y\u03031, . . . , y\u0303T ] \u2208 RM\u00d7T from M. Taking the (reduced) singular value decomposition provides Y\u0303 = U\u0303\u03a3\u0303V\u0303 \u22a4 where U\u0303 \u2208 RM\u00d7M , \u03a3\u0303 \u2208 RM\u00d7M is a diagonal matrix and V\u0303 \u2208 RT\u00d7M .\nDefinition 1. The singular values {s1, s2, . . . , sM} of Y\u0303 are the elements on the diagonal of \u03a3\u0303 and are listed in decreasing order. Y\u0303 is said to be approximately low rank if there exists an N such that si \u226b sj for all positive integers i \u2264 N < j and N \u226a M .\nThe definition characterizes the rankness of the matrix by its singular values. For example, in the special case that rank(Y\u0303) = N then si >> 0 for all i \u2264 N and si = 0 otherwise. It also suggests an intuitive way to study the rankness of the data matrix (and therefore the model that generated it) by plotting its singular values. For an approximately low rank Y\u0303, there would only be a small number of large singular values, and the rest relatively small.\nIs this a plausible assumption for heterogeneous-agent models? We argue that it is. Figure\n2 plots the singular values for data generated by four well-known heterogenous-agent models: Krusell-Smith model (Krusell and Smith 1998), a One-Asset HANK (Section 3.1), a Two-Asset HANK (Auclert et al. 2021a) and a heterogeneous-firm model (Khan and Thomas 2008). For all four models, we generate repeated cross-sectional data with M = 300 and T = 10, 000.7 The figure shows that all four models appear to be approximately low rank. Krusell-Smith appears to have N = 2, one very large singular value and one smaller one, with the rest being negligible. The One-Asset HANK model appears to have a slightly higher approximate rank. Though the Two-Asset HANK has a much larger rank than the Krusell-Smith or the One-Asset HANK models, with N = 7, it is still approximately low rank. The same applies for the model with heterogeneous firms. That even complex heterogeneous agent models feature an approximately low-rank structure suggests the generality with which Assumption 2 holds in the existing class of heterogeneous-agent models.\nFrom a theoretical standpoint, Bayer et al. (2024) provide an intuitive discussion for why one might expect the possibility of a significant dimension reduction using insights from the sequence space method.8 In Section 4.1, we provide a sufficient condition on equilibrium matrices of M for whether there exists a low-rank representation. Furthermore, Assumption 2 appears to be plausible empirically. Sargent and Selvakumar (2023) construct a dataset of quarterly time-series of percentiles of private income, post-tax income and consumption from the Consumer Expenditure Survey (CEX) and show that the data matrix is approximately low rank.\nThe \u201dindirect inference\u201d part of our strategy comes from the use of an auxiliary model to approximate M.9 We leverage Assumption 2 to consider a Dynamic Factor Model (DFM) with N factors as such a model, where the approximation quality improves the closer M is to being exactly low rank.\nMoreover, a question remains as to how one might calculate the likelihood of the DFM when M is large. For example, computational feasibility might preclude estimation for a larger number of observables.10 In the next section, we provide a solution that builds upon results of Sargent and Selvakumar (2023).\n7A short description of the simulations can be found in Appendix C. The One-Asset HANK serves as our benchmark model and is described in Section 3.1.\n8For a full discussion, see Bayer et al. (2024) Appendix C.2 9An auxiliary model is a model that well-approximates M but whose likelihood is easier to compute.\n10See Section 2.2 for further discussion."
        },
        {
            "heading": "2.1 Computing the likelihood of high-dimensional factor models",
            "text": "This section further develops insights in Sargent and Selvakumar (2023) to compute the likelihood of a high-dimensional factor model with N factors. Let xt \u2208 RN\u00d71 be a vector of unobserved factors at time t = 1, . . . , T . We will suppose that they are generated by the linear state-space model\nxt+1 = Axt+Cwt+1 (1)\nyt = Gxt+vt,\nwhere shocks wt+1 \u223c N (0, IN\u00d7N ), measurement errors vt \u223c N (0,R) and ws \u22a5 v\u03c4 for all s, \u03c4 ; here A \u2208 RN\u00d7N , C \u2208 RN\u00d7N and G \u2208 RM\u00d7N and R \u2208 RM\u00d7M . In addition, we make the following assumptions.\nAssumption 3. Dynamic Factor Model (1) satisfies the following restrictions\n1. M \u226b N\n2. G,A has full column rank (i.e. rank(G) = rank(A) = N ) 3. \u2225\u2225G\u22a4G\u2225\u2225 = O(M), where \u2225\u00b7\u2225 denotes the Frobenius norm\n4. R = \u03c32vIM for some \u03c3v > 0\nThe first condition states that a large number of observables are generated by relatively few factors. The second condition requires that the columns of G and A to be linearly independent. In\na HA model, the rows of G represents the policy function of different agents, so this condition is equivalent to assuming enough heterogeneity in the cross-section. For example, in a two-agent New Keynesian model with many shocks, rank(G) = 2 and the condition is not satisfied. This condition highlights our identification strategy which exploits the rich heterogeneity in the cross-section. The assumption on A simply means that there is no redundant state.\nThe third condition concerns the asymptotic property of the model when the number of observables grows and is standard in the factor analysis literature (e.g. Chamberlain and Rothschild 1982, Stock and Watson 2002, Bai and Ng 2006). We have in mind an underlying \u201dgrand\u201d model that defines the measurement equation for each potential observable. For instance, a HANK model includes an infinite number of consumption policies, one for each household. In this context, the assumption means that the second moment of the cross-sectional consumption policy exists and the sampling of the observables is purely random such that the Law of Large Number holds. Lastly, the fourth condition is the standard assumption that the measurement error is homoscedastic. We make this for ease of exposition, though it can be loosened if necessary.\nBefore delving into the large-M theory, let\u2019s recall the celebrated VAR representation of linear\nstate-space model:\nProposition 1. There exists an infinite-order VAR representation of DFM (1) in yt, given by\nyt = \u221e\u2211 j=1 B\u221ej yt\u2212j +at (2) E[at y\u22a4t\u2212j ] = 0 for all j \u2265 1 E[at aTt ] =: \u2126 B\u221ej = G(A\u2212KG)j\u22121K \u2200j \u2265 1 (3) rank(B\u221ej ) = N \u2200j \u2265 1\nwhere K = A\u03a3\u221eG\u22a4\u2126\u22121 and \u03a3\u221e = CC\u22a4+KRK\u22a4+(A\u2212KG)\u03a3\u221e(A\u2212KG)\u22a4\nProposition 1 demonstrates the formula for forming the best forecast for yt, given the information up to time t\u2212 1. In general, one needs to use the whole history yt\u22121 to form the forecast, and finite truncation of the history induces non-trivial efficiency loss. Nonetheless, we show that this is not necessary if the number of observables is large.\nLemma 1. Under Assumption 3, as the number of observables grows (M \u2192 \u221e), the matrix A\u2212KG \u2192 0\nCorollary 1. When A\u2212KG = 0, E[xt+1 |yt] = Kyt and E[yt+1 |yt] = GKyt\nWhat\u2019s the intuition? When the number of observables is large, one can estimate the hidden state xt accurately using only information contained in yt. Then by the Markovian property of the model, one can use merely the time-t information to form the best forecast for yt+1.\nWith these preliminary results done, we now state the two main theoretical results justifying\nour estimation procedure.\nTheorem 1. Suppose Assumption 3 holds. Then as M \u2192 \u221e,\n1. B\u221ej \u2192 0 \u2200j \u2265 2. Furthermore, lim sup(M j\u22121 \u2225\u2225B\u221ej \u2225\u2225) < \u221e \u2200j \u2265 2\n2. The infinite-order VAR representation of DFM (1) collapses to a first-order VAR representation where\nyt = B \u221e 1 yt\u22121+at (4) E[at y\u22a4t\u22121] = 0 E[at aTt ] =: \u2126 B\u221e1 = GK \u2200j \u2265 1 rank(B\u221e1 ) = N\nTheorem 1 states that in a high-dimensional DFM, the observables yt has a low-rank VAR(1) representation. This motivates the use of the first-order VAR (4) to evaluate the DFM-implied likelihood, bypassing any Kalman filter computation. Theorem 2 validates this algorithm.\nTheorem 2. Suppose Assumption 3 holds. Let \u2113DFM (Y;A,C,G,R) denote the likelihood of Y implied by DFM (1), and let \u21131(Y;A,C,G,R) denote that implied by the first-order VAR (4). Then as M \u2192 \u221e,\nE |\u2113DFM (Y;A,C,G,R)\u2212 \u21131(Y;A,C,G,R)| \u2192 0 (5)\nIn other words, the bias from using the first-order VAR to evaluate the likelihood vanishes asymptotically. Therefore, one may approximate the likelihood of the DFM by computing the likelihood of the first-order VAR, with its approximation quality improving as M increases."
        },
        {
            "heading": "2.2 Reduced-rank first-order VAR",
            "text": "Our theoretical results in the previous subsection lay the groundwork for a fast algorithm that computes the likelihood of DFM (3) that, under Assumption 1 and 2 is a good approximation for the likelihood implied by M. One final question remains of how to compute the rank-N first-order VAR coefficients B\u221e1 , given the model M. Anderson and Rubin (1949) and Anderson (1951) were among the first to propose strategies to estimate reduced-rank VARs in a two step procedure. The first is to estimate the unrestricted OLS coefficient matrices, and then impose the restrictions in the second step. We pursue a different, computationally efficient route by building on the Dynamic Mode Decomposition (DMD). The DMD, introduced by Schmidt and Sesterhenn (2010) and later developed by Tu et al. (2014), is a workhorse tools in the fluid dynamics literature. Existing applications of the DMD also include epidemiology, neuroscience and video processing (Brunton and Kutz (2022)).\nWe use and extend part of the the DMD algorithm to suit our own purposes in the following way. Our approach involves two sets of data, the empirical data {yt}T+1t=1 and simulated data from the model {y\u0303t}J+1t=1 . Using the simulated data, create two matrices by stacking the observations of y\u0303t for t = 1, . . . , J + 1 in the form11\nY\u0303 = [y\u03031, y\u03032, . . . , y\u0303J ] Y\u0303 \u2032 = [y\u03032, y\u03033, . . . , y\u0303J+1]\nFor a desired rank, call it N , the DMD estimates the reduced-rank VAR associated with the\nsimulated data by solving\nB\u0303 = argmin rank(B)=N \u2225\u2225\u2225Y\u0303\u2032 \u2212BY\u0303\u2225\u2225\u2225 (6) where \u2225\u00b7\u2225 denotes the Frobenius norm. To compute B\u0303, represent Y with a reduced Singular Value Decomposition (SVD)\nY\u0303 = U\u0303\u03a3\u0303V\u0303 \u22a4\nwhere U\u0303 is M \u00d7M , \u03a3\u0303 is M \u00d7M and V\u0303 is T \u00d7M . We compress Y\u0303 by using its N largest singular\n11Within the context of this paper, we implement the DMD on simulated data. In more conventional applications, they are real-world data. For example, in Sargent and Selvakumar (2023) they are percentiles of the real consumption distribution.\nvalues:\nY\u0303 \u2248 U\u03a3V\u22a4,\nwhere U = U\u0303[:, : N ], \u03a3 = \u03a3\u0303[: N, : N ] has N singular values as its only non-zero entries, and V\u22a4 = V\u0303 \u22a4 [: N, :]. Here U is M \u00d7 T , V is T \u00d7N , \u03a3 is N \u00d7N , and V\u22a4 is N \u00d7 T .12\nWe use this reduced-order SVD approximation of Y\u0303 to compute13\nB\u0303 = Y\u0303 \u2032 Y\u0303 + , (7)\nwhere by construction B\u0303 is rank N . The covariance matrix of the residuals, a\u0303t = y\u0303t \u2212 B\u0303y\u0303t\u22121, is computed via\n\u2126\u0303 = 1T\u22121 T\u2211 t=1 a\u0303ta\u0303 \u22a4 t (8)\nFinally, to calculate the likelihood, first compute the residuals with empirical data a\u0302t =\nyt\u2212B\u0303 yt\u22121. Then the log-likelihood is standard, given by\nf(y1, . . . ,yT+1) = T\u2211 t=1 \u22121 2 log(2\u03c0)\u2212 1 2 log det(\u2126\u0303)\u2212 1 2 a\u0302\u22a4t \u2126\u0303 \u22121a\u0302t (9)\nA discerning reader at this point might question why not evaluate the likelihood of DFM (1) with the Kalman filter? The answer is that evaluating the likelihood via the Kalman filter requires knowing the matrices A,C,G,R, which itself must be estimated from the simulated data. To see why this might be a problem, consider an example where M = 300 and M is approximately rank N = 2. Then estimating D requires estimating 606 parameters of A,C,G,R14 Since they will also depend on structural parameters, one would need to insert an additional loop in the estimation procedure, making it highly computationally inefficient.\n12Note that all we need here is a truncated SVD, which can be very efficiently computed using existing machine-learning packages (e.g. scikit-learn).\n13See Sargent and Selvakumar (2023, sec. 2.1) for the full details of the DMD algorithm. 14G has 300\u00d7 2 parameters, R has one parameter, A has 2 parameters and C has 3 parameters."
        },
        {
            "heading": "2.3 Estimation strategy",
            "text": "The above sections set out the theoretical and computational arguments for our estimation strategy. To recap succinctly, the logic is as follows: Assumption 2 implies that a dynamic factor model with N factors is a plausibly good auxiliary model with which to approximate the likelihood of M. Yet, it is unclear how one should fit such a DFM and compute its likelihood. Assumptions 1 and 3 imply that such a likelihood can be approximated by that of a rank-N first-order VAR in yt; and that the approximate quality improves as M becomes large. We compute the rank-N first-order VAR and the associated likelihood by extending the Dynamic Mode Decomposition algorithm.\nAlgorithm 1 Likelihood approximation\n1. Fix some structural parameters \u03b8\n2. Simulate time-series y\u03031(\u03b8), . . . , y\u0303J+1(\u03b8) from M for a large J and create data matrices Y\u0303(\u03b8) and Y\u0303 \u2032 (\u03b8)\n3. Choose the rank, N , as discussed in section 2.4\n4. Calculate B\u0303(\u03b8) and \u2126\u0303(\u03b8) in (7) and (8) 5. Approximate the log- likelihood f(y1, . . . ,yT+1 |\u03b8) implied by M by computing (9)\nAlgorithm 1 presents pseudo-code for approximating the likelihood of observable data {yt}\nimplied by M."
        },
        {
            "heading": "2.4 How to choose N?",
            "text": "A natural question in our strategy is what is approximate rank of M. In this section, we suggest a multitude of heuristic and quantitative procedures that offers insights into an appropriate choice of N .\nGiven a simulated data set Y\u0303 from M, the common heuristic test used by Dynamic Mode Decomposition practitioners is to plot the singular values like in Figure 2.15. An example of this can be seen in Figure 3.\nGavish and Donoho (2014) adopt a more quantitative approach and find the optimal threshold N . Assuming a generating model like (1) with measurement error covariance matrix R = \u03c3IM ,\n15See, for example, Brunton and Kutz (2022, sec. 7.2))\nthey show that the optimal threshold is\nN = \u03bb(MT ) \u221a T\u03c3\nwhere \u03bb(\u03b2) = \u221a 2(\u03b2 + 1) + 8\u03b2\n\u03b2+1+ \u221a \u03b22+14\u03b2+1 . The authors prove that for a fixed low-rank (say\nN\u2217) factor model, the choice of N dominates the rule-of-thumb approach in terms of asymptotic mean squared error, when M,T \u2192 \u221e such that MT \u2192 \u03b2 \u2208 (0, 1].\nFrom the principle components literature, Bai and Ng (2002) show that consistent estimation of\nthe number of factors can be attained by minimizing the information criterion16\nIC(n) = V (n) + n\n( M + T\nMT\n) log ( MT\nM + T\n) (10)\nwhere V (n) = (MT )\u22121 \u2211M\ni=1 \u2211T t=1(a n it) 2.\nFinally, we propose a method for choosing N for our particular setting. Given simulated data Y\u0303, and a fixed N , calculate the N -rank VAR coefficient matrix B\u0303N (where we note the dependence on N for clarity). Then, calculate the VAR residuals by\na\u0303t = y\u0303t \u2212 B\u0303N y\u0303t\u22121\nDenote R2m,N as the individual-level R 2 for the VAR regression for m = 1, . . . ,M (i.e. for each\nrow of yt), given by\nR2m,N = 1\u2212 \u2211T t=2 a\u0303 2 m,t\u2211T\nt=2 ym,t\u2212 1 T \u2211T t=2 ym,t\u22121\nwhere a\u0303m,t is the m-th element of a\u0303t. Then, calculate the aggregate R2N of the approximating\nmodel by a weighted sum of the cross-sectional R2m for m = 1, . . . ,M .\n16Though the analysis in Bai and Ng (2002) is done for principle components estimation of factor models, the same theory applies to any other consistent estimation procedure, as M,T \u2192 \u221e.\nR2N = 1 M M\u2211 m=1 w(m)R2m,N (11)\nwhere w(m) is some weighting function.17\nOur proposed N is the value above which the aggregate R2 no longer increases. Indeed, if M is indeed approximately low rank, R2N convereges as N increases. Intuitively, this signals that increasing the number of factors in the DFM does not improve the forecasting ability of the approximate model. We therefore select the appropriate N such that the difference R2N \u2212R2N\u22121 is sufficiently close to zero.\nModel validation An additional implication of our Proposition 1 is that the VAR residuals a\u0303t\nmust be serially uncorrelated. We use this result as an additional check to validate our choice of N . Importantly, there is nothing in the first-order VAR that imposes such a restriction, it follows from the innovations representation of the DFM (1). To check this restriction, we construct the sample covariance matrix (12) and check how close it is to the zero matrix.\nE\u0302[at+1 a \u22a4 t ] =\n1 T T\u2211 t=1 a\u0303t+1a\u0303 \u22a4 t (12)"
        },
        {
            "heading": "3 Illustration with a canonical HANK model",
            "text": "We consider a small-scale HANK model as the laboratory of our method. The model features both aggregate shocks (e.g. TFP) that are common to RA business cycle models (e.g., Christiano, Eichenbaum, and Evans 2005, Smets and Wouters 2007) and a cross-sectional shock that directly affects the income distribution (e.g. Bayer et al. 2024)."
        },
        {
            "heading": "3.1 Model",
            "text": "Time is discrete and runs forever, t = 0, 1, . . . .\n17In our example below, we fix an equal weighting scheme, and sample individuals from the stationary distribution of M.\nHousehold There is a unit measure of infinitely-lived households in the economy. Households face idiosyncratic risk to their labor productivity e and also transition risk to their employment status s \u2208 {E,U}. For simplicity, we assume that the productivity process is independent of the employment status and that both idiosyncratic risks are exogenous to the business cycle.18 As a result, the productivity distribution is time-invariant. The average productivity E(e) is normalized to 1.\nHouseholds can save and borrow through a risk-free asset, subject to an ad-hoc borrowing constraint a \u2265 a. The Bellman equation of a household with asset a, productivity e, and employment status s at time t is given by:\nVt(a, e, s) = max c,a\u2032\n{ c1\u2212\u03c3\n1\u2212 \u03c3 \u2212 \u03c6ht(e)\n1+\u03d5\n1 + \u03d5 + \u03b2Et\n[ Vt+1(a \u2032, e\u2032, s\u2032)|s, e ]}\nc+ a\u2032 = (1\u2212 \u03c4t)yt(e, s) + (1 + rt)a\nyt(e, s) = [1{s = E}+ 1{s = U} \u00b7 b]wtht(e)e\na\u2032 \u2265 a\nwhere rt is realized real return of the asset at time t, \u03c4t is labor tax, and yt is real labor income. When employed (s = E), the household supplies its labor service ht(e) to the unions at real wage per efficiency unit wt and earns yt(e, E) \u2261 wtht(e)e. The hour choice ht(e) is determined by the union through a time-varying allocation rule of the form:\nht(e) = nt e\u03bet\u222b\nsit=E e1+\u03betit di\n\u2200e\nwhere nt \u2261 \u222b sit=E ht(eit)eit di is the total efficiency unit of labor. The variable \u03bet governs the dispersion of labor income, with the uniform allocation rule nested in the case \u03bet = 0. We will assume that \u03bet follows an AR(1) process and call it the income-dispersion shock. Finally, when unemployed (s = U ), the household receives unemployment benefits from the government which replaces a fraction b of her labor earnings, so that yt(e, U) \u2261 b \u00b7 wtht(e)e.\n18In particular, the productivity still evolves during unemployment.\nFirms Final-goods firms operate in a perfectly competitive market. They demand labor services from the unions and transform them into the final goods using a CES technology with elasticity of substitution \u03f5. The firm\u2019s problem is given by:\nmax nit,yt\nPtYt \u2212 \u222b Witnit di\ns.t. Yt = e Zt (\u222b n \u03f5\u22121 \u03f5 it di ) \u03f5 \u03f5\u22121\nwhere Zt is TFP shock. In the symmetric equilibrium where Wit = Wt, nit = nt \u2200i, we have the real wage equation\nwt \u2261 Wt Pt = eZt\nLabor unions A continuum of labor unions operate in a monopolistically competitive market. Each union i sets its real wage wit subject to a quadratic adjustment cost a la Rotemberg (1982) and demands labor nit from the employed households to satisfy the demand from the firms. Following Alves and Violante (2023), we simplify the union\u2019s problem by assuming that the union maximizes the utility of a representative employed household, subject to the exogenous labor allocation rule.\nSpecifically, the union\u2019s problem is given by:\nmax wit+k,nit+k Et \u221e\u2211 k=0 \u03b2k {[ (CEt+k) \u2212\u03c3(1\u2212 \u03c4t)wit+k \u2212 \u03c6\u2126t+kH\u0304\u03d5t+k ] nit+k \u2212 \u03f5 2\u03baw log ( wit+k wit+k\u22121 \u03a0t+k )2}\ns.t. nit+k = ( wit+k wt+k )\u2212\u03f5 nt+k\nwhere CEt+k is total consumption of employed households, H\u0304t+k is total labor hours, \u2126t+k \u2261\u222b sit=E e\u03betit di/ \u222b sit=E e1+\u03betit di is the labor wedge associated with the allocation rule, and \u03a0t+k is the gross inflation rate. In the symmetric equilibrium, the first-order condition leads to the wage Phillips curve:\nlog \u03a0wt = \u03baw [ \u03c6\u21261+\u03d5t n \u03d5 t \u2212\n\u03f5\u2212 1 \u03f5\n(1\u2212 \u03c4t)(CEt )\u2212\u03c3wt ] nt + \u03b2 log \u03a0 w t+1\nGovernment Monetary policy follows standard Taylor rule:\n(1 + rnt+1) = (1 + rss)(\u03a0t) \u03d5\u03c0(Yt) \u03d5yev r t\nGovernment maintains balance budget every period by adjusting the labor tax \u03c4t:\nrtBss + \u222b sit=U yit di = \u03c4t \u222b yit di\nAggregate shocks There are three aggregate shocks: TFP shock, monetary policy shock, and income dispersion shock. Each follows an independent AR(1) process.\nZt = \u03c1zZt\u22121 + \u03c3z\u03f5 Z t vrt = \u03c1rv r t\u22121 + \u03c3r\u03f5 r t\n\u03bet = \u03c1\u03be\u03bet\u22121 + \u03c3\u03be\u03f5 \u03be t\nEquilibrium A rational expectation equilibrium consists of a sequence of policy functions {ct, at, ht}, a sequence of value functions {Vt}, a sequence of prices {wt, rnt ,\u03a0t,\u03a0wt , \u03c4t}, a sequence of aggregate objects {Yt, CEt ,\u2126t, H\u0304t, nt}, a sequence of distribution {Ft}, a sequence of exogenous states {Zt, vrt , \u03bet}, and a sequence of beliefs over prices such that\n1. Given the sequence of value functions, prices, and policy functions, the household Bellman\nequation holds.\n2. Given the sequence of beliefs over prices, all agents optimize.\n3. The evolution of the distribution is consistent with the policy.\n4. The sequence of beliefs over prices is rational.\n5. All markets clear."
        },
        {
            "heading": "3.2 Solution in sequence-space",
            "text": "We employ the sequence-space Jacobian (SSJ) method of Auclert et al. (2021a) to obtain a linearized solution of the model in Section 3.1. Although the original SSJ method is developed for computing\nthe aggregate dynamics, it can be easily extended to obtain a solution for the micro consumption dynamics. Let ct = (c1,t, . . . , cM,t)\u22a4 be the vector of cross-sectional consumption and \u03f5t \u2208 Rr be the vector of fundamental shocks at time t. In our model, \u03f5t consists of three shocks: TFP shock, monetary policy shock, and income dispersion shock. We have the following proposition.\nProposition 2. In the linearized equilibrium, ct has a moving average (MA) representation\nct = css+ \u221e\u2211 j=0 \u03a8cj\u03f5t\u2212j (13)"
        },
        {
            "heading": "Furthermore, the MA coefficient matrix \u03a8cj is given by",
            "text": "\u03a8cj = \u2211 p\u2208P J cpF jIpe\nwhere P denotes the set of aggregate inputs that enter the household\u2019s problem and\n\u2022 J cp \u2208 RM \u00d7R\u221e is the cross-section of gradients of consumption wrt. the future path of aggregate\ninput p\n\u2022 Ipe \u2208 R\u221e\u00d7Rr is the impulse response functions of aggregate input p\n\u2022 F is the shift-forward operator\nIn light of Proposition 2, simulation of the micro consumption dynamics is straightforward, and computation of the MA coefficient matrices is trivial because J cp and I p e are products of the SSJ method.19 In practice, we truncate the horizon at T = 300."
        },
        {
            "heading": "3.3 Calibration",
            "text": "The model is in quarterly frequency. As the purpose of the model is to illustrate our method, we choose a set of parameters directly from the literature. In the steady state, we fix the annual real rate at 2% and set the (annualized) government debt-to-GDP ratio to be .80. The persistent income process is AR(1) and we use the parameters estimated by Krueger, Mitman, and Perri (2016). The EU rate is 6% and the UE rate is 90%, leading to an unemployment rate of 6.25%.20 The slope of the\n19The gradients J cp are computed by backward iteration in the first step of the \u201dFake news algorithm\u201d. 20Our choice is consistent with JOLTS. Due to our timing assumption, the transition rates should be interpreted as the effective rates\nthat take into account the possibility of finding a job within a quarter.\nwage NKPC is set to be .14, consistent with the estimate in Beraja, Hurst, and Ospina (2019). The persistence and standard deviation of the income dispersion shock is taken from Bayer et al. (2024). Table 1 reports the full calibration."
        },
        {
            "heading": "3.4 Estimation",
            "text": "We estimate three model parameters [\u03baw, \u03d5\u03c0, \u03d5y] and six shock parameters [\u03c1z, \u03c3z, \u03c1r, \u03c3r, \u03c1\u03be, \u03c3\u03be] using our method outlined in Algorithm 1, which we label MicroDMD. The observables are a simulated dataset of repeated cross-sections of individual (log) consumption, according to (13). The dimensions of the observables is M = 300, and T = 120. To replicate a realistic dataset, we randomly sample the 300 individual states from the stationary distribution and add i.i.d. measurement errors to the data. The measurement error accounts for 20% of the total variation and its standard error is also estimated along with the parameters of interest. For the simulation step, we set J = 10, 000.\nChoosing N To choose the N of the auxiliary DFM, we simulate time-series of consumption for M = 300 households, each of length T = 10000. We demean the time-series for each household and stack them vertically to create the simulated data matrix Ysim. We perform the battery of tests\noutlined in Section 2.4 to infer an appropriate N . Figure 3 shows the 10 largest singular values from the data matrix Ysim. There are 2 dominant singular values, the third has value 0.025. The rest of singular values are 0.005 and below; we compute that \u03c3(Ysim, 3) = 0.02\nTable 2 computes the R2 statistic in equation (11) and the information criterion of (10) for an increasing N . The first row of the table shows that the R2 doesn\u2019t increase in N after N = 3. It suggests that the first-order VAR is no better at predicting yt+1 given yt if we set N > 3 compared to N = 3. The information criterion, which penalizes large N , is shown in the second row. It falls until N = 3 and then increases again, making N = 3 seemingly the appropriate choice. Finally, we study the residuals associated with the rank-reduced first-order VAR. The third row computes the maximum absolute autocovariance of the VAR residuals at, computed via (12). We find shows relatively significant autocorrelation for N = 1, 2, suggesting that N = 1, 2 is inadequate in satisfying the assumptions that define the first-order VAR. The maximum autocovariance is close to zero, 2.1e\u22124 for N = 3, and remains so for larger N .\nFinally, computing the optimal threshold formula from Gavish and Donoho (2014), with \u03b2 =\nM T = 0.03 gives N = 3. All of these statistics considered, we set N = 3."
        },
        {
            "heading": "3.5 Simulation Results",
            "text": "Figure 5 presents the finite-sample distribution of estimates from our maximum-likelihood estimation of MicroDMD, calculated using 500 Monte-Carlo samples. Table 3 shows the mean parameter estimates and standard deviations from the Monte-Carlo samples. The mean of our estimators are remarkably close to the true values, except for the standard deviation of monetary policy (MP) shock which we underestimate. There are two reasons why the identification power for the size of the MP shock is relatively weak. First, due to the accommodative Taylor rule, the MP shock has small effect on consumption dynamics. Second, since we draw the individuals from the ergodic distribution, they have a similar level of asset holdings, limiting the differential consumption responses from the capital income channel. Moreover, the distributions of the estimates appear well-behaved with small standard deviations around the mean, even with only 500 MC samples. In the next sections, we compare our model with other typical estimation strategies and within that context highlight the benefit of using all the information available in the micro-data."
        },
        {
            "heading": "3.6 Comparison with estimation using aggregate data",
            "text": "Aggregate data only For comparison, we estimate the model parameters using aggregate data in two ways. The first is implemented using the MLE procedure by Auclert et al. (2021a) with only aggregate data (which we label Agg). The aggregate data consists of output, inflation, and nominal rate series, each of length T = 120. For a fair comparison with our method, we add measurement errors to the aggregate data which accounts for 10% of the total variation.21 The result of this estimation is presented in the Agg columns of Table 3. For most parameters, except the size of MP shock and income dispersion shock, the Agg performs worse than MicroDMD both in terms of\n21Since the Auclert et al. (2021a) method computes the likelihood of the aggregate data exactly, without measurement errors, their method will definitely deliver a better estimate than ours. Aruoba et al. (2016) argues that measurement error accounts for 20% of the variation in official US GDP measures. Thus, we view the 10% measurement error as a useful benchmark.\nbias and standard error. Figure 6 shows the distribution of the estimates. It shows that that the finite-sample distribution is much more dispersed and ill-behaved than our method.\nAggregate data plus cross-sectional moments Next, we compare our method against the population approach of including micro data into the estimation by constructing time-series for a few cross-sectional moments (e.g. Bayer et al. 2024 and Mongey and Williams 2017). We label this approach Agg+. The main advantage of this method is its simplicity and speed, since the likelihood of aggregate time-series can be efficiently computed via Kalman filter or using the full variance-covariance matrix in the same way as Agg. However, the ex-ante static aggregation of micro data may induce unnecessary information loss. In principle, MicroDMD makes better use of the micro data by utilizing the DMD algorithm to extract the most informative dynamic structures underlying the micro data.\nTo illustrate this point, we append a cross-sectional moment, the variance of log consumption, to the macro data and redo the aggregate estimation exercise. The results are reported in the Agg+ columnds of Table 3. The inclusion of cross-sectional moment brings the mean estimate closer to the truth and substantially lowers the standard error (compared to Agg), most notably for the estimates of the slope of the wage NKPC and the TFP shock process. Nevertheless, the performance of the estimator is still significantly worse than our method, suggesting that our method retains cross-sectional information beyond simple moments.\nDo these these differences in the estimated parameters translate to meaningful differences in the objects that macro-economists care about? Figure 4 suggest that they do. It plots the impulse responses of aggregate output to the three shocks in the model: TFP, monetary policy and the income dispersion shock. For all three shocks the width of the confidence bands for MicroDMD is significantly reduced compared to both the Agg and Agg+.\nOverall, the results suggest that our method which exploits the rich information contained in the micro data is better able to recover the true parameters of the model, and generally with a lower standard error in finite samples. The results emphasizes the advantage of using micro data in the estimation of heterogeneous-agent models.\nNOTE. The impulse response is wrt. 1 standard deviation shock. Red line is the true value and black line is the mean of the estimates. Shaded area is 90 percent confidence interval computed from 500 Monte Carlo draws.\nNOTE. The statistics are computed using 500 Monte Carlo draws."
        },
        {
            "heading": "3.7 Comparison with other estimation methods using micro data",
            "text": "Frequency-domain estimation The difficulty of exact likelihood evaluation of the micro data is the high dimensionality (M \u00d7 T ) of the variance-covariance matrix. One way to tackle this computational challenge is to evaluate the likelihood in the frequency-domain using the Whittle approximation, as in Hansen and Sargent (1981), Christiano and Vigfusson (2003), and PlagborgM\u00f8ller (2019). We label this method MicroFD. The Whittle approximation decomposes the entire M \u00d7 T -dimensional variance-covariance matrix into the sum of M -dimensional frequency-specific matrices. Thanks to the Fast Fourier Transform, the decomposition and associated likelihood evaluation is fast and only requires the sequence-space solution of the model. A key difference between the frequency-domain estimation method and ours is that it requires large T for accurate approximation, while our method requires large M . Since in reality the cross-sectional dimension of the micro dataset is usually larger than the time dimension, we argue that our method is more suitable in practice. We apply the frequency-domain estimation method to the simulated micro datasets and report the results in the MicroFD columns of Table 4.22 The results suggest that our method dominates the frequency-domain estimation method both in terms of bias and standard error, consistent with the asymptotic theory.\n22The details of the estimation procedure can be found in Appendix B.1.\nNOTE. The statistics are computed using 500 Monte Carlo draws.\nFull-information estimation Liu and Plagborg-M\u00f8ller (2023) develops a full-information likelihood based approach for the estimation of heterogeneous-agent models. Our method is different in two dimensions. First, their method requires both macro and micro data. The macro data is used to infer the conditional distribution of the aggregate states which pin down the (conditional) likelihood of micro data. In contrast, our method can do inference base solely on micro data and can still be intuitively extended to incorporate macro data, a point that we further discuss in Section 4. Second, to infer the aggregate states, their method requires a state-space solution of the model computed using dimension-reduction algorithms such as Winberry (2018). On the other hand, since our indirect inference strategy is simulation-based, we only require the ability to simulate from the model and hence can accommodate sequence-space solutions as well as state-space solutions of the model. That being said, when Liu and Plagborg-M\u00f8ller (2023)\u2019s method is applicable, the full-information nature guarantees that their estimator is more efficient.\nTo summarize, our method provides a practical middle ground between the benchmark fullinformation method and conventional methods \u2013 it enjoys the substantive efficiency gain from using micro data but is no harder to apply \u2013 it can be coded up in only a few lines of code \u2013 than aggregate-data-based methods.\nNOTE. The plots are generated from 500 Monte Carlo draws. Red line is the true value and black line is the mean of the estimates.\nNOTE. The plots are generated from 500 Monte Carlo draws. Red line is the true value and black line is the mean of the estimates."
        },
        {
            "heading": "4 Robustness and extension",
            "text": "In this section, we provide additional analytical and simulation results on the approximation quality of the low-dimensional dynamic factor model and discuss multiple extensions of our method."
        },
        {
            "heading": "4.1 Possibility of low-rank approximation",
            "text": "The validity of our method relies on the assumption that the heterogeneous-agent model generating the micro data can be approximated by a low-dimensional DFM. In Section 2, we argue that this is a reasonable assumption for a wide range of models and suggest heuristic procedures for testing this assumption using data simulated from the model. While these are good enough for practitioners, there is still no theoretical guarantee that a low-rank approximation is possible. Here we fill this gap by deriving a sufficient condition on the sequence-space solution of the model that renders a low-dimensional DFM representation.\nProposition 3. Let P be the set of endogenous aggregate inputs (e.g. real wages), E be the set of exogenous shock processes (e.g. TFP), and J := {J px : p \u2208 P, x \u2208 E} be the set of general equilibrium Jacobians. Suppose all the exogenous shock processes are AR(1). If for any x \u2208 E and p \u2208 P , we have the commutability condition,\nFJ px = J pxF,\nwhere F is the shift-forward operator, then a low-dimensional DFM representation exists.\nIntuitively, FJ px is the effect of the shock on the economy next period, while J pxF is the effect of a news shock on the economy today. The two effects will coincide if the HA distribution doesn\u2019t move in response to shocks, as this is the only endogenous state variable. Although the commutability condition will not hold exactly for most models, the slackness of the condition serves as a lower bound for the low-rank approximation quality.\nWe evaluate the normalized slackness \u2225FJ px \u2212 J pxF\u2225/ \u2225FJ px \u2225 for each shock and input in our small-scale HANK model and report the results in Table 5. There are three endogenous \u201dprices\u201d that the households care about \u2013 real interest rate, average real after-tax labor income, and average hours. Overall, the slackness is about 10% of the Frobenius norm of the GE Jacobian, except for average hours wrt. TFP shock which amounts to 23.8%."
        },
        {
            "heading": "4.2 Bayesian indirect inference",
            "text": "Our method can be easily paired with Bayesian methods to conduct Bayesian indirect inference, which sits within the Approximate Bayesian Computation (ABC) class of algorithms.\nRecall that object we want to target is the posterior distribution p(\u03b8|Y) \u221d f(Y |\u03b8)p(\u03b8). Among others, one simple and intuitive method, proposed by Gallant and McCulloch (2009) and Reeves and Pettit (2005), replaces f(Y |\u03b8) with the approximate likelihood computed in Algorithm 1. The target object is now the pseudo-posterior related to p(\u03b8|Y), analogous to how Section 3.1 maximised a pseudo-likelihood in the frequentist case.23 Of course, in the special case that the auxiliary model nests M then the two posteriors coincide (Drovandi et al. (2015)). Though this may not be exactly satisfied in our heterogeneous-agent model settings (i.e. the M is not exactly a DFM), the proposed tests in Section 2 and associated discussion should offer insights into when the approximation is good.\nWe compute the posterior sampling distributions of the model parameters in Section 3.1 via a Random Walk Metropolis Hastings (RWMH) algorithm. Algorithm 2 provides the pseudo-code for one iteration of the RWMH under our approach. For simplicity, we use a flat prior for all parameters. The computational details can be found in Appendix B.2.\nFigure 7 shows the posterior from 50,000 iterations of RWMH. Both the posterior mode and mean is near the true value. Overall, the posterior is tightly centered around the truth, even though the prior is completely uninformative. The simulation evidence thus suggests that our method can be used to conduct standard Bayesian analysis for HA business-cycle models (e.g. An and Schorfheide 2007).\n23Drovandi et al. (2015) draws the same connection between this methods and the quasi-maximum likliehood approach of Smith Jr (1993).\nAlgorithm 2 Bayesian Indirect Inference with Random Walk Metropolis Hastings For iteration n with structural parameter \u03b8n\u22121:\n1. Draw \u03b8\u2217 \u223c q(\u00b7|\u03b8n\u22121) 2. Approximate likelihood f(Y |\u03b8\u2217) using Algorithm 1\n3. Compute r = min { 1, f(Y |\u03b8\n\u2217)p(\u03b8\u2217) f(Y |\u03b8n\u22121)p(\u03b8n\u22121) } 4. Accept \u03b8\u2217 with probability r 5. if accept, \u03b8n = \u03b8\u2217, else \u03b8n = \u03b8n\u22121"
        },
        {
            "heading": "5 Conclusion",
            "text": "We develop an indirect inference method for estimating HA business-cycle models using micro data. The key idea is to approximate the data-generating process with a low-dimensional dynamic factor model and use the implied likelihood for inference. Employing the Dynamic Mode Decomposition algorithm, the likelihood evaluation is fast and simple. Moreover, our estimation procedure can seamlessly accommodate the sequence-space solution method, while most currently available estimation methods (e.g. Liu and Plagborg-M\u00f8ller 2023) are designed for state-space solution only.\nOur method is based on two assumptions: 1) the HA model is well-approximated by a lowdimensional dynamic factor model, and 2) the cross-sectional dimension of the micro data is large. We show that the first assumption holds in a wide range of HA models and provide a theoretical justification for it. In our simulation study, we show that our method works well on a realistic dataset, verifying the empirical relevance of the second assumption.\nComparing with other conventional methods including time-series estimation with crosssectional moments and frequency-domain estimation, our method delivers a better estimate both in terms of bias and standard error because of the more efficient use of cross-sectional information. As our method is based on approximated likelihood, we show that it can be easily pair with Bayesian methods to conduct Bayesian indirect inference.\nWe conclude with two directions for future research. First, a method for filtering the aggregate shocks using micro data remains to be developed. The sequence-space filtering method in McKay and Wieland (2021) seems promising. Second, it would be intriguing to estimate a calibrated HANK model using acutual micro data (e.g. CEX) and contrast the results with those derived from aggregate data. These disparities will offer new insights into model misspecification and aid in refining our modeling choices. We leave these to future works."
        },
        {
            "heading": "Appendix A Proofs",
            "text": ""
        },
        {
            "heading": "A.1 Proof of Proposition 1",
            "text": "Proof. Associated with state-space system (1) is its innovations representation.24\nx\u0302t+1 = A x\u0302t +Kat (A.1)\nyt = G x\u0302t + at\nwhere x\u0302t = E[xt |yt\u22121], at = yt\u2212E[yt |yt\u22121], at \u22a5 as \u2200t \u0338= s for yt = {ys}s<t and the Hilbert space H(at) = H(yt). Furthermore, \u2126 \u2261 E[at a\u22a4t ] = G\u03a3\u221eG\u22a4+R, where \u03a3\u221e and K satisfy\n\u03a3\u221e = E[xt\u2212x\u0302t][xt\u2212x\u0302t]\u22a4\n= CC\u22a4+KRK\u22a4+(A\u2212KG)\u03a3\u221e(A\u2212KG)\u22a4\nK = A\u03a3\u221eG \u22a4(G\u03a3\u221eG \u22a4+R)\u22121.\nNotice that rank(K) = N . Rearranging (A.1) gives an expression for xt+1 in terms of yt and xt\nx\u0302t+1 = Ax\u0302t +K(yt\u2212Gx\u0302t)\n= (A\u2212KG)x\u0302t +Kyt\nSubstituting into the measurement equation of (A.1) gives\nyt = G[(A\u2212KG)x\u0302t\u22121 +Kyt\u22121] + at = GKyt\u22121+G(A\u2212KG)x\u0302t\u22121 + at\nNotice that B\u221e1 := GK is a rank N matrix. Moreover, so it A\u2212KG. Iterating backward gives us the desired result\nyt = \u221e\u2211 j=1 B\u221ej yt\u2212j +at (A.2) E[at y\u22a4t\u2212j ] = 0 for all j \u2265 1 E[at aTt ] = \u2126 = G\u03a3\u221eG\u22a4+R B\u221ej = G(A\u2212KG)j\u22121K \u2200j \u2265 1 (A.3)\nwhere rank(B\u221ej ) = N \u2200j \u2265 1.\n24A detailed derivation can be found in Lungqvist and Sargent (2018), Ch. 2"
        },
        {
            "heading": "A.2 Proof of Lemma 1",
            "text": "Proof. Consider a sequence of models {MM} indexed by the number of observables M \u2208 N. For each M , the model MM is given by\nxt+1 = Axt+Cwt+1\nyt = GM xt+vt,\nwhere shocks wt+1 \u223c N (0, IN\u00d7N ), measurement errors vt \u223c N (0,RM ) and ws \u22a5 v\u03c4 for all s, \u03c4 . Note that the matrices A,C \u2208 RN\u00d7N are fixed across M , meaning that the transition equation of the unobserved state is invariant to the number of observables. In the following, \u2225\u00b7\u2225 denotes the Frobenius norm.\nBy Proposition 1, we have\nKM = A\u03a3\u221e,M G \u22a4 M (GM \u03a3\u221e,M G \u22a4 M +RM ) \u22121\nwhere \u03a3\u221e,M \u2208 GL(N,R) solves the matrix Ricatti equation\n\u03a3\u221e,M = CC \u22a4+KM RM K \u22a4 M +(A\u2212KM GM )\u03a3\u221e,M (A\u2212KM GM )\u22a4 (A.4)\n= A\u03a3\u221e,M A \u22a4+CC\u22a4\u2212A\u03a3\u221e,M G\u22a4(GM \u03a3\u221e,M G\u22a4M +RM )\u22121GM \u03a3\u221e,M A\u22a4 (A.5)\nSuch an invertible solution always exists and is unique under the maintained stability assumption. Furthermore, by construction, \u03a3\u221e,M = E[xt\u2212E[xt | yt\u22121]][xt\u2212E[xt | yt\u22121]]\u22a4.\nWe will first show that \u03a3\u221e,M \u2192 CC\u22a4 = E[xt\u2212E[xt | xt\u22121]][xt\u2212E[xt | xt\u22121]]\u22a4. By Assumption 3, rank(GM ) = N , so we have the normal equation\nxt = (G \u22a4 M GM ) \u22121G\u22a4M yt\u2212(G\u22a4M GM )\u22121G\u22a4M vt\nCombine with the state transition equation and we obtain\nxt = A(G \u22a4 M GM ) \u22121G\u22a4M yt\u22121\u2212A(G\u22a4M GM )\u22121G\u22a4M vt\u22121+Cwt\nPut F [xt | yt\u22121] := A(G\u22a4M GM )\u22121G\u22a4M yt\u22121. The forecast variance of this linear predictor is\nE[xt\u2212F [xt | yt\u22121]][xt\u2212F [xt | yt\u22121]]\u22a4 = A(G\u22a4M GM )\u22121G\u22a4M RM GM (G\u22a4M GM )\u22121A\u22a4+CC\u22a4\n= \u03c32v A(G \u22a4 M GM ) \u22121A\u22a4+CC\u22a4\nwhere we have use the assumption that RM = \u03c32vIM . Now, by Assumption 3, as M \u2192 \u221e,\n\u2225\u2225(G\u22a4M GM )\u22121\u2225\u2225\u2192 0. Therefore, we have\u2225\u2225\u2225E[xt\u2212F [xt | yt\u22121]][xt\u2212F [xt | yt\u22121]]\u22a4 \u2212CC\u22a4\u2225\u2225\u2225 = \u03c32v \u2225\u2225\u2225A(G\u22a4M GM )\u22121A\u22a4\u2225\u2225\u2225 \u2264 \u03c32v \u2225A\u2225 2 \u2225\u2225\u2225(G\u22a4M GM )\u22121\u2225\u2225\u2225\u2192 0\nWe conclude that E[xt\u2212F [xt | yt\u22121]][xt\u2212F [xt | yt\u22121]]\u22a4 \u2192 CC\u22a4 pointwise as M \u2192 \u221e. Finally, since conditional expectation minimizes mean-square errors, we have\n\u03a3\u221e,M = E[xt\u2212E[xt | yt\u22121]][xt\u2212E[xt | yt\u22121]]\u22a4\n\u2aaf E[xt\u2212F [xt | yt\u22121]][xt\u2212F [xt | yt\u22121]]\u22a4\nwhere \u2aaf represents the Loewner order.25 Clearly, we must have CC\u22a4 \u2aaf \u03a3\u221e,M because E[xt | xt\u22121,y\nt\u22121] = E[xt | xt\u22121]. Then by the continuity and anti-symmetry of the Loewner order, we conclude that \u03a3\u221e,M \u2192 CC\u22a4 pointwise as M \u2192 \u221e.\nWe are ready to prove that \u2225A\u2212KM GM\u2225 \u2192 0. By the matrix Ricatti equation (A.5), we have\u2225\u2225\u2225(A\u03a3\u221e,M )\u22121(\u03a3\u221e,M \u2212CC\u22a4)(A\u22a4)\u22121\u2225\u2225\u2225 = \u2225\u2225\u2225IN \u2212G\u22a4M (GM \u03a3\u221e,M G\u22a4M +RM )\u22121GM \u03a3\u221e,M\u2225\u2225\u2225 Let M \u2192 \u221e and we have the limit\u2225\u2225\u2225IN \u2212G\u22a4M (GM \u03a3\u221e,M G\u22a4M +RM )\u22121GM \u03a3\u221e,M\u2225\u2225\u2225\u2192 0 Note that\n\u2225A\u2212KM GM\u2225 = \u2225\u2225\u2225A\u2212A\u03a3\u221e,M G\u22a4M (GM \u03a3\u221e,M G\u22a4M +RM )\u22121GM\u2225\u2225\u2225\n\u2264 \u2225A\u2225 \u2225\u2225\u2225IN \u2212\u03a3\u221e,M G\u22a4(GM \u03a3\u221e,M G\u22a4M +RM )\u22121GM\u2225\u2225\u2225\n= \u2225A\u2225 \u2225\u2225\u2225IN \u2212G\u22a4M (GM \u03a3\u221e,M G\u22a4M +RM )\u22121GM \u03a3\u221e,M\u2225\u2225\u2225\nwhere the last equality follows from taking transpose and the symmetry of RM and \u03a3\u221e,M . Let M \u2192 \u221e and we have \u2225A\u2212KM GM\u2225 \u2192 0, as desired.\nWe can further compute the convergence rate. Note that\nM \u2225A\u2212KM GM\u2225 \u2264 M \u2225A\u2225 \u2225\u2225\u2225(A\u03a3\u221e,M )\u22121(\u03a3\u221e,M \u2212CC\u22a4)(A\u22a4)\u22121\u2225\u2225\u2225\n\u2264 \u2225A\u2225 \u2225\u2225(A\u03a3\u221e,M )\u22121\u2225\u2225\u2225\u2225\u2225M(\u03a3\u221e,M \u2212CC\u22a4)\u2225\u2225\u2225\u2225\u2225\u2225(A\u22a4)\u22121\u2225\u2225\u2225\n\u2264 \u03c32v \u2225A\u2225 3 \u2225\u2225(A\u03a3\u221e,M )\u22121\u2225\u2225\u2225\u2225\u2225(A\u22a4)\u22121\u2225\u2225\u2225 \u2225\u2225\u2225\u2225\u2225 ( 1 M G\u22a4M GM )\u22121\u2225\u2225\u2225\u2225\u2225 By Assumption 3 and our result that \u03a3\u221e,M \u2192 CC\u22a4, the RHS converges to some positive number\n25For any pair of positive semidefinite matrices A,B \u2208 RN\u00d7N , A \u2aaf B iff B \u2212A is positive semidefinite.\nas M \u2192 \u221e. Thus, we conclude that lim supM\u2192\u221eM \u2225A\u2212KM GM\u2225 < \u221e."
        },
        {
            "heading": "A.3 Proof of Corollary 1",
            "text": "Proof. Manipulating the innovations representation from the proof of Proposition 1 gives\nx\u0302t+1 = Axt+K(yt\u2212Gx\u0302t) (A.6)\n= (A\u2212KG)x\u0302t +Kyt (A.7)\nDefine x\u0303t := E[xt |yt]. So, x\u0302t+1 = Ax\u0303t.Next, suppose A\u2212KG = 0. Then,\nx\u0302t+1 = Kyt (A.8) Ax\u0303t = ALyt (A.9)\nfor L = \u03a3\u221eG\u2126\u22121 such that K = AL. AE[xt |yt] = ALyt. Assuming an invertible A, we have that\nE[xt |yt] = Lyt\ni.e. that a forecast of xt using all past observables yt is equivalent to just using the current observables vector yt."
        },
        {
            "heading": "A.4 Proof of Theorem 1",
            "text": "Proof. Consider the case j = 2. By the definition of Frobenius norm, we have\n\u2225B\u221e2 \u2225 = \u2225G(A\u2212KG)K\u2225\n= \u221a tr{K\u22a4(A\u2212KG)\u22a4G\u22a4G(A\u2212KG)K}\n= \u221a tr{(A\u2212KG)\u22a4(G\u22a4G)(A\u2212KG)(KK\u22a4)}\n= \u221a tr { (A\u2212KG)\u22a4 ( 1\nM G\u22a4G\n) [M(A\u2212KG)](KK\u22a4) } (A.10)\nBy the matrix Ricatti equation (A.4), we have\n\u03c32v KK \u22a4 = \u03a3\u221e \u2212CC\u22a4\u2212(A\u2212KG)\u03a3\u221e(A\u2212KG)\u22a4\nBy Lemma 1, as M \u2192 \u221e, the RHS goes to 0. Thus, we have KK\u22a4 \u2192 0 as M \u2192 \u221e. Take lim sup of equation (A.10) and use the continuity of tr and multiplication:\nlim sup M\u2192\u221e\n\u2225B\u221e2 \u2225 =\n\u221a tr { 0 \u00b7 (\nlim M\u2192\u221e\n1\nM G\u22a4G ) \u00b7 [ lim sup M\u2192\u221e M(A\u2212KG) ] \u00b7 0 }\nBy Assumption 3, limM\u2192\u221e 1M G \u22a4G exists and is finite. By Lemma 1, lim supM\u2192\u221eM(A\u2212KG) is finite. We conclude that lim supM\u2192\u221e \u2225B\u221e2 \u2225 = 0 and hence \u2225B\u221e2 \u2225 \u2192 0. Clearly, the case j > 2 can be proved in the same way, as (A\u2212KG)j \u2192 0. Inspecting equation (A.10), we can further conclude that for all j \u2265 1\nlim sup M\u2192\u221e\nM j\u22121 \u2225\u2225B\u221ej \u2225\u2225 < \u221e\nGiven that \u2225\u2225B\u221ej \u2225\u2225\u2192 0 for all j \u2265 2, the infinite-order VAR (2) collapses to the first-order VAR\n(4), as claimed."
        },
        {
            "heading": "A.5 Proof of Theorem 2",
            "text": "Proof. Using the infinite-order VAR (2), we can write the DFM likelihood as\n\u2113DFM (Y;A,C,G,R) = T\u2211 t=2 \u2113(yt | yt\u22121;A,C,G,R)\n= \u22121 2 T\u2211 t=2 log|\u2126|+ yt\u2212 t\u22121\u2211 j=1 B\u221ej yt\u2212j \u22a4\u2126\u22121 yt\u2212 t\u22121\u2211 j=1 B\u221ej yt\u2212j  \nwhere \u2126 = G\u03a3\u221eG\u22a4+R is the variance-covariance matrix of the innovation. Similarly, using the first-order VAR (4), we can write the likelihood as\n\u21131(Y;A,C,G,R) = T\u2211 t=2 \u2113(yt | yt\u22121;A,C,G,R)\n= \u22121 2 T\u2211 t=2 { log|\u2126|+ ( yt\u2212B\u221e1 yt\u22121 )\u22a4 \u2126\u22121 ( yt\u2212B\u221e1 yt\u22121 )} Subtract the two expressions and we obtain\n|\u2113DFM (Y;A,C,G,R)\u2212 \u21131(Y;A,C,G,R)|\n= 1\n2 \u2223\u2223\u2223\u2223\u2223\u2223\u2223 T\u2211 t=2   t\u22121\u2211 j=2 B\u221ej yt\u2212j \u22a4\u2126\u22121  t\u22121\u2211 j=2 B\u221ej yt\u2212j + 2  t\u22121\u2211 j=2 B\u221ej yt\u2212j \u22a4\u2126\u22121 (at)  \u2223\u2223\u2223\u2223\u2223\u2223\u2223\n\u2264 1 2 T\u2211 t=2 \u03bbmax(\u2126\u22121) \u2225\u2225\u2225\u2225\u2225\u2225 t\u22121\u2211 j=2 B\u221ej yt\u2212j \u2225\u2225\u2225\u2225\u2225\u2225 2 + 2  t\u22121\u2211 j=2 B\u221ej yt\u2212j \u22a4\u2126\u22121 (at) \nwhere \u03bbmax(\u2126\u22121) denotes the largest eigenvalue of \u2126\u22121 and at = yt\u2212 \u2211t\u22121 j=1B \u221e j yt\u2212j .\nIt suffices to show that 1. E \u2225\u2225\u2225\u2211t\u22121j=2B\u221ej yt\u2212j\u2225\u2225\u22252 \u2192 0 for all t = 1, . . . , T\n2. lim supM\u2192\u221e \u03bbmax(\u2126\u22121) < \u221e 3. E(B\u221ej yt\u2212j)\u22a4\u2126\u22121 at = 0 for all j \u2265 2 and t = 1, . . . , T\nClaim 1. Using the measurement equation for yt, we have\n\u2225\u2225\u2225\u2225 1\u221aM yt \u2225\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225\u2225 1\u221aM G \u2225\u2225\u2225\u2225 \u2225xt\u2225+ \u2225\u2225\u2225\u2225 1\u221aM vt \u2225\u2225\u2225\u2225 = \u221a tr { 1 M G\u22a4G } \u00b7 \u2225xt\u2225+ \u221a\u221a\u221a\u221a 1 M M\u2211 i=1 v2i,t\nNote that the distribution of \u2225xt\u2225 is invariant to M and has finite mean. By Assumption 3, we have\u221a tr { 1\nM G\u22a4G\n} \u00b7 \u2225xt\u2225 a.s.\u2192 \u03bb \u2225xt\u2225\nfor some \u03bb > 0. By SLLN, we have \u221a\u221a\u221a\u221a 1 M M\u2211 i=1 v2i,t a.s.\u2192 \u03c3v It follows that lim supM\u2192\u221e \u2225\u2225\u2225 1\u221a\nM yt \u2225\u2225\u2225 < \u221e almost surely. For any j \u2265 2, by Theorem 1, we have lim sup M\u2192\u221e \u2225\u2225B\u221ej yt\u2212j\u2225\u2225 \u2264 lim sup M\u2192\u221e ( \u221a M \u2225\u2225B\u221ej \u2225\u2225) \u2225\u2225\u2225\u2225 1\u221aM yt\u2212j\n\u2225\u2225\u2225\u2225 = lim sup\nM\u2192\u221e \u221a M \u2225\u2225B\u221ej \u2225\u2225)\ufe38 \ufe37\ufe37 \ufe38\n=0\n\u00b7 lim sup M\u2192\u221e \u2225\u2225\u2225\u2225 1\u221aM yt\u2212j \u2225\u2225\u2225\u2225\ufe38 \ufe37\ufe37 \ufe38\n<\u221e a.s.\nThus, \u2225\u2225B\u221ej yt\u2212j\u2225\u2225 a.s.\u2192 0 for all j \u2265 2 and t = 1, . . . , T . It follows that when M sufficiently large,\u2225\u2225\u2225\u2211t\u22121j=2B\u221ej yt\u2212j\u2225\u2225\u22252 is uniformly bounded above almost surely. Then by Dominated Convergence\nTheorem, we have E \u2225\u2225\u2225\u2211t\u22121j=2B\u221ej yt\u2212j\u2225\u2225\u22252 \u2192 0 for all t = 1, . . . , T .\nClaim 2. Fix M . By Spectral Theorem, there exists P,D \u2208 RM\u00d7M such that P\u22a4P = IM , D is diagonal, and G\u03a3\u221eG\u22a4 = P\u22a4DP. Then\n\u2126 = G\u03a3\u221eG \u22a4+R = P\u22a4DP+ \u03c32vP \u22a4P = P\u22a4(D+ \u03c32vIM )P\nIt follows that \u2126\u22121 = P\u22a4(D + \u03c32vIM )\u22121P. Since all the entry of D is non-negative, the largest eigenvalue of \u2126\u22121 is smaller than 1/\u03c32v . Then clearly lim supM\u2192\u221e \u03bbmax(\u2126\u22121) < \u221e\nClaim 3. Fix j \u2265 2. As shown in Claim 2, we can write \u2126\u22121 = P\u22a4(D+ \u03c32vIM )\u22121P. Then\n(B\u221ej yt\u2212j) \u22a4\u2126\u22121 at = (PB \u221e j yt\u2212j) \u22a4(D+ \u03c32vIM ) \u22121(Pat)\n= M\u2211 i=1\n1\ndi + \u03c32 \u2212 v (PB\u221ej yt\u2212j)i \u00b7 (Pat)i\nwhere (\u00b7)i denote the i entry of the vector. Clearly, for any i, (PB\u221ej yt\u2212j)i \u2208 H(yt\u22121) and (Pat)i \u2208 H(at). Then by the orthogonality condition at \u22a5 H(yt\u22121), we have\nE[(PB\u221ej yt\u2212j)i \u00b7 (Pat)i] = 0 \u2200i\nIt follows that E(B\u221ej yt\u2212j)\u22a4\u2126\u22121 at = 0, as desired.\nBy the three claims, the proof is complete and we conclude that\nlim M\u2192\u221e\nE|\u2113DFM (Y;A,C,G,R)\u2212 \u21131(Y;A,C,G,R)| = 0"
        },
        {
            "heading": "A.6 Proof of Proposition 2",
            "text": "Proof. WLOG, let css = 0. As shown in Auclert et al. (2021a), up to first-order, the household\u2019s policy can be written as\nct = \u221e\u2211 j=0 \u2211 p\u2208P \u2202 c \u2202pj Et[p\u0303t+j ]\nwhere \u2202 c\u2202pj \u2208 R M is the derivative of individual policy wrt. the j-period ahead aggregate input p \u2208 P and p\u0303t+j denotes the deviation of p from its steady-state value. Put p\u0303t: := (p\u0303t, p\u0303t+1, . . . )\u22a4. Using the impulse response functions, we can write\nEt[p\u0303t:] = Et\u22121[p\u0303t:] + Ipe \u03f5t = F Et\u22121[p\u0303t\u22121:] + Ipe \u03f5t\nwhere F is the shift forward operator. Iterate backward and we obtain the MA representation\nEt[p\u0303t:] = \u221e\u2211 j=0 F jIpe \u03f5t\u2212j\nLet J cp be the infinite-dimensional matrix of which the j column is \u2202 c\u2202pj . Substitute back into the\npolicy function:\nct = \u2211 p\u2208P J cp Et[p\u0303t:] = \u2211 p\u2208P J cp \u221e\u2211 j=0 F jIpe \u03f5t\u2212j = \u221e\u2211 j=0 \u2211 p\u2208P\nJ cpF jIpe\ufe38 \ufe37\ufe37 \ufe38 \u03a8cj \u03f5t\u2212j"
        },
        {
            "heading": "A.7 Proof of Proposition 3",
            "text": "Proof. Let Ipe,x denote the impulse response functions of p wrt. an exogenous shock to x. Then\nIpe,x = J px Ixe\nwhere Ixe = (1, \u03c1x, \u03c12x, . . . )\u22a4 is the impulse response function of x wrt. the shock and \u03c1x \u2208 (0, 1) is the associated AR(1) coefficient. Recall that by Proposition 2, we have\nct = \u221e\u2211 j=0 \u2211 p\u2208P J cpF jIpe \u03f5t\u2212j\n= \u221e\u2211 j=0 \u2211 p\u2208P \u2211 x\u2208E J cpF jIpe,x\u03f5xt\u2212j\n= \u221e\u2211 j=0 \u2211 x\u2208E \u2211 p\u2208P J cpF jJ px Ixe \u03f5xt\u2212j\nClearly, if FJ px = J pxF , then F jJ px = J pxF j \u2200j \u2208 N. Using this condition, we have\nct = \u221e\u2211 j=0 \u2211 x\u2208E \u2211 p\u2208P J cpF jJ px Ixe \u03f5xt\u2212j\n= \u221e\u2211 j=0 \u2211 x\u2208E \u2211 p\u2208P J cpJ pxF jIxe \u03f5xt\u2212j = \u2211 x\u2208E \u2211 p\u2208P J cpJ px  \u221e\u2211 j=0 F jIxe \u03f5xt\u2212j\n Note that\n\u221e\u2211 j=0 F jIxe \u03f5xt\u2212j = \u221e\u2211 j=0 \u03c1jxIxe \u03f5xt\u2212j = Ixe \u221e\u2211 j=0 \u03c1jx\u03f5 x t\u2212j = Ixe xt\nThus, the policy function becomes\nct = \u2211 x\u2208E \u2211 p\u2208P J cpJ px Ixe xt = \u2211 x\u2208E Gx xt\nwhere Gx := \u2211 p\u2208P J cpJ p x Ixe is the impulse response function of ct wrt. shock to x. Now, let xt be the vector of the shock process x. Then we have the low-dimensional DFM representation\nxt+1 = Axt+\u03f5t+1\nct = Gxt\nwhere A is the diagonal matrix of the AR(1) coefficients and G is the matrix from stacking Gx."
        },
        {
            "heading": "Appendix B Estimation details",
            "text": ""
        },
        {
            "heading": "B.1 Frequency-domain estimation",
            "text": "We use the same 500 simulated micro datasets (M \u00d7 T ) as in the main estimation exercise. Recall that by Proposition 2, the de-meaned data has a MA representation\nct = \u221e\u2211 j=0 \u03a8cj\u03f5t\u2212j + vt, \u03f5t \u223c N(0,\u03a3e)\nwhere vt \u223c N(0,R) is measurement error. For a given set of parameters \u03b8, we can efficiently compute the MA coefficient matrices \u03a8cj using the SSJ method.\nFollowing Hansen and Sargent (1981), we approximate the likelihood using Whittle approximation:\nL(c; \u03b8) = \u22121 2 T\u22121\u2211 j=0 [ log 2\u03c0 + log(detS(\u03c9j ; \u03b8)) + tr(S(\u03c9j ; \u03b8)\u22121I(c;\u03c9j)) ] (B.1)\nwhere \u03c9j := 2\u03c0jT , S(\u03c9j ; \u03b8) is the spectral density of c at frequency \u03c9j , and I(c;\u03c9j) is the periodogram of the data at frequency \u03c9j . By definition, the periodogram is given by\nI(c;\u03c9j) := 1\nT ( T\u2211 t=1 ct exp(\u2212i\u03c9jt) )( T\u2211 t=1 ct exp(i\u03c9jt) )\u2032\nBy the MA representation, the spectral density is given by\nS(\u03c9j ; \u03b8) =  \u221e\u2211 j=0 \u03a8cj(\u03b8) exp(\u2212i\u03c9jj) \u03a3e  \u221e\u2211 j=0 \u03a8cj(\u03b8) exp(i\u03c9jj) \u2032 +R Note that both I(c;\u03c9j) and S(\u03c9j ; \u03b8) are M -dimensional matrices and can be computed by applying the Discrete Fourier Transform to the data matrix c and MA coefficient array {\u03a8cj : j = 0, . . . , T}. Also, the symmetry of Fourier transform implies that we only need to evaluate the summands in (B.1) for j = 0, . . . , \u230aT\u221212 \u230b\nGiven a dataset c, we construct the likelihood using the formula above and find the parameter that maximizes the likelihood using standard optimization algorithm. The distribution of the estimates is plotted in Figure D.1."
        },
        {
            "heading": "B.2 Random Walk Metropolis Hastings estimation",
            "text": "We apply a simple Random Walk Metropolis Hastings Algorithm to sample from the posterior distribution of the parameters. We also use a tuned proposal covariance matrix and adaptive step size proposed by Atchade\u0301 and Rosenthal (2005) and Haario et al. (2001).\nWe set the prior of the parameters to be flat. We initialize the MCMC sampler at the mode of the posterior distribution and generate 50,000 draws, discarding the first 10,000."
        },
        {
            "heading": "Appendix C Simulation details",
            "text": "Below, we provide a short summary of the models used to generate Figure 2. In all cases, our simulated dataset has 300 units in the cross-section, of length T = 10, 000.\nKrusell-Smith: We use a standard Krusell-Smith model, the code of which is available on the SHADE-econ github page. We simulate the cross-section of consumption, M = 300, T = 10, 000 with one aggregate shock, TFP.\nOne-Asset HANK: This is the same model in Section 3, with two aggregate shocks, TFP and Monetary policy; and one cross-sectional shock to income dispersion.\nTwo-Asset HANK: We implement the two-asset HANK model, the code of which is available on the SHADE-econ github page. There are three aggregate shocks \u2013 TFP, government spending and r\u2217.\nHetero. Firms: We implement a version of the model with heterogeneous-firms by Winberry (2021), using the code provided by Liu and Plagborg-M\u00f8ller (2023). There is only one aggregate shock \u2013 TFP."
        },
        {
            "heading": "Appendix D Supplementary tables and figures",
            "text": "NOTE. The plots are generated from 500 Monte Carlo draws. Red line is the true value and black line is the mean of the estimates."
        }
    ],
    "title": "Estimating HANK with Micro Data*",
    "year": 2024
}