{
    "abstractText": "Multi-institutional collaborations are key for learning generalizable MRI synthesis models that translate sourceonto target-contrast images. To facilitate collaboration, federated learning (FL) adopts decentralized training and mitigates privacy concerns by avoiding sharing of imaging data. However, FL-trained synthesis models can be impaired by the inherent heterogeneity in the data distribution, with domain shifts evident when common or variable translation tasks are prescribed across sites. Here we introduce the first personalized FL method for MRI Synthesis (pFLSynth) to improve reliability against domain shifts. pFLSynth is based on an adversarial model that produces latents specific to individual sites and source-target contrasts, and leverages novel personalization blocks to adaptively tune the statistics and weighting of feature maps across the generator stages given latents. To further promote site specificity, partial model aggregation is employed over downstream layers of the generator while upstream layers are retained locally. As such, pFLSynth enables training of a unified synthesis model that can reliably generalize across multiple sites and translation tasks. Comprehensive experiments on multi-site datasets clearly demonstrate the enhanced performance of pFLSynth against prior federated methods in multi-contrast MRI synthesis.",
    "authors": [
        {
            "affiliations": [],
            "name": "Onat Dalmaz"
        },
        {
            "affiliations": [],
            "name": "Muhammad U Mirza"
        },
        {
            "affiliations": [],
            "name": "G\u00f6kberk Elmas"
        },
        {
            "affiliations": [],
            "name": "Muzaffer \u00d6zbey"
        },
        {
            "affiliations": [],
            "name": "Salman UH Dar"
        },
        {
            "affiliations": [],
            "name": "Emir Ceyani"
        },
        {
            "affiliations": [],
            "name": "Salman Avestimehr"
        },
        {
            "affiliations": [],
            "name": "Tolga \u00c7ukur"
        }
    ],
    "id": "SP:d896ce3d7ac59c06d981c5c27476c90d4d55daea",
    "references": [
        {
            "authors": [
                "S. Bakas"
            ],
            "title": "Advancing The Cancer Genome Atlas glioma MRI collections with expert segmentation labels and radiomic features",
            "venue": "Sci. Data, vol. 4, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "K. Krupa",
                "M. Bekiesi\u0144ska-Figatowska"
            ],
            "title": "Artifacts in Magnetic Resonance Imaging",
            "venue": "Pol. J. Radiol., vol. 80, pp. 93\u2013106, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "J.E. Iglesias",
                "E. Konukoglu",
                "D. Zikic",
                "B. Glocker",
                "K. Van Leemput",
                "B. Fischl"
            ],
            "title": "Is Synthesizing MRI Contrast Useful for Inter-modality Analysis?",
            "venue": "in Med. Image Comput. Comput. Assist. Interv.,",
            "year": 2013
        },
        {
            "authors": [
                "V. Sevetlidis",
                "M.V. Giuffrida",
                "S.A. Tsaftaris"
            ],
            "title": "Whole Image Synthesis Using a Deep Encoder-Decoder Network",
            "venue": "Simul. Synth. Med. Imaging, 2016, pp. 127\u2013137.",
            "year": 2016
        },
        {
            "authors": [
                "T. Joyce",
                "A. Chartsias",
                "S.A. Tsaftaris"
            ],
            "title": "Robust multi-modal MR image synthesis",
            "venue": "Med. Image Comput. Comput. Assist. Interv., 2017, pp. 347\u2013355.",
            "year": 2017
        },
        {
            "authors": [
                "W. Wei"
            ],
            "title": "Fluid-attenuated inversion recovery MRI synthesis from multisequence MRI using three-dimensional fully convolutional networks for multiple sclerosis",
            "venue": "J. Med. Imaging, vol. 6, no. 1, p. 014005, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S.U. Dar",
                "M. Yurt",
                "L. Karacan",
                "A. Erdem",
                "E. Erdem",
                "T. \u00c7ukur"
            ],
            "title": "Image Synthesis in Multi-Contrast MRI With Conditional Generative Adversarial Networks",
            "venue": "IEEE Trans. Med. Imag., vol. 38, no. 10, pp. 2375\u20132388, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "G.A. Kaissis",
                "M.R. Makowski",
                "D. R\u00fceckert",
                "R.F. Braren"
            ],
            "title": "Secure, privacy-preserving and federated machine learning in medical imaging",
            "venue": "Nat. Mach. Intell., vol. 2, no. 6, pp. 305\u2013311, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "W. Li"
            ],
            "title": "Privacy-Preserving Federated Brain Tumour Segmentation",
            "venue": "Mach. Learn. Med. Imag., 2019, pp. 133\u2013141.",
            "year": 2019
        },
        {
            "authors": [
                "M.J. Sheller",
                "G.A. Reina",
                "B. Edwards",
                "J. Martin",
                "S. Bakas"
            ],
            "title": "Multiinstitutional Deep Learning Modeling Without Sharing Patient Data: A Feasibility Study on Brain Tumor Segmentation",
            "venue": "Med. Image Comput. Comput. Assist. Interv., 2019, pp. 92\u2013104.",
            "year": 2019
        },
        {
            "authors": [
                "N. Rieke"
            ],
            "title": "The future of digital health with federated learning",
            "venue": "npj Digital Medicine, vol. 3, no. 1, p. 119, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H.R. Roth"
            ],
            "title": "Federated Learning for Breast Density Classification: A Real-World Implementation",
            "venue": "DART, DCL, 2020, p. 181\u2013191.",
            "year": 2020
        },
        {
            "authors": [
                "X. Li",
                "Y. Gu",
                "N. Dvornek",
                "L.H. Staib",
                "P. Ventola",
                "J.S. Duncan"
            ],
            "title": "Multi-site fMRI analysis using privacy-preserving federated learning and domain adaptation: ABIDE results",
            "venue": "Med. Image. Anal., vol. 65, p. 101765, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Q. Liu",
                "C. Chen",
                "J. Qin",
                "Q. Dou",
                "P. Heng"
            ],
            "title": "FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space",
            "venue": "Comput. Vis. Pattern Recognit., 2021, pp. 1013\u20131023.",
            "year": 2021
        },
        {
            "authors": [
                "H.B. McMahan",
                "E. Moore",
                "D. Ramage",
                "S. Hampson",
                "B.A. y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "AISTATS, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J. Wang"
            ],
            "title": "A Field Guide to Federated Optimization",
            "venue": "arXiv:2107.06917, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "T. Li",
                "A.K. Sahu",
                "A.S. Talwalkar",
                "V. Smith"
            ],
            "title": "Federated Learning: Challenges, Methods, and Future Directions",
            "venue": "IEEE Signal. Process. Mag., vol. 37, pp. 50\u201360, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "N. Rieke"
            ],
            "title": "The future of digital health with federated learning",
            "venue": "npj Digital Medicine, vol. 3, 12 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Sheller"
            ],
            "title": "Federated learning in medicine: facilitating multiinstitutional collaborations without sharing patient data",
            "venue": "Sci. Rep., vol. 10, 07 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H.R. Roth"
            ],
            "title": "Federated Whole Prostate Segmentation in MRI with Personalized Neural Architectures",
            "venue": "Med. Image Comput. Comput. Assist. Interv. Cham: Springer, 2021, pp. 357\u2013366.",
            "year": 2021
        },
        {
            "authors": [
                "X. Li",
                "M. Jiang",
                "X. Zhang",
                "M. Kamp",
                "Q. Dou"
            ],
            "title": "FedBN: Federated Learning on Non-IID Features via Local Batch Normalization",
            "venue": "Int. Conf. Learn. Represent., 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C.I. Bercea",
                "B. Wiestler",
                "D. Rueckert",
                "S. Albarqouni"
            ],
            "title": "FedDis: Disentangled Federated Learning for Unsupervised Brain Pathology Segmentation",
            "venue": "arXiv:2103.03705, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Pati"
            ],
            "title": "The Federated Tumor Segmentation (FeTS) Challenge",
            "venue": "arXiv:2105.05874, 2021. 10",
            "year": 2021
        },
        {
            "authors": [
                "D. Yang"
            ],
            "title": "Federated semi-supervised learning for COVID region segmentation in chest CT using multi-national data from China, Italy, Japan",
            "venue": "Med. Image. Anal., vol. 70, p. 101992, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Yan",
                "J. Wicaksana",
                "Z. Wang",
                "X. Yang",
                "K.-T. Cheng"
            ],
            "title": "Variation- Aware Federated Learning With Multi-Source Decentralized Medical Image Data",
            "venue": "IEEE J. Biomed. Health Inform., vol. 25, no. 7, pp. 2615\u2013 2628, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Park",
                "G. Kim",
                "J. Kim",
                "B. Kim",
                "J.C. Ye"
            ],
            "title": "Federated Split Task- Agnostic Vision Transformer for COVID-19 CXR Diagnosis",
            "venue": "Adv. Neural Inf. Process. Syst., 2021.",
            "year": 2021
        },
        {
            "authors": [
                "P. Guo",
                "P. Wang",
                "J. Zhou",
                "S. Jiang",
                "V.M. Patel"
            ],
            "title": "Multi-institutional Collaborations for Improving Deep Learning-based Magnetic Resonance Image Reconstruction Using Federated Learning",
            "venue": "arXiv:2103.02148, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C.-M. Feng",
                "Y. Yan",
                "H. Fu",
                "Y. Xu",
                "L. Shao"
            ],
            "title": "Specificity-Preserving Federated Learning for MR Image Reconstruction",
            "venue": "arXiv:2112.05752, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "G. Elmas"
            ],
            "title": "Federated Learning of Generative Image Priors for MRI Reconstruction",
            "venue": "arXiv:2202.04175, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "R. Vemulapalli",
                "H. Van Nguyen",
                "S.K. Zhou"
            ],
            "title": "Unsupervised Cross- Modal Synthesis of Subject-Specific Scans",
            "venue": "Int. Conf. Comput. Vis., 2015, pp. 630\u2013638.",
            "year": 2015
        },
        {
            "authors": [
                "A. Jog",
                "A. Carass",
                "S. Roy",
                "D.L. Pham",
                "J.L. Prince"
            ],
            "title": "Random forest regression for magnetic resonance image synthesis",
            "venue": "Med. Image. Anal., vol. 35, pp. 475\u2013488, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Huang",
                "L. Shao",
                "A.F. Frangi"
            ],
            "title": "Cross-Modality Image Synthesis via Weakly Coupled and Geometry Co-Regularized Joint Dictionary Learning",
            "venue": "IEEE Trans. Med. Imag., vol. 37, no. 3, pp. 815\u2013827, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "C. Bowles"
            ],
            "title": "Pseudo-healthy Image Synthesis for White Matter Lesion Segmentation",
            "venue": "Simul. Synth. Med. Imaging, 2016, pp. 87\u201396.",
            "year": 2016
        },
        {
            "authors": [
                "A. Chartsias",
                "T. Joyce",
                "R. Dharmakumar",
                "S.A. Tsaftaris"
            ],
            "title": "Adversarial Image Synthesis for Unpaired Multi-modal Cardiac Data",
            "venue": "Simul. Synth. Med. Imaging, 2017, pp. 3\u201313.",
            "year": 2017
        },
        {
            "authors": [
                "D. Nie"
            ],
            "title": "Medical Image Synthesis with Deep Convolutional Adversarial Networks",
            "venue": "IEEE Trans. Biomed. Eng., vol. 65, no. 12, pp. 2720\u20132730, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "K. Armanious"
            ],
            "title": "MedGAN: Medical image translation using GANs",
            "venue": "Comput. Med. Imaging Graph., vol. 79, p. 101684, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "B. Yu",
                "L. Zhou",
                "L. Wang",
                "Y. Shi",
                "J. Fripp",
                "P. Bourgeat"
            ],
            "title": "Ea-GANs: Edge-Aware Generative Adversarial Networks for Cross-Modality MR Image Synthesis",
            "venue": "IEEE Trans. Med. Imag., vol. 38, no. 7, pp. 1750\u2013 1762, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "T. Zhou",
                "H. Fu",
                "G. Chen",
                "J. Shen",
                "L. Shao"
            ],
            "title": "Hi-Net: Hybrid- Fusion Network for Multi-Modal MR Image Synthesis",
            "venue": "IEEE Trans. Med. Imag., vol. 39, no. 9, pp. 2772\u20132781, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Luo"
            ],
            "title": "Edge-preserving MRI image synthesis via adversarial network with iterative multi-scale fusion",
            "venue": "Neurocomputing, vol. 452, pp. 63\u201377, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "B. Zhan"
            ],
            "title": "LR-cGAN: Latent representation based conditional generative adversarial network for multi-modality MRI synthesis",
            "venue": "Biomed. Signal Process. Control, vol. 66, p. 102457, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Yurt",
                "S.U. Dar",
                "A. Erdem",
                "E. Erdem",
                "K.K. Oguz",
                "T. \u00c7ukur"
            ],
            "title": "mustGAN: multi-stream Generative Adversarial Networks for MR Image Synthesis",
            "venue": "Med. Image. Anal., vol. 70, p. 101944, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Sharma",
                "G. Hamarneh"
            ],
            "title": "Missing MRI pulse sequence synthesis using multi-modal generative adversarial network",
            "venue": "IEEE Trans. Med. Imag., vol. 39, no. 4, pp. 1170\u20131183, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "D. Lee",
                "J. Kim",
                "W.-J. Moon",
                "J.C. Ye"
            ],
            "title": "CollaGAN: Collaborative GAN for Missing Image Data Imputation",
            "venue": "Comput. Vis. Pattern Recognit., 2019, pp. 2487\u20132496.",
            "year": 2019
        },
        {
            "authors": [
                "H. Li"
            ],
            "title": "DiamondGAN: Unified Multi-modal Generative Adversarial Networks for MRI Sequences Synthesis",
            "venue": "Med. Image Comput. Comput. Assist. Interv., 2019, pp. 795\u2013803.",
            "year": 2019
        },
        {
            "authors": [
                "G. Wang"
            ],
            "title": "Synthesize High-Quality Multi-Contrast Magnetic Resonance Imaging From Multi-Echo Acquisition Using Multi-Task Deep Generative Model",
            "venue": "IEEE Trans. Med. Imag., vol. 39, no. 10, pp. 3089\u20133099, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "O. Dalmaz",
                "M. Yurt",
                "T. \u00c7ukur"
            ],
            "title": "ResViT: Residual vision transformers for multi-modal medical image synthesis",
            "venue": "IEEE Trans. Med. Imag., pp. 1\u20131, 2022. doi:10.1109/TMI.2022.3167808.",
            "year": 2022
        },
        {
            "authors": [
                "J. Song",
                "J.C. Ye"
            ],
            "title": "Federated CycleGAN for Privacy-Preserving Image-to-Image Translation",
            "venue": "arXiv:2106.09246, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Che"
            ],
            "title": "Federated Multi-View Learning for Private Medical Data Integration and Analysis",
            "venue": "ACM Trans. Intell. Syst. Technol., vol. 13, no. 4, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "G. Xie"
            ],
            "title": "FedMed-GAN: Federated Domain Translation on Unsupervised Cross-Modality Brain Image Synthesis",
            "venue": "arXiv:2201.08953, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "G. Xie",
                "J. Wang",
                "Y. Huang",
                "Y. Zheng",
                "F. Zheng",
                "Y. Jin"
            ],
            "title": "FedMed- ATL: Misaligned Unpaired Brain Image Synthesis via Affine Transform Loss",
            "venue": "arXiv:2201.12589, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "X. Huang",
                "S.J. Belongie"
            ],
            "title": "Arbitrary style transfer in real-time with adaptive instance normalization",
            "venue": "Int. Conf. Comput. Vis., pp. 1510\u2013 1519, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S. Khan",
                "J. Huh",
                "J.C. Ye"
            ],
            "title": "Switchable and Tunable Deep Beamformer Using Adaptive Instance Normalization for Medical Ultrasound",
            "venue": "IEEE Trans. Med. Imag., vol. 41, no. 2, pp. 266\u2013278, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Gu",
                "J.C. Ye"
            ],
            "title": "AdaIN-based tunable CycleGAN for Efficient Unsupervised Low-Dose CT Denoising",
            "venue": "IEEE Trans. Comput. Imag., vol. PP, pp. 1\u20131, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "H. Zhang",
                "H. Li",
                "J. Dillman",
                "N. Parikh",
                "L. He"
            ],
            "title": "Multi-Contrast MRI Image Synthesis Using Switchable Cycle-Consistent Generative Adversarial Networks",
            "venue": "Diagnostics, vol. 12, p. 816, 03 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Denck",
                "J. Guehring",
                "A. Maier",
                "E. Rothgang"
            ],
            "title": "MR-contrast-aware image-to-image translations with generative adversarial networks",
            "venue": "Int. J. Comput. Assist. Rad. Surge., vol. 16, 06 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Beers"
            ],
            "title": "High-resolution medical image synthesis using progressively grown generative adversarial networks",
            "venue": "arXiv:1805.03144, 2018.",
            "year": 1805
        },
        {
            "authors": [
                "K. Armanious"
            ],
            "title": "MedGAN: Medical image translation using GANs",
            "venue": "Comput. Med. Imaging Grap., vol. 79, p. 101684, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M. Rasouli",
                "T. Sun",
                "R. Rajagopal"
            ],
            "title": "FedGAN: Federated Generative Adversarial Networks for Distributed Data",
            "venue": "arXiv:2006.07228, 2020.",
            "year": 2006
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep Residual Learning for Image Recognition",
            "venue": "Comput. Vis. Pattern Recognit., 2016, pp. 770\u2013778.",
            "year": 2016
        },
        {
            "authors": [
                "Y. Wang",
                "W. Huang",
                "F. Sun",
                "T. Xu",
                "Y. Rong",
                "J. Huang"
            ],
            "title": "Deep Multimodal Fusion by Channel Exchanging",
            "venue": "Adv. Neural Inf. Process. Syst., 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H.B. McMahan",
                "E. Moore",
                "D. Ramage",
                "S. Hampson",
                "B.A. y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "AISTATS, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "B.H. Menze"
            ],
            "title": "The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)",
            "venue": "IEEE Trans. Med. Imag., vol. 34, no. 10, pp. 1993\u20132024, 2015.",
            "year": 1993
        },
        {
            "authors": [
                "E. Bullitt"
            ],
            "title": "Vessel Tortuosity and Brain Tumor Malignancy",
            "venue": "Academic radiology, vol. 12, pp. 1232\u201340, 11 2005.",
            "year": 2005
        },
        {
            "authors": [
                "P.J. LaMontagne"
            ],
            "title": "OASIS-3: Longitudinal Neuroimaging, Clinical, and Cognitive Dataset for Normal Aging and Alzheimer Disease",
            "venue": "medRxiv:2019.12.13.1901490, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "C.-M. Feng",
                "Y. Yan",
                "H. Fu",
                "Y. Xu",
                "L. Shao"
            ],
            "title": "Specificity-Preserving Federated Learning for MR Image Reconstruction",
            "venue": "arXiv:2112.05752, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Sharma",
                "G. Hamarneh"
            ],
            "title": "Missing MRI Pulse Sequence Synthesis Using Multi-Modal Generative Adversarial Network",
            "venue": "IEEE Trans. Med. Imag., vol. 39, pp. 1170\u20131183, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Heusel",
                "H. Ramsauer",
                "T. Unterthiner",
                "B. Nessler",
                "S. Hochreiter"
            ],
            "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
            "venue": "Adv Neural Inf Process Syst, 2017, p. 6629\u20136640.",
            "year": 2017
        },
        {
            "authors": [
                "R. Shwartz-Ziv",
                "N. Tishby"
            ],
            "title": "Opening the black box of deep neural networks via information",
            "venue": "arXiv:1703.00810, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "F. Mo",
                "A. Borovykh",
                "M. Malekzadeh",
                "H. Haddadi",
                "S. Demetriou"
            ],
            "title": "Layer-wise characterization of latent information leakage in federated learning",
            "venue": "arXiv:2010.08762, 2020.",
            "year": 2010
        },
        {
            "authors": [
                "T. Han"
            ],
            "title": "Breaking medical data sharing boundaries by using synthesized radiographs",
            "venue": "Sci Adv, vol. 6, no. 49, p. eabb7973, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Ziller",
                "D. Usynin",
                "M. Knolle",
                "K. Hammernik",
                "D. Rueckert",
                "G. Kaissis"
            ],
            "title": "Complex-valued deep learning with differential privacy",
            "venue": "arXiv:2110.03478, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Q. Feng",
                "C. Guo",
                "F. Benitez-Quiroz",
                "A. Martinez"
            ],
            "title": "When Do GANs Replicate? On the Choice of Dataset Size",
            "venue": "Int. Conf. Comput. Vis., 2021, pp. 6701\u20136710.",
            "year": 2021
        },
        {
            "authors": [
                "J. Wolterink",
                "A.M. Dinkla",
                "M. Savenije",
                "P. Seevinck",
                "C. Berg",
                "I. Isgum"
            ],
            "title": "Deep MR to CT Synthesis Using Unpaired Data",
            "venue": "Simul. Synth. Med. Imaging, 2017, pp. 14\u201323.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Ge"
            ],
            "title": "Unpaired MR to CT Synthesis with Explicit Structural Constrained Adversarial Learning",
            "venue": "Int. Symp. Biomed. Imaging, 2019, pp. 1096\u20131099.",
            "year": 2019
        },
        {
            "authors": [
                "C.-B. Jin"
            ],
            "title": "Deep CT to MR Synthesis Using Paired and Unpaired Data",
            "venue": "Sensors, vol. 19, no. 10, p. 2361, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Korkmaz",
                "S.U. Dar",
                "M. Yurt",
                "M. \u00d6zbey",
                "T. \u00c7ukur"
            ],
            "title": "Unsupervised MRI Reconstruction via Zero-Shot Learned Adversarial Transformers",
            "venue": "IEEE Trans. Med. Imag., vol. 41, no. 7, pp. 1747\u20131763, 2022.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014 federated learning, personalization, MRI, synthesis, translation, heterogeneity, domain shift\nI. INTRODUCTION\nMRI offers non-invasive assessment of anatomy withrich diagnostic information accumulated over multiple tissue contrasts [1]. Yet, it suffers from prolonged scan times due to its limited signal-to-noise ratio (SNR) efficiency. Costs associated with multi-contrast protocols can prohibit comprehensive acquisitions or repeat runs of corrupted contrasts during an exam [2]. Contrast translation is a promising solution wherein missing images in the protocol are synthesized from a subset of acquired images [3]. Adoption of deep learning has paved the way for centrally-trained models that offer leaps in synthesis performance [4]\u2013[7]. Unfortunately, training of generalizable models requires diverse datasets that are difficult to curate centrally due to patient privacy risks [8].\nThis study was supported in part by a TUBITAK BIDEB scholarship awarded to O. Dalmaz, and by TUBA GEBIP 2015 and BAGEP 2017 fellowships awarded to T. C\u0327ukur (Corresponding author: Tolga C\u0327ukur).\nO. Dalmaz, U. Mirza, G. Elmas, M. Ozbey, SUH. Dar and T. C\u0327ukur are with the Department of Electrical and Electronics Engineering, and the National Magnetic Resonance Research Center, Bilkent University, Ankara, Turkey (e-mail: {onat,mumirza,gokberk,muzaffer,salman, cukur}@ee.bilkent.edu.tr).\nE. Ceyani and S. Avestimehr are with the Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA 90089 USA (e-mail: {ceyani@, avestime@}usc.edu).\nFederated learning (FL) addresses this limitation based on decentralized model training across multiple institutions [9]\u2013 [14]. An FL server sporadically aggregates locally-trained models to compute a shared global model [15], [16], and then broadcasts the global model onto each site for further training. Thus, collaborative training is performed on diverse datasets from multiple sites without sharing of imaging data [17]. However, the resultant models can be impaired by the data heterogeneity naturally evident in multi-site datasets [18], [19]. Previous studies on FL-based medical imaging have introduced prominent approaches to cope with data heterogeneity in segmentation [14], [20]\u2013[24], classification [13], [25], [26], and reconstruction [27]\u2013[29] tasks. However, to our knowledge, no prior study has attempted to address data heterogeneity in federated MRI synthesis.\nLearning-based MRI synthesis recovers unavailable targetcontrast images of an anatomy provided as input acquired source-contrast images [30], [31]. Models are trained on datasets comprising samples from the desired source-target configuration [32]. In a restricted setup, a multi-site model can be built for a single task with a common configuration (e.g., T1\u2192T2 at all sites where source\u2192target), resulting in implicit heterogeneity across sites due to variations in sequence parameters or imaging hardware [27], [29]. In a more flexible setup, a multi-site model can be built for multiple tasks with variable configuration across sites (e.g., T1\u2192T2 in some, T2\u2192PD in others). This will result in explicit heterogeneity due to differences in the identity of source and target contrasts. Both implicit and explicit heterogeneity can induce notable performance losses in synthesis models [7].\nHere, we introduce a novel personalized FL method for MRI Synthesis (pFLSynth) that effectively addresses implicit and explicit heterogeneity in multi-site datasets. pFLSynth employs a unified adversarial model that produces latent variables specific to individual sites and source-target contrasts. To cope with heterogeneity, novel personalization blocks are introduced that adaptively modulate feature maps across the generator given these site- and contrast-specific latents. For further personalization, we propose partial network aggregation on downstream generator layers, while upstream layers are kept local. These design elements enable pFLSynth to reliably generalize across multiple sites and synthesis tasks with a unified model. Comprehensive experiments on multi-site MRI data clearly demonstrate the superior performance of pFLSynth against prior federated models. Code for pFLSynth is available at: https://github.com/icon-lab/pFLSynth.\nar X\niv :2\n20 7.\n06 50\n9v 2\n[ ee\nss .I\nV ]\n2 3\nA ug\n2 02\n2\n2 Contributions \u2022 We introduce the first personalized FL method for MRI\nsynthesis to improve performance and flexibility in multisite collaborations. \u2022 To perform diverse tasks with a unified model, our personalized architecture adaptively modulates feature maps to each site and source-target configuration. \u2022 We propose partial network aggregation on downstream layers to maintain site generality, while upstream layers are kept local to maintain site specificity."
        },
        {
            "heading": "II. RELATED WORK",
            "text": "Multi-contrast MRI synthesis has witnessed a recent surge in deep learning methods [33]\u2013[35]. Earlier studies proposed convolutional neural networks (CNN) with pixel-wise loss terms [4]\u2013[6]. To improve capture of tissue details, generative adversarial networks (GAN) were also introduced based on adversarial loss terms that indirectly learn the distribution of target images [7], [36]\u2013[40]. Commonly, maximal performance has been aimed by training a singular model for each source-target configuration [41]. To mitigate computational burden, later studies have instead proposed unified models capable of performing multiple tasks [42]\u2013[46]. However, despite demonstrated success in learning-based synthesis, prior studies have predominantly reported centralized models that require cross-site transfer of imaging data [8].\nTo remedy privacy concerns, FL conducts decentralized training via communicating model parameters [17]. Given their potential for collaborative studies, FL methods have been proposed for several imaging tasks including segmentation [9], [10], [14], [20], classification [12], [21], [25], [26], reconstruction [27]\u2013[29], [47], and unconditional image generation [48]. Yet, the potential of decentralized procedures in multi-modal medical image translation remains largely unexplored. FL has recently been adopted for multi-contrast MRI synthesis based on cycle-consistent models [49], [50]. Promising results were reported for training singular models built independently for each separate source-target configuration. Yet, to our knowledge, no prior FL study has attempted to address heterogeneity in multi-site datasets during MRI synthesis.\nHere, we introduce the first FL method for personalized MRI synthesis that addresses heterogeneity existent for both common and variable source-target configurations across sites (Fig. 1). To do this, pFLSynth introduces novel personalization blocks in the generator of an adversarial model (Fig. 2). Receiving site- and contrast-specific latent variables, these blocks adaptively tune the statistics of feature maps via adaptive instance normalization [51], and tune the attributed importance of feature channels via adaptive channel weighting. Several recent studies have considered adaptive normalization for imaging tasks [47], [52]\u2013[55]. In [52], normalization was used for ultrasound imaging to switch among various output types in a centralized model. In [47], [53], [54], normalization was used to lower model complexity in a cycle-consistent architecture. In [55], normalization was used to synthesize images at intermediate echo times (TE) via a centralized model. Unlike prior studies, we employ normalization layers to personalize a federated model to each individual site\nand source-target configuration, and couple them with novel channel weighting layers for improved specificity.\npFLSynth employs shared generators across sites, albeit the generator is split centrally in its residual bottleneck to enable partial aggregation. A recent study on MRI reconstruction has proposed sharing the encoder in a UNet backbone while keeping the decoder unshared to enhance site specificity in decoded representations [28]. In contrast, here we propose to share the downstream layers of the residual generator while keeping upstream layers local to maintain specificity in encoded representations. Taken together, these unique aspects enable pFLSynth to effectively address heterogeneity in multicontrast data for reliable MRI synthesis."
        },
        {
            "heading": "III. THEORY",
            "text": ""
        },
        {
            "heading": "A. MRI Synthesis with Adversarial Models",
            "text": "Adversarial models have become pervasive in MRI synthesis due to their sensitivity for high-frequency features [7], [43], [56], [57]. For adversarial learning, a generator G synthesizes a target image (x\u0302t = G(xs)) given as input a source image (xs), while a discriminator D distinguishes actual (xt) and synthetic (x\u0302t) target images. Assuming spatially-registered images, a GAN is typically trained to minimize: Lsyn(D, \u03b8) = Exs,xt [\u2212(D(xt)\u2212 1)2 \u2212D(G(xs))2 +\u03bbpix||xt \u2212G(xs)||1], (1) where E denotes expectation, D are training data comprising source-target images, \u03b8 = {\u03b8G, \u03b8D} are model parameters, the first two terms reflect an adversarial loss, the last term reflects a pixel-wise loss with relative weight \u03bbpix. The traditional approach uses centralized training, where data samples from multiple institutions are aggregated in a single repository [8]. This, however, introduces privacy risks for patients.\nAlternatively, decentralized training can be performed via communication between an FL server hosting a global generator (G with \u03b8G), and sites keeping local copies (Gk for site k \u2208 {1, ..,K}) [49]. Discriminators are typically unshared across sites to minimize risk of information leak [58]. In each communication round, local copies are initialized with the global model transmitted by the server (\u03b8kG \u2190 \u03b8G). Local models are then trained to minimize a local synthesis loss:\n(\u03b8kG, \u03b8 k D) = argmin \u03b8k Lksyn(Dk, \u03b8k), (2)\nwhere Dk are training data (xksc , x k tc ), and (sc, tc) denotes the cth source-target configuration at site k (c \u2208 {1, .., C}). After each round, local models are aggregated on the server [15]:\n\u03b8G = K\u2211 k=1 \u03b1k\u03b8kG. (3)\n\u03b1k denote relative site weights typically set to n k\nn , where n is the total number of training samples and nk is the number of training samples at site k. The trained global model (G\u03b8\u2217 ) is eventually used for local inference:\nx\u0302ktc = G \u2217(xksc). (4)\nDomain shifts due to implicit and explicit data heterogeneity will lead to a compromise model among various sites and synthesis tasks, leading to suboptimal performance.\nDALMAZ et al.: ONE MODEL TO UNITE THEM ALL: PERSONALIZED FEDERATED LEARNING OF MULTI-CONTRAST MRI SYNTHESIS 3\nFig. 1: pFLSynth is a personalized federated learning model for MRI synthesis. The proposed architecture contains a mapper to produce siteand task-specific latents. Latents modulate feature maps via novel personalization blocks (PB) inserted across the generator. Partial network aggregation is adopted with shared downstream albeit unshared upstream layers to further promote personalization. These design elements enable pFLSynth to offer robust performance under data heterogeneity within and across sites."
        },
        {
            "heading": "B. Personalized Federated Learning of MRI Synthesis",
            "text": "For improved performance in multi-site studies, we propose a personalized FL method for MRI synthesis, pFLSynth (Fig. 1). pFLSynth leverages novel personalization blocks (PB) composed of Adaptive Instance Normalization (AdaIN) [51] and novel Adaptive Channel Weighting (AdaCW) layers (Fig. 2). Partial network aggregation is adopted to further improve site specificity and communication efficiency.\nB.1 Personalized Architecture: pFLSynth is an adversarial model that receives a source image along with site and sourcetarget configuration information as input. The generator employs a mapper that produces latent variables w for modulating feature maps and a synthesizer for source-to-target translation.\nMapper (M ): M is an LM -layer multi-layer perceptron (MLP). Receiving a binary vector for site index (vk \u2208 ZK2 , Z2 = {0, 1}) and a binary vector for indices of source and target contrasts (uc \u2208 Z2C2 ), it produces a latent vector:\nwkc = M(v k \u2295 uc), (5)\nwhere wkc \u2208 RJ ,\u2295 is concatenation, J is latent dimensionality. Parametrized with \u03b8M , M produces site- and task-specific latents to drive PBs in the synthesizer.\nSynthesizer (S): Receiving as input a source image xs and latent vector wkc , S generates a target image x\u0302t. The backbone is inspired by the ResNet model [7], [59] with a residual bottleneck between an encoder and a decoder (see Fig. 2). For each site and source-target configuration, synthesizer feature maps elicit divergent statistics across spatial and channel dimensions [60]. To mitigate this heterogeneity, we introduce novel PBs inserted after each convolutional block in the synthesizer except for the final decoder block. Let {S1, S2, \u00b7 \u00b7 \u00b7 , SLS} with parameters \u03b8S1,..,LS be the set of synthesizer stages. At the ith stage, input feature maps fi\u22121 \u2208 RFi\u22121,Hi\u22121,Wi\u22121 (Fi\u22121,\nHi\u22121, Wi\u22121 are the number of channels, height and width) are processed via CBi: gi = CBi(fi\u22121) \u2208 RFi,Hi,Wi , (6) where CBi denotes the ith convolutional block.\nPBi first transforms wkc into scale and bias vectors \u03b3i, \u03b2i \u2208 RFi , and then normalizes the statistics of gi:\n\u03b3i(k, c) = Q \u03b3 i w k c + b \u03b3 i ; \u03b2i(k, c) = Q \u03b2 i w k c + b \u03b2 i , (7)\ng \u2032\ni = AdaIN(gi, \u03b3i, \u03b2i) =  \u03b3i[1] gi[1]\u2212\u00b5(gi[1])1 \u03c3(gi[1]) + \u03b2i[1]1 ...\n\u03b3i[Fi] gi[Fi]\u2212\u00b5(gi[Fi])1\n\u03c3(gi[Fi]) + \u03b2i[Fi]1 (8) where Q\u03b3,\u03b2i \u2208 RFi,J and b \u03b3,\u03b2 i \u2208 RFi are learnable projections, 1 \u2208 RHj ,Wj is a matrix of ones, \u00b5, \u03c3 compute the mean and standard deviation of individual channels gi[j] \u2208 RHi,Wi .\nNext, a novel AdaCW layer weights feature channels according to a site-task relevance vector CWi \u2208 RFi , produced by an LCW -layer MLP given the latents wkc :\nCWi(k, c) = MLP(w k c ) (9)\nfi = AdaCW(g \u2032 i, CWi) =  g \u2032 i[1] CWi[1]1 ...\ngi[Fi] CWi[Fi]1  (10) where denotes Hadamard product, and fi is the output feature map. The overall mapping through the synthesizer is a cascade of projections through CBs and PBs: x\u0302t = CBLS \u25e6 PBLS\u22121 \u25e6 CBLS\u22121 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 PB1 \u25e6 CB1(xs, wkc ), (11) where \u25e6 denotes functional composition, and Si := PBi \u25e6CBi for i < Ls while SLs := CBLs (i.e., the final decoder block).\n4\nFig. 2: pFLSynth\u2019s generator comprises a fully-connected mapper and a convolutional synthesizer. The mapper produces site- and taskspecific latents wkc , given indices for site identity vk and source-target configuration uc. The synthesizer computes intermediate feature maps to translate between source and target images. A personalization block (PB) is inserted following each convolutional block that receives wkc . Each PB adaptively modulates feature maps to alter the statistics via AdaIN and channel weights via AdaCW layers.\nPartial Network Aggregation (PNA): S is split at stage LU into two disjoint subsets SU = {S1, \u00b7 \u00b7 \u00b7 , SLU } and SD = {SLU+1, \u00b7 \u00b7 \u00b7 , SLS} where LU is selected from {1, \u00b7 \u00b7 \u00b7 , LS}. The set of upstream stages SU with parameters \u03b8kSU , k \u2208 {1, \u00b7 \u00b7 \u00b7 ,K} are kept locally at each site. In contrast, the set of downstream stages SD with parameters \u03b8SD are shared. Thus, pFLsynth only aggregates SD to improve site specificity in source-image representations. Here, PNA is only exercised on CBs within S. While it can also be adopted for PBs, we observed that using local PBs persistently throughout S performs similarly. Thus, to lower communication costs and potential for information leakage, all PBi with parameters \u03b8kPBi where i \u2208 {1, \u00b7 \u00b7 \u00b7 , LS \u2212 1} are kept locally.\nDiscriminator (D): A local discriminator Dk with parameters \u03b8kD is trained at site k based on a conditional patch-based architecture [7]. Given the source image, Dk estimates the probability that x is an actual target image:\npD = D(x, xs), (12) where x can be an actual or synthetic target image.\nB.2 Federated Training: Decentralized learning is performed for P communication rounds between the server and individual sites (Alg. 1). The downstream synthesizer SD and mapper M are shared across sites, albeit the upstream synthesizer SU and discriminator D are unshared. In the first round, the server randomly initializes a global generator G = {SD,M} with parameters \u03b8G = {\u03b8SD , \u03b8M}. At the start of each round, the server broadcasts the global SD and M to the sites:\n\u03b8kSD \u2190 \u03b8SD ; \u03b8 k M \u2190 \u03b8M ; k = 1, 2, \u00b7 \u00b7 \u00b7 ,K (13)\nLocal SU and D are set to their states in the previous round: \u03b8kSU \u2190 \u03b8 k SU ; \u03b8 k D \u2190 \u03b8kD, (14) A local generator is then formed as: Gk = {(SkU t SkD),Mk}. (15)\nAlgorithm 1: Training of pFLSynth Data: {D1, \u00b7 \u00b7 \u00b7 ,DK} from K sites Input: P : number of communication rounds E: number of local epochs G: global generator with params. \u03b8G = {\u03b8SD , \u03b8M} G1, \u00b7 \u00b7 \u00b7 , GK : local generators with \u03b8G1 , \u00b7 \u00b7 \u00b7 , \u03b8GK D1, \u00b7 \u00b7 \u00b7 , DK : local discriminators with \u03b8D1 , \u00b7 \u00b7 \u00b7 , \u03b8DK Opt(): optimizer for parameter updates FedAvg(): federated averaging Output: \u03b8\u2217\nGk personalized generators\n1 Randomly initialize \u03b8G and \u03b8D1 , \u00b7 \u00b7 \u00b7 , \u03b8DK 2 for p = 1 to P do 3 for k = 1 to K do 4 \u03b8kSD \u2190 \u03b8SD , \u03b8 k M \u2190 \u03b8M // receive global 5 for e = 1 to E do 6 Calculate \u2207\u03b8k\nG Lksyn(Dk) based on Eq. 17\n7 \u03b8kG \u2190 \u03b8 k G \u2212Opt(\u2207\u03b8k G Lksyn(Dk)) 8 Calculate \u2207\u03b8k D Lksyn(Dk) based on Eq. 17 9 \u03b8kD \u2190 \u03b8 k D \u2212Opt(\u2207\u03b8k\nD Lksyn(Dk))\n10 \u03b8SD,M \u2190 FedAvg(\u03b8 k SD,M ) // aggregate\nNext, each local generator is trained for E epochs. Note that pFLSynth consolidates different synthesis tasks within and across sites. Thus, local training data Dk comprise multiple source-target configurations at site k:\nconfigk = {(s1, t1), \u00b7 \u00b7 \u00b7 , (sC , tC)}, (16) where a fixed number of configurations C is assumed at each site. Given x\u0302ktc = G k(xksc , v k, uc), local models are trained to minimize a compound local synthesis loss across tasks:\nLksyn(Dk, \u03b8k) = C\u2211 c=1 Exksc ,xktc [\u2212(D k(xktc , x k sc)\u2212 1) 2\u2212\nDk(x\u0302ktc , x k sc) 2 + \u03bbpix||xktc \u2212 x\u0302 k tc ||1]. (17)\nAt the end of a round, each site sends its downstream synthesizer and mapper to the server for aggregation [61]:\n\u03b8SD = K\u2211 k=1 nk n \u03b8kSD ; \u03b8M = K\u2211 k=1 nk n \u03b8kM . (18)\nDuring inference, each site forms a personalized generator as in Eq. 15 to perform contrast translation:\nx\u0302ktc = G k\u2217(xksc , v k, uc). (19) Note that our federated generator performs an adaptive sourceto-target mapping at each site and for each synthesis task."
        },
        {
            "heading": "IV. METHODS",
            "text": ""
        },
        {
            "heading": "A. Datasets",
            "text": "Experiments were conducted on four multi-contrast MRI datasets: IXI1 , BRATS [62], MIDAS [63], and OASIS [64]. IXI and MIDAS contain data from healthy subjects, BRATS contains data from glioma patients, and OASIS contains data from subjects with cognitive decline. Each dataset was treated as a separate site in the FL setup. Subjects within each site were split into non-overlapping training, validation, and test sets. In IXI, T1-, T2-, and Proton Density (PD)-weighted\n1https://brain-development.org/ixi-dataset/\nDALMAZ et al.: ONE MODEL TO UNITE THEM ALL: PERSONALIZED FEDERATED LEARNING OF MULTI-CONTRAST MRI SYNTHESIS 5\nFig. 3: Federated synthesis under a common task configuration within and across sites, T1\u2192T2. Source, reference target, and synthetic target images from competing methods are shown. Representative results are displayed for a) IXI and b) BRATS. Overall, pFLSynth synthesizes images with fewer artifacts and lower noise levels compared to other federated methods.\nimages from 53 subjects were analyzed with a (25,10,18) split. In BRATS, T1-, T2-, and Fluid Attenuation Inversion Recovery (FR)-weighted images from 55 subjects were analyzed with a (25,10,20) split. In MIDAS, T1- and T2-weighted images from 66 subjects were analyzed with a (48,5,13) split. In OASIS, T1, T2-, and FR-weighted images from 48 subjects were analyzed with a (22,9,17) split. Across the four datasets, the training set contained 2780, 2500, 3874, 2780 cross-sections per sourcetarget configuration, respectively."
        },
        {
            "heading": "B. Competing Methods",
            "text": "We demonstrated pFLSynth against a centralized benchmark [7] and federated baselines [47], [49], [58], [65], [66]. For each method, hyperparameter selection was performed via identical cross-validation procedures. All models shared generators across sites but used a separate local discriminator per site and per source-target configuration for improved performance.\nCentral: A non-federated synthesis model was considered following data aggregation across sites [7]. Loss function and architecture matched pFLSynth, albeit the mapper and PBs were excluded. The Central model serves as a privacyviolating benchmark for pFLSynth.\nFedGAN: A federated synthesis model was implemented with matching loss function and architecture to pFLSynth, but without the mapper and PBs [58]. FedGAN aggregated the entire generator, so it serves as a non-personalized benchmark.\nFedMRI: A federated model proposed for maintaining site specificity in MRI reconstruction was considered [65]. FedMRI used a UNet backbone with a shared encoder and site-specific decoders [65]. For adversarial synthesis, a discriminator with matching loss function to pFLSynth was used.\nFedMM: Federated implementation of a unified synthesis model (MM-GAN) was considered [66]. FedMM used a UNet generator and multiple input-output channels to cope with different source-target configurations [66]. The entire generator\nwas aggregated. For fair comparison, curriculum learning was omitted to ensure standard sample selection.\nFedMed: A recent federated model for MRI synthesis was considered [49]. FedMed (abbreviated from FedMed-GAN) used a UNet generator that was entirely aggregated [49]. FedMed was originally proposed for unpaired synthesis. Only the forward mapping generator was retained for paired synthesis, and matching loss function to pFLSynth was used. For\n6\nFig. 4: Federated synthesis under a variable task configuration within and across sites (T2\u2192T1 and T2\u2192PD in IXI, T2\u2192T1 and FR\u2192T2 in BRATS, T1\u2192T2 and T2\u2192T1 in MIDAS, T2\u2192T1 and T2\u2192FR in OASIS). Source, reference target, and synthetic target images from competing methods are shown. Representative results are displayed for a) T2\u2192PD in IXI and b) T2\u2192FR in OASIS. Overall, pFLSynth synthesizes images with fewer artifacts and lower noise levels compared to other federated methods.\nfair comparison, differential privacy procedures were omitted. FedCycle: Originally proposed for low-dose CT denoising, FedCycle (abbreviated from FedCycleGAN) is a federated model based on a switchable backbone to reduce model complexity [47]. FedCycle was implemented with a UNet generator and adaptive normalization as in [47]. To adopt FedCycle for paired synthesis, only the forward mapping generator was retained, and matching loss function to pFLSynth was used. The switching mechanism was used to adapt the model to different source-target configurations as in pFLSynth."
        },
        {
            "heading": "C. Architectural Details",
            "text": "M in pFLSynth had LM = 6 layers. S followed an encoder-bottleneck-decoder structure with LS = 15 stages. The encoder had 3 stages (e1\u2212e3), each with a convolutional layer followed by a PB, and kernel sizes were 7, 3, 3 across stages. The bottleneck had 9 residual layers (r1 \u2212 r9) with kernel size 3 [59], each residual layer followed by a PB. The decoder had 3 convolutional layers (d1 \u2212 d3) of kernel sizes 3, 3, 7, with the first two layers followed by PBs. D had 5 convolutional layers of kernel size 4. An FL setup with K = 4 different datasets was considered, so a site index of vk \u2208 Z42 was used. Given multiple source-task configurations, uniform random sample selection was utilized for learning different tasks. The datasets examined included T1, T2, PD and FR contrasts, so a source-target configuration index of uc \u2208 Z82 was formed. The mapper received these indices and produced a latent vector wkc \u2208 R512. The MLP in AdaCW had 2 layers with 64 hidden neurons. The splitting layer in the synthesizer (LU ) was selected as r5 based on validation performance."
        },
        {
            "heading": "D. Modeling Procedures",
            "text": "For fair comparison, all competing models were trained using the same discriminator (Eq. 12) and compound synthesis loss (Eq. 17). Hyperparameters including pixel-wise\nloss weight, number of communication rounds, number of epochs, and learning rate were selected via cross-validation. A common set of hyperparameters that yielded near-optimal results across models and datasets were selected. Training was performed via the Adam optimizer with batch size of 1 and \u03b21 = 0.5, \u03b22 = 0.999. For centralized models, training lasted 150 epochs. Training lasted P=150 rounds for federated models with E=1 local epochs each. Learning rate was set as 0.0002 during the initial 75 epochs and linearly decayed to 0 during the remaining epochs. The pixel-wise loss weight was set to \u03bbpix = 100. Models were implemented using PyTorch and Nvidia RTX 3090 GPUs. Performance was evaluated via PSNR, Structural Similarity Index (SSIM), and Fretchet Inception Distance (FID) [67] metrics between synthetic and reference target-contrast images. Results were reported as mean and standard deviation across test subjects. Wilcoxon signed-rank tests were conducted to assess the significance of PSNR/SSIM differences among competing methods."
        },
        {
            "heading": "V. RESULTS",
            "text": ""
        },
        {
            "heading": "A. Common Task Configuration",
            "text": "FL experiments were conducted in a four-site setup based on IXI, BRATS, MIDAS, and OASIS datasets. We first demonstrated pFLSynth for decentralized modeling of a single synthesis task across sites (i.e., T1\u2192T2). This configuration reflects the influence of implicit heterogeneity in the data distribution due to cross-site differences in scanners and protocols used to collect the same MRI contrasts. Performance metrics for competing methods are listed in Table I. pFLSynth outperforms all federated baselines at each site (p<0.05), and offers on par performance with the central benchmark. On average across sites, pFLSynth achieves 0.8dB PSNR, 1.4% SSIM, 5.8 FID improvement over competing FL baselines. Representative images are displayed in Fig. 3. pFLSynth\nDALMAZ et al.: ONE MODEL TO UNITE THEM ALL: PERSONALIZED FEDERATED LEARNING OF MULTI-CONTRAST MRI SYNTHESIS 7\nIXI BRATS MIDAS OASIS T1\u2192T2 T2\u2192PD T1\u2192T2 FR\u2192T2 T1\u2192T2 T2\u2192T1 T1\u2192T2 T2\u2192FR P \u21d1 25.2\u00b11.3 26.8\u00b11.8 24.5\u00b11.0 21.8\u00b11.5 22.7\u00b11.0 20.8\u00b11.0 24.3\u00b11.2 21.9\u00b11.4 Central S \u21d1 89.6\u00b12.9 86.6\u00b16.4 90.4\u00b11.1 81.7\u00b15.1 70.2\u00b16.7 67.1\u00b12.7 81.6\u00b13.6 79.8\u00b13.6\nF \u21d3 29.1 59.1 45.3 85.1 48.0 11.5 39.1 26.4\nFe de\nra te\nd m\nod el\ns\nP \u21d1 27.9\u00b11.2 31.2\u00b11.0 25.6\u00b11.0 23.4\u00b11.0 25.9\u00b10.6 25.1\u00b11.1 23.4\u00b11.8 21.1\u00b11.9 pFLSynth S \u21d1 93.7\u00b11.5 97.2\u00b10.5 92.4\u00b11.2 88.8\u00b11.8 89.0\u00b11.2 84.1\u00b12.1 81.7\u00b13.3 76.4\u00b15.2\nF \u21d3 8.6 22.1 23.2 42.6 10.4 12.4 46.7 29.7 P \u21d1 26.8\u00b11.1 29.7\u00b10.9 23.9\u00b11.1 20.1\u00b11.5 20.7\u00b10.9 25.4\u00b11.0 22.0\u00b11.6 19.9\u00b11.8\nFedGAN S \u21d1 92.2\u00b11.7 96.2\u00b10.6 89.9\u00b11.5 81.5\u00b12.7 76.4\u00b13.2 85.5\u00b12.1 78.2\u00b13.3 75.2\u00b15.5 F \u21d3 14.4 26.0 42.4 107.0 117.2 14.3 47.7 40.1 P \u21d1 27.0\u00b11.1 30.7\u00b10.9 23.7\u00b11.1 22.7\u00b10.9 24.9\u00b10.5 23.1\u00b10.9 21.8\u00b11.3 20.0\u00b11.5 FedMRI S \u21d1 92.7\u00b11.5 96.6\u00b10.5 89.8\u00b11.7 87.8\u00b11.6 86.7\u00b11.2 79.0\u00b12.2 77.7\u00b12.7 72.3\u00b15.0 F \u21d3 12.6 26.0 43.4 56.6 21.6 33.8 40.4 38.0 P \u21d1 27.1\u00b11.0 30.3\u00b10.8 24.2\u00b10.8 22.9\u00b10.9 17.5\u00b10.8 24.0\u00b11.4 21.7\u00b11.3 19.2\u00b11.3 FedMM S \u21d1 92.7\u00b11.6 96.5\u00b10.5 90.5\u00b11.1 87.9\u00b11.8 62.5\u00b13.2 80.8\u00b12.5 72.6\u00b14.2 68.8\u00b16.2 F \u21d3 13.4 24.8 36.4 46.6 169.6 26.9 45.4 39.7 P \u21d1 26.2\u00b10.9 28.7\u00b10.8 22.8\u00b11.1 20.1\u00b11.0 25.5\u00b10.7 24.0\u00b11.4 21.4\u00b11.2 19.6\u00b11.8 FedMed S \u21d1 91.5\u00b11.6 95.1\u00b10.7 88.2\u00b11.6 82.3\u00b12.0 88.9\u00b11.2 80.8\u00b12.5 75.7\u00b13.3 71.2\u00b15.3 F \u21d3 18.8 29.4 59.9 121.4 19.9 17.3 53.9 39.9 P \u21d1 26.8\u00b11.0 28.7\u00b10.8 23.5\u00b10.9 18.9\u00b11.0 17.3\u00b10.9 25.5\u00b11.0 21.6\u00b11.3 19.7\u00b11.4 FedCycle S \u21d1 92.5\u00b11.5 95.6\u00b10.7 89.3\u00b11.0 80.1\u00b12.7 62.0\u00b13.2 84.0\u00b12.2 65.7\u00b15.1 64.0\u00b18.4 F \u21d3 16.5 31.0 56.1 139.1 176.8 14.0 49.8 47.9\nTABLE II: PSNR (P, dB), SSIM (S, %), and FID (F) performance in a variable set of synthesis tasks. FR denotes FLAIR.\nreduces artifacts and noise, with the closest tissue depiction to the ground-truth target images, particularly near pathology.\nB. Variable Task Configuration We then demonstrated pFLSynth for simultaneous modeling of diverse tasks within and across sites (T1\u2192T2 and T2\u2192PD in IXI, T1\u2192T2 and FR\u2192T2 \u2013FR denotes FLAIR\u2013 in BRATS, T1\u2192T2 and T2\u2192T1 in MIDAS, T1\u2192T2 and T2\u2192FR in OASIS). This variable configuration creates explicit heterogeneity rendering suboptimal performance in a unified synthesis model, even for the central benchmark. Yet, we hypothesized that the personalized pFLSynth model should cope more reliably with heterogeneity. Performance metrics are listed in Table II. pFLSynth outperforms all federated baselines at each site (p<0.05), except for MIDAS where FedCycle has higher PSNR/on par SSIM in T2\u2192T1, FedGAN has higher PSNR/SSIM in T2\u2192T1, FedMed has on par SSIM in T1\u2192T2, and for OASIS where FedMRI has lower FID in T1\u2192T2. On average across tasks and sites, pFLSynth achieves 2.0dB PSNR, 5.0% SSIM, 24.9 FID improvement over federated baselines. pFLSynth also performs competitively against the central benchmark improving PSNR by 2.0dB, SSIM by 7.0%, FID by 18.5. Representative images are shown in Fig. 4. Again, pFLSynth reduces artifacts and noise to accurately depict brain tissue. Altogether, the findings on common and variable configurations indicate the robustness of pFLSynth against implicit and explicit heterogeneity in MRI synthesis."
        },
        {
            "heading": "C. Ablation Studies",
            "text": "Ablation studies were conducted to assess the contributions of major design elements in pFLSynth. First, pFLSynth was compared against variants where AdaIN layers were ablated, AdaCW layers were ablated, full network aggregation was\nused, and the mapper was ablated (i.e., site and sourcetarget configuration indices directly input to PBs). Performance metrics for the common task configuration are listed in Table III. pFLSynth outperforms all ablated variants at each site, except for the mapper-ablated variant that yields slightly lower FID in IXI, and the AdaIN-ablated variant that yields slightly lower FID in BRATS. These results demonstrate the importance of PBs, partial network aggregation, and the mapper to synthesis performance. We then compared pFLSynth against variants where site index was removed, and source-target configuration index was removed. Performance in the variable task configuration is summarized in Table IV. pFLSynth outperforms ablated variants across sites and tasks, except for occasionally albeit slightly higher PSNR and\n8 IXI BRATS MIDAS OASIS T1\u2192T2 T2\u2192PD T1\u2192T2 FR\u2192T2 T1\u2192T2 T2\u2192T1 T1\u2192T2 T2\u2192FR\nP \u21d1 27.9\u00b11.2 31.2\u00b11.0 25.6\u00b11.0 23.4\u00b11.0 26.0\u00b10.6 25.1\u00b11.1 23.4\u00b11.8 21.1\u00b11.9 pFLSynth S \u21d1 93.7\u00b11.5 97.2\u00b10.5 92.4\u00b11.2 88.8\u00b11.8 89.0\u00b11.2 84.1\u00b12.1 81.7\u00b13.3 76.4\u00b15.2\nF \u21d3 8.6 22.1 23.2 42.6 10.4 12.4 46.7 29.7 P \u21d1 27.6\u00b11.2 30.9\u00b11.0 25.2\u00b11.0 18.5\u00b11.0 25.6\u00b10.6 23.8\u00b11.0 23.2\u00b11.7 19.7\u00b11.9\nw/o vk S \u21d1 93.4\u00b11.4 97.0\u00b10.4 91.5\u00b11.1 80.5\u00b12.0 89.2\u00b11.2 82.2\u00b11.2 74.9\u00b14.5 71.1\u00b16.3 F \u21d3 13.7 22.5 36.2 147.9 17.8 25.0 49.4 39.7 P \u21d1 27.4\u00b11.2 30.3\u00b10.9 24.6\u00b11.1 7.1\u00b18.7 25.8\u00b10.6 25.2\u00b11.0 22.2\u00b11.6 20.5\u00b12.0 w/o uc S \u21d1 93.6\u00b11.4 97.0\u00b10.4 91.2\u00b11.2 29.7\u00b12.3 88.9\u00b11.1 84.7\u00b12.1 80.5\u00b13.1 74.5\u00b15.4 F \u21d3 9.8 22.7 23.5 162.8 10.6 13.0 47.3 31.1\nTABLE IV: Performance for pFLSynth and variant models ablated of site index (vk) and source-target configuration index (uc).\nSSIM with variants in MIDAS. These results demonstrate the importance of using personalized latents per site and sourcetarget configuration.\nNext, we examined the effect of delayed insertion of a specific site or translation task to the FL setup. To do this, spare digits were reserved in the site index and source-target configuration index to code late joiners included halfway during the training. For delayed site analysis, a single site was held out in the common task configuration and pFLSynth was compared to a variant with ablated site index. For delayed task analysis, a single task was held out in the variable task configuration and pFLSynth was compared to a variant with ablated source-target configuration index. Performance metrics for the held-out site and task are listed in Table V (there were unsubstantial differences for non-held-out sites/tasks). Models with delayed site or task perform competitively with the original model including all sites and tasks, and they outperform variants with ablated site or source-target configuration index. These results suggest that pFLSynth shows a degree of inherent reliability against delayed insertion.\nLastly, we assessed the influence of PBs and PNA on possible information leakage among FL sites. Recent studies posit layer-wise measures to assess leakage in network models [68]. Thus, we measured the similarity of activation maps in local synthesizer layers to assess the potential for leakage [69]. A random set of 50 training source images were selected from each site and projected separately through all local synthesizers. Similarity for a given source image was taken as Spearman\u2019s correlation coefficient between the activation maps it elicits in separate sites. Fig. 5 displays similarity for pFLSynth, variants with ablated PBs and/or PNA, and a variant that shared PBs across sites. Similarity of activation maps is lowered by inclusion of both PBs and PNA. Unshared PBs in pFLSynth further reduce similarity in activation maps, suggesting enhanced reliability against leakage."
        },
        {
            "heading": "D. Complexity and Communication Efficiency",
            "text": "A practical concern for FL models is efficiency in decentralized settings. For each model, Table VI lists number of total versus communicated parameters. While pFLSynth has moderately higher complexity, it has comparable training and inference times to other models (unreported). Importantly, as pFLSynth does not communicate upstream generator layers, it performs competitively in communication efficiency, showcasing an added benefit of PNA."
        },
        {
            "heading": "VI. DISCUSSION",
            "text": "Federated MRI synthesis has to operate under distributional heterogeneity in multi-site imaging data [21]. A recent study has considered FedAvg optimization of cycle-consistent models for MRI synthesis [49]. However, no prior study has proposed a dedicated mechanism to address data heterogeneity beyond the level inherently offered by FedAvg. To our knowledge, pFLSynth is the first FL method to personalize a global synthesis model to each individual site and translation task. Experiments on multi-site MRI data demonstrate that pFLSynth offers on par performance to a central benchmark while outperforming federated baselines. Our results suggest that pFLSynth can improve generalizability and flexibility in multi-site collaborations by permitting training on imaging\nDALMAZ et al.: ONE MODEL TO UNITE THEM ALL: PERSONALIZED FEDERATED LEARNING OF MULTI-CONTRAST MRI SYNTHESIS 9\ndata from diverse sources and protocols. FL avoids transfer of imaging data to mitigate patient privacy risks. Yet, inference attacks might leak information about training data from model parameters [8]. Here, we considered an FL setup where only generators were shared across sites. Yet, the discriminators were never communicated since our experiments in the initial phases of the study indicated that sharing discriminators did not elicit any performance benefit. This setup is reported to be relatively resilient against inference attacks [70]. Nevertheless, potential risks can be further minimized by adopting differentially private training [49], [71], or by extending the size and diversity of the training datasets to implicitly improve privacy [72]. Future studies are warranted to systematically examine the privacy properties of FL-based methods in multi-contrast MRI synthesis.\nHere, we primarily considered an FL setup where all sites contributed to the entire training process. When late-joining sites were present, successful learning was achieved given a reasonable number of communication rounds after inclusion. An alternative is to use local mappers at each site to learn sitespecific latents without needing a site index. Certain scenarios might involve inference at a site that was completely excluded from training. In those cases, an average model across training sites might yield suboptimal performance at a held-out site. To improve performance, transfer-learning procedures can be adopted to fine-tune the trained model on a compact dataset from the held-out site [7].\nThe proposed method can be developed along several technical lines. We performed training on paired datasets with registered source and target images from a matching set of subjects. Utilization of unpaired data can facilitate compilation of broader datasets for training substantially complex models. In those cases, cycle-consistent [73], [74] or semi-supervised training [75] strategies can be adopted. Unpaired learning might further be enhanced via dedicated spatial alignment modules [50]. We demonstrated pFLSynth for one-to-one tasks with a single source and a single target contrast. When information to synthesize the target modality is not sufficiently evident in a single source modality, pFLSynth can be generalized to perform many-to-one mapping [41], [43], [66]. Recent studies have reported benefits for training centralized models based on transformers instead of CNNs [46], [76]. Relatively light transformer modules might be preferable to maintain reasonable computation and communication costs."
        },
        {
            "heading": "VII. CONCLUSION",
            "text": "We introduced a novel personalized FL method for multicontrast MRI synthesis based on an adversarial model equipped with personalization blocks and partial network aggregation. Benefits over prior federated methods were demonstrated for brain image synthesis considering different public datasets and source-target configurations. Improved generalization against implicit and explicit domain shifts renders\npFLSynth a promising candidate for multi-site collaborations. pFLSynth might also be adopted for other image translation tasks involving CT or PET, and other dense-prediction tasks such as reconstruction or super-resolution.\nREFERENCES\n[1] S. Bakas et al., \u201cAdvancing The Cancer Genome Atlas glioma MRI collections with expert segmentation labels and radiomic features,\u201d Sci. Data, vol. 4, 2017. [2] K. Krupa and M. Bekiesin\u0301ska-Figatowska, \u201cArtifacts in Magnetic Resonance Imaging,\u201d Pol. J. Radiol., vol. 80, pp. 93\u2013106, 2015. [3] J. E. Iglesias, E. Konukoglu, D. Zikic, B. Glocker, K. Van Leemput, and B. Fischl, \u201cIs Synthesizing MRI Contrast Useful for Inter-modality Analysis?\u201d in Med. Image Comput. Comput. Assist. Interv., 2013, pp. 631\u2013638. [4] V. Sevetlidis, M. V. Giuffrida, and S. A. Tsaftaris, \u201cWhole Image Synthesis Using a Deep Encoder-Decoder Network,\u201d in Simul. Synth. Med. Imaging, 2016, pp. 127\u2013137. [5] T. Joyce, A. Chartsias, and S. A. Tsaftaris, \u201cRobust multi-modal MR image synthesis,\u201d in Med. Image Comput. Comput. Assist. Interv., 2017, pp. 347\u2013355. [6] W. Wei et al., \u201cFluid-attenuated inversion recovery MRI synthesis from multisequence MRI using three-dimensional fully convolutional networks for multiple sclerosis,\u201d J. Med. Imaging, vol. 6, no. 1, p. 014005, 2019. [7] S. U. Dar, M. Yurt, L. Karacan, A. Erdem, E. Erdem, and T. C\u0327ukur, \u201cImage Synthesis in Multi-Contrast MRI With Conditional Generative Adversarial Networks,\u201d IEEE Trans. Med. Imag., vol. 38, no. 10, pp. 2375\u20132388, 2019. [8] G. A. Kaissis, M. R. Makowski, D. Ru\u0308eckert, and R. F. Braren, \u201cSecure, privacy-preserving and federated machine learning in medical imaging,\u201d Nat. Mach. Intell., vol. 2, no. 6, pp. 305\u2013311, 2020. [9] W. Li et al., \u201cPrivacy-Preserving Federated Brain Tumour Segmentation,\u201d in Mach. Learn. Med. Imag., 2019, pp. 133\u2013141. [10] M. J. Sheller, G. A. Reina, B. Edwards, J. Martin, and S. Bakas, \u201cMultiinstitutional Deep Learning Modeling Without Sharing Patient Data: A Feasibility Study on Brain Tumor Segmentation,\u201d in Med. Image Comput. Comput. Assist. Interv., 2019, pp. 92\u2013104. [11] N. Rieke et al., \u201cThe future of digital health with federated learning,\u201d npj Digital Medicine, vol. 3, no. 1, p. 119, 2020. [12] H. R. Roth et al., \u201cFederated Learning for Breast Density Classification: A Real-World Implementation,\u201d in DART, DCL, 2020, p. 181\u2013191. [13] X. Li, Y. Gu, N. Dvornek, L. H. Staib, P. Ventola, and J. S. Duncan, \u201cMulti-site fMRI analysis using privacy-preserving federated learning and domain adaptation: ABIDE results,\u201d Med. Image. Anal., vol. 65, p. 101765, 2020. [14] Q. Liu, C. Chen, J. Qin, Q. Dou, and P. Heng, \u201cFedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space,\u201d in Comput. Vis. Pattern Recognit., 2021, pp. 1013\u20131023. [15] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, \u201cCommunication-efficient learning of deep networks from decentralized data,\u201d in AISTATS, 2017. [16] J. Wang et al., \u201cA Field Guide to Federated Optimization,\u201d arXiv:2107.06917, 2021. [17] T. Li, A. K. Sahu, A. S. Talwalkar, and V. Smith, \u201cFederated Learning: Challenges, Methods, and Future Directions,\u201d IEEE Signal. Process. Mag., vol. 37, pp. 50\u201360, 2020. [18] N. Rieke et al., \u201cThe future of digital health with federated learning,\u201d npj Digital Medicine, vol. 3, 12 2020. [19] M. Sheller et al., \u201cFederated learning in medicine: facilitating multiinstitutional collaborations without sharing patient data,\u201d Sci. Rep., vol. 10, 07 2020. [20] H. R. Roth et al., \u201cFederated Whole Prostate Segmentation in MRI with Personalized Neural Architectures,\u201d in Med. Image Comput. Comput. Assist. Interv. Cham: Springer, 2021, pp. 357\u2013366. [21] X. Li, M. Jiang, X. Zhang, M. Kamp, and Q. Dou, \u201cFedBN: Federated Learning on Non-IID Features via Local Batch Normalization,\u201d in Int. Conf. Learn. Represent., 2021. [22] C. I. Bercea, B. Wiestler, D. Rueckert, and S. Albarqouni, \u201cFedDis: Disentangled Federated Learning for Unsupervised Brain Pathology Segmentation,\u201d arXiv:2103.03705, 2021. [23] S. Pati et al., \u201cThe Federated Tumor Segmentation (FeTS) Challenge,\u201d arXiv:2105.05874, 2021.\n10\n[24] D. Yang et al., \u201cFederated semi-supervised learning for COVID region segmentation in chest CT using multi-national data from China, Italy, Japan,\u201d Med. Image. Anal., vol. 70, p. 101992, 2021. [25] Z. Yan, J. Wicaksana, Z. Wang, X. Yang, and K.-T. Cheng, \u201cVariationAware Federated Learning With Multi-Source Decentralized Medical Image Data,\u201d IEEE J. Biomed. Health Inform., vol. 25, no. 7, pp. 2615\u2013 2628, 2021. [26] S. Park, G. Kim, J. Kim, B. Kim, and J. C. Ye, \u201cFederated Split TaskAgnostic Vision Transformer for COVID-19 CXR Diagnosis,\u201d in Adv. Neural Inf. Process. Syst., 2021. [27] P. Guo, P. Wang, J. Zhou, S. Jiang, and V. M. Patel, \u201cMulti-institutional Collaborations for Improving Deep Learning-based Magnetic Resonance Image Reconstruction Using Federated Learning,\u201d arXiv:2103.02148, 2021. [28] C.-M. Feng, Y. Yan, H. Fu, Y. Xu, and L. Shao, \u201cSpecificity-Preserving Federated Learning for MR Image Reconstruction,\u201d arXiv:2112.05752, 2021. [29] G. Elmas et al., \u201cFederated Learning of Generative Image Priors for MRI Reconstruction,\u201d arXiv:2202.04175, 2022. [30] R. Vemulapalli, H. Van Nguyen, and S. K. Zhou, \u201cUnsupervised CrossModal Synthesis of Subject-Specific Scans,\u201d in Int. Conf. Comput. Vis., 2015, pp. 630\u2013638. [31] A. Jog, A. Carass, S. Roy, D. L. Pham, and J. L. Prince, \u201cRandom forest regression for magnetic resonance image synthesis,\u201d Med. Image. Anal., vol. 35, pp. 475\u2013488, 2017. [32] Y. Huang, L. Shao, and A. F. Frangi, \u201cCross-Modality Image Synthesis via Weakly Coupled and Geometry Co-Regularized Joint Dictionary Learning,\u201d IEEE Trans. Med. Imag., vol. 37, no. 3, pp. 815\u2013827, 2018. [33] C. Bowles et al., \u201cPseudo-healthy Image Synthesis for White Matter Lesion Segmentation,\u201d in Simul. Synth. Med. Imaging, 2016, pp. 87\u201396. [34] A. Chartsias, T. Joyce, R. Dharmakumar, and S. A. Tsaftaris, \u201cAdversarial Image Synthesis for Unpaired Multi-modal Cardiac Data,\u201d in Simul. Synth. Med. Imaging, 2017, pp. 3\u201313. [35] D. Nie et al., \u201cMedical Image Synthesis with Deep Convolutional Adversarial Networks,\u201d IEEE Trans. Biomed. Eng., vol. 65, no. 12, pp. 2720\u20132730, 2018. [36] K. Armanious et al., \u201cMedGAN: Medical image translation using GANs,\u201d Comput. Med. Imaging Graph., vol. 79, p. 101684, 2020. [37] B. Yu, L. Zhou, L. Wang, Y. Shi, J. Fripp, and P. Bourgeat, \u201cEa-GANs: Edge-Aware Generative Adversarial Networks for Cross-Modality MR Image Synthesis,\u201d IEEE Trans. Med. Imag., vol. 38, no. 7, pp. 1750\u2013 1762, 2019. [38] T. Zhou, H. Fu, G. Chen, J. Shen, and L. Shao, \u201cHi-Net: HybridFusion Network for Multi-Modal MR Image Synthesis,\u201d IEEE Trans. Med. Imag., vol. 39, no. 9, pp. 2772\u20132781, 2020. [39] Y. Luo et al., \u201cEdge-preserving MRI image synthesis via adversarial network with iterative multi-scale fusion,\u201d Neurocomputing, vol. 452, pp. 63\u201377, 2021. [40] B. Zhan et al., \u201cLR-cGAN: Latent representation based conditional generative adversarial network for multi-modality MRI synthesis,\u201d Biomed. Signal Process. Control, vol. 66, p. 102457, 2021. [41] M. Yurt, S. U. Dar, A. Erdem, E. Erdem, K. K. Oguz, and T. C\u0327ukur, \u201cmustGAN: multi-stream Generative Adversarial Networks for MR Image Synthesis,\u201d Med. Image. Anal., vol. 70, p. 101944, 2021. [42] A. Sharma and G. Hamarneh, \u201cMissing MRI pulse sequence synthesis using multi-modal generative adversarial network,\u201d IEEE Trans. Med. Imag., vol. 39, no. 4, pp. 1170\u20131183, 2020. [43] D. Lee, J. Kim, W.-J. Moon, and J. C. Ye, \u201cCollaGAN: Collaborative GAN for Missing Image Data Imputation,\u201d in Comput. Vis. Pattern Recognit., 2019, pp. 2487\u20132496. [44] H. Li et al., \u201cDiamondGAN: Unified Multi-modal Generative Adversarial Networks for MRI Sequences Synthesis,\u201d in Med. Image Comput. Comput. Assist. Interv., 2019, pp. 795\u2013803. [45] G. Wang et al., \u201cSynthesize High-Quality Multi-Contrast Magnetic Resonance Imaging From Multi-Echo Acquisition Using Multi-Task Deep Generative Model,\u201d IEEE Trans. Med. Imag., vol. 39, no. 10, pp. 3089\u20133099, 2020. [46] O. Dalmaz, M. Yurt, and T. C\u0327ukur, \u201cResViT: Residual vision transformers for multi-modal medical image synthesis,\u201d IEEE Trans. Med. Imag., pp. 1\u20131, 2022. doi:10.1109/TMI.2022.3167808. [47] J. Song and J. C. Ye, \u201cFederated CycleGAN for Privacy-Preserving Image-to-Image Translation,\u201d arXiv:2106.09246, 2021. [48] S. Che et al., \u201cFederated Multi-View Learning for Private Medical Data Integration and Analysis,\u201d ACM Trans. Intell. Syst. Technol., vol. 13, no. 4, 2022. [49] G. Xie et al., \u201cFedMed-GAN: Federated Domain Translation on Unsupervised Cross-Modality Brain Image Synthesis,\u201d arXiv:2201.08953, 2022. [50] G. Xie, J. Wang, Y. Huang, Y. Zheng, F. Zheng, and Y. Jin, \u201cFedMedATL: Misaligned Unpaired Brain Image Synthesis via Affine Transform Loss,\u201d arXiv:2201.12589, 2022. [51] X. Huang and S. J. Belongie, \u201cArbitrary style transfer in real-time with adaptive instance normalization,\u201d Int. Conf. Comput. Vis., pp. 1510\u2013 1519, 2017. [52] S. Khan, J. Huh, and J. C. Ye, \u201cSwitchable and Tunable Deep Beamformer Using Adaptive Instance Normalization for Medical Ultrasound,\u201d IEEE Trans. Med. Imag., vol. 41, no. 2, pp. 266\u2013278, 2022. [53] J. Gu and J. C. Ye, \u201cAdaIN-based tunable CycleGAN for Efficient Unsupervised Low-Dose CT Denoising,\u201d IEEE Trans. Comput. Imag., vol. PP, pp. 1\u20131, 2021. [54] H. Zhang, H. Li, J. Dillman, N. Parikh, and L. He, \u201cMulti-Contrast MRI Image Synthesis Using Switchable Cycle-Consistent Generative Adversarial Networks,\u201d Diagnostics, vol. 12, p. 816, 03 2022. [55] J. Denck, J. Guehring, A. Maier, and E. Rothgang, \u201cMR-contrast-aware image-to-image translations with generative adversarial networks,\u201d Int. J. Comput. Assist. Rad. Surge., vol. 16, 06 2021. [56] A. Beers et al., \u201cHigh-resolution medical image synthesis using progressively grown generative adversarial networks,\u201d arXiv:1805.03144, 2018. [57] K. Armanious et al., \u201cMedGAN: Medical image translation using GANs,\u201d Comput. Med. Imaging Grap., vol. 79, p. 101684, 2019. [58] M. Rasouli, T. Sun, and R. Rajagopal, \u201cFedGAN: Federated Generative Adversarial Networks for Distributed Data,\u201d arXiv:2006.07228, 2020. [59] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep Residual Learning for Image Recognition,\u201d in Comput. Vis. Pattern Recognit., 2016, pp. 770\u2013778. [60] Y. Wang, W. Huang, F. Sun, T. Xu, Y. Rong, and J. Huang, \u201cDeep Multimodal Fusion by Channel Exchanging,\u201d in Adv. Neural Inf. Process. Syst., 2020. [61] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, \u201cCommunication-efficient learning of deep networks from decentralized data,\u201d in AISTATS, 2017. [62] B. H. Menze et al., \u201cThe Multimodal Brain Tumor Image Segmentation Benchmark (BRATS),\u201d IEEE Trans. Med. Imag., vol. 34, no. 10, pp. 1993\u20132024, 2015. [63] E. Bullitt et al., \u201cVessel Tortuosity and Brain Tumor Malignancy,\u201d Academic radiology, vol. 12, pp. 1232\u201340, 11 2005. [64] P. J. LaMontagne et al., \u201cOASIS-3: Longitudinal Neuroimaging, Clinical, and Cognitive Dataset for Normal Aging and Alzheimer Disease,\u201d medRxiv:2019.12.13.1901490, 2019. [65] C.-M. Feng, Y. Yan, H. Fu, Y. Xu, and L. Shao, \u201cSpecificity-Preserving Federated Learning for MR Image Reconstruction,\u201d arXiv:2112.05752, 2021. [66] A. Sharma and G. Hamarneh, \u201cMissing MRI Pulse Sequence Synthesis Using Multi-Modal Generative Adversarial Network,\u201d IEEE Trans. Med. Imag., vol. 39, pp. 1170\u20131183, 2020. [67] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, \u201cGANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium,\u201d in Adv Neural Inf Process Syst, 2017, p. 6629\u20136640. [68] R. Shwartz-Ziv and N. Tishby, \u201cOpening the black box of deep neural networks via information,\u201d arXiv:1703.00810, 2017. [69] F. Mo, A. Borovykh, M. Malekzadeh, H. Haddadi, and S. Demetriou, \u201cLayer-wise characterization of latent information leakage in federated learning,\u201d arXiv:2010.08762, 2020. [70] T. Han et al., \u201cBreaking medical data sharing boundaries by using synthesized radiographs,\u201d Sci Adv, vol. 6, no. 49, p. eabb7973, 2020. [71] A. Ziller, D. Usynin, M. Knolle, K. Hammernik, D. Rueckert, and G. Kaissis, \u201cComplex-valued deep learning with differential privacy,\u201d arXiv:2110.03478, 2021. [72] Q. Feng, C. Guo, F. Benitez-Quiroz, and A. Martinez, \u201cWhen Do GANs Replicate? On the Choice of Dataset Size,\u201d in Int. Conf. Comput. Vis., 2021, pp. 6701\u20136710. [73] J. Wolterink, A. M. Dinkla, M. Savenije, P. Seevinck, C. Berg, and I. Isgum, \u201cDeep MR to CT Synthesis Using Unpaired Data,\u201d in Simul. Synth. Med. Imaging, 2017, pp. 14\u201323. [74] Y. Ge et al., \u201cUnpaired MR to CT Synthesis with Explicit Structural Constrained Adversarial Learning,\u201d in Int. Symp. Biomed. Imaging, 2019, pp. 1096\u20131099. [75] C.-B. Jin et al., \u201cDeep CT to MR Synthesis Using Paired and Unpaired Data,\u201d Sensors, vol. 19, no. 10, p. 2361, 2019. [76] Y. Korkmaz, S. U. Dar, M. Yurt, M. O\u0308zbey, and T. C\u0327ukur, \u201cUnsupervised MRI Reconstruction via Zero-Shot Learned Adversarial Transformers,\u201d IEEE Trans. Med. Imag., vol. 41, no. 7, pp. 1747\u20131763, 2022."
        }
    ],
    "title": "One Model to Unite Them All: Personalized Federated Learning of Multi-Contrast MRI Synthesis",
    "year": 2022
}