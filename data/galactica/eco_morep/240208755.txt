Modeling subrational agents, such as humans or economic households, is inherently challenging due to the difficulty in calibrating reinforcement learning models or collecting data that involves human subjects. Existing work highlights the ability of Large Language Models (LLMs) to address complex reasoning tasks and mimic human communication, while simulation using LLMs as agents shows emergent social behaviors, potentially improving our comprehension of human conduct. In this paper, we propose to investigate the use of LLMs to generate synthetic human demonstrations, which are then used to learn subrational agent policies though Imitation Learning. We make an assumption that LLMs can be used as implicit computational models of humans, and propose a framework to use synthetic demonstrations derived from LLMs to model subrational behaviors that are characteristic of humans (e.g., myopic behavior or preference for risk aversion). We experimentally evaluate the ability of our framework to model sub-rationality through four simple scenarios, including the well-researched ultimatum game and marshmallow experiment. To gain confidence in our framework, we are able to replicate well-established findings from prior human studies associated with the above scenarios. We conclude by discussing the potential benefits, challenges and limitations of our framework. Equal contribution Applied Research Team (ART) IT Department, Bank of Italy, Rome, Italy. This research work was carried out when Andrea Coletta was employed at J.P. Morgan AI Research. The views expressed in this paper are those of the authors and do not necessarily reflect those of the Bank of Italy. J.P. Morgan AI Research, California, USA. J.P. Morgan AI Research, New York, USA. Correspondence to: Andrea Coletta <andrea.coletta@bancaditalia.it>, Kshama Dwarakanath <kshama.dwarakanath@jpmorgan.com>, Penghang Liu <penghang.liu@jpmchase.com>. 1. 