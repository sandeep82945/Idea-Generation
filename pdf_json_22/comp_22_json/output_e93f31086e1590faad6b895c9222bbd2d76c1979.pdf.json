{
    "abstractText": "Detecting violence in real-time videos is not an easy task even for the most advanced deep learning architectures, considering the subtle details of human behavior that differentiate an ordinary from a violent action. Even with the advances of deep learning, human activity recognition (HAR) in videos can only be achieved at a huge computational cost, most of the time also requiring special hardware for reaching an acceptable accuracy. The author presents in this paper a novice method for violence detection, a sub-area of HAR, which outperforms in speed and accuracy the state of the art methods. The method is based on features extracted from the Pose estimator method OpenPose. These features are then transformed into more representative elements in the context of violence detection, which are then submitted to a LSTM neural network to learn how to identify violence. This work was inspired by the violencedetector.org, the first open-source project for violence detection in real-time videos. KEyWoRDS 3DCNN, Deep Neural Networks, Human Action Recognition, LSTM, OpenPose, Spatio-Temporal Features",
    "authors": [
        {
            "affiliations": [],
            "name": "Felipe Boris De Moura"
        }
    ],
    "id": "SP:eac75de4cbb86b1d19acf6c36f05d1853a8265d4",
    "references": [
        {
            "authors": [
                "S. Akt\u0131",
                "G.A. Tataroglu",
                "H.K. Ekenel"
            ],
            "title": "Vision-based fight detection from surveillance cameras",
            "venue": "Ninth International Conference on Image Processing Theory, Tools and Applications (IPTA),",
            "year": 2019
        },
        {
            "authors": [
                "N. Almaadeed",
                "O. Elharrouss",
                "S. Al-Maadeed",
                "A. Bouridane",
                "A. Beghdadi"
            ],
            "title": "A novel approach for robust multi human action recognition and summarization based on 3D convolutional neural networks. https:// arxiv.org/abs/1907.11272v4",
            "year": 2021
        },
        {
            "authors": [
                "S. Arif",
                "J. Wang",
                "T. Ul Hassan",
                "Z. Fei"
            ],
            "title": "3D-CNN-based fused feature maps with LSTM applied to action recognition",
            "venue": "Future Internet,",
            "year": 2019
        },
        {
            "authors": [
                "C.A. Caetano",
                "Jr."
            ],
            "title": "Motion-based representations for activity recognition [Unpublished doctoral dissertation",
            "venue": "Universidade Federal de Minas Gerais, MG,",
            "year": 2020
        },
        {
            "authors": [
                "Z. Cao",
                "G. Hidalgo",
                "T. Simon",
                "Wei",
                "S.-E",
                "Y. Sheikh"
            ],
            "title": "OpenPose: Realtime multi-person 2D pose estimation using part affinity fields",
            "year": 2019
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural Computation,",
            "year": 1997
        },
        {
            "authors": [
                "A. Javidani",
                "A. Mahmoudi-Aznaveh"
            ],
            "title": "Learning representative temporal features for action recognition",
            "year": 2021
        },
        {
            "authors": [
                "J. Kim",
                "W. Jung",
                "H. Kim",
                "J. Lee"
            ],
            "title": "CyCNN: A rotation invariant CNN using polar mapping and cylindrical convolution layers",
            "year": 2020
        },
        {
            "authors": [
                "S.T. Kim",
                "H.J. Lee"
            ],
            "title": "Lightweight stacked hourglass network for human pose estimation",
            "venue": "Applied Sciences (Basel,",
            "year": 2020
        },
        {
            "authors": [
                "S. Kiranyaz",
                "O. Avci",
                "O. Abdeljaber",
                "T. Ince",
                "M. Gabbouj",
                "D.J. Inman"
            ],
            "title": "1D convolutional neural networks and applications: A survey",
            "venue": "Mechanical Systems and Signal Processing,",
            "year": 2020
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinto"
            ],
            "title": "ImageNet classification with deep convolutional neural networks. NIPS\u201912",
            "venue": "Proceedings of the 25th International Conference on Neural Information Processing Systems,",
            "year": 2012
        },
        {
            "authors": [
                "Y. LeCun",
                "P. Haffner",
                "L. Bottou",
                "Y. Bengio"
            ],
            "title": "Object recognition with gradient-based learning",
            "venue": "Lecture Notes in Computer Science,",
            "year": 1999
        },
        {
            "authors": [
                "B. Mahasseni",
                "S. Todorovic"
            ],
            "title": "Regularizing long short term memory with 3D human-skeleton sequences for action recognition",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 3054-3062)",
            "year": 2016
        },
        {
            "authors": [
                "S.N. Muralikrishna",
                "B. Muniyal",
                "U.D. Acharya",
                "R. Holla"
            ],
            "title": "Enhanced human action recognition using fusion of skeletal joint dynamics and structural features",
            "venue": "Hindawi Journal of Robotics,",
            "year": 2020
        },
        {
            "authors": [
                "J.C. Nunez",
                "R. Cabido",
                "J.J. Pantrigo",
                "A.S. Montemayor",
                "J.F. Velez"
            ],
            "title": "Convolutional neural networks and long short-term memory for skeleton-based human activity and hand gesture recognition",
            "venue": "Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "F. Partika"
            ],
            "title": "Welcome to the first open source project for real time violence detection",
            "year": 2020
        },
        {
            "authors": [
                "B. Peixoto",
                "B. Lavi",
                "J.P.P. Martin",
                "S. Avila",
                "Z. Dias",
                "A. Rocha"
            ],
            "title": "Toward subjective violence detection in videos",
            "venue": "Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP)",
            "year": 2019
        },
        {
            "authors": [
                "M. Perez",
                "A.C. Kot",
                "A. Rocha"
            ],
            "title": "Detection of real-world fights in surveillance videos",
            "venue": "Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP)",
            "year": 2019
        },
        {
            "authors": [
                "H.H. Pham",
                "L. Khoudour",
                "A. Crouzil",
                "P. Zegers",
                "S.A. Velastin"
            ],
            "title": "Exploiting deep residual networks for human action recognition from skeletal data",
            "venue": "Computer Vision and Image Understanding,",
            "year": 2018
        },
        {
            "authors": [
                "K. Pijackova",
                "T. Gotthans"
            ],
            "title": "Radio modulation classification using deep learning architectures",
            "venue": "Proceedings of the 31st International Conference Radioelektronika,",
            "year": 2021
        },
        {
            "authors": [
                "M. Schuster",
                "K.K. Paliwal"
            ],
            "title": "Bidirectional recurrent neural networks",
            "venue": "Transactions on Signal Processing,",
            "year": 1997
        },
        {
            "authors": [
                "F. Serpush",
                "M. Rezaei"
            ],
            "title": "Complex human action recognition in live videos using hybrid FR-DL method",
            "year": 2020
        },
        {
            "authors": [
                "M. Sharif",
                "M.A. Khan",
                "F. Zahid",
                "J.H. Shah",
                "T. Akram"
            ],
            "title": "Human action recognition: A framework of statistical weighted segmentation and rank correlation-based selection, pattern analysis, and applications",
            "venue": "Journal of Control Engineering and Applied Informatics,",
            "year": 2019
        },
        {
            "authors": [
                "A. Sherstinsky"
            ],
            "title": "Fundamentals of recurrent neural network (RNN) and long short-term memory (LSTM) network",
            "venue": "Physica D. Nonlinear Phenomena,",
            "year": 2020
        },
        {
            "authors": [
                "M. Soliman",
                "M. Kamal",
                "M. Nashed",
                "Y. Mostafa",
                "B. Chawky",
                "D. Khattab"
            ],
            "title": "Violence recognition from videos using deep learning techniques",
            "venue": "Proceedings of the 9th International Conference on Intelligent Computing and Information Systems (ICICIS\u201919)",
            "year": 2019
        },
        {
            "authors": [
                "S. Sudhakaran",
                "O. Lanz"
            ],
            "title": "Learning to detect violent videos using convolutional long short-term memory",
            "venue": "Proceedings of the 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)",
            "year": 2017
        },
        {
            "authors": [
                "N. Tasnim",
                "M.K. Islam",
                "Baek",
                "J.-H"
            ],
            "title": "2021). deep learning based human activity recognition using spatiotemporal image formation of skeleton joints",
            "venue": "MDPI Applied Sciences,",
            "year": 2021
        },
        {
            "authors": [
                "A. Ullah",
                "J. Ahmad",
                "K. Muhammad",
                "M. Sajjad",
                "S.W. Baik"
            ],
            "title": "Action recognition in video sequences using deep bi-directional lstm with cnn features",
            "venue": "IEEE Access : Practical Innovations, Open Solutions,",
            "year": 2017
        },
        {
            "authors": [
                "R.R. Varior",
                "M. Haloi",
                "G. Wang"
            ],
            "title": "Gated siamese convolutional neural network architecture for human reidentification",
            "venue": "Lecture Notes in Computer Science,",
            "year": 2016
        },
        {
            "authors": [
                "Y. Yu",
                "X. Si",
                "C. Hu",
                "J. Zhang"
            ],
            "title": "A review of recurrent neural networks: LSTM cells and network architectures",
            "venue": "MIT Neural Computation,",
            "year": 2019
        },
        {
            "authors": [
                "M.D. Zeiler",
                "R. Fergus"
            ],
            "title": "Visualizing and understanding convolutional networks",
            "venue": "In Proceedings of the European Conference on Computer Vision (pp. 818-833)",
            "year": 2013
        }
    ],
    "sections": [
        {
            "text": "DOI: 10.4018/IJCVIP.304462\nThis article published as an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/) which permits unrestricted use, distribution, and production in any medium,\nprovided the author of the original work and original publication source are properly credited.\n*Corresponding Author\nDetecting violence in real-time videos is not an easy task even for the most advanced deep learning architectures, considering the subtle details of human behavior that differentiate an ordinary from a violent action. Even with the advances of deep learning, human activity recognition (HAR) in videos can only be achieved at a huge computational cost, most of the time also requiring special hardware for reaching an acceptable accuracy. The author presents in this paper a novice method for violence detection, a sub-area of HAR, which outperforms in speed and accuracy the state of the art methods. The method is based on features extracted from the Pose estimator method OpenPose. These features are then transformed into more representative elements in the context of violence detection, which are then submitted to a LSTM neural network to learn how to identify violence. This work was inspired by the violencedetector.org, the first open-source project for violence detection in real-time videos."
        },
        {
            "heading": "KEyWoRDS",
            "text": "3DCNN, Deep Neural Networks, Human Action Recognition, LSTM, OpenPose, Spatio-Temporal Features"
        },
        {
            "heading": "INTRoDUCTIoN",
            "text": "In this paper, the author demonstrates a novel method for violence detection in real-time videos, allowing machines to better interpret human actions in videos, therefore being able to differentiate between ordinary and violent behavior.\nThis differentiation is very complex since there is no single parameter that indicates the existence of violence individually; actually, a complex combination of several parameters and its variations is required (Ullah et al., 2017), such as position of individuals\u2019 parts, contact points between them, and how each part moves along the time (Serpush & Rezaei, 2020). Additionally, human activity recognition (HAR) also requires multiple frames analysis, which increases exponentially the amount of parameters to be processed (Sharif et al., 2019).\nCurrently, the field of computer vision is dominated by convolutional neural networks (CNNs) (Varior et al., 2016; Zeiler & Fergus, 2013); the same has proven its worth becoming the basic\nconstruction block for many deep learning architectures. The convolutions turn two-dimensions images into more abstract elements called activation maps, which hold the learned abstract representations of the images. Therefore, CNNs are capable of learning image details such as contours, edges and patterns based on the pixel intensity variations of the image (Almaadeed et al., 2021), and keeping immune to small transformations in the input image (e.g., translation, scaling, skewing, and distortion) (LeCun et al., 1989). Unfortunately, convolutions normally result in a large footprint over an expensive computational cost.\nSince CNNs were initially designed for 2D image classification, for dealing with multiple frames in HAR, one additional temporal dimension had to be added to this architecture. This new architecture is known as 3DCNN. This additional dimension enhances even more the computational cost (Akt\u0131 et al., 2020) as described before.\nIn order to address this increase in the computational cost, the author\u2019s method translates the real-time data in which the neural network can learn the subtleness that indicates violence in a simpler and lighter approach, based on more representative features than the pixel intensity variation patterns (Javidani & Mahmoudi-Aznaveh, 2018).\nFor this purpose, the author\u2019s method implies the use of OpenPose (Cao et al., 2019) for localizing the anatomical key points in human bodies by what OpenPose refers to as part affinity fields (PAFs). PAFs learn to associate body parts with individuals in the image, and this process achieves high accuracy and real-time performance (Kim & Lee, 2020). OpenPose (Cao et al., 2019) defines each individual through 18 key points (Figure 1).\nThose relationships between joint points are transformed into more representative features (please see the next section) to feed a long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) network, which will then learn these features over a temporal series extracted from the video. Figure 2 illustrates the processing flow.\nThe LSTM (Varior et al., 2016) is an important component of this method, since it is a special type of recurrent neural network (Schuster & Paliwal, 1997; Sherstinsky, 2020) . However, differently from feed forward networks (e.g., CNNs), this architecture includes recursive loops that retain information obtained in the previous time slots (Arif et al., 2019). This means that the output is conditional on the context of the input sequence, not only what has been presented as input, allowing dealing with temporal distributed information, and having a nature of remembering information for a period of time as default behavior. This capability allows the network to learn the subtle differences in the displacements of the key points along the time series (Nunez et al., 2018; Sudhakaran & Lanz, 2017) ."
        },
        {
            "heading": "AZIMUTHAL DISPLACEMENT AND CENTRoID DISTANCE",
            "text": "As Figure 3 shows, the key points from frame t are compared with frame t+1 yielding a displacement gradient vector; this vector is then converted to azimuthal coordinates (Figure 4). This transformation turns this feature into location and scale invariant, since the displacement of the key point is considered, instead of its position. The magnitude and angle of the gradient vector indicate displacement speed and direction of the moviment, respectively, which proved to be more meaningful than the raw coordinates of the key points for action recognition, namely violence detection, in the case of this study (Kim et al., 2020).\nAnother representative aspect of human interaction is the distance between the individuals (Muralikrishna et al., 2020); the features extracted from the distance are very relevant for the violence detection. Figure 5 demonstrates how this feature is acquired.\nFor each individual in a particular frame t, the distance of all key points is calculated against the centroid function c of the nearest individual. This calculation is performed for all frames along the time series, yielding 18 features per individual.\nThe gradient vector of the azimuthal displacement also yields 18 features, both are 36 features for each individual at a particular frame.\nThe final result is a feature vector of 36 azimuthal displacement elements and 36 coordinates (x and y axes) elements for the centroid distances. These 72 features are stacked through the time series (Tasmin et al., 2021) (the author\u2019s experiment utilized 64 frames) yielding a 72x63 feature matrix per individual (Figure 6)."
        },
        {
            "heading": "NETWoRK ARCHITECTURE",
            "text": "The author\u2019s architecture uses two sequences of one dimension convolution, bath normalization, maximum pooling and dropout to process the extracted features from OpenPose (Cao et al., 2019) translated into azimuthal displacement and centroid distances (Figure 7).\nThe one-dimension convolution provides a very effective feature extraction method over a low computational cost, since the convolution happens only over the time dimension (Kiranyaz et al., 2020).\nAssuming the function O as the order of complexity of the convolution, and having a NxN image, with a KxK kernel, the complexity comparison between one and two dimensions is as follows:\n\u2022 One Dimension \u21d2 \u22c5O N K( ) \u2022 Two Dimensions \u21d2 \u22c5O N K( )2 2\nThese two sequences of convolutions, normalizations and maximum poolings allow the second layer to operate in a higher level of abstraction, yielding a more compact representation of the most relevant information (Krizhevsky et al., 2012).\nThis compact representation then feeds the LSTM network, exploring its ability to capture longterm temporal dependencies without being so affected by common problems from other recurrent neural networks, such as vanishing gradients (Mahasseni & Todorovic, 2016; Yu et al., 2019).\nThe last layer in this architecture is a fully connected one, which performs the final classification putting the LSTM output into the two final classes, namely violence and nonviolence (Caetano, 2020; Peixoto et al., 2019)."
        },
        {
            "heading": "EVALUATIoN",
            "text": ""
        },
        {
            "heading": "Dataset",
            "text": "The author used the Real Life Violence Situations Dataset (Soliman et al., 2019) for this evaluation; it contains 1000 videos of violence and 1000 videos of nonviolence collected from YouTube.\nThe creators identified a shortage in available datasets related to violence between individuals; this was the motivation for this dataset."
        },
        {
            "heading": "Test Methodology",
            "text": "In order to ensure test equivalence, the author used the same randomized set of 20% of the dataset for both network architectures; this sample was not part of the training. The author defined the frame resolution being 224x224 pixels, which is also the default for MobileNet avoiding unnecessary resizing.\nThe tests were performed at the same computer, running Tensorflow 2.5 over a RTX 2080ti GPU. The timings were measured during the inference of both architectures, but, for their network, the author\nalso added the OpenPose (Cao et al., 2019) feature extraction time plus the feature transformation time, using the azimuthal displacement and centroid distances method proposed here.\nOpenPose (Cao et al., 2019) can work with different networks; for these tests the author utilized the MobileNet v2.\nBoth network architectures receive sequences of images as input, but the author\u2019s method works internally with a deeper level of detail, since OpenPose (Cao et al., 2019) extracts individuals\u2019 features in each frame. Then, these transformed features feed the author\u2019s proposed LSTM architecture."
        },
        {
            "heading": "Comparison Criteria",
            "text": "For their comparison, the author utilized the 3DCNN architecture (Pijackova & Gotthans, 2021), since several authors consider it as a state of the art method for HAR. As the author discussed in the previous section, 3DCNN is computationally expensive, due to its iterative processing nature along the axes x and y for 2D convolution, and axes x, y, and z, in case of 3D convolution (Figure 8)."
        },
        {
            "heading": "RESULTS",
            "text": "Table 1 demonstrates the test results: The author\u2019s method achieved superior precision, recall, and F1-score in comparison with 3DCNN, considering the same validation dataset sample the researcher described in the subsection Test Methodology.\nAs Table 2 shows, for the same tests, the author\u2019s method achieved an incredible speed, more than 14 times faster than the 3DCNN, with almost half of the size considering the lighter model. In order to keep a fair comparison, the author is considering the model size and speed as the sum of OpenPose (Cao et al., 2019) (2,1MB/0.0076s) and his network architecture (6,8MB/0.0019s), since both are required to perform HAR.\nThe author expected the superior speed, due to the simplicity of this method, compared with the high computational cost described in the previous subsection for 3DCNN (Pham et al., 2018)."
        },
        {
            "heading": "VIoLENCEDETECToR.oRG",
            "text": "Technology utilization for violence prevention is the objective of violencedetector.org (Partika, 2020), which is the first open source project for violence detection in real time videos.\nThe work the author presented in this paper directly benefits this project. Considering its lightweight, this method can run under limited resources, for example edge devices.\nThose devices can monitor violence and trigger local responses to avoid situations or even alert authorities. Their simplicity may allow the development of smart security devices at a low cost, generating a new device ecosystem for violence prevention."
        },
        {
            "heading": "CoNCLUSIoN",
            "text": "Detecting violence in videos is not an easy task, even for the most advanced deep learning architectures. In this paper, the author presented a novel method based on the transformation of OpenPose (Cao et al., 2019) features, which has proven to be very effective for violence detection in real-time videos.\nThis method yields a compact footprint and superior accuracy, compared with other state of the art techniques for the same purposes, with incredible gains in speed confirmed by a realistic benchmark (subsection Results) (Perez et al., 2019).\nUltimately, the author has open-sourced this work as part of violencedetector.org (Partika, 2020), the first open source project for violence detection in real-time videos, which aims to democratize this technology, allowing the development of new smart security devices and enabling violence prevention at low cost."
        },
        {
            "heading": "ACKNoWLEDGMENT",
            "text": "The author acknowledge the work done by the authors of OpenPose (Cao et al., 2019), as well as Real Life Violence Situations dataset (Soliman et al., 2019)."
        },
        {
            "heading": "FUNDING AGENCy",
            "text": "The Open Access Processing fee for this article was covered in full by the authors of this article."
        }
    ],
    "title": "Simple Approach for Violence Detection in Real-Time Videos Using Pose Estimation With Azimuthal Displacement and Centroid Distance as Features",
    "year": 2022
}