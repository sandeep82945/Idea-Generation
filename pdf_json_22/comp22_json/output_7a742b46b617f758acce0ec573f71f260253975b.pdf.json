{
    "abstractText": "Interpretability, trustworthiness, and usability are key considerations in high-stake security applications, especially when utilizing deep learning models. While these models are known for their high accuracy, they behave as black boxes in which identifying important features and factors that led to a classification or a prediction is difficult. This can lead to uncertainty and distrust, especially when an incorrect prediction results in severe consequences. Thus, explanation methods aim to provide insights into the inner working of deep learning models. However, most explanation methods provide inconsistent explanations, have low fidelity, and are susceptible to adversarial manipulation, which can reduce model trustworthiness. This paper provides a comprehensive analysis of explainable methods and demonstrates their efficacy in three distinct security applications: anomaly detection using system logs, malware prediction, and detection of adversarial images. Our quantitative and qualitative analysis1 reveals serious limitations and concerns in state-of-the-art explanation methods in all three applications. We show that explanation methods for security applications necessitate distinct characteristics, such as stability, fidelity, robustness, and usability, among others, which we outline as the prerequisites for trustworthy explanation methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Dipkamal Bhusal"
        },
        {
            "affiliations": [],
            "name": "Rosalyn Shin"
        },
        {
            "affiliations": [],
            "name": "Ajay Ashok Shewale"
        },
        {
            "affiliations": [],
            "name": "Monish Kumar Manikya"
        },
        {
            "affiliations": [],
            "name": "Michael Clifford"
        },
        {
            "affiliations": [],
            "name": "Sara Rampazzi"
        },
        {
            "affiliations": [],
            "name": "Nidhi Rastogi"
        }
    ],
    "id": "SP:3d76ccc796df414f6ec840b8f1449cb831f330b8",
    "references": [
        {
            "authors": [
                "Martin Abadi",
                "Andy Chu",
                "Ian Goodfellow",
                "H Brendan McMahan",
                "Ilya Mironov",
                "Kunal Talwar",
                "Li Zhang"
            ],
            "title": "Deep learning with differential privacy",
            "venue": "In Proceedings of the 2016 ACM SIGSAC CCS",
            "year": 2016
        },
        {
            "authors": [
                "Julius Adebayo",
                "Justin Gilmer",
                "Michael Muelly",
                "Ian Goodfellow",
                "Moritz Hardt",
                "Been Kim"
            ],
            "title": "Sanity checks for saliency maps",
            "venue": "NeurIPS",
            "year": 2018
        },
        {
            "authors": [
                "Neda AfzaliSeresht"
            ],
            "title": "Explainable Intelligence for Comprehensive Interpretation of Cybersecurity Data in Incident Management",
            "venue": "Ph. D. Dissertation. Victoria University",
            "year": 2022
        },
        {
            "authors": [
                "Chirag Agarwal",
                "Nari Johnson",
                "Martin Pawelczyk",
                "Satyapriya Krishna",
                "Eshika Saxena",
                "Marinka Zitnik",
                "Himabindu Lakkaraju"
            ],
            "title": "Rethinking Stability for Attribution-based Explanations",
            "venue": "ICLR",
            "year": 2022
        },
        {
            "authors": [
                "Bushra A Alahmadi",
                "Louise Axon",
                "Ivan Martinovic"
            ],
            "title": "2022. 99% False Positives: A Qualitative Study of SOC Analysts",
            "venue": "Perspectives on Security Alarms. In Proceedings of the 31st USENIX Security,",
            "year": 2022
        },
        {
            "authors": [
                "J Alammar"
            ],
            "title": "Ecco: an open source library for the explainability of transformer language models",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations",
            "year": 2021
        },
        {
            "authors": [
                "David Alvarez Melis",
                "Tommi Jaakkola"
            ],
            "title": "Towards robust interpretability with self-explaining neural networks",
            "venue": "NeurIPS",
            "year": 2018
        },
        {
            "authors": [
                "David Alvarez-Melis",
                "Tommi S Jaakkola"
            ],
            "title": "On the robustness of interpretability methods",
            "venue": "arXiv preprint arXiv:1806.08049",
            "year": 2018
        },
        {
            "authors": [
                "Vijay Arya",
                "Rachel KE Bellamy",
                "Pin-Yu Chen",
                "Amit Dhurandhar",
                "Michael Hind",
                "Samuel C Hoffman",
                "Stephanie Houde",
                "Q Vera Liao",
                "Ronny Luss",
                "Aleksandra Mojsilovi\u0107"
            ],
            "title": "One explanation does not fit all: A toolkit and taxonomy of ai explainability techniques",
            "venue": "arXiv preprint arXiv:1909.03012",
            "year": 2019
        },
        {
            "authors": [
                "Sebastian Bach",
                "Alexander Binder",
                "Gr\u00e9goire Montavon",
                "Frederick Klauschen",
                "Klaus-Robert M\u00fcller",
                "Wojciech Samek"
            ],
            "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
            "venue": "PloS one 10,",
            "year": 2015
        },
        {
            "authors": [
                "Daniel S Berman",
                "Anna L Buczak",
                "Jeffrey S Chavis",
                "Cherita L Corbett"
            ],
            "title": "A survey of deep learning methods for cyber security",
            "venue": "Information 10,",
            "year": 2019
        },
        {
            "authors": [
                "Astrid Bertrand",
                "Rafik Belloum",
                "James R Eagan",
                "Winston Maxwell"
            ],
            "title": "How cognitive biases affect XAI-assisted decision-making: A systematic review",
            "venue": "In Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
            "year": 2022
        },
        {
            "authors": [
                "Umang Bhatt",
                "Adrian Weller",
                "Jos\u00e9 MF Moura"
            ],
            "title": "Evaluating and Aggregating Feature-based Model Explanations",
            "venue": "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence",
            "year": 2020
        },
        {
            "authors": [
                "Francesco Bodria",
                "Fosca Giannotti",
                "Riccardo Guidotti",
                "Francesca Naretto",
                "Dino Pedreschi",
                "Salvatore Rinzivillo"
            ],
            "title": "Benchmarking and survey of explanation methods for black box models",
            "year": 2021
        },
        {
            "authors": [
                "Joy Buolamwini"
            ],
            "title": "When the robot doesn\u2019t see Dark skin",
            "venue": "https://www",
            "year": 2018
        },
        {
            "authors": [
                "Jill Burstein",
                "Christy Doran",
                "Thamar Solorio"
            ],
            "title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
            "venue": "In Proceedings of the 2019 NAACL: Human Language Technologies,",
            "year": 2019
        },
        {
            "authors": [
                "Kirill Bykov",
                "Anna Hedstr\u00f6m",
                "Shinichi Nakajima",
                "Marina M-C H\u00f6hne"
            ],
            "title": "NoiseGrad\u2014Enhancing Explanations by Introducing Stochasticity to Model Weights",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Nicholas Carlini",
                "David Wagner"
            ],
            "title": "Towards evaluating the robustness of neural networks",
            "venue": "In 2017 ieee symposium on security and privacy (sp)",
            "year": 2017
        },
        {
            "authors": [
                "Prasad Chalasani",
                "Jiefeng Chen",
                "Amrita Roy Chowdhury",
                "Xi Wu",
                "Somesh Jha"
            ],
            "title": "Concise explanations of neural networks using adversarial training",
            "venue": "In International Conference on Machine Learning",
            "year": 2020
        },
        {
            "authors": [
                "Chris Crowley",
                "John Pescatore"
            ],
            "title": "Common and best practices for security operations centers: Results of the 2019 SOC survey",
            "year": 2019
        },
        {
            "authors": [
                "Hoa Khanh Dam",
                "Truyen Tran",
                "Trang Pham",
                "Shien Wee Ng",
                "John Grundy",
                "Aditya Ghose"
            ],
            "title": "Automatic feature learning for vulnerability prediction",
            "year": 2017
        },
        {
            "authors": [
                "Anupam Datta",
                "Shayak Sen",
                "Yair Zick"
            ],
            "title": "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems",
            "venue": "IEEE symposium on security and privacy (SP)",
            "year": 2016
        },
        {
            "authors": [
                "Ann-Kathrin Dombrowski",
                "Maximillian Alber",
                "Christopher Anders",
                "Marcel Ackermann",
                "Klaus-Robert M\u00fcller",
                "Pan Kessel"
            ],
            "title": "Explanations can be manipulated and geometry is to blame",
            "venue": "NeurIPS",
            "year": 2019
        },
        {
            "authors": [
                "Finale Doshi-Velez",
                "Been Kim"
            ],
            "title": "Towards a rigorous science of interpretable machine learning",
            "venue": "arXiv preprint arXiv:1702.08608",
            "year": 2017
        },
        {
            "authors": [
                "Min Du",
                "Feifei Li",
                "Guineng Zheng",
                "Vivek Srikumar"
            ],
            "title": "Deeplog: Anomaly detection and diagnosis from system logs through deep learning",
            "venue": "In Proceedings of the 2017 ACM SIGSAC conference on computer and communications security",
            "year": 2017
        },
        {
            "authors": [
                "Cynthia Dwork",
                "Vitaly Feldman"
            ],
            "title": "Privacy-preserving prediction",
            "venue": "In Conference On Learning Theory. PMLR,",
            "year": 2018
        },
        {
            "authors": [
                "Upol Ehsan",
                "Mark O Riedl"
            ],
            "title": "Human-centered explainable ai: Towards a reflective sociotechnical approach",
            "venue": "In HCI International 2020-Late Breaking Papers: Multimodality and Intelligence: 22nd HCI International Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Gabriel Erion",
                "Joseph D Janizek",
                "Pascal Sturmfels",
                "Scott M Lundberg",
                "Su- In Lee"
            ],
            "title": "Improving performance of deep learning models with axiomatic attribution priors and expected gradients",
            "venue": "Nature machine intelligence 3,",
            "year": 2021
        },
        {
            "authors": [
                "Christian Etmann",
                "Sebastian Lunz",
                "Peter Maass",
                "Carola-Bibiane Sch\u00f6nlieb"
            ],
            "title": "On the connection between adversarial robustness and saliency map interpretability",
            "year": 2019
        },
        {
            "authors": [
                "Reuben Feinman",
                "Ryan R Curtin",
                "Saurabh Shintre",
                "Andrew B Gardner"
            ],
            "title": "Detecting adversarial samples from artifacts",
            "venue": "arXiv preprint arXiv:1703.00410",
            "year": 2017
        },
        {
            "authors": [
                "Yuyou Gan",
                "Yuhao Mao",
                "Xuhong Zhang",
                "Shouling Ji",
                "Yuwen Pu",
                "Meng Han",
                "Jianwei Yin",
                "Ting Wang"
            ],
            "title": " Is your explanation stable?\" A Robustness Evaluation Framework for Feature Attribution",
            "venue": "In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security",
            "year": 2022
        },
        {
            "authors": [
                "Amirata Ghorbani",
                "Abubakar Abid",
                "James Zou"
            ],
            "title": "Interpretation of neural networks is fragile",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "arXiv preprint arXiv:1412.6572",
            "year": 2014
        },
        {
            "authors": [
                "Wenbo Guo",
                "Dongliang Mu",
                "Jun Xu",
                "Purui Su",
                "Gang Wang",
                "Xinyu Xing"
            ],
            "title": "Lemna: Explaining deep learning based security applications",
            "venue": "In proceedings of the 2018 ACM SIGSAC conference on computer and communications security",
            "year": 2018
        },
        {
            "authors": [
                "Dongqi Han",
                "Zhiliang Wang",
                "Wenqi Chen",
                "Ying Zhong",
                "Su Wang",
                "Han Zhang",
                "Jiahai Yang",
                "Xingang Shi",
                "Xia Yin"
            ],
            "title": "DeepAID: interpreting and improving deep learning-based anomaly detection in security applications",
            "venue": "In Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security",
            "year": 2021
        },
        {
            "authors": [
                "Frederik Harder",
                "Matthias Bauer",
                "Mijung Park"
            ],
            "title": "Interpretable and differentially private predictions",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Swetha Hariharan",
                "Anusha Velicheti",
                "AS Anagha",
                "Ciza Thomas",
                "N Balakrishnan"
            ],
            "title": "Explainable Artificial Intelligence in Cybersecurity: A Brief Review",
            "venue": "4th International Conference on Security and Privacy (ISEA-ISAP)",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition",
            "year": 2016
        },
        {
            "authors": [
                "Anna Hedstr\u00f6m",
                "Leander Weber",
                "Daniel Krakowczyk",
                "Dilyara Bareeva",
                "Franz Motzkus",
                "Wojciech Samek",
                "Sebastian Lapuschkin",
                "Marina Marina M.-C. H\u00f6hne"
            ],
            "title": "Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond",
            "venue": "Journal of Machine Learning Research 24,",
            "year": 2023
        },
        {
            "authors": [
                "Juyeon Heo",
                "Sunghwan Joo",
                "Taesup Moon"
            ],
            "title": "Fooling neural network interpretations via adversarial model manipulation",
            "venue": "NeurIPS",
            "year": 2019
        },
        {
            "authors": [
                "Mengdi Huai",
                "Jinduo Liu",
                "Chenglin Miao",
                "Liuyi Yao",
                "Aidong Zhang"
            ],
            "title": "Towards automating model explanations with certified robustness guarantees",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Susmit Jha",
                "Sunny Raj",
                "Steven Lawrence Fernandes",
                "Sumit Kumar Jha",
                "Somesh Jha",
                "Gunjan Verma",
                "Brian Jalaian",
                "Ananthram Swami"
            ],
            "title": "Attributiondriven causal analysis for detection of adversarial examples",
            "year": 2019
        },
        {
            "authors": [
                "Bin Jia",
                "Cailing Dong",
                "Zhijiang Chen",
                "Kuo-Chu Chang",
                "Nichole Sullivan",
                "Genshe Chen"
            ],
            "title": "Pattern discovery and anomaly detection via knowledge graph",
            "venue": "21st International Conference on Information Fusion (FUSION)",
            "year": 2018
        },
        {
            "authors": [
                "Pieter-Jan Kindermans",
                "Kristof T Sch\u00fctt",
                "Maximilian Alber",
                "Klaus-Robert M\u00fcller",
                "Dumitru Erhan",
                "Been Kim",
                "Sven D\u00e4hne"
            ],
            "title": "Learning how to explain neural networks: Patternnet and patternattribution",
            "year": 2017
        },
        {
            "authors": [
                "Narine Kokhlikyan",
                "Vivek Miglani",
                "Miguel Martin",
                "Edward Wang",
                "Bilal Alsallakh",
                "Jonathan Reynolds",
                "Alexander Melnikov",
                "Natalia Kliushkina",
                "Carlos Araya",
                "Siqi Yan",
                "Orion Reblitz-Richardson"
            ],
            "title": "Captum: A unified and generic model interpretability library for PyTorch",
            "year": 2020
        },
        {
            "authors": [
                "Josua Krause",
                "Adam Perer",
                "Kenney Ng"
            ],
            "title": "Interacting with predictions: Visual inspection of black-box machine learning models",
            "venue": "In Proceedings of the 2016 CHI conference on human factors in computing systems",
            "year": 2016
        },
        {
            "authors": [
                "Christopher Kruegel",
                "William Robertson"
            ],
            "title": "Alert verification determining the success of intrusion attempts",
            "venue": "DIMVA",
            "year": 2004
        },
        {
            "authors": [
                "Alexey Kurakin",
                "Ian Goodfellow",
                "Samy Bengio"
            ],
            "title": "Adversarial machine learning at scale",
            "venue": "arXiv preprint arXiv:1611.01236",
            "year": 2016
        },
        {
            "authors": [
                "Pavel Laskov"
            ],
            "title": "Practical evasion of a learning-based classifier: A case study",
            "venue": "In 2014 IEEE symposium on security and privacy",
            "year": 2014
        },
        {
            "authors": [
                "Yann LeCun",
                "L\u00e9on Bottou",
                "Yoshua Bengio",
                "Patrick Haffner"
            ],
            "title": "Gradientbased learning applied to document recognition",
            "venue": "Proc. IEEE 86,",
            "year": 1998
        },
        {
            "authors": [
                "Pantelis Linardatos",
                "Vasilis Papastefanopoulos",
                "Sotiris Kotsiantis"
            ],
            "title": "Explainable ai: A review of machine learning interpretability methods",
            "venue": "Entropy 23,",
            "year": 2020
        },
        {
            "authors": [
                "Scott M Lundberg",
                "Su-In Lee"
            ],
            "title": "A unified approach to interpreting model predictions",
            "venue": "NeurIPS",
            "year": 2017
        },
        {
            "authors": [
                "Xingjun Ma",
                "Bo Li",
                "Yisen Wang",
                "Sarah M Erfani",
                "Sudanthi Wijewickrema",
                "Grant Schoenebeck",
                "Dawn Song",
                "Michael E Houle",
                "James Bailey"
            ],
            "title": "Characterizing adversarial subspaces using local intrinsic dimensionality",
            "year": 2018
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards deep learningmodels resistant to adversarial attacks",
            "year": 2017
        },
        {
            "authors": [
                "Brendan McMahan",
                "Eider Moore",
                "Daniel Ramage",
                "Seth Hampson",
                "Blaise Aguera y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "In Artificial intelligence and statistics",
            "year": 2017
        },
        {
            "authors": [
                "Tim Miller",
                "Piers Howe",
                "Liz Sonenberg"
            ],
            "title": "Explainable AI: Beware of inmates running the asylum or: How I learnt to stop worrying and love the social and behavioural sciences",
            "year": 2017
        },
        {
            "authors": [
                "Smitha Milli",
                "Ludwig Schmidt",
                "Anca D Dragan",
                "Moritz Hardt"
            ],
            "title": "2019. Model reconstruction from model explanations",
            "venue": "In Proceedings of the Conference on Fairness, Accountability,",
            "year": 2019
        },
        {
            "authors": [
                "W James Murdoch",
                "Chandan Singh",
                "Karl Kumbier",
                "Reza Abbasi-Asl",
                "Bin Yu"
            ],
            "title": "Definitions, methods, and applications in interpretable machine learning",
            "venue": "Proceedings of the National Academy of Sciences 116,",
            "year": 2019
        },
        {
            "authors": [
                "Azqa Nadeem",
                "Dani\u00ebl Vos",
                "Clinton Cao",
                "Luca Pajola",
                "Simon Dieck",
                "Robert Baumgartner",
                "Sicco Verwer"
            ],
            "title": "SoK: ExplainableMachine Learning for Computer Security Applications",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Ian E Nielsen",
                "Dimah Dera",
                "Ghulam Rasool",
                "Ravi P Ramachandran",
                "Nidhal Carla Bouaynaya"
            ],
            "title": "Robust explainability: A tutorial on gradient-based attribution methods for deep neural networks",
            "venue": "IEEE Signal Processing Magazine 39,",
            "year": 2022
        },
        {
            "authors": [
                "Megan Nyre-Yu",
                "Elizabeth Morris",
                "Blake Cameron Moss",
                "Charles Smutz",
                "Michael Smith"
            ],
            "title": "Explainable AI in Cybersecurity Operations: Lessons Learned from xAI Tool Deployment",
            "venue": "In Proceedings of the Usable Security and Privacy (USEC) Symposium,",
            "year": 2022
        },
        {
            "authors": [
                "Megan Nyre-Yu",
                "Elizabeth Susan Morris",
                "Blake Cameron Moss",
                "Charles Smutz",
                "Michael Smith"
            ],
            "title": "Considerations for Deploying xAI Tools in the Wild: Lessons Learned from xAI Deployment in a Cybersecurity Operations Setting",
            "venue": "Technical Report. Sandia National Lab.(SNL-NM),",
            "year": 2021
        },
        {
            "authors": [
                "Nicolas Papernot",
                "Patrick McDaniel",
                "Ian Goodfellow",
                "Somesh Jha",
                "Z Berkay Celik",
                "Ananthram Swami"
            ],
            "title": "Practical black-box attacks against machine learning",
            "venue": "In Proceedings of the 2017 ACM on Asia conference on computer and communications security",
            "year": 2017
        },
        {
            "authors": [
                "GDLT Parra",
                "Luis Selvera",
                "Joseph Khoury",
                "Hector Irizarry",
                "Elias Bou-Harb",
                "Paul Rad"
            ],
            "title": "Interpretable federated transformer log learning for cloud threat forensics",
            "venue": "In Proceedings of the Network and Distributed Systems Security (NDSS) Symposium",
            "year": 2022
        },
        {
            "authors": [
                "Neel Patel",
                "Reza Shokri",
                "Yair Zick"
            ],
            "title": "Model explanations with differential privacy",
            "venue": "ACM Conference on Fairness, Accountability, and Transparency",
            "year": 2022
        },
        {
            "authors": [
                "Wolter Pieters"
            ],
            "title": "Explanation and trust: what to tell the user in security and AI",
            "venue": "Ethics and information technology 13,",
            "year": 2011
        },
        {
            "authors": [
                "Pengrui Quan",
                "Supriyo Chakraborty",
                "Jeya Vikranth Jeyakumar",
                "Mani Srivastava"
            ],
            "title": "On the amplification of security and privacy risks by post-hoc explanations in machine learning models",
            "year": 2022
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin"
            ],
            "title": " Why should i trust you?\" Explaining the predictions of any classifier",
            "venue": "In 22nd ACM SIGKDD",
            "year": 2016
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin"
            ],
            "title": "Anchors: Highprecision model-agnostic explanations",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Marko Robnik-\u0160ikonja",
                "Marko Bohanec"
            ],
            "title": "Perturbation-based explanations of prediction models",
            "venue": "In Human and machine learning",
            "year": 2018
        },
        {
            "authors": [
                "Cynthia Rudin"
            ],
            "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
            "venue": "Nature machine intelligence 1,",
            "year": 2019
        },
        {
            "authors": [
                "Cynthia Rudin",
                "Chaofan Chen",
                "Zhi Chen",
                "Haiyang Huang",
                "Lesia Semenova",
                "Chudi Zhong"
            ],
            "title": "Interpretable machine learning: Fundamental principles and 10 grand challenges",
            "venue": "Statistic Surveys",
            "year": 2022
        },
        {
            "authors": [
                "Ramprasaath R Selvaraju",
                "Michael Cogswell",
                "Abhishek Das",
                "Ramakrishna Vedantam",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "venue": "In Proceedings of the IEEE international conference on computer vision",
            "year": 2017
        },
        {
            "authors": [
                "Reza Shokri",
                "Martin Strobel",
                "Yair Zick"
            ],
            "title": "On the privacy risks of model explanations",
            "venue": "In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",
            "year": 2021
        },
        {
            "authors": [
                "R. Shokri",
                "Marco Stronati",
                "Congzheng Song",
                "Vitaly Shmatikov"
            ],
            "title": "Membership Inference Attacks Against Machine Learning Models",
            "venue": "IEEE Symposium on Security and Privacy (SP)",
            "year": 2017
        },
        {
            "authors": [
                "Avanti Shrikumar",
                "Peyton Greenside",
                "Anshul Kundaje"
            ],
            "title": "Learning important features through propagating activation differences",
            "venue": "In International conference on machine learning",
            "year": 2017
        },
        {
            "authors": [
                "Avanti Shrikumar",
                "Peyton Greenside",
                "Anna Shcherbina",
                "Anshul Kundaje"
            ],
            "title": "Not just a black box: Learning important features through propagating activation differences",
            "year": 2016
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrea Vedaldi",
                "Andrew Zisserman"
            ],
            "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
            "year": 2013
        },
        {
            "authors": [
                "Dylan Slack",
                "Sophie Hilgard",
                "Emily Jia",
                "Sameer Singh",
                "Himabindu Lakkaraju"
            ],
            "title": "Fooling lime and shap: Adversarial attacks on post hoc explanationmethods",
            "venue": "In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Smilkov",
                "Nikhil Thorat",
                "Been Kim",
                "Fernanda Vi\u00e9gas",
                "Martin Wattenberg"
            ],
            "title": "Smoothgrad: removing noise by adding noise",
            "year": 2017
        },
        {
            "authors": [
                "Kacper Sokol",
                "Peter Flach"
            ],
            "title": "Explainability fact sheets: a framework for systematic assessment of explainable approaches",
            "venue": "In Proceedings of the 2020 Conference on Fairness, Accountability,",
            "year": 2020
        },
        {
            "authors": [
                "Pascal Sturmfels",
                "Scott Lundberg",
                "Su-In Lee"
            ],
            "title": "Visualizing the impact of feature attribution baselines",
            "venue": "Distill 5,",
            "year": 2020
        },
        {
            "authors": [
                "Mukund Sundararajan",
                "Ankur Taly",
                "Qiqi Yan"
            ],
            "title": "Axiomatic attribution for deep networks",
            "venue": "In International conference on machine learning. PMLR,",
            "year": 2017
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wojciech Zaremba",
                "Ilya Sutskever",
                "Joan Bruna",
                "Dumitru Erhan",
                "Ian Goodfellow",
                "Rob Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "year": 2013
        },
        {
            "authors": [
                "Thijs van Ede",
                "Hojjat Aghakhani",
                "Noah Spahn",
                "Riccardo Bortolameotti",
                "Marco Cova",
                "Andrea Continella",
                "Maarten van Steen",
                "Andreas Peter",
                "Christopher Kruegel",
                "Giovanni Vigna"
            ],
            "title": "DEEPCASE: Semi-Supervised Contextual Analysis of Security Events",
            "venue": "IEEE Security and Privacy",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "NeurIPS",
            "year": 2017
        },
        {
            "authors": [
                "Luca Vigano",
                "Daniele Magazzeni"
            ],
            "title": "Explainable security",
            "venue": "IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)",
            "year": 2020
        },
        {
            "authors": [
                "Jingyuan Wang",
                "Yufan Wu",
                "Mingxuan Li",
                "Xin Lin",
                "Junjie Wu",
                "Chao Li"
            ],
            "title": "Interpretability is a kind of safety: An interpreter-based ensemble for adversary defense",
            "venue": "In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
            "year": 2020
        },
        {
            "authors": [
                "Alexander Warnecke",
                "Daniel Arp",
                "Christian Wressnegger",
                "Konrad Rieck"
            ],
            "title": "Evaluating explanation methods for deep learning in security",
            "venue": "IEEE european symposium on security and privacy (EuroS&P)",
            "year": 2020
        },
        {
            "authors": [
                "Wei Xu",
                "Ling Huang",
                "Armando Fox",
                "David Patterson",
                "Michael I Jordan"
            ],
            "title": "Detecting large-scale system problems by mining console logs",
            "venue": "In Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles",
            "year": 2009
        },
        {
            "authors": [
                "Puyudi Yang",
                "Jianbo Chen",
                "Cho-Jui Hsieh",
                "Jane-Ling Wang",
                "Michael Jordan"
            ],
            "title": "Ml-loo: Detecting adversarial examples with feature attribution",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Chih-Kuan Yeh",
                "Been Kim",
                "Sercan Arik",
                "Chun-Liang Li",
                "Tomas Pfister",
                "Pradeep Ravikumar"
            ],
            "title": "On completeness-aware concept-based explanations in deep neural networks",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Suleiman Y Yerima",
                "Sakir Sezer"
            ],
            "title": "Droidfusion: A novel multilevel classifier fusion approach for android malware detection",
            "venue": "IEEE transactions on cybernetics 49,",
            "year": 2018
        },
        {
            "authors": [
                "Matthew D Zeiler",
                "Rob Fergus"
            ],
            "title": "Visualizing and understanding convolutional networks",
            "venue": "In Computer Vision\u2013ECCV",
            "year": 2014
        },
        {
            "authors": [
                "Xinyang Zhang",
                "Ningfei Wang",
                "Hua Shen",
                "Shouling Ji",
                "Xiapu Luo",
                "TingWang"
            ],
            "title": "Interpretable deep learning under fire",
            "year": 2020
        },
        {
            "authors": [
                "Yan Zhou",
                "Murat Kantarcioglu"
            ],
            "title": "On transparency of machine learning models: A position paper",
            "venue": "In AI for Social Good Workshop",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "CCS CONCEPTS \u2022 Security and privacy \u2192 Systems security; \u2022 Computing methodologies \u2192 Machine learning algorithms; Artificial intelligence.\n1An Institutional Review Board (IRB) approval was taken prior to interviewing experts for qualitative study.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ARES 2023, August 29-September 1, 2023, Benevento, Italy \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0772-8/23/08. . . $15.00 https://doi.org/10.1145/3600160.3600193\nKEYWORDS Explainability, Security Analytics, Trustworthiness, Usability ACM Reference Format: Dipkamal Bhusal, Rosalyn Shin, AjayAshok Shewale,Monish KumarManikya Veerabhadran, Michael Clifford, Sara Rampazzi, and Nidhi Rastogi. 2023. SoK:Modeling Explainability in Security Analytics for Interpretability, Trustworthiness, and Usability. In The 18th International Conference on Availability, Reliability and Security (ARES 2023), August 29-September 1, 2023, Benevento, Italy. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3600160. 3600193"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Deep learning (DL) models have shown high performance in a variety of security applications, such as security log analysis [87], vulnerability detection [21], and malware detection [95]. However, since these models operate by learning complex, non-linear relationships between inputs and outputs, security researchers find it difficult to interpret their inner workings or explain them to other researchers. As a result, deep learning models may not offer insights for their classification decisions, making it difficult to evaluate whether their decision criteria align with domain expert knowledge [5].\nFor instance, in the context of intrusion detection, security researchers require insights into the underlying behavior of intrusion\nar X\niv :2\n21 0.\n17 37\n6v 2\n[ cs\n.C R\n] 1\n3 Ju\ndetection systems, which inspect network packet features such as source and destination IP addresses, protocol types, packet lengths, payload contents, and sequence numbers, and then classify packets as suspicious or not. In a port scanning attack, model decisions are trustworthy if the model pinpoints the presence of a high number of connections with low duration and low login success rate (see Figure 1), reflecting both the structure of the attack and the analyst\u2019s domain knowledge for that attack.\nAnother challenge resulting from the inherent opacity of deep learning models for security applications is the ethical concern of bias and discrimination in training data [15]. In the context of facial recognition systems, models have been shown to exhibit bias against marginalized groups, particularly people of color, due to a lack of training data diversity. This can lead to false positives or false negatives, as well as perpetuate harmful stereotypes and discriminatory behavior. An explanation method can identify the characteristics, such as facial features, that the deep learning model used to make decisions. Researchers can then address ethical concerns by adjusting relevant feature weights, and by adding more racially and ethnically diverse data.\nInterest in achieving interpretability of DL models has led to research following two main approaches: 1) designing intrinsically interpretable models and 2) using post-hoc explanation methods [59]. The latter category involves techniques that analyze model decisions with respect to input features and identify the features most significant to those model decisions. Examples of these methods include LIME (Local Interpretable Model-agnostic Explanations), SHAP (SHapley Additive exPlanations), Integrated Gradient, PatternNet, and Grad-CAM (Gradient-weighted Class Activation Mapping) [45, 70, 75, 85]. However, not all explanation methods are suitable for implementation in the security domain, which typically utilizes recurrent neural networks (RNNs) and feed-forward neural networks (FNNs) [91]. For example, GradCAM [75] is only applicable to convolutional neural networks (CNNs) for images, and PatternNet is limited to FNNs and CNNs. Additionally, there are security and privacy concerns associated with the use of explanation methods in the security domain because of differences in design assumptions, requirements, dataset characteristics, and the model itself [15, 76].\nIn this paper, we systematically evaluate the use of state-ofthe-art explainability models in security monitoring and provide a critical analysis of their effectiveness. We also review peer-reviewed articles on explainable security applications, such as malware detection, network intrusion detection, and adversarial object detection. To ensure we captured the most relevant and recent scientific work in this domain, we employed an approach that involved selecting papers that were published after 2010 and that match at least one of the following criteria: (a) publication in high-ranking journals such as NeurIPS, AAAI, USENIX Security, ACM CCS, SIGSAC, IEEE Security & Privacy, and ACL, (b) high citation rates, (c) authored by researchers actively publishing on that topic, and (d) availability of open-source code, publicly available libraries. We searched relevant papers across several academic databases, including Google Scholar, IEEE Xplore, and the ACM Digital Library.\nMain Contributions. This research makes the following main contributions.\n(1) We perform both quantitative and qualitative analysis to evaluate the efficacy of the most representative explanation methods against distinct security threats. The explanation methods include LIME [70], SHAP [53], Gradient [80], GradientXInput [79], Integrated Gradient [85], DeepLIFT [78], GradientShap [29], Occlusion [96], and DeepAID [36]. (2) We provide a comprehensive analysis of the performance of explanation methods for three distinct security threats. Based on our literature study, we determine that existing research provides inadequate evaluation approaches for a quantitative study. (3) We outline important evaluation methods suitable for security explanations. These methods are covered in detail in Section 4. (4) Through qualitative analysis, we determine that existing explanation methods have very poor usability. To the best of our knowledge, this is the first work to present this form of analysis in security.\nKey Findings. The following are our key findings from the quantitative and qualitative evaluations using various explanation methods to explain models used for different security use cases.\n(1) Explanation methods display high disparity in attributing feature relevance, raising reliability concerns (Section 5.1). (2) Back-propagation-based explanationmethods (e.g., Integrated Gradient and DeepLIFT) outperform perturbation methods (e.g., LIME, SHAP) in identifying relevant input features of a model prediction (Section 5.2). (3) The choice of explanation method significantly impacts the distribution of attributed features.While the gradientmethod is capable of capturing class-discriminative behavior and revealing dispersion in feature distributions of benign and adversarial samples, other methods, such as IntegratedGradient, fail to capture statistically significant dispersion (Section 5.3). (4) Evaluating the explanation method effectiveness using only quantitativemetrics leads tomisleading results because these methods can assign high importance to irrelevant features. Conversely, relying exclusively on qualitative explanation evaluations produces incomplete results, as the accuracy, stability, and reliability of the explanations cannot be objectively measured. Consequently, comprehensive evaluations of an explanation method should incorporate both quantitative and qualitative evaluation approaches. However, many explanation methods such as SHAP, GradientShap, DeepLift, Gradient, Integrated Gradient, and Occlusion overlook at least one of these evaluation approaches. (5) It is crucial to ensure that end users can comprehend and understand explainable models. Like DL models, explanation methods are susceptible to adversarial manipulation. An adversary can exploit explanations to produce random feature attributions or reconstruct significant portions of datasets, compromising privacy. Thus, ensuring that DL model explainability does not compromise reliability is crucial. (6) Qualitative evaluation of explainability model usability by security experts highlights that explainable methods need to improve both explanation quality and coherence between\nthe explanations and expert knowledge. Poor coherence may result in explanations that have little applicability to the DL model\u2019s decision-making process.\nThis paper is structured as follows: In Section 2, we provide an overview of explainable models, discuss their importance in comprehending black box models, and examine their essential properties. In Section 3, we categorize widely used explanation methods, examine their strengths and weaknesses, and discuss different evaluation criteria in Section 4. In Section 5, we describe three distinct explanation method applications and examine their performance both qualitatively and quantitatively. In Section 6, we identify major security and privacy concerns related to explanation methods based on our study and experiments. Finally, in Section 7, we present challenges and promising research directions to improve the applicability of explainable AI to security monitoring."
        },
        {
            "heading": "2 EXPLAINABILITY",
            "text": "Background. Taking inspiration from DARPA\u2019s Explainable AI (XAI) program, Vigano et al. proposed [89], XSec, a high-level framework, and characteristics of explainable security, challenges, and research opportunities. However, the framework, like [91], does not analyze methods and solely focuses on providing contexts and example cases. In [38], Hariharan et al. briefly overview explanation methods for four security areas: intrusion detection, malware detection, access control, and threat intelligence, but do not provide guidance on the implementation [38]. While Warnecke et al. [91] evaluated explanation methods for deep learning in security and provided quantitative insights for security researchers, qualitative analysis is missing, which may be the preference of some users who want easier-to-understand or more intuitive explanations. In [60], Nadeem et al. provide several examples to illustrate the use of explainable methods from both designers\u2019 and users\u2019 perspectives, providing a comprehensive review of explanation methods. However, they do not evaluate explanation methods, and they provide only one use-case with LIME [70] and SHAP [53], which has been previously deemed unreliable in the prior research [91].\nMeasuring a model\u2019s accuracy when testing a classification model does not sufficiently describe its performance in real-world applications [24]. The opaque nature of the deep learning model impedes their otherwise impressive performance in diverse fields, making it difficult for end-users to comprehend their functionality and decision-making criteria [74]. Therefore, to achieve finegrained evaluation, gaining deeper insight into the working of the black-box model is important [68]. Explainability helps to gain such insights, thus promoting trustworthiness and ensuring equity, confidentiality, and dependability [68].\nDefinition. The terms \"explainability\" and \"interpretability\" are sometimes used interchangeably; however, they have distinct meanings. Interpretable machine learning involves using constraints to create models easily understood by end-users; for instance, a decision tree [73]. However, achieving high performance with interpretable models can be challenging, particularly for raw data like system logs, where complex deep learning models often outperform simpler models. As a result, monitoring systems for malware classification, vulnerability detection, and anomaly detection have increasingly relied on black-box models to improve performance\n[11]. This reliance on black box models has heightened the need for post-hoc explanation methods that provides explanations for non-interpretable black box models.\nClassification. Post-hoc explanation methods can be classified into different categories based on various factors, including granularity of explanations (local vs. global), supported models (model-agnostic vs. model-specific), and type of explanations (feature attribution, rules or counterfactual) [14]. Table 1 provides a mapping of various explanation methods to their corresponding class. a. Based on Granularity: Explanation methods can either be local or global [59]. Global explanations provide an overall understanding of the model\u2019s behavior across multiple instances. They identify the most important features that the model uses to make decisions. This approach can help assess model biases and strengths and guide improvements. Local explanations help explain the decision-making process of the model for a specific instance by identifying relevant input features that contribute to the model output. b. Based on supported models: Amodel-agnostic explanation method can be used to interpret any type of black-box model (e.g., LIME), whereas a model-specific method can be used to interpret only specific types of networks. For example, GradCAM is used to explain predictions of convolutional neural networks only. c. Type of explanations: Post-hoc explanation methods use feature attribution, rules, or counterfactuals to provide explanations on a given test instance. (a) Feature attribution assigns a relevance score to each feature, indicating its importance in the model\u2019s prediction for the given instance (e.g., LIME [70]). Feature attribution is also known as importance, relevance, contribution, or scores. Formally, given a black box model \ud835\udc53 (.) and a test instance \ud835\udc65 = \ud835\udc651, \ud835\udc652, ....\ud835\udc65\ud835\udc41 , where 1...\ud835\udc41 refers to the input features, a feature attribution-based explanation method \ud835\udf19 (\ud835\udc65) returns a vector \ud835\udc3c\ud835\udc58 (\ud835\udc65) that provides the relevance of the \ud835\udc58 features. The relevance score can be analyzed to identify the crucial features responsible for a model prediction, making feature attribution-based explanation methods the most popular post-hoc explanation technique [14]. (b) Rule-based explanations, commonly used for tabular data, provide a decision rule of the form \ud835\udc65 \u2192 \ud835\udc66, where \ud835\udc65 represents conditions on input features, and \ud835\udc66 represents the model prediction (e.g., ANCHOR [71]). (c) Counterfactual-based explanations attempt to find the closest instance of opposite prediction, such that the difference in feature distribution of the two samples provides explanations for the model prediction (e.g., DeepAID[36]).\nFocus of this paper. In this paper, we evaluate feature attributionbased explanation methods, which can be further classified into perturbation and gradient-basedmethods. Perturbation-basedmethods modify the input and obtain feature attribution by either fitting a local interpretable model or observing the change in the corresponding model prediction, as demonstrated by LIME [70], SHAP [53], and Occlusion [96]. In contrast, gradient-based methods either propagate the model decision to the input layer or utilize the gradient of the output to explain the local decision of neural networks, as exemplified by Gradient [80] and Integrated Gradient [85].\nUsing explanations. Understanding a black-box model prediction on test samples is crucial in security, as it can identify potential model weaknesses that attackers can exploit. From the perspective of a system designer, black-box explanations can help improve the\nsecurity system. For instance, a developer designing a malware classifier can evaluate explanations of the model\u2019s predictions on test cases to ensure that it relies on relevant malware features and not on spurious features. From the perspective of a security analyst, explanations can mitigate blind faith in a machine-learning model. Alerts generated by deep learning models with explanation insights can help security analysts make informed decisions about security threats and improve the overall security of the system[5].\nEvaluating explanations. Explanation methods must satisfy certain properties that guarantee their goodness and usefulness in real-life applications [52, 62, 72]. These properties include accuracy (capturing relevant features), fidelity (approximating the model prediction), stability (producing consistent results), and certainty (reflecting the certainty of the model). Although many explanation methods focus on the goodness of the explanations [52, 62], the usability and human factors of these methods receive insufficient attention [28, 83]. While quantitative evaluation of explanation methods is crucial, qualitative evaluation is equally important since human end-users will ultimately utilize these explanations. In Section 4, we provide details on the qualitative and quantitative evaluation of explanation methods."
        },
        {
            "heading": "3 EXPLANATION METHODS",
            "text": "We briefly introduce the explanation methods evaluated in this paper. Table 1 provides an overview of different methods along with their supported models, the granularity of explanation, explanation type and supported dataset.\nLIME and LEMNA [35, 70]: LIME (Local Interpretable Modelagnostic Explanation) and LEMNA (Local Explanation Methods using Nonlinear Approximation) are techniques used to generate local explanations for black box models. LIME utilizes linear regression to generate an interpretable surrogate model by perturbing a test instance and creating new data samples. The weights of the surrogate model now explain predictions, where higher positive\nweights mean high attributing features. However, LIME\u2019s explanations can also be unreliable and manipulable through careful data manipulation [81]. LIME\u2019s assumption of feature independence and lack of feature combination can limit its applicability in practical applications. LEMNA addresses this limitation using a mixture regression model with a fused lasso penalty to capture feature dependency. However, recent studies have shown that LEMNA may not perform as well as LIME and SHAP in security applications[91].\nSHAP [53]: SHAP (Shapley Additive Explanation), like LIME, generates local explanations for black box models. However, it takes a game-theoretic approach to compute the Shapley values for each feature and then uses them for feature attribution. SHAP introduces fundamental properties for feature attribution and shows that Shapley values only satisfy these properties. Like LIME, SHAP generates new samples around a given sample, obtains predictions from the black box model, and uses the dataset to fit an interpretable linear model. However, unlike LIME, SHAP weights the new instances according to the weight a coalition would receive in the Shapley value estimation rather than their closeness to the original sample. While SHAP is computationally slower than LIME, it offers more robust explanations by incorporating game-theoretic principles but is susceptible to data manipulations through careful data manipulations comprising the reliability of the explanations[81].\nOcclusion [96]: Occlusion is a perturbation-based explanation method that replaces the input features with baselines and computes the difference in output. The user can provide baseline values depending on the use case, and the corresponding output difference provides the feature attribution.\nGradient [80]: Gradient, also known as saliency, computes the gradient of the class score with respect to the input. This gradient measures the change in output predictions given the change in input features. This score is used as feature attribution. A simple improvement over this method was proposed in [79], called GradientXInput, which multiplies the gradient with input features.\nIntegrated gradient (IG) [85]: Gradient simply calculates the gradient of the output with respect to the input feature, but even if a network relies heavily on a particular feature, the gradient of the class score concerning that featuremay have small magnitudes. This issue can occur in deep neural networks (DNNs) due to saturation in the training process. To address this problem, the integrated gradient method accumulates gradients along a linear path from the baseline to the given test sample rather than using simple gradients. The selection of the baseline depends on the specific use-case [84].\nGradientSHAP [29] combines ideas from SHAP [53] and SmoothGrad [82] with integrated gradient [85]. Instead of picking one baseline, it randomly selects a baseline from a distribution of baselines, most commonly from the training set, computes attributions, and averages the result.\nDeepLIFT [78]: DeepLIFT computes the feature attribution score by comparing the activation of each neuron to a \u2018reference activation.\u2019 The difference between the two contributes to the feature attribution score. Similar to integrated gradient, the reference activation is selected based on the specific problem at hand, which is critical for obtaining accurate feature attribution. This method is equivalent to layer-wise relevance propagation (LRP) [10].\nDeepAID [36]: DeepAID proposes an explanation method for security applications using optimization based on specific security\nconstraints. This method finds a reference normal sample \ud835\udc65\u2217 for a given test anomaly \ud835\udc65 such that the difference between \ud835\udc65\u2217 and \ud835\udc65 provides the explanation.\nLimitations: Despite the availability of a wide range of explanation methods for gaining insight into model predictions, there are inherent issues with existing feature attribution methods. Many explanation methods exhibit class-invariant behavior, producing similar feature attribution regardless of the predicted class [61]. Some methods have also been shown to be insensitive to model parameters; even when random numbers replace the parameters of a trained neural network, the feature attribution does not change significantly [2]. Furthermore, explanation methods can produce highly unstable explanations [8] and are susceptible to adversarial manipulations leading to unreliable results [33, 41, 81, 97]. Gradientbased explanation methods, in particular, can help an adversary better estimate gradients in black-box attacks [69] and even reconstruct the underlying model [58]."
        },
        {
            "heading": "4 EVALUATION OF EXPLANATION METHODS",
            "text": "We present comprehensive testing and evaluation metrics to ensure that explanations are of high quality and practical value consistent, and meet consensus. Drawing on our extensive study, we outline critical evaluation methods suitable for security explanations. First, we provide three major categories to evaluate explainability methods [24], a functionally-grounded explanation as one group and application- and human-grounded as another group."
        },
        {
            "heading": "4.1 Functionally-grounded",
            "text": "To assess the efficacy of a proposed explanation method, we need to employ quantitative metrics that utilize formal definitions and properties of explanation quality. This evaluation approach does not necessitate human validation and instead relies entirely on the definitions of relevant features of explanation methods and their mathematical representations. The following are the functionallygrounded evaluation criteria:\n1. Faithfulness: It measures the accuracy of an explanation method in capturing relevant features for a given test sample. It is evaluated by computing the correlation between feature attribution and probability drops when relevant features are modified [7].\n2. Monotonicity: It measures the attribution faithfulness by evaluating if incrementally adding important features improves the model performance [9]. A monotonic increase in performance as more features are added indicates that the explanation method captured the relevant features.\n3. Continuity by Local Lipschitz Estimate: It measures the coherence of the explanation method for similar test inputs. Called explanation continuity, this metric is evaluated using the Lipschitz constant [7] to compare the explanations generated for a given test input and its neighbor samples in a neighborhood of size \ud835\udf16 .\n4. Max-Sensitivity: It approximates change of explanation under slight perturbation using Monte Carlo sampling [13]. A lower max-sensitivity indicates a better explanation method.\n5. Relative output stability: It measures the stability of an explanation with respect to changes in the output logits of the model [4].\n6. Sparsity: It evaluates the feasibility of an explanation by measuring the number of features deemed relevant. It is computed\nusing the Gini Index applied to the vector of the absolute values of attributions [19].\n7. Complexity: It measures the entropy of the fractional contribution of any \ud835\udc56\ud835\udc61\u210e feature to the total magnitude of the attribution [13]. It uses all features of the given test sample to explain a model prediction.\n8. Model parameter randomization : It measures the difference in feature attributions produced by an explanation method, provided that the model parameters are randomly modified. The difference is computed as the correlation between the original feature attribution and the new attribution [2]"
        },
        {
            "heading": "4.2 Application- and Human-grounded",
            "text": "A formal evaluation process should validate an explanation method with real-world applications. And the most effective way involves designing experiments and evaluation by domain experts since the expert can confirm the usability of the explanation method in assisting them with their tasks. For security-related applications, we propose to utilize the following evaluation criteria and measure the usability of an explanation method:\n1. Expertise Level: It measures the expertise required to effectively comprehend and utilize explanation methods. Suppose an explanation contains numerous security domain-specific jargon or complex logic that must be understood to comprehend the decision explanations. In that case, the method will necessitate a high level of expertise (e.g., Level 5). Conversely, if the method provides simple explanations (e.g., \"Malware x should be deleted because it will leak what you type in a web browser\"), it requires little or no expert knowledge, making it Level 1.\n2. Explanation Type: For textual or tabular datasets, the explanation typically involves ranking the input features according to the importance of the explanation method in making its prediction rather than visual representations, as in image datasets.\n3. Coherence [57]: It evaluates the consistency of the explanation methods with the end-users domain knowledge. An end-user of an explanation method may possess prior knowledge about the application and data, allowing them to assess the validity of the explanation output and if they align with the expert\u2019s experience.\n4. Actionability [83]: It measures the usability of explanations with respect to a given application, as an end-user typically prefers explanations that can be applied as guidelines towards a final decision [47]. The simplicity of the explanation is particularly relevant, as it should be concise and to the point such that no further inquiry into the problem is required."
        },
        {
            "heading": "5 USE CASES",
            "text": "In this section, we present three use cases, each of which corresponds to a distinct security threat: network log anomaly detection, malware detection, and adversarial image object detection. For each use case, we conduct a quantitative analysis using a deep learning model trained on a dataset representative of that specific threat. We also analyze several explanation methods suitable for that data type. Multiple explanation metrics are used to evaluate the explanation methods using Captum [46] and Quantus [40]. To complement the quantitative analysis, we also conduct qualitative analysis by interviewing security experts specializing in one or more threats\naddressed by the three use cases. We pose questions covering the advantages, benefits, and challenges associated with using explanation methods in solving domain-specific problems. The questions align with the four criteria discussed in section 3. Code is available2."
        },
        {
            "heading": "5.1 Network Log Anomaly Detection",
            "text": "A Security Operations Center (SOC) analyst receives an alert from a security monitoring tool that analyzes system logs. To investigate this alert, the analyst utilizes the explanation tool to analyze the sequence of log events and validate the alert.\nModel & Dataset: For log anomaly detection, we opted to use the widely used DeepLog architecture [26] and Hadoop Distributed File System (HDFS) [92] dataset. The HDFS dataset comprises of logs generated from running map-reduce jobs on 200 Amazon EC2 nodes and comprises 11.2 million log entries, with 2.9% labeled as anomalies. The log entries are created from 29 distinct log events, each mapped to a unique log key and arranged in a sequence. We trained a Long Short-Term Memory (LSTM) sequence model [25] with a window size of 10. This implies that a history of 10 event sequences in the logs is required to predict the next event.\nWe instantiated the network, trained it on the HDFS training dataset, and saved the final trained model for inference. We then assessed the model\u2019s performance in anomaly detection by testing it on normal and anomalous test datasets. Evaluation exhibits high performance for system anomaly detection with a precision score of 95.28%, recall score of 93.37%, and f1-score of 94.32%.\nEvaluating Explanation Methods: We perform a test case analysis to evaluate different explanation methods\u2019 attribution of the input sequence. Since the input should be an anomalous sequence, we choose the following sequence classified as anomalous by DeepLog, [4, 10, 9, 13, 6, 7, 10, 13, 6, 10]. Next, we apply various explanation methods to compute the attribution weight of each event in the test sequence. Table 2 shows the attribution weights assigned to log events in the sequence, with positive weights highlighted in green. The darker the color, the higher the weight. The red color signifies a negative influence on determining the anomalous event. (a) Quantitative Analysis: Although Gradient, InputXGrad, and DeepLift assign the highest relevance to the first event ID-4 in the sequence, other methods suggest negative relevance for event ID-4 in relation to the anomalous prediction. Additionally, LIME assigns zero weight to certain event IDs, implying that they had no 2https://tinyurl.com/3dcbnxb4\nrelevance to the model prediction. Integrated Gradient (IG), LIME, and Occlusion assign the highest attribution to event ID-10. On the other hand, DeepAID (as shown in Table 3) follows a different approach; it identifies the closest normal instance to the anomaly and replaces the event ID corresponding to the anomaly with a benign log event. In the given test sequence, DeepAID highlights event ID-9 as the point where an anomaly occurs in the sequence.\nA SOC analyst can use the information presented in Table 2 in evaluating a model-generated alarm. The explanation methods are used to examine prior events that are attributed as highly relevant. Alternatively, using DeepAID, the analyst can inspect the single target log event and directly observe the decision criteria of the deep learning model instead of blindly relying on it. On the flip side, we observe inconsistencies in the explanations provided by existing methods, which raises concerns about their reliability. Therefore, we conduct qualitative analysis to validate the findings and assess the quality of the explanations.\n(b) Qualitative Analysis: We interviewed a security expert with over 10 years of experience who is well-versed in network log anomaly detection and the HDFS dataset but not familiar with explainability methods. The expert was asked to interpret the Tables 2, 3. The expert needed guidance in navigating the information presented in tables, especially the meaning of the colors in Table 2. After a brief introduction, the expert began analyzing the explanations based on their prior knowledge and experience, demonstrating coherence. The expert expressed surprise over the choice of event ID-4 as dark green (a very important feature) by half the models. According to the expert, event ID-10, not 4, should have been the top choice, and 4 should not have been chosen at all. The expert admitted that the result could vary for different datasets but still believed that 4 should not be a top choice. The expert wished to see confidence levels of rankings among the methods, without which it was difficult to choose which model(s) to use for decision-making purposes."
        },
        {
            "heading": "5.2 Malware Classification",
            "text": ""
        },
        {
            "heading": "A security researcher designs a PDF malware classifier and wants to investigate if the black box model utilizes relevant properties of",
            "text": "malware PDFs in making accurate predictions.\nModel & Dataset: We trained a classifier to perform PDF malware detection, utilizing the Mimicus dataset [35], which is specifically designed for malicious PDF detection. It includes 135 features such as document structures, counts of JavaScript, counts of JS objects, numbers of sections, and fonts presented in a tabular format. To build the PDF malware classifier, we trained a 3-layer neural network similar to [91] and evaluated the performance on the test set. The model achieved an accuracy of 0.996, a precision of 0.994, and a recall of 0.997.\nEvaluating Explanation Methods: We utilized several explanation methods to identify the top 10 relevant features for a given set of malware PDFs in our testing phase. Table 5 summarizes the relevant features identified by each explanation method, ordered by their level of importance. (a) Quantitative Analysis: Most methods, except for Occlusion, successfully capture key malicious features in PDF malware files such as \ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc61_ \ud835\udc57\ud835\udc60 , \ud835\udc5d\ud835\udc5c\ud835\udc60_\ud835\udc52\ud835\udc5c \ud835\udc53 _\ud835\udc5a\ud835\udc56\ud835\udc5b, and \ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc61_ \ud835\udc57\ud835\udc4e\ud835\udc63\ud835\udc4e\ud835\udc60\ud835\udc50\ud835\udc5f\ud835\udc56\ud835\udc5d\ud835\udc61 [50]. However,\nthe order of these features varies considerably across the different methods. GradientXInput, Integrated Gradient, DeepLIFT, and GradientShap are gradient-based approaches with similar attribution rankings. Most methods also assign relevance to non-indicative features of maliciousness, such as \ud835\udc58\ud835\udc52\ud835\udc66\ud835\udc64\ud835\udc5c\ud835\udc5f\ud835\udc51\ud835\udc60_\ud835\udc5b\ud835\udc62\ud835\udc5a, \ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b, and \ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc61_\ud835\udc4f\ud835\udc5c\ud835\udc65_\ud835\udc59\ud835\udc52\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5f . In contrast, perturbation-based methods such as LIME, SHAP, and Occlusion showed very different feature rankings.\nAlthough feature rankings are insightful, they do not necessarily align with domain expert knowledge of PDF malware. To assist endusers without expertise in explanations, an explanation method should provide actionable guidelines on effectively utilizing the explanations.\n(b) Qualitative Analysis: We interviewed a security expert with over 10 years of experience. They are well-versed in building defense systems against malware attacks. The expert had used explanation methods once before, therefore, was familiar with explainability methods. The expert was asked to interpret Table 5 and immediately reported that they would only use the models with \u2018pos_image_avg\u2019 as the top feature, followed by \u2018pos_image_max\u2019 and \u2018pos_image_min\u2019 (e.g., IG, DeepLift). If these features were not present, they would discard the models. The expert found the features identified by SHAP interesting (e.g., \u2018author_dot\u2019) as they had not previously considered it an indicator of malware. However, the expert noted the need for further explanation on why certain features were chosen as top predictors, such as actual examples where the feature was helpful and a verbal explanation."
        },
        {
            "heading": "5.3 Adversarial Image Detection",
            "text": ""
        },
        {
            "heading": "A security researcher detecting threats to autonomous systems utilizes",
            "text": "explanation methods to comprehend model predictions and explore adversarial image detection.\nDeep learning models can be made to produce false predictions by making imperceptible perturbations to the model inputs [34, 55, 65, 86]. These are known as adversarial attacks. Two main\nNormal class: Bird Gradient Integrated Gradients\nAdversarial class: Cat Gradient Integrated Gradients"
        },
        {
            "heading": "10 images for class Bird, with the adversarial class as Cat.",
            "text": "Explanation methods used are Gradient and Integrated Gradient.\napproaches have been used to build defenses against these attacks: a) modify network training to build more robust models (e.g., adversarial training [34, 49, 55]), and b) detect and remove manipulated examples that can cause a deep learning model to produce incorrect predictions without modifying neural network training [31, 54]. Recent research shows a strong connection between explainability and adversarial robustness [19, 30]. They have shown that adversarial training promotes concise and stable explanations and that training a model to achieve stable explanations can improve its adversarial robustness. Additionally, some studies have investigated using feature attribution techniques to detect adversarial samples [43, 90, 93].\nModel & Dataset: For the MNIST classifier, we used LeNet [51], whereas, for the CIFAR-10 classifier, we use ResNet-50 [39]. The Cleverhans [64] library generates the adversarial samples from the projected gradient descent (PGD) [55] attack for both classifiers. The PGD attack searches for adversarial samples by iteratively modifying the given test sample with perturbations and clipping them within an \ud835\udf16 neighborhood.\nUsing Explanation Methods: Features attributed to adversarial samples have been shown to differ from their benign counterparts due to adversarial perturbations. This causes model predictions to change, and explanation methods to identify different features as important. Figure 2 presents heatmaps for corresponding benign and adversarial images of an example bird image in the CIFAR-10\ndataset. Due to space limitations, we show the usage of two explainable methods\u2013 gradient and integrated gradient. The heatmaps for the benign and adversarial images are visibly different. Research using the CIFAR-10 and MNIST datasets has shown that differences in feature attribution can be computed using statistics of dispersion, such as median absolute deviation (MAD) (see Figure 3(l)), which is the median of the absolute deviations from the vector\u2019s median. Other statistical measures, such as the interquartile range (IQR), have been used for the same datasets with positive outcomes. Based on this observation, we use a threshold to reject adversarial samples.\n(a) Quantitative Analysis: We applied the following threshold strategy based on our observation of the statistical dispersion of feature attribution:\n(1) For the MNIST dataset, we classified a sample as adversarial if the coefficient of IQR of its feature attribution was less than 0.925; otherwise, we classified it as benign. We used the median absolute deviation to classify a sample as adversarial if the MAD of its feature attribution was greater than 0.011; otherwise, we classified it as benign. (2) For the CIFAR-10 dataset, we classified a sample as adversarial if the coefficient of IQR of its feature attribution was less than 0.5; otherwise, we classified it as benign. We used the median absolute deviation to classify a sample as adversarial if the MAD of its feature attribution was less than 0.75; otherwise, we classified it as benign.\nOur experimental results (see Table 6) show that employing an appropriate statistical dispersion measure and threshold strategy specific to each dataset is possible to detect adversarial samples. However, this method relies on explanation methods that produce feature attributions that are statistically distinct for benign and adversarial samples. Unfortunately, the integrated gradient does not provide statistically significant discriminative information on feature attribution for adversarial samples (see Figure 3(r)). This\napproach is also not suitable for high-dimensional images like ImageNet and cannot detect mixed confidence attacks like C&W [18]. Moreover, different datasets necessitate the usage of distinct statistical methods and thresholds.\n(b) Qualitative Analysis: We interviewed two security experts experienced in adversarial image detection who possess an awareness of explainable methods. The experts were asked to interpret Figure 2 and share observations from the heatmaps. One expert was critical of the explanations, as they found little difference between the normal and adversarial heatmaps. The other expert observed disparities in the heatmaps between the benign and adversarial samples, with the colored pixels seen as concentrated in the benign heatmap and scattered in the adversarial counterpart. However, both experts were critical of the explanations, with one remarking that they \u201cneed to be validated\u201d before being used in decision-making. We presented additional information through Figure 3(l). One reviewer suggested that the new information could help build defenses but cautioned that an adaptive attack could be designed to influence the distribution of feature attribution. Both reviewers inquired whether the dispersion was observable across all image types and using all explanation methods, which was not the case."
        },
        {
            "heading": "5.4 Analysis",
            "text": "A trustworthy explanation method should achieve better results in quantitative and qualitative evaluation. For quantitative evaluation, we computed the average performance of all methods and summarized the results in Table 7, assigning an overall rating to each method. Gradient-based methods consistently outperformed perturbation methods (LIME, SHAP, Occlusion).\nAmong the gradient methods, Integrated Gradient achieved the highest scores in faithfulness, max-sensitivity, model parameter randomization, and sparsity. Additionally, Integrated Gradient was preferred over other explanation methods in the qualitative analysis of log anomaly detection and malware detection. Occlusion provided poor explanations qualitatively but achieved good scores in faithfulness, sparsity, and model parameter randomization, quantitatively (see Table 7).\nExpert adaptability to explanation methods was surprisingly positive. We initially expected the security experts to require extensive guidance or prior knowledge of explainable methods. However, during the interviews, they were able to analyze the explanations after providing a short introduction to explanations. All experts acknowledged that the current explainability methods lack the ability to provide a meaningful explanation for real-world tasks. Consequently, our study shows that from the usability perspective of security end-users, the current explanation methods do not meet user requirements. A summary of the qualitative analysis is provided in Table 4."
        },
        {
            "heading": "6 CONCERNS WITH EXPLANATIONS",
            "text": ""
        },
        {
            "heading": "6.1 Security concerns",
            "text": "Error tolerance: In non-security applications, errors in explanations, such as including insignificant details like a few pixels in an image, are usually tolerable as they do not have significant consequences. Partially correct explanations can be sufficient for an intuitive understanding. However, in security explanations, there is no room for error [35]. Security applications require high-quality and robust explanations. Even a tiny mistake, such as a single byte of code in binary analysis or one log sequence in system logs, can result in severe misunderstandings. For instance, in Section 5, a SOC analyst relies on an explanation method to evaluate an alert to diagnose a threat to confirm or dismiss the alert. However, an incorrect explanation or alert validation can affect the organization\u2019s cyber infrastructure. Therefore, an explanation method must ensure that it provides accurate impressions to an analyst.\nEvaluation of explanation: Explanations are evaluated based on their quality and usefulness. Quality is measured using quantitative metrics (discussed in Section 4). Although several metrics are proposed in the literature, there is a lack of uniformity in their acceptance and evaluation [91]. Additionally, relying solely on quantitative metrics can lead to insufficient and inaccurate evaluations. As mentioned in Section 5.4, an explanation method that produces inaccurate explanations may still receive high scores, which can\nmislead an end-user about its performance. The usefulness of an explanation is determined by its benefits to an end-user. Therefore, qualitative evaluation can complement quantitative evaluation and improve the comprehension of explanations, enhancing its usability. As discussed in Section 4, explanation methods should also be evaluated based on usability criteria such as coherence and actionability."
        },
        {
            "heading": "6.2 Privacy concerns",
            "text": "An adversary can exploit model explanations to strengthen their attack, potentially compromising the integrity and confidentiality of a model. For instance, an adversary can use model explanations to obtain sensitive information from a dataset using inference attacks [77]. As demonstrated in [76], an adversary can reconstruct a significant portion of a dataset using model explanation methods. Gradient-based explanations perform better than perturbationbasedmethods but still disclose important training data information. Studies [23, 41, 97] have also shown that explanations can be manipulated to produce targeted explanations, thus deceiving an end-user about a model\u2019s performance. Therefore, it is essential to be aware of any explanation method risks and to design model explanations that protect both data and models [67]."
        },
        {
            "heading": "6.3 End-user concerns",
            "text": "Explanation methods play a crucial role in security applications, forming an essential part of security analysts\u2019 workflow. However, integrating explanation solutions into existing security tools can be challenging, as they must be usable and computationally efficient [63]. A study shows that analysts are still hesitant to use explanation methods due to the additional time for investigation and the complex nature of explanations [20]. Therefore, it is essential to evaluate the intuitiveness, coherence, and usability of explanations for different datasets and explanation methods [3, 12].\nComprehensibility: The comprehensibility of explanations can be affected by the type of explanation method or the dataset. While feature attribution-based methods are suitable for all datasets and models, they provide feature relevance scores that require domain expertise to understand [5]. For example, when we analyze the results of Section 5.1 and 5.2, the explanations are less intuitive given that there is no detailed (or short) verbal explanation that led to such detection. Rule-based explanations, such as ANCHOR, [71] can improve explanation comprehension if they support both the model and dataset. Nevertheless, any explanation method employed in security applications must provide accurate and reliable\ninformation that can be both easily and rapidly understood and does not require additional thought or processing by the end user."
        },
        {
            "heading": "7 CHALLENGES & RESEARCH DIRECTIONS",
            "text": ""
        },
        {
            "heading": "7.1 Transformer Models & Explanations",
            "text": "Transformer models are widely used in natural language sequence tasks due to their ability to capture long-term dependencies in language resulting in state-of-the-art results [88]. This property makes them suitable for sequence prediction in security. However, ensuring transparency in these models is crucial to understand model predictions. Although explanation methods can be applied to transformer models, caution must be exercised due to their heavy reliance on skip connections and attention operators. For instance, the attention mechanism assigns weights to tokenized words in a sequence, but this approach lacks faithfulness and stability [16]. Post-hoc explanation methods like hidden state evolution and neuron activation can be used to understand transformer predictions [6]. However, transformer models are complex and computationally slow, requiring careful evaluation before practical deployment in security applications."
        },
        {
            "heading": "7.2 Stable, reliable and robust explanations",
            "text": "The lack of reliability, stability and usability in existing explanation methods has hindered their acceptance in security applications [32, 63]. For example, adversarial attacks can manipulate these methods to provide false explanations, inhibiting end-user adoption [41, 76]. Therefore, a research direction that has been explored is to develop certifiably robust explanation methods that provide certified robustness guarantees for the explanations [42]. A challenging but contrasting task is to design inherently interpretable models that perform as well as complex deep learning models [73]."
        },
        {
            "heading": "7.3 Addressing accuracy, privacy, and trust",
            "text": "The trustworthiness of black box machine learning models depends on their accuracy, privacy, and explainability [37]. However, improving one of these properties can compromise others, as improving interpretability with model explanation can compromise privacy, and enhancing accuracy with complex models can hinder\nmodel understanding [98]. We propose a pipeline for a system log anomaly detector to address this trade-off, as shown in Figure 4. Our proposed pipeline includes a deep learning model for anomaly detection and post-hoc explanation methods to provide model prediction understanding. Additionally, incorporating contextual knowledge with knowledge graphs [44], we believe alert validation and explanation understanding can be significantly improved as demonstrated in previous works [48].\nTo address privacy concerns in explanation methods, we can adopt privacy-preserving techniques like differentially privatemodel training or federated learning [1, 22, 27, 56, 66, 67]. Differentially private model training can ensure that an adversary cannot exploit black-box model explanations to leak information about the training data [22, 67]. Federated learning also provides a privacypreserving solution due to its distributed nature [56] and has been used in federated log learning for threat forensics [66]. Our proposed pipeline can address the trade-off between accuracy, privacy, and explainability by incorporating these privacy-preserving techniques."
        },
        {
            "heading": "8 CONCLUSION",
            "text": "This research paper provides insights into the explainability of deep learning models for security applications. We study the importance of transparency and interpretability of deep learning model predictions by evaluating several post-hoc explanation methods against three security applications. Quantitative analysis reveals that backpropagation-based explanation methods (e.g., Integrated Gradient) are effective in practical settings. However, the qualitative analysis also shows issues related to the actionability and usability of explanation methods in real-world applications. Overall, it is necessary to develop explanation methods that are reliable, robust, and user-friendly to enhance the practical application of deep learning models in real-world security settings. Finally, we propose a novel pipeline to address the trade-offs between privacy, accuracy, and explainability, thereby enhancing the utility of explainable methods in real-world security applications."
        }
    ],
    "title": "SoK: Modeling Explainability in Security Analytics for Interpretability, Trustworthiness, and Usability",
    "year": 2023
}