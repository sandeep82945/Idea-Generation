{
    "abstractText": "Deep neural networks (DNNs) have been proven to be sensitive towards perturbations on input samples, and previous works highlight that adversarial samples are even more vulnerable than normal ones. In this work, this phenomenon is illustrated from the perspective of sharpness via visualizing the input loss landscape of models. We first show that adversarial samples locate in steep and narrow local minima of the loss landscape (high sharpness) while normal samples, which differs distinctly from adversarial ones, reside in the loss surface that is more flatter (low sharpness). Based on this, we propose a simple and effective sharpness-based detector to distinct adversarial samples by maximizing the loss increment within the region where the inference sample is located. Considering that the notion of sharpness of a loss landscape is relative, we further propose an adaptive optimization strategy in an attempt to fairly compare the relative sharpness among different samples. Experimental results show that our approach can outperform previous detection methods by large margins (average +6.6 F1 score) for four advanced attack strategies considered in this paper across three text classification tasks. Our codes are publicly available at https://github.com/ ruizheng20/sharpness_detection.",
    "authors": [
        {
            "affiliations": [],
            "name": "Rui Zheng"
        },
        {
            "affiliations": [],
            "name": "Shihan Dou"
        },
        {
            "affiliations": [],
            "name": "Yuhao Zhou"
        },
        {
            "affiliations": [],
            "name": "Qin Liu"
        },
        {
            "affiliations": [],
            "name": "Tao Gui"
        },
        {
            "affiliations": [],
            "name": "Qi Zhang"
        },
        {
            "affiliations": [],
            "name": "Zhongyu Wei"
        },
        {
            "affiliations": [],
            "name": "Xuanjing Huang"
        },
        {
            "affiliations": [],
            "name": "Menghan Zhang"
        }
    ],
    "id": "SP:c391981dab573347df70d50f49f35312d30d535b",
    "references": [
        {
            "authors": [
                "Moustafa Alzantot",
                "Yash Sharma",
                "Ahmed Elgohary",
                "Bo-Jhang Ho",
                "Mani Srivastava",
                "Kai-Wei Chang."
            ],
            "title": "Generating natural language adversarial examples",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Gilad Cohen",
                "Guillermo Sapiro",
                "Raja Giryes."
            ],
            "title": "Detecting adversarial samples using influence functions and nearest neighbors",
            "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Laurent Dinh",
                "Razvan Pascanu",
                "Samy Bengio",
                "Yoshua Bengio."
            ],
            "title": "Sharp minima can generalize for deep nets",
            "venue": "Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Javid Ebrahimi",
                "Anyi Rao",
                "Daniel Lowd",
                "Dejing Dou."
            ],
            "title": "HotFlip: White-box adversarial examples for text classification",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 31\u201336,",
            "year": 2018
        },
        {
            "authors": [
                "Reuben Feinman",
                "Ryan R. Curtin",
                "Saurabh Shintre",
                "Andrew B. Gardner."
            ],
            "title": "Detecting adversarial samples from artifacts",
            "venue": "CoRR, abs/1703.00410.",
            "year": 2017
        },
        {
            "authors": [
                "Marguerite Frank",
                "Philip Wolfe."
            ],
            "title": "An algorithm for quadratic programming",
            "venue": "Naval research logistics quarterly, 3(1-2):95\u2013110.",
            "year": 1956
        },
        {
            "authors": [
                "Ji Gao",
                "Jack Lanchantin",
                "Mary Lou Soffa",
                "Yanjun Qi."
            ],
            "title": "Black-box generation of adversarial text sequences to evade deep learning classifiers",
            "venue": "2018 IEEE Security and Privacy Workshops, SP Workshops 2018, San Francisco, CA, USA, May 24, 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Siddhant Garg",
                "Goutham Ramakrishnan."
            ],
            "title": "BAE: BERT-based adversarial examples for text classification",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6174\u20136181, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Ian J. Goodfellow",
                "Oriol Vinyals."
            ],
            "title": "Qualitatively characterizing neural network optimization problems",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Jens Hauser",
                "Zhao Meng",
                "Dami\u00e1n Pascual",
                "Roger Wattenhofer."
            ],
            "title": "BERT is robust! A case against synonym-based adversarial examples in text classification",
            "venue": "CoRR, abs/2109.07403.",
            "year": 2021
        },
        {
            "authors": [
                "Mohit Iyyer",
                "John Wieting",
                "Kevin Gimpel",
                "Luke Zettlemoyer."
            ],
            "title": "Adversarial example generation with syntactically controlled paraphrase networks",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
            "year": 2018
        },
        {
            "authors": [
                "Di Jin",
                "Zhijing Jin",
                "Joey Tianyi Zhou",
                "Peter Szolovits."
            ],
            "title": "Is BERT really robust? A strong baseline for natural language attack on text classification and entailment",
            "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The",
            "year": 2020
        },
        {
            "authors": [
                "Simon Lacoste-Julien."
            ],
            "title": "Convergence rate of frank-wolfe for non-convex objectives",
            "venue": "CoRR, abs/1607.00345.",
            "year": 2016
        },
        {
            "authors": [
                "Kimin Lee",
                "Kibok Lee",
                "Honglak Lee",
                "Jinwoo Shin."
            ],
            "title": "A simple unified framework for detecting outof-distribution samples and adversarial attacks",
            "venue": "Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Pro-",
            "year": 2018
        },
        {
            "authors": [
                "Hao Li",
                "Zheng Xu",
                "Gavin Taylor",
                "Christoph Studer",
                "Tom Goldstein."
            ],
            "title": "Visualizing the loss landscape of neural nets",
            "venue": "Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS",
            "year": 2018
        },
        {
            "authors": [
                "Linyang Li",
                "Ruotian Ma",
                "Qipeng Guo",
                "Xiangyang Xue",
                "Xipeng Qiu."
            ],
            "title": "BERT-ATTACK: Adversarial attack against BERT using BERT",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Xiaoya Li",
                "Jiwei Li",
                "Xiaofei Sun",
                "Chun Fan",
                "Tianwei Zhang",
                "Fei Wu",
                "Yuxian Meng",
                "Jun Zhang."
            ],
            "title": "kFolden: k-fold ensemble for out-of-distribution detection",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Zongyi Li",
                "Jianhan Xu",
                "Jiehang Zeng",
                "Linyang Li",
                "Xiaoqing Zheng",
                "Qi Zhang",
                "Kai-Wei Chang",
                "Cho-Jui Hsieh."
            ],
            "title": "Searching for an effective defender: Benchmarking defense against adversarial word substitution",
            "venue": "Proceedings of the 2021 Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Shiyu Liang",
                "Yixuan Li",
                "R. Srikant."
            ],
            "title": "Enhancing the reliability of out-of-distribution image detection in neural networks",
            "venue": "6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Con-",
            "year": 2018
        },
        {
            "authors": [
                "Jieyu Lin",
                "Jiajie Zou",
                "Nai Ding."
            ],
            "title": "Using adversarial attacks to reveal the statistical bias in machine reading comprehension models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Na Liu",
                "Mark Dras",
                "Wei Emma Zhang."
            ],
            "title": "Detecting textual adversarial examples based on distributional characteristics of data representations",
            "venue": "Proceedings of the 7th Workshop on Representation Learning for NLP, pages 78\u201390, Dublin, Ireland. As-",
            "year": 2022
        },
        {
            "authors": [
                "Qin Liu",
                "Rui Zheng",
                "Bao Rong",
                "Jingyi Liu",
                "ZhiHua Liu",
                "Zhanzhan Cheng",
                "Liang Qiao",
                "Tao Gui",
                "Qi Zhang",
                "Xuanjing Huang."
            ],
            "title": "Flooding-X: Improving BERT\u2019s resistance to adversarial attacks via lossrestricted fine-tuning",
            "venue": "Proceedings of the 60th An-",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Xingjun Ma",
                "Bo Li",
                "Yisen Wang",
                "Sarah M. Erfani",
                "Sudanthi N.R. Wijewickrema",
                "Grant Schoenebeck",
                "Dawn Song",
                "Michael E. Houle",
                "James Bailey."
            ],
            "title": "Characterizing adversarial subspaces using local intrinsic dimensionality",
            "venue": "6th International",
            "year": 2018
        },
        {
            "authors": [
                "Andrew L. Maas",
                "Raymond E. Daly",
                "Peter T. Pham",
                "Dan Huang",
                "Andrew Y. Ng",
                "Christopher Potts."
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human",
            "year": 2011
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu."
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada,",
            "year": 2018
        },
        {
            "authors": [
                "Rishabh Maheshwary",
                "Saket Maheshwary",
                "Vikram Pudi."
            ],
            "title": "Generating natural language attacks in a hard label black box setting",
            "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications",
            "year": 2021
        },
        {
            "authors": [
                "John Morris",
                "Eli Lifland",
                "Jack Lanchantin",
                "Yangfeng Ji",
                "Yanjun Qi."
            ],
            "title": "Reevaluating adversarial examples in natural language",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3829\u20133839, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Edoardo Mosca",
                "Shreyash Agarwal",
                "Javier Rando Ram\u00edrez",
                "Georg Groh."
            ],
            "title": "that is a suspicious reaction!\u201d: Interpreting logits variation to detect NLP adversarial attacks",
            "venue": "Proceedings of the 60th Annual Meeting of the",
            "year": 2022
        },
        {
            "authors": [
                "Maximilian Mozes",
                "Pontus Stenetorp",
                "Bennett Kleinberg",
                "Lewis Griffin"
            ],
            "title": "Frequency-guided word substitutions for detecting textual adversarial 11291",
            "year": 2021
        },
        {
            "authors": [
                "Marwan Omar",
                "Soohyeon Choi",
                "DaeHun Nyang",
                "David Mohaisen."
            ],
            "title": "Robust natural language processing: Recent advances, challenges, and future directions",
            "venue": "CoRR, abs/2201.00768.",
            "year": 2022
        },
        {
            "authors": [
                "Shuhuai Ren",
                "Yihe Deng",
                "Kun He",
                "Wanxiang Che."
            ],
            "title": "Generating natural language adversarial examples through probability weighted word saliency",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1085\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 Conference on Empiri-",
            "year": 2013
        },
        {
            "authors": [
                "Eric Wallace",
                "Pedro Rodriguez",
                "Shi Feng",
                "Ikuya Yamada",
                "Jordan Boyd-Graber."
            ],
            "title": "Trick me if you can: Human-in-the-loop generation of adversarial examples for question answering",
            "venue": "Transactions of the Association for Computational Linguistics, 7:387\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Jiayi Wang",
                "Rongzhou Bao",
                "Zhuosheng Zhang",
                "Hai Zhao."
            ],
            "title": "Distinguishing non-natural from natural adversarial samples for more robust pre-trained language model",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 905\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Tianlu Wang",
                "Xuezhi Wang",
                "Yao Qin",
                "Ben Packer",
                "Kang Li",
                "Jilin Chen",
                "Alex Beutel",
                "Ed Chi."
            ],
            "title": "CATgen: Improving robustness in NLP models via controlled adversarial text generation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in",
            "year": 2020
        },
        {
            "authors": [
                "Xiaosen Wang",
                "Yifeng Xiong",
                "Kun He."
            ],
            "title": "Randomized substitution and vote for textual adversarial example detection",
            "venue": "CoRR, abs/2109.05698.",
            "year": 2021
        },
        {
            "authors": [
                "Yisen Wang",
                "Xingjun Ma",
                "James Bailey",
                "Jinfeng Yi",
                "Bowen Zhou",
                "Quanquan Gu."
            ],
            "title": "On the convergence and robustness of adversarial training",
            "venue": "Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Zhiheng Xi",
                "Rui Zheng",
                "Tao Gui",
                "Qi Zhang",
                "Xuanjing Huang."
            ],
            "title": "Efficient adversarial training with robust early-bird tickets",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8318\u20138331, Abu",
            "year": 2022
        },
        {
            "authors": [
                "KiYoon Yoo",
                "Jangho Kim",
                "Jiho Jang",
                "Nojun Kwak."
            ],
            "title": "Detection of adversarial examples in text classification: Benchmark and baseline via robust density estimation",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 3656\u20133672,",
            "year": 2022
        },
        {
            "authors": [
                "Fuxun Yu",
                "Zhuwei Qin",
                "Chenchen Liu",
                "Liang Zhao",
                "Yanzhi Wang",
                "Xiang Chen."
            ],
            "title": "Interpreting and evaluating neural network robustness",
            "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Yuan Zang",
                "Fanchao Qi",
                "Chenghao Yang",
                "Zhiyuan Liu",
                "Meng Zhang",
                "Qun Liu",
                "Maosong Sun."
            ],
            "title": "Word-level textual adversarial attacking as combinatorial optimization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Wei Emma Zhang",
                "Quan Z. Sheng",
                "Ahoud Alhazmi",
                "Chenliang Li."
            ],
            "title": "Adversarial attacks on deeplearning models in natural language processing: A survey",
            "venue": "ACM Trans. Intell. Syst. Technol., 11(3):24:1\u2013 24:41.",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Jake Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12,",
            "year": 2015
        },
        {
            "authors": [
                "Xinze Zhang",
                "Junzhe Zhang",
                "Zhenhua Chen",
                "Kun He."
            ],
            "title": "Crafting adversarial examples for neural machine translation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Rui Zheng",
                "Bao Rong",
                "Yuhao Zhou",
                "Di Liang",
                "Sirui Wang",
                "Wei Wu",
                "Tao Gui",
                "Qi Zhang",
                "Xuanjing Huang."
            ],
            "title": "Robust lottery tickets for pre-trained language models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Yichao Zhou",
                "Jyun-Yu Jiang",
                "Kai-Wei Chang",
                "Wei Wang"
            ],
            "title": "Learning to discriminate perturbations",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 11282\u201311298 July 9-14, 2023 \u00a92023 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "Despite the popularity and success of pre-trained language models (PLMs), they are vulnerable to textual adversarial attacks (Garg and Ramakrishnan, 2020; Zhang et al., 2020). These attacks are designed to generate semantically consistent and syntactically correct adversarial samples that can fool the model into making incorrect predictions (Ren et al., 2019; Maheshwary et al., 2021). Adversarial vulnerability raises concerns about the safe practice of NLP systems in a variety of tasks\n\u2217Equal contribution. \u2020Corresponding author.\n(Wallace et al., 2019; Zhang et al., 2021; Lin et al., 2021).\nIn machine learning, there are two main streams to counter adversarial attacks: adversarial detection and defense (Cohen et al., 2020). The purpose of detection is to distinguish the adversarial samples from the normal ones and discard them during the inference phase (Mozes et al., 2021; Yoo et al., 2022), while defense aims to predict the correct results of adversarial texts (Li et al., 2021b; Zheng et al., 2022; Omar et al., 2022; Liu et al., 2022b; Xi et al., 2022). The detect-discard strategy is an important step towards a robust model and can be integrated with existing defense methods. A significant challenge in adversarial detection is to explore an effective characteristic for recognition.\nThe existing state-of-the-art adversarial detection methods can be broadly classified into two categories: 1) perturbation-based methods (Mozes et al., 2021; Mosca et al., 2022; Wang et al., 2022) and 2) distribution-based methods (Yoo et al., 2022; Liu et al., 2022a). The perturbation-based methods assume that adversarial samples are more sensitive to perturbations in the input space than normal\n11282\nsamples. These methods are based on the model\u2019s reaction when the input words are perturbed by substitution (Mozes et al., 2021; Wang et al., 2021) or deletion (Mosca et al., 2022). However, these methods rely on empirically designed perturbations and it is difficult to find an optimal perturbation in the discrete text space. More importantly, no attempt has been made to explore why the sensitivity assumption is valid or to provide more details for this assumption.\nWe delve into the input loss landscape to characterize the model\u2019s sensitivities with respect to normal and adversarial samples. By visualizing the input loss landscape of the model, we observe a significant difference between the adversarial and normal samples: the loss surfaces on local minima with respect to adversarial samples are steep and narrow (high sharpness), while those of normal samples are much flatter (low sharpness). The above-mentioned significant distinction makes it eligible for distinguishing adversarial samples from normal ones. However, it remains a challenge on how to effectively measure the sharpness of an input loss landscape.\nIn this work, we formulate the sharpness calculation as a constrained optimization problem whose objective is to find a neighbor within the region where the inference sample is located to maximize the loss increment. The convergence quality of this constrained optimization problem can be assessed by \u201cFrank-Wolfe gap\u201d (i.e., the gap between the global optimum and the current estimate) (Frank and Wolfe, 1956; Lacoste-Julien, 2016). With this criterion, we find that samples tend to converge to different levels, which hinders a fair comparison of relative sharpness between samples (Dinh et al., 2017). Therefore, we design an adaptive optimization strategy that guides the solutions to converge gradually to the same level, thereby significantly improving the detection performance. Our contributions are as follows:\n\u2022 We analyze the geometric properties of the input loss landscape. We reveal that the adversarial samples have a deep and sharp local minima on the input loss landscape.\n\u2022 We propose a detection metric based on the sharpness of input loss landscape, which can be formulated as a constrained optimization problem.\n\u2022 We design an adaptive optimization strategy to guide the calculation of sharpness to converge\nto the same level, which can further improve the detection performance."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Textual Adversarial Attack",
            "text": "Unlike image attacks that operate in a highdimensional continuous input space, text perturbation needs to be performed in a discrete input space (Zhang et al., 2020). Text attacks typically generate adversarial samples by manipulating characters (Ebrahimi et al., 2018; Gao et al., 2018), words (Ren et al., 2019; Jin et al., 2020; Li et al., 2020; Alzantot et al., 2018; Zang et al., 2020; Maheshwary et al., 2021), phrases (Iyyer et al., 2018), or even the entire sentence (Wang et al., 2020). The most widely used word-level attacks use the greedy algorithm (Ren et al., 2019) and combinatorial optimization (Alzantot et al., 2018) to search for the minimum number of substitute words. Moreover, these attacks guarantee the fluency of adversarial samples in semantics (Li et al., 2020) or embedding space (Jin et al., 2020) to generate more stealthy adversarial samples. Recent studies have shown that most of the adversarial samples generated are of low quality, unnatural, and rarely appear in reality (Hauser et al., 2021; Wang et al., 2022)."
        },
        {
            "heading": "2.2 Textual Adversarial Detection",
            "text": "Existing adversarial detection methods are mainly divided into two categories: 1) perturbation-based methods and 2) distribution-based methods. Zhou et al. (2019) propose a discriminator that learns to recognize word-level adversarial substitutions and then correct them. Yoo et al. (2022) assume that the representation distribution of original samples follows a multivariate Gaussian and use robust density estimation (Feinman et al., 2017) to determine the likelihood of a sentence being perturbed. Liu et al. (2022a) introduce the local intrinsic dimensionality (Ma et al., 2018) from image processing to text domain. Wang et al. (2022) apply the anomaly detector to identify unnatural adversarial samples and then use textual transformations to mitigate the adversarial effect. Mozes et al. (2021) find that word-level adversarial attacks tend to replace input words with less frequent ones, and exploit the frequency property of adversarial word substitutions to detect adversarial samples. Mosca et al. (2022) introduce a logits-based metric to capture the model\u2019s reaction when the input words are omitted. However, these methods rely on empirically\ndesigned word-level perturbations, making it difficult to find an optimal perturbation."
        },
        {
            "heading": "3 Delving into Input Loss Landscape",
            "text": "Our aim is to better understand adversarial samples, and thereby derive a potentially effective detector. In this section, we investigate the geometric properties of the input loss landscape and show a clear correlation between the sharpness of loss landscape and adversarial samples."
        },
        {
            "heading": "3.1 Visualizing Loss Landscape",
            "text": "Assume we have a PLM h with a loss function \u2113(x0, y), where x0 is the normal input text, y is the label and h(x0) denotes the output logit. As the labels are unknown to the user in adversarial sample detection, we use the \u201cpredicated\u201d label y\u2217 = argmaxy p(y|x0) in place of the golden label y. Following the visualization method proposed by (Goodfellow and Vinyals, 2015) and (Li et al., 2018), we project the high-dimensional loss surface into a 2D hyperplane, where two projection vectors \u03b1 and \u03b2 are chosen and normalized as the basis vectors for the x and y axes. Then the loss values around the input x can be calculated as:\nV (i, j) = \u2113(x+ i \u00b7\u03b1+ j \u00b7 \u03b2, y\u2217). (1)\nThe coordinate (i, j) denotes the distance the origin moves along \u03b1 and \u03b2 directions, and V (i, j) is the corresponding loss value that measures the confidence in the model prediction y\u2217 when perturbing the original input x. In Appendix A.1, we show more details about the input loss landscape."
        },
        {
            "heading": "3.2 Results",
            "text": "Figs. 2(a) and (b) show two visualizations of the loss surface in the input embedding space, which gives an intuition of the huge difference between the normal and adversarial samples: (1) The adversarial samples\u2019 loss surface has a deep and sharp bottom, while the normal samples\u2019 has a much flatter local minimum. (2) By visualizing the contour map, we find that adversarial samples are located in a very narrow valley on the loss landscape, while the normal ones reside in a wide area. The above observations suggest that, the adversarial samples are more sensitive to perturbations than normal samples. As shown in Figure 2(c), once small perturbations are injected into the inputs of adversarial samples, their loss will increase significantly and the predictions are easily flipped. The significant difference in the sharpness of the input loss landscape makes it eligible for distinguishing adversarial samples from normal ones.\nThis difference stems from two inherent properties of model training and adversarial sample generation. First, the model training progressively minimizes the loss of each normal training sample, while the adversarial samples are not available during the training process. Thus, normal samples are in general relatively far away from the decision boundary (Yu et al., 2019). Second, attackers aim to generate human-imperceptible adversarial perturbations, so the attack process stops once the perturbation successfully fools the model, which often results in just-cross-boundary adversarial samples (Alzantot et al., 2018; Li et al., 2020)."
        },
        {
            "heading": "4 Proposed Method",
            "text": "In this section, we first show how a detector can be potentially designed by using loss sharpness to distinguish between adversarial and normal samples."
        },
        {
            "heading": "4.1 Sharpness of Input Loss Landscape",
            "text": "The sharpness of \u2113 (for the model) at x measures the maximum increase of the prediction loss when moving x to a nearby input. Thus, we have the objective:\nmax \u2225x\u2212x0\u2225F\u2264\u03f5\n\u2113(x, y\u2217), (2)\nwhere x is an input within a Frobenius ball around normal sample x0 with radius \u03f5. This maximization problem is typically nonconcave with respect to the input x.\nClassical first-order optimization algorithms, such as projected gradient descent (PGD) (Madry et al., 2018), can be used to estimate sharpness. Starting from a given input x0, PGD generate a sequence {xk} of iterates that converge to the optimal solution. If the current estimates xk goes beyond the \u03f5-ball, it is projected back to the \u03f5-ball:\nxki = \u220f( xk\u22121i + \u03b7 \u00b7 sign(\u2207x\u2113(xk\u22121i ,yi)) ) ,\nwhere \u03b7 is the step size, sign(\u00b7) denotes the sign function and \u220f \u2225\u03b4\u2225\u2264\u03f5(\u00b7) is the projection function"
        },
        {
            "heading": "4.2 Convergence Analysis",
            "text": "The non-convexity of loss function in deep neural network makes the constrained optimization problem in Equation (2) also non-convex. How well this non-convex optimization is solved directly affects the ability to distinguish adversarial samples from normal ones. Since the gradient norm of \u2113 is not an appropriate criterion for non-convex objectives, we introduce the \u201cFrank-Wolfe (FW) gap\u201d (Frank and Wolfe, 1956) to measure the gap between global optimum and current estimate. Consider the FW gap of Equation (2) at xk(Wang et al., 2019):\ng(xk) = max x\u2208X\n\u2329 x\u2212 xk,\u2207xf(xk) \u232a , (3)\nwhere X = {x|\u2225x \u2212 x0\u2225F \u2264 \u03f5} is the input domain of the \u03f5-ball around normal sample x0, f(xk) = \u2113(xk, y\u2217) and \u27e8\u00b7\u27e9 is the inner product. An appealing property of FW gap is that it is invariant to an affine transformation of the domain {x|\u2225x \u2212 x0\u2225F \u2264 \u03f5} in Equation (2) and is not tied to any specific choice of norm, unlike the\ncriterion \u2225\u2207xf(xk)\u2225. Moreover, we always have g(xk) \u2265 0, and a smaller value of g(xk) indicates a better solution of the constrained optimization problem.\nThe FW gap has the following closed form solutions and can be computed for free in our proposed algorithm:\ng(xk) =max x\u2208X\n\u2329 x\u2212 xk,\u2207xf(xk) \u232a\n=max x\u2208X\n\u2329 x\u2212 x0 + x0 \u2212 xk,\u2207xf(xk) \u232a\n=max x\u2208X\n\u2329 x\u2212 x0,\u2207xf(xk) \u232a\n+ \u2329 x0 \u2212 xk,\u2212\u2207xf(xk, y\u2217) \u232a\n= \u221a \u03f5\u2225\u2207xf(xk)\u2225F \u2212 \u2329 xk \u2212 x0,\u2207xf(xk) \u232a .\nThe sample-wise criterion g(xk) reflects the convergence quality of xk with respect to both input constraint and the loss function. Optimal convergence where g(xk) = 0 is achieved when 1) \u2207xf(xk) = 0, i.e., xk is a stationary point in the interior of X ; or 2) xk\u2212x0 = \u221a\u03f5 \u00b7sign(\u2207xf(xk)), that is, local maximum point of f(xk) is reached on the boundary of X . The FW gap allows monitoring and controlling the convergence quality of the sharpness optimization among different samples."
        },
        {
            "heading": "4.3 Adaptive Optimization",
            "text": "As shown in Figure 3, optimizing the maximization problem in Equation (2) at a fixed step size leads to different FW gaps among the samples. However, the concept of sharpness of a minimum is relative, and it is difficult to fairly compare the sharpness of different minima when the convergence quality of Equation (2) is not the same. Thus, the inconsistent convergence quality reduces the disparity between normal and adversarial samples. It motivates us to\nmonitor and control the quality of convergence to the identical level for all samples. Therefore, we propose to optimize the sharpness by adaptively decreasing the step size (increasing convergence quality) and stop the optimization process when a predefined convergence criterion is reached. Our proposed adaptive step size at the k-th step is:\n\u03b7k = min { 0,\ngmin \u2212 g(xk) gmin \u2212 g(x0)\n\u00b7 \u03b70 } , (4)\nwhere \u03b70 is the initial step size and gmin is the predefined convergence criterion. According to the estimation of the FW gap, the step size decreases linearly towards zero as the optimization proceeds, and is zero after the convergence criterion is achieved. We use the early stopping strategy to save computational overhead during inference by halting the optimization process when the FW gap is less than gmin. For non-convex objective, the first-order optimization method requires at most O(1/g2min) iterations to find an approximate stationary point with gap smaller than gmin."
        },
        {
            "heading": "5 Experimental Setup",
            "text": "We validate the effectiveness of the proposed method on three classification benchmarks: IMDB (Maas et al., 2011), SST-2 (Socher et al., 2013) and AGNews (Zhang et al., 2015). The first two are binary sentiment analysis tasks that classify reviews into positive or negative sentiment, and the last one is a classification task in which articles are categorized as world, sports, business or sci/tech. We use the widely used BERTBASE as the target model and use three attacks to generate adversarial samples for detection."
        },
        {
            "heading": "5.1 Baselines",
            "text": "We compare our proposed detectors based on sharpness of input loss landscape (Sharpness) with several strong baselines in adversarial sample detection. MD (Lee et al., 2018): A simple yet effective method for detecting out-of-distribution and adversarial samples in the image processing domain. The main idea is to induce a generative classifier under Gaussian discriminant analysis, which results in a detection score based on Mahalanobis distance. DISP (Zhou et al., 2019): A novel framework learns to identify perturbations and can correct malicious perturbations. To detect adversarial attacks, the perturbation discriminator verifies the likelihood that a token in the text has been perturbed. FGWS (Mozes et al., 2021) leverages the frequency properties of adversarial word substitution to detect adversarial samples. Word-level attacks have a tendency to replace the input word with a less frequent one. RDE (Yoo et al., 2022): To model the probability density of the entire sentence, which uses parametric density estimation for features and generates the likelihood of a sentence being perturbed. MDRE (Liu et al., 2022a) is a multi-distance representation ensemble method based on the distribution characteristics of adversarial sample representations."
        },
        {
            "heading": "5.2 Adversarial Attacks",
            "text": "We selected three widely used attack methods according to the experimental setting used in previous work. PWWS (Ren et al., 2019) is based on a greedy algorithm that uses word saliency and prediction probability to determine word substitution order and maintains a very low word substitution rate. TextFooler (Jin et al., 2020) first identifies important words in the sentence and then replaces them with semantically similar and gram-\nmatically correct synonyms until the prediction changes. BERT-Attack (Li et al., 2020) uses BERT to generate adversarial text, so that the generated adversarial samples are fluent and semantically preserved. TextFooler-adj (Morris et al., 2020) adjusts constraints to better preserve semantics and syntax, which makes adversarial samples less detectable."
        },
        {
            "heading": "5.3 Evaluation Metrics",
            "text": "Following previous works, we use the following three metrics to measure the effectiveness of a method in detecting adversarial samples. (1) Detection accuracy (ACC) corresponds to the maximum classification probability over all possible thresholds. (2) F1-score (F1) is defined as the harmonic mean of precision and recall. (3) Area Under ROC (AUC) is a threshold-independent metric that can be interpreted as the probability that a positive sample is assigned a higher detection score than a negative sample. The ROC curve describes the relationship between the true positive rate (TPR) and the false positive rate (FPR). For all three metrics, a higher value indicates better performance."
        },
        {
            "heading": "5.4 Implementation Details",
            "text": "We fine-tune the BERT-based victim model using the official default settings. For SST-2, we use the officially provided validation set, while for IMDB and AGNews, we use 10% of the data in the training set as the validation set. The validation set and the adversarial samples generated based the\nvalidation set are used for the selection of hyperparameters and thresholds. All three attacks are implemented using TextAttack framework with the default parameter settings.1 Following Mozes et al. (2021), we build a balanced set consisting of 2, 000 test instances and 2, 000 adversarial samples to evaluate the detectors. For SST-2, we use all 1, 872 test data to construct the balanced set. Hyperparameters and decision thresholds of the proposed methods are presented in Appendix A.3."
        },
        {
            "heading": "6 Experimental Results and Analysis",
            "text": "In this section, we show the performance of the proposed method in a comprehensive way and investigate the effect of hyperparameters on performance."
        },
        {
            "heading": "6.1 Main Results",
            "text": "Unless specifically stated otherwise, we follow a common practice (Mozes et al., 2021; Yoo et al., 2022; Mosca et al., 2022) to ensure that our detection mechanism is tested on successful adversarial samples that can actually fool the model. Table 1 reports the detection performance of our method under various configurations. We can observe that: 1) Compared with previous detection methods, the proposed detector based on sharpness achieves significant improvements in three evaluation metrics. This demonstrates the effectiveness of sharpness of the input loss landscape in detecting adversar-\n1https://github.com/QData/TextAttack\nial samples. 2) The performance of FGWS decreases under TextFooler and BERT-Attack, which are more subtle attacks with less significant frequency differences, as FGWS relies on the occurrence of rare words. FGWS also performs poorly on AGNews, most likely because it covers four different news domains, resulting in its low word frequency. These results are consistent with the results reported by Yoo et al. (2022). 3) DISP is a threshold-independent method and therefore AUC metric is not applicable. DISP does not perform well except on AGNews dataset."
        },
        {
            "heading": "6.2 More Rigorous Metric",
            "text": "TNR@95TPR is short for true negative rate (TNR) at 95% true positive rate (TPR), which is widely\nused in out-of-distribution detection (Li et al., 2021a; Liang et al., 2018). But to our knowledge, no textual adversarial sample detector has been evaluated using this metric. TNR@95TPR can be interpreted as the probability of a normal sample being correctly classified (Acc-) when the probability of an adversarial sample being correctly classified (Acc+) is as high as 95%. As can be seen in Table 3, with this strict evaluation metric, there is a significant advantage for our prediction-loss-based detector, while FGWS fails to detect the normal samples at all."
        },
        {
            "heading": "6.3 More Model",
            "text": "In previous experiments, all results are based on the BERT-base model, and we also evaluate the performance of the proposed method on RoBERTabase (Liu et al., 2019). Table 2 shows the detection results using RoBERTa as the victim model. The overall trend among detection methods is similar to Table 1. From the results in Tables 1 and 2, it can be concluded that our proposed methods perform as stable as the traditional statistical-based methods (MD and RDE) under different experimental settings, while empirically designed DISP and FGWS do not perform consistently."
        },
        {
            "heading": "6.4 Ablation Study",
            "text": "To better illustrate the contribution of adaptive optimization strategy to the proposed detector, we perform an ablation study by removing adaptive\noptimization (w/o Adaptive). The experimental results are shown in Table 4. We can observe that the adaptive optimization strategy is important for the sharpness calculation. The inconsistent convergence quality reduces the disparity between normal and adversarial samples."
        },
        {
            "heading": "6.5 Hyper-parameter Investigation",
            "text": ""
        },
        {
            "heading": "6.5.1 Detection Threshold",
            "text": "To investigate the influence of detection thresholds, we analyze the performance with different thresholds on the three datasets, as shown in Figure 6. The performance of the proposed detector gradually improves as the threshold increases, but when the threshold is too large, the results of the detectors are concentrated in one certain category, leading to a decrease in performance. The peak performance of both detectors occurs near the midpoint of the potential thresholds, indicating that our method performs well on both normal and adversarial samples."
        },
        {
            "heading": "6.5.2 Parameters of Optimization",
            "text": "Figure 5 shows the detection performance with different step sizes and numbers of steps. In order\nto show more intuitively the effect of optimization steps and size on the AUC metric, we preserve the results within 2 percents below the highest value, and the rest of the data are shown as light-colored blocks in Figure 5. We can observe that the proposed detector achieve sufficiently consistent performance under various optimization parameters (i.e., the number of steps K and step size \u03b7), and the detection performance is decided by \u03b4K \u2248 K \u00d7 \u03b7."
        },
        {
            "heading": "7 Conclusion",
            "text": "Our work starts from a finding: adversarial samples locate in steep and narrow local minima of the loss landscape while normal samples, which differs distinctly from adversarial ones, reside in the loss surface that is more flatter. Based on this, we propose a simple and effective sharpness-based detector that uses an adaptive optimization strategy to compute sharpness. Experimental results have demonstrated the superiority of our proposed method compared to baselines, and analytical experiments have further verified the good performance of our method\nunder different parameters.\nLimitations\nIn this work, we propose a detector that aims to detect adversarial samples via sharpness of input loss landscape for model. However, the computational cost of the sharpness is high because it requires at most K-step gradient descents. Moreover, in this work, we mainly considered word-level adversarial sample detection as often studied in previous work, while character-level and sentence-level adversarial samples are not studied. These two problems will be explored in our future work."
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors wish to thank the anonymous reviewers for their helpful comments. This work was partially funded by National Natural Science Foundation of China (No.62206057,62076069,61976056), Shanghai Rising-Star Program (23QA1400200), and Natural Science Foundation of Shanghai (23ZR1403500), except the fourth author Qin Liu, who is funded by Graduate Fellowship from University of Southern California."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Input Loss Landscape In Figure 7, we comprehensively show the differences between the input loss landscapes of normal and adversarial samples on three datasets. Adversarial samples are generated by the three textual adversarial attacks that we used in the experimental section. Front elevation view of the input loss landscape on IMDB is shown in Figure 8. The sharp input loss landscape of adversarial samples is not a coincidence; it is a general phenomenon.\nA.2 Detection Score As a supplement, we show the detection score distributions of the proposed detectors and other baseline methods on the SST-2 and IMDB datasets in Figure 9. Our detection scores are still more discriminative than the other baselines.\nA.3 Hyperparameters The optimal hyperparameter values are taskspecific, but the following range of possible values works well in all tasks: 1) the number of steps K: 1, 2, . . . , 10; 2) step size \u03b7 is tuned via a grid search within the range of [2e\u22123, 2e\u22122] with interval 2e\u22122; 3) decision threshold is chosen via a grid search within the range of [0, 1] with interval 1e\u22122.\nACL 2023 Responsible NLP Checklist"
        },
        {
            "heading": "A For every submission:",
            "text": ""
        },
        {
            "heading": "3 A1. Did you describe the limitations of your work?",
            "text": "The limitation section is after the conclusion part of the thesis.\n7 A2. Did you discuss any potential risks of your work? Our work don\u2019t have potetial risk."
        },
        {
            "heading": "3 A3. Do the abstract and introduction summarize the paper\u2019s main claims?",
            "text": "The abstract is at the beginning of the article and the introduction is Section 1.\n7 A4. Have you used AI writing assistants when working on this paper? Left blank.\nB 7 Did you use or create scientific artifacts? Left blank.\nB1. Did you cite the creators of artifacts you used? No response.\nB2. Did you discuss the license or terms for use and / or distribution of any artifacts? No response.\nB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? No response.\nB4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? No response.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? No response.\nB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. No response.\nC 3 Did you run computational experiments? Section 5 and Section 6.\nC1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? No response.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\nC2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? No response.\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? No response."
        },
        {
            "heading": "3 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did",
            "text": "you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? Section 5\nD 7 Did you use human annotators (e.g., crowdworkers) or research with human participants? Left blank.\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? No response.\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants\u2019 demographic (e.g., country of residence)? No response.\nD3. Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? No response.\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response.\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response."
        }
    ],
    "title": "Detecting Adversarial Samples through Sharpness of Loss Landscape",
    "year": 2023
}