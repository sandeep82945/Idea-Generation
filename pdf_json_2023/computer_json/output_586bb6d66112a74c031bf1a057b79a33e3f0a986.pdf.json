{
    "abstractText": "We introduce Trustera1, the first functional system that redacts personally identifiable information (PII) in real-time spoken conversations to remove agents\u2019 need to hear sensitive information while preserving the naturalness of live customer-agent conversations. As opposed to post-call redaction, audio masking starts as soon as the customer begins speaking to a PII entity. This significantly reduces the risk of PII being intercepted or stored in insecure data storage. Trustera\u2019s architecture consists of a pipeline of automatic speech recognition, natural language understanding, and a live audio redactor module. The system\u2019s goal is three-fold: redact entities that are PII, mask the audio that goes to the agent, and at the same time capture the entity, so that the captured PII can be used for a payment transaction or caller identification. Trustera is currently being used by thousands of agents to secure customers\u2019 sensitive information.",
    "authors": [
        {
            "affiliations": [],
            "name": "Evandro Gouv\u00eaa"
        },
        {
            "affiliations": [],
            "name": "Ali Dadgar"
        },
        {
            "affiliations": [],
            "name": "Shahab Jalalvand"
        },
        {
            "affiliations": [],
            "name": "Rathi Chengalvarayan"
        },
        {
            "affiliations": [],
            "name": "Badrinath Jayakumar"
        },
        {
            "affiliations": [],
            "name": "Ryan Price"
        },
        {
            "affiliations": [],
            "name": "Nicholas Ruiz"
        },
        {
            "affiliations": [],
            "name": "Jennifer McGovern"
        },
        {
            "affiliations": [],
            "name": "Srinivas Bangalore"
        },
        {
            "affiliations": [],
            "name": "Ben Stern"
        }
    ],
    "id": "SP:49c1d4bcd6767e83874a4779bccea64ff5a885a3",
    "references": [
        {
            "authors": [
                "Gartner"
            ],
            "title": "COVID-19 crisis accelerates rise of virtual call centers",
            "venue": "2020.",
            "year": 2020
        },
        {
            "authors": [
                "Amber Stubbs",
                "Christopher Kotfila",
                "\u00d6zlem Uzuner"
            ],
            "title": "Automated systems for the de-identification of longitudinal clinical narratives: Overview of 2014 i2b2/uthealth shared task track 1",
            "venue": "Journal of biomedical informatics, vol. 58, pp. S11\u2013S19, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "Ishna Neamatullah",
                "Margaret M Douglass",
                "H Lehman Li-wei",
                "Andrew Reisner",
                "Mauricio Villarroel",
                "William J Long",
                "Peter Szolovits",
                "George B Moody",
                "Roger G Mark",
                "Gari D Clifford"
            ],
            "title": "Automated de-identification of free-text medical records",
            "venue": "BMC medical informatics and decision making, vol. 8, no. 1, pp. 32, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "Ido Cohn",
                "Itay Laish",
                "Genady Beryozkin",
                "Gang Li",
                "Izhak Shafran",
                "Idan Szpektor",
                "Tzvika Hartman",
                "Avinatan Hassidim",
                "Yossi Matias"
            ],
            "title": "Audio deidentification: A new entity recognition task",
            "venue": "arXiv preprint arXiv:1903.07037, 2019.",
            "year": 1903
        },
        {
            "authors": [
                "Sahar Ghannay",
                "Antoine Caubriere",
                "Yannick Esteve",
                "Antoine Laurent",
                "Emmanuel Morin"
            ],
            "title": "End-to-end named entity extraction from speech",
            "venue": "arXiv preprint arXiv:1805.12045, 2018.",
            "year": 1805
        },
        {
            "authors": [
                "Mohamed Hatmi",
                "Christine Jacquin",
                "Emmanuel Morin",
                "Sylvain Meignier"
            ],
            "title": "Named entity recognition in speech transcripts following an extended taxonomy",
            "venue": "First Workshop on Speech, Language and Audio in Multimedia, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "Katsuhito Sudoh",
                "Hajime Tsukada",
                "Hideki Isozaki"
            ],
            "title": "Incorporating speech recognition confidence into discriminative named entity recognition of speech data",
            "venue": "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, 2006, pp. 617\u2013624.",
            "year": 2006
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Li Deng",
                "Dong Yu",
                "George E Dahl",
                "Abdel-rahman Mohamed",
                "Navdeep Jaitly",
                "Andrew Senior",
                "Vincent Vanhoucke",
                "Patrick Nguyen",
                "Tara N Sainath"
            ],
            "title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
            "venue": "IEEE Signal Process. Mag., vol. 29, no. 6, pp. 82\u201397, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "Slava Katz"
            ],
            "title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer",
            "venue": "IEEE Trans. Acoust., Speech, Signal Process., vol. 35, no. 3, pp. 400\u2013401, 1987.",
            "year": 1987
        },
        {
            "authors": [
                "Christopher Cieri",
                "David Miller",
                "Kevin Walker"
            ],
            "title": "The Fisher corpus: A resource for the next generations of speech-to-text",
            "venue": "LREC, 2004, vol. 4, pp. 69\u201371.",
            "year": 2004
        },
        {
            "authors": [
                "Oleksii Kuchaiev",
                "Jason Li",
                "Huyen Nguyen",
                "Oleksii Hrinchuk",
                "Ryan Leary",
                "Boris Ginsburg",
                "Samuel Kriman",
                "Stanislav Beliaev",
                "Vitaly Lavrukhin",
                "Jack Cook"
            ],
            "title": "Nemo: a toolkit for building AI applications using neural modules",
            "venue": "arXiv preprint arXiv:1909.09577, 2019.",
            "year": 1909
        },
        {
            "authors": [
                "Mehryar Mohri",
                "Fernando Pereira",
                "Michael Riley"
            ],
            "title": "Speech recognition with weighted finite-state transducers",
            "venue": "Springer Handbook of Speech Processing, pp. 559\u2013584. Springer, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "F. Pedregosa",
                "G. Varoquaux",
                "A. Gramfort",
                "V. Michel",
                "B. Thirion",
                "O. Grisel",
                "M. Blondel",
                "P. Prettenhofer",
                "R. Weiss",
                "V. Dubourg",
                "J. Vanderplas",
                "A. Passos",
                "D. Cournapeau",
                "M. Brucher",
                "M. Perrot",
                "E. Duchesnay"
            ],
            "title": "Scikit-learn: Machine learning in Python",
            "venue": "Journal of Machine Learning Research, vol. 12, pp. 2825\u2013 2830, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "Subhabrata Mukherjee",
                "Ahmed Hassan Awadallah"
            ],
            "title": "Uncertainty-aware self-training for text classification with few labels",
            "venue": "arXiv preprint arXiv:2006.15315, 2020.",
            "year": 2006
        },
        {
            "authors": [
                "David Malah"
            ],
            "title": "A novel approach for VAD adaptation in nonstationary noise environments",
            "venue": "Tech. Rep. HA6153000-981015-09TM, AT&T Labs-Research, 1996.",
            "year": 1996
        },
        {
            "authors": [
                "Alex Graves",
                "Santiago Fern\u00e1ndez",
                "Faustino Gomez",
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
            "venue": "Proceedings of the 23rd international conference on Machine learning, 2006, pp. 369\u2013376.",
            "year": 2006
        },
        {
            "authors": [
                "Alex Graves"
            ],
            "title": "Sequence transduction with recurrent neural networks",
            "venue": "arXiv preprint arXiv:1211.3711, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "Daniel Pressel",
                "Sagnik Ray Choudhury",
                "Brian Lester",
                "Yanjie Zhao",
                "Matt Barta"
            ],
            "title": "Baseline: A library for rapid modeling, experimentation and development of deep learning algorithms targeting NLP",
            "venue": "Proceedings of Workshop for NLP Open Source Software (NLP- OSS). 2018, pp. 34\u201340, Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Somshubra Majumdar",
                "Jagadeesh Balam",
                "Oleksii Hrinchuk",
                "Vitaly Lavrukhin",
                "Vahid Noroozi",
                "Boris Ginsburg"
            ],
            "title": "Citrinet: Closing the gap between nonautoregressive and autoregressive end-to-end models for automatic speech recognition",
            "venue": "arXiv preprint arXiv:2104.01721, 2021.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014 speech recognition, live entity redaction, human-computer interaction"
        },
        {
            "heading": "1. INTRODUCTION",
            "text": "Gartner Research projects [1] that the customer experience workforce that work from home (WFH) will increase from 5% in 2017 to 35% by 2023. This presents an increased risk of breaches of sensitive customer data. An IBM work from home study in 2020 reported that 42% of United Statesbased respondents who were newly WFH worked with PII in their job. More than half had not been provided with updated guidelines on how to handle PII while working from home, despite being required to handle PII to support customer service requests2. In addition to PCI-DSS violations, payment information falls under the scope of GDPR, and violations can cost businesses up to C20 million or 4% of the annual worldwide turnover of the preceding financial year3.\nIn order to safeguard customer PII, we present Trustera, a real-time dialog mediation system that intercepts and redacts\n\u2217The author performed the work while at Interactions LLC. 1https://www.interactions.com/products/trustera/ 2https://newsroom.ibm.com/2020-06-22-IBM-Security-Study-Finds-\nEmployees-New-to-Working-from-Home-Pose-Security-Risk 3https://gdpr.eu/fines/\nsensitive personal information so call center agents can complete natural conversations with customers without needing to hear PII such as payment card information (PCI) and social security numbers (SSNs). To prevent agents from hearing customer PII, Trustera mutes the audio sent to the agent as soon as it detects the beginning of PCI, such as payment card numbers and the security code on the back of the card (CVV). Trustera is currently being used by thousands of agents to secure customers\u2019 sensitive information. Its architecture consists of a pipeline of online automatic speech recognition (ASR), natural language understanding (NLU) and a live audio redactor (LAR) module.\nTo the best of our knowledge, Trustera is the first AIdriven audio redaction service that redacts PII in real-time and protects the entire call. Other previously proposed solutions use DTMF suppression or Conversational AI chatbots to protect PII, but they require agents to manually turn on the service in order to protect a portion of a call. While the concept of redaction is not new, most redaction experiments [2, 3] cover written text, which do not contain noisy input induced by ASR errors. Some commercial companies4 provide an ASR redaction service for streaming ASR transcripts, but it does not mask PII in the audio, so the call is not protected in real-time. Trustera identifies and redacts PII entities in the audio as soon as the entity is uttered, minimizing the exposure of PII to agents and to downstream call recordings. Offline audio de-identification [4] is the closest to our work; however, it redacts PII on pre-recorded audio as opposed to live conversations and does not have latency constraints.\nAnother related research area is spoken entity detection. [5] experiment with cascading ASR followed by NLU module, and compares it to an integrated end-to-end approach. Currently Trustera uses the cascaded pipeline approach in order to minimize the CPU load, since call center servers usually lack GPUs. End-to-end approaches usually require GPUs or they consume a lot of CPU resource, which is costprohibitive for call centers. Notable works of cascading ASR and NLU include [6] and [7].\n4https://docs.aws.amazon.com/transcribe/latest/dg/pii-redaction.html\nar X\niv :2\n30 3.\n09 43\n8v 1\n[ cs\n.C L\n] 1\n6 M\nar 2\n02 3"
        },
        {
            "heading": "2. SYSTEM ARCHITECTURE",
            "text": "Trustera\u2019s goal is to (1) identify PII entities, (2) mask their mention in the audio and transcripts, and (3) capture the entity values for secure transaction processing. Figure 1 demonstrates secure credit card number redaction by Trustera. We describe an early lab version of our solution in the paragraphs below.\nIncoming stereo audio is decoded with two ASR decoders for the agent and caller. Live audio redactor (LAR) monitors the partial hypotheses and it triggers the NLU module as soon as a digit is detected in the hypothesis. NLU predicts the type of the entity and it informs LAR if the coming entity needs to be redacted. Then, LAR masks the audio till the end of entity is decided. The canonical value of the entity is extracted and sent to the payment system to complete the transaction."
        },
        {
            "heading": "2.1. ASR module",
            "text": "The ASR module is a finite state machine (FSM) consisting of a hybrid DNN-HMM BiLSTM acoustic model [8], a 4-gram language model [9] and a pronunciation dictionary.\nThe acoustic model consists of bidirectional long shortterm memory (BiLSTM) architecture with a projection layer trained in PyTorch to predict tied context-dependent triphone HMM states and optimized with cross-entropy with stochastic gradient descent. There are 108 phonemes and approximately 42,369 tied context-dependent HMM states modeled in the output layer. The network is trained on 11 million utterances sampled at 8kHz with a mu-law compression, about 40,000 hours of anonymized and hand-transcribed utterances, collected from a variety of customer care applications. The input features used for training the acoustic model are represented with 20ms frames of 81-dimensional log-spectrum features computed every 10ms, with no frame stacking. The network has four layers each with 600 memory cells and 300 projection units with a total number of about 41M model parameters. 8-bit quantization is applied to the acoustic model weight matrices to speed up the math-intensive operations of LSTM inference. Post training quantization is done using dynamic range mapping without calibration and results in a negligible loss in accuracy.\nThe language model (LM) is an interpolation of three 4- gram Katz back-off models [9]: (1) trained on Fisher/ Switchboard [10] LM training data (\u224820M words); (2) trained on in-domain human transcribed data of (\u22487M words) and (3)\ntrained on \u224810,000 calls automatically transcribed using an offline/accurate end-to-end pre-trained ASR [11] (\u224810M words). The interpolation weights are tuned to minimize the perplexity of the resulting LM on a separate validation set. The vocabulary is limited to the top 35K most frequent words with hand-crafted pronunciation for common words and grapheme-to-phoneme generated pronunciation for the rest of the vocabulary.\nThe stereo calls are decoded using two independent FSM decoders [12]. All the ASR decoder parameters are tuned to reduce the computation costs and increase accuracy. However, the trade-off between cost and the increased WER still favors CPUs. Since the redaction must happen in real-time, the audio stream is decoded as soon as it reaches Trustera, and the partial ASR hypotheses are passed to the NLU and LAR modules for PII redaction."
        },
        {
            "heading": "2.2. NLU module",
            "text": "The NLU model is a logistic regression classifier with a liblinear solver and l2-norm penalty optimizer, implemented using sklearn [13]. The model is trained to detect the type of entity as soon as a digit word is recognized by the ASR decoder. The feature vector is formed by the recognized digit followed by 20 previous tokens in the context. The context tokens are from both caller and agent channels merged. We use 3-gram features to prepare the feature vector and concatenate a dialog state vector with binary features representing conversation time, previously detected entities, and transaction history.\nWe augment the training set using uncertainty-aware selftraining [14]. To do so, a teacher model is trained using the available supervised data and then applied to several batches of unlabeled data. Then, a combination of easy and hard samples are used to augment the training set."
        },
        {
            "heading": "2.3. LAR module",
            "text": "LAR monitors the partial hypotheses generated by the ASR decoders from agent and caller channels. As soon as a digit is recognized in the partial hypothesis, LAR triggers the NLU module to predict the type of that entity. If the predicted entity is a sensitive one to be redacted, then LAR masks the audio using a beep sound. Audio masking continues until the end of the entity is decided. The end of the entity is a decided if the caller utters more than two non-digit words or silence of more than 3 seconds happens. When boundary and type of the entity are detected, then LAR extracts the value of that entity and send it to the payment system. Word to digit normalization is done by the LAR module based on well-defined grammars. We avoid using DNN-based NLP models to normalize the entities simply for the sake of reducing CPU usage."
        },
        {
            "heading": "2.4. Voice activity detection (VAD)",
            "text": "Voice activity detection in Trustera is a two-stage process, where frames are marked as being speech or silence based on an energy-adaptive VAD method [15], and the classification is then smoothed to determine the actual speech endpoints. Only frames classified as being speech are then sent to the decoder engine. Each channel decoder pipeline has its own VAD and end-pointer. We explored the alternative of detecting the voice activity before the audio is sent to each engine. Even though the idea of detecting the active channel and only decoding audio from that channel seems appealing, in practice it did not reduce CPU usage, and it reduced word accuracy, since speakers often overlap when speaking. Sending audio to only one channel at a time prevented the engine from fully recognizing what was spoken."
        },
        {
            "heading": "2.5. Architecture considerations",
            "text": "Trustera is optimized for real-time processing with minimal CPU usage, so that audio redaction is triggered as close to the actual speaking time and at the same time maximum number of concurrent calls can be handled on a single CPU. This is especially important for payment card security codes (e.g., CVV), which are only 3-4 digits in length. While advanced acoustic models (AMs) and end-to-end ASR models would increase transcription and entity classification accuracy, the large look-ahead requirements of the sequence models mean that the PII would be leaked before the audio frames were decoded. This is because (1) end-to-end models are typically built on neural network layers with mechanisms that perform better with large input contexts like self-attention which need future context; (2) they use aggressive down-sampling to compress the length of the input sequence which reduces the granularity of any alignment between outputs and inputs; and (3) they are trained with popular algorithms like CTC [16] or Transducer [17] loss which learns to align the audio input sequence and label output sequence automatically by marginalizing over all possible valid alignments, without a strict requirement that the alignment corresponds to the physical occurrence of speech."
        },
        {
            "heading": "3. EXPERIMENTS",
            "text": "In this section, we report the relative results with regard to the best possible systems available as open source. For ASR, we show the WER performance with regard to an offline pretrained end-to-end ASR [11]. For NLU, we report the performance with regard to an offline BiLSTM-CRF classifier [18]."
        },
        {
            "heading": "3.1. Data",
            "text": "Table 1 shows the statistics of the data set that is used to build the NLU modules. The training set consists of 11K real calls between agents and customers. The PII were annotated by our\ninternal labeling lab. We annotated the type of PII, the actual words that were spoken when the customer was uttering the PII, and a canonical value representing the PII. The canonical value reflects the intended PII value written in a standard format (e.g. \u201cexpiration date is February no no January twenty twenty five\u201d \u2192 \u201c01/25\u201d). The ASR models and the training data are described in Section 2.1."
        },
        {
            "heading": "3.2. Results",
            "text": "We first evaluate Trustera\u2019s ASR performance and then the ASR+NLU which shows the performance of the whole system for redacting the PII entities in real-time. All the inference experiments are conducted on CPUs. We run 8k (or 16k for the end-to-end ASR) samples/second to 30 jobs on servers with 12 core Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz with multi-threading turned off. Each server had 130GB of memory, but actual memory usage was lower."
        },
        {
            "heading": "3.2.1. ASR performance",
            "text": "The ASR performance is measured by word error rate (WER) for the overall conversation and for the entity segments. The CPU usage is reported as CPUvsAudio, often called the realtime factor, and it is computed as the ratio between the CPU time, as reported by the operating system, and the audio duration.\nAs the baseline ASR system, we use a publicly available off-the-shelf end-to-end ASR system from Nvidia-Nemo [11]. This is a non-autoregressive convolutional connectionist temporal classification (CTC) model [19]. We use a greedy search decoder without any additional language model to decode the audio samples. In Table 2, we refer to this system as E2E-pretrained-offline. All the ASR results in table 2 are computed relatively with regard to this baseline system.\nIn order to show the impact of the in-domain data, we fine-tune the end-to-end model using our training set in Table 1. We continue the tuning step for 100 epochs and we call it E2E-tuned-offline. As it can be seen in Table 2, the real-time factor doesn\u2019t change, however the overall WER improves by a factor of 20.5%. This improvement is larger on the agent side (29.0%) and smaller on the caller side (10.3%). Although the tuned model works well on the overall conversation, it fares poorly in recognizing the entity segments. Except for bank account and zip codes, where the WER of the tuned system reduces, all other entities got worse. We believe\nthis happens due to the lack of enough entity samples in the tuning set. We had to remove numeric PIIs from the tuning set, as we are not allowed to keep intact PII in the servers running the E2E engine.\nThen, we use the tuned end-to-end model and run the decoder in a an online mode (E2E-tuned-online). We experience salient reduction in CPU usage by a factor of 78.7%, but at the same time all WER and SER measures degrade dramatically compared to E2E-pretrained-offline.\nFinally we evaluate Trustera in an online mode. As mentioned in section 2.1, we use a hybrid BiLSTM acoustic model along with a 4-gram language model for the ASR module. As it can be seen, Trustera\u2019s online ASR decoder reduces the CPU consumption by 53.2%. Moreover, it improves the WER by 21.5%, 8.2% and 7.8% respectively for agent, caller and overall. It also outperforms the baseline system when recognizing all the entity segments. For example, all the routing numbers are recognized correctly with Trustera\u2019s ASR. Also WER/SER of the credit card numbers improved substantially. Unlike the end-to-end models, where the acoustic model is pretrained on generic and diverse (e.g. wideband audio, clean speech etc) data and then fine-tuned on the indomain sets, Trustera\u2019s acoustic model is trained from scratch on in-domain data and other data that are acoustically very similar to the call center audio (e.g. voicemail). Moreover, Trustera takes advantage of an in-domain language model that has been trained on a large amount of digit sequences.\nResults show that although this early lab system uses older ASR technology compared to the state-of-the-art end-to-end system, its performance is efficient in terms of CPU usage and accuracy. Note that usually the end-to-end systems need GPU which is not available in the call-center servers."
        },
        {
            "heading": "3.2.2. NLU performance",
            "text": "The second part of Table 2 shows the precision/recall difference between an oracle offline NLU and our online model used in Trustera. A prediction is counted as true positive if the begin, end and type of the entity is predicted correctly. The same ASR system is used to generate the hypotheses for the NLU modules. The offline NLU takes advantage of fully stable hypotheses whereas the online NLU uses unstable (partial) hypotheses only.\nAs the baseline, we use an offline BiLSTM-CRF model trained on the 11k calls (in Table 1) and it is applied to the stable ASR hypotheses with full access to the left and right context of the dialogue (BiLSTM-CRF-offline).\nTrustera\u2019s NLU uses a logistic regression model, due to the architecture constraints, and it has access to the left context only (Log-Reg-online). This model is also trained on 11k calls, but applied to the unstable hypotheses in order to mimic the real-life application. Considering CCNUM, as one of the most important entities, we observe that Trustera\u2019s NLU precision drops relatively by 24.3% compared to BiLSTM-CRF. We believe that drop is due to the BiLSTM-CRF model benefiting from backward information (i.e. observing a 16-digit credit card number). Recall is a more relevant measure of redaction performance. We observe that CCNUM recall drops by 11.6%, relatively. In the case of CVV, Trustera indeed outperforms BiLSTM-CRF by 3.7%.\nOur experiments show that there is a large gap between stable and unstable hypotheses when it comes to offline NLU versus online. Moreover, the right context in BiLSTM-CRF plays an important role in predicting the type of entity."
        },
        {
            "heading": "3.3. Analysis",
            "text": "Trustera may leak sensitive entities too. The cause of the leak usually comes from ASR errors, word-to-digit normalization, customer hesitations and agent interruptions. For example, the PII start detector will not fire if an entity\u2019s beginning digit is misrecognized as a non-digit token. Also the word-to-digit normalization may fail if the interpretation is ambiguous like, \u201cone hundred twenty\u2192 120 or 100-20\u201d or \u201cdouble oh seven\u201d."
        },
        {
            "heading": "4. CONCLUSION",
            "text": "Trustera, a real-time dialog mediation system that intercepts and redacts sensitive personal information, was presented. It is the first system that monitors and redacts the sensitive information from entire calls in real-time so that the agent cannot hear customer PII. Trustera redacts and captures payment information without interrupting the natural conversation, and sends the information to the payment system. Trustera significantly reduces the risk of sensitive information leaks, and is highly efficient in terms of CPU usage and latency."
        },
        {
            "heading": "5. REFERENCES",
            "text": "[1] Gartner, \u201cCOVID-19 crisis accelerates rise of virtual call centers,\u201d 2020.\n[2] Amber Stubbs, Christopher Kotfila, and O\u0308zlem Uzuner, \u201cAutomated systems for the de-identification of longitudinal clinical narratives: Overview of 2014 i2b2/uthealth shared task track 1,\u201d Journal of biomedical informatics, vol. 58, pp. S11\u2013S19, 2015.\n[3] Ishna Neamatullah, Margaret M Douglass, H Lehman Li-wei, Andrew Reisner, Mauricio Villarroel, William J Long, Peter Szolovits, George B Moody, Roger G Mark, and Gari D Clifford, \u201cAutomated de-identification of free-text medical records,\u201d BMC medical informatics and decision making, vol. 8, no. 1, pp. 32, 2008.\n[4] Ido Cohn, Itay Laish, Genady Beryozkin, Gang Li, Izhak Shafran, Idan Szpektor, Tzvika Hartman, Avinatan Hassidim, and Yossi Matias, \u201cAudio deidentification: A new entity recognition task,\u201d arXiv preprint arXiv:1903.07037, 2019.\n[5] Sahar Ghannay, Antoine Caubriere, Yannick Esteve, Antoine Laurent, and Emmanuel Morin, \u201cEnd-to-end named entity extraction from speech,\u201d arXiv preprint arXiv:1805.12045, 2018.\n[6] Mohamed Hatmi, Christine Jacquin, Emmanuel Morin, and Sylvain Meignier, \u201cNamed entity recognition in speech transcripts following an extended taxonomy,\u201d in First Workshop on Speech, Language and Audio in Multimedia, 2013.\n[7] Katsuhito Sudoh, Hajime Tsukada, and Hideki Isozaki, \u201cIncorporating speech recognition confidence into discriminative named entity recognition of speech data,\u201d in Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, 2006, pp. 617\u2013624.\n[8] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Process. Mag., vol. 29, no. 6, pp. 82\u201397, 2012.\n[9] Slava Katz, \u201cEstimation of probabilities from sparse data for the language model component of a speech recognizer,\u201d IEEE Trans. Acoust., Speech, Signal Process., vol. 35, no. 3, pp. 400\u2013401, 1987.\n[10] Christopher Cieri, David Miller, and Kevin Walker, \u201cThe Fisher corpus: A resource for the next generations of speech-to-text.,\u201d in LREC, 2004, vol. 4, pp. 69\u201371.\n[11] Oleksii Kuchaiev, Jason Li, Huyen Nguyen, Oleksii Hrinchuk, Ryan Leary, Boris Ginsburg, Samuel Kriman, Stanislav Beliaev, Vitaly Lavrukhin, Jack Cook, et al., \u201cNemo: a toolkit for building AI applications using neural modules,\u201d arXiv preprint arXiv:1909.09577, 2019.\n[12] Mehryar Mohri, Fernando Pereira, and Michael Riley, \u201cSpeech recognition with weighted finite-state transducers,\u201d in Springer Handbook of Speech Processing, pp. 559\u2013584. Springer, 2008.\n[13] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, \u201cScikit-learn: Machine learning in Python,\u201d Journal of Machine Learning Research, vol. 12, pp. 2825\u2013 2830, 2011.\n[14] Subhabrata Mukherjee and Ahmed Hassan Awadallah, \u201cUncertainty-aware self-training for text classification with few labels,\u201d arXiv preprint arXiv:2006.15315, 2020.\n[15] David Malah, \u201cA novel approach for VAD adaptation in nonstationary noise environments,\u201d Tech. Rep. HA6153000-981015-09TM, AT&T Labs-Research, 1996.\n[16] Alex Graves, Santiago Ferna\u0301ndez, Faustino Gomez, and Ju\u0308rgen Schmidhuber, \u201cConnectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proceedings of the 23rd international conference on Machine learning, 2006, pp. 369\u2013376.\n[17] Alex Graves, \u201cSequence transduction with recurrent neural networks,\u201d arXiv preprint arXiv:1211.3711, 2012.\n[18] Daniel Pressel, Sagnik Ray Choudhury, Brian Lester, Yanjie Zhao, and Matt Barta, \u201cBaseline: A library for rapid modeling, experimentation and development of deep learning algorithms targeting NLP,\u201d in Proceedings of Workshop for NLP Open Source Software (NLPOSS). 2018, pp. 34\u201340, Association for Computational Linguistics.\n[19] Somshubra Majumdar, Jagadeesh Balam, Oleksii Hrinchuk, Vitaly Lavrukhin, Vahid Noroozi, and Boris Ginsburg, \u201cCitrinet: Closing the gap between nonautoregressive and autoregressive end-to-end models for automatic speech recognition,\u201d arXiv preprint arXiv:2104.01721, 2021."
        }
    ],
    "title": "TRUSTERA: A LIVE CONVERSATION REDACTION SYSTEM",
    "year": 2023
}