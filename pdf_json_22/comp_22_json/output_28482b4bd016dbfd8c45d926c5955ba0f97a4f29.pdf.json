{
    "abstractText": "We present a multidimensional deep learning implementation of a stochastic branching algorithm for the numerical solution of fully nonlinear PDEs. This approach is designed to tackle functional nonlinearities involving gradient terms of any orders, by combining the use of neural networks with a Monte Carlo branching algorithm. In comparison with other deep learning PDE solvers, it also allows us to check the consistency of the learned neural network function. Numerical experiments presented show that this algorithm can outperform deep learning approaches based on backward stochastic differential equations or the Galerkin method, and provide solution estimates that are not obtained by those methods in fully nonlinear examples.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiang Yu Nguwi"
        },
        {
            "affiliations": [],
            "name": "Guillaume Penent"
        },
        {
            "affiliations": [],
            "name": "Nicolas Privault"
        }
    ],
    "id": "SP:aa92cbf5e1350340cd6dd366d5a5c6ce2ad30426",
    "references": [
        {
            "authors": [
                "C. Beck",
                "S. Becker",
                "P. Cheridito",
                "A. Jentzen",
                "A. Neufeld"
            ],
            "title": "Deep splitting method for parabolic PDEs",
            "venue": "SIAM J. Sci. Comput.,",
            "year": 2021
        },
        {
            "authors": [
                "S. Becker",
                "R. Braunwarth",
                "M. Hutzenthaler",
                "A. Jentzen",
                "Ph. von Wurstemberger"
            ],
            "title": "Numerical simulations for full history recursive multilevel Picard approximations for systems of high-dimensional partial differential equations",
            "venue": "Commun. Comput. Phys.,",
            "year": 2020
        },
        {
            "authors": [
                "C. Beck",
                "W. E",
                "A. Jentzen"
            ],
            "title": "Machine learning approximation algorithms for highdimensional fully nonlinear partial differential equations and second-order backward stochastic differential equations",
            "venue": "J. Nonlinear Sci.,",
            "year": 2019
        },
        {
            "authors": [
                "J.C. Butcher"
            ],
            "title": "Coefficients for the study of Runge-Kutta integration processes",
            "venue": "J. Austral. Math. Soc.,",
            "year": 1963
        },
        {
            "authors": [
                "J.F. Chassagneux"
            ],
            "title": "Linear multi-step schemes for BSDEs",
            "venue": "Preprint arXiv:1306.5548v1,",
            "year": 2013
        },
        {
            "authors": [
                "J.A.S. Chakraborty"
            ],
            "title": "L\u00f3pez-Mimbela. Nonexplosion of a class of semilinear equations via branching particle representations",
            "venue": "Advances in Appl. Probability,",
            "year": 2008
        },
        {
            "authors": [
                "T.H.G.M. Constantine"
            ],
            "title": "Savits. A multivariate Faa di Bruno formula with applications",
            "venue": "Trans. Amer. Math. Soc.,",
            "year": 1996
        },
        {
            "authors": [
                "P. Cheridito",
                "H.M. Soner",
                "N. Touzi",
                "N. Victoir"
            ],
            "title": "Second-order backward stochastic differential equations and fully nonlinear parabolic PDEs",
            "venue": "Comm. Pure Appl. Math.,",
            "year": 2007
        },
        {
            "authors": [
                "P. Deuflhard",
                "F. Bornemann"
            ],
            "title": "Scientific Computing with Ordinary Differential Equations, volume 42 of Texts in Applied Mathematics",
            "year": 2002
        },
        {
            "authors": [
                "W. E",
                "M. Hutzenthaler",
                "A. Jentzen",
                "T. Kruse"
            ],
            "title": "On multilevel Picard numerical approximations for high-dimensional nonlinear parabolic partial differential equations and highdimensional nonlinear backward stochastic differential equations",
            "venue": "Journal of Scientific Computing,",
            "year": 2019
        },
        {
            "authors": [
                "W. E",
                "M. Hutzenthaler",
                "A. Jentzen",
                "T. Kruse"
            ],
            "title": "Multilevel Picard iterations for solving smooth semilinear parabolic heat equations",
            "venue": "Partial Differential Equations and Applications,",
            "year": 2021
        },
        {
            "authors": [
                "A. Fahim",
                "N. Touzi",
                "X. Warin"
            ],
            "title": "A probabilistic numerical method for fully nonlinear parabolic PDEs",
            "venue": "Ann. Appl. Probab.,",
            "year": 2011
        },
        {
            "authors": [
                "W. Guo",
                "J. Zhang",
                "J. Zhuo"
            ],
            "title": "A monotone scheme for high-dimensional fully nonlinear PDEs",
            "venue": "Ann. Appl. Probab.,",
            "year": 2015
        },
        {
            "authors": [
                "M. Hou",
                "H. Fu",
                "Z. Hu",
                "J. Wang",
                "Y. Chen",
                "Y. Yang"
            ],
            "title": "Numerical solving of generalized Black-Scholes differential equation using deep learning based on blocked residual connection",
            "venue": "Digital Signal Processing,",
            "year": 2022
        },
        {
            "authors": [
                "J. Han",
                "A. Jentzen",
                "W. E"
            ],
            "title": "Deep learning-based numerical methods for highdimensional parabolic partial differential equations and backward stochastic differential equations",
            "venue": "Preprint arXiv:1706.04702,",
            "year": 2017
        },
        {
            "authors": [
                "J. Han",
                "A. Jentzen",
                "W. E"
            ],
            "title": "Solving high-dimensional partial differential equations using deep learning",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2018
        },
        {
            "authors": [
                "M. Hutzenthaler",
                "A. Jentzen",
                "T. Kruse"
            ],
            "title": "Overcoming the curse of dimensionality in the numerical approximation of parabolic partial differential equations with gradient-dependent nonlinearities",
            "venue": "Found. Comput. Math.,",
            "year": 2022
        },
        {
            "authors": [
                "M. Hutzenthaler",
                "A. Jentzen",
                "T. Kruse",
                "T.A. Nguyen"
            ],
            "title": "Multilevel Picard approximations for high-dimensional semilinear second-order PDEs with Lipschitz nonlinearities",
            "year": 2009
        },
        {
            "authors": [
                "P. Henry-Labord\u00e8re"
            ],
            "title": "Counterparty risk valuation: a marked branching diffusion approach",
            "venue": "Preprint arXiv:1203.2369,",
            "year": 2012
        },
        {
            "authors": [
                "P. Henry-Labord\u00e8re",
                "N. Oudjane",
                "X. Tan",
                "N. Touzi",
                "X. Warin"
            ],
            "title": "Branching diffusion representation of semilinear PDEs and Monte Carlo approximation",
            "venue": "Ann. Inst. H. Poincare\u0301 Probab. Statist.,",
            "year": 2019
        },
        {
            "authors": [
                "P. Henry-Labord\u00e8re",
                "N. Touzi"
            ],
            "title": "Branching diffusion representation for nonlinear Cauchy problems and Monte Carlo approximation",
            "venue": "Ann. Appl. Probab.,",
            "year": 2021
        },
        {
            "authors": [
                "E. Hairer",
                "C. Lubich",
                "G. Wanner"
            ],
            "title": "Geometric numerical integration, volume 31 of Springer Series in Computational Mathematics",
            "year": 2006
        },
        {
            "authors": [
                "S. Huang",
                "G. Liang",
                "T. Zariphopoulou"
            ],
            "title": "An approximation scheme for semilinear parabolic PDEs with convex and coercive Hamiltonians",
            "venue": "SIAM J. Control Optim.,",
            "year": 2020
        },
        {
            "authors": [
                "K. Hornik"
            ],
            "title": "Approximation capabilities of multilayer feedforward networks",
            "venue": "Neural networks,",
            "year": 1991
        },
        {
            "authors": [
                "C. Hur\u00e9",
                "H. Pham",
                "X. Warin"
            ],
            "title": "Deep backward schemes for high-dimensional nonlinear PDEs",
            "venue": "Math. Comp.,",
            "year": 2020
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "N. Ikeda",
                "M. Nagasawa",
                "S. Watanabe"
            ],
            "title": "Branching Markov processes I, II, III",
            "venue": "J. Math. Kyoto Univ.,",
            "year": 1969
        },
        {
            "authors": [
                "S. Ioffe",
                "C. Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "In Proceedings of the 32nd International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "Preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "N.V. Krylov"
            ],
            "title": "Boundedly nonhomogeneous elliptic and parabolic equations",
            "venue": "Math. USSR, Izv.,",
            "year": 1983
        },
        {
            "authors": [
                "W. Lefebvre",
                "G. Loeper",
                "H. Pham"
            ],
            "title": "Differential learning methods for solving fully nonlinear PDEs",
            "venue": "Digital Finance,",
            "year": 2023
        },
        {
            "authors": [
                "J.A. L\u00f3pez-Mimbela"
            ],
            "title": "A probabilistic approach to existence of global solutions of a system of nonlinear differential equations",
            "venue": "In Fourth Symposium on Probability Theory and Stochastic Processes (Spanish) (Guanajuato,",
            "year": 1996
        },
        {
            "authors": [
                "L. Lyu",
                "Z. Zhang",
                "M. Chen",
                "J. Chen"
            ],
            "title": "MIM: A deep mixed residual method for solving high-order partial differential equations",
            "venue": "Journal of Computational Physics,",
            "year": 2022
        },
        {
            "authors": [
                "H.P. McKean"
            ],
            "title": "Application of Brownian motion to the equation of Kolmogorov-PetrovskiiPiskunov",
            "venue": "Comm. Pure Appl. Math.,",
            "year": 1975
        },
        {
            "authors": [
                "R.I. McLachlan",
                "K. Modin",
                "H. Munthe-Kaas",
                "O. Verdier"
            ],
            "title": "Butcher series: a story of rooted trees and numerical methods for evolution equations",
            "venue": "Asia Pac. Math. Newsl.,",
            "year": 2017
        },
        {
            "authors": [
                "J.Y. Nguwi",
                "G. Penent",
                "N. Privault"
            ],
            "title": "A fully nonlinear Feynman-Kac formula with derivatives of arbitrary orders",
            "venue": "Journal of Evolution Equations, 23:Paper No. 22,",
            "year": 2023
        },
        {
            "authors": [
                "S. Peng"
            ],
            "title": "Probabilistic interpretation for systems of quasilinear parabolic partial differential equations",
            "venue": "Stochastics Stochastics Rep.,",
            "year": 1991
        },
        {
            "authors": [
                "\u00c9. Pardoux",
                "S. Peng"
            ],
            "title": "Backward stochastic differential equations and quasilinear parabolic partial differential equations. In Stochastic partial differential equations and their applications (Charlotte",
            "venue": "NC, 1991),",
            "year": 1992
        },
        {
            "authors": [
                "G. Penent",
                "N. Privault"
            ],
            "title": "Numerical evaluation of ODE solutions by Monte Carlo enumeration of Butcher series",
            "venue": "BIT Numerical Mathematics,",
            "year": 2022
        },
        {
            "authors": [
                "H. Pham",
                "X. Warin",
                "M. Germain"
            ],
            "title": "Neural networks-based backward scheme for fully nonlinear PDEs",
            "venue": "Partial Differ. Equ. Appl., 2(1):Paper No. 16,",
            "year": 2021
        },
        {
            "authors": [
                "A.V. Skorokhod"
            ],
            "title": "Branching diffusion processes",
            "venue": "Teor. Verojatnost. i. Primenen.,",
            "year": 1964
        },
        {
            "authors": [
                "J. Sirignano",
                "K. Spiliopoulos"
            ],
            "title": "DGM: A deep learning algorithm for solving partial differential equations",
            "venue": "Journal of Computational Physics,",
            "year": 2018
        },
        {
            "authors": [
                "H.M. Soner",
                "N. Touzi",
                "J. Zhang"
            ],
            "title": "Wellposedness of second order backward SDEs",
            "venue": "Probab. Theory Related Fields,",
            "year": 2012
        },
        {
            "authors": [
                "X. Tan"
            ],
            "title": "A splitting method for fully nonlinear degenerate parabolic PDEs",
            "venue": "Electron. J. Probab., 18:no. 15,",
            "year": 2013
        }
    ],
    "sections": [
        {
            "text": "Keywords: Fully nonlinear PDE, deep neural network, deep Galerkin, deep BSDE, branching process, random tree, Monte Carlo method. Mathematics Subject Classification (2020): 35G20, 35K55, 35K58, 60H30, 60J85, 65C05."
        },
        {
            "heading": "1 Introduction",
            "text": "This paper is concerned with the numerical solution of fully nonlinear partial differential equations (PDEs) of the form\u2202tu(t, x) + 1 2 \u2206u(t, x) + f ( \u2202\u03bb1u(t, x), . . . , \u2202\u03bbnu(t, x) ) = 0,\nu(T, x) = \u03d5(x), (t, x) = (t, x1, . . . , xd) \u2208 [0, T ]\u00d7 Rd, (1.1)\n\u2217nguw0003@e.ntu.edu.sg \u2020pene0001@e.ntu.edu.sg \u2021nprivault@ntu.edu.sg\nar X\niv :2\n20 3.\n03 23\n4v 2\n[ m\nat h.\nd \u2265 1, where \u2206 = d\u2211\ni=1\n\u22022/\u2202x2i is the standard d-dimensional Laplacian, \u2202tu(t, x) = \u2202u(t, x)/\u2202t,\nand f is a smooth function of the derivatives\n\u2202\u03bbiu(t, x) = \u2202\u03bb\ni 1\n\u2202x1 \u00b7 \u00b7 \u00b7 \u2202\n\u03bbid\n\u2202xd u(t, x1, . . . , xd), (x1, . . . , xd) \u2208 Rd,\n\u03bbi = (\u03bbi1, . . . , \u03bb i d) \u2208 Nd, i = 1, . . . , n. As is well known, standard numerical schemes for solving (1.1) by e.g. finite differences or finite elements suffer from the curse of dimensionality as their computational cost grows exponentially with the dimension d.\nThe deep Galerkin method (DGM) has been developed in [SS18] for the numerical solu-\ntion of (1.1) by training a neural network function v(t, x) using the loss function( \u2202tv(t, x) + 1\n2 \u2206v(t, x) + f\n( \u2202\u03bb1v(t, x), . . . , \u2202\u03bbnv(t, x) ))2 + (v(T, x)\u2212 \u03d5(x))2 . (1.2)\nSee [LZCC22] for recent improvements of the DGM using deep mixed residuals (MIM) with numerical applications to linear PDEs, and [HFH+22] for the blocked residual connection method (DLBR) applied to a linear (generalized) Black-Scholes equation.\nOn the other hand, probabilistic schemes provide a promising direction to overcome the curse of dimensionality. For example, when f(u(t, x)) = ru(t, x) does not involve any derivative of u, the solution of the PDE\u2202tu(t, x) + 1 2 \u2206u(t, x) + ru(t, x) = 0,\nu(T, x) = \u03d5(x), (t, x) = (t, x1, . . . , xd) \u2208 [0, T ]\u00d7 Rd,\nadmits the probabilistic representation\nu(0, x) = erTE[\u03d5(x+WT )],\nwhere (Wt)t\u22650 is a standard Brownian motion. This method can be implemented on a bounded domain D \u2282 Rd based on the universal approximation theorem and the L2 minimality property\nu(0, \u00b7) = inf v E [( erT\u03d5(X +WT )\u2212 v(X) )2] ,\nwhere X is a uniform random vector on D and the infimum in v is taken over a neural functional space.\nProbabilistic representations for the solutions of first order nonlinear PDEs can also be obtained by representing u(t, x) as u(t, x) = Y t,xt , (t, x) \u2208 [0, T ]\u00d7R, where (Y t,xs )t\u2264s\u2264T is the\nsolution of a backward stochastic differential equation (BSDE), see [Pen91], [PP92]. The BSDE method has been implemented in [HJE18] using a deep learning algorithm in the case where f depends on the first order derivative, i.e. \u03bbi1 + \u00b7 \u00b7 \u00b7 + \u03bbid \u2264 1, 1 \u2264 i \u2264 n, see also [HPW20] for recent improvements. The BSDE method extends to second order fully nonlinear PDEs by the use of second order backward stochastic differential equations, see e.g. [CSTV07], [STZ12], and [HJE17, BEJ19], and [PWG21], [LLP23], for deep learning implementations. However, this approach does not apply to nonlinearities in gradients of order strictly greater than two, see Examples e) and f) below.\nNumerical solutions of semilinear PDEs have also been obtained by the multilevel Picard method (MLP), see [EHJK19, HJKN20, EHJK21, HJK22], with numerical experiments provided in [BBH+20]. However, this approach is currently restricted to first order gradient nonlinearities, similarly to the deep splitting algorithm of [BBC+21]. In addition, the main use of the MLP and deep splitting methods is to provide pointwise estimates, whereas this paper focuses on functional estimation of solutions using neural networks.\nIn this context, the use of stochastic branching diffusion mechanisms [Sko64], [INW69], represents an alternative to the DGM and BSDE methods, see [McK75] for an application to the Kolmogorov-Petrovskii-Piskunov (KPP) equation, [CLM08] for existence of solutions of parabolic PDEs with power series nonlinearities, [HL12] for more general PDEs with polynomial nonlinearities, and [HLT21] for an application to semilinear and higher-order hyperbolic PDEs. This approach has been applied in e.g. [LM96], [HLOT+19] to polynomial gradient nonlinearities, see also [FTW11], [Tan13], [GZZ15], [HLZ20] for finite difference schemes combined with Monte Carlo estimation for fully nonlinear PDEs with gradients of order up to 2.\nExtending such approaches to nonlinearities involving gradients of order greater than two involves technical difficulties linked to the integrability of the Malliavin-type weights used in repeated integration by parts argument, see page 199 of [HLOT+19]. Such higher order nonlinearities are also not covered by multilevel Picard [BBH+20] and deep splitting [BBC+21] methods, or by BSDE methods [HJE18, BEJ19], which are limited to first and second order gradients, respectively.\nIn [NPP23], a stochastic branching method that carries information on (functional) nonlinearities along a random tree has been introduced, with the aim of providing Monte Carlo\nschemes for the numerical solution of fully nonlinear PDEs with gradients of arbitrary orders.\nIn this paper, we present a deep learning implementation of the method of [NPP23] using Monte Carlo sampling, the law of large numbers, and the universal approximation theorem. Our approach to the numerical solution of the PDE (1.1) is based on the following steps:\ni) The solution of PDE (1.1) is written as the conditional expectation of a functional of a\nrandom coding tree via the fully nonlinear Feynman-Kac formula Theorem 1 in [NPP23], see (2.2) below.\nii) The conditional expectation is approximated by a neural network function through the\nL2-minimality property and the universal approximation theorem.\nWe start by testing our method on the Allen-Cahn equation (4.1), for which we report a performance comparable to that of the deep BSDE and deep Galerkin methods, see Figure 2. This is followed by an example (4.2) involving an exponential nonlinearity without gradient term, in which our method outperforms the deep Galerkin method and performs comparably to deep BSDE method in dimension d = 5, see Figure 3. We also consider a multidimensional Burgers equation (4.3) for which the deep branching method is more stable than the deep Galerkin and deep BSDE methods in dimension d = 15, see Figure 5. Next, we consider a Merton problem (4.6) to which the deep Galerkin method does not apply since its loss function involves a division by the second derivative of the neural network function. We also note that the deep branching method overperforms the deep BSDE method in this case, see Figure 6. Finally, we consider higher order functional gradient nonlinearities in Equations (4.7) and (4.8), to which the deep BSDE, multilevel Picard and deep splitting methods do not apply. In those cases, our method also outperforms the deep Galerkin method in both dimensions d = 1 and d = 5, see Figures 8 and 9.\nWe also note that since the deep branching method is based on a direct Monte Carlo estimation, it allows for checking the consistency between the Monte Carlo samples and the learned neural network function, which is not possible with the deep Galerkin method and deep BSDE methods, see Figure 7.\nOur algorithm, similarly to other branching diffusion methods, suffers from a time explosion phenomenon due to the use of a branching process. Nevertheless, our method can perform better than the deep Galerkin and deep BSDE methods in small time and in higher dimensions, see Figure 2 for the Allen-Cahn equation and Figure 5 for the Burgers equation.\nOther approaches to the solution of evolution equations by carrying information on nonlinearities along trees include [But63], see also Chapters 4-6 of [DB02] and [MMMKV17] for ordinary differential equations (ODEs), with applications ranging from geometric numerical integration to stochastic differential equations, see for instance [HLW06] and references therein. On the other hand, the stochastic branching method does not use series truncations and it can be used to estimate an infinite series, see [PP22] for an application to ODEs.\nThis paper is organized as follows. The extension of the fully nonlinear Feynman-Kac formula of [NPP23] to a multidimensional setting is presented in Section 2, and the deep learning algorithm is described in Section 3. Section 4 presents numerical examples in which our method can outperform the deep BSDE and deep Galerkin methods.\nThe Python codes and numerical experiments run in this paper are available at\nhttps://github.com/nguwijy/deep_branching.\nNotation\nWe denote by N = {0, 1, 2, . . . } the set of natural numbers, and let C0,\u221e([0, T ]\u00d7Rd) be the set of functions u : [0, T ] \u00d7 Rd \u2192 R such that u(t, x) is continuous in the variable t and infinitely x-differentiable. For a vector x = (x1, . . . , xd) \u22a4 \u2208 Rd, we let |x| =\nd\u2211 i=1\n|xi|, and let 1p be the vector of 1 at position p and 0 elsewhere. We also consider the linear order \u227a on Rd such that (k1, . . . , kd) = k \u227a l = (l1, . . . , ld) if one of the following holds:\ni) |k| < |l|;\nii) |k| = |l| and k1 < l1;\niii) |k| = |l|, k1 = l1, . . . ki = li, and ki+1 < li+1 for some 1 \u2264 i < d."
        },
        {
            "heading": "2 Fully nonlinear Feynman-Kac formula",
            "text": "In this section we extend the construction of [NPP23] to the case of multidimensional PDEs of the form \u2202tu(t, x) + 1 2 \u2206u(t, x) + f ( \u2202\u03bb1u(t, x), . . . , \u2202\u03bbnu(t, x) ) = 0,\nu(T, x) = \u03d5(x), (t, x) = (t, x1, . . . , xd) \u2208 [0, T ]\u00d7 Rd, (2.1)\nwhere \u03bbi = (\u03bbi1, . . . , \u03bb i d) \u2208 Nd, i = 1, . . . , n, with the integral formulation\nu(t, x) = \u222b Rd \u03c6(T \u2212 t, y\u2212 x)\u03d5(y)dy+ \u222b T t \u222b Rd \u03c6(s\u2212 t, y\u2212 x)f ( \u2202\u03bb1u(t, y), . . . , \u2202\u03bbnu(t, x) ) dyds, where \u03c6(t, x) := e\u2212x 2/(2t)/ \u221a 2\u03c0t, and (t, x) \u2208 [0, T ] \u00d7 Rd. We refer to e.g. Theorem 1.1 in [Kry83] for sufficient conditions for existence and uniqueness of smooth solutions to such fully nonlinear PDEs in the second order case. Our fully nonlinear Feynman-Kac formula [NPP23] relies on the construction of a branching coding tree, based on the definition of a set C of codes and its associated mechanismM. In what follows, we use the notation\n(a1, . . . , an) \u222a (b1, . . . , bm) := (a1, . . . , an, b1, . . . , bm)\nfor any sequences (a1, . . . , an), (b1, . . . , bm) or real numbers. In addition, for any function g : Rn \u2192 R, we let g\u2217 be the operator mapping C0,\u221e([0, T ] \u00d7 Rd) to C0,\u221e([0, T ] \u00d7 Rd) and defined by\ng\u2217(u)(t, x) := g ( \u2202\u03bb1u(t, x), . . . , \u2202\u03bbnu(t, x) ) , (t, x) \u2208 [0, T ]\u00d7 Rd.\nIn the sequel, we also let \u2202\u03bb := \u2202 \u03bb1 z1 \u00b7 \u00b7 \u00b7 \u2202\u03bbnzn , and \u2202\u00b5 := \u2202 \u00b51 x1 \u00b7 \u00b7 \u00b7 \u2202\u00b5dxd , \u03bb = (\u03bb1, . . . , \u03bbn) \u2208 N n, \u00b5 = (\u00b51, . . . , \u00b5d) \u2208 Nd. Definition 2.1 We let C denote the set of operators from C0,\u221e([0, T ]\u00d7Rd) to C0,\u221e([0, T ]\u00d7 Rd), called codes, and defined as\nC := { Id, (a\u2202\u03bbf) \u2217, \u2202\u00b5, : \u03bb \u2208 Nn, \u00b5 \u2208 Nd, a \u2208 R } ,\nwhere Id denotes the identity on C0,\u221e([0, T ]\u00d7 Rd).\nFor example, for \u03bd \u2208 Nn, \u00b5 \u2208 Nd, a \u2208 R and k \u2208 N we have\nc(u)(T, x) =  \u03d5(x), if c = Id, a\u2202\u03bdf ( \u2202\u03bb1\u03d5(x), . . . , \u2202\u03bbm\u03d5(x) ) , if c = (a\u2202\u03bdf) \u2217,\n\u2202\u00b5\u03d5(x), if c = \u2202\u00b5.\nThe mechanismM is then defined as a mapping on C byM(Id) := {f \u2217}, and\nM(g\u2217) := \u22c3\n1\u2264p\u2264n \u03bbp=0\n{( f \u2217, (\u22021pg) \u2217)}\n\u22c3 1\u2264p\u2264n, 1\u2264s\u2264|\u03bbp| 1\u2264\u03bd1+\u00b7\u00b7\u00b7+\u03bdn\u2264|\u03bbp|\n1\u2264|k1|,...,|ks|, 0\u227al1\u227a\u00b7\u00b7\u00b7\u227als\nki1+\u00b7\u00b7\u00b7+k i s=\u03bdi, i=1,...,n\n|k1|l1j+\u00b7\u00b7\u00b7+|ks|l s j=\u03bb p j , j=1,...,d\n (\u22021pg)\u2217,\nd\u220f i=1 \u03bbpi !(\u2202\u03bdf) \u2217\n\u220f 1\u2264r\u2264s 1\u2264q\u2264n kqr ! (lr1! \u00b7 \u00b7 \u00b7 lrd!) kqr  \u22c3 1\u2264r\u2264s 1\u2264q\u2264n ( \u2202lr+\u03bbq , . . . , \u2202lr+\u03bbq\ufe38 \ufe37\ufe37 \ufe38 kqr times ) \n\u22c3 \u22c3 i,j=1,...,n k=1,...,d {( \u22121 2 (\u22021i+1jg) \u2217, \u2202\u03bbi+1k , \u2202\u03bbj+1k )} , g\u2217 \u2208 C,\nand\nM(\u2202\u00b5)\n:= \u22c3\n1\u2264s\u2264|\u00b5|, 1\u2264\u03bd1+\u00b7\u00b7\u00b7+\u03bdn\u2264|\u00b5| 1\u2264|k1|,...,|ks|, 0\u227al1\u227a\u00b7\u00b7\u00b7\u227als\nki1+\u00b7\u00b7\u00b7+k i s=\u03bdi, i=1,...,n\n|k1|l1j+\u00b7\u00b7\u00b7+|ks|l s j=\u00b5j , j=1,...,d\n \nd\u220f i=1\n\u00b5i!\u220f 1\u2264r\u2264s 1\u2264q\u2264n kqr ! (lr1! \u00b7 \u00b7 \u00b7 lrd!) kqr (\u2202\u03bdf) \u2217  \u22c3 1\u2264r\u2264s 1\u2264q\u2264n ( \u2202lr+\u03bbqu, . . . , \u2202lr+\u03bbqu\ufe38 \ufe37\ufe37 \ufe38 kqr times )  ,\n\u00b5 \u2208 Nd. Given \u03c1 : R+ \u2192 (0,\u221e) a probability density function (PDF) on R+ with tail distribution function F and N (0, \u03c32) a d-dimensional independent centered normal distribution with variance \u03c32, we consider the functional H(t, x, c) constructed in Algorithm 1 along a random coded tree started at (t, x, c) \u2208 [0, T ]\u00d7 Rd \u00d7 C, using independent random samples on a probability space \u2126.\nAlgorithm 1 Coding tree algorithm TREE(t, x, c) Input: t \u2208 [0, T ], x \u2208 Rd, c \u2208 C Output: H(t, x, c) \u2208 R H(t, x, c)\u2190 1 \u03c4 \u2190 a random variable drawn from the distribution of \u03c1 if t+ \u03c4 > T then\nW \u2190 a random vector drawn from N (0, T \u2212 t) H(t, x, c)\u2190 H(t, x, c)\u00d7 c(u)(T, x+W )/F (T \u2212 t)\nelse q \u2190 the size of the mechanism setM(c) I \u2190 a random element drawn uniformly fromM(c) H(t, x, c)\u2190 H(t, x, c)\u00d7 q/\u03c1(\u03c4) for all cc \u2208 I do\nW \u2190 a random vector drawn from N (0, \u03c4) H(t, x, c)\u2190 H(t, x, c)\u00d7 TREE(t+ \u03c4, x+W, cc)\nend for end if\nAs in Theorem 1 in [NPP23], the following Feynman-Kac type identity\nu(t, x) = E[H(t, x, Id)] (2.2)\nfor the solution of (1.1) holds under suitable integrability conditions on H(t, x, Id) and smoothness assumptions on the coefficients of (1.1), see the appendix for calculation details."
        },
        {
            "heading": "3 Deep branching solver",
            "text": "Instead of evaluating (2.2) at a single point (t, x) \u2208 [0, T ] \u00d7 Rd, we use the L2-minimality property of expectation to perform a functional estimation of u(\u00b7, \u00b7) as u(\u00b7, \u00b7) = v\u2217(\u00b7, \u00b7) on the support of a random vector (\u03c4,X) on [0, T ]\u00d7 Rd such that H(\u03c4,X, Id) \u2208 L2(\u2126), where\nv\u2217 = argmin {v : v(\u03c4,X)\u2208L2}\nE [ (H(\u03c4,X, Id)\u2212 v(\u03c4,X))2 ] . (3.1)\nTo evaluate (2.2) on [0, T ]\u00d7D, where D is a bounded domain of Rd, we can choose (\u03c4,X) to be a uniform random vector on [0, T ] \u00d7 D. Similarly, to evaluate (2.2) on {0} \u00d7 D, we may let \u03c4 \u2261 0 and let X be a uniform random vector on D.\nIn order to implement the deep learning approximation, we parametrize v(\u00b7, \u00b7) in the functional space described below. Given \u03c3 : R \u2192 R an activation function such as \u03c3ReLU(x) := max(0, x), \u03c3tanh(x) := tanh(x) or \u03c3Id(x) := x, we define the set of layer functions L\u03c3d1,d2 by\nL\u03c3d1,d2 := { L : Rd1 \u2192 Rd2 : L(x) = \u03c3(Ax+ b), x \u2208 Rd1 , A \u2208 Rd2\u00d7d1 , b \u2208 Rd2 } , (3.2)\nwhere d1 \u2265 1 is the input dimension, d2 \u2265 1 is the output dimension, and the activation function \u03c3 is applied component-wise to Ax + b. Similarly, when the input and output dimensions are the same, we define the set of residual layer functions L\u03c1,resd by\nL\u03c3,resd := { L : Rd \u2192 Rd : L(x) = x+ \u03c3(Ax+ b), x \u2208 Rd, A \u2208 Rd\u00d7d, b \u2208 Rd } , (3.3)\nsee [HZRS16]. Then, we denote by NN\u03c3,l,md := { Ll \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 L0 : Rd \u2192 R : L0 \u2208 L\u03c3d,m, Ll \u2208 L \u03c3Id m,1, Li \u2208 L\u03c3,resm , 1 \u2264 i < l } the set of feed-forward neural networks with one output layer, l \u2265 1 hidden residual layers each containing m \u2265 1 neurons, where the activation functions of the output and hidden layers are respectively the identity function \u03c3Id and \u03c3. Any v(\u00b7; \u03b8) \u2208 NN\u03c3,l,md is fully determined by the sequence\n\u03b8 := ( A0, b0, A1, b1, . . . , Al\u22121, bl\u22121, Al, bl )\nof ((d+ 1)m+ (l \u2212 1)(m+ 1)m+ (m+ 1)) parameters. Since by the universal approximation theorem, see e.g. Theorem 1 of [Hor91], \u221e\u22c3\nm=1 NN\u03c3,l,md is dense in the L2 functional space, the optimization problem (3.1) can be approximated by\nv\u2217 \u2248 argmin v\u2208NN\u03c3,l,md+1\nE [ (H(\u03c4,X, Id)\u2212 v(\u03c4,X))2 ] . (3.4)\nBy the law of large numbers, (3.4) can be further approximated by\nv\u2217 \u2248 argmin v\u2208NN\u03c3,l,md+1 N\u22121 N\u2211 i=1 (Hi \u2212 v(\u03c4i, Xi))2 , (3.5)\nwhere for all i = 1, . . . , N , (\u03c4i, Xi) is drawn independently from the distribution of (\u03c4,X) and Hi is drawn from H\u03c4i,Xi,Id using Algorithm 1. However, the approximation (3.5) may perform poorly when the variance of Hi is too high. To solve this issue, we use the expression\nv\u2217 \u2248 argmin v\u2208NN\u03c3,l,md+1 N\u22121 N\u2211 i=1\n( M\u22121\nM\u2211 j=1 Hi,j \u2212 v(\u03c4i, Xi)\n)2 , (3.6)\nwhere for j = 1, . . . ,M , Hi,j is drawn independently from H\u03c4i,Xi,Id using Algorithm 1.\nFinally, the deep branching method using the gradient descent method to solve the\noptimization in (3.6) is summarized in Algorithm 2.\nAlgorithm 2 Deep branching method Input: The learning rate \u03b7 and the number of epochs P Output: v(\u00b7, \u00b7; \u03b8) \u2208 NN\u03c3,l,md+1 (\u03c4i, Xi)1\u2264i\u2264N \u2190 random vectors drawn from the distribution of (\u03c4,X) (Hi,j)1\u2264i\u2264N\n1\u2264j\u2264M \u2190 random variables generated by TREE(\u03c4i, Xi, Id) in Algorithm 1\nInitialize \u03b8 for i\u2190 1, . . . , P do\nL\u2190 N\u22121 N\u2211 i=1\n( M\u22121\nM\u2211 j=1 Hi,j \u2212 v(\u03c4i, Xi; \u03b8) )2\n\u03b8 \u2190 \u03b8 \u2212 \u03b7\u2207\u03b8L end for\nRemark 3.1 In the implementation of Algorithm 2, we perform the following additional steps:\ni) \u03b7 \u2190 \u03b7/10 after every \u230aP/3\u230b steps.\nii) Instead of using \u03b7 to update \u03b8 directly, Adam algorithm is used to update \u03b8, see [KB14].\niii) \u03c3tanh is used because the target PDE solution (1.1) is smooth.\niv) A batch normalization layer is added after the activation function in (3.2)-(3.3) when\n\u03c3 \u0338= \u03c3Id, see [IS15].\nv) \u03c1 is chosen to be the PDF of exponential distribution with rate \u2212(log 0.95)/T .\nvi) Given xmin < xmax and xmid = (xmin + xmax)/2, we take\nD := [xmin, xmax]\u00d7 {xmid} \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 {xmid},\nand we let (\u03c4,X) be the uniform random vector on {0} \u00d7D."
        },
        {
            "heading": "4 Numerical examples",
            "text": "The numerical examples below are run in Python using PyTorch with the default initialization scheme for \u03b8, and the default values N = 1000, P = 3000, \u03b7 = 0.01, l = 6, m = 20. Except if otherwise stated, runtimes are expressed in minutes and the examples have been run on Google Colab with a Tesla P100 GPU.\nFor comparisons with the deep BSDE and deep Galerkin methods, we select the configurations such that all methods have comparable or similar runtimes. For the deep BSDE method of [HJE18, BEJ19], the time discretization of (0, T/5, 2T/5, 3T/5, 4T/5, T ) and 1000 (resp. 100, 000) number of samples are used in the case of d = 1 (resp. d > 1).\nFor the deep Galerkin method of [SS18], 10, 000 samples are respectively generated on {0} \u00d7 [xmin, xmax]d, (0, T ) \u00d7 [xmin, xmax]d, and {T} \u00d7 [xmin, xmax]d. In our experiment, such generation works better than generating 10, 000 samples respectively on {0}\u00d7D, (0, T )\u00d7D, and {T} \u00d7 D. In addition, we found that batch normalization and learning rate decay in Remark 3.1 do not work well with deep Galerkin method, hence they are not used in the simulation below for the deep Galerkin method. The learning rate for the deep Galerkin method is fixed to be \u03b7 = 0.001 throughout the training.\nThe analysis of error is performed on the grid of D\u0303 = (0, xmin+i\u2206x, xmid, . . . , xmid)0\u2264i\u2264100, where \u2206x = (xmax \u2212 xmin)/100. In each of the 10 independent runs, the statistics of the runtime (in seconds) and the Lp error 100\u22121\n\u2211 x\u2208D\u0303 |true(x) \u2212 predicted(x)|p are recorded. In\nmultidimensional examples with d \u2265 2, every figure is plotted as a function of x1 on the horizontal axis, after setting (x2, . . . , xd) = (0, . . . , 0).\na) Allen-Cahn equation\nConsider the equation\n\u2202tu(t, x) + 1\n2 \u2206u(t, x) + u(t, x)\u2212 u3(t, x) = 0, (4.1)\nwhich admits the traveling wave solution\nu(t, x) = \u22121 2 \u2212 1 2 tanh\n( 3\n4 (T \u2212 t)\u2212 d\u2211 i=1 xi 2 \u221a d\n) , (t, x) \u2208 [0, T ]\u00d7 Rd.\nTable 1 summarizes the results of 10 independent runs, with M = 100, 000, T = 0.5, xmin = \u22128, and xmax = 8.\nWe check in Table 1 and Figure 1 that all three algorithms show a similar accuracy for the numerical solution of the Allen-Cahn equation, while the deep branching method appears more stable.\nFigure 2 compares the L1 errors of deep learning methods, showing that although the deep branching method has an explosive behavior, under comparable runtimes it can perform better than the deep Galerkin and deep BSDE methods in small time, in both dimensions d = 1 and 10. Figure 2 and Table 2 have been run on a RTX A4000 GPU.\nTable 2 ensures that the experiments of Figure 2 are performed within comparable runtimes.\nb) Exponential nonlinearity\nConsider the equation\n\u2202tu(t, x) + \u03b1\nd d\u2211 i=1 \u2202xiu(t, x) + 1 2 \u2206u(t, x) + e\u2212u(t,x)(1\u2212 2e\u2212u(t,x))d = 0, (4.2)\nwhich admits the traveling wave solution\nu(t, x) = log 1 +( d\u2211 i=1 xi + \u03b1(T \u2212 t) )2 , (t, x) \u2208 [0, T ]\u00d7 Rd. Table 3 summarizes the results of 10 independent runs, with M = 30, 000 (resp. M = 3, 000) in dimension d = 1 (resp. d = 5), \u03b1 = 10, T = 0.05, xmin = \u22124, and xmax = 4.\nIn the case of exponential nonlinearity, our method appears significantly more accurate than the deep Galerkin method, and performs comparably to the deep BSDE method in dimension d = 5.\nc) Burgers equation\nNext, we consider the multidimensional Burgers equation\n\u2202tu(t, x) + d2\n2 \u2206u(t, x) +\n( u(t, x)\u2212 2 + d\n2d\n)( d\nd\u2211 k=1 \u2202xku(t, x)\n) = 0, (4.3)\nwith traveling wave solution\nu(t, x) = exp\n( t+ d\u22121 \u2211d i=1 xi ) 1 + exp ( t+ d\u22121 \u2211d i=1 xi\n) , x = (x1, . . . , xd) \u2208 Rd, t \u2208 [0, T ], (4.4) see \u00a7 4.5 of [HJE17], and \u00a7 4.2 of [Cha13]. Figure 4 presents estimates of the solution of the Burgers equation (4.3) with solution (4.4) in dimensions d = 5 and d = 20, with comparisons to the outputs of the deep Galerkin method [SS18] and of the deep BSDE method [HJE18].\nWe note in Figure 4\u2212b) that the deep branching method is more stable than the deep Galerkin and deep BSDE methods in dimension d = 15. In particular, the deep BSDE estimate explodes under comparable runtimes, as shown in Figure 5. Figure 5 and Table 4 have been run on a RTX A4000 GPU.\nTable 4 ensures that the experiments of Figure 5 are performed within comparable runtimes.\nd) Merton problem\nLet (Xs)t\u2208[0,T ] be the solution of the controlled SDE\ndXs = (\u00b5\u03c0sXs \u2212 cs)ds+ \u03c0s\u03c3XsdBs\nstarted at Xt = x, where (cs)s\u2208[0,T ] and (\u03c3s)s\u2208[0,T ] are square-integrable adapted processes. We consider the Merton problem\nu(t, x) = inf (\u03c0s)t\u2264s\u2264T , (cs)t\u2264s\u2264T\nE [ e\u2212\u03c1(T\u2212t)X1\u2212\u03b3T\n1\u2212 \u03b3 + \u222b T t e\u2212\u03c1(s\u2212t)c1\u2212\u03b3s 1\u2212 \u03b3 ds ] , (4.5)\nwhere \u03b3 \u2208 (0, 1). The solution u(t, x) of (4.5) satisfies the Hamilton-Jacobi-Bellman (HJB) equation\n\u2202tu(t, x) + sup \u03c0,c\n( (\u03c0\u00b5x\u2212 c)\u2202xu(t, x) + \u03c02\u03c32x2\n2 \u22022xu(t, x) +\nc1\u2212\u03b3\n1\u2212 \u03b3\n) = \u03c1u(t, x),\nwhich, by first order condition, can be rewritten as\n\u2202tu(t, x)\u2212 (\u00b5\u2202xu(t, x))\n2\n2\u03c32\u22022xu(t, x) +\n\u03b3\n1\u2212 \u03b3 (\u2202xu(t, x))\n1\u22121/\u03b3 = \u03c1u(t, x), (4.6)\nand admits the solution\nu(t, x) = x1\u2212\u03b3(1 + (\u03b1\u2212 1)e\u2212\u03b1(T\u2212t))\u03b3\n\u03b1\u03b3(1\u2212 \u03b3) , (t, x) \u2208 [0, T ]\u00d7 R,\nwhere \u03b1 := (2\u03c32\u03b3\u03c1 \u2212 (1 \u2212 \u03b3)\u00b52)/(2\u03c32\u03b32). As the loss function used in the deep Galerkin method uses a division by the second derivatives of the neural network function, see (1.2) and (4.6), it explodes when the second derivatives of the learned neural network function becomes small during the training. Hence, in Table 5, we only present the outputs of the deep branching method and of the deep BSDE method of [BEJ19] which deals with second order gradient nonlinearities. Table 5 summarizes the results of 10 independent runs, with\n\u00b5 = 0.03, \u03c3 = 0.1, \u03b3 = 0.5, \u03c1 = 0.01, T = 0.1 on the interval [xmin, xmax] = [100, 200], where we take M = 10, 000 in the deep branching method.\nAn anomaly was detected on the third run when using \u03c3 = \u03c3tanh, and it disappeared after changing the activation function to \u03c3 = \u03c3ReLU.\nIn Figure 7, we plot the Monte Carlo samples generated by Algorithm 1 and the learned neural network function v(\u00b7, \u00b7; \u03b8), see Algorithm 2, for \u03c3 = \u03c3ReLU and for \u03c3 = \u03c3tanh on the third run.\nFigure 7 shows the consistency, or lack thereof, between the Monte Carlo samples and the learned neural network function, which cannot be observed when using the deep Galerkin or deep BSDE method.\ne) Third order gradient log nonlinearity\nThis Example e) and the next Example f) use nonlinearities in terms of third and fourth order gradients, to which the deep BSDE method does not apply. For this reason, comparisons are done only with respect to the Galerkin method. Consider the equation\n\u2202tu(t, x) + \u03b1\nd d\u2211 i=1 \u2202xiu(t, x) + log\n( 1\nd d\u2211 i=1 ( \u22022xiu(t, x) )2 + ( \u22023xiu(t, x)\n)2) = 0, (4.7)\nwhich admits the solution\nu(t, x) = cos\n( d\u2211\ni=1\nxi + \u03b1(T \u2212 t) ) , (t, x) \u2208 [0, T ]\u00d7 Rd.\nTable 6 summarizes the results of 10 independent runs, with M = 6, 000 in dimension d = 1 (resp. M = 200 in dimension d = 5), \u03b1 = 10, T = 0.02, xmin = \u22123, and xmax = 3.\nIn the case of log nonlinearity with a third order gradient term, our method appears more accurate than the deep Galerkin method in dimensions d = 1 and d = 5. Figure 8 presents a numerical comparison on the average performance of 10 runs.\nf) Fourth order gradient cosine nonlinearity\nConsider the equation\n\u2202tu(t, x) + \u03b1\nd d\u2211 i=1 \u2202xiu(t, x) + u(t, x)\u2212 ( \u2206u(t, x) 12d )2 + 1 d d\u2211 i=1 cos ( \u03c0\u22024xiu(t, x) 4! ) = 0, (4.8)\nwhich admits the solution\nu(t, x) = \u03c6\n( d\u2211\ni=1\nxi + \u03b1(T \u2212 t) ) , (t, x) \u2208 [0, T ]\u00d7 Rd,\nwhere \u03c6(y) := y4 + y3 + by2 + cy + d for y \u2208 R, b = \u221236/47, c = 24b, d = 4b2, and \u03b1 = 10.\nTable 7 summarizes the results of 10 independent runs, with M = 2, 500 in dimension\nd = 1 (resp. M = 50 in dimension d = 5), \u03b1 = 10, T = 0.04, xmin = \u22125, and xmax = 5.\nIn the case of cosine nonlinearity with a fourth order gradient, our method appears more accurate than the deep Galerkin methods in dimensions d = 1 and d = 5. Figure 9 presents a numerical comparison on the average performance of 10 runs."
        },
        {
            "heading": "A Multidimensional extension",
            "text": "In this section we sketch the argument extending Theorem 1 in [NPP23] to the multidimensional case, and leading to (2.2). For this, given g \u2208 C0,\u221e([0, T ]\u00d7Rd) and \u00b5 \u2208 Nn such that\n|\u00b5| \u2265 1, we will use the multivariate Faa\u0300 di Bruno formula\n\u2202\u00b5g \u2217(u)(t, x) =\n( d\u220f\ni=1\n\u00b5i! ) \u2211 1\u2264\u03bd1+\u00b7\u00b7\u00b7+\u03bdn\u2264|\u00b5|\n1\u2264s\u2264|\u00b5|\n(\u2202\u03bdg) \u2217(u)(t, x) \u2211 1\u2264|k1|,...,|ks|, 0\u227al1\u227a\u00b7\u00b7\u00b7\u227als ki1+\u00b7\u00b7\u00b7+kis=\u03bdi, i=1,...,n\n|k1|l1j+\u00b7\u00b7\u00b7+|ks|lsj=\u00b5j , j=1,...,d\n\u220f 1\u2264r\u2264s 1\u2264q\u2264n (\u2202lr+\u03bbqu(t, x)) kqr kqr ! (lr1! \u00b7 \u00b7 \u00b7 lrd!) kqr ,\n(A.1) see Theorem 2.1 in [CS96], applied to the function g\u2217(u)(t, x) := g ( \u2202\u03bb1u(t, x), . . . , \u2202\u03bbnu(t, x) ) .\nWe have\n\u2202tg \u2217(u) +\n1 2 \u2206g\u2217(u)\n= n\u2211\np=1\n\u2202\u03bbp ( \u2202tu+ 1\n2 \u2206u\n) (\u22021pg) \u2217(u) + 1\n2 n\u2211 i=1 n\u2211 j=1 d\u2211 k=1 (\u2202\u03bbi+1ku) (\u2202\u03bbj+1ku) (\u22021i+1jg) \u2217(u)\n= \u2212 n\u2211\np=1\n(\u22021pg) \u2217(u)\u2202\u03bbpf\n\u2217(u) + 1\n2 n\u2211 i=1 n\u2211 j=1 d\u2211 k=1 (\u2202\u03bbi+1ku) (\u2202\u03bbj+1ku) (\u22021i+1jg) \u2217(u)\n= \u2212 n\u2211\np=1\n1{\u03bbp=0}(\u22021pg) \u2217(u)f \u2217(u)\n\u2212 n\u2211\np=1\n(\u22021pg) \u2217(u)\n( d\u220f\ni=1\n\u03bbpi ! ) \u2211 1\u2264\u03bd1+\u00b7\u00b7\u00b7+\u03bdn\u2264|\u03bbp|\n1\u2264s\u2264|\u03bbp|\n(\u2202\u03bdf) \u2217(u) \u2211 1\u2264|k1|,...,|ks|, 0\u227al1\u227a\u00b7\u00b7\u00b7\u227als ki1+\u00b7\u00b7\u00b7+kis=\u03bdi, i=1,...,n\n|k1|l1j+\u00b7\u00b7\u00b7+|ks|lsj=\u03bb p j , j=1,...,d\n\u220f 1\u2264r\u2264s 1\u2264q\u2264n (\u2202lr+\u03bbqu) kqr kqr ! (lr1! \u00b7 \u00b7 \u00b7 lrd!) kqr\n+ 1\n2 n\u2211 i=1 n\u2211 j=1 d\u2211 k=1 (\u2202\u03bbi+1ku) (\u2202\u03bbj+1ku) (\u22021i+1jg) \u2217(u).\nRewriting the above equation in integral form yields\ng\u2217(u)(t, x) = \u222b Rd \u03c6(T \u2212 t, y \u2212 x)g(\u03d5(y))dy\n+ \u222b T t \u222b Rd\n\u03c6(s\u2212 t, y \u2212 x)( n\u2211 p=1 1{\u03bbp=0}(\u22021pg) \u2217(u)f \u2217(u)\u2212 1 2 n\u2211 i=1 n\u2211 j=1 d\u2211 k=1 (\u2202\u03bbi+1ku(s, y)) (\u2202\u03bbj+1ku(s, y)) (\u22021i+1jg) \u2217(u)\n+ n\u2211\np=1\n(\u22021pg) \u2217(u)\n( d\u220f\ni=1\n\u03bbpi ! ) \u2211 1\u2264\u03bd1+\u00b7\u00b7\u00b7+\u03bdn\u2264|\u03bbp|\n1\u2264s\u2264|\u03bbp|\n(\u2202\u03bdf) \u2217(u) \u2211 1\u2264|k1|,...,|ks|, 0\u227al1\u227a\u00b7\u00b7\u00b7\u227als ki1+\u00b7\u00b7\u00b7+kis=\u03bdi, i=1,...,n\n|k1|l1j+\u00b7\u00b7\u00b7+|ks|lsj=\u03bb p j , j=1,...,d\n\u220f 1\u2264r\u2264s 1\u2264q\u2264n (\u2202lr+\u03bbqu(s, y)) kqr kqr ! (lr1! \u00b7 \u00b7 \u00b7 lrd!) kqr ) dyds.\n(A.2)\nSimilarly, for \u00b5 \u2208 Nd, by the Faa\u0300 di Bruno formula (A.1) we have\n\u2202\u00b5u(t, x) = \u222b Rd \u03c6(T \u2212 t, y \u2212 x)\u2202\u00b5u(T, y)dy\n+ \u222b T t \u222b Rd \u2211 1\u2264\u03bd1+\u00b7\u00b7\u00b7+\u03bdn\u2264|\u00b5|\n1\u2264s\u2264|\u00b5|\n\u2211 1\u2264|k1|,...,|ks|, 0\u227al1\u227a\u00b7\u00b7\u00b7\u227als\nki1+\u00b7\u00b7\u00b7+k i s=\u03bdi, i=1,...,n\n|k1|l1j+\u00b7\u00b7\u00b7+|ks|l s j=\u00b5j , j=1,...,d\nd\u220f i=1\n\u00b5i!\u220f 1\u2264r\u2264s 1\u2264q\u2264n kqr ! (lr1! \u00b7 \u00b7 \u00b7 lrd!) kqr (\u2202\u03bdf) \u2217(u) \u2211 1\u2264r\u2264s 1\u2264q\u2264n ( \u2202lr+\u03bbqu(s, y) )kqrdyds. (A.3)\nCombining (A.2) and (A.3) yields the equation\nc(u)(t, x) = \u222b \u221e \u2212\u221e \u03c6(T\u2212t, y\u2212x)c(u)(T, y)dy+ \u2211\nZ\u2208M(c)\n\u222b T t \u222b \u221e \u2212\u221e \u03c6(s\u2212t, y\u2212x) \u220f z\u2208Z z(u)(s, y)dyds,\n(A.4)\n(t, x) \u2208 [0, T ] \u00d7 R, for any code c \u2208 C, as in Lemma 2.3 of [NPP23]. The dimension-free argument of Theorem 1 in [NPP23] then shows that (2.2) holds provided that H(t, x, Id) is integrable and the solution of (A.4) is unique."
        }
    ],
    "title": "A deep branching solver for fully nonlinear partial differential equations",
    "year": 2023
}