{
    "abstractText": "A wide variety of battery models are available, and it is not always obvious which model \u2018best\u2019 describes a dataset. This paper presents a Bayesian model selection approach using Bayesian quadrature. The model evidence is adopted as the selection metric, choosing the simplest model that describes the data, in the spirit of Occam\u2019s razor. However, estimating this requires integral computations over parameter space, which is usually prohibitively expensive. Bayesian quadrature offers sample-efficient integration via model-based inference that minimises the number of battery model evaluations. The posterior distribution of model parameters can also be inferred as a byproduct without further computation. Here, the simplest lithium-ion battery models, equivalent circuit models, were used to analyse the sensitivity of the selection criterion to given different datasets and model configurations. We show that popular model selection criteria, such as root-mean-square error and Bayesian information criterion, can fail to select a parsimonious model in the case of a multimodal posterior. The model evidence can spot the optimal model in such cases, simultaneously providing the variance of the evidence inference itself as an indication of confidence. We also show that Bayesian quadrature can compute the evidence faster than popular Monte Carlo based solvers.",
    "authors": [
        {
            "affiliations": [],
            "name": "Masaki Adachi"
        },
        {
            "affiliations": [],
            "name": "Yannick Kuhn"
        },
        {
            "affiliations": [],
            "name": "Birger Horstmann"
        },
        {
            "affiliations": [],
            "name": "Arnulf Latz"
        },
        {
            "affiliations": [],
            "name": "Michael A. Osborne"
        },
        {
            "affiliations": [],
            "name": "David A. Howey"
        }
    ],
    "id": "SP:6f1bea6436a9699abba6f2c9f6155219c921ea70",
    "references": [
        {
            "authors": [
                "M. Adachi",
                "S. Hayakawa",
                "S. Hamid",
                "M. J\u00f8rgensen",
                "H. Oberhauser",
                "M.A. Osborne"
            ],
            "title": "SOBER: Scalable batch Bayesian optimization and quadrature using recombination constraints",
            "venue": "arXiv preprint arXiv:2301.11832.",
            "year": 2023
        },
        {
            "authors": [
                "M. Adachi",
                "S. Hayakawa",
                "M. J\u00f8rgensen",
                "H. Oberhauser",
                "M.A. Osborne"
            ],
            "title": "Fast Bayesian inference with batch Bayesian quadrature via kernel recombination",
            "venue": "Advances in neural information processing systems (NeurIPS), 35.",
            "year": 2022
        },
        {
            "authors": [
                "A. Aitio",
                "S.G. Marquis",
                "P. Ascencio",
                "D.A. Howey"
            ],
            "title": "Bayesian parameter estimation applied",
            "year": 2020
        },
        {
            "authors": [
                "A.M. Bizeray",
                "J.H. Kim",
                "S.R. Duncan",
                "D.A. Howey"
            ],
            "title": "Identifiability and parameter estimation of the single particle lithium-ion battery model",
            "venue": "IEEE Trans. Control. Syst. Technol., 27(5), 1862\u20131877.",
            "year": 2018
        },
        {
            "authors": [
                "J. Calderwood"
            ],
            "title": "A physical hypothesis for ColeDavidson behavior",
            "venue": "IEEE transactions on dielectrics and electrical insulation, 10(6), 1006\u20131011.",
            "year": 2003
        },
        {
            "authors": [
                "H.R. Chai",
                "R. Garnett"
            ],
            "title": "Improving quadrature for constrained integrands",
            "venue": "The 22nd International Conference on Artificial Intelligence and Statistics, 2751\u20132759. PMLR.",
            "year": 2019
        },
        {
            "authors": [
                "M. Doyle",
                "T.F. Fuller",
                "J. Newman"
            ],
            "title": "Modeling of galvanostatic charge and discharge of the lithium/polymer/insertion cell",
            "venue": "J. Electrochem. Soc., 140(6), 1526.",
            "year": 1993
        },
        {
            "authors": [
                "J.M. Escalante",
                "S. Sahu",
                "J.M. Foster",
                "B. Protas"
            ],
            "title": "On uncertainty quantification in the parametrization of Newman-type models of lithium-ion batteries",
            "venue": "J. Electrochem. Soc., 168(11), 110519.",
            "year": 2021
        },
        {
            "authors": [
                "T. Gunter",
                "M.A. Osborne",
                "R. Garnett",
                "P. Hennig",
                "S.J. Roberts"
            ],
            "title": "Sampling for inference in probabilistic models with fast Bayesian quadrature",
            "venue": "Advances in neural information processing systems, 27.",
            "year": 2014
        },
        {
            "authors": [
                "W.K. Hastings"
            ],
            "title": "Monte Carlo sampling methods using Markov chains and their applications",
            "venue": "Biometrika, 57(1), 97\u2013109. URL https://doi:10.1093/ biomet/57.1.97.",
            "year": 1970
        },
        {
            "authors": [
                "S. Hayakawa",
                "H. Oberhauser",
                "T. Lyons"
            ],
            "title": "Positively weighted kernel quadrature via subsampling",
            "venue": "Advances in neural information processing systems (NeurIPS), 35.",
            "year": 2022
        },
        {
            "authors": [
                "H. He",
                "R. Xiong",
                "J. Fan"
            ],
            "title": "Evaluation of lithium-ion battery equivalent circuit models for state of charge estimation by an experimental approach",
            "venue": "Energies, 4(4), 582\u2013598.",
            "year": 2011
        },
        {
            "authors": [
                "J. Huang",
                "M. Papac",
                "R. O\u2019Hayre"
            ],
            "title": "Towards robust autonomous impedance spectroscopy analysis: A calibrated hierarchical Bayesian approach for electrochemical impedance spectroscopy (eis) inversion",
            "venue": "Electrochim. Acta,",
            "year": 2021
        },
        {
            "authors": [
                "F. Hutter",
                "H. Hoos",
                "K. Leyton-Brown"
            ],
            "title": "An efficient approach for assessing hyperparameter importance",
            "venue": "International conference on machine learning (ICML), 754\u2013762. PMLR.",
            "year": 2014
        },
        {
            "authors": [
                "H. Jeffreys"
            ],
            "title": "The theory of probability",
            "venue": "OUP Oxford.",
            "year": 1998
        },
        {
            "authors": [
                "P. Kemper",
                "D. Kum"
            ],
            "title": "Extended single particle model of Li-ion batteries towards high current applications",
            "venue": "IEEE VPPC, 1\u20136. IEEE.",
            "year": 2013
        },
        {
            "authors": [
                "G. Kitagawa"
            ],
            "title": "A Monte Carlo filtering and smoothing method for non-Gaussian nonlinear state space models",
            "venue": "Proceedings of the 2nd U.S.-Japan Joint Seminar on Statistical Time Series Analysis, 110.",
            "year": 1993
        },
        {
            "authors": [
                "Y. Kuhn",
                "H. Wolf",
                "A. Latz",
                "B. Horstmann"
            ],
            "title": "EP-BOLFI: Measurement-noise-aware parameterization of continuum battery models from electrochemical measurements applied to full-cell GITT measurements",
            "venue": "arXiv preprint arXiv:2208.03289.",
            "year": 2022
        },
        {
            "authors": [
                "J. Liu",
                "F. Ciucci"
            ],
            "title": "The Gaussian process distribution of relaxation times: A machine learning tool for the analysis and prediction of electrochemical impedance spectroscopy data",
            "venue": "Electrochim. Acta, 331,",
            "year": 2020
        },
        {
            "authors": [
                "N. Metropolis",
                "A.W. Rosenbluth",
                "M.N. Rosenbluth",
                "A.H. Teller",
                "E. Teller"
            ],
            "title": "Equation of state calculations by fast computing machines",
            "venue": "Chem. Phys., 21(6), 1087\u20131092.",
            "year": 1953
        },
        {
            "authors": [
                "R.H. Milocco",
                "J.E. Thomas",
                "B. Castro"
            ],
            "title": "Generic dynamic model of rechargeable batteries",
            "venue": "J. Power Sources, 246, 609\u2013620.",
            "year": 2014
        },
        {
            "authors": [
                "Y. Miyazaki",
                "R. Nakayama",
                "N. Yasuo",
                "Y. Watanabe",
                "R. Shimizu",
                "D.M. Packwood",
                "K. Nishio",
                "Y. Ando",
                "M. Sekijima",
                "T. Hitosugi"
            ],
            "title": "Bayesian statistics-based analysis of ac impedance spectra",
            "venue": "AIP Advances, 10(4), 045231.",
            "year": 2020
        },
        {
            "authors": [
                "K.P. Murphy"
            ],
            "title": "Machine learning: a probabilistic perspective",
            "venue": "MIT press.",
            "year": 2012
        },
        {
            "authors": [
                "I. Murray",
                "R. Adams",
                "D. MacKay"
            ],
            "title": "Elliptical slice sampling",
            "venue": "Proceedings of the thirteenth international conference on artificial intelligence and statistics, 541\u2013548. JMLR Workshop and Conference Proceedings.",
            "year": 2010
        },
        {
            "authors": [
                "M. Osborne",
                "R. Garnett",
                "Z. Ghahramani",
                "D.K. Duvenaud",
                "S.J. Roberts",
                "C. Rasmussen"
            ],
            "title": "Active learning of model evidence using Bayesian quadrature",
            "venue": "Advances in neural information processing systems, 25.",
            "year": 2012
        },
        {
            "authors": [
                "C. Rasmussen",
                "Z. Ghahramani"
            ],
            "title": "Occam\u2019s razor",
            "venue": "Advances in neural information processing systems (NeurIPS), 13.",
            "year": 2000
        },
        {
            "authors": [
                "S. Santhanagopalan",
                "Q. Guo",
                "P. Ramadass",
                "R.E. White"
            ],
            "title": "Review of models for predicting the cycling performance of lithium ion batteries",
            "venue": "J. Power Sources, 156(2), 620\u2013628.",
            "year": 2006
        },
        {
            "authors": [
                "J.S. Speagle"
            ],
            "title": "dynesty: a dynamic nested sampling package for estimating Bayesian posteriors and evidences",
            "venue": "Monthly Notices of the Royal Astronomical Society, 493(3), 3132\u20133158. Appendix A. DERIVATION OF CANONICAL FORM",
            "year": 2020
        },
        {
            "authors": [
                "borne"
            ],
            "title": "showed that active learning sampling could efficiently reduce the number of samples. The active learning scheme guides the next query point to minimise the integral variance, exploiting the GP surrogate model information. A function called acquisition function formu",
            "year": 2012
        },
        {
            "authors": [
                "2014 Gunter et al",
                "Chai",
                "2019). Garnett"
            ],
            "title": "To accommodate the wide dynamic range, log transformation is widely applied in the BQ community. However, log-warped GP inevitably results in sampling from log space, leading to ineffective exploration",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Keywords: Bayesian methods, identifiability, parameter estimation, battery, lithium-ion"
        },
        {
            "heading": "1. INTRODUCTION",
            "text": "The lithium-ion battery is key to decarbonising power grids and electrifying vehicles. However, its behaviour can be challenging to model, control, and diagnose, and this is a practical hindrance to obtaining the optimal performance. This is compounded by the available data from operational batteries being typically limited to just three measurements: voltage, current, and temperature. Estimating internal states from these time-varying three variables is challenging or even mathematically impossible due to parameter identifiability issues (Bizeray et al., 2018). Degradation further complicates matters since the number of parameters to be identified becomes larger when considering long-term ageing.\nThere are dozens of plausible models for Li-ion batteries, owing to differing assumptions and levels of approx-\n? This work has been accepted to IFAC 2023. Code is publicly available at: https://github.com/Battery-IntelligenceLab/BayesianModelSelection. The authors acknowledge support by Toyota Motor Corporation, Oxford Clarendon Fund, Oxford Kobe scholarship, the German Aerospace Center (DLR), and the Helmholtz Association through grant no KW-BASF-6, and contributes to the research performed at CELEST\nimation. While electrochemists might prefer continuum models, such as the Doyle-Fuller-Newman model (Doyle et al., 1993), that give understanding of internal chemical reactions and transport, control engineers prefer simpler approaches such as equivalent circuit models (ECMs) (He et al., 2011), for fast control and fewer parameters. Other models exist in a spectrum between these (e.g. from simple to more complex: ECM \u2192 EHM (Milocco et al., 2014) \u2192 SPM (Santhanagopalan et al., 2006) \u2192 SPMe (Kemper and Kum, 2013) \u2192 DFN). System identification is the foundation of an estimation and control system, determining predictive accuracy, quick response, and reliability.\nHowever, the \u2018best\u2019 model should be ascertained based on quantifiable performance metrics. Importantly, the optimal model strongly depends on the dataset D and user requirements. A widely accepted approach for defining \u2018good\u2019 models is Occam\u2019s razor, where the simplest model to reasonably reproduce a given dataset is considered the best. Simplest here relates to the number of parameters to be identified. Rasmussen and Ghahramani (2000) showed that such a metric could be evaluated via Bayesian model evidence, obtained for a model M by integrating out (i.e. averaging over) the parameters \u03b8 from the likelihood,\nar X\niv :2\n21 0.\n17 29\n9v 4\n[ st\nat .M\nE ]\n5 A\npr 2\n(1) 1 RC pair (2) 2 RC pairs (3) 3 RC pairs\nR0\nC1\nR1 R2\nC2\nR3\nC3\nR0 R1\nC1 C2C1\nR2R1 R0\nRe[Z]\nIm [Z\n]\nIm [Z\n]\nIm [Z\n]\nRe[Z] Re[Z]\nIm pe\nda nc\ne [\nFig. 1. Model selection from three RC pair models.\np(D jM ) = Z p(D j ; M )dp( ); (1) where p( ) is the prior distribution and p(D j ; M ) is the likelihood. The mean evidenceE[p(D jM )] gives the probability of reproducing a given dataset D with a given model M , the degree of model t penalised by model complexity. The variance quanti es its uncertainty. Surprisingly, Bayesian model selection of battery models has barely been reported, except for Miyazaki et al. (2020). Although Bayesian parameter estimation (Aitio et al., 2020; Escalante et al., 2021), and probabilistic modelling works (Huang et al., 2021; Liu and Ciucci, 2020) exist, most Bayesian approaches in the battery community use Markov chain Monte Carlo (MCMC) (Metropolis et al., 1953; Hastings, 1970), a user-friendly but sampleine cient approach for inference. Recent work (Kuhn et al., 2022) on parameterisation applied a sample-e cient solver with Bayesian optimisation, none of the above solvers o er evidence computation. This is because estimating the evidence requires prohibitive integral computation, and this is particularly challenging when the likelihood is non-closed-form and/or expensive. A typical practice in such cases is to adopt the Bayesian information criterion (BIC), which is a coarse approximation of the evidence that assumes the posterior is a unimodal Gaussian. Unfortunately battery parameter estimation can produce multimodal or non-Gaussian posterior distributions (Aitio et al., 2020; Escalante et al., 2021), and ignoring this may cause overcon dence|previous work (Miyazaki et al., 2020) demonstrates that the identi cation of the best model using a variant of BIC gradually worsens as the posterior multimodality increases. This paper introduces Bayesian quadrature (BQ) as a novel technique for sample-e cient model evidence and parameter posterior estimation, and applies this to battery equivalent circuit models using synthetic data for demonstration purposes.\n2. BATTERY MODEL FORMULATION\nWe selected ECMs for proof-of-concept here since they are relatively simple battery models that nonetheless offer identi cation challenges. Parameter identi ability for ECMs is often examined in the frequency domain, for example via electrochemical impedance spectroscopy (EIS)\ndata, although time domain data may also be used. Several plausible ECMs are usually compared when tting EIS data, but the process is subjective and based on the user\u2019s electrochemical understanding of the target battery. For simplicity, we chose a simple resistance-capacitance (RC) pair model|this may represent various physical processes, for example kinetics and double layer capacitance, or an approximation of di usion. Fig. 1 illustrates the circuit con gurations and typical Nyquist plots of three variations of RC circuit models. The number of RC parallel connection components corresponds to the number of the semi-circles in a Nyquist plot. This correspondence is key for identifying the model from spectra. As the semi-circle shape implies, the real and imaginary parts of spectra have a mathematical relationship (Kramers-Kronig relations in Debye relaxation), where one part of spectra can be derived from the other via an equation. We extend this formulation to make the model better suited for statistical inference using the hyperbolic formulation (Calderwood, 2003); we improve this, permitting non-dimensionalised parameterisation without positivity constraint, as follows. For a general circuit with N total RC pairs plus an additional series resistanceR0, where Ri is the resistance of i -th RC pair [ ], ln( ! i ) is the rescaled frequency scale to make the scale independent of the given frequency range of the dataset, rescaled with the breakpoint frequency ! i := 1= i [rad/s], i := Ri Ci is the time constant of the i -th RC pair [s], Ci is the capacitance of the i -th RC-pair [F], f is the frequency [Hz] and ! := 2 f is the angular frequency [rad/s], one can de ne the total resistance Rtotal , the log of this r total (which is positive), the dimensionless resistance ofi -th RC-pair r i (constrained between zero and one), the unconstrained dimensionless resistancer 0i , the scaling factorRim , and the weight of i -th hyperbolic secant distribution wi , as follows:\nRre := R0 + NX\ni =1\nRi := Rtotal := exp( r total ); (2)\nr i := Ri\nRtotal := exp [ exp(r 0i )] ; (3)\nRim := 2\nNX i =1 Ri (4)\ni := Ri\nP N i =1 Ri\n: (5)\nFrom this, the real and imaginary parts of the impedance (Re[Z ], Im[Z ]) , are given by (see Appendix A)\nRe[Z ] = Rre\n\"\nr 0 + NX\ni =1\nr i 2 [1 tanh(ln ! i )]\n#\n; (6)\nIm[Z ] = Rim|{z} scaling factor\n\" NX i =1 i sech (ln! i ) # | {z } mixture of hyperbolic secant distributions : (7)\nThe frequency range is also standardised according to the frequencies in the available dataset,\n! ; ! := E[ln ! ]; p Var[ln ! ]; (8)\n! std ; stdi := ln ! !\n! ; ln i + ! ! ; (9)\nwhere the mean \u00b5\u03c9 and standard deviation \u03c3\u03c9 of logarithmic angular frequency 1 , ln\u03c9 can be calculated from the given frequency range of the dataset, and from this we define a standardised frequency scale \u03c9std and standardised time constants \u03c4 stdi . These may be related to the actual time constants and capacitances (noting that \u03c4i is the unstandardised form of the time constant) via\nln\u03c9\u03c4i := ln\u03c9 \u2212 \u03c3\u03c9\u03c4 stdi \u2212 \u00b5\u03c9, (10)\nCi = \u03c4i\nriRtotal . (11)\nThe parameters to be fitted are unconstrained standardised ones \u03b8 := {rtotal, r\u2032i, \u03c4 stdi }. This formulation is similar to the distribution of relaxation times modelling. This canonical form provides three benefits: separation of scaling factor, unconstrained prior distribution selection for all parameters, and integral-friendly formulation. Separating the scaling factors can decompose parameter estimation problems into problems of estimating magnitudes (Rre) and ratios (ri), permitting fair comparison over varied magnitudes of resistance. Logarithmically transformed parameters enable non-negativity constraints over resistance, allowing arbitrary prior distributions to be used for Bayesian inference (for instance, ri is constrained between zero and one, but r\u2032i is unconstrained). The mixture of hyperbolic secant distributions offers several integral identities to analytically calculate the expectation and variance (see Appendix C). Moreover, this formulation interprets the imaginary part as a probability distribution function, allowing statistical analysis (see section 5)."
        },
        {
            "heading": "3. BAYESIAN INFERENCE FORMULATION",
            "text": "We wish to select the likeliest model from the abovementioned three RC pair options. In Bayesian inference, we need to assume a prior distribution p(\u0398) := \u03c0(\u0398) and a likelihood function p(D|\u0398,M) := `true(\u0398). The prior distribution is a probability distribution reflecting one\u2019s prior assumptions about possible parameters. For instance, we adopt here a multivariate normal distribution \u03c0(\u0398) := N (\u0398;\u00b5\u03c0,\u03a3\u03c0). The mean vector \u00b5\u03c0 represents our guess of plausible parameter values and the covariance matrix \u03a3\u03c0 reflects our assumption on the uncertainty of each parameter, and correlations between parameters. The likelihood function `true(\u0398) is a probability distribution to evaluate how the selected parameter set \u0398 can reproduce the given dataset D. Here we assume a univariate Gaussian with zero mean 0 and homoskedastic noise, meaning the noise variance \u03c3noise does not vary over frequency. The squared error evaluates how similar the observed data yobs and ECM predicted data yecm are. Now, with the assumed prior p(\u0398) and likelihood function p(D|\u0398,M), Bayes\u2019 rule defines the parameter posterior as p(\u0398,M |D) and the model evidence p(D|M), all as follows:\np(D|\u0398,M) := `true(\u0398) := m\u220f j N (errj(\u03b8); 0, \u03c32noise), (12)\np(D|M) := N (E\u03c0[`true(\u0398)],Var\u03c0[`true(\u0398)]) , (13)\np(\u0398|D,M) = p(D|\u0398,M)p(\u0398) p(D|M) = `true(\u0398)\u03c0(\u0398) E\u03c0[`true(\u0398)] , (14)\n1 Where necessary we assume arguments of logarithms are divided by appropriate units, e.g. 1 [rad/s], to ensure they are dimensionless.\nwhere\nD := {yobs, \u03c9std} \u2208 Rm\u00d72, (15) \u03b8 := {rtotal, r\u2032i, \u03c4 stdi } \u2208 Rd\u22121, (16)\n\u0398 := {\u03b8, \u03c32noise} \u2208 Rd, (17) yecm,j(\u03b8) := {yre,j , yim,j} = M(\u03b8, \u03c9stdj ), (18)\nerrj(\u03b8) := [yobs,j \u2212 yecm,j(\u03b8)]2 , (19) where subscript \u2018obs\u2019 refers to measured data, subscript \u2018ecm\u2019 to modelled data, and M is the model (equations (6)- (7)). The posterior p(\u0398|D,M) is a conditional probability distribution that reflects our updated estimate of the parameter space based on the observed data D. We use dimensionless and unconstrained r\u2032i and \u03c4 std i as inputs of the model for arbitrary prior selection and fair comparison of models. The number of parameters to be estimated is d = 2 + 2N , as the scaling factor rtotal and experimental noise variance \u03c32noise are shared over all models."
        },
        {
            "heading": "4. BAYESIAN QUADRATURE MODELLING",
            "text": "We wish to estimate both the parameter posterior distribution p(\u0398|D,M) and the evidence p(D|M). We also wish to minimise the number of times that the likelihood `true(\u0398) must be queried, as this could be a computationally demanding operation in a more complex model. This problem requires a sample-efficient Bayesian inference solver. Bayesian quadrature (BQ) offers sample efficiency and solves for the posterior and the evidence in one go. This is a surrogate-model-based numerical integration approach, solving the integral as an inference problem by modelling the likelihood function `true(\u0398) with a Gaussian process (GP). Define `(\u0398) as the surrogate likelihood function modelled by a GP. The key result is that BQ can recast the problem of Bayesian inference into one of function approximation. The more accurately `(\u0398) can predict `true(\u0398), the more accurately the posterior and evidence can be estimated via replacing `true(\u0398) with `(\u0398) in Eqs. (13) - (14). To achieve this, Adachi et al. (2022) proposed BASQ, a discrete approximation of the kernel integral using a kernel recombination method (Hayakawa et al., 2022), yielding the following evidence computations:\nLEM := lnE\u03c0[`(\u0398)] \u2248 ln L\u2211 k Wk\u00b5f (Xk) + \u03b2, (20)\nLEV := lnVar\u03c0[`(\u0398)] \u2248 ln L\u2211 k,l WkWl\u03c3f (Xk, Xl) + 2\u03b2,\n(21)\nwhere LEM and LEV refer to log evidence mean and log evidence variance, \u00b5f and \u03c3f are the predictive mean and covariance of the likelihood surrogate model `(\u0398), \u03b2 is the scaling constant, Wk,Wl and Xk, Xl are the positive weights and point configurations discretised by the kernel recombination. Recall that LEM gives the degree of model fit and the LEV quantifies the uncertainty of the fit. However, the prior work on this (Adachi et al., 2022) assumed a narrower dynamic range of likelihood, whereas the battery model typically produces 10700 likelihood values. This is way beyond a typical numerical overflow limit. Thus, we improve here the prior work by adopting a four-layered warped GP method to accommodate the wide dynamic range of likelihood. (See Appendix B)"
        },
        {
            "heading": "5. IDENTIFIABILITY",
            "text": "To evaluate the model evidence as a model selection criterion, we compare results against three classical metrics related to identifiability: number of data points m, signal-tonoise ratio (SNR), and Jensen-Shannon divergence (JS). Owing to the integral-friendly model formulation, most parts of these can be calculated analytically. The number of data points is controllable here because data are synthetically generated and equispaced over log angular frequency space. Both SNR and JS are calculated using the imaginary part of the impedance. As the canonical form can be regarded as a mixture of hyperbolic secant distributions, such statistical analysis can be applied. While SNR evaluates the identifiability along the impedance magnitude axis, JS does so along the frequency axis."
        },
        {
            "heading": "5.1 Signal-to-noise ratio",
            "text": "The SNR is the log fraction of the impedance variance over the noise variance, representing how much the signal is more distinct than the noise, defined as:\nSNR := ln VarP (ln\u03c9)[Im[Z]]\n\u03c32noise . (22)\nLarger SNR means a more distinct and identifiable signal. The canonical form of the model provides an analytical form for the SNR (see derivation in Appendix C.2)."
        },
        {
            "heading": "5.2 Jensen-Shannon divergence",
            "text": "The JS divergence is a distance metric quantifying how one probability distribution Pi(x) is similar to a second reference probability distribution Pj(x), defined as:\nJS := 1\n2\n\u222b ln ( Pi(x)\nMij(x)\n) dPi(x)\n+ 1\n2\n\u222b ln ( Pj(x)\nMij(x)\n) dPj(x),\n(23)\nwhere\nMij(x) := 1\n2\n( Pi(x) + Pj(x) ) (24)\nAs the JS is defined for pairwise comparisons, the number of criteria required increases combinatorially per the number of RC pairs. For simplicity, we only consider the case of two RC pairs, which produces only one JS divergence. This represents how much the selected two peaks in the imaginary parts overlap. A smaller JS divergence means a more distinguishable and identifiable signal. While SNR is determined by the noise variance \u03c32noise and scaling factor rtotal, JS is dominated by the time constant difference \u2206\u03c4ij . Again, the canonical form helps solve the integration. Note that this is formulated as noise-free. The integration calculation procedure can be seen in Appendix C.3. The extended JS to include noise \u03c32noise is also guided, but the results shown in this paper are consistently used with noise-free formulation for simplicity."
        },
        {
            "heading": "6. NUMERICAL RESULTS",
            "text": ""
        },
        {
            "heading": "6.1 Selection criteria comparison",
            "text": "We now demonstrate our modified version of BASQ over several cases. We compare the model evidence metric\n(LEM and LEV 2 , eqs. (20) - (21)) with root-mean-square error (RMSE), Bayesian information criterion (BIC), and expected log predictive density (ELPD), based on the maximum a posteriori (MAP) parameter estimates, defined as:\n\u0398MAP := argmax `true(\u0398), (25)\nRMSE := \u221a\u221a\u221a\u221a 1 m m\u2211 j errj(\u03b8MAP), (26)\nBIC := d lnm\u2212 2 ln `true(\u0398MAP), (27)\nELPD := m\u2211 j ln \u222b `true(\u0398)dp(\u0398|D,M). (28)\nThe RMSE is a noise-free formulation that does not consider parameter uncertainty. BIC is an asymptotic approximation of evidence, so it cannot evaluate multimodal likelihoods. ELPD is a similar formulation to the log mean evidence, but the probability measure is changed from prior to posterior. The motivation behind ELPD is to estimate the alternative evidence from MCMC samples, as it cannot estimate evidence when solving Bayesian inference. However, it relies on Monte Carlo (MC) integration, which requires a significant amount of posterior samples, meaning that a plethora of model evaluations `true(\u0398) will run. All these alternative criteria were calculated from the BQ estimated posteriors by post-processing. Moreover, none of these criteria quantify their own uncertainty except BQ.\nWe demonstrate the behaviours of the selection criteria on two different datasets\u2014an easy case (\u2206\u03c4ij = 9.1, ln\u03c32noise = \u22129.97) with results in Table 1, and a hard case (\u2206\u03c4ij = 0.36, ln\u03c3 2 noise = \u22121.6) detailed in Table 2. The easy case is clean data generated with 2 well-separated semi-circles, and the hard case is noisy data generated with an additional third semi-circle with more overlap. The \u201dbetter\u201d column shows which upward or downward direction is better for each criterion. As expected, separated peaks (large \u2206\u03c4ij) and lower noise \u03c3 2 noise boost identifiability. While all criteria selected the true model in the easy case, only the evidence can select the true model in the hard case. 2 LEV values in the tables are standardised via subtracting 2\u03b2 from eq. (21) for a fair comparison between models.\nThe other metrics were unsuccessful in the hard case because of a multimodal posterior in the one RC pair model. As three RC pairs were used to generate the dataset, the posterior distribution of one RC pair parameter inevitably becomes multimodal, such as the peak intensity (\u03bbi). While the evidence correctly incorporates the multimodal distribution shape, RMSE and BIC consider only the largest peak. The BIC estimates the whole distribution from the local curvature at the maximum, which becomes erroneously overconfident in the multimodal case (Murphy, 2012). ELPD\u2019s failure could be due to its rough integral approximation. As the convergence rate of MC integration is O(1/ \u221a n), the posterior samples (n = 1,000) is too few. This means more model evaluations `true(\u0398) are required, which would not scale to slower simulation models.\nIn contrast, the evidence can be estimated simultaneously during training. Moreover, the variance of the evidence successfully points out the lower confidence in the one RC pair model in the hard case, suggesting multimodality. This uncertainty over the selection criterion could avoid overconfidence toward a simpler model. Moreover, the evidence variance in the hard case is generally higher than in the easy case. This also tells us that the hard case dataset is almost unidentifiable, suggesting we should not trust these comparisons. For instance, the evidence mean and ELPD for one RC pair in the easy case are much lower than in the hard case. However, the integral variance is the opposite. Thus, only this metric quantifies its own uncertainty, suggesting the dataset or model is less informative. A similar notion can be found in Jeffreys\u2019 scale for the Bayes factor (Jeffreys, 1998), which claims the evidence is not strong when the difference between the log evidence of two models is lower than 10. This explains that the hard case is unreliable, as the difference in the log evidence shows insufficient plausibility. Contrary to Jeffreys\u2019 scale, log evidence variance is self-contained and does not require the comparison of models. Instead, it can independently spot the unreliability of the estimation.\nIn such an uncertain case, a typical practice is Bayesian model averaging. Rather than selecting one definite model, we sample from a mixture of models with probability proportional to their mean evidence. Averaging can boost predictive accuracy and reduce the uncertainty over predictions, where only evidence offers this method. As such, while the easy cases do not require advanced methods, the evidence with self-check on reliability can assist in deciphering minor differences in hardly identifiable problems."
        },
        {
            "heading": "6.2 Sensitivity analysis",
            "text": "A sensitivity analysis of the evidence metric was performed. We generated 1,024 datasets using two-RC-pair models while varying the following five parameters; the\nnumber of data points m, the scaling factor rtotal, the first resistance r\u20321, the first time constant \u03c4 std 1 , and noise variance \u03c32noise. We calculated the SNR, JS, and the number of data points m for each dataset. In the first step of the analysis, we compared the linear correlations between the evidence estimates. Table 3 shows Pearson\u2019s correlation coefficients. This result aligns with our intuition\u2014for instance, larger data size m and SNR can boost the evidence LEM and confidence (inverse of LEV). However, while the large correlation of evidence with SNR is instinctive, the small correlation with JS is counterintuitive.\nThus, we further investigated the variance analysis via functional ANOVA (Hutter et al., 2014), which models a partition of a functional response according to the main effects and interactions of input parameters. This method can attribute each parameter sensitivity in a nonlinear manner. Table 4 illustrates that the most significant influence over the mean and variance of the evidence is the JS, contrary to the linear correlation results. This can be interpreted as meaning that a smaller JS divergence (more overlapped peaks) destabilises the evidence estimation, resulting in a more considerable variance. This viewpoint is supported by the relatively large negative correlation coefficient between JS and LEV.\nFurther insights can be obtained via residual analysis. The residual is defined as follows:\nZpred := slope\u00d7 BIC + intercept, (29) residual := (Zpred \u2212 logE\u03c0[\u00b5e(\u0398)])2 . (30)\nAs the BIC is an approximation of the LEM, the BIC and LEV have a linear relationship. While a linear regression model with BIC can predict log evidence mean reasonably, it fails to predict in hard cases, as shown in the section 6.1. Residual refers to the squared error between the BIC and log evidence mean. Table 4 shows that the residual is mainly caused by the JS divergence and less influenced by SNR or the number of data points m. This also suggests that the BIC cannot distinguish between the models with overlapped peaks, namely, a multimodal posterior."
        },
        {
            "heading": "6.3 Computation efficiency",
            "text": "Lastly, we compared the computation efficiency of our modified version of BASQ with the existing MCMC solvers elliptical slice sampling (ESS) (Murray et al., 2010) and dynamic nested sampling (Speagle, 2020). Note that amongst MCMC samplers, only nested sampling can estimate the evidence. For ESS, we approximated the evidence using ELPD via posterior samples. Therefore, the estimation with ESS should converge to a larger value than the\nactual evidence. The BASQ computation was performed using both CPU and GPU. 3\nFig. 2 compares the learning curve of the above four samplers versus computation time, using the easy case dataset shown in Table 1. While BASQ in a GPU converges at 18 seconds, BASQ in a CPU converges at 131 seconds. Both ESS and nested sampling do not converge in this time. Fig. 2 contrasts the sample efficiency of the samplers. As BASQ is a parallel sampler, we generate 100 samples per iteration. The sampling efficiency of BASQ does not change over computation modes and is the best of the selected solvers. This is expected\u2014while the convergence rate of BASQ is O(exp(\u2212cn1/d)) in the Gaussian case (Adachi et al., 2022), that of MCMC is O(1/ \u221a n). Furthermore, even this result does not fully represent BASQ\u2019s potential. While ECMs return model predictions in a millisecond order, more complex models (e.g. DFN model) take seconds to query. Therefore, BASQ for such complex models will be even more beneficial. Recent work shows even faster convergence than BASQ (Adachi et al., 2023)."
        },
        {
            "heading": "B.1 Four-layered BASQ formulation",
            "text": "The likelihood surrogate model `(\u0398) is defined as:\n`(\u0398) \u223c N (`;\u00b5`(\u0398), \u03c3`(\u0398)), (B.1) \u00b5`(\u0398) = K(\u0398,\u0398)K(\u0398,\u0398) \u22121`true(\u0398), (B.2)\n\u03c3`(\u0398,\u0398 \u2032) = K(\u0398,\u0398\u2032)\u2212K(\u0398,\u0398)K(\u0398,\u0398)\u22121K(\u0398,\u0398),\n(B.3)\nwhere `(\u0398) is the surrogate likelihood function modelled by GP, \u0398 is the \u2018observed parameter sets\u2019, and K is the kernel.\nGP is a non-parametric probabilistic model, typically applied to regression tasks in machine learning. GP can flexibly increase the model complexity in accordance with the number of data, thwarting under/over-confidence. GP model shape is determined by the data points \u0398 and the kernel K(\u0398,\u0398\u2032). The kernel maps the correlation between data points into a covariance matrix. Gaussianity of GP provides analytical predictive distribution `(\u0398), with predictive mean \u00b5`(\u0398) and covariance \u03c3`(\u0398,\u0398 \u2032), as\nTable B.1. Four-layered GPs and warped functions at each layer\nLayers e space f space g space h space\nCorrespondence likelihood normalised likelihood square-root norm. likelihood sqrt. norm. log likelihood\nWarp scaling square-root log base GP Forward e e/ exp\u03b2 \u221a 2(f \u2212 \u03b1) log(g + 1)\nBackward f exp\u03b2 \u03b1+ 1 2 g2 exp(h)\u2212 1 h\nGP e \u223c GP(\u00b5e, \u03c3e) f \u223c GP(\u00b5f , \u03c3f ) g \u223c GP(\u00b5g , \u03c3g) h \u223c GP(\u00b5h, \u03c3h)\nMean \u00b5f (x) exp\u03b2 \u03b1+ 1 2\n[ \u00b5g(x)2 + \u03c3g(x, x) ] exp [ \u00b5h(x) + 1 2 \u03c3h(x, x) ] \u00b5h(x)\nCovariance \u03c3f (x, y) exp(2\u03b2) 1 2 \u03c3g(x, y)2 + \u00b5g(x)\u03c3g(x, y)\u00b5g(y) \u00b5g(x)\u00b5g(y)[exp{\u03c3h(x, y)\u2212 1}] \u03c3h(x, y)\nshown in Eqs (B.2) - (B.3). While the predictive mean \u00b5`(\u0398) predicts the likelihood `true(\u0398), predictive covariance \u03c3`(\u0398,\u0398\n\u2032) predicts the uncertainty of the prediction at given \u0398. That is, training GP means minimising the predictive covariance over all possible parameters \u03c0(\u0398), namely, minimising \u222b\u222b \u03c7 \u03c3`(\u0398,\u0398 \u2032)d\u03c0(\u0398)d\u03c0(\u0398\u2032). Such training can be done via querying more observations from the true likelihood D\u0398 = {\u0398, `true(\u0398)}. Hence, the most straightforward training is to sample from the prior \u03c0(\u0398) until the integral variance becomes smaller than a convergence threshold. However, the prior often barely overlaps over the likelihood, resulting in observing unhelpful tiny likelihood values over most samples.\nTo overcome this problem, we consider sample-efficient training that fully exploits the information from GP. Osborne et al. (2012) showed that active learning sampling could efficiently reduce the number of samples. The active learning scheme guides the next query point to minimise the integral variance, exploiting the GP surrogate model information. A function called acquisition function formulated by predictive mean \u00b5`(\u0398) and covariance \u03c3`(\u0398,\u0398\n\u2032) can evaluate where to sample, and optimising it can locate where to sample next. Still, the overhead of the next query guidance is not negligible, and it is an inevitably sequential procedure. Adachi et al. (2022) proposed batch Bayesian quadrature, termed Bayesian Alternately Subsampled Quadrature (BASQ), permitting a lightweight active learning scheme and parallelisation of querying. They adopted the discretised sampling method (Hayakawa et al., 2022) for probability measure rather than an acquisition function. This allows us to query the true function in parallel. As the modern computational environment exploits an efficient parallel computation via a graphical processing unit or a computer cluster in the cloud, such computing power can accelerate inference computation. They demonstrated that BASQ could accelerate Bayesian inference over various synthetic and real-world datasets, including SPMe model inference.\nThe evidence can be calculated via kernel recombination. Kernel recombination is a discrete approximation of continuous kernel integral into weighted summation so as to minimise the integral variance, as such:\n\u222b Q \u03d5(x)dq(x) \u2248 P\u2211 p wp\u03d5(Xp),\nXp \u2208 X, wp \u2208W, X is the discretised samples over the probability measure, W is the positive weights to approximate integration. When we recall our training objective is to minimise the predictive covariance over the probability measure \u03c0(\u0398), this can be formulated as kernel recombination. Hence, we pass the predictive covariance \u03c3`(\u0398,\u0398\n\u2032) as kernel to the kernel recombination algorithm (Hayakawa et al., 2022), which yields the following approximation:\nX,W = recombination[\u03c3`(\u0398,\u0398 \u2032), \u03c0(\u0398)], E\u03c0[`(\u0398)] = \u222b \u03c7 \u00b5`(\u0398)d\u03c0(\u0398),\n\u2248 L\u2211 k Wk\u00b5`(Xk),\nVar\u03c0[`(\u0398)] = \u222b\u222b\n\u03c7\n\u03c3`(\u0398,\u0398 \u2032)d\u03c0(\u0398)d\u03c0(\u0398\u2032),\n\u2248 L\u2211 k,l WkWl\u03c3`(Xk, Xl),\nwhere Xk, Xl \u2208 X, Wk,Wl \u2208 W. However, they adopted square-root warping for fast computation, which assumed a narrow dynamic range in likelihood. Battery models\u2019 likelihood turns out to be very sharp, as the number of data points over the frequency range is typically over a hundred.\nTherefore, we adopted four-layered GPs to accommodate the dynamic range, permitting solving Bayesian inference even in this wide dynamic range case. Functions at each layer are summarised in Table B.1, where Ylog is the observed log-likelihood values, \u03b1 = min[exp(Ylog \u2212 \u03b2)], \u03b2 = max[Ylog]. e space corresponds to the original likelihood space. Square-root warping and log-warping layers are approximated via the moment-matching method (Gunter et al., 2014; Chai and Garnett, 2019). To accommodate the wide dynamic range, log transformation is widely applied in the BQ community. However, log-warped GP inevitably results in sampling from log space, leading to ineffective exploration. As meaningful samples from a very sharp likelihood are localised in only the vicinity of the maximum values, log space exploration is too blunt to\nexplore the original space. The combination of square-root warping and log-warping can overcome this issue using the following relationship:\nf = \u03b1+ 1 2 g2 \u2248 \u03b1+ 1 2 exp(h) exp(h),\nE\u03c0[\u00b5f (\u0398)] = \u03b1+ 1\n2 \u222b \u039e \u00b5g(\u0398)d\u03c0 \u2032(\u0398),\n\u03c0\u2032(\u0398) := \u00b5g(\u0398)\u03c0(\u0398).\nAs such, this doubly warping structure enables us to copy exponentiated function information to both likelihood and prior. Thus, this double structure can sample from sharp exponentiated distribution \u03c0\u2032(\u0398) as well as keep the surrogate model exponentiated \u00b5g(\u0398).\nThe last layer, e, exists to avoid overflow in computation by scaling the whole dynamic range via maximum value. This warping layer can be avoided as such:\nlogE\u03c0[\u00b5e(\u0398)] = logE\u03c0[\u00b5f (\u0398)] + \u03b2,\n\u2248 log L\u2211 k Wk\u00b5f (Xk) + \u03b2,\nlogVar\u03c0[\u03c3e(\u0398)] = logVar\u03c0[\u03c3f (\u0398)] + 2\u03b2,\n\u2248 log L\u2211 k,l WkWl\u03c3f (Xk, Xl) + 2\u03b2,\np(\u0398|D,M) = \u00b5e(\u0398)\u03c0(\u0398) E\u03c0[\u00b5e(\u0398)] = \u00b5f (\u0398)\u03c0(\u0398) E\u03c0[\u00b5f (\u0398)] ."
        },
        {
            "heading": "B.2 Training procedures",
            "text": "Training consists of four processes:\n(1) Subsampling from the exponentiated distribution (2) Kernel recombination for batch sampling (3) GP hyperparameter optimisation (4) Evidence estimation\nWe iterate the above four procedures until the evidence variance reaches plateau. Only the first training procedure is different from the original BASQ (Adachi et al., 2022).\nThe subsampling is to sample from the prior distribution to construct the empirical measure. As the kernel recombination is to select the sparse sample set from subsamples that can minimise the integral variance, subsamples should be sampled from prior but well overlapped from the higher predictive variance of GP `(x). Adachi et al. (2022) adopted uncertainty sampling for faster convergence, which samples from predictive variance \u03c3`(x) and corrected to prior distribution via importance sampling, as such:\ngprop(\u0398) := (1\u2212 r)\u00b5g(\u0398) + rA\u0303(\u0398), 0 \u2264 r \u2264 1 wIS(\u0398) := \u00b5g(\u0398)/gprop(\u0398),\nA\u0303(\u0398) := \u03c3g(\u0398)\u03c0 \u2032(\u0398)/ZA\u0303,\nZA\u0303 := \u222b \u039e \u03c3g(\u0398)d\u03c0 \u2032(\u0398),\n\u03c3g(\u0398) := diag [\u03c3g(\u0398,\u0398)] .\nWe wish to adopt the same strategy for a four-layered GP, but the log-warp layer hinders the application. The predictive variance of the original BASQ can be analytically translated into the mixture of Gaussian with Gaussian\nkernel because the squared Gaussian distribution is still Gaussian. However, the exponentiated Gaussian is no more Gaussian, which becomes a log-normal distribution. As such, we cannot take the same strategy which exploits the Gaussianity. Hence, we employ the heuristical method. The predictive variance is expected to be larger at the midpoints between the observed data points. Thus, sampling from the midpoints with half lengthscale of GP is expected to be good proposal distribution of sampling the uncertainty region, as such:\ngheur(\u0398) := Nheur\u2211 r,s wheurr,s N ( \u0398; \u0398midr,s , Wlength 2 ) ,\n\u0398midr,s := \u0398r + \u0398s\n2 ,\nwheurr,s := \u03c3g(\u0398\nmid r,s )\u03c0 \u2032(\u0398midr,s )\u2211Nheur r,s \u03c3g(\u0398 mid r,s )\u03c0 \u2032(\u0398midr,s ) ,\nwhere \u0398r,\u0398s \u2208 \u0398 are the observed parameters, Wlength is the diagonal covariance matrix whose diagonal elements are the lengthscales of each dimension. Supersampling from this offers the uncertainty sampling, as such:\n\u0398supert \u223c gheur(\u0398) \u2208 RNsuper ,\nZA\u0303 = \u222b \u03c3g(\u0398) \u03c0\u2032(\u0398)\ngheur(\u0398) dgheur(\u0398),\n\u2248 1 Nsuper Nsuper\u2211 t \u03c3g(\u0398 super t ) \u03c0\u2032(\u0398supert ) gheur(\u0398 super t ) ,\nwsuper := A\u0303(\u0398supert )/gheur(\u0398 super t ).\nSequential Monte Carlo (Kitagawa, 1993) permits to sam-\nple from A\u0303(\u0398)."
        },
        {
            "heading": "B.3 Ablation study of layered GPs",
            "text": "We discuss the efficacy of four-layered GP by comparing the results of evidence inference for the easy case introduced in Table 1. We compared the following six configurations in Table B.2. The ground truth of LEM is estimated via exhaustive nested sampling with millions of samples until convergence, which yields 703.7285. The ablation study shows that the four-layered GPs can estimate the most accurate LEV of all compared configurations. GPs without the scaling layer reached the overflow limit, which returned a positive infinite value. GPs without the logarithmic layer scored the lower log evidence mean because the surrogate model cannot accommodate the wide\ndynamic range. Scaled GP with only log warp results was the second best. However, the non-exponentiated prior struggled to find the MAP location. As such, the fourlayered GP, employing all features, was the performant.\nAppendix C. IDENTIFIABILITY DERIVATION\nC.1 Hyperbolic secant distribution identities\n\u222b \u221e \u2212\u221e\nsech (x) dx = \u03c0, (C.1)\u222b \u221e \u2212\u221e sech ( x\u2212 a b ) dx = \u03c0\nb , (C.2)\u222b \u221e\n\u2212\u221e sech (x) ln sech (x) dx = \u2212\u03c0 ln 2, (C.3)\u222b \u221e\n\u2212\u221e sech (x) sech (x\u2212 a) dx = 2acsch(a), (C.4)\u222b \u221e\n\u2212\u221e sech (x)\n2 dx = 2. (C.5)"
        },
        {
            "heading": "C.2 SNR derivation",
            "text": "SNR := ln VarP (ln\u03c9)[Im[Z]]\n\u03c32noise ,\nVarP (ln\u03c9)[Im[Z]] = EP (ln\u03c9)[Im[Z]2]\u2212 EP (ln\u03c9)[Im[Z]]2, EP (ln\u03c9)[Im[Z]] = \u222b\n\u2126\nIm[Z](ln\u03c9)dP (ln\u03c9),\n= exp(rtotal)\u03c0(1\u2212 r0)\n2(b\u2212 a) ,\nEP (ln\u03c9)[Im[Z]2] = exp(2rtotal)(1\u2212 r0)2\n2(b\u2212 a) A,\nwhere\nP (ln\u03c9) := U(ln\u03c9; a, b), a, b := min[ln\u03c9],max[ln\u03c9],\nA := N\u2211 i \u03bb2i + N\u2211 i,j 2\u03bbi\u03bbj\u2206\u03c4ijcsch(\u2206\u03c4ij),\n\u2206\u03c4ij := \u03c3\u03c9(\u03c4 std i \u2212 \u03c4 stdj ).\nEq. (C.2) yields the analytical solution of the first expectation: EP (ln\u03c9)[Im[Z]] = \u222b\n\u2126\nP (Im[Z]|\u03c9)dP (\u03c9),\n= exp(rtotal)\u03c0(1\u2212 r0)\n2(b\u2212 a)\nN\u2211 i=1\n\u03bbi \u03c0\u222b \u221e\n\u2212\u221e sech (\u03c9 + \u2206\u03c4ij) d\u03c9,\n= exp(rtotal)\u03c0(1\u2212 r0)\n2(b\u2212 a)\nN\u2211 i=1 \u03bbi,\n= exp(rtotal)\u03c0(1\u2212 r0)\n2(b\u2212 a) .\nEqs. (C.4) - (C.5) yield the analytical solution of the second expectation:\nEP (ln\u03c9)[Im[Z]2] = \u222b\n\u2126\nP (Im[Z]|\u03c9)2dP (\u03c9),\n= 1\nb\u2212 a\n[ exp(rtotal)\u03c0(1\u2212 r0)\n2 ]2 \u222b \u221e \u2212\u221e [ N\u2211 i=1 \u03bbi \u03c0 sech (\u03c9 + \u2206\u03c4ij) ]2 d\u03c9,\n= 1\nb\u2212 a\n[ exp(rtotal)\u03c0(1\u2212 r0)\n2 ]2 \u222b \u221e \u2212\u221e [ N\u2211 i \u03bb2i \u03c02 sech (\u03c9 + \u2206\u03c4ij) 2 + N\u2211 i,j 2\u03bbi\u03bbj \u03c02 sech (\u03c9) sech (\u03c9 + \u2206\u03c4ij)\n d\u03c9, = 1\nb\u2212 a\n[ exp(rtotal)\u03c0(1\u2212 r0)\n2 ]2  N\u2211 i 2\u03bb2i \u03c02 + N\u2211 i,j 4\u03bbi\u03bbj \u03c02 \u2206\u03c4ijcsch (\u2206\u03c4ij)\n , = exp(2rtotal)(1\u2212 r0)2\n2(b\u2212 a) N\u2211 i \u03bb2i + N\u2211 i,j 2\u03bbi\u03bbj\u2206\u03c4ijcsch (\u2206\u03c4ij)  ."
        },
        {
            "heading": "C.3 JS divergence derivation",
            "text": "Integral computation The JS divergence definition is as follows:\nJS := 1\n2 \u222b P ln ( Pi(x) Mij(x) ) dPi(x)\n+ 1\n2 \u222b P \u2032 ln ( Pj(x) Mij(x) ) dPj(x),\nwhere\nMij(x) := 1\n2\n( Pi(x) + Pj(x) ) To incorporate the information of weights, we adopt the following scaled hyperbolic secant distributions:\nPi(ln\u03c9) := \u03bbi \u03c0\nsech [ \u03bbi(ln\u03c9 + \u03c3\u03c9\u03c4 std i ) ] ,\nP \u2032j(ln\u03c9) := \u03bbj \u03c0\nsech [ \u03bbj(ln\u03c9 + \u03c3\u03c9\u03c4 std j ) ] ,\nwhere \u03c4 stdj > \u03c4 std i . For efficient computation of the integrals, we can adopt the importance sampling. For simplicity, we show the calculation of the first term, given by:\nfirst term = 1\n2 \u222b P Pi(x) gJS(x) ln Pi(x) Mij(x) dgJS(x),\n\u2248 1 2NIS NIS\u2211 q Pi(X IS q ) gJS(XISq ) ln Pi(X IS q ) Mij(XISq ) ,\nXISq \u223c gJS(x) \u2208 RNIS , where\ngJS(x) := 1\n2N N\u2211 i \u03bbi \u03c0 sech [ \u03bbi(x+ \u03c3\u03c9\u03c4 std i ) ] + 1\n4\u03c0 sech\n[ 0.5(x+ \u03c3\u03c9\u03bbi\u03c4 std i + 0.5\u2206ij) ] ,\n\u2206ij := \u03c3\u03c9|\u03bbj\u03c4 stdj \u2212 \u03bbi\u03c4 stdi |,\ngJS(x) is a proposal distribution. As the logarithmic term is a subtraction of two hyperbolic secant distributions, the peak is estimated around the overlapped area, namely the midpoint of the two peaks x + \u03c3\u03c9\u03bbi\u03c4 std i + 0.5\u2206ij . We can solve this integral via Monte Carlo integration. As sampling and evaluation of the probability density function of hyperbolic secant distribution are done within a millisecond order, computation with millions of samples for accuracy is not demanding.\nNoisy JS formulation The above computation assumes Pi(ln\u03c9) and Pj(ln\u03c9) probabilities are noise-free. In reality, the observed impedance is noisy, so we need to include the noise effect in the above formula to be more accurate. Note that the noise magnitude for impedance spectra is not \u03c32noise, but the exponentiated SNR. We assume the noisy distribution as P \u2032i (ln\u03c9), and the marginal probability can be obtained via marginalisation, as such:\nP \u2032i (ln\u03c9|\u03c32n) \u223c N ( P \u2032i ;Pi(ln\u03c9), \u03c3 2 n ) ,\nP \u2032i (ln\u03c9) = \u222b Q Pi(ln\u03c9|\u03c32n)dPi(\u03c32n),\nwhere\n\u03c32n = exp(SNR)\nPi(\u03c3 2 n) = LogNormal(\u03c3 2 n;\u00b5\u03c3, \u03c3\u03c3).\nWith regrad to the prior of \u03c32n, namely Pi(\u03c3 2 n), we can adopt the same prior in Section 4. That is, the prior for experimental noise is to extract the corresponding element in the prior \u03c0(\u0398). So, the JS divergence with noise can be calculated by swapping both Pi(x) and Pj(x) with marginal P \u2032i (x) and P \u2032 j(x)."
        }
    ],
    "title": "Bayesian Model Selection of Lithium-Ion Battery Models via Bayesian Quadrature ?",
    "year": 2023
}