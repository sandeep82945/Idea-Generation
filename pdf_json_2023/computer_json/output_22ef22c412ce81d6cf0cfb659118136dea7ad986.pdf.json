{
    "abstractText": "This work identifies a simple pre-training mechanism that leads to representations exhibiting better continual and transfer learning. This mechanism\u2014the repeated resetting of weights in the last layer, which we nickname \u201czapping\u201d\u2014was originally designed for a meta-continual-learning procedure, yet we show it is surprisingly applicable in many settings beyond both meta-learning and continual learning. In our experiments, we wish to transfer a pre-trained image classifier to a new set of classes, in a few shots. We show that our zapping procedure results in improved transfer accuracy and/or more rapid adaptation in both standard fine-tuning and continual learning settings, while being simple to implement and computationally efficient. In many cases, we achieve performance on par with state of the art meta-learning without needing the expensive higher-order gradients, by using a combination of zapping and sequential learning. An intuitive explanation for the effectiveness of this zapping procedure is that representations trained with repeated zapping learn features that are capable of rapidly adapting to newly initialized classifiers. Such an approach may be considered a computationally cheaper type of, or alternative to, meta-learning rapidly adaptable features with higher-order gradients. This adds to recent work on the usefulness of resetting neural network parameters during training, and invites further investigation of this mechanism.",
    "authors": [
        {
            "affiliations": [],
            "name": "IMPROVES CONTINUAL"
        },
        {
            "affiliations": [],
            "name": "Lapo Frati"
        }
    ],
    "id": "SP:c71e3c8287846c0321209e399f623f239b1620d1",
    "references": [
        {
            "authors": [
                "Ibrahim Alabdulmohsin",
                "Hartmut Maennel",
                "Daniel Keysers"
            ],
            "title": "The impact of reinitialization on generalization in convolutional neural networks",
            "venue": "arXiv preprint arXiv:2109.00267,",
            "year": 2021
        },
        {
            "authors": [
                "Shawn Beaulieu",
                "Lapo Frati",
                "Thomas Miconi",
                "Joel Lehman",
                "Kenneth O Stanley",
                "Jeff Clune",
                "Nick Cheney"
            ],
            "title": "Learning to continually learn",
            "venue": "arXiv preprint arXiv:2002.09571,",
            "year": 2020
        },
        {
            "authors": [
                "Yoshua Bengio",
                "Samy Bengio",
                "Jocelyn Cloutier"
            ],
            "title": "Learning a synaptic learning",
            "venue": "rule. Citeseer,",
            "year": 1990
        },
        {
            "authors": [
                "Yinbo Chen",
                "Zhuang Liu",
                "Huijuan Xu",
                "Trevor Darrell",
                "Xiaolong Wang"
            ],
            "title": "Meta-baseline: Exploring simple metalearning for few-shot learning",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Shibhansh Dohare",
                "A Rupam Mahmood",
                "Richard S Sutton"
            ],
            "title": "Continual backprop: Stochastic gradient descent with persistent randomness",
            "venue": "arXiv preprint arXiv:2108.06325,",
            "year": 2021
        },
        {
            "authors": [
                "Marie-Anne F\u00e9lix",
                "Andreas Wagner"
            ],
            "title": "Robustness and evolution: concepts, insights and challenges from a developmental model system",
            "venue": "Heredity,",
            "year": 2008
        },
        {
            "authors": [
                "Chrisantha Fernando",
                "Jakub Sygnowski",
                "Simon Osindero",
                "Jane Wang",
                "Tom Schaul",
                "Denis Teplyashin",
                "Pablo Sprechmann",
                "Alexander Pritzel",
                "Andrei Rusu"
            ],
            "title": "Meta-learning by the baldwin effect",
            "venue": "In Proceedings of the Genetic and Evolutionary Computation Conference Companion,",
            "year": 2018
        },
        {
            "authors": [
                "Chelsea Finn",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Frankle",
                "David J Schwab",
                "Ari S Morcos"
            ],
            "title": "The early phase of neural network training",
            "venue": "arXiv preprint arXiv:2002.10365,",
            "year": 2020
        },
        {
            "authors": [
                "Lapo Frati",
                "Neil Traft",
                "Nick Cheney"
            ],
            "title": "Omnimage: Evolving 1k image cliques for few-shot learning",
            "venue": "In Proceedings of the Genetic and Evolutionary Computation Conference, GECCO \u201923,",
            "year": 2023
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "A Steven Younger",
                "Peter R Conwell"
            ],
            "title": "Learning to learn using gradient descent",
            "venue": "In Artificial Neural Networks\u2014ICANN 2001: International Conference Vienna,",
            "year": 2001
        },
        {
            "authors": [
                "Elad Hoffer",
                "Itay Hubara",
                "Daniel Soudry"
            ],
            "title": "Fix your classifier: The marginal value of training the last weight layer",
            "venue": "arXiv preprint arXiv:1801.04540,",
            "year": 2018
        },
        {
            "authors": [
                "Khurram Javed",
                "Martha White"
            ],
            "title": "Meta-learning representations for continual learning",
            "venue": "arXiv preprint arXiv:1905.12588,",
            "year": 2019
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Ananya Kumar",
                "Aditi Raghunathan",
                "Robbie Matthew Jones",
                "Tengyu Ma",
                "Percy Liang"
            ],
            "title": "Fine-tuning can distort pretrained features and underperform out-of-distribution",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Brenden M Lake",
                "Ruslan Salakhutdinov",
                "Joshua B Tenenbaum"
            ],
            "title": "Human-level concept learning through probabilistic program induction",
            "year": 2015
        },
        {
            "authors": [
                "Xingjian Li",
                "Haoyi Xiong",
                "Haozhe An",
                "Cheng-Zhong Xu",
                "Dejing Dou"
            ],
            "title": "Rifle: Backpropagation in depth for deep transfer learning through re-initializing the fully-connected layer",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Seyed Iman Mirzadeh",
                "Mehrdad Farajtabar",
                "Hassan Ghasemzadeh"
            ],
            "title": "Dropout as an implicit gating mechanism for continual learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops,",
            "year": 2020
        },
        {
            "authors": [
                "Evgenii Nikishin",
                "Max Schwarzer",
                "Pierluca D\u2019Oro",
                "Pierre-Luc Bacon",
                "Aaron Courville"
            ],
            "title": "The primacy bias in deep reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Vijaya Raghavan T Ramkumar",
                "Elahe Arani",
                "Bahram"
            ],
            "title": "Zonooz. Learn, unlearn and relearn: An online learning paradigm for deep neural networks",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Sachin Ravi",
                "Hugo Larochelle"
            ],
            "title": "Optimization as a model for few-shot learning",
            "venue": "In International conference on learning representations,",
            "year": 2017
        },
        {
            "authors": [
                "Thimo Rohlf",
                "Christopher R Winkler"
            ],
            "title": "Emergent network structure, evolvable robustness, and nonlinear effects of point mutations in an artificial genome model",
            "venue": "Advances in Complex Systems,",
            "year": 2009
        },
        {
            "authors": [
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-.",
            "venue": "hook. PhD thesis, Technische Universita\u0308t Mu\u0308nchen,",
            "year": 1987
        },
        {
            "authors": [
                "Tushar Semwal",
                "Promod Yenigalla",
                "Gaurav Mathur",
                "Shivashankar B Nair"
            ],
            "title": "A practitioners\u2019 guide to transfer learning for text classification using convolutional neural networks",
            "venue": "In Proceedings of the 2018 SIAM international conference on data mining,",
            "year": 2018
        },
        {
            "authors": [
                "Ahmed Taha",
                "Abhinav Shrivastava",
                "Larry S Davis"
            ],
            "title": "Knowledge evolution in neural networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Dmitry Ulyanov",
                "Andrea Vedaldi",
                "Victor Lempitsky"
            ],
            "title": "Instance normalization: The missing ingredient for fast stylization",
            "venue": "arXiv preprint arXiv:1607.08022,",
            "year": 2016
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Charles Blundell",
                "Timothy Lillicrap",
                "Daan Wierstra"
            ],
            "title": "Matching networks for one shot learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Jason Yosinski",
                "Jeff Clune",
                "Yoshua Bengio",
                "Hod Lipson"
            ],
            "title": "How transferable are features in deep neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2014
        },
        {
            "authors": [
                "Sheheryar Zaidi",
                "Tudor Berariu",
                "Hyunjik Kim",
                "J\u00f6rg Bornschein",
                "Claudia Clopath",
                "Yee Whye Teh",
                "Razvan Pascanu"
            ],
            "title": "When does re-initialization work",
            "venue": "arXiv preprint arXiv:2206.10011,",
            "year": 2022
        },
        {
            "authors": [
                "Kaikai Zhao",
                "Tetsu Matsukawa",
                "Einoshin Suzuki"
            ],
            "title": "Retraining: A simple way to improve the ensemble accuracy of deep neural networks for image classification",
            "venue": "In 2018 24th International Conference on Pattern Recognition (ICPR),",
            "year": 2018
        },
        {
            "authors": [
                "Hattie Zhou",
                "Ankit Vani",
                "Hugo Larochelle",
                "Aaron Courville"
            ],
            "title": "Fortuitous forgetting in connectionist networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Frati"
            ],
            "title": "Similarly to the Omniglot case where each class contains very similar characters, the classes in OmnImage are selected to maximize within-class consistency by selecting the 20 most similar images per class via an evolutionary algorithm. See Appendix F.1, Figure 11 for a visual comparison of the datasets used, highlighting the similarity between Omniglot and OmnImage",
            "venue": "OmnImage (Frati et al.,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Biological creatures display astounding robustness, adaptability, and sample efficiency\u2014while artificial systems suffer catastrophic forgetting or struggle to generalize far beyond the distribution of their training examples. It has been observed in biological systems that repeated exposure to stressors can result in the evolution of more robust phenotypes (Rohlf & Winkler, 2009; F\u00e9lix & Wagner, 2008). However, it is not clear what type of stressor during the training of a neural network would most effectively, or efficiently, convey robustness and adaptability to that system at test time.\nIt is common practice to have the training of a machine learning system mimic the desired use-cases at test time as closely as possible. In the case of an image classifier, this would include drawing independent training samples from a distribution identical to the test set (i.i.d. training). When the test scenario is itself a learning process\u2014such as few-shot transfer learning from a limited number of novel examples\u2014training can include repeated episodes of rapid adaptation to small subsets of the whole dataset, thereby mimicking the test scenario. Meta-learning algorithms (learning to learn) for such contexts are able to identify patterns that generalize across individual learning episodes (Schmidhuber, 1987; Bengio et al., 1990; Hochreiter et al., 2001; Ravi & Larochelle, 2017; Finn et al., 2017).\nGoing even further, we are interested in challenging scenarios of online/continual learning with few examples per class. Recent work by Javed & White (2019) and Beaulieu et al. (2020) has tackled this challenging setting with the application of meta-learning. When the learning process itself is differentiable, one way to perform meta-optimization is by differentiating through several model updates using second-order gradients, as is done in MAML (Finn et al., 2017). When applied to episodes of continual learning, this approach introduces an implicit regularization strategy (it\nar X\niv :2\n31 0.\n07 99\n6v 1\n[ cs\n.L G\n] 1\n2 O\npenalizes weight changes that are disruptive to previously learned concepts, without needing a separate heuristic to quantify the disruption).\nOML (Javed & White, 2019) divide their architecture into two parts, where later layers are updated in a fast inner loop but earlier layers are only updated in a slower outer meta-update. Subsequently, ANML (Beaulieu et al., 2020) restructured the OML setup by moving the earlier meta-only layers into a parallel neuromodulatory path which metalearned an attention-like context-dependent multiplicative gating, achieving state of the art performance in sequential transfer.\nHowever, in this work we show that neither the asymmetric training of different parts of the network (like OML), nor a neuromodulatory path (like ANML), nor even expensive second-order updates (like both), are necessary to achieve equivalent performance in this setting. Instead, we reveal that the key contribution of the OML-objective appears to be a previously overlooked1 mechanism: a weight resetting and relearning procedure, which we call \u201czapping\u201d. This procedure consists of frequent random resampling of the weights leading into one or more output nodes of a deep neural network, which we refer to as \u201czapping\u201d those neurons.\nAfter reinitializing the last layer weights, the gradients for the weights in upstream layers quantify how the representation should have been different to reduce the loss given a new set of random weights. This is exactly the situation that the representation will find itself in during transfer. In the case where all weights of the last layer are reset, zapping closely matches transfer learning (when using the common technique of resetting the classifier layer(s) on top of pre-trained feature extraction layers; Yosinski et al. (2014)). Over multiple repetitions, this leads to representations which are more suitable for transfer.\nWe show that:\n\u2022 Dedicating second-order optimization paths for certain layers isn\u2019t necessary and doesn\u2019t explain the performance of meta-learning for continual transfer (Section 3.1). \u2022 The zapping forget-and-relearn mechanism accounts for the majority of the meta-learning improvements observed, and can be useful even without much more expensive higher-order gradients (Section 3.2). \u2022 Representations learned by models pre-trained using zapping are better for general transfer learning, not just continual learning (Section 3.3).\n2 Methods\nAs described in Javed & White (2019) and Beaulieu et al. (2020), we seek to train a model capable of learning a large number of tasks T1..n, in a few shots per task, with many model updates occurring as tasks are presented sequentially. Tasks Ti come from a common domain D. In our experiments we consider the domain to be a natural images dataset and tasks to be individual classes Ci in that dataset. Deep learning models learn hierarchical representations but the exact contribution of each individual level in the hierarchy is still under active research. Recent works show that reinitialization of layers during training can be used as a regularization method (Zhao et al., 2018; Li et al., 2020; Alabdulmohsin et al., 2021; Zhou et al., 2022). But there are several ways in which this reinitialization can be applied. We focus our attention on the last fully connected layer, right before the output. While the information value within this last layer may be marginal (Hoffer et al., 2018) interventions in the last layer will affect the gradient calculation of all the other layers in the network during backpropagation (see Figure 1).\nOur model consists of a small convolutional network with two main parts: a stack of convolutional layers that act as a feature extractor, and a single fully connected layer that acts as a linear classifier using the extracted features (see Appendix D, Figure 9). Our \u201czapping\" procedure consists of re-sampling all the connections corresponding to one of the output classes. Because of the targeted disruption of the weights, the model suddenly forgets how to map the features extracted by previous convolutional layers to the correct class. To recover from this sudden change, the model is shown examples from the forgotten class, one at a time. By taking several optimization steps, the model recovers from the negative effect of zapping. This procedure constitutes the inner loop in our meta-learning setup and is followed by an outer loop update using examples from all classes (Javed & White, 2019). The outer loop update is done with a higher-order gradient w.r.t. to the initial weights of that inner loop iteration (Finn et al., 2017). But, as we will see,\n1In the prior work, weight resampling was employed primarily as a method for maintaining large meta-gradients throughout meta-training and was deemed non-essential Javed & White (2019, V1\u2013Appendix A.1: Random Reinitialization)\nthese inner and outer updates do not actually need to be performed in a nested loop\u2014they can also be arranged in a flat sequence, yielding similar performance with much more efficient training."
        },
        {
            "heading": "2.1 Training Phases",
            "text": "Since we want to learn each class in just a few shots, it behooves us to start with a pre-trained model rather than starting tabula rasa. Therefore our problem set-up involves two stages. Within each stage, we examine multiple possible configuration options, described in more detail in the next sections.\n1. (Sec. 2.1.1) Pre-Training: We use one of the following algorithms to train on a subset of classes: (1) standard i.i.d. pre-training, (2) alternating sequential and batch learning (ASB), or (3) meta-learning through sequential and batch learning (meta-ASB). Each of these may or may not include the zapping procedure to forget and relearn. 2. (Sec. 2.1.2) Transfer: Following pre-training, we transfer the classifier to a separate subset of classes using (1) sequential transfer (continual learning) or (2) standard i.i.d. transfer (fine-tuning)."
        },
        {
            "heading": "2.1.1 Stage 1: Pre-Training",
            "text": "Our pre-training algorithm is described in Algorithm 1, and visualized in Appendix E, Figure 10. Our algorithm is based on the \u201conline-aware\u201d meta-learning (OML, Javed & White (2019)) procedure, which consists of two main components:\n\u2022 Adapting (inner loop; sequential learning): In this phase the learner is sequentially shown a set of examples from a single random class and trained using standard SGD. The examples are shown one at a time, so that the optimization performs one SGD step per image. \u2022 Remembering (outer loop; batch learning): After each adapting phase, the most recent class plus a random sample from all classes are used to perform a single outer-loop batch update. Those samples serve as a proxy of the true meta-loss (learning new things without forgetting anything already learned). The gradient update is taken w.r.t. the initial inner-loop parameters, and those updated initial weights are then used to begin the next inner-loop adaptation phase following the MAML paradigm (Finn et al., 2017).\nCompared to OML we use a different neural architecture (which improves classification performance; Appendix D, Figure 9), and do not draw a distinction between when to update the feature extraction vs. classification layers (updating both in the inner and outer loops). Furthermore, while the original OML procedure included both zapping and higher-order gradients, we ablate the effect of each component by allowing them to be turned on/off as follows.\nIn configurations with zapping (denoted as zap in Algorithm 1), prior to each sequential adaption phase on a single class Ci, the final layer weights corresponding to that class are re-initialized\u2014in other words, they are re-sampled from the initial weight distribution2. We call this procedure zapping as it destroys previously learned knowledge that was stored in those connections.\nIn the meta-learning conditions (denoted as meta in Algorithm 1), the \u201cremembering\u201d update is performed as an outer-loop meta-update on the initial weights of each inner-loop (as described above). However, we also wish to examine the effect of zapping independent of meta-learning, and introduce a new pre-training scenario in which we alternate between the adapting phase and the remembering phase. Different from meta-learning, this new method does not backpropagate through the learning process nor rewind the model to the beginning of the inner loop. Instead, it simply takes normal (non-meta) gradient update steps for each image/batch seen. The weights at the end of each sequence-and-batch are used directly on the next sequence.\nWe refer to this approach as Alternating Sequential and Batch learning (ASB), and the difference between ASB and meta-ASB can be seen visually in Fig. 10. This approach\u2014like Lamarckian inheritance rather than Darwinian evolution (Fernando et al., 2018)\u2014benefits from not throwing away updates within each inner-loop adaptation sequence, but loses the higher-order updates thought to be effective for these continual learning tasks (Javed & White, 2019; Beaulieu et al., 2020). This sequential approach allows us to employ the same zapping procedure as above: resetting the output node of the class which we are about to see a sequence of.\nWe also wished to study how zapping may influence the learning of generalizable features without being coupled with sequential learning. Thus, we also apply zapping to i.i.d. pre-training, which uses standard mini-batch learning with stochastic gradient descent.\n2Weights are sampled from the Kaiming Normal (He et al., 2015) and biases are set to zero.\nAlgorithm 1 Pre-Training: ASB and Meta-ASB, with or without zapping Require: Dataset D : C classes, N examples per class, (H, W, Ch) images Require: Network f : (H, W, Ch)\u2192 C with parameters \u03b8 : [\u03b8conv, \u03b8fc] Require: \u03b7in, \u03b7out inner and outer learning rates Require: K number of sequential inner-loop examples Require: R number of outer-loop \u201cremember\u201d examples Require: S number of outer-loop steps\n1: for iteration = 1, 2, . . . , S do \u25b7 outer loop; remembering 2: C \u223c D \u25b7 Sample one class 3: Xinner \u223c C \u25b7 K examples from class C 4: Xrand \u223c D \u25b7 R examples from the whole dataset 5: Xouter \u2190 Xinner \u222aXrand 6: if zap then 7: Reset connections in \u03b8fc corresponding to class C \u25b7 zapping 8: \u03b80 \u2190 \u03b8 9: for i = 0, . . . , K-1 do \u25b7 inner loop; adapting 10: y\u0302 \u2190 f(Xiinner; \u03b8i) 11: \u03b8i+1 \u2190 \u03b8i \u2212 \u03b7in\u2207\u03b8iL(y\u0302, C) \u25b7 single example SGD 12: if meta then 13: \u03b8 \u2190 \u03b80 \u2212 \u03b7out\u2207\u03b80L(f(Xouter; \u03b8K), Y ) \u25b7 meta batch SGD (expensive) 14: else 15: \u03b8 \u2190 \u03b8K \u2212 \u03b7out\u2207\u03b8KL(f(Xouter; \u03b8K), Y ) \u25b7 standard batch SGD (cheap)"
        },
        {
            "heading": "2.1.2 Stage 2: Transfer",
            "text": "We evaluate our pre-trained models using two different types of transfer learning. In sequential transfer (Appendix A, Alg. 2) the model is trained on a long sequence of different unseen classes (continual learning). Examples are shown one at a time, and a gradient update is performed for each image. While in i.i.d. transfer (Appendix A, Alg. 3) the model is trained on batches of images randomly sampled from unseen classes. In both transfer scenarios, the new classes were not seen during the pre-training phase. There are only 15-30 images per class (few-shot learning). Between the end of pre-training and transfer, the final linear layer of the model is replaced with a new, randomly initialized linear layer, so it can learn a new mapping of features to classes. Fine-tuning is allowed to update all weights in the model (not just the final layer). This setting where all layers are \u201cunfrozen\u201d creates ample opportunity for catastrophic forgetting. Both sequential and i.i.d. transfer use the same set of classes and images\u2014the only difference is how they are presented to the model."
        },
        {
            "heading": "3 Results",
            "text": "We evaluate two significantly different datasets, both in the few-shot regime.\nOmniglot (Lake et al. (2015); handwritten characters, 1600 classes, 20 images per class) is a popular dataset for few-shot learning. Its large number of classes allows us to create very long, challenging trajectories for testing catastrophic forgetting under continual learning. However, due to its simple imagery, it is possible to achieve high accuracy even by learning a linear classifier on top of an untrained network. Thus we also include a dataset consisting of more complex natural images.\nMini-ImageNet (Vinyals et al. (2016); natural images, 100 classes, 600 images per class) contains hundreds of images per class, but in transfer we limit ourselves to 30 training images per class. This allows us to test the common scenario where we are allowed a large, diverse dataset in our pre-training, but our transfer-to dataset is of the limited few-shot variety.\nFurthermore, we evaluate our method on OmnImage (Frati et al., 2023), which contains many classes like Omniglot but uses natural images like Mini-ImageNet. See Appendix F.\nFor each pre-training configuration (Meta-ASB / ASB / i.i.d. and with/without zapping; Section 2.1.1), we report the average performance across 30 trials for transfer/continual learning results (3 random pre-train seeds and 10 random transfer seeds). We sweep over three pre-training learning rates and seven transfer learning rates, and present the performance of the top performing learning rate for each configuration. All architectures are convnets (see Appendix D, Figure 9). We only evaluate the models at the end of training (i.e. no early stopping), but the number of epochs is\ntuned separately for each training method and dataset so as to avoid overfitting. See Appendix C for more details on pre-training and hyperparameters.\nHere we review the results on sequential transfer (Sec. 3.2) and i.i.d. transfer (Sec. 3.3), but in the Appendix we also evaluate cross-domain transfer (Appendix G) and show more comparisons of i.i.d. pre-training with different amounts of zapping (Appendix H)."
        },
        {
            "heading": "3.1 Neuromodulation is Not Necessary",
            "text": "Compared to previous work our Meta-ASB setup doesn\u2019t use heuristics on where/when in the model to apply optimization (like OML; Javed & White (2019)), nor context-dependent gating (like ANML; Beaulieu et al. (2020)), and uses fewer parameters than both prior works (see Appendix B). Despite the smaller architecture and simplified training regime used in our experiments, the continual learning performance is equal or better than both OML and ANML; see Figure 2 for a comparison against the ANML model in the setting where only One Layer is Fine-Tuned and the rest of the model is frozen (called OLFT in Beaulieu et al. (2020)). In the prior work, ANML achieved 63.8% accuracy after sequential learning of 600 classes. In our reimplementation, we show a slightly higher performance of 67% for both ANML and Meta-ASB.\nThis begs the question: if neither the separate optimization of inner-loop layers and outer-loop layers (as in both OML and ANML), nor neuromodulation (as in ANML) are necessary, then exactly what is it about these meta-learning algorithms that is contributing such drastic improvements? As we see from the solid green and red lines in Figure 2, the models trained without zapping\u2014even though they were trained with meta-learning\u2014show significantly lower performance (41.5% and 42.2% vs 67%).\nIn the following sections we will explore further how much both meta-learning and zapping contribute to classification performance in various settings. This is hinted at by the performance of Meta-ASB, ASB, and i.i.d. in Figure 2\u2014zapping consistently improves all of them. However, for all further experiments we will focus on the \u201cunfrozen\u201d setting, where all layers are fine-tuned during transfer, as a better representative of continual learning."
        },
        {
            "heading": "3.2 Continual Learning",
            "text": "We evaluate the pre-trained variants described in 2.1.1 on the sequential transfer task as described in Section 2.1.2. To quickly recap, the entire models are fine-tuned (no weights are frozen) on a few examples from classes not seen during pre-training, the examples are shown one at a time, and an optimization step is taken after each one. In all datasets, the meta-learned models with zapping significantly outperform their non-zapping counterparts, and outperform i.i.d. pre-training by an additional margin (Figure 3).\nOn Omniglot (Figure 4), the best method with zapping achieves state-of-the-art transfer-and-finetune-all-layers validation accuracy of 40.3%3 after continual learning on a sequence of 9000 gradient updates (15 updates on each of 600 classes). The best model without zapping achieves only 28.5%. In fact, when applying zapping to i.i.d. pre-training, we can even achieve better performance (32.3%) than the models which are meta-learned without zapping (28.3%). This\nsuggests that zapping may be an efficient and effective alternative (or complement) to higher-order gradients during pre-training.\nOn Mini-ImageNet (Figure 5), we again see a substantial difference between zapping models and their non-zapping counterparts (except for i.i.d.+zap). The same trends are more noticeable in the training accuracy, where the (zap \u2714, meta \u2714) model is able to retain nearly 100% of its training performance over 20 classes (600 image presentations), while the non-zapping models end up around 60%.\nIn Figure 3, we also include the pre-train validation accuracy: this is the validation accuracy of the pre-trained model on the pre-training dataset, before it was modified for transfer. We observe that ranking models by validation performance is not well correlated with ranking of transfer performance. This lack of pre-training/transfer ranking correlation introduces a dilemma, whereby we may not have a reliable way of judging which models will be better for transfer until we actually try them.\nAcross all three datasets, we have observed that:\n3As compared to the ANML-Unlimited & ANML-FT:PLN models from Beaulieu et al. (2020, SI, Figure S8). See Appendix B, Figure 8 for a direct comparison to ANML-Unlimited.\n1. Zapping is a significant driver of performance improvements (see dashed vs. solid lines per treatment in Figures 2, 4, and 5).\n2. Zapping sometimes also improves normal (pre-)training performance, although this trend is less consistent (Figure 3; more details in Tables 5 & 6 in the Appendix).\n3. Counter-intuitively, even the Alternating Sequential and Batch learning (ASB) sampling routine alone (without meta-gradients) appears to provide some benefits for transfer (see ASB vs i.i.d. in Figure 5). It may sometimes be unnecessary to use the much more expensive and complex higher-order gradients."
        },
        {
            "heading": "3.3 Transfer Learning",
            "text": "Although the zapping and meta-learning methods described in Algorithm 1 were originally designed to learn robust representations for continual learning, we show that they are beneficial for transfer learning in general. Here we feature the results of standard i.i.d. transfer, as described in Section 2.1.2. We train each model for five epochs of unfrozen fine-tuning on mini-batches of data using the Adam optimizer.\nFigure 6 shows results on Omniglot and Mini-ImageNet. As in the continual learning tests, here we also see substantial gains for the models employing zapping over those that do not. When zapping is not employed, models pre-trained with meta-gradients are comparable to those trained simply with standard i.i.d. pre-training. See Table 1 in Appendix A for a detailed comparison of final values.\nDespite both the zapping and ASB pre-training methods stemming from attempts to reduce catastrophic forgetting in continual learning settings, zapping consistently provides advantages over non-zapped models for all pre-training configurations on standard i.i.d. transfer learning. We hypothesize that these two settings\u2014continual and transfer learning\u2014share key characteristics that make this possible. Both cases may benefit from an algorithm which produces more adaptable, robust features that can quickly learn new information while preserving prior patterns that may help in future tasks."
        },
        {
            "heading": "4 Discussion",
            "text": "Across three substantially different datasets (see Appendix F for the third dataset, OmnImage), zapping consistently results in better representations for transferring to few-shot datasets, leading to better performance in both a continual learning and standard transfer setting. In many cases, we are still able to achieve the best performance by just applying zapping and alternating optimizations of sequential learning and batch learning (ASB) without applying any meta-gradients.\nWe even see some benefit from applying zapping directly to i.i.d. training, without any sequential learning component. This raises the question of whether we can match the performance of meta-learning using only zapping and standard i.i.d. training. However, this setting introduces new choices of when and where to reset neurons, since we are learning in batches and not just one class at a time. We include ablations in Appendix H that examine this question; in most cases, more zapping leads to better performance, but it is still outmatched by ASB. However, more investigation of resetting schedules and heuristics will likely lead to discoveries of better hyperparameters.\nIt is reasonable to suppose that the constant injection of noise by resetting weights during training helps to discover weights which are not as affected by this disruption, thus building some resilience to the shock of re-initializing layers. If the improved performance can be attributed to noise injections reducing the co-adaptation of layers (Yosinski et al., 2014), thus increasing their resilience, it begs the question of how it relates to other co-adaptation reducing mechanisms such as dropout, which is also shown to improve continual (Mirzadeh et al., 2020) and transfer (Semwal et al., 2018) learning.\nThe approaches explored here include pre-training by alternating between sequential learning on a single class and batches sampled from all pre-training classes (ASB), and resetting classifier weights prior to training on a new class (zapping). The information accumulated by repeating these simple methods across many tasks during the pre-training process mimics the condition experienced during transfer learning at test time. We thus argue that the results above demonstrate a simple yet effective version of meta-learning\u2014one without expensive meta-gradients to backpropagate through tasks."
        },
        {
            "heading": "5 Related work",
            "text": "As we have shown, the zapping operation of resetting last-layer weights provides clear performance improvements, but what about random weights enables this improved learning capability? The work of Frankle et al. (2020) investigates the dynamics of learning during the early stages of network training. They show that large gradients lead to substantial weight changes in the beginning of training, after which gradients and weight signs stabilize. Their work suggests that these initial drastic changes are linked to weight-level noise. Dohare et al. (2021) also investigate the relationship between noise and learning, showing that stochastic gradient descent is not enough to learn continually, and that this problem can be alleviated by repeated injections of noise. Rather than resetting classification neurons of the last layer, they choose weights to reset based on a pruning heuristic.\nThe reinitialization of weights in a neural network during training is an interesting emerging topic, with many other works investigating this phenomenon in a number of different settings. Like us, Zhao et al. (2018) periodically reinitialize the last layer of a neural network during training. Their focus is on ensemble learning for a single dataset, rather than transfer learning. Li et al. (2020) also periodically reinitialize the last layer, but they do it during transfer, rather than pre-training. Both Alabdulmohsin et al. (2021) and Zhou et al. (2022) investigated the idea of reinitialization of upper layers of the network, building upon the work of Taha et al. (2021). They show performance improvements in the few-shot regime. However, they they focus on learning of a single dataset rather than transfer learning. Zaidi et al. (2022) evaluate an extensive number of models to find under which circumstances reinitialization helps, although their work is also specific to training on a single dataset.\nNikishin et al. (2022) apply a similar mechanism to deep reinforcement learning. They find that periodically resetting the final layers of the Q-value network is beneficial across a broad array of settings. Some concurrent work (Ramkumar\net al., 2023) study the application of resetting to a version of online learning where data arrives in mega-batches. They employ resetting as a compromise between fine-tuning a pre-trained network and training a new network from scratch.\nOne major difference between these prior works and our investigation is the zapping + ASB routine, where we forget one class at a time and focus on relearning that class. This meta-learning-like setting often achieves substantial gains beyond i.i.d.+zapping, and gives us a new lens through which to view weight resetting."
        },
        {
            "heading": "6 Conclusion & Future Work",
            "text": "We have revealed the importance of \u201czapping\u201d for pre-training, and its connection to meta-learning. We have shown that zapping can lead to significant gains in transfer learning performance across multiple settings and datasets. The concept of forgetting and relearning has been investigated in other recent works, and our observations add to the growing evidence of the usefulness of this concept. We often see beneficial effects even without meta-learning or episodes of sequential learning, but the application of zapping to this setting is still relatively unexplored. Our work opens the door to further study and the discovery of ever more powerful uses of this zapping mechanism.\nAside from the benefits of zapping, our results highlight the disruptive effect of the re-initialization of last layers in general. Resetting of the final layer is routine in the process of fine-tuning a pre-trained model (Yosinski et al., 2014), but the impact of this \u201ctransfer shock\u201d is still not fully clear. For instance, it was only recently observed that fine-tuning in this way underperforms on out-of-distribution examples, and Kumar et al. (2022) suggest to freeze the lower layers of a network to allow the final network to stabilize. A deeper understanding of these mechanisms could significantly benefit many areas of neural network research.\nFinally, this work explores a simpler approach to meta-learning than meta-gradient approaches. It does so by repeatedly creating transfer shocks during pre-training, encouraging a network to learn to adapt to them. Future work should explore other methods by which we can approximate transfer learning during pre-training, how to influence the features learned to maximize transfer performance with the most computational efficiency, and how the benefits of zapping scale to larger models."
        },
        {
            "heading": "Acknowledgments",
            "text": "This material is based upon work supported by the Broad Agency Announcement Program and Cold Regions Research and Engineering Laboratory (ERDCCRREL) under Contract No. W913E521C0003, National Science Foundation under Grant No. 2218063, and Defense Advanced Research Projects Agency under Cooperative Agreement No. HR0011-18-2-0018. Computations were performed on the Vermont Advanced Computing Core supported in part by NSF Award No. OAC-1827314, and also by hardware donations from AMD as part of their HPC Fund. We would also like to thank Sara Pelivani for pointing out that the neuromodulatory network in ANML was not necessary, and for the interesting discussion that resulted from this observation."
        },
        {
            "heading": "A Transfer Learning Scenarios",
            "text": "Here we describe our two transfer learning test cases from Section 2.1.2 in more formal detail. Algorithm 2 describes continual learning (results Section 3.2). Algorithm 3 describes i.i.d. learning (results Section 3.3). In both cases, the last layer of the pre-trained network is replaced with a new layer, and all layers are updated during the transfer process.\nAlgorithm 2 Sequential Transfer Protocol (adapted from Beaulieu et al. (2020), Algorithm 2) Require: C \u2190 sequential trajectory of N unseen classes Require: \u03b8 \u2190 pre-trained weights of the network Require: \u03b2 \u2190 learning rate hyperparameter\n1: Strain = [ ] 2: for n = 1, 2, . . . , N do 3: Straj \u223c Cn \u25b7 get training examples from next class 4: Strain = Strain \u22c3 Straj \u25b7 add to sequential transfer train set 5: for i = 1, 2, . . . , k do 6: \u03b8 \u2190 \u03b8 \u2212 \u03b2\u2207\u03b8L(\u03b8, S(k)traj) \u25b7 SGD update on a single image 7: record L(\u03b8, Strain) \u25b7 eval current \u03b8 on classes trained so far 8: Stest = (\n\u22c3 1...n Ci)\u2212 Strain \u25b7 held-out examples from classes seen so far\n9: record L(\u03b8, Stest) \u25b7 eval current \u03b8 on held-out examples\nAlgorithm 3 I.I.D. Transfer Protocol Require: Dtr,Dte \u2190 training and held-out examples from N unseen classes from domain D Require: \u03b8 \u2190 pre-trained weights of the network Require: \u03b2 \u2190 learning rate hyperparameter Require: E \u2190 number of training epochs\n1: for i = 1, 2, . . . , E do 2: for i = 1, 2, . . . , N do \u25b7 N is the number of batches in Dtr 3: Bi \u223c Dtr \u25b7 uniformly sample next batch from domain Dtr without replacement 4: \u03b8 \u2190 \u03b8 \u2212 \u03b2\u2207\u03b8L(\u03b8,Bi) \u25b7 standard batch SGD update 5: record L(\u03b8,Dtr) \u25b7 eval current \u03b8 on all classes 6: record L(\u03b8,Dte) \u25b7 eval current \u03b8 on all held-out examples\nB Separate Weights for Inner and Outer Loops\nAs mentioned in Section 2, the meta-train phase is split between inner and outer loops. To incentivize the discovery of generalizable features the ANML and OML models train different parts of the model at different times. The last layer weights (pln in Figure 7a) are only updated in the inner loop while the whole network (rln + pln in Figure 7a) are updated only in the outer loop. The rationale of this choice is that the network can leverage meta-gradients in the outer loop to find features that can improve the inner loop.\nIn a similar fashion the ANML model only updates the neuromodulation weights (NM in Figure 7b) using metagradients in the outer loop, while instead the rest of the weights (rln + pln in Figure 7b). This design choice aims to break the symmetry of inner/outer loops and incentivize the outer loop to refine/integrate what was learned during the inner loop. See Table 2 for a comparison of the various methods.\nUnlike ANML and OML, our Convnet updates all weights in every phase. We remove the NM layers from the ANML model, yet perform just as well in one-layer fine-tuning (Figure 2) or better in unfrozen transfer (Figure 8). Our architecture is also shallower than OML (3 conv layers instead of 6)."
        },
        {
            "heading": "C Hyperparameters",
            "text": "We use the Adam optimizer (Kingma & Ba, 2014) with a standard cross-entropy loss. We train all models to convergence on all our datasets, and we take the final checkpoint as our pre-trained model for continual/transfer tests. In the i.i.d. pre-training setting, we do not have the single-class inner loop, so we must decide how often to zap in a different manner. We investigate the effect of zapping at different frequencies and a different number of classes (from a single one to all of them at once) to determine the optimal configuration (Appendix H).\nSee Tables 3 and 4 for a listing of pre-training hyperparameters used for our experiments.\n4In Meta-ASB, we train for 25,000 steps instead of 9,000. It is not usually necessary to train beyond ~18,000 steps, but we do typically need to train longer than non-meta-ASB, and we don\u2019t usually see any detriment in training longer than needed."
        },
        {
            "heading": "D Network Architecture",
            "text": "See Figure 9 for a depiction of the neural network architecture used in this work. The Mini-ImageNet and OmnImage datasets consist of images of size 84x84. For these, we use a typical architecture consisting of four convolutional blocks and one fully-connected final layer. Each convolutional block consists of: convolution, InstanceNorm (Ulyanov et al., 2016), ReLU activation, and max pool layers, in that order. All convolutional layers have 256 output channels. An architecture similar to this has been used to good effect for much exploratory research in few-shot learning\u2014in particular, we were inspired by \u201cFew-Shot Meta-Baseline\u201d (Chen et al., 2021).\nFor Omniglot, we use 28x28 single-channel images, and so the architecture is slightly different. Instead of four convolutional blocks, we use three. Also, we skip the final pooling layer."
        },
        {
            "heading": "E Alternating Sequential and Batch Learning",
            "text": "See Figure 10 for a visual depiction of the Alternating Sequential and Batch (ASB) learning procedure, and its meta-learning variant."
        },
        {
            "heading": "F OmnImage Dataset",
            "text": "While Mini-ImageNet contains more challenging imagery, it has far fewer classes than Omniglot, and thus cannot form a very long continual learning trajectory. To better test the effect of zapping in a continual learning setting that uses natural images, we test our models on a different subset of ImageNet with a shape similar to Omniglot (1000 classes, 20 images per class), called OmnImage (Frati et al., 2023). Similarly to the Omniglot case where each class contains very similar characters, the classes in OmnImage are selected to maximize within-class consistency by selecting the 20 most similar images per class via an evolutionary algorithm. See Appendix F.1, Figure 11 for a visual comparison of the datasets used, highlighting the similarity between Omniglot and OmnImage, and see Frati et al. (2023), for full details of the dataset.\nF.1 Dataset Comparison\nF.2 Continual Learning\nThe OmnImage dataset represents a difficult continual learning problem, since it consists of a long trajectory of complex natural images (300 test classes, 6000 total training images). Although all configurations struggle to achieve good accuracy on OmnImage, we see elevated transfer performance for zapping configurations (Figure 12b). Note that Figure 12 shows results when all the weights are updated during continual learning, like ANML-unlimited setting from Beaulieu et al. (2020).\nF.3 Transfer Learning\nThe effect of {Meta-}ASB on OmnImage is particularly striking (Figure 13) when performing standard transfer learning. The best testing performance with zapping (30.5%) is almost double the performance of standard i.i.d. pre-training (16.3%), after 5 epochs, and more importantly each zapping treatment outperforms their no-zapping counterparts (see Figures 6 & 13)."
        },
        {
            "heading": "G Cross-Domain Transfer",
            "text": "In the main text, we pre-train on a subset of classes from a dataset, and transfer to a unique disjoint subset of classes on the same dataset. But the typical use case for a pre-trained model is to transfer to a different dataset\u2014potentially even a significantly different domain. Here we test whether zapping provides benefits when transferring across datasets. We find that it does slightly boost performance in this setting, but the results or more mixed and more investigation is warranted.\nFigures 14 shows cross-domain i.i.d. transfer onto the Omniglot dataset, using models which were pre-trained on other datasets. We can see that, while the non-zapping models do catch up with sufficient training epochs, all the zapping models adapt to the new domain faster (all dashed lines are above their corresponding solid lines).\nFigure 15 instead shows the results of sequential transfer (i.e. training on a single image at a time). Zapping does help transfer from OmnImage to Omniglot, but not from Mini-Imagenet. In general, OmnImage seems to benefit most from zapping, both within-domain and across domains. We see this again in transferring OmnImage to Mini-ImageNet, shown in Figure 16.\nThus we see that models with zapping are often capable of faster adaptation in transferring to a new domain, and sometimes exhibit better final performance as well. However, by comparison to Figure 3 and 1, we can see that it is still generally beneficial to pre-train on a domain which is similar to the transferred domain, when possible. Zapping\ncan help improve performance of a given pre-training setting, but it does not fully overcome the effects of a large distribution shift."
        },
        {
            "heading": "H Performing Zapping in i.i.d. Pre-Training",
            "text": "During ASB, the model is presented with episodes of sequential learning on a single class, so we always reset the last layer weights corresponding to the class which is about to undergo sequential learning. In the i.i.d. case, there is no such \"single task training phase\". Instead, we choose to reset K classes (out of N total classes) on a cadence of once per E epochs. This introduces two new hyperparameters (K,E) for us to sweep over.\nHere we show results for all three datasets attempting different numbers of neurons for resets (differing values of K). We also tried resetting less often than once per epoch (E > 1), but resetting every epoch was typically better. In fact, in most cases below, the best scenario is to reset all last layer weights at the beginning of every epoch. Fine-tuning trajectories are shown in Figures 17, 18, and 19; and final performance is summarized in Tables 7 and 8.\nThese experiments show the potential for applying zapping to standard batch gradient descent. Our variants with more zapping generally see improved transfer, even though the effect is not as substantial as when it is paired with ASB. It stands to reason that there should be some point at which too much resetting becomes detrimental, and we have not tried to reset more often than once per epoch (E < 1) in the i.i.d. setting, so this would be a great starting point for future work."
        }
    ],
    "title": "RESET IT AND FORGET IT: RELEARNING LAST-LAYER WEIGHTS",
    "year": 2023
}