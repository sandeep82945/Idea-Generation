{
    "abstractText": "This paper considers the problem of inferring the causal effect of a variable Z on a dependently censored survival time T . We allow for unobserved confounding variables, such that the error term of the regression model for T is correlated with the confounded variable Z. Moreover, T is subject to dependent censoring. This means that T is right censored by a censoring time C, which is dependent on T (even after conditioning out the effects of the measured covariates). A control function approach, relying on an instrumental variable, is leveraged to tackle the confounding issue. Further, it is assumed that T and C follow a joint regression model with bivariate Gaussian error terms and an unspecified covariance matrix such that the dependent censoring can be handled in a flexible manner. Conditions under which the model is identifiable are given, a two-step estimation procedure is proposed, and it is shown that the resulting estimator is consistent and asymptotically normal. Simulations are used to confirm the validity and finite-sample performance of the estimation procedure. Finally, the proposed method is used to estimate the causal effect of job training programs on unemployment duration.",
    "authors": [
        {
            "affiliations": [],
            "name": "Gilles Crommen"
        },
        {
            "affiliations": [],
            "name": "Jad Beyhum"
        },
        {
            "affiliations": [],
            "name": "Ingrid Van Keilegom"
        }
    ],
    "id": "SP:2f66769e9f98544978638fbc6dbfe7210c60f4fa",
    "references": [
        {
            "authors": [
                "A. Abadie",
                "J. Angrist",
                "G. Imbens"
            ],
            "title": "Instrumental variables estimates of the effect",
            "year": 2002
        },
        {
            "authors": [
                "J.H. Aldrich",
                "F.D. Nelson"
            ],
            "title": "subsidized training on the quantiles of trainee earnings",
            "year": 1991
        },
        {
            "authors": [
                "A. Basu",
                "J. Ghosh"
            ],
            "title": "Identifiability of the multinormal and other distributions under",
            "venue": "Journal of the American Statistical Association,",
            "year": 1978
        },
        {
            "authors": [
                "J. Beyhum",
                "Florens",
                "J.-P",
                "I. Van Keilegom"
            ],
            "title": "A nonparametric instrumental approach to endogeneity in competing risks models. arXiv preprint arXiv:2105.00946",
            "year": 2021
        },
        {
            "authors": [
                "J. Beyhum",
                "Florens",
                "J.-P",
                "I. Van Keilegom"
            ],
            "title": "Nonparametric instrumental regression with right censored duration outcomes",
            "venue": "Journal of Business & Economic Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "J. Beyhum",
                "L. Tedesco",
                "I. Van Keilegom"
            ],
            "title": "Instrumental variable quantile regression under random right censoring",
            "venue": "arXiv preprint arXiv:2209.01429",
            "year": 2022
        },
        {
            "authors": [
                "G.E. Bijwaard",
                "G. Ridder"
            ],
            "title": "Correcting for selective compliance in a re-employment bonus experiment",
            "venue": "Journal of Econometrics,",
            "year": 2005
        },
        {
            "authors": [
                "G. Blanco",
                "X. Chen",
                "C.A. Flores",
                "A. Flores-Lagunes"
            ],
            "title": "Bounds on average and quantile treatment effects on duration outcomes under censoring, selection, and noncompliance",
            "venue": "Journal of Business & Economic Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "H.S. Bloom",
                "L.L. Orr",
                "S.H. Bell",
                "G. Cave",
                "F. Doolittle",
                "W. Lin",
                "J.M. Bos"
            ],
            "title": "The benefits and costs of jtpa title ii-a programs: Key findings from the national job training partnership act study",
            "venue": "The Journal of Human Resources,",
            "year": 1997
        },
        {
            "authors": [
                "R. Braekers",
                "N. Veraverbeke"
            ],
            "title": "A copula-graphic estimator for the conditional survival function under dependent censoring",
            "venue": "Canadian Journal of Statistics,",
            "year": 2005
        },
        {
            "authors": [
                "S. Centorrino",
                "Florens",
                "J.-P"
            ],
            "title": "Nonparametric estimation of accelerated failure-time models with unobservable confounders and random censoring",
            "venue": "Electronic Journal of Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "V. Chernozhukov",
                "I. Fern\u00e1ndez-Val",
                "A.E. Kowalski"
            ],
            "title": "Quantile regression with censoring and endogeneity",
            "venue": "Journal of Econometrics,",
            "year": 2015
        },
        {
            "authors": [
                "C. Czado",
                "I. Van Keilegom"
            ],
            "title": "Dependent censoring based on parametric copulas",
            "year": 2023
        },
        {
            "authors": [
                "N.W. Deresa",
                "I. Van Keilegom"
            ],
            "title": "Flexible parametric model for survival data subject to dependent censoring",
            "venue": "Biometrical Journal,",
            "year": 2020
        },
        {
            "authors": [
                "N.W. Deresa",
                "I. Van Keilegom"
            ],
            "title": "A multivariate normal regression model for survival data subject to different types of dependent censoring",
            "venue": "Computational Statistics & Data Analysis,",
            "year": 2020
        },
        {
            "authors": [
                "N.W. Deresa",
                "I. Van Keilegom"
            ],
            "title": "On semiparametric modelling, estimation and inference for survival data subject to dependent censoring",
            "year": 2020
        },
        {
            "authors": [
                "T. Emura",
                "Chen",
                "Y.-H"
            ],
            "title": "Analysis of survival data with dependent censoring: CopulaBased",
            "year": 2018
        },
        {
            "authors": [
                "J.C. Escanciano",
                "D. Jacho-Ch\u00e1vez",
                "A. Lewbel"
            ],
            "title": "Identification and estimation of semiparametric two-step models",
            "venue": "Quantitative Economics,",
            "year": 2016
        },
        {
            "authors": [
                "B.R. Frandsen"
            ],
            "title": "Treatment effects with censoring and endogeneity",
            "venue": "Journal of the American Statistical Association,",
            "year": 2015
        },
        {
            "authors": [
                "J.J. Heckman"
            ],
            "title": "Sample selection bias as a specification error",
            "year": 1979
        },
        {
            "authors": [
                "X. Huang",
                "N. Zhang"
            ],
            "title": "Regression survival analysis with an assumed copula for dependent",
            "year": 2008
        },
        {
            "authors": [
                "E.L. Kaplan",
                "P. Meier"
            ],
            "title": "censoring: A sensitivity analysis approach",
            "year": 1958
        },
        {
            "authors": [
                "S. Khan",
                "E. Tamer"
            ],
            "title": "Inference on endogenously censored regression models using",
            "venue": "Journal of the American Statistical Association,",
            "year": 2009
        },
        {
            "authors": [
                "J. Li",
                "J. Fine",
                "A. Brookhart"
            ],
            "title": "Instrumental variable additive hazards models",
            "year": 2015
        },
        {
            "authors": [
                "T. Martinussen",
                "S. Vansteelandt"
            ],
            "title": "Instrumental variables estimation with competing",
            "year": 2020
        },
        {
            "authors": [
                "L. Oxley",
                "M. McAleer"
            ],
            "title": "Econometric issues in macroeconomic models with generated",
            "year": 1993
        },
        {
            "authors": [
                "L.F. Richardson"
            ],
            "title": "Ix. the approximate arithmetical solution by finite differences",
            "year": 1911
        },
        {
            "authors": [
                "Rivest",
                "L.-P",
                "M.T. Wells"
            ],
            "title": "A martingale approach to the copula-graphic estimator for",
            "venue": "Physical Character,",
            "year": 2001
        },
        {
            "authors": [
                "J.M. Robins",
                "D.M. Finkelstein"
            ],
            "title": "Correcting for noncompliance and dependent censoring",
            "year": 2000
        },
        {
            "authors": [
                "P.H.C. Sant\u2019Anna"
            ],
            "title": "Program evaluation with right-censored data",
            "year": 2016
        },
        {
            "authors": [
                "S. Sperlich"
            ],
            "title": "A note on non-parametric estimation with predicted variables",
            "year": 2009
        },
        {
            "authors": [
                "N. Staplin",
                "A. Kimber",
                "D. Collett",
                "P. Roderick"
            ],
            "title": "Dependent censoring in piecewise exponential survival models",
            "venue": "Statistical Methods in Medical Research,",
            "year": 2015
        },
        {
            "authors": [
                "A. Sujica",
                "I. Van Keilegom"
            ],
            "title": "The copula-graphic estimator in censored nonparametric location-scale regression models",
            "venue": "Econometrics and Statistics,",
            "year": 2018
        },
        {
            "authors": [
                "G. Tauchen"
            ],
            "title": "Diagnostic testing and evaluation of maximum likelihood models",
            "venue": "Journal of Econometrics,",
            "year": 1985
        },
        {
            "authors": [
                "E.J. Tchetgen Tchetgen",
                "S. Walter",
                "S. Vansteelandt",
                "T. Martinussen",
                "M. Glymour"
            ],
            "title": "Instrumental variable estimation in a survival context",
            "venue": "Epidemiology (Cambridge,",
            "year": 2015
        },
        {
            "authors": [
                "A. Tsiatis"
            ],
            "title": "A nonidentifiability aspect of the problem of competing risks",
            "venue": "Proceedings of the National Academy of Sciences - PNAS,",
            "year": 1975
        },
        {
            "authors": [
                "J.M. Wooldridge"
            ],
            "title": "Econometric Analysis of Cross Section and Panel Data",
            "year": 2010
        },
        {
            "authors": [
                "J.M. Wooldridge"
            ],
            "title": "Control function methods in applied econometrics",
            "venue": "The Journal of human resources,",
            "year": 2015
        },
        {
            "authors": [
                "C. Zheng",
                "R. Dai",
                "P.N. Hari",
                "M. Zhang"
            ],
            "title": "Instrumental variable with competing risk model. Statistics in Medicine, 36(8):1240\u20131255",
            "year": 2017
        },
        {
            "authors": [
                "M. Zheng",
                "J.P. Klein"
            ],
            "title": "Estimates of marginal survival for dependent competing risks based on an assumed copula",
            "year": 1995
        },
        {
            "authors": [
                "Basu",
                "Ghosh"
            ],
            "title": "1978) shows that the right hand side does not go",
            "year": 1978
        },
        {
            "authors": [
                "Basu",
                "Ghosh"
            ],
            "title": "1978), the right hand side of the equation can only be equal to 0, 1 or \u221e. It follows that this argument can be replicated when \u03c021 < 0. Case 6: Two of \u03c0jk, with j, k = 1, 2 are equal",
            "year": 1978
        }
    ],
    "sections": [
        {
            "text": "Keywords: dependent censoring, causal inference, instrumental variable, control function, survival analysis."
        },
        {
            "heading": "1 Introduction",
            "text": "When estimating the effect of a variable Z on a censored survival time T , unmeasured confounding can be a possible source of bias. Under certain assumptions, the instrumental variable (IV) approach allows us to negate this bias without having to observe any of the confounding variables. Within survival analysis, IV methods have recently been receiving increased attention to estimate causal effects on censored outcomes. However, almost all of these approaches assume that the censoring time is (conditionally) independent of the survival time. In this work, we propose an IV method that can identify causal effects while allowing for dependent censoring. Precisely, let T depend log-linearly on a vector of observed covariates X, a confounded variable Z and some error term, denoted by uT , which represents unobserved heterogeneity. There is a confounding issue when Z and uT are correlated. A common example is when Z is a non-randomized treatment variable,\nar X\niv :2\n20 8.\n04 18\n4v 3\n[ m\nat h.\neven after conditioning on the covariates X. This correlation with the error term implies that the causal effect of Z on T cannot be identified from the conditional distribution of T on (X,Z). Further, we introduce a right censoring mechanism by way of the censoring time C, such that only the minimum of T and C is observed through the follow-up time Y = min{T,C} and the censoring indicator \u2206 = 1(T \u2264 C). We do not assume that T and C are independent, even after conditioning on (X,Z). This possible dependence creates an additional statistical issue since the distribution of T cannot be recovered from that of (Y,\u2206) without further assumptions."
        },
        {
            "heading": "1.1 Approach",
            "text": "The confounding issue is tackled by the control function approach. This method uses an instrumental variable W\u0303 and the observed covariates X to split uT into two parts, one which is correlated with Z and one which is not. It is important that W\u0303 is a valid instrumental variable for Z, which means that W\u0303 is independent of uT , sufficiently correlated with Z and only affects T through Z. The part of uT that is correlated with Z is the control function V , which is assumed to be linear in uT . This control function, also denoted by g\u03b3 , is a parametric function of (Z,X, W\u0303 ) that is assumed to capture all confounding. Note that the parameter \u03b3 is unknown. In this work, we propose two possible control functions for which the form follows from the relation specified between Z and (X, W\u0303 ). Moreover, the control function allows us to estimate the causal effect of Z on T from the conditional distribution of T on (X,Z, V ).\nTo allow for dependent censoring, we let T and C, conditional on (X,Z, V ), follow a joint Gaussian regression model with an unspecified covariance matrix. The need for this assumption is explained in Section 2.1, where it is formally introduced. Moreover, it is shown that the model is identifiable, which means that we can identify not only the causal effect of Z on T but also the association parameter between T and C. This can be seen as surprising, since we only observe the minimum of T and C through the follow-up time Y and the censoring indicator \u2206. In order to estimate the model parameters, a two-step estimation method is proposed. The first step estimates the parameter \u03b3, which is required to construct the regressor V . Therefore, this control function V can also be seen as a generated regressor. The second step uses maximum likelihood to estimate parameters of interest such as the correlation between T and C and the causal effect of Z on T . Note that the second step uses the generated regressor V , such that a correction for the randomness coming from the first step needs to be applied to get asymptotically valid standard errors. To implement this correction, we treat the two steps as a joint generalized method of moments estimator with their moment conditions stacked in one vector. This allows us to prove consistency and asymptotic normality of the parameter estimates. Using various simulation settings, we show that the estimator demonstrates excellent finite sample performances. We illustrate the procedure by evaluating the effect of federally funded job training services on unemployment duration in the United States."
        },
        {
            "heading": "1.2 Related literature",
            "text": "This paper is firstly related to the literature on dependent censoring. In the survival analysis literature, it is usually assumed that the survival time T is independent of the right censoring time C, which is called independent censoring. However, it is easy to think of situations where this assumption is not a reasonable one to make. A common example of the independent censoring assumption being doubtful can be found in transplant studies. The survival time (time to death) is likely dependent on the censoring time (time to transplant), since selection for transplant is based on the patient\u2019s medical condition. In this case we would expect a positive dependence between\nT and C, as usually the most ill patients are selected for transplant (Staplin et al., 2015). In the literature, many methods have been proposed to handle dependent censoring. An important result comes from Tsiatis (1975), who proved that it is impossible to identify the joint distribution of two failure times by their minimum in a fully nonparametric way. Because of this, more information about the dependence and/or marginal distributions of T and C is needed to identify their joint distribution. The most popular approaches are based on copulas, and Zheng and Klein (1995) were the first to apply this idea. Under the assumption of a fully known copula for the joint distribution of T and C, a nonparametric estimator of the marginals was proposed. This estimator is called the copula-graphic estimator, which extends the Kaplan and Meier (1958) estimator to the dependent censoring case. Rivest and Wells (2001) further investigated the copula-graphic estimator for Archimedean copulas. Note that both of these methods rely on a completely known copula. In particular, this means that the association parameter specifying the dependence between T and C is assumed to be known, which is often not the case in practice. The copula methods were extended to include covariates by Braekers and Veraverbeke (2005), Huang and Zhang (2008) and Sujica and Van Keilegom (2018) among others. Nevertheless, these methods still rely on a fully known copula. More recently a new method was proposed by Czado and Van Keilegom (2023), which does not require the association parameter to be known. As a trade-off, this requires the marginals to be fully parametric for the association parameter to be identifiable. Deresa and Van Keilegom (2020c) and Deresa and Van Keilegom (2020a) propose a semiparametric and parametric transformed joint regression model respectively, where the transformed variables T and C follow a bivariate normal distribution after adjusting for covariates. Deresa and Van Keilegom (2020b) extends the parametric transformed joint regression model to allow for different types of censoring. The present paper relies on a similar Gaussian model as Deresa and Van Keilegom (2020a), but nevertheless differs from it as we allow for confounding. Therefore, our method can be seen as a generalization of the one proposed by Deresa and Van Keilegom (2020a). The added complication comes from the generated regressor that is introduced by the control function.\nNext, the present work falls within the instrumental variable and control function literature. A confounding issue could occur due to a multitude of reasons such as noncompliance (Angrist et al., 1996), sample selection (Heckman, 1979), measurement error or omitting relevant variables. The control function approach used in this work has been discussed extensively in the literature on confounding and endogeneity by Lee (2007), Navarro (2010) and Wooldridge (2015) among others. The idea is that adding an appropriate parametric control function to the regression, which is estimated in the first stage using a valid instrument, solves the confounding issue. The advantages of this approach are that it is computationally simple, and that it can handle complicated models that are nonlinear in the confounded variable in a parsimonious manner. It is interesting to note that using the control function method creates a generated regressor problem. See Pagan (1984), Oxley and McAleer (1993) and Sperlich (2009) for an overview of possible methods and issues raised when using generated regressors. Moreover, Escanciano et al. (2016) look at a general framework for two-step estimators with a non-parametric first step. In this work, they consider the example of a control function estimator for a binary choice model with an endogenous regressor.\nFinally, the last string of research linked to this paper is that of instrumental variable methods for right censored data. We first discuss methods assuming that the censoring mechanism is independent. Some papers follow a nonparametric approach assuming that both Z and W\u0303 are categorical: Frandsen (2015), Sant\u2019Anna (2016) and Beyhum et al. (2022a). Other approaches are semiparametric, such as Bijwaard and Ridder (2005), Li et al. (2015), Tchetgen Tchetgen et al. (2015), Chernozhukov et al. (2015) and Beyhum et al. (2022b) among others. Note that Tchetgen Tchetgen et al. (2015) also propose a control function approach. Centorrino and Florens (2021) study nonparametric estimation with continuous regressors. Confounding has also been\ndiscussed in a competing risks framework by Richardson et al. (2017), Zheng et al. (2017), Martinussen and Vansteelandt (2020) and Beyhum et al. (2021). Research on confounding within a dependent censoring framework is sparse. Firstly, Robins and Finkelstein (2000) look at a correction for noncompliance and dependent censoring. However, they make the strong assumption that conditional on the treatment arm and the recorded history of six time-dependent covariates, C does not further depend on T . It is clear that this assumption is violated if there is a variable affecting both T and C that is not observed. Secondly, Khan and Tamer (2009) discuss an endogenously censored regression model, but they make a strong assumption (IV2, page 110 in Khan and Tamer (2009)) regarding the relationship between the instruments and the covariates. An example of this assumption being violated is when the support of the natural logarithm of C given Z and X is the whole real line, which is allowed for in our model. Finally, Blanco et al. (2020) look at treatment effects on duration outcomes under censoring, selection, and noncompliance. However, they derive bounds on the causal effect of Z on T instead of point estimates."
        },
        {
            "heading": "1.3 Outline",
            "text": "This paper is structured as follows: Section 2 specifies the model to be studied and describes some distributions such that the expected log-likelihood can be defined. Section 3.1 derives the identification results and Section 3.2 outlines the estimation procedure. Section 3.3 shows consistency and asymptotic normality for the estimator described in Section 3.2. Section 3.4 describes how the asymptotic variance can be estimated. The technical details for the three theorems outlined in Section 3 can be found in Appendix C. Simulation results and an empirical application regarding the impact of Job Training Partnership Act (JTPA) programs on time until employment are described in Sections 4 and 5 respectively. The code used for both of these sections can be found on https://github.com/GillesCrommen."
        },
        {
            "heading": "2 The model",
            "text": ""
        },
        {
            "heading": "2.1 Model specification",
            "text": "Let T and C be the logarithm of the survival time and the censoring time respectively. Contrary to the usual approach in survival analysis, the survival and censoring time are not assumed to be independent, even after conditioning on the measured covariates. Because T and C censor each other, only one of them is observed through the follow-up time Y = min{T,C} and the censoring indicator \u2206 = 1(T \u2264 C). The measured covariates that have an influence on both T and C are given by X = (1, X\u0303>)> and Z, where X\u0303 and Z are of dimension m and 1 respectively. We are interested in estimating the causal effect of Z on T , which is denoted by \u03b1T . A structural joint regression model can be specified as follows:{\nT = X>\u03b2T + Z\u03b1T + uT C = X>\u03b2C + Z\u03b1C + uC , (1)\nwhere Z is a confounded variable and (uT , uC) are error terms representing the unobserved heterogeneity. As mentioned in the introduction, there is a confounding issue when Z and (uT , uC) are correlated. This is the case when Z, conditional on X, is non-randomized (e.g. a treatment indicator when there is non-compliance). Moreover, assuming Z \u22a5\u22a5 (uT , uC), where \u22a5\u22a5 denotes statistical independence, would lead to biased estimates of the causal effect \u03b1T since E[ZuT ] 6= 0. Note that Deresa and Van Keilegom (2020a) assume Z \u22a5\u22a5 (uT , uC), with (uT , uC) bivariate Gaussian.\nWe tackle this problem by using the control function approach, an IV method that has been discussed by Lee (2007), Navarro (2010) and Wooldridge (2015) among others. The idea is to split the unobserved confounding variables uT and uC into two parts, one which is correlated with Z and one which is not. The part of uT and uC that is correlated with Z is the control function V, where it is assumed that (uT , uC) is related to V through a linear model. This can be written as\nuT = \u03bbTV + T and uC = \u03bbCV + C ,\nwith (\u03bbT , \u03bbC) \u2208 R2 and ( T , C) \u22a5\u22a5 Z such that only V is correlated to Z. If V were known, we could estimate the causal effect \u03b1T by applying the method of Deresa and Van Keilegom (2020a) to the following model: {\nT = X>\u03b2T + Z\u03b1T + V \u03bbT + T C = X>\u03b2C + Z\u03b1C + V \u03bbC + C . (2)\nHowever, in practice, the regressor V is unknown and needs to be estimated. To do this, we start by introducing a valid scalar instrument for Z, which is denoted by W\u0303 . By valid instrument, it is meant that: (i) W\u0303 \u22a5\u22a5 (uT , uC), (ii) W\u0303 is sufficiently correlated with Z and (iii) W\u0303 only affects (T,C) through Z. Further, we define W = (X>, W\u0303 )> for ease of notation. The instrument allows us to isolate the endogenous variation in Z. This is because, by definition, V needs to be the part of Z that does not depend on W . We achieve this by specifying a mapping g\u03b3 , depending on a parameter \u03b3, such that V = g\u03b3(Z,W ) is indeed the part of Z that does not depend on W. Note that the function g follows from the reduced form (which is specified by the analyst), but the parameter \u03b3 is unknown and needs to be estimated. We discuss specific examples and choices of g in Section 2.2. Moreover, it is assumed that:\n(A1) ( T C ) \u223c N2 (( 0 0 ) , \u03a3 = ( \u03c32T \u03c1\u03c3T\u03c3C \u03c1\u03c3T\u03c3C \u03c3 2 C )) ,\nwith \u03a3 positive definite (\u03c3T , \u03c3C > 0 and |\u03c1| < 1).\n(A2) ( T , C) \u22a5\u22a5 (W,Z).\n(A3) The covariance matrix of (X\u0303>, Z, V ) is full rank and Var(W\u0303 ) > 0.\n(A4) The probabilities P(Y = T | W,Z) and P(Y = C | W,Z) are both strictly positive almost surely.\nWhile Assumptions (A2), (A3) and (A4) are commonly made within a survival or instrumental variable context, Assumption (A1) is not. As mentioned in the introduction, Tsiatis (1975) proved that it is impossible to identify the joint distribution of two failure times by their minimum in a fully nonparametric way. Because of this result, we will need to make some assumptions regarding the dependence and/or marginal distributions of T and C in order to identify their joint distribution. When dependent censoring is still present after conditioning on the covariates, there are two common approaches that can be considered. The first one consists of specifying a fully known copula for the joint distribution of T and C, while leaving the marginals unspecified (see Emura and Chen (2018) for more details). This means that the association parameter, which describes the dependence between T and C, is assumed to be known. As this is often not the case in practice, we opt to use a different method. At the cost of using the fully parametric model that follows from Assumption (A1), it will later be shown by Theorem 1 that we can actually identify the association parameter \u03c1. We deem this to be an acceptable price to pay, as there is no good way of choosing the association parameter in practice."
        },
        {
            "heading": "2.2 Control function",
            "text": "But how do we specify the control function? In the literature, different control functions are proposed which depend on a model for the relationship between Z and W . Following Wooldridge (2010) and Navarro (2010), we give two examples of possible control functions that will be used throughout the paper. Consider first the case where Z is a continuous random variable and the relation between Z and W follows a linear model, that is\nZ = W>\u03b3 + \u03bd with E[\u03bdW ] = 0, (3)\nwhere \u03bd is an unobserved error term and \u03b3 \u2208 Rm+2. In this setting it is natural to set V = g\u03b3(Z,W ) = Z \u2212W>\u03b3 such that V is the confounded part of Z, that is, the part that does not depend on W . Another, more involved, example is when Z is a binary random variable and the relation between Z,W and \u03bd is specified as\nZ = 1(W>\u03b3 \u2212 \u03bd > 0) with \u03bd \u22a5\u22a5W. (4)\nSince we cannot directly separate \u03bd from Z and W , we let\nV = g\u03b3(Z,W ) = Z E[\u03bd |W>\u03b3 > \u03bd] + (1\u2212 Z)E[\u03bd |W>\u03b3 < \u03bd]. (5)\nThen, the function g is known, up to \u03b3, when the distribution of \u03bd is known. This specification of the control function is discussed and justified in Section 19.6.1 and Section 21.4.2 by Wooldridge (2010). If \u03bd \u223c N(0, 1) or \u03bd follows a standard logistic distribution, we have a probit or logit model for Z respectively. Specific expressions of g for the probit and logit model can be found in Appendix A. Moreover, when Z is binary, Tchetgen Tchetgen et al. (2015) give another example of a possible control function:\nV = Z \u2212 P(Z = 1 |W ).\nNote that throughout the paper, we will use V and g\u03b3(Z,W ) interchangeably."
        },
        {
            "heading": "2.3 Useful distributions and definitions",
            "text": "Using the assumptions that have been made so far, some conditional distributions and densities are derived. They are useful in proving the identification theorem and to define the estimator in Section 3. The expected log-likelihood function is also defined.\nFor a given \u03b8 = (\u03b2T , \u03b1T , \u03bbT , \u03b2C , \u03b1C , \u03bbC , \u03c3T , \u03c3C , \u03c1) > and \u03b3, we define FT |W,Z(\u00b7 | w, z, \u03b3; \u03b8) and FC|W,Z(\u00b7 | w, z, \u03b3; \u03b8) as the conditional distribution function of T and C given W = w = (x>, w\u0303)> and Z = z, respectively. Thanks to Assumptions (A1) and (A2), we have that:\nFT |W,Z(t | w, z, \u03b3; \u03b8) = \u03a6 ( t\u2212 x>\u03b2T \u2212 z\u03b1T \u2212 g\u03b3(z, w)\u03bbT\n\u03c3T\n) ,\nFC|W,Z(c | w, z, \u03b3; \u03b8) = \u03a6 ( c\u2212 x>\u03b2C \u2212 z\u03b1C \u2212 g\u03b3(z, w)\u03bbC\n\u03c3C\n) ,\nwith \u03a6 the cumulative distribution function of a standard normal variable. It follows that for a given \u03b3 and \u03b8, the conditional density functions of T and C given W = w and Z = z are, respectively:\nfT |W,Z(t | w, z, \u03b3; \u03b8) = \u03c3T\u22121\u03c6 ( t\u2212 x>\u03b2T \u2212 z\u03b1T \u2212 g\u03b3(z, w)\u03bbT\n\u03c3T\n) ,\nfC|W,Z(c | w, z, \u03b3; \u03b8) = \u03c3C\u22121\u03c6 ( c\u2212 x>\u03b2C \u2212 z\u03b1C \u2212 g\u03b3(z, w)\u03bbC\n\u03c3C\n) ,\nwhere \u03c6 is the density function of a standard normal variable. For ease of notation, define bC = y\u2212x>\u03b2C\u2212z\u03b1C\u2212g\u03b3(z, w)\u03bbC and bT = y\u2212x>\u03b2T \u2212z\u03b1T \u2212g\u03b3(z, w)\u03bbT . The sub-distribution function FY,\u2206|W,Z(\u00b7, 1 | w, z, \u03b3; \u03b8) of (Y,\u2206) given (W,Z) and (\u03b3, \u03b8) can be derived as follows:\nFY,\u2206|W,Z(y, 1 | w, z, \u03b3; \u03b8) = P(Y \u2264 y,\u2206 = 1 |W = w,Z = z) = P(Y \u2264 y, T \u2264 C |W = w,Z = z) = P( T \u2264 bT , bC \u2212 bT + T \u2264 C).\nThis expression is equivalent to\u222b bT \u2212\u221e P( C \u2265 bC \u2212 bT + e | T = e)f T (e) de.\nSince ( C | T = e) \u223c N ( \u03c1\u03c3C\u03c3T e, \u03c3 2 C(1\u2212 \u03c12) ) and T \u223c N(0, \u03c32T ), it follows that\nfY,\u2206|W,Z(y, 1 | w, z, \u03b3; \u03b8) = 1\n\u03c3T\n[ 1\u2212 \u03a6 ( bC \u2212 \u03c1\u03c3C\u03c3T bT \u03c3C(1\u2212 \u03c12)1/2 )] \u03c6 ( bT \u03c3T ) .\nUsing the same arguments, it can be shown that\nfY,\u2206|W,Z(y, 0 | w, z, \u03b3; \u03b8) = 1\n\u03c3C\n[ 1\u2212 \u03a6 ( bT \u2212 \u03c1\u03c3T\u03c3C bC \u03c3T (1\u2212 \u03c12)1/2 )] \u03c6 ( bC \u03c3C ) .\nSince P(Y \u2264 y) = P(T \u2264 y) + P(C \u2264 y)\u2212 P(T \u2264 y, C \u2264 y),\nwe have that\nFY |W,Z(y | w, z, \u03b3; \u03b8) = \u03a6 ( bT \u03c3T ) + \u03a6 ( bC \u03c3C ) \u2212 \u03a6 ( bT \u03c3T , bC \u03c3C ; \u03c1 ) ,\nwhere \u03a6(\u00b7, \u00b7, \u03c1) is the distribution function of a bivariate normal distribution with covariance matrix( 1 \u03c1 \u03c1 1 ) . Further, let\nS = (Y,\u2206, X\u0303, W\u0303 , Z) with distribution function G on G = R\u00d7 {0, 1} \u00d7 Rm \u00d7 R\u00d7 R,\nand ` : G \u00d7 \u0393\u00d7\u0398\u2192 R : (s, \u03b3, \u03b8) 7\u2192 `(s, \u03b3, \u03b8) = logfY,\u2206|W,Z(y, \u03b4 | w, z, \u03b3; \u03b8),\nwhere \u0398 \u2282 {\u03b8 : (\u03b2T , \u03b1T , \u03bbT , \u03b2C , \u03b1C , \u03bbC) \u2208 R2m+6, (\u03c3T , \u03c3C) \u2208 R2>0, \u03c1 \u2208 (\u22121, 1)} is the parameter space of \u03b8 and \u0393 the parameter space of \u03b3 (usually \u0393 \u2282 Rm+2). The expected log-likelihood can be defined as follows:\nL(\u03b3, \u03b8) = E [ `(S, \u03b3, \u03b8) ] = \u222b G `(s, \u03b3, \u03b8) dG(s)."
        },
        {
            "heading": "3 Model identification and estimation",
            "text": ""
        },
        {
            "heading": "3.1 Identification of the model",
            "text": "We will start by showing that model (2) is identifiable in the sense that two different values of the parameter vector (\u03b3, \u03b8) result in two different distributions of S. Let (\u03b3\u2217, \u03b8\u2217) denote the true parameter vector. In order to prove the identifiability of the model, it will be assumed that:\n(A5) \u03b3\u2217 is identified.\nConsidering again the examples from Section 2.2, when Z is a continuous random variable for which (3) holds, it is well known that the assumption that the covariance matrix of (X\u0303, W\u0303 ) is full rank implies Assumption (A5). When Z is a binary random variable for which (4) holds, the assumption that the covariance matrix of (X\u0303, W\u0303 ) is full rank together with a known distributional assumption on \u03bd (e.g. \u03bd \u223c N(0, 1) or \u03bd \u223c Logistic(0, 1)) implies Assumption (A5) as shown by Manski (1988).\nTheorem 1. Under Assumptions (A1)-(A5), suppose that (T1, C1) and (T2, C2) satisfy model (2) with (\u03b3, \u03b8) and (\u03b3\u2217, \u03b8\u2217) as parameter vectors respectively. If fY1,\u22061|W,Z(\u00b7, k | w, z, \u03b3; \u03b8) \u2261 fY2,\u22062|W,Z(\u00b7, k | w, z, \u03b3\u2217; \u03b8\u2217) for almost every (w, z), then\n\u03b3 = \u03b3\u2217 and \u03b8 = \u03b8\u2217.\nThe proof of the theorem can be found in Appendix C. It is based on the proof of Theorem 1 by Deresa and Van Keilegom (2020a). The fact that the proposed joint regression model is identifiable can be seen as surprising, since this means that we can identify the relationship between T and C while only observing their minimum through the follow-up time Y and the censoring indicator \u2206."
        },
        {
            "heading": "3.2 Estimation of the model parameters",
            "text": "We consider estimation when the data consist of an i.i.d. sample {Yi,\u2206i,Wi, Zi}i=1,...,n. Further, it is assumed that:\n(A6) There exists a known function m : (w, z, \u03b3) \u2208 Rm+2\u00d7R\u00d7\u0393 7\u2192 m(w, z, \u03b3) twice continuously differentiable with respect to \u03b3 such that the estimator\n\u03b3\u0302 \u2208 arg max \u03b3\u2208\u0393 n\u22121 n\u2211 i=1 m(Wi, Zi, \u03b3), (6)\nis consistent for the true parameter \u03b3\u2217.\nUsing the first-order conditions of program (6), we obtain that n\u22121 \u2211n\ni=1\u2207\u03b3m(Wi, Zi, \u03b3\u0302) = 0. Hence, Assumption (A6) implies that we possess a consistent Z-estimator of \u03b3. The theory on M - estimators (Newey and McFadden, 1994) allows us to find sufficient conditions for the assumption that \u03b3\u0302 is consistent. Assumption (A6) will hold when (i) the true parameter \u03b3\u2217 belongs to the interior of \u0393, which is compact, (ii) L(\u03b3) = E [ m(W,Z, \u03b3) ] is continuous and uniquely maximized\nat \u03b3\u2217 and (iii) L\u0302(\u03b3) = n\u22121 \u2211n\ni=1m(Wi, Zi, \u03b3) converges uniformly (in \u03b3 \u2208 \u0393) in probability to L(\u03b3). In the case where L\u0302(\u00b7) is concave, (i) can be weakened to \u03b3\u2217 being an element of the interior of a convex set \u0393, while (iii) is only required to hold pointwise rather than uniformly. Returning again to the examples given in Section 2.2, when Z is a continuous random variable for which (3) holds, it is well known that ordinary least squares is an extremum estimation method that consistently estimates \u03b3 under the assumption that the covariance matrix of (X\u0303, W\u0303 ) is full rank. In this case, we\ncan define m(W,Z, \u03b3) = \u2212(Z \u2212W>\u03b3)2. When Z is a binary random variable for which (4) holds and the distribution of \u03bd is known, maximum likelihood estimation can be used to consistently estimate \u03b3 under weak regularity conditions that can be found in Aldrich and Nelson (1991). In this case, we can define m(W,Z, \u03b3) = Z logP(W>\u03b3 > \u03bd)+(1\u2212 Z) logP(W>\u03b3 < \u03bd). After obtaining \u03b3\u0302 from (6), the parameters from model (2) can be estimated using maximum likelihood with the estimates given by the second-step estimator:\n\u03b8\u0302 = (\u03b2\u0302T , \u03b1\u0302T , \u03bb\u0302T , \u03b2\u0302C , \u03b1\u0302C , \u03bb\u0302C , \u03c3\u0302T , \u03c3\u0302C , \u03c1\u0302) = arg max \u03b8\u2208\u0398 L\u0302(\u03b3\u0302, \u03b8), (7)\nwith \u0398 the parameter space as defined before and\nL\u0302(\u03b3\u0302, \u03b8) = 1\nn n\u2211 i=1 logfY,\u2206|W,Z(Yi,\u2206i |Wi, Zi, \u03b3\u0302; \u03b8)\n= 1\nn n\u2211 i=1\n{ \u2206i ( \u2212 log(\u03c3T ) + log [ 1\u2212 \u03a6 ( bCi \u2212 \u03c1 \u03c3C \u03c3T bTi\n\u03c3C(1\u2212 \u03c12)1/2\n)] + log [ \u03c6 ( bTi \u03c3T )])\n+ (1\u2212\u2206i) ( \u2212 log(\u03c3C) + log [ 1\u2212 \u03a6 ( bTi \u2212 \u03c1 \u03c3T \u03c3C bCi\n\u03c3T (1\u2212 \u03c12)1/2\n)] + log [ \u03c6 ( bCi \u03c3C )])} ,\nwith bCi = Yi \u2212X > i \u03b2C \u2212 Zi\u03b1C \u2212 g\u03b3\u0302(Wi, Zi)\u03bbC ,\nbTi = Yi \u2212X > i \u03b2T \u2212 Zi\u03b1T \u2212 g\u03b3\u0302(Wi, Zi)\u03bbT ."
        },
        {
            "heading": "3.3 Consistency and asymptotic normality",
            "text": "In this section, it will be shown that the parameter estimates \u03b8\u0302, as defined in (7), are consistent and asymptotically normal. Theorems 2 and 3 show consistency and asymptotic normality respectively. The proofs can be found in Appendix C. We start by providing some definitions and assumptions that will be useful in stating these theorems. Let\nh`(S, \u03b3 \u2217, \u03b8\u2217) = \u2207\u03b8`(S, \u03b3\u2217, \u03b8\u2217), H\u03b8 = E [ \u2207\u03b8h`(S, \u03b3\u2217, \u03b8\u2217) ] , hm(W,Z, \u03b3 \u2217) = \u2207\u03b3m(W,Z, \u03b3\u2217), H\u03b3 = E [ \u2207\u03b3h`(S, \u03b3\u2217, \u03b8\u2217) ] ,\nM = E [ \u2207\u03b3hm(W,Z, \u03b3\u2217) ] , \u03a8 = \u2212M\u22121hm(W,Z, \u03b3\u2217),\nh\u0303(S, \u03b3\u2217, \u03b8\u2217) = ( hm(W,Z, \u03b3 \u2217)>, h`(S, \u03b3 \u2217, \u03b8\u2217)> )> , H = E [ \u2207\u03b3,\u03b8h\u0303(S, \u03b3\u2217, \u03b8\u2217) ] .\nThe following assumptions will be used in the proofs of Theorems 2 and 3:\n(A7) The parameter space \u0398 is compact and \u03b8\u2217 belongs to the interior of \u0398.\n(A8) There exists a function D(s) integrable with respect to G and a compact neighborhood N\u03b3 \u2286 \u0393 of \u03b3\u2217 such that |`(s, \u03b3, \u03b8)| \u2264 D(s) for all \u03b3 \u2208 N\u03b3 and \u03b8 \u2208 \u0398. (A9) E [ \u2016h\u0303(S, \u03b3\u2217, \u03b8\u2217)\u20162 ] <\u221e and E [ sup\n(\u03b3,\u03b8)\u2208N\u03b3,\u03b8 \u2016\u2207\u03b3,\u03b8h\u0303(S, \u03b3, \u03b8)\u2016\n] <\u221e, with N\u03b3,\u03b8 a neighborhood\nof (\u03b3\u2217, \u03b8\u2217) in \u0393\u00d7\u0398.\n(A10) H>H is nonsingular.\nNote that \u2016\u00b7\u2016 represents the Euclidean norm. Assumption (A8) is necessary to show the consistency and asymptotic normality of the parameter estimates. Sufficient conditions for this assumption are that the support of S is bounded, \u0393 being compact and Assumption (A7). Assumptions (A7), (A9) and (A10) are regularity conditions that are commonly made in a maximum likelihood context. We have the following consistency theorem.\nTheorem 2. Under Assumptions (A1)-(A8), suppose that \u03b3\u0302 and \u03b8\u0302 are parameter estimates as described in (6) and (7) respectively, then\n\u03b3\u0302 p\u2212\u2192 \u03b3\u2217 and \u03b8\u0302 p\u2212\u2192 \u03b8\u2217.\nThe challenge in proving this theorem comes from the fact that we are using a two-step estimation method, meaning that the results from the first step are used in the second step. To ensure consistency of \u03b8\u0302, in the proofs, we show uniform convergence (in \u03b8 \u2208 \u0398) in probability of the empirical likelihood function L\u0302(\u03b3\u0302, \u03b8) in (7) to the true likelihood of the model at \u03b3\u2217. We also have the following asymptotic normality result:\nTheorem 3. Under Assumptions (A1)-(A10), suppose that \u03b8\u0302 is a parameter estimate as described in (7), then\n\u221a n(\u03b8\u0302 \u2212 \u03b8\u2217) d\u2212\u2192 N(0,\u03a3\u03b8),\nwith \u03a3\u03b8 = H \u22121 \u03b8 E [ {h`(S, \u03b3\u2217, \u03b8\u2217) +H\u03b3\u03a8}{h`(S, \u03b3\u2217, \u03b8\u2217) +H\u03b3\u03a8}> ]( H\u22121\u03b8 )> .\nThe difficulty in proving this theorem is related to the fact that the randomness coming from the first step inflates the asymptotic variance of \u03b8\u0302. Hence, ignoring the first step would lead to inconsistent standard errors and confidence intervals that are not asymptotically valid. To obtain correct standard errors, we treat the two steps as a joint generalized method of moments (GMM) estimator with their moment conditions stacked in one vector (Newey and McFadden, 1994). Indeed, given that \u03b3\u0302 and \u03b8\u0302 are consistent by Theorem 2, they are the unique solutions to the first order conditions of their respective objective functions in a neighborhood of \u03b3\u2217 and \u03b8\u2217 (with probability going to 1). Therefore, the two-step estimator is asymptotically equivalent to the GMM estimator corresponding to the following moments:\nE[hm(W,Z, \u03b3)] = 0 and E[h`(S, \u03b3, \u03b8)] = 0,\nfor the first and second step respectively. As a last remark, if we were to remove the correction H\u03b3\u03a8 for the first step, the covariance matrix simplifies to the inverse of Fisher\u2019s information matrix (assuming the model is correctly specified)."
        },
        {
            "heading": "3.4 Estimation of the asymptotic variance",
            "text": "Using the result from Theorem 3, we can construct a consistent estimator \u03a3\u0302\u03b8 for the covariance matrix of the parameters in \u03b8 in the following way:\n\u03a3\u0302\u03b8 = H\u0302 \u22121 \u03b8\n[ n\u22121\nn\u2211 i=1 {h`(Si, \u03b3\u0302, \u03b8\u0302) + H\u0302\u03b3\u03a8\u0302i}{h`(Si, \u03b3\u0302, \u03b8\u0302) + H\u0302\u03b3\u03a8\u0302i}> ]( H\u0302\u22121\u03b8 )> ,\nwhere Si = ( Yi,\u2206i, X\u0303i, W\u0303i, Zi ) and\nh`(Si, \u03b3\u0302, \u03b8\u0302) = \u2207\u03b8`(Si, \u03b3\u0302, \u03b8\u0302), hm(Wi, Zi, \u03b3\u0302) = \u2207\u03b3m(Wi, Zi, \u03b3\u0302), H\u0302\u03b8 = n\u22121 n\u2211 i=1 \u2207\u03b8h`(Si, \u03b3\u0302, \u03b8\u0302),\nH\u0302\u03b3 = n \u22121 n\u2211 i=1 \u2207\u03b3h`(Si, \u03b3\u0302, \u03b8\u0302), M\u0302 = n\u22121 n\u2211 i=1 \u2207\u03b3hm(Wi, Zi, \u03b3\u0302), \u03a8\u0302i = \u2212M\u0302\u22121hm(Wi, Zi, \u03b3\u0302).\nThanks to the asymptotic normality and the consistent estimator for the variance of the estimators, confidence intervals can easily be constructed. Note that since \u03c3T , \u03c3C > 0 and \u03c1 \u2208 (\u22121, 1), their confidence intervals will be constructed using a logarithm and a Fisher\u2019s z-transformation respectively. These transformations project the estimates on the real line, after which the delta method can be used to obtain their standard errors. The confidence intervals can then be constructed and transformed back to the original scale. This procedure makes sure that our confidence intervals are reasonable (e.g. no negative values for the confidence limits of the standard deviation estimates). Also note that instead of calculating h`(Si, \u03b3\u0302, \u03b8\u0302), H\u0302\u03b8 and H\u0302\u03b3 using their analytical expressions, they are approximated. This is due to the complexity of these expressions and the amount of them that would have to be derived. For example, H\u0302\u03b8 is already a (2m+ 9)\u00d7 (2m+ 9) matrix of derivatives where m is the dimension of X\u0303. The calculation of these approximations is done by making use of Richardson\u2019s extrapolation (Richardson, 1911), resulting in more accurate estimates. A general description of the method to approximate the Jacobian matrix can be given as repeated calculations of the central difference approximation of the first derivative with respect to each component of \u03b8, using a successively smaller step size. Richardson\u2019s extrapolation uses this information to estimate what happens when the step size goes to zero. A similar description can be given for the approximation of the Hessian matrices. Note that these calculations can be quite time consuming depending on the required level of accuracy."
        },
        {
            "heading": "4 Simulation study",
            "text": "In this section, a simulation study is performed to investigate the finite sample performance of the proposed two-step estimator. We consider the four combinations of the cases where Z and W\u0303 are continuous or binary random variables. It is assumed that when Z is binary, it follows a logit model. The proposed estimator is compared to three other estimators: one which does not account for the confounding issue, one which assumes T and C are independent and one which uses the proposed method but treats V as observed. The parameters are estimated for samples of 250, 500 and 1000 observations. The first step of the data generating process is as follows:(\nT C\n) \u223c N2 (( 0 0 ) , \u03a3 = ( 1.12 0.75 \u00b7 1.1 \u00b7 1.4 0.75 \u00b7 1.1 \u00b7 1.4 1.42 )) , X\u0303 \u223c N(0, 1).\nWe have 4 different designs depending on whether Z and W\u0303 are assumed to be a continuous or binary random variable:\nDesign 1 W\u0303 \u223c U [0, 2], \u03bd \u223c N(0, 2) Z = W>\u03b3 + \u03bd, Design 2 W\u0303 \u223c Bernoulli(0.5), \u03bd \u223c N(0, 2) Design 3 W\u0303 \u223c U [0, 2], \u03bd \u223c Logistic(0, 1) Z = 1(W>\u03b3 \u2212 \u03bd > 0), Design 4 W\u0303 \u223c Bernoulli(0.5), \u03bd \u223c Logistic(0, 1)\nwith W = (1, X\u0303, W\u0303 )> and \u03b3 = (\u22121, 0.6, 2.3)>. It is also assumed that\n( T , C) \u22a5\u22a5 (X\u0303, W\u0303 , \u03bd), W\u0303 \u22a5\u22a5 (X\u0303, \u03bd) and X\u0303 \u22a5\u22a5 \u03bd.\nFrom this, we can construct:\nDesign 1 V = Z \u2212W>\u03b3.\nDesign 2 Design 3 V = (1\u2212 Z) [( 1 + exp{W>\u03b3} ) log ( 1 + exp{W>\u03b3} ) \u2212W>\u03b3 exp{W>\u03b3} ] Design 4 \u2212Z [( 1 + exp{\u2212W>\u03b3} ) log ( 1 + exp{\u2212W>\u03b3} ) +W>\u03b3 exp{\u2212W>\u03b3} ] .\nFinally, T and C can be constructed for each design in the following way:{ T = \u03b2T,0 + X\u0303\u03b2T,1 + Z\u03b1T + V \u03bbT + T C = \u03b2C,0 + X\u0303\u03b2C,1 + Z\u03b1C + V \u03bbC + C ,\nwhere (\u03b2T,0, \u03b2T,1, \u03b1T , \u03bbT ) = (2.5, 2.6, 1.8, 2) and (\u03b2C,0, \u03b2C,1, \u03b1C , \u03bbC) = (2.8, 1.9, 1.5, 1.2). It follows that Y = min{T,C} and \u2206 = 1(T \u2264 C).\nThis data generating process was repeated 2500 times for the four possible designs. The parameter values were chosen such that there is between 45% and 50% censoring for each design. For each sample size, there are four different estimators. The first, which we call the naive estimator, ignores the confounding issue and therefore does not include V in the model (no estimates for \u03bbT and \u03bbC). The second, which we call the independent estimator, assumes that T and C are independent from each other (no estimates for \u03c1 as it is assumed to be zero). The third, which we call the oracle estimator, uses the control function approach to handle the confounding issue but treats V as if it were observed. The fourth and last, which we call the two-step estimator, uses the two-step estimation method proposed in this article. This means that V is constructed using \u03b3\u0302 from the first step. The estimation is performed in R and uses the package nloptr to maximize certain functions and the package numDeriv for computing the necessary Hessian and Jacobian matrices. The package MASS is used to generate the bivariate normal variables.\nFor each estimator, the bias of each parameter estimate is given together with the empirical standard deviation (ESD) and the root mean squared error (RMSE). Note that, as the bias decreases, these last 2 statistics should converge to the same value. To better explain how these statistics are calculated, we give the formulas for \u03b1T as an example. Let N represent the total amount of simulations with j = 1, ..., N and (\u03b1\u0302T )j the estimate of \u03b1T for the j\u2019th simulation. The ESD and RMSE for \u03b1T are given as follows:\nESD = \u221a\u221a\u221a\u221a(N \u2212 1)\u22121 N\u2211 j=1 [ (\u03b1\u0302T )j \u2212 \u03b1\u0304T ]2 , with \u03b1\u0304T = N \u22121 N\u2211 j=1 (\u03b1\u0302T )j .\nRMSE = \u221a\u221a\u221a\u221aN\u22121 N\u2211 j=1 [ (\u03b1\u0302T )j \u2212 \u03b1T ]2 , with \u03b1T the real parameter value.\nLastly, the coverage rate (CR) shows in which percentage of the simulations the real parameter value is included in the estimated 95% confidence interval.\nTable 1 shows the results for design 4, meaning that both Z and W are binary. The results show a very noticeable bias for the naive estimator for almost each parameter estimate. Note that this bias remains the same as the sample size increases. The table also shows that the estimated standard errors are not asymptotically valid as the CR is inconsistent and does not converge to the expected 95%. We find the same results when looking at the independent estimator but to a lesser extent. The bias is lower compared to the naive estimator but is still noticeable. A difference with the naive estimator is that there is a lot less bias for \u03b1T and \u03b1C . This is to be expected as the independent estimator does take the confounding issue into account. However, the estimated standard errors for \u03b1T and \u03b1C are not asymptotically valid as the CR is inconsistent. It may seem that the CR converge to 95%, but Tables 4 and 6 show that this is not always the case. As with the naive estimator, the bias does not necessarily decrease when the sample size increases. The bias for the proposed two-step estimation method is very close to 0 and is clearly an improvement over the naive and independent estimator. It is also very close to that of the oracle estimator, which treats V as observed. The bias decreases when the sample size increases and the ESD and RMSE converge to the same value, which also decreases as the sample size increases. The CR is around 95%, meaning that we have asymptotically valid standard errors and confidence intervals. From this, it is clear that the two-step estimator performs well, even for small sample sizes. The results for the other designs are similar and can be found in Appendix D."
        },
        {
            "heading": "5 Data application",
            "text": "In this section, we apply the outlined methodology to estimate the effect of Job Training Partnership Act (JTPA) services on time until employment. The data come from a large-scale randomized experiment known as the National JTPA Study and have been analyzed extensively by Bloom et al. (1997), Abadie et al. (2002) and Frandsen (2015) among others. The data and problem investigated is the same as in Frandsen (2015), but the method used nevertheless differs as we allow for dependent censoring. Later in this section, we give our reasoning as to why there could be dependent censoring present in the data\nThis study was performed to evaluate the effectiveness of more than 600 federally funded services, established by the Job Training Partnership Act of 1982, that were intended to increase the employability of eligible adults and out-of-school youths. These services included classroom training, on-the-job training and job search assistance. The JTPA started to fund these programs in October of 1983 and continued funding up until the late 1990\u2019s. Between 1987 and 1989, a little over 20,000 adults and out-of-school youths who applied for JTPA were randomly assigned to be in either a treatment or a control group. Treatment group members were eligible to receive JTPA services, while control group members were not eligible for 18 months. However, due to local program staff not always following the randomization rules closely, about 3% of the control group members were able to participate in JTPA services. It is important to note that we are not comparing JTPA services to no services but rather JTPA services versus no and other services, since control group members were still eligible for non-JTPA training. Between 12 and 36 months after randomization, with an average of 21 months, the participants were surveyed by data collection officers. Next, a subset of 5,468 subjects participated in a second follow-up survey, which focused on the period between the two surveys. The second survey took place between 23 and 48 months after randomization. See Figure 1 for a graphical representation of the interview process.\nIn this application, we will focus our attention on the effect of JTPA programs on the sample of 1,298 fathers who reported having no job at the time of randomization, for which participation data is available. The outcome of interest is the time between randomization and employment. For the individuals that were only invited to the first interview, the outcome is measured completely if an individual is employed by the time of the survey and censored at the time of the interview otherwise. For the fathers that were invited to the second follow-up interview and participated, the outcome is measured completely if an individual is employed by the time of the second followup interview, but is otherwise censored at the second interview date. If the individual does not participate in the second survey after being invited, they will be censored at the time of the first interview. It follows that there could be some dependence between T and C when this decision to go to the second follow-up interview is influenced by them having found a job between the two interview dates. This possible dependence combined with the fact that the data suffer from two-sided noncompliance makes it an appropriate application of the proposed methodology.\nThe instrument W\u0303 will be a binary variable indicating whether an individual is in the control or treatment group (0 and 1 respectively). The confounded variable Z indicates whether they actually participated in a JTPA program (0 for no participation and 1 otherwise). This participation variable is confounded due to individuals moving themselves between the treatment and control group in a non-random way. The covariates include the participant\u2019s age, race (white or non-white), marital status and whether they have a high school diploma or GED. We expect W\u0303 to be a valid instrument because it is randomly assigned, correlated with JTPA participation and should have no impact on time until employment other than through participation in a JTPA funded program. Rows 2 through 5 from Table 2 show that the individual characteristics are balanced across the control and treatment group. This indicates satisfactory random assignment. The first row shows that about 31% of the total sample was assigned to the control group.\nTable 2: Summary statistics for the sample of fathers unemployed at the time of random assignment. The means are shown for the total sample, the control group and the treatment group.\nnaive estimator independent estimator two-step estimator\nT Estimate SE p-value Estimate SE p-value Estimate SE p-value Intercept 4.753 0.223 0.000 4.949 0.318 0.000 4.866 0.370 0.000 Age 0.015 0.006 0.007 0.013 0.006 0.025 0.015 0.008 0.056\nWhite -0.197 0.108 0.068 -0.197 0.118 0.096 -0.187 0.622 0.764 Married -0.331 0.123 0.007 -0.330 0.133 0.013 -0.331 0.132 0.012\nGED -0.166 0.102 0.104 -0.179 0.112 0.109 -0.172 0.217 0.430 \u03b1T -0.218 0.102 0.033 -0.483 0.260 0.063 -0.428 0.210 0.041 \u03bbT -0.113 0.116 0.330 -0.097 0.087 0.268\nC Estimate SE p-value Estimate SE p-value Estimate SE p-value Intercept 6.866 0.134 0.000 6.672 0.164 0.000 6.848 0.403 0.000 Age -0.001 0.002 0.436 -0.001 0.002 0.605 -0.001 0.003 0.605\nWhite 0.012 0.033 0.713 -0.005 0.050 0.913 0.010 1.206 0.993 Married 0.010 0.032 0.746 -0.006 0.075 0.933 0.013 0.661 0.984\nGED -0.063 0.042 0.137 -0.069 0.041 0.093 -0.062 0.177 0.728 \u03b1C -0.062 0.042 0.138 -0.052 0.117 0.655 -0.028 1.335 0.983 \u03bbC 0.006 0.042 0.876 0.015 0.511 0.976\n\u03c3T 1.817 0.040 0.000 1.804 0.038 0.000 1.816 0.038 0.000 \u03c3C 0.323 0.037 0.000 0.285 0.013 0.000 0.323 0.029 0.000 \u03c1 -0.430 0.196 0.028 -0.432 0.176 0.014\nTable 3: Estimation results for the naive, independent and two-step estimator. Given are the parameter estimate, standard error (SE) and the p-value.\nThe last 3 rows of table 2 show summary statistics for variables observed after randomization. It is interesting to note that 13% of the fathers in the control group were nevertheless able to participate in JTPA services compared to 3% for the entire control group. The mean time to employment also seems to be about 30 days shorter for the individuals assigned to the treatment group compared to the control group. The censoring rate is similar for both groups. Figure 2 plots a histogram of the observed follow-up time Y , where darker shading indicates a higher censoring rate. A lot of the censored observations are around the 600 days mark, at which time most of the first follow-up interviews took place. Since everyone in the sample participated in the first follow-up interview, an observation before the date of the first follow-up survey cannot be censored.\nThe results of applying the two-step estimator (using a logit model for Z), compared to other estimators, can be found in Table 3. The naive estimator, which does not treat Z as a confounded variable, seems to underestimate the effect of JTPA services on time until employment compared to the proposed two-step estimator. At a 5% significance level, both of these estimators find a significant effect of JTPA training reducing time until employment. However, the two-step estimate is almost twice the naive estimate which indicates that the individuals participating in the treatment are those with a lower ability to find employment. The independent estimator, which assumes independent censoring, seems to slightly overestimate the size of the effect compared to the proposed two-step estimator, but is not significant at a 5% significance level. Age seems to be (borderline) significant across the estimators as does marriage status and having a high school diploma or GED. Being older seems to increase time until employment, while being married and having a diploma reduces it. Both the naive and the two-step estimator seem to agree that there is a quite strong negative correlation of about -0.43 between T and C.\nFuture research\nIt is important to note that this work is only a first step towards a set of models that will allow for the estimation of causal effects under dependent censoring. A first extension could be to select different parametric marginals and copulas by making use of an information criterion. Up until now, the association parameter has been shown to be identified only for certain combinations of parametric copulas and marginals without confounding or covariates (see Czado and Van Keilegom (2023)). Implementing this would therefore complicate the identifiability proof and the estimation procedure even further. Another line of research is to allow for semi-parametric marginals. This would greatly increase the flexibility of the model and is currently being studied."
        },
        {
            "heading": "A Control function examples",
            "text": "In this first section of the Appendix, we expand on the examples of g\u03b3(Z,W ) given in Section 2.2. More specifically, given the distribution of \u03bd, we give explicit expressions for\ng\u03b3(Z,W ) = Z E[\u03bd |W>\u03b3 > \u03bd] + (1\u2212 Z)E[\u03bd |W>\u03b3 < \u03bd].\nIf we assume that \u03bd follows a standard normal distribution, it can be derived that\ng\u03b3(Z,W ) = (1\u2212 Z) \u03c6(W>\u03b3)\n\u03a6(\u2212W>\u03b3) \u2212 Z \u03c6(W\n>\u03b3)\n\u03a6(W>\u03b3) .\nSimilarly, if we assume that \u03bd follows a standard logistic distribution, we find: g\u03b3(Z,W ) = (1\u2212 Z) [( 1 + exp{W>\u03b3} ) log ( 1 + exp{W>\u03b3} ) \u2212W>\u03b3 exp{W>\u03b3} ] \u2212 Z [( 1 + exp{\u2212W>\u03b3} ) log ( 1 + exp{\u2212W>\u03b3} ) +W>\u03b3 exp{\u2212W>\u03b3} ] .\nBoth of these expressions follow from the following conditional expectations. Let \u03bd \u223c N(0, 1), it follows that:\nE[\u03bd | \u03bd < a] = 1 \u03a6(a) \u222b a \u2212\u221e \u03bd\u03c6(\u03bd) d\u03bd = \u22121 \u03a6(a) \u222b a \u2212\u221e \u03c6\u2032(\u03bd) d\u03bd = \u2212\u03c6(a) \u03a6(a) ,\nE[\u03bd | \u03bd > a] = 1 1\u2212 \u03a6(a) \u222b +\u221e a \u03bd\u03c6(\u03bd) d\u03bd = \u22121 \u03a6(\u2212a) \u222b +\u221e a \u03c6\u2032(\u03bd) d\u03bd = \u03c6(a) \u03a6(\u2212a) .\nIf \u03bd \u223c Logistic(0, 1), it follows that:\nE[\u03bd | \u03bd < a] = (1 + e\u2212a) \u222b a \u2212\u221e \u03bd e\u2212\u03bd (1 + e\u2212\u03bd)2 d\u03bd = \u2212(1 + e\u2212a) log(1 + e\u2212a)\u2212 ae\u2212a,\nE[\u03bd | \u03bd > a] = (1 + ea) \u222b +\u221e a \u03bd e\u2212\u03bd (1 + e\u2212\u03bd)2 d\u03bd = (1 + ea) log(1 + ea)\u2212 aea."
        },
        {
            "heading": "B Technical lemmas",
            "text": "In this section, we prove three lemmas of which Lemma 3 is needed to prove Theorem 2 in Appendix C. Lemma 1 is used in the proof of Lemma 2, and Lemma 2 is used in the proof of Lemma 3. Note that the proofs of the first two lemmas are inspired by the proof of Lemma 1 by Tauchen (1985). For all of these proofs, it is useful to define the open cube I(\u03b3, \u03b8, d) as\nI(\u03b3, \u03b8, d) = { (\u03b3\u0303, \u03b8\u0303) \u2208 N\u03b3 \u00d7\u0398 : ||(\u03b3\u0303, \u03b8\u0303)\u2212 (\u03b3, \u03b8)||\u221e < d } ,\nwhere ||\u00b7||\u221e is the sup-norm and d \u2208 R>0.\nLemma 1. If Assumptions (A1)-(A8) hold, then for all \u03b5 > 0 and (\u03b3, \u03b8) \u2208 N\u03b3 \u00d7\u0398, there exists a d > 0 such that\nE [ sup\n(\u03b3\u0303,\u03b8\u0303)\u2208I(\u03b3,\u03b8,d) |`(S, \u03b3\u0303, \u03b8\u0303)\u2212 `(S, \u03b3, \u03b8)|\n] < \u03b5\n4 .\nProof. Define \u03ba(s, \u03b3, \u03b8, d) = sup\n(\u03b3\u0303,\u03b8\u0303)\u2208I(\u03b3,\u03b8,d) |`(s, \u03b3\u0303, \u03b8\u0303)\u2212 `(s, \u03b3, \u03b8)|,\nwith (s, \u03b3, \u03b8, d) \u2208 G \u00d7N\u03b3 \u00d7\u0398\u00d7R>0. By Assumption (A8), for all (s, \u03b3, \u03b8, d) \u2208 G \u00d7N\u03b3 \u00d7\u0398\u00d7R>0 :\n0 \u2264 \u03ba(s, \u03b3, \u03b8, d) \u2264 sup \u03b3\u0303\u2208N\u03b3 ,\u03b8\u0303\u2208\u0398 |`(s, \u03b3\u0303, \u03b8\u0303)\u2212 `(s, \u03b3, \u03b8)| \u2264 2 \u00b7 D(s).\nLet dn be a sequence that converges to 0 when n \u2192 \u221e and define \u03ban(s, \u03b3, \u03b8) = \u03ba(s, \u03b3, \u03b8, dn). The continuity of `, implies that (use Heine-Cantor theorem) for all (s, \u03b3, \u03b8) \u2208 G \u00d7N\u03b3 \u00d7\u0398 it holds that\nlim n\u2192\u221e \u03ban(s, \u03b3, \u03b8) = 0.\nSince E [ D(s) ] <\u221e, the dominated convergence theorem states that\nlim n\u2192\u221e\nE [ sup\n(\u03b3\u0303,\u03b8\u0303)\u2208I(\u03b3,\u03b8,dn) |`(S, \u03b3\u0303, \u03b8\u0303)\u2212 `(S, \u03b3, \u03b8)|\n] = 0.\nFrom this we can infer that for all \u03b5 > 0 and (\u03b3, \u03b8) \u2208 N\u03b3 \u00d7\u0398, there exists a d > 0 such that\nE [ sup\n(\u03b3\u0303,\u03b8\u0303)\u2208I(\u03b3,\u03b8,d) |`(S, \u03b3\u0303, \u03b8\u0303)\u2212 `(S, \u03b3, \u03b8)|\n] < \u03b5\n4 .\nLemma 2. Under Assumptions (A1)-(A8), there exists a neighborhood B of \u03b3\u2217 such that for all \u03b5 > 0 and \u03b3 \u2208 B, we have\nlim n\u2192\u221e\nP ( sup\n\u03b3\u2208B,\u03b8\u2208\u0398 \u2223\u2223L\u0302(\u03b3, \u03b8)\u2212 L(\u03b3\u2217, \u03b8)\u2223\u2223 < \u03b5) = 1. (8) Proof. Take \u03b3 \u2208 N\u03b3 (with N\u03b3 defined in Assumption (A8)). By Lemma 1, for all \u03b8 \u2208 \u0398, there exists a d\u03b8 > 0 such that\nE [ sup\n(\u03b3\u0303,\u03b8\u0303)\u2208I(\u03b3\u2217,\u03b8,d\u03b8) |`(S, \u03b3\u0303, \u03b8\u0303)\u2212 `(S, \u03b3\u2217, \u03b8)|\n] < \u03b5\n4 . (9)\nGiven these open cubes, we can define I = \u22c3 \u03b8\u2208\u0398 I(\u03b3\u2217, \u03b8, d\u03b8),\nwhich is clearly an open cover of {\u03b3\u2217} \u00d7\u0398.\nBecause \u0398 is assumed to be compact and \u03b3\u2217 fixed, the open cover definition of a compact space tells us that since I is a collection of open subsets that cover {\u03b3\u2217} \u00d7 \u0398, there must exist a finite subcollection {I1, I2, ..., IK} such that\n{\u03b3\u2217} \u00d7\u0398 \u2286 \u22c3\nk=1...,K\nIk,\nwith Ik = I(\u03b3 \u2217, \u03b8k, dk), k = 1, . . . ,K,\nfor some dk > 0. Thanks to the subcollection being finite, we can define d\u0304 = mink=1,...,K dk > 0. Let B\u03b3 be the open ball in \u0393 centered around \u03b3\n\u2217 with a radius of d\u0304. For all k = 1, ...,K and \u03b3 \u2208 B\u03b3 , it holds that (\u03b3, \u03b8k) \u2208 Ik, which means that F is also a finite open cover of B\u03b3\u00d7\u0398. Let B = B\u03b3 \u2229N\u03b3 , \u03b3 \u2208 B and (\u03b3, \u03b8) \u2208 Ik. It follows from (9) that\n\u03b5 4 \u2265 E\n[ |`(S, \u03b3, \u03b8)\u2212 `(S, \u03b3\u2217, \u03b8k)| ] \u2265 |L(\u03b3, \u03b8)\u2212 L(\u03b3\u2217, \u03b8k)|.\nThis means that |L(\u03b3, \u03b8)\u2212 L(\u03b3\u2217, \u03b8k)| \u2264 \u03b5\n4 when (\u03b3, \u03b8) \u2208 Ik. (10)\nWe have\u2223\u2223L\u0302(\u03b3, \u03b8)\u2212 L(\u03b3\u2217, \u03b8)\u2223\u2223 =\n\u2223\u2223\u2223\u2223 1n n\u2211 i=1 `(Si, \u03b3, \u03b8)\u2212 `(Si, \u03b3\u2217, \u03b8k) + `(Si, \u03b3\u2217, \u03b8k)\u2212 L(\u03b3\u2217, \u03b8k) + L(\u03b3\u2217, \u03b8k)\u2212 L(\u03b3\u2217, \u03b8) \u2223\u2223\u2223\u2223\n\u2264 1 n n\u2211 i=1 \u2223\u2223\u2223`(Si, \u03b3, \u03b8)\u2212 `(Si, \u03b3\u2217, \u03b8k)\u2223\u2223\u2223\u2212 E[ sup (\u03b3\u0303,\u03b8\u0303)\u2208Ik |`(S, \u03b3\u0303, \u03b8\u0303)\u2212 `(S, \u03b3\u2217, \u03b8k)| ]\n+ E [ sup\n(\u03b3\u0303,\u03b8\u0303)\u2208Ik |`(S, \u03b3\u0303, \u03b8\u0303)\u2212 `(S, \u03b3\u2217, \u03b8k)|\n] + \u2223\u2223L\u0302(\u03b3\u2217, \u03b8k)\u2212 L(\u03b3\u2217, \u03b8k)\u2223\u2223+ \u2223\u2223\u2223L(\u03b3\u2217, \u03b8k)\u2212 L(\u03b3\u2217, \u03b8)\u2223\u2223\u2223\n\u2264 1 n n\u2211 i=1 sup (\u03b3\u0303,\u03b8\u0303)\u2208Ik |`(Si, \u03b3\u0303, \u03b8\u0303)\u2212 `(Si, \u03b3\u2217, \u03b8k)| \u2212 E\n[ sup\n(\u03b3\u0303,\u03b8\u0303)\u2208Ik |`(S, \u03b3\u0303, \u03b8\u0303)\u2212 `(S, \u03b3\u2217, \u03b8k)|\n] (11)\n+ E [ sup\n(\u03b3\u0303,\u03b8\u0303)\u2208Ik |`(S, \u03b3\u0303, \u03b8\u0303)\u2212 `(S, \u03b3\u2217, \u03b8k)|\n] + \u2223\u2223\u2223L(\u03b3\u2217, \u03b8k)\u2212 L(\u03b3\u2217, \u03b8)\u2223\u2223\u2223. (12)\n+ \u2223\u2223L\u0302(\u03b3\u2217, \u03b8k)\u2212 L(\u03b3\u2217, \u03b8k)\u2223\u2223. (13)\nThe terms in (12) are each bounded by \u03b54 due to (9) and (10) respectively. For (11) and (13), the strong law of large numbers can be applied such that they go to 0 in probability. Because of this, we have that:\nlim n\u2192\u221e\nP ( sup\n(\u03b3,\u03b8)\u2208Ik\n\u2223\u2223\u2223\u2223L\u0302(\u03b3, \u03b8)\u2212 L(\u03b3\u2217, \u03b8)\u2223\u2223\u2223\u2223 < \u03b5 ) = 1,\nfor all k = 1, . . . ,K. Since F is a finite open covering of B \u00d7\u0398, this leads to (8).\nLemma 3. If Assumptions (A1)-(A8) hold, then\nsup \u03b8\u2208\u0398\n\u2223\u2223L\u0302(\u03b3\u0302, \u03b8)\u2212 L(\u03b3\u2217, \u03b8)\u2223\u2223 p\u2212\u2192 0.\nProof. Note that for all \u03b5 > 0:\nP ( sup \u03b8\u2208\u0398 \u2223\u2223\u2223\u2223L\u0302(\u03b3\u0302, \u03b8)\u2212 L(\u03b3\u2217, \u03b8)\u2223\u2223\u2223\u2223 > \u03b5 )\n= P ({ sup \u03b8\u2208\u0398 \u2223\u2223\u2223\u2223L\u0302(\u03b3\u0302, \u03b8)\u2212 L(\u03b3\u2217, \u03b8)\u2223\u2223\u2223\u2223 > \u03b5} \u22c2 {\u03b3\u0302 \u2208 B} )\n+ P ({ sup \u03b8\u2208\u0398 \u2223\u2223\u2223\u2223L\u0302(\u03b3\u0302, \u03b8)\u2212 L(\u03b3\u2217, \u03b8)\u2223\u2223\u2223\u2223 > \u03b5} \u22c2 {\u03b3\u0302 /\u2208 B} ) . (14)\nAssumption (A6) implies that \u03b3\u0302 p\u2212\u2192 \u03b3\u2217. Because of this we know that\nlim n\u2192\u221e\nP ({ sup \u03b8\u2208\u0398 \u2223\u2223\u2223\u2223L\u0302(\u03b3\u0302, \u03b8)\u2212 L(\u03b3\u2217, \u03b8)\u2223\u2223\u2223\u2223 > \u03b5} \u22c2 {\u03b3\u0302 /\u2208 B} ) \u2264 lim n\u2192\u221e P ( \u03b3\u0302 /\u2208 B ) = 0.\nMoreover, we have\nP ({ sup \u03b8\u2208\u0398 \u2223\u2223\u2223\u2223L\u0302(\u03b3\u0302, \u03b8)\u2212 L(\u03b3\u2217, \u03b8)\u2223\u2223\u2223\u2223 > \u03b5} \u22c2 {\u03b3\u0302 \u2208 B} )\n\u2264 P ( sup\n(\u03b3,\u03b8)\u2208B\u00d7\u0398\n\u2223\u2223\u2223\u2223L\u0302(\u03b3, \u03b8)\u2212 L(\u03b3\u2217, \u03b8)\u2223\u2223\u2223\u2223 > \u03b5 ) \u2192 0,\nby Lemma 2. We obtain the result using (14)."
        },
        {
            "heading": "C Proofs of theorems",
            "text": "In this section of the Appendix, the three main theorems regarding identifiability, consistency and asymptotic normality are proven.\nProof of Theorem 1\nBecause of Assumption (A5), we know that \u03b3 is identified. For ease of notation, let \u03b81 = \u03b8 and \u03b82 = \u03b8 \u2217. From the model specification, we know that:\nfYj ,\u2206j |W,Z(y, 1 | w, z, \u03b3; \u03b8j)\n= [ 1\u2212 \u03a6 (( 1\u2212 \u03c1j \u03c3Cj \u03c3Tj ) y \u2212 x>\u03b2Cj \u2212 z\u03b1Cj \u2212 v\u03bbCj + \u03c1j \u03c3Cj \u03c3Tj ( x>\u03b2Tj + z\u03b1Tj + v\u03bbTj ) \u03c3Cj (1\u2212 \u03c1j2) 1/2 )]\n\u00d7 1 \u03c3Tj \u03c6\n( y \u2212 x>\u03b2Tj \u2212 z\u03b1Tj \u2212 v\u03bbTj\n\u03c3Tj\n) , j = 1, 2.\nWe will consider a number of cases that are dependent on the values of the following \u03c0\u2019s:\n\u03c011 = 1\u2212 \u03c11 \u03c3C1 \u03c3T1 , \u03c012 = 1\u2212 \u03c11 \u03c3T1 \u03c3C1 , \u03c021 = 1\u2212 \u03c12 \u03c3C2 \u03c3T2 , \u03c022 = 1\u2212 \u03c12 \u03c3T2 \u03c3C2 .\nCase 1: All \u03c0jk with j, k = 1, 2 are strictly positive. Note that the positivity of \u03c0j1 (j = 1, 2) allows us to rewrite the argument of \u03a6(\u00b7) as:\ny \u2212 x>\u03b2Cj+z\u03b1Cj+v\u03bbCj\u2212\u03c1j\n\u03c3Cj \u03c3Tj\n( x>\u03b2Tj+z\u03b1Tj+v\u03bbTj ) \u03c0j1\u221a\n\u03c32Cj (1\u2212\u03c12j )\n\u03c02j1\n.\nTherefore, we define \u03bej1 (j = 1, 2), whose distribution for a given (W,Z) is specified as:\n(\u03bej1 |W,Z) \u223c N\n( x>\u03b2Cj + z\u03b1Cj + v\u03bbCj \u2212 \u03c1j\n\u03c3Cj \u03c3Tj\n( x>\u03b2Tj + z\u03b1Tj + v\u03bbTj ) \u03c0j1 , \u03c32Cj (1\u2212 \u03c1 2 j ) \u03c02j1 ) .\nThis allows us to rewrite the sub-density as follows:\nfYj ,\u2206j |W,Z(y, 1 | w, z, \u03b3; \u03b8j) = P(\u03bej1 > y |W = w,Z = z)fTj |W,Z(y | w, z, \u03b3; \u03b8j). (15)\nSince fY1,\u22061|X,Z,V (y, 1 | w, z, \u03b3; \u03b81) = fY2,\u22062|W,Z(y, 1 | w, z, \u03b3; \u03b82) for almost every (w, z), it follows from (15) that\nlim y\u2192\u2212\u221e fT1|W,Z(y | w, z, \u03b3; \u03b81) fT2|W,Z(y | w, z, \u03b3; \u03b82) = 1 for almost every (w, z).\nBecause of Proposition A.1. by Deresa and Van Keilegom (2020a), it is implied that:\n\u03b2T1 = \u03b2T2 , \u03b1T1 = \u03b1T2 , \u03bbT1 = \u03bbT2 , \u03c3T1 = \u03c3T2 .\nPutting \u2206j = 0 and repeating the same arguments (with \u03c0j2 > 0, j = 1, 2), we get that:\n\u03b2C1 = \u03b2C2 , \u03b1C1 = \u03b1C2 , \u03bbC1 = \u03bbC2 , \u03c3C1 = \u03c3C2 .\nUsing the expression for FY |W,Z(y | w, z, \u03b3; \u03b8) from Section 2.3, we know that:\n\u03a6\n( y \u2212 x>\u03b2T1 \u2212 z\u03b1T1 \u2212 v\u03bbT1\n\u03c3T1 , y \u2212 x>\u03b2C1 \u2212 z\u03b1C1 \u2212 v\u03bbC1 \u03c3C1 ; \u03c11 ) = \u03a6 ( y \u2212 x>\u03b2T2 \u2212 z\u03b1T2 \u2212 v\u03bbT2\n\u03c3T2 , y \u2212 x>\u03b2C2 \u2212 z\u03b1C2 \u2212 v\u03bbC2 \u03c3C2 ; \u03c12\n) ,\nfor almost every (w, z). From this it is clear that \u03c11 = \u03c12, and thus \u03b81 = \u03b82.\nCase 2: One of \u03c0jk with j, k = 1, 2 is strictly negative and the others are strictly positive. Firstly, we assume that \u03c011 < 0, and therefore it must be the case that \u03c021 > 0. From Case 1 we know that the positivity of \u03c021 implies that\nfY2,\u22062|W,Z(y, 1 | w, z, \u03b3; \u03b82) = P(\u03be21 > y |W = w,Z = z)fT2|W,Z(y | w, z, \u03b3; \u03b82).\nNote that the negativity of \u03c011 allows us to rewrite the argument of \u03a6(\u00b7) as \u2212y \u2212 \u2212 [ x>\u03b2C1+z\u03b1C1+v\u03bbC1\u2212\u03c11 \u03c3C1 \u03c3T1 ( x>\u03b2T1+z\u03b1T1+v\u03bbT1 )] \u03c011\u221a\n\u03c32C1 (1\u2212\u03c121) \u03c0211\n.\nAs before, we define a variable \u03b6j1 (j = 1, 2) whose distribution for given (W,Z) is specified as:\n(\u03b6j1 |W,Z) \u223c N (\u2212[x>\u03b2Cj + z\u03b1Cj + v\u03bbCj \u2212 \u03c1j \u03c3Cj\u03c3Tj (x>\u03b2Tj + z\u03b1Tj + v\u03bbTj)] \u03c0j1 , \u03c32Cj (1\u2212 \u03c1 2 j ) \u03c02j1 ) .\nThis can be used to find that:\nfY1,\u22061|W,Z,(y, 1 | w, z, \u03b3; \u03b81) = P(\u03b611 > \u2212y |W = w,Z = z)fT1|W,Z(y | w, z, \u03b3; \u03b81), = P(\u2212\u03b611 < y |W = w,Z = z)fT1|W,Z(y | w, z, \u03b3; \u03b81), = P(\u03be11 < y |W = w,Z = z)fT1|W,Z(y | w, z, \u03b3; \u03b81).\nBecause fY1,\u22061|W,Z(y, 1 | w, z, \u03b3; \u03b81) = fY2,\u22062|W,Z(y, 1 | w, z, \u03b3; \u03b82), we have that:\nP(\u03be21 > y |W = w,Z = z) = P(\u03be11 < y |W = w,Z = z)\u00d7 fT1|W,Z(y | w, z, \u03b3; \u03b81) fT2|W,Z(y | w, z, \u03b3; \u03b82) .\nTaking the limit on both sides when y approaches \u2212\u221e, it follows that the left-hand side goes to 1. However, Lemma 2.4 by Basu and Ghosh (1978) shows that the right hand side does not go to 1, leading to a contradiction. Using the same arguments, it can be shown that one strictly negative \u03c0 and three strictly positive \u03c0\u2019s always leads to a contradiction.\nCase 3: Two of \u03c0jk with j, k = 1, 2 are strictly negative and the other two are strictly positive. When \u03c011 and \u03c012 are both either strictly positive or strictly negative, Case 2 shows that this leads to a contradiction. Therefore, one of \u03c011, \u03c012 and one of \u03c021, \u03c022 is strictly positive. Assuming \u03c011 > 0 and \u03c012 < 0, we know from Case 2 that if \u03c021 < 0 we get a contradiction. Hence, we further assume that \u03c021 > 0 and \u03c022 < 0. We now define\n(\u03b6j2 |W,Z) \u223c N (\u2212[x>\u03b2Tj + z\u03b1Tj + v\u03bbTj \u2212 \u03c1j \u03c3Tj\u03c3Cj (x>\u03b2Cj + z\u03b1Cj + v\u03bbCj)] \u03c0j2 , \u03c32Tj (1\u2212 \u03c1 2 j ) \u03c02j2 ) ,\n(\u03bej2 |W,Z) \u223c N\n([ x>\u03b2Tj + z\u03b1Tj + v\u03bbTj \u2212 \u03c1j\n\u03c3Tj \u03c3Cj\n( x>\u03b2Cj + z\u03b1Cj + v\u03bbCj )] \u03c0j2 , \u03c32Tj (1\u2212 \u03c1 2 j ) \u03c02j2 ) .\nSince fY1,\u22061|W,Z(y, 0 | w, z, \u03b3; \u03b81) = fY2,\u22062|W,Z(y, 0 | w, z, \u03b3; \u03b82), the same arguments as before can be used to show that:\nP(\u03be12 < y |W = w,Z = z)\u00d7 fC1|W,Z(y | w, z, \u03b3; \u03b81) fC2|W,Z(y | w, z, \u03b3; \u03b82) = P(\u03be22 < y |W = w,Z = z).\nIt follows that\nlim y\u2192+\u221e fC1|W,Z(y | w, z, \u03b3; \u03b81) fC2|W,Z(y | w, z, \u03b3; \u03b82) = 1 for almost every (w, z).\nApplication of Proposition A.1. by Deresa and Van Keilegom (2020a) implies that\n\u03b2C1 = \u03b2C2 , \u03b1C1 = \u03b1C2 , \u03bbC1 = \u03bbC2 , \u03c3C1 = \u03c3C2 .\nThe result for \u2206j = 1 and \u03c011, \u03c021 > 0 was already discussed in Case 1. Combining these, it is clear that \u03b81 = \u03b82. It follows that this argument can be replicated for \u03c011, \u03c021 < 0 and \u03c012, \u03c022 > 0.\nCase 4: Three or four of \u03c0jk with j, k = 1, 2 are strictly negative and one or none of the \u03c0\u2019s are strictly positive, respectively. This immediately leads to a contradiction since\n\u03c11 \u03c3C1 \u03c3T1 \u03c11 \u03c3T1 \u03c3C1 = \u03c121 < 1,\nwhich implies that\n\u03c11 \u03c3C1 \u03c3T1 < 1 or \u03c11 \u03c3T1 \u03c3C1 < 1,\nmeaning that at least one of \u03c011 and \u03c012 is strictly positive. This also is the case for \u03c021 and \u03c022. Hence, there are always at least two strictly positive \u03c0\u2019s and we have a contradiction.\nCase 5: One of \u03c0jk with j, k = 1, 2 is equal to zero. Without loss of generality, we assume that \u03c011 = 0. Note that \u03c011 = 0 implies that:\nfY1,\u22061|W,Z(y, 1 | w, z, \u03b3; \u03b81) =\n[ 1\u2212 \u03a6 ( \u2212x>\u03b2C1 \u2212 z\u03b1C1 \u2212 v\u03bbC1 + x>\u03b2T1 + z\u03b1T1 + v\u03bbT1 ) \u03c3C1(1\u2212 \u03c112) 1/2 )]\n\u00d7 1 \u03c3T1 \u03c6\n( y \u2212 x>\u03b2T1 \u2212 z\u03b1T1 \u2212 v\u03bbT1\n\u03c3T1\n) .\nBecause y is no longer included in the argument of \u03a6(\u00b7), we can rewrite this as\nfY1,\u22061|W,Z(y, 1 | w, z, \u03b3; \u03b81) = p\u00d7 1\n\u03c3T1 \u03c6\n( y \u2212 x>\u03b2T1 \u2212 z\u03b1T1 \u2212 v\u03bbT1\n\u03c3T1\n) , with 0 < p < 1.\nUsing what we have learned from the previous cases, and assuming that \u03c021 > 0, we have that\np\u00d7 fT1|W,Z(y | w, z, \u03b3; \u03b81) = P(\u03be21 > y |W = w,Z = z)fT2|W,Z(y | w, z, \u03b3; \u03b82).\nTaking the limit where y \u2192 \u2212\u221e we get that\np = lim y\u2192\u2212\u221e fT2|W,Z(y | w, z, \u03b3; \u03b82) fT1|W,Z(y | w, z, \u03b3; \u03b81) for almost every (w, z).\nNote that this is a contradiction since, according to Lemma 2.3 by Basu and Ghosh (1978), the right hand side of the equation can only be equal to 0, 1 or \u221e. It follows that this argument can be replicated when \u03c021 < 0.\nCase 6: Two of \u03c0jk, with j, k = 1, 2 are equal to zero. Note that it cannot be the case that \u03c0j1 and \u03c0j2 are both zero since |\u03c1j | < 1 with j = 1, 2. Therefore, one of \u03c011, \u03c012 and one of \u03c021, \u03c022 needs to be zero. To avoid contradictions, as seen in Case 5, the only possibilities are \u03c011 = \u03c021 = 0 or \u03c012 = \u03c022 = 0. Without loss of generality, we will assume that \u03c011 = \u03c021 = 0. Using the same arguments as in Case 5, it follows that\np1 p2 = lim y\u2192\u00b1\u221e fT2|W,Z(y | w, z, \u03b3; \u03b82) fT1|W,Z(y | w, z, \u03b3; \u03b81) for almost every (w, z),\nwith 0 < p1, p2 < 1. As discussed before, the right hand side can only be equal to 0, 1 or \u221e. Therefore, the only way this does not lead to a contradiction is if the right hand side is equal to 1 and p1 = p2. Using Proposition A.1. by Deresa and Van Keilegom (2020a) combined with p1 = p2, it can easily be shown that \u03b81 = \u03b82.\nCase 7: Three or four of \u03c0jk, with j, k = 1, 2 are equal to zero. Note that if \u03c011 = 0, it must be the case that \u03c012 > 0. The same holds for \u03c021 and \u03c012, leading to a contradiction.\nProof of Theorem 2\nNote that (i) ` is continuous in \u03b8, since it consists of well known other continuous functions and the definition of \u0398. Because of the continuity of ` and Assumptions (A7)-(A8), it is clear that L(\u03b3, \u03b8) is also continuous in \u03b8. Furthermore, Lemma 3 shows that (ii) L\u0302(\u03b3\u0302, \u03b8) converges uniformly (in \u03b8 \u2208 \u0398) in probability to L(\u03b3\u2217, \u03b8) under Assumptions (A1)-(A8). Moreover, we also have that (iii) L(\u03b3\u2217, \u03b8) is uniquely maximized at L(\u03b3\u2217, \u03b8\u2217). Indeed, let \u03b8 \u2208 \u0398 such that \u03b8 6= \u03b8\u2217 and define\nR = fY,\u2206|X,Z,V (Y,\u2206 | X,Z,W, \u03b3\u2217; \u03b8) fY,\u2206|X,Z,V (Y,\u2206 | X,Z,W, \u03b3\u2217; \u03b8\u2217) .\nAssumptions (A1)-(A5) guarantee that \u03b8\u2217 is identifiable according to Theorem 1. Hence, R is not 1 almost everywhere. As a result, by the strict version of Jensen\u2019s inequality, we have\n\u2212log ( E[R] ) < E [ \u2212 log(R) ] .\nThis means that L(\u03b3\u2217, \u03b8\u2217)\u2212 L(\u03b3\u2217, \u03b8) = E [ \u2212 log(R) ] > \u2212log ( E[R] ) = \u2212log(1) = 0,\nwhich shows (iii). Given (i)-(iii) and Assumption (A7), Theorem 2.1 from Newey and McFadden (1994) tells us that\n\u03b8\u0302 p\u2212\u2192 \u03b8\u2217.\nCombined with Assumption (A6), this leads to the desired result.\nProof of Theorem 3\nSince Assumptions (A1)-(A8) guarantee that Theorem 2 holds, we know that (i) \u03b8\u0302 p\u2212\u2192 \u03b8\u2217. By Assumption (A6), using straightforward calculus and the continuous mapping theorem, it can\nbe shown that (ii) h\u0303(S, \u03b3, \u03b8) = ( hm(W,Z, \u03b3) >, h`(S, \u03b3, \u03b8) >)> is continuously differentiable in a neighborhood N\u03b3,\u03b8 of (\u03b3\u2217, \u03b8\u2217) with probability approaching one. Theorem 2 also shows that L(\u03b3, \u03b8) is uniquely maximized at \u03b8\u2217 for a given \u03b3, which combined with Assumptions (A6) and (A7) implies that (iii) E [ h\u0303(S, \u03b3\u2217, \u03b8\u2217) ] = 0. Given (i)-(iii), combined with Assumptions (A6), (A7), (A9) and (A10), Theorem 6.1 from Newey and McFadden (1994) tells us that\n\u221a n(\u03b8\u0302 \u2212 \u03b8\u2217) d\u2212\u2192 N(0,\u03a3\u03b8)\nwith \u03a3\u03b8 = H \u22121 \u03b8 E [ {h`(S, \u03b3\u2217, \u03b8\u2217) +H\u03b3\u03a8}{h`(S, \u03b3\u2217, \u03b8\u2217) +H\u03b3\u03a8}> ]( H\u22121\u03b8 )> .\nD Tables"
        }
    ],
    "title": "An instrumental variable approach under dependent censoring",
    "year": 2023
}