{
    "abstractText": "This manuscript proposed an effective hybrid method based on multi-level convolutional neural network (ML-CNN) and iterative local search (ILS) to solve job shop scheduling problems (JSSP) with small-scale training samples and less time. In the proposed method, ML-CNN is proposed to find the global path of JSSP instance from the genetic algorithm (GA); ML-CNN learned global path is fed into ILS to search for the best local path, it is the key to attach excellent adaptivity. In order to find the global path from optimal solutions, the proposed ML-CNN treated the JSSP as some sub-classification tasks first. Each subclass corresponding to one sub-operation prioritizes a particular machine according to the production environment. Significantly, the proposed ML-CNN designed two-level inputs to represent production statutes. The detailed-level input channel records fourteen detailed statutes, while the system-level input channel records four systematic statutes. Two-level inputs are fed into one dimensional (1-D) CNN to extract rich hidden features to predict the priority of each suboperation using a support vector machine (SVM) classifier. At last, the global path (priority sequences) could be obtained, which is encoded as the input of ILS to find the best local path. The authors trained and tested the proposed method on 82 public JSSP instances. The results indicated that the proposed method could obtain optimal solutions for small-scale instances and outperform others regarding makespan and computation time for large-scale JSSP instances. INDEX TERMS Job shop scheduling problem (JSSP), intelligent production, CNN, multi-level features.",
    "authors": [
        {
            "affiliations": [],
            "name": "XIAORUI SHAO"
        },
        {
            "affiliations": [],
            "name": "CHANG SOO KIM"
        }
    ],
    "id": "SP:779bf0878e86a051891cacba6b5ec54db3282fd3",
    "references": [
        {
            "authors": [
                "M.E. Aydin",
                "E. \u00d6ztemel"
            ],
            "title": "Dynamic job-shop scheduling using reinforcement learning agents",
            "venue": "Robotics and Autonomous Systems, vol. 33, no. 2, pp. 169\u2013178, 2000, doi: 10.1016/S0921- 8890(00)00087-7.",
            "year": 2000
        },
        {
            "authors": [
                "K. Dehghan-Sanej",
                "M. Eghbali-Zarch",
                "R. Tavakkoli-Moghaddam",
                "S.M. Sajadi",
                "S.J. Sadjadi"
            ],
            "title": "Solving a new robust reverse job shop scheduling problem by meta-heuristic algorithms",
            "venue": "Engineering Applications of Artificial Intelligence, vol. 101, no. February, p. 104207, 2021, doi: 10.1016/j.engappai.2021.104207.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Fu",
                "J. Ding",
                "H. Wang",
                "J. Wang"
            ],
            "title": "Two-objective stochastic flow-shop scheduling with deteriorating and learning effect in Industry 4.0-based manufacturing system",
            "venue": "Applied Soft Computing Journal, vol. 68, pp. 847\u2013855, 2018, doi: 10.1016/j.asoc.2017.12.009.",
            "year": 2018
        },
        {
            "authors": [
                "X. Shao",
                "C.S. Kim"
            ],
            "title": "Self-Supervised Long-Short Term Memory Network for Solving Complex Job Shop Scheduling Problem",
            "venue": "KSII Transactions on Internet and Information Systems, vol. 15, no. 8, pp. 2993\u20133010, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M.R. Garey",
                "D.S. Johnson",
                "Ravi Sethi"
            ],
            "title": "The Complexity of Flowshop and Jobshop Scheduling",
            "venue": "Mathematics of Operations Research, vol. 1, no. 2, pp. 117\u2013129, 1976.",
            "year": 1976
        },
        {
            "authors": [
                "H.W. Ge",
                "L. Sun",
                "Y.C. Liang",
                "F. Qian"
            ],
            "title": "An effective PSO and AIS-based hybrid intelligent algorithm for job-shop scheduling",
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics Part A:Systems and Humans, vol. 38, no. 2, pp. 358\u2013368, 2008, doi: 10.1109/TSMCA.2007.914753.",
            "year": 2008
        },
        {
            "authors": [
                "Z. Wang",
                "J. Zhang",
                "S. Yang"
            ],
            "title": "An improved particle swarm optimization algorithm for dynamic job shop scheduling problems with random job arrivals",
            "venue": "Swarm and Evolutionary Computation, vol. 51, no. September, p. 100594, 2019, doi: 10.1016/j.swevo.2019.100594.",
            "year": 2019
        },
        {
            "authors": [
                "M. Omar",
                "A. Baharum",
                "Y.A. Hasan"
            ],
            "title": "a Job-Shop Scheduling Problem (JSSP) Using Genetic Algorithm (GA)",
            "venue": "2006.",
            "year": 2006
        },
        {
            "authors": [
                "M. Mahdi",
                "A. Salehipour",
                "T.C.E. Cheng"
            ],
            "title": "A meta-heuristic to solve the just-in-time job-shop scheduling problem",
            "venue": "vol. 288, pp. 14\u2013 29, 2021, doi: 10.1016/j.ejor.2020.04.017.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Zang"
            ],
            "title": "Hybrid Deep Neural Network Scheduler for Job-Shop Problem Based on Convolution Two-Dimensional Transformation",
            "venue": "Computational Intelligence and Neuroscience, vol. 2019, no. ii, 2019, doi: 10.1155/2019/7172842.",
            "year": 2019
        },
        {
            "authors": [
                "T. Gabel",
                "M. Riedmiller"
            ],
            "title": "Adaptive Reactive Job-Shop Scheduling With Reinforcement Learning Agents",
            "venue": "International Journal of Information Technology and Intelligent Computing, vol. 2, no. 4, 2008, [Online]. Available: http://ml.informatik.unifreiburg.de/_media/publications/gr07.pdf.",
            "year": 2008
        },
        {
            "authors": [
                "D.Y. Sha",
                "C.Y. Hsu"
            ],
            "title": "A hybrid particle swarm optimization for job shop scheduling problem",
            "venue": "Computers and Industrial Engineering, vol. 51, no. 4, pp. 791\u2013808, 2006, doi: 10.1016/j.cie.2006.09.002.",
            "year": 2006
        },
        {
            "authors": [
                "T. Jiang",
                "C. Zhang",
                "H. Zhu",
                "G. Deng"
            ],
            "title": "Energy-efficient scheduling for a job shop using grey wolf optimization algorithm with doublesearching mode",
            "venue": "Mathematical Problems in Engineering, vol. 2018, 2018, doi: 10.1155/2018/8574892.",
            "year": 2018
        },
        {
            "authors": [
                "T. Jiang",
                "C. Zhang"
            ],
            "title": "Application of Grey Wolf Optimization for Solving Combinatorial Problems: Job Shop and Flexible Job Shop Scheduling Cases",
            "venue": "IEEE Access, vol. 6, pp. 26231\u201326240, 2018, doi: 10.1109/ACCESS.2018.2833552.",
            "year": 2018
        },
        {
            "authors": [
                "D.E. Goldberg"
            ],
            "title": "Genetic Algorithms and Machine Learning Metaphors",
            "venue": "Machine Learning, vol. 3, pp. 95\u201399, 1988.",
            "year": 1988
        },
        {
            "authors": [
                "M. Kurdi"
            ],
            "title": "An effective new island model genetic algorithm for job shop scheduling problem",
            "venue": "Computers and Operations Research, vol. 67, pp. 132\u2013142, 2016, doi: 10.1016/j.cor.2015.10.005.",
            "year": 2016
        },
        {
            "authors": [
                "A.M. Moghadam",
                "K.Y. Wong",
                "H. Piroozfard"
            ],
            "title": "An efficient genetic algorithm for flexible job-shop scheduling problem",
            "venue": "IEEE International Conference on Industrial Engineering and Engineering Management, vol. 2015-Janua, pp. 1409\u20131413, 2014, doi: 10.1109/IEEM.2014.7058870.",
            "year": 2015
        },
        {
            "authors": [
                "C.C. Lin",
                "D.J. Deng",
                "Y.L. Chih",
                "H.T. Chiu"
            ],
            "title": "Smart Manufacturing Scheduling with Edge Computing Using Multiclass Deep Q Network",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 15, no. 7, pp. 4276\u20134284, 2019, doi: 10.1109/TII.2019.2908210.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Lecun",
                "Y. Bengio",
                "G. Hinton"
            ],
            "title": "Deep learning",
            "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015, doi: 10.1038/nature14539.",
            "year": 2015
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, vol. 2016-Decem, pp. 770\u2013778, 2016, doi: 10.1109/CVPR.2016.90.",
            "year": 2016
        },
        {
            "authors": [
                "S. Shao",
                "S. McAleer",
                "R. Yan",
                "P. Baldi"
            ],
            "title": "Highly Accurate Machine Fault Diagnosis Using Deep Transfer Learning",
            "venue": "IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS, vol. 15, no. 4, pp. 2446\u20132455, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Xiaorui Shao",
                "L. Wang",
                "C.S. Kim",
                "I. Ra"
            ],
            "title": "Fault Diagnosis of Bearing Based on Convolutional Neural Network Using Multi- Domain Features",
            "venue": "KSII Transactions on Internet and Information Systems, vol. 15, no. 5, pp. 1610\u20131629, May 2021, doi: 10.3837/tiis.2021.05.002.",
            "year": 2021
        },
        {
            "authors": [
                "M.A. Hearst",
                "S.T. Dumais",
                "E. Osuna",
                "J. Platt",
                "B. Scholkopf"
            ],
            "title": "Support vector machines",
            "venue": "IEEE Intelligent Systems and their Applications, vol. 13, no. 4, pp. 18\u201328, 1998.",
            "year": 1998
        },
        {
            "authors": [
                "LEO BREIMAN"
            ],
            "title": "Random forests",
            "venue": "Machine Learning, vol. 45, pp. 5\u201332, 2001, doi: 10.1201/9780367816377-11.",
            "year": 2001
        },
        {
            "authors": [
                "X. Shao",
                "C. PU",
                "Y. ZHANG",
                "C.S. KIM"
            ],
            "title": "Domain Fusion CNN- LSTM for Short-Term Power Consumption Forecasting",
            "venue": "IEEE Access, vol. 8, pp. 188352\u2013188362, 2020, doi: 10.1109/ACCESS.2020.3031958.",
            "year": 1883
        },
        {
            "authors": [
                "X. Shao",
                "C.-S. Kim",
                "P. Sontakke"
            ],
            "title": "Accurate Deep Model for Electricity Consumption Forecasting Using Multi-channel and Multi- Scale Feature Fusion CNN\u2013LSTM",
            "venue": "Energies, vol. 13, no. 8, p. 1881, Apr. 2020, doi: 10.3390/en13081881.",
            "year": 1881
        },
        {
            "authors": [
                "G.R. Weckman",
                "C.V. Ganduri",
                "D.A. Koonce"
            ],
            "title": "A neural network job-shop scheduler",
            "venue": "Journal of Intelligent Manufacturing, vol. 19, no. 2, pp. 191\u2013201, 2008, doi: 10.1007/s10845-008-0073-9.",
            "year": 2008
        },
        {
            "authors": [
                "M.H. Sim",
                "M.Y.H. Low",
                "C.S. Chong",
                "M. Shakeri"
            ],
            "title": "Job shop scheduling problem neural network solver with dispatching rules",
            "venue": "IEEE International Conference on Industrial Engineering and Engineering Management, vol. 2020-Decem, pp. 514\u2013518, 2020, doi: 10.1109/IEEM45057.2020.9309776.",
            "year": 2020
        },
        {
            "authors": [
                "W. Zhang",
                "T. G Diettench"
            ],
            "title": "A Reinforcement Learning Approach to Job-shop Scheduling",
            "venue": "14th Int. Jt Conf. Artif. Intell, 1995, pp. 1114\u20131120.",
            "year": 1995
        },
        {
            "authors": [
                "H. Mao",
                "M. Alizadeh",
                "I. Menache",
                "S. Kandula"
            ],
            "title": "Resource management with deep reinforcement learning",
            "venue": "HotNets 2016 - Proceedings of the 15th ACM Workshop on Hot Topics in Networks, pp. 50\u201356, 2016, doi: 10.1145/3005745.3005750.",
            "year": 2016
        },
        {
            "authors": [
                "S. Luo"
            ],
            "title": "Dynamic scheduling for flexible job shop with new job insertions by deep reinforcement learning",
            "venue": "Applied Soft Computing Journal, vol. 91, p. 106208, 2020, doi: 10.1016/j.asoc.2020.106208.",
            "year": 2020
        },
        {
            "authors": [
                "B.A. Han",
                "J.J. Yang"
            ],
            "title": "Research on adaptive job shop scheduling problems based on dueling double DQN",
            "venue": "IEEE Access, vol. 8, pp. 186474\u2013186495, 2020, doi: 10.1109/ACCESS.2020.3029868.",
            "year": 1864
        },
        {
            "authors": [
                "J. Park",
                "J. Chun",
                "S.H. Kim",
                "Y. Kim",
                "J. Park"
            ],
            "title": "Learning to schedule job-shop problems: representation and policy learning using graph neural network and reinforcement learning",
            "venue": "International Journal of Production Research, vol. 59, no. 11, pp. 3360\u20133377, 2021, doi: 10.1080/00207543.2020.1870013.",
            "year": 2021
        },
        {
            "authors": [
                "D. Molina",
                "A. Latorre",
                "F. Herrera"
            ],
            "title": "SHADE with Iterative Local Search for Large-Scale Global Optimization",
            "venue": "2018 IEEE Congress on Evolutionary Computation, CEC 2018 - Proceedings, pp. 1252\u20131259, 2018, doi: 10.1109/CEC.2018.8477755.",
            "year": 2018
        },
        {
            "authors": [
                "L. Wen",
                "X. Li",
                "L. Gao",
                "Y. Zhang"
            ],
            "title": "A New Convolutional Neural Network-Based Data-Driven Fault Diagnosis Method",
            "venue": "IEEE Transactions on Industrial Electronics, vol. 65, no. 7, pp. 5990\u20135998, 2018, doi: 10.1109/TIE.2017.2774777.",
            "year": 2018
        },
        {
            "authors": [
                "X. Shao",
                "C. Soo Kim",
                "D. Geun Kim"
            ],
            "title": "Accurate Multi-Scale Feature Fusion CNN for Time Series Classification in Smart Factory",
            "venue": "Computers, Materials & Continua, vol. 65, no. 1, pp. 543\u2013561, 2020, doi: 10.32604/cmc.2020.011108.",
            "year": 2020
        },
        {
            "authors": [
                "M. Shorfuzzaman",
                "M. Masud"
            ],
            "title": "On the Detection of COVID-19 from Chest X-Ray Images Using CNN-based Transfer Learning",
            "venue": "Computers, Materials & Continua, vol. 64, no. 3, pp. 1359\u20131381, 2020, doi: 10.32604/cmc.2020.011326.",
            "year": 2020
        },
        {
            "authors": [
                "A.F. Agarap"
            ],
            "title": "Deep Learning using Rectified Linear Units (ReLU)",
            "venue": "arXiv:1803.08375, vol. [cs.NE], pp. 2\u20138, 2018, [Online]. Available: http://arxiv.org/abs/1803.08375.",
            "year": 1803
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "ImageNet Classification with Deep Convolutional Neural Networks",
            "venue": "Neural Information Processing Systems 2012, 2012, pp. 1\u20139, doi: 10.1201/9781420010749.",
            "year": 2012
        },
        {
            "authors": [
                "S. Liu",
                "H. Ji",
                "M.C. Wang"
            ],
            "title": "Nonpooling Convolutional Neural Network Forecasting for Seasonal Time Series With Trends",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems, pp. 1\u201310, 2019, doi: 10.1109/tnnls.2019.2934110.",
            "year": 2019
        },
        {
            "authors": [
                "L. Asadzadeh"
            ],
            "title": "A local search genetic algorithm for the job shop scheduling problem with intelligent agents",
            "venue": "Computers and Industrial Engineering, vol. 85, pp. 376\u2013383, 2015, doi: 10.1016/j.cie.2015.04.006.",
            "year": 2015
        },
        {
            "authors": [
                "H. Fisher",
                "G.L. Thompson"
            ],
            "title": "Probabilistic Learning Combinations of Local Job-Shop Scheduling Rules",
            "venue": "Industrial Scheduling, 1963, pp. 225\u2013251.",
            "year": 1963
        },
        {
            "authors": [
                "D. Applegate",
                "W. Cook"
            ],
            "title": "A Computational study of the job-shop scheduling problem",
            "venue": "ORSA journal on computing, vol. 3, no. 2, pp. 149\u2013156, 1991, doi: 10.1287/ijoc.3.2.149.",
            "year": 1991
        },
        {
            "authors": [
                "T. Yamada",
                "R. Nakano"
            ],
            "title": "Job shop scheduling",
            "venue": "IEEE Control Engineering Series, p. 134, p. 134, 1997.",
            "year": 1997
        },
        {
            "authors": [
                "Robert H. Storer",
                "S.D. Wu",
                "R. Vaccari"
            ],
            "title": "New Search Spaces for Sequencing Problems with Application to Job Shop Scheduling",
            "venue": "Management Science, vol. 38, no. 10, pp. 1495\u20131509, 1992.",
            "year": 1992
        },
        {
            "authors": [
                "S. Lawrence"
            ],
            "title": "An Experimental Investigation of heuristic Scheduling Techniques",
            "venue": "Supplement to Resource Constrained Project Scheduling, 1984.",
            "year": 1984
        },
        {
            "authors": [
                "E. Demirkol",
                "S. Mehta",
                "R. Uzsoy"
            ],
            "title": "Benchmarks for shop scheduling problems",
            "venue": "European Journal of Operational Research, vol. 109, no. 1, pp. 137\u2013141, 1998, doi: 10.1016/S0377-2217(97)00019-2.",
            "year": 1998
        }
    ],
    "sections": [
        {
            "text": "VOLUME XX, 2017 1\nneural network (ML-CNN) and iterative local search (ILS) to solve job shop scheduling problems (JSSP) with small-scale training samples and less time. In the proposed method, ML-CNN is proposed to find the global path of JSSP instance from the genetic algorithm (GA); ML-CNN learned global path is fed into ILS to search for the best local path, it is the key to attach excellent adaptivity. In order to find the global path from optimal solutions, the proposed ML-CNN treated the JSSP as some sub-classification tasks first. Each subclass corresponding to one sub-operation prioritizes a particular machine according to the production environment. Significantly, the proposed ML-CNN designed two-level inputs to represent production statutes. The detailed-level input channel records fourteen detailed statutes, while the system-level input channel records four systematic statutes. Two-level inputs are fed into one dimensional (1-D) CNN to extract rich hidden features to predict the priority of each suboperation using a support vector machine (SVM) classifier. At last, the global path (priority sequences) could be obtained, which is encoded as the input of ILS to find the best local path. The authors trained and tested the proposed method on 82 public JSSP instances. The results indicated that the proposed method could obtain optimal solutions for small-scale instances and outperform others regarding makespan and computation time for large-scale JSSP instances.\nINDEX TERMS Job shop scheduling problem (JSSP), intelligent production, CNN, multi-level features.\nI. INTRODUCTION\nThe job shop scheduling problem (JSSP) is critical to the\nbuilding of intelligent factories regarding intelligent\nproduction [1], [2], saving resources [3], [4], and green production. It aims at arranging \ud835\udc5b jobs to execute at \ud835\udc5a\nfunctional machines with minimum makespan. However, a\nvery challenging issue existed in the arranging process: the NP-hard problem [4], [5]. Significantly, one \ud835\udc5b \u00d7 \ud835\udc5a JSSP instance requires to search (\ud835\udc5b!)\ud835\udc5a times to process this\ninstance. The searching space will increase exponentially with\nthe increase of jobs or machines.\nMany researchers tried to solve this challenging problem\nwith dispatching rules, population-based [6], [7], GA-based\n[8], heuristic methods [2], [9], and learning-based methods [4],\n[10], [11]. Among them, dispatching rules such as first-in-\nfirst-out (FIFO), shortest processing time first (SPTF), etc., are\nmost straightforward but not accurate enough. On the\ncontrary, the population-based and GA-based methods aim at\nfinding relative optimal solutions that are the closest to optimal\nsolutions for JSSP. Especially, Population-based methods\nmainly contain particle swarm optimization (PSO), grey wolf\noptimization (GWO), and ant colony optimization (ACO).\nE.g., Sha et al. [12] applied PSO with tabu search to solve\nJSSP. Jiang et al. [13] applied an improved GWO with two\nsearching modes to solve JSSP. Moreover, They [14] designed\na discrete GWO algorithm to solve both job shop and flexible\njob shop scheduling problems (FJSSP) problems. The GA-\nbased methods use chromosomes to represent potential\nsolutions, numerical values to represent genes, and mate to\nproduce the next generation to search for local-optimal\nsolutions [15]. E.g., Omar et al. [8] utilized the general GA to process one 5 \u00d7 5 JSSP instance. Mohamed [16] proposed a\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nVOLUME XX, 2017 9\nnew island GA model for dealing with JSSP. An evolution\nmodel is used to improve search diversification, and a new\nmigration selection mechanism is to delay premature\nconvergence. Moghadam et al. [17] applied GA to deal with\nFJSSP, in which operation order-based global selection is used\nto assign a machine. For the application of heuristic methods\nfor JSSP, Mahdi et al. [9] applied a meta-heuristic algorithm\nto a special kind of JSSP, just-in-time job-shop scheduling\nproblem. Dehghan-Sanej et al. [2] proposed a new meta-\nheuristic algorithm to solve reverse job shop scheduling\nproblems using a robust programming approach.\nHowever, GA-based and heuristic methods require much\ntime and powerful hardware to search such near-optimal\nsolution spaces and even cannot solve when meeting a large\nJSSP instance (the dimension of instance). Especially, today\u2019s\nproduction process is more and more complex with the\ndevelopment of the industry, which requires us to develop a\nfast and effective method for JSSP [18].\nLuckily, artificial intelligence (AI) technology gives us a\nnew view to process JSSP since it trains the model offline\nwhile solving the given JSSP instances online, giving the\nsolution fast. Moreover, its excellent feature extraction\ncapacity ensures the accuracy of the solution. It has obtained\nexcellent performance in kinds of applications, such as\ncomputer vision [19], [20], object detection [21], fault\ndiagnosis [22], [23], etc.\nAI technology for JSSP, also named Learning-based\nmethods for JSSP, consists of two branches: general learning\nand reinforcement learning (RL) methods. The general\nlearning method treats JSSP as a classification problem, which\nis to learn the production knowledge to predict the priority of\neach suboperation. Some shallow machine learning methods\nsuch as SVM [24], random forest (RF) [25], and neural\nnetwork (NN) could be utilized for solving JSSP. Moreover,\nNN consists of shallow NN and deep NN (DNN). Shallow NN\nhas less than three layers, while DNN is more than it [26].\nCompared to SVM, RF, and shallow NN, DNN can extract\nricher hidden features by properly designing more hidden\nlayers [27] to solve JSSP more accurately. E.g., Weckman et\nal. [28] utilized GA to generate the training samples. A NN\nwith three layers is employed to predict each operation's\npriority with four system-level features: operation, processing\ntime, reminding time, and machine load features, respectively.\nMotivated by Weckman, Sim et al. [29] combined NN and\ndispatching rules for solving JSSP. Besides fully connected\nNN, the convolutional neural network (CNN) and recurrent\nneural network (RNN) have been utilized for solving JSSP. E.\ng., Zang et al. [10] proposed a hybrid deep neural network\n(HDNN) to solve JSSP. They used fully NN to extract the\nproduction hidden statutes from the ten 1-D inputs. Two-\ndimension (2-D) CNN is used to extract the features from the\ncartesian product of 1-D inputs. The comparison results\nindicated that it performs better than SVM, RF, and shallow\nNN. Shao et al. [4] applied a special RNN, long short-term\nmemory (LSTM) to extract the hidden features from eighteen\ndetailed-level features with a self-supervised mechanism. The\nextracted features are combined with system-level features\nidentified by the K-means algorithm to predict the priority of\neach suboperation.\nThe RL-based method for JSSP is to learn the Q-values\nfunction, a pair of actions and reward. In which JSSP is\ndescribed as a Markov decision process (MDP), the current state \ud835\udc60 will transfer into new states \ud835\udc60\u2032 with action \ud835\udc4e could\nreceive one reward \ud835\udc5f, the object is to find the largest rewards\nto arrange all suboperations for one JSSP. E.g., references [1],\n[11], [30] utilized RL to process JSSP. Extended from RL,\ndeep reinforcement learning (DRL) utilizes the deep learning\npart to predict the reward, and the reinforcement learning part\nis to process each suboperation. It has been utilized for JSSP\nto overcome high computing existing in RL, simultaneously\nkeeping high accuracy. E. g., Mao, et al. [31] and Luo [32]\napplied deep reinforcement learning (DRL) to process JSSP.\nLin et al. [18] proposed a novel DRL method with seven\ndispatching rules to process JSSP; Bao et al. [33] applied a\ndueling double deep Q network (DDQN) within eighteen\nheuristic rules to solve complex JSSP, the experiments\nindicated the proposed method outperforms any single\nheuristic rules. Park et al. [34] developed a novel DRL-based\nmethod for JSSP, in which a graph neural network (GNN) is\nused to learn node features to select the best rule.\nAlthough the current deep learning-based and DRL-based\nmethods have obtained good performance for solving JSSP,\nthey are still far from optimal methods and can be improved\ndue to some shortcomings. For the deep learning-based\nmethod, most current methods just used system-level or\ndetailed-level features to predict the priority of each\nsuboperation except for Shao's method [4], which cannot\nextract the full information of the product environment.\nMoreover, some of them require extra calculations, increasing\nthe model's complexity. For instance, Zang's method [10]\nneeds to transform the original features using a\ncartesian product; Shao\u2019s method [4] requires identifying\nsystem-level features using the K-means algorithm. For the\nDRL-based methods, the solution\u2019s effectiveness and\nrobustness are highly upon selecting reward function and\nproper status representation, which may limit its usage.\nBesides, most of the current deep learning-based and DRL-\nbased methods require training and solving the JSSP instance-\nby-instance except for [33], [34]. How to develop one adaptive\nmodel to solve JSSP is still worth considering. Motivated by\nthose, this manuscript proposed an effective hybrid method\nbased on ML-CNN and ILS [35] to solve JSSP with small-\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nVOLUME XX, 2017 9\nscale training samples and less time. The proposed method\nutilized ML-CNN to extract the full hide features to find the\nglobal path through GA. Two-level inputs are designed to\nrepresent the production statutes, and 1-D CNN is used to\nextract the hidden patterns. Especially, fourteen detailed-level\nfeatures and four system-level features are adopted. The two-\nlevel features are fused and fed into one SVM classifier to get\nthe priority of each machine (global path), which is encoded\nas the input of ILS to search for the best local path. Moreover,\nthe instance-free (size-independent) priority-to-solution\nalgorithm enables the proposed method can process any JSSP\ninstances within the one trained model. Therefore, the\nproposed method could effectively process complex JSSP\ninstances within less time.\nThe main contributions of this manuscript could be\nsummarized as follows:\n\u2022 To our best of understanding, this manuscript is the first to try to solve JSSP using deep learning to find the global path and ILS to search for the optimal local path under the learned global path. \u2022 The proposed method could effectively process complex JSSP instances within less time. A deep learning structural ML-CNN is designed to extract the rich hidden features from system-level and detailed-level statutes. The extracted features use those two hidden features to reflect production environments fully. Moreover, the ML-CNN extracted hidden features are processed with an SVM classifier to predict the sub-operations priority. Thereby, it could find the global path accurately. Moreover, ILS enables it to search for the best local path to increase performance. \u2022 The proposed method has good adaptivity, it trains the model on some small-scale JSSP instances offline while solving large differ-scale JSSP instances online. For the future complex manufacturing process, the proposed method only needs to train once to obtain the fast and effective solution in one limited duration. \u2022 The effectiveness of each part in the proposed method has been explored.\nThe rest of the manuscript is arranged as follows. Section II\ngives some pre-knowledges, including JSSP, the workflow for\nsolving JSSP using deep learning technologies, and CNN.\nSection III explains the proposed method for JSSP in detail.\nThe experimental analysis is given in Section IV. Section V\ndiscussed the proposed method in-depth. The conclusion is\nconducted in Section VI."
        },
        {
            "heading": "II. METHODOLOGY",
            "text": ""
        },
        {
            "heading": "A. JSSP",
            "text": "One \ud835\udc5b \u00d7 \ud835\udc5a JSSP instance is to arrange \ud835\udc5b = {\ud835\udc3d1, \ud835\udc3d2, \u2026 , \ud835\udc3d\ud835\udc57 , \u2026 \ud835\udc3d\ud835\udc5b}, \ud835\udc57 \u2208 [1, \ud835\udc5b] jobs to execute at \ud835\udc5a = {\ud835\udc401, \ud835\udc402, \u2026 ,\ud835\udc40\ud835\udc56 , \u2026 ,\ud835\udc40\ud835\udc5a}, \ud835\udc56 \u2208 [1,\ud835\udc5a] machines with minimum makespan. Each job \ud835\udc3d\ud835\udc57 contains some suboperations\n{\ud835\udc3d\ud835\udc57,1, \ud835\udc3d\ud835\udc57,2, \u2026 , \ud835\udc3d\ud835\udc57,\ud835\udc58, \u2026 , \ud835\udc3d\ud835\udc57,\ud835\udc5a} need to be processed at different functional machines \ud835\udc40\ud835\udc56 with given time \ud835\udc47 = {\ud835\udc47\ud835\udc57,1, \ud835\udc47\ud835\udc57,2, \u2026 , \ud835\udc47\ud835\udc57,\ud835\udc58, \u2026 , \ud835\udc47\ud835\udc57,\ud835\udc5a}, \ud835\udc58 \u2208 [1,\ud835\udc5a]. Namely, the \ud835\udc58 \ud835\udc61\u210e suboperation of job \ud835\udc3d\ud835\udc57 requires to be processed at given machine \ud835\udc40\ud835\udc56 within allowed time \ud835\udc47\ud835\udc57,\ud835\udc58. The whole process will end until process all suboperations. To simply the JSSP problem, there\nare five constraints:\nConstraint 1: Each sub-operation only needs to be\nprocessed once at a particular machine \ud835\udc40\ud835\udc56.\nConstraint 2: Each functional machine \ud835\udc40\ud835\udc56 only could\nprocess one sub-operation at the same time.\nConstraint 3: Each job \ud835\udc3d\ud835\udc57 has special orders \ud835\udc42\ud835\udc57 = {\ud835\udc42\ud835\udc57,1, \ud835\udc42\ud835\udc57,2, \u2026 , \ud835\udc42\ud835\udc57,\ud835\udc58, \u2026 , \ud835\udc42\ud835\udc57,\ud835\udc5a} for processing \ud835\udc5a sub-operations with certain time, which is given in one processing time sequence \ud835\udc47\ud835\udc57 = {\ud835\udc47\ud835\udc57,1, \ud835\udc47\ud835\udc57,2, \u2026 , \ud835\udc47\ud835\udc57,\ud835\udc58 , \u2026 , \ud835\udc47\ud835\udc57,\ud835\udc5a}. That is, the job \ud835\udc3d\ud835\udc57\u2032\ud835\udc60 first sub-operation \ud835\udc3d\ud835\udc57,1 needs to be processed at machine \ud835\udc42\ud835\udc57,1; \ud835\udc3d\ud835\udc57\u2032\ud835\udc60 \ud835\udc58 \ud835\udc61\u210e sub-operation \ud835\udc3d\ud835\udc57,\ud835\udc58 needs to be processed at machine \ud835\udc42\ud835\udc57,\ud835\udc58.\nConstraint 4: There is no set-up time and broken time for\neach machine.\nConstraint 5: The transmission times during the production\nprogression is negligible.\nFollowing the above five constraints, one \ud835\udc5b \u00d7 \ud835\udc5a JSSP is\nexpressed in one operation order table and one processing time\ntable, as shown in (1) and (2).\n\ud835\udc42 = [ \ud835\udc421,1 \ud835\udc421,2 \u2026 \ud835\udc421,\ud835\udc58 \u2026 \ud835\udc421,\ud835\udc5a \ud835\udc422,1 \ud835\udc422,2 \u2026 \ud835\udc422,\ud835\udc58 \u2026 \ud835\udc422,\ud835\udc5a \u22ee \u22ee \u2026 \u22ee \u2026 \u22ee \ud835\udc42\ud835\udc57,1 \ud835\udc42\ud835\udc57,2 \u2026 \ud835\udc42\ud835\udc57,\ud835\udc58 \u2026 \ud835\udc42\ud835\udc57,\ud835\udc5a \u22ee \u22ee \u22ee \u22ee \u22ee \u22ee \ud835\udc42\ud835\udc5b,1 \ud835\udc42\ud835\udc5b,2 \u2026 \ud835\udc42\ud835\udc5b,\ud835\udc58 \u2026 \ud835\udc42\ud835\udc5b,\ud835\udc5a]\n(1)\n\ud835\udc47 = [ \ud835\udc471,1 \ud835\udc471,2 \u2026 \ud835\udc471,\ud835\udc58 \u2026 \ud835\udc471,\ud835\udc5a \ud835\udc472,1 \ud835\udc472,2 \u2026 \ud835\udc472,\ud835\udc58 \u2026 \ud835\udc472,\ud835\udc5a \u22ee \u22ee \u2026 \u22ee \u2026 \u22ee \ud835\udc47\ud835\udc57,1 \ud835\udc47\ud835\udc57,2 \u2026 \ud835\udc47\ud835\udc57,\ud835\udc58 \u2026 \ud835\udc47\ud835\udc57,\ud835\udc5a \u22ee \u22ee \u22ee \u22ee \u22ee \u22ee \ud835\udc47\ud835\udc5b,1 \ud835\udc47\ud835\udc5b,2 \u2026 \ud835\udc47\ud835\udc5b,\ud835\udc58 \u2026 \ud835\udc47\ud835\udc5b,\ud835\udc5a]\n(2)\nOne 6 \u00d7 6 JSSP is given to illustrate and explain JSSP, as\nshown in Fig. 1. Where the left job\u2019s operation order table records the order of job \ud835\udc3d\ud835\udc57\u2032\ud835\udc60 suboperation \ud835\udc42\ud835\udc57 = {\ud835\udc42\ud835\udc57,1, \ud835\udc42\ud835\udc57,2, \u2026 , \ud835\udc42\ud835\udc57,\ud835\udc58, \u2026 , \ud835\udc42\ud835\udc57,\ud835\udc5a}, which satisfies constrain 3. For instance, the first job \ud835\udc3d1 contains six suboperations {\ud835\udc421,1, \ud835\udc421,2, \ud835\udc421,3, \ud835\udc421,4, \ud835\udc421,5, \ud835\udc421,6} need to be processed at functional machines with the order of {2,1,3,4,5,6} under the\ncertain time {13,12,22,13,16,12} described in the processing\ntime table (right). Especially, the first sub-operation of the first job \ud835\udc3d1,1 requires running thirteen units at machine two (see red color); and the second suboperation needs to be processed at\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nVOLUME XX, 2017 9\nmachine one with twelve units (see blue color). The\ncorresponding solution is given in Fig. 2. The makespan is\ninstance, the makespan is 206."
        },
        {
            "heading": "B. SOLVING JSSP USING GENERAL LEARNING METHODS",
            "text": "This manuscript uses the ML-CNN to find the global path\nthrough optimal solutions (GA). Solving JSSP using general\nlearning methods mainly consists of five steps, as shown in\nFig. 3. Step 1 is generating some JSSP instances randomly to\ngenerate the training samples. Step 2 gives corresponding\nsolutions for step 1 generated JSSP instances using optimal\nmethods such as PSO and GA. At the same time, it will\ncalculate input features and give corresponding labels. The\ninput features could be obtained according to some formulas\nsuch as suboperation order index, processing times, etc.,\nwhich will be discussed in the next section. The label is the\npriority of each sub-operation at the functional machine.\nFor the JSSP instance described in Fig. 1, the 6 \u00d7 6 JSSP\ninstance is split into 36 sub-operations; each suboperation is to\nfind the best position (label) in the certain machine to the minimum the makespan. The label sequence for the job \ud835\udc3d1 and \ud835\udc3d2 are {{4,5,4,5,4,5}, {1,2,2,3,2,2}}, respectively. In step 3,\nwe will build some learning models such as SVM, NN, and\nCNN to learn the pattern to predict the priority of each\nsuboperation. After training the model, the trained model will\npredict the new JSSP instance's priority in step 4. The last step\nis to convert the priority sequence into a JSSP pattern using\nthe converting algorithm. In those steps, step 3 is the key to\nensuring the accuracy for solving the JSSP instance. After the\nabove five steps, we could obtain the JSSP pattern, and it will\nbe used as the global pattern to search for the best local path\nusing the ILS algorithm."
        },
        {
            "heading": "C. CNN",
            "text": "CNN is a classic feedforward NN, which has been widely used in many real-world applications such as image classification [36], time-series processing [37], and medical caring [38] due to its excellent feature extraction capacity. Two critical components: convolutional and pooling layers are the key to ensuring good feature extraction capacity. The convolution c while others are the same, as defined in (4).\n\ud835\udc65\ud835\udc59 = \ud835\udc53(\ud835\udc65\ud835\udc59\u22121 \u2217 \ud835\udc53\ud835\udc57 \ud835\udc59 + \ud835\udc4f), \ud835\udc57 \u2208 \ud835\udc3e (3) \ud835\udc53(\ud835\udc65) = { 0, \ud835\udc65 < 0\n\ud835\udc65, \ud835\udc5c\ud835\udc61\u210e\ud835\udc52\ud835\udc64\ud835\udc56\ud835\udc60\ud835\udc52 (4)\nPooling operation is used to reduce the dimension of the\nhidden features to speed up the training process again. The\npooled features are scale-invariance, enabling the extracted\nfeatures to be more robust. There are maximum, minimum,\nand pooling operations, and they calculate the maximum,\nminimum, and averaged values of the features as the output.\nAfter getting the last output values, the CNN updated the\nhidden layers with a gradient descent algorithm. More details\nabout CNN can be found from [40]."
        },
        {
            "heading": "III. THE PROPOSED METHOD FOR JSSP",
            "text": "The proposed method for JSSP includes offline training and online solving, as shown in Fig. 4. The offline training\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nVOLUME XX, 2017 9\nFIGURE 4. The framework of the proposed method for solving JSSP\nprocess is to train the proposed ML-CNN with small-scale JSSP instances, and the trained ML-CNN is to find the global path in online solving. More details are described in the following subsections."
        },
        {
            "heading": "A. OFFLINE TRAINING FOR ML-CNN",
            "text": "Offline training for the proposed ML-CNN with small-scale\nJSSP instances contains four steps: Input construction,\nfeatures extraction, feature fusion, and classification,\nrespectively. The structure of the proposed method is\ndescribed in the left part of Fig. 4. Moreover, the input\nconstruction part contains two channels: detailed-level and\nsystem-level input channels. Correspondingly, the feature\nextraction part consists of two parts: feature extraction from\ndetailed-level and system-level channels. A more detailed\nexplanation about each part is given in the following parts.\nInput Construction. The proposed ML-CNN utilized both detailed-level and system-level features as the input to reflect the production environments fully. Significantly, the authors defined fourteen detailed-level features and four system-level features in Table 1. The system-level features are the same as the reference [28]. Especially, the authors classified operation features. i.e., the suboperations' order in each machine into first, middle, later, and last, recorded as [0,1,2,3,4]; classified processing time and reminding time into short, middle, and long, recorded as [0,1,2]; classified machine load into light,\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nVOLUME XX, 2017 9\nand heavy, recorded as [0,1] to reflect the system-level production statutes through four aspects. Moreover, the authors adopted a one-hot encoder to encode system-level features into byte digits expression as they have no true values and priorities. Therefore, the system-level channel inputs have a length of 12=4+3+3+2. The two-channel inputs could be written as:\n{ \ud835\udc3c\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61\ud835\udc51\ud835\udc52\ud835\udc61\ud835\udc4e\ud835\udc56\ud835\udc59\ud835\udc52\ud835\udc51 = {\ud835\udc47\ud835\udc56,\ud835\udc57 , \ud835\udc58, \u2026 , \ud835\udc5c\ud835\udc60}\n\ud835\udc3c\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61\ud835\udc60\ud835\udc66\ud835\udc60\ud835\udc61\ud835\udc52\ud835\udc5a = {\ud835\udc42\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b, \ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc50\ud835\udc52\ud835\udc60\ud835\udc60\ud835\udc56\ud835\udc5b\ud835\udc54, \ud835\udc45\ud835\udc52\ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc5b\ud835\udc54, \ud835\udc3f\ud835\udc5c\ud835\udc4e\ud835\udc51} (5)\nFeature Extraction. Two-channel inputs are fed into a parallel 1-D CNN without polling layers to extract the hidden features as the dimension of those inputs are relatively less. Dropping polling layers could extract more representative features, motivated by [41]. Three 1-D CNN layers with the increasing output nodes from 16 to 64 are employed to mine the rich hidden features. The format of the CNN layer is defined as: Conv1D(output_nodes,filter_size,stride). The extracted features are denoted as (6), where \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc631\ud835\udc37() is a 1- D convolutional operation.\n{ \ud835\udc53\ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc51\ud835\udc52\ud835\udc61\ud835\udc4e\ud835\udc56\ud835\udc59\ud835\udc52\ud835\udc51 = \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc631\ud835\udc37(\ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc631\ud835\udc37(\ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc631\ud835\udc37(\ud835\udc3c\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61\ud835\udc51\ud835\udc52\ud835\udc61\ud835\udc4e\ud835\udc56\ud835\udc59\ud835\udc52\ud835\udc51))) \ud835\udc53\ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc60\ud835\udc66\ud835\udc60\ud835\udc61\ud835\udc52\ud835\udc5a = \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc631\ud835\udc37(\ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc631\ud835\udc37(\ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc631\ud835\udc37(\ud835\udc3c\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61\ud835\udc60\ud835\udc66\ud835\udc60\ud835\udc61\ud835\udc52\ud835\udc5a))) (6)\nFeature Fusion. The extracted features from two channels are fused using one concatenate operation. It is used as the input of the SVM classifier to predict each suboperation's priority. Moreover, one dense layer with 100 nodes is used to extract deeper production patterns. The mined fusion features are expressed as follows:\n\ud835\udc53\ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc53\ud835\udc62\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b = \ud835\udc37\ud835\udc52\ud835\udc5b\ud835\udc60\ud835\udc52(\ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc4e\ud835\udc61\ud835\udc52(\ud835\udc53\ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc51\ud835\udc52\ud835\udc61\ud835\udc4e\ud835\udc56\ud835\udc59\ud835\udc52\ud835\udc51, \ud835\udc53\ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc60\ud835\udc66\ud835\udc60\ud835\udc61\ud835\udc52\ud835\udc5a)) (7)\nOutput and Update. The fusion features \ud835\udc53\ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc53\ud835\udc62\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b are fed into one SVM classifier with \ud835\udc5a outputs to predict the priority of each suboperation. SVM could map the extracted features into the higher dimension to the separating hyperplane with the largest geometrical interval to classify each sample. This process could be written as (8). Noticed that the fusion features are obtained from the trained model with dense(m) as the output. i.e., we train the model first without SVM and then calculate the fusion features as the SVM\u2019s input. Moreover, the proposed ML-CNN utilized a gradient descent algorithm with optimizer \"Adam\" to find the best convergence path for updating the NN.\n\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc66 = \ud835\udc46\ud835\udc49\ud835\udc40(\ud835\udc53\ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc53\ud835\udc62\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b) (8)"
        },
        {
            "heading": "B. ON-LINE SOLVING",
            "text": "After training the model, we utilize the trained model \u2018ML-\nCNN()\u2019 to predict the priority of the new JSSP instance (which\ncould be differ-scale instances) at step 5 of the online solving\nprocess, as shown in (9).\n\ud835\udc43\ud835\udc5f\ud835\udc56\ud835\udc5c\ud835\udc5f\ud835\udc56\ud835\udc61\ud835\udc66\u2032 = \ud835\udc40\ud835\udc3f \u2212 \ud835\udc36\ud835\udc41\ud835\udc41(\ud835\udc3c\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61\ud835\udc51\ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc59\ud835\udc52\ud835\udc51 \u2032 , \ud835\udc3c\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61\ud835\udc60\ud835\udc66\ud835\udc60\ud835\udc61\ud835\udc52\ud835\udc5a \u2032 ) (9)\nWhere \ud835\udc3c\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61\ud835\udc51\ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc59\ud835\udc52\ud835\udc51 \u2032 and \ud835\udc3c\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61\ud835\udc60\ud835\udc66\ud835\udc60\ud835\udc61\ud835\udc52\ud835\udc5a \u2032 are corresponding new instance's input features. Then, the priority sequence is decoded into JSSP pattern \ud835\udc4b = [\ud835\udc5a, \ud835\udc5b, 2] using one priority-\nto-solution algorithm at step 6, same as [10], [28]. Where \ud835\udc4b\nrecords the start time of each operation at a certain machine; the first column of each element \ud835\udc4b[: , : ,0] is the corresponding\njob while the second \ud835\udc4b[: , : ,1] is the corresponding\nsuboperation index at a certain machine. For instance, the\nmachine one in the Figure 2 has six elements, denoted as [[2,1],\n[3,2], [6,2], [5,3], [1,2], [4,3]]. It means operation 2.1 will be\nprocessed the first at machine one, and then operation 3.2,\n6.2\u2026, etc.\nThe converted solution is encoded as the input (\ud835\udc3a\ud835\udc59\ud835\udc5c\ud835\udc4f\ud835\udc4e\ud835\udc59\ud835\udc5d\ud835\udc4e\ud835\udc61\u210e) using algorithm 1 to find the best local path in step 7 by the ILS algorithm. The process of ILS is to find the best local path, as shown in (10).\n\ud835\udc46\ud835\udc5c\ud835\udc59\ud835\udc62\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b = \ud835\udc3c\ud835\udc3f\ud835\udc46(\ud835\udc3a\ud835\udc59\ud835\udc5c\ud835\udc4f\ud835\udc4e\ud835\udc59_\ud835\udc5d\ud835\udc4e\ud835\udc61\u210e) (10)\nWhere \ud835\udc3c\ud835\udc3f\ud835\udc46() is applied ILS algorithm, as described in Algorithm 2. \ud835\udc3a\ud835\udc59\ud835\udc5c\ud835\udc4f\ud835\udc4e\ud835\udc59\ud835\udc5d\ud835\udc4e\ud835\udc61\u210e is generated from algorithm 1, which guides the ILS to find the best local path and speed up this process. Moreover, the global path generation algorithm, as described in Algorithm 1.\nAlgorithm 1: The global path generation algorithm\nInput: The ML-CNN generated JSSP pattern \ud835\udc4b, machine numbers\n\ud835\udc5a, job numbers \ud835\udc5b.\nOutput: The global path \ud835\udc3a\ud835\udc59\ud835\udc5c\ud835\udc4f\ud835\udc4e\ud835\udc59_\ud835\udc5d\ud835\udc4e\ud835\udc61\u210e which satisfies ILS\u2019s input.\n1. \ud835\udc57\ud835\udc5c\ud835\udc4f = \ud835\udc4b[: , : ,0], suboperation = X[: , : ,1], \ud835\udc5a\ud835\udc4e\ud835\udc50\u210e\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc60 = [] 2. for i=0 to \ud835\udc5a : 3. machineopeation = [] 4. for j= 0 to \ud835\udc5b: 5. machineopeation. append([i, job[i, j], sub_operation[i, j]]) 6. \ud835\udc5a\ud835\udc4e\ud835\udc50\u210e\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc60. append(machine_operation) 7. \ud835\udc60\ud835\udc5c\ud835\udc5f\ud835\udc61\ud835\udc52\ud835\udc51 = \ud835\udc60\ud835\udc5c\ud835\udc5f\ud835\udc61\ud835\udc52\ud835\udc51(\ud835\udc5a\ud835\udc4e\ud835\udc50\u210e\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc60, \ud835\udc58\ud835\udc52\ud835\udc66 = \ud835\udc65: (\ud835\udc65[1], \ud835\udc65[2])) 8. \ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc60 = [], \ud835\udc5a\ud835\udc4e\ud835\udc50\u210e\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc60 = [] 9. for i in \ud835\udc60\ud835\udc5c\ud835\udc5f\ud835\udc61\ud835\udc52\ud835\udc51: 10. \ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc60. \ud835\udc4e\ud835\udc5d\ud835\udc5d\ud835\udc52\ud835\udc5b\ud835\udc51(\ud835\udc56[1]) 11. \ud835\udc5a\ud835\udc4e\ud835\udc50\u210e\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc60. \ud835\udc4e\ud835\udc5d\ud835\udc5d\ud835\udc52\ud835\udc5b\ud835\udc51(\ud835\udc56[0])\nReturn \ud835\udc3a\ud835\udc59\ud835\udc5c\ud835\udc4f\ud835\udc4e\ud835\udc59\ud835\udc5d\ud835\udc4e\ud835\udc61\u210e = \ud835\udc57\ud835\udc5c\ud835\udc4f\ud835\udc60 + \ud835\udc5a\ud835\udc4e\ud835\udc50\u210e\ud835\udc56\ud835\udc5b\ud835\udc52\ud835\udc60\nIn Algorithm 2, we defined maximum iteration \ud835\udc40\ud835\udc3c as 100000, maximum no improved value \ud835\udc40\ud835\udc41\ud835\udc3c is 0.02, and acceptance criteria as \ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b < \ud835\udc40\ud835\udc3c and \ud835\udc41\ud835\udc3c\ud835\udc47 < \ud835\udc40\ud835\udc41\ud835\udc3c \u2217 \ud835\udc40\ud835\udc3c . If the makespan did not reduce 0.02 compared to the previous step within \ud835\udc40\ud835\udc41\ud835\udc3c \u2217 \ud835\udc40\ud835\udc3c steps, the algorithm would be stopped. Otherwise, it will run to search for the best local path until 100000 steps. The advantage of this setting is that a better solution can be found in a particular duration. Moreover, the proposed method adopted the random search method to search the neighborhood\u2019s solution, implemented by \ud835\udc41\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc4f\ud835\udc5c\ud835\udc5f\ud835\udc46\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc50\u210e() . Significantly, the ILS algorithm randomly selects one job and then selects two operations of this job randomly to the swap.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nVOLUME XX, 2017 9\nAlgorithm 2: The applied ILS algorithm to search for the best local path for solving\nJSSP\nInput: Algorithm 1 generated global path \ud835\udc3a\ud835\udc59\ud835\udc5c\ud835\udc4f\ud835\udc4e\ud835\udc59\ud835\udc5d\ud835\udc4e\ud835\udc61\u210e.\nOutput: The best local path \ud835\udc46\ud835\udc5c\ud835\udc59\ud835\udc62\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b and corresponding\nmakespan \ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc52.\nGiven maximum iteration \ud835\udc40\ud835\udc3c = 100000, maximum no improved\nvalue \ud835\udc40\ud835\udc41\ud835\udc3c = 0.02, no improved solution times \ud835\udc41\ud835\udc3c\ud835\udc47 = 0, history\nrecord list \ud835\udc3b. 1. \ud835\udc600 \u2190 \ud835\udc3a\ud835\udc59\ud835\udc5c\ud835\udc4f\ud835\udc4e\ud835\udc5d\ud835\udc4e\ud835\udc61\u210e\n2. \ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc52 \u2190 \ud835\udc5a\ud835\udc4e\ud835\udc58\ud835\udc52\ud835\udc60\ud835\udc5d\ud835\udc4e\ud835\udc5b(\ud835\udc60 0) #Caculate the makespan\nof the initial solution.\n3. \ud835\udc3b = [\ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc52 \ud835\udc53\ud835\udc5c\ud835\udc5f \ud835\udc56 \ud835\udc56\ud835\udc5b \ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc54\ud835\udc52(5)] #Defined history\nrecords\n4. While \ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b < \ud835\udc40\ud835\udc3c and \ud835\udc41\ud835\udc3c\ud835\udc47 < \ud835\udc40\ud835\udc41\ud835\udc3c \u2217 \ud835\udc40\ud835\udc3c: 5. \ud835\udc60\u2032 = \ud835\udc41\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc4f\ud835\udc5c\ud835\udc5f\ud835\udc46\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc50\u210e(\ud835\udc600) #Generate the\nneighborhood\u2019s solution\n6. \ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc52\u2032 \u2190 \ud835\udc5a\ud835\udc4e\ud835\udc58\ud835\udc52\ud835\udc60\ud835\udc5d\ud835\udc4e\ud835\udc5b(\ud835\udc60\u2032) #Caculate the makespan 7. if \ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc52\u2032 \u2265 \ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61_\ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc52: 8. \ud835\udc41\ud835\udc3c\ud835\udc47 = \ud835\udc41\ud835\udc3c\ud835\udc47 + 1 9. else: 10. \ud835\udc41\ud835\udc3c\ud835\udc47 = 0 11. \ud835\udc60\ud835\udc5c\ud835\udc59\ud835\udc62\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b = \ud835\udc60\u2032 12. \u210e = \ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b%\ud835\udc3b #Update the history record 13. if \ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc52\u2032 < \ud835\udc3b[\u210e] or \ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc52\u2032 < \ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc52 : 14. \ud835\udc600 = \ud835\udc60\u2032 15. \ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc52 = \ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc52\n\u2032 16. \ud835\udc3b[\u210e] = \ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc52\u2032\nReturn \ud835\udc60\ud835\udc5c\ud835\udc59\ud835\udc62\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b, \ud835\udc4f\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc52"
        },
        {
            "heading": "IV. Experimental Verification",
            "text": "To verify the proposed method\u2019s effectiveness, the authors\nimplemented the proposed method based on an operating\nsystem of ubuntu 16. 03 with 23GB memory at a speed rate of\n3.6 GHz. The Tensorflow-backend Keras is used to implement\nML-CNN, and the programming language is python 3.6.\nMoreover, the loss function of ML-CNN is categorical cross-\nentropy; and the activation function in each hidden layer is\nReLU, except for the last dense layer is \u2018softmax.\u2019"
        },
        {
            "heading": "A. DATA AND TRAINING PROCESS",
            "text": "Data. Considering the performance and training time, the\nauthors randomly generate 2, 000 small-scale 5 \u00d7 5 JSSP\ninstances (step 1 in Fig. 3), whose processing time ranges from\n10 to 100. GA [42] is used to generate the corresponding\nsolutions (step 2 described in Fig. 3). Simultaneously each\nJSSP instance is split into 25 subproblems, and we calculate\ntwo-level features for each subproblem. The corresponding\nlabel is the priority position at each machine, as we described in Section 2.2. Thus, we could get 50, 000=2, 000\u00d75\u00d75\nsamples, which will be used to train and test the model.\nTraining Process. The authors used the 5-fold cross-\nvalidation method to moderately train and test the proposed\nML-CNN. i.e., the data was split into five parts; four are used\nas training parts, while the rest is for testing at one loop. It will\ncontinue to run five times to evaluate each part. In each\nrunning, 80% of the training set is used for training, the 20%\nof them is to validate and find the best model with the early\nstop strategy in five patience. Moreover, we set the \u2018val_acc\u2019\nas the evaluation metric and the training epoch as 100 when\ntraining the model. If the validation accuracy did not increase\nwith five epochs in each training process, the training process\nwould be ended. The previous best model is saved for testing;\notherwise, it will run until 100 epochs. The whole training\nprocess is shown in Fig. 5.\nFIGURE 5. The workflow for training the proposed ML-CNN."
        },
        {
            "heading": "B. COMPARATIVE CLASSIFICATION ACCURACY",
            "text": "As the deep learning-based method treats the JSSP as one\nclassification problem, it is necessary to see its classification\nperformance.\nTo validate the proposed ML-CNN\u2019s effectiveness, the authors compared the proposed ML-CNN to other leading methods, including SVM [24], RF [25], shallow NN [28], and HDNN [10]. The authors implemented SVM and RF using default configurations of the 'sklearn' library. The shallow NN has the structure of \u2018Input( \ud835\udc3c\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61\ud835\udc60\ud835\udc66\ud835\udc60\ud835\udc61\ud835\udc52\ud835\udc5a )->Dense(12)>Dense(12)->Dense(10)->Output(\ud835\udc5a) ', which strictly follows the author's paper. HDNN is trained with the author's code, available at github.com/zangzelin/HDNNMv2.0.\nThe comparison results use the 5-fold cross-validation method, as shown in Table 2. The results indicated that the proposed method performed the best for all folds and averaged metrics, whose average accuracy is 59.60%. SVM and ANN perform the worse, whose averaged accuracy is 45.57%. The performance of those methods could be ranked as: Proposed>HDNN>RF>SVM>ANN.\nAlthough the proposed method's accuracy is not very high, it does not require massive training samples, which could save a lot of computing resourcing. Besides, it trains the model on small-scale JSSP instances and could deal with large-scale complex JSSP in-stances. With the increasing complexity of the manufacturing industry, GA-based and population-based methods may not solve very complex JSSP instances within\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nVOLUME XX, 2017 9\nthe required time duration. The proposed method trains the model offline on small-scale JSSP instance samples. The trained model is used to predict the priority of each machine for each suboperation online for large-scale JSSP instances. The priority will be encoded as \ud835\udc3a\ud835\udc59\ud835\udc5c\ud835\udc4f\ud835\udc4e\ud835\udc59\ud835\udc5d\ud835\udc4e\ud835\udc61\u210e to guide ILS to search best local path to solve JSSP. Noticed that we only need one approximate GA solution as the global path to help ILS find the best local path. Therefore, the proposed method could satisfy our needs with an accuracy of 59.60%."
        },
        {
            "heading": "C. THE EFFECTIVENESS OF EACH COMPONENT IN ML-CNN",
            "text": "The authors did an ablation study to explore each component's\neffectiveness in the proposed deep model ML-CNN.\nSignificantly, the authors designed CNN with system-level\n(CNNsystem) input to verify detailed-level input's effectiveness. Designed CNN with detailed-level (CNNdetailed) input to verify system-level input\u2019s effectiveness. Designed ML-CNN\nwithout SVM (ML-CNNwo) to validate its effectiveness, respectively. The comparison results in average accuracy, as\nshown in Fig. 6. The results showed that the application of\ndetail-level has improved performance by 31.92%, which\ncould be conducted by comparing the proposed method with\nCNNsystem. The application of detailed-level input has improved the performance by 0.71%, which could be\nconducted by comparing the proposed with CNNdetailed. Moreover, the SVM has improved 0.05% of average accuracy\ncompared to ML-CNNwo. As one JSSP instance is split into one complex multi-classification problem, the classification\naccuracy is hard to improve. Therefore, each component in the\nproposed method is essential."
        },
        {
            "heading": "D. THE CAPACITY FOR SOLVING JSSP",
            "text": "To validate the effectiveness of the proposed method for\nsolving JSSP, the authors test the proposed method on 82\nfamous JSSP instances, as described in Table 3. The job\nranges from 6 to 50, and the machine ranges from 6 to 20.\n(1) Compared to general learning methods\nFirstly, we compared the proposed method with recent general\ndeep methods, including SVM [24], RF [25], shallow NN [28],\nHDNN [10], and GA on six JSSP instances, which is to\ncompare them fairly reported in [10]. The comparative results\nin makespan and solving time, as shown in Table 4. Noticed\nthat the authors trained the model with small-scale JSSP instances, especially on 5\u00d7 5 JSSP instances, and tested on\nlarge-scale JSSP instances for the proposed method. For the\nHDNN, we adopted the results reported in [10], which is\ntrained by the same-scale JSSP instances. Also, we trained the\nHDNN(1) on 246 same-scale JSSP instances to compare.\nThe findings show that the proposed method is the best among learning-based methods in makespan. It is near to optimal solution for all instances within tolerable processing time. Although other learning methods require less time, the solution is far from optimal. Comparing HDNN, the proposed method could process the JSSP instance more effectively within almost \u00bc time. Compared to the GA, the proposed method showed a significant priority in makespan and solving time. Besides, HDNN(1) shows that it cannot give solutions for abz7 and yn1 in two days (the HDNN needs two days to generate 1000 10\u00d7 10 JSSP solutions with GA for training the model while ML-CNN only needs 3 minutes to generate 2000 5\u00d7 5 training samples). Another finding shows that SVM almost does not require time to process while its performance is the worst. To quantify those methods\u2019 effectiveness, we calculate their scheduling scores as follows:\n\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52\ud835\udc60 = \ud835\udc36\ud835\udc5c\ud835\udc5d\ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc59/\ud835\udc36\ud835\udc4e\ud835\udc59 (11) Where \ud835\udc36\ud835\udc4e\ud835\udc59 is the makespan of the algorithm, \ud835\udc36\ud835\udc5c\ud835\udc5d\ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc59 is the makespan of the optimal method, which indicates the scheduling performance compared to the optimal method. The average scores indicated that the proposed method outperforms others within scores of 0.8958, which proved that the proposed method is near-optimal again. According to the\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nVOLUME XX, 2017 9\naverage scores, those methods could be ranked as: Optimal>Proposed>GA>proposed>HDNN>RF >ANN > SVM. Moreover, only the proposed method, HDNN's scores are higher than 0.8, and the proposed method has improved by 4.46% compared to HDNN.\nIn summary, the proposed method could effectively and fast deal with complex differ-scale JSSP instances with smallscale training samples, providing in-time and objective evidence to manage the production activities.\n(2) Compared to DRL and others\nTo validate the proposed method\u2019s effectiveness and priority compared to the DRL-based method, we compared the proposed method with the recent most accurate DRL-based method: DDQN [33]. To explore the Internal mechanism of the proposed method, we designed other three subexperiments, including GA, ILS, and proposed method without ILS (Proposedwo) on the above 82 JSSP instances. Also, we give an optimal method to calculate the scheduling scores of the proposed method to illustrate its effectiveness. The results in terms of makespan are shown in Table 5, Table 6, and Table 7. The results indicated that the proposed method outperforms all the above methods. Especially, the proposed method won 68=10+36+22 best rankings out of 82 JSSP instances; DDQN won 19=2+12+7 best rankings; ILS won 25=1+23+1 best rankings. GA and Proposedwo did not win any instances. Although the proposed method failed for some cases such as ft20, la03, and la18; It won 68 instances out of 82, which indicated that the proposed method has a good adaptability. The evidence showed that the proposed method has a significant priority compared to the above methods. The effectiveness of those methods could be ranked as: The proposed>ILS>DDQN>GA>Proposedwo.\nMoreover, the scheduling scores showed that the proposed method could get the 21 optimal solutions out of 82 JSSP instances, which could be conducted from the scheduling scores with 1.0. Besides, each method\u2019s scheduling score is given in Fig. 7. The results confirmed the proposed method\u2019s effectiveness for almost all JSSP instances.\nWe give a boxplot to see the distribution for each method\u2019s scheduling score, as shown in Fig. 8. The results showed that the proposed method mainly ranges from 0.8905 to 1.0. Especially, 25% range from 0.9994 to 1.0, 50% are higher than 0.9529, and 75% are higher than 0.8905. The proposed method's lowest score is 0.7186, higher than Proposedwo's highest score of 0.6629. It indicated that ILS plays the main role in solving the JSSP instance. Compared to others, the proposed method showed an advanced priority except for DDQN obtained lowest value is higher than the proposed method. The proposed deep ML-CNN model has guided the ILS to improve the minimum score from 0.6396 to 0.7196.\nTo see the difference between the proposed method and others, we calculate the average scheduling score for each method, as shown in Fig. 9. The results showed that the proposed method obtained average score is 92.94; DDQN is\n90.83, GA is 70.08, ILS is 90.58, and Proposedwo only obtained44.83. The results confirmed the priority of the proposed method again. Compared to DDQN, the proposed method has improved 2.11% of the scheduling score. Compared to GA, the proposed method has improved by 22.86%, which indicates that GA cannot find the best local path, so the scheduling score is still low. Compared to the Proposedwo (ML-CNN), the proposed method has significantly improved 48.11%; it indicated that ILS could find the best local path under the GA approximate solution to improve scheduling score; Compared to the ILS, the utilization of ML-CNN has improved 2.36%. In summary, ML-CNN could help the proposed method find the global path; ILS is the key to searching the best local path; By combining the ML-CNN and ILS, the proposed method could solve JSSP more effectively within less time. Moreover, even though ML-CNN obtained a scheduling score of 44.83%, it saves much more time than GA (see Table 4) and can guide the proposed method to find the global path.\ncomparative method.\nTo quantify the difference between the proposed method and others, we calculate the \ud835\udc5d-value of the \ud835\udc61 -test for each\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nVOLUME XX, 2017 9\nmethod, as shown in Table 8. The results indicated that the proposed method has a significant difference with GA and Proposedwo as the \ud835\udc5d-value is much less than 0.05. Although all the methods are far from the optimal method, the proposed method outperforms others, and it obtained the highest \ud835\udc5d - value of 9.74 e-14. Those methods could be divided into four levels according to their \ud835\udc5d-values to optimal as: The proposed>ILS->DDQN->GA->Proposedwo. Moreover, the proposed method trains the ML-CNN offline and gives the solution online, saving much time. In summary, the proposed method has absolute priority for solving JSSP.\nTABLE IX. THE \ud835\udc61-TEST RESULTS AMONG COMPARATIVE METHODS.\nMethods DDQN GA Proposedwo ILS Proposed Optimal DDQN 1.0 7.25e-24 5.46e-79 0.8452 0.0787 4.40 e-22\nGA 1.0 4.44e-29 8.12e-22 1.07e-27 4.11e-44 Proposedwo 1 6.04e-73 3.87e-80 1.36 e-107\nILS 1.0 0.0766 4.01 e-17 Proposed 1.0 9.74 e-14 Optimal 1.0"
        },
        {
            "heading": "V. DISCUSSION",
            "text": "JSSP is critical to intelligent production, which aims to arrange\neach suboperation at the functional machine with the\nminimum makespan, as shown in Fig. 2. This manuscript\nproposed a novel learning-based method to solve JSSP, as\nshown in Fig. 4. The proposed method consists of two parts:\noffline training and online solving. A novel deep model ML-\nCNN is proposed in offline training to find the approximate\nglobal path from the GA solution. Significantly, it utilized\ndetailed-level and system-level inputs to reflect the production\nstatutes fully, as defined in Table 1. Each input is fed into 1-D\nCNN to mine the hidden features, and the extracted features\nare fused to predict the priority of each suboperation.\nMoreover, the proposed ML-CNN applied SVM to map the\nhidden features into one higher dimension to the separating\nhyperplane with the largest geometrical interval to classify\neach sample more accurately. The predicted priority is\nencoded into the ILS algorithm using algorithm 1 and\nalgorithm 2 to find the best local path of the JSSP instance.\nTherefore, the proposed method could effectively solve JSSP.\nTo validate the effectiveness of the proposed ML-CNN, this manuscript adopted one five-fold cross-validation method to evaluate it fairly, as shown in Fig. 5. We train the proposed method five times, and averaged metric is used to evaluate. The experiments verified that the proposed method is more accurate than SVM, RF, NN, and current HDNN methods. The results showed that the proposed method could attach an accuracy of 59.60%, which can be found in Table 2. Although its accuracy is not very high, it gives an approximate global path to guide the ILS algorithm to search for the best local path to solve JSSP.\nTo validate each component\u2019s effectiveness in the MLCNN, the authors designed four sub-experiments: CNNsystem to verify detailed-level input\u2019s effectiveness; CNNdetailed to verify system-level input\u2019s effectiveness; ML-CNNwo to validate SVM's effectiveness. The results confirmed each\ncomponent's effectiveness. Significantly, the application of detail-level has improved performance by 31.92%; The application of detailed-level input has improved the performance by 0.71%; The SVM has improved by 0.05%, as shown in Fig. 6.\nTo see the effectiveness of the proposed method for solving JSSP, the authors trained the model on some small-scale JSSP instances and tested it on 82 JSSP instances. Firstly, the authors compared the proposed method with current generallearning methods, including SVM, RF, ANN, and HDNN. The results indicated that the proposed method is more effective than SVM, RF, ANN, and HDNN in terms of makespan and solving time, which could be conducted from\nTable 4. The results indicated that the proposed could obtain a 0.8958 scheduling score. Besides, the proposed method did not require additional transformation compared to HDNN, which could save a lot of training time. Moreover, the proposed method trains ML-CNN on small-scale training samples while it could solve differ-scale real-time JSSP instances. With the increasing complexity in the production, it may require solving even 1000 \u00d7 1000 JSSP instances in the future. The proposed method provides us with a way to solve it.\nSecondly, the authors compared the proposed method with DDQN, GA, Proposedwo, and ILS to show its priority. The results indicated that the proposed method outperforms others. Especially, the proposed method won 68 best rankings out of 82 JSSP instances, it could be conducted from Table 5, Table 6, and Table 7. Besides, the scheduling scores showed that the proposed method could get the 21 optimal solutions out of 82 JSSP instances, which could be conducted from Fig. 7. The proposed method's scores mainly range from 0.8905 to 1.0; it is near to optimal solution, as shown in Fig. 8.\nThe average scores showed that the proposed method obtains 92.94% of the optimal method. It shows an absolute priority compared to others, which could be conducted from Fig. 9. Besides, the authors designed three experiments to show each component's effectiveness. The results indicated that GA could not find the best local path; The utilization of ILS has improved the scheduling score of 48.11%, it is conducted by comparing the proposed method and Proposedwo. The ML-CNN could help the proposed method find the approximate global path, conducted by comparing ILS and the proposed method. In summary, ML-CNN could help the proposed method to find the global path. ILS is the key to searching for the best local path; By combining the ML-CNN and ILS, the proposed method could solve JSSP more effectively within less time.\nTo quantify the difference between the proposed method and others, we calculate the \ud835\udc5d-value of the \ud835\udc61 -test for each method, as shown in Table 8. The results indicated that the proposed method significantly differs from GA and Proposedwo and outperforms others. Those methods could be ranked as: The proposed>ILS>DDQN>GA>Proposedwo.\nThe proposed method's time complexity is around the O(n) for solving one JSSP instance, and its response time is in the second-level, which can process large-scale JSSP instances\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nVOLUME XX, 2017 9\nmore effectively.\nAlthough the proposed method has obtained the average scheduling score of 92. 94%, it still could be improved by using one more accurate global path guiding deep model. Especially, using more proper features in the ML-CNN to improve it. Moreover, the FJSSP is more and more common and important with the rapid development of the modern industry. However, it is more complex than JSSP as one suboperation of one FJSSP instance can be processed by any given machine while JSSP only can be processed by one. i.e., we need to select jobs and machines at the same time for FJSSP while JSSP only requires job selection. Therefore, the effectiveness of the proposed method for the FJSSP needs to be considered and discussed in the future."
        },
        {
            "heading": "VI. CONCLUSION",
            "text": "This manuscript has proposed a fast and effective method to solve JSSP. Firstly, it trains ML-CNN offline to find the approximate global path from the GA solution. The ML-CNN utilizes fourteen detailed-level inputs to reflect detailed production statuses, while four types of system-level input reflect the common statutes of the production process. 1-D CNN extracts the hidden features from two-channel inputs in a parallel mode. The extracted features are fused as the input of the SVM classifier to predict each sub-operations priority. Moreover, the proposed ML-CNN only needs to train once on small-scale JSSP instances offline, and the trained model could give an approximate global path for large differ-scale JSSP instances. Secondly, ML-CNN learned global path is used as the input of ILS to search for the best local path to solve JSSP online effectively.\nMassive experiments have confirmed the effectiveness of the proposed method for solving JSSP, it obtained a nearoptimal solution (92.94% of scheduling scores) within less time. It has improved by 2.11% compared with the current most accurate DDQN. Moreover, in-depth analysis has confirmed the effectiveness of each part in the proposed method. Especially, ML-CNN could help the proposed method find the global path, and ILS is the key to searching for the best local path. In summary, the proposed method could provide an effective and in-time solution for decisionmakers to measure production activities in advance.\nAs discussed above, the proposed ML-CNN\u2019s performance could be improved using more proper features, and the proposed method's effectiveness for FJSSP needs to be discussed. In the future, the authors will utilize more proper features to find the global path for solving FJSSP."
        }
    ],
    "title": "An Adaptive Job Shop Scheduler Using Multi-Level Convolutional Neural Network and Iterative Local Search",
    "year": 2022
}