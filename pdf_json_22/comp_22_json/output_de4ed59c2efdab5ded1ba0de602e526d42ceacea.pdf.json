{
    "abstractText": "We present a novel depth completion approach agnostic to the sparsity of depth points, that is very likely to vary in many practical applications. State-of-the-art approaches yield accurate results only when processing a specific density and distribution of input points, i.e. the one observed during training, narrowing their deployment in real use cases. On the contrary, our solution is robust to uneven distributions and extremely low densities never witnessed during training. Experimental results on standard indoor and outdoor benchmarks highlight the robustness of our framework, achieving accuracy comparable to state-ofthe-art methods when tested with density and distribution equal to the training one while being much more accurate in the other cases. Our pretrained models and further material are available in our project page.",
    "authors": [
        {
            "affiliations": [],
            "name": "Andrea Conti"
        },
        {
            "affiliations": [],
            "name": "Matteo Poggi"
        },
        {
            "affiliations": [],
            "name": "Stefano Mattoccia"
        }
    ],
    "id": "SP:f1508c47606adda70a700d897a10ee9b3ac1a0a3",
    "references": [
        {
            "authors": [
                "Shubhra Aich",
                "Jean Marie Uwabeza Vianney",
                "Md Amirul Islam",
                "Mannat Kaur Bingbing Liu"
            ],
            "title": "Bidirectional attention network for monocular depth estimation",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2021
        },
        {
            "authors": [
                "Tianqi Chen",
                "Emily Fox",
                "Carlos Guestrin"
            ],
            "title": "Stochastic gradient hamiltonian monte carlo",
            "venue": "Proceedings of the 31st International Conference on Machine Learning,",
            "year": 2014
        },
        {
            "authors": [
                "Xinjing Cheng",
                "Peng Wang",
                "Ruigang Yang"
            ],
            "title": "Depth estimation via affinity learned with convolutional spatial propagation network",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Christopher Choy",
                "JunYoung Gwak",
                "Silvio Savarese"
            ],
            "title": "4d spatio-temporal convnets: Minkowski convolutional neural networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Jifeng Dai",
                "Haozhi Qi",
                "Yuwen Xiong",
                "Yi Li",
                "Guodong Zhang",
                "Han Hu",
                "Yichen Wei"
            ],
            "title": "Deformable convolutional networks",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),",
            "year": 2017
        },
        {
            "authors": [
                "David Eigen",
                "Christian Puhrsch",
                "Rob Fergus"
            ],
            "title": "Depth map prediction from a single image using a multi-scale deep network",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Abdelrahman Eldesokey",
                "Michael Felsberg",
                "Karl Holmquist",
                "Michael Persson"
            ],
            "title": "Uncertainty-aware cnns for depth completion: Uncertainty from beginning to end",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Huan Fu",
                "Mingming Gong",
                "Chaohui Wang",
                "Kayhan Batmanghelich",
                "Dacheng Tao"
            ],
            "title": "Deep ordinal regression network for monocular depth estimation",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Andreas Geiger",
                "Philip Lenz",
                "Raquel Urtasun"
            ],
            "title": "Are we ready for autonomous driving? the kitti vision benchmark suite",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2012
        },
        {
            "authors": [
                "Clement Godard",
                "Oisin Mac Aodha",
                "Michael Firman",
                "Gabriel J. Brostow"
            ],
            "title": "Digging into self-supervised monocular depth estimation",
            "venue": "IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "Vitor Guizilini",
                "Rares Ambrus",
                "Wolfram Burgard",
                "Adrien Gaidon"
            ],
            "title": "Sparse auxiliary networks for unified monocular depth prediction and completion",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Mu Hu",
                "Shuling Wang",
                "Bin Li",
                "Shiyu Ning",
                "Li Fan",
                "Xiaojin Gong"
            ],
            "title": "Penet: Towards precise and efficient image guided depth completion",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2021
        },
        {
            "authors": [
                "Gao Huang",
                "Zhuang Liu",
                "Laurens Van Der Maaten",
                "Kilian Q Weinberger"
            ],
            "title": "Densely connected convolutional networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "Proceedings of the 32nd International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Xiaowen Jiang",
                "Valerio Cambareri",
                "Gianluca Agresti",
                "Cynthia I Ugwu",
                "Adriano Simonetto",
                "Pietro Zanuttigh",
                "Fabien Cardinaux"
            ],
            "title": "A Low Memory Footprint Quantized Neural Network for Depth Completion of Very Sparse Time-of- Flight Depth Maps",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops,",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Karsch",
                "Ce Liu",
                "Sing Bing Kang"
            ],
            "title": "Depth transfer: Depth extraction from video using non-parametric sampling",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2014
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "CoRR, abs/1412.6980,",
            "year": 2015
        },
        {
            "authors": [
                "Jin Han Lee",
                "Myung-Kyu Han",
                "Dong Wook Ko",
                "Il Hong Suh"
            ],
            "title": "From big to small: Multi-scale local planar guidance for monocular depth estimation, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Juan-Ting Lin",
                "Dengxin Dai",
                "Luc Van Gool"
            ],
            "title": "Depth estimation from monocular images and sparse radar data",
            "venue": "In International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2020
        },
        {
            "authors": [
                "Sifei Liu",
                "Shalini De Mello",
                "Jinwei Gu",
                "Guangyu Zhong",
                "Ming-Hsuan Yang",
                "Jan Kautz"
            ],
            "title": "Learning affinity via spatial propagation networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Gregor Luetzenburg",
                "Aart Kroon",
                "Anders Bj\u00f8rk"
            ],
            "title": "Evaluation of the apple iphone 12 pro lidar for an application in geosciences",
            "venue": "Scientific Reports,",
            "year": 2021
        },
        {
            "authors": [
                "Fangchang Ma",
                "Guilherme Venturelli Cavalheiro",
                "Sertac Karaman"
            ],
            "title": "Self-supervised sparse-to-dense: Self-supervised depth completion from lidar and monocular camera",
            "venue": "In ICRA,",
            "year": 2019
        },
        {
            "authors": [
                "Fangchang Ma",
                "Sertac Karaman"
            ],
            "title": "Sparse-to-dense: Depth prediction from sparse depth samples and a single image",
            "venue": "In ICRA,",
            "year": 2018
        },
        {
            "authors": [
                "David J.C. MacKay"
            ],
            "title": "A practical bayesian framework for backpropagation networks",
            "venue": "Neural Computation,",
            "year": 1992
        },
        {
            "authors": [
                "Pushmeet Kohli Nathan Silberman",
                "Derek Hoiem",
                "Rob Fergus"
            ],
            "title": "Indoor segmentation and support inference from rgbd images",
            "venue": "In ECCV,",
            "year": 2012
        },
        {
            "authors": [
                "D.A. Nix",
                "A.S. Weigend"
            ],
            "title": "Estimating the mean and variance of the target probability distribution",
            "venue": "In Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN\u201994),",
            "year": 1994
        },
        {
            "authors": [
                "Jinsun Park",
                "Kyungdon Joo",
                "Zhe Hu",
                "Chi-Kuei Liu",
                "In So Kweon"
            ],
            "title": "Non-local spatial propagation network for depth completion",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Soumith Chintala",
                "Gregory Chanan",
                "Edward Yang",
                "Zachary DeVito",
                "Zeming Lin",
                "Alban Desmaison",
                "Luca Antiga",
                "Adam Lerer"
            ],
            "title": "Automatic differentiation in pytorch",
            "venue": "In NIPS 2017 Workshop on Autodiff,",
            "year": 2017
        },
        {
            "authors": [
                "Matteo Poggi",
                "Filippo Aleotti",
                "Fabio Tosi",
                "Stefano Mattoccia"
            ],
            "title": "On the uncertainty of self-supervised monocular depth estimation",
            "venue": "In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Siyuan Qiao",
                "Yukun Zhu",
                "Hartwig Adam",
                "Alan Yuille",
                "Liang-Chieh Chen"
            ],
            "title": "Vip-deeplab: Learning visual perception with depth-aware video panoptic segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Thinal Raj",
                "Fazida Hanim Hashim",
                "Aqilah Baseri Huddin",
                "Mohd Faisal Ibrahim",
                "Aini Hussain"
            ],
            "title": "A survey on lidar scanning",
            "venue": "mechanisms. Electronics,",
            "year": 2020
        },
        {
            "authors": [
                "Ren\u00e9 Ranftl",
                "Katrin Lasinger",
                "David Hafner",
                "Konrad Schindler",
                "Vladen Koltun"
            ],
            "title": "Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer",
            "venue": "In IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI),",
            "year": 2020
        },
        {
            "authors": [
                "Olga Russakovsky",
                "Jia Deng",
                "Hao Su",
                "Jonathan Krause",
                "Sanjeev Satheesh",
                "Sean Ma",
                "Zhiheng Huang",
                "Andrej Karpathy",
                "Aditya Khosla",
                "Michael Bernstein"
            ],
            "title": "Imagenet large scale visual recognition challenge",
            "venue": "International journal of computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Jie Tang",
                "Fei-Peng Tian",
                "Wei Feng",
                "Jian Li",
                "Ping Tan"
            ],
            "title": "Learning guided convolutional network for depth completion",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Jonas Uhrig",
                "Nick Schneider",
                "Lukas Schneider",
                "Uwe Franke",
                "Thomas Brox",
                "Andreas Geiger"
            ],
            "title": "Sparsity invariant cnns",
            "venue": "In 2017 International Conference on 3D Vision (3DV),",
            "year": 2017
        },
        {
            "authors": [
                "Max Welling",
                "Yee Whye Teh"
            ],
            "title": "Bayesian learning via stochastic gradient langevin dynamics",
            "venue": "In Proceedings of the 28th International Conference on International Conference on Machine Learning,",
            "year": 2011
        },
        {
            "authors": [
                "Alex Wong",
                "Safa Cicek",
                "Stefano Soatto"
            ],
            "title": "Learning topology from synthetic data for unsupervised depth completion",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Wong",
                "Xiaohan Fei",
                "Stephanie Tsuei",
                "Stefano Soatto"
            ],
            "title": "Unsupervised depth completion from visual inertial odometry",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 1899
        },
        {
            "authors": [
                "Alex Wong",
                "Stefano Soatto"
            ],
            "title": "Unsupervised depth completion with calibrated backprojection layers",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Saining Xie",
                "Ross Girshick",
                "Piotr Doll\u00e1r",
                "Zhuowen Tu",
                "Kaiming He"
            ],
            "title": "Aggregated residual transformations for deep neural networks",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "Yao Yao",
                "Zixin Luo",
                "Shiwei Li",
                "Tian Fang",
                "Long Quan"
            ],
            "title": "Mvsnet: Depth inference for unstructured multi-view stereo",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Yiming Zhao",
                "Lin Bai",
                "Ziming Zhang",
                "Xinming Huang"
            ],
            "title": "A surface geometry model for lidar depth completion",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Depth perception is pivotal to a variety of applications in robotics, scene understanding and more, and for this reason, it has been intensively investigated for decades. Among popular systems leveraging depth estimation, it is worth mentioning autonomous driving [9], path planning and aug-\nmented reality. To date, accurate depth perception is demanded either to multi-view imaging approaches [44] or to specifically designed sensors such as ToF (Time of Flight) or LiDAR (Light Detection and Ranging). Although more expensive than standard cameras, depth sensors usually allow for higher accurate measurements even though at a lower spatial resolution. On the one hand, ToF sensors are cheap, small, and have been recently integrated into mobile consumer devices [16, 23]. They perturb the scene through coded signals unable to cope with outdoor daytime environments. To limit power consumption, a sparse emitting pattern is used, yielding meaningful depth measures for only a few points in the scene (\u223c500 points) [16]. On the other hand, LiDAR sensors employ a moving array of laser emitters scanning the scene and outputting a point cloud [33], which becomes a sparse depth map once projected over the image camera plane due to its much higher resolution. Devices leveraging such technology are expensive and bulky however, being applicable even in daylight outdoor environments, became standard for autonomous driving applications [37]. Since all these depth sensors provide \u2013 for different reasons \u2013 only sparse information, techniques aimed at recovering a dense depth map from an RGB image and a few measurements have gained much popularity in recent years [25, 29, 3]. ar X iv :2 21 2. 00 79\n0v 1\nUnfortunately, in real scenarios LiDAR and ToF sensors are affected by additional issues other than sparsity, which may easily lead even to sparser depth points often unevenly distributed. For instance, the noise originating from multi-path interference \u2013 when multiple bouncing rays from different scene points collide on the same pixel \u2013 might lead the sensor to invalidate the measurement and consequently reduce density. Moreover, low-reflectivity surfaces/materials absorb the whole emitted signal while others reflect it massively, leading to saturation. Despite the two opposite behaviors, depth cannot be reliably measured in both cases, possibly leading to large, unobserved regions.\nState-of-the-art depth completion techniques are fragile and fail at reconstructing the structure of the scene for areas where no depth points are available or when the sparsity changes significantly compared to the one used at training time. Indeed, the incapacity to deal with uneven spatial distributions of the sparse depth points \u2013 which will be unveiled in this work \u2013 threatens the possibility of deploying such solutions in different practical contexts. Moreover, this behaviour also prevents their seamless deployment when using a different sensor inferring the depth according to a spatial pattern different from the one used while training (e.g., switching from an expensive Velodyne [38] LiDAR system to a cheaper one).\nUnfortunately, as reported in this paper and shown in"
        },
        {
            "heading": "2. Related Work",
            "text": "Depth Prediction. Except for a few attempts to solve monocular depth prediction through non-parametric approaches [17], the practical ability to solve this ill-posed problem has been achieved only with the deep learning revolution. At first, deploying plain convolutional neural networks [6] and then, through more complex approaches. Specifically, [8] casts the problem as a classification task, [1] exploits a bidirectional attention mechanism, [19] introduces novel local planar guidance layers to better perform the decoding phase, [32] jointly computes panoptic segmentation to improve depth prediction performance, [34] unifies multiple depth sources to coherently train a neural network to better generalize. The previous methods require a massive quantity of training data to achieve proper performance in unknown environments thus self-supervised paradigms gained much attention. For instance, [10] relies on a supervisory signal extracted from a monocular video stream.\nDepth Completion. Depth completion aims at densifying the sparse depth map obtained by an active depth sensor, providing sparser to denser measurements depending on the technology \u2013 e.g., as evident by comparing Radar [20] versus LiDAR [9] sensors. This task has been tackled either by leveraging an additional RGB image or barely using the sparse depth data. Although most methods rely on learning-based paradigms, [45] proposes a non-parametric handcrafted method. Regarding deep-learning methods, [25] was among the first to tackle the problem by jointly feeding a neural network with the RGB frame and the sparse depth points to densify the latter. Observing that manipulating sparse data is sub-optimal for convolutions, [37, 4] proposed custom convolutional layers explicitly taking into account sparsity. Eventually, guided spatial propagation techniques have demonstrated superior performance. At first, [21] proposed a network able to learn local affinities to guide the depth expansion, this strategy was improved initially by [3] and then by [29]. Based on a similar principle, [36] proposes content-dependent and spatially-variant kernels for multi-modal feature fusion. [7] performs depth completion also modeling the confidence of the sparse input depth and the densified output. In a parallel track, a few works focused on unsupervised training strategies for depth completion [24, 41, 40, 42]. Finally, [11] proposes an approach to deal with depth completion and depth prediction. Even though this seems similar to our research, it is only loosely related. First, it cannot deal with different sparsities but only with the total absence of the sparse depth points. Second, to achieve their goal, they need a specific training procedure and an additional branch to handle the availability of sparse depth data. In contrast, our peculiar network design addresses both issues.\nUncertainty Estimation. Evaluating the estimated value\u2019s uncertainty (or confidence) is essential in many cir-\ncumstances. For neural networks, it has been widely explored either the use of Bayesian frameworks [26, 39, 2] or strategies jointly predicting the mean and variance of the network\u2019s output distribution [28]. For depth completion, [7] proposed to jointly compute the confidence of the sparse input depth and of the densified output. While, for monocular depth prediction, [31] has deeply investigated uncertainty for self-supervised approaches."
        },
        {
            "heading": "3. Sparsity Agnostic Framework",
            "text": "To tackle depth completion, we start from our previous observations. Specifically, as pointed out by [37, 4], 2D convolutions struggle to manipulate sparse information. Additionally, we further notice that the density of such input depth data and its spatial distribution \u2013 which could be highly uneven \u2013 might lead state-of-the-art networks to catastrophic failures, as depicted at the bottom of Figure 1. Moreover, we argue that these networks mostly rely on the sparse depth input overlooking the image content substantially ignoring the geometric structure depicted in it.\nSpAgNet relies on an encoder-decoder structure with skip connections, as depicted in Figure 2. However, unlike current depth completion techniques [29, 3, 11, 7], we do not feed the encoder with sparse depth information for the reasons previously outlined. We extract instead features from the RGB frame only in order to get rid of the sparse input data and, consequently, its density. This strategy allows us to constrain the network to exploit the image content fully and, as we will discuss later, to enforces the network to extract the geometry of the scene from RGB.\nThe decoding step predicts \u2013 iteratively and at multiple scales \u2013 dense depth from the RGB image and fuse it with the sparse input data. The first iterative step takes the input features extracted from the RGB image and generates a lower scale depth map and a confidence map. Then, the next iterative steps process the same inputs plus the depth map and its confidence, both augmented with the sparse input points computed in the previous iteration. Moreover, since\neach intermediate depth map provides information up to a scale factor, we scale it according to the sparse input points before each augmenting step. We do so due to the ill-posed nature of monocular depth prediction. Experimental results will corroborate our design choice, especially when dealing with a few sparse input points. At the end of the iterative steps, we apply the non-local spatial propagation module proposed in [29] to refine the depth map inferred by the network. Figure 2 describes the whole framework."
        },
        {
            "heading": "3.1. Encoder Architecture",
            "text": "Since our framework encodes features from the image only, we can leverage as encoding backbone any pretrained network. Such backbone is pre-trained on ImageNet [35]. Among the multiple choices [12, 14, 43] we choose ResNeXt50 [43] due to its good trade-off between performance and speed. Specifically, it downsamples the image to scales 12 , 1 4 , 1 8 , 1 16 and 1 32 and the features used in the decoding step as input and skip connection."
        },
        {
            "heading": "3.2. Scale and Place Module",
            "text": "In our proposal, the core Scale and Place (S&P) module is in charge of inferring a dense and scaled depth map and its confidence. It takes as input the backbone features, the output of the previous S&P module at a different scale, and the sparse depth points as depicted in Figure 2.\nSpecifically, S&P leverages the input features to jointly generate an initial up-to-scale depth map and its confidence deploying a stem block composed of two convolutional layers and two heads in charge of generating them. Each convolutional layer consists of a 2D convolution, a batch normalization [15] and a Leaky ReLU. Then in the Scale step, the S&P module performs a weighted linear regression to scale the depth map according to the available sparse input points, weighted by means of confidence. The parameters of the weighted linear regression can be computed in closed form and in a differentiable way, as described in Eq. 1 where pi is the predicted depth value and ci its confidence\ncorresponding to an available input sparse point si.\n\u03b2 = \u2211 i ci(pi \u2212 p\u0302)(si \u2212 s\u0302)\u2211\ni ci(pi \u2212 p\u0302)2 \u03b1 = s\u0302\u2212 \u03b2p\u0302 (1)\np\u0302 = \u2211 i cipi\u2211 i ci s\u0302 = \u2211 i cisi\u2211 i ci\nThen, in the Place step, for those points where a sparse input depth value is available, we replace the corresponding value in the scaled depth map with it. Additionally, we update the same point in the confidence map with the highest score. The latter step can be summarized as follows\nD\u0302[x, y] =\n{ Ds[x, y] if H[x, y] = 0\nH[x, y] if H[x, y] \u0338= 0 (2)\nC\u0302[x, y] =\n{ Cs[x, y] if H[x, y] = 0\n1 if H[x, y] \u0338= 0 (3)\nwhere Ds is the scaled depth map, Cs is the confidence map and H is a sparse depth map containing zeros where an input sparse depth point is not available. The predicted confidence has an empirically chosen range of [0.1 .. 0.9] while we associate confidence 1 to each valid value in H .\nWe apply the S&P module at scales 18 , 1 4 and 1 2 . The module at 18 computes the initial depth and confidence maps leveraging only the RGB features. The others take in input also the up-sampled dense depth and confidence maps from the previous module in order to iteratively correct the prediction relying on both the predicted depth and the injected sparse points. Thus, with this strategy, the decoder does not deal directly with sparse data in any of its steps. Nonetheless, the network can locate and effectively leverage reliable sparse information. An example of this mechanism is\nshowed in Figure 3, where can be clearly seen how the network learns to locate the most reliable depth values as those closer to the groundtruth depth.\nIt is worth noting that confidence plays a crucial role in the S&P module. At first, in the Scale step, it helps to locate outliers in the estimated depth map enabling to soften their impact when performing the scaling procedure. Additionally, in the Place step, assigning the highest confidence to the sparse input points enables the network to rely on them effectively. Nonetheless, SpAgNet also exploits the other predicted depth points according to their estimated confidence.\nSince the S&P module needs the sparse data at multiple scales, we down-sample it by employing a non-parametric sparsity aware pooling: moving a 3\u00d73 window with stride 2, we assign the mean of the available measures in its neighbourhood to each coordinate, we iteratively apply this process to reach lower resolutions. This approach leads to a densification of the sparse depth map and helps, at all scales, to include even the meagre few sparse points available to a large field of view."
        },
        {
            "heading": "3.3. Non-Local Spatial Propagation",
            "text": "Spatial propagation concerns the diffusion of information in a localized position to its neighbourhoods. This strategy represents a common practice in the depth completion literature [21, 3, 29, 13] and can be achieved by a neural network in charge of learning the affinity among neighbours. Let X = (xm,n) \u2208 RM\u00d7N be a 2D depth map to be refined through propagation, at step t it acts as follows:\nxtm,n = w c m,nx t\u22121 m,n + \u2211 (i,j)\u2208Nm,n wi,jm,nx t\u22121 i,j (4)\nWhere (m,n) is the reference pixel currently being updated, (i, j) \u2208 Nm,n the coordinate of the pixels in its\nneighborhood, wi,jm,n the affinity weights, and w c m,n the affinity weight of the reference pixel:\nwcm,n = 1\u2212 \u2211\n(i,j)\u2208Nm,n\nwi,jm,n (5)\nThe various existing methods differ by the choice of the neighborhood and by the normalization procedure of the affinity weights, the latter necessary to ensure stability during propagation [21, 3, 29]. Within SpAgNet, we implement the non-local approach [29], letting the network dynamically decide the neighborhood using deformable convolutions [5]. Formally:\nNm,n = {xm+p,n+q | (p, q) \u2208 f\u03d5(I,H, n,m)} (6) p, q \u2208 R\nWhere I and H are the RGB image and the sparse depth, and f\u03d5(\u00b7) is the neural network determining the neighbourhood. The non-local propagation module requires in input an initial depth map generated through two convolutional blocks from the last S&P block output, scaled using the fullresolution sparse depth points. However, in this case, we do not perform a weighted scaling to obtain the best result on the entire frame. Finally, as usual, the sparse depth points override the predicted output. The resulting depth map is then fed along with features to two convolutional blocks to generate the guiding features and confidence required by the propagation module."
        },
        {
            "heading": "3.4. Loss Function",
            "text": "At each scale, we train the network by supervising the depth obtained by the S&P module before Place step. The confidence weights the loss of each depth prediction, and a regularization term (controlled by \u03b7) enforces the network to maintain the confidence as higher as possible. Following [29], we compute both L1 and L2 losses. Our loss function, at a specific scale, is described by Eq. 7 where Cs and Ds are respectively confidence and depth at a specific scale s. Confidence is not computed for the full size scale, hence C0 = 1. Finally, it is worth mentioning that lower scales are weighted less through an exponential decay factor \u03b3.\nL = n\u2211 s=0 \u03b3s 1 N m\u2211 i Csi L 12 i \u2212 \u03b7 lnCsi (7) L12i = |Dsi \u2212Gi|+ |Dsi \u2212Gi|2"
        },
        {
            "heading": "4. Experimental Results",
            "text": "We have implemented SpAgNet in PyTorch [30] training with 2 NVIDIA RTX 3090 and using the ADAM optimizer [18] with \u03b21 = 0.9 and \u03b22 = 0.999. The final model requires 35 milliseconds to perform a prediction on a image\nof 640\u00d7480 resolution employing a single NVIDIA RTX 3090 GPU."
        },
        {
            "heading": "4.1. Datasets",
            "text": "NYU Depth V2. The NYU Depth V2 [27] dataset is an indoor dataset containing 464 indoor scenes gathered with a Kinect sensor. We follow the official train/test split as previous works relying on the pre-processed subset by Ma et al. [25] using 249 scenes for training (\u223c50K samples) and 215 scenes (654 samples) for testing. Each image has been down-sampled to 320\u00d7240 and then center cropped to 304\u00d7228. As a common practice on this dataset, 500 random points per image have been extracted to simulate sparse depth. We train our network for 15 epochs starting with a learning rate 10\u22123 and decreasing it every 3 epochs by 0.1, setting \u03b3 = 0.4 and \u03b7 = 0.1. We use batch size 24 (12 for each GPU); hence the network is extremely fast to converge since the whole training accounts less than 30K steps. We apply color and brightness jittering and horizontal flips to limit overfitting.\nKITTI Depth Completion (DC). KITTI DC [37] is an outdoor dataset containing over 90K samples, each one providing RGB information and aligned sparse depth information (with a density of about 5%) retrieved by a highend Velodyne HDL-64E LiDAR sensor. The images have 1216\u00d7352 resolution, and the dataset provides a standard split to train (86K samples), validate (7K samples) and test (1K samples). The groundtruth has been obtained temporally accumulating multiple LiDAR frames and filtering errors [37], leading to a final density of about 20%. On this dataset we train for 10 epochs with batch size 8 (4 for each GPU), starting with learning rate 10\u22123 and we decrease it every 3 epochs by 0.1, we set \u03b3 = 0.4 and \u03b7 = 20.0. Data augmentation follows the same scheme used for NYU."
        },
        {
            "heading": "4.2. Evaluation",
            "text": "In this section, we assess the performance of our proposal and state-of-the-art methods deploying the dataset mentioned above. Following standard practice [29, 3], we\nuse the following metrics: RMSE = \u221a\n1 N \u2211 i |Di \u2212Gi|2,\nMAE = 1N \u2211 i |Di \u2212Gi| and REL = 1 N \u2211 i \u2223\u2223\u2223Di\u2212GiGi \u2223\u2223\u2223. For evaluation purposes, in addition to the standard protocol deployed in this field [29, 3], we also thoroughly evaluate the robustness of the networks on the two datasets in much more challenging scenarios but always training with the standard procedure (i.e., using 500 points on NYU and 64 LiDAR lines on KITTI). Since KITTI DC is thought for autonomous driving tasks and the sparse depth is acquired with an high end 64 Lines Lidar which provides in output always the same pattern, we simulate the switch to a cheaper device providing in output less lines assessing the capability of SpAgNet to generalize over sparse depth density. On\nNYU Depth V2, sparse depth points are traditionally extracted randomly from the groundtruth [25, 29, 3] which is almost dense. Thus, we test i) the extreme case of having only 5 random points, ii) the impact of having large empty areas and iii) the impact of changing the sparsity pattern. We implement ii) sampling from the groundtruth a triangular tiling dot pattern aimed at simulating the output of a commercial VCSEL [23] ToF sensor and then randomly shifting this pattern to leave behind large empty areas where no sparse hints are available while iii) extracting from the groundtruth sparse points with the pattern of a Livox Mid70 [22]. All these patterns are showed in Figure 4. We take into account the publicly pre-trained state-of-the-art models available either on NYU Depth V2 or KITTI DC and we take care to guarantee that each architecture sees exactly the same sparse points while being evaluated.\nResults on NYU Depth v2. Table 1 compares stateof-art methods and our proposal on the NYU dataset using different input configurations: in the upper portion by changing the number of samples and in the lower portion by changing the pattern type. From the table, we can notice that our proposal achieves competitive results, being very close to NLSPN and better than other methods when the number of points used is the same as the training phase (i.e., 500). Similar behaviour occurs with 200 points. However, when the density of input points decreases further, SpAgNet vastly outperforms the state-of-the-art. The performance gap with other methods gets much higher when decreasing the density further. For instance, with 50 points, the RMSE by SpAgNet is 0.272 m, while the second one (NLSPN) accounts for 0.423 m. Notably, with only 5 points, the same metrics are 0.467 m and 1.033 m (NLSPN), further emphasizing the ability of our proposal to deal even with meagre input points, in contrast to our competitors. It is worth observing that our method outperforms competitors with randomly selected input points starting from 100.\nThe bottom portion of Table 1 reports the outcome of the evaluation with different spatial distributions and their average density of depth input points. Specifically, we report results with the two distributions depicted in the rightmost images of Figure 4. From the table, we can observe that when the spatial distribution covers the whole image,\nas in the case of the Livox-like pattern, SpAgNet and NLSPN achieve similar performance while other methods fall behind. However, when the input points do not cover significant portions of the scene and the density decreases further, like in the shifted-grid case, our method dramatically outperforms all competitors by a large margin.\nFigure 5 shows qualitatively how SpAgNet compares to CSPN and NLSPN on an NYU sample when using 500 random points, 5 points and the shifted grid. It highlights how only our method yields meaningful and compelling results with 5 points and the shifted grid, leveraging the image content much better than competitors, thanks to the proposed architectural design. At the same time, our network achieves results comparable to competitors with 500 randomly distributed points. This fact further highlights that the robustness of SpAgNet is traded with the capacity of entirely leveraging the sparse depth information when fully available.\nResults on KITTI DC. Once we assessed the performance on the indoor NYU dataset, we report in Table 2 the evaluation on KITTI DC. From the table, we can notice that with 64 lines, SpAgNet results almost comparable to the best one, NLSPN. However, by reducing the number of\nlines from 32 to 4, our network gets always the best performance with an increasing gap. Interestingly, PackNet-SAN [11], which has been specifically trained to perform well in both depth completion (64 lines) and depth prediction (0 lines) is not able to deal with fewer lines. Indeed, the accuracy it achieves when processing 16, 8 or 4 lines is even lower than the one achieved when performing depth prediction, i.e. with RMSE equal to 2.233 mm. We ascribe this\nbehaviour to the fact that they train an external encoding branch to extract features from sparse data and feed them to the network by means of a sum operation. Even though such a branch applies a special and bulky sparse convolution operator [4], it does not seem capable of generalizing to fewer points. On the contrary, the whole network seems to suffer of the same issues of fully convolutional models, resulting effective only when fed with 64 LiDAR lines or none \u2013 the only configurations observed during training.\nFigure 6 shows, on an image of the KITTI DC dataset and for three different numbers of lines, the outcome of NLSPN, PENet and our network. In contrast to competitors, SpAgNet consistently infers meaningful depth maps, even when the number of lines decreases. This behaviour can be perceived better by looking at the error maps. For instance, it is particularly evident with 4 lines, focusing on the road surface and the far and background objects.\nAdditional qualitative results are reported as videos in the supplementary material and in our project page."
        },
        {
            "heading": "4.3. Ablation Study",
            "text": "Finally, we carry out an ablation study concerning the main components of SpAgNet to measure their effectiveness. Specifically, in Table 3, we conduct two main studies, respectively, to evaluate (a) the impact of i) the Scale step of the S&P modules (while the Place step is strictly necessary, being it the entry point to input the sparse depth points needed to perform completion), ii) the usage of confidence and iii) the non-local propagation head, and (b) results achieve with different backbones.\nFrom (a), we can notice that with 500 sparse points, scaling does not significantly improve since the network already learns to generate an output that is almost in scale. How-\never, with only 5 points, applying a global scaling procedure helps retrieve the correct scale even in regions lacking depth measurements. Focusing on confidence, it turns out to be effective with high and low densities of input points. Finally, Non-Local Spatial Propagation further boosts performance in both cases.\nIn (b), most backbones yield comparable results when\ntested with 500 points, with ResNeXt50 achieving slightly better results. A significant gap in accuracy emerges when testing the same networks with only 5 points, with ResNeXt50 achieving the best results again."
        },
        {
            "heading": "5. Conclusion",
            "text": "This paper proposes a sparsity agnostic framework for depth completion relying on a novel Scale and Place (S&P) module. Injecting sparse depth points to it rather than to convolutions allows us to improve the robustness of the architecture even when facing uneven and sparse distributions of input depth points. In contrast, existing state-of-the-art solutions are not robust in such circumstances and are often unable to infer meaningful results. Experimental results demonstrate the ability of our network to be competitive with state-of-the-art facing standard input distributions, while resulting much better when dealing with uneven ones.\nAcknowledgement. We gratefully acknowledge Sony Depthsensing Solutions SA/NV for funding this research and Valerio Cambareri for the constant supervision through the project and his feedback on this manuscript."
        }
    ],
    "title": "Sparsity Agnostic Depth Completion",
    "year": 2022
}