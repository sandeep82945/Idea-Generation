{
    "abstractText": "As tools for content editing mature, and artificial intelligence (AI) based algorithms for synthesizing media grow, the presence of manipulated content across online media is increasing. This phenomenon causes the spread of misinformation, creating a greater need to distinguish between \u201creal\u201d and \u201cmanipulated\u201d content. To this end, we present VIDEOSHAM, a dataset consisting of 826 videos (413 real and 413 manipulated). Many of the existing deepfake datasets focus exclusively on two types of facial manipulations\u2014swapping with a different subject\u2019s face or altering the existing face. VIDEOSHAM, on the other hand, contains more diverse, context-rich, and human-centric, high-resolution videos manipulated using a combination of 6 different spatial and temporal attacks. Our analysis shows that state-of-the-art manipulation detection algorithms only work for a few specific attacks and do not scale well on VIDEOSHAM. We performed a user study on Amazon Mechanical Turk with 1200 participants to understand if they can differentiate between the real and manipulated videos in VIDEOSHAM. Finally, we dig deeper into the strengths and weaknesses of performances by humans and SOTAalgorithms to identify gaps that need to be filled with better AI algorithms. We present the dataset here1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Trisha Mittal"
        },
        {
            "affiliations": [],
            "name": "Ritwik Sinha"
        },
        {
            "affiliations": [],
            "name": "Viswanathan Swaminathan"
        },
        {
            "affiliations": [],
            "name": "John Collomosse"
        },
        {
            "affiliations": [],
            "name": "Dinesh Manocha"
        }
    ],
    "id": "SP:5750021fb646264dd6dd9d402c4f854d08211b16",
    "references": [
        {
            "authors": [
                "Darius Afchar",
                "Vincent Nozick",
                "Junichi Yamagishi",
                "Isao Echizen"
            ],
            "title": "Mesonet: a compact facial video forgery detection network",
            "venue": "IEEE International Workshop on Information Forensics and Security (WIFS),",
            "year": 2018
        },
        {
            "authors": [
                "Shruti Agarwal",
                "Hany Farid",
                "Tarek El-Gaaly",
                "Ser-Nam Lim"
            ],
            "title": "Detecting deep-fake videos from appearance and behavior",
            "venue": "IEEE International Workshop on Information Forensics and Security (WIFS),",
            "year": 2020
        },
        {
            "authors": [
                "Omar Ismael Al-Sanjary",
                "Ahmed Abdullah Ahmed",
                "Ghazali Sulong"
            ],
            "title": "Development of a video tampering dataset for forensic investigation",
            "venue": "Forensic science international,",
            "year": 2016
        },
        {
            "authors": [
                "Jennifer Allen",
                "Baird Howland",
                "Markus Mobius",
                "David Rothschild",
                "Duncan J Watts"
            ],
            "title": "Evaluating the fake news problem at the scale of the information ecosystem",
            "venue": "Science Advances,",
            "year": 2020
        },
        {
            "authors": [
                "Irene Amerini",
                "Lamberto Ballan",
                "Roberto Caldelli",
                "Alberto Del Bimbo",
                "Giuseppe Serra"
            ],
            "title": "A sift-based forensic method for copy\u2013move attack detection and transformation recovery",
            "venue": "IEEE transactions on information forensics and security,",
            "year": 2011
        },
        {
            "authors": [
                "Katie Anderson"
            ],
            "title": "Getting acquainted with social networks and apps: combating fake news on social media",
            "venue": "Library Hi Tech News,",
            "year": 2018
        },
        {
            "authors": [
                "M Bashar",
                "K Noda",
                "N Ohnishi",
                "K Mori"
            ],
            "title": "Exploring duplicated regions in natural images",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2010
        },
        {
            "authors": [
                "Johnny Botha",
                "Heloise Pieterse"
            ],
            "title": "Fake news and deepfakes: A dangerous threat for 21st century information security",
            "venue": "In ICCWS 2020 15th International Conference on Cyber Warfare and Security. Academic Conferences and publishing limited,",
            "year": 2020
        },
        {
            "authors": [
                "Jim Brunner"
            ],
            "title": "Fox news runs digitally altered images in coverage of seattle\u2019s protests, capitol hill autonomous zone \u2014 the seattle",
            "year": 2020
        },
        {
            "authors": [
                "Joon Son Chung",
                "Arsha Nagrani",
                "Andrew Zisserman"
            ],
            "title": "Voxceleb2: Deep speaker recognition",
            "venue": "arXiv preprint arXiv:1806.05622,",
            "year": 2018
        },
        {
            "authors": [
                "Davide Cozzolino",
                "Giovanni Poggi",
                "Luisa Verdoliva"
            ],
            "title": "Efficient dense-field copy\u2013move forgery detection",
            "venue": "IEEE Transactions on Information Forensics and Security,",
            "year": 2015
        },
        {
            "authors": [
                "Brian Dolhansky",
                "Russ Howes",
                "Ben Pflaum",
                "Nicole Baram",
                "Cristian Canton Ferrer"
            ],
            "title": "The deepfake detection challenge (dfdc) preview dataset",
            "year": 1910
        },
        {
            "authors": [
                "Alvaro Figueira",
                "Luciana Oliveira"
            ],
            "title": "The current state of fake news: challenges and opportunities",
            "venue": "Procedia Computer Science,",
            "year": 2017
        },
        {
            "authors": [
                "Haiying Guan",
                "Mark Kozak",
                "Eric Robertson",
                "Yooyoung Lee",
                "Amy N Yates",
                "Andrew Delgado",
                "Daniel Zhou",
                "Timothee Kheyrkhah",
                "Jeff Smith",
                "Jonathan Fiscus"
            ],
            "title": "Mfc datasets: Large-scale benchmark datasets for media forensic challenge evaluation",
            "venue": "IEEE Winter Applications of Computer Vision Workshops (WACVW),",
            "year": 2019
        },
        {
            "authors": [
                "David G\u00fcera",
                "Edward J Delp"
            ],
            "title": "Deepfake video detection using recurrent neural networks",
            "venue": "IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),",
            "year": 2018
        },
        {
            "authors": [
                "Alexandros Haliassos",
                "Konstantinos Vougioukas",
                "Stavros Petridis",
                "Maja Pantic"
            ],
            "title": "Lips don\u2019t lie: A generalisable and robust approach to face forgery detection, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Douglas Harris"
            ],
            "title": "Deepfakes: False pornography is here and the law cannot protect you",
            "venue": "Duke L. & Tech. Rev.,",
            "year": 2018
        },
        {
            "authors": [
                "Kaiming He",
                "Georgia Gkioxari",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Mask r-cnn",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Yinan He",
                "Bei Gan",
                "Siyu Chen",
                "Yichun Zhou",
                "Guojun Yin",
                "Luchuan Song",
                "Lu Sheng",
                "Jing Shao",
                "Ziwei Liu"
            ],
            "title": "Forgerynet: A versatile benchmark for comprehensive forgery analysis",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Liming Jiang",
                "Wayne Wu",
                "Ren Li",
                "Chen Qian",
                "Chen Change Loy"
            ],
            "title": "Deeperforensics-1.0: A large-scale dataset for real-world face forgery detection",
            "venue": "arXiv preprint arXiv:2001.03024,",
            "year": 2020
        },
        {
            "authors": [
                "Hasam Khalid",
                "Shahroz Tariq",
                "Simon S Woo"
            ],
            "title": "Fakeavceleb: A novel audio-video multimodal deepfake dataset",
            "venue": "arXiv preprint arXiv:2108.05080,",
            "year": 2021
        },
        {
            "authors": [
                "Fouad Khelifi",
                "Ahmed Bouridane"
            ],
            "title": "Perceptual video hashing for content identification and authentication",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology,",
            "year": 2017
        },
        {
            "authors": [
                "Dahun Kim",
                "Sanghyun Woo",
                "Joon-Young Lee",
                "In So Kweon"
            ],
            "title": "Deep video inpainting",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Dahun Kim",
                "Sanghyun Woo",
                "Joon-Young Lee",
                "In So Kweon"
            ],
            "title": "Recurrent temporal aggregation framework for deep video inpainting",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Andrea Kleinsmith",
                "Nadia Bianchi-Berthouze"
            ],
            "title": "Affective body expression perception and recognition: A survey",
            "venue": "IEEE Transactions on Affective Computing,",
            "year": 2013
        },
        {
            "authors": [
                "Pavel Korshunov",
                "S\u00e9bastien Marcel"
            ],
            "title": "Deepfakes: a new threat to face recognition? assessment and detection",
            "venue": "arXiv preprint arXiv:1812.08685,",
            "year": 2018
        },
        {
            "authors": [
                "Iryna Korshunova",
                "Wenzhe Shi",
                "Joni Dambre",
                "Lucas Theis"
            ],
            "title": "Fast face-swap using convolutional neural networks",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Ronak Kosti",
                "Jose M Alvarez",
                "Adria Recasens",
                "Agata Lapedriza"
            ],
            "title": "Emotic: Emotions in context dataset",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,",
            "year": 2017
        },
        {
            "authors": [
                "Patrick Kwon",
                "Jaeseong You",
                "Gyuhyeon Nam",
                "Sungwoo Park",
                "Gyeongsu Chae"
            ],
            "title": "Kodf: A large-scale korean deepfake detection dataset",
            "venue": "arXiv preprint arXiv:2103.10094,",
            "year": 2021
        },
        {
            "authors": [
                "Lingzhi Li",
                "Jianmin Bao",
                "Hao Yang",
                "Dong Chen",
                "Fang Wen"
            ],
            "title": "Faceshifter: Towards high fidelity and occlusion aware face swapping",
            "year": 1912
        },
        {
            "authors": [
                "Yuezun Li",
                "Siwei Lyu"
            ],
            "title": "Exposing deepfake videos by detecting face warping artifacts",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),",
            "year": 2019
        },
        {
            "authors": [
                "Yuezun Li",
                "Xin Yang",
                "Pu Sun",
                "Honggang Qi",
                "Siwei Lyu"
            ],
            "title": "Celebdf: A new dataset for deepfake forensics",
            "year": 2019
        },
        {
            "authors": [
                "Sheng-Yang Liao",
                "Tian-Qiang Huang"
            ],
            "title": "Video copy-move forgery detection and localization based on tamura texture features",
            "venue": "In 2013 6th international congress on image and signal processing (CISP),",
            "year": 2013
        },
        {
            "authors": [
                "Haomiao Liu",
                "Ruiping Wang",
                "S. Shan",
                "Xilin Chen"
            ],
            "title": "Deep supervised hashing for fast image retrieval. 2016",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Yaqi Liu",
                "Chao Xia",
                "Xiaobin Zhu",
                "Shengwei Xu"
            ],
            "title": "Two-stage copy-move forgery detection with self deep matching and proposal superglue",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Chengjiang Long",
                "Arslan Basharat",
                "Anthony Hoogs",
                "Priyanka Singh",
                "Hany Farid"
            ],
            "title": "A coarse-to-fine deep convolutional neural network framework for frame duplication detection and localization in forged videos",
            "venue": "In CVPR Workshops,",
            "year": 2019
        },
        {
            "authors": [
                "Toqeer Mahmood",
                "Tabassam Nawaz",
                "Aun Irtaza",
                "Rehan Ashraf",
                "Mohsin Shah",
                "Muhammad Tariq Mahmood"
            ],
            "title": "Copy-move forgery detection technique for forensic analysis in digital images",
            "venue": "Mathematical Problems in Engineering,",
            "year": 2016
        },
        {
            "authors": [
                "Falko Matern",
                "Christian Riess",
                "Marc Stamminger"
            ],
            "title": "Exploiting visual artifacts to expose deepfakes and face manipulations",
            "venue": "IEEE Winter Applications of Computer Vision Workshops (WACVW),",
            "year": 2019
        },
        {
            "authors": [
                "Trisha Mittal",
                "Uttaran Bhattacharya",
                "Rohan Chandra",
                "Aniket Bera",
                "Dinesh Manocha"
            ],
            "title": "Emotions don\u2019t lie: An audio-visual deepfake detection method using affective cues",
            "venue": "In Proceedings of the 28th ACM international conference on multimedia,",
            "year": 2020
        },
        {
            "authors": [
                "Trisha Mittal",
                "Pooja Guhan",
                "Uttaran Bhattacharya",
                "Rohan Chandra",
                "Aniket Bera",
                "Dinesh Manocha"
            ],
            "title": "Emoticon: Context-aware multimodal emotion recognition using frege\u2019s principle",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "AP News"
            ],
            "title": "False video of joe biden claims democrat candidate greeted wrong us state at a rally \u2014 euronews",
            "venue": "(Accessed on 04/07/2022)",
            "year": 2020
        },
        {
            "authors": [
                "Yuval Nirkin",
                "Yosi Keller",
                "Tal Hassner"
            ],
            "title": "Fsgan: Subject agnostic face swapping and reenactment",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Ivan Perov",
                "Daiheng Gao",
                "Nikolay Chervoniy",
                "Kunlin Liu",
                "Sugasa Marangonda",
                "Chris Um\u00e9",
                "Mr Dpfks",
                "Carl Shift Facenheim",
                "Luis RP",
                "Jian Jiang"
            ],
            "title": "Deepfacelab: A simple, flexible and extensible face swapping framework",
            "venue": "arXiv preprint arXiv:2005.05535,",
            "year": 2020
        },
        {
            "authors": [
                "KR Prajwal",
                "Rudrabha Mukhopadhyay",
                "Vinay P Namboodiri",
                "CV Jawahar"
            ],
            "title": "A lip sync expert is all you need for speech to lip generation in the wild",
            "venue": "In Proceedings of the 28th ACM International Conference on Multimedia,",
            "year": 2020
        },
        {
            "authors": [
                "Hua Qi",
                "Qing Guo",
                "Felix Juefei-Xu",
                "Xiaofei Xie",
                "Lei Ma",
                "Wei Feng",
                "Yang Liu",
                "Jianjun Zhao"
            ],
            "title": "Deeprhythm: Exposing deepfakes with attentional visual heartbeat rhythms",
            "venue": "In Proceedings of the 28th ACM International Conference on Multimedia,",
            "year": 2020
        },
        {
            "authors": [],
            "title": "Where are they looking",
            "venue": "PhD thesis, Massachusetts Institute of Technology,",
            "year": 2016
        },
        {
            "authors": [
                "Andreas Rossler",
                "Davide Cozzolino",
                "Luisa Verdoliva",
                "Christian Riess",
                "Justus Thies",
                "Matthias Nie\u00dfner"
            ],
            "title": "Faceforensics++: Learning to detect manipulated facial images",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Seung-Jin Ryu",
                "Matthias Kirchner",
                "Min-Jeong Lee",
                "Heung- Kyu Lee"
            ],
            "title": "Rotation invariant localization of duplicated image regions based on zernike moments",
            "venue": "IEEE Transactions on Information Forensics and Security,",
            "year": 2013
        },
        {
            "authors": [
                "Ekraam Sabir",
                "Jiaxin Cheng",
                "Ayush Jaiswal",
                "Wael AbdAlmageed",
                "Iacopo Masi",
                "Prem Natarajan"
            ],
            "title": "Recurrent convolutional strategies for face manipulation detection in videos",
            "venue": "Interfaces (GUI),",
            "year": 2019
        },
        {
            "authors": [
                "Conrad Sanderson"
            ],
            "title": "The vidtimit database",
            "venue": "Technical report, IDIAP,",
            "year": 2002
        },
        {
            "authors": [
                "Aliaksandr Siarohin",
                "St\u00e9phane Lathuili\u00e8re",
                "Sergey Tulyakov",
                "Elisa Ricci",
                "Nicu Sebe"
            ],
            "title": "First order motion model for image animation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Laura Silver"
            ],
            "title": "Misinformation and fears about its impact are pervasive in 11 emerging economies",
            "year": 2020
        },
        {
            "authors": [
                "Russell Spivak"
            ],
            "title": "deepfakes\u201d: The newest way to commit one of the oldest crimes",
            "venue": "Geo. L. Tech. Rev.,",
            "year": 2018
        },
        {
            "authors": [
                "Lichao Su",
                "Tianqiang Huang",
                "Jianmei Yang"
            ],
            "title": "A video forgery detection algorithm based on compressive sensing",
            "venue": "Multimedia Tools and Applications,",
            "year": 2015
        },
        {
            "authors": [
                "Justus Thies",
                "Michael Zollh\u00f6fer",
                "Matthias Nie\u00dfner"
            ],
            "title": "Deferred neural rendering: Image synthesis using neural textures",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2019
        },
        {
            "authors": [
                "Justus Thies",
                "Michael Zollhofer",
                "Marc Stamminger",
                "Christian Theobalt",
                "Matthias Nie\u00dfner"
            ],
            "title": "Face2face: Real-time face capture and reenactment of rgb videos",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Soumya Tripathy",
                "Juho Kannala",
                "Esa Rahtu"
            ],
            "title": "Icface: Interpretable and controllable face reenactment using gans",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Junke Wang",
                "Zuxuan Wu",
                "Jingjing Chen",
                "Yu-Gang Jiang"
            ],
            "title": "M2tr: Multi-modal multi-scale transformers for deepfake detection",
            "venue": "arXiv preprint arXiv:2104.09770,",
            "year": 2021
        },
        {
            "authors": [
                "Qi Wang",
                "Zhaohong Li",
                "Zhenzhen Zhang",
                "Qinglong Ma"
            ],
            "title": "Video inter-frame forgery identification based on optical flow consistency",
            "venue": "Sensors & Transducers,",
            "year": 2014
        },
        {
            "authors": [
                "Duncan J Watts",
                "David M Rothschild",
                "Markus Mobius"
            ],
            "title": "Measuring the news and its impact on democracy",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2021
        },
        {
            "authors": [
                "Yue Wu",
                "Wael Abd-Almageed",
                "Prem Natarajan"
            ],
            "title": "Busternet: Detecting copy-move image forgery with source/target localization",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Yuxing Wu",
                "Xinghao Jiang",
                "Tanfeng Sun",
                "Wan Wang"
            ],
            "title": "Exposing video inter-frame forgery based on velocity field consistency",
            "venue": "In 2014 IEEE international conference on acoustics, speech and signal processing (ICASSP),",
            "year": 2014
        },
        {
            "authors": [
                "Xin Yang",
                "Yuezun Li",
                "Siwei Lyu"
            ],
            "title": "Exposing deep fakes using inconsistent head poses",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2019
        },
        {
            "authors": [
                "Xin Yang",
                "Yuezun Li",
                "Siwei Lyu"
            ],
            "title": "Exposing deep fakes using inconsistent head poses",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2019
        },
        {
            "authors": [
                "Ran Yi",
                "Zipeng Ye",
                "Juyong Zhang",
                "Hujun Bao",
                "Yong-Jin Liu"
            ],
            "title": "Audio-driven talking face video generation with learning-based personalized head pose",
            "venue": "arXiv preprint arXiv:2002.10137,",
            "year": 2020
        },
        {
            "authors": [
                "Jiangning Zhang",
                "Liang Liu",
                "Zhucun Xue",
                "Yong Liu"
            ],
            "title": "Apb2face: audio-guided face reenactment with auxiliary pose and blink signals",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2020
        },
        {
            "authors": [
                "Dong-Ning Zhao",
                "Ren-Kui Wang",
                "Zhe-Ming Lu"
            ],
            "title": "Inter-frame passive-blind forgery detection for video shot based on similarity analysis",
            "venue": "Multimedia Tools and Applications,",
            "year": 2018
        },
        {
            "authors": [
                "Peng Zhou",
                "Ning Yu",
                "Zuxuan Wu",
                "Larry S Davis",
                "Abhinav Shrivastava",
                "Ser-Nam Lim"
            ],
            "title": "Deep video inpainting detection",
            "venue": "arXiv preprint arXiv:2101.11080,",
            "year": 2021
        },
        {
            "authors": [
                "Bojia Zi",
                "Minghao Chang",
                "Jingjing Chen",
                "Xingjun Ma",
                "Yu-Gang Jiang"
            ],
            "title": "Wilddeepfake: A challenging real-world dataset for deepfake detection",
            "venue": "ACM International Conference on Multimedia,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "The proliferation of accessible video editing software and artificial intelligence (AI) tools has led to an increase in manipulated video content [23, 20]. While digital manipulation is commonplace in the creative process, in some cases video manipulation has a malicious intent. Social media often amplifies such false information through the circulation of manipulated videos [7, 14]. A recent survey by\n*Work done as an intern at Adobe Research. 1VIDEOSHAM dataset link.\nPew Research Center showed that exposure to such false information is of widespread concern [54]. Therefore, there has been a significant increase in cases of misinformation, fraud and cybercrimes in the last decade. Such video manipulations pose a great threat to politics and can manipulate elections [62, 5], alter political narratives, weaken the public\u2019s trust in a country\u2019s leadership, and an increase hatred among various social groups. Another common occurrence is corporate frauds and scams where people use altered audio to impersonate other people to extort cash and other resources. Lastly, many video manipulations often result in numerous cybercrimes [18, 55, 9]. To further illustrate our motivations in this work, we depict such instances of video manipulations in Figure 1.\nThis leads to an important question\u2014how do we detect manipulated content? The current arsenal of techniques involve the use of AI which in turn requires tremendous amounts of data. In the past decade alone, there has been a surge in the number of benchmark deepfake datasets [27, 49, 13] which manipulate the facial features of subjects in images and videos. We summarize recent deepfake datasets in Table 1.\nBut facial manipulations represent only a fraction of all manipulated content circulated on social media. For example, modifications also include changing the background context (Figure 1b), text and audio (Figure 1c) in media, aesthetic edits, adding/removing entities (Figure 1a), and temporal edits (Figure 1d). These manipulations can be performed in a matter of clicks due to the availability of state of the art video editing tools like Adobe AfterEffectsTM, Adobe LightroomTM, Filmora, GIMP, and many others. To our knowledge, no benchmark video dataset exists that extends beyond deepfake-only facial manipulations to include the vast range of manipulations described above.\nar X\niv :2\n20 7.\n13 06\n4v 3\n[ cs\n.C V\n] 8\nD ec\nMain Contributions\nWe release a new manipulated high-resolution video dataset called VIDEOSHAM (Figure 6). VIDEOSHAM offers the following benefits over existing manipulated video datasets:\n1. Beyond Faces (Deepfakes): The videos in VIDEOSHAM are manipulated using six spatial and temporal attacks (See Table 2) manipulating videos at the scene level targeting, not just faces, but also the background context, text and audio, aesthetic edits, adding/removing entities, and temporal edits (See Figure 2).\n2. Beyond Images: Although there exist image manipulation datasets that go beyond faces, they cannot be used to detect video manipulations, which require dedicated video datasets. The latter, however, are hard to create due to the manual labor involved. In this work, we go beyond images to release the first video manipulation dataset containing beyond-face manipulations.\nVIDEOSHAM consists of 413 real-world videos and their corresponding manipulated versions (total 826 videos). The videos have diverse scene backgrounds, are context-rich, and contain up to 9 subjects on average. VIDEOSHAM is the largest dataset containing manipulated videos generated by professional video editors with varied attacks. A user study conducted on Amazon Mechanical Turk (AMT) to understand the kind of attack methods that mislead humans the most. In addition, we analyze the performance of existing state of the art deepfake detection algorithms and video forensics algorithms on VIDEOSHAM. We find that these techniques are less than 50% effective in distinguishing between a real and a manipulated video.\nWe elaborate more about VIDEOSHAM in Section 3. In Section 4 we present our findings from the user study and evaluation of detection models. And, finally in Section 5, we discuss some promising ideas to help these attacks."
        },
        {
            "heading": "2. Related Work",
            "text": "In this section, we discuss previous works in detection of manipulated and deceptive media content. To begin with, we first discuss the video manipulation techniques used to create such fake videos in Section 2.1. Then in Section 2.2, we summarize various datasets and benchmarks for video manipulations. We also survey different techniques used for detecting deepfake videos in Section 2.3 and generic video forensic methods in Section 2.4."
        },
        {
            "heading": "2.1. Video Manipulation Techniques/Attacks",
            "text": "Manipulation techniques, or attacks, are broadly categorized as spatial [6], temporal [23], and geometric [23]\nin the literature (see Table 2). Basic examples of spatial attacks include copy-move and image/video splicing which correspond to spatially or temporally shifting an object to a different location in the same video or a different video, respectively. Retouching, another common attack, involves aesthetic edits like adjusting brightness, contrast, and other parameters of digital content. More recently, people have used AI to alter facial features to create deepfake videos. AI-based techniques are comprised of two major attack approaches, Face Swapping [43, 45] and Face Reenactment [58, 57, 53]. Face Swapping switches the subject\u2019s face with the face of another person and Face Reenactment alters the subject\u2019s facial expressions. Temporal attacks involve swapping, duplicating, inserting, and deleting frames of video, giving the impression that the video has been sped up or slowed down. Finally, geometric attacks include operations like cropping and rotations."
        },
        {
            "heading": "2.2. Video Manipulation Datasets",
            "text": "Creating benchmarks of video manipulations is a challenging task as this may require per-frame manipulations. Some of the datasets (like Khelifi et al. [23], MTVFD [4], Liao et al. [34], Su et al. [56], Media Forensics Challenge [15]) are very small in volume containing 7 \u2212 200 videos each, these datasets are also not publicly available. Most of these videos have 0 or 1 subjects present in the frame with very little background context. More\nrecently, AI-synthesized attacks like face swapping, face re-enactment, and audio-driven face re-enactment have led to the creation of datasets like UADFV [66], FaceForensics++ [49], DeeperForensics1.0 [21], WildDeepFake [72]. Because these datasets are generated using learning methods; some of these datasets have upto 100k videos. However all of these datasets have strictly 1 subject per video with the face being predominant part of the frame with no background context at all. Many datasets are missing audio except DFDC [13], DF-TIMIT [27], KoDF [30], FakeAVCeleb [22], ForgeryNet [20] and SR-DF [60]."
        },
        {
            "heading": "2.3. Deepfake Detection Methods",
            "text": "The goal of the deepfake detection approaches is to algorithmically distinguish fake videos from real videos. A large portion of these methods are focused on detecting visual artifacts especially on the finer regions of the face (like eyes, mouth and teeth [39]). Some approaches specifically focus on abnormalities like inconsistent head pose orientations [67], asynchronous lip movement and speech [17] and unnatural eye blinking [32]. Prior work have also observed and exploited the fact that temporal coherence is not enforced effectively in the synthesis process of deepfakes [51, 16] and exploit this in detection methods. More recently, interesting affective computing approaches that focus on correlated emotion signals from audio-visual cues [40, 3], and detecting signals like heart rate and breathing rate [47] from the videos have also been proposed. However, it is clear that due to the nature of the datasets (single-person, face-centered videos), these approaches focus only on facial cues and audio cues."
        },
        {
            "heading": "2.4. Video Forensic Methods",
            "text": "Developments in video forensics literature focus on two specific attacks; Copy-Move and Splicing (Row 1 in Table 2) and Temporal attacks (Row 6 in Table 2). Most conventional copy-move forgery detection methods mainly consist of three components [12]: (1) feature extraction, (2) matching, and (3) post-processing. A variety of features have been explored, e.g., DCT (Discrete Cosine Transform) [38], DWT (Discrete Wavelet Transform) and KPCA (Kernel Principal Component Analysis) [8], Zernike moments [50]. Consequently, some end-to-end deep learning based copy-move forgery detection methods were proposed [63, 64, 32]. However these efforts are limited to images. Another interesting development, still in naive stages is deep learning methods to detect inpainting in videos [71]. Some of the methods in detecting temporal attacks (also called as intra-frame manipulations) use the consistency of velocity field [65] and optical flow [61]. These methods can recognize frame insertion and frame deletion attacks. Similarly, Zhao et al. [70] use inter-frame similarity analysis to detect frame duplications in the videos. Finally, Long et\nal. [37] propose a coarse-to-fine framework based on deep Convolutional Neural Networks (CNN) to detect potential frame duplications."
        },
        {
            "heading": "3. Our Dataset- VIDEOSHAM",
            "text": "In this section, we present details on the dataset creation process (Section 3.1) and discuss some of the salient features and characteristics of VIDEOSHAM (Section ??)."
        },
        {
            "heading": "3.1. Creation and Annotation Process",
            "text": ""
        },
        {
            "heading": "3.1.1 Source Videos",
            "text": "We have a total of 836 videos comprising of 413 original videos and 413 manipulated versions, each corresponding to one of the original videos. We obtain our source videos from an online video website (vimeo 2) and only include videos attributed with a CC-BY (Creative Commons) license. In addition, we avoid videos with brands, children, objectionable content, TV show/movie clips and videos with copyrighted music. We trim these original videos to a specific length (upto 5\u221230 seconds) before we perform any manipulation attack."
        },
        {
            "heading": "3.1.2 Manipulation Attacks",
            "text": "We employ a total of 6 manipulation attacks for creating our dataset. As per prior literature, we also categorize these attacks into spatial and temporal attacks3. We visually show the distribution of the attacks in Figure 3a (attacks outlined in blue are spatial attacks, outlined in pink are temporal attacks). We describe each of the attack below.\n\u2212 ATTACK 1 (Adding an entity/subject): In this attack we select an entity or a subject from some other sources and place them in the current video. This attack is somewhat similar to copy-move attack.\n\u2212 ATTACK 2 (Removing an entity/subject): In this attack, we basically select an entity or a subject in the video and remove it from all the frames and fill in the gap with background settings. To do this, we used content-aware fill in Adobe AfterEffectsTM and some deep learning methods for generating masks [19] and performing video inpainting [24, 25].\n\u2212 ATTACK 3 (Background/Color Change): We focus on a particular aspect of the video, and change the background of the video, or color of a small entity in the video.\n2www.vimeo.com. 3We do not use geometric attacks, as they have been shown to be easily\ndetected.\n\u2212 ATTACK 4 (Text Replaced/Added): We perform edits like adding some text in the video or removing or replacing already existing text in the video.\n\u2212 ATTACK 5 (Frames Duplication / Removal/ Dropping): This attack is specifically to render the video temporally inconsistent. We choose to perform one of these manipulations, randomly duplicating frames, removing or dropping frames in the video. This also includes slowing down a video.\n\u2212 ATTACK 6 (Audio Replaced): Audio modality is a very important aspect for videos. To manipulate this, we replace the existing audio with some other audio.\nWe visually depict the 4 spatial attacks (ATTACK 1, ATTACK 2, ATTACK 3, and ATTACK 4) in Figure 6."
        },
        {
            "heading": "3.1.3 Manipulated Videos",
            "text": "We worked with 3 professional video editors hired on Upwork 4. The editors were shortlisted based on their experience and were well-versed with Adobe AfterEffectsTM, the software used for creating these edits. Each editor was assigned tasks, i.e. source videos, start and end timestamp to be edited and a one-line description of the manipulation to be performed. We provide all videos and the attacks performed for every video. Dataset Analysis: In Figure 3a, we present the distribution of attacks for the 413 videos, each lasting 1 \u2212 31 seconds. The average length of videos in our dataset is around 8 seconds long. We also run an object detection model 5\n4www.upwork.com. 5https://github.com/roboflow-ai.\nto count the number of people/agents in every video (Figure 3b). More than 80% of the videos in our dataset contains at least one subject."
        },
        {
            "heading": "4. Experiments and Results",
            "text": "We elaborate on three experiments we perform to highlight the importance, novelty and usecase of VIDEOSHAM. To begin with, we present the analysis of how well humans fair in detecting these attacks in Section 4.1, followed by analysis of the performance of state-of-the-art deepfake detection methods and video forensic techniques in Section 4.2. Finally in Section 4.3, we present some ideas and preliminary results for using interdisciplinary ideas for detecting such attacks."
        },
        {
            "heading": "4.1. Expt 1: How Well Do Humans Perform?",
            "text": "Setup: We first shortlist 60 videos from VIDEOSHAM. Out of these, 30 videos are real and the remaining 30 are manipulated (5 videos per attack). We recruit human participants from Amazon Mechanical Turk (AMT) and show each video to 20 participants. The participants are requested to watch the full video; followed by two questions. In the first question, the participants are asked to respond to the following prompt in either a yes or no - \u201cDo you believe this video has been manipulated/edited to misrepresent facts?\u201d. And, in the second question we ask them to explain in a sentence what they felt was manipulated with the following prompt- \u201cIf you answered YES above, what region or aspect of this video, do you believe is manipulated.\u201d. Note that participants are not informed whether videos are manipulated or not. They are also not informed about the set of attacks. We show the setup in Figure 4 which was used to collect a total of 1200 responses from AMT participants.\nStudy Analysis: We summarize the responses of the user study in Table 3. Both the real and manipulated videos receive 600 responses each. We observe that out of the 600 responses (corresponding to the 30 real videos), 342, i.e., 57% were correctly identified as real. Similarly, out of 600 responses for the manipulated videos, 389 were incorrectly identified as real, i.e., 35.2% of these responses correctly identified manipulated videos. Analyzing the responses by the type of attack, we observe that human participants are able to identify 45% of the videos manipulated using ATTACK 6. For the other attack types, the proportion of manipulated videos labeled as \u2018fake\u2019 ranges from 13-31%. Furthermore, we notice that human participants are able to more successfully identify manipulated videos that are modified using temporal attacks (ATTACK 5 and ATTACK 6) than spatial attacks (ATTACK 1\u2212 ATTACK 4). Moreover, we also received some number of responses from participants explaining their rationale behind reporting a manipulated video. From the responses received, there is no clear evidence that suggests that participants are able to identify the manipulated region/kind in case of spatial edits. But, they were somewhat able to correctly identify the manipulated edit in case of temporal attacks. This would imply that a subset of our selection of attacks are indiscernible to the human eye.\nStatistical Tests: Next we consider statistical tests to see if humans are able to tell a real video from a manipulated video. We consider the following quantities for this test, define p1 = P (declaring video real|real video). Also, let p2 = P (declaring video real|manipulated video). If humans are able to tell real videos apart from manipulated videos, we expect p1 to be larger that p2. Hence, we test the one-sided statistical hypothesis:\nH0 : p1 = p2 against H1 : p1 > p2.\nWe test this hypothesis with the test statistic (p1 \u2212 p2)6. In Table 3 we present the difference of proportions as well as the one-sided p\u2212value of the test for each attack type. The first thing to note is that when combining across all attack types (last row), we see that even though p1 is slightly bigger than p2, this difference is not statistically significant (p\u2212value of 0.177). This suggests that our edits are not discernible to human evaluators. When we break it down by attack type, we observe that only for ATTACK 6 (audio replacement), humans are more likely to declare such edits as manipulations (p\u2212value < 0.001). For ATTACK 4 (text replaced or added), there is weak statistical evidence of humans detecting this manipulation (p\u2212value of 0.097). For all other attacks, there is no statistical evidence that humans can tell when a video has been manipulated using that strategy. It is particularly telling that when an entity/subject is\n6Given our sample size, we have a 86% statistical power of detecting a difference if the true values are p1 = 0.75 and p2 = 0.74.\n(A) 1200 responses (20 participants\u00d7 (30 real + 30 manipulated videos))\nGT #Resp Rep Rep p1 p2 p1 \u2212 p2 p\u2212 value CI(l) CI(u)(Total) Real Fake\nadded or removed (ATTACKs 1 and 2), more of our human subjects declare such manipulated videos as real than they declare unedited videos. This shows how modern editing tools can be used to manipulate videos in a way that humans have no way of telling such edits just by looking at the video. This observation establishes the need to build high quality video manipulation detection algorithms that can label manipulated videos at scale."
        },
        {
            "heading": "4.2. Expt 2: How Well Do Machines Perform?",
            "text": "To answer this question better, we evaluate state-of-theart deepfake detection methods and video forensics techniques on VIDEOSHAM. Deepfake Detection Methods: We evaluate Li et al. [32], XceptionNet [49] and Mittal et al. [40] on VIDEOSHAM. Deepfake videos generated using data-driven methods can only synthesize face images of a fixed size, and they must undergo an affine warping to match the configuration of the source\u2019s face. Due to resolution inconsistencies between warped face and background context, there are various artifacts on the synthesized faces. Li et al. [32] detects such artifacts by comparing the generated face areas and their surrounding regions with a dedicated Convolutional Neural Network (CNN) model. On the other hand, XceptionNet [49] is a transfer learning model which is also a CNN\narchitecture, which was originally trained for the classical object detection task and later finetuned for deepfake detection on FaceForensics++ dataset. Finally, Mittal et al. propose an approach that simultaneously exploits the audio (speech) and video (face) modalities and also the perceived emotion features extracted from both the modalities to detect any falsification or alteration in the input video. They use the correlation between the modalities to detect a fake video.\nVideo Forensics Techniques: We evaluate Long et al. [37] and Liu et al. [36] on VIDEOSHAM. Both of these methods are state-of-the-art methods in video forensics literature. While, Long et al. [37] is specifically for detecting cases of frame duplications in a video, Liu et al. [36] specifically focus on detecting copy-move attacks. For all the methods, we use pretrained models and report the results when evaluated on VIDEOSHAMin Table 4. Study Analysis: All the 5 shortlisted methods are less then 50% accurate on VIDEOSHAM. This is understandable, as all the deepfake methods (Li et al. [32], MesoNet [2], and Mittal et al. [40]) are trained specifically to look for manipulations in faces. Moreover, these method are not used to inferencing on videos with more or less than 1 person in the frame and with so much context information. Hence, we observe that these methods are only inferring based on artifacts caught near the face regions in the\nVIDEOSHAM videos. We also observe that, Mittal et al. specifically are able to detect some of the temporal manipulations well; which is because the method is trained to look for correlation between audio and visual modalities. Similarly, even the video forensics techniques are specifically performing well on attacks that they have been trained for, i.e. ATTACK 5 for Long et al. and ATTACK 1 and ATTACK 2 for Liu et al. [35]. ATTACK 3 (color change) and ATTACK 4 (text replacement) tend to remain hard to be detected by most of these methods."
        },
        {
            "heading": "4.3. Expt 3: Beyond DeepFake Detection and Video",
            "text": "Forensic Techniques\nOne can observe from the experiments in the previous section, that all the methods are largely dependent on the visual artifacts. However, given the diversity of attacks used to manipulate videos, we hypothesize the use of inter-agent and multimodal analysis models for detecting such manipulations. We show preliminary results in Figure 5. Strategy 1 (Gaze): To begin with, we believe that tracking gaze of subjects can be useful for detection experiments. Gaze following is a task in computer vision to identify objects and regions that the subject of interest is focusing on. The idea behind this strategy is to identify manipulated images by using gaze following to locate \u201cabsent\u201d targets and/or \u201cout-of-context\u201d subjects in the video. To perform some preliminary analysis we deploy GazeFollow [48]. More specifically, for each frame, we begin by obtain the spatial coordinates of the subject\u2019s head\u2019s bound-\ning box and pass this information as input to the gaze tracking algorithm, GazeFollow [48], which outputs the location of the subject\u2019s gaze. The final step in this strategy is to run an object detector to obtain a confidence score cg corresponding to an object present at the gaze location. A low confidence score indicates a manipulated frame. Strategy 2 (Affect): In this strategy we propose the use of affective cues. When we track and look for affective disparities in affective state of different subjects. Prior works in psychology [26] and empirical works [41] that subjects in social settings often share affective states. We use facial expressions, body postures and scene understanding to perceive the affective states of all subjects. We use the model EmotiCon [41] trained on EMOTIC dataset [29] to perceive these affective states and obtain an affective confidence score ca. By empirically assigning a threshold, \u03c4 on the two confidence scores, we flag a video as manipulated. We observe that these two techniques help detect ATTACK 1 and ATTACK 2 significantly well. We add quantitative results for the same in Table 5. We show two qualitative results of these ideas in Figure 5.\nExperiment 3 shows that, in addition to human assessment, specialized deepfake detection techniques, and video forensics, other approaches that are not intended for identifying manipulated videos can be used."
        },
        {
            "heading": "5. Conclusion and Future Directions",
            "text": "Our goal with the expt 1 (Section 4.1) and expt 2 (Section 4.2) was to understand how well humans can detect some of the manipulations that occur today circulated on social media. We also wanted to understand if the developments in the deepfake detection and video forensic literature match up to these manipulation attacks. Finally, through expt 3 (Section 4.3) we want to propagate the idea of using ideas beyond detection of visual artifacts for scalable models for video manipulation detection.\nWe conclude from expt 1 (Section 4.1) and expt 2 (Section 4.2) that both humans and machines (5 methods shortlisted) struggle to detect these manipulations successfully. We believe that these are attacks of concern, as they are going undetected even by human participants. Moreover, we emphasize that these manipulations play a big role in many real-world video manipulations (Figure 1).\nMore generally, we believe that computer vision algorithms perform almost comparable to humans in most of these ATTACKS. However, most methods are very attackspecific and do not generalize well to other attacks. Mostly every deepfake detection method fails to handle videos with more than 1 subject and hence have a very limited scope. Also, importantly most of the deepfake detection methods require huge amounts of training samples; and this is not a realistic assumption. It is important to build methods which can be less computationally intensive and at the same time\nare also able to generalize well. Similarly, methods in video forensics also are only able to handle very specific attacks. These are less dependent on data, but computationally expensive as they are more or less, inference based methods.\nWe believe following are some knowledge gaps and research agendas that can help the society combat the increasing problem of misinformation, frauds and cybercrimes occurring due to manipulated media content shared online.\n1. There is a need to build detection models focused on more diverse attacks or video manipulations. Through VIDEOSHAM, we attempted to include some of the attacks that have not been studied before owing to a lack of a dataset. We hope this dataset can be a step towards achieving better detection models for all the 6 attacks.\n2. Moreover it is important to increase the scope of detection ideas being used currently for detecting manipulations. Current methods are extremely focused on visual perception. Our goal through experiment 3 was to show through very preliminary analysis that ideas based on inter-agent dynamics and multimodal cues can be a promising literature source. Another promising idea, is to include domain knowledge in detecting manipulations; as humans we have some contextual information which the detection models severely suffer from.\n3. Largely all existing methods require a significant amount of training data to train the models. But, with newer manipulations and attacks on videos, it will become impossible to keep up with detection models for the same. We need to reduce the dependence on training data build detection models that are as generalizable as possible to potential attacks."
        },
        {
            "heading": "6. Ethical Considerations",
            "text": "We note that our dataset sources videos from an online video website that are attributed with a CC-BY license, and we do not retain any metadata corresponding to the creators of the videos. In addition, we do not collect any personal information of the human participants in the subsequent user study conducted on AMT. We expect that our dataset is an effort towards mitigating and fighting against malicious manipulations of online digital content."
        },
        {
            "heading": "Acknowledgements",
            "text": "This research is partially supported by ARO Grant W911NF2110026."
        }
    ],
    "title": "Video Manipulations Beyond Faces: A Dataset with Human-Machine Analysis",
    "year": 2022
}