{
    "abstractText": "In active learning, acquisition functions define informativeness directly on the representation position within the model manifold. However, for most machine learning models (in particular neural networks) this representation is not fixed due to the training pool fluctuations in between active learning rounds. Therefore, several popular strategies are sensitive to experiment parameters (e.g. architecture) and do not consider model robustness to out-of-distribution settings. To alleviate this issue, we propose a grounded second-order definition of information content and sample importance within the context of active learning. Specifically, we define importance by how often a neural network \u201dforgets\u201d a sample during training artifacts of second order representation shifts. We show that our definition produces highly accurate importance scores even when the model representations are constrained by the lack of training data. Motivated by our analysis, we develop Gaussian Switch Sampling (GauSS). We show that GauSS is setup agnostic and robust to anomalous distributions with exhaustive experiments on three indistribution benchmarks, three out-of-distribution benchmarks, and three different architectures. We report an improvement of up to 5% when compared against four popular query strategies. Our code is available at https://github.com/olivesgatech/gauss. Impact Statement\u2014With the ever increasing demand for deep learning products in safety-critical applications, the acquisition of suitable training data has significantly increased in complexity. In several instances, labeling large quantities of data is associated with insurmountable costs (e.g. medical applications) while other instances require data diversity at scale with numerous edge cases (e.g. autonomous vehicles). Active learning offers a promising solution to both of these problems by selecting data to both improve annotation efficiency and data quality. For practical deployment, these algorithms must select robust datasets and further function in a wide variety of training setups in order to guarantee design requirements. However, existing algorithms base their acquisition function on the representation approaches which results in high performance fluctuations over training setups and robustness metrics. For instance, an algorithm might perform exceedingly well with one neural network architecture but underperform with another. Our work introduces a secondorder active learning approach for robustness and portability. We see our work as the first step of many in bringing theoretical active learning algorithms to real world deployment.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ryan Benkert"
        },
        {
            "affiliations": [],
            "name": "Mohit Prabhushankar"
        }
    ],
    "id": "SP:89bb238483c90f1deb3e58eb623419f9ab541bf0",
    "references": [
        {
            "authors": [
                "G. Adomavicius",
                "A. Tuzhilin"
            ],
            "title": "Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2005
        },
        {
            "authors": [
                "Rahaf Aljundi",
                "Francesca Babiloni",
                "Mohamed Elhoseiny",
                "Marcus Rohrbach",
                "Tinne Tuytelaars"
            ],
            "title": "Memory aware synapses: Learning what (not) to forget",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Ghassan AlRegib",
                "Mohit Prabhushankar"
            ],
            "title": "Explanatory paradigms in neural networks",
            "venue": "arXiv preprint arXiv:2202.11838,",
            "year": 2022
        },
        {
            "authors": [
                "Jordan T. Ash",
                "Ryan P. Adams"
            ],
            "title": "On the difficulty of warm-starting neural network training",
            "year": 1910
        },
        {
            "authors": [
                "Jordan T Ash",
                "Chicheng Zhang",
                "Akshay Krishnamurthy",
                "John Langford",
                "Alekh Agarwal"
            ],
            "title": "Deep batch active learning by diverse, uncertain gradient lower bounds",
            "year": 1906
        },
        {
            "authors": [
                "Yoram Baram",
                "Ran El Yaniv",
                "Kobi Luz"
            ],
            "title": "Online choice of active learning algorithms",
            "venue": "Journal of Machine Learning Research,",
            "year": 2004
        },
        {
            "authors": [
                "William H. Beluch",
                "Tim Genewein",
                "Andreas N\u00fcrnberger",
                "Jan M. K\u00f6hler"
            ],
            "title": "The power of ensembles for active learning in image classification",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2018
        },
        {
            "authors": [
                "Ryan Benkert",
                "Oluwaseun Joseph Aribido",
                "Ghassan AlRegib"
            ],
            "title": "Explaining deep models through forgettable learning dynamics",
            "venue": "IEEE International Conference on Image Processing (ICIP),",
            "year": 2021
        },
        {
            "authors": [
                "Ryan Benkert",
                "Oluwaseun Joseph Aribido",
                "Ghassan"
            ],
            "title": "AlRegib. Example forgetting: A novel approach to explain and interpret deep neural networks in seismic interpretation",
            "venue": "IEEE Transactions on Geoscience and Remote Sensing,",
            "year": 2022
        },
        {
            "authors": [
                "Ryan Benkert",
                "Mohit Prabhushankar",
                "Ghassan AlRegib"
            ],
            "title": "Forgetful active learning with switch events: Efficient sampling for out-ofdistribution data",
            "venue": "IEEE International Conference on Image Processing (ICIP),",
            "year": 2022
        },
        {
            "authors": [
                "Sylvain Calinon",
                "Florent Guenter",
                "Aude Billard"
            ],
            "title": "On learning, representing, and generalizing a task in a humanoid robot",
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics),",
            "year": 2007
        },
        {
            "authors": [
                "Zhiyuan Chen",
                "Bing Liu"
            ],
            "title": "Lifelong machine learning",
            "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Adam Coates",
                "Andrew Ng",
                "Honglak Lee"
            ],
            "title": "An analysis of single-layer networks in unsupervised feature learning",
            "venue": "In Proceedings of the fourteenth international conference on artificial intelligence and statistics,",
            "year": 2011
        },
        {
            "authors": [
                "David A Cohn",
                "Zoubin Ghahramani",
                "Michael I Jordan"
            ],
            "title": "Active learning with statistical models",
            "venue": "Journal of artificial intelligence research,",
            "year": 1996
        },
        {
            "authors": [
                "Cody Coleman",
                "Christopher Yeh",
                "Stephen Mussmann",
                "Baharan Mirzasoleiman",
                "Peter Bailis",
                "Percy Liang",
                "Jure Leskovec",
                "Matei Zaharia"
            ],
            "title": "Selection via proxy: Efficient data selection for deep learning",
            "year": 1906
        },
        {
            "authors": [
                "Luke Nicholas Darlow",
                "Elliot J. Crowley",
                "Antreas Antoniou",
                "Amos J. Storkey"
            ],
            "title": "CINIC-10 is not imagenet or CIFAR-10",
            "year": 2018
        },
        {
            "authors": [
                "Sebastian Farquhar",
                "Yarin Gal",
                "Tom Rainforth"
            ],
            "title": "On statistical bias in active learning: How and when to fix it, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Yarin Gal",
                "Zoubin Ghahramani"
            ],
            "title": "Bayesian convolutional neural networks with bernoulli approximate variational inference",
            "venue": "arXiv preprint arXiv:1506.02158,",
            "year": 2015
        },
        {
            "authors": [
                "Yarin Gal",
                "Riashat Islam",
                "Zoubin Ghahramani"
            ],
            "title": "Deep bayesian active learning with image data",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Yonatan Geifman",
                "Ran El-Yaniv"
            ],
            "title": "Deep active learning over the long",
            "venue": "tail. CoRR,",
            "year": 2017
        },
        {
            "authors": [
                "Daniel Gissin",
                "Shai Shalev-Shwartz"
            ],
            "title": "Discriminative active learning",
            "venue": "arXiv preprint arXiv:1907.06347,",
            "year": 2019
        },
        {
            "authors": [
                "Chuan Guo",
                "Geoff Pleiss",
                "Yu Sun",
                "Kilian Q Weinberger"
            ],
            "title": "On calibration of modern neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Lei Han",
                "Kean Ming Tan",
                "Ting Yang",
                "Tong Zhang"
            ],
            "title": "Local uncertainty sampling for large-scale multi-class logistic regression",
            "venue": "arXiv preprint arXiv:1604.08098,",
            "year": 2016
        },
        {
            "authors": [
                "Elmar Haussmann",
                "Michele Fenzi",
                "Kashyap Chitta",
                "Jan Ivanecky",
                "Hanson Xu",
                "Donna Roy",
                "Akshita Mittel",
                "Nicolas Koumchatzky",
                "Clement Farabet",
                "Jose M Alvarez"
            ],
            "title": "Scalable active learning for object detection",
            "venue": "IEEE Intelligent Vehicles Symposium (IV),",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Thomas Dietterich"
            ],
            "title": "Benchmarking neural network robustness to common corruptions and perturbations",
            "venue": "Proceedings of the International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Steven CH Hoi",
                "Rong Jin",
                "Jianke Zhu",
                "Michael R Lyu"
            ],
            "title": "Batch mode active learning and its application to medical image classification",
            "venue": "In Proceedings of the 23rd international conference on Machine learning,",
            "year": 2006
        },
        {
            "authors": [
                "Neil Houlsby",
                "Ferenc Husz\u00e1r",
                "Zoubin Ghahramani",
                "M\u00e1t\u00e9 Lengyel"
            ],
            "title": "Bayesian active learning for classification and preference learning",
            "venue": "arXiv preprint arXiv:1112.5745,",
            "year": 2011
        },
        {
            "authors": [
                "Wei-Ning Hsu",
                "Hsuan-Tien Lin"
            ],
            "title": "Active learning by learning",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "Alex Kendall",
                "Yarin Gal"
            ],
            "title": "What uncertainties do we need in bayesian deep learning for computer vision",
            "venue": "In Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "James Kirkpatrick",
                "Razvan Pascanu",
                "Neil C. Rabinowitz",
                "Joel Veness",
                "Guillaume Desjardins",
                "Andrei A. Rusu",
                "Kieran Milan",
                "John Quan",
                "Tiago Ramalho",
                "Agnieszka Grabska-Barwinska",
                "Demis Hassabis",
                "Claudia Clopath",
                "Dharshan Kumaran",
                "Raia Hadsell"
            ],
            "title": "Overcoming catastrophic forgetting",
            "venue": "in neural networks. CoRR,",
            "year": 2016
        },
        {
            "authors": [
                "Andreas Kirsch",
                "Joost van Amersfoort",
                "Yarin Gal"
            ],
            "title": "Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning",
            "year": 1906
        },
        {
            "authors": [
                "Suraj Kothawade",
                "Nathan Beck",
                "Krishnateja Killamsetty",
                "Rishabh Iyer"
            ],
            "title": "Similar: Submodular information measures based active learning in realistic scenarios",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Gukyeong Kwon",
                "Mohit Prabhushankar",
                "Dogancan Temel",
                "Ghassan AlRegib"
            ],
            "title": "Distorted representation space characterization through backpropagated gradients",
            "venue": "IEEE International Conference on Image Processing (ICIP),",
            "year": 2019
        },
        {
            "authors": [
                "Gukyeong Kwon",
                "Mohit Prabhushankar",
                "Dogancan Temel",
                "Ghassan AlRegib"
            ],
            "title": "Backpropagated gradient representations for anomaly detection",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Gukyeong Kwon",
                "Mohit Prabhushankar",
                "Dogancan Temel",
                "Ghassan AlRegib"
            ],
            "title": "Novelty detection through model-based characterization of neural networks",
            "venue": "IEEE International Conference on Image Processing (ICIP),",
            "year": 2020
        },
        {
            "authors": [
                "Jinsol Lee",
                "Ghassan AlRegib"
            ],
            "title": "Gradients as a measure of uncertainty in neural networks",
            "year": 2008
        },
        {
            "authors": [
                "Jinsol Lee",
                "Ghassan AlRegib"
            ],
            "title": "Open-set recognition with gradientbased representations",
            "venue": "IEEE International Conference on Image Processing (ICIP),",
            "year": 2021
        },
        {
            "authors": [
                "Mohit Prabhushankar",
                "Ghassan AlRegib"
            ],
            "title": "Contrastive reasoning in neural networks",
            "venue": "arXiv preprint arXiv:2103.12329,",
            "year": 2021
        },
        {
            "authors": [
                "Mohit Prabhushankar",
                "Ghassan AlRegib"
            ],
            "title": "Introspective learning: A two-stage approach for inference in neural networks",
            "year": 2021
        },
        {
            "authors": [
                "Mohit Prabhushankar",
                "Gukyeong Kwon",
                "Dogancan Temel",
                "Ghassan AlRegib"
            ],
            "title": "Contrastive explanations in neural networks",
            "venue": "IEEE International Conference on Image Processing (ICIP),",
            "year": 2020
        },
        {
            "authors": [
                "Hippolyt Ritter",
                "Aleksandar Botev",
                "David Barber"
            ],
            "title": "Online structured laplace approximations for overcoming catastrophic forgetting",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Dan Roth",
                "Kevin Small"
            ],
            "title": "Margin-based active learning for structured output spaces",
            "venue": "In European Conference on Machine Learning,",
            "year": 2006
        },
        {
            "authors": [
                "Greg Schohn",
                "David Cohn"
            ],
            "title": "Less is more: Active learning with support vector machines",
            "venue": "In ICML,",
            "year": 2000
        },
        {
            "authors": [
                "Ozan Sener",
                "Silvio Savarese"
            ],
            "title": "Active learning for convolutional neural networks: A core-set approach",
            "venue": "arXiv preprint arXiv:1708.00489,",
            "year": 2017
        },
        {
            "authors": [
                "Burr Settles"
            ],
            "title": "Active learning literature survey",
            "year": 2009
        },
        {
            "authors": [
                "Guangyuan Shi",
                "Jiaxin Chen",
                "Wenlong Zhang",
                "Li-Ming Zhan",
                "Xiao- Ming Wu"
            ],
            "title": "Overcoming catastrophic forgetting in incremental fewshot learning by finding flat minima",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "D. Temel",
                "G. Kwon",
                "M. Prabhushankar"
            ],
            "title": "AlRegib. CURE-TSR: Challenging unreal and real environments for traffic sign recognition",
            "venue": "In Neural Information Processing Systems (NeurIPS) Workshop on Machine Learning for Intelligent Transportation Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Daniel Ting",
                "Eric Brochu"
            ],
            "title": "Optimal subsampling with influence functions",
            "venue": "In Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Mariya Toneva",
                "Alessandro Sordoni",
                "Remi Tachet des Combes",
                "Adam Trischler",
                "Yoshua Bengio",
                "Geoffrey J Gordon"
            ],
            "title": "An empirical study of example forgetting during deep neural network learning",
            "venue": "arXiv preprint arXiv:1812.05159,",
            "year": 2018
        },
        {
            "authors": [
                "Simon Tong"
            ],
            "title": "Active learning: theory and applications, volume 1",
            "venue": "Stanford University USA,",
            "year": 2001
        },
        {
            "authors": [
                "Simon Tong",
                "Daphne Koller"
            ],
            "title": "Support vector machine active learning with applications to text classification",
            "venue": "Journal of machine learning research,",
            "year": 2001
        },
        {
            "authors": [
                "Joost Van Amersfoort",
                "Lewis Smith",
                "Yee Whye Teh",
                "Yarin Gal"
            ],
            "title": "Uncertainty estimation using a single deep deterministic neural network",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Dan Wang",
                "Yi Shang"
            ],
            "title": "A new active labeling method for deep learning",
            "venue": "In 2014 International joint conference on neural networks (IJCNN),",
            "year": 2014
        }
    ],
    "sections": [
        {
            "text": "Impact Statement\u2014With the ever increasing demand for deep learning products in safety-critical applications, the acquisition of suitable training data has significantly increased in complexity. In several instances, labeling large quantities of data is associated with insurmountable costs (e.g. medical applications) while other instances require data diversity at scale with numerous edge cases (e.g. autonomous vehicles). Active learning offers a promising solution to both of these problems by selecting data to both improve annotation efficiency and data quality. For practical deployment, these algorithms must select robust datasets and further function in a wide variety of training setups in order to guarantee design requirements. However, existing algorithms base their acquisition function on the representation approaches which results in high performance fluctuations over training setups and robustness metrics. For instance, an algorithm might perform exceedingly well with one neural network architecture but underperform with another. Our work introduces a secondorder active learning approach for robustness and portability. We see our work as the first step of many in bringing theoretical active learning algorithms to real world deployment.\nIndex Terms\u2014Active Learning, Learning Dynamics, Example Forgetting\nI. INTRODUCTION A core factor in the success of machine learning algorithms is the selection of suitable data. While several samples contain\nThis paragraph of the first footnote will contain the date on which you submitted your paper for review. It will also contain support information, including sponsor and financial support acknowledgment. For example, \u201cThis work was supported in part by the U.S. Department of Commerce under Grant BS123456.\u201d\nThis paragraph will include the Associate Editor who handled your paper.\nvaluable information for a given task, other samples may be redundant with little additional information or anomalous with contradicting features. Active Learning [14], [47] is a paradigm in machine learning that selects the most informative samples from a large unlabelled data pool for annotation and training. Due to its intuitive practicality, active learning has already impacted multiple industrial sectors including manufacturing [52], robotics [11], recommender systems [1], medical imaging [27], and autonomous vehicles [24].\nAt the core of every active learning approach stands its method to select the next set of annotation samples - its acquisition function. The function establishes a ranking based on information content and selects interactively from the most informative samples. Therefore, the success of an acquisition function heavily relies on 1) the definition of high information content and on 2) the information content approximation when the model representations are constrained due to the lack of annotated training data. In other words, a successful acquisition function relies on what it considers \u201cinformative\u201d and how well the model produces importance scores. Hence, an effective definition is both accurate in defining sample importance and simple to approximate by a constrained model.\nIntuitively, we would assume that an effective definition of informativeness would result in superior generalization performance. In our analysis, we find this assumption to be incorrect. In fact, definitions that produce accurate importance estimates can even hurt generalization performance for early active learning rounds when the model is heavily constrained. For a simple explanation, consider a toy classifier that distinguishes cats from dogs. The samples with the highest information content are frequently anomalous or subject to data noise. In our example, this could be an image where both a cat and a dog are present or the presence of a rare breed that significantly differs from the remaining data points. Within the context of active learning, training on highly informative (or anomalous) samples results in an inaccurate representation space and thus a lower generalization performance on the test set. We sketch such a scenario in Figure 1 and further show this behavior empirically in later sections. With acquisition functions that produce \u201ctoo accurate\u201d information content scores, the resulting model overfits to outlier data and therefore results in lower performance. Even though several existing approaches do not consider this essential design argument, they can still be effective in practical scenarios. In particular, existing algorithms (willingly or unwillingly) introduce noise within the acquisition function that results in inaccurate information content estimates and hence better performance.\nHowever, existing approaches define sample importance on the model representation directly which is noisy in early rounds due to the lack of data. As an example, entropy sampling [55] selects samples with the highest softmax entropy, a direct manifestation of the model representation.\nEven though existing methods can be effective, relying on the representation directly has two hazardous implications: First, the acquisition function performance is highly dependent on experimental parameters (e.g. architecture) that directly influence the model representation. For this reason, several methods perform well for one parameter constellation but not in another. Second, the acquisition function does not consider model robustness to samples that do not originate from the training distribution (out-of-distribution). In particular, the model is incapable of reliably selecting informative samples that increase robustness due to the inherently unreliable model representations of outliers [26]. We show a toy example of two different protocols exposed to out-of-distribution settings in Figure 2. In active learning round N, we show two different protocols with similar test set performance. However, when exposed to out-of-distribution samples, the performance of both protocols significantly diverges and protocol 2 outperforms protocol 1 by a large margin. While papers on active learning generalization are ubiquitous, out-of-distribution active learning studies are significantly less common [34], [10]. For this purpose, popular active learning protocols lack robustness qualities and performance is unpredictable, especially in outof-distribution scenarios. In fact, our experiments in Section VI-A show significant performance variations of popular active learning approaches in out-of-distribution settings.\nIn this paper, we propose a grounded approach to defining information content in active learning. Specifically, we consider a second-order approach where \u201cinformative\u201d is defined by representation shifts rather than the representation directly. We consider a sample most informative if it was \u201cforgotten\u201d the most during training. In practice, evaluating \u201cforgetting\u201d requires annotations which are not present within the unlabeled pool. For this purpose, we approximate \u201cforgetting\u201d as a switch within the sample prediction. This characteristic is especially important for selecting samples that make our model more robust to out-of-distribution samples. In a detailed analysis, we\nshow that our practical \u201cswitch\u201d definition is both exceedingly accurate in assessing sample importance as well as simple to approximate by a constrained model. To showcase the quality of our approach, we show samples with a high information content in Figure 3 according to different definitions of information content. Specifically, we show highly informative samples as by our definition and two popular definitions in active learning literature. We see that our informative samples are exceedingly harder to distinguish or are ambiguous which implies a higher information content. For instance, our \u201cinformative\u201d samples are ambiguous images with complex shapes or ambiguous colors (e.g. the frog image in the top row; third image from the right side). In contrast, the other definitions consider clear images informative that are highly similar among each other. For instance, entropy contains five similar car images that are clearly distinguishable.\nIn order to provide favorable generalization performance, we further extend our method with an interactive sampling technique based on a gaussian mixture model. Our approach produces a targeted noise vector that successfully induces enough noise for model generalization while biasing the selection batch to highly informative samples that improve model robustness. We call our method Gaussian Switch Sampling or GauSS in short. We validate our algorithm with exhaustive experiments and compare against popular protocols used in practical active learning pipelines. Overall, we find that GauSS is robust to setup changes, performs favorably in outof-distribution settings, and achieves up to 5% improvement in terms of accuracy over popular existing strategies."
        },
        {
            "heading": "II. RELATED WORK",
            "text": ""
        },
        {
            "heading": "A. First Order and Second Order Active Learning",
            "text": "In active learning, a strong research branch involves exploring different acquisition functions and their definitions of information content. In this context, the majority of existing approaches leverage the model representation directly to define\nthe next acquisition batch. Due to the direct dependency, we refer to these approaches as first-order acquisition functions. To the best of our knowledge, second-order approaches remain largely unexplored.\nOverall, active learning approaches differ in their definition of information content for samples within the unlabeled data pool. In this context, several approaches define information content with generalization difficulty. As an example, several approaches define sample importance using softmax probabilities [55], [44] where information content is related to the output logits of the network. For instance, entropy sampling queries samples that have the highest softmax entropy while least confidence sampling queries samples with the lowest prediction probability (or confidence) score. Closely related to this concept, the authors in [45], [53] query samples based on decision boundary proximity. For instance, [52] query samples with support vector machines. [19] define importance as model uncertainty and select the following sample batch based on Monte Carlo dropout. Finally [7] use an ensemble of classifiers to query the next set of samples. In contrast, other approaches define information content based on data representation within the dataset. Acquisition functions in this field are based on the assumption that representative samples best approximate the overall dataset structure and improve performance. In this context, a large group of approaches focus on constructing the core-set of the unlabeled data pool [46], [20]. Furthermore, [21] use a discriminative approach, where the authors reformulate the active learning problem as an adversarial training problem. Specifically, they formulate active learning as a binary classification problem and select samples that minimize the differences between the labeled and unlabeled pool. Finally, there are several approaches that consider the combination of both data representation and generalization\ndifficulty within their definition of information content. In several cases [5], [24], an ordering is established based on generalization difficulty and the representative component is introduced by sampling interactively from the ordering. [33] integrate both generalization difficulty and data representation by extending [28] to diverse batch acquisitions. Finally, [6], [29] deploy \u201dmix and match\u201d meta-active learning approaches, that switch strategies each active learning round.\nEven though several approaches are effective, they directly depend on the model representation of the data and are susceptible to setup shifts or out-of-distribution samples."
        },
        {
            "heading": "B. Out-of-Distribution Analysis",
            "text": "Within the field of deep learning, the importance of outof-distribution samples within the context of robustness is well established [26]. In this context, several approaches involve measurements of uncertainty [18], [31], [54], gradient representations [35], [36], [37], [39], [38], [42], [40], [3], or model calibration [22], [41]. For instance, [54] reject out-ofdistribution samples by ranking samples uncertainty scores from a deterministic neural network. [36], detect anomalies with by imposing a gradient constraint on the model that distinguishes inliers from outliers, [22] introduce temperature scaling as an effective method to calibrate neural networks. With a few exceptions, out-of-distibution scenarios remain largely unexplored within the context of active learning with the scarce examples [34] being first-order approaches."
        },
        {
            "heading": "C. Efficient Active Learning and Theory",
            "text": "Apart from developing new active learning acquisition functions, there has been extensive research regarding efficient active learning as well as its practical implementation. [15] improve computational efficiency by approximating the target\nmodel with a smaller proxy model. Further, [24] consider scalability issues of active learning for object detection. In particular they investigate the implications of varying diverse and uncertain active learning components on large-scale data.\nA substantial research direction is further dedicated to theoretical aspects of active learning. For instance, [56], [50], [23] consider subsampling in regression problems. Several works also focus on understanding the active learning process itself. In this context, [17] investigate the implicit bias inherent within the active learning process.\nIn this work, we explore the capabilities of different protocols to assess information content. This represents a substantial contribution for designing efficient active learning paradigms as well as theoretic explanations for a acquisition function performance."
        },
        {
            "heading": "D. Continual Learning",
            "text": "In addition to active learning, our work is related to the field of continual learning and catastrophic forgetting [12], [32], [43], [51]. In this context, significant research efforts involve reducing forgetting through model constraints [2], [32], loss exploration [48], or augmentation [8], [9]. While the approaches effectively reduce forgetting, they do not consider (or estimate) information content in active learning settings\nMost significantly, our work shares a strong connection with the study of [51] on forgetting events. Within the article, the authors quantify neural networks forgetting in the training set and empirically show dependencies between forgetting and several important machine learning topics such as dataset compression and label noise. Even though the paper provides an interesting framework for quantifying neural network forgetting, the quantification requires labels and is unsuitable for information content estimation in practical active learning settings. In contrast, our work expands the concepts of [51] to function without annotation and develop a grounded definition of information content for active learning."
        },
        {
            "heading": "III. DEFINING SAMPLE INFORMATION WITH LEARNING DYNAMICS",
            "text": "Active learning consists of iteratively selecting a set of unlabeled samples for annotation. We refer to a single iteration as active learning round. In this section we formulate information content as learning difficulty within each active learning round. Intuitively, we define informative as the frequency in which a network \u201cforgets\u201d unlabeled samples during training. We introduce active learning and neural network forgetting singularly and further analyze our definition in comparison to existing definitions of informativeness."
        },
        {
            "heading": "A. Active Learning",
            "text": "In active learning, the objective is to improve data efficiency iteratively by involving the model in the data annotation process. In this context, consider a dataset D where a small subset Dtrain represents the initial training data and Dpool represents the unlabelled data pool for the selection process. The goal within each active learning round is to select a batch\nof b samples X\u2217 = {x\u22171, ..., x\u2217b} that improves the model accuracy the most when added to the training data Dtrain. The selection of X\u2217 can be defined as\nX\u2217 = argmax x1,...,xb\u2208Dpool a(x1, ..., xb|fw(x)) (1)\nwhere a represents the acquisition function and fw is the fully trained deep model conditioned on the parameters w. In this context, we define first-order algorithms and second-order algorithms by the manner in which they are conditioned on fw:\nDefinition 1 (First- and Second-Order Active Learning). We define an active learning algorithm as first-order when the acquisition function is conditioned on the fully trained neural network fw exclusively. Further, an algorithm is of secondorder when the acquisition function is conditioned on dynamic fluctuations of fw caused by external interference (e.g. training)."
        },
        {
            "heading": "B. Defining Importance with Forgetting Events",
            "text": "In this paper, we define importance through the learning difficulty of different unlabeled samples. Specifically, we build upon [51] and measure information content with the frequency in which samples are \u201dforgotten\u201d during training. Intuitively, a sample is \u201dforgotten\u201d if it was classified correctly (\u201dlearnt\u201d) at time t and subsequently misclassified (\u201dforgotten\u201d) at a later time t\u2032 > t.\nWithout loss of generalizability, we consider recognition tasks where the objective is to predict a label y\u0303i of sample xi that corresponds to the ground truth annotation yi. The accuracy of the model for sample xi at an arbitrary time t can be defined as,\naccti = 1y\u0303ti=yi . (2)\nIn Equation 2, 1y\u0303ti=yi describes a binary variable that indicates whether the classification was correct at time t. Specifically, it reduces to one if xi was correctly predicted or zero if the sample was misclassified. Based on our notation, we define a sample as \u201dforgotten\u201d if the accuracy decreases within two subsequent time steps:\nf ti = int(acc t i < acc t\u22121 i ) \u2208 [1, 0]. (3)\nHere, f ti is a binary result of accuracy variables and reduces to one if the accuracy within subsequent optimization steps decreases (a switch from correctly to incorrectly classified). Similar to [51], we define the binary event f ti as a forgetting event at time t. Based on these terminologies, we define the information content of each sample in a simple and intutive manner as follows:\nDefinition 2 (Information in Active Learning). Within the context of active learning, we quantify information by the amount of forgetting events that occur for a given sample each round. Specifically, we consider samples with the largest amount of forgetting events as the most informative while fewer forgetting events imply redundant information.\nIn contrast to existing definitions, forgetting events are not based on the representation directly but are second-order artifacts of decision boundary shifts. We reason that this characteristic is favorable for distribution shifts and setup choices."
        },
        {
            "heading": "C. Prediction Switches",
            "text": "Even though forgetting events are simple to formulate and provide an intuitive measure for importance, the computation is not tractable for practical active learning protocols. Specifically Equation 4 requires the label annotation yi which is unavailable for the unlabeled set Dpool by definition. For this purpose, we approximate forgetting events with prediction switches. A prediction switch occurs when the model output prediction y\u0303 changes within time intervals. Formally, we write\nsti = 1y\u0303ti !=y\u0303 t\u22121 i . (4)\nAnalogous to our previous definition, we call sti a switch event at time t.\nIV. INFORMATION DEFINITIONS IN ACTIVE LEARNING\nIn this section, we investigate the extent in which prediction switches differ to existing importance definitions. Specifically, we want to answer 1) how effectively do prediction switches estimate information content? and 2) how accurate are the importance scores when the representation is constrained?\nIn practical settings, sample representations are constrained by the size of the training set Dtrain and can result in inaccurate importance quantification, especially when the definition is based on the representation directly. For this purpose, we conduct our analysis by artificially removing the constraint imposed by limited data and base our importance scores on fully trained model representations - i.e. the output of the acquisition function is based on representations derived from a model trained on both Dtrain and Dpool. Even though this analysis puts our method at a clear disadvantage we note that our importance rankings are significantly more accurate than existing protocols."
        },
        {
            "heading": "A. Accurate Importance Scores vs. Performance Trade-off",
            "text": "In active learning, acquisition functions define sample importance by the information content added to the dataset. However, informative samples in practice are frequently irregular and can result in insufficient data representations when trained on informative samples exclusively. Using our cat/dog example classifier from the introduction, a training set with informative samples could exclusively contain rare cat/dog breeds that are underrepresented within the underlying distribution. A classifier trained on this constrained dataset will result in low generalization performance on a test set containing mostly well represented breeds. Within the context of active learning, acquisition functions that render importance scores that are \u201ctoo accurate\u201d frequently result in significantly lower performance than the random baseline or other acquisition functions with \u201cless accurate\u201d importance scores.\nIn our analysis, we consider scenarios where the representation space is not constrained by limited data and therefore significantly improves the importance score accuracy of the compared strategies. For this reason, effective practical acquisition functions perform significantly lower than the random baseline and exhibit lower generalization performance with higher information content within the acquisition batch.\nB. Importance in Optimal Settings We investigate which acquisition functions provide the most informative samples when acquisition functions produce accurate measurements. For this purpose, we consider an optimal setting where we first train a model on the full CIFAR10 training data and derive the sample representations from the fully trained model. The following importance scores (e.g. softmax entropy) are subsequently calculated with the optimal sample representations. For our proposed definition, we count the switch events while training on the full training data and query the samples ordered from most to least switch events. In this section, we compare optimal settings only as the discussion on practical implementations with limited annotation access is thoroughly discussed in Section VI.\nIn all of our active learning experiments, we start with an initial training set Dtrain of 128. In each round, we query 1024 additional samples with the repsective active learning strategy. For our active learning models we use a resnet18, as well as a resnet-34 architecture [25]. We optimize with the adam variant of SGD and a learning rate of 10\u22124. No augmentations are used for our active learning experiments. For our optimal representations, we train a network on the full CIFAR-10 dataset for 200 epochs and extract the representations for our active learning experiments. For the fully trained model, we optimize with SGD and use a multi step learning rate scheduler for training. Specifically, our initial learning rate is 0.1 and we divide the learning rate by 5 in epochs 100, 125, 150, and 175 respectively. We further use the augmentations random crop, random horizontal flip, and cutout. We choose this setup to derive competitive representations for the acquisition functions. We show the results of our analysis in Figure 4. Specifically, we compare switch events to entropy sampling [55], random sampling, least confidence sampling [55], and coreset [46]. We note that querying with optimal representations results in significantly lower generalization performance than the random baseline due to the high importance score accuracy. This is especially true for early rounds where only a limited amount of training data is available for the active learning model. Samples with a high information content are typically aberrant outliers that are disruptive to the representation space. Therefore, a more significant deviation from random sampling implies a higher information content. We note, that querying with switch events results in the most profound difference compared to the other strategies and implies the highest information content within each query. It is important to note that most existing active learning protocols outperform random sampling in practical settings - i.e. when the sample representations are inaccurate due to limited training data. In these cases, limited representation capabilities \u201chelp\u201d the acquisition function by introducing\na random noise factor within the query. Several strategies even introduce targeted sampling techniques to counteract accurate information scores [5], [24]. In GauSS we do this by sampling from a gaussian mixture model."
        },
        {
            "heading": "C. Approximation of Optimal Representations",
            "text": "We further investigate the approximation quality of different importance definitions with respect to their optimal counter part. As described within the previous section, most existing strategies outperform the random baseline while their optimal counter part significantly underperforms in terms of generalization accuracy. We follow that accuracy alone is not a sufficient metric to capture the optimal approximation capabilities of an active learning protocol. For this purpose, we measure protocol similarity through information content within the training set of each active learning round. In this context, we evaluate the importance score for each sample within the training set and compare the distributions of the importance scores for each round through statistical difference metrics. If the samples contain similar information, the resulting importance scores render similar distributions and vice versa. Specifically, we perform the following steps for each active learning round: 1) We evaluate the importance score for every training sample. 2) We estimate the distribution by creating histograms over the importance scores for every acquisition function. 3) We evaluate the statistical difference between different acquisition function pairs (in our case we use a smoothed KullbackLeibler divergence). We show our results in Figure 5. The different rows show the active learning protocols while the columns show the optimal counter parts. We compare switch events against entropy sampling and least confidence sampling as these are the most competitive definitions in our previous analysis. Further, we use forgetting events as the optimal counterpart of our switch event protocol. We choose this setup, as switch events should not only approximate their optimal switch event counterpart but should ideally approximate our original importance definition based on sample forgetting, not prediction switches. In our experiments, we use the same active learning setup as the previous section for experiments with optimal representations; The non-optimal protocol setup is identical with the exception of the data representation source - i.e. we follow the normal protocol workflow and gather sample representations from the model trained on Dtrain. We further generate importance scores for the histograms with forgetting events and compare histograms by calculating the Kullback-Leibler divergence smoothed by a gaussian filter. We average our result over the first 20 rounds.\nOverall, we note that switch events show the closest distribution similarity to all optimal strategies indicating the most accurate information content approximation within the round-wise training sets. We reason that switch events are not dependent on the model representation directly and are therefore more accurate in identifying the most informative batch regardless of the importance definition. This further manifests itself visually in the distribution histograms of individual rounds (Figure 6). We see that switch events exhibit the visually most similar distribution to optimal forgetting\nevents distribution than the other strategies. In particular, we note that the other strategies query more samples with less amounts of forgetting events than the switch event acquisition function which results in different distribution shapes. As a final note, we add that even though we compare distributions with forgetting events we observe a similar behavior when comparing against other importance definitions."
        },
        {
            "heading": "V. GAUSSIAN SWITCH SAMPLING",
            "text": "Based on our initial analysis, we reason that switch events are effective for estimating information for unlabeled samples. Further, we reason that the second-order nature is potentially more robust to different training setups and out-of-distribution settings. However, we noted in our experiments that effective importance scores bias queries towards outliers that inhibit generalization capabilities. For this purpose, we force targeted random noise into our acquisition batch by probabilistic sampling with a bias towards higher switch events. Specifically, we model the switch event distribution with a two component gaussian mixture model and sample from the component with the higher switch event mean.\nBuilding on the classification of [51] into forgettable and unforgettable data, we model each sample within the unlabeled sampling pool to be either less-switching U or frequently switching F . The total amount of switch events Stotal can then be modeled as the sum of the switch events from both less switching SU and frequently switching samples SF :\nStotal = SU + SF (5)\nWhen we query from the unlabeled pool, we would ideally target the most informative samples or, more specifically, the samples originating from class F . For this reason, we fit a gaussian mixture model with two components (one for U and one for F ) to the switch event distribution and assign the lessswitching distribution and frequently switching distribution to the gaussian mixture with the lower and higher mean respectively. During selection, we distinguish frequently switching samples from less switching samples with the separate gaussian components and sample the next acquisition batch from the frequenly switching distribution. Mathematically, we model the switch event distribution as\nStotal \u223c \u03c9uNu(\u00b5u, \u03c3u) + \u03c9fNf (\u00b5f , \u03c3f ) (6) where \u03c9u/f , \u00b5u/f , and \u03c3u/f refer to the weight, mean, and standard deviation of the singular distribution components U and F respectively. We subsequently establish the acquisition batch by sampling data points with probabilities from the second switch event distribution component Nf :\nX\u2217 \u223c Nf (\u00b5f , \u03c3f ) (7) In Figure 7, we show a toy example of our approach. Specifically, we show a histogram over the switch events occurring for a given round and sketch the ideally fitted gaussian mixture components over the histogram distribution. In this case, we would sample from the right (green) gaussian component. We call our method Gaussian Switch Sampling or GauSS in short."
        },
        {
            "heading": "VI. EXPERIMENTS",
            "text": "With our experiments we want to answer two central questions: 1) How well does GauSS perform in conventional active learning where the only representation constraint is imposed by Dtrain?, and 2) How does the active learning performance change in out-of-distribution scenarios? We feel that both 1) and 2) are realistic scenarios in practical deployment and represent a comprehensive study of the generalization capabilites of GauSS."
        },
        {
            "heading": "A. Numerical Comparison",
            "text": "In our experiments, we compare GauSS against four popular active learning protocols. Specifically, we compare against entropy sampling [55], coreset [46], active learning by learning (ALBL) [29], least confidence sampling [55], and BatchBald [33]. We choose this constellation as it provides a large variety of importance definitions: Entropy sampling and least confidence sampling define sample importance with generalization\ndifficulty while coreset maximizes the diversity of the training pool; active learning by learning represents a bandit style approach that interactively switches between coreset and least confidence during each round. It represents a fusion protocol that combines both generalization difficulty and data diversity. Finally, Batchbald is a recent bayesian approach that extends the popular BALD [28] method.\nWe consider three out-of-distribution settings as well as three in-distribution settings. For our out-of-distribution analysis, we sample our training set from the CIFAR10 training set and measure accuracy on CIFAR10-C [26], STL10 [13], and CINIC10 [16]. We choose these benchmarks, as they contain examples of data corruption and domain shifts - both of which are common in practical machine learning pipelines: Both STL10 and CINIC10 represent examples of cross domain out-of-distribution. STL10 is a difficult dataset with 8000 test samples and nine out of ten overlapping classes with CIFAR10 (we only consider the overlapping classes for accuracy calculations). CINIC10 is significantly larger than CIFAR10 with a test set of 90000 images and fully overlapping classes with CIFAR10. Furthermore, CIFAR10-C is an artificially corrupted version of CIFAR10 with 20 different corruption types on five different severity levels. We consider all corruptions except \u201clabels\u201d, \u201cshot noise\u201d, and \u201cspeckle noise\u201d and test on level two as well as level five each round. We choose this setup as it includes realistic data corruptions on a wide variety of severity levels. Our in-distribution analysis considers the datasets CIFAR10, CINIC10, and CURE-TSR [49]. Both CIFAR10 and CINIC10 are popular in-distribution benchmarks with increasing difficulty and dataset size (60000 images for CIFAR10 and 270000 images for CINIC10). CURE-TSR represents the practically relevant application of traffic sign recognition with corrupted and uncorrupted training and test sets. The dataset is especially challenging as traffic signs occur at different frequencies resulting in a high class imbalance. Hence, an effective algorithm must perform on both well represented as well as underrepresented classes to improve generalization performance on the test set. In our experiments, ee consider the uncorrupted training and test set only. For all datasets except CURE-TSR, we start with\nan initial training pool of 128 randomly chosen samples and iteratively query 1024 samples each round according to the respective protocols. For BatchBald, we implement a bayesian model by applying monte-carlo droupout. Similar to the setup of [33], we apply a dropout layer with a dropout probability of 0.4 before the final fully connected layer and sample 70 monte-carlo instances. When experimenting with CURE-TSR, we start with an initial training pool of 32 samples and query an additional 32 samples each round. We choose this setup as it\nsignificantly increases the difficulty for traffic sign recognition. We further use three popular network architectures: Resnet18 [25], resnet-34 [25], and densenet-121 [30]. Similar to our previous experiments, we optimize with the Adam variant of SGD with a learning rate of 10\u22124 and no learning rate scheduler. No data augmentations are used and we use pretrained model weights for our out-of-distribution experiments as well as our experiments on the CIFAR10 dataset. We reason that this represents a practical scenario where data is abundant in one domain but scarce in another. Finally, we retrain our model from scratch each round to prevent warm starting [4]. For all constellations, our results are averaged over 5 random seeds. Overall, this amounts to a total of 450 separate active learning experiments (we count all CIFAR10-C datasets as one experiment).\nIn several cases, the performance of active learning strategies differs across selection rounds. For instance, a strategy may show a strong accuracy increase for early rounds, and underperform in later rounds. Therefore, we quantify algorithm performance by summarizing the margin in which it outperforms (or underperforms) the random baseline. With random baseline, we refer to sampling random data points from the unlabeled pool each round. First, we calculate the difference to the random baseline for each round by subtracting the random baseline accuracy curve from the accuracy curve of the respective strategy. Second, we combine the difference values by integrating over the subtraction curve across several active learning rounds. In short, the metric amounts to the\ndifference in accuracy area compared to the random baseline. For reference, we provide the absolute area of the random baseline in the first row of each architecture. A negative difference (random accuracy is higher) contributes negatively to the integral and positive differences (comparison accuracy is higher) contributes positively to the integral. Hence, a negative integral indicates an overall higher random accuracy curve while a positive integral indicates an overall outperformance with respect to the random baseline. In addition, the higher the integral the more significant the positive difference and the larger the outperformance margin. By quantifying performance with differences to the same reference point, we remove biases from absolute values in each round. For instance, the difference in early rounds contributes to the integral with the same magnitude as later rounds even though the absolute accuracy is higher in later stages. For our results on CIFAR10C, we average the difference curve accross all corruptions and levels. We show the integration results in Table I.\nWe note, that GauSS shows the overall best performance over both in-distribution, and out-of-distribution benchmarks in nearly every experiment constellation. In particular, we note that GauSS shows an overall better performance than the random baseline in nearly every constellation. In contrast, the other strategies can outperform or underperform the random baseline. For instance, entropy sampling outperforms the random baseline in several cases (e.g. CIFAR10-C with resnet-34), but underperforms in other cases (e.g. CIFAR10 in-distribution test set with densenet-121). Moreover, we note that GauSS is more consistent accross different dataset and architecture choices. In all instances except a few outliers, GauSS outperforms the random baseline. In contrast, we find high fluctuations with competitive protocols. For instance, least confidence sampling significantly outperforms the random baseline on CIFAR10-C and CIFAR10 when using resnet-18, but underperforms when switching the architecture to densenet-121. We reason that GauSS queries more consistently as sample queries and model representation are not directly coupled. If the model representation severely suffers within a specific experiment configuration (e.g. when densenet-121 is used instead of resnet-18), the acquisition batch severely degenerates. In contrast GauSS does not query based on the representation exclusively. Therefore switching experiment components does not affect the overall protocol performance. Finally, we discuss the behavior of GauSS in settings with high class imbalances on CURE-TSR. In our experiments, GauSS outperforms on two of the three architecture choices and consistently shows positive improvements over the random baseline (i.e. positive integral values). Similar to our previous observations, competitive strategies are strong in one setting but underperform the random baseline in another. For instance, entropy sampling underperforms the random baseline on resnet-18 but outperforms on resnet-34. We follow that GauSS proposes consistent and accurate data samples even when high imbalances are present.\nIn addition to Table I, we provide absolute values of the different strategies (as well as the random baseline) in Table II. Specifically, we show accuracy values in rounds one, four, eight, and nine on CIFAR10, as well as its corrupted variant\nCIFAR10-C. We choose this constellation as it represents one early round, one intermediate round, and two late-stage rounds. Further, we conduct a t-test to determine the statistical significance of our results, where one refers to a statistically significant difference to GauSS, and zero to a statistically insignificant difference with GauSS. We note, that GauSS matches or outperforms the competing strategies across all rounds: GauSS either outperforms or is statistically insignificant with respect to the outperforming strategy. In contrast, existing strategies are either competitive in early rounds or later rounds but rarely show consistent performance across the entire active learning experiment."
        },
        {
            "heading": "B. Learning Curves",
            "text": "We further plot the learning curves of several experiment configurations in Figure 8. Specifically, we show the results of several configurations using a resnet-18 architecture on indistribution, as well as out-of-distribution experiments. As outof-distribution examples, we plot the CIFAR10-C corruptions brightness level two, JPEG compression level five, as well as the CINIC10 test set when training on CIFAR10. Further, we plot the in-distribution datasets CIFAR10, CININC10, and CURE-TSR. The learning curves, further confirm our observations from Table I. GauSS qualitatively outperforms the other strategies over a large variety of in-distribution, and out-of-distribution experiments. We further note, that the performance of GauSS is especially strong in early rounds where the training set contains few samples. We reason that the representation is especially inaccurate for early active learning rounds and that resulting importance metric is severly affected for the competitive strategies. For later rounds, the model develops a more accurate representation space and we observe GauSS outperforming by a significantly smaller margin. A further explanation for the superior performance is the targeted sampling approach in GauSS. As previously discussed, representation noise can be beneficial in scenarios where accurate importance definitions result in difficult outlier selections that can potentially damage the model representation (see our experiments with optimal representations in Section IV-B). This is especially true for the two protocols least confidence sampling, and entropy sampling that have a forcefully added random query noise due to the limited representation capabilities of the model. In contrast, GauSS adds targeted (or biased) noise that, in combination with accurate importance measurements, results in a more conclusive data representation when the representation is severely susceptible to outlier samples.\nWhile we only show a small subset of learning curves, we note that this behavior is consistent across the other experiment constellations as well but are omitted due to space limitations. With 16 different CIFAR10-C corruptions at two levels as well as the remaining benchmarks this amounts to 555 different active learning curves. In summary, we can report an improvement of up to 5% compared to the other strategies.\nC. Incremental Analysis Within this subsection, we investigate the performance of GauSS in settings with higher task complexity. For this pur-\npose, we consider an incremental analysis where we perform active learning on a subset of the CIFAR100 dataset with 5, 10, 20, and 50 of the 100 classes respectively. In our experiments, we use a resnet-18 architecture and opt for a similar setup as our previous experiments on CIFAR10. In particular, we start with an initial training set size of 128 and query 1024 samples each round until the entire unlabeled pool is fully annotated. We show the resulting learning curves of the four data subsets in Figure 9. We note that settings with less classes contain fewer samples in the data pool and are therefore fully queried in less rounds.\nOverall, GauSS shows favorable qualities with a higher\nnumber of tasks. While lower task quantities do not show distinguishable trends, GauSS performs well in complex settings with higher amounts of tasks. In particular, we note that GauSS performs well regardless of the active learning round. In contrast, the competitive strategies show a strong performance in early or later active learning rounds exclusively. As an example, entropy and least confidence sampling perform well in later rounds and underperform in early stages. On the other hand, Batchbald is particularly strong in early stages but is outperformed later stages where the data representations are more mature."
        },
        {
            "heading": "VII. CONCLUSION",
            "text": "In this paper, we introduced a grounded definition of information content for active learning protocols. In contrast to existing approaches, our definition is not based on the representation directly but defines importance with decision boundary shifts - or neural network \u201cforgetting\u201d. For practical usage, we approximate \u201cforgetting\u201d with prediction switches and find that prediction switches produce accurate importance scores when compared to other definitions. Finally, we develop a novel acquisition function that samples interactively with a gaussian mixture model. We validate our algorithm empirically with exhaustive experiments and compare against popular protocols used in practical active learning pipelines. Overall, GauSS is robust to setup changes, performs favorably in outof-distribution settings, and achieves up to 5% improvement in terms of accuracy."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "The authors would like to thank the remaining members of OLIVES and the anonymous reviewers for their feedback. In particular, we would like to thank Dr. Gukyeong Kwon for fruitful discussions within the preliminary phase of the project. This work was funded by a Ford-Gerogia Tech Alliance Project."
        }
    ],
    "title": "Gaussian Switch Sampling: A Second Order Approach to Active Learning",
    "year": 2023
}