{
    "abstractText": "We address the task of simultaneous part-level reconstruction and motion parameter estimation for articulated objects. Given two sets of multi-view images of an object in two static articulation states, we decouple the movable part from the static part and reconstruct shape and appearance while predicting the motion parameters. To tackle this problem, we present PARIS: a self-supervised, end-to-end architecture that learns part-level implicit shape and appearance models and optimizes motion parameters jointly without any 3D supervision, motion, or semantic annotation. Our experiments show that our method generalizes better across object categories, and outperforms baselines and prior work that are given 3D point clouds as input. Our approach improves reconstruction relative to state-ofthe-art baselines with a Chamfer-L1 distance reduction of 3.94 (45.2%) for objects and 26.79 (84.5%) for parts, and achieves 5% error rate for motion estimation across 10 object categories.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiayi Liu"
        },
        {
            "affiliations": [],
            "name": "Ali Mahdavi-Amiri"
        },
        {
            "affiliations": [],
            "name": "Manolis Savva"
        },
        {
            "affiliations": [],
            "name": "Simon Fraser"
        }
    ],
    "id": "SP:bc537d4374386eacca2b03b5cb3878035997bb45",
    "references": [
        {
            "authors": [
                "Ben Abbatematteo",
                "Stefanie Tellex",
                "George Konidaris"
            ],
            "title": "Learning to generalize kinematic models to novel objects",
            "venue": "In Proceedings of the Conference on Robot Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan T. Barron",
                "Ben Mildenhall",
                "Dor Verbin",
                "Pratul P. Srinivasan",
                "Peter Hedman"
            ],
            "title": "Mip-NeRF 360: Unbounded anti-aliased neural radiance fields",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Angel X. Chang",
                "Thomas A. Funkhouser",
                "Leonidas J. Guibas",
                "Pat Hanrahan",
                "Qi-Xing Huang",
                "Zimo Li",
                "Silvio Savarese",
                "Manolis Savva",
                "Shuran Song",
                "Hao Su",
                "Jianxiong Xiao",
                "Li Yi",
                "Fisher Yu"
            ],
            "title": "ShapeNet: An information-rich 3D model repository",
            "venue": "CoRR, abs/1512.03012,",
            "year": 2015
        },
        {
            "authors": [
                "Ruihang Chu",
                "Zhengzhe Liu",
                "Xiaoqing Ye",
                "Xiao Tan",
                "Xiaojuan Qi",
                "Chi-Wing Fu",
                "Jiaya Jia"
            ],
            "title": "Command-driven articulated object understanding and manipulation",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Fernando de Goes",
                "William Sheffler",
                "Kurt Fleischer"
            ],
            "title": "Character articulation through profile curves",
            "venue": "ACM Transactions on Graphics,",
            "year": 2022
        },
        {
            "authors": [
                "Boyang Deng",
                "JP Lewis",
                "Timothy Jeruzalski",
                "Gerard Pons- Moll",
                "Geoffrey Hinton",
                "Mohammad Norouzi",
                "Andrea Tagliasacchi"
            ],
            "title": "NASA: Neural articulated shape approximation",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "Zicong Fan",
                "Omid Taheri",
                "Dimitrios Tzionas",
                "Muhammed Kocabas",
                "Manuel Kaufmann",
                "Michael J. Black",
                "Otmar Hilliges"
            ],
            "title": "ARCTIC: A dataset for dexterous bimanual handobject manipulation",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Martin A. Fischler",
                "Robert C. Bolles"
            ],
            "title": "Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography",
            "venue": "Commun. ACM,",
            "year": 1981
        },
        {
            "authors": [
                "Samir Yitzhak Gadre",
                "Kiana Ehsani",
                "Shuran Song"
            ],
            "title": "Act the part: Learning interaction strategies for articulated object part discovery",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Jun Gao",
                "Wenzheng Chen",
                "Tommy Xiang",
                "Alec Jacobson",
                "Morgan McGuire",
                "Sanja Fidler"
            ],
            "title": "Learning deformable tetrahedral meshes for 3D reconstruction",
            "venue": "In Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Guo",
                "Guanying Chen",
                "Yuchao Dai",
                "Xiaoqing Ye",
                "Jiadai Sun",
                "Xiao Tan",
                "Errui Ding"
            ],
            "title": "Neural deformable voxel grid for fast optimization of dynamic view synthesis",
            "venue": "In Proceedings of the Asian Conference on Computer Vision (ACCV),",
            "year": 2022
        },
        {
            "authors": [
                "Sanjay Haresh",
                "Xiaohao Sun",
                "Hanxiao Jiang",
                "Angel X. Chang",
                "Manolis Savva"
            ],
            "title": "Articulated 3D human-object interactions from RGB videos: An empirical analysis of approaches and challenges",
            "venue": "In 3DV,",
            "year": 2022
        },
        {
            "authors": [
                "Nick Heppert",
                "Muhammad Zubair Irshad",
                "Sergey Zakharov",
                "Katherine Liu",
                "Rares Andrei Ambrus",
                "Jeannette Bohg",
                "Abhinav Valada",
                "Thomas Kollar"
            ],
            "title": "CARTO: Category and joint agnostic reconstruction of articulated objects",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Cheng-Chun Hsu",
                "Zhenyu Jiang",
                "Yuke Zhu"
            ],
            "title": "Ditto in the house: Building articulation models of indoor scenes through interactive perception",
            "year": 2023
        },
        {
            "authors": [
                "Ruizhen Hu",
                "Wenchao Li",
                "Oliver Van Kaick",
                "Ariel Shamir",
                "Hao Zhang",
                "Hui Huang"
            ],
            "title": "Learning to predict part mobility from a single static snapshot",
            "venue": "ACM Transactions on Graphics,",
            "year": 2017
        },
        {
            "authors": [
                "Ajinkya Jain",
                "Rudolf Lioutikov",
                "Caleb Chuck",
                "Scott Niekum"
            ],
            "title": "ScrewNet: Category-independent articulation model estimation from depth images using screw theory",
            "venue": "In Proceedings of the International Conference on Robotics and Automation (ICRA),",
            "year": 2021
        },
        {
            "authors": [
                "Rishabh Jain",
                "Mohd. Nayab Zafar",
                "J.C. Mohanta"
            ],
            "title": "Modeling and analysis of articulated robotic arm for material handling applications",
            "venue": "IOP Conference Series: Materials Science and Engineering,",
            "year": 2019
        },
        {
            "authors": [
                "Zhenyu Jiang",
                "Cheng-Chun Hsu",
                "Yuke Zhu"
            ],
            "title": "Ditto: Building Digital Twins of Articulated Objects from Interaction",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Xiaolong Li",
                "He Wang",
                "Li Yi",
                "Leonidas Guibas",
                "A Lynn Abbott",
                "Shuran Song"
            ],
            "title": "Category-level articulated object pose estimation",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Gengxin Liu",
                "Qian Sun",
                "Haibin Huang",
                "Chongyang Ma",
                "Yulan Guo",
                "Li Yi",
                "Hui Huang",
                "Ruizhen Hu"
            ],
            "title": "Semi-weakly supervised object kinematic motion prediction",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Liu Liu",
                "Wenqiang Xu",
                "Haoyuan Fu",
                "Sucheng Qian",
                "Qiaojun Yu",
                "Yang Han",
                "Cewu Lu"
            ],
            "title": "AKB-48: A real-world articulated object knowledge base",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "William E. Lorensen",
                "Harvey E. Cline"
            ],
            "title": "Marching Cubes: A high resolution 3D surface construction algorithm",
            "venue": "In Proc. SIGGRAPH,",
            "year": 1987
        },
        {
            "authors": [
                "Yongsen Mao",
                "Yiming Zhang",
                "Hanxiao Jiang",
                "Angel X Chang",
                "Manolis Savva"
            ],
            "title": "MultiScan: Scalable RGBD scanning for 3D environments with articulated objects",
            "venue": "In Advances in neural information processing systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul P. Srinivasan",
                "Matthew Tancik",
                "Jonathan T. Barron",
                "Ravi Ramamoorthi",
                "Ren Ng"
            ],
            "title": "NeRF: Representing scenes as neural radiance fields for view synthesis",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "Kaichun Mo",
                "Shilin Zhu",
                "Angel X. Chang",
                "Li Yi",
                "Subarna Tripathi",
                "Leonidas J. Guibas",
                "Hao Su"
            ],
            "title": "PartNet: A large-scale benchmark for fine-grained and hierarchical part-level 3D object understanding",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Kaichun Mo",
                "Leonidas J. Guibas",
                "Mustafa Mukadam",
                "Abhinav Gupta",
                "Shubham Tulsiani"
            ],
            "title": "Where2Act: From pixels to actions for articulated 3D objects",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Jiteng Mu",
                "Weichao Qiu",
                "Adam Kortylewski",
                "Alan Yuille",
                "Nuno Vasconcelos",
                "Xiaolong Wang"
            ],
            "title": "A-sdf: Learning disentangled signed distance functions for articulated shape representation",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Thomas M\u00fcller",
                "Alex Evans",
                "Christoph Schied",
                "Alexander Keller"
            ],
            "title": "Instant neural graphics primitives with a multiresolution hash encoding",
            "venue": "ACM Transactions on Graphics,",
            "year": 2022
        },
        {
            "authors": [
                "Atsuhiro Noguchi",
                "Xiao Sun",
                "Stephen Lin",
                "Tatsuya Harada"
            ],
            "title": "Neural articulated radiance field",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Jeong Joon Park",
                "Peter Florence",
                "Julian Straub",
                "Richard Newcombe",
                "Steven Lovegrove"
            ],
            "title": "DeepSDF: Learning continuous signed distance functions for shape representation",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Keunhong Park",
                "Utkarsh Sinha",
                "Peter Hedman",
                "Jonathan T. Barron",
                "Sofien Bouaziz",
                "Dan B Goldman",
                "Ricardo Martin- Brualla",
                "Steven M. Seitz"
            ],
            "title": "HyperNeRF: A higherdimensional representation for topologically varying neural radiance fields",
            "venue": "ACM Transactions on Graphics,",
            "year": 2021
        },
        {
            "authors": [
                "Albert Pumarola",
                "Enric Corona",
                "Gerard Pons-Moll",
                "Francesc Moreno-Noguer"
            ],
            "title": "D-NeRF: Neural radiance fields for dynamic scenes",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Charles Ruizhongtai Qi",
                "Li Yi",
                "Hao Su",
                "Leonidas J. Guibas"
            ],
            "title": "PointNet++: Deep hierarchical feature learning on point sets in a metric space",
            "venue": "In Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Radu Bogdan Rusu",
                "Nico Blodow",
                "Michael Beetz"
            ],
            "title": "Fast point feature histograms (FPFH) for 3D registration",
            "venue": "In Proceedings of the International Conference on Robotics and Automation (ICRA),",
            "year": 2009
        },
        {
            "authors": [
                "Manolis Savva",
                "Abhishek Kadian",
                "Oleksandr Maksymets",
                "Yili Zhao",
                "Erik Wijmans",
                "Bhavana Jain",
                "Julian Straub",
                "Jia Liu",
                "Vladlen Koltun",
                "Jitendra Malik"
            ],
            "title": "Habitat: A platform for embodied ai research",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "Aliaksandr Siarohin",
                "Oliver J. Woodford",
                "Jian Ren",
                "Menglei Chai",
                "Sergey Tulyakov"
            ],
            "title": "Motion representations for articulated animation",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Sungjoon Choi",
                "Qian-Yi Zhou",
                "Vladlen Koltun"
            ],
            "title": "Robust reconstruction of indoor scenes",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "Andrew Szot",
                "Alexander Clegg",
                "Eric Undersander",
                "Erik Wijmans",
                "Yili Zhao",
                "John Turner",
                "Noah Maestre",
                "Mustafa Mukadam",
                "Devendra Singh Chaplot",
                "Oleksandr Maksymets",
                "Aaron Gokaslan",
                "Vladim\u0131\u0301r Vondru\u0161",
                "Sameer Dharur",
                "Franziska Meier",
                "Wojciech Galuba",
                "Angel Chang",
                "Zsolt Kira",
                "Vladlen Koltun",
                "Jitendra Malik",
                "Manolis Savva",
                "Dhruv Batra"
            ],
            "title": "Habitat 2.0: Training home assistants to rearrange their habitat",
            "venue": "In Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Edgar Tretschk",
                "Ayush Tewari",
                "Vladislav Golyanik",
                "Michael Zollh\u00f6fer",
                "Christoph Lassner",
                "Christian Theobalt"
            ],
            "title": "Nonrigid neural radiance fields: Reconstruction and novel view synthesis of a dynamic scene from monocular video",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Wei-Cheng Tseng",
                "Hung-Ju Liao",
                "Lin Yen-Chen",
                "Min Sun"
            ],
            "title": "CLA-NeRF: Category-Level Articulated Neural Radiance Field",
            "venue": "In Proceedings of the International Conference on Robotics and Automation (ICRA),",
            "year": 2022
        },
        {
            "authors": [
                "Peng Wang",
                "Lingjie Liu",
                "Yuan Liu",
                "Christian Theobalt",
                "Taku Komura",
                "Wenping Wang"
            ],
            "title": "NeuS: Learning neural implicit surfaces by volume rendering for multi-view reconstruction",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Xiaogang Wang",
                "Bin Zhou",
                "Yahao Shi",
                "Xiaowu Chen",
                "Qinping Zhao",
                "Kai Xu"
            ],
            "title": "Shape2Motion: Joint analysis of motion parts and attributes from 3D shapes",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Zhou Wang",
                "A.C. Bovik",
                "H.R. Sheikh",
                "E.P. Simoncelli"
            ],
            "title": "Image quality assessment: from error visibility to structural similarity",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2004
        },
        {
            "authors": [
                "Fangyin Wei",
                "Rohan Chabra",
                "Lingni Ma",
                "Christoph Lassner",
                "Michael Zollhoefer",
                "Szymon Rusinkiewicz",
                "Chris Sweeney",
                "Richard Newcombe",
                "Mira Slavcheva"
            ],
            "title": "Self-supervised Neural Articulated Shape and Appearance Models",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Tianhao Wu",
                "Fangcheng Zhong",
                "Andrea Tagliasacchi",
                "Forrester Cole",
                "Cengiz Oztireli"
            ],
            "title": "D2NeRF: Self-supervised decoupling of dynamic and static objects from a monocular video",
            "venue": "In Advances in neural information processing systems,",
            "year": 2022
        },
        {
            "authors": [
                "Fanbo Xiang",
                "Yuzhe Qin",
                "Kaichun Mo",
                "Yikuan Xia",
                "Hao Zhu",
                "Fangchen Liu",
                "Minghua Liu",
                "Hanxiao Jiang",
                "Yifu Yuan",
                "He Wang",
                "Li Yi",
                "Angel X. Chang",
                "Leonidas J. Guibas",
                "Hao Su"
            ],
            "title": "SAPIEN: A simulated part-based interactive environment",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Zihao Yan",
                "Ruizhen Hu",
                "Xingguang Yan",
                "Luanmin Chen",
                "Oliver van Kaick",
                "Hao Zhang",
                "Hui Huang"
            ],
            "title": "RPM-Net: recurrent prediction of motion and parts from point cloud",
            "venue": "ACM Transactions on Graphics,",
            "year": 2019
        },
        {
            "authors": [
                "Wentao Yuan",
                "Zhaoyang Lv",
                "Tanner Schmidt",
                "Steven Lovegrove"
            ],
            "title": "STaR: Self-supervised tracking and reconstruction of rigid objects in motion with neural rendering",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Articulated objects consist of interconnected static and movable parts that exhibit motion. Such objects are ubiquitous in real life (e.g., drawers, ovens, chairs, laptops, staplers). Thus, perception and understanding of articulated object structure is important in many areas including robotics [49, 25, 9, 23], animation [39, 7], and industrial design [19]. Articulated object motion analysis enables robots to manipulate objects more effectively [11]. Acquiring digital replicas of articulated objects [20, 16] also enables simulating object articulation in applications involving robotic agents and embodied AI [37, 41].\nPrior work on articulated object understanding uses supervised learning which requires 3D supervision and articulation annotation [45, 50]. Unfortunately, such supervisory data is expensive and unavailable at scale. Another line of prior work assumes a known object category and learns separate models for each category [21, 29, 47, 43]. This makes generalization to arbitrary unseen objects diffi-\ncult. Recently, Jiang et al. [20] proposed Ditto: a categoryagnostic approach for motion and part geometry prediction from a pair of 3D point clouds. However, this approach is limited in generalization to unseen object categories and it does not address detailed appearance reconstruction.\nWe make the observation that perception of articulated objects involves two subproblems: reconstruction and motion analysis. These subproblems are tightly intertwined since knowing the complete geometry of an object makes motion analysis easier, while knowing the motion parameters of an object provides a signal for better reconstruction of the object given observations in different articulation states. Our insight is that by leveraging the intertwined nature of articulated object perception we can avoid reliance on explicit 3D data and motion parameter supervision.\nIn this paper, we propose PARIS: a self-supervised approach for joint reconstruction and motion analysis of articulated objects. By observing an articulated object in two states (see Figure 1), our method reconstructs the shape and appearance of the static and movable parts in two implicit neural fields, while predicting the articulation motion parameters. The separated neural fields are composited using the estimated motion parameters to set up self-supervisory losses relying only on the input RGB images.\nThus, our approach is category-agnostic and does not require any 3D data or supervisory signals for part segmenta-\nar X\niv :2\n30 8.\n07 39\n1v 1\n[ cs\n.C V\n] 1\n4 A\nug 2\n02 3\ntion, motion parameters, or object category semantics. In summary:\n\u2022 We address joint reconstruction and motion analysis for articulated objects including part-level shape and appearance, and motion estimation given RGB images of the object in only two static states.\n\u2022 We develop PARIS: a category-agnostic, selfsupervised and end-to-end approach that jointly performs reconstruction and motion analysis without 3D supervision, motion parameter or semantic annotation.\n\u2022 We systematically evaluate our approach on synthetic and real data, and show we significantly outperform prior work and baselines in shape and appearance quality, and motion parameter estimation accuracy."
        },
        {
            "heading": "2. Related Work",
            "text": "Movable part segmentation and analysis. The analysis of part mobility is a well-established challenge for understanding the kinematics of articulated objects [1, 28, 14, 11]. With more 3D data and annotations of articulation being collected for articulated objects, recent works favor tackling this problem from 3D inputs in a data-driven fashion. ScrewNet [18] uses a recurrent neural network to predict articulation without part segmentation, from a sequence of depth images. Assuming part segmentation is given, Hu et al. [17] estimate the part mobilities by mapping a point cloud to a class of motion sequences via metric learning.\nAlthough many approaches have been proposed to conduct semantic segmentation on 3D shapes [35, 52], the obtained part segmentation does not necessarily conform to mobilities. Considering this obstacle, later works address the mobility part segmentation and analysis collectively. Taking a single point cloud as input, Shape2Motion [45] and Li et al. [21] propose to learn a category-level model to address this coupled task in a supervised way. These models have only limited generalization to arbitrary unseen objects since a separate trained model is required for each category. To mitigate dependency on the object category, Yan et al. [50] and Abdul-Rashid et al. [2] design crosscategory networks to predict part segmentation and kinematic hierarchy from a point cloud. Liu et al. [22] learn part motion parameters from an over-segmented 3D scan in a semi-supervised manner. Chu et al. [5] proposes a categoryagnostic method to manipulate the articulated parts to predefined states driven by a user command.\nThe above works all focus on understanding the articulation and 3D structure of a point cloud. Our work reconstructs part-level surface and appearance jointly with motion estimation from only RGB images, which are more readily available compared to 3D or depth inputs. The closest work is Ditto [20] which also produces part-level surface and articulation parameters by observing two states of\nthe object as the input. Another similar concurrent work is CARTO [15] which reconstructs object surfaces with motion estimated from stereo images. The main differences are the following: 1) Ditto takes a pair of 3D point clouds as input, while we use two sets of multi-view RGB images of the object in two states; 2) Ditto and CARTO only produce geometry (no texture or other surface appearance) and CARTO cannot reconstruct parts; 3) The work mentioned above requires 3D supervision and articulation annotation during training, while our approach is self-supervised only with images.\nImplicit models for articulated objects. Neural implicit models are increasingly popular because of their continuous and topology-free representation. In early works, shape and deformation modeling of articulated objects with implicit functions requires 3D supervision e.g., NASA [8] for human objects and A-SDF [29] for general articulated objects. With the success of differentiable rendering techniques, shape and appearance models can be learned from multi-view RGB images. This enables reconstruction of static scenes [26, 44, 3], rigidly moving objects [51], as well as deformable objects [31, 33, 13, 12], and dynamically changing scenes [34, 42, 48]. Following up on ASDF [29], Wei et al. [47] proposed a category-level shape and appearance representation for general articulated objects. Conditioning on an articulated latent code, the network can recover the underlying shape and appearance of an unseen object and generate articulated states by interpolating in the latent space. Similarly, with 2D segmentation maps and annotation of articulation as extra supervision, CLA-NeRF [43] can output additional 2D segmentation and estimate part pose via inverse rendering as post-processing.\nWhat differentiates our work from the above is that we decouple the articulated parts in both shape and appearance without knowing the object category. Simultaneously, we estimate motion parameters in an end-to-end manner so that we can explicitly manipulate the articulated object to unseen state. STaR [51] and D2NeRF [48] share the same strategy in decoupling two components by learning separate fields using motion as a cue but they focus on modeling dynamic scenes from an RGB video, whereas we take two sets of RGB images of the object in different (nondynamic) states. Practically, our setting is more scalable as observations of common articulated objects in different states emerge naturally without operator intervention (e.g., a folding chair on two different days, once when used and once when put away folded). This setting introduces more challenges as our input is more sparse and exhibits occlusions both across views (tightly connected parts) and between states (e.g., in Figure 1 end state a large portion of the static part is occluded)."
        },
        {
            "heading": "3. Problem Statement",
            "text": "Considering an articulated object from an unknown category, our input is composed of two arbitrary articulation states of the object: start state t = 0 and end state t = 1. At each state t, a set of multi-view RGB images It with corresponding camera parameters are given. We assume only one part is moving in this pair of observations, where we call the moving part the movable part and the part remaining still the static part. Our first goal is to decouple the two parts in terms of both geometry and appearance. With a part-level shape and appearance model in hand, we can articulate the object to unseen states and render the object in new states from arbitrary views easily with articulated motion control.\nOur second goal is followed by articulated motion estimation. We assume the movable part exhibits either rotation or translation only. The motion type is required to estimate the motion parameters (joint axis and joint state). If it is not given, we first optimize the transformation as a SE(3) group to classify the motion type as a pre-processing step. Once the motion type is known we parameterize the joint accordingly. For a revolute joint, we parameterize it with a pivot point p \u2208 R3 and a rotation in the form of a unit quaternion q \u2208 R4, \u2225q\u2225 = 1. For a prismatic joint, we parameterize it with the joint axis as a unit vector a \u2208 R3, \u2225a\u2225 = 1 and translation distance d along this axis. Now we have a rotation function fp,q and a translation function fa,d using these parameters. Depending on the motion type given, one of the transformation function T \u2208 {fp,q, fa,d} will be plugged into the training pipeline\nto optimize the motion parameters jointly."
        },
        {
            "heading": "4. Method",
            "text": "Our method is an end-to-end framework that learns a part-level representation and estimates articulation jointly from object-level observations in a fully self-supervised fashion. We separate the static and movable parts by leveraging motion as a cue. Since the motion accounts for the inconsistency between two states, we optimize the motion parameters by registering the moving parts from the input state t to a canonical state t\u2217. During registration, the component that agrees with the transformation is extracted as the movable part. And the one remaining still is extracted as the static part. Figure 2 illustrates the design of our pipeline.\nNext, we explain our architecture (Section 4.1), and the supervision losses (Section 4.2) in detail."
        },
        {
            "heading": "4.1. Composite Neural Radiance Fields",
            "text": "We learn the static and mobile fields compositely during training, and the two fields share the same network architecture which is built upon Instant-NGP [30]. Note that we design each field to model a static scene (one part in one state), meaning neither of the fields is conditioned on the state t as the input. We are not modeling the continuous dynamics for the movable part since we only have multi-views of the object at two discrete states for supervision. We build the motion relation between two states by learning an explicit transformation function (defined in Section 3) that maps a canonical state to the two input states instead of relying on learned dynamics embedded in the field implicitly, which\ndifferentiates our method from other works studying the reconstruction of the dynamic scenes [48, 34]. Theoretically, any state t can be chosen for the movable part. To balance the signal backpropagated from the loss on the two input states, we use t\u2217 = 0.5 as a canonical state to be learned in the field for the movable part. See the supplement for details about the choice of canonical state.\nThe static field FS represents the static part that remains still in any state, and the mobile field FM represents the movable part in the canonical state (t\u2217 = 0.5). Formally, they are represented as FS(xt,dt) = \u03c3S(xt), cS(xt,dt) and FM (xt\u2217 ,dt\u2217) = \u03c3M (xt\u2217), cM (xt\u2217 ,dt\u2217) where xt \u2208 R3 is a point sampled along a ray at state t with direction, dt \u2208 R3. \u03c3(x) \u2208 R is the density value of the point x, and c(x,d) is the RGB color predicted from the point x from a view direction d. For an input state t \u2208 {0, 1}, we transform the point from xt to xt\u2217 and the view direction from dt to dt\u2217 to query the field FM , using the transformation func-\ntion T which is defined in Section 3. Learning an accurate articulation and segmentation establishes a correspondence from state t to t\u2217.\nWe adopt a similar training pipeline to the original NeRF [26] and extend the ray marching and volumetric rendering procedure to compose the two fields. For each training iteration, we independently sample points to produce rendering for each input state t \u2208 {0, 1} and supervise the results respectively. During ray marching for each state t, we first uniformly sample points xt along each ray to query the field FS , then we transform xt to xt\u2217 to query the field FM . The summation of the output densities \u03c3sum = \u03c3\nS(xt) + \u03c3M (xt\u2217) is used to construct the probability density function to guide the stratified sampling in the second round of ray marching.\nLet x(h) = o + hd be a point along a ray r = o + hd emitted from the center of the projection o with direction d. Considering a near and far bound [hn, hf ], we composite\nthe output from two fields to calculate the color C\u0302(r) for the ray r by integrating the weighted sum of the two colors for each point along the ray:\nC\u0302(r) = \u222b hf hn ( wS(h) \u00b7 cS(h) + wM (h) \u00b7 cM (h) ) dh (1)\nwhere we simplify our notation as c(h) \u2261 c(x(h),d). We define the weights wS(h) = T (h) \u00b7 \u03b1S(h) and wM (h) = T (h) \u00b7 \u03b1M (h), where T (h) is the transmittance at the point x(h) accumulated from the two fields with the summation density \u03c3sum(s). This is defined as:\nT (h) = exp ( \u2212 \u222b h hn \u03c3sum(s) \u00b7 \u03b4s ds ) (2)\nwhere \u03b4s = hs+1 \u2212 hs is the distance between adjacent samples. The intuition of this additive composition is that sample points from either field with a high-density value can terminate the ray during rendering. This strategy is also adopted in STaR [51] and D2NeRF [48]."
        },
        {
            "heading": "4.2. Supervision Losses",
            "text": "We supervise the learning of motion parameters and neural radiance fields jointly with two shared loss functions. The overall loss function is defined as L = Lrgb + \u03bbmaskLmask where the optimizer for motion parameters only considers Lrgb and Lmask during training. Lrgb defines the photometric loss between the rendering RGB value with ground truth RGB value C(r) for a pixel hit by ray r:\nLrgb = \u2225C\u0302(r)\u2212 C(r)\u222522. (3)\nThe mask loss Lmask is defined on the opacity O(r) for each pixel with a binary mask M(r) of the object:\nLmask = BCE (O(r),M(r)) (4)\nwhere BCE is the binary cross-entropy loss and \u03bbmask = 0.1 during training. The opacity O(r) is computed as:\nO(r) = \u222b hf hn w(h) dh. (5)\nWe empirically find that the mobile field easily accumulates noise in regions overlapping with the static field, especially when the static part is a large volume. The static part hides noise inside its volume during composition and we lack supervision on the points behind the surface with volumetric rendering. To alleviate this problem, we add a regularization term Lprob. The intuition is to encourage the composited color for each ray to be only contributed from one field instead of both. We define the ratio that the color of a ray r contributed from the mobile field as a fraction of the opacity values:\nPM (r) = OM (r)\nOM (r) +OS(r) . (6)\nThen we define our regularization term Lprob on this ratio for each ray as:\nLprob = H(PM (r)), (7) H(x) = x \u00b7 log(x) + (1\u2212 x) \u00b7 log(1\u2212 x). (8)\nBy minimizing Lprob for all the pixels in each iteration, we force PM (r) to be close to either 0 or 1. Thus, the density for each position will be encouraged to accumulate in only one field. We weigh this regularization term with 0.001 and only apply it to the loss for optimizing parameters for implicit fields as it does not provide a meaningful signal for motion estimation."
        },
        {
            "heading": "5. Experiments",
            "text": ""
        },
        {
            "heading": "5.1. Dataset",
            "text": "Synthetic dataset. The synthetic 3D models we use for evaluation are from the PartNet-Mobility dataset [49, 27, 4], a large-scale dataset for articulated objects across 46 categories. We select instances across 10 categories to conduct our experiments. For each articulation state, we randomly sample 64-100 views covering the upper hemisphere of the object to simulate capturing in the real world. Then we render RGB images and acquire camera parameters and object masks using Blender [6] to create our training data. Real-world dataset. The real data we use for experiments is from the MultiScan dataset [25], scanning real-world indoor scenes with articulated objects in multiple states. We use the reconstructed mesh of an object in two states as ground truth for evaluation, and the real RGB frames as training data."
        },
        {
            "heading": "5.2. Baselines",
            "text": "A-SDF. A-SDF [29] learns a category-level model that reconstructs object meshes given ground truth SDF samples. To compare the generalization ability of the model across categories, we train the model on the 10 testing examples and retrieve them during testing. We follow the method that DeepSDF [32] proposed to generate SDF samples for each testing case. Note that A-SDF cannot predict part-level geometry or estimate explicit motion parameters, we only measure the quality of surface reconstruction on the whole\nobject for comparison.\nDitto. Ditto [20] learns a category-agnostic model that reconstructs part meshes and estimates motion parameters (motion type, joint axis, joint state) from a pair of 3D point clouds. We leverage their released pre-trained model which is trained on Shape2Motion [45] dataset across 4 categories to do the comparison. And we test instances across 10 categories (3 seen and 7 unseen by Ditto, marked in Table 1) to compare the generalization ability regarding the part-level surface reconstruction and motion estimation. We sample point clouds on the surface of the ground truth meshes of the object in two given articulation states to feed as their input for both synthetic and real data experiments.\nOurs-ICP. None of the baseline methods mentioned above can produce the appearance of the object or even render the object in arbitrary views since they did not consume RGB information as their input. Here, we implemented a naive baseline approach as illustrated in Figure 4. We learn two separate neural implicit surfaces F0, F1 from the two given sets of multi-view images, which are backboned with NeuS [44] and Instant-NGP [30]. Then we compare the quality of the novel view synthesis at two given articulated states with this baseline model. Additionally, we apply Constructive Solid Geometry (CSG) algorithm between the two neural SDFs and extract the part-level geometry with Marching-Cube [24]. Specifically, the intersection region of the two fields can be extracted as the static part. By subtracting one field from the other, we can obtain the movable part in two states accordingly. We can further compute the\ntransformation between the movable parts in two states by using global registration [36, 10, 40] and generalized ICP registration [38] algorithms."
        },
        {
            "heading": "5.3. Evaluation Metrics",
            "text": "Part-level geometry. We use the Chamfer-L1 distance (CD) as the metric to evaluate the quality of the reconstructed meshes. We measure the CD on the whole reconstructed surface (CD-w) to compare it with all the baselines. We also measure the CD on the predicted static part (CDs) and movable part (CD-m) separately to compare them with all the baselines except for A-SDF since it cannot predict part segmentation without ground truth SDFs for parts. To be more specific, we sample 10,000 points on each surface to compute the distance from both the prediction to the ground truth and the other way around, then we average the two distances to report as the final metrics. The CD values shown are multiplied by 1,000 as in A-SDF and Ditto. Motion estimation. To evaluate the joint axis, we measure the angular error (Ang Err) for both types of joints. This metric computes the orientation difference between the predicted axis direction and the ground truth, ranging from 0 to 90 degrees. We also measure the position error (Pos Err) for revolute joints. This metric computes the minimum distance between the rotation axis and the ground truth which takes the position of the pivot point into account. The value is multiplied by 10 shown in Table 2. To evaluate the joint state, we measure the geodesic distance (in degree) for revolute joints and translation error for prismatic joints. Novel view synthesis. To evaluate the quality of the appearance model, we measure the Peak Signal-to-noise Ratio (PSNR) and the Structural Similarity Index (SSIM) [46] for rendered results from novel views. For each instance, we render 50 novel views of the object in each state and average the values. We achieve comparable rendering quality with the ICP baseline. See the supplement for details."
        },
        {
            "heading": "5.4. Experiment Results",
            "text": "Reconstruction and part segmentation. The quantitative results are in Table 1 and a qualitative comparison is in Figure 3. Note that our method reconstructs the canonical state of the movable part as the output, whereas Ditto [20] outputs the start state. Thus, we transform our predicted movable part from the canonical state to the start state using the predicted motion for these results.\nFor the whole object reconstruction, we observe in Table 1 that Ditto outperforms for Fridge, Washer, Oven, and Storage. Since Ditto takes ground truth 3D point clouds as input, they have more information about the inner space of these objects than we do with limited image views. So it is understandable that they perform better when a deep container is involved. For part segmentation, we outperform 7 out of 10 cases for both the static and movable parts. We have a big improvement over the movable part on average and outperform the baselines on average. From Table 1, we observe that Ditto performs well in the three seen categories (above the dashed line), but the quality drops significantly for unseen categories (below the dashed line). Also, we produce more geometric details and better segmentation, which indicates we have a better generalization ability. Motion estimation. We show the quantitative results of the joint axis and joint state estimation in Table 2 and Table 3. The predicted axis is also visualized in Figure 3. We observe that our motion estimation is significantly more accurate than other baselines for both revolute and prismatic joints. Since Ditto predicts a wrong motion type for two of the testing examples, we report a failure (F) in Table 3 for joint state evaluation and denote \u2217 beside the numbers in Table 2 for joint axis evaluation. Novel view synthesis and articulation generation. We show the qualitative results of generating the intermediate articulation using the motion we predict from an arbitrary view in Figure 5. Our high-quality rendering results for the intermediate states demonstrate that our decoupling of the part appearance model is accurate, and we can effectively\ngenerate arbitrary unseen states and render from arbitrary views with our part-level implicit representation and predicted motion parameters. Ablation studies. We conduct an ablation for the effectiveness of the regularization term in improving reconstruction quality, motion estimation, and appearance. The results are in Table 4. We observe that the regularization can indeed improve the surface reconstruction for both part and object on average, especially for the movable part where we do not have sufficient views to observe the whole part (e.g., Washer, Oven). We also show qualitative results in Figure 7 to have a closer look at the changes in the movable part for the two most effective cases.\nWe have an ablation for the choice of canonical state reconstructed in the mobile field. We choose to reconstruct a \u201cvirtual\u201d state at t\u2217 = 0.5 in our pipeline. This better fits our problem statement as it allows us to utilize supervision signals from both given states to predict the motion and segmentation. In Table 6, we show the impact on performance of using given state t\u2217 = 0 (more similar to STaR[51]) as the canonical state instead. We note a performance reduction across all aspects except for appearance for which the results are comparable, with a particularly large degradation in movable part geometric accuracy.\nWe also carry out an ablation over the number of input views by downsampling the views with farthest point sampling. In Table 7, we reproduce 64 view results from Tables 1 to 3 and report results for 32 to 4 views. We observe a decline in performance with decreasing views, with a significant drop below 16 views. This is expected given the challenge of learning NeRFs in the few input regime, and constitutes an interesting direction for future work. Real-world examples. Comparison with Ditto for two real cases is shown in Figure 6 and Table 5. Since Ditto and our method both assume that the geometries of the object in two states are pre-aligned, we both suffer from the error in alignment and reconstruction on the given mesh. Ditto and our method can both estimate accurate motion parameters.\nDitto produces cleaner segmentation in the first case, but a less accurate geometry in the second case."
        },
        {
            "heading": "5.5. Limitations",
            "text": "To obtain an accurate decoupling of the parts and motion parameters, our method relies on a sufficient number of multi-view observations of the object for both static and movable parts. Our inputs exhibit occlusions both across views (tightly connected parts) and between states (e.g. in Figure 1 end state, a large portion of the static part is occluded). This increases the level of difficulty in finding 3D correspondence from RGB images only. We observe that our method tends to fail to estimate the correct motion parameters for revolute joints when the movable part is 1) severely occluded across views (e.g., the door of the cabinet is fully closed); or 2) highly geometrically symmetric. We illustrate these failure modes in Figure 8 by rendering objects from a novel view in sequential states. This figure illustrates how our predicted motion manipulates the movable part to satisfy the supervision at two given states in the wrong way. Specifically, the door of the fridge is wrongly rotating around an axis away from the body, and the seat of the chair is rotating around the correct joint axis but in an opposite direction. We also observe that the optimization on the very thin parts is inclined to be unstable which can be alleviated by increasing the number of viewpoints."
        },
        {
            "heading": "6. Conclusion",
            "text": "We addressed the task of joint part-level shape and appearance reconstruction and motion parameter estimation for articulated objects. Our work is the first to tackle this task from multi-view RGB images observing the object in two arbitrary states. We evaluated our method on synthetic and real data to systematically study the challenges in this task. Our experiments show that we can recover shape, appearance, and motion parameters better than prior work and baselines. However, the task remains challenging especially for cases with severe occlusion. Extending to objects with multiple parts is another challenge for future work. Moreover, we assumed the two given object states are aligned in world coordinates, an assumption shared with prior work. We hope our work inspires more research into articulated object reconstruction without 3D supervision.\nAcknowledgements. This work was funded in part by a Canada Research Chair grant (CRC-2019-00298), NSERC Discovery grants (RGPIN-2019-06489, RGPIN2022-03111), and NSERC Discovery Launch Supplement (DGECR-2022-00359), and enabled in part by support from WestGrid and Compute Canada. We thank Hanxiao Jiang, Han-Hung Lee, Yongsen Mao, Yilin Liu for helpful discussions, as well as Angel X. Chang, Sanjay Haresh, Ning Wang, and Qirui Wu for proofreading."
        }
    ],
    "title": "PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects",
    "year": 2023
}