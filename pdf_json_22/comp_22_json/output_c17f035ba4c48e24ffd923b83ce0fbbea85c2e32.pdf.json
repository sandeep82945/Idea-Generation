{
    "abstractText": "In this paper, we introduce a novel neural network training framework that increases model\u2019s adversarial robustness to adversarial attacks while maintaining high clean accuracy by combining contrastive learning (CL) with adversarial training (AT). We propose to improve model robustness to adversarial attacks by learning feature representations that are consistent under both data augmentations and adversarial perturbations. We leverage contrastive learning to improve adversarial robustness by considering an adversarial example as another positive example, and aim to maximize the similarity between random augmentations of data samples and their adversarial example, while constantly updating the classification head in order to avoid a cognitive dissociation between the classification head and the embedding space. This dissociation is caused by the fact that CL updates the network up to the embedding space, while freezing the classification head which is used to generate new positive adversarial examples. We validate our method, Contrastive Learning with Adversarial Features(CLAF), on the CIFAR-10 dataset on which it outperforms both robust accuracy and clean accuracy over alternative supervised and self-supervised adversarial learning methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Adir Rahamim"
        },
        {
            "affiliations": [],
            "name": "Itay Naeh"
        }
    ],
    "id": "SP:24421eb32c3669eac62f1c5480e46f48e09ee4dd",
    "references": [
        {
            "authors": [
                "Christian Szegedy",
                "Wojciech Zaremba",
                "Ilya Sutskever",
                "Joan Bruna",
                "Dumitru Erhan",
                "Ian Goodfellow",
                "Rob Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "arXiv preprint arXiv:1312.6199, 2013. 1, 2",
            "year": 2013
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "arXiv preprint arXiv:1412.6572, 2014. 1, 2",
            "year": 2014
        },
        {
            "authors": [
                "Nicolas Papernot",
                "Patrick McDaniel",
                "Somesh Jha",
                "Matt Fredrikson",
                "Z Berkay Celik",
                "Ananthram Swami"
            ],
            "title": "The limitations of deep learning in adversarial settings",
            "venue": "2016 IEEE European symposium on security and privacy (EuroS&P). IEEE, 2016, pp. 372\u2013387. 1",
            "year": 2016
        },
        {
            "authors": [
                "Cihang Xie",
                "Jianyu Wang",
                "Zhishuai Zhang",
                "Zhou Ren",
                "Alan Yuille"
            ],
            "title": "Mitigating adversarial effects through randomization",
            "venue": "arXiv preprint arXiv:1711.01991, 2017. 1",
            "year": 1991
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "arXiv preprint arXiv:1706.06083, 2017. 1, 2, 4, 5",
            "year": 2017
        },
        {
            "authors": [
                "Hongyang Zhang",
                "Yaodong Yu",
                "Jiantao Jiao",
                "Eric Xing",
                "Laurent El Ghaoui",
                "Michael Jordan"
            ],
            "title": "Theoretically principled trade-off between robustness and accuracy",
            "venue": "International Conference on Machine Learning. PMLR, 2019, pp. 7472\u20137482. 1, 2, 4, 5",
            "year": 2019
        },
        {
            "authors": [
                "Spyros Gidaris",
                "Praveer Singh",
                "Nikos Komodakis"
            ],
            "title": "Unsupervised representation learning by predicting image rotations",
            "venue": "arXiv preprint arXiv:1803.07728, 2018. 1",
            "year": 1803
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 9729\u20139738. 1, 2",
            "year": 2020
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "International conference on machine learning. PMLR, 2020, pp. 1597\u2013 1607. 1, 2, 4",
            "year": 2020
        },
        {
            "authors": [
                "Prannay Khosla",
                "Piotr Teterwak",
                "Chen Wang",
                "Aaron Sarna",
                "Yonglong Tian",
                "Phillip Isola",
                "Aaron Maschinot",
                "Ce Liu",
                "Dilip Krishnan"
            ],
            "title": "Supervised contrastive learning",
            "venue": "arXiv preprint arXiv:2004.11362, 2020. 1, 2, 3, 4, 5",
            "year": 2004
        },
        {
            "authors": [
                "Harini Kannan",
                "Alexey Kurakin",
                "Ian Goodfellow"
            ],
            "title": "Adversarial logit pairing",
            "venue": "arXiv preprint arXiv:1803.06373, 2018. 2",
            "year": 1803
        },
        {
            "authors": [
                "Yinpeng Dong",
                "Fangzhou Liao",
                "Tianyu Pang",
                "Hang Su",
                "Jun Zhu",
                "Xiaolin Hu",
                "Jianguo Li"
            ],
            "title": "Boosting adversarial attacks with momentum",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 9185\u20139193. 2",
            "year": 2018
        },
        {
            "authors": [
                "Cihang Xie",
                "Mingxing Tan",
                "Boqing Gong",
                "Alan Yuille",
                "Quoc V Le"
            ],
            "title": "Smooth adversarial training",
            "venue": "arXiv preprint arXiv:2006.14536, 2020. 2",
            "year": 2006
        },
        {
            "authors": [
                "Florian Tramer",
                "Dan Boneh"
            ],
            "title": "Adversarial training and robustness for multiple perturbations",
            "venue": "arXiv preprint arXiv:1904.13000, 2019. 2",
            "year": 1904
        },
        {
            "authors": [
                "Pratyush Maini",
                "Eric Wong",
                "Zico Kolter"
            ],
            "title": "Adversarial robustness against the union of multiple perturbation models",
            "venue": "International Conference on Machine Learning. PMLR, 2020, pp. 6640\u20136650. 2",
            "year": 2020
        },
        {
            "authors": [
                "Pouya Samangouei",
                "Maya Kabkab",
                "Rama Chellappa"
            ],
            "title": "Defense-gan: Protecting classifiers against adversarial attacks using generative models",
            "venue": "arXiv preprint arXiv:1805.06605, 2018. 2",
            "year": 1805
        },
        {
            "authors": [
                "Naveed Akhtar",
                "Jian Liu",
                "Ajmal Mian"
            ],
            "title": "Defense against universal adversarial perturbations",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 3389\u20133398. 2",
            "year": 2018
        },
        {
            "authors": [
                "Weilin Xu",
                "David Evans",
                "Yanjun Qi"
            ],
            "title": "Feature squeezing: Detecting adversarial examples in deep neural networks",
            "venue": "arXiv preprint arXiv:1704.01155, 2017. 2",
            "year": 2017
        },
        {
            "authors": [
                "Spyros Gidaris",
                "Andrei Bursuc",
                "Nikos Komodakis",
                "Patrick P\u00e9rez",
                "Matthieu Cord"
            ],
            "title": "Learning representations by predicting bags of visual words",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 6928\u20136938. 2",
            "year": 2020
        },
        {
            "authors": [
                "Mathilde Caron",
                "Piotr Bojanowski",
                "Armand Joulin",
                "Matthijs Douze"
            ],
            "title": "Deep clustering for unsupervised learning of visual features",
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 132\u2013149. 2",
            "year": 2018
        },
        {
            "authors": [
                "Raia Hadsell",
                "Sumit Chopra",
                "Yann LeCun"
            ],
            "title": "Dimensionality reduction by learning an invariant mapping",
            "venue": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906). IEEE, 2006, vol. 2, pp. 1735\u20131742. 2",
            "year": 2006
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748, 2018. 2",
            "year": 1807
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770\u2013778. 3, 4",
            "year": 2016
        },
        {
            "authors": [
                "Jonathan Uesato",
                "Jean-Baptiste Alayrac",
                "Po-Sen Huang",
                "Robert Stanforth",
                "Alhussein Fawzi",
                "Pushmeet Kohli"
            ],
            "title": "Are labels required for improving adversarial robustness",
            "venue": "arXiv preprint arXiv:1905.13725, 2019. 3",
            "year": 1905
        },
        {
            "authors": [
                "Tianlong Chen",
                "Sijia Liu",
                "Shiyu Chang",
                "Yu Cheng",
                "Lisa Amini",
                "Zhangyang Wang"
            ],
            "title": "Adversarial robustness: From self-supervised pre-training to fine-tuning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 699\u2013708. 3",
            "year": 2020
        },
        {
            "authors": [
                "Ziyu Jiang",
                "Tianlong Chen",
                "Ting Chen",
                "Zhangyang Wang"
            ],
            "title": "Robust pre-training by adversarial contrastive learning",
            "venue": "NeurIPS, 2020. 3",
            "year": 2020
        },
        {
            "authors": [
                "Minseon Kim",
                "Jihoon Tack",
                "Sung Ju Hwang"
            ],
            "title": "Adversarial self-supervised contrastive learning",
            "venue": "arXiv preprint arXiv:2006.07589, 2020. 3, 4, 5",
            "year": 2006
        },
        {
            "authors": [
                "Zuxuan Wu",
                "Tom Goldstein",
                "Larry S Davis",
                "Ser- Nam Lim"
            ],
            "title": "That: Two head adversarial training for improving robustness at scale",
            "venue": "arXiv preprint arXiv:2103.13612, 2021. 3",
            "year": 2021
        },
        {
            "authors": [
                "Richard Zhang",
                "Phillip Isola",
                "Alexei A Efros"
            ],
            "title": "Colorful image colorization",
            "venue": "European conference on computer vision. Springer, 2016, pp. 649\u2013666. 4",
            "year": 2016
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "2009. 4",
            "year": 2009
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014 Adversarial defense, adversarial training, contrastive learning\n1. INTRODUCTION\nIn recent years, Deep Neural Networks (DNNs) have shown phenomenal performance in a wide range of tasks, but at the same time, these networks are susceptible to adversarial attacks[1, 2, 3] \u2013 malicious small (sometimes imperceptible) perturbation added to an image that alters the output of the network. The model\u2019s ability to resist adversarial attacks is critical for real-life applications, such as autonomous vehicles. Various methods have been proposed to achieve trained models with better robustness to adversarial attacks including [4, 5, 6]. Among them, the most promising is adversarial training: train the model using both clean and adversarial inputs generated by some attacker, e.g Fast Gradient Sign Method(FGSM)[2], Projected Gradient Descent(PGD)[5]\nand TRADES[6].\nRecently, Self-Supervised Learning (SSL) has emerged as a technique for learning representation without the need for labeled data. Representations obtained by self-supervised pretraining can be easily transferred to other downstream tasks with promising results, for example, discriminating between image rotations[7]. Contrastive learning proves to be an effective SSL technique with promising results[8, 9]. Specifically, CL frameworks are trained to learn the representation of unlabeled data by choosing an anchor and pulling it and its positive samples together in embedding space, while at the same time pushing it far apart from many negative samples. Since CL framework assumes no labels are available, the positive pair is often different augmentations of the sample, and a negative pair often consist of the sample and randomly chosen samples from the current batch. Recent research, Supervised Contrastive Learning (SCL) [10] extends the CL framework by leveraging label information, where embeddings from the same class are pulled closer together than embeddings from different classes. This enriches the set of positive samples per anchor, by using samples of the same class as the anchor, in addition to many negative samples.\nIn this work, we propose to consider an adversarial example as a positive example. Now, given the clean sample data augmentations and its adversarial counterpart, we pull together their feature representation in the embedding space and push it apart from many negative samples, to achieve better clean and robust accuracy. We will adopt the recent SCL framework for better results.\nA question that must be asked is how to craft the adversarial samples during contrastive learning, as adversarial images are images with intentionally perturbed pixels that cause misclassification, and in contrastive learning framework we only have an encoder and a projection head model and no linear classifier, which is necessary for the creation of adversarial samples. To overcome this hurdle, we leverage linear evaluation protocol[9]. Within its learning process, the network is practically divided into two separately trained sections: the encoder, from the input to the embedding space, which\nar X\niv :2\n20 3.\n08 95\n9v 3\n[ cs\n.L G\n] 1\n1 Se\np 20\n22\nis trained contrastively, and the classification head, which is trained with labeled data. Producing an adversarial example requires the whole network. When training the encoder, it changes and dissociates itself from the classification head a bit every epoch, until the embedding space represents information in a way which is irrelevant to the classification head. This cognitive dissociation between the two parts causes the previously trained classification head to be unsuitable in developing new adversarial examples for the updated encoder. We will need to maintain the linear classifier up-to-date with the current feature representations learned by the encoder, as in each encoder training epoch we learn new latent representation space, and using an outdated classifier will result in feature inconsistency and irrelevant adversarial examples. We suggest that for every encoder training epoch, we add a training, for a fixed and relatively small number of epochs, a linear classifier on top of the frozen encoder, that will be used to craft the adversarial samples that will be used in SCL encoder training. Note that the classification head is a single linear layer, thus its training is relatively fast, and it does not add substantial overhead to the total training time.\nWith this in mind, we introduce Contrastive Learning with Adversarial Features(CLAF), a supervised contrastive adversarial training framework to achieve state-of-the-art model robustness. CLAF consists of an encoder, projection head, and classifier. We start with a regular SCL phase for a fixed number of epochs where we train only the encoder and projection head to let them stabilize. Then, we add the adversarial samples to the set of positive samples. To do so, in each training step, we first train for a fixed and relatively small number of epochs a linear classifier, then we freeze it and use it on top of the encoder to create the desirable adversarial examples. In the last phase, we freeze the encoder parameters and train a final classification head.\n2. RELATED WORK\nAdversarial robustness. Adversarial robustness is an emerging topic in deep learning since [1] first showed their existence. To increase model robustness, [2] proposed the adversarial training method. First, it generates adversarial inputs by some attacker, then these examples are used to update the network parameters. This process can be interpreted as solving the following min-max optimization problem:\nmin \u03b8\n1\nn n\u2211 i=1 max x\u2032i\u2208B(xi, ) `(f\u03b8(x \u2032 i), yi) (1)\nWhere D = (xi, yi)i=1,...,n denotes the train set composed of pairs of clean image xi with yi as its label, f\u03b8(x) represents a model parameterized by weighs \u03b8. ` denotes the cross-entropy loss and B(xi, ) denotes the `p-norm ball centered at xi with radius , where the most common norm is `\u221e.\nThe inner maximization problem aims to find an adversarial version of the current sample xi that maximizes the training loss. Over the years, several methods proposed to generate adversarial examples. An early method by [2] proposed the fast gradient sign method(FGSM), which generates adversarial examples with a single gradient step that maximizes the loss. Follow-up work by [5] using projected gradient descent(PGD) to create models with better robustness to multistep attacks. PGD is an iterative variant of the gradient-based attack with the addition of multiple random restarts. Since then, the PGD attack and the adversarial training framework have been tested with different variations, for example, penalize the difference between logits of clean samples and their adversarial counterparts[11], use momentum to improve adversary[12], use of different activation function[13] and generalization to multiple types of adversarial attacks[14, 15]. Recent TRADES[6] method achieves high robustness levels by replacing the cross-entropy loss of adversarial training with a loss that promotes logits pair between the natural sample logits and the adversarial sample logits.\nBeside adversarial training, other defense mechanisms have been proposed, for example, DefenseGAN[16] method trains a GAN to learn the data distribution to generate a denoised version of adversarial example. Perturbation Rectifying Network (PRN) [17] use \u2019pre-input\u2019 layers to the model, and if a perturbation is detected, we use the PRN output for label prediction instead of the actual image. feature squeezing[18] is a method proposed to reduce the search space available to an adversary by merging samples that correspond to many different feature vectors in the original space into a single sample.\nSelf-supervised learning. In recent years, self-supervised learning has gained great popularity in the computer vision community, because of the costly operation of creating labeled datasets. SSL is generally divided into two parts: Pretraining a model on a pretext task using unlabeled data only for learning a good feature representation, then fine-tuning it for downstream tasks(e.g., image classification) using (fewshot) labeled data. Extensive studies have been conducted on self-supervised learning [19, 20]. Among these, methods based on contrastive learning[21, 22] achieved state-ofthe-art results, with learned features that surpass the feature learned in a supervised manner on many downstream tasks[9, 8]. CL feature representation learning is done by maximizing the agreement of positive pairs(two random augmentations of the original sample) relative to a large number of negative pairs. Recent work proposed SCL[10] that extends the CL framework by using label information and extends the set of positive samples with samples of the same class. Let us briefly discuss the SCL framework that our work is related to. For each input sample x, we generate two random aug-\nmentations of it - t1(x), t2(x), then both augmentation are separately input to the same base encoder f(\u00b7)(ResNet[23] backbone), resulting in a pair of representation vectors. The representation vectors are fed to a two-layer multi-layer perceptron (MLP) network g(\u00b7) that maps representations to the space where contrastive loss is applied, and the loss can be defined as:\nLSC = \u2211 i\u2208I \u22121 |P (i)| \u2211 p\u2208P (i) log exp(zi \u00b7 zp/\u03c4)\u2211 a\u2208A(i) exp(zi \u00b7 za/\u03c4) (2)\nWhere, i \u2208 I \u2261 {1, ..., 2N} is the index of an arbitrary augmented sample, z` = g(f(tj(x))), the \u00b7 symbol denotes the inner (dot) product, \u03c4 \u2208 R+ is an scalar temperature parameter, A(i) \u2261 I \\ i, P (i) \u2261 {p \u2208 A(i) : yp = yi} is the set of indices of all positives in the multiviewed batch distinct from i and |P (i)| is its cardinality. The definition of P (i) spots the difference between CL and SCL, in SCL the set of positive samples is much larger and consists not only of sample with different augmentation, but all samples in the multiviewed batch with the same label.\nAdversarial training and self-supervised learning. Most recent contrastive learning literature shows their use to improve natural accuracy. However, it has been shown that feature consistency w.r.t perturbation can improve adversarial robustness[24], and in this work we believe that feature consistency can further boost adversarial robustness, as adversarial examples might be the result of non-smooth feature space, i.e attacking a sample with small perturbation results in large feature change and misclassification. Few recent works explore the connection between SSL and adversarial training. [25] proposed to use several known self-supervised task losses like predicting rotation, permutation, and correct patches to pretrain a model and study the effect on its robustness. [26, 27] used SSL to unsupervised learn robust feature representation and improve model robustness. Both methods proposed a new way to generate adversarial examples based on the CL loss instead of regular label-based losses(e.g., cross-entropy).[28] proposed to train two encoders: clean trained and adversarially trained, and use two loss functions: contrastive loss to minimize feature inconsistency between natural and adversarial samples, and CE loss to promote high classification accuracy. The adversarial samples are used in the self-supervised feature representation training to achieve feature robustness.\n3. CONTRASTIVE LEARNING WITH ADVERSARIAL FEATURES(CLAF)\nIn this section, we introduce CLAF, a supervised contrastive learning framework with robust feature representation. The method learns robust feature representations and achieves state-of-the-art robust accuracy while also achieving the\nAlgorithm 1 Pseudo-code of CLAF SCL with adversarial examples stage phase in PyTorch-like style.\n# g: projection head # f: encoder # c: classification head # eps: perturbation bound # K: attack steps # optimizer_classifier: optimizer for c parameters # optimizer_features: optimizer for f and g parameters\n# Linear classifier training for _ in range(N):\nfor x, y in loader: # x: data, y: labels # generate adversarial examples with attacker x_adv = attack(f, c, x, y, K, eps)\nlogits_adv = c.forward(f.forward(x_adv)) # SGD update: linear classifier loss = CrossEntropyLoss(logits_adv, y) loss.backward() optimizer_classifier.step()\n# Encoder and projection head training for x, y in loader:# x: data, y: labels\nx1 = aug(x) # a randomly augmented version x2 = aug(x) # another randomly augmented version # generate adversarial examples with attacker x_adv = attack(f, c, x, y, K, eps)\n# compute latent vectors f1 = g.forward(f.forward(x1)) f2 = g.forward(f.forward(x2)) f_adv = g.forward(f.forward(x_adv)) features = torch.cat([f1, f2, f_adv], dim=1)\n# compute supervised contrastive loss loss = scl_loss(features, y) loss.backward() optimizer_features.step()\nhighest clean accuracy compared to previous adversarial training-oriented methods. Our framework is composed of five main parts:\n\u2022 Data augmentation module - Given an input sample x, it generates two random augmentations of it, each includes a small subset of the sample information and represents a different view of it.\n\u2022 Neural network encoder f(\u00b7) that maps an input sample x\u2032 to a feature representation vector v = f(x\u2032) \u2208 Rd.\n\u2022 Projection head - A small neural network g(\u00b7) that maps representation v to the space where contrastive loss is applied z = g(v) \u2208 Rp.\n\u2022 Classification head - Single linear layer c(\u00b7) used to map the feature representation vector to a prediction vector l = c(v) \u2208 Rn, where n is the number of classes.\n\u2022 Attacker module - Given a model, input sample, and true label, the attacker generates an adversarial sample similar to the original sample (under given distance metric) that causes misclassification.\nThe method training is divided into three stages:\nSupervised contrastive learning. We start with vanilla supervised contrastive learning, same as [10], for a relatively\nsmall number of epochs. Given the current batch of size N , for each sample in the batch, we generate two random augmentations of the sample, obtaining 2N augmented samples D1 = {xi}i=2Ni=1 . For each augmented sample xi \u2208 D1 we calculate the latent vector zi = g(f(xi)) \u2208 Rp, where f is the encoder and g is the projection head. We update networks f and g to minimize LSC .\nSupervised contrastive learning with adversarial examples. In this stage our goal is to add for each sample\u2019s positive set another positive example - an adversarial image. However, as discussed earlier, in order to create adversarial examples using known adversaries, e.g. PGD[5], we need a classifier model. To overcome this hurdle, we train a linear classifier in parallel to the encoder training. We use a single linear layer that maps representation vector v to a prediction vector l \u2208 Rn, where n is the number of classes. At the beginning of each training epoch, we adversarially train, for a fixed and relatively small number of epochs, a linear classifier on top of the frozen encoder f . We perform this classifier retraining after every encoder parameter change in order to keep the linear classifier updated with the current feature representation learned by the encoder, and afterward train the encoder with reliable adversarial samples. Later, in the attack algorithm, we use the encoder and the linear classifier as one pipeline to craft, for each sample, the adversarial example. Now, we get 2N augmented samples and another N adversarial samples, obtaining in-total 3N samples used for training \u2013 D2{xi}3Ni=1. The training continues the same way as in the regular SCL phase - for each sample xi \u2208 D2 we calculate it\u2019s latent vector zi and update network f and g to minimize LSC . The description of this step can be found in Algorithm 1.\nLinear evaluation. To evaluate the quality of the learned visual representation, we leverage the widely-used linear evaluation criteria[9, 29]. We freeze the encoder f weights and on top of it we train a linear layer c(\u00b7) using clean samples and labeled data. Since we keep the encoder untouched, this test can be seen as a proxy to the representation learned.\n4. EXPERIMENTS\nWe now present an empirical evaluation of the proposed method by measuring clean accuracy and robust accuracy and show a comparison of our method with previous adversarial learning methods. The code to reproduce the experimental results is available at https://github.com/ AdirRahamim/CLAF."
        },
        {
            "heading": "4.1. Experimental settings",
            "text": "We use CIFAR10[30] as the benchmark dataset in our experiment. It has 50k training images and 10k test images. For\nthe encoder network f(\u00b7) we used ResNet-18 backbone[23]. We take the output after the average pooling layer as the representation vector v, thus getting a vector of size 512. As the projection head, we used a MLP with a single hidden layer of size 512 and an output vector of size 128. For encoder and projection head optimization we used SGD optimizer, with an initial learning rate of 0.05 with cosine learning rate decay. We experimented with a batch size of 256, and we ran the supervised contrastive learn phase for 60 epochs and supervised contrastive learn with adversarial examples phase for another 140 epochs(200 epochs in total).\nFor training of the classifier used to generate adversarial examples (denoted as c in Algorithm 1), we used a single linear layer of size 10. To generate adversarial examples during adversarial training of the classifier, we used the `\u221e PGD attack[5] with hyperparameters = 8/255, \u03b7 = 2/255 and k = 5, and we trained it each time for 5 epochs using Adam optimizer with a learning rate of 0.001. We used the cross-entropy(CE) loss for training. For the linear evaluation phase, we train a linear layer on top of the frozen encoder f . We train the linear layer for 100 epochs with Adam optimizer with an initial learning rate of 0.001 and cosine learning rate decay. We used the cross-entropy(CE) loss for training. For robustness evaluation, we used the `\u221e PGD attack[5] with hyperparameters \u03b7 = 2/255, k = 10 and = 8/255 or = 16/255."
        },
        {
            "heading": "4.2. Main results",
            "text": "We report the clean and robust accuracy of previous state-ofthe-art methods and our method in Table 1. We achieve a new state-of-the-art robust accuracy against `\u221e PGD-10 attack for both = 8/255 and = 16/255, significantly outperforms all recent baselines by a large margin, while keeping the highest clean accuracy(92.4%) among all methods proposed to improve robust accuracy by adversarial training. Compared to\nstandard supervised contrastive learning[10], our method significantly improves robust accuracy, which demonstrates that minimizing feature-space domain shift between natural and adversarial images indeed improves robust accuracy. Compared to other adversarially trained models, our method outperforms both AT[5] and TRADES[6] methods. Moreover, our method outperforms the recent adversarial self-supervised contrastive learning method - RoCL-AT[27], which adversarially pre-trains an encoder with adversarial examples founded by self-supervised contrastive loss and performs linear evaluation with supervised adversarial training.\n5. CONCLUSION\nIn this paper, we proposed a novel method for improving model robustness to adversarial attacks. We proposed a new idea of adding adversarial examples to sample positive set and adversarially train a linear classifier to generate them. We suggested the novel idea to retrain the linear classifier at each encoder train epoch to avoid feature inconsistency and keep linear classifier up-to-date with learned latent representation space. We demonstrated that adversarial robustness can be improved by maximizing the similarity between a transformed sample and generated adversarial sample of it, using the principle of supervised contrastive learning. We validated our method against the previous state-of-the-art methods and achieved superior clean and robust accuracy.\n6. ABLATION STUDIES\nAdversarial linear evaluation. In the linear evaluation phase, we freeze the encoder f and on top of it we naturally\ntrain linear layer c. We can adversarially train the linear layer c in the same adversarial training procedure. We adversarially train a linear classifier with adversarial images generated by `\u221e PGD[5] attack with parameters k = 10, = 8/255, \u03b7 = 2/255. The results in Table 4 shows that naturally trained linear classifier in the linear evaluation phase performs better than adversarially trained linear classifier.\nNatural trained linear classifier. In Algorithm 1, the classifier c is adversarially trained. However, it is possible to train it with natural examples. The comparative study in Table 2 shows that the adversarial training of the linear classifier c improves both the natural and adversarial accuracy of the model.\nUse linear classifier c in the linear evaluation phase. In the linear evaluation step, we train a reinitialized linear classifier.\nWe also experimented to use the linear classifier c from Algorithm 1, that we used to craft the adversarial examples, for the linear evaluation phase and train its last state for another 100 epochs. Table 6 shows that using linear classifier c achieves similar results, with a small improvement of robustness under `\u221e PGD attack with = 16/255.\nReinitialize linear classifier c in each epoch. At the beginning of each encoder train epoch, we train the linear classifier c for a small number of epochs, where we continue the training from the last state. We also examined reinitializing c before each training phase. Table 5 shows that reinitializing the linear classifier improves robust accuracy against `\u221e PGD attack - in 0.4% for = 8/255 and for = 16/255 it improves in 1.2%, however it achieves 0.2% lower accuracy on natural images.\nRobustness under more PGD attack steps. We further validate the robustness of CLAF under various number of PGD attack iterations. The results on table 3 shows that our method remains robust under higher number of steps (e.g., 60.03% accuracy under 100 steps).\n7. REFERENCES\n[1] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus, \u201cIntriguing properties of neural networks,\u201d arXiv preprint arXiv:1312.6199, 2013. 1, 2\n[2] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy, \u201cExplaining and harnessing adversarial examples,\u201d arXiv preprint arXiv:1412.6572, 2014. 1, 2\n[3] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami, \u201cThe limitations of deep learning in adversarial settings,\u201d in 2016 IEEE European symposium on security and privacy (EuroS&P). IEEE, 2016, pp. 372\u2013387. 1\n[4] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille, \u201cMitigating adversarial effects through randomization,\u201d arXiv preprint arXiv:1711.01991, 2017. 1\n[5] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu, \u201cTowards deep learning models resistant to adversarial attacks,\u201d arXiv preprint arXiv:1706.06083, 2017. 1, 2, 4, 5\n[6] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan, \u201cTheoretically principled trade-off between robustness and accuracy,\u201d in International Conference on Machine Learning. PMLR, 2019, pp. 7472\u20137482. 1, 2, 4, 5\n[7] Spyros Gidaris, Praveer Singh, and Nikos Komodakis, \u201cUnsupervised representation learning by predicting image rotations,\u201d arXiv preprint arXiv:1803.07728, 2018. 1\n[8] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick, \u201cMomentum contrast for unsupervised visual representation learning,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 9729\u20139738. 1, 2\n[9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton, \u201cA simple framework for contrastive learning of visual representations,\u201d in International conference on machine learning. PMLR, 2020, pp. 1597\u2013 1607. 1, 2, 4\n[10] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan, \u201cSupervised contrastive learning,\u201d arXiv preprint arXiv:2004.11362, 2020. 1, 2, 3, 4, 5\n[11] Harini Kannan, Alexey Kurakin, and Ian Goodfellow, \u201cAdversarial logit pairing,\u201d arXiv preprint arXiv:1803.06373, 2018. 2\n[12] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li, \u201cBoosting adversarial attacks with momentum,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 9185\u20139193. 2\n[13] Cihang Xie, Mingxing Tan, Boqing Gong, Alan Yuille, and Quoc V Le, \u201cSmooth adversarial training,\u201d arXiv preprint arXiv:2006.14536, 2020. 2\n[14] Florian Tramer and Dan Boneh, \u201cAdversarial training and robustness for multiple perturbations,\u201d arXiv preprint arXiv:1904.13000, 2019. 2\n[15] Pratyush Maini, Eric Wong, and Zico Kolter, \u201cAdversarial robustness against the union of multiple perturbation models,\u201d in International Conference on Machine Learning. PMLR, 2020, pp. 6640\u20136650. 2\n[16] Pouya Samangouei, Maya Kabkab, and Rama Chellappa, \u201cDefense-gan: Protecting classifiers against adversarial attacks using generative models,\u201d arXiv preprint arXiv:1805.06605, 2018. 2\n[17] Naveed Akhtar, Jian Liu, and Ajmal Mian, \u201cDefense against universal adversarial perturbations,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 3389\u20133398. 2\n[18] Weilin Xu, David Evans, and Yanjun Qi, \u201cFeature squeezing: Detecting adversarial examples in deep neural networks,\u201d arXiv preprint arXiv:1704.01155, 2017. 2\n[19] Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Pe\u0301rez, and Matthieu Cord, \u201cLearning representations by predicting bags of visual words,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 6928\u20136938. 2\n[20] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze, \u201cDeep clustering for unsupervised learning of visual features,\u201d in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 132\u2013149. 2\n[21] Raia Hadsell, Sumit Chopra, and Yann LeCun, \u201cDimensionality reduction by learning an invariant mapping,\u201d in 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906). IEEE, 2006, vol. 2, pp. 1735\u20131742. 2\n[22] Aaron van den Oord, Yazhe Li, and Oriol Vinyals, \u201cRepresentation learning with contrastive predictive coding,\u201d arXiv preprint arXiv:1807.03748, 2018. 2\n[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, \u201cDeep residual learning for image recognition,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770\u2013778. 3, 4\n[24] Jonathan Uesato, Jean-Baptiste Alayrac, Po-Sen Huang, Robert Stanforth, Alhussein Fawzi, and Pushmeet Kohli, \u201cAre labels required for improving adversarial robustness?,\u201d arXiv preprint arXiv:1905.13725, 2019. 3\n[25] Tianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, and Zhangyang Wang, \u201cAdversarial robustness: From self-supervised pre-training to fine-tuning,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 699\u2013708. 3\n[26] Ziyu Jiang, Tianlong Chen, Ting Chen, and Zhangyang Wang, \u201cRobust pre-training by adversarial contrastive learning.,\u201d in NeurIPS, 2020. 3\n[27] Minseon Kim, Jihoon Tack, and Sung Ju Hwang, \u201cAdversarial self-supervised contrastive learning,\u201d arXiv preprint arXiv:2006.07589, 2020. 3, 4, 5\n[28] Zuxuan Wu, Tom Goldstein, Larry S Davis, and SerNam Lim, \u201cThat: Two head adversarial training for improving robustness at scale,\u201d arXiv preprint arXiv:2103.13612, 2021. 3\n[29] Richard Zhang, Phillip Isola, and Alexei A Efros, \u201cColorful image colorization,\u201d in European conference on computer vision. Springer, 2016, pp. 649\u2013666. 4\n[30] Alex Krizhevsky, Geoffrey Hinton, et al., \u201cLearning multiple layers of features from tiny images,\u201d 2009. 4"
        }
    ],
    "title": "ROBUSTNESS THROUGH COGNITIVE DISSOCIATION MITIGATION IN CONTRASTIVE ADVERSARIAL TRAINING",
    "year": 2022
}