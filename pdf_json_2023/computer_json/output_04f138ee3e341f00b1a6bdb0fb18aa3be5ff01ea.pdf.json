{
    "abstractText": "Abstract\u0105\u0142 To deliver ultra-high resolution 360degree video (such as 8K, 12K, or even higher) across the internet, viewport-dependent streaming becomes necessary to save bandwidth. During viewport switches, clients and servers will instantly exchange coordination info and contents for the given viewports. However, those viewport switches pose a serious challenge for video encoding because the temporal dependency between contents within changing viewports is unpredictable. In existing practices, it is commonly noted that GOP (Group of Pictures) size in a bitstream intrinsically prohibits the reduction of the viewport switch latency, such as Motion-to-photon (MTP) latency, or motion-to-high-quality (MTHQ) latency. In this paper, we presented a Scalable Video Coding (SVC) based bitstream schema, which can structurally remove the impacts of GOP in viewport-dependent streaming and provide instant viewport switches within one-frame time (the best possible). In addition, combined with tiling, this new coding schema allows an efficient packing of the non-adjacent regions within a viewport of 360-degree video. Our experiments also show that the overall encoding with this SVC-based approach is faster than with multistream approaches. Compared with current 360-degree video streaming solutions based on MPEG-I OMAF, our approach is superior in terms of viewport switch latency, simplicity of viewport packing, and encoding performance.",
    "authors": [
        {
            "affiliations": [],
            "name": "Gang Shen"
        },
        {
            "affiliations": [],
            "name": "Mingyang Ma"
        },
        {
            "affiliations": [],
            "name": "Guangxin Xu"
        }
    ],
    "id": "SP:0165ae29fc11c476ab507c5603999778f9078e78",
    "references": [
        {
            "authors": [
                "Atsuro Ichigaya",
                "Yukihiro Nishida"
            ],
            "title": "Required bit rates analysis for a new broadcasting service using HEVC/H",
            "venue": "IEEE Transactions on Broadcasting,",
            "year": 2016
        },
        {
            "authors": [
                "Miska M Hannuksela",
                "Ye-Kui Wang",
                "Ari Hourunranta"
            ],
            "title": "An overview of the OMAF standard for 360 video",
            "venue": "Data Compression Conference (DCC),",
            "year": 2019
        },
        {
            "authors": [
                "B Choi",
                "YK Wang",
                "MM Hannuksela",
                "Y Lim",
                "A Murtaza"
            ],
            "title": "Information technology\u2013coded representation of immersive media (MPEG- I)\u2013part 2: Omnidirectional media",
            "venue": "format. ISO/IEC,",
            "year": 2017
        },
        {
            "authors": [
                "Dimitri Podborski",
                "Jangwoo Son",
                "Gurdeep Singh Bhullar",
                "Robert Skupin",
                "Yago Sanchez",
                "Cornelius Hellge",
                "Thomas Schierl"
            ],
            "title": "HTML5 MSE playback of MPEG 360 VR tiled streaming: JavaScript implementation of MPEG-OMAF viewport-dependent video profile with HEVC tiles",
            "venue": "In Proceedings of the 10th ACM Multimedia Systems Conference,",
            "year": 2019
        },
        {
            "authors": [
                "Yu You",
                "Ari Hourunranta",
                "Emre B Aksu"
            ],
            "title": "OMAF4Cloud: Standardsenabled 360\u00b0 video creation as a service",
            "venue": "SMPTE Motion Imaging Journal,",
            "year": 2020
        },
        {
            "authors": [
                "Qi Zhang",
                "Jianchao Wei",
                "Shanshe Wang",
                "Siwei Ma",
                "Wen Gao"
            ],
            "title": "RealVR: Efficient, economical, and quality-of-experience-driven VR video system based on MPEG OMAF",
            "venue": "IEEE Transactions on Multimedia,",
            "year": 2022
        },
        {
            "authors": [
                "Rachel Albert",
                "Anjul Patney",
                "David Luebke",
                "Joohwan Kim"
            ],
            "title": "Latency requirements for foveated rendering in virtual reality",
            "venue": "ACM Transactions on Applied Perception (TAP),",
            "year": 2017
        },
        {
            "authors": [
                "Gary J Sullivan",
                "Jens-Rainer Ohm",
                "Woo-Jin Han",
                "Thomas Wiegand"
            ],
            "title": "Overview of the high efficiency video coding (HEVC) standard",
            "venue": "IEEE Transactions on circuits and systems for video technology,",
            "year": 2012
        },
        {
            "authors": [
                "Peter De Rivaz",
                "Jack Haughton"
            ],
            "title": "Av1 bitstream & decoding process specification",
            "venue": "The Alliance for Open Media,",
            "year": 2018
        },
        {
            "authors": [
                "Mehmet N Akcay",
                "Burak Kara",
                "Saba Ahsan",
                "Ali C Begen",
                "Igor Curcio",
                "Emre Aksu"
            ],
            "title": "Head-motion-aware viewport margins for improving user experience in immersive video",
            "venue": "In ACM Multimedia Asia,",
            "year": 2021
        },
        {
            "authors": [
                "Mathias Wien",
                "Heiko Schwarz",
                "Tobias Oelbaum"
            ],
            "title": "Performance analysis of SVC",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology,",
            "year": 2007
        },
        {
            "authors": [
                "Benjamin Bross",
                "Ye-Kui Wang",
                "Yan Ye",
                "Shan Liu",
                "Jianle Chen",
                "Gary J Sullivan",
                "Jens-Rainer Ohm"
            ],
            "title": "Overview of the versatile video coding (VVC) standard and its applications",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "An Optimal SVC Bitstream Schema for Viewport-dependent 360-degree Video Streaming\nGang Shen NEX Intel\nHillsboro, US gang.shen@intel.com\nMingyang Ma School of Software Engineering\nUniversity of Science and Technology of China Hefei, China\nmamingyang@mail.utstc.edu.cn\nGuangxin Xu DCAI Intel\nShanghai, China guangxin.xu@intel.com\nAbstract\u2014Abstracta\u0328\u0142 To deliver ultra-high resolution 360- degree video (such as 8K, 12K, or even higher) across the internet, viewport-dependent streaming becomes necessary to save bandwidth. During viewport switches, clients and servers will instantly exchange coordination info and contents for the given viewports. However, those viewport switches pose a serious challenge for video encoding because the temporal dependency between contents within changing viewports is unpredictable. In existing practices, it is commonly noted that GOP (Group of Pictures) size in a bitstream intrinsically prohibits the reduction of the viewport switch latency, such as Motion-to-photon (MTP) latency, or motion-to-high-quality (MTHQ) latency. In this paper, we presented a Scalable Video Coding (SVC) based bitstream schema, which can structurally remove the impacts of GOP in viewport-dependent streaming and provide instant viewport switches within one-frame time (the best possible). In addition, combined with tiling, this new coding schema allows an efficient packing of the non-adjacent regions within a viewport of 360-degree video. Our experiments also show that the overall encoding with this SVC-based approach is faster than with multistream approaches. Compared with current 360-degree video streaming solutions based on MPEG-I OMAF, our approach is superior in terms of viewport switch latency, simplicity of viewport packing, and encoding performance.\nIndex Terms\u2014360-degree Video, OMAF, Scalable Video Coding (SVC), Motion-to-photon (MTP), Virtual Reality (VR), AV1"
        },
        {
            "heading": "I. BACKGROUND",
            "text": "Ultra-high resolution (like 8K, 12K, and even higher) 360- degree video contents are becoming desirable in the market. Meanwhile, the bandwidth to support such high-resolution content becomes a challenge for the network, even with modern codecs like HEVC or AV1. It is estimated that it may take 80-100Mbps to support one 8K 60FPS stream [1] if encoded in HEVC with medium-quality settings.\nWhile most of the content in a spherical surface is not visible to the client device during consumption, viewportdependent streaming, in which only content within the client\u2019s field of view (FOV) is delivered, intuitively becomes a viable approach (and the only viable approach) for bandwidth-saving and large-scale distributions. In MPEG-I OMAF (Omnidirectional MediA Format), two profiles, advanced video coding (AVC)-based and high-efficiency video coding (HEVC)-based viewport-dependent video profile, as well as three corresponding tile-based streaming approaches, are specified [2], [3]. To\nfacilitate viewport-dependent streaming, OMAF specifically uses HEVC MCTS (Motion Constraint Tile Set) in one of the approaches to separate 360-degree video frames into independently decodable tiles, so that contents in any viewports can be packaged, combined, and delivered. Several commercial implementations can be found on different platforms including Android, iOS, Windows, and the Web [4]\u2013[7]. Besides, VR video designers will benefit from an OMAF creator system built on the Nokia cloud stream processing platform where end users can upload, edit and preview 6K 360-degree video and watch it with a video link [8]. Although the latest endto-end system based on OMAF v2 can process and display 8K 60 fps VR video [9], there are challenges like heavy computations, complex file packaging, and viewport switch latency. This paper introduces an optimal bitstream schema for 360-degree video, based on SVC, to cope with current challenges in OMAF. Particularly, it offers an unparallel answer to the interactive latency during the viewport switch, which is critical and common for viewport-dependent streaming. In the following, section II illustrates challenges in 360-degree video format and streaming; section III gives the design of this SVC-based new coding schema; section IV provides a reference implementation by AV1 SVC; section V shows encoding experiments and performance analysis; section VI gives conclusions."
        },
        {
            "heading": "II. THE CHALLENGES IN VIEWPORT-DEPENDENT 360-DEGREE VIDEO STREAMING",
            "text": "A. Viewport Change and MTP/MTHQ Latency\nThe success of viewport-dependent streaming relies on the efficiency of the exchange between viewport information and partial content to cover that viewport. While this exchange (shown in Fig. 1) saves significant bandwidth, it introduces an interactive latency between server and client, which can tamper the visual quality during viewport changes. In addition, to support large-scale distribution, the server in Fig. 1 cannot feasibly rely on real-time transcoding to prepare streams for individual and dynamic viewports of many clients. It is necessary to have an optimal content (bitstream) schema which can support ultra-low latency for exchange and scalability for distribution.\nar X\niv :2\n30 4.\n05 65\n4v 1\n[ cs\n.M M\n] 1\n2 A\npr 2\n02 3\nThe exchange/interactive latency is defined in MPEG-I as MTP (Motion-to-Photon) latency when the single quality of content is used, or MTHQ (Motion-to-High-Quality) latency in the case of two levels of resolutions/qualities of contents are used. Human eyes are sensitive to detect visual quality gaps - according to [10], this latency is required to be as little as 50ms to give users completely immersive experiences, which VR or 360-degree video products in today\u2019s market can hardly achieve.\nThere are known methods to reduce this latency in the industry, based on either video encoding, packing, or network transport. The popular approach as for now, in MPEG-I OMAF, provides a combined approach of multi-bitrate or multi-resolution packing, region-wise packing (RWP), bitstream rewriting, and tile-based coding to offer a bandwidthsaving solution [2]. However, it can only mitigate this interactive latency by adding additional encoding tracks of long and short Group of Pictures (GOP) sizes - i.e., during the viewport change, the content will be selected and transported from short GOP tracks. But this approach will increase the transcoding workloads and bandwidth for transporting short GOP track.\nWhen a viewport change happens in live VR/360 video viewport-dependent streaming, the access to the content changes both spatially - from one spot to another spot, and temporally - from this frame to the next frame. The distribution server (Fig. 2) - usually at the edge network, which will receive full content from the source, and need to select partial contents, according to the FOV (sent by the client), within the bitstream(s). However, the selection of partial contents can occur meaningfully with IDR frames - the first frame of GOP. The longer GOP structure, the longer delay to make the viewport change; Meanwhile, the shorter GOP, the higher bitrate. Therefore, the GOP size is a difficult factor to decide, in commercial implementations of viewport-dependent 360 video live streaming."
        },
        {
            "heading": "B. Projections and Packing of Viewport Contents",
            "text": "Meanwhile, the 360-degree video needs specific projections and packing methods to be encoded in 2D rectangular frames for encoding and network transport. Due to the projections from a spherical space to a planar space, the contents for a given FOV are not always adjacent in 2D frames. This will cause difficulties and complexity for collecting, arranging, and packing contents of FOV into one rectangular, which is the usual requirement of the interfaces of either encoders or decoders.\nFor example, in equirectangular projection (shown in Fig. 3), a field of view could be composed of nonadjacent regions (marked by orange ovals) by the left and right edges.\nCube map projection has the same issue. As shown in Fig. 4, suppose (1) the given viewport is coming top-right-back corner; (2) the content sphere is projected to a cube; (3) the six surfaces of the cube are packed into a 2D rectangular based on OMAF specification [3], the contents for this viewport are distributed in nonadjacent regions marked by orange ovals.\nThose nonadjacent regions of one given viewport are challenges for region-wise packing (RWP), tile-based encoding on the server side, as well as late-binding for decoders in client devices. Even with tile-based video encoding (like HEVC), it still needs a delicate bitstream rewriting algorithm, specified in MPEG-I OMAF v2.0( [3], 4.2.3), to pack those nonadjacent regions into a 2D rectangular or a conformant bitstream(s) for single or multiple decoders on the client devices. Packing multiple regions into one rectangular is geometrically difficult, and costly when using paddings."
        },
        {
            "heading": "III. AN OPTIMAL SVC-BASED BITSTREAM SCHEMA",
            "text": "Based on discussions in section II, the viewport-dependent 360-degree video streaming faces challenges from the temporal domain of video coding - GOP size\u2019s impact to interactive latency, as well as from the spatial domain of video coding - nonadjacent regions of a given FOV. It suggests that an advanced coding schema may be needed for 360-degree video.\nIn fact, conventional video coding also faces challenges to adapt to various demands of the user ends and enhance encoding performance, where the SVC and tiles are developed respectively. The main concept of SVC is to provide partially removable video streaming by separating it into a base layer and enhanced layers. Tiles divide a video frame into independent parts and a tile is a rectangle of superblock whose spatial referencing is limited to be within the tile boundary. Different tiles within a frame can be encoded separately which facilitates multi-threading capabilities.\nTherefore, we come up with an optimal video coding schema (Fig. 5) based on SVC and Tiles, which will overcome the difficulties of conventional coding used in MPEG-I OMAF.\nThere are two key elements in this new coding schema: (1) to use enhance layer in SVC for high-quality contents while using base layer for low-quality contents; (2) to use tiles (no coding dependency between tiles) segment one frame into multiple regions.\nIn MPEG-I OMAF, AVC-based and HEVC-based viewportdependent OMAF video profiles combine high-quality contents of FOV with full view of low-quality contents using multiple streams (Fig. 6). The low-quality contents will be the backup when high-quality contents cannot cover the FOV when rendering on client devices. Comparably, the base layer of SVC can be the backup and the tiles of enhanced layer can compose the contents (even nonadjacent regions) for any given FOVs.\nFurthermore, the enhanced layer can have only spatial dependency on base layer. This implies the only temporal dependency in this structure is on base layer. Since all frames in base layer will be transported to client, the high-quality tiles in enhanced layer can be accessed freely without the constraint of GOP.\nThe detailed design of the coding schema is listed as following:\n1) It has two layers: low-resolution (or low-quality) as base layer and high-resolution (or high-quality) as enhanced layer. It is possible to extend to multiple enhanced layers (for example, for Simulcast). Also, the base layer can have temporal dependency - i.e., it can have I-frame, Pframe and GOP (Group of Pictures), so that the video compression benefits can be achieved. The GOP in this proposal refers to \u201cClose GOP\u201d. 2) Frames in enhanced layer will have spatial dependency on based layer only. In the Fig. 5, the dependency between enhanced layer and base layer is on the same frame, but it is viable for frames in enhanced layer have this dependency on a few previous frames in base layer: for example, the frame 3 in enhanced layer may have dependencies in frame 1 or 2 in base layer. The key is, keep temporal dependency between frames out of enhanced layer. 3) Frames in the base layer will have a temporal dependency on the multiple previous based layer frames. For example, frame 3 in the base layer may have dependencies on frame 1 or 2 in the base layer. 4) The base layer may only have a single tile/slice to save bitrate. 5) When streaming to a client device, only content (grey tiles) within the given FOV in enhanced layer and full tiles in base layer will be delivered. The location of each tile will be carried in manifest message in bitstream (like SEI message [11] in HEVC or OBU header in AV1) of each frame.\nDuring viewport changes on 360-degree video (or panorama video) streaming, this coding structure can offer immediate and random access to any tiles in the enhanced layer. The nonadjacent regions of a viewport can be overcome if bitstream can accommodate the position information of each tile, then no need for an additional packing method like that defined in MPEG-I OMAF.\nThe grey tiles in Fig. 5 indicate contents/tiles for a given FOV, respectively at time 0, 1, 2, 3. Certainly, FOV may not change so drastically from frame to frame. This is for illustration of the viability and flexibility of this bitstream schema:\n1) From frame 0 to frame 1, new FOV needs to decode new T6, T8, T9 tiles (T5 exists in Frame 0; T5 is decodable). Since those tiles depend on base layer only, those can be decoded. 2) Frame 2 shows that the content for a new FOV includes nonadjacent tiles: T4, T6, T7, T9. It is still feasible if SEI message (or header information) can provide the position info of each tile. 3) Frame 3 shows that it allows to have different quantity of tiles in this structure. It is useful for some formats (e.g., ERP) of 360-degree video where t the projection may make the uneven distribution of pixels for viewports.\nSo, by weaving the contents into tiles and layers, this coding schema removes both the constraints of GOP structure and the need of region packing. Furthermore, tiles and layers are built-in concepts or tools in modern codecs like AV1 and VVC. Based on this coding schema, the end-to-end implementations of 360-degree video streaming solutions can be greatly simplified."
        },
        {
            "heading": "IV. REFERENCE IMPLEMENTATION USING AV1 SVC",
            "text": "This structure is implementable by modern codec standards. For example, in AV1 SVC, the structure can be built upon L2T1 [12], with additional tile specifications.\nFig. 6 is L2T1 scalability structure in AV1 SVC. To support the structure in this proposal, there are a few extensions to make:\n1) Remove the temporal dependency between frames in enhanced layer, marked by a\u0328r\u030cXa\u0328s\u0301 in the Fig. 6. 2) The base layer can be encoded as a single tile to save bitrate. The enhanced layer needs to be encoded as multiple tiles to enable a quick viewpoint switch. 3) Use \u201cTile Group OBU\u201dor \u201cTile List OBU\u201dto select and pack tiles for a given FOV. Based on the current AV1 SVC Spec, \u201cTile Group OBU\u201dis more reasonable for this case. 4) Each tile should be packed or delimited independently - it will help independent handling of high-resolution tiles in the transport layer. 5) Enhanced layer can refer to one frame in the base layer like Fig. 6(a) or multiple previous frames in the base layer like Fig. 6(b).\nHere, a reference implementation to show how to use \u201cTiled Group OBU\u201dto support the viewport switch and deliver content for FOV is shown in Fig. 7:\n1) The encoder will encode a high-quality layer for the entire picture, with tiling to split full content into small regions. According to AV1 SVC, multiple Tile Group OBUs can be applied on those tiles and in this case, each Tile Group OBU will cover one and only one tile - the \u201cT#\u201din Fig. 7 refers to one tile. The base layer has the same tiling on lower resolution or lower quality - it does not affect the algorithm if the base layer has a single tile. The output of the encoder is illustrated as (1) in Fig. 7. 2) The server will receive full content as the output of encoder (1); Meanwhile, the server will also receive viewport information (FOV info) from clients - only one client is shown in Fig. 7. 3) The server will apply the tile selection and repackaging (bitstream rewriting) process on the full content frames (1), according to the viewport info from a given client. The tile selection will be a mapping from the viewport parameter (Fig. 8) to a list of tiles within the viewer\u2019s FOV - For example, T1, T2, T4, and T5 in (2) in Fig. 7. Then, the server will extract those tiles (based on the syntax of AV1 SVC) and repackage them into a new \u201cframe\u201dwith only tiles within FOV in the enhanced layer and all tiles in the base layer.\na) Since every tile is inside one Tile Group OBU and the Tile Group OBU has no other tiles, the tile\u2019s position is identified by\u201ctg_start\u201dand \u201ctg_end\u201dvalues in OBU. b) After the repackaging of selected tiles, a temporal delimiter OBU should be applied which will identify the boundary between frames. c) The output of this step - the content for FOV, is illustrated by (2) in Fig. 7. The tiles outside FOV are skipped.\n4) Once the client receives all high-resolution tile groups (greyed tiles), it can generate unselected and unreceived tiles as \u201cskipped\u201dtiles (tiles in blue color, in (3) of Fig. 7), from the base layer, as long as:\na) At frame-level, CDF (Cumulative distribution function) update is set to being disabled and global MV (Motion Vector) is set to zero. b) At the tile level, the transform skip param is set to true, and \u201cuse global MV\u201dis set to true.\n5) According to AV1 SVC specification, those generated \u201cskipped\u201dtiles are decodable. After decoding, the regions of skipped tiles in the enhanced layer will be filled by taking and scaling up the counterparts from the base layer. There is no change needed on the decoder.\nThen, a bitstream rewriter can be implemented to select and construct a \u201cviewport-dependent frame\u201d(shown as (2) in Fig. 7) accordingly. It will rewrite each target tile in each enhanced layer. Each superblock in the target tile will be rewritten to the same structure. According to the AV1 specification, the syntax of superblock should be rewritten as followings:\n1) The partition syntax is written as None. 2) The skip syntax is written as True for no transform\ncoefficients.\n3) is_inter syntax is True and it only refers to the base layer. 4) The motion vector is set as zero. 5) As required by the specification, use_obmc syntax is set\nto False to use the simple translations. Here are pseudo codes for the bitstream rewriter, to create\na \u201cviewport-dependent frame\u201d(2) in Fig. 7:\nAlgorithm 1: Bitstream Rewriter 1 foreach enhance layer do 2 foreach target tile do 3 foreach superblock do 4 rewrite_sb(superblock)\nAlgorithm 2: Rewrite_sb 1 write_partion_mode(PARTITION_NONE) 2 write_skip(True) 3 write_is_inter(True) 4 write_ref_frames(REF_TO_BASE_LAYER_ONLY) 5 write_inter_mode(zero_mv) 6 write_use_obmc(False)\nFig. 9 shows that how GOP size affects MTP/MTHQ latency. The entire viewport switch process includes several different factors: such as pose detection, network transport, buffering, decoding and rendering. Among those, GOP size is a structural factor, which forbid immediate viewportdependent content selection on non-I-frames. Without layered bitstream structure like that in the paper, server will have to wait for next I-frame to respond viewport change and it will cost the time about half of GOP size on average. For example, if GOP size is 10 and FPS (frames per second) is 30, latency caused by GOP size will be 167ms. Although this latency\ncan be reduced by a separate track with short GOP size (e.g., using GOP size 3 or 5), such track will increase drastically the bitrate and encoding complexity."
        },
        {
            "heading": "V. EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "A. Environment Setup",
            "text": "We extended the AV1 codec of Alliance for Open Media (AOM) to implement this schema and made comparisons with existing approaches for 360-degree videos. While the new coding schema can achieve instant viewport change and much more advanced than existing approaches (as shown in section 3.1), we would like to further evaluate this new coding schema in two aspects: (1) quality with the same bitrate; (2) encoding time/speed.\nAn Intel Xeon E5-8280 CPU and 192GB memories are used to handle 8K and 4K video encoding. Three 30fps 8K and 4K 360-degree video clips (in Table I) are used in experiments; ERP screenshots are presented in Fig. 12. The encoding parameters are set as the following: cpu-used=7, passes=1, threads=20, end-usage=CBR and real-time mode. Each case will be tested three times and the average of all the experimental results are used."
        },
        {
            "heading": "B. Bitrate and Quality",
            "text": "In industry, the 360-degree video solution usually includes two or three streams, e.g., one high resolution track with long GOP size for long time view, one low resolution track with long GOP size as background and one high resolution with short GOP size for viewport switch. To make the evaluation meaningful, we compare PSNR of two methods in selected\nvideo clips. For existing solution, the PSNR is the average of two high resolution tracks and the scalable ratio is 2.0x, which means the low quality video is half the width and height of the original video.\nIn Fig. 10 & 11, method A and B are common three-track methods stated as above, and the long GOP size is 30 and the short GOP size is 5 and 3, respectively. The new SVC-based coding schema includes two structures in Fig. 6: enhancement layer refers only one frame and multiple frames of base layer (denoted as \u201cSVC\u201dand \u201cSVC refs\u201drespectively in Fig. 10 & 11).\nFig. 10 shows that SVC-based and three-track method can achieve a similar quality per bitrate. Our method is limited better or worse than the other solution in specific scene or video content. Besides, it is worthy to note that PSNR value can be improved by about 0.5-1dB because of the SVC structure in Fig. 6(b).\nOn the other hand, the transport bitrate is also one of the factors we should consider. According to the previous discussion, our scheme can reduce the latency to one-frame time. If the existing method that wants to reduce the latency to one frame as well, it needs to transport the full picture or at least provide enough margin depending on the specified scene like [17]. Here we do not discuss the transmission strategy for specific video scenes, because our approach requires the only about 1/6 of enhanced layers and simplify the adjustment of the strategy accordingly, which means considerable savings in bitstream."
        },
        {
            "heading": "C. Encoding Time",
            "text": "We consider the encoding time of each method in different encoding bitrates. The result is recorded from the time command in Linux system and the result equals to the sum of user time and system time.\nThe performance between SVC-based and existing threetrack method is shown in Fig. 11. According to the results, the actual encoding time of new SVC-based coding schema is significantly less (>30%) than three-track method in the same bitrate, which implies using SVC can reduce a considerable encoding cost in multi-rate (or multi-resolution) use cases."
        },
        {
            "heading": "VI. CONCLUSIONS",
            "text": "The viewport-dependent streaming becomes necessary for 360-degree videos (and VR content). But the temporal dependency used in modern video codecs makes viewport switch latency correlated to the GOP size - difficult to achieve instant viewport change which is critical for user experiences. In this paper, an SVC-based and tiled-based video coding structure is proposed to resolve the viewport switch challenge and it can reduce interactive latency to one frame time - the best possible time. Secondly, by using tiles in SVC layers, it circumvents the issue of packing the non-adjacent regions, which is a common geometry challenge in non-planar contents, such as 360-degree videos or VR. Furthermore, compared with existing industry approaches, this new coding schema can save encoding cost significantly, while achieving a similar PSNR result in same bitrate.\nOn a side note, the encoding performance of AV1 spatial SVC may be an interesting topic for further study. In our experiments, 2x spatial scale factor is used between the base layer and enhanced layer, because SVC usually needs a smaller scale factor for better quality [18]. Depending on video contents, AV1 SVC encoding may present different quality metrics compared with conventional single layer encoding, but the gap (PSNR) is acceptable in our case - even in the worst case, PSNR gap is less than 1dB (typically 0.5dB).\nIn the end, the new coding schema can be implemented with AV1 SVC [12], without changes in the existing codec standards - i.e., no changes in AV1 decoder. Similar approach can be also implemented in MPEG-I VVC, with subpicture and reference picture resampling (RPR) [19]. In addition, the 360-degree video delivery architecture specified in MPEGI OMAF [3] - such as applications, encoding codecs, and packing formats, can be greatly simplified by using this SVCbased coding schema. This work paves the way for the most efficient viewport-dependent streaming of 360-degree videos, in terms of interactive latency, coding performance, and architecture."
        }
    ],
    "title": "An Optimal SVC Bitstream Schema for Viewport-dependent 360-degree Video Streaming",
    "year": 2023
}