{
    "abstractText": "Natural language processing (NLP) has made significant progress for well-resourced languages such as English but lagged behind for lowresource languages like Setswana. This paper addresses this gap by presenting PuoBERTa, a customised masked language model trained specifically for Setswana. We cover how we collected, curated, and prepared diverse monolingual texts to generate a high-quality corpus for PuoBERTa\u2019s training. Building upon previous efforts in creating monolingual resources for Setswana, we evaluated PuoBERTa across several NLP tasks, including part-of-speech (POS) tagging, named entity recognition (NER), and news categorisation. Additionally, we introduced a new Setswana news categorisation dataset and provided the initial benchmarks using PuoBERTa. Our work demonstrates the efficacy of PuoBERTa in fostering NLP capabilities for understudied languages like Setswana and paves the way for future research directions.",
    "authors": [
        {
            "affiliations": [],
            "name": "Vukosi Marivate"
        },
        {
            "affiliations": [],
            "name": "Moseli Mots\u2019Oehli"
        }
    ],
    "id": "SP:081524c7af080076f6d31ed3f160b942c39066f0",
    "references": [
        {
            "authors": [
                "I. Adebara",
                "A. Elmadany",
                "M. Abdul-Mageed",
                "A. Alcoba Inciarte"
            ],
            "title": "SERENGETI: Massively multilingual language models for Africa",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023. pp. 1498\u20131537. Association for Computational Linguistics, Toronto, Canada",
            "year": 2023
        },
        {
            "authors": [
                "D. Adelani",
                "J. Alabi",
                "A. Fan",
                "J. Kreutzer",
                "X. Shen",
                "M. Reid",
                "D. Ruiter",
                "D. Klakow",
                "P. Nabende",
                "E. Chang",
                "T. Gwadabe",
                "F. Sackey",
                "B.F.P. Dossou",
                "C. Emezue",
                "C. Leong",
                "M. Beukman",
                "S. Muhammad",
                "G. Jarso",
                "O. Yousuf",
                "A. Niyongabo Rubungo",
                "G. Hacheme",
                "E.P. Wairagala",
                "M.U. Nasir",
                "B. Ajibade",
                "T. Ajayi",
                "Y. Gitau",
                "J. Abbott",
                "M. Ahmed",
                "M. Ochieng",
                "A. Aremu",
                "P. Ogayo",
                "J. Mukiibi",
                "F. Ouoba Kabore",
                "G. Kalipe",
                "D. Mbaye",
                "A.A. Tapo",
                "V. Memdjokam Koagne",
                "E. Munkoh-Buabeng",
                "V. Wagner",
                "I. Abdulmumin",
                "A. Awokoya",
                "H. Buzaaba",
                "B. Sibanda",
                "A. Bukula",
                "S. Manthalu"
            ],
            "title": "A few thousand translations go a long way! leveraging pre-trained models for African news translation",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 3053\u20133070. Association for Computational Linguistics, Seattle, United States",
            "year": 2022
        },
        {
            "authors": [
                "D. Adelani",
                "G. Neubig",
                "S. Ruder",
                "S. Rijhwani",
                "M. Beukman",
                "C. Palen-Michel",
                "C. Lignos",
                "J. Alabi",
                "S. Muhammad",
                "P Nabende"
            ],
            "title": "Masakhaner 2.0: Africacentric transfer learning for named entity recognition. In: Proceedings of the 2022 PuoBERTa: A curated language model for Setswana",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022
        },
        {
            "authors": [
                "\u017d. Agi\u0107",
                "I. Vuli\u0107"
            ],
            "title": "JW300: A wide-coverage parallel corpus for low-resource languages",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 3204\u20133210. Association for Computational Linguistics, Florence, Italy",
            "year": 2019
        },
        {
            "authors": [
                "J.O. Alabi",
                "D.I. Adelani",
                "M. Mosbach",
                "D. Klakow"
            ],
            "title": "Adapting pre-trained language models to African languages via multilingual adaptive fine-tuning",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics. pp. 4336\u20134349. International Committee on Computational Linguistics, Gyeongju, Republic of Korea",
            "year": 2022
        },
        {
            "authors": [
                "J. Armengol-Estap\u00e9",
                "C.P. Carrino",
                "C. Rodriguez-Penagos",
                "O. de Gibert Bonet",
                "C. Armentano-Oller",
                "A. Gonzalez-Agirre",
                "M. Melero",
                "M. Villegas"
            ],
            "title": "Are multilingual models the best choice for moderately under-resourced languages? A comprehensive assessment for Catalan",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. pp. 4933\u20134946. Association for Computational Linguistics, Online",
            "year": 2021
        },
        {
            "authors": [
                "M. Aulamo",
                "U. Sulubacak",
                "S. Virpioja",
                "J. Tiedemann"
            ],
            "title": "OpusTools and parallel corpus diagnostics",
            "venue": "Proceedings of The 12th Language Resources and Evaluation Conference. pp. 3782\u20133789. European Language Resources Association",
            "year": 2020
        },
        {
            "authors": [
                "C. Baziotis",
                "B. Zhang",
                "A. Birch",
                "B. Haddow"
            ],
            "title": "When does monolingual data help multilingual translation: The role of domain and model scale",
            "venue": "arXiv preprint arXiv: 2305.14124",
            "year": 2023
        },
        {
            "authors": [
                "F. Burlot",
                "F. Yvon"
            ],
            "title": "Using monolingual data in neural machine translation: a systematic study",
            "venue": "Proceedings of the Third Conference on Machine Translation: Research Papers. pp. 144\u2013155. Association for Computational Linguistics, Brussels, Belgium",
            "year": 2018
        },
        {
            "authors": [
                "L.C. Collection"
            ],
            "title": "Tswana web text corpus (South Africa) based on material from 2019",
            "venue": "https://corpora.uni-leipzig.de?corpusId=tsn-za web 2019,",
            "year": 2019
        },
        {
            "authors": [
                "C.M.B. Dione",
                "D.I. Adelani",
                "P. Nabende",
                "J. Alabi",
                "T. Sindane",
                "H. Buzaaba",
                "S.H. Muhammad",
                "C.C. Emezue",
                "P. Ogayo",
                "A. Aremu",
                "C. Gitau",
                "D. Mbaye",
                "J. Mukiibi",
                "B. Sibanda",
                "B.F.P. Dossou",
                "A. Bukula",
                "R. Mabuya",
                "A.A. Tapo",
                "E. Munkoh-Buabeng",
                "V. Memdjokam Koagne",
                "F. Ouoba Kabore",
                "A. Taylor",
                "G. Kalipe",
                "T. Macucwa",
                "V. Marivate",
                "T. Gwadabe",
                "M.T. Elvis",
                "I. Onyenwe",
                "G. Atindogbe",
                "T. Adelani",
                "I. Akinade",
                "O. Samuel",
                "M. Nahimana",
                "T. Musabeyezu",
                "E. Niyomutabazi",
                "E. Chimhenga",
                "K. Gotosa",
                "P. Mizha",
                "A. Agbolo",
                "S. Traore",
                "C. Uchechukwu",
                "A. Yusuf",
                "M. Abdullahi",
                "D. Klakow"
            ],
            "title": "MasakhaPOS: Partof-speech tagging for typologically diverse African languages",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 10883\u201310900. Association for Computational Linguistics, Toronto, Canada",
            "year": 2023
        },
        {
            "authors": [
                "S. Doddapaneni",
                "R. Aralikatte",
                "G. Ramesh",
                "S. Goyal",
                "M.M. Khapra",
                "A. Kunchukuttan",
                "P. Kumar"
            ],
            "title": "Towards leaving no indic language behind: Building monolingual corpora, benchmark and models for indic languages",
            "venue": "Annual Meeting Of The Association For Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "B.F. Dossou",
                "A.L. Tonja",
                "O. Yousuf",
                "S. Osei",
                "A. Oppong",
                "I. Shode",
                "O.O. Awoyomi",
                "C.C. Emezue"
            ],
            "title": "Afrolm: A self-active learning-based multilingual pretrained language model for 23 african languages",
            "venue": "SustaiNLP 2022 p. 52",
            "year": 2022
        },
        {
            "authors": [
                "R. Eiselen"
            ],
            "title": "Nchlt Setswana roberta language model (2023)",
            "year": 2023
        },
        {
            "authors": [
                "R. Eiselen",
                "M.J. Puttkammer"
            ],
            "title": "Developing text resources for ten South African languages",
            "venue": "LREC. pp. 3698\u20133703",
            "year": 2014
        },
        {
            "authors": [
                "A. Fan",
                "S. Bhosale",
                "H. Schwenk",
                "Z. Ma",
                "A. El-Kishky",
                "S. Goyal",
                "M. Baines",
                "O. Celebi",
                "G. Wenzek",
                "V. Chaudhary",
                "N. Goyal",
                "T. Birch",
                "V. Liptchinsky",
                "S. Edunov",
                "E. Grave",
                "M. Auli",
                "A. Joulin"
            ],
            "title": "Beyond english-centric multilingual machine translation",
            "year": 2020
        },
        {
            "authors": [
                "D. Goldhahn",
                "T. Eckart",
                "U. Quasthoff"
            ],
            "title": "Building large monolingual dictionaries at the Leipzig corpora collection: From 100 to 200 languages",
            "venue": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC\u201912). pp. 759\u2013765. European Language Resources Association (ELRA), Istanbul, Turkey",
            "year": 2012
        },
        {
            "authors": [
                "F. Gyasi",
                "T. Schlippe"
            ],
            "title": "Twi machine translation",
            "venue": "Big Data and Cognitive Computing 7(2), 114",
            "year": 2023
        },
        {
            "authors": [
                "B. Haddow",
                "R. Bawden",
                "A.V. Miceli Barone",
                "J. Helcl",
                "A. Birch"
            ],
            "title": "Survey of low-resource machine translation",
            "venue": "Computational Linguistics 48(3), 673\u2013732",
            "year": 2022
        },
        {
            "authors": [
                "D. Ifeoluwa Adelani",
                "M. Masiak",
                "I. Abebe Azime",
                "J. Oluwadara Alabi",
                "A. Lambebo Tonja",
                "C. Mwase",
                "O. Ogundepo",
                "B.F. Dossou",
                "A. Oladipo",
                "D Nixdorf"
            ],
            "title": "Masakhanews: News topic classification for african languages",
            "venue": "arXiv e-prints pp. arXiv\u20132304",
            "year": 2023
        },
        {
            "authors": [
                "R. Lastrucci",
                "I. Dzingirai",
                "J. Rajab",
                "A. Madodonga",
                "M. Shingange",
                "D. Njini",
                "V. Marivate"
            ],
            "title": "Preparing the vuk\u2019uzenzele and za-gov-multilingual south african multilingual corpora",
            "venue": "Fourth workshop on Resources for African Indigenous Languages (RAIL). p. 18",
            "year": 2023
        },
        {
            "authors": [
                "T. Limisiewicz",
                "D. Malkin",
                "G. Stanovsky"
            ],
            "title": "You can have your data and balance it too: Towards balanced and efficient multilingual models",
            "venue": "Proceedings of the 5th Workshop on Research in Computational Linguistic Typology and Multilingual NLP. pp. 1\u201311. Association for Computational Linguistics, Dubrovnik, Croatia",
            "year": 2023
        },
        {
            "authors": [
                "G. Litre",
                "F. Hirsch",
                "P. Caron",
                "A. Andrason",
                "N. Bonnardel",
                "V. Fointiat",
                "W.O. Nekoto",
                "J. Abbott",
                "C. Dobre",
                "J. Dalboni",
                "A. Steuckardt",
                "G. Luxardo",
                "H. Bohbot"
            ],
            "title": "Participatory detection of language barriers towards multilingual sustainability(ies) in africa",
            "venue": "Sustainability 14(13), 8133",
            "year": 2022
        },
        {
            "authors": [
                "Y. Liu",
                "J. Gu",
                "N. Goyal",
                "X. Li",
                "S. Edunov",
                "M. Ghazvininejad",
                "M. Lewis",
                "L. Zettlemoyer"
            ],
            "title": "Multilingual denoising pre-training for neural machine translation",
            "venue": "arXiv preprint arXiv: 2001.08210",
            "year": 2020
        },
        {
            "authors": [
                "L. Van der Maaten",
                "G. Hinton"
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of machine learning research 9(11)",
            "year": 2008
        },
        {
            "authors": [
                "V. Marivate",
                "M. Mots\u2019Oehli",
                "V. Wagner",
                "R. Lastrucci",
                "I. Dzingirai"
            ],
            "title": "Puoberta + puoberta Setswana language models (Oct 2023)",
            "year": 2023
        },
        {
            "authors": [
                "V. Marivate",
                "T. Sefara",
                "V. Chabalala",
                "K. Makhaya",
                "T. Mokgonyane",
                "R. Mokoena",
                "A. Modupe"
            ],
            "title": "Investigating an approach for low resource language dataset creation, curation and classification: Setswana and Sepedi",
            "venue": "Proceedings of the first PuoBERTa: A curated language model for Setswana 13 workshop on Resources for African Indigenous Languages. pp. 15\u201320. European Language Resources Association (ELRA), Marseille, France",
            "year": 2020
        },
        {
            "authors": [
                "F. Meyer",
                "J. Buys"
            ],
            "title": "Subword segmental language modelling for nguni languages",
            "venue": "Conference On Empirical Methods In Natural Language Processing",
            "year": 2022
        },
        {
            "authors": [
                "W. Nekoto",
                "V. Marivate",
                "T. Matsila",
                "T. Fasubaa",
                "T. Fagbohungbe",
                "S.O. Akinola",
                "S. Muhammad",
                "S. Kabongo Kabenamualu",
                "S. Osei",
                "F. Sackey",
                "R.A. Niyongabo",
                "R. Macharm",
                "P. Ogayo",
                "O. Ahia",
                "M.M. Berhe",
                "M. Adeyemi",
                "M. MokgesiSelinga",
                "L. Okegbemi",
                "L. Martinus",
                "K. Tajudeen",
                "K. Degila",
                "K. Ogueji",
                "K. Siminyu",
                "J. Kreutzer",
                "J. Webster",
                "J.T. Ali",
                "J. Abbott",
                "I. Orife",
                "I. Ezeani",
                "I.A. Dangana",
                "H. Kamper",
                "H. Elsahar",
                "G. Duru",
                "G. Kioko",
                "M. Espoir",
                "E. van Biljon",
                "D. Whitenack",
                "C. Onyefuluchi",
                "C.C. Emezue",
                "B.F.P. Dossou",
                "B. Sibanda",
                "B. Bassey",
                "A. Olabiyi",
                "A. Ramkilowan",
                "A. \u00d6ktem",
                "A. Akinfaderin",
                "A. Bashir"
            ],
            "title": "Participatory research for low-resourced machine translation: A case study in African languages",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020. pp. 2144\u20132160. Association for Computational Linguistics, Online",
            "year": 2020
        },
        {
            "authors": [
                "K. Ogueji",
                "Y. Zhu",
                "J. Lin"
            ],
            "title": "Small data? no problem! exploring the viability of pretrained multilingual language models for low-resourced languages",
            "venue": "Proceedings of the 1st Workshop on Multilingual Representation Learning. pp. 116\u2013126. Association for Computational Linguistics, Punta Cana, Dominican Republic",
            "year": 2021
        },
        {
            "authors": [
                "E.B. Palai",
                "L. O\u2019Hanlon"
            ],
            "title": "Word and phoneme frequency of occurrence in conversational Setswana: a clinical linguistic application",
            "venue": "Southern African Linguistics and Applied Language Studies 22(3-4), 125\u2013142",
            "year": 2004
        },
        {
            "authors": [
                "A. Ragni",
                "K.M. Knill",
                "S.P. Rath",
                "M.J. Gales"
            ],
            "title": "Data augmentation for low resource languages",
            "venue": "INTERSPEECH 2014: 15th Annual Conference of the International Speech Communication Association. pp. 810\u2013814. International Speech Communication Association (ISCA)",
            "year": 2014
        },
        {
            "authors": [
                "S. Ranathunga",
                "E.S.A. Lee",
                "M. Prifti Skenduli",
                "R. Shekhar",
                "M. Alam",
                "R. Kaur"
            ],
            "title": "Neural machine translation for low-resource languages: A survey",
            "venue": "arXiv e-prints pp. arXiv\u20132106",
            "year": 2021
        },
        {
            "authors": [
                "T.L. Scao",
                "T. Wang",
                "D. Hesslow",
                "L. Saulnier",
                "S. Bekman",
                "S. Bari",
                "S.R. Biderman",
                "H. ElSahar",
                "N. Muennighoff",
                "J. Phang",
                "O. Press",
                "C. Raffel",
                "V. Sanh",
                "S. Shen",
                "L. Sutawika",
                "J. Tae",
                "Z.X. Yong",
                "J. Launay",
                "I. Beltagy"
            ],
            "title": "What language model to train if you have one million gpu hours? Conference On Empirical Methods In Natural Language Processing (2022)",
            "year": 2022
        },
        {
            "authors": [
                "L.R. de Souza",
                "R. Nogueira",
                "R. Lotufo"
            ],
            "title": "On the ability of monolingual models to learn language-agnostic representations",
            "venue": "arXiv preprint arXiv: 2109.01942",
            "year": 2021
        },
        {
            "authors": [
                "W. de Vries",
                "M. Bartelds",
                "M. Nissim",
                "M. Wieling"
            ],
            "title": "Adapting monolingual models: Data can be scarce when language similarity is high",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. pp. 4901\u20134907. Association for Computational Linguistics, Online",
            "year": 2021
        },
        {
            "authors": [
                "L. Xue",
                "A. Barua",
                "N. Constant",
                "R. Al-Rfou",
                "S. Narang",
                "M. Kale",
                "A. Roberts",
                "C. Raffel"
            ],
            "title": "Byt5: Towards a token-free future with pre-trained byte-to-byte models",
            "venue": "arXiv preprint arXiv: 2105.13626",
            "year": 2021
        },
        {
            "authors": [
                "L. Xue",
                "N. Constant",
                "A. Roberts",
                "M. Kale",
                "R. Al-Rfou",
                "A. Siddhant",
                "A. Barua",
                "C. Raffel"
            ],
            "title": "mt5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "arXiv preprint arXiv: 2010.11934",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n31 0.\n09 14\n1v 2\n[ cs\n.C L\nKeywords: Setswana, Natural Language Processing, Language Models"
        },
        {
            "heading": "1 Introduction",
            "text": "The development of monolingual models for local languages is essential in better understanding individual languages and further developing tools for those languages [22]. It has been shown that monolingual models can further be improved for downstream tasks later on [37,8] or even allow for cross-lingual transfer [36] and also have a space in improving translation models [9]. Ultimately, the availability of more monolingual models and the curation of data to train them will increase the opportunities for bigger multilingual models [12], as they will have a larger pool of data to learn from, better evaluation for the individual languages and knowledge of the limitations of approaches and knowledge of the various limitations of approaches utilised by monolingual models (e.g. the development of subword segmentation to improve language models for Nguni languages [28]).\nAdditionally, monolingual models will be better suited for tasks that require a deep understanding of a specific language [6].\nIn this work, we focus our efforts on creating a monolingual language model for Setswana. Setswana is a Bantu languages that is spoken in Botswana as well as several regions of South Africa [32]. Previous work has been done on creating monolingual datasets for Setswana (see NCHLT corpus [15]). This work has been valuable in providing a foundation for further research on Setswana. More recently the NCHLT corpus has been used to train language models that are available online without benchmarks5 (as per the writing of this paper). The creation of more models provides diversity in approaches and benchmarks, which can help us to better evaluate the quality of resources available for a given language and understand the strengths and weaknesses of different techniques to natural language processing for Setswana. This paper contributes to the Setswana literature and also provides tools for researchers, and identifies areas where more research is needed.\nPuoBERTa is a customised masked language model trained specifically for Setswana. It was developed to address the gap in natural language processing (NLP) performance for Setswana. This paper covers how we collected, curated, and prepared diverse multilingual texts to generate a high-quality corpus for PuoBERTa\u2019s training. We also built upon previous efforts in creating monolingual resources for Setswana, and evaluated PuoBERTa across several NLP tasks, including part-of-speech (POS) tagging, named entity recognition (NER), and news categorization. Additionally, we introduce a new Setswana news categorization dataset and provide the initial benchmarks using PuoBERTa."
        },
        {
            "heading": "2 Related Work",
            "text": "There have been a few attempts to create a monolingual Setswana language model. TswanaBert 6 [29] is one of the earliest models available for Setswana. It was trained on a combination of datasets from scraped news headlines from social media7 [27] and the Leipzig Corpus 8 [17,10].\nA more recent model based on the NCHLT [15], Autshumato, Leipzig and Webcrawl corpora is the NCHLT RoBERTa Model9 [14]. For both TswanaBert and NCHLT Setswana RoBERTa language models, at the time of writing, there are no online benchmarks that compare the approaches to other work, such as multilingual models that include the Setswana language. As such we later provide benchmarks for these models on Setswana downstream tasks.\nMore recently, work on African language models has focused on creating multilingual models. The multilingual approaches aim to exploit the connections between African languages to allow languages with low resources to benefit from\n5 http://www.rma.nwu.ac.za/handle/20.500.12185/641 6 https://huggingface.co/MoseliMotsoehli/TswanaBert 7 https://zenodo.org/record/5674236 8 https://corpora.uni-leipzig.de/ 9 http://www.rma.nwu.ac.za/handle/20.500.12185/641\nthose that have more resources. Work by Adelani et.al.[2] focused on creating parallel corpora, for machine translation, for a number of African languages including Setswana. The corpora is then used to fine-tune pre-trained translation models such as MT5 [39], MBART [24], ByT5 [38] and M2M-100 [16]. Other work focused on adapting XLM-RoBERTa into African languages including Setswana [5], training multilingual BERT for African languages [31]. Additionally, with the advent of the age of massive large language models, we have models such as BLOOM [35] which have Setswana within and more Afrocentric models such as Serengeti [1] that are now available."
        },
        {
            "heading": "3 Curating Setswana Corpora PuoData and Pre-Training PuoBERTa",
            "text": ""
        },
        {
            "heading": "3.1 Curating Corpora to Train Word and Sentence Representations",
            "text": "Modern language models require a large amount of data to develop. The challenges of obtaining data for low-resource languages have been well-documented [33,34,30]. In this work, we gather a number of datasets to pre-train Setswana language models. The data was includes data from research organisations, books, official government documents, and online content. The final dataset is referred to as PuoData. Puo means \u201dlanguage\u201d in Setswana.\nWe believe that PuoData is a valuable resource for the Setswana language community. We hope that PuoData will be used to develop new and innovative applications that benefit the Setswana-speaking community. One of the input corpora that we gather during this project (starting in 2020) was the JW300 monolingual dataset for Setswana from the OPUS Corpora [7]. However, JW300 is no longer an open source dataset available online. As such, we will refer to PuoData+JW300, which is the dataset including a historical JW300 archive we had access to before it was removed from the OPUS. We provide benchmarks with PuoData and PuoData+JW300 in the interest of language development. PuoData+JW300 is a larger dataset than PuoData alone (see Table 3.1). It contains more text (even if it is text that is religious in nature) than all the other datasets combined.\nThe data, detailed in Table 3.1, contains sources such as the NCHLT Setswana [15] corpus, the South African Constitution 10, the Leipzig Setswana BW and ZA corpora. We also include more recent corpora such as the Vuk\u2019zenzele Setswana Corpora [21]11 and South African Cabinet Speeches [21]12. All of the corpora contain Setswana data only\nPuoData is made available as a single dataset for researchers14. We now focus on how we built pre-trained models with PuoData and PuoData+JW300.\n10 https://www.justice.gov.za/constitution/SAConstitution-web-set.pdf 11 https://huggingface.co/datasets/dsfsi/vukuzenzele-monolingual 12 https://huggingface.co/datasets/dsfsi/gov-za-monolingual 14 https://github.com/dsfsi/PuoData, https://huggingface.co/datasets/dsfsi/PuoData"
        },
        {
            "heading": "3.2 Training the Masked Language Model: PuoBERTa",
            "text": "We first trained two BPE Tokenizers for the PuoBERTa. One tokenizer with PuoData and the other with PuoData+JW300 corpora with 52000 tokens each. The pre-trained masked language models were trained on an NVIDIA Titan RTX with 24 GB of VRAM on an Intel Core i9-9900K with 32GB of RAM. The training information is shown in Table 2.\nThe pre-trained models are made available for download for research and development15 [26]. We next focus on evaluating the new models on a number of downstream tasks.\n15 https://github.com/dsfsi/PuoBERTa , https://huggingface.co/dsfsi/PuoBERTa"
        },
        {
            "heading": "4 Evaluation of PuoBERTa on Downstream Tasks",
            "text": "The PuoBERTa models were fine-tuned and evaluated on three downstream tasks: Named Entity Recognition (NER), Part of Speech (POS) tagging, and news categorisation/classification. The results showed that the PuoBERTamodel achieved state-of-the-art results on all three tasks in terms of F1 score amongst the monolingual models."
        },
        {
            "heading": "4.1 Evaluation on MasakhaNER",
            "text": "We first evaluate PuoBERTa on the MasakhaNER 2.0 [3] benchmark, which is a dataset for named entity recognition in a number of African languages including Setswana. We compare the performance of PuoBERTa to NCHLT RoBERTa and the multilingual models reported in the MasakhaNER 2.0 paper [3], namely Afriberta, AfroXLMR-base and AfroXLMR-large models. The results in Table 3 show that PuoBERTa is very competitive. It does not beat the multilingual models in this case. PuoBERTa+JW300 gets much closer than the rest of the monolingual models."
        },
        {
            "heading": "4.2 Evaluation on MasakhaPOS",
            "text": "For this downstream task we use the same evaluations script as per the MasakhaPOS paper [11] to evaluate our PuoBERTa variants plus the NCHLT RoBERTa. The results are shown in Table 4. In this test PuoBERTa almost beats the best multilingual model (AfroLM) but PuoBERTa+JW300 beats all the models available."
        },
        {
            "heading": "4.3 News Categorisation - Daily News",
            "text": "Dataset information We gather our dataset from the official website of the Government of Botswana\u2019s Daily News16. The website primarily features news\n16 https://dailynews.gov.bw/\ncontent in the English language, while also incorporating some of their materials in Setswana within the designated Dikgang segment. Our compiled dataset, denoted as Daily News - Dikgang, needed annotation of news categories. Notably, the news items were initially devoid of categorisation information. To rectify this, we embarked on the task of categorising these news items, leveraging the International Press Telecommunications Council (IPTC) News Categories (or codes)17. For the categorisation process, we employed the highest hierarchical level of the news code taxonomy. Table 5 provides an overview of these top-level codes, along with their corresponding Setswana translations, which guided our annotation efforts. For this process we involved two expert Setswana annotators who initially annotated the dataset individually and then came together to deal with disagreements on labelling afterwards leading to the final dataset that we use for this paper and also make available. See Appendix A for the Data Statement.\nFollowing the annotation process, it is worth noting that not all annotated data was employed in the subsequent evaluation of models for the classification task. Our strategy involved focusing on the top 10 categories, each comprised of a minimum of 80 labelled samples in the full dataset. This selection criterion was informed by the frequency distribution observed within the collected dataset (see Figure 1). As such the final full dataset has a total of 4867 samples, 3893, 487 and 487 samples for train, dev and test splits respectively.\nIn order to delve into the textual data, we present a 2-dimensional visualisation of the news dataset in Figure 2, where each category is represented by a unique color. This visualisation was constructed by applying a T-distributed Stochastic Neighbour Embedding (TSNE) [25] model to the TF-IDF representation of the news corpus data. The visualisation gives us an initial view of the distribution of the data and how separable the categories might be. We make the complete dataset available alongside this paper18 [26], fostering openness and reproducibility.\n17 https://iptc.org/standards/newscodes/ 18 https://github.com/dsfsi/PuoBERTa\nTraining and performance For the news categorisation task, we performed 5-fold cross-validation using the different models at hand. We performed the cross validation by combining the train and dev sets and then doing an 80/20 (train/validate) split 5 times randomly. We compared PuoBERTa, PuoBERTaJW300 to baselines of the NCHLT RoBERTa Model, and also logistic regression with TFIDF. We use the TFIDF+logistic regression to provide an insight on how well the more powerful models perform as well as make sure we have a more efficient baseline. We further provide the performance on the test set for the models we have. For all the RoBERTa-based models, the fine-tuning was at 5 epochs. The results of our classification task experiments are shown in Table 6.\nThere is a performance improvement by using the RoBERTa based models. The performance improvements by PuoBERTa and PuoBERTaJW300 are even\narts_culture_entertainment_and_media conflict_war_and_peace crime_law_and_justice\ndisaster_accident_and_emergency_incident economy_business_and_finance\neducation environment\nhealth labour\nlifestyle_and_leisure politics\nreligion_and_belief science_and_technology\nsociety sport weather Ne ws C at eg or y\nDaily News - Ditaba: News Category Frequency\nbetter respectively. To note, the logistic regression TFIDF vectorizer is only trained using the news training data while the RoBERTa tokenizers have not been specifically trained using the Daily News data. It is still an interesting result as development of the news dataset is also useful for less powerful models. To better understand the classification performance, we took the best performing model of the PuoBERTaJW300 and show its confusion matrix in Figure 3. We observe in this confusion matrix that the models makes more mistakes when trying to predict economy business finance, politics, society categories."
        },
        {
            "heading": "4.4 Observations Given the Downstream Tasks",
            "text": "PuoBERTa in general is a competitive model. When we include JW300 data (PuoBERTaJW300), the model improves further and performs as state of the art for the downstream tasks. This brings up a question of what is lost by not being able to use JW300 as a language resource for Setswana and other low resource languages covered by JW300? The performance of PuoBERTa is close to that of PuoBERTaJW300 and is reassuring as it would allow more researchers to focus on building better African language corpora."
        },
        {
            "heading": "5 Conclusion and Future Work",
            "text": "In this paper, we presented the creation of a number of artifacts as we pursued creating a state of the art pre-trained Language Model for the Setswana Language. We were able to gather a corpora (of diverse sources) named PuoData that we release along with this paper with a permissive license. We further discuss the PuoData+JW300 corpora which allows us to measure the impact of pretraining with JW300 included in the corpora. For the POS, NER and a new news categorisation task (Daily News - Dikgang) we show that PuoBERTa and PuoBERTaJW300 are competitive language models that have state of the art or near state of the art performance in those downstream tasks.\nWe look forward to seeing how the community uses this model and different application areas. There are a few directions to still be taken that we could not cover in this paper. The corpora needs to be further analysed for bias [18,23,19] and the models evaluated for challenges with domain shift [2]. We have shown that if we include JW300, the corpora size increases 4 fold, but at what cost? Further in comparison with large multilingual models, PuoBERTa and PuoBERTaJW300 are 300 MB (200MB compressed) might benefit from further optimisations that may make it more useful for low resource scenarios (compute, access to internet etc.).\nFor the news categorisation task, there could be more work done to understand how both the simple logistic regression model and other more complex ones can benefit from more feature or label engineering of the dataset to combine labels that might be two similar in hindsight. We also suggest looking into how we can standardise datasets such as [20] etc. to all use a standard categorisation set to enable better transfer learning and evaluation for news."
        },
        {
            "heading": "6 Acknowledgement",
            "text": "We want to acknowledge the feedback received from colleagues at DSFSI at Univeristy of Preotria and Lelapa AI that improved this paper. We would like to acknowledge funding from the ABSA Chair of Data Science, Google and the NVIDIA Corporation hardware Grant. We are thankful to the anonymous reviewers for their feedback."
        },
        {
            "heading": "Appendix: Data Statement for Daily News - Dikgang Categorised News Corpus",
            "text": "Dataset name: Daily News - Dikgang Categorised News Corpus Citations: Cite this paper. Dataset developer(s): Vukosi Marivate (vukosi.marivate@cs.up.ac.za) and Valencia Wagner (valencia.wagner@spu.ac.za) Data statement author(s): Vukosi Marivate Organisation: Data Science for Social Impact Research Group https://dsfsi.github.io), Department of Computer Science, University of Pretoria, South Africa and Sol Plaatje University"
        },
        {
            "heading": "A. CURATION RATIONALE",
            "text": "The motivation for building this dataset was to provide one of the few annotated news categorisation datasets for Setswana. The task required identifying a high-quality Setswana news dataset, collecting the data, and then annotating leveraging the International Press Telecommunications Council (IPTC) News Categories (or codes)19. The identified source was the Daily News20 (Dikgang Section) from the Botswana Government. All copyright for the news content belongs to Daily News. We collected 5000 Setswana news articles. The distribution of final categories for the dataset are shown in Figure 1."
        },
        {
            "heading": "B. LANGUAGE VARIETY",
            "text": "The language of this data set is Setswana (primarily from Botswana)."
        },
        {
            "heading": "C. SPEAKER DEMOGRAPHIC",
            "text": "Setswana is a Bantu languages that is spoken in Botswana as well as several regions of South Africa [32]."
        },
        {
            "heading": "D. ANNOTATOR DEMOGRAPHIC",
            "text": "Two annotators were used to label the news articles based on the Daily News - Dikgang news. Their deomographic information is shown in Table 7."
        },
        {
            "heading": "E. PROVENANCE APPENDIX",
            "text": "The original data is from the Daily News news service from the Botswana Government. 19 https://iptc.org/standards/newscodes/ 20 https://dailynews.gov.bw/"
        }
    ],
    "title": "PuoBERTa: Training and evaluation of a curated language model for Setswana",
    "year": 2023
}