{
    "abstractText": "The minimum covariance determinant (MCD) estimator is ubiquitous in multivariate analysis, the critical step of which is to select a subset of a given size with the lowest sample covariance determinant. The concentration step (C-step) is a common tool for subset-seeking; however, it becomes computationally demanding for highdimensional data. To alleviate the challenge, we propose a depth-based algorithm, termed as FDB, which replaces the optimal subset with the trimmed region induced by statistical depth. We show that the depth-based region is consistent with the MCD-based subset under a specific class of depth notions, for instance, the projection depth. With the two suggested depths, the FDB estimator is not only computationally more efficient but also reaches the same level of robustness as the MCD estimator. Extensive simulation studies are conducted to assess the empirical performance of our estimators. We also validate the computational efficiency and robustness of our estimators under several typical tasks such as principal component analysis, linear discriminant analysis, image denoise and outlier detection on real-life datasets. A R package FDB and potential extensions are available in the Supplementary Materials.",
    "authors": [
        {
            "affiliations": [],
            "name": "Maoyu Zhang\u2217a"
        },
        {
            "affiliations": [],
            "name": "Yan Song\u2020a"
        },
        {
            "affiliations": [],
            "name": "Wenlin Dai\u2021a"
        }
    ],
    "id": "SP:477c4fb45cb0dada4f9d9c2eb4b35d0a98f5eae5",
    "references": [
        {
            "authors": [
                "C.C. Aggarwal",
                "S. Sathe"
            ],
            "title": "Theoretical foundations and algorithms for outlier ensembles,",
            "venue": "ACM SIGKDD Explorations Newsletter,",
            "year": 2015
        },
        {
            "authors": [
                "J. Agull\u00f3",
                "C. Croux",
                "S. Van Aelst"
            ],
            "title": "The multivariate least-trimmed squares estimator,",
            "venue": "Journal of Multivariate Analysis,",
            "year": 2008
        },
        {
            "authors": [
                "F. Alqallaf",
                "S. Van Aelst",
                "V.J. Yohai",
                "R.H. Zamar"
            ],
            "title": "Propagation of outliers in multivariate data,",
            "year": 2009
        },
        {
            "authors": [
                "K. Boudt",
                "P.J. Rousseeuw",
                "S. Vanduffel",
                "T. Verdonck"
            ],
            "title": "The minimum regularized covariance determinant estimator,",
            "venue": "Statistics and Computing,",
            "year": 2020
        },
        {
            "authors": [
                "R. Butler",
                "P. Davies",
                "M. Jhun"
            ],
            "title": "Asymptotics for the minimum covariance determinant estimator,",
            "venue": "The Annals of Statistics,",
            "year": 1993
        },
        {
            "authors": [
                "E.A. Cator",
                "H.P. Lopuha\u00e4"
            ],
            "title": "Central limit theorem and influence function for the MCD estimators at general multivariate distributions,",
            "year": 2012
        },
        {
            "authors": [
                "C.W. Coakley",
                "T.P. Hettmansperger"
            ],
            "title": "A bounded influence, high breakdown, efficient regression estimator,",
            "venue": "Journal of the American Statistical Association,",
            "year": 1993
        },
        {
            "authors": [
                "C. Croux",
                "G. Haesbroeck"
            ],
            "title": "Principal component analysis based on robust estimators of the covariance or correlation matrix: influence functions and efficiencies,",
            "year": 2000
        },
        {
            "authors": [
                "B. De Ketelaere",
                "M. Hubert",
                "J. Raymaekers",
                "P.J. Rousseeuw",
                "I. Vranckx"
            ],
            "title": "Real-time outlier detection for large datasets by RT-DetMCD,",
            "venue": "Chemometrics and Intelligent Laboratory Systems,",
            "year": 2020
        },
        {
            "authors": [
                "M. Debruyne",
                "M. Hubert"
            ],
            "title": "The influence function of the Stahel\u2013Donoho covariance estimator of smallest outlyingness,",
            "venue": "Statistics & probability letters,",
            "year": 2009
        },
        {
            "authors": [
                "R. Dyckerhoff",
                "P. Mozharovskyi",
                "S. Nagy"
            ],
            "title": "Approximate computation of projection depths,",
            "venue": "Computational Statistics & Data Analysis,",
            "year": 2021
        },
        {
            "authors": [
                "M. Fekri",
                "A. Ruiz-Gazen"
            ],
            "title": "Robust weighted orthogonal regression in the errorsin-variables model,",
            "venue": "Journal of Multivariate Analysis,",
            "year": 2004
        },
        {
            "authors": [
                "J. Hardin",
                "D.M. Rocke"
            ],
            "title": "Outlier detection in the multiple cluster setting using the minimum covariance determinant estimator,",
            "venue": "Computational Statistics & Data Analysis,",
            "year": 2004
        },
        {
            "authors": [
                "T. Hastie",
                "R. Tibshirani",
                "J. Friedman"
            ],
            "title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer Science & Business Media",
            "year": 2009
        },
        {
            "authors": [
                "M. Hubert",
                "M. Debruyne",
                "P.J. Rousseeuw"
            ],
            "title": "Minimum covariance determinant and extensions,",
            "venue": "Wiley Interdisciplinary Reviews: Computational Statistics,",
            "year": 2018
        },
        {
            "authors": [
                "M. Hubert",
                "P.J. Rousseeuw"
            ],
            "title": "Robust regression with both continuous and binary regressors,",
            "venue": "Journal of Statistical Planning and Inference,",
            "year": 1997
        },
        {
            "authors": [
                "M. Hubert",
                "P.J. Rousseeuw",
                "K.V. Branden"
            ],
            "title": "ROBPCA: A New Approach to Robust Principal Component Analysis,",
            "venue": "Technometrics, 47,",
            "year": 2005
        },
        {
            "authors": [
                "M. Hubert",
                "P.J. Rousseeuw",
                "S. Van Aelst"
            ],
            "title": "High-breakdown robust multivariate methods,",
            "venue": "Statistical Science,",
            "year": 2008
        },
        {
            "authors": [
                "M. Hubert",
                "P.J. Rousseeuw",
                "K. Vanden Branden"
            ],
            "title": "ROBPCA: a new approach to robust principal component analysis,",
            "year": 2005
        },
        {
            "authors": [
                "M. Hubert",
                "P.J. Rousseeuw",
                "T. Verdonck"
            ],
            "title": "A deterministic algorithm for robust location and scatter,",
            "venue": "Journal of Computational and Graphical Statistics,",
            "year": 2012
        },
        {
            "authors": [
                "M. Hubert",
                "K. Van Driessen"
            ],
            "title": "Fast and robust discriminant analysis,",
            "venue": "Computational Statistics & Data Analysis,",
            "year": 2004
        },
        {
            "authors": [
                "J. Kalina",
                "J. Tichavsk\u1ef3"
            ],
            "title": "The minimum weighted covariance determinant estimator for high-dimensional data,",
            "venue": "Advances in Data Analysis and Classification,",
            "year": 2021
        },
        {
            "authors": [
                "R.Y. Liu"
            ],
            "title": "On a notion of data depth based on random simplices,",
            "venue": "The Annals of Statistics,",
            "year": 1990
        },
        {
            "authors": [
                "H.P. Lopuhaa",
                "P.J. Rousseeuw"
            ],
            "title": "Breakdown points of affine equivariant estimators of multivariate location and covariance matrices,",
            "year": 1991
        },
        {
            "authors": [
                "P.C. Mahalanobis"
            ],
            "title": "On the generalized distance in statistics,",
            "venue": "National Institute of Science of India,",
            "year": 1936
        },
        {
            "authors": [
                "R.A. Maronna",
                "R.H. Zamar"
            ],
            "title": "Robust estimates of location and dispersion for high-dimensional datasets,",
            "year": 2002
        },
        {
            "authors": [
                "W. Milo"
            ],
            "title": "Multivariate statistics: a practical approach,",
            "venue": "Journal of the Royal Statistical Society: Series A,",
            "year": 1990
        },
        {
            "authors": [
                "K. Mosler",
                "P. Mozharovskyi"
            ],
            "title": "Choosing among notions of multivariate depth statistics,",
            "venue": "Statistical Science,",
            "year": 2022
        },
        {
            "authors": [
                "A. Niinimaa",
                "H. Oja"
            ],
            "title": "On the influence functions of certain bivariate medians,",
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological),",
            "year": 1995
        },
        {
            "authors": [
                "H. Oja"
            ],
            "title": "Descriptive statistics for multivariate distributions,",
            "venue": "Statistics & Probability Letters,",
            "year": 1983
        },
        {
            "authors": [
                "G. Pison",
                "P.J. Rousseeuw",
                "P. Filzmoser",
                "C. Croux"
            ],
            "title": "Robust factor analysis,",
            "venue": "Journal of Multivariate Analysis,",
            "year": 2003
        },
        {
            "authors": [
                "U. Porwal",
                "S. Mukund"
            ],
            "title": "Outlier detection by consistent data selection method,",
            "venue": "arXiv preprint arXiv:1712.04129",
            "year": 2017
        },
        {
            "authors": [
                "E. Roelant",
                "S. Van Aelst",
                "G. Willems"
            ],
            "title": "The minimum weighted covariance determinant estimator,",
            "venue": "Metrika,",
            "year": 2009
        },
        {
            "authors": [
                "P.J. Rousseeuw"
            ],
            "title": "Least median of squares regression,",
            "venue": "Journal of the American Statistical Association,",
            "year": 1984
        },
        {
            "authors": [
                "P.J. Rousseeuw",
                "C. Croux"
            ],
            "title": "Alternatives to the median absolute deviation,",
            "venue": "Journal of the American Statistical Association,",
            "year": 1993
        },
        {
            "authors": [
                "P.J. Rousseeuw",
                "K.V. Driessen"
            ],
            "title": "A fast algorithm for the minimum covariance determinant estimator,",
            "year": 1999
        },
        {
            "authors": [
                "P.J. Rousseeuw",
                "B.C. Van Zomeren"
            ],
            "title": "Unmasking multivariate outliers and leverage points,",
            "venue": "Journal of the American Statistical Association,",
            "year": 1990
        },
        {
            "authors": [
                "J. Schreurs",
                "I. Vranckx",
                "B.D. Ketelaere",
                "M. Hubert",
                "J.A.K. Suykens",
                "P.J. Rousseeuw"
            ],
            "title": "Outlier detection in non-elliptical data by kernel MRCD,",
            "venue": "Statistics and Computing,",
            "year": 2021
        },
        {
            "authors": [
                "J.W. Tukey"
            ],
            "title": "Mathematics and the picturing of data,",
            "venue": "Proceedings of the International Congress of Mathematicians, Vancouver,",
            "year": 1975
        },
        {
            "authors": [
                "Y. Vardi",
                "Zhang",
                "C.-H"
            ],
            "title": "The multivariate L1-median and associated data depth,",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2000
        },
        {
            "authors": [
                "G. Willems",
                "H. Joe",
                "R. Zamar"
            ],
            "title": "Diagnosing multivariate outliers detected by robust estimators,",
            "venue": "Journal of Computational and Graphical Statistics,",
            "year": 2009
        },
        {
            "authors": [
                "V.J. Yohai",
                "R.H. Zamar"
            ],
            "title": "High breakdown-point estimates of regression by means of the minimization of an efficient scale,",
            "venue": "Journal of the American Statistical Association,",
            "year": 1988
        },
        {
            "authors": [
                "Y. Zuo"
            ],
            "title": "Multidimensional trimming based on projection depth,",
            "venue": "The Annals of Statistics,",
            "year": 2006
        },
        {
            "authors": [
                "Y. Zuo",
                "R. Serfling"
            ],
            "title": "2000a), \u201cGeneral notions of statistical depth function,",
            "venue": "The Annals of Statistics,",
            "year": 2000
        }
    ],
    "sections": [
        {
            "text": "Keywords: Computationally efficient; High-dimensional data; Outliers; Robustness; Statistical depth.\n\u2217Email: zhangmaoyu@ruc.edu.cn. \u2020Email: 2018000743@ruc.edu.cn. \u2021Corresponding author; email: wenlin.dai@ruc.edu.cn.\nar X\niv :2\n30 5.\n07 81\n3v 1\n[ st\nat .M\nE ]\n1 3"
        },
        {
            "heading": "1 Introduction",
            "text": "The Minimum Covariance Determinant (MCD) estimator (Rousseeuw, 1984) is among the first affine equivariant and highly robust estimators of multivariate location and scatter. Specifically, for a collection of multivariate data, MCD seeks a subset of samples that leads to a sample covariance matrix with the minimum determinant out of all the candidate sets of a specific size. The location and scatter estimators are then defined as the average and a scaled covariance matrix of these samples, respectively. Butler et al. (1993) and Cator and Lopuha\u00e4 (2012) established the consistency and asymptotic normality of the MCD estimator.\nMCD has been applied in various fields such as quality control, medicine, finance, image analysis, and chemistry (Hubert et al., 2008, 2018). Estimating the covariance matrix is the cornerstone of many multivariate statistical methods, so MCD has also been used to develop robust and computationally efficient multivariate techniques, such as principal component analysis (Croux and Haesbroeck, 2000; Hubert et al., 2005b), factor analysis (Pison et al., 2003), classification (Hubert and Van Driessen, 2004), clustering (Hardin and Rocke, 2004), multivariate regression (Rousseeuw et al., 2004b), and others (Hubert et al., 2008). To cater to its broad applications, extensive effort has been made to improve the computational efficiency of the approximation algorithm. For example, Rousseeuw and Driessen (1999) propose the first computationally efficient algorithm, termed FASTMCD; (Hubert et al., 2012) suggest an improved version of FASTMCD, termed DetMCD; De Ketelaere et al. (2020) accelerates DetMCD by refinement of the calculation steps and parallel computation. Furthermore, Boudt et al. (2020) generalizes the MCD to high-dimensional cases as the minimum regularized covariance determinant (MRCD). Other variants include the orthogonalized Gnanadesikan-Kettenring (Maronna and Zamar, 2002), the minimum (regularized) weighted covariance determinant (Roelant et al., 2009; Kalina and Tichavsky\u0300,\n2021), and kernel MRCD for non-elliptical data (Schreurs et al., 2021).\nPractically, the MCD-type algorithms are limited by two factors. First, the computational complexity of the concentration step (C-step), which is critical for such algorithms, is O(np2 + p3), and this severely limits the scalability of the algorithm for massive highdimensional data. Second, the approximation to the true MCD subset becomes less accurate due to the curse of dimensionality. We note that the asymptotic trimmed region induced by a class of statistical depth shares the same form with the asymptotic MCD subset when the data are elliptically symmetric distributed (Butler et al., 1993; Zuo and Serfling, 2000b). This motivates us to investigate the possibility of finding the MCD subset directly by utilizing statistical depth. Statistical depth was first considered for ranking multivariate data from the center outward (Mahalanobis, 1936; Tukey, 1975; Oja, 1983; Liu, 1990; Zuo and Serfling, 2000a; Vardi and Zhang, 2000). Usually, a statistical depth is an increasing function of the centrality of observations, taking values in [0, 1].\nMotivated by the connection mentioned above, we propose a fast depth-based algorithm, denoted as FDB, which approximates the MCD subset with a depth-induced trimmed region. Specifically, we investigate FDB based on two representative depth notions, projection depth and L2 depth, and denote the estimators as, FDBpro and FDBL2 , respectively. Four main advantages of the proposed algorithm are worth mentioning: 1) Asymptotically, FDBpro leads to a trimmed region equivalent to the MCD subset for elliptically symmetric distributions. 2) Both FDBpro and FDBL2 achieve the same level of robustness as the MCD estimator. 3) Empirically, FDB reveals comparable or even better performance than the MCD estimator regarding estimation accuracy. 4) Furthermore, the computational efficiency is dramatically improved by using FDB, especially for high-dimensional cases.\nThe rest of the paper is organized as follows. Section 2 reviews the MCD estimator and some related theoretical properties. Section 3 introduces the idea of FDB estimators and demonstrates the theoretical equivalence between the MCD subsets and the depth-trimmed\nregions. Section 4 investigates the invariance, robustness, and computational complexity of the proposed FDB estimators. In Section 5, we conduct extensive simulation studies to assess the performance of the FDB algorithm and compare it with the existing ones regarding estimation accuracy and computational efficiency. Section 6 applies the proposed methods to several real applications through typical multivariate analysis tasks, including principal component analysis, linear discriminant analysis, image denoise, and outlier detection. We end the paper with some discussion in Section 7. Proofs of the theoretical results and additional simulation results are provided in the Supplementary Material."
        },
        {
            "heading": "2 MCD Estimators",
            "text": "In this section, we review the theoretical property of the MCD estimator as well as three widely utilized approximation algorithms. Let x \u2208 Rp be a random variable from an elliptically symmetric distribution, denoted as ES(f ;\u00b5,\u03a3), whose density is of the form\ng(x) = c|\u03a3|\u22121/2f ( (x\u2212 \u00b5)T\u03a3\u22121(x\u2212 \u00b5) ) ,\nwhere \u03a3 is a symmetric positive definite matrix, and the function f : R+ \u2192 R+ is assumed to be non-increasing so that g(x) is unimodal.\nConsidering random samples x1, . . . ,xn independently generated from the above distri-\nbution, MCD aims to solve the following optimization problem\nH\u0302\u03b1n,MCD = argmin H\u2208Hh\n( det \u03a3\u0302(xH) ) , (1)\nwhere H is an index set of h observations (with b(n+ p+ 1)/2c 6 h 6 n, where bac means the largest integer smaller than or equal to a), \u03b1n = h/n and Hh is the collection of all such sets. Observations with corresponding indices H\u0302\u03b1n,MCD constitute the final MCD subset,\ntermed E\u0302\u03b1n,MCD,. Define \u2206(A,B) = A \u222a B \u2212 A \u2229 B as the difference between sets A and B. The convergence property of E\u0302\u03b1nMCD is revisited in Lemma 1, with the proof provided in Section S1.1 of the Supplementary Material.\nLemma 1 Assume that the random samples xi \u223c ES(f ;\u00b5,\u03a3), i = 1, . . . , n. Then for \u03b1 > 0, we have\nP ( \u2206 ( E\u0302\u03b1n,MCD, E\u03b1 )) \u2192 0\nfor any sequence \u03b1n \u2192 \u03b1 as n \u2192 \u221e, where E\u03b1 = { x \u2208 Rp | (x\u2212 \u00b5)T\u03a3\u22121(x\u2212 \u00b5) \u2264 r2\u03b1 } with P(E\u03b1) = \u03b1.\nGiven an n \u00d7 p data matrix X = (x1, . . . ,xn)\u2032, with its estimated center \u00b5\u0302 and scatter matrix \u03a3\u0302, we denote with D ( xi; \u00b5\u0302, \u03a3\u0302 ) = \u221a (xi \u2212 \u00b5\u0302)T \u03a3\u0302\u22121 (xi \u2212 \u00b5\u0302) the Mahalanobis distance of xi. The C-step described in Algorithm 1 is crucial for MCD-type algorithms. Algorithm 1 The C-step Input: initial subset Hold or the estimates (\u00b5\u0302old, \u03a3\u0302old), subset size h. Output: Hnew or (\u00b5\u0302new, \u03a3\u0302new)\n1: Compute the distances di,old = D ( xi; \u00b5\u0302old, \u03a3\u0302old ) for i = 1, . . . , n. 2: Sort these distances, yielding a permutation \u03c0 for which d\u03c0(1),old 6 d\u03c0(2),old 6 . . . 6 d\u03c0(n),old, and set Hnew = {\u03c0(1), \u03c0(2), . . . , \u03c0(h)}. 3: Update \u00b5\u0302 and \u03a3\u0302 as\n\u00b5\u0302new = 1\nh \u2211 i\u2208Hnew xi and \u03a3\u0302new = 1 h\u2212 1 \u2211 i\u2208Hnew (xi \u2212 \u00b5\u0302new) (xi \u2212 \u00b5\u0302new)T .\nRousseeuw and Driessen (1999) proposed the first computationally feasible algorithm, termed FASTMCD, for approximating the MCD subset. Specifically, they randomly constructed several initial subsets and applied two C-steps for each subset, yielding the ten subsets with the lowest determinant. Then, they took C-step iteratively for these ten subsets until the determinant sequence converged and eventually chose the MCD subset as the one leading to the smallest determinant. Given this, the computational efficiency of\nFASTMCD is thus roughly proportional to the number of the initial subsets. Hubert et al. (2012) proposed an alternative algorithm DetMCD, which replaces random initial subsets (of which there could be many) in FASTMCD with six well-designed deterministic estima-\ntors of (\u00b5,\u03a3), and also involves the C-step in a similar way. Denote the estimates ( \u00b5\u0302, \u03a3\u0302 ) as the location and scatter matrix estimates of the h-subset for which the determinant of the sample covariance matrix is as small as possible. Further, an additional reweighted step is employed in both algorithms to improve the efficiency of the estimators. To be more specific, the estimators are renewed as trimmed estimates for location and scatter,\n\u00b5\u0302re = 1\u2211n\ni=1Wi n\u2211 i=1 Wixi and \u03a3\u0302re = 1\u2211n i=1Wi \u2212 1 n\u2211 i=1 Wi (xi \u2212 \u00b5\u0302re) (xi \u2212 \u00b5\u0302re)T , (2)\nwhere Wi = 1 when D ( xi; \u00b5\u0302, c0\u03a3\u0302 ) \u2264 \u221a \u03c72p,0.975 and 0 otherwise, \u03c72p,\u03b1 is the \u03b1-quantile of\nthe \u03c72p distribution and c0 = mediD2 ( xi, \u00b5\u0302, \u03a3\u0302 ) /\u03c72p,0.5."
        },
        {
            "heading": "3 A Depth-based Alternative",
            "text": "The idea behind the premier step of MCD-type algorithms is to construct outlier-free subsets as the initial values for the C-step. This motivates us to approach such a purpose using statistical depth, a popular tool for robust multivariate data analysis. More importantly, we find the equivalence between the eventual MCD subset and the depth-trimmed region, which avoids the implementation of the iterative C-steps and hence improves the computational efficiency dramatically.\nIn general, a statistical depth notion is a function D : (x, P ) 7\u2192 [0, 1], for x \u2208 Rp and P from some class P of p-variate probability distributions, that provides a centeroutward order for a collection of data. Taking into account the robustness as well as the computational efficiency, to be discussed later, we specifically consider the following two\ndepth notions for the proposed method.\nProjection depth (Zuo and Serfling, 2000a):\nDProj(x;P ) =\n( 1 + sup\n\u2016u\u2032\u2016=1\n|u\u2032x\u2212med (u\u2032y)| MAD (u\u2032y)\n)\u22121 ,\nwhere P is the distribution of y, med(V ) denotes the median of a univariate random variable V , and MAD(V ) = med(|V \u2212 med(V )|) its median absolute deviation from the median. Practically, one may choose a finite number of random directions to approximate the projection depth values.\nL2 depth (Zuo and Serfling, 2000a):\nDL2(x;P ) = (1 + E [\u2016y \u2212 x\u20162]) \u22121 ,\nwhere \u2016 \u00b7 \u20162 is the L2 norm.\nConsider random samples x1, . . . ,xn independently generated from the elliptically sym-\nmetric (ES) distribution. For a given depth function D(\u00b7; ES) and for \u03b1 > 0, we call\nE\u03b1,depth = {x \u2208 Rp | D(x; ES) \u2265 D\u03b1} ,\nand the sample version is\nE\u0302\u03b1n,depth = { x \u2208 Rp | D\u0302n(x; ES) \u2265 D\u03b1n } ,\nthe corresponding \u03b1-trimmed region with P(E\u03b1,depth) = \u03b1.\nLemma 2 (Zuo and Serfling (2000b)) Assume that the random samples xi \u223c ES(f ;\u00b5,\u03a3), i = 1, . . . , n. Then for the projection depth, the depth trimmed region (subset) E\u03b1n,depth sat-\nisfies\nP ( \u2206 ( E\u0302\u03b1n,depth, E\u03b1 )) \u2192 0 as n\u2192\u221e,\nfor any sequence \u03b1n \u2192 \u03b1 > 0 as n\u2192\u221e.\nOther depth notions could also lead to the same conclusion except for the projection depth. We omit those notions since they are either not robust enough or computationally demanding. Also note that the result does not necessarily hold for the L2 depth, although FDBL2 indeed provides satisfactory results in the simulation. Combining Lemmas 1 and 2, it is straightforward that the two subsets are asymptotic equivalent. We state this result formally in the following theorem.\nTheorem 1 Assume that the random samples xi \u223c ES(f ;\u00b5,\u03a3), i = 1, . . . , n. Under the conditions of Lemmas 1 and 2, we have\nP(\u2206(E\u03b1n,depth, E\u03b1n,MCD))\u2192 0, as n\u2192\u221e.\nProof of Theorem 1 is provided in Section S1.2 of the Supplementary Material. Motivated by the above result, we propose to approximate the eventual MCD subset with the depth-trimmed region and avoid the iterative implementation of the C-step. We provide two toy examples in Figure 1 to illustrate such a coincidence. Specifically, we generate data from bivariate normal distributions with unit variance and correlation coefficients 0 and 0.5 for Figure 1(a) and 1(b), respectively. We consider n = 4000 and h = 3000. In both cases, the two subsets match quite well, such that the proportions of the common elements are no less than 97%, which well supports the high effectiveness of the proposed method.\nFor data of low dimensions, both MCD subsets and depth-based trimmed regions can be computed efficiently, and the result matches well, as shown in Figure 1. However, for highdimensional data, the MCD algorithms are severely challenged by the cubically increased\ncomputational complexity, and hence the approximation will be less efficient. To alleviate this challenge, we consider replacing the MCD subsets with the trimmed regions induced by some computationally efficient depth notions. By doing so, we may not only reduce the computational time significantly but also attain comparable or better robust estimation for both the location and scatter matrix, especially in high-dimensional cases. In what follows, we introduce the FDB algorithm.\nAlgorithm 2 FDB Input: xi, i = 1, . . . , n, subset size h, selected depth notion. Output: \u00b5\u0302FDB, \u03a3\u0302FDB. 1: Calculate the depth value for each observation xi, denoted as GD(i). 2: Sort these values, yielding a permutation \u03c0 of 1, . . . , n, for which GD(\u03c0(1)) \u2265, . . . ,\u2265\nGD(\u03c0(n)), and a set Hsub = {\u03c0(1), . . . , \u03c0(h)}. 3: Get the location and the scatter matrix estimates as\n\u00b5\u0302raw = 1\nh \u2211 i\u2208Hsub xi and \u03a3\u0302raw = c1 h \u2211 i\u2208Hsub (xi \u2212 \u00b5raw) (xi \u2212 \u00b5raw)T ,\nwhere c1 = med i D2 ( xi, \u00b5\u0302raw, \u03a3\u03020 ) /\u03c72p,0.5 with \u03a3\u03020 = 1 h \u2211 i\u2208Hsub (xi \u2212 \u00b5raw) (xi \u2212 \u00b5raw)\nT . 4: Apply the reweighted step (2) to the raw estimates, yielding the final FDB estimates, \u00b5\u0302FDB and \u03a3\u0302FDB.\nAlgorithm 2 considers the case of h > p, which is the condition to guarantee the invertibility of estimated matrix (Rousseeuw and Van Zomeren, 1990). All algorithms for the original MCD require that the dimension p be lower than h to obtain an invertible covariance matrix. It is recommended that n > 5p in practice (Rousseeuw and Driessen, 1999).\nAccording to Lemmas 1 and 2, for data from an elliptically symmetric distribution, MCD and FDB algorithms both approximate the optimal subset, though from different perspectives. MCD approaches the solution by combining well selected (or random) initial subset and the iterative implementation of the C-step, which could be computationally demanding for high dimensional data. In contrast, FDB relies on ordering the data from the center outward, and hence its computational complexity is mainly determined by the cost of assigning depth values to each sample.\nThe idea of incorporating depth (outlyingness) to construct MCD estimators has been considered in the literature. For example, the Stahel\u2013Donoho outlyingness (Donoho, 1982), equivalent to the projection depth, is applied to determine an h-subset consisting of the h points with the lowest outlyingness, and the corresponding sample mean and covariance matrix are used as one initial value for the C-step (Hubert et al., 2005a; Schreurs et al., 2021). Debruyne and Hubert (2009) studied the influence function and asymptotic relative efficiency of the estimators obtained directly based on such a subset (without the reweighted step). For the first time, we establish the equivalence of the two subsets, which indicates that the depth-based subset is a reasonable approximation to the optimal subset rather than just one option of the initial value for the C-step."
        },
        {
            "heading": "4 Properties of FDB",
            "text": "This section focuses on the properties of the FDB estimators. Specifically, We discuss three types of properties, that are of main interest for such methods (Maronna and Zamar, 2002; Hubert et al., 2012), invariance, robustness and computational complexity. We show that the proposed estimators are quite satisfactory in these aspects."
        },
        {
            "heading": "4.1 Invariant Properties",
            "text": "Affine equivariance makes the analysis independent of any affine transformation of the data. For any nonsingular p\u00d7 p matrix A and p\u00d7 1 vector v, the estimators \u00b5\u0302 and \u03a3\u0302 are affine equivariant if they satisfy\n\u00b5\u0302 ( XA+ 1nv T ) = \u00b5\u0302(X)A+ v and \u03a3\u0302 ( XA+ 1nv T ) = AT \u03a3\u0302(X)A,\nwhere 1n = (1, 1, . . . , 1)T . The projection depth has been shown affine equivariant (Zuo and Serfling, 2000a; Zuo, 2006), that is the depth value does not vary through affine transformation for any sample, and hence the indexes of samples forming the trimmed region remain the same. Consequently, FDBpro is obviously affine equivariant. For FDBL2 , a similar property holds for rigid transformation (Mosler and Mozharovskyi, 2022), which is a bit more restrictive than the affine transformation. For high dimensional situations, the affine equivariance may be less important under nonstandard data contamination such as componentwise outliers (Alqallaf et al., 2009).\nPermutation invariance provides an effective way to guaranteeing the robustness of analysis to the perturbation of observations. An estimator T (\u00b7) is said to be permutation invariant if T (PX) = T (X) for any permutation matrix P . Permutation invariance holds for both projection depth and L2 depth since they do not involve any random subsets, and hence the depth values remain the same through permutation and so will the h-subset."
        },
        {
            "heading": "4.2 Robustness",
            "text": "Robustness is the property of main interest when outlier contamination of the data is suspected. As aforementioned, the MCD estimator is highly robust that it achieves the highest possible asymptotic breakdown point, about 1/2, with h \u2248 n/2. The robustness of FDB is determined by the property of the employed depth notion. According to Zuo (2006) and Lopuhaa and Rousseeuw (1991), the breakdown points of the trimmed regions induced by the projection depth and L2 depth are both 1/2, with \u03b1 \u2248 0.5. That is, FDBpro and FDBL2 both have a breakdown point as high as that of the MCD estimator. Another indicator is the influence function, which captures the local robustness of estimators. Zuo (2006) and Niinimaa and Oja (1995) showed the influence functions of depth regions induced by projection depth and L2 depth are both bounded. Therefore, FDBpro and FDBL2 are highly robust locally as well as globally.\nThe robustness discussed above is from the theoretical aspect. Practically, both FDB and MCD approximate the theoretically optimal subsets, and their empirical performances do not necessarily match the theoretical results under all the scenarios. This means that the subset selected by the two methods under finite samples may still contain outliers. To show this point, we provide an example in Figure 2, where we generate n = 4000 samples from a 40-dimensional normal distribution with standard normal marginal distributions and a correlation coefficient of 0.5. We consider two levels of contamination, 10% and 40%, and two types of outliers, point and cluster (see for details in Section 5.1), respectively. For the first column with 10% outliers, we set h = b75%nc; for the second column with 40% outliers, h = b50%nc. FDBpro performs perfectly for the first three cases and fails for the last scenario; in contrast, DetMCD is only satisfactory for the first case but fails for the rest. This indicates that for high-dimensional data, FDBpro provides more reliable approximations to the optimal subset."
        },
        {
            "heading": "4.3 Computational complexity",
            "text": "The computational complexity for finding the h-subset by MCDs is O(\u03c8(np2 +p3)). Specifically, for each C-step, it requires computing the covariance matrix and Mahalanobis distances, with complexities O(np2) and O(p3 + np2) respectively, and \u03c8 depends on the number of initial estimates and the times of C-step iteration. For FASTMCD, the number of initial estimates defaults to 500; for DetMCD, it defaults to 6; and for RT-DetMCD, it is further reduced to 2. However, these efforts only reduced the value of \u03c6 but the order term still remains the same.\nIn contrast, the computational complexity of FDBpro for finding the subset is O(knp) with k as the number of projection directions. According to our numerical experiments, The performance of FDBpro is quite stable when the number of random directions is set around 1000; see Figure S2 of the Supplement. Hence, FDBpro leads to a significant improvement over the MCD estimators. We remark that it is possible to further reduce the number of projection directions according to some elaborate generative algorithms (Dyckerhoff et al., 2021). However, these algorithms may instead lengthen the total computational time due to the tedious procedure for searching \u201cbetter\u201d directions, and hence we stick to selecting the directions randomly. For the case of ultra-high dimensional data, we suggest an adaptive rule, k = max(1000, 10p). As for FDBL2 , the computational complexity is O(n2p), which scales linearly with the dimension of data.\nTo show the improvement, we provide some numerical results for computation time in Figure 3. Notably, the speed of DetMCD is also influenced by the way of constructing the initial estimates. Specifically, Qn (Rousseeuw and Croux, 1993) is applied to construct initial estimates for DetMCD, which is computationally demanding. To improve speed, Hubert et al. (2012) suggested substituting Qn with the \u03c4 -scale of Yohai and Zamar (1988)\nwhen n > 1000. We follow this suggestion by using the Qn estimator for DetMCD in Figure 3(a), and the \u03c4 -estimator in Figure 3(b), respectively. For FDBpro, we let k = 1000. All experiments are run using R-package ddalpha for DetMCD on an Intel(R) Xeon(R) with 3.10GHz and 192 GB memory processor.\nFDBs show significant improvement over DetMCD under all the settings. Specifically, in Figure 3(a), the line of DetMCD is steeper than those of the other two methods, which matches well with different orders of dimension p in their theoretical computational complexities aforementioned. In Figure 3(b), both DetMCD and FDBpro reveal linear trends with the increasing sample size, while FDBL2 shows a quadratic trend, though its computation time is the least when n < 4000."
        },
        {
            "heading": "5 Simulations",
            "text": "We conduct extensive simulations with data from symmetric distributions to assess the performance of our proposed algorithms, FDBpro and FDBL2 , and make a comparison with DetMCD (Hubert et al., 2012). Besides, we also provide some exploration for the scenarios of asymmetric distributions in Section S3.1 of the Supplement. To evaluate the estimation results, we use the following five measures (the smaller the better).\n\u2022 An error measure of the location estimator, given by e\u00b5 = \u2016\u00b5\u0302\u2212\u00b50\u2016, where \u00b50 denotes\nthe true sample mean.\n\u2022 An error measure of the scatter estimator, defined as the logarithm of the condition\nnumber of \u03a3\u0302\u03a3\u22121, e\u03a3 = log10(cond(\u03a3\u0302\u03a3\u22121)).\n\u2022 The mean squared error (MSE) of \u03a3, MSE = 1 Sp2 \u2211S s=1 ||\u03a3\u0302\u2212\u03a3||2F . \u2022 The Kullback Leibler (KL) divergence between \u03a3\u0302 and \u03a3, KL ( \u03a3\u0302,\u03a3 ) = trace ( \u03a3\u0302\u03a3\u22121 ) \u2212\nlog ( det ( \u03a3\u0302\u03a3\u22121 )) \u2212 p,, which is identical the KL divergence between the two Gaussian distributions with the same mean.\n\u2022 The computation time t (in seconds) of the whole procedure, including the optimal\nsubset pursuit and the reweighted step."
        },
        {
            "heading": "5.1 Estimation performance",
            "text": "In this subsection, we generate the bulk of non-outlying samples as xi = Gyi, where yi are from Np(0, I) and G is a p \u00d7 p matrix with unit diagonal elements and off-diagonal elements equal to 0.75. The number of outliers is m = bn\u03b5c and \u03b5 denotes the level of contamination. Four contamination types are considered: point, random, cluster, and\nradial outliers. Point outliers are obtained by generating yi \u223c Np ( ra \u221a p, 0.012I ) , where a is a unit vector generated orthogonal to a0 = (1, 1, . . . , 1)T . Random outliers are obtained by generating yi \u223c Np (\u00b5ir, I), where \u00b5ir = rp1/4\u03bd/\u2016\u03bd\u2016 with \u03bd a random vector\nfrom Np(0, I). Cluster outliers are obtained by generating yi \u223c Np ( rp\u22121/4a0, I ) , where r is a constant. Radial outliers are obtained by generating yi \u223c Np(0, 5I). Except for the random outliers, the other three types have been considered by Hubert et al. (2012).\nDifferent contamination levels are considered, namely \u03b5 = 0%, 10%, and 40%. Let h be b0.75nc when \u03b5 = 0% or 10%, and b0.5nc when \u03b5 = 40% for each method under investigation. The number of directions for projection depth is set as k = max(1000, 10p) as suggested in Section 4.3. For each method, we compute the reweighted location vectors \u00b5\u0302X and the reweighted scatter matrices \u03a3\u0302X . The corresponding estimators for the data set Y are obtained as \u00b5\u0302Y = G\u22121\u00b5\u0302X and \u03a3\u0302Y = G\u22121\u03a3\u0302XG\u22121, which are compared to the true values using the aforementioned measures. In this part, the true covariance matrix of Y is \u03a3 = Ip.\nWe first provide a full picture for the performance of each method by conducting sim-\nulation studies under a broad range of settings. To be more specific, we generated 1000 data sets consisting of different types of contamination under a broad range of r and p, and computed the average e\u03a3 with the three methods. The results are illustrated in Figures 4 and 5. Other measures, e\u00b5, MSE, and KL divergence, all reveal similar patterns as e\u03a3 and hence are omitted. As shown in Figure 4, FDBpro and FDBL2 outperform DetMCD across all values of r for both random and cluster outliers under either low or high contamination levels. For the case of point contamination, FDBpro still holds the upper hand, with FDBL2 slightly worse than DetMCD, especially for small r values.\nFigure 5 shows that FDBs is among the best estimators for all the settings. However, in general, DetMCD\u2019s performance deviates more seriously with the increase of dimension p, suggesting that it is less suitable for high-dimensional data. Overall, FDBpro provides the best performance among the three options, that both FDBL2 and DetMCD produce large e\u03a3 under point contamination since their induced subsets may contain outliers, which is consistent with the results in Figure 2. Between FDBL2 and DetMCD, the later is better for point contamination while the former is better (or even the best) for cluster contamination.\nNext, we provide more detailed numerical outputs for typical simulation settings under study. Following Hubert et al. (2012), we consider three options, A: n = 200 and p = 5, B: n = 400 and p = 40, and C: n = 2000 and p = 200, representing low, moderate and high dimensions, respectively. Other settings remain the same as those in Figure 4 except that r is fixed at 5 for point, random, and cluster outliers. We report the average measures over 1000 runs in Tables 1, 2 and 3, corresponding to 0% (clean data), 10% and 40% contamination, respectively.\nFor clean data (Table 1), the three estimators are comparable for low-dimensional cases; FDBs achieve slightly smaller values for e\u00b5, e\u03a3 and KL when the data dimension is moderate or high. More importantly, the running time of DetMCD is reduced in all settings, with the relative computational efficiency, defined as tMCD/tFDB, ranging between 2 and 10 for\nFDBpro, and between 3 and 27 for FDBL2 . Such a comparison of computation time holds similarly when contamination presents, and hence is omitted in the remaining tables.\nWhen the amount of contamination is 10% (Table 2), the performances of three methods are all satisfying under settings with random, cluster or radial contamination, and the comparison is similar to that for clean data. For point contamination, FDBL2 performs the worse for each of the three settings, that it generates the largest values for the four measures of estimation accuracy; DetMCD gets problematic for moderate and high dimensional data (settings B and C), which indicates its deficiency for such cases; in contrast, FDBpro remains robust under all three settings and provides values of measures quite close to corresponding ones for the clean data in Table 1.\nWhen the amount of contamination increases to 40% (Table 3), the three methods all work well and are comparable under settings with random or radial contamination. However, none of them is satisfactory when the contamination presents as point outliers, and this weak performance was also observed for both DetMCD and FASTMCD under similar settings in Hubert et al. (2012). For cluster contamination, DetMCD leads to larger estimation errors in each cell, especially when the dimension of data is moderate or\nhigh; however, both FDBL2 and FDBpro instead remains very stable across all three settings. Additional results for r = 2 and 20 are provided in Tables S1\u2013S4 of the Supplementary Material, from which we may draw similar conclusions for the comparison of the three methods."
        },
        {
            "heading": "5.2 Robustness assessment",
            "text": "In addition, we assess the tolerance of each method to the core-set size h = b\u03b1nc. Specifically, we generate data from Setting B with 40% contamination, which is high enough for most practical implementations, and we set r = 100 for point, random and cluster outliers. Meanwhile, we consider \u03b1 ranging from 0.5 to 0.6, which is the highest value that possibly\nproduces a clean subset. The average e\u03a3 from 1000 replications are reported in Figure 6. FDBpro remain robust for different values of \u03b1 under all the investigated settings; FDBL2 is satisfactory for random, cluster and radial contamination, while its estimation error grows substantially with \u03b1 for point contamination; DetMCD generates table estimation when \u03b1 = 0.5 and 0.55 but becomes deficient when \u03b1 raises up to 0.6 in each plot of Figure 6. The other three measures show similar patterns as illustrated in Figure S1 of the Supplementary Material. In conclusion, FDBpro reaches the strongest tolerance to the core-set size, when the proportion of outliers in the data is very high.\nTo sum it up, FDBs improves the computational efficiency significantly, which is the main motivation of this work, and FDBL2 generally achieves the highest computational efficiency. Besides, we surprisingly find that FDBpro shows superiority in terms of both estimation accuracy and robustness, especially in high-dimensional cases. One may safely substitute DetMCD with FDBpro for practical implementation."
        },
        {
            "heading": "6 Real data examples",
            "text": "In this section, we apply the FDB methods to four real datasets of various dimensions and sizes. The resultant robust multivariate location and scatter estimates are evaluated via some typical tasks in multivariate analysis such as outlier detection, linear discriminant analysis (LDA), principal component analysis (PCA), and image denoise. The same three methods from Section 5 are utilized. The computations are performed in R (R Core Team 2021) on a laptop with a 10-core and 32GB memory processor."
        },
        {
            "heading": "6.1 Robust PCA for forged bank notes data",
            "text": "The first dataset is the forged Swiss bank notes data (Milo, 1990), which is also used in Hubert et al. (2012). The data are of size n = 100 and dimension p = 6, denoted as X \u2208 Rn\u00d7p. Since this dataset includes outliers and highly correlated variables (Rousseeuw et al., 2004a; Willems et al., 2009), we employ the proposed algorithms and DetMCD to get robust estimation (\u00b5\u0302, \u03a3\u0302) first and conduct robust PCA based on these estimates. The classical PCA obtained by sample location and scatter is also demonstrated for comparison. We use the first two principal components P \u2208 Rp\u00d72, which explain over 80% of the total variance. The projections of data on the 2-dimensional PCA subspace, T = {tik} = (X \u2212 1n\u00b5\u0302T )P , are shown in Fig. 7(e)\u20137(h).\nThe score distance (SD) and orthogonal distance (OD) represent the robust distance of samples in the two-dimensional PCA subspace and the orthogonal distance of samples to\nthe PCA subspace, respectively. For each sample xi, we have SDi = \u22112 k=1 t 2 ik/\u03bbk, where \u03bb1\nand \u03bb2 are the first two eigenvalues of \u03a3\u0302, and ODi = \u2211p j=1 e 2 i,j, where X \u2212 TP T = {ei,j}. Figure 7(a)\u20137(d) illustrate diagnostic plots. By two cutoff values in each diagnostic plot, we categorize samples into four types and assign different colors to them in Fig. 7(e)\u2013 7(h). Regular samples gather in the bottom-left region of diagnostic plots with both score\ndistances and orthogonal distances relatively small, which form the main body of the data cloud. Good leverage samples are close to the PCA subspace but far from the regular samples, e.g., the samples 13 and 23 in the bottom-right region of Fig. 7(a)\u20137(c), and the green points in Fig. 7(e)\u2013Fig. 7(g). Orthogonal outliers are far from the PCA subspace but not distinguishable by only observing their projections. With larger orthogonal distances but smaller score distances, they locate on the top-left region of diagonal plots, e.g., samples 11, 62 and 67 in Fig. 7(a)\u20137(c), and red points in Fig. 7(e)\u2013Fig. 7(g). For bad leverage samples, both score and orthogonal distances are large. They lie on the top-right region of diagonal plots and are represented as orange points in projection plots. In general, the three robust methods lead to comparable analysis results and all significantly improve the performance of the classical PCA, which agrees with Theorem 1. However, DetMCD may identify some regular points as outliers according to its specific cutoff value for the\northogonal distance; see the purple points in the third column of Figure 7."
        },
        {
            "heading": "6.2 Outlier detection for phoneme data",
            "text": "In the second example, we detect outliers for a phoneme dataset, which comes from a speech recognition database TIMIT and has been discussed in Hastie et al. (2009). The data includes 1050 speech frames, 1000 of which are \u201cao\u201d and 50 of which are \u201ciy\u201d. Each data frame has been transformed to be a log-periodogram of length 256. First, we reduce the dimensions by smoothing splines. For each sample, we replace the original variables x \u2208 R256 with 50-dimensional variables x\u0303 = NTx, whereN \u2208 R256\u00d750 is the basis matrix of natural cubic splines. We use 50 basis functions with knots uniformly placed over 1, . . . , 256. To this end, we are dealing with data of n = 1050 and p = 50.\nWe take \u201cao\u201d and \u201ciy\u201d to be regular cases and outliers, respectively, and perform outlier detection. To be more specific, we calculate the robust Mahalanobis distances based on FDBs and DetMCD, and classical Mahalanobis distances based on Sample. Then, we treat cases with the top 50 largest distances as outliers, and the remaining ones as regular cases. Table 4 records the number of \u201ciy\u201d that are flagged as outliers, the area under the ROC curve (AUC), and the average computational time (second) of 100 replicates for these methods. Again, the proposed methods and DetMCD perform similarly and they all outperform Sample with higher AUCs. In addition, the proposed methods reveal advantages in computation time."
        },
        {
            "heading": "6.3 Denoise for MNIST data",
            "text": "The Modified National Institute of Standards and Technology (MNIST) database is widely used for training various image processing systems. It contains a large set of handwritten images representing the digits zero through nine. Each digit is stored as a gray-scale image with a size p = 28\u00d7 28. Training data X and testing data Xt of n = 10000 are randomly selected from MNIST. Noises \u223c N (0p, 602Ip) are generated and added to 20% of the images in the training set and all images in the testing set. Our task is to denoise the testing images, which has been considered in Schreurs et al. (2021).\nThe detailed process is similar to the robust PCA in Section 6.1. First, we apply FDBL2 , FDBpro, DetMCD, and Sample to training images and obtain the estimated location and scatter. FDBpro with k = 1000, denoted as FDBpro1000, is also used for better comparison. Here all methods have comparable computational time. Then, we calculate the eigenvectors of the robust estimated scatter. Next, we project the testing data to the subspace spanned by the first K eigenvectors and then transform the projected data, i.e., scores, back to the original space. We denote the reconstructed data as X\u0302t. Fig. 8 illustrates X\u0302t obtained by various methods with K = 75. DetMCD is removed since it returns an error of high condition numbers and hence is not applicable to this example. We can see that the denoised images for the proposed methods are more clear than those for the Sample, which verifies the influence of adding noise on evaluating scatter and the efficiency of the proposed methods. As in Schreurs et al. (2021), we also calculate the mean absolute error MAE =\u2211n i=1 \u2211p j=1 |xt(ij) \u2212 x\u0302t(ij)|/(np) between the original and denoised images. Table 5 shows that the MAE for Sample is obviously larger than those for proposed methods."
        },
        {
            "heading": "6.4 Outlier detection for Musk data",
            "text": "Musk data is commonly used in high-dimensional classification and outlier detection problems (Aggarwal and Sathe, 2015; Porwal and Mukund, 2017). The original data, which\ncan be found in UCI includes 6598 samples, divide into musk and non-musk classes. Each sample has p = 166 features characterizing the molecule structure. Here we use a preprocessed musk dataset of n = 3062, which consists of 2965 non-musk samples as inliers and 97 musk sample as outliers. Our task is to detect the outliers with the proposed methods, DetMCD and Sample. As shown in Fig. 9, our proposed methods and DetMCD can exactly pick out outliers, which are represented as red points, whereas Sample would take many regular samples as outliers. Furthermore, the computational time of FDBL2 , FDBpro, and DetMCD are 13.547, 12.884, and 53.391 seconds, respectively. The proposed methods are computationally more efficient than DetMCD."
        },
        {
            "heading": "7 Discussion",
            "text": "MCD-type methods suffer from high computational complexity due to the iteratively used C-step. To tackle the issue, we directly approximate the MCD subset with the trimmed subset induced by statistical depth. Two depth notions, the projection depth and the L2 depth, are recommended due to high computational efficiency and robustness. In addition, we establish the equivalence between the desired MCD subset and the trimmed subset induced by the projection depth. Bypassing the iteration of the C-step, we manage to reduce the computational complexity from O(\u03c8np2 + \u03c8p3) to O(knp) and O(n2p) with\nFDBpro and FDBL2 , respectively. Moreover, the proposed estimators also reach the same level of robustness as the MCD estimator. We conduct extensive simulation studies and show that our estimators are comparable with MCD-type estimators for low-dimensional data and significantly outperform MCD-type estimators for high-dimensional cases.\nThe real data examples provide strong evidence that FDB is a valuable complement to the toolset of robust multivariate analysis, including but not limited to PCA, LDA, image denoise and outlier detection. The FDB algorithm may benefit other applications that directly or indirectly rely on robust covariance matrix estimation, such as robust linear regression (Coakley and Hettmansperger, 1993), regression with continuous and categorical regressors (Hubert and Rousseeuw, 1997), MCD-regression (Rousseeuw et al., 2004b), multivariate least trimmed squares estimation (Agull\u00f3 et al., 2008), and robust errors-invariables regression (Fekri and Ruiz-Gazen, 2004).\nThe present study primarily focuses on data from elliptical symmetric distributions with enough samples, which may be violated in practical scenarios (Schreurs et al., 2021). In Section S3.1 of the Supplementary Material, we evaluate the effectiveness of our estimators when dealing with skewed distributions. In addition, we did preliminary work to extend the proposed algorithm to the scenario of \u201csmall n, large p\u201d and evaluate our idea with a realworld dataset in Section S3.2 of the Supplementary Material. Our preliminary exploration shows promising results for both cases. Further investigation is needed to address more general scenarios. One possible solution is to adapt depth notions applicable to more general distributions to obtain the h-subset and then apply the kernel trick to map the subset to a feature space, where outlier detection can be conducted. The computational time is significantly reduced for high-dimensional scenarios (n > p); however, the estimation accuracy still desires further improvement. Rather than a shrinkage estimator, it is also of interest to extend the MCD framework by considering a low-rank and sparse estimator to alleviate the curse of dimensionality.\nSupplementary materials\n\u2022 We provide an R-package named FDB and R codes of the FDB algorithm proposed in\nthis paper.\n\u2022 The file of supplement involves proofs of theoretical results, additional simulation\nresults as well as preliminary explorations of potential extensions."
        },
        {
            "heading": "Acknowledgement",
            "text": "We are very grateful to three anonymous referees, an associate editor, and the Editor for their valuable comments that have greatly improved the manuscript. The first two authors contribute equally to the paper."
        }
    ],
    "title": "Fast robust location and scatter estimation: a depth-based method",
    "year": 2023
}