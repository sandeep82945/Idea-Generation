{
    "abstractText": "Artificial intelligence (AI) and human-machine interaction (HMI) are two keywords that usually do not fit embedded applications. Within the steps needed before applying AI to solve a specific task, HMI is usually missing during the AI architecture design and the training of an AI model. The human-in-the-loop concept is prevalent in all other steps of developing AI, from data analysis via data selection and cleaning to performance evaluation. During AI architecture design, HMI can immediately highlight unproductive layers of the architecture so that lightweight network architecture for embedded applications can be created easily. We show that by using this HMI, users can instantly distinguish which AI architecture should be trained and evaluated first since a high accuracy on the task could be expected. This approach reduces the resources needed for AI development by avoiding training and evaluating AI architectures with unproductive layers and leads to lightweight AI architectures. These resulting lightweight AI architectures will enable HMI while running the AI on an edge device. By enabling HMI during an AI uses inference, we will introduce the AI-in-the-loop concept that combines AI\u2019s and humans\u2019 strengths. In our AI-in-the-loop approach, the AI remains the working horse and primarily solves the task. If the AI is unsure whether its inference solves the task correctly, it asks the user to use an appropriate HMI. Consequently, AI will become available in many applications soon since HMI will make AI more reliable and explainable.",
    "authors": [
        {
            "affiliations": [],
            "name": "JULIUS SCH\u00d6NING"
        },
        {
            "affiliations": [],
            "name": "CLEMENS WESTERKAMP"
        }
    ],
    "id": "SP:b096d30fadde5f879e821906ceb04dfba1923750",
    "references": [
        {
            "authors": [
                "S. Ioffe",
                "C. Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "International Conference on Machine Learning (IMCL), vol. 37. JMLR.org, 2015 . doi: 10.48550/arXiv.1502.03167 pp. 448\u2013456.",
            "year": 2015
        },
        {
            "authors": [
                "X. Chen",
                "C. Liang",
                "D. Huang",
                "E. Real",
                "K. Wang",
                "Y. Liu",
                "H. Pham",
                "X. Dong",
                "T. Luong",
                "C.-J. Hsieh",
                "Y. Lu",
                "Q.V. Le"
            ],
            "title": "Symbolic discovery of optimization algorithms",
            "venue": "2023.",
            "year": 2023
        },
        {
            "authors": [
                "M.L. Richter",
                "J. Schoning",
                "A. Wiedenroth",
                "U. Krumnack"
            ],
            "title": "Should you go deeper? optimizing convolutional neural network architectures without training",
            "venue": "20th IEEE International Conference on Machine Learning and Applications (ICMLA). IEEE, 2021 . doi: 10.1109/icmla52953.2021.00159",
            "year": 2021
        },
        {
            "authors": [
                "K. Yang",
                "K. Qinami",
                "L. Fei-Fei",
                "J. Deng",
                "O. Russakovsky"
            ],
            "title": "Towards fairer datasets",
            "venue": "Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. ACM, 2020 . doi: 10.1145/3351095.3375709",
            "year": 2020
        },
        {
            "authors": [
                "T. Wang",
                "J. Zhao",
                "M. Yatskar",
                "K.-W. Chang",
                "V. Ordonez"
            ],
            "title": "Balanced datasets are not enough: Estimating and mitigating gender bias in deep image representations",
            "venue": "IEEE/CVF International Conference on Computer Vision (ICCV). IEEE, 2019 . doi: 10.1109/iccv.2019.00541",
            "year": 2019
        },
        {
            "authors": [
                "C. Westerkamp"
            ],
            "title": "Sep.) Der ventilatoren-koffer - Mittelstand 4.0-Komptenzzentrum Lingen",
            "year": 2020
        },
        {
            "authors": [
                "F. van Veen",
                "S. Leijnen"
            ],
            "title": "Apr.) The neural network zoo",
            "year": 2019
        },
        {
            "authors": [
                "X. Huang",
                "D. Kroening",
                "W. Ruan",
                "J. Sharp",
                "Y. Sun",
                "E. Thamo",
                "M. Wu",
                "X. Yi"
            ],
            "title": "A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability",
            "venue": "Computer Science Review, vol. 37, p. 100270, 2020 . doi: 10.1016/j.cosrev.2020.100270",
            "year": 2020
        },
        {
            "authors": [
                "H.I. Fawaz",
                "G. Forestier",
                "J. Weber",
                "L. Idoumghar",
                "P.-A. Muller"
            ],
            "title": "Adversarial attacks on deep neural networks for time series classification",
            "venue": "International Joint Conference on Neural Networks (IJCNN). IEEE, 2019 . doi: 10.1109/ijcnn.2019.8851936",
            "year": 2019
        },
        {
            "authors": [
                "M.L. Richter"
            ],
            "title": "2023, Feb.) Receptivefieldanalysistoolbox - rfa-toolbox \u00b7 pypi",
            "year": 2023
        },
        {
            "authors": [
                "W. Luo",
                "Y. Li",
                "R. Urtasun",
                "R. Zemel"
            ],
            "title": "Understanding the effective receptive field in deep convolutional neural networks",
            "venue": "Proceedings of the 30th International Conference on Neural Information Processing Systems, ser. NIPS\u201916. Red Hook, NY, USA: Curran Associates Inc., 2016. ISBN 9781510838819 p. 4905\u20134913.",
            "year": 2016
        },
        {
            "authors": [
                "K. Koutini",
                "H. Eghbal-zadeh",
                "M. Dorfer",
                "G. Widmer"
            ],
            "title": "The receptive field as a regularizer in deep convolutional neural networks for acoustic scene classification",
            "venue": "27th European Signal Processing Conference (EUSIPCO). IEEE, 2019 . doi: 10.23919/eusipco.2019.8902732",
            "year": 2019
        },
        {
            "authors": [
                "R. Wang",
                "M. Gong",
                "D. Tao"
            ],
            "title": "Receptive field size versus model depth for single image super-resolution",
            "venue": "IEEE Transactions on Image Processing, vol. 29, pp. 1669\u20131682, 2020 . doi: 10.1109/tip.2019.2941327",
            "year": 2020
        },
        {
            "authors": [
                "M.L. Richter",
                "J. Sch\u00f6ning",
                "A. Wiedenroth",
                "U. Krumnack"
            ],
            "title": "Receptive field analysis for optimizing convolutional neural network architectures without training",
            "venue": "Deep Learning Applications, Volume 4. 10 Springer Nature Singapore, 2022, pp. 235\u2013261 . doi: 10.1007/978-981-19-6153-3_10",
            "year": 2022
        },
        {
            "authors": [
                "O. Russakovsky",
                "J. Deng",
                "H. Su",
                "J. Krause",
                "S. Satheesh",
                "S. Ma",
                "Z. Huang",
                "A. Karpathy",
                "A. Khosla",
                "M. Bernstein",
                "A.C. Berg",
                "L. Fei-Fei"
            ],
            "title": "ImageNet large scale visual recognition challenge",
            "venue": "International Journal of Computer Vision, vol. 115, no. 3, pp. 211\u2013252, 2015 . doi: 10.1007/s11263-015-0816-y",
            "year": 2015
        },
        {
            "authors": [
                "J. Sch\u00f6ning",
                "A. Riechmann",
                "H.-J. Pfisterer"
            ],
            "title": "AI for closed-loop control systems",
            "venue": "14th International Conference on Machine Learning and Computing (ICMLC). ACM, 2022 . doi: 10.1145/3529836.3529952",
            "year": 2022
        },
        {
            "authors": [
                "M. Xie",
                "J.S. Pujol-Roig",
                "F. Michelinakis",
                "T. Dreibholz",
                "C. Guerrero",
                "A.G. Sanchez",
                "W.Y. Poe",
                "Y. Wang",
                "A.M. Elmokashfi"
            ],
            "title": "AI-driven closed-loop service assurance with service exposures",
            "venue": "2020 European Conference on Networks and Communications (EuCNC). IEEE, 2020 . doi: 10.1109/eucnc48522.2020.9200943",
            "year": 2020
        },
        {
            "authors": [
                "D. Walczuch",
                "T. Nitzsche",
                "T. Seidel",
                "J. Schoning"
            ],
            "title": "Overview of closed-loop control systems and artificial intelligence utilization in greenhouse farming",
            "venue": "IEEE International Conference on Omni-layer Intelligent Systems (COINS). IEEE, 2022 . doi: 10.1109/coins54846.2022.9854938",
            "year": 2022
        },
        {
            "authors": [
                "J. Robinson"
            ],
            "title": "Gut reactions: A perceptual theory of emotion",
            "venue": "The British Journal of Aesthetics, vol. 46, no. 2, pp. 206\u2013208, 2006 . doi: 10.1093/aesthj/ayj024",
            "year": 2006
        },
        {
            "authors": [
                "V. Dubljevi\u0107",
                "C. Venero",
                "S. Knafo"
            ],
            "title": "What is cognitive enhancement?",
            "venue": "Cognitive Enhancement. Elsevier,",
            "year": 2015
        },
        {
            "authors": [
                "J. Sch\u00f6ning",
                "J. Kettler",
                "M.I. J\u00e4ger",
                "A. Gunia"
            ],
            "title": "Grand theft auto-based cycling simulator for cognitive enhancement technologies in dangerous traffic situations",
            "venue": "(under review), 2023.",
            "year": 2023
        },
        {
            "authors": [
                "L. Abdi",
                "A. Meddeb",
                "F.B. Abdallah"
            ],
            "title": "Augmented reality based traffic sign recognition for improved driving safety",
            "venue": "Lecture Notes in Computer Science. Springer International Publishing, 2015, pp. 94\u2013102 . doi: 10.1007/978-3-319-17765-6_9",
            "year": 2015
        },
        {
            "authors": [
                "J. Sch\u00f6ning",
                "S. Berkemeyer"
            ],
            "title": "Eyetracking nutritional behaviour and choices",
            "venue": "Communications in Computer and Information Science. Springer Nature Switzerland, 2022, pp. 17\u201331 . doi: 10.1007/978- 3-031-22015-9_2",
            "year": 2022
        },
        {
            "authors": [
                "K.-E. Chang",
                "J. Zhang",
                "Y.-S. Huang",
                "T.-C. Liu",
                "Y.-T. Sung"
            ],
            "title": "Applying augmented reality in physical education on motor skills learning",
            "venue": "Interactive Learning Environments, vol. 28, no. 6, pp. 685\u2013697, 2019 . doi: 10.1080/10494820.2019.1636073",
            "year": 2019
        },
        {
            "authors": [
                "M. Schaarschmidt",
                "M. Uelschen",
                "E. Pulverm\u00fcller",
                "C. Westerkamp"
            ],
            "title": "Framework of software design patterns for energy-aware embedded systems",
            "venue": "Proceedings of the 15th International Conference on Evaluation of Novel Approaches to Software Engineering. SCITEPRESS - Science and Technology Publications, 2020 . doi: 10.5220/0009351000620073",
            "year": 2020
        },
        {
            "authors": [
                "R. Schoenen",
                "H. Yanikomeroglu"
            ],
            "title": "User-in-the-loop: spatial and temporal demand shaping for sustainable wireless networks",
            "venue": "IEEE Communications Magazine, vol. 52, no. 2, pp. 196\u2013203, 2014 . doi: 10.1109/mcom.2014.6736762",
            "year": 2014
        },
        {
            "authors": [
                "C. Evers",
                "R. Kniewel",
                "K. Geihs",
                "L. Schmidt"
            ],
            "title": "The user in the loop: Enabling user participation for self-adaptive applications",
            "venue": "Future Generation Computer Systems, vol. 34, pp. 110\u2013123, 2014 . doi: 10.1016/j.future.2013.12.010",
            "year": 2014
        },
        {
            "authors": [
                "B.H. van der Velden",
                "H.J. Kuijf",
                "K.G. Gilhuijs",
                "M.A. Viergever"
            ],
            "title": "Explainable artificial intelligence (XAI) in deep learning-based medical image analysis",
            "venue": "Medical Image Analysis, vol. 79, p. 102470, 2022 . doi: 10.1016/j.media.2022.102470",
            "year": 2022
        },
        {
            "authors": [
                "M.D. Zeiler",
                "R. Fergus"
            ],
            "title": "Visualizing and understanding convolutional networks",
            "venue": "Computer Vision \u2013 ECCV 2014. Springer International Publishing, 2014, pp. 818\u2013833 . doi: 10.1007/978-3-319-10590-1_53",
            "year": 2014
        }
    ],
    "sections": [
        {
            "text": "AI-in-the-Loop The impact of HMI in AI-based Application\nJULIUS SCH\u00d6NING, Osnabr\u00fcck University of Applied Sciences, Germany CLEMENS WESTERKAMP, Osnabr\u00fcck University of Applied Sciences, Germany Artificial intelligence (AI) and human-machine interaction (HMI) are two keywords that usually do not fit embedded applications. Within the steps needed before applying AI to solve a specific task, HMI is usually missing during the AI architecture design and the training of an AI model. The human-in-the-loop concept is prevalent in all other steps of developing AI, from data analysis via data selection and cleaning to performance evaluation. During AI architecture design, HMI can immediately highlight unproductive layers of the architecture so that lightweight network architecture for embedded applications can be created easily. We show that by using this HMI, users can instantly distinguish which AI architecture should be trained and evaluated first since a high accuracy on the task could be expected. This approach reduces the resources needed for AI development by avoiding training and evaluating AI architectures with unproductive layers and leads to lightweight AI architectures. These resulting lightweight AI architectures will enable HMI while running the AI on an edge device. By enabling HMI during an AI uses inference, we will introduce the AI-in-the-loop concept that combines AI\u2019s and humans\u2019 strengths. In our AI-in-the-loop approach, the AI remains the working horse and primarily solves the task. If the AI is unsure whether its inference solves the task correctly, it asks the user to use an appropriate HMI. Consequently, AI will become available in many applications soon since HMI will make AI more reliable and explainable.\nAdditional Key Words and Phrases: Artificial Intelligence (AI); Human-Machine Interaction (HMI); AI-in-the-loop; Embedded Applications"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Going deeper and deeper is a trend that can be recognized in AI architectures, e.g., for object detection tasks. As illustrated in Fig. 1, the number of parameters in convolutional neural networks (CNN) increased exponentially from InceptionV2 [1] published in 2015 to BASIC-L [2] in 2023 by more than 115%. This ongoing trend of deep artificial neural networks (ANN) leads to the need for more hardware resources, even for just the inference of an ANN. In addition, deeper ANN architectures also increase the hardware resources and the amount of data for training. Considering embedded AI-based applications, the question arises: \"should one go deeper\" [3] by developing AI architecture? To avoid deeper and deeper architecture, leading to the need for more computation on embedded hardware and more extensive data sets, this paper introduces intuitive human-machine interactions (HMI) and user interfaces (UI) for AI-based applications.\nTo identify fields of HMI in AI-based applications systematically, Section 2 gives a recap on the pipeline of applying AI for discussion in which steps intuitive HMI is standard and in which steps the potential of HMI and UI is still unleashed. By focusing on one step, where HMI is not yet common, Section 3 introduces and evaluates a UI focusing on the identification of unproductive CNN layers, ensuring lightweight ANN design. Based on lightweight ANN, a short excursus on AI for a typical task from embedded systems, the closed-loop control, is given in Section 4. Making AI available for safety and security-critical applications, Section 5 introduces the AI-in-the-loop concept, where an interactive HMI will make embedded AI feasible within the upcoming years. Concluding this paper, Section 6,\nar X\niv :2\n30 3.\n11 50\n8v 1\n[ cs\n.H C\n] 2\n1 M\nar 2\n02 3\nthe need for minimal HMI for applicable embedded AI-based applications is discussed, and further research directions are introduced."
        },
        {
            "heading": "2 PIPELINE OF APPLYING AI",
            "text": "In theory, the development of AI is a straightforward process composed of six steps. As illustrated in Fig. 2, applying AI starts with data and an application idea. If the idea or the data came first is a chicken-and-egg problem. Usually, the ideas redefine the data collection and vice versa; the data trigger the ideas.\nAs soon as the application idea and the data lake are settled, selecting the data needed to solve the aimed application is the second step. In the first application iteration, by selecting the data for a noval application task, one should always ask, is a human capable of performing the tasks with the selected data? In our experience, asking this question will increase the success of the AI-based application in the early development phase. In later\nphases, especially in the context of embedded hardware, the data selection could be bolder so that the task might be impossible for a human being on the selection.\nBased on the data selection, the next step in the pipeline is data cleaning and transformation. Due to AI making inferences based on statistical and mathematical procedures, balancing the dataset is necessary for avoiding biases and enforcing the generalization of the tasks by the ANN. Especially by processing image data of people, data cleaning for avoiding biases [4, 5] in gender, ethnicity, and age is a crucial non-trial step. Next to cleaning, transforming the selected data in representation spaces suited for the AI architecture that should be applied is often mandatory. In the application case of predictive maintaining of, e.g., cooling fans [6], the data provided by the fans is usually a time series. Recurrent network architectures can be applied to time series classification tasks without a transformation. However, by transforming little sequences of the time series using, e.g., a Fourier transformation in a 2D representation, a CNN AI architecture can also be applied for predicting the malfunction. Thus, data transformation should always be considered for enabling different AI architectural designs\nDetermining which AI architecture fits the tasks and the data best is the entry point of this step. New kinds of AI architectures are published daily, so that an ANN zoo [7] is available to the developer. Before applying any architecture, one should ensure that the kind of AI architecture in mind is still state-of-the-art for covering the task. Next, to determine the kind of AI architecture, the number of layers and their parameters like kernel size, number of filters, and stride size will be defined, ending the architecture design.\nThe most resourceful and time-consuming step of the pipeline is the training of the AI architecture. The biggest subset of the selected, cleaned, and transformed data, normally 80%, is used to train the parameter ANN. These trainable parameters, also known as weights, should generalize the tasks so that the inference of the ANN will work on unknown data. Another subset of data, normally 10%, is used to evaluate the training performance during training. This evaluation is used to examine if and when overfitting occurs during the training. As soon as overfitting occurs, the ANN does not longer generalize the task; instead, the ANN starts to learn by rote.\nThe final step of the pipeline is applicable AI on embedded hardware. Thus, the AI-based application solves the desired tasks. Once the AI architecture reaches this step, the users come in contact with the AI-based application. Since users tend to explore the AIs\u2019 capacity, the AI-based application will often face adversarial attacks by the users [8, 9]. These adversarial attacks cannot be prohibited; however, the operational design domain of the application should be clearly defined before the data selection step to provide data from all conditions occurring in the operational design domain.\nSo far, the pipeline is a straightforward process. Due to the number of trainable parameters of an ANN architecture, the task performance must be evaluated using unseen data. This subset of data, normally 10%, is not used for any other step. It is exclusively for evaluation. If the evaluation with the unseen dataset does not fit the required performance, the design of the AI architecture needs to be refined, and the refined architecture must be trained again. This cycle of designing, training, and evaluating an AI architecture is very resource expensive.\nNext to the evaluation step, interactive HMI and UI are only standard for the first three steps of the pipeline. For the last three steps, HMI and UI are not yet common."
        },
        {
            "heading": "3 INTERACTIVE DESIGN OF AI ARCHITECTURE",
            "text": "Enabling the user to instantly distinguish which AI architectures do not have unproductive layers, i.e., layers that do not influence the network performance without training the network, is valuable. Using the receptive field analysis (RFA), a UI can be provided to highlight these layers in CNN immediately."
        },
        {
            "heading": "3.1 Receptive Field Analysis",
            "text": "The area on the 2D input that influences the output of a convolutional layer is called a receptive field. Thus the multiple ways of the receptive fields within a CNN architecture can be computed. Using statistical methods, Luo et al. [11] determine the effective receptive field size in each layer, which can be considered to resemble a Gaussian distribution. Since Luo et\nal. computation method depends on the model\u2019s state, this method only works after the initialization of the architecture. Based on the topology of the architecture and the kernel and stride sizes of each layer, Koutini et al. [12], Wang et al. [13], and Richter et al. [3] computational methods determine the receptive field size without initiation. The receptive field can generally be seen as a mathematical upper bound of theoretically extractable data features from the input. Thus, the receptive field is linked to the architecture\u2019s predictive quality, and computational efficiency [14].\nWhen a mismatch between the CNN architecture and the input data resolution occurs, the efficiency and predictive performance of the architecture is weak due to unproductive layers within the architecture. By avoiding unproductive layers, one can adjust the input data resolution to match the architecture to be between the minimum and maximum feasible input resolution; possible resolutions for common CNN architectures are in Tab. 1 of the appendix. The other more envisaged way of avoiding unproductive layers is adjusting the CNN architecture. By removing unproductive layers, the number of trainable parameters decreases so that the training data tend to generalize better to the desired task during the training. As a side effect, the footprint of the architecture decreases, and the effort, both for human and computer during training and inference of these optimized architectures usually decreases, too."
        },
        {
            "heading": "3.2 HMI for Identifying Unproductive Layers",
            "text": "Users can create graph visualizations of CNN architectures with the easy-to-use RFAToolbox [10], a python library. Using RFA, the toolbox marked all unproductive red and border layers with limited productivity orange, as illustrated in Fig. 3 (b). The intuitive color coding UI of the graph visualizations allows the user to optimize the architectures without training. The users should optimize their CNN architecture until only productive, i.e., non-color coded, and border layer exists. Covering the needs of experienced AI developers even better, a command line visualization, as shown in Fig. 4 , is the next development step but has not yet been implemented."
        },
        {
            "heading": "3.3 User Evaluation and Feedback",
            "text": "An essential part of HMI and UI design is user evaluation. For this purpose, 16 computer science, electrical engineering, and mechatronics students evaluated the graph visualizations of the RFA-toolbox. On average, these subjects are in their 2.8th year of study, 24.7 years old, and have common knowledge of AI. All subjects were guided to eight questions in a structured interview. After gathering the demographical information on the age, the field of study, the year of study, and the experience in AI, AI-based image classification on the ImageNet data set [15] was explained, ensuring the same knowledge level of the subjects. By showing the three different, non-color-coded ANN architectures of Fig. 3 (a), the subjects were asked, \"Which neural network solves the task most accurately?\", \"Which neural network solves the task most inaccurately?\" and \"Which neural network has the most parameters?\". After these first three questions, the intuitive color coding UI of the graph visualizations is introduced to the subjects. By showing the subjects the color-coded graph visualizations of the architectures, cf. Fig. 3 (b) following questions were asked \"Which neural network solves the task most accurately?\", \"Which neural network solves the task most inaccurately?\" and \"Which neural network would you try first?\". At the end of the interview, on a blank slide, the subjects were asked, \"How helpful would you find such a software tool?\" and \"Do you think such a UI already exists?\".\nAs depicted in Fig. 5 (a), without the UI, 68.8% of the subjects assumed that the 18 layer ResNet18 architecture solved the task most accurately. The general misbelief might support this conclusion that deeper ANN solves tasks always better, as the trend in state-of-the-art CNN architecture make belief. After introducing the UI 50% of the subjects selected the 17 layer network\u2014the correct answers which solved the image classification task most accurately. The cross-validation of this question, cf. Fig. 5 (b), also supports this results.\nThe question \"Which neural network has the most parameters?\" was answered correctly by 87.5% of the subjects. Only two subjects answered that the 11 layer architecture has the\nmost parameters. Of the important answers on which architecture should be trained first, cf. Fig. 6, 68.8% of the subjects answered the 11 layer architecture, which seems illogical since most subjects stated that the 17 layer architecture might be the most accurate one. After the interview, however, some subjects explained that the training of the smallest architecture is reasonable for evaluation due to the presence of computing power. The subjects argued that this smallest lightweight architecture might already fulfill the requirements of the desired tasks.\nThe question \"How helpful would you find such a software tool?\" all subjects answered that the RFA toolbox is quite helpful, and all subjects want to use this UI during the design of the AI architectures. However, only 50% of the subjects think, that such a UI does not exist. The overall feedback on the HMI for designing AI architecture is very good."
        },
        {
            "heading": "4 AI FOR CLOSE LOOP APPLICATIONS",
            "text": "Lightweight ANN architectures will empower AI-based applications to reach new fields of embedded applications like closed-loop control systems (CLCS) [16\u201318], wearable cognitive enhancement technology [19\u201321], and interactive health devices [22\u201324]. In addition, lightweight AI architectures are next to software design patterns [25], another aspect of energy optimization through software design. By introducing AI in CLCS, the functional safety of AI architectures has to be discussed. Until explainable AI is detailed delved, the safety dilemma of AI remains unsolved. Thus AI-based controller needs to counter the risks. Without HMI, two possible designs can guarantee the functional safety of AI-based controllers. As illustrated in Fig. 7 (a), one strategy is a fall-back path that disables the AI-based controller depending on, e.g., some threshold values. A second strategy is realized in Fig. 7 (b), where a hand-crafted CLCS provides the control values, and the AI-based controller fine-tunes these values in predefined boundaries.\nAnyhow, to improve functional safety, the number of trainable weights should be kept to a minimum by designing the ANN. HMI, like the RFA toolbox, will help the developers in doing so. Less trainable parameters reduce the complexity within the ANN and bring the applicability as well as the explainability of ANN architectures a step closer."
        },
        {
            "heading": "5 AI-IN-THE-LOOP",
            "text": "One often neglected solution is a minimal HMI for safeguarding the task execution in situations where the AI algorithm detects difficulties in inference. Merging the user-in-theloop [26, 27] concept with AI, AI will replace the user in the main loop. As illustrated in Fig. 8, the AI-in-the-loop concept is based on the idea that the AI will do the majority of the workload. As the domain expert, the user will teach and support the AI in overcoming the safety, security, and task issues that the AI architectures cannot realize reliably. By\nintroducing HMI in this step of the pipeline of applying AI, AI-based embedded applications can be available on the market sooner than fully automated AI-based applications.\nThe AI-in-the-loop combines AI\u2019s computational and human conceptual strengths by enabling HMI and UI during the AI\u2019s inferences. With its computational powers during inferences, the AI remains the working horse and primarily solves the task. If the AI is unsure whether it solves the task correctly or if the AI recognizes a safety or security-critical situation, it asks the user by using an appropriate HMI with an interactive UI. By enabling the development of intuitive HMI and UI, an AI architecture\u2019s trained high-dimensional task representation must be translated into a human-understandable problem description. First, UI methods, e.g., for visualizing as well as describing why the AI inferences the current output [28\u201330] already exist, but more research is needed to match the users\u2019 needs within the AI-in-the-loop concept.\nIn combination with an interactive design of AI architectures and removing unproductive layers for ensuring lightweight ANN, the development of minimal HMI supporting AI-inthe-loop, will enable the use of safe and reliable AI in the near further. Fig. 9 plots the users\u2019 workload needed for performing a complex task over the available of systems. Before reaching a full automatization, i.e. the users\u2019 workload do not longer exsist, it may be assumed that human AI cooperation in AI-in-the-loop-based systems will the upcoming logical development step. Until explainable AI is not detailed delved, AI-based applications have unknown blind spots."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "The pipeline of applying AI, as shown in Fig. 2, reveals three steps where Interactive HMI is not common yet. As introduced and evaluated in this work, RFA and other methods improve the step of designing lightweight AI architecture by using HMI. Lightweight AI architectures are the enabler for explainable and interactive AI-based applications, following the AI-in-the-loop concept. Instead of providing fully automated AI-based systems, one\nshould focus on cooperative AI architecture, which uses both the AI\u2019s computational power and human conceptual thinking.\nOne blind spot for interactive HMI is still the step of training AI architecture; nowadays, developers visualize the accuracy and loss of an architecture undertraining. These visualizations are then, next to defined thresholds used for stopping the training. By using, e.g., virtual reality, developers might be capable of walking through the architecture and seeing how the backpropagations shape the trainable parameters. Understanding how each backpropagation shapes the trainable parameters might enable the developers to design better architectures, e.g., that generalize from only a few data points or help developers to optimize the hyperparameter so that, e.g., the local minimum can be avoided.\nBy concluding this work, one should consider embedded AI-based applications with minimal HMI as the next step toward a fully automated AI-based system. Going step by step and using the AI-in-the-loop concept will make applications sooner available in the market. With the experiences gained, having these applications in the market, safe, secure, and reliable AI will be feasible."
        },
        {
            "heading": "APPENDIX A",
            "text": ""
        }
    ],
    "title": "AI-in-the-Loop",
    "year": 2023
}