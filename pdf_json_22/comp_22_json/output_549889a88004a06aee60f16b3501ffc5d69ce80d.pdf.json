{
    "abstractText": "Department of Chemical and Biomolecular Academy St., Newark 19716, Delaware, USA Department of Polymer Science and En Amherst, Amherst 01003, Massachusetts, US Department of Materials Science and Engine Hall, Newark 19716, Delaware, USA \u2020 Electronic supplementary information ( size and resolution, additional segme obtained in this study. See https://doi.org Cite this:Digital Discovery, 2022, 1, 816",
    "authors": [
        {
            "affiliations": [],
            "name": "Shizhao Lu"
        },
        {
            "affiliations": [],
            "name": "Brian Montz"
        },
        {
            "affiliations": [],
            "name": "Todd Emrick"
        },
        {
            "affiliations": [],
            "name": "Arthi Jayaraman *ac"
        }
    ],
    "id": "SP:afbd3a0119e704dd5c147924a67b70bd34b1ecc5",
    "references": [
        {
            "authors": [
                "E.D. Cubuk"
            ],
            "title": "Structure-property relationships from universal signatures of plasticity in disordered solids",
            "venue": "Science, 2017,",
            "year": 2017
        },
        {
            "authors": [
                "C. Huang",
                "X. Chen",
                "Z. Xue",
                "T. Wang"
            ],
            "title": "Effect of structure: A new insight into nanoparticle assemblies from inanimate to animate",
            "venue": "Sci. Adv.,",
            "year": 2020
        },
        {
            "authors": [
                "M. Ge",
                "F. Su",
                "Z. Zhao",
                "D. Su"
            ],
            "title": "Deep learning analysis on microscopic imaging in materials science, Mater",
            "venue": "Today Nano,",
            "year": 2020
        },
        {
            "authors": [
                "A. Baskaran"
            ],
            "title": "Adoption of Image-Driven Machine Learning for Microstructure Characterization and Materials Design: A Perspective, JOM",
            "year": 2021
        },
        {
            "authors": [
                "J.M. Ede"
            ],
            "title": "Deep learning in electron microscopy",
            "venue": "Mach. learn.: sci. technol.,",
            "year": 2021
        },
        {
            "authors": [
                "S.K. Melanthota"
            ],
            "title": "Deep learning-based image processing in optical microscopy",
            "venue": "Biophys. Rev.,",
            "year": 2022
        },
        {
            "authors": [
                "R. Jacobs"
            ],
            "title": "Deep learning object detection in materials science: Current state and future",
            "venue": "directions, Comput. Mater. Sci.,",
            "year": 2022
        },
        {
            "authors": [
                "A. Chowdhury",
                "E. Kautz",
                "B. Yener",
                "D. Lewis"
            ],
            "title": "Image driven machine learning methods for microstructure recognition, Comput",
            "venue": "Mater. Sci.,",
            "year": 2022
        },
        {
            "authors": [
                "M.H. Modarres"
            ],
            "title": "Neural network for nanoscience scanning electron microscope image recognition, Sci",
            "venue": "Rep., 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Q. Luo",
                "E.A. Holm",
                "C. Wang"
            ],
            "title": "A transfer learning approach for improved classi\ue103cation of carbon nanomaterials from TEM images",
            "venue": "Nanoscale Adv.,",
            "year": 2021
        },
        {
            "authors": [
                "S. Akers"
            ],
            "title": "Rapid and \ue104exible segmentation of electron microscopy data using few-shot machine learning, npj Comput",
            "venue": "Mater., 2021,",
            "year": 2021
        },
        {
            "authors": [
                "J. Madsen"
            ],
            "title": "A deep learning approach to identify local structures in atomic-resolution transmission electron microscopy",
            "venue": "images, Adv. Theory Simul., 2018,",
            "year": 2018
        },
        {
            "authors": [
                "W. Li",
                "K.G. Field",
                "D. Morgan"
            ],
            "title": "Automated defect analysis in electron microscopic images, npj Comput",
            "venue": "Mater., 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Han"
            ],
            "title": "A novel transfer learning for recognition of overlapping nano object, Neural",
            "venue": "Comput. Appl., 2022,",
            "year": 2022
        },
        {
            "authors": [
                "E.Z. Qu",
                "A.M. Jimenez",
                "S.K. Kumar",
                "K. Zhang"
            ],
            "title": "Quantifying Nanoparticle Assembly States in a Polymer Matrix through Deep Learning, Macromolecules",
            "year": 2021
        },
        {
            "authors": [
                "S.J. Yang"
            ],
            "title": "Assessing microscope image focus quality with deep learning",
            "venue": "BMC Bioinf., 2018,",
            "year": 2018
        },
        {
            "authors": [
                "C. Senaras",
                "M.K.K. Niazi",
                "G. Lozanski",
                "M.N. Gurcan"
            ],
            "title": "DeepFocus: detection of out-of-focus regions in whole slide digital images using deep learning",
            "venue": "PloS one,",
            "year": 2053
        },
        {
            "authors": [
                "W. Lee"
            ],
            "title": "Robust autofocusing for scanning electron microscopy based on a dual deep learning",
            "year": 2021
        },
        {
            "authors": [
                "O. Furat"
            ],
            "title": "Machine learning techniques for the segmentation of tomographic image data of functional materials, Front",
            "venue": "Mater., 2019,",
            "year": 2019
        },
        {
            "authors": [
                "C.K. Groschner",
                "C. Choi",
                "M.C. Scott"
            ],
            "title": "Machine learning pipeline for segmentation and defect identi\ue103cation from high-resolution transmission electron microscopy data, Microsc",
            "venue": "Microanal.,",
            "year": 2021
        },
        {
            "authors": [
                "R. Jacobs"
            ],
            "title": "Performance and limitations of deep learning semantic segmentation of multiple defects in transmission electron micrographs",
            "venue": "Cell Rep. Phys. Sci.,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Han"
            ],
            "title": "Center-environment feature models for materials image segmentation based on machine learning, Sci",
            "venue": "Rep., 2022,",
            "year": 2022
        },
        {
            "authors": [
                "X. Li"
            ],
            "title": "A transfer learning approach for microstructure reconstruction and structure-property predictions",
            "venue": "Sci. Rep., 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Z. Yang"
            ],
            "title": "Microstructural materials design via deep adversarial learning methodology",
            "venue": "J. Mech. Design,",
            "year": 2018
        },
        {
            "authors": [
                "Z.A. Kudyshev",
                "A.V. Kildishev",
                "A.V.M. Shalaev"
            ],
            "title": "Boltasseva, Machine-learning-assisted metasurface design for high-efficiency thermal emitter optimization, Appl",
            "venue": "Phys. Rev., 2020,",
            "year": 2020
        },
        {
            "authors": [
                "M. Weigert"
            ],
            "title": "Content-aware image restoration: pushing the limits of \ue104uorescence microscopy, Nat",
            "venue": "Methods, 2018,",
            "year": 2018
        },
        {
            "authors": [
                "H. Wang"
            ],
            "title": "Deep learning enables cross-modality superresolution in \ue104uorescence microscopy, Nat",
            "venue": "Methods, 2019,",
            "year": 2019
        },
        {
            "authors": [
                "C. Qiao"
            ],
            "title": "Evaluation and development of deep neural networks for image super-resolution in optical microscopy, Nat",
            "venue": "Methods, 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Y. Luo",
                "L. Huang",
                "Y. Rivenson",
                "A. Ozcan"
            ],
            "title": "Single-shot autofocusing of microscopy images using deep learning",
            "venue": "ACS Photonics,",
            "year": 2021
        },
        {
            "authors": [
                "B. Manifold",
                "E. Thomas",
                "A.T. Francis",
                "A.H. Hill",
                "D. Fu"
            ],
            "title": "Denoising of stimulated Raman scattering microscopy images via deep learning, Biomed",
            "venue": "Opt. Express,",
            "year": 2019
        },
        {
            "authors": [
                "R.F. Laine",
                "G. Jacquemet",
                "A. Krull"
            ],
            "title": "Imaging in focus: an introduction to denoising bioimages in the era of deep learning",
            "venue": "Int. J. Biochem. Cell Biol.,",
            "year": 2021
        },
        {
            "authors": [
                "J. Yosinski",
                "J. Clune",
                "Y. Bengio",
                "H. Lipson"
            ],
            "title": "How transferable are features in deep neural networks",
            "venue": "Adv. Neural Inf. Process Syst.,",
            "year": 2014
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "ImageNet classi\ue103cation with deep convolutional neural networks",
            "venue": "Commun. ACM,",
            "year": 2017
        },
        {
            "authors": [
                "D.S. Kermany"
            ],
            "title": "Identifying medical diagnoses and treatable diseases by image-based deep learning, Cell",
            "year": 2018
        },
        {
            "authors": [
                "D. Shen",
                "G. Wu",
                "H.-I. Suk"
            ],
            "title": "Deep learning in medical image analysis, Annu",
            "venue": "Rev. Biomed. Eng., 2017,",
            "year": 2017
        },
        {
            "authors": [
                "39 L. von Chamier"
            ],
            "title": "Democratising deep learning for microscopy with ZeroCostDL4Mic",
            "venue": "Nat. Commun., 2021,",
            "year": 2021
        },
        {
            "authors": [
                "C.G. Northcutt",
                "A. Athalye",
                "J. Mueller"
            ],
            "title": "Pervasive label errors in test sets destabilize machine learning benchmarks",
            "venue": "arXiv preprint arXiv:2103.14749,",
            "year": 2021
        },
        {
            "authors": [
                "V. Cheplygina",
                "M. de Bruijne",
                "J.P. Pluim"
            ],
            "title": "Not-sosupervised: a survey of semi-supervised, multi-instance, and transfer learning in medical image analysis, Med",
            "venue": "Image Anal., 2019,",
            "year": 2019
        },
        {
            "authors": [
                "M. Peikari",
                "S. Salama",
                "S. Nofech-Mozes",
                "A.L. Martel"
            ],
            "title": "A cluster-then-label semi-supervised learning approach for pathology image classi\ue103cation",
            "venue": "Sci. Rep., 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Pu"
            ],
            "title": "Variational autoencoder for deep learning of images, labels and captions, Adv",
            "venue": "Neural Inf. Process Syst.,",
            "year": 2016
        },
        {
            "authors": [
                "M. Chen",
                "X. Shi",
                "Y. Zhang",
                "D. Wu",
                "M. Guizani"
            ],
            "title": "Deep feature learning for medical image analysis with convolutional autoencoder neural network",
            "venue": "IEEE Trans. Big Data,",
            "year": 2017
        },
        {
            "authors": [
                "X. Liu"
            ],
            "title": "Self-supervised learning: Generative or contrastive",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "O. Ciga",
                "T. Xu",
                "A.L. Martel"
            ],
            "title": "Self supervised contrastive learning for digital histopathology",
            "venue": "Mach. Learn., 2022,",
            "year": 2022
        },
        {
            "authors": [
                "D.J. Matuszewski",
                "I.-M"
            ],
            "title": "Sintorn, TEM virus images: Benchmark dataset and deep learning classi\ue103cation",
            "venue": "Comput. Methods Programs Biomed.,",
            "year": 2021
        },
        {
            "authors": [
                "Y.L. Sun"
            ],
            "title": "Conductive Composite Materials Fabricated from Microbially Produced Protein Nanowires",
            "year": 2018
        },
        {
            "authors": [
                "X. Liu"
            ],
            "title": "Power generation from ambient humidity using protein nanowires",
            "venue": "Nature,",
            "year": 2020
        },
        {
            "authors": [
                "A.F. Smith"
            ],
            "title": "Bioelectronic protein nanowire sensors for ammonia detection",
            "venue": "Nano Res.,",
            "year": 2020
        },
        {
            "authors": [
                "G. Reguera"
            ],
            "title": "Extracellular electron transfer via microbial nanowires",
            "venue": "Nature,",
            "year": 2005
        },
        {
            "authors": [
                "R. Conrad",
                "K. Narayan"
            ],
            "title": "CEM500K, a large-scale heterogeneous unlabeled cellular electron microscopy image dataset for deep learning, Elife",
            "year": 2021
        },
        {
            "authors": [
                "C. Karaba\u011f",
                "J. Verhoeven",
                "C.C.N.R. Miller"
            ],
            "title": "ReyesAldasoro, Texture segmentation: An objective comparison between \ue103ve traditional algorithms and a deep-learning",
            "venue": "UNet architecture, Appl. Sci., 2019,",
            "year": 2019
        },
        {
            "authors": [
                "L. Yao",
                "Z. Ou",
                "B. Luo",
                "C. Xu",
                "Q. Chen"
            ],
            "title": "Machine learning to reveal nanoparticle dynamics from liquid-phase TEM videos",
            "venue": "ACS Cent. Sci.,",
            "year": 2020
        },
        {
            "authors": [
                "L. Tadiello"
            ],
            "title": "The \ue103ller\u2013rubber interface in styrene butadiene nanocomposites with anisotropic silica particles: morphology and dynamic properties, So\ue09d",
            "year": 2015
        },
        {
            "authors": [
                "L. Van der Maaten",
                "G. Hinton"
            ],
            "title": "Visualizing data using tSNE",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2022
        },
        {
            "authors": [
                "X. Wang"
            ],
            "title": "AutoDetect-mNP: an unsupervised machine learning algorithm for automated analysis of transmission electron microscope images of metal nanoparticles",
            "venue": "JACS Au,",
            "year": 2021
        },
        {
            "authors": [
                "G. Laverty"
            ],
            "title": "Ultrashort cationic naphthalene-derived self-assembled peptides as antimicrobial nanomaterials",
            "venue": "Biomacromolecules,",
            "year": 2014
        },
        {
            "authors": [
                "Y.-L. Sun"
            ],
            "title": "Solvent-induced assembly of microbial protein nanowires into superstructured bundles, Biomacromolecules",
            "year": 2021
        },
        {
            "authors": [
                "D. Birant",
                "A.S.T.-D.B.S.C.A.N. Kut"
            ],
            "title": "An algorithm for clustering spatial\u2013temporal data, Data Knowl",
            "year": 2007
        },
        {
            "authors": [
                "F. Pedregosa"
            ],
            "title": "Scikit-learn: Machine learning in Python",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2011
        },
        {
            "authors": [
                "V. Srinivasan"
            ],
            "title": "On the Robustness of Pretraining and Self-Supervision for a Deep Learning-based Analysis of Diabetic Retinopathy, arXiv preprint arXiv:2106.13497 2021",
            "venue": "Digital Discovery,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Digital Discovery\nPAPER\nO pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n0 Se\npt em\nbe r\n20 22\n. D ow\nnl oa\nde d\non 1\n/2 5/\n20 24\n1 0:\n07 :1\n1 PM . T hi s ar tic le is li ce ns ed u nd er a C re at iv e C om m on s A ttr ib ut io nN on C om m er ci al 3 .0\nU np\nor te\nd L\nic en\nce .\nView Article Online View Journal | View Issue\nSemi-supervised\naDepartment of Chemical and Biomolecular Academy St., Newark 19716, Delaware, USA bDepartment of Polymer Science and En Amherst, Amherst 01003, Massachusetts, US cDepartment of Materials Science and Engine Hall, Newark 19716, Delaware, USA \u2020 Electronic supplementary information ( size and resolution, additional segme obtained in this study. See https://doi.org\nCite this:Digital Discovery, 2022, 1, 816\nReceived 23rd June 2022 Accepted 17th September 2022\nDOI: 10.1039/d2dd00066k\nrsc.li/digitaldiscovery\n816 | Digital Discovery, 2022, 1, 816\u20138\nmachine learning workflow for analysis of nanowire morphologies from transmission electron microscopy images\u2020\nShizhao Lu, a Brian Montz,b Todd Emrick b and Arthi Jayaraman *ac\nIn the field of materials science, microscopy is the first and often only accessible method for structural\ncharacterization. There is a growing interest in the development of machine learning methods that can\nautomate the analysis and interpretation of microscopy images. Typically training of machine learning\nmodels requires large numbers of images with associated structural labels, however, manual labeling of\nimages requires domain knowledge and is prone to human error and subjectivity. To overcome these\nlimitations, we present a semi-supervised transfer learning approach that uses a small number of labeled\nmicroscopy images for training and performs as effectively as methods trained on significantly larger\nimage datasets. Specifically, we train an image encoder with unlabeled images using self-supervised\nlearning methods and use that encoder for transfer learning of different downstream image tasks\n(classification and segmentation) with a minimal number of labeled images for training. We test the\ntransfer learning ability of two self-supervised learning methods: SimCLR and Barlow-Twins on\ntransmission electron microscopy (TEM) images. We demonstrate in detail how this machine learning\nworkflow applied to TEM images of protein nanowires enables automated classification of nanowire\nmorphologies (e.g., single nanowires, nanowire bundles, phase separated) as well as segmentation tasks\nthat can serve as groundwork for quantification of nanowire domain sizes and shape analysis. We also\nextend the application of the machine learning workflow to classification of nanoparticle morphologies\nand identification of different type of viruses from TEM images."
        },
        {
            "heading": "Introduction",
            "text": "Researchers working with nanomaterials nd that the materials' structural characterization is a key step in the discovery of novel functional materials.1,2 For most researchers, microscopy imaging is the rst and, in many cases, only accessible means to obtain structural information. Depending on the length scale of interest, commonly used techniques include optical microscopy, transmission electron microscopy (TEM), scanning electron microscopy (SEM), and atomic force microscopy (AFM). Regardless of the technique, each microscopy image is associated with the chemical composition of the material, region/ section of the material that is imaged, and the processing conditions for the imaging. The resulting images may resemble a heat map of intensity highlighting object(s) owing to the\nEngineering, University of Delaware, 150 . E-mail: arthij@udel.edu gineering, University of Massachusetts A ering, University of Delaware, 201 DuPont\nESI) available: t-SNE gures at original ntation model performance results /10.1039/d2dd00066k\n33\nselective staining of some species/sections in the sample.3 Analyzing these microscopy images requires domain knowledge to interpret (e.g., classify images with morphology labels) and/or to detect nuances in different images (e.g., identication of defects, changes in intensity, etc.) as the composition or the focus of the image or the processing condition is changed. Thus, manual interpretation is oen time-consuming, laborintensive, and prone to human subjectivity in interpretation. Therefore, machine learning (ML) has become a valuable tool that can replace time-consuming and subjective manual interpretation of microscopy images with automated, objective, and fast analysis.4\u20138\nRecent advances in deep learning have led to a surge of applications in electron microscopy image analysis for a diverse set of tasks in two main categories: discriminative and generative. Discriminative tasks are tasks like morphology/phase classication,9\u201312 particle/defect detection,13\u201316 image quality assessment,17\u201319 and segmentation20\u201325 where the objective is quantied by how well the model can distinguish (1) between images or (2) between objects and their background. Generative tasks include microstructure reconstruction,26\u201328 super resolution,29\u201331 autofocus32 and denoising33,34 where the objective is generation of images with certain desired traits.\n\u00a9 2022 The Author(s). Published by the Royal Society of Chemistry\nO pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n0 Se\npt em\nbe r\n20 22\n. D ow\nnl oa\nde d\non 1\n/2 5/\n20 24\n1 0:\n07 :1\n1 PM . T hi s ar tic le is li ce ns ed u nd er a C re at iv e C om m on s A ttr ib ut io nN on C om m er ci al 3 .0\nU np\nor te\nd L\nic en\nce .\nDevelopment of modern MLmodels benets in performance from the procurement of big datasets related to a specic task. To bypass the need to collect large training data and reduce the time needed to train the ML model from scratch on that large data, researchers use \u2018transfer learning\u2019 techniques.35 Transfer learning involves leveraging the knowledge of a model previously trained using large training datasets to create a newmodel for another related task. For example, a model that has been trained on ImageNet,36 a large dataset of 1.2 million photographic images of macroscopic objects, can be transferred to learn how to analyze images in another more specic domain [e.g., medical image analysis37,38 and electron microscopy image analysis in material science4\u20138]. The success of transfer learning in the eld of image analysis has paved the way for accessibility to pretrained image learning models for the general public without requiring large computational resources or big data to train from scratch.39\nTransfer learning for microscopy image analysis tasks has traditionally relied on convolutional neural network (CNN) models40 which convert input images into feature maps that hold information for image classication (e.g., assigning a morphology label to a microscopy image) and detection of objects in the image (e.g., identication of a nanoparticle aggregate or domain). In transfer learning, the weights of some layers of the pre-trained CNN are kept as constants and only the weights for the outermost layer are retrained with images for the specic task at hand. In most implementations of transfer learning, the microscopy image dataset for the specic task has to be labeled (i.e., supervised learning) before training the outermost layer. However, CNN models trained on one type of supervised tasks (classication, segmentation, or object detection) can typically only be transferred to the same supervised task for another image dataset which limits the generalized applicability of transfer learning. Further, the typical size of image datasets needed for training ranges from thousands to hundreds of thousands of images, even with transferred models, and the labeling of these large set of images is challenging and prone to error, with a recent study noting (on average) 3.3% labeling errors in large open source datasets.41\nTo overcome limitations of labeling, semi-supervised training workow is another option which typically consists of an unsupervised training of feature encoder requiring no manual labeling and a supervised training of specic downstream task model requiring manual labels.42,43 Chowdhury et al. developed a semi-supervised approach consisting of a feature extractor, a feature selector, and a classier to classify different dendritic microstructures.9 Peikari et al. developed a cluster-then-label semi-supervised approach for classifying pathology images.44 A school of generative architectures called autoencoders have also been used in obtaining pretrained feature maps of images.45,46 An autoencoder architecture involves training of an encoder to condense the information from the original image to a low-dimensional feature map, and a decoder that tries to reconstruct the original image from the feature map. More recently, self-supervised learning of images has emerged as a new form of label-free, unsupervised training.47 In a recent review, Liu et al. attribute the more\n\u00a9 2022 The Author(s). Published by the Royal Society of Chemistry\ncompetitive status of self-supervised learning compared to autoencoders in classication scenarios to the more closely aligned learning goal of self-supervised learning modules to that of vision tasks targeting high-level abstraction such as classication and object identication.48 Through selfsupervised training, the ML model learns a representation of an image by maximizing the similarity between two different transformed versions of the same image. While supervised model training is assisted by the labels associated with each image, self-supervised model training does not rely on labels and learns from the underlying features of the images. The performance of self-supervised transfer learning has been shown to be comparable with supervised transfer learning in big data medical image classication.49,50 For example, Azizi et al. have achieved 70% classication accuracy on dermatology images (using 450 000 images for training) with selfsupervised transfer learning49 and Ciga et al. have achieved 78% classication accuracy on a diverse set of histopathology images (using 40 000 images for training) with self-supervised transfer learning.50 In contrast to the medical imaging eld which traditionally has large data sets, researchers in the so materials domain handle much smaller datasets and have a more diverse range of image analysis tasks. Thus, for selfsupervised transfer learning to be accessible to researchers in somaterials, it has to be adapted to small dataset sizes and be able to handle multiple tasks (e.g., classication and segmentation).\nTransfer learning from CNNs trained with supervised methods has been utilized in nanomaterial classication task in recent years. Moderres et al. have used transfer learning to classify SEM images belonging to different nanomaterial subcategories like particles, patterned surfaces, nanowires, etc.10 Having an unbalanced dataset, they observed higher accuracies for categories that have fewer images. They made the comment that some categories that have fewer images performed better because the features in those categories were distinct and sufficiently clear to be learned by the network. While some categories with higher number of images suffered from having indistinct features. Their dataset size was 18 000 in total with smallest category containing 150 images, and largest category containing 4000 images. Luo et al. used transfer learning to classify carbon nanotube or ber morphologies on an image dataset containing 600 images per morphology class.11 They were able to achieve 91% average accuracy on a four-class dataset, and 85% average accuracy on an eight-class dataset. Matuszewski and Sintorn recently compared the accuracy of different CNN architectures (with transferred weights or trained from scratch) on identifying various viruses from TEM images.51\nIn this article, we present an automated, label-efficient transfer learning workow incorporating self-supervised pretraining that aims to classify nanomaterial morphologies in microscopy images with high accuracy aer training on only a handful of carefully labeled microscopy images. We focus on a semi-supervised transfer learning workow to perform automated classication and segmentation of TEM images taken from one class of nano materials \u2013 protein and peptide\nDigital Discovery, 2022, 1, 816\u2013833 | 817\nO pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n0 Se\npt em\nbe r\n20 22\n. D ow\nnl oa\nde d\non 1\n/2 5/\n20 24\n1 0:\n07 :1\n1 PM . T hi s ar tic le is li ce ns ed u nd er a C re at iv e C om m on s A ttr ib ut io nN on C om m er ci al 3 .0\nU np\nor te\nd L\nic en\nce .\nnanowires \u2013 which are used in a wide variety of applications including exible electronics,52 energy harvest,53 and chemical sensing.54 In all these applications, the nanowire morphologies (e.g., dispersed, aggregated, percolated, phase separated) dictate their performance. In this work, we use TEM images from assembled, synthetically engineered, peptide nanowires and biologically derived (from Geobacter sulfurreducens) protein nanowires55 to demonstrate a semi-supervised transfer learning workow that shows high accuracy in classication and segmentation of these images with <1000 generic unlabeled training images and <10 task-specic labeled images per morphology class. We also demonstrate the broader applicability of our machine learning workow by applying it to two additional TEM image analysis tasks \u2013 for classication of nanoparticle morphologies and for identication of virus types from their TEM images."
        },
        {
            "heading": "Results and discussion",
            "text": ""
        },
        {
            "heading": "Semi-supervised machine learning workow",
            "text": "We illustrate the conceptual workow of semi-supervised transfer learning for microscopy images in Fig. 1. First, a generic image learning model, an encoder, undergoes selfsupervised training (i.e., no labels required during training) on a dataset of generic microscopy images called CEM500k,56 an open-access electron microscopy image dataset curated from various imaging modalities characterizing cellular or biomaterial structures by Conrad and Narayan. We transfer the trained encoder to transform images into feature maps (i.e., distilled and encoded representations of images) for training of downstream tasks. We demonstrate the semi-supervised machine learning workow with a detailed example of transfer learning of nanowire morphologies from generic TEM images. We start by training the encoder with self-supervisedmethods on generic TEM images (Fig. 1A). We implement two self-supervised training methods: SimCLR47 and Barlow-Twins.57 Both methods start by taking a batch of images and generating two randomly augmented images for each image by performing random color/hue/contrast changes, and randomly crop a portion of the image. The augmented images are then turned into feature maps by an encoder with ResNet50 (ref. 58) architecture. The feature maps are input into a projector with three layers of fully connected neurons to generate projections of each image. The projections are then used to calculate and minimize the loss function to train both the encoder and the projector. Through maximizing the similarity between two augmented images of the same image, the encoder is trained to produce feature maps that can represent the images more accurately. The difference between the two methods \u2013 SimCLR47 and Barlow-Twins57- is in the loss function. The loss function of SimCLR method aims to maximize the calculated cosine similarity of projections from the \u201ctrue\u201d pairs of augmented images from the same image and minimize that of the \u201cfalse\u201d pairs of augmented images from different images. The loss function of SimCLRmethod has dependence on the batch size and contrast between images; larger batch size and higher contrast theoretically gives higher ability of discerning \u201ctrue\u201d from \u201cfalse\u201d pairs.\n818 | Digital Discovery, 2022, 1, 816\u2013833\nThe loss function of Barlow-Twins method aims to minimize the redundancy in the representation of the projection by tuning the cross-correlation matrix of projections from the same image to be an identity matrix. The equations of the two loss functions are presented in the methods section with amore detailed explanation. The trained encoder is then transferred to learn the protein/peptide nanowire morphologies (Fig. 1B). For the classication task, we use the simplest linear classier consisting of four neurons equivalent to the number of morphology classes to classify the feature maps. For the image segmentation task, we use U-Net,59 an established deep learning architecture that has been shown to outperform traditional segmentation methods.23,60,61 The original U-Net architecture consists of an encoder and a decoder trained to create accurate segmentation of input images. Instead of the original U-Net architecture, we use our trained encoder with trained weights to establish skip connections between our encoder and a decoder with random initialized weights."
        },
        {
            "heading": "Peptide/protein nanowires \u2013 morphology classication",
            "text": "Protein/peptide nanowires exhibit one of four morphologies when dispersed in solvent \u2013 singular (i.e., isolated nanowire), dispersed (i.e., isolated collection of multiple nanowires), network (i.e., percolated nanowires), and bundle morphologies. Materials with dispersed nanowires are desired for mechanical reinforcement,62 while materials with network morphologies are desired for improving conductivity.52 The singular, dispersed, and network morphologies in this work arise from assembly of synthetic oligopeptides shown in Fig. 2A; the bundle morphologies represent aggregates of protein nanowires harvested from Geobacter sulfurreducens. 100 images from each morphology are employed (Fig. 2B). The magnication of the morphology images varies from image to image, but the length scales are on the same order of magnitude as indicated from the scale bars in Fig. 1B. Because the interest of our study is the type of morphology rather than the length scale of the morphology, we do not include the scale bars in the images for training the machine learning models. Due to differences in the peptide/protein nanowire chemistry, solvent condition and magnication, the objectbackground contrast in each morphology image is different. As the dispersed and network morphologies are harder to visually distinguish, we manually label the nanowires in the images through Microscopy Image Browser (MIB).63 This manual labeling serves two purposes: (1) to provide pixel-level quantication of percolation (in network) or lack thereof (in dispersed) and (2) to provide ground truth labels of nanowires for the segmentation task. A percolation analysis of the clusters of manually labeled nanowires distinguishes the networks (percolated) and dispersed (not percolated) nanowire morphologies. Except for the state-of-the-art (SotA) encoder which we obtained as a published open-access encoder trained with SimCLR method on the full ImageNet dataset (1.26 million images), other models are trained with optimized hyperparameters detailed in the method section. To show whether transferring learned weights from a domain-specic dataset gives better model performance, we trained the two self-supervised encoders on 832\n\u00a9 2022 The Author(s). Published by the Royal Society of Chemistry\nFig. 1 Machine learning workflow for classification and segmentation of microscopy images. Two-step generalized semi-supervised machine learning workflow for microscopy image learning. An image encoder is first trained on generic microscopy images56 (permission from eLife Sciences Publications, Ltd, UK under CC-BY license) without supervision, i.e., self-supervised. Then, the self-supervised image encoder is transferred to convert task-specific microscopy images into feature maps which are used to train multiple models for downstream tasks (e.g., classification, segmentation) (A) for each image in the generic TEM image dataset, two randomly augmented images are generated. To maximize the similarity between the two augmented images, an encoder (ResNet50 (ref. 58)), followed by a projector made of three fully connected neural layers, are trained. (B) The encoder is transferred as is for the task-specific image encoding (i.e., to convert TEM images of nanowire morphologies to featuremaps) followed by supervised, label-efficient training of classifier and decoder for downstream tasks \u2013 classification and segmentation of TEM images.\nO pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n0 Se\npt em\nbe r\n20 22\n. D ow\nnl oa\nde d\non 1\n/2 5/\n20 24\n1 0:\n07 :1\n1 PM . T hi s ar tic le is li ce ns ed u nd er a C re at iv e C om m on s A ttr ib ut io nN on C om m er ci al 3 .0\nU np\nor te\nd L\nic en\nce .\ngeneric TEM images or 832 generic everyday photographic images from ImageNet, both at resolution 224 224. We report the classication accuracies from the linear classier on the feature maps of the test sets (Fig. 2C). The classication accuracy is dened as a ratio of the number of correct morphology class predictions e.g., an image of dispersed morphology predicted as dispersed morphology, over the total number of the test cases. When trained with generic TEM images, the Barlow-Twins method outperforms SimCLR method. When trained with Barlow-Twins method, transferring from domain-specic images, i.e., TEM images, brings higher performance than transferring from everyday images. However, when trained with SimCLR method, transferring from domain-specic images underperforms transferring from images of other domains. We\n\u00a9 2022 The Author(s). Published by the Royal Society of Chemistry\nbelieve that SimCLR performs worse when trained on generic TEM images due to the reduced contrast in generic TEM images compared to that in ImageNet images. Strikingly, feature maps obtained from the Barlow-Twins-TEM encoder obtain >90% classication accuracy when trained with just 8 labeled images per class. With more numbers of labeled images, feature maps obtained from the Barlow-Twins-TEM encoder achieve comparable classication accuracy to that of feature maps obtained from the SotA encoder.\nNanowire morphology classication \u2013 one-shot learning\nAs we observe large uctuations in the accuracy of linear classiers trained with only one labeled image per class (i.e., one-\nDigital Discovery, 2022, 1, 816\u2013833 | 819\nFig. 2 Nanowire chemical composition, TEM images of nanowiremorphologies, and performance of the trainedmodel for classification task. (A) N-[2-(1-Naphthalenyl)acetyl]-L-phenylalanyl-L-phenylalanyl-L-lysyl-L-lysine (NapFFKK) oligopeptide structure that self-assembles into singular, dispersed, and network nanowire morphologies from water and organic solvents. (B) Representative TEM images of the four types of nanowire morphologies labeled in the figure; apart from the three oligopeptide nanowire morphologies, the bundle morphology is obtained from pilA protein nanowires (amino acid sequence of pilA: FTLIELLIVVAIIGILAAIAIPQFSAYRVKAYNSAASSDLRNLKTALESAFADDQTYPPES) harvested biologically from Geobacter sulfurreducens and assembled in organic solvents. (C) Classification accuracy of the trained downstream linear classifiers as a function of number of labeled images used during training of the downstream task. Legend denotes the self-supervision method and the generic image dataset used to train the encoder. Sample size is 100 for each boxplot. Notch of the boxplots indicates 95% confidence interval around the median.\n820 | Digital Discovery, 2022, 1, 816\u2013833 \u00a9 2022 The Author(s). Published by the Royal Society of Chemistry\nO pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n0 Se\npt em\nbe r\n20 22\n. D ow\nnl oa\nde d\non 1\n/2 5/\n20 24\n1 0:\n07 :1\n1 PM . T hi s ar tic le is li ce ns ed u nd er a C re at iv e C om m on s A ttr ib ut io nN on C om m er ci al 3 .0\nU np\nor te\nd L\nic en\nce .\nFig. 3 Knowledge of the underlying distribution of the images can help determine \u201cgood anchor\u201d images for high accuracy one-shot learning. (A and B) \u201cgood anchor\u201d images i.e., a set of labeled images from each morphology, which when used for training give high classification accuracy. \u201cBad anchor\u201d images which when used for training give low classification accuracy. t-SNE64 representations of the test set colored by their true labels and by their predicted labels with \u201cgood anchor\u201d images as training set (C and G) and \u201cbad anchor\u201d images as training sets (F and J). Images of the t-SNE plots of (part C, G, F, and J in original size and resolution) are provided in the ESI as Fig. S1\u2013S4.\u2020 The image count distribution with nanowire pixel density obtained from the manual nanowire labels for the dispersed and network morphology images in the test set with \u201cgood anchors\u201d (D) and \u201cbad anchors\u201d (E) images as training sets, respectively. Solid lines are positions of the two \u201canchors\u201d, and dashed lines are positions of the twomedians. Test set size is 20 for both dispersed and networkmorphology. (H) The prediction accuracies with different relative positions of the two \u201canchors\u201d to the median of the dispersed and network images in the test set. For the 100 samples obtained in Fig. 2C for Barlow-Twins-TEM, 25 resulted on the opposite side, 75 resulted on the same side. (I) The sum of the absolute distance between the anchor and median of the two morphologies with different relative positions of two \u201canchors\u201d to the median of the dispersed and network images in the test set.\nO pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n0 Se\npt em\nbe r\n20 22\n. D ow\nnl oa\nde d\non 1\n/2 5/\n20 24\n1 0:\n07 :1\n1 PM . T hi s ar tic le is li ce ns ed u nd er a C re at iv e C om m on s A ttr ib ut io nN on C om m er ci al 3 .0\nU np\nor te\nd L\nic en\nce .\nshot learning), we want to understand how the selected labeled images or the \u201canchor\u201d images affect accuracy. Using feature maps obtained from the Barlow-Twins-TEM encoder, we show one example of \u201cgood anchors\u201d and \u201cbad anchors\u201d each chosen posteriorly based on the highest and lowest accuracies (Fig. 3A\u2013 B). We use t-distributed Stochastic Neighbor Embedding (tSNE)64 to visualize the feature maps of the test images projected in 2-dimensional space. From the t-SNE plots, we see that while there are few misclassications between the dispersed and network morphologies when the linear classier is trained on \u201cgood anchors\u201d (Fig. 3C and G), most images in dispersed morphology are misclassied as network morphology when the linear classier is trained on \u201cbad anchors\u201d (Fig. 3F and J). To explain the visible difference in the performance of linear classiers trained on different \u201canchors\u201d, we look at the distribution of the nanowire pixel density, i.e., percentage of \u201cnanowire pixels\u201d over all pixels, of the ground truth (i.e., manually labeled images with nanowire pixels and background pixels) for test images in dispersed and network morphologies. The\n\u00a9 2022 The Author(s). Published by the Royal Society of Chemistry\nnanowire pixel density of the two anchor images is on the opposite sides of that of the respective median of the two morphologies for \u201cgood anchors\u201d (Fig. 3D), but on the same sides for \u201cbad anchors\u201d (Fig. 3E). We also show the statistics of all 100 sets of anchors and nd that the accuracy of linear classiers trained on \u201copposite side anchors\u201d is statistically higher than that trained on \u201csame side anchors\u201d (Fig. 3H). We conclude that the \u201copposite side anchors\u201d in our study are better approximates of the medians of the test set than \u201csame side anchors\u201d for having smaller absolute (anchor-to-median) distance as shown in (Fig. 3I), thereby leading to better accuracies."
        },
        {
            "heading": "Nanowire segmentation",
            "text": "Next, we tackle the task of segmentation of nanowires for the dispersed and percolated morphologies. We calculate both the Dice score (eqn (1)) and the Intersection-over-Union (IoU) score (eqn (2)) for each prediction. For assessing the performance of segmentation models, pixel-level classication is the basis for\nDigital Discovery, 2022, 1, 816\u2013833 | 821\nFig. 4 Nanowire segmentation task performance. (A) Original image, manually labeled nanowires, predicted nanowires, and segmentation performance Dice scores and Intersection-over-Union (IoU) scores at five input image resolutions. (B) Dice scores for the nanowire segmentation task. Legend indicates the resolution of the input images. Sample size is 200 for each boxplot. Notch of the boxplots indicates 95% confidence interval around the median.\nO pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n0 Se\npt em\nbe r\n20 22\n. D ow\nnl oa\nde d\non 1\n/2 5/\n20 24\n1 0:\n07 :1\n1 PM . T hi s ar tic le is li ce ns ed u nd er a C re at iv e C om m on s A ttr ib ut io nN on C om m er ci al 3 .0\nU np\nor te\nd L\nic en\nce .\nthe two segmentation scores. On a pixel level, nanowire pixels predicted as nanowire pixels are regarded as true positives (TP), nanowire pixels predicted as background pixels are regarded as false positives (FP), background pixels predicted as background pixels are regarded as true negatives (TN), background\n822 | Digital Discovery, 2022, 1, 816\u2013833\npixels predicted as nanowire pixels are regarded as false negatives (FN).\nThe Dice score is the ratio of two times the intersection of the predicted and the true nanowire pixels over the sum of the number of the predicted and the true nanowire pixels. Dice score is calculated as:\n\u00a9 2022 The Author(s). Published by the Royal Society of Chemistry\nO pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n0 Se\npt em\nbe r\n20 22\n. D ow\nnl oa\nde d\non 1\n/2 5/\n20 24\n1 0:\n07 :1\n1 PM . T hi s ar tic le is li ce ns ed u nd er a C re at iv e C om m on s A ttr ib ut io nN on C om m er ci al 3 .0\nU np\nor te\nd L\nic en\nce .\nDice \u00bc 2TP 2TP\u00fe FP\u00fe FN (1)\nThe Intersection-over-Union (IoU) score is the ratio of the intersection of predicted and true nanowire pixels over the union of the predicted and the true nanowire pixels. The IoU score is calculated as:\nIoU \u00bc TP TP\u00fe FP\u00fe FN (2)\nFor any image, the two metrics are always positively correlated, i.e., if model A is better than B under one metric, it is also better than model B under the other metric.\nOur U-Net model with transferred encoder (trained on resolution of 512 512 unlabeled generic TEM images with the Barlow-Twins method) works well with images of resolutions up to 1024 1024 (Fig. 4A). For segmentation, images containing nanowires present a more difficult problem compared to images with isolated small nanoparticles due to larger intersection area between the nanowires and the background. Of the ve input resolutions, our U-net model trained with images of\nFig. 5 Morphology classification performance on mNP dataset. Represe dispersed nanoparticles, (B) separate clusters, (C) percolating cluster from and DOI: https://doi.org/10.6078/D1S12H. (D) Classification accuracy o labeled images used during training of the downstream task. Legend used to train the encoder. Sample size is 100 for each boxplot. Notch o\n\u00a9 2022 The Author(s). Published by the Royal Society of Chemistry\nresolution 224 underperforms the higher resolution images likely due to poor contrast when the images are resized to such low resolution. We observe a plateau in the Dice score from 8 to 80 labeled images per class and a drop-off from having 8 down to 4 labeled images per class (Fig. 4B). With transferred encoder, our Unet model can achieve good performance (median Dice score > 0.70) with just 8 labeled images per class for training, less than half of the number of test images (20 per class). IoU scores follow the same qualitative trend as Dice scores; the values of IoU scores are always smaller than Dice scores (Fig. S5\u2020). Encoder trained with unlabeled images of different resolutions give statistically similar Dice and IoU scores (Figs. S6 and S7\u2020). With the segmentation of nanowires as an example, we show that trained encoder can capture not only global information that was important for image-level tasks such as morphology classication but also local information relevant to pixel-level tasks such as segmentation.\nBroader applicability of the machine learning workow\nNext, we test the generalizability of our machine learning workow on classication of nanoparticle morphologies. We use TEM images in the AutoDetect-mNP dataset65 which was\nntative TEM images of the three metal nanoparticle morphologies (A)\nthe AutoDetect-mNP dataset, DOI: https://doi.org/10.6078/D1WT44\nf the trained downstream linear classifiers as a function of number of\ndenotes the self-supervision method and the generic image dataset\nf the boxplots indicates 95% confidence interval around the median.\nDigital Discovery, 2022, 1, 816\u2013833 | 823\nO pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n0 Se\npt em\nbe r\n20 22\n. D ow\nnl oa\nde d\non 1\n/2 5/\n20 24\n1 0:\n07 :1\n1 PM . T hi s ar tic le is li ce ns ed u nd er a C re at iv e C om m on s A ttr ib ut io nN on C om m er ci al 3 .0\nU np\nor te\nd L\nic en\nce .\noriginally used for shape analysis of gold metal nanoparticles. Here we use those images for a new task \u2013 to classify the morphologies that the assembled nanoparticles adopt\nFig. 6 Virus identification performance on TEM virus dataset. Representa (C) Coxpox, (D) Influenza, (E) Nipah, (F) Norovirus, (G) Orf, (H) Papilloma, (I x4dwwfwtw3.3. (J) Classification accuracy of the trained downstream li training of the downstream task. Legend denotes the self-supervision Sample size is 100 for each boxplot. Notch of the boxplots indicates 95\n824 | Digital Discovery, 2022, 1, 816\u2013833\nregardless of nanoparticle shape (short or long nanorods or triangular prisms). This repurposed mNP dataset for this assembled nanoparticle morphology classication task\ntive TEM images of the different virus types (A) Astrovirus, (B) Nairovirus, ) Rift Valley from the TEM virus dataset66 DOI: https://doi.org/10.17632/ near classifiers as a function of number of labeled images used during method and the generic image dataset used to train the encoder. % confidence interval around the median.\n\u00a9 2022 The Author(s). Published by the Royal Society of Chemistry\nO pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n0 Se\npt em\nbe r\n20 22\n. D ow\nnl oa\nde d\non 1\n/2 5/\n20 24\n1 0:\n07 :1\n1 PM . T hi s ar tic le is li ce ns ed u nd er a C re at iv e C om m on s A ttr ib ut io nN on C om m er ci al 3 .0\nU np\nor te\nd L\nic en\nce .\ncontains three morphology categories: dispersed nanoparticles (Fig. 5A), separated clusters (Fig. 5B), and percolating cluster (Fig. 5C), each category contains 100 images. With the same training protocol applied in previous sections for peptide/ protein nanowire TEM images, i.e., training and predicting with linear classiers on feature maps obtained from encoders trained with self-supervised methods, we report the classication accuracies from the linear classier on the feature maps of the repurposed mNP dataset's test sets (Fig. 5D). We observe that the accuracies obtained from feature maps trained on both self-supervised methods are comparable to that obtained with the state-of-the-art encoder (SimCLR47 method trained on full ImageNet36 dataset). We note that gold nanoparticles have higher contrast with the background compared to peptide/ protein nanowires with the background in TEM, therefore images in themNP dataset are subject to less background noise. Here, we have shown that with the focus onmorphology and not on the shape of the nanoparticles, self-supervised encoders trained with signicantly fewer number of images can achieve comparable accuracies with the state-of-the-art encoders trained with the ImageNet dataset.\nNext, we test our machine learning workow for the automated task of identifying virus from TEM images. We use an open-access TEM virus dataset66 that contains a diverse set of TEM images of more than 10 types of viruses. We choose 9 types from the dataset for the purpose of our test: Astrovirus (Fig. 6A), Nairovirus (Fig. 6B), Coxpox (Fig. 6C), Inuenza (Fig. 6D), Nipah (Fig. 6E), Norovirus (Fig. 6F), Orf (Fig. 6G), Papilloma (Fig. 6H), Ri Valley (Fig. 6I). These TEM images were parts of whole-slide images of a virus or viruses taken by domain experts that were cut into smaller images. Matuszewski and Sintorn51 state that misinterpretation can happen for two types of viruses that look similar or for images that selected unrepresentative part of the virus or selected the background. We apply our machine learning workow to the TEM virus dataset and report the classication accuracies of the linear classiers trained on the feature maps (Fig. 6J). We notice that the feature maps obtained from encoders trained on generic TEM images (BarlowTwins_TEM, SimCLR_TEM) both underperform those trained on ImageNet images. A possible explanation is that encoders trained on generic TEM images are less tolerable to noisy dataset. Another explanation is that the ImageNet dataset was originally and specically prepared for object identication with each image labeled with an object's name while the CEM500k dataset of TEM images were prepared with no labels on the images for general purpose of self-supervised learning. The virus identication task is similar to the original ImageNet object identication task therefore self-supervised models trained on ImageNet images give higher accuracies."
        },
        {
            "heading": "Conclusions",
            "text": "In summary, we have developed a semi-supervised transfer learning workow and demonstrated its efficacy when applied to the task of learning nanoscale morphologies from small microscopy image datasets and minimum number of manual labels. We show that our encoder trained with <1000 unlabeled\n\u00a9 2022 The Author(s). Published by the Royal Society of Chemistry\nimages achieve comparable performance with state-of-the-art encoder trained with more than one million images. Our downstream task models (e.g., classication of nanowire morphology and segmentation of nanowires) trained on the encoded feature maps can achieve >90% accuracy on classication of nanowire morphologies and >0.70 Dice score on segmentation of nanowires training with <10 labeled images per class. With knowledge of the underlying image distribution of the two morphologies that are harder to visually distinguish, we show that it is possible to achieve >90% accuracy training with just one labeled image per morphology.\nWe also show that broader applicability of our machine learning workow for classication and identication tasks in other microscopy images (e.g., assembled nanoparticles of various shapes, viruses) with limited labeled images for training. While there may exist actionable qualication criteria for manual labeling an image for an object identication task, subtle morphological differences are intrinsically harder for human experts to discern and classify into categories. Our machine learning workow is precisely targeting such morphology classication problems to mitigate human biases. In addition, we also want to emphasize that thoughtful categorization and proper labeling of image data is crucial regardless of dataset size10 and is especially important for data-limited image learning problems.\nThe machine learning microscopy image analysis workow we have presented in this article should enable many fundamental studies in so-, nano-, and bio- materials through democratizing (by using only a few labeled images in training) and automating the structural analysis for feature extraction, morphology classication, and segmentation tasks. For example, one potential application of our machine learning workow could be real-time analysis of in situ microscopy images obtained at regular time intervals during thermal/ solvent annealing of polymer nanocomposites when the nanoparticle morphology in the matrix polymers evolves. Another example could be automated analysis of microscopy images obtained during (shear, temperature, solvent) processing of block-copolymer lms leading to phase transitions from one structure to another. With advances in high-throughput experimentation and characterization instrumentation, machine learning workows such as the one in this article will be valuable in accelerating materials innovation and establishing molecular design\u2013structure relationship."
        },
        {
            "heading": "Methods",
            "text": ""
        },
        {
            "heading": "Chemicals and materials",
            "text": "Fmoc-Lys(boc)-OH, Fmoc-Phe-OH, diisopropylethylamine (DIPEA), hexauorophosphate benzotriazole tetramethyl uronium (HBTU), 1-hydroxybenzotriazole monohydrate (HOBt) piperidine, and 2-chlorotrityl chloride resin was purchased from Creosalus and used as received. Dichloromethane (DCM), N,N-dimethylformamide (DMF), methanol (MeOH), hexanes, and diethyl ether were purchased from Fischer Scientic; the DCM was distilled over calcium hydride prior to use. Isopropyl alcohol (IPA), anhydrous N,N-dimethylformamide,\nDigital Discovery, 2022, 1, 816\u2013833 | 825\nO pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n0 Se\npt em\nbe r\n20 22\n. D ow\nnl oa\nde d\non 1\n/2 5/\n20 24\n1 0:\n07 :1\n1 PM . T hi s ar tic le is li ce ns ed u nd er a C re at iv e C om m on s A ttr ib ut io nN on C om m er ci al 3 .0\nU np\nor te\nd L\nic en\nce .\ntriuoroacetic acid (TFA), and 1-naphthaleneacetic acid (NapAcOH) were purchased from Sigma Aldrich and used as received. 2,2,2-Triuoroethanol was purchased from Oakridge Chemical and used as received. Deuterated dimethyl sulfoxide (DMSO-d6) was purchased from Cambridge Isotopes."
        },
        {
            "heading": "Instrumentation",
            "text": "Transmission electron microscopy (TEM) was performed on an FEI Technai T12 electron microscope using samples prepared on 400 square mesh carbon-coated copper grids (Electron Microscopy Sciences) at 120 kV accelerating voltage.\nSynthesis of N-[2-(1-Naphthalenyl)acetyl]-L-phenylalanyl-Lphenylalanyl-L-lysyl-L-lysine (NapFFKK)\nNapFFKK was synthesized in an analogous manner to that used by Laverty, et al.67 with the detailed procedure below using conventional solid-phase peptide synthesis protocols. 2-Chlorotrityl chloride supported PS resin was loaded into a peptide vessel, weighed, and swollen in DCM (distilled over CaH2 prior to use). The solution was ltered, and a solution of Fmoc-Lys(boc)OH and DIPEA in DCM while agitating with N2(g) for 2 h. The solution was drained, and the resin was washed three times with DCM before capping unreacted chlorotrityl chloride groups with an 80 : 15 : 5 solution of DCM : MeOH : DIPEA for 1 h. The solution was ltered, rinsed three times with DCM, three times with DMF, three times with hexanes, three times with IPA, three times with MeOH, and three times with again with DCM, before drying under vacuum overnight. The peptide vessel was weighed to check capping (complete tomanufacturer's specied 1.6mmol g\u22121), swollen in DCM, rinsed three times with DMF, and deprotected using a 3 : 1 DMF : piperidine solution (5 min and 20 min). The remaining amino acids and naphthalene acetic acid were successively coupled to the functionalized resin, usingHOBt and HBTU as coupling agents, in the following order: FmocLys(boc)-OH, Fmoc-Phe-OH, Fmoc-Phe-OH, NapAcOH. For each coupling reaction, the amino acid or capping group (3 eq.), HBTU (3 eq.) andHOBt (3 eq.) were dissolved in anhydrous DMF, before DIPEA (7 eq.) was added to the solution, mixed, and immediately added to the peptide vessel. The Erlenmeyer ask used was rinsed with 10mLDMF, which was added to the peptide vessel, and the peptide solution was agitated under N2(g) for 2 h. The resin was then ltered, washed three times with DMF, and Fmoc groups removed as previously described. Following the nal addition of NapAcOH, a solution of TFA : TIPS:H2O (96 : 2 : 2) solution was added in three parts over 3 h, ltering between each addition. The ltrate was concentrated under N2(g) or via rotational evaporation, precipitated in ether, and centrifuged. Extensive rinsing with ether was employed to reduce the TFA content; the product was then dried overnight under vacuum and isolated as a white powder.\nBundled nanowire images generation\nPilA-based protein nanowires were isolated from E. coli, puried, and bundled in accordance with the procedure described by Sun, et al.68 and are described in detail below. Protein nanowires were harvested from Geobacter sulfurreducens\n826 | Digital Discovery, 2022, 1, 816\u2013833\nexpressed from E. coli using physical shearing followed ltration and collection in MilliQ water. An aliquot of the aqueous nanowire mixture (30 mL aliquot containing 0.535 mg protein per mL) was to a glass vial and dried under a stream of N2(g). Organic solvent (cyclohexane, THF, DMF, or acetone) was added to a nal concentration of 0.10 mg nanowires per mL of solvent; the mixture was vortexed 5 times for 1 second at high power then allowed to settle for 20 minutes. Samples were vortexed once before transferring 5 mL via micropipette to an oxygen plasmatreated substrate (400 mesh, 3\u20134 nm carbon coated copper TEM grids) and drying in air, either to full solvent evaporation or for 5 min, and residual solvent wicked dry using lter paper (in the case of DMF). The TEM samples were stained for 20 s with 4 mL of a 2 wt% aqueous uranyl acetate stain, wicked dry using lter paper, rinsed three times using water droplet on paralm method and wicked dry aer each rinse, and imaged.\nSingle nanowire images generation\nNapFFKK was dissolved in water to a concentration of either 0.1 mg mL\u22121 or 0.05 mg mL\u22121, vortexed until fully dissolved, and allowed to stand for 1 h. The sample was then vortexed for 0.5 \u2013 1 s twice and 4 mL was transferred to an oxygen plasma treated substrate (400 mesh, 3\u20134 nm carbon coated copper TEM grids) and drying in air for 5 min before wicking dry using lter paper. The TEM samples were stained for 40 s with 4 mL of a 2 wt% aqueous uranyl acetate stain, wicked dry using lter paper, rinsed three times using water droplet on paralm method and wicked dry aer each rinse, and imaged.\nDispersed and network nanowire images generation\nNapFFKK was dissolved in solvent (water, IPA, Acetone, ACN) with an initial concentration of 0.2 mg mL\u22121, vortexed until fully dissolved (except in the case of IPA and ACN, which incompletely dissolved the peptides), and allowed to stand for 1 h. Water and acetone samples were then vortexed for 0.5 \u2013 1 s twice prior to transferring to TEM grids; ACN and IPA aliquots were removed without vortexing to avoid resuspending nondissolved peptides. In all cases, 4 mL was transferred to an oxygen plasma treated substrate (400 mesh, 3\u20134 nm carbon coated copper TEM grids) and drying in air for 5 min (the water sample was wicked dry using lter paper; organic solvents fully evaporated over this time). The TEM samples were stained for 40 s with 4 mL of a 2 wt% aqueous uranyl acetate stain, wicked dry using lter paper, rinsed three times using water droplet on paralm method and wicked dry aer each rinse, and imaged."
        },
        {
            "heading": "Image data preprocessing",
            "text": "For the generic TEM images, we took a subset of 10 000 images from the CEM500K microscopy image dataset56 and randomly selected 832 images from the subset each time we train on generic TEM images. The generic TEM images are of resolution 224 224.\nFor the generic everyday images, we took one image from each class of the ImageNet1k dataset36 resulting in 1000 images of different classes as we do not want the images to be similar. We randomly selected 832 images from the subset each time we\n\u00a9 2022 The Author(s). Published by the Royal Society of Chemistry\nO pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n0 Se\npt em\nbe r\n20 22\n. D ow\nnl oa\nde d\non 1\n/2 5/\n20 24\n1 0:\n07 :1\n1 PM . T hi s ar tic le is li ce ns ed u nd er a C re at iv e C om m on s A ttr ib ut io nN on C om m er ci al 3 .0\nU np\nor te\nd L\nic en\nce .\ntrain on generic everyday images. The generic everyday images are in color and of resolution 224 224. We do not use the original labels of the images from the ImageNet dataset.\nFor the task-specic images of peptide/protein nanowire morphologies, 100 images were chosen for the dispersed, network, and bundle morphologies. Single nanowires require more dilution and are trickier to obtain. Due to difficulty in capturing single nanowires in the morphologies, we chose 25 images containing a single nanowire. We augment the singular morphology with a similar method used recently for underrepresented carbon nanotube morphologies.11 75 images were generated by rotating the original 25 single nanowire image by 90, 180, and 270 resulting in a total of 100 images for the singular morphology. The morphology images are taken at different magnication, ranging from 21k to 400k . All the images of nanowire morphologies are at resolution 2048 2048.\nDue to difficulty in distinguishing the network morphology from dispersed morphology in some cases, we manually labeled the nanowires in images with dispersed and networkmorphology to provide quantitative basis for the qualitative morphology class labels of the two easy-to-confuse morphology classes. We labeled the nanowires with masks colored in blue (to distinguish from the contents in the original grayscale image) throughMicroscopy Image Browser (MIB),63 a MATLAB-based annotation soware, and saved a binary image with the manual labels. Nanowires that we manually masked out with \u201ccolored\u201d masks were labeled as \u201cnanowire pixels\u201d, other pixels are labeled as \u201cbackground pixels\u201d. We then performed DBSCAN,69 a clustering algorithm implemented in scikit-learn package,70 on the manually labeled \u201cnanowire pixels\u201d. For each image with clusters of nanowires found by DBSCAN, we quantify percolation by checking whether there exists a cluster that spans both the horizontal and vertical dimension, i.e., two-dimensional percolation. To check criteria of spanning both dimensions, for each cluster, we check if the coordinate of the rightmost pixel minus that of the lemost pixel is no less than the horizontal dimension minus two, same for the vertical dimension. We have conrmed that all the network images are percolated and all the dispersed images are not percolated. We acknowledge that the denition of percolation, in this case, is local to the image, and not necessarily representative of the material as a whole.\nFrom the images with manually labeled nanowire masks of the dispersed and network morphologies, we created the segmentation ground truth binary maps in different resolutions, i.e. 224 224, 384 384, 512 512, 768 768 and 1024 1024. The binary maps are the standard truth and prediction target for binary segmentation task (for multi-class segmentation tasks, the binary map extend to the number of object class plus one, (for background)). In the binary map, nanowire pixels were given value of 1, whereas background pixels were given value of 0. The distribution of nanowire pixel density i.e., percentage of \u201cnanowire pixels\u201d over all pixels in a segmentation ground truth binary map, of both dispersed and network morphologies was obtained from binary maps of resolution 224 224 (Fig. S8\u2020).\nFor the metal nanoparticle (mNP) dataset, we selected TEM images of nanoparticles from the mNP dataset.65 The mNP\n\u00a9 2022 The Author(s). Published by the Royal Society of Chemistry\ndataset contains TEM images of short or long nanorod assemblies and triangular prism assemblies. The MIB soware63 was used to convert the images of le type .dm4 to .jpg. Aer inspection of the TEM images, we categorized the assembled nanoparticles into three categories: dispersed nanoparticles, separate clusters, and percolating cluster, with each morphology containing 100 images. The mNP images are of resolution 2048 2048.\nFor the TEM virus dataset,66 we used 9 of the 14 virus datasets (Astrovirus, Nairovirus, Coxpox, Inuenza, Nipah, Norovirus, Orf, Papilloma, Ri Valley). Adenovirus, Ebola, Lassa, Marburg, and Rotavirus datasets were the 5 datasets not used because these were easily confused across the categories judging by the confusion matrix provided in the original paper.51 The TEM virus images are of resolution 256 256.\nDescription of self-supervised training of image encoder\nA self-supervised encoder training process consists of four main components: image augmentation, image encoder, projector, and the loss function. We implemented two self-supervised training methods: SimCLR47 and Barlow-Twins.57 The two methods only differ in the loss function. For both methods, batch size (i.e., the number of images to train at a time) is an important parameter. Larger batch sizes oen lead to higher performance and shorter training time. However, batch size is usually limited by the GPU memory available. We chose batch sizes of 16, 32, and 64 to tune during hyperparameter tuning. All self-supervised training of the encoder was done on a single Nvidia P100-PCIE GPU provided by Google Colab Pro subscription.\nFor image augmentation, we performed random cropping, random le-right ip, and random color jittering. The combination of random cropping and random color jittering was shown in the original implementation of SimCLR trained on everyday images to give better performance compared to other augmentation combinations. However, TEM images, being in grayscale, are expected to be more impacted by random cropping. Thus, we chose the method of random cropping as a hyperparameter to tune. We rst chose the side length of the cropped image to be either 14 or 1 2 or a random percentage between 14 and 1 2 of the side length of the original image. Then the le-bottom point of the cropped image was randomly picked so that the cropped image is not out-of-bounds of the original image. Finally, the cropped image was resized to the resolution of the original image. Random le-right ip is set to a probability of 0.5. Random color jittering consists of random hue, random contrast, and random saturation. The hue, contrast, and saturation of the image were shied (each separately with probability of 0.8 and in random order) by a random number applied on all the pixels. For hyperparameter tuning, we also tested a model where we do not crop the image and only perform random le-right ip and random color jittering.\nThe encoder took in the augmented images and distilled the information into feature maps. We used ResNet50 as the encoder architecture. The ResNet50 was initialized with weights trained on ImageNet images as initializing the encoder with transferred weights gives higher performance.71 We removed\nDigital Discovery, 2022, 1, 816\u2013833 | 827\nFig. 7 Differences in vector multiplication when calculating the loss functions for SimCLR and Barlow-Twins methods. (A) With SimCLRmethod, a cosine similarity coefficient is obtained from pairs of projections. (B) With Barlow-Twins method, a cross-correlation matrix is obtained from pairs of projections.\nO pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n0 Se\npt em\nbe r\n20 22\n. D ow\nnl oa\nde d\non 1\n/2 5/\n20 24\n1 0:\n07 :1\n1 PM . T hi s ar tic le is li ce ns ed u nd er a C re at iv e C om m on s A ttr ib ut io nN on C om m er ci al 3 .0\nU np\nor te\nd L\nic en\nce .\nthe classication layer and keep the weights of the encoder to be trainable during training. The output of the encoder, i.e., feature map was of dimension 2048.\nThe projector was constructed with three linear layers with number of nodes: 128, 64, 1024. The number of nodes in the nal layer was also considered as a hyperparameter. All three layers were followed by a batch normalization layer. The rst two layers also had rectied linear units as activation layer. The output of the projector is called the projection.\nFor each batch of images in training, two augmented images were generated from each of the original images. The projections of the images were used as inputs in the loss function. Two projections undergo row vector column vector multiplication and obtain a cosine similarity coefficient (SimCLR method) or undergo column vector row vector multiplication to obtain a cross-correlationmatrix (Barlow-Twinsmethod), as illustrated in Fig. 7. The loss function for SimCLR method consists of two terms: a similarity term measuring the L2-normalized cosine similarity coefficient of a \u201ctrue\u201d pair of projections (coming from the same original image), and a contrast term measuring the L2normalized cosine similarity coefficient of a \u201cfalse\u201d pair of projections (coming from different images) as shown in eqn (3). The loss function for Barlow-Twins method also consists of two terms: an invariance term in the form of L2-normalized sum-ofsquares penalizing the diagonal values in the cross-correlation matrix for deviating from unity, and a redundancy reduction termmeasuring the L2-normalized sum-of-squares of off-diagonal values in the cross-correlation matrix as shown in eqn (4).\nL SimCLR \u00bc X b zAb ; z B b skzAb k2kzBb k2 \u00fe X b log X b 0sb exp\nD zAb ; z B b0 E skzAb k2kzBb0 k2 !!\n(3)\nL Barlow-Twins \u00bc X b\n0 @X\ni\n1\nD zAb;i; z B b;i E kzAb;ik2kzBb;ik2 !2\n\u00fe l X i X jsi\nD zAb;i; z B b;j E kzAb;ik2kzBb;jk2 !21A (4)\n828 | Digital Discovery, 2022, 1, 816\u2013833\nwhere zA and zB are the projections, b indexes the sample in a batch, i indexes the vector component of the projection, s is the temperature parameter analogous to statistical mechanics, we use the recommended value of 0.10 in our trainings, l is the weighting factor for the redundancy reduction term, we use the recommended value of 0.005 in our trainings.\nFor training, we used an initial learning rate of 0.2 and the learning rate decay schedule described in the original SimCLR or Barlow-Twins paper's implementation. All trainings were done with SGD optimizer for 200 epochs.\nProtocol for hyperparameter tuning model selection of selfsupervised encoders\nAll self-supervised training of the encoder was done on a single Nvidia P100-PCIE GPU provided by Google Colab Pro subscription. All training of classication models was done on a single Nvidia K80 GPU provided by Google Colab free version. Three important hyperparameters (batch size, crop method and nal projection layer size) were tuned for optimization of the self-supervised encoders with cross-validation for model selection on the morphology classication task. Five self-supervised encoders were trained with either Barlow-Twins or SimCLR method for each of the different hyperparameter sets we explored, the difference being the random seed used to select the 832 generic TEM images of resolution 224 224 to train. For the classication task, we used the trained encoders to transform the morphology images into feature maps. We used ve different random seeds to split the nanowire morphology images into 80% training data, and 20% test data (not used) while keeping the class distribution balanced in both training and test sets. We performed 4-fold cross-validation on the training data, i.e., 60% of all morphology images as training data and 20% of all morphology images as validation data. We trained a linear classier consisting 4 neurons to classify the feature maps of morphology images (obtained from trained encoder) into their respective morphologies. Thus, for each model hyperparameter set, we had 5 encoders 5 data split 4 folds \u00bc 100 linear classiers trained giving 100 accuracies as statistics. To mitigate overtting, we used an early stopping criterion that stops the training of the linear classier when the validation accuracy stops increasing in 10 consecutive epochs.\n\u00a9 2022 The Author(s). Published by the Royal Society of Chemistry\nO pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n0 Se\npt em\nbe r\n20 22\n. D ow\nnl oa\nde d\non 1\n/2 5/\n20 24\n1 0:\n07 :1\n1 PM . T hi s ar tic le is li ce ns ed u nd er a C re at iv e C om m on s A ttr ib ut io nN on C om m er ci al 3 .0\nU np\nor te\nd L\nic en\nce .\nAdam optimizer with default learning rate of 0.001 was used for training the linear classier.\nFor batch size, we looked at batch sizes of 16, 32 and 64, with 64 being the largest batch size we can use due to limitation of the GPU memory. For the crop method, we looked at Randcrop, i.e., the cropped image length is random between 14 and 1 2 of the original image length; crop25, i.e., the cropped image length is 14 of the original image length; crop50, i.e., the cropped image length is 12 of the original image length and no crop, where the generic images were not cropped during augmentation., We used 64, 256 and 1024 neurons as the size of the last projection layer.\nWhen tuning batch size, we used the Randcrop method and a last projection layer size of 1024. For the batch size, we observed that the classication performance of feature maps obtained with encoders trained with SimCLR method had a strong dependence on batch size while that trained with Barlow-Twins method was insensitive to batch size (Fig. 8A).\nWhen tuning crop method, we used a batch size of 64 and a last projection layer size of 1024. For the crop method, we observed that the classication performance of feature maps obtained with encoders trained with SimCLR method became worse when the crop method was more rigorous, i.e., cropped image is more different from the original image, while those trained with Barlow-Twins were insensitive to the crop method (Fig. 8B).\nWhen tuning the size of the nal projection layer, we used a batch size of 64 and the Randcrop method. Both methods are insensitive to the size of the nal projection layer (Fig. 8C).\nBased on observations of the hyperparameter tuning process, we have chosen batch size of 64, crop method of Randcrop and last projection layer size of 1024 for Barlow-Twins method training and batch size of 64, not cropping and last projection layer size of 1024 for SimCLR method training.\nFor the Barlow-Twins method, we also looked at how the resolution of the images used to train the encoder impacted the classication performance of feature maps (Fig. 9). Given the insensitivity of Barlow-Twins method to batch size, higher\n\u00a9 2022 The Author(s). Published by the Royal Society of Chemistry\nresolution images can be used as input when training the encoder using smaller batch size without losing accuracy.\nProtocol for classication task performance assessment\nAll training of classication models was done on a single Nvidia K80 GPU provided by Google Colab free version. Except for the SimCLR_SotA_ImageNet encoder which was obtained as published from the official SimCLR repository on Github, the other four encoders were the best performing encoders selected from the hyperparameter tuning protocol.\nFor the classication task, we used the trained encoders to transform the morphology images into feature maps. For the peptide/protein nanowire dataset and the mNP dataset, we used different random seeds to split the nanowire morphology images into 80% training data, and 20% test data while keeping the class distribution balanced in both training and test sets. From the 80 training images per class, we then randomly\nancewith featuremaps obtained from encoders, trained with Barlowact of batch size. Barlow-Twins method is insensitive to batch size used atch size is small. (B) Impact of crop method. Barlow-Twins method is opping. (C) Impact of the last projection layer size. Both methods are\nDigital Discovery, 2022, 1, 816\u2013833 | 829\nFig. 10 Detailed schematic of the Unet architecture with transferred encoder.\nO pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n0 Se\npt em\nbe r\n20 22\n. D ow\nnl oa\nde d\non 1\n/2 5/\n20 24\n1 0:\n07 :1\n1 PM . T hi s ar tic le is li ce ns ed u nd er a C re at iv e C om m on s A ttr ib ut io nN on C om m er ci al 3 .0\nU np\nor te\nd L\nic en\nce .\nselected 1, 2, 4, 8, 16, 32 or 80 images per class (with a xed random seed of 42) as the labeled training image set. For the TEM virus dataset, we used the training, validation and test dataset provided by the original paper. We then randomly selected 10, 20, 40, 80, 120, 200 or 600 images per class (with different random seeds) as the labeled training image set. To keep the sample size the same for performance assessment, we used 100 random seeds for SimCLR_SotA_ImageNet encoder and 20 random seeds for the other four locally trained encoders because each locally trained encoders already had 5 replicate models during self-supervised training. The training procedure of the linear classier was the same as described in the hyperparameter tuning protocol."
        },
        {
            "heading": "Segmentation model description",
            "text": "We used the Unet architecture59 as the segmentation model with our trained encoder as the down sampling module. Skip connections were established between the transferred encoder and a decoder with random initialized weights. The exact encoder-decoder network is shown in Fig. 10. The weights of the transferred encoder were not updated, i.e., xed as constants during training.\nProtocol for segmentation task performance assessment\nAll training of segmentationmodels was done on a single Nvidia K80 GPU provided by Google Colab free version. Compared to the classication task, the segmentation task was much slower\n830 | Digital Discovery, 2022, 1, 816\u2013833\n(2 orders of magnitude in terms of training and prediction time) due to the large overhead of the encoder-decoder model. Five encoders were trained with Barlow-Twins method on generic TEM images of resolution of either 224 224, 384 384 or 512 512, i.e., ve encoders for each resolution. For the nanowire morphology images of dispersed and networkmorphologies, we resized the images to resolutions of 224 224, 384 384, 512 512, 768 768 and 1024 1024. For the segmentation task, we used the images as inputs to the encoder-decoder network. We used one xed random seed of 42 to split the nanowire morphology images into 80% training data, and 20% test data, while keeping the class distribution balanced in both training and test sets. We then from the 80 training images per class randomly selected 1, 2, 4, 8, 16, 32 and 80 images per class (with a xed random seed of 42) as the labeled training image set. We used (1 \u2212 Dice score) as the loss function in training of the segmentation model. For the Dice scores or IoU scores, the sample size was 5 encoders 40 images in the test set \u00bc 200. Adam optimizer with default learning rate of 0.001 was used for training the segmentation model. The number of epochs for models with different number of labeled training images were 120, 120, 120, 60, 40, 30 and 20 respectively."
        },
        {
            "heading": "Data availability",
            "text": "The python code for implementing the machine learning models with Keras and Tensorow is available at https:// github.com/arthijayaraman-lab/semi-supervised_learning_\n\u00a9 2022 The Author(s). Published by the Royal Society of Chemistry\nO pe\nn A\ncc es\ns A\nrt ic\nle . P\nub lis\nhe d\non 2\n0 Se\npt em\nbe r\n20 22\n. D ow\nnl oa\nde d\non 1\n/2 5/\n20 24\n1 0:\n07 :1\n1 PM . T hi s ar tic le is li ce ns ed u nd er a C re at iv e C om m on s A ttr ib ut io nN on C om m er ci al 3 .0\nU np\nor te\nd L\nic en\nce .\nmicroscopy_images. The image dataset of nanowire morphologies is deposited on the open-access data repository Zenodo with DOI: https://doi.org/10.5281/zenodo.6377140. All data and models generated during and/or analyzed during the current study are available from the corresponding author upon reasonable request."
        },
        {
            "heading": "Author contributions",
            "text": "S. L. devised the idea and led the machine learning workow development with guidance and feedback from A. J.; B. M. performed the synthesis of the synthetic peptide nanowires and microscopy imaging for the synthetic peptide nanowires and bio-derived protein nanowires with feedback from T. E.; B. M. and S. L. decided on the qualication criteria of different morphologies; S. L. created the segmentation ground truth labels and the morphology class labels; S. L. wrote the python code for implementing the machine learning workow, trained and tested the model performances; S. L., B. M., T. E., and A. J. wrote the manuscript."
        },
        {
            "heading": "Conflicts of interest",
            "text": "The authors declare no competing nancial interest."
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors acknowledge nancial support from the U.S. National Science Foundation, Grant NSF DMREF #1921839 and #1921871."
        }
    ],
    "title": "Semi-supervised machine learning workflow for analysis of nanowire morphologies from transmission electron microscopy images",
    "year": 2022
}