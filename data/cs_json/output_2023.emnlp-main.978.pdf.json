{
    "abstractText": "Group fairness is a central research topic in text classification, where reaching fair treatment between sensitive groups (e.g. women vs. men) remains an open challenge. This paper presents a novel method for mitigating biases in neural text classification, agnostic to the model architecture. Considering the difficulty to distinguish fair from unfair information in a text encoder, we take inspiration from adversarial training to induce Wasserstein independence between representations learned to predict our target label and the ones learned to predict some sensitive attribute. Our approach provides two significant advantages. Firstly, it does not require annotations of sensitive attributes in both testing and training data. This is more suitable for real-life scenarios compared to existing methods that require annotations of sensitive attributes at train time. Secondly, our approach exhibits a comparable or better fairness-accuracy trade-off compared to existing methods. Our implementation is available on Github1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Thibaud Leteno"
        },
        {
            "affiliations": [],
            "name": "Antoine Gourru"
        },
        {
            "affiliations": [],
            "name": "Charlotte Laclau"
        },
        {
            "affiliations": [],
            "name": "R\u00e9mi Emonet"
        },
        {
            "affiliations": [],
            "name": "Christophe Gravier"
        }
    ],
    "id": "SP:a27f63b4f6383ca2efd3a5a0e78d67cd12ecef02",
    "references": [
        {
            "authors": [
                "Martin Arjovsky",
                "Soumith Chintala",
                "L\u00e9on Bottou."
            ],
            "title": "Wasserstein generative adversarial networks",
            "venue": "ICML, pages 214\u2013223. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "Alex Beutel",
                "Jilin Chen",
                "Zhe Zhao",
                "Ed H Chi."
            ],
            "title": "Data decisions and theoretical implications when adversarially learning fair representations",
            "venue": "arXiv preprint arXiv:1707.00075.",
            "year": 2017
        },
        {
            "authors": [
                "Su Lin Blodgett",
                "Solon Barocas",
                "Hal Daum\u00e9 III",
                "Hanna Wallach."
            ],
            "title": "Language (technology) is power: A critical survey of \u201cbias\u201d in nlp",
            "venue": "Proceedings of ACL, pages 5454\u20135476.",
            "year": 2020
        },
        {
            "authors": [
                "Su Lin Blodgett",
                "Lisa Green",
                "Brendan O\u2019Connor"
            ],
            "title": "Demographic dialectal variation in social media: A case study of african-american english",
            "venue": "arXiv preprint arXiv:1608.08868",
            "year": 2016
        },
        {
            "authors": [
                "Tolga Bolukbasi",
                "Kai-Wei Chang",
                "James Y Zou",
                "Venkatesh Saligrama",
                "Adam T Kalai."
            ],
            "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
            "venue": "NeurIPS, 29.",
            "year": 2016
        },
        {
            "authors": [
                "Laura Cabello",
                "Anna Katrine J\u00f8rgensen",
                "Anders S\u00f8gaard."
            ],
            "title": "On the independence of association bias and empirical fairness in language models",
            "venue": "Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201923,",
            "year": 2023
        },
        {
            "authors": [
                "Simon Caton",
                "Christian Haas."
            ],
            "title": "Fairness in machine learning: A survey",
            "venue": "arXiv preprint arXiv:2010.04053.",
            "year": 2020
        },
        {
            "authors": [
                "Myra Cheng",
                "Esin Durmus",
                "Dan Jurafsky."
            ],
            "title": "Marked personas: Using natural language prompts to measure stereotypes in language models",
            "venue": "arXiv preprint arXiv:2305.18189.",
            "year": 2023
        },
        {
            "authors": [
                "Pengyu Cheng",
                "Weituo Hao",
                "Siyang Yuan",
                "Shijing Si",
                "Lawrence Carin"
            ],
            "title": "Fairfil: Contrastive neural debiasing method for pretrained text encoders",
            "year": 2021
        },
        {
            "authors": [
                "Nicolas Courty",
                "R\u00e9mi Flamary",
                "Devis Tuia."
            ],
            "title": "Domain adaptation with regularized optimal transport",
            "venue": "ECML PKDD, pages 274\u2013289. Springer.",
            "year": 2014
        },
        {
            "authors": [
                "Maria De-Arteaga",
                "Alexey Romanov",
                "Hanna Wallach",
                "Jennifer Chayes",
                "Christian Borgs",
                "Alexandra Chouldechova",
                "Sahin Geyik",
                "Krishnaram Kenthapadi",
                "Adam Tauman Kalai"
            ],
            "title": "Bias in bios: A case study of semantic representation bias in a high-stakes",
            "year": 2019
        },
        {
            "authors": [
                "Dina Demner-Fushman",
                "Wendy W Chapman",
                "Clement J McDonald"
            ],
            "title": "What can natural language processing do for clinical decision support",
            "venue": "Journal of biomedical informatics,",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of NAACL-HLT, pages 4171\u20134186.",
            "year": 2019
        },
        {
            "authors": [
                "Yanai Elazar",
                "Yoav Goldberg."
            ],
            "title": "Adversarial removal of demographic attributes from text data",
            "venue": "Proceedings of EMNLP. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Susan T Fiske."
            ],
            "title": "Prejudices in cultural contexts: Shared stereotypes (gender, age) versus variable stereotypes (race, ethnicity, religion)",
            "venue": "Perspectives on psychological science, 12(5):791\u2013799.",
            "year": 2017
        },
        {
            "authors": [
                "Charlie Frogner",
                "Chiyuan Zhang",
                "Hossein Mobahi",
                "Mauricio Araya",
                "Tomaso A Poggio."
            ],
            "title": "Learning with a wasserstein loss",
            "venue": "NeurIPS, 28.",
            "year": 2015
        },
        {
            "authors": [
                "Paula Gordaliza",
                "Eustasio Del Barrio",
                "Gamboa Fabrice",
                "Jean-Michel Loubes."
            ],
            "title": "Obtaining fairness using optimal transport theory",
            "venue": "ICML, pages 2357\u2013 2365. PMLR.",
            "year": 2019
        },
        {
            "authors": [
                "Ishaan Gulrajani",
                "Faruk Ahmed",
                "Martin Arjovsky",
                "Vincent Dumoulin",
                "Aaron C Courville."
            ],
            "title": "Improved training of wasserstein gans",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Xudong Han",
                "Timothy Baldwin",
                "Trevor Cohn."
            ],
            "title": "Balancing out bias: Achieving fairness through balanced training",
            "venue": "arXiv preprint arXiv:2109.08253.",
            "year": 2021
        },
        {
            "authors": [
                "Xudong Han",
                "Timothy Baldwin",
                "Trevor Cohn."
            ],
            "title": "Decoupling adversarial training for fair NLP",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 471\u2013477, Online. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Xudong Han",
                "Timothy Baldwin",
                "Trevor Cohn."
            ],
            "title": "Diverse adversaries for mitigating bias in training",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2760\u20132765,",
            "year": 2021
        },
        {
            "authors": [
                "Xudong Han",
                "Aili Shen",
                "Yitong Li",
                "Lea Frermann",
                "Timothy Baldwin",
                "Trevor Cohn."
            ],
            "title": "Fairlib: A unified framework for assessing and improving fairness",
            "venue": "Proceedings of the The 2022 Conference on Empirical Methods in Natural Language Processing:",
            "year": 2022
        },
        {
            "authors": [
                "Moritz Hardt",
                "Eric Price",
                "Nati Srebro."
            ],
            "title": "Equality of opportunity in supervised learning",
            "venue": "NeurIPS,",
            "year": 2016
        },
        {
            "authors": [
                "Ben Hutchinson",
                "Vinodkumar Prabhakaran",
                "Emily Denton",
                "Kellie Webster",
                "Yu Zhong",
                "Stephen Denuyl"
            ],
            "title": "Social biases in NLP models as barriers for persons with disabilities",
            "venue": "In ACL,",
            "year": 2020
        },
        {
            "authors": [
                "Mariana Jatob\u00e1",
                "Juliana Santos",
                "Ives Gutierriz",
                "Daniela Moscon",
                "Paula Odete Fernandes",
                "Jo\u00e3o Paulo Teixeira."
            ],
            "title": "Evolution of artificial intelligence research in human resources",
            "venue": "Procedia Computer Science, 164:137\u2013142.",
            "year": 2019
        },
        {
            "authors": [
                "Shengyu Jia",
                "Tao Meng",
                "Jieyu Zhao",
                "Kai-Wei Chang."
            ],
            "title": "Mitigating gender bias amplification in distribution by posterior regularization",
            "venue": "Proceedings of the Annual Meeting of ACL.",
            "year": 2020
        },
        {
            "authors": [
                "Ray Jiang",
                "Aldo Pacchiano",
                "Tom Stepleton",
                "Heinrich Jiang",
                "Silvia Chiappa."
            ],
            "title": "Wasserstein fair classification",
            "venue": "Uncertainty in artificial intelligence, pages 862\u2013872. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Masahiro Kaneko",
                "Danushka Bollegala",
                "Naoaki Okazaki."
            ],
            "title": "Debiasing isn\u2019t enough!\u2013on the effectiveness of debiasing mlms and their social biases in downstream tasks",
            "venue": "Proceedings of the International Conference on Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Leonid Kantorovich."
            ],
            "title": "On the translocation of masses",
            "venue": "C.R. (Doklady) Acad. Sci. URSS(N.S.), volume 37(10), pages 199\u2013201.",
            "year": 1942
        },
        {
            "authors": [
                "Patrik Joslin Kenfack",
                "Samira Ebrahimi Kahou",
                "Ulrich A\u00efvodji."
            ],
            "title": "Fairness under demographic scarce regime",
            "venue": "arXiv preprint arXiv:2307.13081.",
            "year": 2023
        },
        {
            "authors": [
                "Svetlana Kiritchenko",
                "Saif M. Mohammad"
            ],
            "title": "Examining gender and race bias in two hundred sentiment analysis systems",
            "year": 2018
        },
        {
            "authors": [
                "Charlotte Laclau",
                "Ievgen Redko",
                "Manvi Choudhary",
                "Christine Largeron."
            ],
            "title": "All of the fairness for edge prediction with optimal transport",
            "venue": "AISTATS, pages 1774\u20131782. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Paul Pu Liang",
                "Chiyu Wu",
                "Louis-Philippe Morency",
                "Ruslan Salakhutdinov."
            ],
            "title": "Towards understanding and mitigating social biases in language models",
            "venue": "ICML, pages 6565\u20136576. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Chandler May",
                "Alex Wang",
                "Shikha Bordia",
                "Samuel R Bowman",
                "Rachel Rudinger."
            ],
            "title": "On measuring social biases in sentence encoders",
            "venue": "Proceedings of NAACL-HLT, pages 622\u2013628.",
            "year": 2019
        },
        {
            "authors": [
                "Gaspard Monge."
            ],
            "title": "M\u00e9moire sur la th\u00e9orie des d\u00e9blais et des remblais",
            "venue": "Histoire de l\u2019Acad\u00e9mie Royale des Sciences, pages 666\u2013704.",
            "year": 1781
        },
        {
            "authors": [
                "Junhyun Nam",
                "Hyuntak Cha",
                "Sungsoo Ahn",
                "Jaeho Lee",
                "Jinwoo Shin."
            ],
            "title": "Learning from failure: Debiasing classifier from biased classifier",
            "venue": "Advances in Neural Information Processing Systems, 33:20673\u2013 20684.",
            "year": 2020
        },
        {
            "authors": [
                "Osonde A Osoba",
                "William Welser IV."
            ],
            "title": "An intelligence in our image: The risks of bias and errors in artificial intelligence",
            "venue": "Rand Corporation.",
            "year": 2017
        },
        {
            "authors": [
                "Sherjil Ozair",
                "Corey Lynch",
                "Yoshua Bengio",
                "Aaron Van den Oord",
                "Sergey Levine",
                "Pierre Sermanet."
            ],
            "title": "Wasserstein dependency measure for representation learning",
            "venue": "Advances in Neural Information Processing Systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Ji Ho Park",
                "Jamin Shin",
                "Pascale Fung."
            ],
            "title": "Reducing gender bias in abusive language detection",
            "venue": "Proceedings of EMNLP, pages 2799\u20132804.",
            "year": 2018
        },
        {
            "authors": [
                "Sundar Pichai"
            ],
            "title": "Google bard. https: //blog.google/technology/ai/ bard-google-ai-search-updates",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Shauli Ravfogel",
                "Yanai Elazar",
                "Hila Gonen",
                "Michael Twiton",
                "Yoav Goldberg."
            ],
            "title": "Null it out: Guarding protected attributes by iterative nullspace projection",
            "venue": "Proceedings of the Annual Meeting of ACL, pages 7237\u20137256. ACL.",
            "year": 2020
        },
        {
            "authors": [
                "Laurent Risser",
                "Alberto Gonz\u00e1lez Sanz",
                "Quentin Vincenot",
                "Jean-Michel Loubes."
            ],
            "title": "Tackling algorithmic bias in neural-network classifiers using wasserstein-2 regularization",
            "venue": "Journal of Mathematical Imaging and Vision, 64(6):672\u2013689.",
            "year": 2022
        },
        {
            "authors": [
                "Yuji Roh",
                "Kangwook Lee",
                "Steven Euijong Whang",
                "Changho Suh."
            ],
            "title": "Fairbatch: Batch selection for model fairness",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Carsten Schwemmer",
                "Carly Knight",
                "Emily D BelloPardo",
                "Stan Oklobdzija",
                "Martijn Schoonvelde",
                "Jeffrey W Lockhart."
            ],
            "title": "Diagnosing gender bias in image recognition systems",
            "venue": "Socius, 6.",
            "year": 2020
        },
        {
            "authors": [
                "Aili Shen",
                "Xudong Han",
                "Trevor Cohn",
                "Timothy Baldwin",
                "Lea Frermann"
            ],
            "title": "2022a. Does representational fairness imply empirical fairness? In Findings of the Association for Computational Linguistics: AACLIJCNLP",
            "year": 2022
        },
        {
            "authors": [
                "Aili Shen",
                "Xudong Han",
                "Trevor Cohn",
                "Timothy Baldwin",
                "Lea Frermann."
            ],
            "title": "Optimising equal opportunity fairness in model training",
            "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2022
        },
        {
            "authors": [
                "Chiappa Silvia",
                "Jiang Ray",
                "Stepleton Tom",
                "Pacchiano Aldo",
                "Jiang Heinrich",
                "Aslanides John."
            ],
            "title": "A general approach to fairness with optimal transport",
            "venue": "Proceedings of AAAI, volume 34, pages 3633\u2013 3640.",
            "year": 2020
        },
        {
            "authors": [
                "Tony Sun",
                "Andrew Gaut",
                "Shirlyn Tang",
                "Yuxin Huang",
                "Mai ElSherief",
                "Jieyu Zhao",
                "Diba Mirza",
                "Elizabeth Belding",
                "Kai-Wei Chang",
                "William Yang Wang."
            ],
            "title": "Mitigating gender bias in natural language processing: Literature review",
            "venue": "Proceedings of the",
            "year": 2019
        },
        {
            "authors": [
                "Yi Chern Tan",
                "L Elisa Celis."
            ],
            "title": "Assessing social and intersectional biases in contextualized word representations",
            "venue": "NeurIPS, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Ewoenam Tokpo",
                "Pieter Delobelle",
                "Bettina Berendt",
                "Toon Calders."
            ],
            "title": "How far can it go?: On intrinsic gender bias mitigation for text classification",
            "venue": "arXiv e-prints, pages arXiv\u20132301.",
            "year": 2023
        },
        {
            "authors": [
                "Luis Caicedo Torres",
                "Luiz Manella Pereira",
                "M Hadi Amini."
            ],
            "title": "A survey on optimal transport for machine learning: Theory and applications",
            "venue": "arXiv preprint arXiv:2106.01963.",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "NeurIPS, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Tianlu Wang",
                "Xi Victoria Lin",
                "Nazneen Fatema Rajani",
                "Bryan McCann",
                "Vicente Ordonez",
                "Caiming Xiong."
            ],
            "title": "Double-hard debias: Tailoring word embeddings for gender bias mitigation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Wei",
                "Boqing Gong",
                "Zixia Liu",
                "Wei Lu",
                "Liqiang Wang."
            ],
            "title": "Improving the improved training of wasserstein gans: A consistency term and its dual effect",
            "venue": "International Conference on Learning Representation (ICLR).",
            "year": 2018
        },
        {
            "authors": [
                "Raymond W Yeung."
            ],
            "title": "A new outlook on shannon\u2019s information measures",
            "venue": "IEEE transactions on information theory, 37(3):466\u2013474.",
            "year": 1991
        },
        {
            "authors": [
                "Brian Hu Zhang",
                "Blake Lemoine",
                "Margaret Mitchell."
            ],
            "title": "Mitigating unwanted biases with adversarial learning",
            "venue": "Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pages 335\u2013 340.",
            "year": 2018
        },
        {
            "authors": [
                "Jieyu Zhao",
                "Tianlu Wang",
                "Mark Yatskar",
                "Ryan Cotterell",
                "Vicente Ordonez",
                "Kai-Wei Chang."
            ],
            "title": "Gender bias in contextualized word embeddings",
            "venue": "Proceedings of NAACL-HLT, pages 629\u2013634.",
            "year": 2019
        },
        {
            "authors": [
                "Jieyu Zhao",
                "Tianlu Wang",
                "Mark Yatskar",
                "Vicente Ordonez",
                "Kai-Wei Chang."
            ],
            "title": "Men also like shopping: Reducing gender bias amplification using corpus-level constraints",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 15790\u201315803 December 6-10, 2023 \u00a92023 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "Machine learning algorithms have become increasingly influential in decision-making processes that significantly impact our daily lives. One of the major challenges that has emerged in research, both academic and industrial, concerns the fairness of these models, i.e. their ability to prevent predictions related to individuals to be based on sensitive attributes such as gender or ethnicity. In this article, we focus on the problem of fairness in the domain of Natural Language Processing (NLP) (Bender et al., 2021; Osoba and Welser IV, 2017; Schwemmer et al., 2020). While many studies already report biases in NLP systems (Sun et al., 2019; Hutchinson et al., 2020; Tan and Celis, 2019;\n\u2217Equal contribution. 1https://github.com/LetenoThibaud/wasserstein_\nfair_classification\nLiang et al., 2021; Bender et al., 2021), these issues become even more significant with the advent of public-ready AI-powered NLP systems such as ChatGPT (OpenAI) or Google Bard (Pichai), making the need for fair NLP solutions even more compelling. As more researchers work to overcome these shortcomings, the first problem is to define what fairness is. Such a definition may hardly be consensual or is at least difficult to establish, as it depends on situational and cultural contexts (Fiske, 2017). In this work, we adopt the most common definition of group fairness, and the one adopted by laws in several countries, which is based on the notion of disparate impact: a prediction model is considered to have a disparate impact if its results disproportionately harm (or benefit) people with certain sensitive attribute values (e.g., women, black people).\nIn this work, we focus on group fairness for neural text classification as it is one of the most ubiquitous tasks in our society, with prominent examples in medical and legal domains (Demner-Fushman et al., 2009) or human resources (Jatob\u00e1 et al., 2019), to name a few. Neural text classification relies on text encoders, which are parameterized and learned functions that map tokens (arbitrary\n15790\ntext chunks) into a latent space of controllable dimension, usually followed by a classification layer. Built upon the Transformers architecture (Vaswani et al., 2017), popular Pre-trained Language Models (PLMs) such as BERT (Devlin et al., 2019), GPT3 (Radford et al., 2019) or Llama (Touvron et al., 2023) leverage self-supervised learning to train the text encoder parameters. In modern NLP pipelines, these PLMs are further fine-tuned on the supervised task at hand. Ultimately, PLMs accumulate uncontrolled levels of unfairness due to unbalanced learning data or algorithmic biases, for instance. This results in observable biases in predictions but also in the latent space as studied in (Zhao et al., 2019; May et al., 2019).\nWe propose a novel approach (see Figure 1), to mitigate bias in text encoders, that aims to tackle bias directly in the latent space on which documents are projected, thus making our model applicable to any text encoder or decoder (e.g. BERT or LLAMA). To proceed, we disentangle the neural signals encoding bias from the neural signals used for prediction. The proposed architecture is based on three components. First, two Multi-Layer Perceptrons (MLPs): the first one whose objective is to predict the sensitive attribute, and the second one is dedicated to the prediction task at hand. Then, a third MLP, referred to as a critic, approximates the Wasserstein distance that acts as a regularizer in our objective function. Our proposition overcomes a major shortcoming of prior studies: they rely on the availability of the sensitive attributes at train time. A constraint that is incompatible with recent regulations as the new European ones, that enforce more stringent requirements for the collection and utilization of protected attributes. Prior studies are thus more difficult to use in practical settings. In the following, we will show that our approach can address this limitation by avoiding the use of this information during both testing and training.\nThe rest of this paper is organized as follows. Section 2 presents recent advances regarding fairness in NLP. Section 3 argues about our motivation and provides the background knowledge to understand our contribution. Section 4 proceeds with the description of the proposed approach, its theoretical analysis, and algorithmic implementation. Section 5 introduces the setting of our experiments, and Section 6 presents the results and interpretation. The last section concludes the paper and gives a couple of hints for possible future research."
        },
        {
            "heading": "2 Related Works",
            "text": "Numerous studies have been conducted on how to tackle bias in machine learning systems. Approaches to reinforce fairness can be divided between pre-, in-, and post-processing methods. In the NLP literature, common pre-processing techniques consist of data rebalancing (Park et al., 2018) or embedding debiasing (Wang et al., 2020; Bolukbasi et al., 2016). Yet, despite the efficiency of those methods, Kaneko et al. (2022) and Tokpo et al. (2023) showed that other biases can be learned during the training or fine-tuning processes. On the other hand, post-processing procedures aim at correcting biases after the learning step, through model calibration (Zhao et al., 2017; Jia et al., 2020) or data projection (Ravfogel et al., 2020) (INLP). We refer to (Sun et al., 2019; Blodgett et al., 2020) for more exhaustive surveys of bias in NLP.\nRecently, adversarial methods (Beutel et al., 2017; Zhang et al., 2018; Elazar and Goldberg, 2018) have been investigated to mitigate biases. Han et al. (2021c,b) respectively suggest using several discriminators where each learns different hidden representations and applying an adversarial approach in- and cross-domain to train the adversary on a different dataset, with methods called Adv and GATE. Differently, recent contributions focus on directly constraining the objective to improve fairness (Shen et al., 2022b; Han et al., 2021a). For instance, by adding some fairness metric, such as the Equal opportunity that we will define later in this paper, to the objective function.\nOur approach is at the crossroad of these two philosophies: on the one hand, we propose to train a biased model whose sole purpose is to predict the sensitive attribute and use this latter to enforce fairness in our main prediction model. On the second hand, we minimize a classifier loss with a regularization term measuring the dependence between the latent representations of the classifier and some unfair representations, using Wasserstein distance. While many works use the Kullback\u2013Leibler (KL) divergence to measure the mutual information between representations, Ozair et al. (2019) show several limitations: the KL-divergence is sensitive to small perturbations in the data, and exploiting it for estimating the mutual information does not scale up properly. Thus, they suggest an improvement thanks to the Wasserstein distance. Other methods based on this distance suggest focusing on the distance between the distributions of predictions to en-\nforce fairness (Jiang et al., 2020; Risser et al., 2022). Finally, most approaches aforementioned depend on the availability of sensitive attribute annotations in the training data, and as Kenfack et al. (2023) recently emphasized, employing proxy-sensitive attributes often worsens the fairness-accuracy tradeoff. They also propose a proxy model to retrieve the missing sensitive attributes, adapted to improve the model\u2019s fairness.\nLimits of existing approaches and positioning Compared to the adversarial approaches previously mentioned, ours is conceptually closer to (Nam et al., 2020), while their methodology is conducted on images rather than textual data. We also distinguish from their research by the use of the Wasserstein distance to evaluate the mutual information between the two models\u2019 representations instead of focusing the learning of the main model on samples going against the prejudice of the biased network. Like Ozair et al. (2019), we exploit the Wasserstein distance as an estimator of the mutual information. However, while they use it to measure mutual information to improve representational learning on images, we consider sensitive attributes in the mutual information estimation and use it to improve the model fairness. Our proposition is related to Risser et al. (2022), yet, we do not use the model outputs directly as we use hidden representations, of fixed-size and task-independent dimensions, of Pretrained Language Models that encode information on sensitive attributes. Additionally, we do not use the Wasserstein distance to compute the distance between each group\u2019s prediction probability but to enforce the independence of the representation from unfair representations. By using those latent representations in the Wasserstein-regularization term, the model is encouraged not to encode the sensitive information in the representation during inference. Similarly, in the field of NLP, a related approach is proposed by Cheng et al. (2021). Their method maximizes the mutual information between pairs of sentence representations and their augmented versions, which vary based on the sensitive attribute. These representations go through the same encoder, ensuring that the input is independent of the sensitive information. However, this does not ensure independence between the prediction and the sensitive attribute (Shen et al., 2022a; Cabello et al., 2023). In contrast, our theoretically grounded approach minimizes the mutual information between representations of the same sentence\nprocessed by two different encoders to make the predictions independent of the sensitive attributes. Moreover, their approach depends on identifying biased attribute words, limiting its applicability to cases where substitute words are accessible. This is a constraint we avoid. Lastly, while previous methods primarily targeted classification issues in images or categorical and numerical data, we introduce an approach well-suited for Natural Language Processing. It can be applied to less-explored scenarios, including continuous sensitive attributes and regression tasks."
        },
        {
            "heading": "3 Preliminaries",
            "text": "In this section, we introduce the notations used throughout this paper. We also present the definitions of the necessary fairness metrics, and the main concepts, mostly related to the Wasserstein distance which are essential for understanding the rest of the paper."
        },
        {
            "heading": "3.1 Motivation",
            "text": "We consider a corpus of n triplets {(xi, yi, si)}ni=1, where xi \u2208 X is a short document or a sentence, yi \u2208 Y is a label and si \u2208 S corresponds to one or multiple variables, referred to as sensitive attributes, such as gender, ethnicity or age. Let us consider binary classification for illustrative purposes. The goal is to predict the outcomes yi by estimating the conditional probability p(y = 1|x = xi) through a scoring function f : X \u2192 {0, 1}. The prediction associated with f is noted y\u0302. For example, in the context of a social network, a classifier can use the descriptors of a message (e.g., a bag of word representation), xi, to predict whether a message is toxic or not, leading to the decision to ban the message and/or the user who wrote it from the social platform."
        },
        {
            "heading": "3.2 Measuring Fairness",
            "text": "In this context, of particular relevance is groupbased fairness, which examines how well outcome (y\u0302) consistency is preserved across sensitive groups (s). Returning to our example, when determining whether a message is toxic, fairness here implies that the decision is consistent for all users, regardless of gender or ethnicity.\nA commonly used group-based metric used to quantify the (un)fairness of a given classifier is the Equality of Opportunity EO (Hardt et al., 2016) that is satisfied if the prediction made by the clas-\nsifier is conditionally independent of the protected attribute, given that the true value is positive (e.g. y = 1). In effect, it means that the same proportion of each group receives a positive outcome. For binary sensitive attribute (s \u2208 {a, a\u0304}) and multi-class objectives, the consensual way of aggregating EO score over classes is the GAP score (De-Arteaga et al., 2019; Ravfogel et al., 2020) defined as follows\nGAP = \u221a 1\n|C| \u2211\nc\u2208C (EOc)2, (1)\nwhere the EO for a given class c \u2208 C is defined by\nEOc =p(y\u0302 = c|y = c, s = a) \u2212 p(y\u0302 = c|y = c, s = a\u0304). (2)\nAdditionally, as fairness often requires determining a trade-off such that reaching equity does not degrade the general classification performance, Han et al. (2021a) proposed the Distance To Optimum (DTO) score. This latter measures the accuracy-fairness trade-off by computing the Euclidean distance from a model to an Utopia point (point corresponding to the best Accuracy and best Fairness values across all the baselines). The goal is to minimize the DTO.\nFinally, we consider the Leakage metric that corresponds to the accuracy of a classification model trained to predict the sensitive attribute from the latent representations. It measures the fairness of the latent representations themselves and demonstrates unfairness when close to 100 of accuracy."
        },
        {
            "heading": "3.3 Fairness as Mutual Information minimization",
            "text": "Mutual Information (MI) is an information-theorybased metric meant to measure the statistical dependence or the amount of information shared between two variables. In our context, given the class of a document predicted by our model along with the value of the sensitive attribute of the document, one can use MI to evaluate the strength of their relationship. Formally, the mutual information is defined as the Kullback-Leibler (KL) divergence between the joint distribution p(x, y) and the product of the marginal distributions:\nMI(x, y) = KL(p(x, y)\u2225p(x)p(y)). (3)\nFairness can therefore be cast as MI minimization between y\u0302, our prediction (conditioned on y,\nthe ground-truth or not), and s, the sensitive attribute, as it will make the prediction and the sensitive attribute less and less dependent. Nevertheless, MI is generally intractable for most real-life scenarios and has strong theoretical limitations as outlined by Ozair et al. (2019). Notably, it requires a number of samples exponential in the value of the Mutual Information to build a high confidence lower bound and it is sensitive to small perturbations in the data sample. Consequently, Ozair et al. (2019) proposed a theoretically sound dependency measure, the Wasserstein Dependency Measure, based on Wasserstein-1 distance :\nMIW (x, y) = W1(p(x, y), p(x)p(y)). (4)\nA key feature of the Wasserstein distance is that it can act as a smooth objective function, as shown in the WGAN approach (Arjovsky et al., 2017). More precisely, the Kantorovich-Rubinstein duality expresses W1(p(x, y), p(x)p(y)) as :\nsup ||C||L\u22641 Ex,y\u223cp(x,y)[C(x, y)]\n\u2212 Ex\u223cp(x),y\u223cp(y)[C(x, y)], (5)\nwhere ||C||L \u2264 1 is the set of all 1-Lipschitz functions. Arjovsky et al. (2017) propose to approximate this measure by using a parameterized function, defined as follows :\nmax \u03c9,||Cw||L\u22641 Ex,y\u223cp(x,y)[C\u03c9(x, y)]\n\u2212 Ex\u223cp(x),y\u223cp(y)[C\u03c9(x, y)], (6)\nwhere C\u03c9 is called the critic and is usually a neural network. Wasserstein distance has been efficiently used in many machine learning fields (Frogner et al., 2015; Courty et al., 2014; Torres et al., 2021) and a particularly interesting application is that of fair machine learning (Jiang et al., 2020; Silvia et al., 2020; Gordaliza et al., 2019; Laclau et al., 2021). See Appendix A.1 for further theoretical details on the Wasserstein Distance. The role of this measure in our contribution is detailed in the subsequent sections."
        },
        {
            "heading": "4 Our contribution",
            "text": "We are now ready to show how one can cast the problem of group fairness as an independence constraint in the intermediate latent space of the MLPs and derive a theoretically sound approach based on the Wasserstein distance to solve it."
        },
        {
            "heading": "4.1 Definition of the Objective Function",
            "text": "In most recent NLP applications, deep classification is performed as a two-step approach: the scoring function f is a composition of two parameterized functions such that f = g \u25e6 h, where g(x) = z \u2208 Rd projects x in low dimensional space and h is a simple, usually one linear layer neural network with softmax activation followed by an argmax referred to as a classification layer. The objective of g is to produce an embedding z linearly separable with respect to the target of interest. For the deep model predicting y, we write fc = gc \u25e6 hc = hc(zc), where the index c stands for classification.\nAs stated earlier, fairness can be defined as minimizing MI(y\u0302, s). As s is neither observable nor allowed to be observed in most real-life scenarios, we make use of a surrogate for s that we call the demonic model. This deep model is a composition of fs = gs \u25e6hs = hs(zs), where s stands for sensitive attribute. Therefore, in the absence of the sensitive attribute, we can use :\nmin MI(y\u0302, s\u0302). (7)\nAs the argmax operation producing the hard predictions following the classification layer is not differentiable, we propose to minimize the MI between the latent representations instead of the network final output, leading to optimizing an upper bound of the latter equation (see details in Appendix):\nmin MI(zy, zs) \u2265 MI(y\u0302, s\u0302). (8) Lastly, we replace MI with MIW for the reasons explained earlier. We propose to optimize simultaneously the Wasserstein dependency measure and the more traditional classification loss (e.g. the cross-entropy). Our objective function writes as follow\nargmin L(y, h(zy)) +\u03b2 W1(p(zy, zs), p(zy)p(zs)), (9)\nwhere L is the loss function aiming at maximizing the accuracy of the model for predicting y while the role of the second term is to constrain the encoder part of the language model to learn fair representations. The hyper-parameter \u03b2 \u2208 R+ is a weight allowing some control on the impact of the penalty as the speed of convergence of the two subobjectives may be different. In the following, we refer to the approach minimizing Equation 9 as WFC for Wasserstein Fair Classification (WFC).\n4.2 Optimization of WFC The overall architecture of WFC is presented in Figure 2. Given a batch of documents along with their sensitive attribute, we start by generating a representation of each document using a PLM. Then, taking these vectors as input, we train two MLPs to predict s and y, respectively. The former is referred to as the demonic model in the remained of this paper. Now, assuming that the MLPs consist of\none hidden layer, for the sake of simplicity, we can extract two embedding vectors for all documents, denoted by zs and zy. Note that the prediction y\u0302 made by the second MLP (in green in Figure 2) can be directly used to compute the first part of our objective function (see Equation 9).\nNow for the second term of the loss, which is given by W1(p(zy, zs), p(zy)p(zs)), we use the approximation proposed by Arjovsky et al. (2017):\nmax \u03c9,||Cw||L\u22641 Ezy ,zs\u223cp(zy ,zs)[C\u03c9(zy, zs)]\n\u2212Ezy\u223cp(zy),zs\u223cp(zs)[C\u03c9(zy, zs)]. (10)\nIn this context, C\u03c9 is a MLP, referred to as the critic in Figure 2. To enforce the Lipschitz constraint, we clamp the weights to given values ([\u22120.01, 0.01]) at each optimization step2. For a batch of documents, the critic takes as input the concatenation of zy and zs on the one hand, and the concatenation of zy and zs randomly drawn from the dataset (equivalent to zy \u223c p(zy), zs \u223c p(zs)), on the other hand. We then follow the training procedure introduced by Arjovsky et al. (2017) which alternate maximizing Equation 10 in the critic parameters for nc iterations and minimizing Equation 9 for nd iterations in the fy classifier parameters. The overview of the training process is detailed in the appendix B.3\nTraining the demonic model We pre-train the demonic model to predict the sensitive attributes and do not update the demonic weights during the training phase of the main model. The benefits are twofold. Firstly, unlike previous works (Caton and Haas, 2020), we require only limited access to sensitive attributes label at training and we do not need access to the labeling of sensitive attributes in the inference regime. As a result, WFC is highly compatible with recent regulations (e.g., US Consumer Financial Protection Bureau). Secondly, the demonic model can now be trained in a few-shot fashion if some examples of the training set are annotated with sensitive attributes. However, when no sensitive attributes are available in the training set, we replace the training data of the demonic part of the architecture with data from another domain (e.g. another dataset) containing sensitive information for the same attribute. For example, for gender,\n2We also tested some more recent improvements of Lipschitz constraint enforcement (Gulrajani et al., 2017; Wei et al., 2018). Interestingly, all lead to poor performance\nwe can leverage generated datasets, like the EEC dataset (Kiritchenko and Mohammad, 2018). Thus, we transfer this knowledge from one dataset to another, working towards fairness autonomy regardless of the inclusion of sensitive attributes within the data."
        },
        {
            "heading": "5 Experimental Protocol",
            "text": "In our experiments, we intensively use the FairLib package (Han et al., 2022), which provides an access to many state-of-the-art methods and datasets."
        },
        {
            "heading": "5.1 Dataset",
            "text": "We employ two widely-used datasets to evaluate fairness in the context of text classification, building upon prior research (Ravfogel et al., 2020; Han et al., 2021c; Shen et al., 2022a). Both datasets are readily available in FairLib.\nBias in Bios (De-Arteaga et al., 2019). This dataset, referred to as \u2018Bios dataset\u2019 in the rest of the paper, consists of brief biographies associated with occupations (a total of 28) and genders (male or female). As per the partitioning prepared by (Ravfogel et al., 2020), the training, validation, and test sets comprise 257, 000, 40, 000 and 99, 000 samples, respectively.\nMoji (Blodgett et al., 2016). This dataset contains tweets written in either \"Standard American English\" (SAE) or \"African American English\" (AAE), annotated with positive or negative polarity. We use the dataset prepared by (Ravfogel et al., 2020), which includes 100, 000 training examples, 8, 000 validation examples, and 8, 000 test examples. The target variable y represents the polarity, while the protected attribute corresponds to the ethnicity, indicated by the AAE/SAE attribute."
        },
        {
            "heading": "5.2 Baselines",
            "text": "Except for the classical cross-entropy loss without fairness constraint (CE) that we run ourselves, we report the results from (Shen et al., 2022a; Han et al., 2022) on these two datasets. The considered baselines are INLP (Ravfogel et al., 2020), the ADV method (Han et al., 2021c), FairBatch (Roh et al., 2021), GATE (Han et al., 2021a) and Con, displaying the dp and eo versions (Shen et al., 2022a)."
        },
        {
            "heading": "5.3 Evaluation Tasks",
            "text": "For training a vanilla text classification model, we follow the protocol proposed by Han et al. (2022):\na frozen BERT encoder followed by a 3-layer MLP. We use accuracy to assess the classification performance. Fairness for all models is evaluated against three metrics presented earlier: GAP, referred to as \u201cFairness\u201d in previous works (Han et al., 2022; Shen et al., 2022a), and the Distance To Optimum (DTO) proposed by Han et al. (2021a) (we follow the methodology of Shen et al. (2022a) and evaluate the DTO on the average fairness and accuracy of the best empirical results for each metric over all models to build the utopia point). Finally, we consider the Leakage score.\nTask 1: Fair Classification We first compare our method against state-of-the-art approaches. We use the representation generated by a base BERT model as an input to the MLPs. For Bios, the demonic MLP is trained on 1% of the training set and obtains 99% accuracy for predicting the sensitive attributes on the test set. Similarly, the demonic MLP obtains 88.5% accuracy on Moji.\nTask 2: Demonic transfer We conduct these experiments for Bios and train a demonic MLP either on the EEC dataset (Kiritchenko and Mohammad, 2018) or the Marked Personas dataset (Cheng et al., 2023). We then evaluate the performance of the demonic MLP to predict gender on the Bios test dataset. When training on the EEC dataset we obtain 98.1% of accuracy, and on the Marked Personas dataset, we obtain 98.4% of accuracy. We repeat Task 1, with those variants of the demonic MLP. We focus on Bios in this experiment. For Moji, it would require to have access to other datasets with the same protected attribute (ethnicity).\nTask 3: Use of representations from different layers In the previous experiments, following approaches presented in (Han et al., 2022), the Wasserstein distance is approximated using the last hidden representations of the 3-layer MLP. We compare this approach, on both datasets, with the use of the first hidden representations of the MLP and with the output logits. For the latter, the Wasserstein is estimated between distributions of different dimensions: for example, in the case of Bios, 2 for the demonic MLP corresponding to the sensitive attributes and 28 for the classification MLP corresponding to the labels.\nTask 4: Independence with predicted hard sensitive attributes To evaluate the impact of using\nthe representation zs, we reproduce Task 1, but replace zs with the sensitive attributes predicted by the demonic MLP: s\u0302 and concatenate zy and s\u0302 dependently and independently when computing the Wasserstein distance. Note that we do not encounter a problem with the non-differentiability for y\u0302 (with the argmax operation as for s\u0302 as mentioned in Section 4.1) since the demonic model is pre-trained."
        },
        {
            "heading": "5.4 Implementation Details",
            "text": "Our architecture is composed of three components: two classifiers and a critic. The details of the MLPs used to parameterize each component are given in Appendix B. We find the best hyperparameters for our models by grid-search cross-validation over the MLP and Critic learning rates, the value of nd (number of batches used to train the main MLP), the layers producing zs and zy, the value of \u03b2 and the value used to clamp the weights to enforce the Lipschitz constraint. The values allowing us\nto obtain the lower DTO during this process are presented in Appendix B. The architecture details of the MLP for the leakage are provided in (Shen et al., 2022a) as we use the same configuration."
        },
        {
            "heading": "6 Results",
            "text": ""
        },
        {
            "heading": "6.1 Task 1: Fair Classification",
            "text": "We compare WFC with text classification baselines. For Moji, (Table 1 and Fig. 3), accuracy of WFC is higher than the accuracy of CE and it is equivalent to competitors. On the fairness metrics, we outperform all other baselines and obtain the best DTO. For Bios (Table 1 and Fig. 3), our method is competitive with the other baselines and ranks 4 out of 9 in terms of accuracy-fairness trade-off. In comparison, with equivalent methods in terms of\nDTO (INLP, FairBatch, and Adv), WFC improves either the performance or the fairness. Especially, WFC has the second-best accuracy compared to baselines. Finally, we note that WFC is more stable in terms of Fairness compared with other approaches having on average the best results for this metric (along with a smaller standard deviation). Eventually, even when our method does not outperform the baselines (e.g., Bios dataset), it still exhibits noteworthy properties, particularly its ability to achieve competitive performances without access to the sensitive attributes in the training set. We evaluate this capability in the next subsection. We also explore the ability of our proposition to improve the leakage. We initially aim at improving the fairness while maintaining the accuracy of the model. Yet, our method allows to improve leakage by increasing the value of \u03b2 in Equation 9, in other words, we give more importance to the Wasserstein regularization in the loss. We note in Table 2 that with a higher \u03b2, the leakage decreases. However, on both datasets, the accuracy, that we want to preserve, decreases and the trade-off worsens as we get better for the leakage. To sum up, reducing leakage makes it more challenging to retrieve sensitive attributes but could result in unintended information loss needed for the classification task affecting the performance. Ultimately, we want to enhance fairness while keeping a good performance and this objective may not necessarily match with a strong leakage improvement."
        },
        {
            "heading": "6.2 Task 2: Demonic Transfer",
            "text": "For this task, we pre-train the demonic model on other datasets. Table 3a shows that we achieve similar results as when the pre-training is done using the same dataset. The average loss of accuracy and fairness are not significant. These results are promising for improving fairness, especially in situations where collecting sensitive data is not feasible or when only partial information is accessible."
        },
        {
            "heading": "6.3 Task 3: Use of representations from different layers",
            "text": "On both datasets (Table 3b), accuracy is rather stable regardless of the layers used to compute the Wasserstein distance. Still, the best results are obtained using the last hidden representations. However, while we note a slight decrease in fairness on Bios when using representations from other layers, the decrease becomes much more significant on Moji. Using the last hidden layer is the best option."
        },
        {
            "heading": "6.4 Task 4: Independence with predicted hard sensitive attributes",
            "text": "We replace zs by the predicted s\u0302 to compute the Wasserstein distance and report the results in Table 3c. We observe, on average, a slight improvement of the accuracy on Bios, and a slight decrease in accuracy on Moji. However, while the decrease in fairness is not significant for Bios, we observe a substantial drop for Moji. As a result, using s\u0302 instead of zs seems to have a neutral impact at best, this may also result, in some cases, in a reduction of both accuracy and fairness."
        },
        {
            "heading": "7 Conclusion",
            "text": "We presented WFC a novel method that enforces fairness constraints using a pre-trained neural network on the sensitive attributes and Wasserstein regularization. Our model is theoretically well-motivated and has interesting properties over existing models. The most important one is the fact that it does not require annotation of the sensitive attribute at\nboth training and inference time. We obtain competitive results compared to baselines on the Bios dataset and outperform them on the fairness score with comparable accuracy on Moji dataset. Furthermore, we present a solution for our algorithm to be trained when sensitive attributes are not available for a given dataset, paving the way for its use under realistic applications. In further studies, we will focus on applying this method using different text encoders or decoders, datasets, and downstream tasks, as this method can generalize to tasks out of the text classification scope, notably, regression and even unsupervised objectives.\nLimitations\nThe proposed approach is rather flexible as it can handle various types of sensitive attributes. However, due to the lack of available datasets, we were not able to assess our performance for continuous sensitive attributes, e.g. age. In addition, we are aware that gender may embrace a n-ary definition, in all our experiments, we were limited to consider-\ning only men vs women classification, due to data availability.\nFor Task 2 defined in the experiments section, we were able to show empirically that our method works well when the demonic model is pre-trained on a different dataset when no sensitive attributes or very few of them are available on the main training dataset. However, we do not provide sufficient generalization guarantees to consider an out-of-thebox large-scale deployment. The next step will be to derive theoretical guarantees inspired by the results of domain adaptation to assess how well this idea can be generalized to other data and under which conditions it might fail or succeed.\nFinally, for Task 1 we did not perform a statistical test to assess the significance of the observed differences. Indeed, most of the results were reported from (Shen et al., 2022a) and we were unable to retrieve the scores for each run.\nEthics Statement\nWe acknowledge the following concerns about our work. First, biases present in the data are UScentered, and concerning the Bias in Bios dataset genders are binary. Furthermore, to conduct our research we need to have access to the sensitive attributes contrary to what privacy measures recommend, and the latter are annotated with a risk of subjectivity."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was funded by the french National Agency for Research (ANR) in the context of the Dik\u00e9 project (ANR-21-CE23-0026). Our experiments utilize the previously mentioned Fairlib framework. We would like to express our gratitude to Xudong Han for his availability and assistance in using it."
        },
        {
            "heading": "A Theoretical background and Proof",
            "text": "A.1 Wasserstein Distance Finding correspondences between two sets of points is a longstanding issue in machine learning. The optimal transport (OT) (Monge, 1781) problem offers an efficient solution to this issue by calculating an optimal one-to-one transport map between the two sets. This map is determined by considering the geometrical proximity of the\nAlgorithm 1: WFC Algorithm Data: D = {(xi, yi, si)}ni=1 the training set, ne the number of epochs, nc and nd the number of\ntraining iterations per epoch for the critic and the classifier respectively, a batch size nb, two neural networks hs(x) and hc(x; \u03b8), a Critic C\u03c9, a weight on the regularization \u03b2\nfor e = 1, ..., ne do for t = 1, ..., nc do\nSample {xi, yi, si}nbi=1 Encode : zs \u2190 {hs(xi)}nbi=1, zy \u2190 {hc(xi)}nbi=1 Concatenate vectors to get Zdep \u2190 [zs,i, zy,i]nbi=1 Shuffle the zs,i vectors. Concatenate vectors to get Zind \u2190 [zs,i, zy,i]nbi=1 grad(w)\u2190 \u2207\u03c9 1nb ( \u2211nb i=1C\u03c9(Zdep,i)\u2212 \u2211nb i=1C\u03c9(Zind,i))\n\u03c9 \u2190 Adam(\u03c9; grad(w)) end for t = 1, ..., nd do\nSample {xi, yi, si}nbi=1 Encode : zs \u2190 {hs(xi)}nbi=1, zy \u2190 {hc(xi)}nbi=1 Concatenate vectors to get Zdep = [zs,i, zy,i] nb i=1 Shuffle the zs,i vectors. Concatenate vectors to get Zind = [zs,i, zy,i] nb i=1\nL \u2190\u2211nbi=1 L(yi, h(zy,i)) L \u2190 L+ \u03b2(\u2211nbi=1C\u03c9(Zdep,i)\u2212 \u2211nb i=1C\u03c9(Zind,i))\n\u03b8 \u2190 Adam(\u03b8;\u2207\u03b8 1nbL) end\nend\npoints in the sets. In practice, OT can be expressed as a problem of aligning two empirical distributions PX1 and PX2 supported on two point sets X1 = {x(i)1 \u2208 Rd}N1i=1 and X2 = {x (j) 1 \u2208 Rd}N2i=1. We consider the Monge-Kantorovich formulation of this problem (Kantorovich, 1942) where the goal is to search for a coupling \u03b3 defined as a joint probability distribution over X1 \u00d7X2 with marginals PX1 and PX2 . This amounts to minimizing the cost of transport w.r.t. some metric l : X1 \u00d7X2 \u2192 R+:\nmin \u03b3\u2208\u03a0(PX1 ,PX2 )\n\u27e8M,\u03b3\u27e9F (11)\nwhere \u27e8\u00b7,\u00b7\u27e9F is the Frobenius dot product, M is a dissimilarity matrix, i.e., Mij = l(x (i) 1 , x (j) 2 ), defining the cost of associating x(i)1 with x (j) 2 and \u03a0(PX1 ,PX2) is a set of doubly stochastic matrices. This problem admits a unique solution \u03b3\u2217 and defines a metric on the space of probability measures called the Wasserstein distance (also known as the Earth-Mover Distance) as follows:\nWM (PX1 ,PX2) = min \u03b3\u2208\u03a0(PX1 ,PX2 ) \u27e8M,\u03b3\u27e9F .\nA.2 Proof that MI(zy, zs) \u2265 MI(y\u0302, s\u0302) We have :\np(y\u0302, zy, s\u0302) = p(y\u0302|zy, s\u0302)p(zy|s\u0302)p(s\u0302) (12) Recall that s\u0302 = hs(zs) and y\u0302 = hc(zy), therefore, y\u0302 is fully determined by zy, hence, p(y\u0302|zy, s\u0302) = p(y\u0302|zy). Then :\np(y\u0302, zy, s\u0302) = p(y\u0302|zy)p(zy|s\u0302)p(s\u0302). (13) Therefore, the three variables follow the markov\nproperty\ns\u0302\u2192 zy \u2192 hc(zy). (14) In such context, it was shown that (Yeung, 1991):\nMI(zy, s\u0302) \u2265 MI(y\u0302, s\u0302). (15) Following the same logic, we have\nMI(zy, zs) \u2265 MI(zy, s\u0302) (16) hence\nMI(zy, zs) \u2265 MI(y\u0302, s\u0302). (17)"
        },
        {
            "heading": "B Experimental details",
            "text": "In this section, we provide additional experimental details, notably, we detail the MLP architectures, give the optimal hyperparameters, and describe the full algorithm of WFC.\nB.1 MLP architectures\nIn Table 4, we present the architectural details of the classifier MLP. We grid searched over the learning rate (lr \u2208 {1e\u2212 5, 1e\u2212 4, 1e\u2212 3, 5e\u2212 5, 5e\u2212 4, 5e\u2212 3}, the number of training batches for classification per epoch nd \u2208 {5, 10, 20}, the value used to clamp the weights to enforce the Lipschitz constraint clamp value \u2208 {0.001, 0.01, 0.1}, the parameter \u03b2 \u2208 {0.5, 1, 2}, the layer used between the first hidden, last hidden, or last layer.\nB.2 Critic architecture\nIn Table 5, we present the architectural details of the Critic, which is a simple multi-layer perceptron. We grid searched over the learning rate lr \u2208 {5e\u2212 5, 5e\u2212 4, 5e\u2212 3}.\nB.3 Algorithm of WFC In Algorithm 1, we provide the detailed algorithm for WFC used in our experiments."
        }
    ],
    "title": "Fair Text Classification with Wasserstein Independence",
    "year": 2023
}