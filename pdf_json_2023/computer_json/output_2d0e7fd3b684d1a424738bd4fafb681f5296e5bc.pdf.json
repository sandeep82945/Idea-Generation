{
    "abstractText": "LiDAR-based place recognition (LPR) is one of the most crucial components of autonomous vehicles to identify previously visited places in GPS-denied environments. Most existing LPR methods use mundane representations of the input point cloud without considering different views, which may not fully exploit the information from LiDAR sensors. In this article, we propose a crossview transformer-based network, dubbed CVTNet, to fuse the range image views (RIVs) and bird\u2019s eye views (BEVs) generated from the LiDAR data. It extracts correlations within the views using intra-transformers and between the two different views using inter-transformers. Based on that, our proposed CVTNet generates a yaw-angle-invariant global descriptor for each laser scan end-to-end online and retrieves previously seen places by descriptor matching between the current query scan and the pre-built database. We evaluate our approach on three datasets collected with different sensor setups and environmental conditions. The experimental results show that our method outperforms the state-of-the-art LPR methods with strong robustness to viewpoint changes and long-time spans. Furthermore, our approach has better real-time performance that can run faster than the typical LiDAR frame rate does. The implementation of our method is released as open source at: https://github.com/BIT-MJY/CVTNet.",
    "authors": [
        {
            "affiliations": [],
            "name": "Junyi Ma"
        },
        {
            "affiliations": [],
            "name": "Guangming Xiong"
        },
        {
            "affiliations": [],
            "name": "Jingyi Xu"
        },
        {
            "affiliations": [],
            "name": "Xieyuanli Chen"
        }
    ],
    "id": "SP:05cf44e92fef00f9bc03b39e19de05579dc9c358",
    "references": [
        {
            "authors": [
                "X. Chen",
                "T. L\u00e4be",
                "A. Milioto",
                "T. R\u00f6hling",
                "J. Behley",
                "C. Stachniss"
            ],
            "title": "OverlapNet: A Siamese Network for Computing LiDAR Scan Similarity with Applications to Loop Closing and Localization",
            "venue": "Autonomous Robots, vol. 46, pp. 61\u201381, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Ma",
                "S. Wang",
                "K. Zhang",
                "Z. He",
                "J. Huang",
                "X. Mei"
            ],
            "title": "Fast and robust loop-closure detection via convolutional auto-encoder and motion consensus",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 18, no. 6, pp. 3681\u20133691, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Liu",
                "S. Zhou",
                "C. Suo",
                "P. Yin",
                "W. Chen",
                "H. Wang",
                "H. Li",
                "Y.-H. Liu"
            ],
            "title": "Lpd-net: 3d point cloud learning for large-scale place recognition and environment analysis",
            "venue": "Proc. of the IEEE/CVF Intl. Conf. on Computer Vision (ICCV), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "K. Vidanapathirana",
                "M. Ramezani",
                "P. Moghadam",
                "S. Sridharan",
                "C. Fookes"
            ],
            "title": "Logg3d-net: Locally guided global descriptor learning for 3d place recognition",
            "venue": "Proc. of the IEEE Intl. Conf. on Robotics & Automation (ICRA), 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Ma",
                "J. Zhang",
                "J. Xu",
                "R. Ai",
                "W. Gu",
                "X. Chen"
            ],
            "title": "Overlaptransformer: An efficient and yaw-angle-invariant transformer network for lidarbased place recognition",
            "venue": "IEEE Robotics and Automation Letters (RA-L), vol. 7, no. 3, pp. 6958\u20136965, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "G. Kim",
                "A. Kim"
            ],
            "title": "Scan context: Egocentric spatial descriptor for place recognition within 3d point cloud map",
            "venue": "Proc. of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "D. Cattaneo",
                "M. Vaghi",
                "A. Valada"
            ],
            "title": "Lcdnet: Deep loop closure detection and point cloud registration for lidar slam",
            "venue": "IEEE Transactions on Robotics, vol. 38, no. 4, pp. 2074\u20132093, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Fan",
                "Z. Song",
                "H. Liu",
                "Z. Lu",
                "J. He",
                "X. Du"
            ],
            "title": "Svt-net: Super light-weight sparse voxel transformer for large scale place recognition.",
            "year": 2022
        },
        {
            "authors": [
                "J. Komorowski"
            ],
            "title": "Minkloc3d: Point cloud based large-scale place recognition",
            "venue": "Proc. of the IEEE Winter Conf. on Applications of Computer Vision (WACV), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M.Y. Chang",
                "S. Yeon",
                "S. Ryu",
                "D. Lee"
            ],
            "title": "Spoxelnet: Spherical voxelbased deep place recognition for 3d point clouds of crowded indoor spaces",
            "venue": "2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2020, pp. 8564\u20138570.",
            "year": 2020
        },
        {
            "authors": [
                "A. Zaganidis",
                "A. Zerntev",
                "T. Duckett",
                "G. Cielniak"
            ],
            "title": "Semantically assisted loop closure in slam using ndt histograms",
            "venue": "2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2019, pp. 4562\u20134568.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Zhou",
                "C. Zhao",
                "D. Adolfsson",
                "S. Su",
                "Y. Gao",
                "T. Duckett",
                "L. Sun"
            ],
            "title": "Ndt-transformer: Large-scale 3d point cloud localisation using the normal distribution transform representation",
            "venue": "Proc. of the IEEE Intl. Conf. on Robotics & Automation (ICRA), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "H. Wang",
                "C. Wang",
                "L. Xie"
            ],
            "title": "Intensity scan context: Coding intensity and geometry relations for loop closure detection",
            "venue": "Proc. of the IEEE Intl. Conf. on Robotics & Automation (ICRA), 2020.",
            "year": 2020
        },
        {
            "authors": [
                "L. Luo",
                "S.-Y. Cao",
                "B. Han",
                "H.-L. Shen",
                "J. Li"
            ],
            "title": "Bvmatch: Lidarbased place recognition using bird\u2019s-eye view images",
            "venue": "IEEE Robotics and Automation Letters (RA-L), vol. 6, no. 3, pp. 6076\u20136083, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "P. Yin",
                "L. Xu",
                "J. Zhang",
                "H. Choset"
            ],
            "title": "Fusionvlad: A multi-view deep fusion networks for viewpoint-free 3d place recognition",
            "venue": "IEEE Robotics and Automation Letters (RA-L), vol. 6, no. 2, pp. 2304\u20132310, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "X. Ye",
                "J. Ma"
            ],
            "title": "Neighborhood manifold preserving matching for visual place recognition",
            "venue": "IEEE Transactions on Industrial Informatics, pp. 1\u201310, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "R. Arandjelovic",
                "P. Gronat",
                "A. Torii",
                "T. Pajdla",
                "J. Sivic"
            ],
            "title": "Netvlad: Cnn architecture for weakly supervised place recognition",
            "venue": "Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 2016. AUTHOR et al.: TITLE 9",
            "year": 2016
        },
        {
            "authors": [
                "L. Li",
                "M. Yang",
                "C. Wang",
                "B. Wang"
            ],
            "title": "Hybrid filtering framework based robust localization for industrial vehicles",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 14, no. 3, pp. 941\u2013950, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Wang",
                "Z. Sun",
                "C.-Z. Xu",
                "S.E. Sarma",
                "J. Yang",
                "H. Kong"
            ],
            "title": "Lidar iris for loop-closure detection",
            "venue": "Proc. of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS), 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M.A. Uy",
                "G.H. Lee"
            ],
            "title": "Pointnetvlad: Deep point cloud based retrieval for large-scale place recognition",
            "venue": "Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "C.R. Qi",
                "H. Su",
                "K. Mo",
                "L.J. Guibas"
            ],
            "title": "Pointnet: Deep learning on point sets for 3d classification and segmentation",
            "venue": "Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "R. Dub\u00e9",
                "A. Cramariuc",
                "D. Dugas",
                "H. Sommer",
                "M. Dymczyk",
                "J. Nieto",
                "R. Siegwart",
                "C. Cadena"
            ],
            "title": "Segmap: Segment-based mapping and localization using data-driven descriptors",
            "venue": "The International Journal of Robotics Research, vol. 39, no. 2-3, pp. 339\u2013355, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Zhao",
                "P. Yin",
                "G. Yi",
                "S. Scherer"
            ],
            "title": "Spherevlad++: Attention-based and signal-enhanced viewpoint invariant descriptor",
            "venue": "arXiv preprint arXiv:2207.02958, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Cui",
                "X. Chen"
            ],
            "title": "Ccl: Continual contrastive learning for lidar place recognition",
            "venue": "IEEE Robotics and Automation Letters, vol. 8, no. 8, pp. 4433\u20134440, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Proc. of the Advances in Neural Information Processing Systems (NIPS), 2017.",
            "year": 2017
        },
        {
            "authors": [
                "A. Zadeh",
                "P.P. Liang",
                "N. Mazumder",
                "S. Poria",
                "E. Cambria",
                "L.-P. Morency"
            ],
            "title": "Memory fusion network for multi-view sequential learning",
            "venue": "Proc. of the Conference on Advancements of Artificial Intelligence (AAAI), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Pham",
                "Hai",
                "Liang",
                "Paul Pu",
                "Manzini",
                "Thomas",
                "Morency",
                "Louis-Philippe",
                "P\u00f3czos",
                "Barnab\u00e1s"
            ],
            "title": "Found in Translation: Learning Robust Joint Representations by Cyclic Translations between Modalities",
            "venue": "Proc. of the Conference on Advancements of Artificial Intelligence (AAAI), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "N. Carlevaris-Bianco",
                "A.K. Ushani",
                "R.M. Eustice"
            ],
            "title": "University of michigan north campus long-term vision and lidar dataset",
            "venue": "Intl. Journal of Robotics Research (IJRR), vol. 35, no. 9, pp. 1023\u20131035, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "A. Geiger",
                "P. Lenz",
                "R. Urtasun"
            ],
            "title": "Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite",
            "venue": "Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2012.",
            "year": 2012
        },
        {
            "authors": [
                "J. Ma",
                "X. Chen",
                "J. Xu",
                "G. Xiong"
            ],
            "title": "Seqot: A spatial-temporal transformer network for place recognition using sequential lidar data",
            "venue": "IEEE Transactions on Industrial Electronics, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Johnson",
                "M. Douze",
                "H. J\u00e9gou"
            ],
            "title": "Billion-scale similarity search with gpus",
            "venue": "IEEE Trans. on Big Data, vol. 7, no. 3, pp. 535\u2013547, 2019.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014 LiDAR Place Recognition; Autonomous Driving; Transformer Network; Multi-View Fusion\nI. INTRODUCTION\nPlace recognition provides the current global location of the vehicle in the previously seen environments, which is an important component of robotic simultaneous localization and mapping (SLAM) and global localization. During online operation, it retrieves the reference scan in the database most similar to the current query by directly regressing the similarity [1], [2] or descriptor matching [3], [4], [20]. LiDAR-based place recognition methods [5]\u2013[7] can be applied to large-scale outdoor environments due to their robustness to illumination and weather changes. Different representation forms of LiDAR data have been exploited as input for LPR methods, such as 3D point clouds [4], [20], voxels [8]\u2013[10], normal distributions transform cells [11], [12], bird\u2019s eye views (BEVs) [6], [13], [14], and range image views (RIVs) [1], [5]. However, few methods fuse them together to exploit the advances of different views. Yin et al. [15] proposed FusionVLAD combining the different input forms, but they simply used fully connected\nThis work was supported by the National Natural Science Foundation of China under Grant 52372404.\nJ. Ma, G. Xiong, and J. Xu are with the Beijing Institute of Technology. X. Chen is with the National University of Defense Technology.\n\u2217Corresponding author email: xieyuanli.chen@nudt.edu.cn\nlayers to fuse features, thus not yet thoroughly digging out the inner relations between different representations.\nTo fill this gap, a novel cross-view transformer network named CVTNet is proposed for LiDAR-based place recognition in this work. It utilizes the combination of multi-layer RIVs and BEVs generated from 3D LiDARs mounted on autonomous vehicles. Exploiting the correspondence between the RIVs and BEVs of the same LiDAR scan as shown in Fig. 1, these two input views are first compressed into aligned sentence-like features. Then, our proposed CVTNet uses intra- and inter-transformers to extract the correlation within and cross the aligned sentences from different views. Based on the proposed novel transformer network, our method generates a yaw-angle-invariant global descriptor for each place in an end-to-end manner and, in the end, achieves robust place recognition based on descriptor matching.\nThe main contribution of this article is an end-to-end transformer network that fuses RIVs and BEVs of LiDAR data to achieve reliable long-term place recognition. Unlike the existing LPR methods using mundane input forms, our method exploits both RIVs and BEVs of LiDAR data. Furthermore, we extend the vanilla single-channel RIVs and BEVs to multi-layer formats, which enables our transformer network to reweigh the LiDAR data from different ranges and heights, thus generating more reliable and distinctive features. To our best knowledge, this is the first work to dig out the latent alignment of RIV and BEV features from LiDAR point clouds and use the property to generate distinguishable representations for places. Our proposed CVTNet is also the first network exploiting cross-attention to fuse the multiview LiDAR data and generate yaw-angle-invariant global descriptors for LiDAR place recognition. Benefiting from the devised yaw-angle-invariant architecture, CVTNet is robust to viewpoint changes and achieves reliable place recognition even when the car drives in opposite directions, which is thoroughly evaluated on the NCLT, KITTI, and self-recorded datasets. ar X\niv :2\n30 2.\n01 66\n5v 2\n[ cs\n.C V\n] 6\nO ct\n2 02\n3\nThe experimental results show that our proposed method outperforms the state-of-the-art methods in terms of both loop closure detection and place recognition. The ablation studies on the intra- and inter-transformer modules and the validation experiments on yaw-angle-invariance are further provided.\nIn sum, we make three claims that our approach is able to (i) achieve state-of-the-art performance in place recognition and loop closure detection in outdoor large-scale environments using multi-view LiDAR data, (ii) recognize places well despite different viewpoints based on the proposed yaw-angleinvariant architecture, (iii) achieve online operation with a runtime less than 50 ms faster than the typical LiDAR frame rate. All claims are supported by our experimental evaluation on multiple datasets.\nThe remainder of this paper is organized as follows. Section II will investigate the related works. In Section III, the architecture of our proposed CVTNet and the related mathematical derivation will be described in detail. Section IV will present the experimental setups and results as well as corresponding analysis. Finally, Section V concludes the paper."
        },
        {
            "heading": "II. RELATED WORK",
            "text": "Place recognition has been widely investigated in robotics and computer vision [16], [17]. In this article, we only focus on LiDAR-based place recognition (LPR).\nLPR is a classic research topic, and many traditional handcrafted methods have been proposed [6], [13], [14], [18], [19]. Recently, more and more learning-based methods have shown significant improvements with the development of neural network techniques. Parts of existing works use the raw point cloud or its voxelized and segmented forms as the input of the model. For example, Uy et al. [20] first combine the commonly used PointNet [21] with a differentiable VLAD architecture [17], providing the basic framework for the following methods to extract global descriptors from point clouds. Liu et al. [3] proposed LPD-Net which utilizes a graph neural network to aggregate multiple local features extracted from point clouds. SegMap proposed by Dube\u0301 et al. [22] first extracts the segments from the point clouds as the input of the devised convolution neural network. LoGG3D-Net by Vidanapathirana et al. [4] exploits sparse convolution directly on raw point clouds, and considers both local consistency loss and scene-level loss. In contrast, MinkLoc3D by Komorowski [9] combines the sparse voxelized representation and convolutions to tackle the unordered set problem. More recently, LCDNet proposed by Cattaneo et al. [7] uses an adapted PV-RCNN architecture to extract features that are used to generate global descriptors for point clouds and estimate the relative poses between the query and reference scan.\nAlthough point cloud-based LPR methods have achieved reasonable results, most of them cannot operate online, thus hindering the deployment in real-world applications. To tackle this, many other works use image-like representations of LiDAR data, i.e., RIVs or BEVs, to improve the recognition efficiency and investigate more significant properties. For example, OverlapNet [1] utilizes the range images to directly regress out the overlap values and relative yaw angles\nbetween query and reference scans. Following the overlap conception, OverlapTransformer [5] takes advantage of the naturally yaw-angle-equivariance and builds a yaw-angleinvariant architecture. SphereVLAD++ by Zhao et al. [23] also uses RIVs to extract viewpoint-invariant global descriptors. Different from RIVs, BEVs lose some fine depth information but maintain stable height information, which is useful for long-term localization. For example, Scan Context [6] and Intensity Scan Context [13] both utilize the top-down view to encode each point cloud to a feature matrix. LiDAR-Iris [19] further extends the context-based methods to the frequency domain using Fourier transform. Inspired by LiDAR-Iris, BVMatch [14] also refers to the advantages of frequency domain, and extracts keypoints and BVFT descriptors for bag-of-words based retrieval. CCL [24] proposes to use contrastive learning to extract more distinctive descriptor of places to achieve better LPR results.\nAll the LPR methods mentioned above only use one single input format of point clouds without combining different formats to further enhance the representative ability of the extracted features and descriptors. FusionVLAD by Yin et al. [15] is the first work that considers the inputs from different views of raw point clouds. However, it only applies fully connected networks to combine the extracted features, which can not fully exploit different information from multi-views. Transformer [25] has recently been deployed to better extract features for place recognition [5], [8], [12]. However, there is no work successfully using transformers to fuse the features from different input forms or views. In this article, we propose a novel transformer network to extract the correlation within and between RIVs and BEVs. We explicitly align the RIV and BEV features from the same LiDAR data, thus better finding the relations between the sentence-like features from the dual views using cross-transformer and extracting more distinct global descriptors for more accurate place recognition."
        },
        {
            "heading": "III. PROPOSED METHOD",
            "text": "The overview of our proposed method is depicted in Fig. 2. CVTNet consists of three modules, a multi-view multi-layer generator (MMG), an aligned feature encoder (AFE), and a multi-view fusion module (MVF). MMG first splits the region by range and height intervals, and conducts spherical and topdown projection respectively to generate multi-layer input data (see Sec. III-A). The following AFE takes the multi-layer RIVs and BEVs as inputs and uses a fully convolutional encoder to generate sentence-like features. It then applies an intratransformer to extract the inner correlation of the compressed features of each view (see Sec. III-B). Finally, the enhanced feature volumes are fed to the MVF module. It first uses an inter-transformer to fuse the aligned features from different views and then applies NetVLAD with multi-layer perceptrons (MLPs) on both single-view and fused features to generate the final 1-D global descriptor (see Sec. III-C). The final descriptor is invariant against yaw-angle rotation (see Sec. IIID). We train our network using the triplet loss with overlap labels to better distinguish the positive and negative examples (see Sec. III-E).\nAUTHOR et al.: TITLE 3\nIntraTransformer IntraTransformer\nNetVLAD NetVLAD InterTransformer NetVLAD M L P M L P M L P M L P M L P M L P Global Descriptor Multi-Layer Range Images OverlapNetLeg Multi-Layer BEVs OverlapNetLeg Sphere View Top-Down View\nIntraTransformer\nIntraTransformer\nNetVLAD\nNetVLAD\nInterTransformer\nNetVLAD\nM L P\nM L P\nM L P\nM L P\nM L P\nM L P\nMulti-Layer RIVs\nOverlapNetLeg\nMulti-Layer BEVs\nOverlapNetLeg\nRange Image View\nBird s Eye View\nAligned Feature Encoder Multi-View Fusion Module Multi-View Multi-Layer Generator\nGlobal Descriptor\nWord-Aligned\nWord-Aligned\nFig. 2: Pipeline overview of our proposed CVTNet. It first generates multi-layer RIV and BEV images for the two separate branches, and encodes the aligned sentence-like features respectively using the combination of OverlapNetLeg and the intra-transformer module. Then, the inter-transformer module is utilized to extract the inner correlation cross views. In the end, NetVLAD with MLP is exploited on three branches respectively to generate the final yaw-angle-invariant global descriptors for fast retrieval of place recognition."
        },
        {
            "heading": "A. Multi-View Multi-Layer Generator",
            "text": "It is important for LPR methods to extract discriminative features from different scans. Existing works obtain RIVs or BEVs using single spherical or top-down projection leads to data loss, thus degrading retrieving performance. To fully exploit all information from LiDAR data while maintaining fast speed, we propose to use multiple projections at different ranges/heights and form multi-layer inputs based on space splits. By doing that, our proposed network learns to weigh information from different ranges/heights differently, thus extracting more representative features from scans.\nMore specifically, we propose the multi-view multilayer generator. For RIVs in spherical coordinate systems, it first discretizes the region by preset range intervals {s0, s1, s2, s3, ..., sq}, obtaining the split space Er = {Er1 , Er2 , Er3 , ..., Erq}. Then, it applies the spherical projection on laser points of the input point cloud P within different range intervals to generate multi-layer range images. Similarly, for BEVs in the Euclidean coordinates, it discretizes space by height intervals {t0, t1, t2, t3, ..., tq} to get Eb = {Eb1, Eb2, Eb3, ..., Ebq} and generates multi-layer BEV images for all the intervals.\nThe correspondence between a LiDAR point pk \u2208 P,pk = (xk, yk, zk) and the pixel coordinates (urk, v r k) in the corresponding RIV Ri can be represented as:( urk vrk ) = ( 1 2 [1\u2212 arctan(yk, xk)/\u03c0]w [1\u2212 (arcsin(zk/rk) + fup) /f]h ) , (1)\nwhere rk = ||pk||2 \u2208 [si\u22121, si] is the range measurement of the corresponding point, f = fup + fdown is the vertical fieldof-view of the sensor, and w, h are the width and height of the resulting range image.\nWe project the same LiDAR points to BEVs by the topdown projection. The correspondence between the same LiDAR point pk \u2208 P and the pixel coordinates (ubk, vbk) in the corresponding BEV image Bj can be represented as:(\nubk vbk\n) = ( 1 2 [1\u2212 arctan(yk, xk)/\u03c0]w\n[r\u2032k/f \u2032]h\n) , (2)\nSentence (from BEV)\nSentence (from range image)\nSphere View\nTop-Down View\n......\n... ...\n... ...\nwhere r\u2032k = ||(xk, yk)||2 \u2208 [tj\u22121, tj ] and f \u2032 is the maximum sensing range.\nIn this work, we let the RIV and BEV images have the same width and height for better alignment, leading to urk = u b k. This leads to the fact that the columns of the RIV and BEV images with the same index are both from the same spatial sector, as illustrated in Fig. 1. Finally, MMG generates the multi-layer RIVs R = {Ri} and multi-layer BEVs B = {Bj}, as illustrated in Fig. 3."
        },
        {
            "heading": "B. Aligned Feature Encoder",
            "text": "To better fuse different representations of LiDAR data, our CVTNet utilizes an aligned feature encoder to extract coarse features of each representation and align them for later fusion. It first concatenates all RIVs R and BEVs B along the channel dimension obtaining R and B with the size of q\u00d7h\u00d7w. Then it applies two OverlapNetLeg [1], [5] modules with the same architecture to compress inputs into sentence-like features. The OverlapNetLeg is a full convolutional encoder only to compress RIVs and BEVs in the vertical dimension without changing the width dimension to avoid the discretization error to the yaw equivariance. The output coarse features from each branch are then enhanced by an intra-transformer. We denote the enhanced features as Ar0 = AFEr(R) for the RIV branch, and Ab0 = AFEb(B) for the BEV branch, both with the size of c\u00d7 1\u00d7 w, where c refers to the feature channel number.\nAs proved in [5], the features from the RIV branch are yaw-angle-equivariant:\nAr0C = AFEr(\u03a0r(R\u03b8P)), (3)\nwhere \u03a0r(\u00b7) is the spherical projection detailed in Eq. (1), C represents the column shift of the feature by matrix right multiplication, \u03b8 is the yaw angle change, and R\u03b8 is the yaw rotation matrix of the point cloud P .\nIn this article, we further show that our proposed BEV branch is also yaw-angle-equivariant. According to the topdown projection \u03a0b(\u00b7) detailed in Eq. (2), a yaw rotation of a point corresponds to a specific horizontal shift on BEV as:(\nub\u2032k vb\u2032k\n) = ( ubk + s\nvbk\n) , s = 1\n2\n[ 1\u2212 \u03b8\u03c0\u22121 ] w, (4)\nBC = \u03a0b(R\u03b8P), (5)\nwhere C and R\u03b8 are the same column shift and yaw rotation. As the OverlapNetLeg and intra-transformer have proved to be yaw-angle-equivariant [5], for the BEV branch, we therefore also obtain the yaw-angle-equivariant features:\nAb0C = AFEb(\u03a0b(R\u03b8P)), (6)\nIt is crucial to align the features from different modalities before using the cross-transformer to fuse the different views and extract the cross-modal correlations [26], [27]. To this end, we first make sure the inputs of inter-transformer from the two different views are spatially aligned. We let the input RIV and BEV images have the same width and height, and thus urk = u b k holds as clarified in Sec. III-A. The same laser point is projected to the same column of the two images, leading to one space sector corresponding to the same column index of the two images. To keep the alignments, we further design our network architecture only to compress the height dimension and maintain the information from one space sector aggregated to the same column of the two output features. We concatenate the output feature of the OverlapNetLeg and the feature enhanced by intra-transformer along the channel dimension. Therefore, the output features Ar0 and Ab0 of AFE are devised to be word-aligned along the width dimension."
        },
        {
            "heading": "C. Multi-View Fusion Module",
            "text": "Our CVTNet fuses the multi-view features in the devised multi-view fusion module. It consists of an inter-transformer and three NetVLAD-MLPs combos [17] as depicted in Fig. 2. It first uses the proposed inter-transformer module to fuse the features from two branches of different views, as detailed in Fig. 4. In the inter-transformer module, there are also two branches with l stacked transformer blocks to extract the correlation of different views by using the query feature from one view, and the key and value features from the other view.\nFor the RIV branch, we denote the feature volume extracted by the multi-head self-attention (MHSA) of the lth transformer block as Arl , and the cross-attention mechanism of the transformer can then be formulated as:\nA\u0304rl = Attention(Q r l\u22121,K b 0,Vb0) = softmax (Qrl\u22121Kb0T\u221a dk ) Vb0 , (7) where Qrl\u22121 is substantially the query split of the layernormalized Arl\u22121 from the RIV, and Ar0 is the output feature from the AFE in the RIV branch. Kb0,Vb0 are the key and\nvalue splits of the layer-normalized Ab0, which is the output feature from the AFE in the BEV branch. dk represents the dimension of splits. A\u0304rl is then fed into the feed-forward network (FFN) and layer normalization (LN) to generate the attention-enhanced feature Arl of the lth transformer block, which can be formulated as:\nArl = FFN(LN(A\u0304 r l + LN(A r l\u22121))) + LN(A\u0304 r l + LN(A r l\u22121)). (8)\nFor the BEV branch, we conduct the same operations symmetrically:\nA\u0304bl = Attention(Q b l\u22121,K r 0,Vr0 ) = softmax (Qbl\u22121Kr0T\u221a dk ) Vr0 , (9)\nAbl = FFN(LN(A\u0304 b l + LN(A b l\u22121))) + LN(A\u0304 b l + LN(A b l\u22121)). (10)\nWe then concatenate Arl with Abl from the last transformer block and obtain the intermediate fused feature Afl .\nIn the end, our MVF uses the NetVLAD-MLPs combos on both the RIV and BEV features Ar0 and Ab0, as well as the cross-fused feature Afl . The NetVLAD-MLPs has been widely used in LPR [3], [7], [12] to generate the global descriptor. We also exploit three NetVLAD-MLPs combos to compress the features from two intra-transformers and one cross-view inter-transformer into the global descriptors [gr, gb, gf ]."
        },
        {
            "heading": "D. Yaw-Angle Invariance",
            "text": "We further mathematically prove that the multi-view fused descriptor generated by CVTNet is yaw-angle-invariant, which makes our method very robust to viewpoint changes. We have shown in Sec. III-B that the outputs of the aligned feature encoder are yaw-angle-equivariant in Eq. (3) and Eq. (6). As introduced by [20], NetVLAD is permutation-invariant, and the column shift of its input caused by the yaw rotation of the raw point cloud can be regarded as the re-order of a set of 1-D vectors over the channel dimension [5]. Therefore, the outputs of NetVLAD in the RIV and BEV branches are not affected by the yaw rotation, and the global descriptors gr and\ngb are yaw-angle-invariant. In the rest, we prove that the fused cross-view feature gf is also yaw-angle-invariant.\nNote that Ar0 and Ab0 share the same shift C caused by one yaw rotation of one LiDAR data, which has been introduced in Sec. III-B. Therefore, for the first transformer block in the inter-transformer after rotation, the Eq. (7) becomes:\nA\u0303r1 = Attention(CQr0, CKb0, CVb0),\n= softmax (CQr0(CKb0)T\u221a\ndk\n) CVb0 ,\n= C softmax (Qr0Kb0T\u221a\ndk\n) Vb0 = CA\u0304r1. (11)\nSimilarly, for the following stacked cross-transformer blocks we also have:\nA\u0303rl = Attention(CQrl\u22121, CKb0, CVb0),\n= softmax (CQrl\u22121(CKb0)T\u221a\ndk\n) CVb0 ,\n= C softmax (Qrl\u22121Kb0T\u221a\ndk\n) Vb0 = CA\u0304rl , (12)\nwhich means the outputs of the MHSA of the cross-view transformer blocks are yaw-angle-equivariant. Note that in Eq. (11) and Eq. (12) the shift C is left-multiplied since the first dimension of the input feature is width dimension and the second one is embedding dimension. Because FFN and LN are also yaw-angle-equivariant towards features [5], and the last concatenation is operated on the channel dimension, the intertransformer outputs yaw-angle-equivariant cross-view feature volumes. Then the NetVLAD with MLP is used to generate the yaw-angle-invariant global descriptor gf . Since the three parts are all yaw-angle-invariant, our final global descriptor is yaw-angle-invariant."
        },
        {
            "heading": "E. Network Training",
            "text": "Following [1], [5], we use more suitable overlap between pairs of scans to supervise the network rather than the ground truth distances. For each training tuple, we utilize one query descriptor Ga, kp positive descriptors {Gp}, and kn negative descriptors {Gn} to compute lazy triplet loss:\nLT (Gq, {Gp}, {Gn}) =\nkp(\u03b1+max p (d(Gq,Gp)))\u2212 \u2211 kn (d(Gq,Gn)), (13)\nwhere \u03b1 is the margin to avoid negative loss and d(\u00b7) is the squared Euclidean distance. In our work, A pair of scans with overlap larger than 0.3 is regarded as a positive sample, otherwise a negative sample. In addition, we set kp = 6, kn = 6, and \u03b1 = 0.5 for the triplet loss. We use the triplet loss to make the distance between the query and all sampled negative global descriptors much larger than the distance between the query and the hardest positive global descriptors."
        },
        {
            "heading": "IV. EXPERIMENTAL EVALUATION",
            "text": "The experimental evaluation is designed to showcase the performance of our approach and to support the claims that our approach is able to: (i) achieve state-of-the-art performance in place recognition and loop closure detection in outdoor largescale environments using multi-view LiDAR data, (ii) recognize places well despite different viewpoints based on the proposed yaw-angle-invariant architecture, (iii) achieve fast operation with a runtime less than 50 ms.\nA. Implementation and Experimental Setup\nTo evaluate localization performance of our proposed method, we utilize three datasets with different environmental conditions and sensor setups, including publicly available NCLT dataset [28] and KITTI odometry sequences [29], as well as our self-recorded datasets in urban driving environments. Our self-recorded dataset is collected by an AGV equipped with a LiDAR sensor (HESAI PandarXT, 32-beam), a wide-angle camera (SENSING SG2, HFOV 106\u25e6, VFOV 56\u25e6), an RTK GNSS (Asensing INS570D, 0.1\u25e6 error in roll/pitch, 0.2\u25e6 error in yaw, 2 cm + 1 ppm in RTK position), a mini computer (Intel Xeon E, 3.5 GHz, 80 W), and a Nivida Tesla T4 (16-GB memory, 70 W), as shown in Fig. 5.\n1) RIV Inputs: For RIV inputs of NCLT and self-recorded datasets with 32-beam LiDAR data, we set range intervals as {[0m, 15m], [15m, 30m], [30m, 45m], [45m, 60m]} to generate 4-layer range images. We also generate the range image using all the laser points within 60m, and concatenate it with 4-layer range images to get the final RIV input with the size of 5\u00d732\u00d7900. For KITTI sequences, we set the range intervals as {[0m, 15m], [15m, 30m], [30m, 45m], [45m, 80m]} to keep the same maximum range as existing works [1], [5] for fair comparisons.\n2) BEV Inputs: For the BEV inputs of the NCLT dataset and self-recorded dataset, we set height intervals to {[\u22124m, 0m], [0m, 4m], [4m, 8m], [8m, 12m]} to generate 4-layer BEV images, which are also concatenated with the BEV image projected from all the laser points within 60m. We set the height intervals of KITTI sequences to {[\u22123m,\u22121.5m], [\u22121.5m, 0m], [0m, 1.5m], [1.5m, 5m]}, adapted to its data distribution. The multi-layer BEVs have the same size as the multi-layer RIVs.\n3) CVTNet Configuration: Our CVTNet uses the same OverlapNetLeg as OverlapTransformer [5]. For all the transformer modules, we set the embedding dimension dmodel = 256, the number of heads nhead = 4, and the intermediate dimension of the feed-forward layer dffn = 1024. We use one selfattention transformer block in each intra-transformer module following [5], and two cross-transformer blocks in the intertransformer module, which is validated as the best configuration in the following ablation study. For all the NetVLAD modules, we set the intermediate feature dimension dinter = 1024, the output feature dimension doutput = 256, and the number of clusters dK = 64. The output descriptor of CVTNet is a vector of size 1\u00d7 768. For evaluation conducted on each dataset, our proposed CVTNet is trained for 30 epochs, using the ADAM optimizer to update the weights of the network with an initial learning rate of 5e-6 and a weight decay of 0.9 applied every 5 steps. Other related training configuration has been clarified in Sec. III-E. The test setups follow the previous work [30], which will also be introduced in the following sections. We will release the implementation of CVTNet as open source to provide more network details and evaluation configurations.\n4) Baseline Configuration: In the following experiments, we compare our proposed CVTNet with both state-of-theart methods PointNetVLAD [20], Scan Context [6] without shift matching for real-time retrieval, MinkLoc3D [9], OverlapTransformer [5], and the other multi-view LPR method FusionVLAD [15]. The descriptors generated by all the abovementioned learning-based baseline methods are with the size of 1\u00d7256 except for FusionVLAD which generates the fused 1\u00d72048 descriptor, all following their original configurations. The feature matrix generated by Scan Context is set to 20 \u00d7 60. Note that we reimplement FusionVLAD according to its original paper since it is not open-source, and the other parameter configurations of all the baseline methods in the experiments also follow their original papers."
        },
        {
            "heading": "B. Evaluation for Place Recognition",
            "text": "The first experiment supports our claim that our approach achieves state-of-the-art place recognition and loop closure detection in outdoor large-scale environments using multi-view LiDAR data.\nFollowing [5], we use the NCLT dataset for evaluating longterm place recognition, where sequence 2012-01-08 is used to train the models and sequences 2012-02-05, 2012-06-15, 2013-02-23, and 2013-04-05 are used to evaluate the recognition performance. Besides NCLT dataset, we also recorded our own dataset with multiple reverse driving conditions repeated in the same environment, which are frequent cases in autonomous driving but challenging to place recognition methods. We use sequence-1 as the database for training and two other runs (sequence-2 and sequence-3) as query sequences for evaluation. They all share the same route but driving from different directions. In contrast, KITTI odometry sequences are used for evaluating loop closure detection, where we use sequences 03\u201310 for training, sequence 02 for validation, and sequence 00 for evaluation.\nWe use average top 1 recall (AR@1), top 5 recall (AR@5), and top 20 recall (AR@20) as the metrics of the evaluation conducted on NCLT and the self-recorded datasets. We also use AUC and F1 max scores as the evaluation metrics for the KITTI dataset as introduced in [5]. The evaluation results are shown in Tab. I and Tab. II As can be seen, our proposed CVTNet has better performance than all the state-of-the-art baseline methods on the place recognition metrics. Compared to OverlapTransformer, CVTNet utilizes the combination of RIVs and BEVs as input rather than only range images, which increases the performance of recognizing places with large time gaps by 7.1% \u223c 28.4% on AR@1 of the NCLT dataset. In contrast to FusionVLAD which also considers different views, our proposed CVTNet outperforms it by 14.6% \u223c 35.1% on AR@1 of the NCLT dataset, and 2.1% \u223c 4.2% on AR@1 of the self-recorded dataset. For loop closure detection on the KITTI dataset, our method also significantly outperforms all baseline methods."
        },
        {
            "heading": "C. Ablation Study on Transformer Modules",
            "text": "This ablation study investigates the effect of intra- and inter-transformer modules. We compare our CVTNet with the variants of not using transformer modules. We evaluate all variants using the average recall AR@N on sequence 2012- 02-05 and 2013-02-23 of the NCLT dataset. The results are shown in Fig. 6. As can be seen, our CVTNet with both intra- and inter-transformer modules consistently outperforms all variants. Besides, CVTNet with only an inter-transformer outperforms the one using only an intra-transformer, which indicates that the inter-transformer contributes more to enhancing the discrimination of the final global descriptor than the intra-transformer. The benefit of using two types of transformers is more prominent in place recognition with a larger time gap, which can be drawn by comparing the results between two sequences recorded from different times."
        },
        {
            "heading": "D. Ablation Study on Cross-Transformer Blocks",
            "text": "This ablation study aims to show the effectiveness of the number of cross-transformer blocks in the inter-transformer module of CVTNet. Note that we only compare our method with 1 \u223c 3 cross-transformer blocks to maintain low memory consumption. The experimental results on the NCLT dataset are shown in Tab. III. As can be seen, the inter-transformer with two cross-transformer blocks outperforms the ones with\none and three cross-transformer blocks. The reason could be that more transformer blocks might need more training data and time to obtain good performance. We also show the average runtime to generate one global descriptor for sequence 2012-01-08 of the NCLT dataset in Tab. III. It can be seen that more cross-transformer blocks cost more time to extract features. Thus, we use 2 transformer blocks in our CVTNet to keep effectiveness and efficiency."
        },
        {
            "heading": "E. Study on Multi-Layer Input",
            "text": "In this experiment, we use self-recorded datasets to validate the superiority of using multi-layer input for CVTNet. We modify the input channel number of the first layer of OverlapNetLeg to receive the single-channel input. The experimental results are shown in Tab. IV. As can be seen, CVTNet with multi-layer input outperforms the baseline with only single-channel input. Our proposed multi-layer input explicitly tackles the problem of occluded laser points in the background, providing more input point cloud information, thus enabling the network to reweight data from different subspaces to enhance the discrimination of the extracted features.\nTo further validate the effectiveness of the proposed multilayer form of inputs, we also apply it to another network, FusionVLAD as also shown in Tab. IV. The multi-layer input form also improves the performance of FusionVLAD, which indicates that our proposed multi-layer input form generalizes well into different methods. Still, our method significantly outperforms the FusionVLAD with multi-layer inputs due to the devised novel intra- and inter-transformer network."
        },
        {
            "heading": "F. Study on Yaw-Angle Invariance",
            "text": "We have shown the robustness of our proposed CVTNet to viewpoint changes using the self-recorded dataset in Sec. IV-B. In this experiment, we further design a more straightforward way to validate that our devised network architecture is strictly invariant to the yaw rotation of LiDAR scans obtained at the same place. We rotate each query scan of sequence 2012- 02-05 along the yaw-axis in steps of 30 degrees and search the same places in the database built from sequence 2012-01- 08. We compare our method with the state-of-the-art baseline methods and use Recall@1 as the criterion to show the effect of yaw angle changes on place recognition performance. The experimental results are shown in Fig. 7. As can be seen, CVTNet outperforms the other methods consistently in terms of Recall@1 in all the rotation conditions. Both CVTNet and OverlapTransformer are not affected by the rotation along the yaw-axis due to the devised yaw-rotation-invariant architectures. FusionVLAD is not significantly affected by yaw-angle rotation but is not strict yaw-rotation-invariant. In contrast, the performance of MinkLoc3D and PointNetVLAD decreases fast with the yaw angle increasing, which means their network architectures are not robust enough to viewpoint changes."
        },
        {
            "heading": "G. Runtime and Memory Consumption",
            "text": "We further compare the real-time performance of our CVTNet with all the baseline methods, and evaluate its memory consumption. We conduct the experiments on a system with an Intel i7-11700K CPU and an Nvidia RTX 3070 GPU. We use NCLT sequence 2012-01-08 as database and sequence 2012-02-05 as query to calculate the total runtime of global descriptor generation and top 20 candidates retrieval within 28127 database laser scans for each query laser scan. FAISS library [31] is used here to accelerate the searching process except for Scan Context which can only use an ad-hoc retrieval function for shift-based matching [6]. Fig. 8 shows the running frequency v.s. top 1 recall. The running frequency of CVTNet is 29.85Hz (33.50ms in total, 15.03ms for descriptor generation, and 18.47ms for retrieval) which is about 3 times faster than a typical LiDAR sensor frame rate at 10Hz, while achieving the state-of-the-art place recognition performance. In addition, the number of parameters of CVTNet is 21.38M, showing the small memory consumption of our approach."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "In this article, a novel cross-view transformer network has been presented for LiDAR-based place recognition. Point clouds have been projected to multi-layer RIVs and BEVs as the inputs of our method instead of the mundane singlechannel form. The intra-transformer has been utilized in the proposed network to capture the inner correlation within the\nindividual view, while the inter-transformer has been used to extract the cross-view correlation between different views. The experimental comparisons of our method with the state-ofthe-art methods have validated its superior performance in both place recognition and loop closure detection. Moreover, the experimental results have also shown that our proposed CVTNet is yaw-angle-invariant against viewpoint changes and operates online which can be used for real autonomous driving applications.\nREFERENCES\n[1] X. Chen, T. La\u0308be, A. Milioto, T. Ro\u0308hling, J. Behley, and C. Stachniss, \u201cOverlapNet: A Siamese Network for Computing LiDAR Scan Similarity with Applications to Loop Closing and Localization,\u201d Autonomous Robots, vol. 46, pp. 61\u201381, 2021. [2] J. Ma, S. Wang, K. Zhang, Z. He, J. Huang, and X. Mei, \u201cFast and robust loop-closure detection via convolutional auto-encoder and motion consensus,\u201d IEEE Transactions on Industrial Informatics, vol. 18, no. 6, pp. 3681\u20133691, 2022. [3] Z. Liu, S. Zhou, C. Suo, P. Yin, W. Chen, H. Wang, H. Li, and Y.-H. Liu, \u201cLpd-net: 3d point cloud learning for large-scale place recognition and environment analysis,\u201d in Proc. of the IEEE/CVF Intl. Conf. on Computer Vision (ICCV), 2019. [4] K. Vidanapathirana, M. Ramezani, P. Moghadam, S. Sridharan, and C. Fookes, \u201cLogg3d-net: Locally guided global descriptor learning for 3d place recognition,\u201d in Proc. of the IEEE Intl. Conf. on Robotics & Automation (ICRA), 2022. [5] J. Ma, J. Zhang, J. Xu, R. Ai, W. Gu, and X. Chen, \u201cOverlaptransformer: An efficient and yaw-angle-invariant transformer network for lidarbased place recognition,\u201d IEEE Robotics and Automation Letters (RA-L), vol. 7, no. 3, pp. 6958\u20136965, 2022. [6] G. Kim and A. Kim, \u201cScan context: Egocentric spatial descriptor for place recognition within 3d point cloud map,\u201d in Proc. of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS), 2018. [7] D. Cattaneo, M. Vaghi, and A. Valada, \u201cLcdnet: Deep loop closure detection and point cloud registration for lidar slam,\u201d IEEE Transactions on Robotics, vol. 38, no. 4, pp. 2074\u20132093, 2022. [8] Z. Fan, Z. Song, H. Liu, Z. Lu, J. He, and X. Du, \u201cSvt-net: Super light-weight sparse voxel transformer for large scale place recognition.\u201d AAAI, 2022. [9] J. Komorowski, \u201cMinkloc3d: Point cloud based large-scale place recognition,\u201d in Proc. of the IEEE Winter Conf. on Applications of Computer Vision (WACV), 2021. [10] M. Y. Chang, S. Yeon, S. Ryu, and D. Lee, \u201cSpoxelnet: Spherical voxelbased deep place recognition for 3d point clouds of crowded indoor spaces,\u201d in 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2020, pp. 8564\u20138570. [11] A. Zaganidis, A. Zerntev, T. Duckett, and G. Cielniak, \u201cSemantically assisted loop closure in slam using ndt histograms,\u201d in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2019, pp. 4562\u20134568. [12] Z. Zhou, C. Zhao, D. Adolfsson, S. Su, Y. Gao, T. Duckett, and L. Sun, \u201cNdt-transformer: Large-scale 3d point cloud localisation using the normal distribution transform representation,\u201d in Proc. of the IEEE Intl. Conf. on Robotics & Automation (ICRA), 2021. [13] H. Wang, C. Wang, and L. Xie, \u201cIntensity scan context: Coding intensity and geometry relations for loop closure detection,\u201d in Proc. of the IEEE Intl. Conf. on Robotics & Automation (ICRA), 2020. [14] L. Luo, S.-Y. Cao, B. Han, H.-L. Shen, and J. Li, \u201cBvmatch: Lidarbased place recognition using bird\u2019s-eye view images,\u201d IEEE Robotics and Automation Letters (RA-L), vol. 6, no. 3, pp. 6076\u20136083, 2021. [15] P. Yin, L. Xu, J. Zhang, and H. Choset, \u201cFusionvlad: A multi-view deep fusion networks for viewpoint-free 3d place recognition,\u201d IEEE Robotics and Automation Letters (RA-L), vol. 6, no. 2, pp. 2304\u20132310, 2021. [16] X. Ye and J. Ma, \u201cNeighborhood manifold preserving matching for visual place recognition,\u201d IEEE Transactions on Industrial Informatics, pp. 1\u201310, 2022. [17] R. Arandjelovic, P. Gronat, A. Torii, T. Pajdla, and J. Sivic, \u201cNetvlad: Cnn architecture for weakly supervised place recognition,\u201d in Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 2016.\n[18] L. Li, M. Yang, C. Wang, and B. Wang, \u201cHybrid filtering framework based robust localization for industrial vehicles,\u201d IEEE Transactions on Industrial Informatics, vol. 14, no. 3, pp. 941\u2013950, 2018. [19] Y. Wang, Z. Sun, C.-Z. Xu, S. E. Sarma, J. Yang, and H. Kong, \u201cLidar iris for loop-closure detection,\u201d in Proc. of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS), 2020. [20] M. A. Uy and G. H. Lee, \u201cPointnetvlad: Deep point cloud based retrieval for large-scale place recognition,\u201d in Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition, 2018. [21] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, \u201cPointnet: Deep learning on point sets for 3d classification and segmentation,\u201d in Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition, 2017. [22] R. Dube\u0301, A. Cramariuc, D. Dugas, H. Sommer, M. Dymczyk, J. Nieto, R. Siegwart, and C. Cadena, \u201cSegmap: Segment-based mapping and localization using data-driven descriptors,\u201d The International Journal of Robotics Research, vol. 39, no. 2-3, pp. 339\u2013355, 2020. [23] S. Zhao, P. Yin, G. Yi, and S. Scherer, \u201cSpherevlad++: Attention-based and signal-enhanced viewpoint invariant descriptor,\u201d arXiv preprint arXiv:2207.02958, 2022. [24] J. Cui and X. Chen, \u201cCcl: Continual contrastive learning for lidar place recognition,\u201d IEEE Robotics and Automation Letters, vol. 8, no. 8, pp. 4433\u20134440, 2023. [25] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in Proc. of the Advances in Neural Information Processing Systems (NIPS), 2017. [26] A. Zadeh, P. P. Liang, N. Mazumder, S. Poria, E. Cambria, and L.-P. Morency, \u201cMemory fusion network for multi-view sequential learning,\u201d in Proc. of the Conference on Advancements of Artificial Intelligence (AAAI), 2018. [27] Pham, Hai and Liang, Paul Pu and Manzini, Thomas and Morency, Louis-Philippe and Po\u0301czos, Barnaba\u0301s, \u201cFound in Translation: Learning Robust Joint Representations by Cyclic Translations between Modalities,\u201d in Proc. of the Conference on Advancements of Artificial Intelligence (AAAI), 2019. [28] N. Carlevaris-Bianco, A. K. Ushani, and R. M. Eustice, \u201cUniversity of michigan north campus long-term vision and lidar dataset,\u201d Intl. Journal of Robotics Research (IJRR), vol. 35, no. 9, pp. 1023\u20131035, 2016. [29] A. Geiger, P. Lenz, and R. Urtasun, \u201cAre we ready for Autonomous Driving? The KITTI Vision Benchmark Suite,\u201d in Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2012. [30] J. Ma, X. Chen, J. Xu, and G. Xiong, \u201cSeqot: A spatial-temporal transformer network for place recognition using sequential lidar data,\u201d IEEE Transactions on Industrial Electronics, 2022. [31] J. Johnson, M. Douze, and H. Je\u0301gou, \u201cBillion-scale similarity search with gpus,\u201d IEEE Trans. on Big Data, vol. 7, no. 3, pp. 535\u2013547, 2019."
        }
    ],
    "title": "CVTNet: A Cross-View Transformer Network for LiDAR-Based Place Recognition in Autonomous Driving Environments",
    "year": 2023
}