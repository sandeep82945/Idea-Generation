{
    "abstractText": "In supervised learning for image denoising, usually the paired clean images and noisy images are collected or synthesised to train a denoising model. L2 norm loss or other distance functions are used as the objective function for training. It often leads to an over-smooth result with less image details. In this paper, we regard the denoising task as a problem of estimating the posterior distribution of clean images conditioned on noisy images. We apply the idea of diffusion model to realize generative image denoising. According to the noise model in denoising tasks, we redefine the diffusion process such that it is different from the original one. Hence, the sampling of the posterior distribution is a reverse process of dozens of steps from the noisy image. We consider three types of noise model, Gaussian, Gamma and Poisson noise. With the guarantee of theory, we derive a unified strategy for model training. Our method is verified through experiments on three types of noise models and achieves excellent performance.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yutong Xie"
        },
        {
            "affiliations": [],
            "name": "Mingze Yuan"
        },
        {
            "affiliations": [],
            "name": "Bin Dong"
        },
        {
            "affiliations": [],
            "name": "Quanzheng Li"
        }
    ],
    "id": "SP:66195d6e1d71090f92388cba2e6fd569d4ee486b",
    "references": [
        {
            "authors": [
                "Lalit Chaudhary",
                "Yogesh Yogesh"
            ],
            "title": "A comparative study of fruit defect segmentation techniques",
            "venue": "International Conference on Issues and Challenges in Intelligent Computing Techniques (ICICT),",
            "year": 2019
        },
        {
            "authors": [
                "Shen Cheng",
                "Yuzhi Wang",
                "Haibin Huang",
                "Donghao Liu",
                "Haoqiang Fan",
                "Shuaicheng Liu"
            ],
            "title": "Nbnet: Noise basis learning for image denoising with subspace projection",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Kostadin Dabov",
                "Alessandro Foi",
                "Vladimir Katkovnik",
                "Karen Egiazarian"
            ],
            "title": "Image denoising with blockmatching and 3d filtering. In Image processing: algorithms and systems, neural networks, and machine learning, volume 6064",
            "year": 2006
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Shi Guo",
                "Zifei Yan",
                "Kai Zhang",
                "Wangmeng Zuo",
                "Lei Zhang"
            ],
            "title": "Toward convolutional blind denoising of real photographs",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "arXiv preprint arXiv:2006.11239,",
            "year": 2006
        },
        {
            "authors": [
                "Xiaowan Hu",
                "Ruijun Ma",
                "Zhihong Liu",
                "Yuanhao Cai",
                "Xiaole Zhao",
                "Yulun Zhang",
                "Haoqian Wang"
            ],
            "title": "Pseudo 3d auto-correlation network for real image denoising",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Yoonsik Kim",
                "Jae Woong Soh",
                "Gu Yong Park",
                "Nam Ik Cho"
            ],
            "title": "Transfer learning from synthetic to realnoise denoising with adaptive instance normalization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Zhifeng Kong",
                "Wei Ping",
                "Jiaji Huang",
                "Kexin Zhao",
                "Bryan Catanzaro"
            ],
            "title": "Diffwave: A versatile diffusion model for audio synthesis",
            "venue": "arXiv preprint arXiv:2009.09761,",
            "year": 2020
        },
        {
            "authors": [
                "Yao Li",
                "Xueyang Fu",
                "Zheng-Jun Zha"
            ],
            "title": "Cross-patch graph convolutional network for image denoising",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Liu",
                "Zhenyue Qin",
                "Saeed Anwar",
                "Pan Ji",
                "Dongwoo Kim",
                "Sabrina Caldwell",
                "Tom Gedeon"
            ],
            "title": "Invertible denoising network: A light solution for real noise removal",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101,",
            "year": 2017
        },
        {
            "authors": [
                "Chenlin Meng",
                "Yang Song",
                "Jiaming Song",
                "Jiajun Wu",
                "Jun-Yan Zhu",
                "Stefano Ermon"
            ],
            "title": "Sdedit: Image synthesis and editing with stochastic differential equations",
            "venue": "arXiv preprint arXiv:2108.01073,",
            "year": 2021
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Soumith Chintala",
                "Gregory Chanan",
                "Edward Yang",
                "Zachary DeVito",
                "Zeming Lin",
                "Alban Desmaison",
                "Luca Antiga",
                "Adam Lerer"
            ],
            "title": "Automatic differentiation in pytorch",
            "year": 2017
        },
        {
            "authors": [
                "Sathish Ramani",
                "Thierry Blu",
                "Michael Unser"
            ],
            "title": "Monte-carlo sure: A black-box optimization of regularization parameters for general denoising algorithms",
            "venue": "IEEE Transactions on image processing,",
            "year": 2008
        },
        {
            "authors": [
                "Chao Ren",
                "Xiaohai He",
                "Chuncheng Wang",
                "Zhibo Zhao"
            ],
            "title": "Adaptive consistency prior based deep network for image denoising",
            "venue": "In proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Chitwan Saharia",
                "Jonathan Ho",
                "William Chan",
                "Tim Salimans",
                "David J Fleet",
                "Mohammad Norouzi"
            ],
            "title": "Image super-resolution via iterative refinement",
            "venue": "arXiv preprint arXiv:2104.07636,",
            "year": 2021
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "arXiv preprint arXiv:2011.13456,",
            "year": 2020
        },
        {
            "authors": [
                "Radu Timofte",
                "Shuhang Gu",
                "Jiqing Wu",
                "Luc Van Gool"
            ],
            "title": "Ntire 2018 challenge on single image super-resolution: Methods and results",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition workshops,",
            "year": 2018
        },
        {
            "authors": [
                "Yutong Xie",
                "Dufan Wu",
                "Bin Dong",
                "Quanzheng Li"
            ],
            "title": "Trained model in supervised deep learning is a conditional risk minimizer",
            "venue": "arXiv preprint arXiv:2202.03674,",
            "year": 2022
        },
        {
            "authors": [
                "Zongsheng Yue",
                "Hongwei Yong",
                "Qian Zhao",
                "Deyu Meng",
                "Lei Zhang"
            ],
            "title": "Variational denoising network: Toward blind noise modeling and removal",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Syed Waqas Zamir",
                "Aditya Arora",
                "Salman Khan",
                "Munawar Hayat",
                "Fahad Shahbaz Khan",
                "Ming-Hsuan Yang",
                "Ling Shao"
            ],
            "title": "Cycleisp: Real image restoration via improved data synthesis",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Syed Waqas Zamir",
                "Aditya Arora",
                "Salman Khan",
                "Munawar Hayat",
                "Fahad Shahbaz Khan",
                "Ming-Hsuan Yang",
                "Ling Shao"
            ],
            "title": "Learning enriched features for real image restoration and enhancement",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Kai Zhang",
                "Wangmeng Zuo",
                "Yunjin Chen",
                "Deyu Meng",
                "Lei Zhang"
            ],
            "title": "Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising",
            "venue": "IEEE transactions on image processing,",
            "year": 2017
        },
        {
            "authors": [
                "Kai Zhang",
                "Wangmeng Zuo",
                "Lei Zhang"
            ],
            "title": "Ffdnet: Toward a fast and flexible solution for cnn-based image denoising",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Hongyi Zheng",
                "Hongwei Yong",
                "Lei Zhang"
            ],
            "title": "Deep convolutional dictionary learning for image denoising",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Image denoising [4, 16, 26] has been studied for many years. Suppose x is a clean image and y is a noisy image. The noise model can be written as follows:\ny = N (x; z, params) , (1)\nwhere z represents the source of noise, params represents the parameters of the noise model. Current supervised learning methods focus on training a denoising model using paired clean images and noisy images. Hence, collecting or synthesising such pairs as training data is important. Usually, the denoising model is trained by the following loss function:\nL(\u03b8) = Ex,y [d (x, f(y; \u03b8))] , (2)\nwhere f(\u00b7; \u03b8) is a neural network and d (\u00b7, \u00b7) is a distance metric. This methodology of supervised learning in essence\nis to define the denoising task as a training problem of determined mapping, from the noisy image y to the clean one x. However, the denoising model trained in this manner often leads to a result with average effect. From the Bayesian perspective, y conditioned on x follows a posterior distribution:\nq (x | y) = q (y | x) q (x) q (y) . (3)\nWhen d(\u00b7, \u00b7) in Eq. (2) is L2 norm distance, the trained model will be an estimation of E [x | y], i.e. the posterior mean. This explains why denoised result in usual supervised learning is over-smooth.\nTo avoid the average effect, we regard image denoising as a problem of estimation of posterior distribution q (x | y). Hence, we do not train a denoising model representing a determined mapping. Instead, we train a generative denoising model. Recently, the diffusion model has achieve tremendous success in the domain of image generative tasks [7, 10, 14, 18\u201320]. In the original diffusion model, diffusion process transform clean images x to total Gaussian noise by adding little Gaussian noise and reducing the signal of x step by step. The sampling of target distribution is realized by a reverse process with hundreds and thousands of iterations from total Gaussian noise to clean images. Though it is a powerful generative model, applying the diffusion model directly to image denoising is not a desirable way. Its illustration is shown in Fig. 1. It is timeconsuming without any acceleration trick and does not fully utilized the residual information in noisy images. Interestingly, we observe that the diffusion process is similar to the noise model defined in Eq. (1), though the noise in diffusion model is limited to Gaussian noise and the signal of x is reduced along the diffusion process. If the diffusion process is completely consistent with the noise model, it is possible that we can begin the reverse process with noisy images, rather than total Gaussian noise and reduce the iteration greatly. For this purpose, we propose a new diffusion model for denoising tasks. The diffusion process in our method is designed according to the specific noise model such that they are consistent. As a result, the reverse pro-\nar X\niv :2\n30 2.\n02 39\n8v 1\n[ cs\n.C V\n] 5\nF eb\n2 02\ncess can start from the noisy image to realize sampling of q (x | y). We show the idea in Fig. 2. The details of design for diffusion process and reverse process, model training and sampling algorithms are described in Sec. 3.\nIn summary, our main contributions are: (1) We propose a new diffusion model designed for image denoising tasks. (2) We design the diffusion process, model training strategy and sampling algorithms for three types noise models, Gaussian, Gamma and Poisson. (3) Our experiments show that our proposed method is feasible."
        },
        {
            "heading": "2. Related Works",
            "text": ""
        },
        {
            "heading": "2.1. Supervised Learning",
            "text": "Learning from paired noisy-clean data is the mainstream in image denoising. Given paired noisy-clean data, it is straightforward to train supervised denoising methods. Albeit breakthrough performance has been achieved, the success of most existing deep denoisers heavily depend on supervised learning with large amount of paired noisy-clean images, which can be further divided into synthetic pairs [6, 9, 11, 23, 24, 26, 27, 29] and realistic pairs [1, 3, 8, 12, 17, 25, 28]."
        },
        {
            "heading": "2.2. Original Diffusion Model",
            "text": "In the original diffusion model [7], the diffusion process is defined as:\nxt+1 = \u03b1txt + \u03b2tz, t = 0, ..., (4)\nwhere x0 is the clean image and z follows the standard multi-variable Gaussian distribution. \u03b1t is smaller than but near 1, and \u03b2t is a very small value. When T is large enough, xT approximately follows the standard multivariable Gaussian distribution. The reverse process is the inverse of the diffusion process. Therefore, the reverse process from a random Gaussian noise will lead to a sample of q (x0)."
        },
        {
            "heading": "3. Method",
            "text": "In this section, we provide a more detailed description of the presented approach. The organization is as follows: In Sec. 3.1 we introduce the basic framework of diffusion model designed for image denoising tasks. We present the application for three kinds of noise models in Sec. 3.2. Finally, Sec. 3.3 is the further discussion. All full proofs and derivation in this section can be found in Appendix."
        },
        {
            "heading": "3.1. Basic Framework",
            "text": "Suppose the noise model is known and we present it using following form:\ny = N (x; z, params) , (5)\nwhere z represents the source of noise, params represents the parameters of the noise model. Here, y \u223c y is the noisy image and x \u223c x is the clean image. In this paper, we use different font to distinguish random variables and their samples like x and x. Our target is to realize sampling of q (x | y).\nSince we adapt the idea of the original diffusion model, next we introduce the definition of diffusion process and reverse process."
        },
        {
            "heading": "3.1.1 Diffusion Process",
            "text": "Let t = 0, 1, ..., N , we construct N + 1 random variables, xt, through a sequence of parameters in Eq. (5), {params1, params2, ..., paramsN}. Here, we define that x0 = x, and let paramsN be params in Eq. (5). Given x0 \u223c x0, we have the following definition for diffusion process along t:\nxt | x0 = N (x0; z, paramst) , t = 1, ..., N. (6)\nSuch definition indicates that xN = y. In the rest of this paler, for the sake of convenience we use x0 and xN to represent x and y respectively. Thus, x0 is a clean image and xN is the noisy image to be denoised. Usually, the sequence of {paramst} can be regarded as a discrete sampling of a continuous (and monotonous) function Params(t).\nAccording to Eq. (6), xt, t = 1, ..., N \u2212 1 are also noisy images with a noise level different from xN . When paramst, t = 1, ..., N , is fixed, the distribution of xt has\nbeen determined by the distribution of x0 and the noise model. We have\nq (xt) = \u222b x0 q (xt | x0) q (x0) dx0, (7)\nwhere q (x0) is the probability density function of x0 and q (xt | x0) is related to the noise model defined in Eq. (6). However, the relation between xt, i = 1, ..., N is not defined. Here, we do not provide the specific assumption for the relation and we discuss it in Sec. 3.2. Nevertheless, the diffusion process is related to the noise model in denoising task by the definition above."
        },
        {
            "heading": "3.1.2 Reverse Process",
            "text": "Since our target distribution is q (x0 | xN ) where xN is the given noisy image. Suppose the model is represented by p\u03b8 (x0 | xN ). We define the reverse process as a Markov chain:\np\u03b8 (xt | xt+1, ...,xN ) = p\u03b8 (xt | xt+1,xN ) . (8)\nThen, we have the following derivation:\np\u03b8 (x0 | xN )\n= \u222b x1:N\u22121 p\u03b8 (x0,x1, ...,xN\u22121 | xN ) dx1 \u00b7 \u00b7 \u00b7 dxN\u22121\n= \u222b x1:N\u22121 t=N\u22121\u220f t=0 p\u03b8 (xt | xt+1, ...,xN ) dx1 \u00b7 \u00b7 \u00b7 dxN\u22121\n= \u222b x1:N\u22121 t=N\u22121\u220f t=0 p\u03b8 (xt | xt+1,xN ) dx1 \u00b7 \u00b7 \u00b7 dxN\u22121.\n(9)\nHere, Eq. (8) is utilized in the third equation. The subscript 1 : N \u2212 1 in the notation x1:N\u22121 is an abbreviation for {x1, ...,xN\u22121}. For the sake of convenience, we continue to use this abbreviation in the rest of this paper. Equation (9) indicates that we realize the sampling of q (x0 | xN ) through iterative sampling of p\u03b8 (xt | xt+1,xN ). Therefore, we have the following sampling algorithm.\nAlgorithm 1 The general sampling process Input: noisy image xN and model p\u03b8. Output: x0.\n1: for t = N \u2212 1, ..., 0 do 2: Sample xt by p\u03b8 (xt | xt+1,xN ). 3: end for"
        },
        {
            "heading": "3.1.3 Derivation of Model Training",
            "text": "In fact, p\u03b8 (x0 | xN ) is a multiple\u2013hidden-variable model with xt, t = 1, ..., N \u2212 1. Based on the definition of the\ndiffusion process and reverse process, we can derive the evidence lower bound objective (ELBO) as follows:\nL = Ex0,xN [\u2212 log p\u03b8 (x0 | xN )] \u2264 Eq0:N [ \u2212 log p\u03b8 (x0:N\u22121 | xN )\nq (x1:N\u22121 | x0,xN ) ] = Eq0:N [ \u2212 log \u220ft=N\u22121 t=0 p\u03b8 (xt | xt+1,xN ) q (x1:N\u22121 | x0,xN ) ] . (10)\nThe proof is in Appendix. The further derivation of Eq. (10) depends on the assumption of diffusion process and will be shown in Sec. 3.2."
        },
        {
            "heading": "3.2. Application",
            "text": "In this part, we discuss three types of noise models for image denoising, Gaussian, Gamma and Poisson noise, as examples. We will give the specific assumption for the diffusion process, derive the objective loss function for training, and show the full sampling algorithms."
        },
        {
            "heading": "3.2.1 Gaussian Noise",
            "text": "The Gaussian noise model in denoising task is defined by the following form:\nxN = x0 + \u03c3z, z \u223c N (0, I) , (11)\nwhereN (0, I) is standard Gaussian distribution with independent components.\nWe select 0 = \u03c30 < \u03c31 < \u03c32 < \u00b7 \u00b7 \u00b7 < \u03c3N\u22121 < \u03c3N = \u03c3. Therefore, {\u03c3t} is a monotonically increasing sequence. Let\nxt+1 = xt+ \u221a \u03c32t+1 \u2212 \u03c32t zt+1, zt+1 \u223c N (0, I) , (12)\nwhere t = 0, 1, ..., N \u2212 1. Then we have:\nxt = x0 + ( t\u22121\u2211 i=0 \u221a \u03c32i+1 \u2212 \u03c32i zi ) = x0 + \u03c3tz, z \u223c N (0, I) ,\n(13)\nwhere t = 1, ..., N . Equation (12) defines the relation between xt, t = 1, ..., N and we have\nq (xt | x0:t\u22121) = q (xt | xt\u22121) . (14)\nThus, xt, t = 1, ..., N are a Markov chain. By now, we define a full diffusion process for Gaussian Noise.\nFrom Eq. (14), we can derive the following two properties:\nq (x1:N\u22121 | x0,xN ) = N\u22121\u220f t=1 q (xt | x0,xt+1) . (15)\nq (xt | xt+1:N ) = q (xt | xt+1) . (16)\nThe proof of Eq. (15) and Eq. (16) are in Appendix. Applying Eq. (15) to Eq. (10), we have that:\nL \u2264 Eq0:N\n[ \u2212 log \u220ft=N\u22121 t=0 p\u03b8 (xt | xt+1,xN )\u220fN\u22121 t=1 q (xt | x0,xt+1) ]\n= Eq0:N [ \u2212 N\u22121\u2211 t=1 log p\u03b8 (xt | xt+1,xN ) q (xt | x0,xt+1) \u2212 log p\u03b8 (x0 | x1,xN ) ]\n= N\u22121\u2211 t=1 Eq0,t+1,N [DKL (q (xt | x0,xt+1) \u2016p\u03b8 (xt | xt+1,xN ))]\n+ Eq0,1,N [\u2212 log p\u03b8 (x0 | x1,xN )] . (17)\nBecause Eq. (16) indicates that given xt+1, xt is not dependent on xN when t < N \u2212 1. Therefore, we can further assume that\np\u03b8 (xt | xt+1,xN ) = p\u03b8 (xt | xt+1) . (18)\nAs a result, Eq. (17) is simplified as L \u2264 N\u22121\u2211 t=1 Eq0,t+1,N [DKL (q (xt | x0,xt+1) \u2016p\u03b8 (xt | xt+1))]\n+ Eq0,1,N [\u2212 log p\u03b8 (x0 | x1)] . (19)\nNow, we consider the main part of the loss function Eq0,t+1,N [DKL (q (xt | x0,xt+1) \u2016p\u03b8 (xt | xt+1))]. We begin with the analytical form of q (xt | x0,xt+1). We have the conclusion that:\nq (xt | x0,xt+1) \u223c N (\u00b5\u0303t, \u03c3\u0303tI) , t = 0, 1, ..., N \u2212 1, (20)\nwhere\n\u00b5\u0303t = \u03c32t \u03c32t+1 xt+1 + \u03c32t+1 \u2212 \u03c32t \u03c32t+1 x0, \u03c3\u0303t = \u03c3t \u03c3t+1 \u221a \u03c32t+1 \u2212 \u03c32t .\n(21)\nTherefore, we assume that p\u03b8 (xt | xt+1), t = 1, ..., N \u2212 1, also follows a Gaussian distribution:\np\u03b8 (xt | xt+1) \u223c N ( \u00b5\u03b8,t+1(xt+1), \u03c3\u0303tI ) , (22)\nwhere\n\u00b5\u03b8,t+1(xt+1) = \u03c32t \u03c32t+1 xt+1+ \u03c32t+1 \u2212 \u03c32t \u03c32t+1 f (xt+1, t+ 1; \u03b8) .\n(23)\nAlgorithm 2 The training process for Gaussian noise Input: {x0}, {\u03c3t}, f (\u00b7, \u00b7, \u03b8). Output: trained f (\u00b7, \u00b7, \u03b8).\n1: while \u03b8 is not converged do 2: Random select x0 and sample t from {1, ..., N} uniformly. 3: Sample xt from N (x0, \u03c32t I). 4: Compute grad by \u2207\u03b8 \u2016f(xt, t; \u03b8)\u2212 x0\u201622. 5: Update \u03b8 by grad. 6: end while\nAlgorithm 3 The sampling process for Gaussian noise Input: noisy image xN and trained f (\u00b7, \u00b7, \u03b8). Output: x0.\n1: for t = N \u2212 1, ..., 1 do 2: Sample xt by\nN ( \u03c32t \u03c32t+1 xt+1 + \u03c32t+1 \u2212 \u03c32t \u03c32t+1 f (xt+1, t+ 1; \u03b8) , \u03c3\u0303tI ) .\n(27) 3: end for 4: x0 = f (x1, 1, \u03b8)\nHere, f (xt+1, t+ 1; \u03b8) is a neural network with input of (xt+1, t + 1). Since q (xt | x0,xt+1) and p\u03b8 (xt | xt+1) have the same covariance matrix, we can derive that\nDKL (q (xt | x0,xt+1) \u2016p\u03b8 (xt | xt+1)) = \u2225\u2225\u00b5\u03b8,t+1(xt+1)\u2212 \u00b5\u0303t\u2225\u222522 . (24)\nNeglecting the constant coefficient, minimizing Eq. (24) is equivalent to minimize\n\u2016f(xt+1, t+ 1; \u03b8)\u2212 x0\u201622 (25)\nNext, we turn to Eq0,1,N [\u2212 log p\u03b8 (x0 | x1)], the last term in Eq. (19). If we assume p\u03b8 (x0 | x1) follows some distribution, sampling from it may introduce extra undesired noise. Hence, practically we replace the sampling by E [x0 | x1]. As a result, we train p\u03b8 by Eq0,1,N [ \u2016f (x1, 1; \u03b8)\u2212 x0\u201622\n] instead of Eq0,1,N [\u2212 log p\u03b8 (x0 | x1)].\nCombining with the above analysis, we give the final objective loss function as follows:\nL = Eq t=N\u22121\u2211 t=0 \u2016f(xt+1, t+ 1; \u03b8)\u2212 x0\u201622 . (26)\nAt last, we show the full training and sampling algorithms in Algorithm 2 and Algorithm 3."
        },
        {
            "heading": "3.2.2 Gamma Noise",
            "text": "The Gamma noise model in denoising task is defined by the following form:\nxN = \u03b7 x0, \u03b7i \u223c 1\n\u03b1 G (\u03b1, 1) , (28)\nwhere \u03b1 > 1 and G (\u03b1, 1) is a Gamma distribution with parameters of \u03b1 and 1. represents component-wise multiplication. For the sake of convenience, we neglect it in notation if not ambiguous.\nWe select \u03b10 = \u221e > \u03b11 > \u00b7 \u00b7 \u00b7 > \u03b1N = \u03b1. Therefore, {\u03b1t} is a monotonically decreasing sequence. Let\nx1 = \u03b71x0, \u03b71,i \u223c 1\n\u03b11 G (\u03b11, 1) , (29)\nand\nxt+1 = \u03b1t \u03b1t+1 \u03b6t+1xt, \u03b6t+1,i \u223c B (\u03b1t+1, \u03b1t \u2212 \u03b1t+1) (30)\nwhere t = 1, ..., N \u2212 1 and B (\u03b1t+1, \u03b1t \u2212 \u03b1t+1) is a Beta distribution with parameters of \u03b1t+1 and \u03b1t \u2212 \u03b1t+1. Then we have:\nxt = \u03b7tx0, \u03b7t,i \u223c 1\n\u03b1t G (\u03b1t, 1) , (31)\nwhere t = 0, 1, ..., N \u2212 1. The proof of Eq. (31) is in Appendix. Equation (30) define the relation between xt, t = 1, ..., N . By now, we define a full diffusion process for Gamma Noise. Obviously, Eq. (14) holds according to Eq. (30). Thus, xt, t = 1, ..., N are also a Markov chain.\nBased on the analysis in Sec. 3.2.1, we know that all the equations from Eq. (15) to Eq. (19) still hold.\nNow, we consider the main part of the loss function Eq0,t+1,N [DKL (q (xt | x0,xt+1) \u2016p\u03b8 (xt | xt+1))]. We begin with the analytical form of q (xt | x0,xt+1). We have the conclusion that:( \u03b1txt \u2212 \u03b1t+1xt+1\nx0\n) i \u223c G (\u03b1t \u2212 \u03b1t+1, 1) , t = 1, ..., N \u2212 1.\n(32)\nHere, the division is component-wise operation. Thus, q (xt | x0,xt+1) can be represented by\nxt = x0\u03c4 t + \u03b1t+1xt+1\n\u03b1t , \u03c4t,i \u223c G (\u03b1t \u2212 \u03b1t+1, 1) . (33)\nTherefore, we assume that p\u03b8 (xt | xt+1), t = 1, ..., N \u2212 1 has the following form:\nxt = f (xt+1, t+ 1; \u03b8) \u03c4 t + \u03b1t+1xt+1\n\u03b1t , \u03c4t,i \u223c G(\u03b1t\u2212\u03b1t+1, 1)\n(34)\nAlgorithm 4 The training process for Gamma noise Input: {x0}, {\u03b1t}, f (\u00b7, \u00b7, \u03b8). Output: trained f (\u00b7, \u00b7, \u03b8).\n1: while \u03b8 is not converged do 2: Random select x0 and sample t from {1, ..., N} uniformly. 3: Sample \u03b7t from G (\u03b1t, 1) 4: xt = \u03b7txt . 5: Compute grad by \u2207\u03b8 \u2016f(xt, t; \u03b8)\u2212 x0\u201622. 6: Update \u03b8 by grad. 7: end while\nAlgorithm 5 The sampling process for Gamma noise Input: noisy image xN , {\u03b1t}, and trained f (\u00b7, \u00b7, \u03b8). Output: x0.\n1: for t = N \u2212 1, ..., 1 do 2: Sample \u03c4 t from G(\u03b1t \u2212 \u03b1t+1, 1). 3: xt =\n1 \u03b1t\n(f(xt+1, t+ 1; \u03b8)\u03c4 t + \u03b1t+1xt+1) 4: end for 5: x0 = f (x1, 1, \u03b8)\nwhere f (xt+1, t+ 1; \u03b8) is a neural network with input of (xt+1, t+ 1). Then we can derive that\nDKL (q (xt | x0,xt+1) \u2016p\u03b8 (xt | xt+1)) = \u2211 i (\u03b1t \u2212 \u03b1t+1) ( log f\u03b8,i x0 + x0 f\u03b8,i \u2212 1 ) .\n(35)\nHere, f\u03b8 is the abbreviation of f (xt+1, t+ 1; \u03b8). Suppose f\u03b8\u2217 is the optimal function minimizing Eq. (35), we can prove that it is also the optimal function for the following optimization problem:\nmin f\u03b8\nEq \u2016f(xt+1, t+ 1; \u03b8)\u2212 x0\u201622 . (36)\nThe proof is in Appendix. Hence, training f\u03b8 by minimize KL divergence is equivalent to train f\u03b8 by L2 norm loss function with x0 as labels.\nAs for Eq0,1,N [\u2212 log p\u03b8 (x0 | x1)], the last term in Eq. (19), we adapt the same strategy described in Sec. 3.2.1. As a result, the final objective loss function is as follows:\nL = Eq t=N\u22121\u2211 t=0 \u2016f(xt+1, t+ 1; \u03b8)\u2212 x0\u201622 . (37)\nAt last, we show the full training and sampling algorithms in Algorithm 4 and Algorithm 5."
        },
        {
            "heading": "3.2.3 Poisson Noise",
            "text": "The Poisson noise model in denoising task is defined by the following form:\nxN = P (\u03bbx0)\n\u03bb , (38)\nwhere \u03bb > 0 and P (\u03bbx0) is a Poisson distribution with parameters of \u03bbx0.\nWe select\u221e = \u03bb0 > \u03bb1 > \u00b7 \u00b7 \u00b7 > \u03bbN = \u03bb. Therefore, {\u03bbt} is a monotonically decreasing sequence. Let\nxN | x0 \u223c P (\u03bbNx0)\n\u03bbN , (39)\nand\nxt | xt+1,x0 \u223c \u03bbt+1xt+1 + P ((\u03bbt \u2212 \u03bbt+1)x0)\n\u03bbt , (40)\nwhere t = 1, ..., N \u2212 1. Then we have:\nxt | x0 \u223c P (\u03bbtx0)\n\u03bbt , (41)\nwhere t = 1, ..., N \u2212 1. The proof of Eq. (41) is in Appendix. Equation (40) define the relation between xt, t = 1, ..., N . by now, we define a full diffusion process for Poison Noise. According to Eq. (40), we have\nq (xt | x0,xt+1:N ) = q (xt | x0,xt+1) . (42)\nFrom Eq. (42), we can derive that\nq (x1:N\u22121 | x0,xN ) = N\u22121\u220f t=1 q (xt | x0,xt+1:N )\n= N\u22121\u220f t=1 q (xt | x0,xt+1) .\n(43)\nApplying Eq. (43) to Eq. (10), we can derive the same result as Eq. (17): L \u2264 N\u22121\u2211 t=1 Eq0,t+1,N [DKL (q (xt | x0,xt+1) \u2016p\u03b8 (xt | xt+1,xN ))]\n+ Eq0,1,N [\u2212 log p\u03b8 (x0 | x1,xN )] . (44)\nHowever, xN in p\u03b8 (xt | xt+1,xN ) cannot be removed. Thus, Eq. (19) does not hold for Poisson noise model.\nNow, we consider the main part of the loss function Eq0,t+1,N [DKL (q (xt | x0,xt+1) \u2016p\u03b8 (xt | xt+1))]. We have known the analytical form of q (xt | x0,xt+1) from Eq. (40). Therefore, we assume that p\u03b8 (xt | xt+1,x0), t = 1, ..., N \u2212 1 has the following form:\np\u03b8 (xt | xt+1,xN )\n\u223c\u03bbt+1xt+1 + P ((\u03bbt \u2212 \u03bbt+1) f(xt+1,xN , t+ 1; \u03b8)) \u03bbt ,\n(45)\nwhere f(xt+1,xN , t+ 1; \u03b8) is a neural network with input of (xt+1,xN , t + 1). Denote f(xt+1,xN , t + 1; \u03b8) as f\u03b8\nAlgorithm 6 The training process for Poisson noise Input: {x0}, {\u03bbt}, f (\u00b7, \u00b7, \u03b8). Output: trained f (\u00b7, \u00b7, \u03b8).\n1: while \u03b8 is not converged do 2: Random select x0 and sample t from {1, ..., N \u2212 1} uniformly. 3: Sample xN from\nP(\u03bbNx0) \u03bbN\n. 4: Samplext from 1\u03bbt (\u03bbt+1xt+1 + P ((\u03bbt \u2212 \u03bbt+1)x0))\n5: Compute grad by\u2207\u03b8 \u2016f(xt,xN , t; \u03b8)\u2212 x0\u201622. 6: Update \u03b8 by grad. 7: end while\nAlgorithm 7 The sampling process for Poisson noise Input: noisy image xN , {\u03bbt} and trained f (\u00b7, \u00b7, \u03b8). Output: x0.\n1: for t = N \u2212 1, ..., 1 do 2: Sample \u03c4 t from P ((\u03bbt \u2212 \u03bbt+1) f(xt+1,xN , t; \u03b8)) 3: xt =\n1 \u03bbt\n(\u03bbt+1xt+1 + \u03c4 t) 4: end for 5: x0 = f (x1,xN , 1, \u03b8)\nfor simplicity, then we can derive that\nDKL (q (xt | x0,xt+1) \u2016p\u03b8 (xt | xt+1,xN )) = (logx0 \u2212 log f\u03b8) (\u03bbt \u2212 \u03bbt+1)x0\n\u2212 (\u03bbt \u2212 \u03bbt+1) (x0 \u2212 f\u03b8) . (46)\nSimilar to the analysis in Sec. 3.2.2, we attempt to transform the original optimization problem to another equivalent one. Suppose f\u03b8\u2217 is the optimal function minimizing Eq. (46), we can prove that it is also the optimal function for the following optimization problem:\nmin f\u03b8\nEq \u2016f(xt+1,xN , t+ 1; \u03b8)\u2212 x0\u201622 . (47)\nThe proof is in Appendix. Hence, training f\u03b8 by minimize KL divergence is equivalent to train f\u03b8 by L2 norm loss function with x0 as labels.\nAs for Eq0,1,N [\u2212 log p\u03b8 (x0 | x1,xN )], the last term in Eq. (44), we still adapt the same strategy described in Sec. 3.2.1. As a result, the final objective loss function is as follows:\nL = Eq t=N\u22121\u2211 t=0 \u2016f(xt+1,xN , t+ 1; \u03b8)\u2212 x0\u201622 . (48)\nAt last, we show the full training and sampling algorithms in Algorithm 6 and Algorithm 7."
        },
        {
            "heading": "3.3. Discussion",
            "text": "In this section, we have discussed three types of noise models in the denoising task, Gaussian, Gamma and Pois-\nson noise. The diffusion process of Gaussian and Gamma noise can be defined as a Markov chain, representing the evolution from clean images to noisy images. Gaussian distribution itself is additive. Thus, its diffusion process is only related to Gaussian distribution. While Beta distribution is introduced to define the diffusion process for Gamma noise. As for Poisson noise, the difference is clear. From definition, its diffusion process can also be regarded as another form of Markov chain, which is conditioned on x0 and represents an evolution from noisier images to less noisy images. Hence, the definition of diffusion process is related to the statistical property of the noise model.\nAbout the derivation of objective loss functions of model training, the basic idea is to minimize KL divergence. In the case of Gamma noise and Poisson noise, we transform the original complex optimization problem to L2 norm loss minimization through optimization equivalence. As a result, the model training for three noise models are highly consistent. Though the input of models are slightly different, they can all be written as\nminEq,t \u2016f\u03b8,t \u2212 x0\u201622 . (49)\nIn other words, given t the model training amounts to estimate the posterior mean, E [x0 | xt+1,xN ]. Therefore, p\u03b8 (xt | xt+1,xN ) is constructed through replacing the x0 in q (xt | x0,xt+1) by the posterior mean."
        },
        {
            "heading": "4. Experiment",
            "text": "We conduct extensive experiments to evaluate our approach, including Gaussian noise, Gamma noise and Poisson noise with different noise levels.\nDataset and Implementation Details We evaluate the proposed method for gray images in the two benchmark datasets: KOdak dataset and CSet9. DIV2K [21] and CBSD500 dataset [2] are used as training datasets. Original images are color RGB natural images and we transform them to gray images when training and testing. We use traditional supervised learning (Eq. (2)) with as L2 norm loss as the baseline for comparison. The same modified U-Net [5] with about 70 million parameters is used for all methods. When training, we randomly clip the training images to patches with the resolution of 128 \u00d7 128. AdamW optimizer [13] is used to train the network. We train each model with the batch size of 32. To reduce memory, we utilize the tricks of cumulative gradient and mixed precision training. The learning rate is set as 1\u00d710\u22124. All the models are implemented in PyTorch [15] with NVidia V100. The pixel value range of all clean images are [0, 255] and the parameters of noise models are built on it. When training models, images will be scaled to [\u22121, 1]. After generating samples, they will be scaled back to [0, 255]. For each\ntype of noise model, we choose two noise level and different number of diffusion steps, N . We list them here:\n\u2022 Gaussian: \u03c3 = 25 (N = 20), \u03c3 = 50 (N = 40);\n\u2022 Gamma: \u03b1 = 26 (N = 20), \u03b1 = 6.5 (N = 40);\n\u2022 Poisson: \u03bb = 0.2 (N = 20), \u03bb = 0.1 (N = 40).\nAnother setting which is not discussed in Sec. 3 is how to construct the sequence of noise model parameters. In our experiments, we adapt a simple but effective way in which the sequence is constructed such that the standard deviation of xt | x0 is linearly increased from t = 0 to t = N . The more details of implementation are described in Appendix.\nResults Figure 3 shows the generated samples for different noise models. Compared to supervised learning, our generated results are visually pleasing, containing better image details. The good visual quality of generated samples verifies that our method is feasible for generative denoising tasks. We also compute the PSNR and SSIM to evaluate our method as shown in Tab. 1. We discover that there is a gap between supervised learning and generated samples in terms of metrics. This is understandable. Because noisy images have lost much original image information, it is hard to generate completely identical details to those lost ones. For each noisy images, we generate 100 samples and compute the mean of them as the estimation of p (x | y), which are also illustrated in Tab. 1. Apparently, the metrics of sample mean is close to supervised learning, which further verifies that the posterior distribution estimated by our method is effective. We also investigate the effect of the number of steps, N . Due to the limit of paper length, we discuss it in Appendix."
        },
        {
            "heading": "5. Conclusion",
            "text": "In this paper, we apply the framework of diffusion models to generative image denoising tasks and propose a new diffusion model based on the image noise model. Different to the original diffusion model, we redefine the diffusion process according to the specific noise models and derive the model training and sampling algorithms. Interestingly, we find that model training for Gaussian, Gamma and Poisson noise is unified to a highly consistent strategy. Our experiments verify that our method is feasible for generative image denoising. In the future, we hope to extend our method to other noise model and evaluate its performance on other dataset."
        },
        {
            "heading": "A. Proofs",
            "text": "A.1. The Proof of Eq. (10) in Sec. 3.1.3\nL \u2264 Eq0:N\n[ \u2212 log \u220ft=N\u22121 t=0 p\u03b8 (xt | xt+1,xN ) q (x1:N\u22121 | x0,xN ) ] . (10)\nThe full derivation is as follows.\nProof.\nL = Ex0,xN [\u2212 log p\u03b8 (x0 | xN )]\n= Ex0,xN\n[ \u2212 log \u222b X1,...,XN\u22121 p\u03b8 (x0,x1, ...,xN\u22121 | xN )\ndx1 \u00b7 \u00b7 \u00b7 dxN\u22121]\n= Ex0,xN\n[ \u2212 log \u222b X1,...,XN\u22121 p\u03b8 (x0,x1, ...,xN\u22121 | xN ) q (x1, ...,xN\u22121 | x0,xN )\nq (x1, ...,xN\u22121 | x0,xN ) dx1 \u00b7 \u00b7 \u00b7 dxN\u22121]\n\u2264 Ex0,xN [ \u2212 \u222b X1,...,XN\u22121 q (x1, ...,xN\u22121 | x0,xN )\nlog p\u03b8 (x0,x1, ...,xN\u22121 | xN ) q (x1, ...,xN\u22121 | x0,xN )\ndx1 \u00b7 \u00b7 \u00b7 dxN\u22121 ]\n= Ex0,xN [ Ex1,...,xN\u22121|x0,xN [ \u2212 log p\u03b8 (x0,x1, ...,xN\u22121 | xN )\nq (x1, ...,xN\u22121 | x0,xN ) ]] = Ex0,x1,...,xN\u22121,xN [ \u2212 log p\u03b8 (x0,x1, ...,xN\u22121 | xN )\nq (x1, ...,xN\u22121 | x0,xN ) ] = Eq0:N [ \u2212 log p\u03b8 (x0:N\u22121 | xN )\nq (x1:N\u22121 | x0,xN )\n] .\nA.2. The Proof of Eq. (15) in Sec. 3.2.1\nq (x1:N\u22121 | x0,xN ) = N\u22121\u220f t=1 q (xt | x0,xt+1) . (15)\nProof. According to Eq. (14), we have the following derivation:\nq (x1:N\u22121 | x0,xN )\n= N\u22121\u220f t=1 q (xt | x0,xt+1:N )\n= N\u22121\u220f t=1 q (xt,xt+2:N | x0,xt+1) q (xt+2:N | x0,xt+1)\n= N\u22121\u220f t=1 q (xt+2:N | x0,xt+1,xt) q (xt | x0,xt+1) q (xt+2:N | x0,xt+1)\n= N\u22121\u220f t=1 q (xt+2:N | xt+1) q (xt | x0,xt+1) q (xt+2:N | xt+1)\n= N\u22121\u220f t=1 q (xt | x0,xt+1) .\nEquation (14) is applied in the fourth equation.\nA.3. The Proof of Eq. (16) in Sec. 3.2.1\nq (xt | xt+1:N ) = q (xt | xt+1) . (16)\nProof.\nq (xt | xt+1:N )\n= q (xt,xt+2:N | xt+1) q (xt+2:N | xt+1) = q (xt | xt+1) q (xt+2:N | xt+1,xt)\nq (xt+2:N | xt+1)\n= q (xt | xt+1) q (xt+2:N | xt+1) q (xt+2:N | xt+1) =q (xt | xt+1) .\nEquation (14) is applied in the third equation.\nA.4. The Proof of Eq. (20) and Eq. (21) in Sec. 3.2.1\nq (xt | x0,xt+1) \u223c N (\u00b5\u0303t, \u03c3\u0303tI) , t = 0, 1, ..., N \u2212 1, (20)\nwhere\n\u00b5\u0303t = \u03c32t \u03c32t+1 xt+1 + \u03c32t+1 \u2212 \u03c32t \u03c32t+1 x0, \u03c3\u0303t = \u03c3t \u03c3t+1 \u221a \u03c32t+1 \u2212 \u03c32t .\n(21)\nProof. Firstly, we have the following derivation according to Bayesian Equation.\nq (xt | x0,xt+1) = q (xt,xt+1 | x0) q (xt+1 | x0)\n= q (xt | x0) q (xt+1 | xt)\nq (xt+1 | x0) .\nSince q (xt | x0), q (xt+1 | xt) and q (xt+1 | x0) all follow known Gaussian distribution, we can derive the analytical form for q (xt | x0,xt+1). It is easy to verify that it also follows Gaussian distribution and the parameters are derived as Eq. (21).\nA.5. The Proof of Eq. (31) in Sec. 3.2.2\nxt = \u03b7tx0, \u03b7t,i \u223c 1\n\u03b1t G (\u03b1t, 1) , (31)\nwhere t = 0, 1, ..., N \u2212 1.\nProof. Firstly, we have a known conclusion from probability theory: Suppose X \u223c G (\u03b1, 1) , Y \u223c G (\u03b2, 1), then\nX\nX + Y \u223c B (\u03b1, \u03b2) ,\nand X + Y is independent to XX+Y . Based on it, we have a corollary: Suppose U is independent to V and U \u223c B(\u03b1, \u03b2), V \u223c G(\u03b1+ \u03b2, 1), then\nUV \u223c G(\u03b1, 1).\nNow, we prove Eq. (31) according to the definition in Eq. (29) and Eq. (30). It is easy to verify that x1 | x0 is satisfied. Assume xt\u22121 | x0 follows \u03b7t\u22121x0, then\nxt | x0 \u223c \u03b1t\u22121 \u03b1t \u03b6t\u03b7t\u22121x0,\nwhere \u03b7t\u22121 \u223c 1\u03b1t\u22121G (\u03b1t\u22121, 1), \u03b6t \u223c B (\u03b1t+1, \u03b1t\u2212 \u03b1t+1). Since \u03b6t and \u03b7t\u22121 are independent, \u03b6t\u03b7t\u22121 \u223c 1 \u03b1t\u22121 G(\u03b1t, 1). Considering the coefficient, Eq. (31) is proved.\nA.6. The Proof of Eq. (32) in Sec. 3.2.2\n( \u03b1txt \u2212 \u03b1t+1xt+1\nx0\n) i \u223c G (\u03b1t \u2212 \u03b1t+1, 1) , t = 1, ..., N \u2212 1.\n(32)\nProof. Similar to Appendix A.4, we still have\nq (xt | x0,xt+1) = q (xt,xt+1 | x0) q (xt+1 | x0)\n= q (xt | x0) q (xt+1 | xt)\nq (xt+1 | x0) .\nSince each component is independent, we prove Eq. (32) by components. We have known the following probability density functions:\nq (xt | x0) = 1\n\u0393 (\u03b1t) ( \u03b1txt x0 )\u03b1t\u22121 exp { \u2212\u03b1txt x0 } \u00b7 \u03b1t x0 ,\nq (xt+1 | x0) = 1\n\u0393 (\u03b1t+1)\n( \u03b1t+1xt+1\nx0 )\u03b1t+1\u22121 \u00b7 exp { \u2212\u03b1t+1xt+1\nx0\n} \u00b7 \u03b1t+1 x0 ,\nq (xt+1 | xt) = \u0393 (\u03b1t)\n\u0393 (\u03b1t+1) \u0393 (\u03b1t \u2212 \u03b1t+1) ( \u03b1t+1xt \u03b1txt )\u03b1t+1\u22121 \u00b7 (\n1\u2212 \u03b1t+1xt+1 \u03b1txt )\u03b1t\u2212\u03b1t+1\u22121 \u00b7 \u03b1t+1 \u03b1txt .\nApplying the equations above, we have that\nq (xt | x0) q (xt+1 | xt) q (xt+1 | x0)\n= 1\n\u0393 (\u03b1t) ( \u03b1txt x0 )\u03b1t\u22121 exp { \u2212\u03b1txt x0 } \u00b7 \u03b1t x0\n\u00b7 \u0393 (\u03b1t) \u0393 (\u03b1t+1) \u0393 (\u03b1t \u2212 \u03b1t+1) ( \u03b1t+1xt+1 \u03b1txt )\u03b1t+1\u22121 \u00b7 (\n1\u2212 \u03b1t+1xt+1 \u03b1txt )\u03b1t\u2212\u03b1t+1\u22121 \u00b7 \u03b1t+1 \u03b1txt\n/  ( \u03b1t+1xt+1 x0 )\u03b1t+1\u22121 \u0393 (\u03b1t+1) exp { \u2212\u03b1t+1xt+1 x0 } \u00b7 \u03b1t+1 x0  = 1\n\u0393 (\u03b1t \u2212 \u03b1t+1) (\u03b1txt)\n\u03b1t\u22121 (\u03b1t+1xt+1) \u03b1t+1\u22121 x \u03b1t+1\u22121 0\nx\u03b1t\u221210 (\u03b1txt) \u03b1t+1\u22121 (\u03b1t+1xt+1) \u03b1t+1\u22121 \u00b7 exp { \u2212\u03b1txt\nx0 + \u03b1t+1xt+1 x0 } \u00b7 (\n1\u2212 \u03b1t+1xt+1 \u03b1txt )\u03b1t\u2212\u03b1t+1\u22121 \u03b1t x0 \u00b7 \u03b1t+1 \u03b1txt \u00b7 x0 \u03b1t+1\n= 1 \u0393 (\u03b1t \u2212 \u03b1t+1) (\u03b1txt)\n\u03b1t\u2212\u03b1t+1\nx \u03b1t\u2212\u03b1t+1 0\nexp { \u2212\u03b1txt \u2212 \u03b1t+1xt+1\nx0 } \u00b7 ( \u03b1txt \u2212 \u03b1t+1xt+1\n\u03b1txt\n)\u03b1t\u2212\u03b1t+1\u22121 \u00b7 1 xt\n=\n( 1 x0 )\u03b1t\u2212\u03b1t+1 \u0393 (\u03b1t \u2212 \u03b1t+1) exp { \u2212\u03b1txt \u2212 \u03b1t+1xt+1 x0 } \u00b7 (\u03b1txt \u2212 \u03b1t+1xt+1)\u03b1t\u2212\u03b1t+1\u22121 \u00b7 \u03b1t.\nTherefore, let \u03c4t = \u03b1txt\u2212\u03b1t+1xt+1\nx0 , then \u03c4t follows\nG (\u03b1t \u2212 \u03b1t+1, 1). Thus, Eq. (32) is proved.\nA.7. The Derivation of Eq. (35) in Sec 3.2.2\nDKL (q (xt | x0,xt+1) \u2016p\u03b8 (xt | xt+1)) = \u2211 i (\u03b1t \u2212 \u03b1t+1) ( log f\u03b8,i x0,i + x0,i f\u03b8,i \u2212 1 ) .\n(35)\nHere, f\u03b8 is the abbreviation of f (xt+1, t+ 1; \u03b8).\nProof.\nDKL (q (xt | x0,xt+1) \u2016p\u03b8 (xt | xt+1))\n= \u222b q log q (xt | x0,xt+1) p\u03b8 (xt | xt+1) dxt\n= \u2211 i \u222b q ( (\u03b1t \u2212 \u03b1t+1) log f\u03b8,i x0,i \u2212 (\u03b1txt \u2212 \u03b1t+1xt+1)\n\u00b7 ( 1\nx0,i \u2212 1 f\u03b8,i\n)) dxt\n= \u2211 i (\u03b1t \u2212 \u03b1t+1) log f\u03b8,i x0,i \u2212 E [x0,i\u03c4t,i] ( 1 x0,i \u2212 1 f\u03b8,i ) = \u2211 i (\u03b1t \u2212 \u03b1t+1) log f\u03b8,i x0,i \u2212 (\u03b1t \u2212 \u03b1t+1)x0,i ( 1 x0,i \u2212 1 f\u03b8,i\n) = \u2211 i (\u03b1t \u2212 \u03b1t+1) ( log f\u03b8,i x0,i + x0,i f\u03b8,i \u2212 1 ) .\nA.8. The Proof of Eq. (36) in Sec 3.2.2\nSuppose f\u03b8\u2217 is the optimal function minimizing Eq. (35), it is also the optimal function for the following optimization problem:\nmin f\u03b8\nEq \u2016f(xt+1, t+ 1; \u03b8)\u2212 x0\u201622 . (36)\nProof. Minimizing KL divergence is equivalent to Eq \u2211 i [ (\u03b1i\u22121 \u2212 \u03b1i) ( log f(xt+1, t; \u03b8)i x0 + x0,i f(xt+1, t; \u03b8)i )] .\nGiven t and xt, according to [22] the optimal f(xt+1, t; \u03b8) satisfies that\narg min \u03b2\n\u222b q(x0 | xt+1) \u2211 i ( log \u03b2i x0,i + x0,i \u03b2i ) dx0.\nWe compute the gradient of \u03b2 for the right part and derive that \u222b q(x0 | xt+1) ( 1\n\u03b2 \u2212 x0 \u03b22\n) dx0.\nLet the gradient to be 0 (we can neglect the situation where \u03b2 = 0), we have\u222b\nq(x0 | xt+1) (\u03b2 \u2212 x0) dx0 = 0.\nWhen \u03b2 = E [x0 | xt+1], it is the optimal solution. Therefore, the optimal f(xt+1, t; \u03b8) should be E [x0 | xt+1]. That is to say, training f(xt+1, t; \u03b8) by minimizing KL divergence is equivalent to minimize\nEq \u2016f(xt+1, t; \u03b8)\u2212 x0\u201622 .\nA.9. The Proof of Eq. (41) in Sec 3.2.3\nxt | x0 \u223c P (\u03bbtx0)\n\u03bbt , (41)\nwhere t = 1, ..., N \u2212 1.\nProof. Firstly, Eq. (41) holds when t = N . Assume that xt+1 | x0 holds, then\nxt | x0 \u223c \u03bbt+1xt+1 | x0 + P ((\u03bbt \u2212 \u03bbt+1)x0)\n\u03bbt\n\u223c P ((\u03bbt+1)x0) + P ((\u03bbt \u2212 \u03bbt+1)x0) \u03bbt \u223c P (\u03bbtx0) \u03bbt .\nA.10. The Derivation of Eq. (46) in Sec 3.2.3\nDKL (q (xt | x0,xt+1) \u2016p\u03b8 (xt | xt+1)) = \u2211 i (log x0,i \u2212 log f\u03b8,i) (\u03bbt \u2212 \u03bbt+1)x0,i\n\u2212 (\u03bbt \u2212 \u03bbt+1) (x0,i \u2212 f\u03b8,i) ,\n(46)\nwhere f(xt+1,xN , t+ 1; \u03b8) is denoted as f\u03b8 for simplicity.\nProof. Let \u03c4t = \u03bbtxt \u2212 \u03bbt+1xt+1. For simplicity, we regard all vectors as variables. Then, we have the following derivation\nDKL (q (xt | x0, xt+1) \u2016p\u03b8 (xt | xt+1)) = \u2211 xt q (xt | x0, xt+1) log q (xt | x0, xt+1) p\u03b8 (xt | xt+1)\n= \u2211 \u03c4t (\u03bbt \u2212 \u03bbt+1)\u03c4t x\u03c4t0 \u03c4t! e\u2212(\u03bbt\u2212\u03bbt+1)x0\n\u00b7 log\n(\u03bbt \u2212 \u03bbt+1)\u03c4t x\u03c4t0 \u03c4t! e\u2212(\u03bbt\u2212\u03bbt+1)x0 (\u03bbt \u2212 \u03bbt+1)\u03c4t f\u03c4t\u03b8 \u03c4t! e\u2212(\u03bbt\u2212\u03bbt+1)f\u03b8\n= \u2211 \u03c4t (\u03bbt \u2212 \u03bbt+1)\u03c4t x\u03c4t0 \u03c4t! e\u2212(\u03bbt\u2212\u03bbt+1)x0\n\u00b7 ((log x0 \u2212 log f\u03b8) \u03c4t \u2212 (\u03bbt \u2212 \u03bbt+1) (x0 \u2212 f\u03b8)) = (log x0 \u2212 log f\u03b8)E [\u03c4t]\u2212 (\u03bbt \u2212 \u03bbt+1) (x0 \u2212 f\u03b8) = (log x0 \u2212 log f\u03b8) (\u03bbt \u2212 \u03bbt+1)x0 \u2212 (\u03bbt \u2212 \u03bbt+1) (x0 \u2212 f\u03b8) .\nTherefore, Eq. (46) is proved.\nA.11. The Proof of Eq. (47) in Sec 3.2.3\nSuppose f\u03b8\u2217 is the optimal function minimizing Eq. (46), it is also the optimal function for the following optimization problem:\nmin f\u03b8\nEq \u2016f(xt+1,xN , t+ 1; \u03b8)\u2212 x0\u201622 . (47)\nProof. Minimizing KL divergence is equivalent to\nEx0,xt+1 \u2211 i (log x0,i \u2212 log f\u03b8,i) (\u03bbt \u2212 \u03bbt+1)x0,i\n\u2212 (\u03bbt \u2212 \u03bbt+1) (x0,i \u2212 f\u03b8,i) .\nGiven t and xt, according to [22] the optimal f(xt+1,xN , t; \u03b8) satisfies that\narg min s\n\u222b q(x0 | xt+1,xN ) \u2211 i ((log x0,i \u2212 log si)\n\u00b7 (\u03bbt \u2212 \u03bbt+1)x0,i \u2212 (\u03bbt \u2212 \u03bbt+1) (x0,i \u2212 si)) dx0.\nWe compute the gradient of s for the right part and derive that\u222b q(x0 | xt+1,xN ) ( \u2212 (\u03bbt \u2212 \u03bbt+1)\nx0 s\n+ (\u03bbt \u2212 \u03bbt+1) ) dx0.\nLet the gradient to be 0, we have\u222b q(x0 | xt+1,xN ) ( \u2212x0 s + 1 ) dx0\n=\u2212 E [x0 | xt+1,xN ] s + 1 = 0.\nWhen s = E [x0 | xt+1,xN ], it is the optimal solution. Therefore, the optimal f(xt+1,xN , t; \u03b8) should be E [x0 | xt+1,xN , ]. That is to say, training f(xt+1,xN , t; \u03b8) by minimizing KL divergence is equivalent to minimize\nEq \u2016f(xt+1,xN , t; \u03b8)\u2212 x0\u201622 ."
        },
        {
            "heading": "B. Experimental Details",
            "text": "Parameters Sequence For simplicity, suppose {\u03b1t} is the parameters sequence used for constructing the diffusion process. Let \u03c3\u03b1t represent the standard deviation of xt | x0 from t = 0 to t = N where N is the diffusion steps. We have that \u03c3\u03b10 = 0 and \u03c3\u03b1N is known. We set\n\u03c3\u03b1t = t\nN \u2217 \u03c3\u03b1N\nand ascertain the value of \u03b1t according to \u03c3\u03b1t .\nThe Effect of Steps N For Gaussian noise with \u03c3 = 25, Gamma noise with \u03b1 = 26 and Poisson noise with \u03bb = 0.2, we also conduct experiments where N = 10 and compare the result to N = 20. The result is shown in Tab. 2 and Fig. 4. From the metrics and visual quality, our method also achieve satisfied performance even with N = 10."
        }
    ],
    "title": "Diffusion Model for Generative Image Denoising",
    "year": 2023
}