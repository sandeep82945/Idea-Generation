{
    "abstractText": "Response ranking in dialogues plays a crucial role in retrieval-based conversational systems. In a multi-turn dialogue, to capture the gist of a conversation, contextual information serves as essential knowledge to achieve this goal. In this paper, we present a flexible neural framework that can integrate contextual information from multiple channels. Specifically for the current task, our approach is to provide two information channels in parallel, Fusing Conversation history and domain knowledge extracted from Candidate provenance (FCC), where candidate responses are curated, as contextual information to improve the performance of multi-turn dialogue response ranking. The proposed approach can be generalized as a module to incorporate miscellaneous contextual features for other context-oriented tasks. We evaluate our model on the MSDialog dataset widely used for evaluating conversational response ranking tasks. Our experimental results show that our framework significantly outperforms the previous state-of-the-art models, improving Recall@1 by 7% and MAP by 4%. Furthermore, we conduct ablation studies to evaluate the contributions of each information channel, and of the framework components, to the overall ranking performance, providing additional insights and directions for further improvements.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zihao Wang"
        },
        {
            "affiliations": [],
            "name": "Eugene Agichtein"
        },
        {
            "affiliations": [],
            "name": "Jinho Choi"
        }
    ],
    "id": "SP:b0581f408a5bc95b933d5eb49a044ea9ff845ac7",
    "references": [
        {
            "authors": [
                "A. Ahmadvand",
                "I.J. Choi",
                "H. Sahijwani",
                "J. Schmidt",
                "M. Sun",
                "S. Volokhin",
                "Z. Wang",
                "E. Agichtein"
            ],
            "title": "Emory irisbot: An open-domain conversational bot for personalized information access",
            "venue": "Alexa Prize Proceedings",
            "year": 2018
        },
        {
            "authors": [
                "A. Ahmadvand",
                "H. Sahijwani",
                "J.I. Choi",
                "E. Agichtein"
            ],
            "title": "Concet: Entity-aware topic classification for open-domain conversational agents",
            "venue": "Proceedings of the 28th ACM International Conference on Information and Knowledge Management, pp. 1371\u20131380",
            "year": 2019
        },
        {
            "authors": [
                "A. Berger",
                "J. Lafferty"
            ],
            "title": "Information retrieval as statistical translation",
            "venue": "ACM SIGIR Forum, vol. 51, pp. 219\u2013226. ACM New York, NY, USA",
            "year": 2017
        },
        {
            "authors": [
                "C. Burges",
                "T. Shaked",
                "E. Renshaw",
                "A. Lazier",
                "M. Deeds",
                "N. Hamilton",
                "G.N. Hullender"
            ],
            "title": "Learning to rank using gradient descent",
            "venue": "Proceedings of the 22nd International Conference on Machine learning (ICML-05), pp. 89\u201396",
            "year": 2005
        },
        {
            "authors": [
                "J. Chung",
                "C. Gulcehre",
                "K. Cho",
                "Y. Bengio"
            ],
            "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
            "venue": "arXiv preprint arXiv:1412.3555",
            "year": 2014
        },
        {
            "authors": [
                "G.V. Cormack",
                "M.D. Smucker",
                "C.L. Clarke"
            ],
            "title": "Efficient and effective spam filtering and re-ranking for large web datasets",
            "venue": "Information retrieval 14(5), 441\u2013465",
            "year": 2011
        },
        {
            "authors": [
                "J. Dalton",
                "L. Dietz",
                "J. Allan"
            ],
            "title": "Entity query feature expansion using knowledge base links",
            "venue": "Proceedings of the 37th international ACM SIGIR conference on Research and development in information retrieval, pp. 365\u2013374",
            "year": 2014
        },
        {
            "authors": [
                "F. Guo",
                "A. Metallinou",
                "C. Khatri",
                "A. Raju",
                "A. Venkatesh",
                "A. Ram"
            ],
            "title": "Topic-based evaluation for conversational bots",
            "year": 2018
        },
        {
            "authors": [
                "E. Hovy",
                "U. Hermjakob",
                "Lin",
                "C.Y"
            ],
            "title": "The use of external knowledge in factoid qa",
            "venue": "TREC, vol. 2001, pp. 644\u201352",
            "year": 2001
        },
        {
            "authors": [
                "X. Hu",
                "X. Zhang",
                "C. Lu",
                "E.K. Park",
                "X. Zhou"
            ],
            "title": "Exploiting wikipedia as external knowledge for document clustering",
            "venue": "Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 389\u2013396",
            "year": 2009
        },
        {
            "authors": [
                "K. Jokinen",
                "H. Tanaka",
                "A. Yokoo"
            ],
            "title": "Contextmanagementwith topics for spoken dialogue systems",
            "venue": "Proceedings of the 17th international conference on Computational linguistics-Volume 1, pp. 631\u2013637. Association for Computational Linguistics",
            "year": 1998
        },
        {
            "authors": [
                "Y. Luan",
                "Y. Ji",
                "M. Ostendorf"
            ],
            "title": "Lstm based conversation models",
            "venue": "arXiv preprint arXiv:1603.09457",
            "year": 2016
        },
        {
            "authors": [
                "J.D. Mcauliffe",
                "D.M. Blei"
            ],
            "title": "Supervised topic models",
            "venue": "J.C. Platt, D. Koller, Y. Singer, S.T. Roweis (eds.) Advances in Neural Information Processing Systems 20, pp. 121\u2013 128. Curran Associates, Inc.",
            "year": 2008
        },
        {
            "authors": [
                "T. Mikolov",
                "I. Sutskever",
                "K. Chen",
                "G.S. Corrado",
                "J. Dean"
            ],
            "title": "Distributed representations of words and phrases and their compositionality",
            "venue": "Advances in neural information processing systems, pp. 3111\u20133119",
            "year": 2013
        },
        {
            "authors": [
                "L. Pang",
                "Y. Lan",
                "J. Guo",
                "J. Xu",
                "S. Wan",
                "X. Cheng"
            ],
            "title": "Text matching as image recognition",
            "year": 2016
        },
        {
            "authors": [
                "C. Qu",
                "L. Yang",
                "W.B. Croft",
                "J.R. Trippas",
                "Y. Zhang",
                "M. Qiu"
            ],
            "title": "Analyzing and characterizing user intent in information-seeking conversations",
            "venue": "The 41st International ACMSIGIRConference on Research and Development in Information Retrieval",
            "year": 2018
        },
        {
            "authors": [
                "A. Ram",
                "R. Prasad",
                "C. Khatri",
                "A. Venkatesh",
                "R. Gabriel",
                "Q. Liu",
                "J. Nunn",
                "B. Hedayatnia",
                "M. Cheng",
                "A. Nagar",
                "E. King",
                "K. Bland",
                "A. Wartick",
                "Y. Pan",
                "H. Song",
                "S. Jayadevan",
                "G. Hwang",
                "A. Pettigrue"
            ],
            "title": "Conversational ai: The science behind the alexa prize",
            "year": 2018
        },
        {
            "authors": [
                "S.E. Robertson",
                "S. Walker",
                "S. Jones",
                "M.M. Hancock-Beaulieu",
                "M Gatford"
            ],
            "title": "Okapi at trec-3",
            "venue": "Nist Special Publication Sp 109, 109",
            "year": 1995
        },
        {
            "authors": [
                "A. Shashua",
                "A. Levin"
            ],
            "title": "Ranking with large margin principle: Two approaches",
            "venue": "Advances in neural information processing systems, pp. 961\u2013968",
            "year": 2003
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, pp. 5998\u20136008",
            "year": 2017
        },
        {
            "authors": [
                "Z. Wang",
                "A. Ahmadvand",
                "J.I. Choi",
                "P. Karisani",
                "E. Agichtein"
            ],
            "title": "Emersonbot: Informationfocused conversational ai emory university at the alexa prize 2017 challenge",
            "venue": "Proc. Alexa Prize",
            "year": 2017
        },
        {
            "authors": [
                "B. Wu",
                "B. Wang",
                "H. Xue"
            ],
            "title": "Ranking responses oriented to conversational relevance in chatbots",
            "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pp. 652\u2013662",
            "year": 2016
        },
        {
            "authors": [
                "Q. Wu",
                "C.J. Burges",
                "K.M. Svore",
                "J. Gao"
            ],
            "title": "Adapting boosting for information retrieval measures",
            "venue": "Information Retrieval 13(3), 254\u2013270",
            "year": 2010
        },
        {
            "authors": [
                "C. Xiong",
                "Z. Liu",
                "J. Callan",
                "E. Hovy"
            ],
            "title": "Jointsem: Combining query entity linking and entity based document ranking",
            "venue": "Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, pp. 2391\u20132394",
            "year": 2017
        },
        {
            "authors": [
                "R. Yan",
                "Y. Song",
                "H. Wu"
            ],
            "title": "Learning to respond with deep neural networks for retrieval-based human-computer conversation system",
            "venue": "Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, pp. 55\u201364. ACM",
            "year": 2016
        },
        {
            "authors": [
                "L. Yang",
                "M. Qiu",
                "C. Qu",
                "C. Chen",
                "J. Guo",
                "Y. Zhang",
                "W.B. Croft",
                "H. Chen"
            ],
            "title": "Iart: Intent-aware response ranking with transformers in information-seeking conversation systems",
            "year": 2020
        },
        {
            "authors": [
                "L. Yang",
                "M. Qiu",
                "C. Qu",
                "J. Guo",
                "Y. Zhang",
                "W.B. Croft",
                "J. Huang",
                "H. Chen"
            ],
            "title": "Response ranking with deep matching networks and external knowledge in information-seeking conversation systems",
            "venue": "The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 245\u2013254. ACM",
            "year": 2018
        },
        {
            "authors": [
                "J.F. Yeh",
                "C.H. Lee",
                "Y.S. Tan",
                "L.C. Yu"
            ],
            "title": "Topic model allocation of conversational dialogue records by latent dirichlet allocation",
            "venue": "Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific, pp. 1\u20134. IEEE",
            "year": 2014
        },
        {
            "authors": [
                "X. Zhou",
                "L. Li",
                "D. Dong",
                "Y. Liu",
                "Y. Chen",
                "W.X. Zhao",
                "D. Yu",
                "H. Wu"
            ],
            "title": "Multi-turn response selection for chatbots with deep attention matching network",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1118\u20131127",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Response ranking is an essential part of dialogue systems [21, 1], and plays a critical part in information- or search-oriented dialogues where responses may come from diverse yet usually designated sources. As shown in Fig. 1, candidate (1) is the true\nZihao Wang Emory University, 201 Dowman Dr, Atlanta, e-mail: zihao.wang2@emory.edu\nEugene Agichtein Emory University, 201 Dowman Dr, Atlanta, e-mail: eugene.agichtein@emory.edu\nJinho Choi Emory University, 201 Dowman Dr, Atlanta, e-mail: jinho.choi@emory.edu\n1\nar X\niv :2\n30 4.\n00 18\n0v 1\n[ cs\n.C L\n] 3\n1 M\nresponse, while the other two are negative candidates. Semantically, it is reasonably easy to differentiate between candidates (2) and (3). However, it is challenging to differentiate candidates (1) and (2), since they describe the same procedure with minor differences. In these cases, it is critical to have other surrounding knowledge, such as contextual information, to make distinguishing decisions.\nPrevious research [29, 26] has extensively considered modeling of conversation history with external knowledge [27] for a better understanding of the conversation flow. However, most of the previous work did not take into account the source (provenance) information of candidate responses, such as the domain information where domain-related candidates are curated. This domain knowledge was ignored or treated separately excluded from the overall ranking model, especially in componentand retrieval-based conversation systems [17]. Instead, we show that it is significant to jointly model the conversation history and domain information from the provenance of candidate responses, as input to built-in parallel information channels in the ranking model, which would allow the model to benefit from both sources of evidence. To validate our idea, implemented as the FCC models in this paper, we compare experimental results with previous state-of-the-art models, in addition to ablation studies to analyze the effect of domain information of candidates to response ranking performance. Our experiments are performed on the established benchmark MSDialog dataset, widely used for conversation ranking tasks. Our results show that our model significantly improves the performance, by 3-8% margins on recall@1, recall@2, and MAP, over the previous state-of-the-art models. Our ablation studies confirm that domain information of candidates have their advantages over the state-of-the-art models. Furthermore, we improve the modeling of conversation history by implementing the self-attention mechanism on previous turns and validate its effect to ranking performance by ablation studies.\nIn this paper, we tackle the response ranking problem by introducing a new aspect, as the candidate provenance, to the end-to-end response ranking pipeline. In addition, we apply the self-attention to model conversation dependency related to previous utterances. Therefore, our contributions can be summarized as: (1) proposing a extensible framework that incorporates domain information associated to candidate response; (2) applying self-attention layers to improve the modeling of temporal relationship on conversation history; (3) conducting studies on the impact of domain information of candidate responses and self-attention layers to the ranking performance."
        },
        {
            "heading": "2 Related Work",
            "text": "We now briefly review related work to place our contribution in context. First, we review general learning to rank approaches, which we adapt to the conversational setting. Second, we summarize the most recent response rankingmodels as a transition to our model. Next, we review topic modeling and classification in dialogues as it is important and relevant to response ranking in dialogue systems. Last, we review ranking tasks integrating external knowledge."
        },
        {
            "heading": "2.1 Learning to rank",
            "text": "Learning to rank approaches have applications in various fields, such as information retrieval and natural language processing. BM25 [18] and its variants have been widely received as reliable baseline methods. Later, supervised machine learning was adapted to ranking tasks, such as the SVM ranking model proposed by [19]. As neural networks started arising, Ranknet[4] and LambdaMart[23] were a series of improvements based on gradient descent methods. However, these algorithms highly rely on the richness of extracted features, while feature selection methods often compromise semantic meanings."
        },
        {
            "heading": "2.2 Neural response ranking models",
            "text": "The upsurge of Word2Vec[14] and the development of neural network models facilitated learning-to-rank performance, and they are quickly adapted to dialogue response ranking tasks. Variations of Convolutional Neural Networks (CNN) [25], Recurrent Neural Networks (RNN) [12], and the combination of the two [5] have been explored to push the frontline forward. Most recently, the sequential matching network (SMN) [22], deep matching network with external knowledge [27], deep\nattention model [16] and the intent-aware model [26] have achieved state of the art respectively."
        },
        {
            "heading": "2.3 Topic modeling and classification in dialogues",
            "text": "Topic modeling and classification are critically important to understand users\u2019 topics of interest in a conversation, and it is critical to a dialogue system to acquire candidates from knowledge sources based on topic modeling and classification. [11] defined topic trees to use topical information for conversational robustness. Latent Dirichlet Allocation (LDA) was applied by [28] to detect topics in conversational systems. However, when applied to dialogues, unsupervised models can only infer topics from lexical statistics, which are not always consistent with conversation context. Supervised methods such as the supervised LDA by [13] and a Deep Average Networkbased model [8] further improved topic understanding in either text or dialogues. Most recently, [2] proposed an entity-aware topic classification model to facilitate the understanding of topics with entities. After all, the above models missed the link between topics and conversation context."
        },
        {
            "heading": "2.4 Ranking with integration of external knowledge",
            "text": "Integration of external knowledge has a long history in document ranking and retrieval tasks [3, 6, 7]. Various sources of knowledge are utilized to improve the performance of ranking. [9] uses well-constructedWordNet and QA typology to improve performance on a Question-Answerign system. Wikipedia was used as an external knowledge to improve the document clustering tasks by [10]. [24] incorporates entities for document ranking. In this paper, we utilize both knowledge source information associated to candidate responses, and conversation history to perform the response ranking task in a multi-turn conversation."
        },
        {
            "heading": "3 Approach and Implementation",
            "text": "In this section, we, in sequence, define the problem setting (Section 3.1), give an overview of the proposed approach (Section 3.2), explain the framework architecture in detail, and present two specific settings for ablation studies (Section 3.3)."
        },
        {
            "heading": "3.1 Task Formulation",
            "text": "We now formulate the response ranking more formally. Given a dialogue D, at turn \ud835\udc61, there is a conversation history {\ud835\udc620, \ud835\udc621,...,\ud835\udc62\ud835\udc61}, and a given set of response candidates {\ud835\udc500, \ud835\udc501, ..., \ud835\udc50 \ud835\udc57 , ..., \ud835\udc50\ud835\udc58} with their associated domain information {\ud835\udc5d0, \ud835\udc5d1, ..., \ud835\udc5d \ud835\udc57 , ..., \ud835\udc5d\ud835\udc58}, from which they are curated. The instantiated task is to leverage conversation history and candidates\u2019 domain information to make ranking decisions to user utterances."
        },
        {
            "heading": "3.2 Approach Overview",
            "text": "We approach the conversational response ranking problem with a bi-channel endto-end pipeline, to fuse contextual information both from conversation history and candidate responses. First, conversation history interacts with candidate responses and their domain information turn by turn, respectively, to build up interaction representations in each channel. And then, self-attention is applied to each channel to model conversation dependencies. Finally, the output from both channels are concatenated for ranking."
        },
        {
            "heading": "3.3 Model Architecture",
            "text": "This section first introduces representation modules of the framework, including interaction matrix representation, textual feature representation, and latent ranking representation. We then describe the specific implementation integrating domain information from candidate provenance besides conversation history, taking advantage of both contextual information sources. Following that, we give the description of self-attention layers on conversation history. Furthermore, we designate two other\nframework settings for ablation studies. Finally, we elaborate on the generalization of our model as a flexible framework. The initial interactions between the two channels adopt the basic structure of the \ud835\udc37\ud835\udc40\ud835\udc41 model by [27] for the following reasons. First, the interaction matrix in \ud835\udc37\ud835\udc40\ud835\udc41 has its advantage over other text-matching representations [15]. Second, this representation consists of both embedding and hidden state features, which has performed well in the previous state of the art ranking models [26]. Third, the use of CNN to capture high-level n-gram textual features has been proven to be effective. Last, the GRU module can model sequential relationships. Our proposed framework with ablation studies has improvement over the \ud835\udc37\ud835\udc40\ud835\udc41 models and is a fair comparison to their performance.\nInteraction matrix representation. At conversation turn \ud835\udc57 , \ud835\udc62 \ud835\udc57 , \ud835\udc50\ud835\udc58 , or \ud835\udc5d\ud835\udc58 is represented by a sequence of word embeddings \ud835\udc38 \ud835\udc57\ud835\udc62 , \ud835\udc38 \ud835\udc58\ud835\udc50 or \ud835\udc38 \ud835\udc58\ud835\udc5d , and fed into a shared BiGRU to get hidden states, \ud835\udc3b \ud835\udc57\ud835\udc62 , \ud835\udc3b\ud835\udc58\ud835\udc50 , and \ud835\udc3b\ud835\udc58\ud835\udc5d respectively. The embedding interaction matrix between an utterance and a candidate response is calculated by \ud835\udc40\ud835\udc50\ud835\udc52\ud835\udc62\ud835\udc52 = \ud835\udc38\n\ud835\udc57 \ud835\udc62 \u00b7 (\ud835\udc38 \ud835\udc58\ud835\udc50 )\ud835\udc47 . The hidden state interaction matrix is calculated by \ud835\udc40 \ud835\udc50\u210e \ud835\udc62\u210e = \ud835\udc3b \ud835\udc57 \ud835\udc62 \u00b7 (\ud835\udc3b\ud835\udc58\ud835\udc50 )\ud835\udc47 . The same procedure is actualized to have \ud835\udc40 \ud835\udc5d\ud835\udc52\ud835\udc62\ud835\udc52 and \ud835\udc40 \ud835\udc5d\u210e \ud835\udc62\u210e between an utterance and domain information of a candidate response. Textual feature representation. The interaction matrix representation is fed into a CNN layer, obtaining \ud835\udc50\ud835\udc62,\u2217 \ud835\udc57 (* denotes either candidate response \ud835\udc50 or topic information \ud835\udc5d), the n-gram textual feature representation for each turn in the conversation. Latent conversation history representation.We have a GRU or a self-attention layer formodeling conversation history.However, the self-attention layer ismore potent in various tasks [20]. This module \ud835\udc37\ud835\udc40\ud835\udc41\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b is applied to the comprehensive conversation history features \ud835\udc36\u2217\ud835\udc62 = [\ud835\udc50 \ud835\udc62,\u2217 0 , \ud835\udc50 \ud835\udc62,\u2217 1 , ..., \ud835\udc50 \ud835\udc62,\u2217 \ud835\udc61 ]. The hidden states \ud835\udc45\u2217\ud835\udc62 = [\ud835\udc5f \ud835\udc62,\u2217 0 , \ud835\udc5f \ud835\udc62,\u2217 1 , ..., \ud835\udc5f \ud835\udc62,\u2217 \ud835\udc61 ] from the module are concatenated for ranking.\nModel architectures.Here, we fit the representation modules in the \ud835\udc37\ud835\udc40\ud835\udc41\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b model setting, the proposed framework with domain information and GRU layers \ud835\udc39\ud835\udc36\ud835\udc36\ud835\udc3a\ud835\udc45\ud835\udc48 , and that with domain information and attention layers (\ud835\udc39\ud835\udc36\ud835\udc36\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b). All the models are shown in Fig. 2 with different legends.\n\u2022 The \ud835\udc37\ud835\udc40\ud835\udc41\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b model is developed to explore how the self-attention layer affects the ranking performance. This model takes candidate responses and dialogue history as input to obtain interaction matrices \ud835\udc40\ud835\udc50\ud835\udc52\ud835\udc62\ud835\udc52 and \ud835\udc40 \ud835\udc50\u210e \ud835\udc62\u210e . The CNN\nlayer takes in interaction matrices and outputs a textual feature representation \ud835\udc36\ud835\udc50\ud835\udc62 . A self-attention layer is applied to \ud835\udc36\ud835\udc50\ud835\udc62 to acquire a latent conversation history representation. \u2022 The \ud835\udc39\ud835\udc36\ud835\udc36\ud835\udc3a\ud835\udc45\ud835\udc48 model is developed to explore how domain information affects ranking performance. It takes candidate responses, their corresponding domain information, and dialogue history to create interaction matrices \ud835\udc40\ud835\udc50\ud835\udc52\ud835\udc62\ud835\udc52 , \ud835\udc40 \ud835\udc50\u210e \ud835\udc62\u210e , \ud835\udc40 \ud835\udc5d\ud835\udc52 \ud835\udc62\ud835\udc52 ,\nand \ud835\udc40 \ud835\udc5d\u210e\ud835\udc62\u210e . The CNN layers take in interaction matrices and output textual feature representations \ud835\udc36\ud835\udc50\ud835\udc62 and \ud835\udc36 \ud835\udc5d \ud835\udc62 . The GRU layers take textual feature representations and output latent conversation history representation \ud835\udc45\ud835\udc50\ud835\udc62 and \ud835\udc45 \ud835\udc5d \ud835\udc62 .\n\u2022 The \ud835\udc39\ud835\udc36\ud835\udc36\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b, model follows the same flow as the \ud835\udc39\ud835\udc36\ud835\udc36\ud835\udc3a\ud835\udc45\ud835\udc48 model, but instead of GRU layers, two self-attention layers are applied to obtain latent conversation history representations. \u2022 The ranking layer takes in \ud835\udc45\ud835\udc50\ud835\udc62 for the \ud835\udc37\ud835\udc40\ud835\udc41\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b model, and \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61 (\ud835\udc45\ud835\udc50\ud835\udc62 , \ud835\udc45 \ud835\udc5d \ud835\udc62 )\nfor the \ud835\udc39\ud835\udc36\ud835\udc36\ud835\udc3a\ud835\udc45\ud835\udc48 and the \ud835\udc39\ud835\udc36\ud835\udc36\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b models, and outputs a ranking score for each candidate response.\nFramework Generalization. The domain information and previous utterances can be replaced with, and the parallel structure of the framework can further be expanded to channel in, other contextual features, such as outsourced external knowledge, as an integral part of the end-to-end neural ranking pipeline, to enhance the contextual enrichment.\nFramework Summary. In summary, we presented our new framework for conversational response ranking, FCC, which introduces the following new ideas compared to prior work: 1. an introduction of candidate provenance as a new channel to add to conversation history, generating a compact yet comprehensive representation of a dialogue; 2. an implementation of self-attention layers to improve the modeling of multi-turn dependency; 3. our channelized framework easily being expanded to integrate other contextual features in parallel to further enhance contextual enrichment."
        },
        {
            "heading": "4 Experiments",
            "text": "In this section,we describe experiments in three parts. First, we describe the benchmark MSDialog dataset in Section 4.1.Next, we describe experimental procedures in Section 4.2, which include three experiments: 1. A study on the performance of \ud835\udc39\ud835\udc36\ud835\udc36\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b; 2. An ablation study comparing a self-attention layer and a GRU layer to model multi-turn dependency; 3. An ablation study on the effect of domain information of candidates on the ranking performance. Last, we summarize experimental results comparing with the state-of-the-art baselines in Section 4.3."
        },
        {
            "heading": "4.1 Dataset",
            "text": "The MSDialog conversational dataset is collected from the Microsoft products online forum, which discusses issues in a miscellaneous assortment of domains. It includes more than 35,000 conversations and more than 337,000 utterances. We use the subset MSDialog-ResponseRank dataset processed by [16]. In the MSDialogue dataset, candidate responses are extracted from conversations discussing various issues. These issues are summarised in the \"title\" fields in the dataset, which is a fair comparison to domain information of specific components in retrieval-based dialogue systems. Therefore, we take \"title\" fields as our domain information for candidates and this information is reasonably straightforward and easy to get in a dialogue system.\nWe use Matchzoo1 as the data preprocessing tool. Each ranking list, which has one true response and nine candidate responses, is converted to a pair-wise ranking setting. Each true response will be ranked against each candidate response."
        },
        {
            "heading": "4.2 Experimental Setup",
            "text": "We have over 173k samples in the training set, and 37k and 35k in the validation and testing sets. We implement our models using Pytorch 2. For CNN layers, we\n1 https://github.com/NTMC-Community/MatchZoo 2 https://pytorch.org/\nuse two convolution and max-pooling sub-layers with the number of filters [16, 16], convolutional kernels [3, 3], max-pooling kernels [2, 2] and strides [1, 1]. The self-attention layer has two heads and two encoder blocks. We train on the ranking corpus to gain pre-trained embeddings with dimension 200 with the Word2Vec tool [14]. The maximum number of turns in a dialogue is 10. The maximum sequence length for utterance and candidate response is 90 and 30 for the domain information sequence. The batch size is 50. We tune all parameters by the validation dataset."
        },
        {
            "heading": "4.3 Model evaluation",
            "text": "In this section, we first report the performance of \ud835\udc39\ud835\udc36\ud835\udc36\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b, comparing with the state-of-the-art baselines in response ranking and response selection fields. And then, we show the results of ablation studies on the impact of domain information and self-attention layers. Experiment results are reported in Table 1.\nMain results.We evaluate \ud835\udc39\ud835\udc36\ud835\udc36\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b, on \ud835\udc4510@1, \ud835\udc4510@2, \ud835\udc4510@5, and \ud835\udc40\ud835\udc34\ud835\udc43. The results show that \ud835\udc39\ud835\udc36\ud835\udc36\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b, has an improvement on all four metrics over the state of the art \ud835\udc3c \ud835\udc34\ud835\udc45\ud835\udc47 models, especially on R10@1, R10@2, and MAP, which all have significance p-value < 0.05. The performance on recall@1 has the most significant 7.7% improvement, which is most important since a dialogue system usually picks the best candidate to return to a user.\nThe ablation study on domain information. To study the impact of domain information compared with the \ud835\udc37\ud835\udc40\ud835\udc41 models, we evaluate \ud835\udc39\ud835\udc36\ud835\udc36\ud835\udc3a\ud835\udc45\ud835\udc48 on the same metrics. The results show that with an extra channel to integrate domain information from candidates to the \ud835\udc37\ud835\udc40\ud835\udc41 architecture, the ranking performance improves significantly, with margins between 2.2% to 38.9% corresponding to different metrics. The ranking performance not only surpasses the \ud835\udc37\ud835\udc40\ud835\udc41 models but has significant improvements on recall@1 and MAP over \ud835\udc3c \ud835\udc34\ud835\udc45\ud835\udc47 models, with margins of 5.0% and 2.2%. This comparison confirms the positive effect of domain information from the candidates. The domain information provides\nThe ablation study on self-attention layers.We conduct an ablation study on the effect of self-attention layers over conversation history. The \ud835\udc37\ud835\udc40\ud835\udc41\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b model has improvement over the DMN-PRF model with margins ranging from 1.6% to 10.4%. The \ud835\udc39\ud835\udc36\ud835\udc36\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b model surpasses the performance of the \ud835\udc39\ud835\udc36\ud835\udc36\ud835\udc3a\ud835\udc45\ud835\udc48 model with improvement ranging from 1.0% to 2.4%. From the results, it is clear that the self-attention layer impacts positively on the ranking performance. Furthermore, we analyze the non-optimal rate (percentage of cases in which the true response is not ranked first) as shown in Fig. 3, to explore the effectiveness of the self-attention layer conditioning on conversation history length. It is demonstrated that the non-optimal rate drops from about 30% to 20% as the length of conversation history increases until a sudden surge on conversations with 10 and 11 (maximum length) turns. It is reasonable to conjecture that when the conversation only has a few turns, such as 2 or 3, the model is not fed with enough contextual information to make an optimal decision. While in the opposite case, the model isn\u2019t sophisticated enough\nto isolate effective information from over-long conversations. The self-attention layers are most effective on conversations with 4 to 9 turns."
        },
        {
            "heading": "5 Discussion and Conclusion",
            "text": "In this paper, we proposed a flexible framework (FCC) capable of incorporating miscellaneous contextual resources for response ranking in multi-turn dialogue systems. To validate the framework, we implemented embedding domain information of candidates with self-attention layers to improve the relevance modeling between utterances and candidate responses. Specifically, the domain information adds a second source to interact with utterances, a mechanism to either confirm or alleviate the semantic matching just between conversation history and candidates. One of the examples as a demonstration here: \u2013Utterance: ...message telling me I am not on the internet while I am ... \u2013Candidate 1: You will be ...running a trouble shooter... to fix some common issues with Window Update. (Domain Info: Adobe Flash Player in Edge and IE is not updating from vulnerability.) \u2013Candidate 2: Let\u2019s try running ... trouble shooter to help resolve app issues from the Windows Store. (Domain Info: Internet Issues.) The trained \ud835\udc37\ud835\udc40\ud835\udc41\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b model ranked Candidate 1 first, without knowing the domain information. However, FCC models successfully ranked Candidate 2 first since the domain knowledge directly points to the intention of the user. This example clearly supports our claim that domain knowledge from the source of candidates enhances the effectiveness of a response ranking model. Our overall result supports our claim as well, by outperforming existing stateof-the-art models, with ablation studies to show that both domain information of candidates and self-attention layers lead to critical increments in the performance respectively and conjunctively. In the future, we would like to investigate on a diversified stream of contextual information feeding into and expanding our framework, and develop hierarchical semantic representations for multi-turn conversations to enrich the information input to improve the capability of modeling longer conversation history."
        }
    ],
    "title": "FCC: Fusing Conversation History and Candidate Provenance for Contextual Response Ranking in Dialogue Systems",
    "year": 2023
}