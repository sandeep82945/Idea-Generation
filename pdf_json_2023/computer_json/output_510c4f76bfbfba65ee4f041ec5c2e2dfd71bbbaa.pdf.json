{
    "abstractText": "As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there\u2019s still a dark cloud lingering overhead \u2013 will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs. To achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs\u2014a facet neglected by previous methods. (2) Knowledge Distortion: Altering parameters with the aim of editing factual knowledge can irrevocably warp the innate knowledge structure of LLMs. Experimental results vividly demonstrate that knowledge editing might inadvertently cast a shadow of unintended consequences on LLMs, which warrant attention and efforts for future works1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhoubo Li"
        },
        {
            "affiliations": [],
            "name": "Ningyu Zhang"
        },
        {
            "affiliations": [],
            "name": "Yunzhi Yao"
        },
        {
            "affiliations": [],
            "name": "Mengru Wang"
        },
        {
            "affiliations": [],
            "name": "Xi Chen"
        },
        {
            "affiliations": [],
            "name": "Huajun Chen"
        }
    ],
    "id": "SP:4b808824f11a0bd10197b4065b80ab41f6c9adc6",
    "references": [
        {
            "authors": [
                "Lukas Berglund",
                "Meg Tong",
                "Max Kaufmann",
                "Mikita Balesni",
                "Asa Cooper Stickland",
                "Tomasz Korbak",
                "Owain Evans"
            ],
            "title": "The reversal curse: Llms trained on \u201da is b\u201d fail to learn \u201db is a",
            "venue": "CoRR, abs/2309.12288,",
            "year": 2023
        },
        {
            "authors": [
                "Nicola De Cao",
                "Wilker Aziz",
                "Ivan Titov"
            ],
            "title": "Editing factual knowledge in language models",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Jin Chen",
                "Zheng Liu",
                "Xu Huang",
                "Chenwang Wu",
                "Qi Liu",
                "Gangwei Jiang",
                "Yuanhao Pu",
                "Yuxuan Lei",
                "Xiaolong Chen",
                "Xingmei Wang",
                "Defu Lian",
                "Enhong Chen"
            ],
            "title": "When large language models meet personalization: Perspectives of challenges and opportunities",
            "venue": "CoRR, abs/2307.16376,",
            "year": 2023
        },
        {
            "authors": [
                "Yuheng Chen",
                "Pengfei Cao",
                "Yubo Chen",
                "Kang Liu",
                "Jun Zhao"
            ],
            "title": "Journey to the center of the knowledge neurons: Discoveries of language-independent knowledge neurons and degenerate knowledge neurons",
            "venue": "CoRR, abs/2308.13198,",
            "year": 2023
        },
        {
            "authors": [
                "Siyuan Cheng",
                "Ningyu Zhang",
                "Bozhong Tian",
                "Zelin Dai",
                "Feiyu Xiong",
                "Wei Guo",
                "Huajun Chen"
            ],
            "title": "Editing language model-based knowledge graph embeddings",
            "venue": "CoRR, abs/2301.10405,",
            "year": 2023
        },
        {
            "authors": [
                "Roi Cohen",
                "Eden Biran",
                "Ori Yoran",
                "Amir Globerson",
                "Mor Geva"
            ],
            "title": "Evaluating the ripple effects of knowledge editing in language models",
            "venue": "CoRR, abs/2307.12976,",
            "year": 2023
        },
        {
            "authors": [
                "Damai Dai",
                "Li Dong",
                "Yaru Hao",
                "Zhifang Sui",
                "Baobao Chang",
                "Furu Wei"
            ],
            "title": "Knowledge neurons in pretrained transformers",
            "venue": "Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Damai Dai",
                "Yifan Song",
                "Jingjing Xu",
                "Zhifang Sui",
                "Lei Li"
            ],
            "title": "Calibrating factual knowledge in pretrained language models. In Findings of the Association for Computational Linguistics: EMNLP 2022",
            "venue": "Abu Dhabi, United Arab Emirates,",
            "year": 2022
        },
        {
            "authors": [
                "Mor Geva",
                "Roei Schuster",
                "Jonathan Berant",
                "Omer Levy"
            ],
            "title": "Transformer feed-forward layers are key-value memories",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Mor Geva",
                "Jasmijn Bastings",
                "Katja Filippova",
                "Amir Globerson"
            ],
            "title": "Dissecting recall of factual associations in auto-regressive language models",
            "venue": "CoRR, abs/2304.14767,",
            "year": 2023
        },
        {
            "authors": [
                "Anshita Gupta",
                "Debanjan Mondal",
                "Akshay Krishna Sheshadri",
                "Wenlong Zhao",
                "Xiang Lorraine Li",
                "Sarah Wiegreffe",
                "Niket Tandon"
            ],
            "title": "Editing commonsense knowledge in GPT",
            "venue": "CoRR, abs/2305.14956,",
            "year": 2023
        },
        {
            "authors": [
                "Thomas Hartvigsen",
                "Swami Sankaranarayanan",
                "Hamid Palangi",
                "Yoon Kim",
                "Marzyeh Ghassemi"
            ],
            "title": "Aging with GRACE: lifelong model editing with discrete key-value adaptors",
            "venue": "CoRR, abs/2211.11031,",
            "year": 2022
        },
        {
            "authors": [
                "Peter Hase",
                "Mohit Bansal",
                "Been Kim",
                "Asma Ghandeharioun"
            ],
            "title": "Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models",
            "venue": "CoRR, abs/2301.04213,",
            "year": 2023
        },
        {
            "authors": [
                "Evan Hernandez",
                "Belinda Z. Li",
                "Jacob Andreas"
            ],
            "title": "Measuring and manipulating knowledge representations in language models",
            "venue": "CoRR, abs/2304.00740,",
            "year": 2023
        },
        {
            "authors": [
                "Xanh Ho",
                "Anh-Khoa Duong Nguyen",
                "Saku Sugawara",
                "Akiko Aizawa"
            ],
            "title": "Constructing A multihop QA dataset for comprehensive evaluation of reasoning steps",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Jason Hoelscher-Obermaier",
                "Julia Persson",
                "Esben Kran",
                "Ioannis Konstas",
                "Fazl Barez"
            ],
            "title": "Detecting edit failures in large language models: An improved specificity benchmark",
            "venue": "Association for Computational Linguistics,",
            "year": 2023
        },
        {
            "authors": [
                "Ari Holtzman",
                "Peter West",
                "Luke Zettlemoyer"
            ],
            "title": "Generative models as a complex systems science: How can we make sense of large language model behavior",
            "venue": "CoRR, abs/2308.00189,",
            "year": 2023
        },
        {
            "authors": [
                "Zeyu Huang",
                "Yikang Shen",
                "Xiaofeng Zhang",
                "Jie Zhou",
                "Wenge Rong",
                "Zhang Xiong"
            ],
            "title": "Transformer-patcher: One mistake worth one neuron",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Zeyu Huang",
                "Yikang Shen",
                "Xiaofeng Zhang",
                "Jie Zhou",
                "Wenge Rong",
                "Zhang Xiong"
            ],
            "title": "Transformer-patcher: One mistake worth one neuron",
            "year": 2023
        },
        {
            "authors": [
                "Gabriel Ilharco",
                "Marco T\u00falio Ribeiro",
                "Mitchell Wortsman",
                "Ludwig Schmidt",
                "Hannaneh Hajishirzi",
                "Ali Farhadi"
            ],
            "title": "Editing models with task arithmetic",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Yoichi Ishibashi",
                "Hidetoshi Shimodaira"
            ],
            "title": "Knowledge sanitization of large language models",
            "venue": "CoRR, abs/2309.11852,",
            "year": 2023
        },
        {
            "authors": [
                "Ziwei Ji",
                "Nayeon Lee",
                "Rita Frieske",
                "Tiezheng Yu",
                "Dan Su",
                "Yan Xu",
                "Etsuko Ishii",
                "Yejin Bang",
                "Andrea Madotto",
                "Pascale Fung"
            ],
            "title": "Survey of hallucination in natural language generation",
            "venue": "ACM Comput. Surv.,",
            "year": 2023
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Xiaopeng Li",
                "Shasha Li",
                "Shezheng Song",
                "Jing Yang",
                "Jun Ma",
                "Jie Yu"
            ],
            "title": "PMET: precise model editing in a transformer",
            "venue": "CoRR, abs/2308.08742,",
            "year": 2023
        },
        {
            "authors": [
                "Edward Lorenz"
            ],
            "title": "The butterfly effect",
            "venue": "World Scientific Series on Nonlinear Science Series A,",
            "year": 2000
        },
        {
            "authors": [
                "Kyle Mahowald",
                "Anna A. Ivanova",
                "Idan Asher Blank",
                "Nancy Kanwisher",
                "Joshua B. Tenenbaum",
                "Evelina Fedorenko"
            ],
            "title": "Dissociating language and thought in large language models: a cognitive perspective",
            "venue": "CoRR, abs/2301.06627,",
            "year": 2023
        },
        {
            "authors": [
                "Kevin Meng",
                "Arnab Sen Sharma",
                "Alex J. Andonian",
                "Yonatan Belinkov",
                "David Bau"
            ],
            "title": "Massediting memory in a transformer",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Eric Mitchell",
                "Charles Lin",
                "Antoine Bosselut",
                "Chelsea Finn",
                "Christopher D. Manning"
            ],
            "title": "Fast model editing at scale",
            "venue": "In The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Eric Mitchell",
                "Charles Lin",
                "Antoine Bosselut",
                "Christopher D. Manning",
                "Chelsea Finn"
            ],
            "title": "Memorybased model editing at scale",
            "venue": "International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Yasumasa Onoe",
                "Michael J.Q. Zhang",
                "Shankar Padmanabhan",
                "Greg Durrett",
                "Eunsol Choi"
            ],
            "title": "Can lms learn new entities from descriptions? challenges in propagating injected knowledge",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Patrick S.H. Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander H. Miller"
            ],
            "title": "Language models as knowledge bases",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing,",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Vipula Rawte",
                "Amit P. Sheth",
                "Amitava Das"
            ],
            "title": "A survey of hallucination in large foundation models",
            "venue": "CoRR, abs/2309.05922,",
            "year": 2023
        },
        {
            "authors": [
                "Denny Vrandecic",
                "Markus Kr\u00f6tzsch"
            ],
            "title": "Wikidata: a free collaborative knowledgebase",
            "venue": "Commun. ACM,",
            "year": 2014
        },
        {
            "authors": [
                "Ben Wang",
                "Aran Komatsuzaki"
            ],
            "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax",
            "year": 2021
        },
        {
            "authors": [
                "Jiaan Wang",
                "Yunlong Liang",
                "Zengkui Sun",
                "Yuxuan Cao",
                "Jiarong Xu"
            ],
            "title": "Cross-lingual knowledge editing in large language models",
            "venue": "CoRR, abs/2309.08952,",
            "year": 2023
        },
        {
            "authors": [
                "Peng Wang",
                "Ningyu Zhang",
                "Xin Xie",
                "Yunzhi Yao",
                "Bozhong Tian",
                "Mengru Wang",
                "Zekun Xi",
                "Siyuan Cheng",
                "Kangwei Liu",
                "Guozhou Zheng",
                "Huajun Chen"
            ],
            "title": "Easyedit: An easy-to-use knowledge editing framework for large language models",
            "venue": "CoRR, abs/2308.07269,",
            "year": 2023
        },
        {
            "authors": [
                "Xiaozhi Wang",
                "Kaiyue Wen",
                "Zhengyan Zhang",
                "Lei Hou",
                "Zhiyuan Liu",
                "Juanzi Li"
            ],
            "title": "Finding skill neurons in pre-trained transformer-based language models",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Suhang Wu",
                "Minlong Peng",
                "Yue Chen",
                "Jinsong Su",
                "Mingming Sun"
            ],
            "title": "Eva-kellm: A new benchmark for evaluating knowledge editing of llms",
            "venue": "CoRR, abs/2308.09954,",
            "year": 2023
        },
        {
            "authors": [
                "Yang Xu",
                "Yutai Hou",
                "Wanxiang Che",
                "Min Zhang"
            ],
            "title": "Language anisotropic cross-lingual model editing",
            "venue": "Association for Computational Linguistics,",
            "year": 2023
        },
        {
            "authors": [
                "Yunzhi Yao",
                "Peng Wang",
                "Bozhong Tian",
                "Siyuan Cheng",
                "Zhoubo Li",
                "Shumin Deng",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "title": "Editing large language models: Problems, methods, and opportunities",
            "venue": "CoRR, abs/2305.13172,",
            "year": 2023
        },
        {
            "authors": [
                "Yue Zhang",
                "Yafu Li",
                "Leyang Cui",
                "Deng Cai",
                "Lemao Liu",
                "Tingchen Fu",
                "Xinting Huang",
                "Enbo Zhao",
                "Yu Zhang",
                "Yulong Chen",
                "Longyue Wang",
                "Anh Tuan Luu",
                "Wei Bi",
                "Freda Shi",
                "Shuming Shi"
            ],
            "title": "Siren\u2019s song in the AI ocean: A survey on hallucination in large language models",
            "venue": "CoRR, abs/2309.01219,",
            "year": 2023
        },
        {
            "authors": [
                "Ce Zheng",
                "Lei Li",
                "Qingxiu Dong",
                "Yuxuan Fan",
                "Zhiyong Wu",
                "Jingjing Xu",
                "Baobao Chang"
            ],
            "title": "Can we edit factual knowledge by in-context learning",
            "venue": "CoRR, abs/2305.12740,",
            "year": 2023
        },
        {
            "authors": [
                "Zexuan Zhong",
                "Zhengxuan Wu",
                "Christopher D. Manning",
                "Christopher Potts",
                "Danqi Chen"
            ],
            "title": "Mquake: Assessing knowledge editing in language models via multi-hop questions",
            "venue": "CoRR, abs/2305.14795,",
            "year": 2023
        },
        {
            "authors": [
                "MEND Mitchell"
            ],
            "title": "2022a) develops an efficient method for locally editing language models using just a single input-output pair. Essentially, MEND employs a technique to manipulate the gradient of fine-tuned language models which leverages a low-rank decomposition of gradients. Here we adopt the COUNTERFACT (Meng et al., 2022) to evaluate the locality while training MEND",
            "venue": "For convenience,",
            "year": 2022
        },
        {
            "authors": [
                "Meng"
            ],
            "title": "2022), conceptualizes the MLP module as a straightforward key-value store",
            "venue": "ROME ROME,",
            "year": 2022
        },
        {
            "authors": [
                "MEMIT MEMIT (Meng"
            ],
            "title": "2023) builds upon ROME to insert many memories by modifying the MLP weights of a range of critical layers. We test the ability of MEMIT using their source code and all hyper-parameters follow the same default settings",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "1 INTRODUCTION\nDespite their impressive abilities, Large Language Models (LLMs) such as ChatGPT are unaware of events occurring after their training phase and may inadvertently generate harmful or offensive content. To address this concern, the concept of knowledge editing for LLMs has been proposed (Cao et al., 2021; Dai et al., 2022; Mitchell et al., 2022a;b; Meng et al., 2022; 2023; Ilharco et al., 2023; Onoe et al., 2023; Zhong et al., 2023; Yao et al., 2023), which provides an efficient way to change the behavior of LLMs without resorting to an exhaustive retraining or continuous training procedure. While existing knowledge editing approaches for LLMs have demonstrated impressive results, an ancient Chinese poetic wisdom resonates a dark cloud lingering overhead: A single hair can move the whole body. The poetic phrase underscores the notion that even minor alterations to LLMs may lead to significant outcomes, akin to the butterfly effect observed in chaos theory (Lorenz, 2000). This raises a new critical research question: does knowledge editing for LLMs introduce irreversible unforeseen side effects?\n\u2217Corresponding author. 1Code is available at https://github.com/zjunlp/PitfallsKnowledgeEditing.\nar X\niv :2\n31 0.\n02 12\n9v 2\n[ cs\n.C L\n] 2\n1 N\nov 2\n02 3\nIn Figure 1, we depict two types of side effects caused by knowledge editing: Knowledge Conflict and Knowledge Distortion. Using pilot experiments as an illustration, as depicted in Figure 2 (a) top, applying one single edit (i) Marie\u2019s husband is Pierre \u2192 Jacques to modify a fact within LLMs is straightforward using previous knowledge editing techniques. However, as the number of edits grows, these methods might encounter problems, especially when edits exhibit potential correlation. For example, consider a subsequent edit (ii) Jacques\u2019s wife is Marie \u2192 Maurice (REVERSE EDIT), both edit (i) and (ii) individually try to achieve their intended goals, but the model might still respond with Marie when prompted with Jacques is the husband of . This situation exemplifies the Knowledge Conflict issue. Similarly, as shown in the following composite setting (Figure 2 (a) bottom), two edits can incur a conflict due to a common logical rule (COMPOSITE EDIT), thereby leading to an inconsistency of knowledge in the model.\nFurther, as shown in Figure 2 (b) top, we conduct a ROUND-EDIT experiment, which means we try to restore an edited LLM (with edit (i)) via another subsequent edit ((ii)). Unfortunately, we obverse that the model after ROUND-EDIT has significantly reduced the capability to provide Scranton and America when prompting with Where was Joe Biden born? indicating that the implicit knowledge structure in LLMs has been disrupted. This situation exemplifies the Knowledge Distortion issue, which illustrates that knowledge editing may have an irreversible damage to LLMs.\nInspired by these observations, we pioneer the investigation to uncover the potential pitfalls of knowledge editing and categorize these pitfalls into two major types: knowledge conflict and knowledge distortion. To analyze the extent of these pitfalls, we create benchmark datasets tailored to prevailing methods. Specifically, we construct a dataset named CONFLICTEDIT, comprising pairs of edits that could cause knowledge conflict, and introduce a set of new metrics designed to quantify the magnitude of knowledge conflicts. We empirically observe that previous knowledge editing approaches all suffer from the knowledge conflict issue. Then, to explore the knowledge distortion issue, we create the ROUNDEDIT dataset and conduct ROUND-EDIT experiments using various knowledge editing techniques. Our findings illustrate that previous knowledge editing approaches can indeed lead to adverse consequences on the implicit knowledge structure of LLMs. We further design a simple-yet-effect method called Multi-label Edit (MLE) that combines multiple correct labels of the edit to a single process, as depicted in Figure 2 (b) bottom, which is capable to alleviate knowledge distortion and restore similar behavior compared with the original model."
        },
        {
            "heading": "2 EXPLORING THE PITFALLS OF KNOWLEDGE EDITING FOR LLMS",
            "text": ""
        },
        {
            "heading": "2.1 OVERVIEW",
            "text": "In this section, we briefly introduce the definition of knowledge editing and the common evaluation paradigms in previous works. Next, we outline our evaluation criterion and delve into the specifics of our analysis for knowledge editing. The representative methods used are also summarized here.\nDefinition of Knowledge Editing for LLMs With daily updating of factual knowledge, LLMs always struggle to keep the parameters up-to-date. Knowledge editing releases the model from expensive retraining through precisely updating several outdated factual pieces of knowledge. Suppose a factual knowledge as a triplet (s, r, o), an edit e = (s, r, o \u2192 o\u2217) modifies the object from o to o\u2217 for given subject s and relation r. After applying this edit to a language model f\u03b8 (where \u03b8 denotes the model\u2019s parameters), there is a knowledge update applied to the model, that is{\nko = (s, r, o) kn = (s, r, o \u2217)\n(1)\nwhere ko is the old knowledge and kn is the new one. Generally, we witness an update from ko to kn through the variation of their generation probabilities.\nVanilla Evaluation The current evaluation criterion primarily focuses on various samples around an isolated edit e = (s, r, o \u2192 o\u2217). The post-edit model f\u03b8\u2032 (edited model) is expected to respond o\u2217 to related samples I(e) (excluding e) while still keeping the original output to unrelated samples O(e). Commonly, instances sampling in the neighbor of e are used to build I. Meng et al. (2022) samples O from a unrelated set {(s\u2032, r, o); s\u2032 \u0338= s}, which restricts the actuating scope of e. The Reliability metric evaluates results on the fact (s, r, o\u2217), the Generalization and Locality metric evaluate results on I and O respectively. These metrics effectively constrain certain edits to an expected actuating scope (Meng et al., 2022; Wang et al., 2023b; Yao et al., 2023). In our experiments, for each edit e, we compute the results by averaging the performance of the label over the edit itself e and the related samples I(e).\nMotivation and Evaluation Principle Since LLMs can be regarded as large knowledge bases (Petroni et al., 2019), knowledge editing is expected to handle thousands of edits while maintaining knowledge coherence and integrity. In previous works, multiple edits are applied either through sequential editing (Huang et al., 2023b) or mass editing (Meng et al., 2023) settings preventing the model from obvious performance drop. Unfortunately, current isolated evaluations fail to consider the interaction between the accumulating edits which could lead to subtle side effects of the model\u2019s knowledge. Hence, we start exploring the pitfalls of knowledge editing for LLMs from typical edits and focus on two new issues, namely, Knowledge Conflict (\u00a72.2) and Knowledge Distortion (\u00a72.3). Intuitively, The knowledge conflict setting can examine how two edits may interact or contradict each other, resulting in misinformation. On the other hand, The knowledge distortion setting can analyze potentially irreversible damage to knowledge structure with mass edits. Rather than isolated edits, these settings can comprehensively evaluate the ability of knowledge editing methods to process numerous edits without degradation, contradiction, or loss of unrelated knowledge.\nEditing Methods In the following Sections, we build benchmarks CONFLICTEDIT and ROUNDEDIT and conduct experiments to evaluate knowledge editing approaches as follows (we do not consider approaches that preserve parameters in LLMs):\n\u2022 Fine-tuning (FT) updates the model\u2019s parameters by gradient descent in a certain layer through Adam and early stop strategy to maximize the probability of the editing target.\n\u2022 MEND Mitchell et al. (2022a) utilizes a hypernetwork, which gives a low-rank update to the original fine-tuning gradients based on the edit knowledge.\n\u2022 ROME Meng et al. (2022) uses causal tracing to locate the key layer associated with the edit knowledge, and then impose a update to the MLP module.\n\u2022 MEMIT Meng et al. (2023) follows the locating methods in ROME and is capable of updating multiple layers at one time when editing massive knowledge."
        },
        {
            "heading": "2.2 KNOWLEDGE CONFLICT ANALYSIS",
            "text": "Note that a robust knowledge editing method should be supposed to deal with multiple updates of a specific fact. However, with the increasing extent of edits, there is a possibility that interference occurs between different edits, causing the former edit invalid. Particularly, when the edits are logically connected, it\u2019s challenging yet crucial to nullify previous modifications; failing to do so can lead to inconsistencies in the post-edited model. To this end, we define a notion of Knowledge Conflict and attempt to explore when and why the conflicts occur, and how to handle them."
        },
        {
            "heading": "2.2.1 PROBLEM DEFINITION",
            "text": "Knowledge Conflict Consider the scenario depicted in Figure 2(a) top, where two consecutive edits, e1:Marie\u2019s husband is Pierre \u2192 Jacques and e2:Jacques\u2019s wife is Marie \u2192 Maurice, are applied to a language model. These two edits both modify the fact (Jacques, HusbandOf, ?). Ideally, a reliable and robust edit method should retain only the latest change. However, the current editing method may inadvertently retain knowledge from previous edits, leading to a logical inconsistency, resulting in the knowledge conflict issue. There are many factors that can contribute to the emergence of knowledge conflicts and here we consider two major scenarios: REVERSE EDIT and COMPOSITE EDIT.\nReverse Edit This scenario causes conflict by editing the facts with reverse relations. We formalize this situation as a pair of consecutive edits that contain reverse relations, that is\nReverse Edit : { e1 = (s1, r1, o1 \u2192 o2) e2 = (o2, r2, s1 \u2192 s2)\n(2)\nwhere r1 and r2 are reverse relations, such as HusbandOf and WifeOf in the example. Take the case in Figure 2(a) top as an example, the reverse edit makes two updates to the model, but they simultaneously change the fact (Jacques, HusbandOf, ?). Hence, the change can be represented as the following: {\nko = (s1, r1, o2) kn = (s2, r1, o2) (3)\nwhere we can instantiate these two facts as ko:Marie\u2019s husband is Jacques edited by e1 and kn:Maurice\u2019s husband is Jacques edited by e2 in the example above. We focus on this kind of fact pair which may lead to confusion when we ask related question \u2018Jacques is the husband of who? \u2019 to the post-edit model.\nComposite Edit Furthermore, we explore a more complex situation, where the edits are associated with a fact that will not be influenced by editing. The example illustrated in Figure 2(a) bottom exhibits this situation that we edit e1:Hamlet was written in English \u2192 French and then e2:Shakespeare wrote in French \u2192 German while preserving a tied fact kf :The notable work of Shakespeare is Hamlet, and we also formalize it as\nComposite Edit :  kf = (s1, r, s2)\ne1 = (s1, r1, o1 \u2192 o2) e2 = (s2, r2, o2 \u2192 o3)\n(4)\nwhere r \u2227 r1 \u2192 r2 is a logical rule, for example NotableWork\u2227WrittenIn\u2192Language. e1 and e2 will both edit the fact (Hamlet,WrittenIn,?). Thus, after executing the edits, the confusing knowledge update can be represented as{\nko = (s1, r1, o2) kn = (s1, r1, o3) (5)\nAfter knowledge editing, we expect the post-edit model to answer German to the question What language was Halmet written in?"
        },
        {
            "heading": "2.2.2 EVALUATION",
            "text": "To analyze the Knowledge Conflict issue, we design a new dataset CONFLICTEDIT to evaluate the performance of existing knowledge editing methods.\nSetup We construct our CONFLICTEDIT dataset from WikiData (Vrandecic & Kro\u0308tzsch, 2014), which consist of two types of edits, namely REVERSE EDIT and COMPOSITE EDIT, as defined in Equation 2 and Equation 4. To begin with, we follow Ho et al. (2020) to obtain several reverse and composite logical rules in WikiData. We construct CONFLICTEDIT by sampling thousands of data of REVERSE EDIT and COMPOSITE EDIT edits respectively. In the COMPOSITE EDIT setup, we respectively utilize GPT2-XL (1.5B) (Radford et al., 2019) and GPT-J (6B) (Wang & Komatsuzaki, 2021) to confirm the correctness of the tied fact kf before experiments. Also, we use GPT-4 (OpenAI, 2023) to generate neighbor prompts for every relation (Dataset construction details are in Appendix A.1.1). Besides, we take SINGLE and COVERAGE EDIT as references. SINGLE EDIT refers to editing the latest knowledge update directly, which evaluates the model\u2019s ability to directly modify the knowledge (s, r, o1 \u2192 o3). COVERAGE EDIT is based on a pair of direct edits sharing the common (s, r), that is\nCoverage Edit : { e1 = (s, r, o1 \u2192 o2) e2 = (s, r, o2 \u2192 o3)\n(6)\nwhich focuses on a covered knowledge update, referring to the editing target of the last edit e2. We evaluate the result under SINGLE EDIT e2 and COVERAGE EDIT pair (e1, e2) as references.\nMetrics We design a new metric Conflict Score (CS), which weighs how well a knowledge editing method handles the knowledge conflict issue. We measure CS by calculating the ratio that the new fact kn is more possible than the old fact ko after knowledge editing, that is\nCS = I(pf \u03b8 \u2032 (kn) > pf \u03b8 \u2032 (ko)) (7)\npf \u03b8 \u2032 (k) is the probability of emitting the target object in k by probing the the prompts of (s, r). Further, we utilize two modes, Explicit (CSexp) and Implicit (CSimp), to calculate pf \u03b8 \u2032 (kn) (Details are in Appendix B). As stated above, the old fact ko is the reason causing Knowledge Conflict, we design the metric Conflict Magnitude (CM) to estimate the decrease of the probability of ko:\nCM = pf\u03b8m (ko)\u2212 pf\u03b8\u2032 (ko)\npf\u03b8m (ko) (8)\nwhere \u03b8m is the intermediate model parameters after edit e1. For the convenience of analyzing the experimental results and comparing the difference of evaluation paradigm, we take Success Score (Succ) as a metric revealing the basic editing performance, that is\nSucc = I(pf \u03b8 \u2032 (o | (s, r)) > pf\u03b8 (o | (s, r))). (9)\nIn the COMPOSITE EDIT setup, the tied fact kf is also significant to compose the knowledge conflict, thus we consider the probability change of kf as Tied Fact Damage (TFD), calculated similarly to Equation 8 where we substitute the ko with kf ."
        },
        {
            "heading": "2.2.3 RESULTS ANALYSIS",
            "text": "We conduct experiments on GPT2-XL and GPT-J and report the results in Table 1. In this part, we report the results of SINGLE, COVERAGE EDIT, REVERSE EDIT and COMPOSITE EDIT settings, and further discuss a unified view to expound the internal reason for the results.\nSingle & Coverage Edit Note that for the COVERAGE EDIT setting, knowledge editing techniques are tasked with not only incorporating new knowledge but also editing existing information to ensure knowledge consistency. The CS scores derived from COVERAGE EDIT can reveal the Reliability and Generalization performance subsequent to implementing a pair of edits. Notably, the results might be influenced by the initial edit within the pair, setting it apart from the Single Succ scores. Empirically, we notice that FT achieves higher scores in GPT-J but lower scores in GPT2-XL because of its disadvantage in generalization referring to the previous observation in Mitchell et al. (2022a). ROME is effective in both GPT2-XL and GPT-J, showing its comparable ability to edit knowledge. MEND and MEMIT obtain lower scores, especially in the CM metric, which indicates the failure of knowledge editing in this case.\nReverse Edit FT and MEND utilize Prompt(s1, r1, o1 \u2192 o2) as input, and update model parameters by gradient descent, which is more effective in detecting the update of reverse edit. Thus FT and MEND obtain higher scores on CM which indicates the vanish of the old knowledge ko through the latest editing. ROME and MEMIT take the subject as a key to attach the edit to all the involved prompts Prompt(s1, r1), thus applying the reverse edit in totally distinct ways. The CS scores indicate a total failure of ROME and MEMIT in this setup, which may be caused by the poor performance on reverse relation reasoning discovered by Berglund et al. (2023). When evaluating the CM metric, both MEND and MEMIT retain the original knowledge within the model, posing a significant challenge to maintaining knowledge consistency in LLMs.\nComposite Edit We observe that both FT and MEND demonstrate superior performance in this setup. In contrast, ROME and MEMIT show promising results specifically in the editing process, but the edited facts remain disconnected from other information. Further, we notice that most of previous knowledge editing methods will trigger damage on tied facts with very high TFD score, Note that for the COMPOSITE EDIT setting, knowledge editing approaches are anticipated to modify facts while also taking into account the associations of knowledge already learned within the LLMs.\nA Unified View We analyze three types of knowledge editing scopes based on Mitchell et al. (2022b)\u2019s Editing Scope concept to reveal their differences in input space editing. In Figure 3 (a), the COVERAGE EDIT setup presents a binary input space choice with completely overlapping editing scopes. Figure 3 (b) shows that the REVERSE EDIT edits are linked through reverse facts within their scope but completely overlap in logical implication. In Figure 3 (c), COMPOSITE EDIT involves two edits with non-overlapping scopes but establishes a logical chain through an existing tied fact, ensuring logical consistency. This motivates the employment of detection technologies for the target knowledge, grounded in the symbolic logical rules of KGs, to circumvent potential knowledge discrepancies."
        },
        {
            "heading": "2.3 KNOWLEDGE DISTORTION ANALYSIS",
            "text": "Despite the capability of current knowledge editing methods to update single factual knowledge, Knowledge Conflict reflects the drawbacks of current knowledge editing approaches facing multiple edits. We further analyze whether current knowledge editing approaches will damage the implicit knowledge structure within LLMs or not. Here, we specify the concept of Knowledge Distortion and explore the mechanism of how knowledge editing works and impacts LLMs."
        },
        {
            "heading": "2.3.1 PROBLEM DEFINITION",
            "text": "Knowledge Distortion As stated in Meng et al. (2022), knowledge editing for LLMs can change the distribution of the next token, thus editing the model\u2019s output. Intuitively, fine-tuning on unbalanced data causes preference of certain generations, thus applying one edit at a time seems an extremely unbalanced update to the model. Particularly, in the multi-label scene, for a given relation r, there is a set of objects Obj = {oi; i = 1, 2, 3, 4, ...} that correspond to the subject and relation pair (s, r). When we perform an edit (s, r, o\u2217 \u2192 o1) (where o\u2217 \u0338\u2208 Obj), the distribution of nexttoken probabilities over Obj tends to favor o1. However, weakening the preference for generating other correct objects in (s, r, oi); i = 2, 3, 4, . . . is undesirable for a robust editing method. Unfortunately, most editing methods exhibit a negative impact on the correct objects beyond the target, that is, the knowledge structure for the (s, r) will be distorted.\nRound-Edit Note that it is not easy to analyze Knowledge Distortion through a single edit, since the distribution of the original model over labels in Obj is absent to be referred. Therefore, we design a ROUND-EDIT setting which is\nRound-Edit : { e1 = (s, r, o1 \u2192 o\u2217) e2 = (s, r, o \u2217 \u2192 o1) (10)\nwhere o1 is the target object, and o\u2217 is the intermediate object. After applying the ROUND-EDIT, we expect that the post-edit model be consistent with the original model or even perform better."
        },
        {
            "heading": "2.3.2 EVALUATION",
            "text": "Setup To investigate the performance and mechanism of knowledge distortion caused by knowledge editing, we construct a dataset ROUNDEDIT from WikiData, which contains EASY and HARD splits. First of all, we collect several relations that possibly match multiple objects and then filter the subjects. Then we select one object in the object list of a multi-labeled (s, r) as the target object of ROUND-EDIT. Considering the mechanism that ROME entails other semantically related tokens together with the target object to the subject (Geva et al., 2023), we introduce two settings EASY and HARD (Dataset construction details are in Appendix A.1.2). As illustrated in Figure 4 (a), the edits in EASY tie an object to a certain subject by migrating a range of similar semantic objects, while HARD edits object that semantically distant from the true labels of (s, r) as depicted in Figure 4 (b). If other labels fall within the Migration Range, the post-edit model not only shows a predilection for the target object but also reinforces its confidence in these other labels. If not, they are overlooked.\nMetrics Inspired by the teacher forcing generation in Meng et al. (2022), we define three new metrics to measure the Knowledge Distortion of the edited model. Distortion (D) estimates the JS divergence of the distribution on objects in Obj before and after ROUND-EDITING, that is:\nD = JS(pf\u03b8 (Obj | (s, r)), pf\u03b8\u2032 (Obj | (s, r))), (11) where D denotes the Distortion, p(Obj | \u00b7) is a normalized probability distribution over Obj. This metric measures the extent of knowledge distortion. Moreover, inspired by Cohen et al. (2023), we introduce a more specific metric called Ignore Rate (IR). The IR metric quantifies the extent to which the objects in Obj (excluding the target object o1) are disregarded or overlooked following the process of knowledge editing. IR is calculated as follows:\nIR = \u2211\no\u2208Obj o\u0338=o1\nI(pf\u03b8 (o | (s, r)) > pf\u03b8\u2032 (o | (s, r))), (12)\nwhere p(o | (s, r)) is the generative probability of o. Furthermore, we design Failure Rate (FR) metric to count the ratio of the case where their IR > 0.5, that is\nFR = I(IR > 0.5). (13)\nTo make a reference to the basic performance on the dataset, we also calculate the Success Score (Succ) as same as \u00a72.2, which computes the average Succ of the edits in ROUND-EDIT."
        },
        {
            "heading": "2.3.3 RESULTS ANALYSIS",
            "text": "We conduct experiments on GPT2-XL and GPT-J and then summarize the results in Table 2. The variance of the distribution on Obj is illustrated in Figure 5 over EASY and HARD settings. Based on the results, we design an effective method Multi-Label Edit (MLE) to address this problem.\nAnalysis As depicted in Table 2, a notable knowledge distortion is observed in FT and MEND, as evidenced by their high IR and FR values. ROME and MEMIT, while still exhibiting some level of distortion, demonstrate lower values compared to FT and MEND. This indicates that ROME and MEMIT have integrated unique mechanisms to minimize disturbances to the implicit knowledge structure in LLMs during knowledge editing. Further, we observe an interesting phenomenon from the EASY and HARD scenarios: when the Succ metric reaches high values, FT and MEND not only underperform but also demonstrate minimal variation across the two datasets; In contrast, ROME and MEMIT not only demonstrate an advantage in absolute values but also show a big gap in IR and FR between EASY and HARD settings. This leads to the conclusion that the editing approaches of ROME and MEMIT effectively achieve scope transfer (better generalization ability) through semantic relations. Moreover, as shown in Figure 5, we illustrate an example of |Obj| = 5 in EASY and HARD respectively to display the inner change of the distribution over Obj. We observe a clear knowledge distortion emerges which inadvertently impacts the LLMs\u2019 implicit knowledge structure, particularly when the structure encapsulates a diverse semantic spectrum (e.g., HARD setting).\nA Simple Solution: Multi-Label Edit (MLE) To be noted, we aim to evaluate how knowledge distortion affects the generation performance of post-edited models via its impact on IR and FR metrics. The results in Figure 2 indicate that even ROME and MEMIT exhibit poor performance in the EASY scenario, affecting over 50% of the true labels and cases. To this end, we introduce a simple solution: Multi-Label Edit (MLE), tailored for such multi-label scenarios. Specifically, as shown in Figure 2, we take three objects (one-to-many triples taken from existing KG which is included in our datasets) as the editing target, expecting to preserve the performance over the true labels (More details in Appendix C). From the results in Figure 2, it is evident that MLE can help mitigate the influence of knowledge distortion, which is also demonstrated by cases in Figure 5."
        },
        {
            "heading": "3 RELATED WORK",
            "text": "Recently, there has been a surge in knowledge analysis and editing for LLMs. This encompasses efforts to demystify the knowledge storage mechanisms inherent in these \u201cblack-box\u201d neural networks (Geva et al., 2021; Dai et al., 2022). Researchers have delved into deconstructing the Transformer architecture by exploring the concept of knowledge/skill neurons (Dai et al., 2022; Chen et al., 2023b; Wang et al., 2022), and complex system science (Holtzman et al., 2023). Additionally, there\u2019s a keen interest in developing effective strategies to edit knowledge within LLMs (Wu et al., 2023; Xu et al., 2023; Hase et al., 2023; Cheng et al., 2023; Hernandez et al., 2023; Gupta et al., 2023; Zhong et al., 2023; Hartvigsen et al., 2022; Cohen et al., 2023; Hoelscher-Obermaier et al., 2023; Li et al., 2023; Wang et al., 2023a). One kind of knowledge editing approaches aim to modify the parameters of LLMs in an effort to rectify model parameters that produce undesirable outputs (Meng et al., 2022; Mitchell et al., 2022a; Meng et al., 2023). Conversely, there are approaches that leave the parameters of LLMs untouched (Mitchell et al., 2022b; Dong et al., 2022; Zheng et al., 2023; Hartvigsen et al., 2022; Huang et al., 2023a), by melding an auxiliary network with the pristine, unmodified model."
        },
        {
            "heading": "4 DISCUSSION AND CONCLUSION",
            "text": "The side effect of Knowledge Conflict for LLMs This paper illustrates that knowledge editing can bring in side effects of knowledge conflict, leading to knowledge inconsistency and may even enhance the hallucination of LLMs (Ji et al., 2023; Rawte et al., 2023; Zhang et al., 2023). We argue that conflict detection via logical rules or KG reasoning may avoid knowledge conflict. However, the side effect of knowledge editing for LLMs is far from satisfactory, which needs efforts for future works by inspiring from other domains (e.g., Cognitive Science (Mahowald et al., 2023))\nUpdating and Recovering Knowledge in LLMs Knowledge editing technologies can serve as a scaffold to update, sanitize, or personalize LLMs (Ishibashi & Shimodaira, 2023; Chen et al., 2023a). Note that we cannot prevent some individuals from intentionally or unintentionally making erroneous knowledge edits, leading us to restore the LLMs. Thus, empirical observation of knowledge distortion in this paper brings about a severe issue to updating and recovering knowledge in LLMs. Although we introduce Multi-Label Edit to mitigate knowledge distortion, more work should be developed to better understand the knowledge learning and updating mechanism in LLMs.\nETHICAL CONSIDERATIONS\nThe datasets and models presented in this paper are designed for exploratory analysis of LLMs. It is crucial to note that data from WikiData isn\u2019t always error-free, and GPT-4 might also introduce biases. As a result, when constructing datasets of knowledge editing for LLMs, there is a risk of surfacing knowledge infused with offensive language or prejudicial content. During our experiments, we have carefully reviewed all data, ensuring the removal of any toxic or offensive content.\nREPRODUCIBILITY STATEMENT\nCode is available in https://github.com/zjunlp/PitfallsKnowledgeEditing. We provide technical details of dataset construction including CONFLICTEDIT and ROUNDEDIT in Appendix A.1.1 and Appendix A.1.2, respectively. We provide detailed experimental settings in Appendix B. We provide more technical details of Multi-Label Edit (MLE) in Appendix C."
        },
        {
            "heading": "A DATASETS DETAILS",
            "text": "A.1 CONSTRUCTION OF DATASETS\nA.1.1 CONFLICTEDIT\nWe construct our CONFLICTEDIT dataset based on WikiData (Vrandecic & Kro\u0308tzsch, 2014), using the raw data and some logical rules mined from it. From the original WikiData data to the CONFLICTEDIT dataset, we have taken the following steps:\nStep 1 First, we determine the binary and ternary logic rules we need using the method of Ho et al. (2020) and manually select rules/relations, thereby obtaining the candidate sets of relations in REVERSE EDIT and COMPOSITE EDIT, respectively.\nStep 2 Next, based on the nominated relations and corresponding logical rules, we construct WikiData Queries and use the WikiData Query Service to mine an average of 50,000 factual combinations that meet the requirements for each logical rule.\nStep 3 We sample from each factual combination present in the logical rules, excluding those with descriptions in numeric formats or those with identical entities. To ensure accuracy, we validate the integrity of the associated fact while constructing the COMPOSITE EDIT dataset for each model (e.g., GPT-XL, GPT-J). Specifically, we use the prompt associated with the fact as an input, prompting the model to generate outputs of a predetermined length using the Top-1 next-token generation approach. We then verify whether these outputs include the object of the fact.\nStep 4 Finally, we select 2,500 data points each from binary and ternary factual combinations. However, due to GPT-2 XL\u2019s comparatively weaker performance in verifying the accuracy of tied facts against GPT-J, we only select 2,000 data points for it. We then sample edited target entities under each relation to assemble the final REVERSE EDIT and COMPOSITE EDIT datasets. Additionally, using samples from both datasets, we create the COVERAGE EDIT dataset to serve as an experimental benchmark.\nStep 5 We\u2019ve manually crafted editing prompts for each relation, drawing inspiration from their descriptions on WikiData. Based on this, we employ GPT-4 (OpenAI, 2023) to generate 20 alternative phrasings for each relation\u2019s prompt. From these, we handpick the 10 most appropriate prompts for our generalization evaluation.\nFor training MEND, samples with locality facts are essential. Prioritizing training efficacy, we opt not to utilize the NQ dataset, as done by ZsRE. Instead, our sampling is anchored in COUNTERFACT (Meng et al., 2022). Table 3 illustrates the data structure we\u2019ve established for both REVERSE EDIT and COMPOSITE EDIT, also highlighting a specific example from COMPOSITE EDIT.\nA.1.2 ROUNDEDIT\nWe construct ROUNDEDIT dataset using data sourced from WikiData, enriched by some 1-to-n relations mined from it. Transitioning from the raw WikiData data to the ROUNDEDIT dataset required several stages:\nStep 1 We first manually select some 1-to-n relations and construct WikiData Queries, utilizing the WikiData Query Service to mine 5000 (s, r) pairs along with the correct object set Obj. From each relation, we selectively sample, ensuring that the \u2019n\u2019 value in the 1-n relations remains below 10. Furthermore, we exclude samples with descriptions in a numeric format.\nStep 2 For each LLM, we perform next-token generation tests on the raw data samples, filtering these samples based on generation probabilities. To delineate the EASY and HARD dataset categories, we examine the correlation between semantic relevance and generation probabilities. Using this analysis, we measure the semantic relationship among the correct labels and subsequently sample to establish both the EASY and HARD categories, each comprising 2,500 data points.\nStep 3 Similarly, we manually create editing prompts for every relation, drawing from their respective descriptions on WikiData. Based on this, we utilize GPT-4 (OpenAI, 2023) to generate 20 rephrased prompts. Of these, we handpick the top 10 prompts, incorporating them into the generalization evaluation.\nWe also sample locality facts from COUNTERFACT (Meng et al., 2022) to train the MEND. Figure 4 showcases selected examples from the EASY and HARD settings.\nB IMPLEMENTING DETAILS\nFT For basic Fine-Tuning (FT), we follow Meng et al. (2022) to re-implement their study, which uses Adam (Kingma & Ba, 2014) with early stopping to minimize \u2212logPG\u2032 [o\u2217 | p], changing only mlpproj weights at selected layer 1 in GPT2-XL and layer 21 in GPT-J. For both models, all hyper-parameters follow default settings. To ensure fairness in the experiments, we always use the unconstrained fine-tuning approach.\nMEND Mitchell et al. (2022a) develops an efficient method for locally editing language models using just a single input-output pair. Essentially, MEND employs a technique to manipulate the gradient of fine-tuned language models which leverages a low-rank decomposition of gradients. Here we adopt the COUNTERFACT (Meng et al., 2022) to evaluate the locality while training MEND. For convenience, we train MEND respectively on our CONFLICTEDIT and ROUNDEDIT datasets using EasyEdit (Wang et al., 2023b). The hyper-parameters follow default settings both on GPT2-XL and GPT-J.\nROME ROME, as proposed by Meng et al. (2022), conceptualizes the MLP module as a straightforward key-value store. For instance, if the key represents a subject and the value encapsulates\nknowledge about that subject, then the MLP can reestablish the association by retrieving the value that corresponds to the key. In order to add a new key-value pair, ROME applies a rank-one modification to the weights of the MLP, effectively injecting the new information directly. This method allows for more precise and direct modification of the model\u2019s knowledge through knowledge locating. We directly apply the code and MLP weight provided by the original paper and keep the default setting for hyper-parameters.\nMEMIT MEMIT (Meng et al., 2023) builds upon ROME to insert many memories by modifying the MLP weights of a range of critical layers. We test the ability of MEMIT using their source code and all hyper-parameters follow the same default settings.\nExplicit (CSexp) and Implicit (CSimp) Here, \u2019Explicit\u2019 and \u2019Implicit\u2019 are in reference to the second edit. When computing pf\n\u03b8 \u2032 (kn), we can utilize the edit target (s2, r2, o\u22172) from the second edit\ne2 : (s2, r2, o2 \u2192 o\u22172) as the input for calculating kn, which we term the Explicit mode. Conversely, we employ a more relevant latent target associated with the first edit as the input for computing kn, and this mode is referred to as the Implicit mode. Table 5 lists the edit tatget of each dataset split in every mode."
        },
        {
            "heading": "C DETAILS OF MULTI-LABEL EDIT",
            "text": "To address the issue of Knowledge Distortion in knowledge editing, we introduce a novel approach termed Multi-Label Editing (MLE). This method diverges from conventional knowledge editing. Our objective is to simultaneously edit groups of knowledge that possess the same (s, r) into the model, leveraging MEMIT\u2019s multi-editing capabilities during the process. For instance, when we make an edit Joe Biden was born in California \u2192 Florida, we can consider the differences in the knowledge structures of California and Florida, recall their related semantics within the model (such as America), and then regard \u2019Florida\u2019 and \u2019America\u2019 as labels utilizing MLE to edit.\nOur findings indicate that such a concurrent editing approach not only prevents unnecessary knowledge updates but also ensures the model doesn\u2019t favor a particular label. Through rigorous experimental results, we demonstrate that MLE serves as an effective strategy to mitigate knowledge distortion issues. By identifying and incorporating related or essential knowledge within the model prior to the editing of a specific piece of knowledge, we can reduce the potential disruption to the LLM\u2019s inherent knowledge structure caused by the editing process."
        }
    ],
    "year": 2023
}