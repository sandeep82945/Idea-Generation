{
    "abstractText": "The emergent ability of Large Language Models to use a small number of examples to learn to perform in novel domains and tasks, also called in-context learning (ICL). In this work, we show that a much smaller model can be trained to perform ICL by fine-tuning towards a specialized training objective, exemplified on the task of domain adaptation for neural machine translation. With this capacity for ICL, the model can take advantage of relevant fewshot examples to adapt its output towards the domain. We compare the quality of this domain adaptation to traditional supervised techniques and ICL with a 40B-parameter Large Language Model. Our approach allows efficient batch inference on a mix of domains and outperforms state-of-the-art baselines in terms of both translation quality and immediate adaptation rate, i.e. the ability to reproduce a specific term after being shown a single example.",
    "authors": [
        {
            "affiliations": [],
            "name": "Raphael Reinauer"
        },
        {
            "affiliations": [],
            "name": "Patrick Simianer"
        },
        {
            "affiliations": [],
            "name": "Johannes E. M. Mosig"
        },
        {
            "affiliations": [],
            "name": "Joern Wuebker"
        }
    ],
    "id": "SP:86801057713c21c1750883a5d8178d617d448afa",
    "references": [
        {
            "authors": [
                "Roee Aharoni",
                "Yoav Goldberg."
            ],
            "title": "Unsupervised domain clusters in pretrained language models",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Ankur Bapna",
                "Naveen Arivazhagan",
                "Orhan Firat."
            ],
            "title": "Simple, scalable adaptation for neural machine translation",
            "venue": "CoRR, abs/1909.08478.",
            "year": 2019
        },
        {
            "authors": [
                "Ankur Bapna",
                "Orhan Firat."
            ],
            "title": "Simple, scalable adaptation for neural machine translation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
            "year": 2019
        },
        {
            "authors": [
                "Rachel Bawden",
                "Fran\u00e7ois Yvon."
            ],
            "title": "Investigating the translation performance of a large multilingual language model: the case of BLOOM",
            "venue": "Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pages 157\u2013170,",
            "year": 2023
        },
        {
            "authors": [
                "segments. Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Bram Bulte",
                "Arda Tezcan."
            ],
            "title": "Neural fuzzy repair: Integrating fuzzy matches into neural machine translation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1800\u20131809, Florence, Italy. Association for",
            "year": 2019
        },
        {
            "authors": [
                "Vince Emery",
                "Karl Kadie",
                "Mary Laplante."
            ],
            "title": "Multilingual Marketing Content: Growing International Business with Global Content Value Chains",
            "venue": "Outsell.",
            "year": 2011
        },
        {
            "authors": [
                "Xavier Garcia",
                "Yamini Bansal",
                "Colin Cherry",
                "George Foster",
                "Maxim Krikun",
                "Melvin Johnson",
                "Orhan Firat."
            ],
            "title": "The unreasonable effectiveness of fewshot learning for machine translation",
            "venue": "Proceedings of the 40th International Conference on Machine",
            "year": 2023
        },
        {
            "authors": [
                "Amr Hendy",
                "Mohamed Abdelrehim",
                "Amr Sharaf",
                "Vikas Raunak",
                "Mohamed Gabr",
                "Hitokazu Matsushita",
                "Young Jin Kim",
                "Mohamed Afify",
                "Hany Hassan Awadalla"
            ],
            "title": "How good are GPT models at machine translation? A comprehensive evaluation",
            "year": 2023
        },
        {
            "authors": [
                "Diederik Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "International Conference on Learning Representations.",
            "year": 2014
        },
        {
            "authors": [
                "Nagata",
                "Toshiaki Nakazawa",
                "Michal Nov\u00e1k",
                "Martin Popel",
                "Maja Popovi\u0107."
            ],
            "title": "Findings of the 2022 conference on machine translation (WMT22)",
            "venue": "Proceedings of the Seventh Conference on Machine Translation (WMT), pages 1\u201345, Abu Dhabi, United",
            "year": 2022
        },
        {
            "authors": [
                "Philipp Koehn."
            ],
            "title": "Europarl: A parallel corpus for statistical machine translation",
            "venue": "Proceedings of machine translation summit x: papers, pages 79\u201386.",
            "year": 2005
        },
        {
            "authors": [
                "Oleksii Kuchaiev",
                "Jason Li",
                "Huyen Nguyen",
                "Oleksii Hrinchuk",
                "Ryan Leary",
                "Boris Ginsburg",
                "Samuel Kriman",
                "Stanislav Beliaev",
                "Vitaly Lavrukhin",
                "Jack Cook",
                "Patrice Castonguay",
                "Mariya Popova",
                "Jocelyn Huang",
                "Jonathan M. Cohen"
            ],
            "title": "Nemo: a toolkit",
            "year": 2019
        },
        {
            "authors": [
                "Jessy Lin",
                "Geza Kovacs",
                "Aditya Shastry",
                "Joern Wuebker",
                "John DeNero."
            ],
            "title": "Automatic correction of human translations",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
            "year": 2022
        },
        {
            "authors": [
                "Yu A Malkov",
                "Dmitry"
            ],
            "title": "A Yashunin",
            "year": 2020
        },
        {
            "authors": [
                "Yasmin Moslem",
                "Rejwanul Haque",
                "John D. Kelleher",
                "Andy Way."
            ],
            "title": "Adaptive machine translation with large language models",
            "venue": "Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pages 227\u2013237, Tampere,",
            "year": 2023
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "BLEU: A method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Guilherme Penedo",
                "Quentin Malartic",
                "Daniel Hesslow",
                "Ruxandra Cojocaru",
                "Alessandro Cappelli",
                "Hamza Alobeidli",
                "Baptiste Pannier",
                "Ebtesam Almazrouei",
                "Julien Launay"
            ],
            "title": "The RefinedWeb dataset for Falcon LLM: Outperforming curated corpora",
            "year": 2023
        },
        {
            "authors": [
                "M. Pham",
                "Jitao Xu",
                "Josep Maria Crego",
                "Fran\u00e7ois Yvon",
                "Jean Senellart."
            ],
            "title": "Priming neural machine translation",
            "venue": "Conference on Machine Translation.",
            "year": 2020
        },
        {
            "authors": [
                "Matt Post."
            ],
            "title": "A call for clarity in reporting BLEU scores",
            "venue": "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186\u2013 191, Brussels, Belgium. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Ricardo Rei",
                "Jos\u00e9 G.C. de Souza",
                "Duarte Alves",
                "Chrysoula Zerva",
                "Ana C Farinha",
                "Taisiya Glushkova",
                "Alon Lavie",
                "Luisa Coheur",
                "Andr\u00e9 F.T. Martins."
            ],
            "title": "COMET-22: Unbabel-IST 2022 submission for the metrics shared task",
            "venue": "Proceedings of the",
            "year": 2022
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Making monolingual sentence embeddings multilingual using knowledge distillation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, pages 5998\u20136008.",
            "year": 2017
        },
        {
            "authors": [
                "David Vilar",
                "Markus Freitag",
                "Colin Cherry",
                "Jiaming Luo",
                "Viresh Ratnakar",
                "George Foster."
            ],
            "title": "Prompting PaLM for translation: Assessing strategies and performance",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Lin-",
            "year": 2023
        },
        {
            "authors": [
                "Jitao Xu",
                "Josep Crego",
                "Jean Senellart."
            ],
            "title": "Boosting neural machine translation with similar translations",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1580\u20131590, Online. Association for Computational",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large Language Models (LLMs) have demonstrated few-shot learning capabilities on various natural language processing tasks, as highlighted by Brown et al. (2020) or Garcia et al. (2023). When prompted with suitable example translations, they can compete with neural machine translation (NMT) models, built and trained specifically for translating between languages (Vilar et al., 2023). Interestingly, one can adapt LLMs to specific domains merely by adding example translations to their prompt at inference time (Moslem et al., 2023). This ability to adapt to specific tasks and domains is known as in-context learning (ICL). In contrast to traditional fine-tuning methods, ICL does not require a separate set of customized parameters for each domain, which implies major efficiency gains through batched inference.\n\u2020 Equal contribution.\nIn this paper, we integrate ICL for domain adaptation into NMT systems in multiple steps. We compare our method for adapting NMT systems to traditional fine-tuning approaches, as well as to the domain adaptation abilities of an open-source LLM. Specifically, our main contributions are the following:\n1. We evaluate an unmodified NMT system\u2019s ICL capacity for domain adaptation and demonstrate its limitations.\n2. We propose a training scheme to improve an NMT model\u2019s ICL capability.\n3. We show that ICL can be combined with more traditional adaptation methods to further improve domain adaptation performance.\n4. We compare our method to the performance of the open-source LLM FALCON-40B (Penedo et al., 2023) on a machine translation task with ICL for domain adaptation."
        },
        {
            "heading": "2 Related Work",
            "text": "Bulte and Tezcan (2019) improve the translation performance of an NMT model by integrating translation fuzzy-matched pairs from a translation memory as input to an NMT model. This idea was further expanded by Pham et al. (2020) and Xu et al. (2020), who for a given source segment use sentence embeddings to retrieve similar examples and compared different schemes for integrating those as cues into the NMT network.\nOur approach differs in that we only train on the tokens belonging to the translation and not on the tokens provided as context, which we show to work better. In addition, Pham et al. (2020)\u2019s training procedure differs, as they train their model from scratch, using training data from multiple domains and evaluate on those same domains, while we train on general domain data and evaluate on a new domain that is not in the training data. Furthermore,\nar X\niv :2\n30 9.\n08 59\n0v 1\n[ cs\n.C L\n] 1\n5 Se\np 20\n23\nwe focus on the multi-domain adaptation task using light-weight adapters. This approach not only allows us to extend to new domains without retraining the full model, but also offers a more practical and efficient strategy for real-world applications.\nThe authors of (Moslem et al., 2023) investigated the capabilities of a proprietary LLM, specifically GPT-3.5, for adaptive machine translation using ICL. Their extensive experiments showed that GPT-3.5 can adapt well to in-domain sentence pairs and/or terminology."
        },
        {
            "heading": "3 Experiments",
            "text": "We conduct a series of experiments to develop NMT systems that exceed at few-shot ICL domain adaptation. Here we present the experiments in a logical order, where we start with the baseline models described in Section 3.1 and subsequently introduce several stages of development. In stages 0 and 1 we attempt ICL with the unmodified and domain-fine-tuned baseline models, respectively. Then, in STAGE 2, we fine-tune the baseline model to the task of domain ICL, instead of a particular domain. Finally, we combine ICL and domain adaptation through fine-tuning in STAGE 3. Our experimental progression was guided by the metrics and datasets that we introduce in Sections 3.5 and 3.6, respectively."
        },
        {
            "heading": "3.1 Models",
            "text": "Throughout this paper, we work with an NMT system and the FALCON-40B LLM, which we both describe here."
        },
        {
            "heading": "3.1.1 FALCON LLM",
            "text": "To provide a direct comparison with LLMs and their capacity for ICL, we conduct experiments with the decoder-only Transformer language model FALCON-40B (Penedo et al., 2023), specifically the non-instruction-tuned variant1. Inference is done with greedy decoding. Following previous work (Bawden and Yvon, 2023; Garcia et al., 2023; Hendy et al., 2023) (inter-alia) the model is prompted to perform translation without specific fine-tuning towards the machine translation task.\nA simple prompt template is used for all k-shot experiments with FALCON-40B, see Figure 1.\nIn preliminary experiments we found that k = 0\n1The model is available from the huggingface platform: https://huggingface.co/tiiuae/falcon-40b\ndoes not work well with this specific model2 \u2013 the outputs tend to be entirely hallucinated."
        },
        {
            "heading": "3.1.2 NMT Systems",
            "text": "The baseline model that we use as the starting point for all further experiments is a Transformer (Vaswani et al., 2017) model with 12 encoder layers and two decoder layers, implemented with the NVIDIA NeMo toolkit (Kuchaiev et al., 2019). The embedding size is 1,024 with a feed-forward network dimension of 4,096. The model has a joint vocabulary of 32,768 tokens, while embedding matrices are specific to the encoder, decoder, and output projection modules, i.e. parameters are not shared between them. The model was trained to support a maximum input size of 1,536 tokens by augmenting the training data with random concatenations of parallel sentences. We evaluate the model using greedy decoding.\nFor the experiments presented here, the baseline model is either fine-tuned in full (STAGE 2A and STAGE 2B), or light-weight adapters (Bapna and Firat, 2019) are added to the model (STAGE 1 and STAGE 3). We choose full-model fine-tuning on out-of-domain data to adapt the NMT model to a new task \u2013 translating with an increased context of related examples \u2013 and adapter layers for learning from in-domain data.\nThe adapters we use follow Bapna et al. (2019)\u2019s formulation, but with layer normalization applied after the bottleneck rather than before it. We use a bottleneck width of 256 and insert adapters in every layer of the decoder and every other layer of the encoder.\nWe always fine-tune with the ADAM optimizer (Kingma and Ba, 2014) and early stopping based on validation loss."
        },
        {
            "heading": "3.2 STAGE 0 & STAGE 1: ICL with a Standard NMT Model",
            "text": "Motivated by the few-shot learning capabilities of LLMs, we examine the ability of a standard English-to-German NMT model to adapt to a domain given only similar and relevant translation\n2For k = 0 the prompt contains only the single source sentence as input and the target language followed by a colon.\npairs as additional context, i.e., without changing the model\u2019s parameters.\nTo find similar source segments in the translation memory, we search for nearest neighbours in an embedding space. We use the multi-lingual sentence embedding model3 from the sentence transformer library (Reimers and Gurevych, 2020) to embed the source sides of all segment pairs. Then we employ hnswlib (Malkov and Yashunin, 2020) to find the approximate nearest neighbours: Each source sentence in the domain-specific datasets is first encoded with the sentence-embedding model and then added to an index. For the sake of simplicity in this paper, we will refer to the approximate nearest neighbors simply as nearest neighbors. To measure the similarity between a pair of segments s and s\u2032, we use the cosine distance of the corresponding embedding vectors vs and vs\u2032, i.e.,\nd(s, s\u2032) := 1\u2212 vs \u00b7 vs \u2032\n\u2225vs\u22252\u00b7\u2225vs\u2032\u22252 .\nFor a given source s and target segment t, we identify its nearest neighbours s1, s2, ..., sk, using the the cosine distance above. Each source sentence si is paired with a reference translation ti for i = 1, ..., k. We sort the pairs by their distance to s in the embedding space, i.e.,\nd(s, s1) \u2264 d(s, s2) \u2264 ... \u2264 d(s, sk) .\nOur assumption is that similar segments should have similar translations. For STAGE 0 of the experiments, we treat the context sentences and actual source text as one body of text, separated only by a single space, ordering the segments from least similar to most similar, with the current source segment s at the end. As a result, the input of the encoder is\n<bos> sk sk\u22121 ... s1 s <eos>\nwhile for the decoder, we use the prefix:\n<bos> tk tk\u22121 ... t1\nwhere <bos> and <eos> represent the beginning-ofsentence and end-of-sentence tokens, respectively. The model\u2019s task is then to continue from the target prefix by generating a translation of the source segment s.\nIn our experiments, we evaluated the translation performance using a varying number k of nearest neighbors, specifically k \u2208 {1, 2, 5}.\n3Model name on https://www.sbert.net/: all-MiniLM-L6-v2\nIn STAGE 1 we run additional experiments where we fine-tune the model for each domain, using the in-domain training data in the original format. This domain-specific fine-tuning is performed by injecting adapter layers (Bapna and Firat, 2019) into the network while freezing the rest of the model, and leveraging a standard negative log-likelihood (NLL) loss for training. For each domain, we then test the fine-tuned model directly (0-shot in Tables 3 and 4) as well as with ICL (k-shot with k \u0338= 0).\nAdapters are trained towards convergence, i.e. until there is no further improvement in terms of validation loss."
        },
        {
            "heading": "3.3 STAGE 2A & STAGE 2B: Fine-Tuning towards ICL",
            "text": "To improve the model\u2019s capability to use nearest neighbor examples in the context, we further finetune the full model on out-of-domain data, namely News-Commentary4 (Kocmi et al., 2022), which contains roughly 450K parallel segments. For validation we use a sample of 2K parallel segments from EuroParl5 (Koehn, 2005). For this full model fine-tuning we do not train until convergence, but apply aggressive early stopping: Training is stopped when the validation loss does not decrease by at least 0.1 twice in a row, validating for every 1% of an epoch. This is to encourage the model to only learn the new task and data format, but not adapt to a new data distribution.\nInstead of directly concatenating the nearest neighbors to the training examples, we add a special separation token \u2013 <sep> \u2013 to separate the source and target segments. We then construct the training instances for the encoder as:\n<bos> sk <sep> sk\u22121 <sep> ... <sep> s1 <sep> s <eos>\nand for the decoder as:\n<bos> tk <sep> tk\u22121 <sep> ... <sep> t1 <sep> t <eos> (1)\nand compute the NLL loss on all tokens of (1). This training loss is identical to the one used in Pham et al. (2020). We denote this procedure as STAGE 2A.\nFor STAGE 2B the idea is that the model should learn to predict the target segment from the source\n4From the WMT\u201923 evaluation campaign: https://data. statmt.org/news-commentary/v18.1/\n5Also from the WMT\u201923 evaluation campaign: https: //www.statmt.org/europarl/v10/\nsegment using the nearest neighbor translations but not learn to predict tk, ..., t1 as in (Pham et al., 2020). Hence we mask the NLL training loss such that it is computed only on the tokens that belong to the target segment t, excluding all context tokens, thus fully focusing the training signal on translating t in the context of its k nearest neighbors.\nWe then use the same format as in STAGE 2A for training, while at inference time we provide the decoder with a prefix containing the ICL examples:\n<bos> tk <sep> tk\u22121 <sep> ... <sep> t1 <sep>\nFinally, we measure quality of the predicted translation t\u0302 by computing BLEU and COMET scores with the target segment t as reference.\nFor both STAGE 2A and STAGE 2B, the k-nearest neighbors for each segment in the training data and validation data are extracted from the entire NewsCommentary dataset as described in Section 3.2."
        },
        {
            "heading": "3.4 STAGE 3: Combining ICL and Domain Adaptation",
            "text": "To combine STAGE 2B\u2019s ICL capacity with adapterbased domain adaptation, we add adapters to the model from STAGE 2B using the same configuration as for the STAGE 1 experiments. Again, we train separate adapter layers for each domain.\nEach example from the training set is annotated with its nearest neighbors from the same training set, excluding itself."
        },
        {
            "heading": "3.5 Metrics",
            "text": "For evaluating translation quality, we used the SacreBLEU framework (Post, 2018) that implements the BLEU metric (Papineni et al., 2002). We also evaluate with reference-based COMET (Rei et al., 2022) to compare the model outputs to the reference translations in the test data."
        },
        {
            "heading": "3.6 Datasets",
            "text": "We run our experiments with the English-German language pair on 8 domains from the ACED- and MDNS corpus collections, which we describe in this section. Statistics for all datasets are provided in Table 1."
        },
        {
            "heading": "3.6.1 ACED corpus",
            "text": "The ACED corpus (Lin et al., 2022) is comprised of three distinct datasets, namely Asics, Emerson, and Digitalocean, each consisting of English-German sentences extracted from various domains. ACED is a real-world benchmark containing data derived from translations performed by humans."
        },
        {
            "heading": "3.6.2 MDNS corpus",
            "text": "The MDNS corpus (Aharoni and Goldberg, 2020) is a multi-domain corpus containing EnglishGerman parallel text from five diverse domains (IT, Koran, Law, Medical, Subtitles). It was specifically created for evaluating domain-adaptation."
        },
        {
            "heading": "4 Results",
            "text": "Here we discuss the experimental results, progressing from STAGE 0 to STAGE 3. All results are depicted separately for ACED- and MDNS corpora in Tables 3 and 4 respectively."
        },
        {
            "heading": "4.1 STAGE 0: ICL with Baseline NMT Model",
            "text": "When we add nearest neighbors to the inputs and target prefixes we first observe that the automated metrics are mostly improved across all datasets. Notably, the result with 1-shot nearest neighbors is the best in this group of experiments. Additionally we find that the 5-shot result often degrades below the baseline.\nSpecifically for the Medical and Subtitles corpora of MDNS, we find that the model fails to improve over the baseline for all k.\nThe cosine distance of the nearest neighbors seems to be a viable indicator of performance in this set of experiments, e.g. when comparing the results for ACED Emerson & Digitalocean, where the average cosine distance (see Table 2) for k = 1 is much lower for Emerson at 0.13, versus 0.3 for Digitalocean. We find a moderate, statistically insignificant, negative Pearson correlation (r = \u22120.43) between the average cosine distances for k = 1 and the difference in BLEU scores between the STAGE 0 1-shot experiment and the baseline.\nWhile BLEU indicates improvement (COMET reduces only for k > 1), we find that the model\u2019s behavior is in fact degenerate. Specifically, the model often fails to produce any output after the given prefix and instead predicts <eos> immediately, which leads to empty translations. We find that the rates of empty translations are 8.5%, 8.1%, and 9.1% for k = 1, 2, and 5 respectively. In contrast, the baseline system has a 0% rate of empty outputs. This is despite the model being specifically trained to support inputs covering the full context-width in pre-training."
        },
        {
            "heading": "4.2 STAGE 1: Combining ICL with Domain Fine-Tuning",
            "text": "For STAGE 1 we first observe that the model can be effectively adapted to each domain by training adapters (see the STAGE 1, 0-shot results in Tables 3 and 4). A notable exception is MDNS Subtitles, where adaptation only slightly improves over the baseline. This result is, however, consistent with other work (Aharoni and Goldberg, 2020).\nWhen we combine the trained adapters with ICL, we find no improvements over STAGE 1\u2019s 0-shot results, with the exception of ACED Asics.\nPerformance drops catastrophically for the MDNS Medical & Subtitles corpora. The rate\nof empty translations also increases dramatically6, with a rate of up to 63.1% for the 1-shot result on MDNS Medical (up from 8.0% at STAGE 0)."
        },
        {
            "heading": "4.3 STAGE 2A & STAGE 2B: Fine-Tuning towards ICL",
            "text": "When we compare the STAGE 2B (fine-tuning with the masked loss as described in Section 3.3) to the STAGE 0 results, we find that adding the separator and fine-tuning the model leads to generally improved scores on the ACED corpora for all k.\nBLEU Results on MDNS corpora show slightly worse performance compared to the STAGE 0 results in 3 out of 5 corpora for k = 1, but the averages are still improved. COMET scores are however consistently improved for this comparison. We also find that the scores for k = 2 and k = 1 are very close, with 2-shot being ahead of 1-shot by 0.6% BLEU points on average on ACED data, and 1-shot being ahead of 2-shot by 0.2 BLEU points on MDNS. Which is in contrast to what we have observed in STAGE 0. k = 5 still performs worse,\n6Empty translation rates of STAGE 1 for each k over all corpora: 1-shot: 20.0%, 2-shot: 20.6%, 5-shot: 13.6%.\nbut we observe high relative gains compared to the 5-shot STAGE 0 result.\nWhen comparing STAGE 2A and STAGE 2B, i.e. the masked loss and the standard NLL loss the results are inconclusive.\nWe further observe that STAGE 2B exhibits almost negligible rates of producing empty translations, at 0.3%, 0.8%, and 1.2% for k = 1, 2, 5 respectively."
        },
        {
            "heading": "4.4 STAGE 3: Combining ICL and Domain Adaptation",
            "text": "When combining ICL with adapters trained with nearest neighbor annotated data, we observe the globally best results for the NMT models. Compared to STAGE 1, which is also fine-tuned towards each domain, we observe greatly improved results on all automatic metrics. STAGE 3 2-shot delivers the best result on ACED, with an improvement of 2.5 BLEU points compared to the runner-up in terms of average BLEU STAGE 1 1-shot. On MDNS, STAGE 3 1-shot improves over the runnerup STAGE 1 0-shot by 3.8 points.\nEspecially the scores for MDNS Koran improve\nwell above all previous models, with a relative improvement of 101% compared to the baseline. The models seem to be able to make better use of close nearest neighbors in this dataset, which are often substrings of one another. See Section 4.6 for a detailed analysis of the copying behavior on the ACED Asics dataset.\nThe rate of empty translations is reduced to 0.0% for all k.\nWe further notice that the results for 1- and 2- shot ICL are very similar, and that the scores for 5-shot are also improved."
        },
        {
            "heading": "4.5 FALCON: Adapting Both to a Task and a Domain at the Same Time",
            "text": "The FALCON-40B LLM proves to excel at ICL, learning a task and adapting to a domain at the same time. Notably, scores improve with higher values of k, which is the opposite behavior to what we have observed with NMT models. When nearest neighbors are close to the test data, as they are for the ACED Emerson and MDNS IT datasets, we find results that are close to the best STAGE 3 results.\nFALCON-40B\u2019s generation speed is however very slow at an average of 2.6 tokens per second in the 1-shot setting.\nAlso note that we have no means at this time to check whether parts of the test data are contained in FALCON\u2019s training data."
        },
        {
            "heading": "4.6 Qualitative Analysis",
            "text": "Maintaining consistency in translations is an important quality criterion in the localization industry, and is a major motivator in the use of translation memories, which help ensure that marketing materials, for example, are uniform in the promised features and functions of the products being advertised (Emery et al., 2011). In NMT models, this consistency is traditionally increased by fine-tuning a translation model for a specific domain, which we denote by \"STAGE 1 with 0-shot\". In this section, we compare the fine-tuning approach with our ICL, specifically \"STAGE 3 with 1-shot\". We evaluate translation consistency on the Asics dataset. For that purpose we select segments s in the test data for which the source nearest neighbor s\u2032 in the Asics train data differs by exactly one word. These segments s are denoted as word-substitution segments. For each pair (s, s\u2032), we then use two sources and one target t\u2032 in the ICL prompt and the other target t as reference to compare the generated\ntranslation to. We define the fraction of pairs for which the generated translation exactly matches the reference as the word substitution accuracy (WSA). The results are in Table 6.\nThe translation for STAGE 3 1-shot achieves a WSA score of 74.6%, compared to 57.14% for the fine-tuning approach (STAGE 1 0-shot), whereas the non-adapted model only produces the exact reference translation in 1.7% of cases."
        },
        {
            "heading": "5 Conclusions",
            "text": "We have shown that a standard NMT system can be trained to be an effective in-context learner in domain adaptation tasks. We find that this is most effective when we combine generic fine-tuning towards the ICL task and training adapter layers for a specific domain with nearest neighbor annotated data.\nWhen the model is not fine-tuned towards the task, we find that ICL works to some extent, but shows degenerate behavior.\nWhile LLMs like FALCON-40B can adapt to the MT task with ICL, this comes at the cost of increased compute. Generally, the results with the LLM still underperform our dedicated MT models."
        }
    ],
    "title": "Neural Machine Translation Models Can Learn to be Few-shot Learners",
    "year": 2023
}