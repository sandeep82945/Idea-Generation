{
    "abstractText": "Container-based technologies empower cloud tenants to develop highly portable software and deploy services in the cloud at a rapid pace. Cloud privacy, meanwhile, is important as a large number of container deployments operate on privacy-sensitive data, but challenging due to the increasing frequency and sophistication of attacks. State-of-the-art confidential container-based designs leverage process-based trusted execution environments (TEEs), but face security and compatibility issues that limits their practical deployment. We propose Parma, an architecture that provides lift-andshift deployment of unmodified containers while providing strong security protection against a powerful attacker who controls the untrusted host and hypervisor. Parma leverages VM-level isolation to execute a container group within a unique VM-based TEE. Besides container integrity and user data confidentiality and integrity, Parma also offers container attestation and execution integrity based on an attested execution policy. Parma execution policies provide an inductive proof over all future states of the container group. This proof, which is established during initialization, forms a root of trust that can be used for secure operations within the container group without requiring any modifications of the containerized workflow itself (aside from the inclusion of the execution policy.) We evaluate Parma on AMD SEV-SNP processors by running a diverse set of workloads demonstrating that workflows exhibit 0\u201326% additional overhead in performance over running outside the enclave, with a mean 13% overhead on SPEC2017, while requiring no modifications to their program code. Adding execution policies introduces less than 1% additional overhead. Furthermore, we have deployed Parma as the underlying technology driving Confidential Containers on Azure Container Instances.",
    "authors": [
        {
            "affiliations": [],
            "name": "Matthew A. Johnson"
        },
        {
            "affiliations": [],
            "name": "Stavros Volos"
        },
        {
            "affiliations": [],
            "name": "Ken Gordon"
        },
        {
            "affiliations": [],
            "name": "Sean T. Allen"
        },
        {
            "affiliations": [],
            "name": "Christoph M. Wintersteiger"
        }
    ],
    "id": "SP:0eb4eb7edf8222c2756ee637474989ad78d33ff7",
    "references": [
        {
            "authors": [
                "Sergei Arnautov",
                "Bohdan Trach",
                "Franz Gregor",
                "Thomas Knauth",
                "Andre Martin",
                "Christian Priebe",
                "Joshua Lind",
                "Divya Muthukumaran",
                "Dan O\u2019Keeffe",
                "Mark L. Stillwell",
                "David Goltzsche",
                "Dave Eyers",
                "R\u00fcdiger Kapitza",
                "Peter Pietzuch",
                "Christof Fetzer"
            ],
            "title": "SCONE: Secure linux containers with intel SGX",
            "venue": "In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI",
            "year": 2016
        },
        {
            "authors": [
                "Raad Bahmani",
                "Ferdinand Brasser",
                "Ghada Dessouky",
                "Patrick Jauernig",
                "Matthias Klimmek",
                "Ahmad-Reza Sadeghi",
                "Emmanuel Stapf"
            ],
            "title": "CURE: A security architecture with customizable and resilient enclaves",
            "year": 2010
        },
        {
            "authors": [
                "Paul Barham",
                "Boris Dragovic",
                "Keir Fraser",
                "Steven Hand",
                "Tim Harris",
                "Alex Ho",
                "Rolf Neugebauer",
                "Ian Pratt",
                "Andrew Warfield"
            ],
            "title": "Xen and the art of virtualization",
            "venue": "In Symposium on Operating Systems Principles (SOSP",
            "year": 2003
        },
        {
            "authors": [
                "Andrew Baumann",
                "Marcus Peinado",
                "Galen Hunt"
            ],
            "title": "Shielding applications from an untrusted cloud with Haven",
            "venue": "In 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI),",
            "year": 2014
        },
        {
            "authors": [
                "Ferdinand Brasser",
                "David Gens",
                "Patrick Jauernig",
                "Ahmad-Reza Sadeghi",
                "Emmanuel Stapf"
            ],
            "title": "SANC- TUARY: ARMing TrustZone with user-space enclaves",
            "venue": "In Proc. of Network and Distributed System Security Symposium,",
            "year": 2019
        },
        {
            "authors": [
                "Ferdinand Brasser",
                "Patrick Jauernig",
                "Frederik Pustelnik",
                "Ahmad-Reza Sadeghi",
                "Emmanuel Stapf"
            ],
            "title": "Trusted container extensions for container-based confidential computing",
            "venue": "In arXiv,",
            "year": 2022
        },
        {
            "authors": [
                "David Champagne",
                "Ruby B. Lee"
            ],
            "title": "Scalable architectural support for trusted software",
            "venue": "In HPCA -",
            "year": 2010
        },
        {
            "authors": [],
            "title": "Online; accessed 21 November 2022",
            "year": 2022
        },
        {
            "authors": [
                "Victor Costan",
                "Ilia Lebedev",
                "Srinivas Devadas"
            ],
            "title": "Sanctum: Minimal hardware extensions for strong software isolation",
            "venue": "In 25th USENIX Security Symposium (USENIX Security",
            "year": 2016
        },
        {
            "authors": [
                "Docker. https://www.docker.com"
            ],
            "title": "Online; accessed 21 November 2022",
            "year": 2022
        },
        {
            "authors": [
                "Dmitry Evtyushkin",
                "Jesse Elwell",
                "Meltem Ozsoy",
                "Dmitry Ponomarev",
                "Nael Abu Ghazaleh",
                "Ryan Riley"
            ],
            "title": "Iso-X: a flexible architecture for hardware-managed isolated execution",
            "venue": "In 2014 47th Annual IEEE/ACM International Symposium on Microarchitecture,",
            "year": 2014
        },
        {
            "authors": [
                "Xinyang Ge",
                "Hsuan-Chi Kuo",
                "Weidong Cui"
            ],
            "title": "Hecate: Lifting and shifting on-premises workloads to an untrusted cloud",
            "venue": "In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security,",
            "year": 2022
        },
        {
            "authors": [],
            "title": "Online; accessed 21 November 2022",
            "year": 2022
        },
        {
            "authors": [
                "David Kaplin",
                "Jeremy Powell",
                "Tom Woller"
            ],
            "title": "AMD memory encryption",
            "venue": "https: //amd.wpenginepowered.com/wordpress/media/ 2013/12/AMD_Memory_Encryption_Whitepaper_ v9-Public.pdf,",
            "year": 2021
        },
        {
            "authors": [
                "Dayeol Lee",
                "David Kohlbrenner",
                "Shweta Shinde",
                "Krste Asanovi\u0107",
                "Dawn Song"
            ],
            "title": "Keystone: An open framework for architecting trusted execution environments",
            "venue": "In Proceedings of the Fifteenth European Conference on Computer Systems, EuroSys \u201920,",
            "year": 2020
        },
        {
            "authors": [
                "Xupeng Li",
                "Xuheng Li",
                "Christoffer Dall",
                "Ronghui Gu",
                "Jason Nieh",
                "Yousuf Sait",
                "Gareth Stockwell"
            ],
            "title": "Design and verification of the Arm Confidential Compute Architecture",
            "venue": "In 16th USENIX Symposium on Operating Systems Design and Implementation",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan M. McCune",
                "Bryan J. Parno",
                "Adrian Perrig",
                "Michael K. Reiter",
                "Hiroshi Isozaki"
            ],
            "title": "Flicker: An execution infrastructure for tcb minimization",
            "venue": "In Proceedings of the 3rd ACM SIGOPS/EuroSys European Conference on Computer Systems",
            "year": 2008
        },
        {
            "authors": [
                "Ralph Palutke",
                "Andreas Neubaum",
                "Johanne Gotzfried"
            ],
            "title": "SEVGuard: Protecting user mode applications using Secure Encrypted Virtualization",
            "venue": "In International Conference on Security and Privacy in Communication Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Christian Priebe",
                "Divya Muthukumaran",
                "Joshua Lind",
                "Huanzhou Zhu",
                "Shujie Cui",
                "Vasily A. Sartakov",
                "Peter Pietzuch"
            ],
            "title": "SGX-LKL: Securing the Host OS Interface for Trusted Execution",
            "venue": "In arXiv,",
            "year": 2020
        },
        {
            "authors": [
                "Shweta Shinde",
                "Dat Le Tien",
                "Shruti Tople",
                "Prateek Saxena"
            ],
            "title": "Panoply: Low-TCB Linux applications with SGX enclaves",
            "venue": "In Network and Distributed System Security Symposium (NDSS),",
            "year": 2017
        },
        {
            "authors": [
                "G. Edward Suh",
                "Dwaine Clarke",
                "Blaise Gassend",
                "Marten van Dijk",
                "Srinivas Devadas"
            ],
            "title": "AEGIS: architecture for tamper-evident and tamper-resistant processing",
            "venue": "In ACM International Conference on Supercomputing 25th Anniversary Volume,",
            "year": 2003
        },
        {
            "authors": [
                "He Sun",
                "Kun Sun",
                "Yuewu Wang",
                "Jiwu Jing",
                "Haining Wang"
            ],
            "title": "TrustICE: hardware-assisted isolated computing environments on mobile devices",
            "venue": "In 2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks,",
            "year": 2015
        },
        {
            "authors": [
                "Chia-Che Tsai",
                "Donald E. Porter",
                "Mona Vij"
            ],
            "title": "Graphene-SGX: A practical library OS for unmodified applications on SGX",
            "venue": "USENIX Annual Technical Conference (ATC),",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "We propose Parma, an architecture that provides lift-andshift deployment of unmodified containers while providing strong security protection against a powerful attacker who controls the untrusted host and hypervisor. Parma leverages VM-level isolation to execute a container group within a unique VM-based TEE. Besides container integrity and user data confidentiality and integrity, Parma also offers container attestation and execution integrity based on an attested execution policy. Parma execution policies provide an inductive proof over all future states of the container group. This proof, which is established during initialization, forms a root of trust that can be used for secure operations within the container group without requiring any modifications of the containerized workflow itself (aside from the inclusion of the execution policy.)\nWe evaluate Parma on AMD SEV-SNP processors by running a diverse set of workloads demonstrating that workflows exhibit 0\u201326% additional overhead in performance over running outside the enclave, with a mean 13% overhead on SPEC2017, while requiring no modifications to their program code. Adding execution policies introduces less than 1% additional overhead. Furthermore, we have deployed Parma as the underlying technology driving Confidential Containers on Azure Container Instances."
        },
        {
            "heading": "1 Introduction",
            "text": "Since the launch of the large-scale Infrastructure-as-a-Service (IaaS) offerings from Amazon (AWS in 2006), Microsoft\n(Azure in 2008), and Google (GCP in 2008), there has been a continuous trend towards cloud computing, which allows customers to leverage capability and cost advantages through economies of scale. This was made possible through virtualization [9], whereby virtual machines (VMs) allow the efficient use of large bare-metal compute architectures (hosts) by using a hypervisor to coordinate sharing between multiple tenants according to their expressed usage requirements. However, while VMs provide a way for users to quickly obtain additional compute capacity and maximize the utilization of existing hardware (and/or avoid the cost of maintaining peak capacity by utilizing a public cloud), it is still necessary to configure, deploy, manage, and maintain VMs using traditional techniques.\nIn recent years, container-based technologies, such as Docker [19] and Kubernetes [25] have arisen to address this orthogonal need, providing a lightweight solution for creating a set of machine configurations, called containers, which can be deployed onto hardware (virtualized or physical) as a group via an automated process. Container technology provides multiple separated user-space instances which are isolated from one another via kernel software. Unlike VMs, containers run directly on the host system (sharing its kernel) and as such do not need to emulate devices or maintain large disk files. Further, containers defined according to the OCI Distribution Specification [30] specify dependencies as layers which can be shared between different containers, making them amenable to caching and thus speeding up deployment while reducing storage costs for multiple containers. The success of containerization technology for on-premises systems has led to major cloud providers developing their own Container-asa-Service (CaaS) offerings [1,3,22] which provide customers the ability to maintain and deploy containers in the public cloud. In CaaS offerings, containers run in a per-group utility VM (UVM) which provides hypervisor-level isolation between containers running from different tenants on the same host. While the container manager and container shim running on the host are responsible for pulling images from the container registry, bringing up the utility VM, and orchestrat-\nar X\niv :2\n30 2.\n03 97\n6v 3\n[ cs\n.C R\n] 7\nM ar\ning container execution, an agent running within the utility VM (the guest agent) coordinates the container workflow as directed by the host-side container shim.\nCloud computing poses unique risks. Although VMs and VM-isolated container groups provide strong isolation between tenants, they are deployed by the cloud service provider (CSP) and coordinated by the CSP\u2019s hypervisor. As such, the host operating system (including the container manager and container shim) and the hypervisor all lie within the Trusted Computing Base (TCB). Research into confidential cloud computing attempts to reduce the attack surface by leveraging specialized hardware-enforced Trusted Execution Environments (TEEs) [8, 11, 13, 15, 20, 27, 29, 37, 38], which enable user workloads to be protected inside enclaves even if the host\u2019s software is compromised or controlled by a malicious entity. TEEs available from major CPU vendors can be either process-based, such as Intel SGX [23] and ARM TrustZone [6], or VM-based, such as AMD SEV-SNP [4, 26], Intel TDX [24] and ARM CCA [5, 28]. VM-based TEEs offer hardware-level isolation of the VM, preventing the host operating system and the hypervisor from having access to the VM\u2019s memory and registers.\nWith CaaS, container execution is orchestrated by a hostside shim that communicates with the guest agent, which coordinates the activity of the container group within the UVM. The UVM can be hardware-isolated within a TEE enclave, but the container images are controlled by the host, as is the order in which they are mounted, the container environment variables, the commands that are sent to the containers via the bridge between the container shim and the guest agent, and so forth. This means that a compromised host can overcome the hardware isolation of the VM by injecting malicious containers. This risk of attack, be it from malicious or compromised employees of the CSP or external threats, limits the extent to which containerization can be used in the cloud for sensitive workloads in industries like finance and healthcare.\nThe naive solution to this problem is to run the guest agent\nand container shim within the same VM-based TEE. This removes the CSP from the TCB, but it also removes the CSP\u2019s ability to orchestrate and automate the container workflow. In addition, the container owner is then in the TCB. The container images are controlled by the container owner, as is the order in which they are mounted, the container environment variables, and the commands that are sent to the containers. The end-user of the confidential container (e.g., a customer of a bank, a patient providing data to a doctor) must trust that the container owner has and will run only the expected commands. This also leaves image integrity and data confidentiality and integrity unsolved.\nOur work. We present Parma, an architecture that implements the confidential containers abstraction on a state-ofthe-art containerd [14] stack running on processors with VM-based TEE support (i.e., AMD SEV-SNP processors). Parma provides a lift-and-shift experience and the ability to run unmodified containers pulled from (unmodified) container registries while providing strong security guarantees: container attestation and integrity, meaning that only customerspecified containers can run within the TCB and any means of container tampering is detected by the TCB; and user data confidentiality and integrity, meaning that only the TCB has access to the user\u2019s data and any means of data tampering is detected by the TCB.\nParma provides strong protection for the container\u2019s root filesystem (comprised of the container image layers and writeable scratch space) and the user\u2019s data. For container image layers (pulled in plaintext by the untrusted container manager and stored in a host-side block device), Parma mounts the device as an integrity-protected read-only filesystem (using dmverity) and relies on the filesystem driver to enforce integrity checking upon an access to the filesystem. For confidentiality and integrity of privacy-sensitive data stored in a block device (e.g., writeable scratch space of the container\u2019s root filesystem) or blob storage (e.g., remote blobs holding user data),\nParma relies on block-level encryption and integrity (using dm-crypt + dm-integrity) to decrypt memory-mapped blocks, guaranteeing that data appears in plaintext only within the VM\u2019s hardware-protected memory.\nFinally, Parma provides container attestation rooted in a hardware-issued attestation by enforcing attested userspecified execution policies. We have augmented the guest agent to enforce the execution policy such that it only executes commands (submitted by the untrusted container shim) which are explicitly allowed by the user, as seen in Figure 1. The policy is attested by encoding its measurement in the attestation report as an immutable field at UVM initialization. As a result of including the execution policy, the hardware-issued attestation forms an inductive proof over the future state of the container group. The attestation can then be used downstream for operations needed by secure computation. For example, remote verifiers may release keys (governing the user\u2019s encrypted data) to only those container groups which can present an attestation report encoding the expected execution policy and measurement of the utility VM.\nContributions: The main contributions of our work are:\n\u2022 Parma, a novel security architecture for confidential containerized workloads. Parma establishes security guarantees rooted in an inductive proof over all future states of the container group provided by the introduction of an attested execution policy.\n\u2022 an implementation of Parma which forms the basis for Confidential Containers on Azure Container Instances [2] and is publicly available on GitHub 1.\n\u2022 neither requiring changes to existing containers, nor container image signing. Instead, the execution policy ensures that only the actions the user has explicitly expressed are allowed to take place within the container group, maintaining support for existing CaaS deployment practices.\n\u2022 an evaluation of our implementation with standard benchmarks for computation, network, and database activity. We compare a base container system to containers running within a TEE enclave with and without Parma. We demonstrate that Parma introduces 0\u201326% additional overhead in performance over running outside the enclave, with a mean 13% overhead on SPEC2017, and that adding execution policies introduces less than 1% additional overhead.\nThere were also significant implementation challenges, including SEV-SNP enablement in the hypervisor, bounce\n1https://github.com/microsoft/hcsshim/tree/main/pkg/ securitypolicy\nbuffers for I/O, Linux enlightenment for SEV-SNP including attestation report fetching, and hardening the hypervisor interface. These are not claimed as contributions."
        },
        {
            "heading": "2 Background",
            "text": "We will begin by introducing the technological dependencies of Parma, namely Trusted Execution Environments (TEEs) and the AMD SEV-SNP architecture."
        },
        {
            "heading": "2.1 Trusted Execution Environments",
            "text": "There have been many proposed security architectures which aim to provide a TEE [8, 11, 15, 23, 26, 27]. The goal of a TEE is to isolate workloads from the host system in order to protect them against manipulation whilst running. Most architectures provide secure environments, typically called enclaves, which run in parallel with the underlying host operating system. As such, the Trusted Computing Base (TCB) contains the required hardware which provides the needed security capabilities and the software to utilize it to maintain the guarantees of the TEE. While each TEE architecture has its own idiosyncrasies, there are desirable qualities which increase their utility:\nSmall TCB Minimizing the TCB is essential to reduce the attack surface of the TEE.\nStrong Isolation The enclaves must be isolated from the host at all times, including the register state and memory.\nAttestable State The boot-up process and state of the TEE must be verifiable using attestation.\nMinimal Overhead High performance costs incurred by using the TEE greatly minimize utility.\nMinimal Adoption Cost While perhaps not a goal shared by all TEEs, greater utility is achieved if running code in the TEE does not require significant reworking of a workflow (e.g., rewriting software to target an enclave-specific subset of a language, requiring custom tool-chains)."
        },
        {
            "heading": "2.2 AMD Secure Encrypted VirtualizationSecure Nested Paging",
            "text": "The commercially available TEE offering from AMD is called Secure Encrypted Virtualization (SEV) [26] and targets cloud servers. As indicated in the name, it is focused on protecting Virtual Machines (VMs) from a malicious host or hypervisor. We use a specialization of SEV called SEV-SNP [4] (Secure Nested Paging). AMD SEV-SNP is available in AMD\u2019s EPYC Milan processors and extends the SEV and SEV-ES (Encrypted State) technologies, which offer isolation of a VM by providing encrypted memory and CPU register state. AMD\nSEV-SNP adds memory integrity protection to ensure that a VM is able to read the most recent values written to an encrypted memory page. In doing so, it provides protection against data replay, corruption, remapping- and aliasing-based attacks."
        },
        {
            "heading": "2.2.1 Platform Security Processor",
            "text": "The Platform Security Processor firmware (PSP) implements the security environment for hardware-isolated VMs. The PSP provides a unique identity to the CPU by deriving the Versioned Chip Endorsement Key (VCEK) from chip-unique secrets and the current TCB version. The PSP also provides ABI functions for managing the platform, the life-cycle of a guest VM, and data structures utilized by the PSP to maintain integrity of memory pages."
        },
        {
            "heading": "2.2.2 Memory Encryption",
            "text": "AMD Secure Memory Encryption (SME) [26] is a generalpurpose mechanism for main memory encryption that is flexible and integrated into the CPU architecture. It is provided via dedicated hardware in the on-die memory controllers that provides an Advanced Encryption Standard (AES) engine. This encrypts data when it is written to DRAM, and then decrypts it when read, providing protection against physical attacks on the memory bus and/or modules. The key used by the AES engine is randomly generated on each system reset and is not visible to any process running on the CPU cores. Instead, the key is managed entirely by the PSP. Each VM has memory encrypted with its own key, and can choose which data memory pages they would like to be private. Private memory is encrypted with a guest-specific key, whereas shared memory may be encrypted with a hypervisor key."
        },
        {
            "heading": "2.2.3 Secure Nested Paging",
            "text": "The memory encryption provided by AMD-SEV is necessary but not sufficient to protect against runtime manipulation. In particular, it does not protect against integrity attacks such as:\nReplay The attacker writes a valid past block of data to a memory page. This is of particular concern if the attacker knows the unencrypted data.\nData Corruption If the attacker can write to a page then even if it is encrypted they can write random bytes, corrupting the memory.\nMemory Aliasing A malicious hypervisor maps two or more guest pages to the same physical page, such that the guest corrupts its own memory.\nMemory Re-Mapping A malicious hypervisor can also map one guest page to multiple physical pages, so that the guest has an inconsistent view of memory where only a subset of the data it wrote appears."
        },
        {
            "heading": "2.2.4 Reverse Map Table",
            "text": "The relationship between guest pages and physical pages is maintained by a structure called a Reverse Map Table (RMP). It is shared across the system and contains one entry for every 4k page of DRAM that may be used by VMs. The purpose of the RMP is to track the owner for each page of memory, and control access to memory so that only the owner of the page can write it. The RMP is used in conjunction with standard x86 page tables to enforce memory restrictions and page access rights. When running in an AMD SEV-SNP VM, the RMP check is slightly more complex. AMD-V 2-level paging (also called Nested Paging) is used to translate a Guest Virtual Address (GVA) to a Guest Physical Address (GPA), and then finally to a System Physical Address (SPA). The SPA is used to index the RMP and the entry is checked [4]."
        },
        {
            "heading": "2.2.5 Page Validation",
            "text": "Each RMP entry contains the GPA at which a particular page of DRAM should be mapped. While the nested page tables ensure that each GPA can only map to one SPA, the hypervisor may change these tables at any time. Thus, inside each RMP entry is a Validated bit, which is automatically cleared to zero by the CPU when a new RMP entry is created for a guest. Pages which have the validated bit cleared are not usable by the hypervisor or as a private guest page. The guest can only use the page after it sets the Validated bit via a new instruction, PVALIDATE. Only the guest is able to use PVALIDATE, and each guest VM can only validate its own memory. If the guest VM only validates the memory corresponding to a GPA once, then the injective mapping between GPAs and SPAs is guaranteed."
        },
        {
            "heading": "2.2.6 Attestation",
            "text": "The PSP can issue hardware attestation reports capturing various security-related attributes, constructed or specified during initialization and runtime. Among other information, the resulting attestation report contains the guest launch measurement, the host data, and the report data. The attestation report is signed by the VCEK.\nInitialization. During VM launch, the PSP initializes a cryptographic digest context used to construct the measurement of the guest. The hypervisor can insert data into the the guest\u2019s memory address space at the granularity of a page, during which the cryptographic digest context is updated with the data, thereby binding the measurement of the guest with all operations that the hypervisor took on the guest\u2019s memory contents. A special page is added by the hypervisor to the guest memory, which is populated by the PSP with an encryption key that establishes a secure communication channel between the PSP and the guest. Once the VM launch completes, the PSP finalizes the cryptographic digest which is\nencoded as the guest launch measurement in the attestation report. The hypervisor may provide 256-bits of arbitrary data to be encoded as host data in the attestation report.\nRuntime. The PSP generates attestation reports on behalf of a guest VM. The request and response are submitted via the secure channel established during the guest launch, ensuring that a malicious host cannot impersonate the guest VM. Upon requesting a report, the guest may supply 512-bits of arbitrary data to be encoded in the report as report data."
        },
        {
            "heading": "3 Parma Architecture",
            "text": "In this section, we present Parma, an architecture that implements the confidential containers abstraction using attested execution policies. We first describe the container platform which forms the basis of the Parma design and implementation (3.1) and then provide a detailed description of the threat model (3.2).\nFinally, we present the security guarantees under the threat model and how Parma provides these guarantees via a collection of design principles (3.3). The guiding principle of Parma is to provide an inductive proof over the state of a container group, rooted in the attestation report produced by the PSP. The standard components and lifecycle for the container platform (the CPLAT) are largely unchanged, with the exception of the guest agent, whose actions become circumscribed by the execution policy (3.4). Thus constrained, the future state of the system can be rooted in the measurement performed during guest initialization."
        },
        {
            "heading": "3.1 Container Platform",
            "text": "The container platform (the CPLAT) is a group of components built around the capabilities of containerd [14]. containerd is a daemon which manages the complete container life-cycle, from image pull and storage to container execution and supervision to low-level storage and network attachments. containerd supports the Open Container Initiative (OCI) [30] image and runtime specifications, and provides the substrate for multiple container offerings, such as Docker [19], Kubernetes [25], and various cloud offerings [1, 3, 22]. Clients interact with containerd via a client interface, such as ctr, nerdctl, or crictl. containerd supports both running bare metal containers (i.e., those that run directly on the host) and also containers that run within a utility VM (UVM).\nOur work focuses on VM-isolated containers. The CPLAT interfaces with a custom container shim running in the host operating system. The container shim interacts with (i) host services to bring up a UVM required for launching a new pod and (ii) the guest agent running in the UVM. The guest agent is responsible for creating containers and spawning a runc instance for starting the container. In essence, the container\nshim-guest agent path allows the CPLAT components running in the host operating system to execute containers in isolated guest VMs. The execution of a VM-isolated container on Linux using CPLAT involves three high-level steps (as seen in Figure 2): (i) pull the container\u2019s image, (ii) launch a pod for hosting the container, (iii) start a container.\nImage pull Pulling images entails downloading them from a container registry (unless they are already cached on the machine). Once the pull is done, the image is unpacked and the image for each layer of the container is stored as a virtual hard drive.\nPod launch Launching a pod entails creating and launching a UVM along with the guest agent. Thereafter, the container shim interacts with the guest agent to create and start containers. At the end of the pod launch, the container shim creates and starts a sandbox/pause container that holds the Linux namespaces for future containers.\nContainer start Starting a container requires that the guest agent mounts the container\u2019s root filesystem into the UVM; the root filesystem comprises the container layers and a writeable scratch layer. In doing so, the container shim (i) attaches each container layer (in the OCI specification) to the UVM and (ii) creates and attaches to the UVM a writeable sandbox virtual hard drive. The container layer and sandbox devices are then mounted by the guest agent into the UVM as an overlay root filesystem. Finally, the guest agent creates a runtime bundle that contains the overlay filesystem path and configuration data (compiled using the OCI runtime specification.) The runtime bundle is passed to the runc instance, which subsequently starts the container."
        },
        {
            "heading": "3.2 Threat Model",
            "text": "We consider a strong adversary who controls the entire host system software, including the hypervisor and the host operating system along with all services running within it. However, we trust the CPU package, including the platform security processor (PSP) and the AMD SEV-SNP implementation, which provides hardware-based isolation of the guest\u2019s address space from the system software. We also trust the firmware running on the PSP and its measurements of the guest VM, including the guest agent.\nSuch an adversary can:\n\u2022 tamper with the container\u2019s OCI runtime specification;\n\u2022 tamper with block devices storing the read-only container image layers and the writeable scratch layer;\n\u2022 tamper with container definitions, including the overlay filesystem (i.e., changing the order or injecting rogue\nlayers), adding, altering, or removing environment variables, altering the user command, and the mount sources and destinations from the UVM.\n\u2022 add, delete, and make arbitrary modifications to network messages, i.e., fully control the network.\n\u2022 request execution of arbitrary commands in the UVM and in individual containers.\n\u2022 request debugging information from the UVM and running containers, such as access to I/O, the stack, or container properties.\nThese capabilities provide the adversary with the ability to gain access to the address space of the guest operating system.\nOut of Scope. Anything not mentioned here, e.g., sidechannel attacks, are outside the scope of our threat model."
        },
        {
            "heading": "3.3 Security Guarantees",
            "text": "Under the threat model presented in Section 3.2, we wish to provide strong confidentiality and integrity guarantees for the container and for customer data. The provided security guarantees are based on the following principles:\nHardware-based Isolation of the UVM. The memory address space and disks of the VM must be protected from the\nhost and other VMs by hardware-level isolation. Parma relies on the SEV-SNP hardware guarantee that the memory address space of the UVM cannot be accessed by host system software.\nIntegrity-protected Filesystems. Any block device or blob storage is mounted in the UVM as an integrity-protected file system. The file system driver enforces integrity checking upon an access to the file system, ensuring that the host system software cannot tamper with the data and container images. In Parma, a container file system is expressed as an ordered sequence of layers, where each layer is mounted as a separate device and then assembled into an overlay filesystem [31]. First, Parma verifies as each layer is mounted that the dmverity root hash [18] for the device matches a layer that is enumerated in the policy. Second, when the container shim requests the mounting of an overlay filesystem that assembles multiple layer devices, Parma verifies that the specific ordering of layers is explicitly laid out in the execution policy for one or more containers.\nEncrypted Filesystems. Any block device or blob storage that holds privacy-sensitive data is mounted as an encrypted filesystem. The filesystem driver decrypts the memorymapped block upon an access to the filesystem. The decrypted block is stored in hardware-isolated memory space, ensuring that host system software cannot access the plaintext data. The writable scratch space of the container is mounted with dm-crypt [16] and dm-integrity [17], and this is enforced\nby the execution policy. The encryption key for the writeable scratch space is ephemeral and is provisioned initially in hardware-protected memory and erased once the device is mounted.\nUVM Measurement. The UVM, its operating system and the guest agent are crytographically measured during initialization by the TEE and this measurement can be requested over a secure channel at any time by user containers. The AMD SEV-SNP hardware performs the measurement and encodes it in the signed attestation report as discussed in Section 2.2.6.\nVerifiable Execution Policy. The user must be provided with a mechanism to verify that the active execution policy (see below in (Section 3.4)) in a container group is what they expect it to be. The execution policy is defined and measured independently by the user and it is then provided to the CaaS deployment system. The host measures the policy (e.g., using SHA-512) and places this measurement in the immutable host data of the report as described in Section 2.2.6. The policy itself is passed to the UVM by the container shim, where it is measured again to ensure that its measurement matches the one encoded as host data in the report.\nRemote Attestation. Remote verifiers (i.e., tenants, external services, attestation services) need to verify an attestation report so that they can establish trust in a secure communication channel with the container group running within the UVM. In particular, remote verifiers need to to verify that the UVM has booted the expected operating system, the correct guest agent, and further that the guest agent is configured with the\nexpected execution policy. In Parma, the UVM (including privileged containers) can request an attestation report using the secure channel established between the PSP and the UVM, as detailed in Section 2.2.6. The requester generates an emphemeral token (e.g., TLS public key pair or a sealing/wrapping public key) which is presented as a runtime claim in the report; the token\u2019s cryptographic digest is encoded as report data in the report. A remote verifier can then verify that (i) the report has been signed by a genuine AMD processor using a key rooted to AMD\u2019s root certificate authority, (ii) the guest launch measurement and host data match the expected VM measurement and the digest of the expected execution policy, (iii) the report data matches the hash digest of the runtime claim presented as additional evidence.\nOnce the verification completes, the remote verifier that trusts the UVM (including the guest OS, guest agent and the execution policy) trusts that the UVM and the container group running with it will not reveal the private keys from which the public tokens have been generated, e.g., TLS private key, sealing/wrapping private key. The remote verifer can utilize the runtime claim accordingly. For instance,\n\u2022 a TLS public key can be used for establishing a TLS connection with the attested container group. As such, the remote verifier can trust there is no replay or man-inthe-middle attack;\n\u2022 a sealing public key can be used to seal (via encryption) a request or response intended only for the attested containers;\n\u2022 a wrapping public key can be used by a key management service to wrap and release encryption keys required by the VM\u2019s container group for decrypting encrypted remote blob storage. As such, the remote verifier can trust that only trustworthy and attested container groups can unwrap the encryption keys. Figure 3 illustrates this process."
        },
        {
            "heading": "3.4 Execution Policy",
            "text": "As discussed in our threat model, the container shim is not trusted as it could be under the control of an attacker. This implies that any action which the container shim requests the guest agent undertake inside the UVM is suspect (see Section 3.2 for a list of malicious host actions). Even if the current state of the container group is valid, there is no guarantee that the host will not compromise it in the future, and thus no way for the attestation report to be used as a gate on access to secure customer data. The attestation report on its own simply records the UVM OS, the guest agent, and the container runtime versions in use. It is not able to make any claims about the container group the host will subsequently orchestrate.\nFor example, the host can start the user container group in a manner which is expected by an attestation service until such time as it acquires some desired secure information, and then load a series of containers which open the container group to a remote code execution attack. The attestation report, obtained during initialization, cannot protect against this. Even updating it via providing additional runtime data to the PSP (as described in Section 2.2.6) does not help, because the vulnerability is added by the host after the attestation report has been consumed by the external service.\nTo address this vulnerability, we introduce the concept of an execution policy. Authored by the customer, it describes what actions the guest agent is allowed to take throughout the lifecycle of the container group. The guest agent is altered to consult this policy before taking any of the actions in Table 1, providing information to the policy that is used to make decisions. These actions each have a corresponding enforcement point in the execution policy which will either allow or deny the action. In our implementation the policy is defined using the Rego policy language [34]. A sample enforcement point can be seen in Listing 1.\n1 default mount_device := {\"allowed\": false} 2 3 device_mounted(target) { 4 data.metadata.devices[target] 5 } 6 7 mount_device := {\"metadata\": [addDevice], 8 \"allowed\": true} { 9 not device_mounted(input.target)\n10 some container in data.policy.containers 11 some layer in container.layers 12 input.deviceHash == layer\n13 addDevice := { 14 \"name\": \"devices\", 15 \"action\": \"add\", 16 \"key\": input.target , 17 \"value\": input.deviceHash 18 } 19 }\nListing 1: Sample enforcement point. Here, as in our implementation, the policy is expressed in Rego [34].\nA novel feature of our implementation is the ability for a policy to manipulate its own metadata state (maintained by the guest agent). This provides an attested mechanism for the execution policy to build a representation of the state of the container group, allowing for more complex interactions. For example, in the rule shown in Listing 1, the enforcement point for mounting a device creates a metadata entry for the device which will be used to prevent other devices from being mounted to the same target path.\nThe result of making this small change to the guest agent is that the state space of the container group is bounded by a state machine, in which transitions between states correspond to the actions described above. Each transition is executed atomically and comes with an enforcement point.\nInduction. The state machine starts as a system that is fully measured and attested, including the execution policy with all its enforcement points, with the root of trust being the PSP hardware (n = 1). All possible transitions are described by the execution policy. Regardless of which (n) transitions have been taken after that, each of the actions listed in Table 1 cannot break integrity or confidentiality without deliberate modification of the respective enforcement point, which would have had to happen before the initial measurement (n+ 1). Any sequence of such transitions therefore maintains integrity and confidentiality. Our enforcement points are carefully designed to maintain these properties. Note that confidentiality is modulo acceptance of the UVM and the execution policy. That is, the end-user must verify that the attestation report they receive from Parma is bound to a UVM and an execution policy that uses the end-user\u2019s data in a manner they accept."
        },
        {
            "heading": "4 Evaluation",
            "text": "We used benchmarking tools to evaluate several typical containerized workloads for reductions in computation throughput, network throughput, and database transaction rates to ensure that Parma does not introduce significant computational overhead. In all cases we demonstrate that Parma provides confidentiality for containerized workloads with minimal costs to performance (typically less than 1% additional overhead over running in an enclave).\nEach benchmarking experiment is conducted using two machines: (1) a DELL PowerEdge R7515 with an AMD EPYC 7543P 32-Core Processor and 128GB of memory for\nhosting the container runtime and (2) a benchmarking client (to avoid impact of any benchmarking software upon the evaluation) with the same configuration connected to (1) on the same subnet via 10GBit Ethernet. (1) is running Windows Server 2022 Datacenter (22H2) and an offline version of the Azure Container Instances CPLAT (i.e., containerd, cri, and hcsshim). The UVM runs a patched version of 5.15 Linux which includes AMD and Microsoft patches to provide AMD SEV-SNP enlightenment. (2) is running Ubuntu 20.04 with Linux kernel 5.15.\n4.1 nginx Web services are a common use case for containerization, and so we benchmark the popular nginx webserver using the wrk2 [41] benchmarking tool. Each test is run for 60 seconds on 10 threads, simulating 100 concurrent users making 200 requests per second (for a total of 12000 requests per test). We repeat the tests 20 times for each of three configurations:\nBase A baseline nginx container running outside the SEVSNP enclave,\nSEV-SNP The same container running within the SEV-SNP enclave,\nParma The same container again within the enclave and with an attested execution policy,\nand measure the latency. The results are shown in Figure 4. The median curves are computed over all experiments per configuration, and the histograms are composed of all latency samples which were gathered. We observe an increase in latency by the introduction of SEV-SNP, as expected, and also a very minor increase in latency when adding the execution policy. However, it is worth noting that these effects are only reliably observed in aggregate, i.e., all median curves are within the first quartiles of each other.\n4.2 redis The in-memory key/value database redis provides another useful benchmark for containerized compute. It supports a diverse set of data structures, such as hashtables, sets, and lists. We perform our benchmarking using the provided redis-benchmark with 25 parallel clients and a subset of tests, the results of which can be seen in Table 2. Looking at the geometric mean over all actions, we see a performance overhead of 18% added by operating within the AMD SEVSNP enclave, and a further 1% when using Parma. The performance overhead is attributed to increased TLB pressure arising from large working sets which exhibit poor temporal reuse in TLBs and trigger page table walks. In SEV-SNPenabled systems, table page table walks incur additional metadata checks (in the Reverse Page Table) to ensure that the page is indeed owned by the VM."
        },
        {
            "heading": "4.3 SPEC2017",
            "text": "We also evaluate Parma by measuring the computation performance overhead using the SPEC2017 intspeed benchmarks [36]. The benchmark programs are compiled and run on the bare metal hardware. When containerized, they are provided with 32 cores and 32 GB of memory. As can be seen in Table 3 AMD SEV-SNP adds a performance overhead of 13% on average, and Parma adds less than 1% on top of this. By looking at the individual benchmarks in Figure 5, SEV-SNP introduces a wide range (1-38%) of performance overhead in SPECint benchmarks. The overheads are down to (i) the increased TLB pressure (further exacerbated in SEV-SNP setups as discussed in redis benchmark); 631.deepsjeng, the most memory-intensive benchmark in SPECint introduces the second-highest overhead and (ii) the additional overhead for accessing the encrypted scratch space; 620.omnetpp, the most IO-intensive benchmark (due to large test inputs) introduces the highest overhead (38%)."
        },
        {
            "heading": "4.4 NVidia Triton Inference Server",
            "text": "Finally, we evaluate Parma by running a machine learning (ML) inference workload based on NVidia\u2019s Triton Inference Server [39]. Models (trained offline) and their parameters are used by the server to serve requests via a REST API.\nThe confidential ML inference server is deployed via a container group that comprises two containers: an (unmodified) Triton inference container, and a sidecar container that mounts an encrypted remote blob (holding the ML model) using dm-crypt and dm-integrity. (The sidecar container also implements the attestation workflow described in Figure 3 to release the encryption key.) The filesystem (and the contained ML model) is made available to the Triton inference container.\nWe evaluate the inference servers using NVidia\u2019s perf-analyzer system, allowing us to measure the overhead introduced by SEV-SNP and Parma, as shown in Figure 6. For each of the three configurations (Base, SNP, and Parma) we run four different experiments with 1 to 4 concurrent clients\nmaking continuous inference requests. In Figure 6 we report the median throughput for these experiments over 3 trials and observce a performance overhead of 26% when running in the AMD SEV-SNP enclave. As before, the additional overhead from Parma is 1%. The overheads share the same root cause in the increased TLB pressure as was previous described in the redis benchmark."
        },
        {
            "heading": "5 Related Work",
            "text": "A number of TEE container runtimes have been proposed to enable running applications or containers on Intel SGX [7,10,33,35,40]. A common feature across these proposals is the use of a library OS running in-enclave. By design, a library OS provides a subset of OS features, and so can only run some containers without modification. In addition, the application\u2019s network interface, the interface between the library OS, and\nthe actual out-of-enclave OS are security boundaries. This has a performance impact, as the library OS must provide a secure communication mechanism to the out-of-enclave OS and validate all data that crosses the boundary. In contrast, Parma provides an actual OS in-enclave, can run unmodified containers, and only the application\u2019s network interface is a security boundary. Additionally, Parma provides the inductive proof of all future states via the attested execution policy.\nHecate [21] uses AMD VM privilege levels (VMPLs) to run a nested hypervisor and a guest OS within the same AMD SEV-SNP isolated VM. This allows an unmodified guest OS to be run as a confidential VM, modulo kernel code integrity. However, Hecate does not address attestation, file system integrity and confidentiality, or execution policy. In addition, Hecate allows guest OS administrators full access to the VM.\nBrasser et al. have concurrently proposed TCX, a collection of trusted container extensions for running containers securely within hardware-isolated utility VMs on AMD SEV processors [12]. TCX relies (a) on a root VM (akin to the SGX quoting enclave) for bootstrapping the utility VM, thus increasing the TCB, and (b) on a secure channel established between the container owner and the utility VM for preventing untrusted entities from submitting container commands to the VM. The latter design choice does not support the CaaS deployment model, wherein the container owner and cloud service provider personas are different, thus requiring that the CSP submit container commands to the utility VM. In addition, the container owner is in the TCB. The end-user of the confidential container (e.g., a bank customer, a patient submitting medical data to their doctor) has no attestation over what container commands have been or will be run.\nSEVGuard explores running user-mode applications on guest SEV-protected VMs without a guest-side kernel component [32]. SEVGuard relies on an existing kernel virtualization API for interaction with host kernel features and provides\nsupport for calling shared libraries on the host. While SEVGuard offers a low TCB, it is vulnerable to attacks on the kernel virtualization API and shared libraries and does not provide secure persistent storage."
        },
        {
            "heading": "6 Future Work",
            "text": "While Parma provides a solid foundation for confidential containers, there are some limitations to this technique which invite future investigation."
        },
        {
            "heading": "6.1 Trusted Computing Base",
            "text": "Parma reduces the TCB by removing the need to trust the host, the hypervisor, and the CSP, but it could be smaller. In particular, by trusting the UVM we necessarily import the UVM OS (e.g., Linux) into the TCB, as well as the standard libraries needed to implement other elements like the guest agent. However, much of that code is entirely vestigial in the context of providing a container runtime. One potential line of inquiry would be to explore ways of reducing this aspect of the UVM to the smallest possible kernel and the barest necessities needed by the guest agent, runc and other tools to further reduce the attack surface."
        },
        {
            "heading": "6.2 Policy Flexibility",
            "text": "One downside of having an execution policy which is measured during initialization and then subsequently used for attestation-based security operations is that the release policies will necessarily be tied to a fixed version of the execution policy. If container images need to change, e.g., due to necessary security updates upon discovery of a vulnerability, it requires not only an update to the execution policy but also an update to all release policies as well. In many scenarios this is a desirable property, but users may want to choose to loosen how the policy defines which actions it allows. A promising area of future research would be to find a manner in which to provide this flexibility without sacrificing the post-verifiable inductive proof over the state of the container group which Parma provides."
        },
        {
            "heading": "6.3 Writeable Filesystems Freshness",
            "text": "Parma relies on dm-integrity for block-level integrity of writeable filesystems. While dm-integrity provides integrity protection based on authentication tags, the latter are vulnerable to replay attacks and do not provide any freshness guarantees. Freshness could be provided using update-able integrity trees (e.g., Merkle Trees) but at a huge latency and bandwidth overhead when a data block (i.e., leaf block in the tree) is updated due to a chain of updates to all intermediate blocks lying with the root-leaf path. A promising area of future research would\nbe to explore security-performance trade-offs for writeable filesystems with freshness guarantees."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper we have introduced Parma, a novel method for providing confidential computation for containerized workflows via the introduction of an attested execution policy. Further, we have demonstrated that Parma adds less than 1% additional performance overhead beyond that added by the underlying TEE (i.e., AMD SEV-SNP). We also outline how the security properties of the system provide an inductive proof over the future state of the container group rooted in the attestation report. This provides the ability (via remote attestation) for external third-parties to securely communicate with containers, enabling a wide range of containerized workflows which require confidential access to secure data.\nAvailability\nThe open source implementation of Parma is available as part of the hcsshim system (https://github.com/microsoft/ hcsshim/tree/main/pkg/securitypolicy) and is the technology which enables Confidential Azure Container Instances."
        },
        {
            "heading": "Acknowledgements",
            "text": "Thanks to Istvan Haller for help with the SPEC2017 benchmark and helpful conversations."
        }
    ],
    "title": "Parma: Confidential Containers via Attested Execution Policies",
    "year": 2023
}