{
    "abstractText": "The accuracy and interpretability of a (non-life) insurance pricing model are essential qualities to ensure fair and transparent premiums for policy-holders, that reflect their risk. In recent years, classification and regression trees (CARTs) and their ensembles have gained popularity in the actuarial literature, since they offer good prediction performance and are relatively easy to interpret. In this paper, we introduce Bayesian CART models for insurance pricing, with a particular focus on claims frequency modelling. In addition to the common Poisson and negative binomial (NB) distributions used for claims frequency, we implement Bayesian CART for the zero-inflated Poisson (ZIP) distribution to address the difficulty arising from the imbalanced insurance claims data. To this end, we introduce a general MCMC algorithm using data augmentation methods for posterior tree exploration. We also introduce the deviance information criterion (DIC) for tree model selection. The proposed models are able to identify trees which can better classify the policy-holders into risk groups. Simulations and real insurance data will be used to illustrate the applicability of these models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yaojun Zhang"
        },
        {
            "affiliations": [],
            "name": "Lanpeng Ji"
        },
        {
            "affiliations": [],
            "name": "Georgios Aivaliotis"
        },
        {
            "affiliations": [],
            "name": "Charles Taylor"
        }
    ],
    "id": "SP:7c9d463e666b8fea951b897996797384ea0ffc56",
    "references": [
        {
            "authors": [
                "E. Ohlsson",
                "B. Johansson"
            ],
            "title": "Non-life Insurance Pricing with Generalized Linear Models",
            "year": 2010
        },
        {
            "authors": [
                "M. Denuit",
                "X. Mar\u00e9chal",
                "S. Pitrebois",
                "J.-F"
            ],
            "title": "Walhin, Actuarial Modelling of Claim Counts: Risk Classification, Credibility and Bonus-malus Systems",
            "year": 2007
        },
        {
            "authors": [
                "R. Henckaerts",
                "K. Antonio",
                "M. Clijsters",
                "R. Verbelen"
            ],
            "title": "A data driven binning strategy for the construction of insurance tariff classes",
            "venue": "Scandinavian Actuarial Journal, vol. 2018, no. 8, pp. 681\u2013705, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "R. Henckaerts",
                "M.-P. C\u00f4t\u00e9",
                "K. Antonio",
                "R. Verbelen"
            ],
            "title": "Boosting insights in insurance tariff plans with tree-based machine learning methods",
            "venue": "North American Actuarial Journal, vol. 25, no. 2, pp. 255\u2013285, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J.A. Nelder",
                "R.W. Wedderburn"
            ],
            "title": "Generalized linear models",
            "venue": "Journal of the Royal Statistical Society: Series A (General), vol. 135, no. 3, pp. 370\u2013384, 1972.",
            "year": 1972
        },
        {
            "authors": [
                "C. Blier-Wong",
                "H. Cossette",
                "L. Lamontagne",
                "E. Marceau"
            ],
            "title": "Machine learning in P&C insurance: A review for pricing and reserving",
            "venue": "Risks, vol. 9, no. 1, p. 4, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Wuthrich",
                "M. Merz"
            ],
            "title": "Statistical Foundations of Actuarial Learning and its Applications. Springer Actuarial, Open Access, https://link.springer.com/book/10.1007/978-3-031-12409-9, Available at SSRN: https://ssrn.com/abstract=3822407, 2022",
            "year": 2022
        },
        {
            "authors": [
                "M.V. Wuthrich",
                "C. Buser"
            ],
            "title": "Data Analytics for Non-Life Insurance Pricing (January 9, 2023)",
            "venue": "Available at SSRN: https://ssrn.com/abstract=2870308,",
            "year": 2022
        },
        {
            "authors": [
                "M. Denuit",
                "A. Charpentier",
                "J. Trufin"
            ],
            "title": "Autocalibration and Tweedie-dominance for insurance pricing with machine learning",
            "venue": "Insurance: Mathematics and Economics, vol. 101, pp. 485\u2013497, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M.V. W\u00fcthrich"
            ],
            "title": "Bias regularization in neural network models for general insurance pricing",
            "venue": "European Actuarial Journal, vol. 10, no. 1, pp. 179\u2013202, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Quan"
            ],
            "title": "Insurance Analytics with Tree-Based Models",
            "venue": "PhD thesis, University of Connecticut,",
            "year": 2019
        },
        {
            "authors": [
                "C. Hu",
                "Z. Quan",
                "W.F. Chong"
            ],
            "title": "Imbalanced learning for insurance using modified loss functions in tree-based models",
            "venue": "Insurance: Mathematics and Economics, vol. 106, pp. 13\u201332, 2022. 42",
            "year": 2022
        },
        {
            "authors": [
                "S. Meng",
                "Y. Gao",
                "Y. Huang"
            ],
            "title": "Actuarial intelligence in auto insurance: Claim frequency modeling with driving behavior features and improved boosted trees",
            "venue": "Insurance: Mathematics and Economics, vol. 106, pp. 115\u2013127, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Lindholm",
                "F. Lindskog",
                "J. Palmquist"
            ],
            "title": "Local bias adjustment, duration-weighted probabilities, and automatic construction of tariff cells",
            "venue": "Duration-Weighted Probabilities, and Automatic Construction of Tariff Cells (October 24, 2022), 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H.A. Chipman",
                "E.I. George",
                "R.E. McCulloch"
            ],
            "title": "Bayesian CART model search",
            "venue": "Journal of the American Statistical Association, vol. 93, no. 443, pp. 935\u2013948, 1998.",
            "year": 1998
        },
        {
            "authors": [
                "D.G. Denison",
                "B.K. Mallick",
                "A.F. Smith"
            ],
            "title": "A Bayesian CART algorithm",
            "venue": "Biometrika, vol. 85, no. 2, pp. 363\u2013377, 1998.",
            "year": 1998
        },
        {
            "authors": [
                "A.R. Linero"
            ],
            "title": "A review of tree-based Bayesian methods",
            "venue": "Communications for Statistical Applications and Methods, vol. 24, no. 6, pp. 543\u2013559, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "H.A. Chipman",
                "E.I. George",
                "R.E. McCulloch"
            ],
            "title": "BART: Bayesian additive regression trees",
            "venue": "The Annals of Applied Statistics, vol. 4, no. 1, pp. 266\u2013298, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "E.B. Prado",
                "A.C. Parnell",
                "K. Murphy",
                "N. McJames",
                "A. O\u2019Shea",
                "R.A. Moral"
            ],
            "title": "Accounting for shared covariates in semi-parametric Bayesian additive regression trees",
            "venue": "arXiv preprint arXiv:2108.07636, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J.S. Murray"
            ],
            "title": "Log-linear Bayesian additive regression trees for multinomial logistic and count regression models",
            "venue": "Journal of the American Statistical Association, vol. 116, no. 534, pp. 756\u2013 769, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Hill",
                "A. Linero",
                "J. Murray"
            ],
            "title": "Bayesian additive regression trees: A review and look forward",
            "venue": "Annual Review of Statistics and its Application, vol. 7, pp. 251\u2013278, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A.R. Linero",
                "Y. Yang"
            ],
            "title": "Bayesian regression tree ensembles that adapt to smoothness and sparsity",
            "venue": "Journal of the Royal Statistical Society. Series B (Statistical Methodology), vol. 80, no. 5, pp. 1087\u20131110, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "V. Rockov\u00e1",
                "S. Van der Pas"
            ],
            "title": "Posterior concentration for Bayesian regression trees and forests",
            "venue": "Annals of Statistics, vol. 48, no. 4, pp. 2108\u20132131, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A.R. Linero",
                "D. Sinha",
                "S.R. Lipsitz"
            ],
            "title": "Semiparametric mixed-scale models using shared Bayesian forests",
            "venue": "Biometrics, vol. 76, no. 1, pp. 131\u2013144, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S.C. Lee"
            ],
            "title": "Delta boosting implementation of negative binomial regression in actuarial pricing",
            "venue": "Risks, vol. 8, no. 1, p. 19, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S.C. Lee"
            ],
            "title": "Addressing imbalanced insurance data through zero-inflated Poisson regression with boosting",
            "venue": "ASTIN Bulletin: The Journal of the IAA, vol. 51, no. 1, pp. 27\u201355, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "X.-L. Meng",
                "D.A. Van Dyk"
            ],
            "title": "Seeking efficient data augmentation schemes via conditional and marginal augmentation",
            "venue": "Biometrika, vol. 86, no. 2, pp. 301\u2013320, 1999. 43",
            "year": 1999
        },
        {
            "authors": [
                "D.A. Van Dyk",
                "X.-L. Meng"
            ],
            "title": "The art of data augmentation",
            "venue": "Journal of Computational and Graphical Statistics, vol. 10, no. 1, pp. 1\u201350, 2001.",
            "year": 2001
        },
        {
            "authors": [
                "D.J. Spiegelhalter",
                "N.G. Best",
                "B.P. Carlin",
                "A. Van Der Linde"
            ],
            "title": "Bayesian measures of model complexity and fit",
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 64, no. 4, pp. 583\u2013639, 2002.",
            "year": 2002
        },
        {
            "authors": [
                "H.A. Chipman",
                "E.I. George",
                "R.E. McCulloch"
            ],
            "title": "Bayesian treed models",
            "venue": "Machine Learning, vol. 48, no. 1, pp. 299\u2013320, 2002.",
            "year": 2002
        },
        {
            "authors": [
                "H. Chipman",
                "E. George",
                "R. McCulloch"
            ],
            "title": "Bayesian treed generalized linear models",
            "venue": "Bayesian Statistics, vol. 7, pp. 323\u2013349, 2003.",
            "year": 2003
        },
        {
            "authors": [
                "E.I. George"
            ],
            "title": "Bayesian model selection",
            "venue": "Encyclopedia of Statistical Sciences Update, vol. 3, 1998.",
            "year": 1998
        },
        {
            "authors": [
                "A.R. Linero"
            ],
            "title": "Bayesian regression trees for high-dimensional prediction and variable selection",
            "venue": "Journal of the American Statistical Association, vol. 113, no. 522, pp. 626\u2013636, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "E. Saha"
            ],
            "title": "Theory of posterior concentration for generalized bayesian additive regression trees",
            "venue": "arXiv preprint arXiv:2304.12505, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Y. Wu",
                "H. Tjelmeland",
                "M. West"
            ],
            "title": "Bayesian CART: Prior specification and posterior simulation",
            "venue": "Journal of Computational and Graphical Statistics, vol. 16, no. 1, pp. 44\u201366, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "J. Bleich",
                "A. Kapelner",
                "E.I. George",
                "S.T. Jensen"
            ],
            "title": "Variable selection for BART: An application to gene regulation",
            "venue": "The Annals of Applied Statistics, vol. 8, no. 3, pp. 1750\u20131781, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "Y. Liu",
                "V. Ro\u010dkov\u00e1",
                "Y. Wang"
            ],
            "title": "Variable selection with ABC bayesian forests",
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 83, no. 3, pp. 453\u2013481, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "B.P. Kindo",
                "H. Wang",
                "E.A. Pe\u00f1a"
            ],
            "title": "Multinomial probit Bayesian additive regression trees",
            "venue": "Stat, vol. 5, no. 1, pp. 119\u2013131, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "P.J. Green"
            ],
            "title": "Reversible jump MCMC computation and Bayesian model determination",
            "venue": "Biometrika, vol. 82, pp. 711\u2013732, 1995.",
            "year": 1995
        },
        {
            "authors": [
                "M.T. Pratola"
            ],
            "title": "Efficient Metropolis\u2013Hastings proposal mechanisms for Bayesian regression tree models",
            "venue": "Bayesian Analysis, vol. 11, no. 3, pp. 885\u2013911, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "A. Kapelner",
                "J. Bleich"
            ],
            "title": "BartMachine: Machine learning with Bayesian additive regression trees",
            "venue": "arXiv preprint arXiv:1312.2171, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "M.A. Tanner",
                "W.H. Wong"
            ],
            "title": "The calculation of posterior distributions by data augmentation",
            "venue": "Journal of the American Statistical Association, vol. 82, no. 398, pp. 528\u2013540, 1987.",
            "year": 1987
        },
        {
            "authors": [
                "G. Celeux",
                "F. Forbes",
                "C.P. Robert",
                "D.M. Titterington"
            ],
            "title": "Deviance information criteria for missing data models",
            "venue": "Bayesian Aanalysis, vol. 1, no. 4, pp. 651\u2013673, 2006. 44",
            "year": 2006
        },
        {
            "authors": [
                "A. Gelman",
                "J. Hwang",
                "A. Vehtari"
            ],
            "title": "Understanding predictive information criteria for Bayesian models",
            "venue": "Statistics and Computing, vol. 24, no. 6, pp. 997\u20131016, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "D.J. Spiegelhalter",
                "N.G. Best",
                "B.P. Carlin",
                "A. Van der Linde"
            ],
            "title": "The deviance information criterion: 12 years on",
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 76, no. 3, pp. 485\u2013493, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "S. Watanabe",
                "M. Opper"
            ],
            "title": "Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theory",
            "venue": "Journal of Machine Learning Research, vol. 11, no. 12, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "M. Zhou",
                "L. Li",
                "D. Dunson",
                "L. Carin"
            ],
            "title": "Lognormal and gamma mixed negative binomial regression",
            "venue": "Proceedings of the International Conference on Machine Learning. International Conference on Machine Learning, NIH Public Access, vol. 2012, 2012, p. 1343.",
            "year": 2012
        },
        {
            "authors": [
                "J. Rodrigues"
            ],
            "title": "Bayesian analysis of zero-inflated distributions",
            "venue": "Communications in Statistics- Theory and Methods, vol. 32, no. 2, pp. 281\u2013289, 2003.",
            "year": 2003
        },
        {
            "authors": [
                "J. Diebolt",
                "C.P. Robert"
            ],
            "title": "Estimation of finite mixture distributions through Bayesian sampling",
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological), vol. 56, no. 2, pp. 363\u2013375, 1994.",
            "year": 1994
        },
        {
            "authors": [
                "M.V. W\u00fcthrich"
            ],
            "title": "The balance property in neural network modelling",
            "venue": "Statistical Theory and Related Fields, vol. 6, no. 1, pp. 1\u20139, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Naya",
                "J.I. Urioste",
                "Y.-M. Chang",
                "M. Rodrigues-Motta",
                "R. Kremer",
                "D. Gianola"
            ],
            "title": "A comparison between Poisson and zero-inflated Poisson regression models with an application to number of black spots in Corriedale sheep",
            "venue": "Genetics Selection Evolution, vol. 40, pp. 1\u201316, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "T. Therneau",
                "B. Atkinson"
            ],
            "title": "Rpart: Recursive partitioning and rregression trees, R package",
            "year": 2019
        },
        {
            "authors": [
                "M.A. Wolny\u2013Dominiak"
            ],
            "title": "Trzesiok, Insurancedata: A collection of insurance datasets useful in risk classification in non-life insurance, R package version",
            "year": 2014
        },
        {
            "authors": [
                "H. Chipman",
                "E. George",
                "R. Hahn",
                "R. McCulloch",
                "M. Pratola",
                "R. Sparapani"
            ],
            "title": "Bayesian additive regression trees, computational approaches",
            "venue": "Wiley StatsRef: Statistics Reference Online, pp. 1\u201323, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "J. He",
                "S. Yalov",
                "P.R. Hahn"
            ],
            "title": "XBART: Accelerated Bayesian additive regression trees",
            "venue": "The 22nd International Conference on Artificial Intelligence and Statistics, PMLR, 2019, pp. 1130\u20131138.",
            "year": 2019
        },
        {
            "authors": [
                "R. Sparapani",
                "C. Spanbauer",
                "R. McCulloch"
            ],
            "title": "Nonparametric machine learning and efficient computation with Bayesian additive regression trees: The bart r package",
            "venue": "Journal of Statistical Software, vol. 97, pp. 1\u201366, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "N. Arsov",
                "M. Pavlovski",
                "L. Kocarev"
            ],
            "title": "Stability of decision trees and logistic regression",
            "venue": "Preprint, https://arxiv.org/pdf/1903.00816.pdf, 2019. 45",
            "year": 1903
        },
        {
            "authors": [
                "H. Chipman",
                "R.E. McCulloch"
            ],
            "title": "Hierarchical priors for bayesian cart shrinkage",
            "venue": "Statistics and Computing, vol. 10, pp. 17\u201324, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "H.A. Chipman",
                "E.I. George",
                "R.E. McCulloch"
            ],
            "title": "Managing multiple models",
            "venue": "International Workshop on Artificial Intelligence and Statistics, PMLR, 2001, pp. 41\u201348.",
            "year": 2001
        },
        {
            "authors": [
                "M. Banerjee",
                "Y. Ding",
                "A.-M. Noone"
            ],
            "title": "Identifying representative trees from ensembles",
            "venue": "Statistics in Medicine, vol. 31, no. 15, pp. 1601\u20131616, 2012. 46",
            "year": 2012
        }
    ],
    "sections": [
        {
            "text": "The accuracy and interpretability of a (non-life) insurance pricing model are essential qualities to ensure fair and transparent premiums for policy-holders, that reflect their risk. In recent years, classification and regression trees (CARTs) and their ensembles have gained popularity in the actuarial literature, since they offer good prediction performance and are relatively easy to interpret. In this paper, we introduce Bayesian CART models for insurance pricing, with a particular focus on claims frequency modelling. In addition to the common Poisson and negative binomial (NB) distributions used for claims frequency, we implement Bayesian CART for the zero-inflated Poisson (ZIP) distribution to address the difficulty arising from the imbalanced insurance claims data. To this end, we introduce a general MCMC algorithm using data augmentation methods for posterior tree exploration. We also introduce the deviance information criterion (DIC) for tree model selection. The proposed models are able to identify trees which can better classify the policy-holders into risk groups. Simulations and real insurance data will be used to illustrate the applicability of these models.\nKeywords: Bayesian CART; claims frequency; DIC; Insurance pricing; MCMC; negative\nbinomial distribution; zero-inflated Poisson distribution."
        },
        {
            "heading": "1 Introduction",
            "text": "An insurance policy refers to an agreement between an insurance company (the insurer) and a policy-holder (the insured), in which the insurer promises to charge the insured a certain fee for some unpredictable losses of the customer within a period of time, usually one year. The charged fee is called a premium which includes a pure premium and other loadings such as operational costs. For each policy, the pure premium is determined by multiple explanatory variables (such as characteristics of the policy-holders, the insured objects, the geographical region, etc.), also called risk factors [1]. The premium charged reflects the customer\u2019s degree of risk; a higher premium suggests a potential higher risk, and vice versa. Therefore, it is necessary to use risk factors to classify policy-holders\n\u2217mmyz@leeds.ac.uk \u2020l.ji@leeds.ac.uk \u2021G.Aivaliotis@leeds.ac.uk \u00a7c.c.taylor@leeds.ac.uk\nar X\niv :2\n30 3.\n01 92\n3v 3\n[ st\nat .M\nL ]\n1 D\nec 2\nwith similar risk profiles into the same tariff class. The insureds in the same group, all having similar risk characteristics, will pay the same reasonable premium. The process of constructing these tariff classes is also known as risk classification; see, e.g., [2, 3]. In the basic formula of non-life insurance pricing, the pure premium is obtained by multiplying the expected claims frequency with the conditional expectation of severity, assuming independence between frequency and severity; see, e.g., [4]. Hence, modelling the claims frequency represents an essential first step in non-life insurance pricing. In this paper, we propose efficacious approaches (namely, Bayesian CARTs or BCART models) to analyze imbalanced insurance claims frequency data.\nDue to its flexibility in modelling a large number of distributions in the exponential family, generalized linear models (GLMs), developed in [5], have been the industry-standard predictive models for insurance pricing [2, 6]. Explanatory variables enter a GLM through a linear predictor, leading to interpretable effects of the risk factors on the response. Extensions of GLMs to generalized additive models (GAMs) to capture the nonlinear effects of risk factors sometimes offer more flexible models. However, both GLMs and GAMs often fail to identify the complex interactions among risk factors. Another popular classical method based on Bayesian statistics, the credibility method, was introduced to deal with multi-level factors and lack of data issues; see, e.g., [1, 7]. Because of the limitations of these classical statistical methods and equipped with continually developing technologies, further research has recently turned to machine learning techniques. Several machine learning methods such as neural networks, regression trees, bagging techniques, random forests and boosting machines have been introduced in the context of insurance by adopting actuarial loss distributions in these models to capture the characteristics of insurance claims. We refer to [8] for a recent literature review on this topic and [9\u201311] for more detailed discussion.\nInsurance pricing models are heavily regulated and they must meet specific requirements before being deployed in practice, which posts some challenges for machine learning methods; see [4]. Therein, it is stressed that pricing models must be transparent and easy to communicate to all the stakeholders and that the insurer has the social role of creating solidarity among the policy-holders so that the use of machine learning for pricing should in no way lead to an extreme penalization of risk or discrimination. The latter has also been noted recently in, e.g., [12, 13] where it is claimed that prediction accuracy on an individual level should not be the ultimate goal in insurance pricing; one also needs to ensure the balance property. Bearing these points in mind, researchers have concluded that tree-based models are good candidates for insurance pricing [4, 14\u201317]. More precisely, the use of CART, first introduced in [18], partitions a portfolio of policy-holders into smaller groups of homogeneous risk profiles based on some risk factors in which a constant prediction is used for each sub-group. This results in a highly transparent model and automatically induces solidarity among the policy-holders in a sub-group. Although a large number of scholars have carried out empirical and theoretical studies on the effectiveness of CART, limitations of the forward-search recursive partitioning method used in CART have been identified. In particular, the predictive performance tends to be low, and it is known to be unstable: small variations in the training set can result in greatly different trees and different predictions for the same test examples. Due to these limitations, more complex tree-based models that combine multiple trees in an ensemble have been popular in insurance prediction and pricing, but these ensemble techniques usually introduce\nadditional difficulties in model transparency. In this paper and the sequel, we propose BCART models for insurance claims prediction. Instead of making an ensemble of trees, we look for one good tree, which can improve the prediction ability whilst ensuring model transparency, by adopting a Bayesian approach applied to CART.\nBCART models were first introduced by Chipman et al. [19] and Denison et al. [20], independently. The method has two basic components, prior specification (for the tree and its terminal node parameters) and a stochastic search. The method is to obtain a posterior distribution given the prior, thus leading the stochastic search towards more promising tree models. Compared with the tree that CART generates by a greedy forward-search recursive partitioning method, the BCART model generates a much better tree by an effective Bayesian-motivated stochastic search algorithm. This has been justified by simulation examples (with Gaussian-distributed data) in the aforementioned papers. Here, we show another simulation example with Poisson-distributed data to illustrate the effectiveness of BCART. Specifically, we simulate 5,000 Poisson-distributed observations where the Poisson intensity depends on two explanatory variables (or covariates) x1 and x2 as illustrated in Figure 1. (See also Subsection 4.2.1 for a slightly more general simulation example.) It is clear from the figure that the optimal partition of the covariate space consists of four regions where the data in each region should follow a homogeneous Poisson distribution. Note that the \u201cstandard\u201d CART will not be able to find the correct partition of the data as the Poisson intensities are almost uniform for both marginal distributions (see Figure 1) and no matter how the first split is chosen, it is difficult to distinguish different Poisson intensities on the resulting subsets. In contrast, the proposed Poisson BCART can retrieve the optimal tree structure since it has the ability to explore the tree space in a global way (for example, it can modify previously chosen splits).\nSince BCART models and their ensemble version \u2013 the Bayesian Additive Regression Trees (BART) models \u2013 generally outperform other machine learning models, they have been extensively studied in the literature; see, e.g., [21\u201325] and references therein. In particular, their excellent empirical performance has also motivated works on their theoretical foundations; see [26, 27]. However, in most of these studies, the focus has been on Gaussian-distributed data, with some exceptions such as [24, 28]. It turns out that a data augmentation approach is needed when dealing with general non-Gaussian data. The existing algorithms do not seem to be directly applicable to insurance data for prediction and pricing. To cover this gap, as a first step we propose BCART models for claims frequency taking account special features of insurance data such as the high number of zeros and involvement of exposures. We refer to [29, 30] for a review of claims frequency modelling which also includes some nice analyses on exposures.\nThe main contributions of this paper are as follows:\n\u2022 We give a general MCMC algorithm for the BCART models applied to any distributed data, where a data augmentation may be needed. In doing so, we follow some ideas in [31, 32].\n\u2022 We introduce a novel model selection method for BCART models based on the deviance information criterion (DIC). Note that DIC was introduced in [33] which appeared a few\nyears after the introduction of BCART [19]. The effectiveness of this approach is illustrated by several designed simulation examples and real insurance data.\n\u2022 We implement the BCART for Poisson, NB and ZIP distributions which are not currently available in any existing R packages. In particular, we introduce two different ways of incor-\nporating exposure in the NB and ZIP models, following the lines of study in [29, 30]. The simulation examples and real insurance data analysis show the applicability of these proposed BCART models.\n\u2022 To date, Bayesian tree-based models have not attracted enough attention compared to other machine learning methods in the actuarial community. This first step of applying BCART\nfor claims frequency modelling will open the door for more sophisticated tree-based models to meet the needs of the insurance industry.\nOutline of the rest of the paper: In Section 2, we review the BCART framework which includes an extension with data augmentation and a model selection method using DIC. Section 3 introduces the notation for insurance claims frequency data and five BCART models including a Poisson model, two NB models and two ZIP models. In Section 4, we discuss the applicability of the proposed BCART models using three simulation examples and a real insurance claims dataset. Section 5 concludes the paper."
        },
        {
            "heading": "2 Bayesian CART",
            "text": "We shall briefly review the BCART framework of the seminal paper [19]. We begin with the general structure of a CART model. Consider a data set (X,y) = ( (x1, y1), (x2, y2), . . . , (xn, yn) )\u22a4 with n observations. For the i-th observation, xi = (xi1, xi2, . . . , xip) is a vector of p explanatory variables (or covariates) sampled from a space X , while yi is a response variable sampled from a space Y. For our purpose of claims frequency modelling, Y will be a set of non-negative integers.\nA CART has two main components: a binary tree T with b terminal nodes which induces a partition of the covariate space X , denoted by {A1, . . . ,Ab}, and a parameter \u03b8 = (\u03b81,\u03b82, . . . ,\u03b8b) which associates the parameter value \u03b8t with the t-th terminal node. Note that here we do not specify the dimension and range of the parameter \u03b8t which should be clear in the considered context\nbelow. If xi is located in the t-th terminal node (i.e., xi \u2208 At), then yi has a distribution f ( yi | \u03b8t ) , where f represents a parametric family indexed by \u03b8t.\nBy associating observations with the b terminal nodes in the tree T , we can represent the data set as\n(X,y) = ( (X1,y1), (X2,y2) . . . , (Xb,yb) )\u22a4 ,\nwhere yt = (yt1, . . . ytnt) \u22a4 with nt denoting the number of observations and ytj denoting the j-th observation in the t-th terminal node, andXt is an analogously defined nt\u00d7p design matrix. We shall make the typical assumption that conditionally on (\u03b8, T ), response variables within a terminal node are independent and identically distributed (IID), and they are also independent across terminal nodes. The CART model likelihood in this case will take the form\np(y | X,\u03b8, T ) = b\u220f\nt=1\nf ( yt | \u03b8t ) = b\u220f t=1 nt\u220f i=1 f ( yti | \u03b8t ) . (1)\nIt is worth noting that instead of the IID assumption within the terminal nodes more general models can be considered, see, e.g., [34, 35] and the references therein.\nGiven that (\u03b8, T ) determines a CART model, a Bayesian analysis of the problem is conducted by specifying a prior distribution p(\u03b8, T ), and inference about \u03b8 and T will be based on the joint posterior p(\u03b8, T |y) using a suitable MCMC algorithm. Since \u03b8 indexes the parametric model whose dimension depends on the number of terminal nodes of the tree, it is usually convenient to apply the relationship\np(\u03b8, T ) = p(\u03b8 | T )p(T ) (2)\nand specify the tree prior distribution p(T ) and the terminal node parameter prior distribution p(\u03b8 | T ), respectively. This strategy, introduced by [36], offers several advantages for Bayesian model selection as outlined in [19].\n2.1 Specification of tree prior p(T )\nThe prior for T has two components: a tree topology and a decision rule for each of the internal/branch nodes. We shall adopt the branching process prior for the topology of T proposed by\nChipman et al. [19]. Due to its computational effectiveness using Metropolis-Hastings (MH) search algorithms, this prior specification has been the most popular in the literature. A draw from this prior is obtained by generating, for each node at depth d (with d = 0 for the root node), two child nodes with probability\np(d) = \u03b3 (1 + d) \u2212\u03c1 , (3)\nwhere \u03b3 > 0, \u03c1 \u2265 0 are parameters controlling the structure and size of the tree. This process iterates for d = 0, 1, . . . , until we reach a depth at which all the nodes cease growing. Note that p(d) is not a probability mass function, but instead is the probability of a given node at depth d being converted to a branch node. A sufficient condition for the termination of this branching process is that \u03c1 > 0, and the case \u03c1 = 0 corresponds to the Galton-Watson process, see, e.g., [37]. We refer to [38] for further theoretical discussion of this prior. Clearly, \u03b3 controls the overall rate of branching at a node, and the larger \u03c1 becomes, the less likely that deeper nodes will branch, resulting in relatively smaller trees. In [19], some simulations about the number of terminal nodes associated with the values of the pair (\u03b3, \u03c1) are carried out, which have been used as a guidance when choosing these parameters to generate trees with a certain number of terminal nodes.\nAfter the tree topology is generated, each internal node is associated with a decision rule of the form xl < cl or xl \u2208 Cl according to whether xl is a continuous or a categorical explanatory variable, where xl is selected independently and uniformly among the available explanatory variables for each internal node, and the split value cl or split category subset Cl are selected uniformly among those available for the selected variable xl. In practice, we only consider the overall set of possible split values to be finite; if the l-th variable is continuous, the grid for the variable is either uniformly spaced or given by a collection of observed quantiles of {xil, i = 1, 2, . . . , n}. If the l-th variable is categorical, the split category subset Cl is usually selected uniformly among all possible subsets. However, this approach may not be efficient in the (Bayesian) tree search, particularly when the number of categorical levels of xl is large. Instead, we shall adopt the same treatment of categorical variables as in the traditional CART greedy search algorithm. For example, in the Poisson case this is done as follows: calculate for each available categorical level, say k, of xl in that node the empirical frequency \u03bb\u0304k(xl) and use this empirical frequency \u03bb\u0304k(xl) as a numerical replacement for the categorical level k of xl. A subset Cl will be selected uniformly based on the ordered values \u03bb\u0304k(xl).\nCertainly, the design of tree prior can be more intricate than the one proposed in [19]. There have been several alternatives discussed in the literature. In a recent contribution [27], the convergence of the posterior distribution with a near-minimax concentration rate is studied, where it is shown that the original proposal given by (3) does not decay at a fast enough rate to guarantee the optimal rate of convergence. Instead, a sufficient condition for optimality is induced by the following probability\np(d) = \u03b3d, for some 0 < \u03b3 < 1/2.\nMost recently, it is noted in [39] that the original proposal (3) can still offer better empirical solutions. We believe further theoretical and empirical studies in this direction are still needed. An alternative to the branching process prior is to specify a prior directly on the number of leaves and\na conditionally-uniform prior on the space of trees. In [20], a Poisson-distributed prior is used for the number of leaves, and then a uniform prior over valid trees (i.e., trees with no empty bottom leaves) with that number of leaves is imposed. As noticed by [40], the uniform prior over valid trees in [20] tends to produce more unbalanced trees than balanced ones. Instead, they propose a pinball prior which can generate balanced or skewed trees by adjusting a hyper-parameter. Furthermore, instead of uniformly selecting the split value, a normal distribution is used for the split value in their simulation and real data analysis in [40]. Recently, some other tree priors have also been introduced for the purpose of variable selection (particularly when p > n), see, e.g., [27, 38, 41, 42]. In [38], the author proposes a sparsity-inducing Dirichlet prior for the splitting proportions of the explanatory variables, resulting in this prior allows the model to perform a fully Bayesian variable selection. Furthermore, in [27, 42] a spike-and-tree variant is proposed by injecting one more layer on top of the prior used in [20], that is, a prior over the active set of explanatory variables.\nIn our current implementation, we adopt the uniform specification for both variable and split value in each of the internal nodes, which is natural and simple. It is also noted in [19] that it would be beneficial to incorporate expert knowledge on the prior specification (i.e., using a nonuniform prior), however, our simulation studies in Section 4.2.1 show that using the uniform prior is able to identify the correct splitting rules even in the presence of noise variables. This seems to be a consequence of the Metropolis-Hastings random search steps, which tends to not accept noise splitting variables. We refer to [41] for some relevant discussions with the same conclusion."
        },
        {
            "heading": "2.2 Specification of the terminal node parameter prior p(\u03b8 | T )",
            "text": "When choosing p(\u03b8 | T ), it is vital to realize that employing priors that allow for analytical simplification can greatly reduce the computational burden of posterior calculation and exploration. This is especially true for the choice of the form p(\u03b8 | T ) for which it is possible to analytically margin out \u03b8 to obtain the integrated likelihood\np(y | X, T ) = \u222b p(y | X,\u03b8, T )p(\u03b8 | T )d\u03b8 = b\u220f t=1 \u222b f ( yt | \u03b8t ) p(\u03b8t)d\u03b8t\n= b\u220f t=1 \u222b nt\u220f i=1 f ( yti | \u03b8t ) p(\u03b8t)d\u03b8t, (4)\nwhere in the second equality we assume that conditional on the tree T with b terminal nodes as above, the parameters \u03b8t, t = 1, 2, . . . , b, have IID priors p(\u03b8t), which is a common assumption. Examples where this integration has a closed-form expression can be found in, e.g., [19, 21], particularly for Gaussian-distributed data y. When no such priors can be found, we have to resort to the technique of data augmentation (see, e.g., [24, 28, 43]) which will be discussed later. Combining the integrated likelihood p(y | X, T ) with tree prior p(T ), allows us to calculate the posterior of T\np(T | X,y) \u221d p(y | X, T )p(T ). (5)\nWhen using MCMC to conduct Bayesian inference, T can be updated using an MH algorithm\nwith the right-hand side of (5) used to compute the acceptance ratio. These MH simulations can be used to stochastically search the posterior space over trees to determine the high posterior probability trees from which we can choose a best one. The posterior sequence for \u03b8 is then obtained using an additional Gibbs sampler. It is worth noting that by integrating out \u03b8 in (4) we avoid the possible complexities associated with reversible jumps between continuous spaces of varying dimensions [22, 44]."
        },
        {
            "heading": "2.3 Stochastic search of posterior trees and parameters",
            "text": "Starting from the root node, the MCMC algorithm for simulating a Markov chain sequence of pairs( \u03b8(1), T (1) ) , ( \u03b8(2), T (2) ) , . . . , using the posterior given in (5), is given in Algorithm 1.\nAlgorithm 1 One step of the MCMC algorithm for updating the BCART parameterized by (\u03b8, T ) Input: Data (X,y) and current values ( \u03b8(m), T (m) ) 1: Generate a candidate value T \u2217 with probability distribution q ( T (m), T \u2217\n) 2: Set the acceptance ratio \u03b1 ( T (m), T \u2217 ) = min { q ( T \u2217,T (m) ) q(T (m),T \u2217) p(y|X,T \u2217) p(y|X,T (m)) p(T \u2217) p(T (m)) , 1\n} 3: Update T (m+1) = T \u2217 with probability \u03b1 ( T (m), T \u2217 ) , otherwise, set T (m+1) = T (m)\n4: Sample \u03b8(m+1) \u223c p ( \u03b8 | T (m+1),X,y ) Output: New values ( \u03b8(m+1), T (m+1)\n) In Algorithm 1, commonly used proposals (or transitions) for q(\u00b7, \u00b7) include grow, prune, change and swap (see [19]), which are usually selected equal probability (i.e., 1/4 each). Other proposals have been suggested to improve the mixing of simulated trees, but these are often difficult to put into practice; see, e.g., [40, 45]. One of the appealing features of these four proposals is that grow and prune steps are reversible counterparts of one another and both change and swap steps are\nindependently reversible. As noticed in [19], this is very attractive for the calculation of \u03b1 ( T (m), T \u2217 ) in Algorithm 1, since there are substantial cancellations in the ratio (see also [46] for detailed calculations). In our implementation, we consider these four proposals detailed as follows:\n\u2022 Grow: Randomly select a terminal node. Split it into two new child nodes and randomly assign it a decision rule according to the prior specified in Section 2.1 until the resulting two\nchild nodes satisfy a minimum observation requirement. If no such decision rule exists, draw a new terminal node (without replacement) and try again. If no such terminal node exists, stop grow.\n\u2022 Prune: A terminal node is randomly selected. The chosen node and its sibling node are pruned into the direct parent node which then becomes a new terminal node.\n\u2022 Change: apply one of the following two types of change to a selected internal node:\n\u2013 Change1: Reassign randomly only the split value/category subset according to the prior\nspecified in Section 2.1.\n\u2013 Change2: Reassign randomly both the splitting variable and the corresponding split\nvalue/category subset according to the prior specified in Section 2.1.\nIn each of the above changes, randomly select an internal node with the reassignment selected at random from a set (without replacement) until the updated nodes satisfy the minimum observation requirement. If no such reassignment exists, draw a new internal node (without replacement) and try again. If no such internal node exists, stop change.\n\u2022 Swap: Randomly pick a parent-child pair which are both internal nodes and swap their decision rules until the updated nodes satisfy the minimum observation requirement. If no such parent-\nchild pair exists, stop swap.\nRemark 1 (a). Note that in step 4 of Algorithm 1, sampling of \u03b8(m+1) is needed only for those nodes that were involved in the proposed move from T (m) to T \u2217 and only when this move was accepted.\n(b). In comparison to [19], we apply two types of change moves as discussed in [20]. The introduction of these two types of change is helpful to improve the mixing of posterior trees, as demonstrated by our simulation study in Section 4.2.1. Moreover, it is noted that a swap between a parent-child pair with splits using the same variable is impossible. Considering this in our implementation improves the computational efficiency."
        },
        {
            "heading": "2.4 MCMC algorithm with data augmentation",
            "text": "In this section, we discuss the case where there is no obvious prior distribution p(\u03b8t) such that the integration in (4) is of closed-form, particularly, for non-Gaussian data y. In this case, we shall use a data augmentation method in implementing the MCMC algorithm. Some special cases have been discussed in [22, 24, 28, 43].\nThe term data augmentation originated from Tanner and Wong\u2019s data augmentation algorithm [47]. It is introduced purely for computational purposes and a latent variable is required so that the original distribution is the marginal distribution of the augmented one. We refer to [32] for an overview of data augmentation and relevant theory. For our purpose, we augment the data y by introducing a latent variable z = (z1, z2, . . . , zn) so that the integration in (7) below is computable for augmented data (y, z). To this end, we shall follow the idea of marginal augmentation introduced in [31] (see also [32]). In their framework, our parameter \u03b8 can be interpreted as a working parameter, and thus the integrated likelihood is given as\np(y | X, T ) = \u222b p(y, z | X, T )dz, (6)\nwhere\np(y, z | X, T ) = \u222b p(y, z | X,\u03b8, T )p(\u03b8 | T )d\u03b8 = b\u220f t=1 \u222b f ( yt, zt | \u03b8t ) p(\u03b8t)d\u03b8t\n= b\u220f t=1 \u222b nt\u220f i=1 f ( yti, zti | \u03b8t ) p(\u03b8t)d\u03b8t, (7)\nwith zt = (zt1, zt2, . . . , ztnt) defined according to the partition of X and with obvious independence assumed. Following Scheme 3 of [31] (see also Section 3 of [32]), we propose the following Algorithm\n2 to simulate a Markov chain sequence of pairs ( \u03b8(1), T (1) ) , ( \u03b8(2), T (2) ) , . . . , starting from the root node.\nAlgorithm 2 One step of the MCMC algorithm for updating the BCART parameterized by (\u03b8, T ) using data augmentation\nInput: Data (X,y) and current values ( \u03b8(m), T (m), z(m) ) 1: Generate a candidate value T \u2217 with probability distribution q ( T (m), T \u2217\n) 2: Sample z(m+1) \u223c p(z | X,y,\u03b8(m), T (m))\n3: Set the acceptance ratio \u03b1 ( T (m), T \u2217 ) = min\n{ q ( T \u2217,T (m)\n) q(T (m),T \u2217) p ( y,z(m+1)|X,T \u2217 ) p(y,z(m)|X,T (m)) p(T \u2217) p(T (m)) , 1 } 4: Update T (m+1) = T \u2217 with probability \u03b1 ( T (m), T \u2217 ) , otherwise, set T (m+1) = T (m)\n5: Sample \u03b8(m+1) \u223c p ( \u03b8 | T (m+1),X,y, z(m+1) ) Output: New values ( \u03b8(m+1), T (m+1), z(m+1) )\nNote that in some cases introducing one latent variable z is insufficient to obtain a closed-form for the integration in (7); more latent variables may be required. In that case, we can easily extend Algorithm 2 to include multivariate latent variables and use the Gibbs sampler in step 2. Clearly, the more latent variables used, the slower the convergence of the Markov chain sequence. As discussed in [32], it is an \u201cart\u201d to search for efficient data augmentation schemes. We discus this point later for the claims frequency models.\nRemark 2 Similar to Algorithm 1, in step 2 and step 5 of Algorithm 2 the sampling is needed only for those nodes that were involved in the proposed move from T (m) to T \u2217, and step 5 is needed only when this move was accepted."
        },
        {
            "heading": "2.5 Posterior tree selection and prediction",
            "text": "The MCMC algorithms described in the previous section can be used to search for desirable trees. However, as discussed in [19] and illustrated below in our analysis, the algorithms quickly converge and then move locally in that region for a long time, which occurs because proposals make local moves over a sharply peaked multimodal posterior. Instead of making long runs of search to move from one mode to another better one, we follow the idea of [19] to repeatedly restart the algorithm. As many trees are visited by each run of the algorithm, we need a method to identify those trees which are of most interest. Moreover, the structure of trees in the convergence regions is mostly determined by the hyper-parameters \u03b3, \u03c1 which also need to be chosen appropriately. In [19], the integrated likelihood p(y | X,T ) is used as a measure to choose good trees from one run of the\nalgorithm, though other measures, like residual sum of squares, could also be introduced. However, there is no discussion on how the tree prior hyper-parameters \u03b3, \u03c1 should be determined optimally. A natural way to deal with this is to use cross-validation which, however, requires repeated model fits and is very computationally expensive. In this paper, we propose to use DIC for choosing appropriate \u03b3, \u03c1, and thus introduce a three-step approach for selecting an \u201coptimal\u201d tree among those visited. To this end, we first give a definition of DIC for a Bayesian CART. We refer to [33, 48\u201350] for more detailed discussion of DIC and its extensions.\nConsider the tree T with b terminal nodes and parameters \u03b8t, t = 1, 2, . . . , b, previously defined. We first introduce DIC for each node using the standard definition, the DIC for the tree is then defined as the sum of the DIC of all terminal nodes in the tree due to the independence assumption. For node t, we call\nD(\u03b8t) = \u22122 log(f(yt | \u03b8t)) = \u22122 nt\u2211 i=1 log(f(yti | \u03b8t)) (8)\nthe deviance.\nAnalogously to Akaike\u2019s information criterion (AIC), Spiegelhalter et al. [33] proposed the DIC\nbased on the principle DIC=\u201cgoodness of fit\u201d+\u201ccomplexity\u201d, which is defined as\nDICt = D(\u03b8t) + 2pDt,\nwhere \u03b8t = Epost(\u03b8t) is the posterior mean (with Epost denoting expectation over the posterior distribution of \u03b8 given data y), and pDt is the effective number of parameters given by\npDt = D(\u03b8t)\u2212D(\u03b8t) = \u22122Epost(log(f(yt | \u03b8t))) + 2 log(f(yt | \u03b8t))\n= 2 nt\u2211 i=1 ( log(f(yti | \u03b8t))\u2212 Epost(log(f(yti | \u03b8t))) ) . (9)\nThe DIC of the tree T with b terminal nodes is then defined as\nDIC := b\u2211 t=1 DICt = D(\u03b8) + 2pD, (10)\nwhere D(\u03b8) = \u2211b t=1D(\u03b8t) and pD = \u2211b t=1 pDt are the deviance and effective number of parameters of the tree.\nNext, we introduce DIC for tree models with data augmentation. Depending on whether the latent variable z is treated as a parameter or not, there are three types of likelihoods leading to eight versions of DIC as discussed in [48]. Due to the complexity in implementing any of those eight and motivated by the idea that DIC=\u201cgoodness of fit\u201d+\u201ccomplexity\u201d, we introduce a new DIC for node t in the tree as follows\nDICt = D(\u03b8t) + 2qDt, (11)\nwhere D(\u03b8t) is the deviance defined through the data yt (as in (8)) which represents the goodness of fit, and qDt is the effective number of parameters defined through the augmented data (yt, zt) as follows\nqDt = \u22122Epost(log(f(yt, zt | \u03b8t))) + 2 log(f(yt, zt | \u03b8t))\n= 2 nt\u2211 i=1 ( log(f(yti, zti | \u03b8t))\u2212 Epost(log(f(yti, zti | \u03b8t))) ) , (12)\nwhere \u03b8t = Epost(\u03b8t), and in this case Epost denotes expectation over the posterior distribution of \u03b8 given augmented data (y, z). As we will see below, for the frequency models, qDt is approximately the dimension of \u03b8t as the sample size nt in node t tends to infinity. Similarly, the DIC of tree T with b terminal nodes is thus defined as\nDIC = D(\u03b8) + 2qD, (13)\nwhere qD = \u2211b t=1 qDt.\nRemark 3 (a). Note that DIC is defined using plug-in prediction densities f(yti | \u03b8t) in (9) (similarly f(yti, zti | \u03b8t) in (12)). More recently, a new criterion called WAIC was introduced by Watababe [51] (see also [49, 50]), where in its definition the plug-in prediction density is replaced by the full prediction density Epost(f(yti | \u03b8t)). When the explicit expression is not available, this posterior expectation is usually computed by a Monte Carlo algorithm as S\u22121 \u2211S k=1 f(yti | \u03b8k), where \u03b8k is simulated from the posterior distribution of \u03b8t. In the following section, we will see that this posterior expectation can be obtained explicitly for the Poisson model, but not for other models. It turns out that using WAIC gives the same selected model as DIC in our simulation examples. Additionally, since it involves Monte Carlo algorithm and as such could be considerably more computationally expensive, we suggest using DIC.\n(b). It is worth noting that if the independence assumption within the terminal nodes is violated (e.g., [34, 35]), the DIC may also be used as a tool for model selection but the formulation would not be of the simple summation form as in (8). We refer to [33] for examples and relevant discussions.\nNow, we are ready to introduce the three-step approach for selecting an \u201coptimal\u201d tree from the MCMC algorithms. Let ms < me be two user input integers which represent the belief that the optimal number of terminal nodes lies in [ms,me]. In practice, these can be estimated first by using some other methods, e.g., a standard CART model. The three-step approach is described in Table 1. In what follows, the tree selected by using the three-step approach will be called an \u201coptimal\u201d tree.\nRemark 4 (a). The relation between hyper-parameters (\u03b3j , \u03c1j) and the distribution of the number of terminal nodes of tree has been illustrated in [19]. It does not seem hard to set values for (\u03b3j , \u03c1j) so that the MCMC algorithms will converge to a region of trees with required j terminal nodes. It is also worth noting that the distribution of the number of terminal nodes is also affected by the data in hand, which can be seen from the calculation of the acceptance ratio in the MCMC algorithms.\nIn our simulations and real data analysis below, we have to select a relatively larger \u03c1 in order to achieve our goals.\n(b). In Step 2, the so-called data likelihood p(y | X,\u03b8, T ), rather than the integrated likelihood p(y | X, T ), is used, which is due to our interest in the fit of the parametric model to data. The simulations and real data in Section 4 indicate that these two types of likelihood show a consistency in the ordering of their values, and thus we suspect there is no big difference using either of them.\nSuppose T with b terminal nodes and parameter \u03b8 is the optimal tree obtained from the above three-step approach. For a given new x the predicted y\u0302 using this tree model is defined as\ny\u0302 | x = b\u2211\nt=1\nE(y | \u03b8t)I(x\u2208At), (14)\nwhere I(\u00b7) denotes the indicator function and {At}bt=1 is the partition of X by T .\nRemark 5 An alternative prediction given x can be defined using the full predictive density as\ny\u0302 | x = b\u2211\nt=1\nEpost(E(y | \u03b8t))I(x\u2208At). (15)\nHowever, for the frequency models the explicit expression can be found only for the Poisson case, and for other models the Monte Carlo method is needed to estimate the posterior expectation. Thus, we shall use (14) for simplicity."
        },
        {
            "heading": "3 Bayesian CART claims frequency models",
            "text": "In this section, we introduce the BCART for insurance claims frequency by specifying the response distribution in the general framework introduced in Section 2. We shall discuss three commonly used distributions in the literature to model the claim numbers, namely, Poisson, NB and ZIP distributions; see, e.g., [10, 29, 30]. To this end, we first introduce the claims data. A claims data\nset with n policy-holders can be described by (X,v,N) = ( (x1, v1, N1), . . . , (xn, vn, Nn) )\u22a4 , where xi = (xi1, . . . , xip) \u2208 X consists of rating variables (e.g., area, driver age, car brand in car insurance); Ni is the number of claims reported, and vi \u2208 (0, 1] is the exposure in yearly units which is used to quantify how long the policy-holder is exposed to risk. The goal is to explain and predict the claims information Ni based on the rating variables xi and the exposure vi for each individual policy i,\nwhich leads to the claims frequency, i.e., the number of claims filed per unit year of exposure to risk. We will discuss below how this can be done with BCART models."
        },
        {
            "heading": "3.1 Poisson model",
            "text": "Consider a tree T with b terminal nodes as discussed in Section 2. In a Poisson model, we assume\nNi | xi, vi \u223c Poi vi b\u2211 t=1 \u03bbtI(xi\u2208At)  for the i-th observation where At is a partition of X . Here we use the standard notation \u03bbt for claims frequency rather than the generic notation \u03b8t for the parameter in terminal node t. Essentially, we have specified the distribution f(yi | \u03b8t) for terminal node t (see Section 2) as\nfP ( m | \u03bbt ) = P ( Ni = m | \u03bbt ) = e\u2212\u03bbtvi(\u03bbtvi) m\nm! , m = 0, 1, 2, . . . , (16)\nfor the i-th observation such that xi \u2208 At. Note that, for simplicity, here and hereafter, the exposure vi and xi will be compressed in some notation. Based on the discussions in Section 2.2, we choose the gamma prior for \u03bbt with hyper-parameters \u03b1, \u03b2 > 0, that is,\np (\u03bbt) = \u03b2\u03b1\u03bbt\n\u03b1\u22121e\u2212\u03b2\u03bbt \u0393(\u03b1) , (17)\nwith \u0393(\u00b7) denoting the gamma function. As in Section 2, for terminal node t we define the associated data as ( Xt,vt,Nt) = ((Xt1, vt1, Nt1), . . . , (Xtnt , vtnt , Ntnt) )\u22a4 . With the above gamma prior, the integrated likelihood for terminal node t can be obtained as\npP ( Nt | Xt,vt ) = \u222b \u221e 0 fP ( Nt | \u03bbt ) p(\u03bbt)d\u03bbt\n= \u222b \u221e 0 nt\u220f i=1 e\u2212\u03bbtvti(\u03bbtvti) Nti Nti! \u03b2\u03b1\u03bbt \u03b1\u22121e\u2212\u03b2\u03bbt \u0393(\u03b1) d\u03bbt\n= \u03b2\u03b1 \u220fnt i=1 v Nti ti\n\u0393(\u03b1) \u220fnt\ni=1Nti! \u222b \u221e 0 \u03bb \u2211nt i=1 Nti+\u03b1\u22121 t e \u2212( \u2211nt i=1 vti+\u03b2)\u03bbtd\u03bbt\n= \u03b2\u03b1 \u220fnt i=1 v Nti ti\n\u0393(\u03b1) \u220fnt\ni=1Nti!\n\u0393( \u2211nt\ni=1Nti + \u03b1) ( \u2211nt i=1 vti + \u03b2) \u2211nt i=1 Nti+\u03b1 .\n(18)\nClearly, from (18), we see that the posterior distribution of \u03bbt, conditional on Nt, is given by\n\u03bbt | Nt \u223c Gamma  nt\u2211 i=1 Nti + \u03b1, nt\u2211 i=1 vti + \u03b2  . (19)\nThe integrated likelihood for the tree T is thus given by\npP ( N | X,v, T ) = b\u220f t=1 pP ( Nt | Xt,vt ) . (20)\nNext, we discuss the DIC for this tree, focusing on DICt for terminal node t. First, we have\nD (\u03bbt) = \u22122 nt\u2211 i=1 log fP(Nti | \u03bbt) = \u22122 nt\u2211 i=1 ( \u2212\u03bbtvti +Nti log (\u03bbtvti)\u2212 log (Nti!) ) , (21)\nand by (19) we get the posterior mean for \u03bbt as\n\u03bbt = Epost(\u03bbt) = \u2211nt i=1Nti + \u03b1\u2211nt i=1 vti + \u03b2 . (22)\nFurthermore, we derive that\nD (\u03bbt) = Epost ( D (\u03bbt) ) = 2\nnt\u2211 i=1 vtiEpost (\u03bbt)\u2212 2 nt\u2211 i=1 NtiEpost ( log (\u03bbt) + log (vti) ) + 2 nt\u2211 i=1 log (Nti!)\n= 2 (\u2211nt i=1Nti + \u03b1\u2211nt i=1 vti + \u03b2 ) nt\u2211 i=1 vti \u2212 2 \u03c8  nt\u2211 i=1 Nti + \u03b1 \u2212 log  nt\u2211 i=1 vti + \u03b2   nt\u2211 i=1 Nti\n\u22122 nt\u2211 i=1 Nti log (vti) + 2 nt\u2211 i=1 log (Nti!) , (23)\nwhere we have used the fact that\nEpost ( log (\u03bbt) ) = \u03c8  nt\u2211 i=1 Nti + \u03b1 \u2212 log  nt\u2211 i=1 vti + \u03b2  , with \u03c8(x) = \u0393\u2032(x)/\u0393(x) being the digamma function. Using (21)\u2013(23), we obtain the effective number of parameters for terminal node t as\npDt = D(\u03bbt)\u2212D(\u03bbt)\n= 2 log  nt\u2211\ni=1\nNti + \u03b1\n\u2212 \u03c8  nt\u2211\ni=1\nNti + \u03b1   nt\u2211\ni=1\nNti,\nand DICt = D ( \u03bbt ) + 2pDt\n= 2 (\u2211nt i=1Nti + \u03b1\u2211nt i=1 vti + \u03b2 ) nt\u2211 i=1 vti \u2212 2 nt\u2211 i=1 Nti log(\u2211nti=1Nti + \u03b1\u2211nt i=1 vti + \u03b2 ) + log (vti) + 2 nt\u2211 i=1 log (Nti!)\n+ 4 log  nt\u2211\ni=1\nNti + \u03b1\n\u2212 \u03c8  nt\u2211\ni=1\nNti + \u03b1   nt\u2211\ni=1\nNti.\nThen the DIC of tree T is obtained using (10).\nRemark 6 Since \u03c8(x) = log(x) \u2212 12x (1 + o(x)), as x \u2192 \u221e, we immediately see that pDt \u2192 1 as nt \u2192 \u221e. This explains the name of effective number of parameters in the Bayesian framework, as 1 is the number of parameters in the terminal node t for Poisson model if a flat prior is assumed for \u03bbt.\nWith the above (19)\u2013(20) and DIC obtained, we can use the three-step approach proposed in Section 2.5 to search for an optimal tree, where (19) and (20) should be used in step 4 and step 2, respectively, in Algorithm 1. Given an optimal tree, the estimated claims frequency \u03bbt in terminal node t can be given by the posterior mean in (22), using (14). It is worth noting that we can obtain the same estimate by using (15) instead."
        },
        {
            "heading": "3.2 Negative binomial models",
            "text": "The NB distribution, a member of mixed Poisson family, offers an effective way to handle overdispersed insurance claims frequency data where excessive zeros are common.\nConsider the tree T with b terminal nodes as before. In the NB model, we assume that Nti | Xti, vti follows a NB distribution for all terminal nodes, t = 1, . . . , b. There are different ways to parameterize the NB distribution, particularly with the exposure, see, e.g., [10, 29]. We shall discuss two models below."
        },
        {
            "heading": "3.2.1 Negative binomial model 1 (NB1)",
            "text": "We first adopt the most common parameterization of the NB distribution, see, e.g., [24]. That is, for terminal node t,\nfNB1(m | \u03bat, \u03bbt) = P (Nti = m | \u03bat, \u03bbt)\n= \u0393(m+ \u03bat)\n\u0393(\u03bat)m!\n( \u03bat\n\u03bat + \u03bbtvti )\u03bat ( \u03bbtvti \u03bat + \u03bbtvti )m , m = 0, 1, . . . , (24)\nwhere \u03bat, \u03bbt > 0. It is easy to show that the mean and variance of Nti are given by\nE(Nti | \u03bat, \u03bbt) = \u03bbtvti, Var(Nti | \u03bat, \u03bbt) = \u03bbtvti ( 1 +\n\u03bbtvti \u03bat\n) . (25)\nThe degree of over-dispersion in relation to the Poisson is controlled by the additional parameter \u03bat in the NB model, which converges to the Poisson model as \u03bat \u2192 \u221e. In NB regression, the lack of simple and efficient algorithms for posterior computation has seriously limited routine applications of Bayesian approaches. Recent studies make Bayesian approaches appealing by introducing data augmentation techniques; see, e.g., [24, 52]. In order to save on total computational time of the algorithm and avoid the difficulty of finding an appropriate prior for \u03bat with corresponding data augmentation, we shall treat the parameter \u03bat as known in the Bayesian framework which can be estimated upfront by using, e.g., the moment matching method. However, in line with the Poisson model, we shall treat \u03bbt as uncertain and use a gamma prior with corresponding data augmentation. Based on the formulas given in (25), we can estimate the parameter \u03bat, using the moment matching method, see, e.g., Chapter 2 of [6] as follows\n\u03ba\u0302t = \u03bb\u03022t V\u0302 2t \u2212 \u03bb\u0302t 1 nt \u2212 1  nt\u2211 i=1 vti \u2212 \u2211nt i=1 v 2 ti\u2211nt i=1 vti  , (26) where\nV\u0302 2t = 1\nnt \u2212 1 nt\u2211 i=1 vti ( Nti vti \u2212 \u03bb\u0302t )2 , \u03bb\u0302t = \u2211nt i=1Nti\u2211nt i=1 vti . (27)\nNext, introducing a latent variable \u03bet = (\u03bet1, \u03bet2, . . . , \u03betnt) \u2208 (0,\u221e)nt , we can define a data augmented likelihood for the i-th data instance in terminal node t as\nfNB1 ( Nti, \u03beti | \u03ba\u0302t, \u03bbt ) = (\u03bbtvti) Nti e\u2212\u03beti\u03bbtvti \u03ba\u0302\u03ba\u0302tt \u03be \u03ba\u0302t+Nti\u22121 ti e \u2212\u03beti\u03ba\u0302t\n\u0393(\u03ba\u0302t)Nti! . (28)\nIt is easily checked that integrating over \u03beti \u2208 (0,\u221e) in (28) yields the marginal distribution (24). Further, we see that \u03beti, given data Nti and parameters, is gamma distributed, i.e.,\n\u03beti | Nti, \u03ba\u0302t, \u03bbt \u223c Gamma (\u03ba\u0302t +Nti, \u03ba\u0302t + \u03bbtvti) . (29)\nGiven the data augmented likelihood in (28), the estimated parameter \u03ba\u0302t using (26), and a conjugate gamma prior for \u03bbt with hyper-parameters \u03b1, \u03b2 > 0 (cf. (17)), we can derive the integrated augmented likelihood for the terminal node t as follows\npNB1 ( Nt, \u03bet | Xt,vt, \u03ba\u0302t ) = \u222b \u221e 0 fNB1 ( Nt, \u03bet | \u03ba\u0302t, \u03bbt ) p(\u03bbt)d\u03bbt\n= \u222b \u221e 0 nt\u220f i=1 [ (\u03bbtvti) Nti e\u2212\u03beti\u03bbtvti \u03ba\u0302\u03ba\u0302tt \u03be \u03ba\u0302t+Nti\u22121 ti e \u2212\u03beti\u03ba\u0302t \u0393(\u03ba\u0302t)Nti! ] \u03b2\u03b1\u03bbt \u03b1\u22121e\u2212\u03b2\u03bbt \u0393(\u03b1) d\u03bbt = \u03ba\u0302\u03ba\u0302tt \u03b2 \u03b1\n\u0393(\u03ba\u0302t)\u0393(\u03b1) nt\u220f i=1 [ vNtiti Nti! \u03be\u03ba\u0302t+Nti\u22121ti e \u2212\u03beti\u03ba\u0302t ] \u0393( \u2211nt i=1Nti + \u03b1) ( \u2211nt i=1 \u03betivti + \u03b2) \u2211nt i=1 Nti+\u03b1 .\n(30)\nMoreover, from the above we see that the posterior distribution of \u03bbt given the augmented data\n(Nt, \u03bet), is given by\n\u03bbt | Nt, \u03bet \u223c Gamma  nt\u2211 i=1 Nti + \u03b1, nt\u2211 i=1 \u03betivti + \u03b2  . The integrated augmented likelihood for the tree T is thus given by\npNB1 ( N , \u03be | X,v, \u03ba\u0302, T ) = b\u220f t=1 pNB1 ( Nt, \u03bet | Xt,vt, \u03ba\u0302t ) . (31)\nNow, we discuss the DIC for this tree. Since we only consider uncertainty for \u03bb but not for \u03ba, the DIC defined in (13) cannot be adopted directly. Thus, using the idea that DIC=\u201cgoodness of fit\u201d+\u201ccomplexity\u201d, we can introduce a new DICt for terminal node t as follows\nDICt = D(\u03bbt) + 2rDt.\nHere, the goodness of fit is given by\nD(\u03bbt) = \u22122 nt\u2211 i=1 log fNB1(Nti | \u03ba\u0302t, \u03bbt),\nand the effective number of parameters rDt is given by\nrDt = 1 + 2 nt\u2211 i=1 ( log(fNB1(Nti, \u03beti | \u03ba\u0302t, \u03bbt))\u2212 Epost(log(fNB1(Nti, \u03beti | \u03ba\u0302t, \u03bbt))) ) , (32)\nwhere 1 represents the number for \u03bat and the second part is for \u03bbt,\n\u03bbt = \u2211nt i=1Nti + \u03b1\u2211nt\ni=1 \u03betivti + \u03b2 ,\nand\nEpost(log(fNB1(Nti, \u03beti | \u03ba\u0302t, \u03bbt))) = \u22122 nt\u2211 i=1 Nti log(vti) + \u03c8  nt\u2211 i=1 Nti + \u03b1 \u2212 log  nt\u2211 i=1 \u03betivti + \u03b2  + 2( \u2211nti=1Nti + \u03b1\u2211nt i=1 \u03betivti + \u03b2 ) nt\u2211 i=1 \u03betivti\n+2 nt\u2211 i=1 ( log (Nti!)\u2212 (\u03ba\u0302t +Nti \u2212 1) log (\u03beti) + \u03beti\u03ba\u0302t ) + 2(log(\u0393(\u03ba\u0302t))\u2212 \u03ba\u0302t log(\u03ba\u0302t)). (33)\nTherefore, a direct calculation shows that the effective number of parameters for terminal node t is given by\nrDt = 1 + 2 log  nt\u2211\ni=1\nNti + \u03b1\n\u2212 \u03c8  nt\u2211\ni=1\nNti + \u03b1   nt\u2211\ni=1\nNti,\nand thus DICt = D ( \u03bbt ) + 2rDt\n= \u22122 nt\u2211 i=1 Nti\nlog(vti) + log  nt\u2211\ni=1\nNti + \u03b1\n\u2212 log  nt\u2211\ni=1\n\u03betivti + \u03b2  + 2( \u2211nti=1Nti + \u03b1\u2211nt\ni=1 \u03betivti + \u03b2 ) nt\u2211 i=1 \u03betivti\n+ 2 nt\u2211 i=1 ( log (Nti!)\u2212 (\u03ba\u0302t +Nti \u2212 1) log (\u03beti) + \u03beti\u03ba\u0302t ) + 2(log(\u0393(\u03ba\u0302t))\u2212 \u03ba\u0302t log(\u03ba\u0302t))\n+ 2 + 4 log  nt\u2211\ni=1\nNti + \u03b1\n\u2212 \u03c8  nt\u2211\ni=1\nNti + \u03b1   nt\u2211\ni=1\nNti."
        },
        {
            "heading": "3.2.2 Negative binomial model 2 (NB2)",
            "text": "We now consider another parameterization of the NB distribution, see, e.g., [10, 29]. Now, for terminal node t,\nfNB2(m | \u03bat, \u03bbt) = P (Nti = m | \u03bat, \u03bbt)\n= \u0393(m+ \u03batvti)\n\u0393(\u03batvti)m!\n( \u03bat\n\u03bat + \u03bbt )\u03batvti ( \u03bbt \u03bat + \u03bbt )m , m = 0, 1, . . . . (34)\nIt is easy to show that the mean of Nti is the same as in (25), but the variance becomes\nVar(Nti | \u03bat, \u03bbt) = \u03bbtvti ( 1 +\n\u03bbt \u03bat\n) . (35)\nThis formulation yields a fixed over-dispersion of size \u03bbt/\u03bat which does not depend on the exposure vti, and thus it is sometimes preferred (see [10]) and has been judged as more effective for real insurance data analysis (see [29]).\nWe use the same way to deal with \u03bat and \u03bbt as in the previous subsection. Using the same\napproach as Chapter 2 of [6], we can estimate the parameter \u03bat as follows\n\u03ba\u0302t = \u03bb\u03022t\nV\u0302 2t \u2212 \u03bb\u0302t , (36)\nwhere V\u0302 2t and \u03bb\u0302t are given in (27). Note that this parameterization offers a simpler estimation for \u03ba\u0302t, and that \u03bb\u0302t is a minimal variance estimator; see [6].\nSimilarly as before, we can define a data augmented likelihood for the i-th data instance in\nterminal node t as\nfNB2 ( Nti, \u03beti | \u03ba\u0302t, \u03bbt ) = (\u03bbtvti) Nti e\u2212\u03beti\u03bbtvti (\u03ba\u0302tvti) \u03ba\u0302tvti\u03be\u03ba\u0302tvti+Nti\u22121ti e \u2212\u03beti\u03ba\u0302tvti\n\u0393(\u03ba\u0302tvti)Nti! . (37)\nFurther, we see that \u03beti, given data Nti and parameters, has a gamma distribution, i.e.,\n\u03beti | Nti, \u03ba\u0302t, \u03bbt \u223c Gamma (\u03ba\u0302tvti +Nti, \u03ba\u0302tvti + \u03bbtvti) . (38)\nGiven the data augmented likelihood in (37), the estimated parameter \u03ba\u0302t using (36), and a conjugate gamma prior for \u03bbt with hyper-parameters \u03b1, \u03b2 > 0, we can derive the integrated augmented likelihood for terminal node t as follows\npNB2 ( Nt, \u03bet | Xt,vt, \u03ba\u0302t ) = \u03b2\u03b1\n\u0393(\u03b1) nt\u220f i=1\n[ (\u03ba\u0302tvti)\n\u03ba\u0302tvtivNtiti \u0393(\u03ba\u0302tvti)Nti! \u03be\u03ba\u0302tvti+Nti\u22121ti e \u2212\u03beti\u03ba\u0302tvti\n] \u0393( \u2211nt\ni=1Nti + \u03b1) ( \u2211nt i=1 \u03betivti + \u03b2) \u2211nt i=1 Nti+\u03b1 .\n(39)\nFrom the above we see that the posterior distribution of \u03bbt, given the augmented data (Nt, \u03bet), is given by\n\u03bbt | Nt, \u03bet \u223c Gamma  nt\u2211 i=1 Nti + \u03b1, nt\u2211 i=1 \u03betivti + \u03b2  . The integrated augmented likelihood for the tree T is thus given by\npNB2 ( N , \u03be | X,v, \u03ba\u0302, T ) = b\u220f t=1 pNB2 ( Nt, \u03bet | Xt,vt, \u03ba\u0302t ) . (40)\nNow, we discuss the DICt for terminal node t of this tree. Similarly, as in the previous subsection, we can easily check that DICt = D ( \u03bbt ) + 2rDt\n= \u22122 nt\u2211 i=1 Nti\nlog(vti) + log  nt\u2211\ni=1\nNti + \u03b1\n\u2212 log  nt\u2211\ni=1\n\u03betivti + \u03b2  + 2( \u2211nti=1Nti + \u03b1\u2211nt\ni=1 \u03betivti + \u03b2 ) nt\u2211 i=1 \u03betivti\n+ 2 nt\u2211 i=1 ( log(\u0393(\u03ba\u0302tvti)) + log (Nti!)\u2212 \u03ba\u0302tvti log(\u03ba\u0302tvti)\u2212 (\u03ba\u0302tvti +Nti \u2212 1) log (\u03beti) + \u03beti\u03ba\u0302tvti )\n+ 2 + 4 log  nt\u2211\ni=1\nNti + \u03b1\n\u2212 \u03c8  nt\u2211\ni=1\nNti + \u03b1   nt\u2211\ni=1\nNti.\nFor the above two NB models, the DIC of tree T is obtained by using (10). With the above formulas derived in the two subsections for NB models, we can use the three-step approach proposed in Section 2.5, together with Algorithm 3, to search for an optimal tree and then obtain predictions for new data.\nRemark 7 (a). In step 4 of Algorithm 3, pNB should be understood as either pNB1 or pNB2. Similar to Algorithms 1 and 2, the sampling steps in Algorithm 3 should be done when necessary.\n(b). It is worth noting that our way of dealing with the parameter \u03ba is different from that in [24]\nAlgorithm 3 One step of the MCMC algorithm for the NB BCART parameterized by (\u03ba,\u03bb, T ) using data augmentation\nInput: Data (X,v,N) and current values ( \u03ba\u0302(m),\u03bb(m), T (m), \u03be(m) ) 1: Generate a candidate value T \u2217 with probability distribution q ( T (m), T \u2217\n) 2: Estimate \u03ba\u0302(m), using (26) (or (36)) 3: Sample \u03be(m+1) \u223c p(\u03be | X,v,N , \u03ba\u0302(m+1),\u03bb(m), T (m)), using (29) (or (38))\n4: Set the acceptance ratio \u03b1 ( T (m), T \u2217 ) = min\n{ q ( T \u2217,T (m)\n) q(T (m),T \u2217) pNB ( N ,\u03be(m+1)|X,v,\u03ba\u0302(m+1),T \u2217 ) pNB(N ,\u03be(m)|X,v,\u03ba\u0302(m),T (m)) p(T \u2217) p(T (m)) , 1 } 5: Update T (m+1) = T \u2217 with probability \u03b1 ( T (m), T \u2217 ) , otherwise, set T (m+1) = T (m)\n6: Sample \u03bb(m+1) \u223c Gamma (\u2211nt i=1Nti + \u03b1, \u2211nt i=1 \u03be (m+1) ti vti + \u03b2 ) Output: New values ( \u03ba\u0302(m+1),\u03bb(m+1), T (m+1), \u03be(m+1) )\nwhere a single \u03ba is sampled from a distribution and used for all terminal nodes. It turns out that that way of dealing with \u03ba cannot give us good estimates in our simulation examples, whereas our way of first estimating \u03ba using moment matching method for each node can give good estimates.\n(c). There are other ways to parameterize the NB distribution, see, e.g., [52]. However, it looks that these ways are normally discussed when there is no exposure involved, so we will not cover them here."
        },
        {
            "heading": "3.3 Zero-Inflated Poisson models",
            "text": "Insurance claims data normally involves a large volume of zeros. Many policy-holders incur no claims, which does not necessarily mean that they were involved in no accidents, but they are probably less risky. In this section, depending on how the exposure is embedded in the model we discuss two ZIP models to better reflect the excessive zeros, see, e.g., [30]."
        },
        {
            "heading": "3.3.1 Zero-Inflated Poisson model 1 (ZIP1)",
            "text": "For terminal node t, we use the following ZIP distribution by embedding the exposure into the Poisson part (see [24])\nfZIP1 ( m | \u00b5t, \u03bbt ) =  1 1 + \u00b5t + \u00b5t 1 + \u00b5t fP (0 | \u03bbt) m = 0,\n\u00b5t 1 + \u00b5t fP (m | \u03bbt) m = 1, 2, . . . ,\n= 1\n1 + \u00b5t I(m=0) + \u00b5t 1 + \u00b5t fP (m | \u03bbt), m = 0, 1, 2, . . . , (41)\nwhere fP(m | \u03bbt) is given as in (16), and 11+\u00b5t \u2208 (0, 1) is the probability that a zero is due to the point mass component. Note that for computational simplicity we consider a model with two parameters rather than three as in [24].\nSimilar to the NB model, a data augmentation scheme is needed for the ZIP model. To this end, we introduce two latent variables \u03d5t = (\u03d5t1, \u03d5t2, . . . , \u03d5tnt) \u2208 (0,\u221e)nt and \u03b4t = (\u03b4t1, \u03b4t2, . . . , \u03b4tnt) \u2208 {0, 1}nt , and define the data augmented likelihood for the i-th data instance in terminal node t by\nfZIP1 ( Nti, \u03b4ti, \u03d5ti | \u00b5t, \u03bbt ) = e\u2212\u03d5ti(1+\u00b5t)\n( \u00b5t (\u03bbtvti) Nti\nNti! e\u2212\u03bbtvti\n)\u03b4ti , (42)\nwhere the support of the function fZIP1 is ( {0} \u00d7 {0, 1} \u00d7 (0,\u221e) ) \u222a ( N\u00d7 {1} \u00d7 (0,\u221e) ) . This means that we impose \u03b4ti = 1 when Nti \u2208 N (i.e., Nti \u0338= 0). It can be shown that (41) is the marginal distribution of the above augmented distribution; see [24] for more details. By conditional arguments, we can also check that \u03b4ti, given data \u03d5ti, Nti = 0 and parameters, has a Bernoulli distribution, i.e.,\n\u03b4ti | \u03d5ti, Nti = 0, \u00b5t, \u03bbt \u223c Bern\n( \u00b5te \u2212\u03bbtvti\n1 + \u00b5te\u2212\u03bbtvti\n) , (43)\nand \u03b4ti = 1, given Nti > 0. Furthermore, \u03d5ti, given data \u03b4ti, Nti and parameters, has an exponential distribution, i.e.,\n\u03d5ti | \u03b4ti, Nti, \u00b5t, \u03bbt \u223c Exp (1 + \u00b5t) . (44)\nIt is noted that the augmented likelihood fZIP1 in (42) can actually be factorized as two gammatype functions parameterized by \u00b5t and \u03bbt respectively. This observation motivates us to assume independent conjugate gamma priors for \u00b5t and \u03bbt with hyper-parameters \u03b1i, \u03b2i > 0, i = 1, 2 (cf. (17)). With these gamma priors, we can derive the integrated augmented likelihood for terminal node t as follows\npZIP1 ( Nt, \u03b4t,\u03d5t | Xt,vt ) = \u222b \u221e 0 \u222b \u221e 0 fZIP1 ( Nt, \u03b4t,\u03d5t | \u00b5t, \u03bbt ) p(\u00b5t)p(\u03bbt)d\u00b5td\u03bbt\n= \u03b2\u03b111\n\u0393 (\u03b11) \u03b2\u03b122 \u0393 (\u03b12) nt\u220f i=1 ( e\u2212\u03d5tiv\u03b4tiNtiti (Nti!) \u2212\u03b4ti )\n\u00d7 \u0393 (\u2211nt i=1 \u03b4ti + \u03b11 )(\u2211nt\ni=1 \u03d5ti + \u03b21 )\u2211nt i=1 \u03b4ti+\u03b11\n\u0393 (\u2211nt i=1 \u03b4tiNti + \u03b12 )(\u2211nt\ni=1 \u03b4tivti + \u03b22 )\u2211nt i=1 \u03b4tiNti+\u03b12 .\n(45)\nMoreover, from the above we see that the posterior distributions of \u00b5t, \u03bbt given the augmented data (Nt, \u03b4t,\u03d5t) are given by\n\u00b5t | Nt, \u03b4t,\u03d5t \u223c Gamma  nt\u2211 i=1 \u03b4ti + \u03b11, nt\u2211 i=1 \u03d5ti + \u03b21  , \u03bbt | Nt, \u03b4t,\u03d5t \u223c Gamma\n nt\u2211 i=1 \u03b4tiNti + \u03b12, nt\u2211 i=1 \u03b4tivti + \u03b22  . The integrated augmented likelihood for the tree T is thus given by\npZIP1 ( N , \u03b4,\u03d5 | X,v, T ) = b\u220f t=1 pZIP1 ( Nt, \u03b4t,\u03d5t | Xt,vt ) . (46)\nNow, we discuss the DIC for this tree which can be derived as a special case of (11) with \u03b8t = (\u00b5t, \u03bbt). To this end, we first focus on the DICt of terminal node t. It follows that\nD ( \u00b5t, \u03bbt ) = \u22122 log fZIP1(Nt | \u00b5t, \u03bbt)\n= \u22122 nt\u2211 i=1 log\n( 1\n1 + \u00b5t I(Nti=0) + \u00b5t 1 + \u00b5t\n(\u03bbtvti) Nti\nNti! e\u2212\u03bbtvti\n) , (47)\nwhere\n\u00b5t = \u2211nt i=1 \u03b4ti + \u03b11\u2211nt i=1 \u03d5ti + \u03b21 , \u03bbt = \u2211nt i=1 \u03b4tiNti + \u03b12\u2211nt i=1 \u03b4tivti + \u03b22 .\nNext, since\nlog fZIP1 ( Nt, \u03b4t,\u03d5t | \u00b5t, \u03bbt ) =\nnt\u2211 i=1 ( \u2212\u03d5ti(1 + \u00b5t) + \u03b4ti log (\u00b5t) + \u03b4tiNti log (\u03bbtvti)\u2212 \u03b4ti\u03bbtvti \u2212 \u03b4ti log (Nti!) ) ,\nwe can derive that\nqDt = \u22122Epost(log fZIP1 ( Nt, \u03b4t,\u03d5t | \u00b5t, \u03bbt ) ) + 2 log fZIP1 ( Nt, \u03b4t,\u03d5t | \u00b5t, \u03bbt ) = 2 log  nt\u2211\ni=1\n\u03b4ti + \u03b11\n\u2212 \u03c8  nt\u2211\ni=1\n\u03b4ti + \u03b11   nt\u2211\ni=1\n\u03b4ti\n+2 log  nt\u2211\ni=1\n\u03b4tiNti + \u03b12\n\u2212 \u03c8  nt\u2211\ni=1\n\u03b4tiNti + \u03b12   nt\u2211\ni=1\n\u03b4tiNti. (48)\nTherefore, DICt can be obtained from (47) and (48) as\nDICt = D ( \u00b5t, \u03bbt ) + 2qDt."
        },
        {
            "heading": "3.3.2 Zero-Inflated Poisson model 2 (ZIP2)",
            "text": "For terminal node t, we use the following ZIP distribution by embedding the exposure into the zero mass part (see [30])\nfZIP2 ( m | \u00b5t, \u03bbt ) =  1 1 + \u00b5tvti + \u00b5tvti 1 + \u00b5tvti e\u2212\u03bbt m = 0,\n\u00b5tvti 1 + \u00b5tvti \u03bbNtit Nti! e\u2212\u03bbt m = 1, 2, . . . ,\n(49)\nwhere 11+\u00b5tvti \u2208 (0, 1) is the probability that a zero is due to the point mass component. This formulation stems from an intuitive inverse relationship between the exposure and the probability of zero mass. This way of embedding exposure has been justified to be more effective in [30].\nSimilar to before, we introduce two latent variables \u03d5t = (\u03d5t1, \u03d5t2, . . . , \u03d5tnt) \u2208 (0,\u221e)nt and \u03b4t = (\u03b4t1, \u03b4t2, . . . , \u03b4tnt) \u2208 {0, 1}nt , and define the data augmented likelihood for the i-th data instance in terminal node t as\nfZIP2 ( Nti, \u03b4ti, \u03d5ti | \u00b5t, \u03bbt ) = e\u2212\u03d5ti(1+\u00b5tvti)\n( \u00b5tvti\u03bb Nti t\nNti! e\u2212\u03bbt\n)\u03b4ti , (50)\nwhere the support of the function fZIP2 is ( {0} \u00d7 {0, 1} \u00d7 (0,\u221e) ) \u222a ( N\u00d7 {1} \u00d7 (0,\u221e) ) . By conditional arguments, we can also check that \u03b4ti, given data \u03d5ti, Nti = 0 and parameters, has a Bernoulli distribution, i.e.,\n\u03b4ti | \u03d5ti, Nti = 0, \u00b5t, \u03bbt \u223c Bern\n( \u00b5tvtie \u2212\u03bbt\n1 + \u00b5tvtie\u2212\u03bbt\n) , (51)\nand \u03b4ti = 1, given Nti > 0. Furthermore, \u03d5ti, given data \u03b4ti, Nti and parameters, has an exponential distribution, i.e.,\n\u03d5ti | \u03b4ti, Nti, \u00b5t, \u03bbt \u223c Exp (1 + \u00b5tvti) . (52)\nAs previously, we assume independent conjugate gamma priors for \u00b5t and \u03bbt with hyper-parameters \u03b1i, \u03b2i > 0, i = 1, 2. Given the data augmented likelihood in (50) and the above gamma priors, we can obtain the integrated augmented likelihood for terminal node t as follows\npZIP2 ( Nt, \u03b4t,\u03d5t | Xt,vt ) =\n\u03b2\u03b111 \u0393 (\u03b11) \u03b2\u03b122 \u0393 (\u03b12) nt\u220f i=1\n( e\u2212\u03d5ti ( vti Nti! )\u03b4ti)\n\u00d7 \u0393 (\u2211nt i=1 \u03b4ti + \u03b11 )(\u2211nt\ni=1 \u03d5tivti + \u03b21 )\u2211nt i=1 \u03b4ti+\u03b11\n\u0393 (\u2211nt i=1 \u03b4tiNti + \u03b12 )(\u2211nt\ni=1 \u03b4ti + \u03b22 )\u2211nt i=1 \u03b4tiNti+\u03b12 .\n(53)\nMoreover, from the above we see that the posterior distributions of \u00b5t, \u03bbt given the augmented data (Nt, \u03b4t,\u03d5t) are given by\n\u00b5t | Nt, \u03b4t,\u03d5t \u223c Gamma  nt\u2211 i=1 \u03b4ti + \u03b11, nt\u2211 i=1 \u03d5tivti + \u03b21  , \u03bbt | Nt, \u03b4t,\u03d5t \u223c Gamma\n nt\u2211 i=1 \u03b4tiNti + \u03b12, nt\u2211 i=1 \u03b4ti + \u03b22  . The integrated augmented likelihood for the tree T is thus given by\npZIP2 ( N , \u03b4,\u03d5 | X,v, T ) = b\u220f t=1 pZIP2 ( Nt, \u03b4t,\u03d5t | Xt,vt, T ) . (54)\nNow, we discuss the DICt of terminal node t. It follows that\nD ( \u00b5t, \u03bbt ) = \u22122 log fZIP2(Nt | \u00b5t, \u03bbt)\n= \u22122 nt\u2211 i=1 log  1 1 + \u00b5tvti I(Nti=0) + \u00b5tvti 1 + \u00b5tvti \u03bbt Nti Nti! e\u2212\u03bbt  , (55) where\n\u00b5t = \u2211nt i=1 \u03b4ti + \u03b11\u2211nt\ni=1 \u03d5tivti + \u03b21 , \u03bbt =\n\u2211nt i=1 \u03b4tiNti + \u03b12\u2211nt\ni=1 \u03b4ti + \u03b22 .\nNext, since\nlog fZIP2 ( Nt, \u03b4t,\u03d5t | \u00b5t, \u03bbt ) =\nnt\u2211 i=1 ( \u2212\u03d5ti(1 + \u00b5tvti) + \u03b4ti log (\u00b5tvti) + \u03b4tiNti log (\u03bbt)\u2212 \u03b4ti\u03bbt \u2212 \u03b4ti log (Nti!) ) ,\nwe can derive the same expression for qDt as in (48). Therefore, we obtain from (55) and (48) that\nDICt = \u22122 nt\u2211 i=1 log  1 1 + \u00b5tvti I(Nti=0) + \u00b5tvti 1 + \u00b5tvti \u03bbt Nti Nti! e\u2212\u03bbt  + 4 log  nt\u2211\ni=1\n\u03b4ti + \u03b11\n\u2212 \u03c8  nt\u2211\ni=1\n\u03b4ti + \u03b11   nt\u2211\ni=1\n\u03b4ti\n+ 4 log  nt\u2211\ni=1\n\u03b4tiNti + \u03b12\n\u2212 \u03c8  nt\u2211\ni=1\n\u03b4tiNti + \u03b12   nt\u2211\ni=1\n\u03b4tiNti.\nFor the above two ZIP models, the DIC of tree T is obtained using (10). With the formulas derived in the above two subsections for ZIP models, we can use the three-step approach proposed in Section 2.5, together with Algorithm 2, to search for an optimal tree and then obtain predictions for new data.\nRemark 8 There are other ways to deal with the data augmentation for ZIP models; see, e.g., [47, 53, 54] where only one latent variable is introduced. The models discussed therein with one latent variable should work more efficiently, but in their constructions no exposure is considered. Since involvement of exposure is one of the key features of insurance claims frequency analysis, we had to introduce two latent variables for data augmentation to facilitate calculations."
        },
        {
            "heading": "4 Simulation and real data analysis",
            "text": "In this section, we illustrate the efficiency of the BCART models introduced in Section 3 by using simulated data and a real insurance claims dataset. In the sequel, we use the abbreviation P-CART\nto denote CART for the Poisson model, and other abbreviations can be similarly understood (e.g., NB1-BCART denotes the BCART for NB model 1)."
        },
        {
            "heading": "4.1 Performance measures",
            "text": "We first introduce some performance measures that will be used for prediction comparisons. Suppose we have obtained a tree with b terminal nodes and the corresponding parameter estimates for \u03b8 which we will use to obtain the prediction N\u0302i given xi, vi for a test data set (X,v,N) with m observations. The number of test data in the terminal node t is denoted by mt, t = 1, . . . , b. The performance measures used are as follows:\nM1: The residual sum of squares (RSS) is given by\nRSS(N) = m\u2211 i=1 (Ni \u2212 N\u0302i)2.\nThis measure is commonly used for Gaussian-distributed data, but here we also use it for non-Gaussian data for comparison.\nM2: RSS based on a sub-portfolio (i.e., those instances in the same terminal node) level is given by\nRSS(N/v) = b\u2211 t=1 (\u2211mt i=1Nti\u2211mt i=1 vti \u2212 y\u0302t )2 ,\nwhere y\u0302t is the estimated frequency for the terminal node t, which is estimated by (14) assuming unit exposure. More specifically, y\u0302t = \u03bbt for Poisson and NB models, and y\u0302t = \u00b5t\u03bbt(1 + \u00b5t) \u22121 for ZIP models. This measure is preferred here as it takes account of accuracy on a (sub)portfolio level (i.e., balance property) other than an individual level. We refer to [12, 13, 55] for more details and discussions of the balance property that is required for insurance pricing.\nM3: Negative log-likelihood (NLL): This is calculated by using the assumed response distribution\nin the terminal node with the estimated parameters. It represents the ex-ante belief of the underlying distribution of the data, and is thus a good measure for model comparison, see, e.g., [30].\nM4: Discrepancy statistic (DS) (cf. [56]), is defined as a weighted version of RSS(N/v), given by\nDS(N/v) = b\u2211 t=1 1 \u03c3\u03022t (\u2211mt i=1Nti\u2211mt i=1 vti \u2212 y\u0302t )2 ,\nwhere y\u0302t is the same as in M2, and \u03c3\u0302 2 t is the estimated variance of frequency for terminal node t. More specifically, \u03c3\u03022t = \u03bbt for the Poisson model, \u03c3\u0302 2 t = \u03bbt(1 + \u03bbt/\u03ba\u0302t) for NB models, and \u03c3\u03022t = \u00b5t\u03bbt(1 + \u00b5t + \u03bbt) (1 + \u00b5t) \u22122 for ZIP models.\nM5: Lift: Model lift indicates the ability to differentiate between low and high claims frequency\npolicy-holders. Sometimes it is called the \u201ceconomic value\u201d of the model. A higher lift illustrates that the model is more capable of separating the extreme values from the average. We refer to [4, 29, 30] and references therein for further discussion on lift. We propose a way to calculate lift for the tree model in the following steps.\nStep 1: Retrieve the predicted frequencies for terminal nodes, y\u0302t, t = 1, . . . , b, for the optimal tree\nobtained from the training procedure.\nStep 2: Set y\u0302min = min b t=1 y\u0302t and y\u0302max = max b t=1 y\u0302t, which identify the least and most risky\ngroups of policy-holders, respectively.\nStep 3: Use test data in the least and most risky groups/nodes to obtain their total sum of\nexposures, say vmin and vmax.\nStep 4: If vmin \u2264 vmax, then sort the data using exposures in descending order in the most risky group. Calculate the cumulative sums of the sorted exposures until the one equal or\ngreater than vmin is achieved and then calculate the corresponding empirical frequency (i.e., ratio of sum of claim numbers and sum of exposures) of these first data involved, say \u03bb (e) max |. The lift is defined as L = \u03bb (e) max |/\u03bb (e) min, where \u03bb (e) min is the empirical frequency of the least risky group.\n[Similarly, If vmin > vmax, then sort the data using exposures in ascending order in the least risky group. Calculate the cumulative sums of the sorted exposures until the one equal or greater than vmax is achieved and then calculate the corresponding empirical frequency of these first data involved, say \u03bb (e) min |. The lift is defined as L = \u03bb (e) max/\u03bb (e) min |, where \u03bb (e) max is the empirical frequency of the most risky group.]\nWe remark that more performance measures and diagnostic approaches can be introduced following ideas in, e.g., [4, 29, 30]. However, this is not the main focus of the present paper, so these are explored elsewhere."
        },
        {
            "heading": "4.2 Simulation examples",
            "text": "We will discuss three simulation examples, namely, Scenarios 1\u20133 below. Scenario 1 aims to illustrate that BCART can do really well for the chessboard data similar to Figure 1 for which CART cannot reasonably do anything. In addition, from this simulation study we also see that BCART can do well with variable selection. In Scenario 2, we shall examine how different BCART models can capture the data over-dispersion. In Scenario 3, we illustrate the effectiveness of ZIP-BCART models for data with exposures."
        },
        {
            "heading": "4.2.1 Scenario 1: Poisson data with noise variables",
            "text": "We simulate a data set {(xi, vi, Ni)}ni=1 with n = 5, 000 independent observations. Here vi \u223c U(0, 1), xi = (xi1, . . . , xi8), with independent components xi1 \u223c U{\u22123,\u22122,\u22121, 1, 2, 3}, xi2 \u223c N(0, 1), xik \u223c\nU(\u22121, 1) for k = 3, 4, xik \u223c N(0, 1) for k = 5, 6, and xik \u223c U{\u22123,\u22122,\u22121, 1, 2, 3} for k = 7, 8. Moreover, Ni \u223c Poi(\u03bb (xi1, xi2) vi), where\n\u03bb (x1, x2) = { 1 if x1x2 \u2264 0, 7 if x1x2 > 0.\nObviously, the designed noise variables xik, k = 3, . . . , 8 are all independent of the response N . We use P-BCART and P-CART for the above simulated data, where xik, k = 1, 7, 8 are treated as categorical. We have included both categorical and continuous variables as noise variables and as significant variables, which is a bit more general than the data shown in Figure 1. Note that the same conclusion can be drawn for numeric xik, k = 1, 7, 8, but to better illustrate the effectiveness of the P-BCART we choose to make them as characters (to increase the splitting possibilities of these variables).\nWe first apply P-CART as implemented in the R package rpart [57]. It is not surprising that P-CART is not able to give us any reasonable tree that can characterize the data, due to its greedy search nature. The smallest tree (except the one with only a root node) that P-CART generated has 25 terminal nodes and the tree found by using cross-validation has 31 terminal nodes. Obviously, both of them are much more complicated than the real model. Furthermore, in these two trees all the noise variables are used, which indicates that P-CART is sensitive to noise.\nNow we discuss the P-BCART applied to the data focusing preliminary on the effect of noise variables to the model. We simply set equal probabilities, i.e.,P(Grow)= 0.2, P(Prune)= 0.2, P(Change1)= 0.2, P(Change2)= 0.2 and P(Swap)= 0.2, for the tree proposals. For the gamma prior of the Poisson intensities \u03bbt we use \u03b1 = 3.2096 and \u03b2 = 0.8 which are selected by keeping\nthe relationship \u03b1/\u03b2 = \u2211n i=1Ni/ \u2211n i=1 vi. It is worth mentioning that the performance of the algorithm does not change much when choosing different pairs of (\u03b1, \u03b2) while keeping their ratio. We also observe the same in other simulation examples, so in the following we will not dwell on their selection.\nIn Table 2 we list the tuned hyper-parameters \u03b3, \u03c1 in the first two columns for which the MCMC algorithms will converge to a region of trees with a certain number of terminal nodes listed (see Step 1 of Table 1). For each fixed hyper-parameter \u03b3 and \u03c1, we run 10000 iterations in the MCMC algorithm and take results after an initial burn-in period of 2000 iterations, after which the posterior probabilities of the tree structures have been settled for some time. This procedure is done with 3 restarts. The fourth column gives the total number of accepted trees after the burn-in period in the MCMC algorithms. The last columns of Table 2 include the total number of times each variable is used in the accepted trees. We see from these columns that all noise variables have a very low selection rate, and as expected, the significant variables x1, x2 are dominating. Besides, at a first glance, it is infered that the noise variables x3 and x4 have a much lower selection rate than the other noise variables which is just because x3 and x4 are simulated using a distribution completely different from those of the significant variables. However, when the experiment is run 10 times, we find that the average selection rates of all noise variables are almost the same independent of their distributions (see Table 3), which is consistent with the expectation.\nIn Figure 2, we illustrate this procedure for j = 4 (the same as that summarized in the third row of Table 2), with plots of the number of terminal nodes, the integrated likelihood pP(N |X,v,T ) and the data likelihood pP(N |X,v,\u03bb,T ) of the accepted trees. The observations are in line with those in [19]; we see from the likelihood plots that the convergence of MCMC can be obtained relatively quickly. Interestingly, the optimal tree is not found in the first round of MCMC which got stuck in a local mode, but the restarts helped where in the second and the third rounds optimal trees can be found. Moreover, we see that there is no big difference shown in the plots of the integrated likelihood and the data likelihood.\nFollowing Step 2 of Table 1, for each j = 2, . . . , 8, we select the optimal tree with maximum data likelihood pP(N |X,v,\u03bb,T ) from the convergence region. The variables used in these optimal trees are listed in Table 4, where we can see that none of these trees involves any of the noise variables. The values for the effective number of parameters pD reflect the number of parameters in the tree if a flat prior for \u03bbt is used. Furthermore, we list the DIC for these trees in the last column of Table 4. Following Step 3 of Table 1 we conclude that the selected optimal tree is the one with 4 terminal nodes which is illustrated in Figure 3. We see that this tree is close to a true optimal one with the almost correct topology and accurate parameter estimates.\nUsing equal probabilities for the proposed tree moves, the above example provides detailed information about how to implement the three-step tree selection procedure in practice and illustrates the effectiveness of the method. Next, we investigate which type of step (particularly, the Change and Swap moves) contributes more to the computational efficiency. To this end, we shall vary the\nprobabilities of the Change and Swap moves, keeping the same probabilities for Grow and Prune moves at 0.2. Different experiments can be designed as in Table 5.\nWe fix \u03b3 = 0.99 and \u03c1 = 15, as for Figure 2. For each of the experiments E1\u2013E4, we run the PBCART MCMC algorithm 10 times and for each run we record the iteration time until an \u201coptimal\u201d tree is found. The average iteration time with the standard deviation (s.d.) of the 10 runs and the average acceptance rates of moves are shown in Table 6. The figures in the second row indicate that the experiment E4 is faster in finding an \u201coptimal\u201d tree than E1\u2013E3 when at least one of the Change moves or/and the Swap move is removed. In particular, the comparison between E1 and E2 confirms the essence of the Swap move, as illustrated also in [19]. Moreover, the acceptance rate of all moves is a weighted average of acceptance rates of all individual moves, and we observe that the acceptance rates of the Change and Swap moves (in particular, the Change1 move) are significantly\ngreater than Grow and Prune moves, which also confirms the significance of the Change and Swap moves (especially, the Change1 move).\nWe also ran several other similar but more complex simulation examples to check the performance of P-BCART, NB-BCART and ZIP-BCART models. Our conclusions from these simulations are: 1) BCART models can retrieve the tree structure (including both topology and parameters) as that used to simulate the data, 2) BCART models are able to avoid choosing noise variables regardless of their distributions, and 3) the Change and Swap moves have significant impacts on the BCART models and it is beneficial to include two types of the Change move."
        },
        {
            "heading": "4.2.2 Scenario 2: ZIP data with varying probability of zero mass component",
            "text": "We simulate a data set {(xi, vi, Ni)}ni=1 with n = 5, 000 independent observations. Here xi = (xi1, xi2), with xik \u223c N(0, 1) for k = 1, 2. We assume vi \u2261 1 for simplicity, since it is not a key feature in this Scenario. Moreover, Ni \u223c ZIP(p0, \u03bb (xi1, xi2)), where\n\u03bb (x1, x2) = { 7 if x1x2 \u2264 0, 1 if x1x2 > 0,\nand p0 \u2208 (0, 1) is the probability of a zero due to the point mass component, for which the value is to be specified. The data is split into two subsets: a training set with n \u2212m = 4, 000 observations and a test set with m = 1, 000 observations.\nFor this Scenario, we aim to examine how the P-BCART, NB-BCART and ZIP-BCART will perform when p0 is varied. Note that since vi \u2261 1, NB1 and NB2 (ZIP1 and ZIP2) will be essentially the same. Intuition tells us that when p0 is small NB-BCART should be good enough to capture the over-dispersion introduced by a small proportion of zeros, but when p0 becomes large ZIP-BCART should perform better for the highly over-dispersed data. This intuition will be confirmed by this study. For simplicity, we shall present two results, one with p0 = 0.05 and the other with p0 = 0.95.\nWe first discuss the simulation with a small probability of zero mass (i.e., p0 = 0.05). In Table 7 we present the hyper-parameters \u03b3, \u03c1 used to obtain MCMC convergence to the region of trees with a certain number of terminal nodes (indicated after the abbreviation of models, e.g., the 2 in ZIP-BCART (2)). The last two columns give the effective number of parameters and DIC of the optimal trees for each model, respectively. We can conclude from the DIC that by using Step 3 in Table 1 we can select the optimal tree with the true 4 terminal nodes for either ZIP-BCART, P-BCART or NB-BCART, and among those, the NB-BCART (with DIC=11192) is the best one. This looks a bit surprising at a first glance because our data are simulated from a ZIP model. We suspect that the reason for this may be two-fold: First, the NB is enough to capture the small overdispersion. Second, we have used data-augmentation in the algorithms and thus it is understandable that the NB-BCART with 1 latent variable (see Section 3.2) could achieve better performance than the \u201creal\u201d ZIP-BCART with 2 latent variables (see Section 3.3). Moreover, we see that even the P-BCART performs better than the ZIP-BCART, for similar reasons.\nNow, let us look at the performance of these models on test data in Table 8. First, we see that for each type of model, ZIP, Poisson and NB, the optimal tree with 4 terminal nodes achieves best RSS(N/v) (0.00162, 0.00108 and 0.00070, respectively) and DS(N/v) (0.000116, 0.000072 and 0.000056, respectively) on test data, which is not surprising as those models retrieve the almost true tree structures. Second, we see from RSS(N) that for each type of model, the performance becomes better as the number of terminal nodes that we want increases, however, the amount of decrement becomes smaller after the optimal trees with 4 terminal nodes have been obtained. We observe the same for negative log-likelihood and lift. It is worth noting that when calculating and comparing lift for different trees, instead of simply following the four steps in M5, in Step 4 we first choose the minimum total sum of exposures among the least and most risky groups in all the trees to be compared and then calculate other values accordingly using this minimum total sum of exposures as the basis. Third, we see that among these three trees with 4 terminal nodes, the one obtained from NB-BCART gives the best performance on test data based on all these performance measures,\nwhich is consistent with the conclusion from training data.\nNext, we consider the simulation with a large probability of zero mass (i.e., p0 = 0.95). The results are displayed in Tables 9 and 10. Similar discussions can be done for this case. In particular, we find that the performance order based on DIC is ZIP-BCART>NB-BCART>P-BCART, which is also consistent with their performance on test data.\nWe also ran several other similar simulation examples to check the performance of P-BCART, NB-BCART and ZIP-BCART with different values for p0. Our conclusion from these simulations is that when the proportion of zeros in the data is small (reflected by small p0) then the NB-BCART or P-BCART performs better than ZIP-BCART, whereas when the proportion of zeros in the data is large then the ZIP-BCART is preferred to NB-BCART and P-BCART. This finding is consistent with the real insurance data discussed below."
        },
        {
            "heading": "4.2.3 Scenario 3: Different ways to incorporate exposure in ZIP models",
            "text": "The purpose of Scenario 3 is to compare two different ways of dealing with exposure, namely, ZIP1BCART and ZIP2-BCART. To this end, we simulate a data set {(xi, vi, Ni)}ni=1 with n = 5, 000 independent observations. Here vi \u223c U(0, 1), xi = (xi1, xi2), with xik \u223c N(0, 1) for k = 1, 2. Moreover, Ni \u223c ZIP(p(\u03c4)i , \u03bb (xi1, xi2) vi), where\n\u03bb (x1, x2) = { 7 if x1x2 \u2264 0, 1 if x1x2 > 0,\nand the probability of zero mass component is given as\np (\u03c4) i =\n\u00b5(xi1, xi2)\nv\u03c4i + \u00b5(xi1, xi2) , with \u00b5(xi1, xi2) \u2261 0.5,\nand some \u03c4 \u2265 0 to be specified below. The data is split into two subsets, namely a training set with n\u2212m = 4, 000 observations and a test set with m = 1, 000 observations.\nIn the above simulation setup, we include exposure in both the Poisson component and the zero mass component. In this way, it is not clear which of ZIP1-BCART and ZIP2-BCART will outperform the other. That being said, we could vary the value of \u03c4 to control the effect of exposure to the zero mass component. We shall consider two extreme cases, one with a very small \u03c4 and the other with a very large \u03c4 . More precisely, for a large \u03c4 we choose \u03c4 = 100. In this case, since many v\u03c4i will be small, we have that p (\u03c4) i will be close to one, which implies that Poisson component should play a minor role in exposure modelling and thus we would expect that ZIP2-BCART has better ability to capture this. On the other hand, for a small value \u03c4 = 0.0001, since many v\u03c4i will be close to 1 we have that p\n(\u03c4) i will be almost independent of vi, which implies that zero mass\ncomponent should play a minor role in exposure modelling and thus we would expect that ZIP1BCART has better ability to capture this. We report DIC for these two cases in Table 11. The model performances on test data are listed in Table 12 for \u03c4 = 100 and Table 13 for \u03c4 = 0.0001. From these tables, we can confirm the above intuition that ZIP1-BCART should perform better for small \u03c4 and worse for large \u03c4 (compared to ZIP2-BCART). We conclude from this simulation study that the ZIP2-BCART works better in capturing the potential stronger effect of the exposure to the zero mass component, which is also illustrated in the real insurance data discussed below."
        },
        {
            "heading": "4.3 Real data analysis",
            "text": "We illustrate our methodology with a real insurance dataset, named dataCar, available from the library insuranceData in R; see [58] for details. This dataset is based on one-year vehicle insurance policies taken out in 2004 or 2005. There are 67,856 policies of which 93.19% made no claims. A summary of the variables used is given in Table 14. We split this dataset into training (80%) and test (20%) data sets, in doing so we keep the balance of zero and non-zero claims in both training and test data sets.\nWe shall apply the BCART models for claims frequency modelling introduced in Section 3 to\ntraining data, where we can use the three-step approach given in Table 1 to choose an optimal tree for each model (and also a global optimal one). We then assess the performance of these obtained trees on test data.\nRunning ANOVA-CART on the training data, we use cross-validation to select the tree size, which has 5 terminal nodes. We also run P-CART in the same way, again resulting in a tree with 5 terminal nodes, and this tree is shown in Figure 4. Then, we apply P-BCART, NB1-BCART, NB2-BCART, ZIP1-BCART and ZIP2-BCART to the same data. Based on the knowledge learnt\nfrom CARTs above, we can tune the hyper-parameters \u03b3, \u03c1, so that the algorithm will converge to a region of trees with number of terminal nodes around 5. Some of these, together with the effective number of parameters and DIC, are shown in Table 15. We see from this table that all the effective numbers of parameters are reasonable for the model used to fit the data. We conclude from the DIC that all of these BCART models select an optimal tree with 5 terminal nodes using the three-step approach, and among these the one from ZIP2-BCART, with the smallest DIC(=25632.5), should be chosen as the global optimal tree to characterize the data.\nIt is interesting to check whether there are similarities in the trees obtained from different models, including the P-CART, particularly as they all have 5 terminal nodes. For the tree from P-CART illustrated in Figure 4, the variable\u201cagecat\u201d is first used and then \u201cveh value\u201d, followed by \u201cagecat\u201d again. The tree from P-BCART (not shown here) also uses \u201cagecat\u201d first, but in the following steps, it uses \u201cveh value\u201d and \u201cveh body\u201d. The trees from NB1-BCART and NB2-BCART look very similar,\nand both of them use \u201cgender\u201d first and then use \u201cagecat\u201d, \u201cveh value\u201d and \u201cveh body\u201d. Further, the trees from ZIP1-BCART and ZIP2-BCART have the same tree structure and select the same splitting variables as the tree from P-BCART, while the split values/categories are slightly different. The optimal tree from ZIP2-BCART is displayed in Figure 5, where the estimated frequency (i.e., the first figure in each node) is calculated through (14) for the ZIP2 model with unit exposure. Comparing the two trees in Figures 4 and 5 we see that ZIP2-BCART model can identify a more risky group (i.e., the one with estimated frequency equal to 0.2674). Moreover, for comparison we also use GLM to fit the data. We find that only the variables \u201cagecat\u201d and \u201cveh body\u201d are significant, in which we also use the interactions between these two variables. In conclusion, though the variables used for different models can differ slightly, there seems to be a consensus that \u201cagecat\u201d, \u201cveh value\u201d and \u201cveh body\u201d are relatively significant variables and \u201cgender\u201d, \u201cveh age\u201d and \u201carea\u201d are less significant.\nNow, we apply the trees to the test data. The performances are given in Table 16. We also include the commonly used GLM, for which the performance looks not as good as the tree models. From the table, we can conclude that for each of the BCART models the tree with 5 terminal nodes that is selected by DIC performs better, in terms of RSS(N/v) and DS(N/v), than the trees with either smaller or larger number of terminal nodes. This confirms that the proposed three-step approach for the tree model selection in each type of models based on DIC works well in real data. Moreover, all the performance measures give the same ranking of models (from best to worst) as follows:\nZIP2-BCART, ZIP1-BCART, NB2-BCART, NB1-BCART, P-BCART, P-CART, ANOVA-CART, GLM.\nThis ranking is, to some extent, consistent with the conclusions from the simulation examples and as expected. We do not know the exact distribution of real insurance data, but we do know that it contains a high proportion of zeros, where the advantage of ZIP comes into play. Further, comparing NB and Poisson distributions, the former is able to handle over-dispersion, so their performance ranking is reasonable. Moreover, the ranking of two ZIP-BCART models and two NB-BCART models are also consistent with the conclusions of [29, 30] where it is justified that the non-standard ways of dealing with exposures (i.e., ZIP2-BCART and NB2-BCART) should better fit real insurance data.\nIn addition to the performance measures, we also record the computation time (in seconds) and memory usage (in megabytes); see the last two columns of Table 16. All computations were performed on a laptop with Processor (3.5 GHz Dual-Core Intel Core i7) and Memory (16 GB 2133 MHz LPDDR3). Clearly, BCART models are far inferior to CARTs and GLM in these two respects and as the number of latent variables increases (from P-BCART to NB-CART to ZIP-BCART) these indicators become worse, but we think with such a large training data these are still acceptable and feasible to use in practice. We remark that there have been prior endeavors to address computing issues; see, e.g., [59\u201361]. We believe these two indicators will be improved after our code is optimized in the future.\nWe conclude this section with some discussions on the stability of the proposed BCART models. Stability is a notion in computational learning theory of how the output of a machine learning algorithm is perturbed by small changes to its inputs. A stable learning algorithm is one for which the prediction does not change much when training data is modified slightly; see, e.g., [62] and references therein. CART models are known to be unstable. It is thus interesting to examine whether the proposed BCART models can be more stable. To this end, we propose the following approach to assess the stability of the P-CART and ZIP2-BCART (as the best) models.\n\u2022 Randomly divide the data into two parts, 80% for training and 20% for testing.\n\u2022 Randomly select 90% of training data for 20 times to construct 20 training subsets, named Data1, Data2, . . . , Data20.\n\u2022 Obtain the optimal tree from P-CART and ZIP2-CART, respectively, for each training set Dataj , j = 1, . . . , 20.\n\u2022 Use the previously obtained trees to get predictions for test data. For each observation in test data, we will have 20 predictions from the 20 P-CART trees for which we calculate the\nvariance, and do the same for the 20 ZIP2-BCART trees to get a variance.\n\u2022 Calculate the mean (over the observations in test data) of those variances for P-CART and ZIP2-BCART, respectively.\nSince variance can capture the amount of variability, we shall use the above obtained mean to assess the stability (in their predicting ability) of a tree-based model. Namely, the smaller the mean the more stable the model that was used to calculate it. We apply it to the dataCar insurance data, the calculated mean for P-CART is 9.319339\u00d710\u22125 and for ZIP2-BCART is 6.896231\u00d710\u22125. This implies that ZIP2-BCART is more stable than P-CART. Additionally, we also compare the 20 trees from P-CART, where we can observe very different trees in terms of number of terminal nodes (ranging from 3 to 8) and splitting variables selected in the trees. Whereas, the 20 trees from ZIP2-BCART also show some stability in terms of number of terminal nodes (all around 5) and splitting variables selected. The same procedure has also been applied to other BCART models and the conclusions are almost the same. Therefore, we conclude from our studies that the proposed BCART models in this paper show some stability that the CART models may not possess."
        },
        {
            "heading": "5 Summary and discussions",
            "text": "This work proposes the use of BCART models for insurance pricing, and in particular, claims frequency prediction. These tree-based models can automatically perform variable selection and detect non-linear effects and possible interactions among explanatory variables. The obtained optimal trees are relatively accurate, stable and are straightforward to interpret by a visualization of the tree structure. These are desirable aspects for insurance pricing. We have introduced the framework of the BCART models and presented MCMC algorithms for general non-Gaussian distributed data where data augmentation may be needed in its implementation. We have included BCART models for Poisson, NB and ZIP distributions, which are the commonly used distributions for claims frequency. For the NB and ZIP models, we explored two different ways to deal with exposures. Remarkably, we conclude from the simulation examples and real data analysis that the non-standard ways of embedding exposures can provide us with better tree models, which is in line with the conclusions of [29, 30]. Furthermore, we introduced a tree model selection approach based on DIC, which has been seen to be an effective approach using both simulation examples and real insurance data. In particular, we conclude from the real insurance data analysis that the ZIP-BCART with exposure embedded in the zero mass component is the best candidate for claims frequency modelling. It is worth remarking that a general zero-inflated NB BCART can be implemented and may further improve the accuracy, but this will require more latent variables to be introduced and will make the convergence of MCMC algorithm harder/slower; see [24] for some insights.\nBelow we comment on potential further improvements of the BCART models for claims frequency\nmodelling.\n\u2022 In the MCMC algorithms we have only used four common proposals, namely, Grow, Prune, Change, and Swap, which have made the algorithm to quickly converge to a local optimal\nregion. In order to make it better explore the tree space, other proposals such as those in [40, 45] can be suggested to improve the mixing of simulated trees. However, we suspect this will significantly increase the computational time, particularly, for high-dimensional large data set and for models requiring data augmentation. To mitigate this effect, we might consider to use a non-uniform choice of splitting variables in the tree prior so as to achieve a better variable selection, e.g., the Dirichlet prior proposed in [38].\n\u2022 The proposed models have imposed several assumptions in order to simplify calculations. For example, we used conjugate prior for the terminal node distributions, and additional\nindependence assumption as in (4) and (45). To further improve the analysis, it might be beneficial to incorporate different specifications of the prior for the same distribution scenario without using conjugate priors or independence, while this may require other techniques such as Laplace approximation (see [35]). We refer also to [63] for an interesting incorporation of some hierarchical priors.\n\u2022 We have proposed to use a single (optimal) tree induced from the BCART models for claims frequency prediction. The main reason for this choice is, as we discussed in the introduction,\nfor ease of interpretation. Since stakeholders and regulators may not be statisticians who are able to understand very complex statistical models, a single tree offers intuitive and visual results to them. Although we have proposed an approach to find one single optimal tree, some sub-optimal trees (in the convergence region of the MCMC) which possess similar/different tree structures, may also be as informative as the single optimal tree and should not be simply ignored. Further research can be done in this direction to make better use of the posterior trees by clustering or merging them; see, e.g., [64, 65].\n\u2022 To further improve the accuracy of these Bayesian tree-based models we could explore BART for claims frequency modelling. The BART models are tree ensembles; each tree in BART\nonly accounts for a small part of the overall fit, potentially improving the performance, but model interpretability needs to be explored before it can be used for insurance pricing. To this end, we believe some insights from [4] would be helpful.\nIn this paper, we have focused on insurance claims frequency. A natural next step is to construct\na full insurance pricing BCART model, including both claims frequency and severity.\nAcknowledgement: We are thankful to the anonymous referee for their constructive suggestions\nwhich have led to a significant improvement of the manuscript."
        }
    ],
    "title": "Bayesian CART models for insurance claims frequency",
    "year": 2023
}