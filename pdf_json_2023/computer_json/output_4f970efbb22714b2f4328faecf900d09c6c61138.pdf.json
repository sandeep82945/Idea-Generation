{
    "abstractText": "While GPT-3 has garnered significant attention for its capabilities in natural language generation, research on its use outside of English is still relatively limited. We focus on how GPT-3 can be fine-tuned for generating synthetic news articles in a low-resource language, namely Danish. The model\u2019s performance is evaluated on the dimensions of human and machine detection in two separate experiments. When presented with either a real or GPT-3 generated news article, human participants achieve a 58.1% classification accuracy. Contrarily, a fine-tuned BERT classifier obtains a 92.7% accuracy on the same task. This discrepancy likely pertains to the fine-tuned GPT-3 model oversampling high-likelihood tokens in its text generation. Although this is undetectable to the human eye, it leaves a statistical discrepancy for machine classifiers to detect. We address how decisions in the experimental design favoured the machine classifiers over the human evaluators, and whether the produced synthetic articles are applicable in a real-world context.",
    "authors": [
        {
            "affiliations": [],
            "name": "Mina Almasi"
        }
    ],
    "id": "SP:d7cd4c30dc1af5159bb2525405ad680a9115b71f",
    "references": [
        {
            "authors": [
                "David Ifeoluwa Adelani",
                "Haotian Mai",
                "Fuming Fang",
                "Huy H. Nguyen",
                "Junichi Yamagishi",
                "Isao Echizen"
            ],
            "title": "Generating Sentiment-Preserving Fake Online Reviews Using Neural Language Models and Their Human- and Machine-Based Detection",
            "year": 2020
        },
        {
            "authors": [
                "Jordi Armengol-Estap\u00e9",
                "Ona de Gibert Bonet",
                "Maite Melero."
            ],
            "title": "On the Multilingual Capabilities of Very Large-Scale English Language Models",
            "venue": "ArXiv:2108.13349 [cs].",
            "year": 2021
        },
        {
            "authors": [
                "Salvador Balkus",
                "Donghui Yan."
            ],
            "title": "Improving Short Text Classification With Augmented Data Using GPT-3",
            "venue": "ArXiv:2205.10981 [cs].",
            "year": 2022
        },
        {
            "authors": [
                "J\u00e9r\u00e9mie Bogaert",
                "Marie-Catherine de Marneffe",
                "Antonin Descampe",
                "Francois-Xavier Standaert."
            ],
            "title": "Automatic and Manual Detection of Generated News: Case Study, Limitations and Challenges",
            "venue": "Proceedings of the 1st International Workshop on Multimedia",
            "year": 2022
        },
        {
            "authors": [
                "Conrad Borchers",
                "Dalia Gala",
                "Benjamin Gilburt",
                "Eduard Oravkin",
                "Wilfried Bounsi",
                "Yuki M Asano",
                "Hannah Kirk."
            ],
            "title": "Looking for a Handsome Carpenter! Debiasing GPT-3 Job Advertisements",
            "venue": "Proceedings of the 4th Workshop on Gender Bias",
            "year": 2022
        },
        {
            "authors": [
                "Association for Computational Linguistics. Danske Medier Research."
            ],
            "title": "Toplisten",
            "venue": "N Dehouche. 2021. Plagiarism in the age of mas-",
            "year": 2022
        },
        {
            "authors": [
                "Kristina Toutanova"
            ],
            "title": "2019. BERT: Pre-training",
            "year": 2019
        },
        {
            "authors": [
                "Sebastian Gehrmann",
                "Hendrik Strobelt",
                "Alexander Rush."
            ],
            "title": "GLTR: Statistical Detection and Visualization of Generated Text",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 111\u2013116,",
            "year": 2019
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi."
            ],
            "title": "The Curious Case of Neural Text Degeneration",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Daphne Ippolito",
                "Daniel Duckworth",
                "Chris CallisonBurch",
                "Douglas Eck."
            ],
            "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Jun Kong",
                "Jin Wang",
                "Xuejie Zhang."
            ],
            "title": "Hierarchical BERT with an adaptive fine-tuning strategy for document classification",
            "venue": "Knowledge-Based Systems, 238:107872.",
            "year": 2022
        },
        {
            "authors": [
                "Angelie Kraft",
                "Hans-Peter Zorn",
                "Pascal Fecht",
                "Judith Simon",
                "Chris Biemann",
                "Ricardo Usbeck."
            ],
            "title": "Measuring Gender Bias in German Language Generation",
            "venue": "Gesellschaft f\u00fcr Informatik, Bonn. Accepted: 2022-09-28T17:10:03Z ISSN: 1617-5468.",
            "year": 2022
        },
        {
            "authors": [
                "Per E Kummervold",
                "Javier De la Rosa",
                "Freddy Wetjen",
                "Svein Arne Brygfjeld."
            ],
            "title": "Operationalizing a National Digital Library: The Case for a Norwegian Transformer Model",
            "venue": "Proceedings of the 23rd Nordic Conference on Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Per E Kummervold",
                "Javier De la Rosa",
                "Freddy Wetjen",
                "Svein Arne"
            ],
            "title": "Norwegian Colossal Corpus Description",
            "venue": "Brygfjeld",
            "year": 2022
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising Sequence-to-Sequence Pretraining for Natural Language Generation, Transla",
            "year": 2020
        },
        {
            "authors": [
                "Junyi Li",
                "Tianyi Tang",
                "Wayne Xin Zhao",
                "Ji-Rong Wen"
            ],
            "title": "Pretrained Language Model for Text Generation: A Survey",
            "year": 2021
        },
        {
            "authors": [
                "R. Thomas McCoy",
                "Paul Smolensky",
                "Tal Linzen",
                "Jianfeng Gao",
                "Asli Celikyilmaz."
            ],
            "title": "How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN",
            "venue": "ArXiv:2111.09509 [cs].",
            "year": 2021
        },
        {
            "authors": [
                "Selina Meyer",
                "David Elsweiler",
                "Bernd Ludwig",
                "Marcos Fernandez-Pichel",
                "David E. Losada."
            ],
            "title": "Do We Still Need Human Assessors? Prompt-Based GPT-3 User Simulation in Conversational AI",
            "venue": "Proceedings of the 4th Conference on Conversational",
            "year": 2022
        },
        {
            "authors": [
                "Bonan Min",
                "Hayley Ross",
                "Elior Sulem",
                "Amir Pouran Ben Veyseh",
                "Thien Huu Nguyen",
                "Oscar Sainz",
                "Eneko Agirre",
                "Ilana Heinz",
                "Dan Roth"
            ],
            "title": "Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey",
            "year": 2021
        },
        {
            "authors": [
                "Steven Moore",
                "Huy A. Nguyen",
                "Norman Bier",
                "Tanvi Domadia",
                "John Stamper."
            ],
            "title": "Assessing the Quality of Student-Generated Short Answer Questions Using GPT-3",
            "venue": "Educating for a New Future: Making Sense of Technology-Enhanced Learning Adoption,",
            "year": 2022
        },
        {
            "authors": [
                "Martin M\u00fcller",
                "Florian Laurent."
            ],
            "title": "Cedille: A large autoregressive French language model",
            "venue": "ArXiv:2202.03371 [cs].",
            "year": 2022
        },
        {
            "authors": [
                "Samuel R\u00f6nnqvist",
                "Jenna Kanerva",
                "Tapio Salakoski",
                "Filip Ginter"
            ],
            "title": "Is Multilingual BERT Fluent in Language Generation",
            "venue": "In Proceedings of the First NLPL Workshop on Deep Learning for Natural Language Processing,",
            "year": 2019
        },
        {
            "authors": [
                "Yi Tay",
                "Dara Bahri",
                "Che Zheng",
                "Clifford Brunk",
                "Donald Metzler",
                "Andrew Tomkins."
            ],
            "title": "Reverse Engineering Configurations of Neural Text Generation Models",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "A.M. Turing."
            ],
            "title": "Computing Machinery and Intelligence",
            "venue": "Mind, 59(236):433\u2013460. Publisher: [Oxford University Press, Mind Association].",
            "year": 1950
        },
        {
            "authors": [
                "Adaku Uchendu",
                "Zeyu Ma",
                "Thai Le",
                "Rui Zhang",
                "Dongwon Lee."
            ],
            "title": "TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Varab",
                "Natalie Schluter."
            ],
            "title": "DaNewsroom: A Large-scale Danish Summarisation Dataset",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 6731\u20136739, Marseille, France. European Language Resources Association.",
            "year": 2020
        },
        {
            "authors": [
                "Haifeng Wang",
                "Jiwei Li",
                "Hua Wu",
                "Eduard Hovy",
                "Yu Sun."
            ],
            "title": "Pre-Trained Language Models and Their Applications",
            "venue": "Engineering.",
            "year": 2022
        },
        {
            "authors": [
                "Y.M. Wazery",
                "Marwa E. Saleh",
                "Abdullah Alharbi",
                "Abdelmgeid A. Ali."
            ],
            "title": "Abstractive Arabic Text Summarization Based on Deep Learning",
            "venue": "Computational Intelligence and Neuroscience, 2022:e1566890. Publisher: Hindawi.",
            "year": 2022
        },
        {
            "authors": [
                "Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-Art Natural Language Processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Shijie Wu",
                "Mark Dredze."
            ],
            "title": "Are All Languages Created Equal in Multilingual BERT? In Proceedings of the 5th Workshop on Representation Learning for NLP, pages 120\u2013130, Online",
            "venue": "Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Rong Yan",
                "Jiang Li",
                "Xiangdong Su",
                "Xiaoming Wang",
                "Guanglai Gao."
            ],
            "title": "Boosting the Transformer with the BERT Supervision in Low-Resource Machine Translation",
            "venue": "Applied Sciences, 12(14):7195. Number: 14 Publisher: Multidisciplinary Digital Pub-",
            "year": 2022
        },
        {
            "authors": [
                "Jingqing Zhang",
                "Yao Zhao",
                "Mohammad Saleh",
                "Peter Liu."
            ],
            "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, pages 11328\u201311339. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Mingyu Zong",
                "Bhaskar Krishnamachari."
            ],
            "title": "Solving Math Word Problems Concerning Systems of Equations with GPT-3",
            "venue": "Proceedings of the Thirteenth AAAI Symposium on Educational Advances in Artificial Intelligence, page 8.",
            "year": 2022
        },
        {
            "authors": [
                "de steder",
                "da"
            ],
            "title": "Milj\u00f8styrelsen i 2019 gav milj\u00f8tilladelse til Baltic Pipe-projektet. Men if\u00f8lge Milj\u00f8- og F\u00f8devareklagen\u00e6vnet burde forholdene v\u00e6re grundigt unders\u00f8gt, allerede inden tilladelsen blev udstedt, og anl\u00e6gsarbejdet kunne begynde",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 16th International Natural Language Generation Conference, pages 54\u201368 September 11\u201315, 2023. \u00a92023 Association for Computational Linguistics\n54"
        },
        {
            "heading": "1 Introduction",
            "text": "In recent years, rapid development in natural language processing, particularly in the area of pretrained language models, has led to significant advancements in various language tasks. State-of-theart models, such as GPT-3 (Brown et al., 2020) and BERT (Devlin et al., 2019), have excelled in tasks such as classification of documents (Kong et al., 2022), text completion (Balkus and Yan, 2022), language translation (Yan et al., 2022) and text summarization (Wazery et al., 2022). These advances have even led some to suggest that we are currently experiencing a whole paradigm shift in NLP with the introduction of pretrained language models (Min et al., 2021).\nHowever, most contemporary studies using GPT3 focus on its performance in English. This is to be expected as the model was almost exclu-\nsively trained on English with less than 8% of training data being non-English (OpenAI, 2020). Still, the few investigations on GPT-3 for nonEnglish text generation show promising results (Kraft et al., 2022; M\u00fcller and Laurent, 2022). This even holds for low-resource languages such as Catalan (Armengol-Estap\u00e9 et al., 2021).\nYet, the more prevalent approach in NLP for low-resource languages has been using smaller language-specific models or multilingual models such as mBERT (Doddapaneni et al., 2021). This is despite multilingual models seemingly lacking in natural language generation tasks, especially for the Nordic languages and other low-resource languages (R\u00f6nnqvist et al., 2019; Wu and Dredze, 2020). In terms of language-specific models, this development has also occurred in Danish NLP with several Danish models appearing based on the likes of BERT and ELECTRA (e.g., TamimiSarnikowski, 2021 and M\u00f8llerh\u00f8j, 2021). Nevertheless, such models are miniscule in size compared to the state-of-the-art language models. For instance, the Danish BERT model by M\u00f8llerh\u00f8j (2021) is trained on 9.7 billion characters. Comparatively, GPT-3\u2019s total training data corresponds to 1.1 trillion characters (OpenAI, 2020).\nIn this paper, we seek to understand how well GPT-3 can perform for a low-resource language such as Danish when optimized for that language through fine-tuning. To our knowledge, this is the first structured assessment of GPT-3\u2019s capabilities in a Danish NLP task. Concretely, we investigate whether GPT-3 can be fine-tuned to produce synthetic news articles that are indistinguishable to real news articles written by journalists. Generating news articles with GPT-3 is a common task with previous work showing remarkable results in English (Brown et al., 2020; Uchendu et al., 2021).\nInspired by a similar study from Ippolito et al. (2020), we make a two-fold evaluation of the model\u2019s performance:\n(A) Human Detection: Can untrained human participants distinguish between real and synthetic articles in an experimental setting?\n(B) Machine Detection: Can machine classifiers be trained to distinguish between real and synthetic articles?\nAs human and machine detection methods presumably apply distinct techniques to spot synthetically generated text (Ippolito et al., 2020), a two dimensional evaluation provides a more nuanced insight into how GPT-3 performs on the task.\nOur findings suggest that a fine-tuned GPT-3 can generate convincing Danish synthetic news, deceiving human readers while being identifiable by a BERT classifier. This demonstrates GPT-3\u2019s capacity to perform succesfully in the context of low-resource languages, but with the drawback of heightened machine-detectability due to an overuse of high-probability tokens."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Natural Language Generation with Pretrained Language Models",
            "text": "Natural language generation (NLG) is a subfield of NLP concerned with the process of producing intelligible language. However, even within this subfield, there are a diverse range of related sub-tasks. Examples of such tasks, which have natural language as the input and output, are summarization, question answering and translation (Celikyilmaz et al., 2021).\nSimilar to other fields in NLP, text generation has evolved rapidly with the paradigm of pretrained language models. These models have been critical for advancing NLG as they understand natural language, express it fluently and are capable of being fine-tuned for a specific domain (Li et al., 2021). Importantly, pretrained language models can generate natural language that is novel rather than just outputting text memorized from the training data. This was demonstrated in McCoy et al. (2021) who found GPT-2 and Transformer-XL to produce novel words and unique syntactic structures not found in the training data.\nThe demonstrated successes of GPT-3 in NLG cannot only be attributed to the sheer amount of\ndata it has seen, but also to the underlying decodertransformer architecture. GPT-based models are built using only decoder blocks which possess a masked self-attention layer that prevents the language model from considering future context (Wang et al., 2022). This architecture is more easily applicable to NLG tasks than the alternative encoder-only structures found in BERT-based models (Lewis et al., 2020)."
        },
        {
            "heading": "2.2 The Fine-Tuning Approach",
            "text": "The groundbreaking paper introducing GPT-3 titled, \u201cLanguage Models are Few-Shot Learners\u201d highlighted its ability to achieve strong performance on various NLP tasks after only seeing a few examples (Brown et al., 2020). This few-shot learning approach can be contrasted with fine-tuning in which the model is updated through re-training with task-specific data. Although GPT-3 arguably excels at text generation from few-shot learning, OpenAI recommends fine-tuning the model for custom applications citing advantages such as higher quality results.1\nRelated research has also already demonstrated the flexibility of GPT-3 by fine-tuning it for a wide variety of tasks. Perhaps the most ambitious fine-tune of GPT-3 is OpenAI\u2019s Codex which was trained on 159 GB of Python files from 54 million GitHub repositories. As a result of this fine-tune, Codex outperformed base GPT-3 on a benchmark on several different coding tasks (Chen et al., 2021). However, fine-tunes of GPT-3 extend beyond just massive applications. A study by Zong and Krishnamachari (2022) on extracting equations from math word problems found an 80% accuracy for a fine-tuned GPT-3 model compared to only 40% accuracy with 3-shot learning. Contrary to the enormous Codex fine-tune, their fine-tune had just seen 1000 examples. Similar small-scale fine-tunes of GPT-3 improved its abilities for assessing students\u2019 short answer questions (Moore et al., 2022) and writing less biased job advertisements (Borchers et al., 2022).\nThe takeaway from these studies is that GPT-3 can improve performance through fine-tuning for specific downstream tasks despite its generalized task excellence from just few-shot learning.\n1https://platform.openai.com/docs/guides/fine-tuning"
        },
        {
            "heading": "2.3 Evaluating Synthetically Generated Text",
            "text": ""
        },
        {
            "heading": "2.3.1 Human Detection",
            "text": "Evaluating whether artificial intelligence can deceive humans dates back to the Turing Test (Turing, 1950). While the current state of AI is incomparable to the 1950s, the underlying idea of judging machines on their human-like performance is still relevant. Much of research using this approach evaluated language models by asking participants to classify whether text excerpts were human-written or synthetically generated (Bogaert et al., 2022; Brown et al., 2020; Uchendu et al., 2021).\nAlthough these classifications provide valuable insight into a language model\u2019s capabilities, they leave many questions as to why and how these models excel. For this reason, other studies ask participants to rate various qualities of the text without knowing whether the text is synthetic or real. The exact qualities that are rated differ across studies. For instance, some studies judge the overall text quality (Zhang et al., 2020) or fluency (Adelani et al., 2020) on a Likert scale. Dou et al. (2022)\u2019s SCARECROW framework offers a more systematic approach to analyzing synthetic text, accessible to laypeople with basic training. It groups common error types within categories, like language errors for grammar and incoherence, and factual errors for incorrect or nonsensical information."
        },
        {
            "heading": "2.3.2 Machine Detection",
            "text": "Although SCARECROW provides a standardized human evaluation of language models, human detection may not be ideal for detecting GPT-3 news articles as low accuracies would suggest. For instance, Clark et al. (2021) found that human evaluators only unmasked GPT-3 news stories with 56% accuracy despite them being trained for the task. Yet, this does not imply that synthetic text cannot be detected at all. In fact, past research on synthetic text detection has found machines to be superior to humans (Ippolito et al., 2020; Meyer et al., 2022; Uchendu et al., 2021). For example, Ippolito et al. (2020) utilized both a bag-of-words logistic regression and a fine-tuned BERT, reporting much greater performance than human evaluators. While the BERT model was optimal, the bag-of-words model did not lag far behind. As formulated by the study, the high performing machine detectors are likely due to the sampling method of language models being skewed towards highlikelihood words. Therefore, synthetic text is more\neasily distinguishable from human language which has greater variability in word choice (Holtzman et al., 2020). This linguistic difference is also noted in other research (Gehrmann et al., 2019; Tay et al., 2020).\nNevertheless, models relying solely on word probabilities are still inferior to more complex language models such as BERT. This may indicate that there are other factors which differentiate real and synthetic articles that language models pick up on with fine-tuning. Just like Ippolito et al. (2020), Uchendu et al. (2021) found that the fine-tuned BERT was the best performing detector across text generated by 19 language models including GPT-3."
        },
        {
            "heading": "3 Data",
            "text": "The real news stories were all sourced from the Danish news site tv2.dk. In October 2022, TV2\u2019s news platform boasted over 3 million unique users (Danske Medier Research, 2022), which is more than half of Denmark\u2019s population. Hence, it makes an excellent representation of typical news content consumed by Danes. These articles were obtained via two channels: directly scraping from TV2 and employing the DaNewsRoom Danish news database (Varab and Schluter, 2020).\nIn the selection process, only article bodies with a minimum length of 100 words were considered, and longer articles were shortened to a maximum of 150 words. Although the exact threshold is somewhat arbitrary, it was kept in this range for two reasons. Firstly, accumulating costs for generating articles with the fine-tuned GPT-3 necessitated that we kept the articles short. Also, using longer articles would entail that each participant would evaluate fewer articles as their time was limited.\nIn total, 1866 real Danish news articles from TV2 were sourced and used for three purposes: Fine-tuning GPT-3 (1209 real articles), providing training/validation data for machine classifiers (609 real articles), and serving as test data in the experiments (48 real articles). Additionally, 657 synthetic articles were generated by the fine-tuned GPT-3 for training the classifiers (609 synthetic articles) and test data in the experiments (48 synthetic articles)."
        },
        {
            "heading": "4 Methods",
            "text": ""
        },
        {
            "heading": "4.1 Fine-Tuning GPT-3",
            "text": "GPT-3, specifically text-davinci-002, was finetuned with 1209 pre-processed real news articles\nusing OpenAI\u2019s API in Python. All articles were formatted to JSONL in accordance with the API documentation.2 The headlines and subheadings were combined to be the prompts for GPT-3, and the corresponding article bodies were completions. All hyperparameters set for the fine-tune are detailed in Appendix A.1. As the training loss for the fine-tune plateaued during the fourth epoch (Figure 1), we ended model training after this epoch."
        },
        {
            "heading": "4.2 Generating Synthetic News Articles",
            "text": "The fine-tuned GPT-3 was then applied to generate synthetic news articles. As in the training phase, the prompts (headline and subheading) came from real news articles.3 When generating the text completions, we modified several of the default hyperparameters based on previous research for similar cases and OpenAI\u2019s general recommendations.4\nFirstly, GPT-3\u2019s temperature sampling method was adjusted by setting the temperature parameter to zero. In temperature sampling, a high temperature means that low probability tokens are more frequently sampled. By setting temperature to zero, the model becomes deterministic, always sampling the most probable token when generating text. We made this adjustment because a high temperature may lead to factual errors as the model \u201ctakes more risks\". Also, Ippolito et al. (2020) found that a zero temperature in conjunction with a frequency penalty was the most successful for generating English news articles. This parameter penalizes a new token based on how frequently it appears in the generated text so far. It can be used along with a\n2https://platform.openai.com/docs/guides/finetuning/prepare-training-data\n3To avoid double-dipping, these headlines and subheadings came from articles that were not part of the 1866 real articles mentioned in the data section.\n4platform.openai.com/docs/api-reference\npresence penalty (penalizes solely based on presence rather than frequency) to decrease the likelihood of sampling repetitive token sequences. As repetitiveness is also a particular concern for GPT3\u2019s text generation (Dehouche, 2021), we add small presence and frequency penalties of 0.2. The full specification including ranges for the hyperparameters are in Appendix A.2.\nThe text completions formed the synthetic news articles, utilized as training/validation data for machine detection and test data for both experiments. Sanity checks were made to verify that these articles were similar to the real news articles in length and structure, but we made no modifications to them whatsoever."
        },
        {
            "heading": "5 Experiment A: Human Detection",
            "text": "Experiment A is designed as a binary classification task where human participants must distinguish between real articles written by humans and GPT3\u2019s synthetic ones."
        },
        {
            "heading": "5.1 Participants",
            "text": "120 participants (66.6% female, age: M = 30.0, SD = 13.7) voluntarily took part in the online study. The study was run on the online platform SoSci Survey (Leiner, 2022) for one week in October 2022. To ensure a wide participant reach, the study was optimized for both computer and smartphone use. Complying with the prerequisites for the study, all participants were adult Danish native speakers."
        },
        {
            "heading": "5.2 Experimental Procedure",
            "text": "In each experimental trial, participants saw a page with one news article and four questions to be answered (see Appendix A.5). Participants were firstly asked to evaluate whether they believed the article body to be written by a human or an artificial intelligence. Subsequently, participants had to rate their confidence on a 5-point Likert scale from completely unsure (1) to completely sure (5). Finally, participants were asked to label whether the article had any distracting language or factual errors. These error types were inspired by the SCARECROW framework but simplified as the full framework would be too complex for untrained evaluators. To ensure participants understood what the error types implied, examples were written beneath each question. The articles were formatted to be closer in appearance with a real news article. This was done by differentiating in the size and color of\nthe headline, subheading and the article body (Appendix A.5). Importantly, it was clearly stated that only the body should be evaluated, not the headline and subheading as those always originated from real news stories.\nIn total, each participant evaluated 16 articles (8 real and 8 synthetic) in a randomly shuffled order. To cover the wide topical variance within news articles, 96 articles were used across all participants. That is, each participant only assessed a sixth of the total article pool, which corresponds to every article being evaluated by 20 unique participants."
        },
        {
            "heading": "5.3 Results",
            "text": ""
        },
        {
            "heading": "5.3.1 Human Accuracy",
            "text": "With 20 assessments of 96 articles, the human results are based on 1920 total classifications. The overall classification accuracy was just 58.1%. This means that participants only performed eight percentage points over chance level which is a comparable result to similar studies conducted in English (see 2.3). Interestingly, when presented with a synthetic news article, participants correctly labeled it as machine-written 53.6% of the time. Contrarily, a true positive rate of 62.6% indicates that participants were better at identifying real news articles as human-written. In addition, it should also be underlined that none of the 96 articles were exclusively classified correctly or incorrectly. The articles that were the easiest to identify were classified correctly 95% of the time, whereas there were only 15% correct classifications for the hardest ones.\nMoreover, none of the 120 participants answered correctly on all 16 articles that they saw, with all of them misclassifying at least one synthetic news article as real news. This implies that the synthetic news articles have fooled all 120 participants to some extent.\nFurthermore, all participants were screened on their news consumption level and prior knowledge of GPT-3. To see whether domain expertise caused enhanced performance, a mixed effects logistic regression model was run with media consumption level and GPT-3 knowledge as fixed effects. The news article ID is used as a random effect to account for variance that is specific to the articles.5\nThe full model output is displayed in Appendix A.6. The baseline/intercept in the model corresponds to a participant who never reads news and\n5accuracy ~ news consumption + gpt-3 knowledge + (1 | article ID)\nnever had heard of GPT-3 prior to the experiment. The output reveals that a higher level of news consumption does not lead to significantly higher accuracies. However, compared to the baseline, we see significant improvements for participants that have heard of GPT-3 before (\u03b2 = 0.327, odds ratio = 0.581, SE\u03b2 = 0.131, p = 0.013) and those participants that have additionally read GPT-3 texts (\u03b2 = 0.478, odds ratio = 0.617, SE\u03b2 = 0.146, p = 0.001). This suggests that having GPT-3 knowledge may give an advantage in demarcating real from synthetic news, although participants who had worked with GPT-3 (highest level of GPT-3 knowledge) did not outperform the baseline."
        },
        {
            "heading": "5.3.2 Confidence and Error Identification",
            "text": "Participants were also asked to rate their confidence in the classification as well as marking error types for each trial. Figure 2 reveals that participants typically abstain from the most extreme confidence ratings of Completely unsure (1) and Completely sure (5). As expected, participants\u2019 accuracy is around the chance level for low confidences. However, even when claiming to be Completely sure, the fraction of correct answers only increases to 69%. For confidences of Fairly sure (4), this drops to only 60% correct answers.\nWe also see some interesting patterns in error type responses by the participants. Figure 3 illustrates which errors were marked for real and synthetic articles respectively. Overall, the pat-\nterns are strikingly similar. The figure reveals that participants most often did not find errors in the articles. When errors then were marked, there was a propensity to find language errors over factual errors for both real and synthetic articles. Despite the similarities, participants were more inclined to identify both factual and language errors for synthetic articles than for real ones. However, this did not necessarily lead to correct classifications. For instance, when participants marked Both errors, articles were almost exclusively labeled as synthetic (89.7% of cases) although Figure 3 reveals that this was often incorrect.\nIn sum, participants struggled with demarcating real news articles from synthetic ones in Experiment A. The overall accuracy was only 58% with classifications of synthetic news articles approaching chance level. Also, all 120 participants were fooled by at least one synthetic article and even the most confident classifications frequently led to wrong responses. Finally, patterns in error types marked by participants are similar for real and synthetic articles which shows the participants\u2019 inability to demarcate the articles by style and content."
        },
        {
            "heading": "6 Experiment B: Machine Detection",
            "text": "Experiment B explores whether it is possible to construct machine classifiers that are capable of distinguishing between real and synthetic articles. This is approached with logistic regression using bagof-words (BOW) and TF-IDF as baseline models. The more advanced language model, NB-BERTLARGE, is then fine-tuned, tested and evaluated against the baselines and human participants."
        },
        {
            "heading": "6.1 Building Classifiers",
            "text": "Two baseline classifiers are constructed using logistic regression with BOW and TF-IDF numerical representations of the vocabulary within the entire corpus (see Appendix A.3 for their hyperparameters). The BOW classifier is the most simple baseline, solely representing word frequencies within each document. TF-IDF provides a more detailed representation by also accounting for a word\u2019s rarity in relation to the entire set of documents.\nExpanding beyond purely vocabulary-based classification, we fine-tune the BERT model, NBBERT-LARGE (Kummervold et al., 2021), for the binary classification task. This BERT model was pretrained on the Norwegian Colossal Corpus which is a diverse collection of textual data\n(Kummervold et al., 2022). Although Norwegian is the primary language of the corpus, the collection contains several languages. Notably, Danish is the biggest language after Norwegian with 13.6% of the corpus being in Danish. We fine-tuned the model with the Trainer API using Hugging Face\u2019s transformers package (Wolf et al., 2020) in Python. The fine-tuning dataset comprised 1218 labeled articles split into a training and validation set (training: 75%, validation: 25%). Half of these were the real news articles from TV2 and the other half synthetic news articles. The test data comprised the same 96 articles that humans evaluated in Experiment A.\nThe hyperparameters for the fine-tuning of BERT are detailed in Appendix A.4. Resulting from an early stopping callback,6 the model was fine-tuned for two epochs, obtaining a validation accuracy of 95.7%."
        },
        {
            "heading": "6.2 Results",
            "text": ""
        },
        {
            "heading": "6.2.1 Classification Accuracies",
            "text": "Table 1 shows the results of both the machine and human detection on the test data of 96 articles. The fine-tuned BERT model outclasses humans at the task with a 92.7% accuracy on the test set as well as the highest F1-score. Also, even the baseline BOW and TF-IDF models performed substantially better than the human average accuracy with accuracies around 80%, indicating that vocabulary discrepancies can demarcate the real and synthetic articles to an extent.\n6based on the validation accuracy\nAn interesting similarity between all machine classifiers is their tendency to classify articles as synthetic. This is most noticeable with fine-tuned BERT which has 12.5% false negatives as opposed to just 2.1% false positives. Remarkably, BERT\u2019s true negative classifications of 97.9% means that the model has only classified a single synthetic article wrong. This propensity to classify articles as synthetic contrasts human participants, who had a bias towards classifying most articles as real."
        },
        {
            "heading": "6.2.2 Classifier Agreement",
            "text": "We turn to examine classifier agreement quantitatively by evaluating their inter-rater reliability using Cohen\u2019s Kappa. Unsurprisingly, this metric reveals that TF-IDF and BOW have an almost perfect agreement, \u03ba = 0.91, z = 3.37, p < 0.05. Moreover, both TF-IDF (\u03ba = 0.62, z = 6.14) and BOW (\u03ba = 0.62, z = 6.11) have a substantial agreement with BERT that is greater than would be expected by chance (both p < 0.05).\nTable 2 gives a qualitative insight into the agreements with examples of how four test articles were classified. Article A was the most commonly misclassified article for humans (17 out of 20 misclassifications). However, interestingly, all three classifiers correctly identified it as synthetic news. Additionally, article B is one of four instances where BERT correctly identified a synthetic news article while both BOW and TF-IDF failed. Oppositely, article C provides an example of BERT\u2019s over-inclination to classify as synthetic. It is one of three articles where BERT misclassified a real news article while BOW and TF-IDF did not. Finally, article D is the only synthetic article that BERT misclassified. However, as Table 2 shows, BOW and TF-IDF also struggled with this article.\nThe overall takeaway remains that these machine detections performed vastly better than human participants. This improvement was clear even for the two baseline models based on BOW and TFIDF. Still, the more sophisticated fine-tuned BERT classifier performed the best by far, with an impressive 92.7% overall accuracy and just a single\nmisclassification of the 48 synthetic articles."
        },
        {
            "heading": "7 Limitations",
            "text": "A few limitations must be addressed in relation to these results. Firstly, several design decisions presumably favoured the machine detectors over the human evaluators. Whereas 78.3% of human participants had never seen GPT-3 produced texts before, all machine classifiers received extensive training on over 1000 labelled articles prior to the final testing. Also, the zero temperature token sampling for generating synthetic articles created an overrepresentation of high-likelihood tokens. This may be identified by the machine detectors, whereas such patterns are are probably too subtle to notice for humans (Ippolito et al., 2020). Also, Dou et al. (2022) show that higher temperatures are associated with GPT-3 making off-prompt errors. Such errors would not be captured by the machine classifiers, whereas humans would more likely identify these more semantic shortcomings.\nMoreover, it must be addressed that human classifications are possibly influenced from being conducted in an experimental setting. Contrary to the machine classifiers, the human participants saw the headline and subheading for all articles. Despite being repeatedly told not to evaluate them, it cannot be dismissed that these extra elements still could have influenced their decision-making process. For instance, a familiar headline could have evoked an intuition for the article being real before reading the article body. On the other hand, one could argue that this was beneficial for humans as they could improve assessments by comparing contents in the headline and subheading to the article body.\nStill, these methodological decisions systematically favored the machine classifiers over the human evaluators. However, asserting that the machine superiority would evaporate based on these considerations is a reach considering how vast the performance gap was.\nAnother limitation relates to the generalizability of the synthetic news articles. Due to experimen-"
        },
        {
            "heading": "Article A",
            "text": ""
        },
        {
            "heading": "Article C",
            "text": "tal constraints, articles were shortened greatly, and may therefore not be comparable to what we consider news in a real-world context. In addition, even if it could write longer articles, our fine-tuned GPT3 model\u2019s capabilities are practically useless in a journalistic context despite producing human-like outputs. This is because inferring a factually correct article body from just a headline requires additional, current context about the world which is inaccessible in this setup. Instead, the only thinkable purposes for this \"headline-to-article news generator\" have malicious undertones such as automating fake news production."
        },
        {
            "heading": "8 Conclusion",
            "text": "As advancements in natural language processing continue to progress rapidly, it is crucial to remember the importance of including and improving upon NLP in low-resource languages. This paper acknowledges this need by conducting a structured assessment of GPT-3\u2019s abilities for Danish natural language generation when fine-tuned for the task.\nOur study shows that GPT-3 can be fine-tuned to produce Danish synthetic news articles that are virtually indistinguishable to real news articles for humans. However, this does not imply that the articles are actually indistinguishable as the human eye is not all-seeing. By constructing a fine-tuned BERT model for the same discrimination task, we\nfind that machine detection of the synthetic news articles was possible to a great extent. Hence, there must have been underlying flaws in GPT-3\u2019s article generations, likely relating to an oversampling of high-likelihood words.\nThe introduction of ChatGPT and GPT-4 will likely impact the findings presented in this paper, lowering detection accuracies further for both humans and machines. Although, as those models are closed-sourced, it would be troublesome to assess whether the testing articles are already part of the training data which poses a methodological challenge. Regardless, as our findings for Danish conform with similar studies in English, we encourage future work on low-resource languages to develop machine detectors which possibly stand the test when human evaluators are deceived."
        },
        {
            "heading": "Supplementary Materials Availability Statement:",
            "text": "All source code used in the project is available from GitHub at https://github.com/drasbaek/finetuninggpt3-danish-news. A dataset with the synthetic articles as well as classifications made by machine detectors is also available on the GitHub. The dataset containing human responses from Experiment A cannot be made available due to GDPR regulations. The real news articles from TV2 are also not made publicly available due to copyright limitations. In the interest of reproducibility, dummy data is made available on the GitHub which mimics the actual data to the greatest possible extent under the circumstances. Contact the authors for more information on the project."
        },
        {
            "heading": "Ethical Considerations",
            "text": "In this paper, we have created a GPT-3 fine-tune that is capable of producing synthetic news. As it may be possible to use it for malicious purposes, the fine-tuned model will not be available to anyone besides the authors. Per January 4, 2024, the authors will also lose access to the model as OpenAI announced all davinci models, including fine-tunes, will depreciate. 7 Nonetheless, we acknowledge that this paper demonstrates the ease of producing such a model, but also how it may be detected.\nFinally, we recognize that the synthetic news produced for this paper could potentially contain societal biases from GPT-3\u2019s training data or from the real news articles used for fine-tuning.\n7https://openai.com/blog/gpt-4-api-general-availability"
        },
        {
            "heading": "Acknowledgements",
            "text": "The expenses associated with fine-tuning and using GPT-3 were funded by a grant from Apart Research, a machine learning safety research organization. This funding did not grant Apart Research any influence on the course of the study. The authors would also like to thank Ross Deans KristensenMcLachlan for supervising the bachelor\u2019s thesis which laid the foundation for the present paper. Finally, we thank IT-vest and Stibofonden for funding the publication of this paper at INLG 2023."
        },
        {
            "heading": "A Appendix",
            "text": ""
        },
        {
            "heading": "A.1 Fine-tuning Parameters for GPT-3",
            "text": "Parameters Value Batch Size 2 Learning Rate Multiplier 0.2 Prompt Loss Weight 0.01 Epochs 4"
        },
        {
            "heading": "A.2 Text Generation Parameters for GPT-3",
            "text": "Parameters Value Value Range Temperature 0 0 to 1 Frequency Penalty 0.2 -2 to 2 Presence Penalty 0.2 -2 to 2 Max Tokens 400 0 to 2048"
        },
        {
            "heading": "A.3 Logistic Regression Parameters for BOW and TFIDF",
            "text": "Parameters Value Solver lfbgs C 10 Random State 2 Max Iterations 250"
        },
        {
            "heading": "A.4 Fine-Tuning Parameters for BERT",
            "text": "Parameters Value Learning Rate 2e-5 Weight Decay 0.01 Batch Size 24 Epochs run 2 Max Epochs 5\nThe fine-tuned BERT was defined with an early stopping callback which stopped model training if the validation accuracy did not improve for 3 epochs. The final model used for inference was thus only run for 2 epochs."
        },
        {
            "heading": "A.5 Experimental Procedure",
            "text": "Illustration of a trial from experiment A. All text was written in Danish in the actual experiment. The article body in the example is synthetically generated. The captions \"HEADLINE\", \"SUBHEADING\" and \"ARTICLE BODY\" did not appear in the actual experiment."
        },
        {
            "heading": "A.6 Logistic Regression Model Output for Predicting Accuracy",
            "text": "Fixed Effect Estimate Standard Error Z-value P-value Intercept 0.33668 0.39335 0.856 0.39204 News_Consumption_2 -0.50311 0.43260 -1.163 0.24484 News_Consumption _3 -0.03473 0.39697 -0.087 0.93028 News_Consumption_4 -0.27516 0.40664 -0.677 0.49862 News_Consumption_5 -0.10105 0.39719 -0.254 0.79817 GPT_Knowledge_2 0.32738 0.13130 2.493 0.01266 GPT_Knowledge_3 0.47842 0.14626 3.271 0.00107 GPT_Knowledge_4 0.37824 0.22513 1.680 0.09293\nFixed Effect Level Participant Response (translated) News_Consumption_1 Never read the news News_Consumption_2 Very rarely read the news News_Consumption_3 Read news every week but not daily News_Consumption_4 Read news once every day News_Consumption_5 Read news multiple times a day GPT_Knowledge_1 Never heard of GPT-3 GPT_Knowledge_2 Heard of GPT-3, but never read anything it wrote or worked with it GPT_Knowledge_3 Heard of GPT-3 and read texts it wrote, but never worked with it GPT_Knowledge_4 Heard of GPT-3, read texts it wrote and\nworked with it\nParticipant Response (original)\nL\u00e6ser aldrig nyheder L\u00e6ser meget sj\u00e6ldent nyheder L\u00e6ser nyheder hver uge men ikke dagligt L\u00e6ser nyheder en gang om dagen L\u00e6ser nyheder flere gange om dagen Aldrig h\u00f8rt om GPT-3 H\u00f8rt om GPT-3, men aldrig l\u00e6st noget den har skrevet eller arbejdet med den H\u00f8rt om GPT-3 og l\u00e6st tekster den har skrevet, men aldrig arbejdet med den H\u00f8rt om GPT-3, l\u00e6st tekster den har skrevet og arbejdet med den"
        },
        {
            "heading": "A.7 Classifier Agreement (Table 2) Danish Original Text",
            "text": ""
        },
        {
            "heading": "Article A",
            "text": "Correct Human BOW TF-IDF BERT Synthetic Real Synthetic Synthetic Synthetic Gr\u00f8nlands regering har besluttet sig for ikke at s\u00f8ge om tilladelse til yderligere olieboringer i det kommende \u00e5r. Det oplyser det gr\u00f8nlandske ministerium for natur, milj\u00f8 og landbrug i en pressemeddelelse. - Vi har besluttet os for ikke at s\u00f8ge om olieboringer i 2023, fordi vi vil bruge tid p\u00e5 at udvikle en ny strategi for den gr\u00f8nlandske \u00f8konomi, som skal danne grundlag for en ny olie- og gasstrategi, lyder det. Regeringen understreger samtidig, at den vil fastholde sin \"vision om et fossilfrit Gr\u00f8nland\". Beslutningen kommer efter et m\u00f8de tirsdag mellem regeringens fire partier. Det er is\u00e6r hensynet til klimaet, der har f\u00e5et regeringen til at droppe yderligere olieboringer. Article B Correct Human BOW TF-IDF BERT Synthetic Synthetic Real Real Synthetic To fotografer og en kulturminister f\u00e5r nu kritik af Pressen\u00e6vnet for at have deltaget i en billedserie, hvor de poserede med v\u00e5ben. Det skriver Pressen\u00e6vnet i en pressemeddelelse. I sagen mod kulturminister Ane Halsboe-J\u00f8rgensen (S) har n\u00e6vnet vurderet, at hun har brudt god presseskik ved at deltage i billedserien \u2019The Gun Series\u2019. - Kulturministeren har ved deltagelse i billedserie med v\u00e5ben og ammunition givet udtryk for, at det er acceptabelt at b\u00e6re v\u00e5ben, uanset om det er i forbindelse med kunstnerisk fotografering eller ej, lyder det i afg\u00f8relsen. Afg\u00f8relsen mod fotografen Rasmus Flindt Pedersen og Jim Lyngvild er mere knibsk. Begge har brudt god presseskik ved deltagelse i billedserien, mener Pressen\u00e6vnet."
        },
        {
            "heading": "Article C",
            "text": "Correct Human BOW TF-IDF BERT Real Real Real Real Synthetic Hensynet til truede dyrearter som hasselmus, birkemus og flagermus i Danmark stikker nu en midlertidig k\u00e6p i hjulet p\u00e5 et enormt naturgasprojekt, der skal forsyne Polen med naturgas fra Norge. Det er Milj\u00f8- og F\u00f8devareklagen\u00e6vnet, der har annulleret projektets milj\u00f8tilladelse, og dermed har sat en stopper for anl\u00e6gsarbejdet af r\u00f8rledningen Baltic Pipe p\u00e5 tv\u00e6rs af Danmark. - Vi er meget kede af afg\u00f8relsen, siger Marian Kaagh, der er vicedirekt\u00f8r i selskabet Energinet, der st\u00e5r for anl\u00e6gsarbejdet I Danmark. I en pressemeddelelse siger hun, at Energinet har arbejdet med en r\u00e6kke tiltag for at sikre gode levevilk\u00e5r for dyrene de steder, hvor r\u00f8rledningen bliver anlagt. Det var et krav, da Milj\u00f8styrelsen i 2019 gav milj\u00f8tilladelse til Baltic Pipe-projektet. Men if\u00f8lge Milj\u00f8- og F\u00f8devareklagen\u00e6vnet burde forholdene v\u00e6re grundigt unders\u00f8gt, allerede inden tilladelsen blev udstedt, og anl\u00e6gsarbejdet kunne begynde. Article D Correct Human BOW TF-IDF BERT Synthetic Real Real Real Real De kommende supersygehuse skal v\u00e6re med til at l\u00f8fte sundhedsv\u00e6senet i Danmark. Men de bliver ikke klar til tiden. Gennemsnitligt er de 16 sygehusbyggerier knap to \u00e5r forsinkede, viser en opg\u00f8relse fra Kvalitetsfonden for sygehusbyggerierne, som TV 2 har f\u00e5et aktindsigt i. Det er et udtryk for, at der er \"en del udfordringer\", som fondens direkt\u00f8r, Morten Hjortenberg, siger det. - Vi havde h\u00e5bet p\u00e5 bedre resultater her halvvejs inde i byggeperioden. Det giver anledning til bekymring og eftertanke om nogle af de beslutninger og prioriteringer, der blev truffet under projekterne, siger han. Fondens opgave er at stille penge til r\u00e5dighed for sygehusbyggerierne og sikre en h\u00f8j kvalitet \u2013 alts\u00e5 det man ofte kalder \"kvalitetsfonde\". Byggeriernes samlede budget er p\u00e5 over 30 milliarder kroner \u2013 heraf st\u00e5r staten for 23 milliarder og regionernes selvfinansierende bidrag p\u00e5 9 milliarder."
        }
    ],
    "title": "Fine-Tuning GPT-3 for Synthetic Danish News Generation",
    "year": 2023
}