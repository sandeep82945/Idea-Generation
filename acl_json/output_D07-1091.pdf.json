{
    "abstractText": "We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level \u2014 may it be linguistic markup or automatically generated word classes. In a number of experiments we show that factored translation models lead to better translation performance, both in terms of automatic scores, as well as more grammatical coherence.",
    "authors": [
        {
            "affiliations": [],
            "name": "Philipp Koehn"
        },
        {
            "affiliations": [],
            "name": "Hieu Hoang"
        }
    ],
    "id": "SP:08a8a77b84ca10a190c367517ca0d1eeb0b0379c",
    "references": [
        {
            "authors": [
                "H. Alshawi",
                "S. Bangalore",
                "S. Douglas"
            ],
            "title": "Automatic acquisition of hierarchical transduction models for machine translation",
            "venue": "Proceedings of the 36th Annual Meeting of the Association of Computational Linguistics (ACL).",
            "year": 1998
        },
        {
            "authors": [
                "A. Birch",
                "M. Osborne",
                "P. Koehn"
            ],
            "title": "CCG supertags in factored statistical machine translation",
            "venue": "Proceedings of the Second Workshop on Statistical Machine Translation, pages 9\u201316, Prague, Czech Republic. Association for Computational Linguistics.",
            "year": 2007
        },
        {
            "authors": [
                "O. Bojar"
            ],
            "title": "English-to-Czech factored machine translation",
            "venue": "Proceedings of the Second Workshop on Statistical Machine Translation, pages 232\u2013239, Prague, Czech Republic. Association for Computational Linguistics.",
            "year": 2007
        },
        {
            "authors": [
                "E. Brill"
            ],
            "title": "Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging",
            "venue": "Computational Linguistics, 21(4).",
            "year": 1995
        },
        {
            "authors": [
                "M. Collins",
                "P. Koehn",
                "I. Kucerova"
            ],
            "title": "Clause restructuring for statistical machine translation",
            "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), pages 531\u2013540, Ann Arbor, Michigan. Association for Computational Linguistics.",
            "year": 2005
        },
        {
            "authors": [
                "M. Galley",
                "J. Graehl",
                "K. Knight",
                "D. Marcu",
                "S. DeNeefe",
                "W. Wang",
                "I. Thayer"
            ],
            "title": "Scalable inference and training of context-rich syntactic translation models",
            "venue": "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Associa-",
            "year": 2006
        },
        {
            "authors": [
                "P. Koehn",
                "M. Federico",
                "W. Shen",
                "N. Bertoldi",
                "H. Hoang",
                "C. Callison-Burch",
                "B. Cowan",
                "R. Zens",
                "C. Dyer",
                "O. Bojar",
                "C. Moran",
                "A. Constantin",
                "E. Herbst"
            ],
            "title": "Open source toolkit for statistical machine translation: Factored translation models and confusion network decoding",
            "venue": "Tech-",
            "year": 2006
        },
        {
            "authors": [
                "P. Koehn",
                "H. Hoang",
                "A. Birch",
                "C. Callison-Burch",
                "M. Federico",
                "N. Bertoldi",
                "B. Cowan",
                "W. Shen",
                "C. Moran",
                "R. Zens",
                "C. Dyer",
                "O. Bojar",
                "A. Constantin",
                "E. Herbst"
            ],
            "title": "Moses: Open source toolkit for statistical machine translation",
            "venue": "Proceedings of the Annual Meeting of the Associa-",
            "year": 2007
        },
        {
            "authors": [
                "P. Koehn",
                "K. Knight"
            ],
            "title": "Feature-rich translation of noun phrases",
            "venue": "41st Annual Meeting of the Association of Computational Linguistics (ACL).",
            "year": 2003
        },
        {
            "authors": [
                "P. Koehn",
                "C. Monz"
            ],
            "title": "Manual and automatic evaluation of machine translation between European languages",
            "venue": "Proceedings on the Workshop on Statistical Machine Translation, pages 102\u2013121, New York City. Association for Computational Linguistics.",
            "year": 2006
        },
        {
            "authors": [
                "P. Koehn",
                "F.J. Och",
                "D. Marcu"
            ],
            "title": "Statistical phrase based translation",
            "venue": "Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL).",
            "year": 2003
        },
        {
            "authors": [
                "P. Koehn",
                "J. Schroeder"
            ],
            "title": "Experiments in domain adaptation for statistical machine translation",
            "venue": "Proceedings of the Second Workshop on Statistical Machine Translation, pages 224\u2013227, Prague, Czech Republic. Association for Computational Linguistics.",
            "year": 2007
        },
        {
            "authors": [
                "Lee",
                "Y.-S."
            ],
            "title": "Morphological analysis for statistical machine translation",
            "venue": "Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL).",
            "year": 2004
        },
        {
            "authors": [
                "L.V. Lita",
                "A. Ittycheriah",
                "S. Roukos",
                "N. Kambhatla"
            ],
            "title": "tRuEcasIng",
            "venue": "Hinrichs, E. and Roth, D., editors, Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 152\u2013159.",
            "year": 2003
        },
        {
            "authors": [
                "I.D. Melamed"
            ],
            "title": "Statistical machine translation by parsing",
            "venue": "Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL\u201904), Main Volume, pages 653\u2013660, Barcelona, Spain.",
            "year": 2004
        },
        {
            "authors": [
                "A. Menezes",
                "C. Quirk"
            ],
            "title": "Microsoft research treelet translation system: IWSLT evaluation",
            "venue": "Proc. of the International Workshop on Spoken Language Translation.",
            "year": 2005
        },
        {
            "authors": [
                "S. Nie\u00dfen",
                "H. Ney"
            ],
            "title": "Toward hierarchical models for statistical machine translation of inflected languages",
            "venue": "Workshop on Data-Driven Machine Translation at 39th Annual Meeting of the Association of Computational Linguistics (ACL), pages 47\u201354.",
            "year": 2001
        },
        {
            "authors": [
                "F.J. Och"
            ],
            "title": "Minimum error rate training for statistical machine translation",
            "venue": "Proceedings of the 41st Annual Meeting of the Association of Computational Linguistics (ACL).",
            "year": 2003
        },
        {
            "authors": [
                "F.J. Och",
                "D. Gildea",
                "S. Khudanpur",
                "A. Sarkar",
                "K. Yamada",
                "A. Fraser",
                "S. Kumar",
                "L. Shen",
                "D. Smith",
                "K. Eng",
                "V. Jain",
                "Z. Jin",
                "D. Radev"
            ],
            "title": "A smorgasbord of features for statistical machine translation",
            "venue": "Proceedings of the Joint Conference on Human Language Technologies and",
            "year": 2004
        },
        {
            "authors": [
                "F. Sadat",
                "N. Habash"
            ],
            "title": "Combination of arabic preprocessing schemes for statistical machine translation",
            "venue": "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1\u20138, Sydney,",
            "year": 2006
        },
        {
            "authors": [
                "H. Schmidt",
                "S. Schulte im Walde"
            ],
            "title": "Robust German noun chunking with a probabilistic context-free grammar",
            "venue": "Proceedings of the International Conference on Computational Linguistics (COLING).",
            "year": 2000
        },
        {
            "authors": [
                "W. Shen",
                "R. Zens",
                "N. Bertoldi",
                "M. Federico"
            ],
            "title": "The JHU Workshop 2006 IWSLT System",
            "venue": "Proc. of the International Workshop on Spoken Language Translation, pages 59\u201363, Kyoto, Japan.",
            "year": 2006
        },
        {
            "authors": [
                "D. Talbot",
                "M. Osborne"
            ],
            "title": "Modelling lexical redundancy for machine translation",
            "venue": "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 969\u2013976, Sydney, Australia. Association",
            "year": 2006
        },
        {
            "authors": [
                "W. Wang",
                "K. Knight",
                "D. Marcu"
            ],
            "title": "Capitalizing machine translation",
            "venue": "Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL).",
            "year": 2006
        },
        {
            "authors": [
                "D. Wu"
            ],
            "title": "Stochastic inversion transduction grammars and bilingual parsing of parallel corpora",
            "venue": "Computational Linguistics, 23(3).",
            "year": 1997
        },
        {
            "authors": [
                "K. Yamada",
                "K. Knight"
            ],
            "title": "A syntax-based statistical translation model",
            "venue": "Proceedings of the 39th Annual Meeting of the Association of Computational Linguistics (ACL).",
            "year": 2001
        },
        {
            "authors": [
                "M. Yang",
                "K. Kirchhoff"
            ],
            "title": "Phrase-based backoff models for machine translation of highly inflected languages",
            "venue": "Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL).",
            "year": 2006
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 868\u2013876, Prague, June 2007. c\u00a92007 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "The current state-of-the-art approach to statistical machine translation, so-called phrase-based models, is limited to the mapping of small text chunks without any explicit use of linguistic information, may it be morphological, syntactic, or semantic. Such additional information has been demonstrated to be valuable by integrating it in pre-processing or postprocessing steps.\nHowever, a tighter integration of linguistic information into the translation model is desirable for two reasons:\n\u2022 Translation models that operate on more general representations, such as lemmas instead of surface forms of words, can draw on richer statistics and overcome the data sparseness problems caused by limited training data.\n\u2022 Many aspects of translation can be best explained on a morphological, syntactic, or semantic level. Having such information available to the translation model allows the direct modeling of these aspects. For instance: reordering at the sentence level is mostly driven\nby general syntactic principles, local agreement constraints show up in morphology, etc.\nTherefore, we extended the phrase-based approach to statistical translation to tightly integrate additional information. The new approach allows additional annotation at the word level. A word in our framework is not only a token, but a vector of factors that represent different levels of annotation (see Figure 1).\nWe report on experiments with factors such as surface form, lemma, part-of-speech, morphological features such as gender, count and case, automatic word classes, true case forms of words, shallow syntactic tags, as well as dedicated factors to ensure agreement between syntactically related items.\nThis paper describes the motivation, the modeling aspects and the computationally efficient decoding methods of factored translation models. We present briefly results for a number of language pairs. However, the focus of this paper is the description of the approach. Detailed experimental results will be described in forthcoming papers.\n868"
        },
        {
            "heading": "2 Related Work",
            "text": "Many attempts have been made to add richer information to statistical machine translation models. Most of these focus on the pre-processing of the input to the statistical system, or the post-processing of its output. Our framework is more general and goes beyond recent work on models that back off to representations with richer statistics (Nie\u00dfen and Ney, 2001; Yang and Kirchhoff, 2006; Talbot and Osborne, 2006) by keeping a more complex representation throughout the translation process.\nRich morphology often poses a challenge to statistical machine translation, since a multitude of word forms derived from the same lemma fragment the data and lead to sparse data problems. If the input language is morphologically richer than the output language, it helps to stem or segment the input in a pre-processing step, before passing it on to the translation system (Lee, 2004; Sadat and Habash, 2006).\nStructural problems have also been addressed by pre-processing: Collins et al. (2005) reorder the input to a statistical system to closer match the word order of the output language.\nOn the other end of the translation pipeline, additional information has been used in post-processing. Och et al. (2004) report minor improvements with linguistic features on a Chinese-English task, Koehn and Knight (2003) show some success in re-ranking noun phrases for German-English. In their approaches, first, an n-best list with the best translations is generated for each input sentence. Then, the n-best list is enriched with additional features, for instance by syntactically parsing each candidate translation and adding a parse score. The additional features are used to rescore the n-best list, resulting possibly in a better best translation for the sentence.\nThe goal of integrating syntactic information into the translation model has prompted many researchers to pursue tree-based transfer models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Melamed, 2004; Menezes and Quirk, 2005; Galley et al., 2006), with increasingly encouraging results. Our goal is complementary to these efforts: we are less interested in recursive syntactic structure, but in richer annotation at the word level. In future work, these approaches may be combined.\nOutputInput"
        },
        {
            "heading": "3 Motivating Example: Morphology",
            "text": "One example to illustrate the short-comings of the traditional surface word approach in statistical machine translation is the poor handling of morphology. Each word form is treated as a token in itself. This means that the translation model treats, say, the word house completely independent of the word houses. Any instance of house in the training data does not add any knowledge to the translation of houses.\nIn the extreme case, while the translation of house may be known to the model, the word houses may be unknown and the system will not be able to translate it. While this problem does not show up as strongly in English \u2014 due to the very limited morphological inflection in English \u2014 it does constitute a significant problem for morphologically rich languages such as Arabic, German, Czech, etc.\nThus, it may be preferably to model translation between morphologically rich languages on the level of lemmas, and thus pooling the evidence for different word forms that derive from a common lemma. In such a model, we would want to translate lemma and morphological information separately, and combine this information on the output side to ultimately generate the output surface words.\nSuch a model can be defined straight-forward as a factored translation model. See Figure 2 for an illustration of this model in our framework.\nNote that while we illustrate the use of factored translation models on such a linguistically motivated\nexample, our framework also applies to models that incorporate statistically defined word classes, or any other annotation."
        },
        {
            "heading": "4 Decomposition of Factored Translation",
            "text": "The translation of factored representations of input words into the factored representations of output words is broken up into a sequence of mapping steps that either translate input factors into output factors, or generate additional output factors from existing output factors.\nRecall the example of a factored model motivated by morphological analysis and generation. In this model the translation process is broken up into the following three mapping steps:\n1. Translate input lemmas into output lemmas\n2. Translate morphological and POS factors\n3. Generate surface forms given the lemma and linguistic factors\nFactored translation models build on the phrasebased approach (Koehn et al., 2003) that breaks up the translation of a sentence into the translation of small text chunks (so-called phrases). This approach implicitly defines a segmentation of the input and output sentences into phrases. See an example in Figure 3.\nOur current implementation of factored translation models follows strictly the phrase-based approach, with the additional decomposition of phrase translation into a sequence of mapping steps. Translation steps map factors in input phrases to factors in output phrases. Generation steps map output factors within individual output words. To reiterate: all translation steps operate on the phrase level, while all generation steps operate on the word level. Since all mapping steps operate on the same phrase segmentation of the input and output sentence into phrase pairs, we call these synchronous factored models.\nLet us now take a closer look at one example, the translation of the one-word phrase ha\u0308user into English. The representation of ha\u0308user in German is: surface-form ha\u0308user | lemma haus | part-of-speech NN | count plural | case nominative | gender neutral.\nThe three mapping steps in our morphological analysis and generation model may provide the following applicable mappings:\n1. Translation: Mapping lemmas \u2022 haus \u2192 house, home, building, shell\n2. Translation: Mapping morphology \u2022 NN|plural-nominative-neutral \u2192\nNN|plural, NN|singular\n3. Generation: Generating surface forms \u2022 house|NN|plural \u2192 houses \u2022 house|NN|singular \u2192 house \u2022 home|NN|plural \u2192 homes \u2022 ...\nWe call the application of these mapping steps to an input phrase expansion. Given the multiple choices for each step (reflecting the ambiguity in translation), each input phrase may be expanded into a list of translation options. The German ha\u0308user|haus|NN|plural-nominative-neutral may be expanded as follows:\n1. Translation: Mapping lemmas { ?|house|?|?, ?|home|?|?, ?|building|?|?, ?|shell|?|? }\n2. Translation: Mapping morphology { ?|house|NN|plural, ?|home|NN|plural, ?|building|NN|plural, ?|shell|NN|plural, ?|house|NN|singular, ... }\n3. Generation: Generating surface forms { houses|house|NN|plural, homes|home|NN|plural, buildings|building|NN|plural, shells|shell|NN|plural, house|house|NN|singular, ... }"
        },
        {
            "heading": "5 Statistical Model",
            "text": "Factored translation models follow closely the statistical modeling approach of phrase-based models (in fact, phrase-based models are a special case of factored models). The main difference lies in the preparation of the training data and the type of models learned from the data."
        },
        {
            "heading": "5.1 Training",
            "text": "The training data (a parallel corpus) has to be annotated with the additional factors. For instance, if we want to add part-of-speech information on the input and output side, we need to obtain part-of-speech tagged training data. Typically this involves running automatic tools on the corpus, since manually annotated corpora are rare and expensive to produce.\nNext, we need to establish a word-alignment for all the sentences in the parallel training corpus. Here, we use the same methodology as in phrase-based models (typically symmetrized GIZA++ alignments). The word alignment methods may operate on the surface forms of words, or on any of the other factors. In fact, some preliminary experiments have shown that word alignment based on lemmas or stems yields improved alignment quality.\nEach mapping step forms a component of the overall model. From a training point of view this means that we need to learn translation and generation tables from the word-aligned parallel corpus and define scoring methods that help us to choose between ambiguous mappings.\nPhrase-based translation models are acquired from a word-aligned parallel corpus by extracting all phrase-pairs that are consistent with the word alignment. Given the set of extracted phrase pairs with counts, various scoring functions are estimated, such as conditional phrase translation probabilities based on relative frequency estimation or lexical translation probabilities based on the words in the phrases.\nIn our approach, the models for the translation steps are acquired in the same manner from a wordaligned parallel corpus. For the specified factors in the input and output, phrase mappings are extracted. The set of phrase mappings (now over factored representations) is scored based on relative counts and word-based translation probabilities.\nThe generation distributions are estimated on the output side only. The word alignment plays no role here. In fact, additional monolingual data may be used. The generation model is learned on a word-for-word basis. For instance, for a generation step that maps surface forms to part-of-speech, a table with entries such as (fish,NN) is constructed. One or more scoring functions may be defined over this table, in our experiments we used both conditional probability distributions, e.g., p(fish|NN) and p(NN|fish), obtained by maximum likelihood estimation.\nAn important component of statistical machine translation is the language model, typically an ngram model over surface forms of words. In the framework of factored translation models, such sequence models may be defined over any factor, or any set of factors. For factors such as part-of-speech tags, building and using higher order n-gram models (7-gram, 9-gram) is straight-forward."
        },
        {
            "heading": "5.2 Combination of Components",
            "text": "As in phrase-based models, factored translation models can be seen as the combination of several components (language model, reordering model, translation steps, generation steps). These components define one or more feature functions that are combined in a log-linear model:\np(e|f) = 1 Z\nexp n\u2211\ni=1\n\u03bbihi(e, f) (1)\nZ is a normalization constant that is ignored in practice. To compute the probability of a translation e given an input sentence f, we have to evaluate each feature function hi. For instance, the feature function for a bigram language model component is (m is the number of words ei in the sentence e):\nhLM(e, f) = pLM(e) = p(e1) p(e2|e1)..p(em|em\u22121)\n(2)\nLet us now consider the feature functions introduced by the translation and generation steps of factored translation models. The translation of the input sentence f into the output sentence e breaks down to a set of phrase translations {(f\u0304j , e\u0304j)}.\nFor a translation step component, each feature function hT is defined over the phrase pairs (f\u0304j , e\u0304j)\ngiven a scoring function \u03c4 : hT(e, f) = \u2211\nj\n\u03c4(f\u0304j , e\u0304j) (3)\nFor a generation step component, each feature function hG given a scoring function \u03b3 is defined over the output words ek only:\nhG(e, f) = \u2211\nk\n\u03b3(ek) (4)\nThe feature functions follow from the scoring functions (\u03c4 , \u03b3) acquired during the training of translation and generation tables. For instance, recall our earlier example: a scoring function for a generation model component that is a conditional probability distribution between input and output factors, e.g., \u03b3(fish,NN,singular) = p(NN|fish).\nThe feature weights \u03bbi in the log-linear model are determined using a minimum error rate training method, typically Powell\u2019s method (Och, 2003)."
        },
        {
            "heading": "5.3 Efficient Decoding",
            "text": "Compared to phrase-based models, the decomposition of phrase translation into several mapping steps creates additional computational complexity. Instead of a simple table look-up to obtain the possible translations for an input phrase, now multiple tables have to be consulted and their content combined.\nIn phrase-based models it is easy to identify the entries in the phrase table that may be used for a specific input sentence. These are called translation options. We usually limit ourselves to the top 20 translation options for each input phrase.\nThe beam search decoding algorithm starts with an empty hypothesis. Then new hypotheses are generated by using all applicable translation options. These hypotheses are used to generate further hypotheses in the same manner, and so on, until hypotheses are created that cover the full input sentence. The highest scoring complete hypothesis indicates the best translation according to the model.\nHow do we adapt this algorithm for factored translation models? Since all mapping steps operate on the same phrase segmentation, the expansions of these mapping steps can be efficiently pre-computed prior to the heuristic beam search, and stored as translation options. For a given input phrase, all possible translation options are thus computed before\ndecoding (recall the example in Section 4, where we carried out the expansion for one input phrase). This means that the fundamental search algorithm does not change.\nHowever, we need to be careful about combinatorial explosion of the number of translation options given a sequence of mapping steps. In other words, the expansion may create too many translation options to handle. If one or many mapping steps result in a vast increase of (intermediate) expansions, this may be become unmanageable. We currently address this problem by early pruning of expansions, and limiting the number of translation options per input phrase to a maximum number, by default 50. This is, however, not a perfect solution. We are currently working on a more efficient search for the top 50 translation options to replace the current bruteforce approach."
        },
        {
            "heading": "6 Experiments",
            "text": "We carried out a number of experiments using the factored translation model framework, incorporating both linguistic information and automatically generated word classes.\nThis work is implemented as part of the open source Moses1 system (Koehn et al., 2007). We used the default settings for this system."
        },
        {
            "heading": "6.1 Syntactically Enriched Output",
            "text": "In the first set of experiments, we translate surface forms of words and generate additional output factors from them (see Figure 4 for an illustration). By adding morphological and shallow syntactic infor-\n1available at http://www.statmt.org/moses/\nmation, we are able to use high-order sequence models (just like n-gram language models over words) in order to support syntactic coherence of the output. Table 1 summarizes the experimental results.\nThe English\u2013German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006). Adding part-of-speech and morphological factors on the output side and exploiting them with 7-gram sequence models results in minor improvements in BLEU. The model that incorporates both POS and morphology (18.22% BLEU vs. baseline 18.04% BLEU) ensures better local grammatical coherence. The baseline system produces often phrases such as zur(to) zwischenstaatlichen(inter-governmental) methoden(methods), with a mismatch between the determiner (singular) and the noun (plural), while the adjective is ambiguous. In a manual evaluation of intra-NP agreement we found that the factored model reduced the disagreement error within noun phrases of length \u2265 3 from 15% to 4%.\nEnglish\u2013Spanish systems were trained on a 40,000 sentence subset of the Europarl corpus. Here, we also used morphological and part-of-speech fac-\ntors on the output side with an 7-gram sequence model, resulting in absolute improvements of 1.25% (only morph) and 0.84% (morph+POS). Improvements on the full Europarl corpus are smaller.\nEnglish-Czech systems were trained on a 20,000 sentence Wall Street Journal corpus. Morphological features were exploited with a 7-gram language model. Experimentation suggests that it is beneficial to carefully consider which morphological features to be used. Adding all features results in lower performance (27.04% BLEU), than considering only case, number and gender (27.45% BLEU) or additionally verbial (person, tense, and aspect) and prepositional (lemma and case) morphology (27.62% BLEU). All these models score well above the baseline of 25.82% BLEU.\nAn extended description of these experiments is in the JHU workshop report (Koehn et al., 2006)."
        },
        {
            "heading": "6.2 Morphological Analysis and Generation",
            "text": "The next model is the one described in our motivating example in Section 4 (see also Figure 2). Instead of translating surface forms of words, we translate word lemma and morphology separately, and generate the surface form of the word on the output side.\nWe carried out experiments for the language pair German\u2013English, using the 52,185 sentence News Commentary corpus2. We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006). German morphological analysis and POS tagging was done using LoPar Schmidt and Schulte im Walde (2000), English POS tagging was done with Brill\u2019s tagger (Brill, 1995), followed by a simple lemmatizer based on tagging results.\nExperimental results are summarized in Table 2. For this data set, we also see an improvement when using a part-of-speech language model \u2014 the BLEU score increases from 18.19% to 19.05% \u2014 consistent with the results reported in the previous section. However, moving from a surface word translation mapping to a lemma/morphology mapping leads to a deterioration of performance to a BLEU score of 14.46%.\nNote that this model completely ignores the surface forms of input words and only relies on the\n2Made available for the WMT07 workshop shared task http://www.statmt.org/wmt07/\nmore general lemma and morphology information. While this allows the translation of word forms with known lemma and unknown surface form, on balance it seems to be disadvantage to throw away surface form information.\nTo overcome this problem, we introduce an alternative path model: Translation options in this model may come either from the surface form model or from the lemma/morphology model we just described. For surface forms with rich evidence in the training data, we prefer surface form mappings, and for surface forms with poor or no evidence in the training data we decompose surface forms into lemma and morphology information and map these separately. The different translation tables form different components in the log-linear model, whose weights are set using standard minimum error rate training methods.\nThe alternative path model outperforms the surface form model with POS LM, with an BLEU score of 19.47% vs. 19.05%. The test set has 3276 unknown word forms vs 2589 unknown lemmas (out of 26,898 words). Hence, the lemma/morph model is able to translate 687 additional words."
        },
        {
            "heading": "6.3 Use of Automatic Word Classes",
            "text": "Finally, we went beyond linguistically motivated factors and carried out experiments with automatically trained word classes. By clustering words together by their contextual similarity, we are able to find statistically similarities that may lead to more generalized and robust models.\nWe trained models on the IWSLT 2006 task (39,953 sentences). Compared to a baseline English\u2013Chinese system, adding word classes on the output side as additional factors (in a model as pre-\nEnglish\u2013Chinese\nModel BLEU baseline (surface) 19.54% surface + word class 21.10%\nOutputInput\nviously illustrated in Figure 4) to be exploited by a 7-gram sequence model, we observe a gain 1.5% BLEU absolute. For more on this experiment, see (Shen et al., 2006)."
        },
        {
            "heading": "6.4 Integrated Recasing",
            "text": "To demonstrate the versatility of the factored translation model approach, consider the task of recasing (Lita et al., 2003; Wang et al., 2006). Typically in statistical machine translation, the training data is lowercased to generalize over differently cased surface forms \u2014 say, the, The, THE \u2014 which necessitates a post-processing step to restore case in the output.\nWith factored translation models, it is possible to integrate this step into the model, by adding a generation step. See Table 4 for an illustration of this model and experimental results on the IWSLT 2006 task (Chinese-English). The integrated recasing model outperform the standard approach with an BLEU score of 21.08% to 20.65%. For more on this experiment, see (Shen et al., 2006)."
        },
        {
            "heading": "6.5 Additional Experiments",
            "text": "Factored translation models have also been used for the integration of CCG supertags (Birch et al., 2007), domain adaptation (Koehn and Schroeder, 2007) and for the improvement of English-Czech translation (Bojar, 2007)."
        },
        {
            "heading": "7 Conclusion and Future Work",
            "text": "We presented an extension of the state-of-the-art phrase-based approach to statistical machine translation that allows the straight-forward integration of additional information, may it come from linguistic tools or automatically acquired word classes.\nWe reported on experiments that showed gains over standard phrase-based models, both in terms of automatic scores (gains of up to 2% BLEU), as well as a measure of grammatical coherence. These experiments demonstrate that within the framework of factored translation models additional information can be successfully exploited to overcome some short-comings of the currently dominant phrasebased statistical approach.\nThe framework of factored translation models is very general. Many more models that incorporate different factors can be quickly built using the existing implementation. We are currently exploring these possibilities, for instance use of syntactic information in reordering and models with augmented input information.\nWe have not addressed all computational problems of factored translation models. In fact, computational problems hold back experiments with more complex factored models that are theoretically possible but too computationally expensive to carry out. Our current focus is to develop a more efficient implementation that will enable these experiments.\nMoreover, we expect to overcome the constraints of the currently implemented synchronous factored models by developing a more general asynchronous framework, where multiple translation steps may operate on different phrase segmentations (for instance a part-of-speech model for large scale reordering)."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was supported in part under the GALE program of the Defense Advanced Research Projects\nAgency, Contract No NR0011-06-C-0022 and in part under the EuroMatrix project funded by the European Commission (6th Framework Programme).\nWe also benefited greatly from a 2006 summer workshop hosted by the Johns Hopkins University and would like thank the other workshop participants for their support and insights, namely Nicola Bertoldi, Ondrej Bojar, Chris CallisonBurch, Alexandra Constantin, Brooke Cowan, Chris Dyer, Marcello Federico, Evan Herbst Christine Moran, Wade Shen, and Richard Zens."
        }
    ],
    "title": "Factored Translation Models",
    "year": 2007
}