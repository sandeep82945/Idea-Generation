{
    "abstractText": "In this study, we propose an effective image fusion method which applies the Wavelet Transform algorithm to the remote sensing image fusion. The wavelet transform decomposes the original image into a series of frequency channels, so two images with distinct characteristics can be fused separately in multiple frequency domains. By applying the Wavelet Transform algorithm, we can get a better effect than other traditional fusion algorithms such as IHS (Intensity, Hue, and Saturation).",
    "authors": [
        {
            "affiliations": [],
            "name": "Shengzhong Zhang"
        }
    ],
    "id": "SP:e8fd9adf6fddc1ffb2e76091a05b8571ff9dd82a",
    "references": [
        {
            "authors": [
                "Yanmei Cui"
            ],
            "title": "A Multi-scale Multi-operator Image Fusion Method Based on Wavelet Transform",
            "venue": "Optical Technique,",
            "year": 1999
        },
        {
            "authors": [
                "Jun Li",
                "Yueqin Zhou",
                "Deren Li"
            ],
            "title": "Wavelet Transform for the Study of High-resolution Panchromatic and Multi-spectral Image Fusion",
            "venue": "Journal of Remote Sensing,",
            "year": 1999
        },
        {
            "authors": [
                "Jun Li",
                "Yueqin Zhou",
                "Deren Li"
            ],
            "title": "Image Local Histogram Matching Filtering Technology Used for Remote Sensing Image Data Fusion",
            "venue": "Acta Geodaetica et Cartographica Sinica,",
            "year": 1999
        },
        {
            "authors": [
                "Yonghong Jia"
            ],
            "title": "Fusion Method for Enhancing the Spatial Resolution of Remote Sensing Multispectral Images",
            "venue": "Remote Sensing Technology and Application,",
            "year": 1997
        },
        {
            "authors": [
                "Kelu Li",
                "Xia Wang",
                "Minru Guo"
            ],
            "title": "Information Fusion Method and Results Evaluation of Wavelet Transform",
            "venue": "Remote Sensing of Land and Resources,",
            "year": 1999
        },
        {
            "authors": [
                "Jianwei Liu",
                "Mengxin Song",
                "Ping Guo"
            ],
            "title": "A Remote Sensing Image Fusion Method Based on HSI and Wavelet Transformation",
            "venue": "Journal of Beijing Normal University (Natural Science),",
            "year": 2008
        }
    ],
    "sections": [
        {
            "text": "Published by Francis Academic Press, UK\n-1-\nTransform algorithm to the remote sensing image fusion. The wavelet transform decomposes the original image into a series of frequency channels, so two images with distinct characteristics can be fused separately in multiple frequency domains. By applying the Wavelet Transform algorithm, we can get a better effect than other traditional fusion algorithms such as IHS (Intensity, Hue, and Saturation).\nKeywords: Wavelet Transform, Remote Sensing, Image Fusion, IHS"
        },
        {
            "heading": "1. Introduction",
            "text": "Image fusion refers to the technology of collecting images about the same target from multiple source channels through a certain process, extracting the information of each channel, and integrating them into a unified image for observation or further processing. The efficient image fusion method can comprehensively process the information of multi-source channels according to the needs, thereby effectively improving the conversion of image information, the reliability of the target detection and recognition of the system, and the degree of system automation. The advantages of these aspects make the application potential of image fusion in medicine, remote sensing, computer vision, weather forecasting, and military target recognition fully recognized. On multiple delivery platforms in aerospace and aviation, the compound fusion of a large number of spectral remote sensing images (among them, the differences in resolution and gray level might be very large) obtained by various remote sensors provides a good processing method for the efficient extraction of information and achieves obvious benefits [1]. In theory, image fusion is relatively simple, but in fact, it is very difficult to achieve real fusion. Even if the theory works, it will encounter difficulties in practical applications, because image fusion technology is a comprehensive technology that integrates multiple disciplines and cross-industries.\nRemote sensing image fusion is a technique for compounding multi-source remote sensing images through advanced image processing technology. Its purpose is to integrate the information of multiple bands of a single sensor or the information provided by different types of sensors to reduce possible redundant multi-sensor information and uncertainty and ambiguity, enhance the target information, transparency of the information in the image to improve the accuracy and reliability of the interpretation. It shows obvious advantages in the following aspects:\n\u2460 Sharpen the image; \u2461 Improve geometric correction accuracy; \u2462 Provide stereo observation\ncapability for stereophotogrammetry; \u2463 Reduce unclear features in the original single data source; \u2464\nComplementary data sets are used to improve the classification quality; \u2465 Use of multi-time domain\ndata for change detection; \u2466 Realize the replacement of the information lost in an image with another\nsensor image data (such as the cloud cover in the visible light band, the shadow of SAR data); \u2467 Overcome the data uncertainty in target extraction and recognition. Remote sensing image fusion has been a research hotspot in the international remote sensing community in recent years [2].\nIn general, image fusion is divided into three levels from low to high: data level, feature level, and decision level fusion. Data level fusion, also known as pixel-level fusion, refers to the process of directly processing the data collected by the sensor to obtain a fused image. It is the foundation of high-level image fusion and is also one of the focuses of current image fusion research [2]. The fusion method referred to in this paper is based on pixel-level fusion.\nThe image fusion processing between different types of remote sensing images must have four\nPublished by Francis Academic Press, UK\n-2-\nconditions, namely:\n\u2460 The fusion image should include different spatial and spectral resolutions; \u2461 The fusion image\nshould be the same area; \u2462 The image should be accurately registered; \u2463 There is no big change in the content of the images acquired at different times [3]."
        },
        {
            "heading": "2. The development of Image Fusion Methods",
            "text": "In the past few decades, there have been many image fusion methods in various literature, such as principal component analysis (PCA), IHS color conversion method, and High-Pass Filtering method (HPF). Teot et al. developed a multi-level image fusion method based on the Laplacian heap. Burt et al. proposed the selection criterion based on the neighborhood, and determined the activity measure of a pixel by calculating the variance in the window centered on a pixel. This fusion method will produce a block effect for the fusion image of the area where the image data difference between the sensors is large. Sheffigara applied PCA to image fusion processing, and used another sensor image to replace the first principal component of the multi-channel remote sensing image (such as TM multispectral image) after principal component transformation to perform fusion processing to improve the spatial resolution of the multi-channel image, and it could stretch the variance and mean of the first principal component; Y\u00e9son et al. confirmed that the principal component replacement method lost its original physical characteristics. The results of Carper et al. showed that IHS color transformation could produce reliable results when the intensity had a good correlation with the full-color image; Chavez adopted a fusion method in which the intensity of the SPOT multispectral data after color conversion was replaced by its panchromatic band image. However, due to the different spectral characteristics of different channel data, the IHS method distorted the original spectral characteristics and produced spectral degradation phenomena. Generally, the HPF method produces a fused image with minimal spectral distortion. However, the results obtained by the above methods depend on the scenery, so the quality of spectral information protection is often unpredictable; Sheffigara adopted a High-Pass Filter (HPF) method for fusion, which used a high-pass filter to filter high spatial resolution images. Then the filtered results were added to multi-spectral image data (low resolution) according to the pixels. Although the method effectively retains the original multispectral information, it filters out most of the texture information when filtering high-resolution images [2] [3]."
        },
        {
            "heading": "2.1. IHS Method",
            "text": "IHS transform has two significant features [4]:\n\u2460 It effectively transforms the red (R), green (G), and blue (B) components of a color image into intensity components which representing spatial information, and hue and saturation components which representing spectral information. This process is called Forward IHS Transform. \u2461 It is reversible and it can transform IHS to RGB, which is called Inverse IHS Transform.\nBecause human eye has a higher resolution for image intensity than for chroma and saturation, according to the characteristics of the human eye's vision and IHS transform, the three-band multispectral image with a low spatial resolution of spatial registration is regarded as blue (B), green (G), and red (R) respectively, and after IHS transform to obtain I, H and S. Then the intensity component is replaced by the high spatial resolution PAN image stretched by contrast (make it gets approximately the same mean and variance as the intensity component image), while the other two components H and S remain unchanged. After the inverse transform of IHS to R', G', B', and then color synthesis, a color image B'G'R' with improved visual resolution is obtained. This is the traditional method of fusion using IHS transform. Although this method can improve the spatial resolution and clarity of multispectral images, it has obvious disadvantages. When an infrared band image is involved in the fusion, the gray value of the multispectral image obtained by the fusion is quite different from the original multispectral image, that is, the spectral characteristics are distorted, which makes interpretation difficult. This is mainly due to the large difference between the high-resolution image and the intensity component, which gets a weak correlation."
        },
        {
            "heading": "2.2. HPF Method",
            "text": "High-Pass Filtering (HPF) fusion is performed by superimposing the geometric information in high-\nPublished by Francis Academic Press, UK\n-3-\nresolution images into low-resolution images pixel by pixel. High-Pass filtering of high-resolution images corresponds to high-frequency components in image space. Therefore, the high-pass filtering of high-resolution images corresponds to high-frequency components in image space.\nMany of the above methods reflect the quality of the fusion image through the improvement of the geometric quality of the image structure information and the improvement of the spatial resolution of the multi-spectral image, which can produce color synthesis results with better visual effects. However, it changes the content of the spectral information. In fact, the fusion method tends to protect the spectral information by generating new channels that are highly correlated with the original image.\nWhen the spatial resolution of the panchromatic image is larger than that of the multispectral image, the practical effect cannot be obtained. In recent years, some new fusion algorithms have emerged for the fusion of multi-resolution image data sets. These methods can preserve the spectral information of the original multispectral image to the greatest extent. The basic idea is a fusion method based on a multi-resolution wavelet transform [3]."
        },
        {
            "heading": "3. Remote sensing image fusion based on wavelet transform",
            "text": ""
        },
        {
            "heading": "3.1. Principle of Remote Sensing Image Fusion Based on Wavelet Transform",
            "text": "Wavelet analysis is a brand-new time/frequency domain signal analysis tool. It can decompose the signal into a series of subband signals with different resolutions, frequency characteristics, and directional characteristics. It is easy to find the correspondence between the transformed wavelet coefficients and the original image content in both spatial and frequency domains. At the same time, the amplitude of the transformed wavelet coefficient changes with the number of decomposition layers to provide the local variation characteristics of the original image grayscale, which provides favorable conditions for the fusion of different remote sensor images.\nThe principle of multi-scale image fusion based on wavelet transform is as follows [1]:\n The wavelet transform is equivalent to decomposing the original image into a series of frequency channels, and then using the pyramid structure to perform fusion processing in different spatial frequency bands of the image, then the details of different images are effectively fused together. And the edges are not abrupt.\n The advantage of multi-scale wavelet decomposition is that two images with distinct characteristics are fused separately in multiple frequency domains. Experiments by Campbell and Robson found that retinal images were processed in different frequency channels. The fusion based on wavelet transform is exactly the fusion transform on different frequency channels. Therefore, it can achieve a visual effect suitable for human eyes.\n The wavelet transform extracts the low frequency of the original image and the high frequency of the three directions respectively. For fusion, due to the different information required, the processing of each part of the information should be different, Therefore, a suitable fusion operator can be selected in specific situations, and the original information can be extracted as needed to obtain a fusion image with remarkable effects. Based on the layered processing of the wavelet pyramid, targeted fusion is performed on different feature domains of each layer. So, the fusion result has an outstanding effect.\n The more layers the wavelet transform has, the richer the fusion frequency range and the richer the details of the fusion result. However, it does not mean that the more the number of layers, the better, because the more the number of decomposed layers, the greater the amount of information lost in the top layer fusion, and they are all irrecoverable losses of the inverse wavelet transform, so the number of layers based on wavelet decomposition should not be too high."
        },
        {
            "heading": "3.2. The Realization of Remote Sensing Image Fusion Based on Wavelet Transform",
            "text": "In this paper, remote sensing images with different resolutions are all images after geometric correction and registration. The fusion process of discrete biorthogonal wavelet transform model includes the following steps:\n One-band high-resolution images and m-band low-resolution images are respectively subjected to\nPublished by Francis Academic Press, UK\n-4-\nn-order biorthogonal wavelet transform to obtain respective low-frequency Low-Low (LL) images and detail/texture images.\n Keeping the low-frequency part of the low-resolution image unchanged, the high-frequency part of the high-resolution image is weighted with the high-frequency part of each low-resolution image to obtain m weighted images.\n The m weighted images are respectively subjected to n-order biorthogonal inverse wavelets transform to obtain m-band fusion images. As shown in Figure 1.\nThe images of the three bands are selected as the input of the three channels of RGB to obtain the\nfalse-color composite image.\nThe weighting coefficient is determined by the user, and the value range is [0, 1]. If the weight of the\nlow-resolution image is \u03b1, the weight of the high-resolution image is (1-\u03b1).\nThe filter lengths of wavelet transform could be 5, 9, and 13, which are selected by the user, together with the transform order and the weighting coefficient \u03b1. IKONOS 4m and 1m, LandSat7 TM 30m, and TM-PAN 15m and TM 30m and SPOT-PAN 10m in Beijing area were used as test images, respectively."
        },
        {
            "heading": "3.3. Factors Affecting the Fusion Effect of Remote Sensing Images",
            "text": ""
        },
        {
            "heading": "3.3.1. The Influence of Wavelet Base on Remote Sensing Image Fusion",
            "text": "The entropy values of the fusion images of the biorthogonal wavelet base with different lengths are higher than those of the original images. Considering the data block size of the wavelet transform, the filter should not be too long, otherwise, the amount of data to be supplemented is too large. Tests were performed with a biorthogonal wavelet base of lengths 5, 9, and 13. When the length of the biorthogonal wavelet base is 5, the fused image is somewhat blurred. Change the filter length to 9, the image is clear, the edge texture is prominent, and the effect is slightly stronger than the IHS fusion image of the same image. When the filter length is 13, the results are similar. Therefore, in practical applications, a filter with a length of 9 can be selected.\nJudging from the clarity of the image after the fusion of wavelet base length of 4~20, when the length of the wavelet base is 4~12, the sharpness of the small-scale texture in the image decreases with the increase of the wavelet base length, while the sharpness of the large-scale texture in the image behaves reverse. When the length of the wavelet base is 12~20, the sharpness of the texture on the smallscale in the image increases with the increase of the length of the wavelet base, while the sharpness of the texture on the large-scale in the image behaves reverse too.\nFrom the theory of wavelet analysis, wavelet transform is an information-preserving transformation. The amount of information before and after the transform keeps unchanged, however, as the length of\nPublished by Francis Academic Press, UK\n-5-\nthe wavelet base changes, the distribution of the amount of information for different frequencies in the low-frequency image and the high-frequency image during its forward wavelet transform also changes accordingly. In the fusion process, if the low-frequency part of the TM image after forward wavelet transform (ie, the spectral information part) and the high-frequency part of the SPOT-PAN image after forward wavelet transform (ie, the spatial information part) act as weighted images, then conducting inverse wavelet transform, the resulting fusion image is the information extracted from the TM image and the SPOT-PAN image in different proportions. The length of the wavelet base can affect this proportional composition, thus leading to the above results. This is the most critical step in choosing the details of the original visible light image. If the selection is unreasonable, a lot of valuable information may be lost. Therefore, when targeting different applications, the wavelet basis with different lengths can be selected for fusion processing to extract the required information according to specific conditions [5].\nBecause this multi-scale multi-operator fusion method based on wavelet pyramid is highly targeted and effectively enhances the edges of the image, the fusion effect is obvious. The method is also applied to other visible light and thermal infrared images in the same field of view, and good results are also achieved."
        },
        {
            "heading": "3.3.2. The Influence of Weighting Coefficient on Remote Sensing Image Fusion",
            "text": "The weighting coefficient is to weight the high-frequency part (\u03b1) of the low-resolution wavelet transform image and the high-frequency part (1-\u03b1) of the high-resolution wavelet transform image. Together with the low-frequency part of the low-resolution wavelet transform image, a transformed image is formed, and then inverse wavelet transform is performed. The weighting coefficient has a great influence on the fusion effect.\nThe larger the \u03b1, the more of the low-resolution image information is retained, and the sharpness of the image decreases. However, since the low-resolution image carries a lot of multi-spectral information, If this information is retained, the false-color composite image of the fused image has high fidelity. The smaller the \u03b1, the more high-resolution image information is embedded into the transformed image, the details and texture of the fused image are clear, and the resolution is improved.\nIn the test, \u03b1 takes the values of 0, 0.1, 0.2, 0.5, and so on. As a result, when the value of \u03b1 is 0, although the false-color composite image of the fused image is prominent, the color diffuses outward at the edge. This is due to the fact that the low-resolution multispectral components are retained too little, resulting in color distortion. When \u03b1 is 0.1, there is a certain improvement in the color of the fusion image, but when it becomes 0.2, the improvement is not large. Therefore, the fusion image should include not only high-resolution details and texture information, but also low-resolution multi-spectral information, and effectively combine them to obtain the best results."
        },
        {
            "heading": "3.3.3. The Influence of Wavelet Transform Order on Remote Sensing Image Fusion",
            "text": "The larger the wavelet transform order, the smaller the proportion of low-frequency components of the transformed image, and the high-frequency components of the high-resolution image introduced in the weighting can be more, the details of the fusion image are more prominent. But similar to the reason above, the color of the false image of the fusion image becomes worse. For images of different sizes, different orders can be selected. When the image is large, the order selected can be larger. Generally, the order is between 3-5.\nIn summary, by appropriately selecting the filter length of the wavelet transform, the transform order,\nand the weighting coefficient, different optimal results can be obtained for different applications."
        },
        {
            "heading": "3.3.4. Comparison of IHS, PCA and Wavelet Transform Fusion Methods",
            "text": "Compared with the traditional data fusion methods such as IHS, PCA, the wavelet fusion model can not only reasonably select the wavelet base and the wavelet transform order for different features of the input image, but also introduce the details of both sides according to the actual needs during the fusion operation. Thereby, it shows stronger pertinence and practicability, and better integration effect.\nCompared with IHS and PCA methods, wavelet fusion has high flexibility. IHS can only fuse 3 bands (RGB) images, and PCA can only fuse 3 or more bands. The wavelet fusion method can fuse a single band or any number of bands. For the fusion of a single black and white image, wavelet fusion is undoubtedly the only choice. The wavelet fusion method can select wavelet base of different lengths and different wavelet transform orders to change the proportion of information occupied by high-frequency\nPublished by Francis Academic Press, UK\n-6-\ncomponents and low-frequency components in the transformed image, and affect the spectral and spatial information in the fused image to meet the requirements of different applications. If necessary, you can also change the weight value to obtain the desired false-color synthesis effect. The IHS and PCA methods do not have such flexibility [5].\nThe research results of Kelu Li [5] showed that the entropy value of the wavelet fusion image is much larger than the original image. Generally speaking, its entropy value will also be greater than that of IHS and PCA methods. From the perspective of spectral distortion, the value of the wavelet fusion method is also smaller. The IHS method is similar and slightly better than the image obtained by the PCA method fusion."
        },
        {
            "heading": "4. Test results and discussion",
            "text": ""
        },
        {
            "heading": "4.1. Test Results",
            "text": "Wavelet fusion is performed on the 2008*2009 IKONOS 4m and 1m images in Beijing, the filter length is 9, the wavelet transform order is 4, and \u03b1 is 0.1. Although the details of the IHS method fusion image are clear, the color tone is obviously unbalanced, and the roofs of Tiananmen Gatetower and other roofs become white and bright, and the distortion is large. Compared with the IHS method, the wavelet fusion method gets a slightly better texture detail information than the IHS method, while the false-color composite image has a worse color effect than the IHS method. However, after synthesizing 4m multispectral image into a color image, the wavelet fusion image is closer to the original multispectral image in terms of color, indicating that the wavelet fusion method can retain the spectral information of the original multispectral image as much as possible, with good fidelity. Although the color hierarchy of the IHS method is outstanding, it is only equivalent to stretching the color, while its visual effect is better, which is suitable for visual interpretation.\nPublished by Francis Academic Press, UK\n-7-\nFigure 2~5 are IKONOS 4m multi-spectral image color synthesis map, 1m high-resolution image, IHS fusion image, and wavelet fusion image. All images are without any processing, and the synthesized bands are 3, 2, and 1. It is obvious that wavelet fusion is closer to the original multispectral image. Observing the Tiananmen Gatetower, IHS image apparently gets too much high-resolution information on the roof, so that its color changes to white. The details of wavelet fusion are visually better than those of IHS. The following table 1 is the entropy value of each band image.\nFigure 4: IHS Fusion Image Figure 5: Wavelet Fusion Image"
        },
        {
            "heading": "4.2. Disadvantages and Improvements of Wavelet Fusion Method",
            "text": "The wavelet fusion method also has disadvantages, mainly related to speed issues. Because wavelet fusion needs to perform forward wavelet transform and inverse wavelet transform for each band, the algorithm is complicated, resulting in slower speed. If the image size becomes larger, the efficiency is lower. Considering human visual perception, the two methods can be combined to perform IHS-Wavelet fusion. First, IHS transform is performed on the three-band multispectral image to obtain I, H, and S. Then, the intensity component and the high-resolution image respectively conduct forward wavelet transform, and the transformed image is subjected to the same weighting process. Then the inverse wavelet transform is performed on the weighted image I'' to obtain I''', and then the inverse IHS transform is performed on I''', H, and S to obtain the fused image.\nThis method can preserve the spectral information of the multi-spectral image to the maximum\nextent, maintain the contrast of the original multispectral image, improve its clarity and spatial resolution, and increase the visual effect of color. And because forward and inverse wavelet transform are only needed to be done for 2 bands images, the speed is faster.\nThe IHS-wavelet fusion method has obvious superiority and adaptive capabilities, as shown in Figure 6. In fact, the PCA-wavelet method can also be implemented in the same way, but it seems that there are not many applications.\nJianwei Liu et al. utilized a similar method of IHS-Wavelet image fusion to get the same conclusion [6]. The image fusion method achieves a better fusion effect in terms of resolution and multi-spectrum, and eliminates the block blurring phenomenon caused by traditional wavelet fusion. As a result, the fusion image not only improves the spatial resolution of the fusion image, increases the image resolution, highlights the target feature information, but also better retains the spectral properties of the original multispectral image.\nPublished by Francis Academic Press, UK\n-8-"
        },
        {
            "heading": "4.3. The Prospect of Image Fusion",
            "text": "Wavelet fusion stands out from many image fusion methods due to its clear texture details, high spectral fidelity, and strong flexibility. The wavelet fusion method can adjust the proportion of highresolution and low-resolution components by changing the input parameters or by changing the input band to obtain a synthetic false-color image that reflects the different ground features. On the other hand, in some applications, wavelet fusion also has limitations. It must be combined with other methods, such as the combination with the IHS method. Different weighted calculations can be performed on different parts of the transformed image as needed, and the ratio of high and low-resolution pixels in the transformed image can be determined by different criteria.\nThe future image fusion will be multi-source image fusion with multiple spatial resolutions, multiple phases, and multiple spectra. And the integration will also be a combination of multiple methods. Generally, different features cannot use the same fusion parameters. If the method and parameters can be selected adaptively according to the ground features, the accuracy and processing efficiency of the target information of the fused image will be greatly improved. When the resolution of high-resolution and low-resolution image fusion is very different, the fusion effect may be relatively poor, which is also one of the problems that need to be solved in the future.\nThe commonly used dyadic wavelet transform can well handle the situation where the resolution differs by 2j times. If the resolution differs by 3 or 5 times, multi-band wavelet algorithms such as three or five-band wavelet should be used. Multi-band wavelet is one of the hotspots in wavelet research today [7]."
        },
        {
            "heading": "Acknowledgement",
            "text": "This research is supported by China Earthquake Science Experiment project, China Earthquake\nAdministration (2019CSES0113)."
        }
    ],
    "title": "Remote Sensing Image Fusion Based on Wavelet Transform",
    "year": 2023
}