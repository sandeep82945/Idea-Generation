{
    "abstractText": "We utilise the power of Large Language Models (LLMs), in particular GPT4, to be prompt engineered into performing an arbitrary task. Here, we give the model some human priors via text, along with some typical procedures for solving the ARC tasks, and ask it to generate the i) broad description of the input-output relation, ii) detailed steps of the input-output mapping, iii) use the detailed steps to perform manipulation on the test input and derive the test output. The current GPT3.5/GPT4 prompt solves 2 out of 4 tested small ARC challenges (those with small grids of 8x8 and below). With tweaks to the prompt to make it more specific for the use case, it can solve more. We posit that when scaled to a multi-agent system with usage of past memory and equipped with an image interpretation tool via Visual Question Answering, we may actually be able to solve the majority of the ARC challenge.",
    "authors": [
        {
            "affiliations": [],
            "name": "John Chong"
        },
        {
            "affiliations": [],
            "name": "Min Tan"
        }
    ],
    "id": "SP:63e388e5440cbdff1d9dae81395897180016405a",
    "references": [
        {
            "authors": [
                "Sam Acquaviva",
                "Yewen Pu",
                "Maxwell Nye",
                "Catherine Wong",
                "Michael Henry Tessler",
                "Josh Tenenbaum"
            ],
            "title": "Larc: Language annotated abstraction and reasoning corpus",
            "venue": "In Proceedings of the Annual Meeting of the Cognitive Science Society,",
            "year": 2021
        },
        {
            "authors": [
                "Jay Alammar"
            ],
            "title": "The illustrated gpt-2 (visualizing transformer language models), 2023a. URL https: //jalammar.github.io/illustrated-gpt2",
            "year": 2023
        },
        {
            "authors": [
                "Jay Alammar"
            ],
            "title": "The illustrated transformer, 2023b. URL https://jalammar.github.io/ illustrated-transformer",
            "year": 2023
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Varun Chandrasekaran",
                "Ronen Eldan",
                "Johannes Gehrke",
                "Eric Horvitz",
                "Ece Kamar",
                "Peter Lee",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Scott Lundberg"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "venue": "arXiv preprint arXiv:2303.12712,",
            "year": 2023
        },
        {
            "authors": [
                "Li Deng"
            ],
            "title": "The mnist database of handwritten digit images for machine learning research",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2012
        },
        {
            "authors": [
                "Tan Chong Min John",
                "Mehul Motani"
            ],
            "title": "Brick tic-tac-toe: Exploring the generalizability of alphazero to novel test environments",
            "venue": "arXiv preprint arXiv:2207.05991,",
            "year": 2022
        },
        {
            "authors": [
                "Tan Chong Min John",
                "Mehul Motani"
            ],
            "title": "Learning, fast and slow: A goal-directed memory-based approach for dynamic environments",
            "venue": "arXiv preprint arXiv:2301.13758,",
            "year": 2023
        },
        {
            "authors": [
                "Patrick Lewis",
                "Ethan Perez",
                "Aleksandra Piktus",
                "Fabio Petroni",
                "Vladimir Karpukhin",
                "Naman Goyal",
                "Heinrich K\u00fcttler",
                "Mike Lewis",
                "Wen-tau Yih",
                "Tim Rockt\u00e4schel"
            ],
            "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yujia Li",
                "David Choi",
                "Junyoung Chung",
                "Nate Kushman",
                "Julian Schrittwieser",
                "R\u00e9mi Leblond",
                "Tom Eccles",
                "James Keeling",
                "Felix Gimeno",
                "Agustin Dal Lago"
            ],
            "title": "Competition-level code generation with alphacode",
            "year": 2022
        },
        {
            "authors": [
                "Arvind Neelakantan",
                "Tao Xu",
                "Raul Puri",
                "Alec Radford",
                "Jesse Michael Han",
                "Jerry Tworek",
                "Qiming Yuan",
                "Nikolas Tezak",
                "Jong Wook Kim",
                "Chris Hallacy"
            ],
            "title": "Text and code embeddings by contrastive pre-training",
            "venue": "arXiv preprint arXiv:2201.10005,",
            "year": 2022
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Joon Sung Park",
                "Joseph C O\u2019Brien",
                "Carrie J Cai",
                "Meredith Ringel Morris",
                "Percy Liang",
                "Michael S Bernstein"
            ],
            "title": "Generative agents: Interactive simulacra of human behavior",
            "venue": "arXiv preprint arXiv:2304.03442,",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Toran Bruce Richards"
            ],
            "title": "Auto-gpt, 2023. URL https://github.com/Significant-Gravitas/ Auto-GPT",
            "year": 2023
        },
        {
            "authors": [
                "Teven Le Scao",
                "Alexander M Rush"
            ],
            "title": "How many data points is a prompt worth",
            "venue": "arXiv preprint arXiv:2103.08493,",
            "year": 2021
        },
        {
            "authors": [
                "Yongliang Shen",
                "Kaitao Song",
                "Xu Tan",
                "Dongsheng Li",
                "Weiming Lu",
                "Yueting Zhuang"
            ],
            "title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
            "venue": "arXiv preprint arXiv:2303.17580,",
            "year": 2023
        },
        {
            "authors": [
                "Yusuxke Shibata",
                "Takuya Kida",
                "Shuichi Fukamachi",
                "Masayuki Takeda",
                "Ayumi Shinohara",
                "Takeshi Shinohara",
                "Setsuo Arikawa"
            ],
            "title": "Byte pair encoding: A text compression scheme that accelerates pattern matching",
            "year": 1999
        },
        {
            "authors": [
                "David Silver",
                "Thomas Hubert",
                "Julian Schrittwieser",
                "Ioannis Antonoglou",
                "Matthew Lai",
                "Arthur Guez",
                "Marc Lanctot",
                "Laurent Sifre",
                "Dharshan Kumaran",
                "Thore Graepel"
            ],
            "title": "A general reinforcement learning algorithm that masters chess, shogi, and go through self-play",
            "year": 2018
        },
        {
            "authors": [
                "Giorgio Vallortigara"
            ],
            "title": "Born knowing: Imprinting and the origins of knowledge",
            "venue": "MIT press,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R Bowman"
            ],
            "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "arXiv preprint arXiv:1804.07461,",
            "year": 2018
        },
        {
            "authors": [
                "Alex Wang",
                "Yada Pruksachatkun",
                "Nikita Nangia",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman"
            ],
            "title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "year": 2022
        },
        {
            "authors": [
                "Qi Wu",
                "Damien Teney",
                "Peng Wang",
                "Chunhua Shen",
                "Anthony Dick",
                "Anton Van Den Hengel"
            ],
            "title": "Visual question answering: A survey of methods and datasets",
            "venue": "Computer Vision and Image Understanding,",
            "year": 2017
        },
        {
            "authors": [
                "Shunyu Yao",
                "Jeffrey Zhao",
                "Dian Yu",
                "Nan Du",
                "Izhak Shafran",
                "Karthik Narasimhan",
                "Yuan Cao"
            ],
            "title": "React: Synergizing reasoning and acting in language models",
            "venue": "arXiv preprint arXiv:2210.03629,",
            "year": 2022
        },
        {
            "authors": [
                "Yongchao Zhou",
                "Andrei Ioan Muresanu",
                "Ziwen Han",
                "Keiran Paster",
                "Silviu Pitis",
                "Harris Chan",
                "Jimmy Ba"
            ],
            "title": "Large language models are human-level prompt engineers",
            "year": 1910
        }
    ],
    "sections": [
        {
            "heading": "1 Background",
            "text": "The ARC Challenge is a very interesting challenge, as it is doing something counter to mainstream deep learning \u2013 learning from very few samples. Deep learning typically uses tens of thousands of samples to do well, for instance learning to classify digits (MNIST) (Deng, 2012) requires around 50,000 training samples. Humans, in comparison, can learn how to identify different animals by just one or two different observations. For instance, my 3 year-old kid can learn how to identify a giraffe in real life for the first time, even though the only other time he was exposed to a giraffe was through a cartoon flash card. Such capabilities are not well endowed in modern AI systems, and that means that such AI systems will need to be trained extensively before deploying in the real world. After deploying them in the real world, they will also be limited in their ability to adapt and learn as the environment changes.\nIn contrast, traditional rule-based systems (e.g. GOFAI) can \u201clearn\u201d quite fast, as any new situation can be interpreted without any learning phase, provided that the situation is already in the system rules given to it. Such a rule-based system could be symbolic systems or expert systems which already have the domain knowledge fed to it by human experts. However, the history of GOFAI has shown that it is difficult to engineer these rules out, and at many times, even humans face difficulty to come up with the rules as they may not be able to express it in words.\nAs you can see, there are shortcomings with the above two approaches, and a new kind of approach will need to be used in order to learn fast and generalise to new situations, in order to even have a chance at solving the ARC Challenge."
        },
        {
            "heading": "2 Next token prediction for self-supervised learning",
            "text": "There is a lot of structure in the world. These structures can be hard to represent via verbal rules, yet children can learn how physics work and how to interact with the world just by observation and action. Personally, I believe that simply observing is not enough \u2013 one has to perform actions in order\nar X\niv :2\n30 6.\n03 55\n3v 1\n[ cs\n.A I]\n6 J\nun 2\n02 3\nto learn how one\u2019s actions can affect the world. However, for tasks like learning language, the next action to take is simply to predict the next token and can be done without interaction with the world. Large Language Models (LLMs) such as GPT2 (Radford et al., 2019), GPT 3.5 (Ouyang et al., 2022) and GPT4 (Bubeck et al., 2023) have utilised an extensive amount of self-supervised learning via next-token prediction in order to learn the structure of text (See Fig. 1). This is a huge breakthrough, as the predominant approach to deep learning - supervised learning - requires extensive human labelling and is expensive and impractical to obtain for large amounts of data. This self-supervised learning approach can generate labels simply by predicting the next token and is easily obtainable from the world\u2019s worth of text on the World Wide Web. For instance, the sentence \"The cat sat on the mat\" can easily be used in at least 5 different prediction tasks (assuming tokens are defined at the word level), as shown below:\n1. The \u2192 cat 2. The cat \u2192 sat 3. The cat sat \u2192 on 4. The cat sat on \u2192 the 5. The cat sat on the \u2192 mat\nHigh sample efficiency. This means that the observations from the world can be reused in multiple input-output pairs and there is very high sample efficiency due to such a self-supervised learning method able to reuse the same sections of text multiple times.\nIterative processing of semantic meaning. Moreover, the Transformer architecture actually allows the embeddings of each token to be infleunced by the most similar and closest neighbours via selfattention (via a combination of token embeddings plus position embeddings), which allows for the input representation to be refined in an iterative fashion, solving the case of ambiguous inputs or polysemy (multiple meanings of the same word). Such a hierarchical structure is illustrated in Fig. 2.\nFeedback connections. I always believe that current deep learning methods suffer from lack of feedback connections to ground the lower levels of processing - it is widely known that in the brain neurons do not just exist in feedforward connections but also have a lot of feedback connections as well. However, recently, observing that increasing the size of Transformers was already sufficient to achieve better and better performance, such as in GPT3.5 and GPT4, I start to wonder if there is indeed a way for Transformers to ground the earlier layers\u2019 processing in the later layers\u2019 processing. I hypothesise that it is actually able to do some form of feedback grounding, because of the skip\nconnections present between decoder blocks, as illustrated in Fig. 1. The embeddings at the lower levels can actually be passed all the way to the later layers (largely unchanged except for LayerNormalisation, which affects all embeddings similarly), and can be processed in the same layer with potential grounding by the embeddings of the later layers. This is extremely powerful, and can actually ground the input processing with knowledge gained at the later part. For instance, in the text \"The following did not happen: John went to the market and bought a bunch of eggs, vegetables and meat.\", we are able to interpret the entire text in the opposite semantic meaning just because of the words \"The following did not happen\" at the beginning of the sentence. In fact, as will be discussed in the next section, this presence of skip connections may be the way prompting and grounding in earlier context is so effective in LLMs."
        },
        {
            "heading": "3 Prompting and Zero-shot/Few-shot learning",
            "text": "Given that LLMs are seemingly able to perform inference at multiple scales of abstraction (see earlier section), this opens an avenue of approaches whereby we can just tell the LLM what we want to do in natural language, and use it to ground the generation. Such an instruction-based method of conditioning generations has proven useful in multiple natural language tasks, as shown in the usage of LLMs flexibly by just an instruction to prompt the task in GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) benchmarks."
        },
        {
            "heading": "3.1 Zero-shot learning",
            "text": "LLMs are also able to do zero-shot learning very well. For instance, it is able to do zero-shot classification of new contexts simply by using semantic meaning of the tokens it has encountered during training: \"You are a classification model meant to classify the context of an input.\nContext A: In the garden Context B: In the hospital Context C: In the mountains Context D: In the sky\nGive the contexts for the following inputs: 1. Wow, the clouds are so fluffy today 2. The IV drip is running out, get a nurse 3. The sheep on the pasture are so pretty 4. Have you watered the flowers today?\nReturn in the following form: Number: Context Letter\"\nChatGPT (GPT3.5, May 3 2023 version) returns the following output, which are in general correct: 1. D: In the sky 2. B: In the hospital 3. C: In the mnountains 4. A: In the garden"
        },
        {
            "heading": "3.2 Few-shot learning",
            "text": "LLMs are also able to do few-shot learning pretty reliably. For instance, it is able to do few-shot classification of odd and even numbers from just a few sample input and output pairs. In order for to generate consistently, it needs to be given the framework of what the task is about and the possible outputs to ground the generation. Here is the example prompt given:\nYou are a classification machine meant to classify between output A and B.\nInput: 5 Output: A\nInput: 7 Output: A\nInput: 8 Output: B\nInput: 10 Output: B\nInput: 13 Output:\nChatGPT (GPT3.5, May 3 2023 version) returns the following output, which is correct: A\nHence, a trained LLM has shown that it can be equipped with the knowledge of a new task either through zero-shot description-based prompting, or few-shot example-based prompting, and can be the basis of a fast learning system that is adaptive to real-world inputs. Given the quick learning ability of LLMs via prompting, it is no wonder why prompt engineering quickly became very popular following the rise of larger LLMs."
        },
        {
            "heading": "4 Getting the LLM to reverse engineer the instruction",
            "text": "LLMs are actually capable of observing multiple input-output pairs and coming up with an instruction to derive the relation between them (Zhou et al., 2022). Furthermore, the Language annotated Abstraction and Reasoning Corpus (LARC) showed that 88% of the original ARC tasks can be represented in a text instruction where another human can solve it without needing the input-output examples (Acquaviva et al., 2021). Another paper has also highlighted the efficiency of prompt-based instructions, as one prompt can be worth 100s of training examples on various classification tasks (Scao & Rush, 2021).\nThe difficulty of the ARC challenge is that the machine (or human) needs to infer instructions based on limited examples. These instructions are usually difficult to deduce, as one needs to find the pattern with very few sample input-output pairs. However, once the instruction is deduced, it is very easily communicable to other humans using text. Hence, we reframe the ARC Challenge with the following steps:\n1. Deduce the input-output mapping rule using the LLM from the input-output examples\n2. Apply this rule to the test input to get the test output"
        },
        {
            "heading": "5 Chain of Thought",
            "text": "It is often difficult to do planning on complicated tasks which involve multiple steps. The ARC Challenge sometimes also involves multiple manipulations of the input image in order to derive the output. For this kind of problems, we can utilize approaches such as Chain of Thought (CoT) prompting (Wei et al., 2022), which uses demonstrations of details like the steps for mathematical\ncomputation to train the language model. Moreover, we do not even need to provide the humanlabelled detailed demonstration as shown in the CoT paper, but can get the LLM to generate its own thoughts. The \"ReAct: Synergizing reasoning and acting in language models\" paper shows how one way of prompting the LLM for it to generate detailed thoughts and act upon it (Yao et al., 2022) - using the Thought, Action, Observation framework.\nHierarchical Planning. CoT is still a largely linear way to do planning, as it involves having the previous action or plan before generating the next one. More recently, LLMs have been utilised in a hierarchical fashion, whereby the first step involves coming up with the broad plan, and the second step is to come up with the details. This is utilised in HuggingGPT (Shen et al., 2023) and AutoGPT (Richards, 2023) to generate an overall plan before breaking down into the detailed steps. This way of hierarchical planning was also used in the Generative Agents paper (Park et al., 2023) to generate a detailed action plan for an agent\u2019s day.\nThis approach of hierarchical planning is actually quite similar to how humans think. We do not have a detailed plan of our day right at the beginning, but think in a broad way like doing work in the morning, lunch, meet friends in afternoon, home in the evening and so on. Then, when prompted why do you want to do this, we go up a layer of abstraction to think about the goals of our lives. When prompted how do you want to do this, we go down a layer of abstraction to think about the specifics of the various plans of our lives. Hence, explicitly prompting the LLM to come up with the broad plan, and then using the broad plan to ground the generation for the detailed plan is a promising approach. It also helps circumvents the problem of the LLM having limited planning abilities, as we can plan the broad steps first, which are usually much shorter than the entire sequence of detailed steps."
        },
        {
            "heading": "6 Grounding in Human Biases",
            "text": "The ARC Challenge is difficult for computers because there is a huge number of possibilities to interpret high-dimensional real-world data, but easy for humans because humans can curate the possibilities based on some innate biases, like that of the Gestalt principles (Todorovic, 2008). In fact, without such innate biases, it can be difficult for anyone to learn quickly in the real world. Vallortigara (2021) wrote a book, \"Born Knowing\", which highlights that chicks come born with plenty of innate biases like preference for animate objects, which could help them learn faster. Similarly, human newborns come with a preference for face-like objects to help with recognition of the mother. Some human behaviours like suckling are also innate, rather than learnt, to facilitate survival.\nAlas, we may not be born tabular rasa like what is done in AlphaZero (Silver et al., 2018). In experiments with AlphaZero, it takes weeks with a single GPU just to learn how to play well enough to win a human (John & Motani, 2022) in a 4-in-a-row Tic-Tac-Toe game in a 7x7 grid with an unplayable position. Simply changing the unplayable position was enough to cause AlphaZero to become weaker than humans, and extensive training of various random unplayable positions was required for it to learn. Hence, for generalisability, pursuing optimality in Reinforcement Learning from a clean slate like that in static games like Chess or Go may not be the way to go. Rather, we need to ground the possibilities of what we need to do or interpret perception with some innate bias or some past experience in order to learn fast and be generalisable.\nSince LLMs like GPT4 are not able to be trained to a new set of input-output due to constrains of API, we utilise prompting to instill the human biases required for the machine to reduce the possibilities of interpreting the input-output pairs of the ARC Challenge."
        },
        {
            "heading": "7 Na\u00efve Method (Single Prompt)",
            "text": "Given that LLMs have proven effective at learning an arbitrary task just by prompting, we try to do a na\u00efve method of getting it to solve ARC tasks just from a single prompt alone. This prompt should be as generalisable as possible and should not be fine-tuned to any single one task.\nUsing the above ideas of grounding in human biases, CoT prompting and getting LLMs to come up with broad descriptions, then detailed steps, and then using the detailed steps to map from test input to test output, we come up with an example prompt for ARC as given below:\n\u201cYou are given a series of inputs and output pairs. These are all in the form of a 2D array, representing a 2D grid, with values from 0-9. The values are not representative of any ordinal ranking. Input/output pairs may not reflect all possibilities, you are to infer the simplest possible relation making use of symmetry and invariance as much as possible.\nThe input can be something like: > entire grid being the sandbox to manipulate > using a part of the grid (individual squares or portions of the grid) to depict instructions of how to do the task. symmetry is important. > using regions of similar value to depict area for answer of the task\nThe output can be something like: > same output size as input after performing action > output one of the fixed predetermined patterns used to classify the input image > using output to show the ordering of objects, such as by size, height, width, position, value\nEach of the input-output relation can be done with one or more actions chained together, which could be something like (not exhaustive): \u2212 object view (defined as continuous squares connected horizontally, vertically and/or diagonally, separated by 0 values) > objects can be of the same value, or different values combined together > objects may be hidden beneath other objects > rotating or shifting objects > changing value of object > objects can be manipulated and mapped to a different number of output squares > different objects may be manipulated differently based on context\n\u2212 overall view > rotation / reflection symmetry > continuation of a pattern > changing values\n\u2212 segment view > combine two segments of the input into one single one based on a simple rule > rule can be certain values are prioritized over others, or combination of values into new ones\nDo the following: \u2212 What is the broad description of the input/output relation that holds for all input/output pairs? \u2212 What is the step by step description of the input/output relation that holds for all input/output pairs? \u2212 Apply this description to the test input and find out the answer \u2019to_be_filled\u2019.\u201d\n[Insert .json for task here with all the input-output pairs in json format, with the test output replaced by \u2019to_be_filled\u2019]\nThe method to derive the json format is simply replacing the output section of the original json format from the ARC Challenge 2 dataset with \u2019to_be_filled\u2019. The code to do so can be found here: https://github.com/tanchongmin/ARC-Challenge/blob/main/arc_challenge.ipynb"
        },
        {
            "heading": "7.1 Example: Public Evaluation Task 157 (66e6c45b.json)",
            "text": ""
        },
        {
            "heading": "7.2 Example: Public Evaluation Task 162 (68b67ca3.json)",
            "text": ""
        },
        {
            "heading": "7.3 Evaluation",
            "text": "This na\u00efve approach has some success with the smaller ARC tasks. So far, with limited testing, this na\u00efve method on GPT3.5 or GPT4 has solved the following tasks out of 4 tested tasks on the Evaluation set: 157 (66e6c45b.json), 162 (68b67ca3.json). These two tasks have failed, although only slightly and are likely to be solved with more specific prompt engineering: 158 (66f2d22f.json), 170 (6ea4a07e.json). See the testing of GPT4 on the ARC Challenge via this url: https://www. youtube.com/watch?v=vt2yG1da8Fg. With some more fine-tuning of the actions that can be performed, I believe we can get it to work for more tasks. The key takeaway is that prompting can help to ground the model to think of feasible solutions it would otherwise not have.\nThat said, the json input for the 2D array is not a great one to extract object-level relations, and the prompt needs to continuously ask for GPT4 to think of the input as an object. The prompt is intended to be very generic and gives the broad input-output relation, along with some tips as to how prior ARC puzzles can be solved. As GPT4 is not that great at doing detailed planning, we follow the hierarchical approach done by HuggingGPT (Shen et al., 2023) or AutoGPT (Richards, 2023), and ask the model to list out the broad description first. Thereafter, after being grounded by the broad description, the model then generates the detailed step by step description. This description is then used to get the answer by applying these steps to the test input.\nInitially, I tried to get GPT4 to output a Python program to handle the manipulation from input to output. While this could work for simple problems, in general, I find that the program output generated may be different from the intention in the step by step description, and in general, the step by step description in words was more accurate. As such, the example prompt above did not ask for a program output from GPT4."
        },
        {
            "heading": "8 Improvements to the Na\u00efve Method",
            "text": "Following my experiments with the na\u00efve method, I have identified the following issues:\n1. Limited understanding of what an object is from the json file\n2. Limited context length to store json representations of large grids / multiple input-output samples\n3. Limited context length to store instructions\n4. Limited fact-checking abilities to determine if the input-output relation derived is correct\nThese are the potential solutions to the above issues:\n1. In order to do the ARC challenge well, it would be good to imbue in the model a sense of what an object is, and also how images look like in the real world. This is because there are some ARC challenges which use concepts like object permanence, gravity, which would be present in real world situations but not for a computer which is only trained on pixels in the ARC challenge. As such, we could take a leaf from the Visual Question Answering (QA) domain (Wu et al., 2017), and give the LLM the ability to ask questions about the input and output images and iteratively refine its input-output relation based on it. This Visual QA could be done with the base model as images in the wild, but should be fine-tuned with past ARC challenge data, as the distribution of pixel information between the real world and ARC challenge dataset may be different, though the concepts may be the same. My hypothesis is that pixel-based representation may be too high-dimensional to model the world, hence, being able to compress it down to low-dimensional text via Visual QA would be a huge plus for interpretability.\n2. Instead of putting all the input-output examples in the same json, we can separately ask the model to give a description for each of the input-output pairs. Then, we can prompt another model to find similarities between each of the descriptions of these input-output pairs and collate to a general input-output representation.\n3. Instead of having only one GPT model to give the prompt for the instructions, we could split the prompt into multiple parts. For instance, the object view can be one model, the overall view can be another model, the segment view can be another model, and so on.\nThis would mean that we can ground the instructions in more fine-grained action space that would increase the likelihood of solving the ARC challenges. We can then select the best performing instruction, by asking the various models to come up with different sets of input-output instructions, and collating them into one pool of potential instructions. Then, we can evaluate all of them and use the best one.\n4. We could have a separate GPT model to evaluate the input-output mapping. This model takes in the pool of potential instructions generated by the above steps, and evaluates them one by one. The moment any of the instructions fails to generate the input-output map of the training cases, it is discarded. This approach of generating more potential mappings and discarding them based on grounding by the training set is used in AlphaCode, where they generate multiple programs by simply changing the hyperparameters or by using more random generations of the LLM, and then eliminate those non-performant ones that do not give the right output in the training cases (Li et al., 2022) (See Fig. 6 for an illustration). Currently, I envision this model to take in just the instruction and the input json, and output the json after the instruction and check that it matches with the actual output json. An alternative is to ask GPT4 to come up with the Python code to do the input-output mapping, and then run the code to check for correct output - I suspect this may be inferior due to problems mapping to the right Python program from the instruction."
        },
        {
            "heading": "9 GPT as a System",
            "text": "With the recent trends of utilising multiple LLMs together as a system, such as in AutoGPT (Richards, 2023), it could potentially allow the model to scale better by off-loading various tasks to different LLM models, and letting all these models work together in a large ecosystem. Such a model is outlined in the Improvements section above, and more can be tuned in order to make the system as performant as possible."
        },
        {
            "heading": "10 Memory as the way ahead",
            "text": "Given that we are not able to train the weights of GPT to fit to the training set of the ARC challenge, using memory is the best way to go about imbuing the model with learnt knowledge. Humans learn very fast because we have memory to ground our current experiences and we can choose the best action based on what we have seen in the past. For example, if I see a snake on Path A, I will avoid Path A next time and choose Path B instead. This instantaneous way of learning is something that is not natural in deep learning, as it typically takes hundreds or thousands or more iterations in order to update the weights sufficiently so that it can learn well for Deep Learning, such as in Deep Reinforcement Learning. A more detailed explanation can be found in \"Learning, Fast and Slow\" (John & Motani, 2023).\nCurrently the na\u00efve method does not use memory of what has been stored earlier. If we were to use memory, I posit the best way to use it will be via text descriptions of the broad and detailed input-output relations stored from the earlier training examples. This would make the memory more generic rather than storing memory of the images. We then have two memories of instructions, one I call BroadInstruct, and the other DetailedInstruct, which details the broad description and detailed steps of earlier instructions from earlier ARC tasks. I can envision a system using it to be as such:\n1. Use the na\u00efve method to determine the broad description of the task\n2. From the broad description, retrieve from a database (e.g. Pinecone) using OpenAI Vector Embeddings (Neelakantan et al., 2022) or similar embeddings to retrieve the top k neighbours from BroadInstruct. k is a hyperparameter that can be tuned, and can be set to 5 by default.\n3. Conditioned on the top k neighbours as context, perform retrieval-augmented generation (Lewis et al., 2020) to generate the refined broad description of the task\n4. Repeat the earlier steps until convergence\nNow, having generated the broad description of the task, we move on to generate the detailed steps.\n1. Use the na\u00efve method with the broad descrption as context to determine the detailed steps of the task\n2. From the generated detailed steps, retrieve from a database (e.g. Pinecone) using OpenAI Vector Embeddings (Neelakantan et al., 2022) or similar embeddings to retrieve the top k neighbours from DetailedInstruct. k is a hyperparameter that can be tuned, and can be set to 5 by default.\n3. Conditioned on the top k neighbours as context, perform retrieval-augmented generation (Lewis et al., 2020) to generate the refined detailed steps of the task\n4. Repeat the earlier steps until convergence\nHence, we can utilise past knowledge of earlier ARC tasks for more accurate conditioning of the broad description and detailed steps needed for future ARC tasks. If the task is solved, we can then add in this broad and detailed description into BroadInstruct and DetailedInstruct respectively.\nApart from imbuing learning ability, retrieval-augmented generation has an added benefit of increasing the consistency of the LLM-generated output, as it is more in line with what is required, which may be helpful with getting the right solution in fewer generations. For more complicated problems (more complex than ARC challenge), in order to constrain memory storage given a limited storage space, we can also selectively store memory based on how surprising it is and how \"emotional\" the experience is. These can be explored in future challenges where there is too much perceptual information and memory storage is a constraint, but for ARC, I believe we can just keep all the memory as the number of ARC tasks are not large."
        },
        {
            "heading": "11 Conclusion",
            "text": "Overall, the ARC challenge is a very unique one, and can serve to pave the way for systems that are fast learning and can generalise well to arbitrary tasks. With the right innate biases via prompting, the right hierarchical structure to condition generation of detailed steps from broad description, a multi-agent architecture to split long prompts up into performant smaller sub-systems, a better way to interpret images using Visual QA, as well as better learning and grounding in past memory, I posit that GPT4 can eventually be made to solve the majority of the ARC tasks."
        }
    ],
    "title": "An Approach to Solving the ARC Challenge",
    "year": 2023
}