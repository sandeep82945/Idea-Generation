{
    "abstractText": "Federated learning (FL) is a promising approach in distributed learning keeping privacy. However, during the training pipeline of FL, slow or incapable clients (i.e., stragglers) slow down the total training time and degrade performance. System heterogeneity, including heterogeneous computing and network bandwidth, has been addressed to mitigate the impact of stragglers. Previous studies tackle the system heterogeneity by splitting a model into submodels, but with less degreeof-freedom in terms of model architecture. We propose nested federated learning (NeFL), a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling. NeFL is implemented by interpreting forward propagation of models as solving ordinary differential equations (ODEs) with adaptive step sizes. To address the inconsistency that arises when training multiple submodels of different architecture, we decouple a few parameters from parameters being trained for each submodel. NeFL enables resource-constrained clients to effectively join the FL pipeline and the model to be trained with a larger amount of data. Through a series of experiments, we demonstrate that NeFL leads to significant performance gains, especially for the worst-case submodel. Furthermore, we demonstrate NeFL aligns with recent studies in FL, regarding pre-trained models of FL and the statistical heterogeneity.",
    "authors": [
        {
            "affiliations": [],
            "name": "Honggu Kang"
        },
        {
            "affiliations": [],
            "name": "Jinwoo Shin"
        },
        {
            "affiliations": [],
            "name": "Joonhyuk Kang"
        }
    ],
    "id": "SP:f1694815888a9b6bcbd2c58422fc3531c5ddbf3a",
    "references": [
        {
            "authors": [
                "Durmus Alp Emre Acar",
                "Yue Zhao",
                "Ramon Matas",
                "Matthew Mattina",
                "Paul Whatmough",
                "Venkatesh Saligrama"
            ],
            "title": "Federated learning based on dynamic regularization",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Andrei Afonin",
                "Sai Praneeth Karimireddy"
            ],
            "title": "Towards model agnostic federated learning using knowledge distillation",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Manoj Ghuhan Arivazhagan",
                "Vinay Aggarwal",
                "Aaditya Kumar Singh",
                "Sunav Choudhary"
            ],
            "title": "Federated learning with personalization layers",
            "venue": "arXiv preprint arXiv:1912.00818,",
            "year": 2019
        },
        {
            "authors": [
                "Thomas Bachlechner",
                "Bodhisattwa Prasad Majumder",
                "Henry Mao",
                "Gary Cottrell",
                "Julian McAuley"
            ],
            "title": "Rezero is all you need: fast convergence at large depth",
            "venue": "In Conference on Uncertainty in Artificial Intelligence (UAI),",
            "year": 2021
        },
        {
            "authors": [
                "Cristian Bucilu\u01ce",
                "Rich Caruana",
                "Alexandru Niculescu-Mizil"
            ],
            "title": "Model compression",
            "venue": "In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),",
            "year": 2006
        },
        {
            "authors": [
                "Bo Chang",
                "Lili Meng",
                "Eldad Haber",
                "Frederick Tung",
                "David Begert"
            ],
            "title": "Multi-level residual networks from dynamical systems view",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "Hong-You Chen",
                "Cheng-Hao Tu",
                "Ziwei Li",
                "Han Wei Shen",
                "Wei-Lun Chao"
            ],
            "title": "On the importance and applicability of pre-training for federated learning",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Ricky T.Q. Chen",
                "Yulia Rubanova",
                "Jesse Bettencourt",
                "David Duvenaud"
            ],
            "title": "Neural ordinary differential equations",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2018
        },
        {
            "authors": [
                "Xiangning Chen",
                "Cho-Jui Hsieh",
                "Boqing Gong"
            ],
            "title": "When vision transformers outperform resnets without pre-training or strong data augmentations",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Ekin Dogus Cubuk",
                "Barret Zoph",
                "Jon Shlens",
                "Quoc Le"
            ],
            "title": "Randaugment: Practical automated data augmentation with a reduced search space",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Luke N. Darlow",
                "Elliot J. Crowley",
                "Antreas Antoniou",
                "Amos J. Storkey"
            ],
            "title": "Cinic-10 is not imagenet or cifar-10",
            "venue": "arXiv preprint arXiv:1810.03505,",
            "year": 2018
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "ImageNet: A large-scale hierarchical image database",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2009
        },
        {
            "authors": [
                "Enmao Diao",
                "Jie Ding",
                "Vahid Tarokh"
            ],
            "title": "HeteroFL: Computation and communication efficient federated learning for heterogeneous clients",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Carl Eckart",
                "Gale Young"
            ],
            "title": "The approximation of one matrix by another of lower rank",
            "year": 1936
        },
        {
            "authors": [
                "Farzin Haddadpour",
                "Mohammad Mahdi Kamani",
                "Aryan Mokhtari",
                "Mehrdad Mahdavi"
            ],
            "title": "Federated learning with compression: Unified analysis and sharp guarantees",
            "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2021
        },
        {
            "authors": [
                "E. Hairer",
                "S.P. N\u00f8rsett",
                "G. Wanner"
            ],
            "title": "Solving Ordinary Differential Equations I Nonstiff problems",
            "year": 2000
        },
        {
            "authors": [
                "Song Han",
                "Jeff Pool",
                "John Tran",
                "William J. Dally"
            ],
            "title": "Learning both weights and connections for efficient neural networks",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2015
        },
        {
            "authors": [
                "Chaoyang He",
                "Murali Annavaram",
                "Salman Avestimehr"
            ],
            "title": "Group knowledge transfer: Federated learning of large cnns at the edge",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Chaoyang He",
                "Murali Annavaram",
                "Salman Avestimehr"
            ],
            "title": "Group knowledge transfer: Federated learning of large cnns at the edge",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Chaoyang He",
                "Zhengyu Yang",
                "Erum Mushtaq",
                "Sunwoo Lee",
                "Mahdi Soltanolkotabi",
                "Salman Avestimehr"
            ],
            "title": "SSFL: Tackling label deficiency in federated learning via personalized selfsupervision",
            "venue": "arXiv preprint arXiv:2110.02470,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2016
        },
        {
            "authors": [
                "Yihui He",
                "Xiangyu Zhang",
                "Jian Sun"
            ],
            "title": "Channel pruning for accelerating very deep neural networks",
            "venue": "In IEEE International Conference on Computer Vision (ICCV),",
            "year": 2017
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531,",
            "year": 2015
        },
        {
            "authors": [
                "Junyuan Hong",
                "Haotao Wang",
                "Zhangyang Wang",
                "Jiayu Zhou"
            ],
            "title": "Efficient split-mix federated learning for on-demand and in-situ customization",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Samuel Horv\u00e1th",
                "Stefanos Laskaridis",
                "Mario Almeida",
                "Ilias Leontiadis",
                "Stylianos Venieris",
                "Nicholas Lane"
            ],
            "title": "FjORD: Fair and accurate federated learning under heterogeneous targets with ordered dropout",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Gao Huang",
                "Yu Sun",
                "Zhuang Liu",
                "Daniel Sedra",
                "Kilian Weinberger"
            ],
            "title": "Deep networks with stochastic depth",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2016
        },
        {
            "authors": [
                "Yuang Jiang",
                "Shiqiang Wang",
                "V\u0131\u0301ctor Valls",
                "Bong Jun Ko",
                "Wei-Han Lee",
                "Kin K. Leung",
                "Leandros Tassiulas"
            ],
            "title": "Model pruning enables efficient federated learning on edge devices",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems (TNNLS),",
            "year": 2022
        },
        {
            "authors": [
                "Minjae Kim",
                "Sangyoon Yu",
                "Suhyun Kim",
                "Soo-Mook Moon"
            ],
            "title": "DepthFL : Depthwise federated learning for heterogeneous clients",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Alexander Kolesnikov",
                "Lucas Beyer",
                "Xiaohua Zhai",
                "Joan Puigcerver",
                "Jessica Yung",
                "Sylvain Gelly",
                "Neil Houlsby"
            ],
            "title": "Big transfer (BiT): General visual representation learning",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Vinod Nair",
                "Geoffrey Hinton"
            ],
            "title": "CIFAR-10 (Canadian Institute for Advanced Research). http://www.cs.toronto.edu/ \u0303kriz/cifar.html",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "Hao Li",
                "Asim Kadav",
                "Igor Durdanovic",
                "Hanan Samet",
                "Hans Peter Graf"
            ],
            "title": "Pruning filters for efficient convnets",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2017
        },
        {
            "authors": [
                "Qinbin Li",
                "Yiqun Diao",
                "Quan Chen",
                "Bingsheng He"
            ],
            "title": "Federated learning on non-iid data silos: An experimental study",
            "venue": "arXiv preprint arXiv:2102.02079,",
            "year": 2021
        },
        {
            "authors": [
                "Qinbin Li",
                "Bingsheng He",
                "Dawn Song"
            ],
            "title": "Model-contrastive federated learning",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Tian Li",
                "Anit Kumar Sahu",
                "Ameet Talwalkar",
                "Virginia Smith"
            ],
            "title": "Federated learning: Challenges, methods, and future directions",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2020
        },
        {
            "authors": [
                "Tian Li",
                "Anit Kumar Sahu",
                "Manzil Zaheer",
                "Maziar Sanjabi",
                "Ameet Talwalkar",
                "Virginia Smith"
            ],
            "title": "Federated optimization in heterogeneous networks",
            "venue": "In Machine Learning and Systems (MLSys),",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Li",
                "Kaixuan Huang",
                "Wenhao Yang",
                "Shusen Wang",
                "Zhihua Zhang"
            ],
            "title": "On the convergence of fedavg on non-iid data",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "Paul Pu Liang",
                "Terrance Liu",
                "Liu Ziyin",
                "Ruslan Salakhutdinov",
                "Louis-Philippe Morency"
            ],
            "title": "Think locally, act globally: Federated learning with local and global representations",
            "venue": "arXiv preprint arXiv:2001.01523,",
            "year": 2020
        },
        {
            "authors": [
                "Tao Lin",
                "Lingjing Kong",
                "Sebastian U Stich",
                "Martin Jaggi"
            ],
            "title": "Ensemble distillation for robust model fusion in federated learning",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Zhuang Liu",
                "Jianguo Li",
                "Zhiqiang Shen",
                "Gao Huang",
                "Shoumeng Yan",
                "Changshui Zhang"
            ],
            "title": "Learning efficient convolutional networks through network slimming",
            "venue": "In IEEE International Conference on Computer Vision (ICCV),",
            "year": 2017
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "SGDR: Stochastic gradient descent with warm restarts",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2017
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2019
        },
        {
            "authors": [
                "Disha Makhija",
                "Nhat Ho",
                "Joydeep Ghosh"
            ],
            "title": "Federated self-supervised learning for heterogeneous clients",
            "venue": "arXiv preprint arXiv:2205.12493,",
            "year": 2022
        },
        {
            "authors": [
                "Brendan McMahan",
                "Eider Moore",
                "Daniel Ramage",
                "Seth Hampson",
                "Blaise Aguera y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2017
        },
        {
            "authors": [
                "Vaikkunth Mugunthan",
                "Eric Lin",
                "Vignesh Gokul",
                "Christian Lau",
                "Lalana Kagal",
                "Steve Pieper"
            ],
            "title": "FedLTN: Federated learning for sparse and personalized lottery ticket networks",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2022
        },
        {
            "authors": [
                "Yuval Netzer",
                "Tao Wang",
                "Adam Coates",
                "Alessandro Bissacco",
                "Bo Wu",
                "Andrew Y. Ng"
            ],
            "title": "Reading digits in natural images with unsupervised feature learning",
            "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,",
            "year": 2011
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga",
                "Alban Desmaison",
                "Andreas Kopf",
                "Edward Yang",
                "Zachary DeVito",
                "Martin Raison",
                "Alykhan Tejani",
                "Sasank Chilamkurthy",
                "Benoit Steiner",
                "Lu Fang",
                "Junjie Bai",
                "Soumith Chintala"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Liangqiong Qu",
                "Yuyin Zhou",
                "Paul Pu Liang",
                "Yingda Xia",
                "Feifei Wang",
                "Ehsan Adeli",
                "Li Fei-Fei",
                "Daniel Rubin"
            ],
            "title": "Rethinking architecture design for tackling data heterogeneity in federated learning",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Rothchild",
                "Ashwinee Panda",
                "Enayat Ullah",
                "Nikita Ivkin",
                "Ion Stoica",
                "Vladimir Braverman",
                "Joseph Gonzalez",
                "Raman Arora"
            ],
            "title": "FetchSGD: Communication-efficient federated learning with sketching",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Ruder"
            ],
            "title": "An overview of gradient descent optimization algorithms",
            "venue": "arXiv preprint arXiv:1609.04747,",
            "year": 2016
        },
        {
            "authors": [
                "Shibani Santurkar",
                "Dimitris Tsipras",
                "Andrew Ilyas",
                "Aleksander Madry"
            ],
            "title": "How does batch normalization help optimization",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2018
        },
        {
            "authors": [
                "Hyowoon Seo",
                "Jihong Park",
                "Seungeun Oh",
                "Mehdi Bennis",
                "Seong-Lyun Kim"
            ],
            "title": "Federated knowledge distillation",
            "venue": "arXiv preprint arXiv:2011.02367,",
            "year": 2020
        },
        {
            "authors": [
                "Christian Szegedy",
                "Vincent Vanhoucke",
                "Sergey Ioffe",
                "Jon Shlens",
                "Zbigniew Wojna"
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Mingxing Tan",
                "Quoc Le"
            ],
            "title": "EfficientNet: Rethinking model scaling for convolutional neural networks",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2019
        },
        {
            "authors": [
                "Chandra Thapa",
                "Seyit Camtepe",
                "Lichao Sun"
            ],
            "title": "Splitfed: When federated learning meets split learning",
            "venue": "AAAI Conference on Artificial Intelligence (AAAI),",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Alexandre Sablayrolles",
                "Gabriel Synnaeve",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Going deeper with image transformers",
            "venue": "In IEEE International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Praneeth Vepakomma",
                "Otkrist Gupta",
                "Tristan Swedish",
                "Ramesh Raskar"
            ],
            "title": "Split learning for health: Distributed deep learning without sharing raw patient data",
            "venue": "arXiv preprint arXiv:1812.00564,",
            "year": 2018
        },
        {
            "authors": [
                "Hongyi Wang",
                "Mikhail Yurochkin",
                "Yuekai Sun",
                "Dimitris Papailiopoulos",
                "Yasaman Khazaeni"
            ],
            "title": "Federated learning with matched averaging",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "Chuhan Wu",
                "Fangzhao Wu",
                "Lingjuan Lyu",
                "Yongfeng Huang",
                "Xing Xie"
            ],
            "title": "Communicationefficient federated learning via knowledge distillation",
            "venue": "Nature Communications,",
            "year": 2022
        },
        {
            "authors": [
                "Sangdoo Yun",
                "Dongyoon Han",
                "Seong Joon Oh",
                "Sanghyuk Chun",
                "Junsuk Choe",
                "Youngjoon Yoo"
            ],
            "title": "Cutmix: Regularization strategy to train strong classifiers with localizable features",
            "venue": "In IEEE International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "Mikhail Yurochkin",
                "Mayank Agarwal",
                "Soumya Ghosh",
                "Kristjan Greenewald",
                "Nghia Hoang",
                "Yasaman Khazaeni"
            ],
            "title": "Bayesian nonparametric federated learning of neural networks",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2019
        },
        {
            "authors": [
                "Sergey Zagoruyko",
                "Nikos Komodakis"
            ],
            "title": "Wide residual networks",
            "venue": "In British Machine Vision Conference (BMVC),",
            "year": 2016
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Cisse",
                "Yann N. Dauphin",
                "David Lopez-Paz"
            ],
            "title": "mixup: Beyond empirical risk minimization",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "Yue Zhao",
                "Meng Li",
                "Liangzhen Lai",
                "Naveen Suda",
                "Damon Civin",
                "Vikas Chandra"
            ],
            "title": "Federated learning with non-iid data",
            "venue": "arXiv preprint arXiv:1806.00582,",
            "year": 2018
        },
        {
            "authors": [
                "Zhuangdi Zhu",
                "Junyuan Hong",
                "Jiayu Zhou"
            ],
            "title": "Data-free knowledge distillation for heterogeneous federated learning",
            "venue": "arXiv preprint arXiv:2105.10056,",
            "year": 2021
        },
        {
            "authors": [
                "Weiming Zhuang",
                "Yonggang Wen",
                "Shuai Zhang"
            ],
            "title": "Divergence-aware federated self-supervised learning",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Federated learning (FL) is a promising approach in distributed learning keeping privacy. However, during the training pipeline of FL, slow or incapable clients (i.e., stragglers) slow down the total training time and degrade performance. System heterogeneity, including heterogeneous computing and network bandwidth, has been addressed to mitigate the impact of stragglers. Previous studies tackle the system heterogeneity by splitting a model into submodels, but with less degreeof-freedom in terms of model architecture. We propose nested federated learning (NeFL), a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling. NeFL is implemented by interpreting forward propagation of models as solving ordinary differential equations (ODEs) with adaptive step sizes. To address the inconsistency that arises when training multiple submodels of different architecture, we decouple a few parameters from parameters being trained for each submodel. NeFL enables resource-constrained clients to effectively join the FL pipeline and the model to be trained with a larger amount of data. Through a series of experiments, we demonstrate that NeFL leads to significant performance gains, especially for the worst-case submodel. Furthermore, we demonstrate NeFL aligns with recent studies in FL, regarding pre-trained models of FL and the statistical heterogeneity."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "The success of deep learning owes much to vast amounts of training data where a large amount of data comes from mobile devices and internet-of-things (IoT) devices. However, privacy regulations on data collection has become a critical concern, potentially impeding further advancement of deep learning (Dat, 2022; Dou et al., 2021). A distributed machine learning framework, federated learning (FL) is getting attention to address these privacy concerns. FL enables model training by collaboratively leveraging the vast amount of data on clients while preserving data privacy. Rather than centralizing raw data, FL collects trained model weights from clients, that are subsequently aggregated on a server by a method (e.g., FedAvg) (McMahan et al., 2017). FL has shown its potential, and several studies have explored to utilize it more practically (Hong et al., 2022; He et al., 2020b; Makhija et al., 2022; Zhuang et al., 2022; He et al., 2021).\nDespite its promising perspective, there exist challenges in regarding systems-related heterogeneity (Kairouz et al., 2021; Li et al., 2020a). For example, clients with heterogeneous resources, including computing power, communication bandwidth, and memory, introduce stragglers that lead to longer delays and training times during the FL pipeline. The FL server can wait for stragglers or drop out stragglers. However, excluding stragglers can lead to a trained model biased predominantly toward data from resource-rich clients. Therefore, FL framework that accommodates clients with heterogeneous resources is required. On the other hand, one another option is to reduce a model size to accommodate resource-poor clients. However, a smaller-sized model could result in a performance\nar X\niv :2\n30 8.\n07 76\n1v 2\n[ cs\n.L G\n] 9\nO ct\n2 02\ndegradation due to limited model capacity. To this end, FL with a single global model may not be efficient for heterogeneous clients.\nIn this paper, we propose Nested Federated Learning (NeFL), a method that embraces existing studies of federated learning in a nested manner (Horva\u0301th et al., 2021; Diao et al., 2021; Kim et al., 2023). While generic FL trains a model with a fixed size and structure, NeFL trains several submodels of adaptive sizes to meet dynamic requirements (e.g., memory, computing and bandwidth dynamics) of each client. We propose to scale down a model into submodels generally (by widthwise or/and depthwise). The proposed scaling method provides more degree-of-freedom (DoF) to scale down a model than previous studies (Horva\u0301th et al., 2021; Diao et al., 2021; Kim et al., 2023). The increased DoF makes submodels be more efficient in size (Tan & Le, 2019) and provides more flexibility on model size and computing cost. The scaling is motivated by interpreting a model forwarding as solving ordinary differential equations (ODEs). The ODE interpretation also motivated us to suggest the submodels with learnable step size parameters and the concept of inconsistency. We also propose a parameter averaging method for NeFL: Nested Federated Averaging (NeFedAvg) for averaging consistent parameters and FedAvg for averaging inconsistent parameters of submodels.\nAdditionally, we verify if NeFL aligns with the recently proposed ideas in FL: (i) pre-trained models improve the performance of FL in both identically independently distributed (IID) and non-IID settings (Chen et al., 2023) and (ii) simply rethinking the model architecture improves the performance, especially in non-IID settings (Qu et al., 2022). Through a series of experiments we observe that NeFL outperforms baselines sharing the advantages of recent studies.\nThe main contributions of this study can be summarized as follows:\n\u2022 We propose a general model scaling method employing the concept of ODE solver to deal with the system heterogeneity.\n\u2022 We propose a method for parameter averaging across generally scaled submodels.\n\u2022 We evaluate the performance of NeFL through a series of experiments and verify the applicability of NeFL over recent studies."
        },
        {
            "heading": "2 RELATED WORKS",
            "text": "Knowledge distillation. Knowledge distillation (KD) aims to compress models by transferring knowledge from a large teacher model to a smaller student model (Hinton et al., 2015). Several studies have explored the integration of knowledge distillation within the context of federated learning (Seo et al., 2020; Zhu et al., 2021). These studies investigate the use of KD to address (i) reducing the model size and transmitting model weights and (ii) fusing the knowledge of several models with different architectures. FedKD (Wu et al., 2022) proposes an adaptive mutual distillation where the level of distillation is controlled based on prediction performance. FedGKT (He et al., 2020a) presents an edge-computing method that employs knowledge distillation to train resourceconstrained devices. The method partitions the model, and clients transfer intermediate features to an offloading server for task offloading. FedDF (Lin et al., 2020) introduces a model fusion technique that employs ensemble distillation to combine models with different architecture. However, it is worth noting that knowledge distillation-based FL requires shared data or shared generative models across clients to get the knowledge distillation loss. Alternatively, clients can transfer trained models to the other clients (Afonin & Karimireddy, 2022).\nCompression and sparsification. Several studies have focused on compressing uploaded gradients by quantization or sparsification to deal with the communication bottleneck (Rothchild et al., 2020; Haddadpour et al., 2021). FetchSGD (Rothchild et al., 2020) proposes a method that compresses model updates using a Count Sketch. Some approaches aim to represent the weights of a larger network using a smaller network (Ha et al., 2017; Bucilua\u030c et al., 2006). Pruning is another technique that can be used to compress models. A model is pruned and retrained after the model is trained which needs additional communication and computational load for FL (Jiang et al., 2022; Mugunthan et al., 2022). A global model keeps its model size during training. Otherwise, the pruned model can be determined at the initialization (Lee et al., 2020) leading a unique pruned model with the limited capacity to be trained.\nDynamic runtime. The neural network can forward with larger numerical error and less computational load or forward with smaller numerical error and more computational load (Chen et al., 2018; Hairer et al., 2000). This approach offers a way to dynamically optimize computation. Split learning (Vepakomma et al., 2018; Thapa et al., 2022; He et al., 2020a) is another technique that addresses the issue of resource-constrained clients by leveraging richer computing resources, such as cloud or edge servers. These methods enable clients with system-related heterogeneity to participate in the FL pipeline.\nModel splitting. Various approaches have been proposed to address the heterogeneity of clients by splitting global network based on their capabilities. LG-FedAvg (Liang et al., 2020) introduced a method to split the model and decouple layers into global and local layers, reducing the number of parameters involved in communication. While FjORD (Horva\u0301th et al., 2021) and HeteroFL (Diao et al., 2021) split a global model widthwise, DepthFL (Kim et al., 2023) splits a global model depthwise. Unlike widthwise scaling, DepthFL incorporates an additional bottleneck layer and an independent classifier for every submodel. It has been studied that deep and narrow models as well as shallow and wide models are inefficient in terms of the number of parameters or floatingpoint operations (FLOPs). Prior studies have shown that carefully balancing the depth and width of a network can lead to improved performance (Zagoruyko & Komodakis, 2016; Tan & Le, 2019). Therefore, a balanced network is expected to contribute to performance gains in the context of FL. It is worth noting that our proposed NeFL provides a scaling method both widthwise and depthwise for submodels to be well-balanced."
        },
        {
            "heading": "3 BACKGROUND",
            "text": "We propose to scale down the models inspired by solving ODEs in a numerical way (e.g., Euler method). Modern deep neural networks stack residual blocks that contain skip-connections that bypass the residual layers. A residual block is written as Yj+1 = Yj + Fj(Yj , \u03d5j), where Yj is the feature map at the jth layer, \u03d5j denotes the jth block\u2019s network parameters, and Fj represents a residual module of the jth block. These networks can be interpreted as solving ODEs by numerical analysis (Chang et al., 2018; He et al., 2016).\nConsider an initial value problem to find y at t given dydt = f(t, y) and y(t0) = y0. We can obtain y at any point by integration: y = y0 + \u222b t t0 f(t, y)dt. It can be approximated by Taylor\u2019s expansion as y = y0 + f(t0, y0) (t\u2212 t0) and approximated after more steps as follows:\nyn+1 = yn + hf(tn, yn) = y0 + hf(t0, y0) + \u00b7 \u00b7 \u00b7+ hf(tn\u22121, yn\u22121) + hf(tn, yn), (1)\nwhere h denotes the step size. An ODE solver can compute with less steps by using larger step size as: yn+1 = y0 + 2hf(t0, y0) + \u00b7 \u00b7 \u00b7 + 2hf(tn\u22121, yn\u22121) when n is odd. The results would be numerically less accurate than fully computing with a smaller step size. Note that the equation looks like the equation of residual connections. An output of a residual block is rewritten as follows:\nYj+1 = Yj + Fj(Yj , \u03d5j) = Y0 + F0(Y0, \u03d50) + \u00b7 \u00b7 \u00b7+ Fj\u22121(Yj\u22121, \u03d5j\u22121) + Fj(Yj , \u03d5j). (2)\nMotivated by this interpreting the neural networks as solving ODEs, we proposed that few residual blocks can be omitted during forward propagation. For example, Y3 = Y0+F0+F1+F2 could be approximated by Y3 = Y0+F0+2F1 omitting the F2 block. Here, we propose learnable step size parameters (Touvron et al., 2021; Bachlechner et al., 2021). Instead of pre-determining the step size parameters (h\u2019s in the equation (1)), we let step size parameters be trained along with the network parameters. For example, when we omit F2 block, output is formulated as Y3 = Y0+ s0F0+ s1F1 where si\u2019s are also optimized. This enables to scale down the network depthwise. Furthermore, it is also possible to scale each residual block by width (e.g., the size of filters in convolutional layers). The theoretical background behind width-wise scaling is provided by the Eckart-YoungMirsky theorem (Eckart & Young, 1936). A width-wise scaled residual block represents the optimal k-rank approximation of the original (full) block (Horva\u0301th et al., 2021).\nOur model scaling method inpired by ODE solver is displayed in Figure 1. The black line represents a function to approximate, while the red line represents output approximated by a full nerual netowk. The blue color line represents the depthwise-scaled submodel. The model omitted to compute block F2(\u00b7) at the point Y2 and instead, larger step size for computing F1(Y1, \u03d51) compensates the omitted block (Chang et al., 2018). The green colored line represents the widthwise-scaled submodel that has less parameters \u03d5 in each block. Following the theorem that a widthwise-scaled model with less parameters is an approximation of a model with more parameters (Eckart & Young, 1936), output from a widthwise-scaled model has larger numerical error."
        },
        {
            "heading": "4 NEFL: NESTED FEDERATED LEARNING",
            "text": "NeFL is FL framework that accommodates resource-poor clients. Instead of requiring every client to train a single global model, NeFL allows resource-poor clients to train submodels that are scaled in both widthwise and depthwise, based on dynamic nature of their environments. This flexibility enables more clients, even with constrained resources, to pariticipate in the FL pipeline, thereby making the model be trained with a more data. In NeFL pipeline, heterogeneous clients have an ability to determine their submodel at every iteration, allowing them to adapt to varying computing or communication bottlenecks in their respective environments. The NeFL server subsequently aggregates the parameters of these different submodels, resulting in a collaborative and comprehen-\nAlgorithm 1 NeFL: Nested Federated Learning Input: Submodels {Fk}Nsk=1, total communication round T , the number of local epochs E 1: for t\u2190 0 to T \u2212 1 do // Global iterations 2: NeFL server broadcasts the weights {\u03b8c,Ns , \u03b8ic,1, . . . \u03b8ic,Ns} to clients in Ct 3: wi \u2190 \u03b8j \u2208 {\u03b81, . . . , \u03b8Ns} \u2200i \u2208 Ct 4: for k \u2190 0 to E \u2212 1 do // Local iterations 5: Client i updates wi \u2200i \u2208 Ct 6: Client i transmits the weights wi to the NeFL server \u2200i \u2208 Ct 7: {\u03b8c,Ns , \u03b8ic,1, . . . \u03b8ic,Ns} \u2190 ParameterAverage({wi}i\u2208Ct ) \u25b7 Algorithm 2\nsive global model. Furthermore, during test time, clients have a flexibility to select an appropriate submodel based on their specific background process burdens, such as runtime memory constraints or CPU/GPU bandwidth dynamics. This allows clients to balance between performance and factors like memory usage or latency, tailoring to their individual needs and constraints. To this end, NeFL is twofold: scaling a model into submodels and aggregating the parameters of submodels.\nNeFL scales a global model FG into Ns submodels F1, . . . ,FNs with corresponding weights \u03b81, . . . , \u03b8Ns . Without loss of generality, we suppose FG = FNs . The scaling method is described in following Section 4.1. During each communication round t, each client in subset Ct (which is subset of M clients) selects one of the Ns submodels based on their respective environments and trains the model by the local epochs E. Then, clients transmit their trained weights {wi}i\u2208Ct to the NeFL server, which aggregates them into {\u03b8c,Ns , \u03b8ic,1, . . . \u03b8ic,Ns} where \u03b8c,k and \u03b8ic,k denote consistent and inconsistent parameters of a submodel k. Note that \u03b8c,k \u2282 \u03b8c,Ns\u2200k. The aggregated weights are then distributed to the clients in Ct+1, and this process continues for the total number of communication rounds."
        },
        {
            "heading": "4.1 MODEL SCALING",
            "text": "We propose a global network scaling method, which combines both widthwise scaling and depthwise scaling. We scale the model widthwise by ratio of \u03b3W and depthwise by ratio of \u03b3D. For example, a global model is represented as \u03b3 = \u03b3W \u03b3D = 1 and a submodel that has 25% parameters of a global model can be scaled by \u03b3W = 0.5 and \u03b3D = 0.5. Flexible widthwise/depthwise scaling provides more degree of freedom to split models. We further describe on the scaling strategies in the following sections."
        },
        {
            "heading": "4.1.1 DEPTHWISE SCALING",
            "text": "Residual networks, such as ResNets (He et al., 2016) and ViTs (Dosovitskiy et al., 2021), have gained popularity due to their significant performance improvements on deep networks. The output of a residual block can be seen as a function multiplied by a step size as Yj+1 = Yj + sjF (Yj , \u03d5j) = Y0 + \u2211 j sjF (Yj , \u03d5j). Note that ViTs\u2019 forward operation with skip connections can be represented in a similar way by choosing F (\u00b7) as either self-attention (SA) layers or feed-forward networks (FFN):\nYj+1 = Yj + sjSA(Yj , \u03d5j), Yj+2 = Yj+1 + sj+1FFN(Yj+1, \u03d5j+1).\nThe depthwise scaling can be implemented by skipping any of residual blocks of ResNets or encoder blocks of ViTs. For example, ResNets (e.g., ResNet18, ResNet34) have a downsampling layer followed by residual blocks, each consisting of two convolutional layers. ResNet18 has 8 residual blocks and ResNet34 has 16 residual blocks. ViT/B-16 has 12 encoder blocks where embedding patches are inserted as inputs to following transformer encoder blocks. Each block can have different number of the parameters and computational complexity. A submodel is scaled by omitting few blocks satisfying the number of parameters to be scaled by \u03b3D according to system requirements of a client. We can observe that skipping a few blocks in a network still allows it to operate effectively (Chang et al., 2018). This observation is in line with the concept of stochastic depth proposed in (Huang et al., 2016), where a subset of blocks is randomly bypassed during training and the all of the blocks are used during testing.\nThe step size parameters s\u2019s can be viewed as dynamically training how much forward pass should be made at each step. They allow each block to adaptively contribute to useful representations, and\nlarger step sizes are multiplied to blocks that provide valuable information. Referring to Figure 1 and numerical analysis methods (e.g., Euler method, Runge-Kutta methods (Hairer et al., 2000)), adaptive step sizes rather than uniformly spaced steps, can effectively reduce numerical errors. Note that DepthFL (Kim et al., 2023) is a special case of the proposed depthwise scaling.1"
        },
        {
            "heading": "4.1.2 WIDTHWISE SCALING",
            "text": "Previous studies have been conducted on reducing the model size in the width dimension (Li et al., 2017; Liu et al., 2017). In the context of NeFL, widthwise scaling is employed to slim down the network by utilizing structured contiguous pruning (ordered dropout in (Horva\u0301th et al., 2021)) along with learnable step sizes. We apply contiguous channel-based pruning to the convolutional networks (He et al., 2017) and node removal to the fully connected layers. The parameters of narrow-width submodel constitute a subset of the parameters of the wide-width model. Consequently, parameters of the slimmest model are trained on any submodel by every client, while the parameters of the larger model is trained less frequently. This approach ensures that the parameters of the slimmest model capture the most of useful representations. Note that each block stated in Section 4.1.1 can have different width, but we suppose throughout the paper that every block has same widthwise scaling ratio \u03b3W without loss of generality.\nWe can obtain further insights from a toy example. Given an optimal linear neural network y = Ax and data from uniform distribution x \u223c X , the optimal widthwise-scaled submodel y = AWx is the best k-rank approximation2 by Eckart\u2013Young\u2013Mirsky theorem (Horva\u0301th et al., 2021; Eckart & Young, 1936). Given a linear neural network of of rank k, widthwise-scaled model of scaling ratio \u03b3W has rank of \u2308\u03b3W k\u2309. Then,\nminEx\u223cX \u2225AWx\u2212Ax\u22252F = min \u2225AW \u2212A\u2225 2 F ,\nwhere F denotes Frobenius norm. In our framework, similar tedency is observed. Inspired by the magnitude-based pruning (Han et al., 2015; Li et al., 2017), we present the L1 norm of weights averaged by the number of weights at each layer of five trained submodels in Figure 3b. The submodel 1 which is the slimmest model, has similar tendency of L1 norm to the widest model and the gap between submodel gets smaller as the model size gets wider. The slimmest model might have learned the most useful representation while additional parameters for larger models obtain still useful, but less useful information."
        },
        {
            "heading": "4.2 PARAMETER AVERAGING",
            "text": "Inconsitency. NeFL enables training several submodels on local data for subsequent aggregation. However, submodels of different model architectures (i.e., different width and depth) have different characteristics (Acar et al., 2021; Chen et al., 2022; Li et al., 2020b; Santurkar et al., 2018). Referring to Figure 1, if depthwise-scaled model omits a block, it can be compensated by optimizing step\n1We suppose a non-contiguous depthwise scaling so that we implement DepthFL without auxiliary bottleneck layers and self-distillation. The details are described in the Appendix B.\n2Note that we have learnable step sizes that can further improve the performance of widthwise scaled submodel over best k-rank approximation (refer to Appendix C).\nAlgorithm 2 ParameterAverage Input: Trained weights from clients W = {wi}i\u2208Ct 1: for submodel index k in {1, . . . , Ns} do 2: Mk \u2190 {wi|wi = \u03b8k}i\u2208Ct 3: for block \u03d5j in \u03b8c,Ns do // NeFedAvg \u25b7 Consistent parameters 4: M\u2032 \u2190M = {Mk}Nsk=1 5: for k in {1, . . . , Ns} do 6: M\u2032 \u2190M\u2032 \\Mk if \u03d5j /\u2208 \u03b8c,k 7: k\u2032 \u2190 0, \u03d5j,0 = \u2205 8: for k in {k|Mk \u2208M\u2032} do 9: \u03d5j,k \\ \u03d5j,k\u2032 \u2190 \u2211 {i|wi\u2208 \u22c3 l\u2265k,Ml\u2208M\u2032 Ml} \u03d5ij,k \\ \u03d5ij,k\u2032/ \u2211 l\u2265k,Ml\u2208M\u2032\n|Ml| 10: k\u2032 \u2190 k 11: \u03b8c,Ns \u2190 \u22c3 j \u03d5j 12: for k in {1, . . . , Ns} do // FedAvg (McMahan et al., 2017) \u25b7 Inconsistent parameters 13: \u03b8ic,k \u2190 \u2211 {i|wi\u2208Mk} \u03b8iic,k/|Mk|\nsize of adjacent blocks. For widthwise-scaled model, numerical errors are induced from each block and they can be mitigated by optimizing step size for each block. We can infer that each submodel requires different step sizes to compensate for the numerical errors according to its respective model architecture. Furthermore, submodels with different model architectures have different loss landscapes. Consider the situation where losses are differentiable. A stationary point of a global model is not necessarily the stationary points of individual submodels (Acar et al., 2021; Li et al., 2020b). Different trainability of neural networks, which varies depending on network architecture, may lead the convergence of submodels to become non-stationary.\nThe motivation led us to introduce a decoupled set of parameters (Liang et al., 2020; Arivazhagan et al., 2019) for respective submodels that require different parameter averaging method. We refer to these different characteristics between submodels as inconsistency. To this end, we propose the concept of separating a few parameters, referred to as inconsistent parameters, from individual submodels. We address step size parameters and batch normalization layers as inconsistent parameters. Note that batch normalization can improve Lipschitzness of loss, which is sensitive to the convergence of FL (Santurkar et al., 2018; Li et al., 2020c). We further provided an ablation study to assess the effectiveness of inconsistent parameters.\nMeanwhile, consistent parameters are averaged across submodels. Since the weights of a submodel are subset of the weights of the largest submodel, the averaging of these weights should be different from conventional FL. The parameters of the submodels are denoted as \u03b81 = {\u03b8c,1, \u03b8ic,1}, . . . , \u03b8Ns = {\u03b8c,Ns , \u03b8ic,Ns} where \u03b8c denotes consistent parameters and \u03b8ic denotes inconsistent parameters. Note that the global parameters, which are broadcasted to clients for the next FL iteration, encompass \u03b8c,Ns , \u03b8ic,1, . . . , \u03b8ic,Ns .\nParameter averaging. We propose averaging method for the uploaded weights from clients in Algorithm 2. Locally trained weights W = {wi}i\u2208Ct from clients are provided as input. The NeFL server verifies which submodel each client\u2019s learned weights correspond to and stores the weights separately by submodel. M = {M1, . . . ,MNs}, where Mk denotes weights set from clients who trained k-th submodel, is set of uploaded weights sorted by submodels. For consistent parameters, the parameters are averaged by Nested Federated Averaging (NeFedAvg). In short, consistent parameters are averaged in a nested manner. The parameters are averaged by weights from clients who trained the parameters in this round t. For nested averaging of parameters, the server accesses parameters block by block since a submodel can have parameters of a block or not. For each block, the server checks which submodel has the block (line 6 in Algorithm 2). Then, parameters are averaged by width in a nested manner. The parameters of a block with the smallest width are included in the parameters of a block with larger width. Hence, the parameters are averaged by weights of clients whose submodels have the block. Meanwhile, the parameters of a block with the largest width is only contained in a submodel that has the largest width. Thus, the parameters are averaged by clients whose submodels have the block with the largest width. For averaging inconsistent parameters, FedAvg (McMahan et al., 2017) is employed. Each submodel has\nthe same size of inconsistent parameters that the inconsistent parameters are averaged for respective submodels. The example for Algorithm 2 is provided in Appendix B.3."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In this section, we demonstrate the performance of our proposed NeFL over baselines. We provide the experimental results of NeFL that trains submodels from scratch. We also demonstrate whether NeFL aligns with the recently proposed ideas in FL through experiments to verify (i) the performance of NeFL with initial weights loaded from pre-trained model (Chen et al., 2023) and (ii) the performance of NeFL with ViTs in non-IID settings (Qu et al., 2022). We conduct experiments using the CIFAR-10 dataset (Krizhevsky et al.) for image classification with ResNets and ViTs.\nExperimental setup. The experiments in Table 1 and Table 2 are evaluated with five submodels (Ns = 5 where \u03b31 = 0.2, \u03b32 = 0.4, \u03b33 = 0.6, \u03b34 = 0.8, \u03b35 = 1) and the experiments in Table 3 are evaluated with three submodels (Ns = 3 where \u03b31 = 0.5, \u03b32 = 0.75, \u03b33 = 1). Pre-trained models we use for evaluation are trained on the ImageNet-1k dataset(Deng et al., 2009; Pyt, 2023). The pretrained weights trained on ImageNet-1k dataset (Deng et al., 2009) are loaded on the initial global models and subsequently NeFL was performed. To take system heterogeneity into account, each client is assigned one of the submodels at each iteration, and statistical heterogeneity was implemented by label distribution skew following the Dirichlet distribution with concentration parameter 0.5 (Yurochkin et al., 2019; Li et al., 2021a). Training details are provided in Appendix B.1.\nComparison with state-of-the-art model splitting FL methods. For fair comparison across different baselines, we designed each submodel to have similar number of parameters (Table 8 in\nAppendix A). As illustrated in Table 1, NeFL outperforms baselines in terms of both the performance of the worst-case submodel (\u03b3 = 0.2) and the average performance across five submodels in IID and non-IID settings. Notably, the performance gain is greater in non-IID settings, which belong to practical FL scenarios. It is worth noting that our proposed depthwise scaling method has performance gain over depthwise scaling baselines, while our proposed widthwise scaling method also has performance gain over widthwise scaling baselines (refer to the Appendix A). Furthermore, beyond the performance gain from using depthwise or widthwise scaled submodels, NeFL provides a federated averaging method that can incorporate widthwise or/and depthwise scaled submodels. This characteristic of embracing any submodel with different architecture extracted from a single global model enhances flexibility, enabling more clients to participate in the FL pipeline.\nPerformance enhancement by employing pre-trained models. We investigate the performance improvement from incorporating pre-trained models into NeFL and verify that NeFL is still effective when employing pre-trained models. Recent studies on FL have figured out that FL gets benefits from pre-trained models even more than centralized learning (Kolesnikov et al., 2020; Chen et al., 2023). It motivates us to evaluate the performance of NeFL on pre-trained models. The pre-trained model that is trained in a common way using ImageNet-1k is loaded from PyTorch (Paszke et al., 2019). Even when a pre-trained model was trained as a full model without any submodel being trained, NeFL made better performance with these pre-trained models. The results in Table 2 show that the performance of NeFL has been enhanced through pre-training in both IID and non-IID settings following the results of the recent studies. Meanwhile, baselines such as HeteroFL and DepthFL, which do not have any inconsistent parameters, have no effective performance gain when trained with pre-trained models compared to to models trained from scratch.\nImpact of model architecture on statistical heterogeneity. We now present an experiment using ViTs and Wide ResNet (Zagoruyko & Komodakis, 2016) on NeFL. Previous studies have examined the effectiveness of ViTs in FL scenarios, and it has been observed that ViTs can effectively alleviate the adverse effects of statistical heterogeneity due to their inherent robustness to distribution shifts (Qu et al., 2022). Building upon this line of research, Table 3 demonstrates that ViTs outperform ResNets in our framework, with the larger number of parameters, in both IID and non-IID settings. Particularly in non-IID settings, ViTs exhibit less performance degradation of average performance when compared to IID settings. Note that when comparing the performance gap between IID and non-IID settings, the worst-case ViT submodel experiences more degradation than the worst-case ResNet submodel. Nevertheless, despite this degradation, ViT still maintains higher performance than ResNet. Consequently, we verify that ViT on NeFL is also effective following the results in Qu et al. (2022).\nAdditional experiments. We provide experimental results of NeFL on other datasets and with different number of clients, along with ablation study in Appendix A. The performance gain of NeFL increases when dealing with more challenging datasets. For example, the performance gain of NeFL with ResNet34 on CIFAR-100 is 7.63% over baselines. We also verify that NeFL shows the best performance across all different number of clients. In our ablation study, we present the effectiveness of inconsistent parameters including learnable step sizes and the performance comparison of proposed scaling methods."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this work, we have introduced Nested Federated Learning (NeFL), a generalized FL framework that addresses the challenges of system heterogeneity. By leveraging proposed depthwise and widthwise scaling, NeFL efficiently divides models into submodels employing the concept of ODE solver, leading to improved performance and enhanced compatibility with resource-constrained clients. We propose to decouple few parameters as inconsistent parameters for respective submodels that FedAvg are employed for averaging inconsistent parameters while NeFedAvg was utilized for averaging consistent parameters. Our experimental results highlight the significant performance gains achieved by NeFL, particularly for the worst-case submodel. Furthermore, we also explore NeFL in line with recent studies of FL such as pretraining and statistical heterogeneity."
        },
        {
            "heading": "Supplementary Material",
            "text": ""
        },
        {
            "heading": "A ADDITIONAL EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "A.1 OTHER DATASET",
            "text": "We evaluate the performance for other dataset such as CIFAR-100 (Krizhevsky et al.), CINIC-10 (Darlow et al., 2018) SVHN (Netzer et al., 2011) and we observe a similar tendency in terms of Top-1 accuracy of the worst-case submodel and average accuracy over submodels. Note that we set total communication round T = 100 for training SVHN. The results are presented in Table 4."
        },
        {
            "heading": "A.2 DIFFERENT NUMBER OF CLIENTS",
            "text": "We conduct further experiments across different numbers of clients. In Table 5, we observe that as the number of clients increases, the performance of NeFL as well as baselines degrades. The results align with previous studies (Kim et al., 2023; Thapa et al., 2022; Wang et al., 2020). The more the number of clients, trained weights deviates further from the weights trained by IID data. While the IID sampling of the training data ensures the stochastic gradient to be an unbiased estimate of the full gradient, the non-IID sampling leads to non-guaranteed convergence and model weight divergence in FL (Li et al., 2020b; 2021b; Zhao et al., 2018). The local clients train their own network with multiple epochs and upload the weights so that the uploaded weights get more deviated. In this regard, our proposed algorithm remains effective across different numbers of clients; however, the performance (e.g., accuracy and convergence) degrades by the data distribution among clients varies more as their number increases."
        },
        {
            "heading": "A.3 ABLATION STUDY",
            "text": "For analyzing and experiments for ablation study we refer NeFL-W that all submodels are scaled widthwise, NeFL-D that all submodels are scaled depthwise and NeFL-WD that submodels are scaled both widthwise and depthwise. We further refer to NeFL-DO that has different initial step sizes with NeFL-D. Referring to Table 12, NeFL-DO has larger magnitude step sizes aligning with the principles of ODE solver, compared to NeFL-D. NeFL-D scales submodels by skipping a subset of blocks of a global model, thus reducing the depth of the model. NeFL-D does not compensate for the skipped blocks by using larger step sizes. For example, a submodel in NeFL-D is given the initial step sizes as s0 = 1, s1 = 1, s2 = 0 and output after Block 2 without Block 2 is Y3 = Y0+F0+F1. Meanwhile, NeFL-DO reduces the size of the global model by skipping a subset of block functions F (\u00b7) and gives larger initial step sizes to compensate it. The step sizes are determined based on the number of blocks that are skipped. For a submodel without Block 2, initial output after Block 2 is initially computed as Y3 = Y0 + F0 + 2F1 given s0 = 1, s1 = 2, s2 = 0. We also refer\nthat submodel with no learnable step sizes by N/L (i.e., constant step sizes are kept with given intial values).\nThe performance comparison between NeFL-WD and NeFL-WD (N/L) as well as the comparison between NeFL-W and FjORD (Horva\u0301th et al., 2021) provides the effectiveness of learnable step sizes and comparison between NeFL-W and HeteroFL (Diao et al., 2021) provides the effectiveness of inconsistent parameters including learnable step sizes. Similarly, the comparison between NeFLD and NeFL-D (N/L) provides the effectiveness of learnables step sizes and comparison between NeFL-D and DepthFL (Kim et al., 2023) provides the effectiveness of inconsistent parameters including learnable step sizes. We summarized the NeFL with various scaled submodels in Table 7. We also provide the parameter sizes and average FLOPs of submodels by scaling in Table 8.\nReferring to the Table 6, the performance improvements of NeFL-WD over NeFL-WD (N/L), NeFLW over FjORD (Horva\u0301th et al., 2021) and NeFL-D over NeFL-D (N/L) provide the effectiveness of learnable step sizes. The effectiveness of the inconsistent parameters including learn step sizes is also verified by NeFL-D over DepthFL (Kim et al., 2023) and NeFL-W over HeteroFL (Diao et al., 2021). We also observe that NeFL-D and NeFL-WD have better performance over widthwise scaling. The performance gap of depthwise scaling over widthwise scaling gets larger for narrow and deep networks. Note that ResNet56 and ResNet110 has smaller (i.e., narrower) channel sizes with more layers (i.e., deeper) than ResNet18 and ResNet34 He et al. (2016). Furthermore, we have a finding that NeFL-D outperforms NeFL-DO in most cases. The rationale comes from the empirical results that trained step sizes are not as large as initial value for NeFL-DO that large initial values for NeFL-DO degrades the trainability of depthwise-scaled submodels.\nWe evaluated the experiments by similar number of parameters for several scaling methods and depthwise scaling requires slightly more FLOPs than widthwise scaling for ResNet18, ResNet34 and ResNet110 and less FLOPs for ResNet56. Usually depthwise scaled models have more FLOPs than widthwise scaled models while scaling by both widthwise and depthwise models are in between. It is because of the model architecture and limited DoF of submodels. ResNets consist of convolution layers that have FLOPs of the parameters multiplied by feature sizes. The feature sizes get smaller as forwarding the layers and depthwise scaled submodels that omitted the latter layers make a model to require more FLOPs than widthwise scaled submodels that is scaled across all the layers.\nIt is worth noting that beyond the performance improvement (including that our proposed scaling method NeFL-W and NeFL-D over baselines in Table 6), NeFL provides the more DoF for widthwise/depthwise scaling that can be determined by the requirements of clients. It results in more clients to be participate in the FL pipeline. Also refer to Table 9 that has different scaling ratio \u03b3. Note that in this case, FjORD (Horva\u0301th et al., 2021) outperforms NeFL-W. In this case with severe scaling factors (the worst model has 4% parameters of a global model), step sizes could not compensate the limited number of parameters and degraded the trainability with auxiliary parameters. However, NeFL-WD shows the best performance over other baselines that verify the well-balanced submodels show the better performance than ill-conditioned (too shallow or too narrow) submodels."
        },
        {
            "heading": "B EXPERIMENTAL DETAILS",
            "text": ""
        },
        {
            "heading": "B.1 TRAINING DETAILS",
            "text": "The experiments in Table 1, Table 2, Table 4, Table 5 and Table 6 are evaluated by a total 500 communication rounds (T ) with 100 clients (M ). At each round, a fraction rate of 0.1 is used, indicating that 10 clients (|Ct| = 10) transmit their weights to the server. During the training process of clients, local batch size of 32 and a local epoch of E = 5 are used. For training, we employ SGD optimizer (Ruder, 2016) without momentum and weight decay. The initial learning rate is set to 0.1 and decreases by a factor of 110 at the halfway point and 3 4 of the total communication rounds. The experiments in Table 3 are evaluated with the number of clients is M = 10, all of whom participate in the NeFL pipeline (with a fraction rate of 1). The experiment consists of T = 100 communication rounds, and each client performs local training for a single epoch (E = 1). We use a cosine annealing learning rate scheduling (Loshchilov & Hutter, 2017) with 500 steps of warmup and an initial learning rate 0.03. The input images are resized to a size of 256 and randomly cropped to a size of 224 with a padding size of 28. Note that utilizing layer normalization layers as consistent parameters, as opposed to BN layers that are inconsistent parameters, yields better performance."
        },
        {
            "heading": "B.2 MODEL ARCHITECTURES",
            "text": "The ResNet18 architecture and ResNet34 architecture consist of four layers, while ResNet56 and ResNet110 have three layers. These layers are composed of blocks with different channel sizes, specifically (64, 128, 256, 512) for ResNet18/32 and (16, 32, 64) for ResNet56/110. Wide ResNet101 2 comprises four layers of bottleneck blocks with channel sizes (128, 256, 512, 1024) (He et al., 2016; Zagoruyko & Komodakis, 2016). The ViT-B/16 architecture consists of twelve layers, with each layer containing blocks comprising self-attention (SA) and feed-forward networks (FFN) (Dosovitskiy et al., 2021). The widthwise splitting for ViT models are implemented by varying the embedding dimension (D in (Dosovitskiy et al., 2021)).\nFor the experiments presented in Table 1, Table 2, Table 4, Table 5 and Table 6, we consider five submodels with \u03b3 = [\u03b31, \u03b32, \u03b33, \u03b34, \u03b35] = [0.2, 0.4, 0.6, 0.8, 1] and \u03b3 = [0.04, 0.16, 0.36, 0.64, 1] for Table 9. Additionally, for Table 3, we use three submodels with \u03b3 = [\u03b31, \u03b32, \u03b33] = [0.5, 0.75, 1]. Submodel details for ResNets and ViTs are detailed in Table 10 (ResNet18), Table 11 (ResNet34),\nTable 12 (ResNet56), Table 14 (ResNet110), Table 15 (Wide ResNet101 2) and Table 13 (ViT-B/16 In the tables, 1\u2019s and 0\u2019s denote the initial values of step sizes. A step size of zero indicates that a submodel does not include the corresponding block. Note that ResNets have a step size parameters for each block while ViTs have different step size parameters to be multiplied with SA and FFN. Submodels in NeFL-W are characterized by \u03b3D = [1, . . . , 1] and \u03b3W with a target size, while submodels in NeFL-D are characterized by \u03b3W = [1, . . . , 1] and \u03b3D with a target size. Submodels in NeFL-WD are characterized by target size \u03b3W\u03b3D. Corresponding number of parameters and FLOPs are provided in Table 8."
        },
        {
            "heading": "B.3 EXAMPLE ON PARAMETER AVERAGING",
            "text": "Consider an example with five submodels and suppose that a convolutional layer from the first block is included in submodel 1, 3, and 5. Assume that submodel 1 and submodel 5 are trained twice (two clients), while submodel 3 is trained three times at a communication round. Then, we have |M2| = |M4| = 0, |M1| = |M5| = 2, and |M3| = 3. Now, delving into the parameter averaging process, the parameters exclusive to submodel 5 (\u03d51,5\\\u03d51,3) are averaged using two updated weights (M5). Likewise, the parameters possessed by submodel 3 but not by submodel 1 (\u03d51,3 \\ \u03d51,1) are averaged using five weights (M5 \u222aM3). Finally, the parameters of submodel 1 \u03d51,1, that is trained seven times, are averaged using seven weights (M5 \u222a M3 \u222a M1). This approach ensures that consistent parameters are appropriately averaged, taking into account their depthwise inclusion and the widthwise number of occurrences across different submodels."
        },
        {
            "heading": "B.4 DATASET",
            "text": "CIFAR10/100. The CIFAR10 dataset consists of 60000 images (train dataset consists of 50000 samples and test dataset consists of 10000 samples). 32 \u00d7 32 \u00d7 3 color images are categorized by\n10 classes, with 6000 images per class (Krizhevsky et al.). For FL with each client has 500 data samples for M = 100, and 5000 data samples for M = 10. We perform data augmentation and pre-processing of random cropping (by 32\u00d7 32 with padding of 4), random horizontal flip, and normalization by mean of (0.4914, 0.4822, 0.4465) and standard deviation of (0.2023, 0.1994, 0.2010).\nCINIC10. The CIFAR10 dataset consists of 270000 32\u00d732\u00d73 color images (train dataset consists of 90000 samples and validation and test dataset consists of 90000 samples respectively) in 10 classes (Darlow et al., 2018). It is constructed from ImageNet and CIFAR10. We perform data augmentation and pre-processing of random cropping (by 32 \u00d7 32 with padding of 4), random horizontal flip, and normalization by mean of (0.47889522, 0.47227842, 0.43047404) and standard deviation of (0.24205776, 0.23828046, 0.25874835).\nSVHN. The SVHN dataset consists of 73257 digits for training and 26032 digits for testing of 32 \u00d7 32 \u00d7 3 color images (Netzer et al., 2011). The deataset is obtained from house numbers in Google Stree view images in 10 classes (digit \u20180\u2019 to digit \u20189\u2019). For FL for M = 100 each client has 732 samples. We perform data augmentation and pre-processing of random cropping (by 32 \u00d7 32 with padding of 2), color jitter (by brightness of 63/255, saturation=[0.5, 1.5] and contrast=[0.2, 1.8] implmented by torchvision.transforms.ColorJitter), and normalization by mean of (0.4376821, 0.4437697, 0.47280442) and standard deviation of (0.19803012, 0.20101562, 0.19703614)."
        },
        {
            "heading": "B.5 BASELINES",
            "text": "HeteroFL & FjORD. HeteroFL (Diao et al., 2021) and FjORD (Horva\u0301th et al., 2021) are widthwise splitting methods designed to address the challenges posed by client heterogeneity in FL. While both methods aim to mitigate the impact of heterogeneity, these are several key differences between them. Firstly, HeteroFL does not utilize separate (i.e., inconsistent) parameters for BN layers in its submodels, whereas FjROD incorporates distinct BN layer for each submodel. This difference in handling BN layers can impact the learning dynamics and model performance under IID settings. Secondly, HeteroFL employs static batch normalization, where BN statistics are updated using the entire dataset after the training process. On the other hand, FjORD updates BN statistics during training. Lastly, HeteroFL utilizes a masked cross-entropy loss to address the statistical heterogeneity among clients. This loss function helps to mitigate the impact of clients with the statistical heterogeneity. In our implementation of HeteroFL, the masked cross-entropy loss is not utilized.\nDepthFL. The model is split depthwise, and an auxiliary bottleneck layer is included as an independent classifier. We implement DepthFL (Kim et al., 2023) without separate bottleneck layers for fair comparison without additional parameters. Then, DepthFL is a special case of NeFL-D without inconsistent parameters. Furthermore, due to the accuracy degradation, we omitted knowledge distillation noted in (Kim et al., 2023). Instead, our DepthFL models incorporate downsampling layers that adjust the feature size to match the input sizes of the classifier. It is important to note that the auxiliary bottleneck layers for submodels in DepthFL can be interpreted as parameter decoupling, as discussed in Section 4.2."
        },
        {
            "heading": "B.6 PRE-TRAINED MODELS",
            "text": "The pre-trained models on Table 2 and Table 3 are trained on ImageNet-1k (Deng et al., 2009) as following recipes (Pyt, 2023):\nResNet18/34. The models are trained by epochs of 90, batch size of 32, SGD optimizer (Ruder, 2016), learning rate of 0.1 with momentum of 0.9 and weight decay of 0.0001, where learning rate is decreased by a factor of 0.1 every 30 epochs.\nWide ResNet101 2. The model is trained by epochs of 90, batch size of 32, SGD optimizer (Ruder, 2016), learning rate of 0.1 with momentum of 0.9 and weight decay of 0.0001, where the learning scheduler is cosine learning rate (Loshchilov & Hutter, 2017) and warming up restarts for 256 epochs.\nViT-B/16. The model is trained by epochs of 300, batch size of 512, AdamW optimizer (Loshchilov & Hutter, 2019) with learning rate of 0.003 and weight decay of 0.3. The learning scheduler is cosine annealing (Loshchilov & Hutter, 2017) after linear warmup method with decay of 0.033 for 30 epochs. Additionally, the random augmentation (Cubuk et al., 2020), random mixup with \u03b1 = 0.2 (Zhang et al., 2018), cutmix of \u03b1 = 1 (Yun et al., 2019), repeated augmentation, label smoothing of 0.11 (Szegedy et al., 2016), clipping gradient norm to 1, model exponential moving average (EMA) are employed."
        },
        {
            "heading": "B.7 DYNAMIC ENVIRONMENT",
            "text": "We simulate a dynamic environment by randomly selecting which submodel to be trained by each client during every communication round. In our experiments for Table 1, Table 2, Table 4, Table 5 and Table 6, we have an equal number of five tiers of clients (M/Ns = 20 for all tiers of clients). The resource-constrained clients (tier 1) randomly select models between \u03b3 = 0.2, 0.4, 0.6, clients in tier 2 randomly select models from the set \u03b3 = 0.2, 0.4, 0.6, 0.8, clients in tier 3 randomly select models from the set \u03b3 = 0.2, 0.4, 0.6, 0.8, 1, clients in tier 4 randomly select models from the set \u03b3 = 0.4, 0.6, 0.8, 1, and the resource-richest clients (tier 5) randomly select models from the set \u03b3 = 0.6, 0.8, 1. In our experiments for Table 3 involving three submodels and 10 clients, the tier 1 clients (3 out of 10 total clients) select \u03b3 = 0.5, tier 2 clients (3 out of 10 total clients) select \u03b3 = 0.75 and tier 3 clients (4 out of 10 total clients) select \u03b3 = 1. By allowing clients to randomly choose from the available submodels, our setup reflects the dynamic nature in which clients may encounter communication computing bottlenecks during each iteration."
        },
        {
            "heading": "B.8 PSEUDOCODE FOR PARAMETER AVERAGING",
            "text": "import numpy as np\nM = [[] for _ in range(Ns)] for i in range(len(uploaded_weights)):\nfor k in range(Ns): if uploaded_weights[i]==theta[k]: # parameters of submodel k\nM[k].append(uploaded_weights[i])\ndef NeFedAvg(M): # consistent parameters for block in theta_c[-1]: # global model parameters\nfor key in block: # depthwise access by block by block num_submodel_uploaded=[], submodel_idx=[], gamma_W_block=[0]\nfor k in range(Ns): if key in theta_c[k]:\nsubmodel_idx.append(k) num_submodels.append(len(M[k])) card = np.cumsum(num_submodels[::-1])[::-1] # cardinality gamma_W_block.append(gamma_W[k])\nfor i in range(len(submodel_idx)): # widthwise access start = math.ceil(param_size*gamma_W_block[i]) end = math.ceil(param_size*gamma_W_block[i+1]) for w in M[submodel_idx[i]]:\ntheta_c_avg[key][start:end]+=w[key][start:end]/card[i] return theta_c_avg\ndef InconsistentParamAvg(M): # inconsistent parameters for k in range(Ns):\nfor key in theta_ic[k]: for w in M[k]: theta_ic_avg[k][key] += w[key]\nreturn theta_ic_avg\nTa bl\ne 12\n:D et\nai ls\nof \u03b3\nof N\neF L\n-D an\nd N\neF L\n-W D\non R\nes N\net 56\nM od el in de x\nM od\nel si ze \u03b3\n\u03b3 W\n\u03b3 D\nN eF\nL -D\n(R es\nN et\n56 )\nL ay\ner 1\n(1 6)\nL ay\ner 2\n(3 2)\nL ay\ner 3\n(6 4)\n1 0.\n2 1\n0. 2\n1, 1,\n0, 0,\n0, 0,\n0, 0,\n0 1,\n1, 0,\n0, 0,\n0, 0,\n0, 0\n1, 1,\n0, 0,\n0, 0,\n0, 0, 0 2 0. 4 1 0. 4 1, 1, 1, 0, 0, 0, 0, 0, 0 1, 1, 1, 0, 0, 0, 0, 0, 0 1, 1, 1, 1, 0, 0, 0, 0, 0 3 0. 6 1 0. 6 1, 1, 1, 1, 0, 0, 0, 0, 0 1, 1, 1, 1, 0, 0, 0, 0, 0 1, 1, 1, 1, 1, 1, 0, 0, 0 4 0. 8 1 0. 8 1, 1, 1, 1, 1, 1, 1, 1, 1 1, 1, 1, 1, 1, 1, 1, 1, 0 1, 1, 1, 1, 1, 1, 1, 0, 0 5 1 1 1 1, 1, 1, 1, 1, 1, 1, 1, 1 1, 1, 1, 1, 1, 1, 1, 1, 1 1, 1, 1, 1, 1, 1, 1, 1, 1\nM od el in de x\nM od\nel si ze \u03b3\n\u03b3 W\n\u03b3 D\nN eF\nL -D\nO (R\nes N\net 56 ) L ay er 1 (1 6) L ay er 2 (3\n2) L\nay er\n3 (6 4) 1 0. 2 1 0. 2 1, 8, 0, 0, 0, 0, 0, 0, 0 1, 8, 0, 0, 0, 0, 0, 0, 0 1, 8, 0, 0, 0, 0, 0,\n0, 0\n2 0.\n4 1\n0. 4\n1, 1,\n7, 0,\n0, 0,\n0, 0,\n0 1,\n1, 7,\n0, 0,\n0, 0,\n0, 0\n1, 1,\n1, 6,\n0, 0,\n0, 0, 0 3 0. 6 1 0. 6 1, 1, 1, 6, 0, 0, 0, 0, 0 1, 1, 1, 6, 0, 0, 0, 0, 0 1, 1, 1, 1, 1, 4, 0, 0, 0 4 0. 8 1 0. 8 1, 1, 1, 1, 1, 1, 1, 1, 1 1, 1, 1, 1, 1, 1, 1, 2, 0 1, 1, 1, 1, 1, 1, 3, 0, 0 5 1 1 1 1, 1, 1, 1, 1, 1, 1, 1, 1 1, 1, 1, 1, 1, 1, 1, 1, 1 1, 1, 1, 1, 1, 1, 1, 1, 1\nM od el in de x\nM od\nel si ze \u03b3\n\u03b3 W\n\u03b3 D\nN eF\nL -W\nD (R\nes N\net 56 ) L ay er 1 (1 6) L ay er 2 (3\n2) L\nay er\n3 (6 4) 1 0. 2 0. 46 0. 43 1, 1, 1, 1, 0, 0, 0, 0, 0 1, 1, 1, 1, 0, 0, 0, 0, 0 1, 1, 1, 1, 0, 0,\n0, 0, 0 2 0. 4 0. 61 0. 66 1, 1, 1, 1, 1, 1, 0, 0, 0 1, 1, 1, 1, 1, 1, 0, 0, 0 1, 1, 1, 1, 1, 1, 0, 0, 0 3 0. 6 0. 77 0. 77 1, 1, 1, 1, 1, 1, 1, 0, 0 1, 1, 1, 1, 1, 1, 1, 0, 0 1, 1, 1, 1, 1, 1, 1, 0, 0 4 0. 8 0. 90 89 1, 1, 1, 1, 1, 1, 1, 1, 0 1, 1, 1, 1, 1, 1, 1, 1, 0 1, 1, 1, 1, 1, 1, 1, 1, 0 5 1 1 1 1, 1, 1, 1, 1, 1, 1, 1, 1 1, 1, 1, 1, 1, 1, 1, 1, 1 1, 1, 1, 1, 1, 1, 1, 1, 1\nTa bl\ne 13\n:D et\nai ls\nof \u03b3\nof N\neF L\n-D an\nd N\neF L\n-W on\nV iT\n-B /1\n6\nM od el in de x\nM od\nel si ze \u03b3\n\u03b3 W\n\u03b3 D\nN eF\nL -D\n(V iT\n-B /1 6) B lo ck\n1 0.\n5 1\n0. 50\n1, 1,\n1, 1,\n1, 1,\n0, 0,\n0, 0,\n0, 0\n2 0.\n75 1\n0. 75\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1, 0,\n0, 0\n3 1\n1 1\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1, 1\nM od el in de x\nM od\nel si ze \u03b3\n\u03b3 W\n\u03b3 D\nN eF\nL -W\n(V iT\n-B /1 6) B lo ck\n1 0.\n5 0.\n5 1\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1, 1\n2 0.\n75 0.\n75 1\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1, 1\n3 1\n1 1\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1, 1\nTa bl\ne 14\n:D et\nai ls\nof \u03b3\nof N\neF L\non R\nes N\net 11\n0\nM od\nel si ze \u03b3\n\u03b3 W\n\u03b3 D\nN eF\nL -D\n(R es\nN et\n11 0)\nL ay\ner 1\n(1 6)\nL ay\ner 2\n(3 2)\nL ay\ner 3\n(6 4)\n0. 2\n1 0.\n20 1,\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1, 0,\n0 1,\n1, 1,\n1, 0,\n0, 0,\n0, 0,\n0, 0,\n0, 0,\n0, 0,\n0, 0,\n0 1,\n1, 1,\n0, 0,\n0, 0,\n0, 0,\n0, 0,\n0, 0,\n0, 0,\n0, 0, 0 0. 4 1 0. 40 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 0. 6 1 0. 60 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0 0. 8 1 0. 80 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0 1 1 1 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\nM od\nel si ze \u03b3\n\u03b3 W\n\u03b3 D\nN eF\nL -W\nD (R\nes N\net 11 0) L ay er 1 (1 6) L ay er 2 (3\n2) L\nay er\n3 (6 4) 0. 2 0. 46 0. 44 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n0, 0,\n0, 0,\n0, 0, 1 0. 4 0. 60 0. 66 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1 0. 6 0. 77 0. 77 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1 0. 8 0. 90 0. 89 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1 1 1 1 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\nM od\nel si ze \u03b3\n\u03b3 W\n\u03b3 D\nN eF\nL -D\n(R es\nN et\n11 0)\nL ay\ner 1\n(1 6)\nL ay\ner 2\n(3 2)\nL ay\ner 3\n(6 4)\n0. 04\n1 0.\n04 1,\n0, 0,\n0, 0,\n0, 0,\n0, 0,\n0, 0,\n0, 0,\n0, 0,\n0, 0,\n0 1,\n0, 0,\n0, 0,\n0, 0,\n0, 0,\n0, 0,\n0, 0,\n0, 0,\n0, 0,\n0 1,\n0, 0,\n0, 0,\n0, 0,\n0, 0,\n0, 0,\n0, 0,\n0, 0,\n0, 0, 0 0. 16 1 0. 16 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 0. 36 1 0. 37 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 0. 64 1 0. 65 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0 1 1 1 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\nM od\nel si ze \u03b3\n\u03b3 W\n\u03b3 D\nN eF\nL -W\nD (R\nes N\net 11 0) L ay er 1 (1 6) L ay er 2 (3\n2) L\nay er\n3 (6 4) 0. 04 0. 26 0. 16 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n0, 0,\n0, 0,\n0, 0, 0 0. 16 0. 42 0. 38 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 0. 36 0. 59 0. 61 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0 0. 64 0. 77 0. 83 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0 1 1 1 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\nTa bl\ne 15\n:D et\nai ls\nof \u03b3\nof N\neF L\non W\nid e\nR es\nN et\n10 1\n2\nM od\nel si ze \u03b3\n\u03b3 W\n\u03b3 D\nN eF\nL -D\n(W id\ne R\nes N\net 10\n1 2)\nL ay\ner 1\n(1 28\n) L\nay er\n2 (2\n56 )\nL ay\ner 3\n(5 12\n) L\nay er\n4 (1\n02 4)\n0. 5\n1 0.\n51 1,\n1, 1\n1, 1,\n1, 1\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1, 0,\n0, 0,\n0, 0,\n0, 0,\n0, 0,\n0, 0,\n0, 0,\n0 1,\n1, 0\n0. 75\n1 0.\n75 1,\n1, 1\n1, 1,\n1, 1\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n0, 0,\n0, 0,\n0, 0,\n0, 0,\n0 1,\n1, 1\n1 1\n1 1,\n1, 1\n1, 1,\n1, 1\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1, 1,\n1 1,\n1, 1\nC INTERPRETING SCALING WITH ODE SOLVER\nWe present a toy example of ODE solvers in Figure 4. The black line representing an actual function is 0.1t+sin(0.2t)+cos(0.3t) and the red line representing a discretized approximation of the actual function (i.e., a full neural network). Here, it was implemented by ODE solver of step size h = 2. The green line representing a widthwise scaled submodel has numerical errors on each step. The blue solid line is a depthwise scaled submodel with step size h = 3. The blue dashed line has forward with same steps, but with optimized step sizes. It denotes that the optimized step sizes can decrease the numerical error. The cyan colored line is implemented by improved Euler method (Hairer et al., 2000) with step size h = 3. It denotes that even with same steps with blue solid lines, and optimizing dy/dt also contributes to decrease the numerical error. The figure shows a toy example why each submodel can still work well with less parameters."
        }
    ],
    "title": "NEFL: NESTED FEDERATED LEARNING FOR HETEROGENEOUS CLIENTS",
    "year": 2023
}