{
    "abstractText": "The importance of panoramic traffic perception tasks in autonomous driving is increasing, so shared networks with high accuracy are becoming increasingly important. In this paper, we propose a multi-task shared sensing network, called CenterPNets, that can perform the three major detection tasks of target detection, driving area segmentation, and lane detection in traffic sensing in one go and propose several key optimizations to improve the overall detection performance. First, this paper proposes an efficient detection head and segmentation head based on a shared path aggregation network to improve the overall reuse rate of CenterPNets and an efficient multi-task joint training loss function to optimize the model. Secondly, the detection head branch uses an anchor-free frame mechanism to automatically regress target location information to improve the inference speed of the model. Finally, the split-head branch fuses deep multi-scale features with shallow fine-grained features, ensuring that the extracted features are rich in detail. CenterPNets achieves an average detection accuracy of 75.8% on the publicly available large-scale Berkeley DeepDrive dataset, with an intersection ratio of 92.8% and 32.1% for driveableareas and lane areas, respectively. Therefore, CenterPNets is a precise and effective solution to the multi-tasking detection issue.",
    "authors": [
        {
            "affiliations": [],
            "name": "Enrico Meli"
        },
        {
            "affiliations": [],
            "name": "Guangqiu Chen"
        },
        {
            "affiliations": [],
            "name": "Tao Wu"
        },
        {
            "affiliations": [],
            "name": "Jin Duan"
        },
        {
            "affiliations": [],
            "name": "Qi Hu"
        },
        {
            "affiliations": [],
            "name": "Dandan Huang"
        },
        {
            "affiliations": [],
            "name": "Hao Li"
        }
    ],
    "id": "SP:137337c17e494eee59323ef5066aee305d78787d",
    "references": [
        {
            "authors": [
                "W. Liu",
                "D. Anguelov",
                "D. Erhan",
                "C. Szegedy",
                "S. Reed",
                "C.-Y. Fu"
            ],
            "title": "SSD: Single Shot MultiBox Detector. InComputer Vision\u2014ECCV",
            "venue": "Eds.; Springer International Publishing: Cham, Switzerland,",
            "year": 2016
        },
        {
            "authors": [
                "J. Redmon",
                "S. Divvala",
                "R. Girshick",
                "A. Farhadi"
            ],
            "title": "You Only Look Once: Unified, Real-Time Object Detection",
            "venue": "In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition (CVPR), Las Vegas, NV, USA,",
            "year": 2016
        },
        {
            "authors": [
                "E. Shelhamer",
                "J. Long",
                "T. Darrell"
            ],
            "title": "Fully Convolutional Networks for Semantic Segmentation",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "year": 2017
        },
        {
            "authors": [
                "H. Zhao",
                "J. Shi",
                "X. Qi",
                "X. Wang",
                "J. Jia"
            ],
            "title": "Pyramid scene parsing network",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "A. Paszke",
                "A. Chaurasia",
                "S. Kim",
                "E. Culurciello"
            ],
            "title": "Enet: A deep neural network architec-ture for real-time semantic segmentation",
            "venue": "arXiv 2016,",
            "year": 2016
        },
        {
            "authors": [
                "Y. Ouyang"
            ],
            "title": "Strong-Structural Convolution Neural Network for Semantic Segmentation",
            "venue": "Pattern Recognit. Image Anal",
            "year": 2019
        },
        {
            "authors": [
                "Z. Wang",
                "W. Ren",
                "Q. Qiu"
            ],
            "title": "LaneNet: Real-Time Lane Detection Networks for AutonomousDriving",
            "venue": "arXiv 2018,",
            "year": 2018
        },
        {
            "authors": [
                "X. Pan",
                "J. Shi",
                "P. Luo",
                "X. Wang",
                "X. Tang"
            ],
            "title": "Spatial As Deep: Spatial CNN for Traffic SceneUndrstanding",
            "venue": "arXiv 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Y. Hou",
                "Z. Ma",
                "C. Liu",
                "C.C. Loy"
            ],
            "title": "Learning Lightweight Lane Detection CNNs by Self Attention Distillation",
            "venue": "In Proceedings of the IEEE/CVFInternationalConference onComputer Vision (ICCV), Seoul, Republic of Korea,",
            "year": 2019
        },
        {
            "authors": [
                "K. He",
                "G. Gkioxari",
                "P. Doll\u00e1r",
                "R. Girshick"
            ],
            "title": "Mask R-CNN",
            "venue": "IEEE Trans. PatternAnal. Mach. Intell",
            "year": 2020
        },
        {
            "authors": [
                "S. Ren",
                "K. He",
                "R. Girshick",
                "J. Sun"
            ],
            "title": "Faster r-cnn: Towards real-time object detection with regionproposalnetworks",
            "venue": "arXiv 2015,",
            "year": 2015
        },
        {
            "authors": [
                "B. Liu",
                "H. Chen",
                "Z. Wang"
            ],
            "title": "LSNet: Extremely Light-Weight Siamese Network For ChangeDetection in Remote Sensing Image",
            "venue": "arXiv 2022,",
            "year": 2022
        },
        {
            "authors": [
                "M. Teichmann",
                "M. Weber",
                "M. Z\u00f6llner",
                "R. Cipolla",
                "R. Urtasun"
            ],
            "title": "MultiNet: Real-time JointSemantic Reasoning for Autonomous Driving",
            "venue": "In Proceedings of the IEEE IntelligentVehicles Symposium (IV), Changshu, China,",
            "year": 2018
        },
        {
            "authors": [
                "F. Yu",
                "H. Chen",
                "X. Wang",
                "W. Xian",
                "Y. Chen",
                "F. Liu",
                "V. Madhavan",
                "T. Darrell"
            ],
            "title": "BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "D. Wu",
                "M. Liao",
                "W. Zhang",
                "X. Wang"
            ],
            "title": "YOLOP: You Only Look Once for Panoptic DrivingPerception",
            "venue": "arXiv 2022,",
            "year": 2022
        },
        {
            "authors": [
                "D. Vu",
                "B. Ngo",
                "H. Phan"
            ],
            "title": "HybridNets: End-to-End Perception Network",
            "venue": "arXiv 2022,",
            "year": 2022
        },
        {
            "authors": [
                "C.-Y. Wang",
                "A. Bochkovskiy",
                "Liao"
            ],
            "title": "H.Y.M. Scaled-yolov4: Scaling cross stage partialnetwork",
            "venue": "arXiv 2020,",
            "year": 2020
        },
        {
            "authors": [
                "S. Liu",
                "L. Qi",
                "H. Qin",
                "J. Shi",
                "J. Jia"
            ],
            "title": "Path aggregation network for instance segmentation",
            "venue": "In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, Salt Lake City, UT, USA,",
            "year": 2018
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep Residual Learning for Image Recognition",
            "venue": "In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Spatial pyramid pooling in deep convolutional networks forvisual recognition",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "year": 2015
        },
        {
            "authors": [
                "T.-Y. Lin",
                "P. Doll\u00e1r",
                "R. Girshick",
                "K. He",
                "S. Belongie"
            ],
            "title": "Feature pyramidnetworks for object detection",
            "venue": "In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "H. Law",
                "J. Deng"
            ],
            "title": "Cornernet: Detecting objects as paired keypoints",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany,",
            "year": 2018
        },
        {
            "authors": [
                "Z. Cao",
                "T. Simon",
                "S.-E. Wei",
                "Y. Sheikh"
            ],
            "title": "Realtime multi-person 2d poseestimation using part affinity fields",
            "venue": "In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "T.-Y. Lin",
                "P. Goyal",
                "R. Girshick",
                "K. He",
                "P. Doll\u00e1r"
            ],
            "title": "Focal loss for denseobject detection",
            "venue": "In Proceedings of the IEEE international Conference onComputer Vision, Venice, Italy,",
            "year": 2017
        },
        {
            "authors": [
                "S.S.M. Salehi",
                "D. Erdogmus",
                "A. Tversky Gholipour"
            ],
            "title": "Loss Function for Image SegmentationUsing 3D Fully Convolutional Deep Networks. In Machine Learning in Medical Imaging, MLMI 2017",
            "venue": "Lecture Notes in Computer,",
            "year": 2017
        },
        {
            "authors": [
                "T.-Y. Lin",
                "P. Goyal",
                "R. Girshick",
                "K. He",
                "P. Doll\u00e1r"
            ],
            "title": "Focal Loss for Dense ObjectDetection",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "year": 2020
        },
        {
            "authors": [
                "I. Loshchilov",
                "F. Hutter"
            ],
            "title": "Decoupled Weight Decay Regularization",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Citation: Chen, G.; Wu, T.; Duan, J.;\nHu, Q.; Huang, D.; Li, H.\nCenterPNets: A Multi-Task Shared\nNetwork for Traffic Perception.\nSensors 2023, 23, 2467. https://\ndoi.org/10.3390/s23052467\nAcademic Editor: Enrico Meli\nReceived: 17 January 2023\nRevised: 20 February 2023\nAccepted: 20 February 2023\nPublished: 23 February 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: traffic perception; multi-task learning; target detection; semantic segmentation"
        },
        {
            "heading": "1. Introduction",
            "text": "In recent years, the rapid development of embedded systems and neural networks has made autonomous driving a popular field in computer vision, where panoramic traffic perception systems play a crucial role in autonomous driving. Research has shown that vehicle onboard camera image processing enables scene understanding, including road target detection, driveable area detection, and lane detection, which greatly reduces overhead compared to the traditional approach of using LIDAR and millimeter wave radar to establish the vehicle\u2019s surroundings. The traffic panorama perception system\u2019s detection precision and decision-making speed significantly influence the vehicle\u2019s judgment and decision-making and determine the safety of autonomous vehicles. However, actual vehicle driver assistance systems, such asthe Advanced Driver Assistance System, have limited computing power and are expensive. Therefore, achieving a good balance between detection accuracy and model complexity from a practical application perspective is a challenge for decision-makers. Current target detection can be broadly divided into one-stage detection models and two-stage detection models. The two-stage detection approach usually starts by acquiring a candidate region and then performing a regression prediction from that candidate region to ensure the accuracy of the detection. However, this step-by-step detection approach is not friendly to embedded systems. The end-to-end, one-stage detection model has the advantage of fast inference speed and is gaining more attention in the field of detection.The use of direct regression bounding boxes in the SSD [1] series, YOLO [2] series, etc. is\nSensors 2023, 23, 2467. https://doi.org/10.3390/s23052467 https://www.mdpi.com/journal/sensors\nSensors 2023, 23, 2467 2 of 15\na milestone in the first stage of detection. FCN [3] was the first to introduce fully convolutional networks to the task of semantic segmentation, although their performance was limited by resolution. PSPNet [4] proposes pyramid pooling to extract multi-scale features to improve detection performance. Enet [5] reduces the size of the feature map. SSN [6] incorporates conditional random field units in the post-processing stage to improve segmentation performance. LaneNet [7] proposes to use a single lane line as the object of instance segmentation. Layer-by-layer convolution is a technique used by spatial CNN [8] that enables the transfer of feature information between ranks in a layer. Enet SAD [9], on the other hand, uses a self-focused distillation method so that the feature maps can learn from each other. Although the above algorithms are effective for their respective single-task detections, they can cause unnecessary network delays if they are used to achieve multi-task detection by acquiring the corresponding features through different task networks one by one. Multitasking networks, however, are better achieved by sharing information between multiple tasks. Mask R-CNN [10] extends Faster R-CNN [11] by mask branching to parallelize the detection task with the segmentation task. A similar approach is used by LSNet [12] for tasks such astarget detection, instance segmentation, etc. An encoder\u2013decoder structure is suggested by MultiNet [13] to carry out the scene perception job concurrently.On the BDD100K dataset [14], YOLOP [15] is the first multi-tasking problem that implements panoramic driving perception, i.e., traffic target detection, driveable area segmentation, and lane detection, with high accuracy and speed at the same time with the help of embedded devices. YOLOP uses an efficient network structure and passes the feature information extracted from the images to the different decoders for their respective detection tasks. However, the use of separate segmentation heads for the driveable area and lane line segmentation tasks leaves room for multi-network optimization, i.e., these tasks can be fused into an overall segmentation task. HybridNets [16] uses a lighter backbone network than YOLOP, and to achieve a higher level of feature fusion, the neck network uses a weighted bi-directional feature network that treats each top-down and bottom-up bidirectional path as a feature network layer. However, the anchor mechanism is used in the detection task to return vehicle position information, which requires pre-clustering of anchor boxes in order to better fit the target size and has cumbersome subsequent processing during the prediction process. In this paper, we propose a more efficient multitasking network after a thorough study of previous approaches and incorporating the idea of an anchor-free architecture. The CenterPNets backbone feature extraction network uses the CSPDarknet [17] module pre-trained on ImageNet and fuses the path aggregation neck network to achieve a good balance between detection accuracy and computational overhead. The PANet [18] decoder uses multi-scale feature data for tasks such as segmentation and detection. For model optimization, CenterPNets employs a multi-task joint loss function. CenterPNets abandons the anchor mechanism with high recall in the detection head in favor of an anchor-free mechanism that returns the target center position information without the need for time-consuming anchor frame clustering and subsequent processing, such as NMS, thereby increasing the network\u2019s overall inference speed. In the segmentation task, shallow features are rich in fine-grained information, which is essential for image segmentation. For this reason, we fuse multi-scale features with shallow features to retain the detailed information of the image and make the segmented edges smoother. In this paper, in addition to using an end-to-end training strategy, we have also tried a frozen training approach. Using the freeze training strategy, this approach has been shown to be effective at preventing information interference from other non-relevant modules in the network and the completed training tasks are instructive for other tasks. To sum up, the main contributions of this research are: (1) This paper proposes an effective end-to-end shared multi-task network structure that can jointly handle three important traffic sensing tasks: lane detection, driveable area segmentation, and road target detection. The network\u2019s encoders and decoders are shared to fully exploit the\nSensors 2023, 23, 2467 3 of 15\ncorrelation between each task\u2019s semantic features, which can help the network reduce model redundancy. (2) The detection head adopts an anchor-free mechanism to directly return the target key point information, size, and offset, without the need for pre-clustering anchor box ratio and tedious subsequent processing, thus enhancing the overall inference speed of the network. (3) In the segmentation head section, similar features from the shared detection task are used and proposed to fuse multi-scale, deep semantic information with shallow features so that the feature information extracted from the segmentation task is rich in fine-grained information, thus enhancing the detail segmentation capability of the model."
        },
        {
            "heading": "2. Methods",
            "text": ""
        },
        {
            "heading": "2.1. Network Architecture",
            "text": "This paper proposes a multi-task traffic panorama perception architecture that can be jointly trained, called CenterPNets. As shown in Figure 1, the structure mainly contains encoders, decoders, and task-independent detection heads to handle the corresponding detection tasks, and there are no redundant parts between the modules, which reduces computational consumption to a certain extent.\nSensors 2023, 23, x FOR PEER REVIEW 3 of 15\nTo sum up, the main contributions of this research are: (1)This paper proposes an effective end-to-end shared multi-task network structure that can jointly handle three important traffic sensing tasks: lane detection, driveable area segmentation, and road target detection. The network\u2019s encoders and decoders are shared to fully exploit the correlation between each task\u2019s semantic features, which can help the network reduce model redundancy. (2)The detection head adopts an anchor-free mechanism to directly return the target key point information, size, and offset, without the need for pre-clustering anchor box ratio and tedious subsequent processing, thus enhancing the overall inference speed of the network. (3)In the segmentation head section, similar features from the shared detection task are used and proposed to fuse multi-scale, deep emantic information with shallow featur s so that the feature infor ation extracted\nfrom th segmentation task is rich in fine-grained information, thus enhanci g the detail segmentation capability of the model."
        },
        {
            "heading": "2. Methods",
            "text": ""
        },
        {
            "heading": "2.1. Network Architecture",
            "text": "This paper proposes a multi-task traffic panorama perception architecture that can be jointly trained, called CenterPNets. As shown in Figure 1, the structure mainly contains encoders, decoders, and task-independent detection heads to handle the correspondi g detection tasks, and there are no redundant parts b tween th modules, which reduces computational consumption to a certain extent.\nIn the encoder part, feature extraction is the core structure in the network, which directly determines the accuracy of the network detection. Many modern networks currently extract features directly using networks that have good detection performance in the ImageNet dataset. One of the most traditional deep networks, Darknet, combines\nIn the encoder part, feature extraction is the core structure in the network, which directly determines the accuracy of the network detection. Many modern networks currently extract features directly using networks that have good detection performance in the Imag Net dataset. One of the most traditional de p networks, Darknet, combines Resnet [19] features to ensure excellent feature representation while avoiding the gradient issues that\nSensors 2023, 23, 2467 4 of 15\ncome with overlying deep networks. CenterPNetsusesCSPDarkNet as the backbone, which combines the advantages of CSPNet and SPP [20] modules to maximize the difference in gradient union, and its use of gradient stream splitting and merging to avoid different layers of learning to duplicate gradient information is effective enough to reduce duplicate gradient learning. As a result, the backbone network of CenterPNets can extract crucial feature information while lowering the network\u2019s computational cost. The feature map extracted by the encoder is passed to the neck structure of the network. The Feature Pyramid Network(FPN) module [21] a feature extractor design for generating multi-scale feature maps to obtain better information. However, the limitation of FPN is that the information features are inherited by a uni-directional flow. As a result, the CenterPNets neck network makes use of the PANet module with the addition of a top-down feature pyramid behind the FPN layer. Through its structural properties, it can effectively compensate for the fact that FPN only enhances the semantic information of the feature pyramid and lacks localization information.\nA. Anchor-free detection head\nAs shown in Figure 2, in the detection head section, CenterPNets integrates information from the P3_out, P4_out, and P5_out multi-level feature maps in the neck network at the same resolution in order to obtain multi-level semantic features, followed by pyramid pooling and attention mechanisms to reinforce the relevant feature information, which is recovered by upsampling to a feature map with 1/4 of the input image resolution. CenterPNets uses an anchor-free mechanism [22] for direct regression prediction, eliminating the need for K-means clustering to determine pre-defined anchor box proportions and tedious NMS follow-up, allowing for direct regression of key point heatmaps, size prediction, and offset prediction, thus improving the overall speed of inference in the network.\nSensors 2023, 23, x FOR PEER REVIEW 4 of 15\nResnet[19] features to ensure excellent feature representation while avoiding the gradient issues that come with overlying deep networks. CenterPNetsusesCSPDarkNet as the backbone, which combin s the advantages of CSPNet and SPP[20] modules to maximize the differ nce in gr dient union, and its use of gradient stream splitting and merging to avoid different layers of learning to uplicat gradient i formation is effective enough to reduce duplicate radient learning. As a result, the backbone network of CenterPNets can extract crucial feature information while lowering the network\u2019s computational cost. The feature map extracted by the encoder is passed to the neck structure of the network. The Feature Pyramid Network(FPN) module[21] a feature extractor design for generating multi-scale feature maps to obtain better information. However, the limitation of FPN is that the information features are inherited by a uni-directional flow. As a result, the CenterPNets neck network makes use of the PANet module with the addition of a top-down feature pyramid behind the FPN layer. Through its structural properties, it can effectively compensate for the fact that FPN only enhances the semantic information of the feature pyramid and lacks localization information. A. Anchor-free detection head\nAs shown in Figure 2, in the detection head section, CenterPNets integrates information from the P3_o t, P4_out, and P5_out multi-level feature maps in the neck network at the same resolution in order to btain multi-level semantic features, followed by pyramid pooling and attention mecha isms to reinforce the r levant feature information, which is recovered by upsampling to a feature map with 1/4 of the i put image resolution.CenterPNets uses an anchor-free mechanism[22] for direct regression prediction, eliminating the need for K-means clustering to determine pre-defined anchor box proportions and tedious NMS follow-up, allowing for direct regression of key point heatmaps, size prediction,and offset prediction, thus improving the overall speed of inference in the network.\nheight of the input image, respectively, C is the category type of the detection target, and R is the output step. In this paper, only the car category label is detected, so C = 1. We use the default output step of R = 4 and deflate the output prediction by R. For the real cate-\ngory labeling point 2RP\u2208 , a low-resolution equivalent point    = R PP\u0302 is used instead. Generate a heat map of key points CHWY \u00d7\u00d7\u2208 ]1,0[\u02c6 during model training, where C represents the number of category labels detected.In this paper, only the car category is detected. 1\u02c6 =Y means that the target to be measured is detected at (x, y) out. 0\u02c6 =Y indicates a background area. For each ground truth keypoint P, we splat it onto a heatmap using a Gaussian kernel\n        \u2212+\u2212 \u2212= 2\n22\nxyc 2 )()( exp p yx pypxY \u03b4 , p\u03b4 is an object size-adaptive\nstandard deviation [23].Whenthere are two Gaussian kernels that overlap, the maximum\nof the input image, respectively, C is the category type of the detection target, and R is the output step. In this paper, only the car category label is detected, so C = 1. We use the default output step of R = 4 and deflate the output prediction by R. For the real category labeling point P \u2208 R2, a low-resolution equivalent point P\u0302 = \u230a P R \u230b is used instead. Generate a heat map of key points Y\u0302 \u2208 [0, 1]W\u00d7H\u00d7C during model training, where C represents the number of category labels detected.In this paper, only the car category is detected. Y\u0302 = 1 means that the target to be measured is detected at (x, y) out.Y\u0302 = 0 indicates a background area. For each ground truth keypoint P, we splat it onto a heatmap using a Gaussian\nkernel Yxyc = exp ( \u2212 (x\u2212px) 2+(y\u2212py)2\n2\u03b42p\n) , \u03b4p is an object size-adaptive standard deviation [23].\nWhen there are two Gaussian kernels that overlap, the maximum value of the elements\nSensors 2023, 23, 2467 5 of 15\nis taken in this paper [24]. The difference between the predicted and real heatmaps is the pixel-wise focus loss [25].\nLk = \u2212 1 N \u2211xyc  (1\u2212 Y\u0302xyc) \u03b1 log(Y\u0302xyc) Yxyc = 1 (1\u2212Yxyc)\u03b2(Y\u0302xyc)\n\u03b1 otherwise log(1\u2212 Y\u0302xyc) otherwise\n(1)\nwhere \u03b1 and \u03b2 are hyperparameters of focal loss and N is the number of critical points. In our experiments, we used \u03b1 = 2, \u03b2 = 4 [23].\nSize prediction: Assume that the kth bounding box has coordinates ( xk1, y k 1, x k 2, y k 2 ) and a width and\nheight sk = ( xk2 \u2212 xk1, yk2 \u2212 yk1 ) . The coordinates of its center point are pk = ( xk1+x k 2 2 , yk1+y k 2 2 ) .\nWe calculate the predicted loss using L1 only at the center of the target.\nLsize = 1 N\nN\n\u2211 k=1 \u2223\u2223S\u0302pk \u2212 sk\u2223\u2223 (2) Offset prediction: The output feature map will contain accuracy errors when remapping to the original\nimage size because the decoder outputs features at a resolution that is one-fourth that of the original input image. As a result, an extra local offset is applied for each key point to make up for the inaccuracy.\nLo f f = 1 N \u2211p \u2223\u2223\u2223O\u0302p\u0303 \u2212 ( pR \u2212 p\u0303)\u2223\u2223\u2223 (3) where O\u0303 p\u0303 denotes the offset of the network prediction, P denotes the image centroid coordinates, and R denotes the heatmap scaling factor.\nB. Segmentation heads incorporating fine-grained features\nAs shown in Figure 3, the segmented head section outputs 3 categories of labels, namely background, road trafficable area, and road lane lines. There is a correlation between the feature information of the detection task and the segmentation task, so CenterPNets shares the same feature mapping between the two and upsamples the feature fusion with the shallow, fine-grained feature P1 layer with rich localization information based on the detection feature mapping, thus enhancing the network\u2019s ability to segment image edge details. Finally, we recover the output features to the original image resolution (W, H, 3), storing the probability values for each pixel category label.\nSensors 2023, 23, x FOR PEER REVIEW 5 of 15\nvalue of the elements is taken in this paper [24]. The difference between the predicted and real heatmaps is the pixel-wise focus loss [25].\n   \n \n\n\u2212 \u2212\n=\u2212 \u2212=\nxyc xyc\nxycxyc\nxycxycxyc\nk\notherwiseY otherwiseYY YYY N L )\u02c6log( )\u02c6()(\n)\u02c6log()\u02c6(\n1 1\n11 1 \u03b1\u03b2\n\u03b1\n(1)\nwhere \u03b1 and \u03b2 are hyperparameters of focal loss and N is the number of critical points. In our experiments, we used \u03b1=2, \u03b2=4 [23].\nSize prediction: Assume that the kth bounding box has coordinates ( )kkkk yxyx 2211 ,,, and a width and\nheight ( )kkkk yyxx 1212k ,s \u2212\u2212= .The coordinates of its center point are       ++= 2 , 2 2121 kkkk k yyxxp . We calculate the predicted loss using L1 only at the center of the target.\n =\n\u2212= N\nk kpsize sSN L k 1\n1 \u02c6 (2)\nOffset prediction: The output feature map will contain accuracy errors when remapping to the original\nimage size be ause the decoder outputs features at a resolution that is one-fourth that of the original input image. As a result, an extra local offset is applied for each key point to make up for the inaccuracy.\n       \u2212\u2212= p poff pR pO N L ~\u02c6 ~ 1 (3)\nwhere p~ ~O denotes the offset of the network prediction, P denotes the image centroid co-\nordinates, and R denotes the heatmap scaling factor. B. Segmentation heads incorporating fine-grained features\nAs shown in Figure 3, the segmented head section outputs 3 categories of labels, namely background, road trafficable area, and road lane lines. There is a correlation between the feature information of the detection task and the segmentation task, so CenterPNets shares the same feature mapping between the two and upsamples the feature fusion with the shallow, fine-grained feature P1 layer with rich localization information based on the detection feature mapping, thus enhancing the network\u2019s ability to segment image edge details. Finally, we recover the output features to the original image resolutio (W, H, 3), storing the p obabil ty v lues for each pixel category label.\nFigure 3. Illustration of the branching process of the segmented head.\nSensors 2023, 23, 2467 6 of 15"
        },
        {
            "heading": "2.2. Loss of Function for Joint Multi-Task Training",
            "text": "The end-to-end network is trained by CenterPNets using a multi-task loss function, which sums two components to represent the entire loss function.\nLall = \u03b1Ldet + \u03b2Lseg (4)\nwhere Ldet is the target detection loss and Lseg is the semantic segmentation loss. \u03b1, \u03b2 are the balance factors of the loss function in order to keep the detection task in the same order of magnitude as the segmentation task.\nLdet = Lk + \u03bbsizeLsize + \u03bbo f f Lo f f (5)\nwhere Lsize, Loff use the ordinary L1 loss function, which is used to regress the width and height and centroid offsets, respectively. For heat map losses, Lk is calculated by focal loss.\nLk = \u2212 1 N \u2211xyc  ( 1\u2212 Y\u0302 xyc )\u03b1 log ( Y\u0302xyc ) Yxyc = 1( 1\u2212Yxyc )\u03b2(Y\u0302xyc)\u03b1 otherwise (6)\nMulti-class mixture loss is used for multi-class segmentation of backgrounds, driveable areas, and lane lines. Semantic segmentation is difficult due to the uneven distribution of data. Therefore, CenterPNets combines Tversky loss LTversky [26] and focus loss LFocal [27] to predict the class to which the pixel belongs. LTversky performs well on the class imbalance problem and is optimized for score maximization, while LFocal aims to minimize classification errors between pixels and focuses on hard labeling.\nLseg = LTversky + LFocal (7)\nLTversky = C\u2212 C\u22121 \u2211\nC=0\nTPp(c) TPp(c) + \u03d5FNp(c) + (1\u2212 \u03d5)FPp(c)\n(8)\nLFocal = \u2212\u03bb 1 N C\u22121 \u2211 c=0 N \u2211 n=1 gn(c)(1\u2212 pn(c))\u03b3 log(pn(c)) (9)\nwhere TPp(c), FNp(c), and FPp(c) are classes of true positives, false negatives, and false positives. Pn(c) is the predicted probability of a class of pixels. gn(c) is denoted as the true annotation category C for pixel n. C is the number of classes in Equation (8) and N is the total number of pixels in the input image in Equation (9)."
        },
        {
            "heading": "3. Results",
            "text": "3.1. Setting 3.1.1. Dataset Setting\nThe experiments in this paper use image data from the Berkeley DeepDrive dataset (BDD100K) to train and validate the model. Existing multi-task networks are trained against datasets from three tasks on BDD100K to help compare performance with other models. In the target detection task, \u201ccar, truck, bus, train\u201d are combined into a single category label \u201ccar,\u201d as MultiNet, YOLOP, and HybridNets can only detect vehicle category labels. Basic enhancements such as rotation, scaling etc. are used in image pre-processing.\n3.1.2. Implementation Details\nThis studyperforms backbone initialization by using CSPDarkNet weights pre-trained on ImageNets. The optimizer uses AdamW [28], where \u03b3 = 1 \u00d7 10\u22123, \u03b21 = 0.9, \u03b22 = 0.999, \u03be = 1 \u00d7 10\u22128, \u03bb = 1 \u00d7 10\u22122. The learning rate is a non-linear approach with its initial value set to 1\u00d7 10\u22125. Optimization uses L1 and focal loss in the target detection task, where \u03bbsize = 0.1, \u03bbo f f = 1. For the driveable area and lane splitting, the\nSensors 2023, 23, 2467 7 of 15\nmodel uses a combination of Tversky loss and focal loss. In this study, 200 cycles are trained on RTXA4000.\n3.1.3. Evaluation Indicators\nIn the traffic target detection task, performance is evaluated with the help of mAP50. mAP50 is calculated by averaging the average accuracy of the categories below a single IoU threshold of 0.5.\nAP = n\u22121 \u2211 i=1 (ri+1 \u2212 ri)Pinter(ri + 1) (10)\nwhere r1, r2, . . . , rn are the recall values corresponding to the first interpolation of the precision interpolation segment in ascending order.\nmAP = \u2211 k i=1 APi\nk (11)\nIn the semantic segmentation task, the IoU metric is used to evaluate the driveable area and lane line segmentation. In this paper, mIoU is represented as the average IoU per class and the IoU metric for individual classes. In order to illustrate the validity of the experiment more favorably, accuracy has been added as an additional criterion.\nIoU = Bt \u2227 Bp Bt \u222a Bp\n(12)\nwhere Bp is the predicted bounding box and Bt is ground truth bounding box."
        },
        {
            "heading": "3.2. Experimental Analysis of Multi-Tasking Networks",
            "text": "In this section, we first train the model end-to-end, then compare it with other representative models in the corresponding tasks and illustrate the effect of each module on the network and the effectiveness of multi-task network learning by means of ablation experiments and freeze-out training, respectively.\n3.2.1. Road Target Detection Tasks\nThe CenterPNets algorithm istestedfor vehicle target detection on the BDD100K dataset and the algorithms are compared with MultiNet, Faster R-CNN, YOLOP, and HybridNets, and their experimental results are shown in Table 1.\nAs shown in Table 1, CenterPNets uses detection accuracy (mAP50) and recall (recall) as evaluation metrics. The CenterPNets model outperforms MultiNet and Faster R-CNN networks in terms of detection accuracy, but falls short of YOLOP and HybridNets. Since YOLOP uses a network structure based on the anchor box mechanism of YOLOV4, it has a high recall rate in feature regression by generating a dense anchor box approach, which allows the network to perform target classification and bounding box coordinate regression directly on this basis; HybridNets also uses a similar mechanism. CenterPNets, on the other hand, uses an anchor-free box mechanism, which results in average regression box quality because the anchor-free mechanism only predicts at locations closer to the center of\nSensors 2023, 23, 2467 8 of 15\nthe real box. As a result, it underperforms in terms of performance metrics when compared to YOLOP and HybridNets. In this study, we use the same image and video data to verify the inference speed of the model. As can be seen from Table 2, the number of model parameters in this study has increased compared to the benchmark algorithm, which is due to the deeper network structure we have used to ensure detection performance. Secondly, the overall structure of the network in this studyis more integrated and eliminates tedious subsequent processing, etc., which optimizes the network to a certain extent. As can be seen from Table 2, the CenterPNets algorithm has an inference speed of 8.709 FPS when we perform unified image inference, compared to 5.719 FPS for HybridNets network inference, which shows a roughly 1.5-times improvement in inference speed, as the anchorless framework mechanism eliminates tedious subsequent processing. To further verify the reliability of the experiments, CenterPNetswastested uniformly using video data for inference, and it can be seen that the inference performance of CenterPNets is still very good.\nTo further evaluate the effectiveness of CenterPNets in real road traffic scenarios, images of road scenes at different times of the day areselected from the BDD100K test set for experimental effectiveness testing. The YOLOP, HybridNets, and CenterPNets algorithms for traffic target recognition tasks at various times of the day are visually compared in Figure 4. The first row displays the results of the YOLOP test, the second row the results of the HybridNets test, and the third row the results of the CenterPNets test. Orange circles denote false negatives, and red circles false positives. The CenterPNets shared network architecture is a further improvement compared to YOLOP and HybridNets. As shown, it can be seen that YOLOP and HybridNets both have a certain degree of missed and false vehicle target detection, while the CenterPNets algorithm has better vehicle target detection capabilities and more accurate bounding boxes in different environments.\n3.2.2. Driveable Area and Lane Detection Tasks\nA. Travelable area segmentation tasks\nCenterPNets uses the IoU metric to evaluate the driveable area segmentation capability and is compared with the algorithms MultiNet, PSPNet, YOLOP, and HybridNets, whose experimental results are shown in Table 3. The driveable portion of the image and the backdrop are the only things the CenterPNets model needs to differentiate between. Comparing the five driveable area detection networks, Table 3 demonstrates that the CenterPNets algorithm had the highest mIoU performance of 92.8%, an improvement of 1.3% and 2.3% over YOLOP and HybridNets, respectively. Due to the feature correlation between the road vehicle detection task and the road travel area segmentation task, the CenterPNets shared network can effectively information correlation between the two; secondly, CenterPNets first fuses deep multiscale features and combines shallow feature information so that the extracted semantic feature information has local fine-grained information at the same time, smoothing the road edge segmentation. For the driveable area segmentation task, in Figure 5, red in the depiction is a false positive and orange is a false negative. The CenterPNets method is more precise than YOLOP and HybridNets region segmentation, as demonstrated by a visual comparison of the CenterPNets network with those two algorithms. YOLOP considers the intersection of bounding boxes while concentrating on determining the class to which the pixel belongs. As a result, the YOLOP model\u2019s detection suffers from some lane line and road\nSensors 2023, 23, 2467 9 of 15\narea misdetection as well as an inability to precisely segment the driveable portion of the road. For the neck network, HybridNets uses a BiFPN architecture, in which information from different receptive fields is combined from different feature map levels by weighting parameters, an improvement over the YOLOP segmentation structure but still with regional underdetection. The CenterPNets algorithm uses the PANet architecture of the neck network to fuse different scale features to make the global information richer, while taking advantage of the correlation between multi-task features and combining it with rich shallow fine-grained feature information to ensure that the network captures more detailed information. The driveable area segmentation task can therefore be effectively improved by the CenterPNets network. The CenterPNets method exhibits some inadequate area segmentation at complicated junctions, as seen in the picture, but the overall highway driveable area may be more precisely segregated from the backdrop and lane lines. Sensors 2023, 23, x FOR PEER REVIEW 9 of 15\n(a)\n(b)\n(c)\nFigure 4. Target detection visualization comparison results. (a) YOLOP, (b) HybridNets, (c) CenterPNets.\nFigure 4. Target detection visualization comparison results. (a) YOLOP, (b) HybridNets, (c) CenterPNets.\nSensors 2023, 23, 2467 10 of 15\nSensors 2023, 23, x FOR PEER REVIEW 10 of 15 For the driveable area segmentation task, in Figure 5, red in the depiction is a false positive and orange is a false negative. The CenterPNets method is more precise than YOLOP and HybridNets region segmentation, as demonstrated by a visual comparison of the CenterPNets network with those two algorithms. YOLOP considers the intersection of bounding boxes while concentrating on determining the class to which the pixel belongs. As a result, the YOLOP model\u2019s detection suffers from some lane line and road area misdetection as well as an inability to precisely segment the driveable portion of the road. For the neck network, HybridNets uses a BiFPN architecture, in which information from different receptive fields is combined from different feature map levels by weighting parameters, an improvement over the YOLOP segmentation structure but still with regional underdetection. The CenterPNets algorithm uses the PANet architecture of the neck network to fuse different scale features to make the global information richer, while taking advantage of the correlation between multi-task features and combining it with rich shallow fine-grained feature information to ensure that the network captures more detailed information. The driveable area segmentation task can therefore be effectively improved by the CenterPNets network. The CenterPNets method exhibits some inadequate area segmentation at complicated junctions, as seen in the picture, but the overall highway driveable area may be more precisely segregated from the backdrop and lane lines.\nFigure 5. Comparative results of t e visualization f the driveable area segmentation. (a) YOLOP, ( ) HybridNets, (c) CenterPNets.\nB. Lane area splitting task\nLane detection is one of the main challenges for autonomous driving. CenterPNets uses accuracy and IoU as evaluation metrics for lane detection and compares the algorithms with ENet, SCNN, YOLOP, and HybridNets, whose experimental results are shown in Table 4.\nSensors 2023, 23, 2467 11 of 15\nAs shown in Table 4, CenterPNets\u2019 shared network multi-tasking architecture accomplished both the driving area and lane line segmentation tasks in the segmentation head section, with the CenterPNets algorithm achieving the best performance results of 86.20% accuracy and 32.1% IoU, an improvement in performance compared to other detection networks. As shown in Figure 6 on the lane segmentation task, with the orange circles showing false negatives and the red circles false positives, a visual comparison of the CenterPNets network with YOLOP and HybridNet shows that there is a degree of lane pixel underdetection in YOLOP and HybridNets, while the features extracted by the CenterPNets algorithm have richer global fine-grained information. As a result, it excels in lane detection, and the outcomes of lane segmentation are more continuous and complete.\nSensors 2023, 23, x FOR PEER REVIEW 11 of 15 B. Lane area splitting task Lane detection is one of the main challenges for autonomous driving. CenterPNets uses accuracy and IoU as evaluation metrics for lane detection and compares the algorithms with ENet, SCNN, YOLOP, and HybridNets, whose experimental results are shown in Table 4. Table 4. Performance comparison of lane detection tasks.\nModel Accuracy(%) Lane Line IoU(%) ENet 34.12 14.64\nSCNN 35.79 15.84 YOLOP 70.50 26.20\nHybridNets 85.40 31.60 CenterPNets 86.20 32.10\nAs shown in Table 4, CenterPNets\u2019 shared network multi-tasking architecture accomplished both the driving ar a and lane line segmentation tasks in t e segmentation head section, with the CenterPNets algorithm achieving the best performance results of 86.20% accuracy and 32.1% IoU, an improvement in performance compared to other detection networks. As shown in Figure 6 on the lane segmentation task, with the orange circles showing false negatives and the red circles false positives, a visual comparison of the CenterPNets network with Y and HybridNet shows t at there is a degre of lane pix l underde ection in YOLOP and HybridNets, while th fe tures extracted by the CenterPNets algorithm have richer global fine-grained information. As a result, it x els in lane detection, and the outcomes of lane seg entation are more continuous and complete.\n(a)\n(b)\nSensors 2023, 23, x FOR PEER REVIEW 12 of 15\n(c)\nFigure 6. Comparison of lane split visualization results. (a)YOLOP, (b) HybridNets, (c) CenterPNets.\nA. Split task ablation experiment CenterPNets is further used to analyze the impact of modules such as multi-scale feature information (MFI),spatial pyramidal pooling (SPP), attention mechanism (Attention), and superficial feature information (SCI) on the segmentation task. As can be seen from Experiments 1 and 2 in Table 5, by introducing multi-scale information fusion, the driveable area IoU and accuracy improved by 3.8% and 1.9%, respectively, and the lane detection IoU and accuracy improved by 2.8% and 2.3%, respectively, thus demonstrating the effectiveness of multi-level contextual feature information for the segmentation task. Experiments 2, 3, 4, and 5 show that the spatial pyramid pooling and attention mechanism effectively enhance the road-related area features, with 1.0% and 1.8% improvements in the IoU and accuracy of the lane lines, respectively.\nTable 5. Experimental analysis of semantic segmentation task ablation.\nExperimental Serial Number MFI SPP Attention SCI\nDriveable Lane Line IoU (%) Acc (%) IoU (%) Acc (%)\n1 --- --- --- --- 78.7 93.2 28.3 81.2 2 \u221a --- --- --- 82.5 95.1 31.1 83.5 3 \u221a \u221a --- --- 82.2 94.6 31.3 84.8 4 \u221a \u221a \u221a --- 82.9 94.8 31.5 85.5 5 \u221a \u221a \u221a \u221a 82.6 94.2 32.1 85.3\n3.2.3. Training Method Comparison Experiment In order to verify the effectiveness of joint multi-task training, this paper compares the impact of the multi-task training approach and the single-task training approach on the overall performance of the network. Table 6 shows a comparison of the performance of these two schemes on their specific tasks. It can be seen that the overall performance of the model in this paper using a multi-task training scheme outperforms the performance of the individual tasks. More importantly, the multi-task model can save a significant amount of inference time compared to performing the respective tasks individually.\nFigure 6. Comparison of l split visualization results. (a) YOL P, (b) HybridNets, (c) CenterPNets.\nC. Split task ablation experiment\nCenterPNets is further used to analyze the impact of modules such as multi-scale feature information (MFI),spatial pyramidal pooling (SPP), attention mechanism (Atten-\nSensors 2023, 23, 2467 12 of 15\ntion), and superficial feature information (SCI) on the segmentation task. As can be seen from Experiments 1 and 2 in Table 5, by introducing multi-scale information fusion, the driveable area IoU and accuracy improved by 3.8% and 1.9%, respectively, and the lane detection IoU and accuracy improved by 2.8% and 2.3%, respectively, thus demonstrating the effectiveness of multi-level contextual feature information for the segmentation task. Experiments 2, 3, 4, and 5 show that the spatial pyramid pooling and attention mechanism effectively enhance the road-related area features, with 1.0% and 1.8% improvements in the IoU and accuracy of the lane lines, respectively.\nIn order to verify the effectiveness of joint multi-task training, this paper compares the impact of the multi-task training approach and the single-task training approach on the overall performance of the network. Table 6 shows a comparison of the performance of these two schemes on their specific tasks. It can be seen that the overall performance of the model in this paper using a multi-task training scheme outperforms the performance of the individual tasks. More importantly, the multi-task model can save a significant amount of inference time compared to performing the respective tasks individually.\nFigure 7 shows the results of some of the CenterPNets tests, where yellow is the lane line, red is the driveable area, and the green border is the traffic vehicle target. As can be seen, CenterPNets performed relatively well in most cases.CenterPNets exploits the correlation between the detection task and the segmentation task based on contextual information in order to help the training model converge more quickly. Therefore, CenterPNets in this paper can perform the traffic perception task more easily. In general, CenterPNets can perform the detection task well in the vast majority of scenarios. However, there are still some lane prediction interruptions and missed detections at complex intersections.\nSensors 2023, 23, 2467 13 of 15\nSensors 2023, 23, x FOR PEER REVIEW 13 of 15\nFigure 7 shows the results of some of the CenterPNets tests, where yellow is the lane line, red is the driveable area, and the green border is the traffic vehicle target. As can be seen , CenterPNets performed relatively well in most cases.CenterPNets exploits the correlation between the detection task and the segmentation task based on contextual information in order to help the training model converge more quickly. Therefore, CenterPNets in this paper can perform the traffic perception task more easily.In general,CenterPNets can perform the detection task well in the vast majority of scenarios. However, there are still some lane prediction interruptions and missed detections at complex intersections.\nIn this paper, the effectiveness of multi-task network detection is systematically described, and a perceptual structure called theCenterPNets shared codec is proposed to integrate multi-scale feature information through a path aggregation network, which is used for direct regression to target key points.In the semantic segmentation task, the detailed information of the image is enhanced by fusing the multi-level features of the path aggregation network with shallow fine-grained information and building an effective training loss function to improve accuracy and performance.CenterPNets achieved an average detection accuracy of 75.8% on the publicly available large-scale Berkeley DeepDrive dataset, with an average intersection ratio of 92.8% in the driveable area and 32.1% in the lane area, respectively. Compared to the baseline algorithm, CenterPNets showed a 2.3% and 0.5% improvement in the cross-merge ratio for the roadway driveable area and lane line segmentation tasks, respectively. More importantly, CenterPNets achieved more accurate traffic segmentation tasks with relatively fast inference compared to other multi-task detection networks.\nAuthor Contributions:Conceptualization, T.W. and G.C.; methodology, T.W., J.D., G.C., Q.H., and D.H.; software, T.W. and H.L.; validation, T.W.; formal analysis, T.W.; investigation, T.W. and J.D.; resources, G.C. and J.D.; data curation, T.W. and H.L.; writing\u2014original draft preparation, G.C., T.W., and J.D.; writing\u2014review and editing, T.W.; supervision, T.W.; funding acquisition, J.D. and G.C. All authors have read and agreed to the published version of the manuscript.\nFunding: This research was funded by National Major Research Instrument Development Project and Jilin Provincial Science and Technology Department Development Project, grant numbers 62127813 and 20210203181SF.\nInstitutional Review Board Statement: Not applicable.\nIn this , t ff i l i-t s et ork detection is systematically describe , t l t t ll t terP ets shared codec is proposed to integrat l i l i ti t rough a ath aggregation network, which is used for ir t i i ts.In the se antic segmentation task, the detailed infor ti t i i f si g the ulti-level features of the path aggregati i ll fi i e i for ation and building an e fective traini loss f ti t i ce. enterPNets achieved an avera e detection accuracy of 75.8% on the publicly avail ble large-scale Berkel y DeepDrive dataset, with an average intersection ratio of 92.8% in the driveable area and 32.1% in the la e are , resp ctively. Compared to th baseline lgorithm, CenterPNets showed a 2.3% and 0.5% improve ent in the cross-merge ratio for the roadw y driveabl ar a and lane line segm ntation tasks, respectively. More imp tantly, CenterPNets achieved more accurate tr ffic segmentation tasks with relatively fast inference compared to ther multi-task detection networks.\nAuthor Contributio s: ceptualization, T.W. and G.C.; methodology, T.W., J.D., G.C., Q.H. and D. .; soft are, . . an . .; vali ation, T. .; for al analysis, T.W.; investigation, T.W. and J.D.; resources, G.C. and J.D.; data curation, T.W. and H.L.; writing\u2014original draft preparation, G.C., T.W., and J.D.; writing\u2014review and editing, T.W.; supervision, T.W.; funding acquisition, J.D. and G.C. All authors have read and agreed to the published version of the manuscript.\nFunding: This research was funded by National Major Research Instrument Development Project and Jilin Provincial Science and Technology Department Development Project, grant numbers 62127813 and 20210203181SF.\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nData Availability Statement: Not applicable.\nAcknowledgments: The authors are grateful to the reviewers for enhancing the clarity and completeness of this article.\nConflicts of Interest: The authors declare no conflict of interest.\nSensors 2023, 23, 2467 14 of 15\nReferences 1. Liu, W.; Anguelov, D.; Erhan, D.; Szegedy, C.; Reed, S.; Fu, C.-Y. SSD: Single Shot MultiBox Detector. InComputer Vision\u2014ECCV\n2016; Leibe, B., Matas, J., Sebe, N., Welling, M., Eds.; Springer International Publishing: Cham, Switzerland, 2016; pp. 21\u201337. [CrossRef]\n2. Redmon, J.; Divvala, S.; Girshick, R.; Farhadi, A. You Only Look Once: Unified, Real-Time Object Detection. In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition (CVPR), Las Vegas, NV, USA, 27\u201330 June 2016; pp. 779\u2013788. [CrossRef] 3. Shelhamer, E.; Long, J.; Darrell, T. Fully Convolutional Networks for Semantic Segmentation. IEEE Trans. Pattern Anal. Mach. Intell. 2017, 39, 640\u2013651. [CrossRef] [PubMed] 4. Zhao, H.; Shi, J.; Qi, X.; Wang, X.; Jia, J. Pyramid scene parsing network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 21\u201326 July 2017. 5. Paszke, A.; Chaurasia, A.; Kim, S.; Culurciello, E. Enet: A deep neural network architec-ture for real-time semantic segmentation. arXiv 2016, arXiv:1606.02147. 6. Ouyang, Y. Strong-Structural Convolution Neural Network for Semantic Segmentation. Pattern Recognit. Image Anal. 2019, 29, 716\u2013729. [CrossRef] 7. Wang, Z.; Ren, W.; Qiu, Q. LaneNet: Real-Time Lane Detection Networks for AutonomousDriving. arXiv 2018, arXiv:1807.01726. 8. Pan, X.; Shi, J.; Luo, P.; Wang, X.; Tang, X. Spatial As Deep: Spatial CNN for Traffic SceneUndrstanding. arXiv 2017, arXiv:1712.06080. 9. Hou, Y.; Ma, Z.; Liu, C.; Loy, C.C. Learning Lightweight Lane Detection CNNs by Self Attention Distillation. In Proceedings of\nthe IEEE/CVFInternationalConference onComputer Vision (ICCV), Seoul, Republic of Korea, 27 October\u20132 November 2019; pp. 1013\u20131021. [CrossRef]\n10. He, K.; Gkioxari, G.; Doll\u00e1r, P.; Girshick, R. Mask R-CNN. IEEE Trans. PatternAnal. Mach. Intell. 2020, 42, 386\u2013397. [CrossRef] [PubMed] 11. Ren, S.; He, K.; Girshick, R.; Sun, J. Faster r-cnn: Towards real-time object detection with regionproposalnetworks. arXiv 2015, arXiv:1506.01497. 12. Liu, B.; Chen, H.; Wang, Z. LSNet: Extremely Light-Weight Siamese Network For ChangeDetection in Remote Sensing Image. arXiv 2022, arXiv:2201.09156. 13. Teichmann, M.; Weber, M.; Z\u00f6llner, M.; Cipolla, R.; Urtasun, R. MultiNet: Real-time JointSemantic Reasoning for Autonomous Driving. In Proceedings of the IEEE IntelligentVehicles Symposium (IV), Changshu, China, 26\u201330 June 2018; pp. 1013\u20131020. [CrossRef] 14. Yu, F.; Chen, H.; Wang, X.; Xian, W.; Chen, Y.; Liu, F.; Madhavan, V.; Darrell, T. BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), Seattle, WA, USA, 13\u201319 June 2020; pp. 2633\u20132642. [CrossRef] 15. Wu, D.; Liao, M.; Zhang, W.; Wang, X. YOLOP: You Only Look Once for Panoptic DrivingPerception. arXiv 2022, arXiv:2108.11250. [CrossRef] 16. Vu, D.; Ngo, B.; Phan, H. HybridNets: End-to-End Perception Network. arXiv 2022, arXiv:2203.09035. 17. Wang, C.-Y.; Bochkovskiy, A.; Liao, H.Y.M. Scaled-yolov4: Scaling cross stage partialnetwork. arXiv 2020, arXiv:2011.08036. 18. Liu, S.; Qi, L.; Qin, H.; Shi, J.; Jia, J. Path aggregation network for instance segmentation. In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, Salt Lake City, UT, USA, 18\u201322 June 2018; pp. 8759\u20138768. 19. He, K.; Zhang, X.; Ren, S.; Sun, J. Deep Residual Learning for Image Recognition. In 2016 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR);IEEE: 2016. In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 27\u201330 June 2016.\n20. He, K.; Zhang, X.; Ren, S.; Sun, J. Spatial pyramid pooling in deep convolutional networks forvisual recognition. IEEE Trans. Pattern Anal. Mach. Intell. 2015, 37, 1904\u20131916. [CrossRef] [PubMed] 21. Lin, T.-Y.; Doll\u00e1r, P.; Girshick, R.; He, K.; Belongie, S. Feature pyramidnetworks for object detection. In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21\u201326 July 2017; pp. 2117\u20132125. 22. Zhou, X.; Wang, D.; Kr\u00e4henb\u00fchl, P. Objects as points. arXiv 2019, arXiv:190407850. 23. Law, H.; Deng, J. Cornernet: Detecting objects as paired keypoints. In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany, 8\u201314 September 2018; pp. 734\u2013750. 24. Cao, Z.; Simon, T.; Wei, S.-E.; Sheikh, Y. Realtime multi-person 2d poseestimation using part affinity fields. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21\u201326 July 2017; pp. 7291\u20137299. 25. Lin, T.-Y.; Goyal, P.; Girshick, R.; He, K.; Doll\u00e1r, P. Focal loss for denseobject detection. In Proceedings of the IEEE international Conference onComputer Vision, Venice, Italy, 22\u201329 October 2017; pp. 2980\u20132988. 26. Salehi, S.S.M.; Erdogmus, D.; Gholipour, A. Tversky Loss Function for Image SegmentationUsing 3D Fully Convolutional Deep\nNetworks. In Machine Learning in Medical Imaging, MLMI 2017; Lecture Notes in Computer, Science; Wang, Q., Shi, Y., Suk, H.I., Suzuki, K., Eds.; Springer: Cham, Switzerland, 2017; Volume 10541.\nSensors 2023, 23, 2467 15 of 15\n27. Lin, T.-Y.; Goyal, P.; Girshick, R.; He, K.; Doll\u00e1r, P. Focal Loss for Dense ObjectDetection. IEEE Trans. Pattern Anal. Mach. Intell. 2020, 42, 318\u2013327. [CrossRef] [PubMed] 28. Loshchilov, I.; Hutter, F. Decoupled Weight Decay Regularization. arXiv 2019, arXiv:1711.05101.\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content."
        }
    ],
    "title": "CenterPNets: A Multi-Task Shared Network forTraffic Perception",
    "year": 2023
}