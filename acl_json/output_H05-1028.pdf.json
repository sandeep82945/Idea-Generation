{
    "abstractText": "To improve the robustness in multimodal input interpretation, this paper presents a new salience driven approach. This approach is based on the observation that, during multimodal conversation, information from deictic gestures (e.g., point or circle) on a graphical display can signal a part of the physical world (i.e., representation of the domain and task) of the application which is salient during the communication. This salient part of the physical world will prime what users tend to communicate in speech and in turn can be used to constrain hypotheses for spoken language understanding, thus improving overall input interpretation. Our experimental results have indicated the potential of this approach in reducing word error rate and improving concept identification in multimodal conversation.",
    "authors": [
        {
            "affiliations": [],
            "name": "Joyce Y. Chai"
        },
        {
            "affiliations": [],
            "name": "Shaolin Qu"
        }
    ],
    "id": "SP:c52ce22207b5df1ccd12a079eaa1279008348578",
    "references": [
        {
            "authors": [
                "S. Bangalore",
                "M. Johnston"
            ],
            "title": "Integrating Multimodal Language Processing with Speech Recognition",
            "venue": "Proceedings of ICSLP.",
            "year": 2000
        },
        {
            "authors": [
                "P. Brown",
                "V.J. Della Pietra",
                "P.V. deSouza",
                "Lai",
                "J. C",
                "R.L. Mercer"
            ],
            "title": "Class-based n-gram models of natural language",
            "year": 1992
        },
        {
            "authors": [
                "D. Byron",
                "T. Mampilly",
                "V. Sharma",
                "T. Xu"
            ],
            "title": "Utilizing Visual Attention for Cross-Modal Coreference Interpretation",
            "venue": "Spring Lecture Notes in Computer Science: Proceedings of Context-05, page 83-96.",
            "year": 2005
        },
        {
            "authors": [
                "J. Cassell",
                "M. Stone",
                "B. Douville",
                "S. Prevost",
                "B. Achorn",
                "M. Steedman",
                "N. Badler",
                "C. Pelachaud"
            ],
            "title": "Modeling the interaction between speech and gesture",
            "venue": "Cognitive Science Society.",
            "year": 1994
        },
        {
            "authors": [
                "J.Y. Chai",
                "Z. Prasov",
                "J. Blaim",
                "R. Jin"
            ],
            "title": "Linguistic Theories in Efficient Multimodal Reference Resolution: an Empirical Investigation",
            "venue": "The 10th International Conference on Intelligent User Interfaces (IUI-05), pp. 43-50, San Diego, CA.",
            "year": 2005
        },
        {
            "authors": [
                "J.Y. Chai",
                "P. Hong",
                "Zhou",
                "M. X",
                "Z. Prasov"
            ],
            "title": "Optimization in Multimodal Interpretation",
            "venue": "Proceedings of ACL, pp. 1-8, Barcelona, Spain.",
            "year": 2004
        },
        {
            "authors": [
                "J.Y. Chai",
                "P. Hong",
                "M. Zhou"
            ],
            "title": "A Probabilistic Approach to Reference Resolution in Multimodal User Interfaces",
            "venue": "Proceedings of 9th International Conference on Intelligent User Interfaces (IUI-04), pp. 70-77, Madeira, Portugal.",
            "year": 2004
        },
        {
            "authors": [
                "C. Chelba",
                "F. Jelinek"
            ],
            "title": "Structured language modeling",
            "venue": "Computer Speech and Language, 14(4):283\u2013332.",
            "year": 2000
        },
        {
            "authors": [
                "Cohen, P., Johnston, M., McGee, D., Oviatt, S., Pittman, J.",
                "Smith, I., Chen, L.,",
                "J. Clow"
            ],
            "title": "Quickset: Multimodal Interaction for Distributed Applications",
            "venue": "Proceedings of ACM Multimedia, 31\u2013 40.",
            "year": 1996
        },
        {
            "authors": [
                "J. Eisenstein",
                "C. Christoudias."
            ],
            "title": "A salience-based approach to gesture-speech alignment",
            "venue": "Proceedings of HLT/NAACL\u201904.",
            "year": 2004
        },
        {
            "authors": [
                "D. Gildea",
                "T. Hofmann"
            ],
            "title": "Topic-based language models using EM",
            "venue": "Proceedings of Eurospeech.",
            "year": 1999
        },
        {
            "authors": [
                "Z.M. Griffin"
            ],
            "title": "Gaze durations during speech reflect word selection and phonological encoding",
            "venue": "Cognition 82, B1-B14.",
            "year": 2001
        },
        {
            "authors": [
                "B.J. Grosz",
                "A.K. Joshi",
                "S. Weinstein"
            ],
            "title": "Centering: A framework for modeling the local coherence of discourse",
            "venue": "Computational Linguistics, 21(2).",
            "year": 1995
        },
        {
            "authors": [
                "J.K. Gundel",
                "N. Hedberg",
                "R. Zacharski"
            ],
            "title": "Cognitive Status and the Form of Referring Expressions in Discourse",
            "venue": "Language 69(2):274-307.",
            "year": 1993
        },
        {
            "authors": [
                "Harper",
                "M..",
                "C. White",
                "W. Wang",
                "M. Johnson",
                "R. Helzerman"
            ],
            "title": "The Effectiveness of Corpus-Induced Dependency Grammars for Post-processing Speech",
            "venue": "Proceedings of the",
            "year": 2000
        },
        {
            "authors": [
                "P. Heeman."
            ],
            "title": "POS tags and decision trees for language modeling",
            "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Process (EMNLP).",
            "year": 1999
        },
        {
            "authors": [
                "C. Huls",
                "E. Bos",
                "W. Classen"
            ],
            "title": "Automatic Referent Resolution of Deictic and Anaphoric Expressions",
            "venue": "Computational Linguistics, 21(1):59-79.",
            "year": 1995
        },
        {
            "authors": [
                "F. Jelinek"
            ],
            "title": "Self-organized language modeling for speech recognition",
            "venue": "Waibel, A. and Lee, K. F. (Eds). Readings in Speech Recognition, pp. 450-506.",
            "year": 1990
        },
        {
            "authors": [
                "M. Johnston"
            ],
            "title": "Unification-based Multimodal parsing, Proceedings of COLING-ACL",
            "year": 1998
        },
        {
            "authors": [
                "M. Johnston",
                "S. Bangalore",
                "Visireddy G.",
                "A. Stent",
                "P. Ehlen",
                "M. Walker",
                "S. Whittaker",
                "P. Maloor"
            ],
            "title": "MATCH: An Architecture for Multimodal Dialog Systems, in Proceedings of the 40 ACL, Philadelphia, pp",
            "venue": "376-383.",
            "year": 2002
        },
        {
            "authors": [
                "S.M. Katz"
            ],
            "title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer",
            "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, 35(3).",
            "year": 1987
        },
        {
            "authors": [
                "A. Kehler"
            ],
            "title": "Cognitive Status and Form of Reference in Multimodal Human-Computer Interaction, Proceedings of AAAI\u201901",
            "year": 2000
        },
        {
            "authors": [
                "R. Kneser",
                "H. Ney"
            ],
            "title": "Improved clustering techniques for class-based statistical language modeling",
            "venue": "Eurospeech\u201993, pp. 973-976.",
            "year": 1993
        },
        {
            "authors": [
                "F. Landragin",
                "N. Bellalem",
                "L. Romary"
            ],
            "title": "Visual Salience and Perceptual Grouping in Multimodal Interactivity",
            "venue": "In: First International Workshop on Information Presentation and Natural Multimodal Dialogue, Verona, Italy, pp. 151-155.",
            "year": 2001
        },
        {
            "authors": [
                "S. Lappin",
                "H. Leass"
            ],
            "title": "An algorithm for pronominal anaphora resolution",
            "venue": "Computational Linguistics, 20(4).",
            "year": 1994
        },
        {
            "authors": [
                "S. Oviatt"
            ],
            "title": "Mutual Disambiguation of Recognition Errors in a Multimodal Architecture",
            "venue": "Proceedings of CHI.",
            "year": 1999
        },
        {
            "authors": [
                "R. Pieraccini",
                "K. Dayandhi",
                "J. Bloom",
                "Dahan",
                "J.-G",
                "M. Phillips",
                "B.R. Goodman",
                "K.V. Prasad"
            ],
            "title": "Multimodal Conversational Systems for Automobiles",
            "venue": "Communications of the ACM,",
            "year": 2004
        },
        {
            "authors": [
                "D. Roy",
                "N. Mukherjee"
            ],
            "title": "Towards Situated Speech Understanding: Visual Context Priming of Language Models",
            "venue": "Computer Speech and Language, 19(2): 227-248.",
            "year": 2005
        },
        {
            "authors": [
                "C. Samuelsson",
                "W. Reichl"
            ],
            "title": "A class-based Language Model for Large Vocabulary Speech Recognition Extracted from Part-of-Speech Statistics",
            "venue": "IEEE ICASSP\u201999.",
            "year": 1999
        },
        {
            "authors": [
                "W. Schuler"
            ],
            "title": "Using model-theoretic semantic interpretation to guide statistical parsing and word recognition in a spoken language interface",
            "venue": "Proceedings of ACL, Sapporo, Japan.",
            "year": 2003
        },
        {
            "authors": [
                "C. Wai",
                "R. Pierraccinni",
                "H. Meng"
            ],
            "title": "A Dynamic Semantic Model for Rescoring Recognition Hypothesis",
            "venue": "Proceedings of the ICASSP.",
            "year": 2001
        },
        {
            "authors": [
                "W. Wang",
                "Harper. M."
            ],
            "title": "The superARV language model: In Investigating the effectiveness of tightly integrating multiple knowledge sources",
            "venue": "Proceedings of EMNLP, 238\u2013 247.",
            "year": 2002
        },
        {
            "authors": [
                "V. Zue",
                "J. Glass",
                "D. Goodine",
                "H. Leung",
                "M. Phillips",
                "J. Polifroni",
                "S. Seneff"
            ],
            "title": "Integration of Speech Recognition and Natural Language Processing in the MIT Voyager System",
            "venue": "Proceedings of the ICASSP.",
            "year": 1991
        }
    ],
    "sections": [
        {
            "text": "Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 217\u2013224, Vancouver, October 2005. c\u00a92005 Association for Computational Linguistics\nTo improve the robustness in multimodal input interpretation, this paper presents a new salience driven approach. This approach is based on the observation that, during multimodal conversation, information from deictic gestures (e.g., point or circle) on a graphical display can signal a part of the physical world (i.e., representation of the domain and task) of the application which is salient during the communication. This salient part of the physical world will prime what users tend to communicate in speech and in turn can be used to constrain hypotheses for spoken language understanding, thus improving overall input interpretation. Our experimental results have indicated the potential of this approach in reducing word error rate and improving concept identification in multimodal conversation."
        },
        {
            "heading": "1 Introduction",
            "text": "Multimodal conversational systems promote more natural and effective human machine communication by allowing users to interact with systems through multiple modalities such as speech and gesture (Cohen et al., 1996; Johnston et al., 2002; Pieraccini et al., 2004). Despite recent advances, interpreting what users communicate to the system is still a significant challenge due to insufficient recognition (e.g., speech recognition) and understanding (e.g., language understanding) performance. Significant improvement in the robustness of multimodal interpretation is crucial if multimodal systems are to be effective and practical for real world applications.\nPrevious studies have shown that, in multimodal conversation, multiple modalities tend to complement each other (Cassell et al. 1994). Fusing two or more modalities can be an effective means of reducing recognition uncertainties, for example, through mutual disambiguation (Oviatt 1999). For semantically-rich modalities such as speech and penbased gesture, mutual disambiguation usually happens at the fusion stage where partial semantic representations from individual modalities are disambiguated and combined into an overall interpretation (Johnston 1998, Chai et al., 2004a). One problem is that some critical but low probability information from individual modalities (e.g., recognized alternatives with low probabilities) may never reach the fusion stage. Therefore, this paper addresses how to use information from one modality (e.g., deictic gesture) to directly influence the semantic processing of another modality (e.g., spoken language understanding) even before the fusion stage.\nIn particular we present a new salience driven approach that uses gesture to influence spoken language understanding. This approach is based on the observation that, during multimodal conversation, information from deictic gestures (e.g., point or circle) on a graphical interface can signal a part of the physical world (i.e., representation of the domain and task) of the application which is salient during the communication. This salient part of the physical world will prime what users tend to communicate in speech and thus in turn can be used to constrain hypotheses for spoken language understanding. In particular, this approach incorporates a notion of salience from deictic gestures into language models for spoken language processing. Our experimental results indicate the potential of this approach in reducing word error rate and improving concept identification from spoken utterances.\n217\nIn the following sections, we first introduce the current architecture for multimodal interpretation. Then we describe our salience driven approach and present empirical results.\n2\n3\nInput Interpretation\nInput interpretation is the identification of semantic meanings in user inputs. In multimodal conversation, user inputs can come from multiple channels (e.g., speech and gesture). Thus, most work on input interpretation is based on semantic fusion that includes individual recognizers and a sequential integration processes as shown in Figure 1. In this approach, a system first creates possible partial meaning representations from recognized hypotheses (e.g., N-best lists) independently of other modalities. For example, suppose a user says \u201cwhat is the price of this painting\u201d and at the same time points to a position on the screen. The partial meaning representations from the speech input and the gesture input are shown in (a-b) in Figure 1. The system uses the partial meaning representations to disambiguate each other and combines compatible partial representations together into an overall semantic representation as in Figure1(c).\nIn this architecture, the partial semantic representations from individual modalities are crucial for mutual disambiguation during multimodal fusion. The quality of partial semantic representations depends on how individual modalities are processed. For example, if the speech input is recognized as \u201cwhat is the prize of this pant\u201d, then the partial representation from the speech input will not be created in the first place. Without a candidate partial representation, it is not likely for multimodal fusion to reach an overall meaning of the input given this late fusion architecture.\nThus, a problem with the semantics-based fusion approach is that information from multiple modalities is only used during the fusion stage to disambiguate or combine partial semantic representations. This late use of information from other sources in the pipelined process can cause the loss of some low probability information (e.g., recognized alternatives with low probabilities which did not make it to the Nbest list) which could be very crucial in terms of the overall interpretation. It is desirable to use information from multiple sources at an earlier stage before partial representations are created from individual modalities. For example, in ((Bangalore and Johnston 2000), a finite-state approach was applied to tightly couple multimodal language processing (e.g., gesture and speech) and speech recognition to improve recognition hypotheses. To further address this issue, in this paper, we present a salience driven approach that particularly applies gesture information (e.g., pen-based deictic gestures) to robust spoken language understanding before multimodal fusion."
        },
        {
            "heading": "Related Work on Salience Modeling",
            "text": "We first give a brief overview on the notion of salience and how salience modeling is applied in earlier work on natural language and multimodal language processing.\nLinguistic salience describes the accessibility of entities in a speaker/hearer\u2019s memory and its implication in language production and interpretation. Many theories on linguistic salience have been developed, including how the salience of entities affects the form of referring expressions as in the Givenness Hierarchy (Gundel et al., 1993) and the local coherence of discourse as in the Centering Theory (Grosz et al., 1995). Salience modeling is used for both language generation and language interpretation; the latter is more relevant to our work. Most salience-based interpretation has focused on reference resolution for both linguistic referring expressions (e.g., pronouns) (Lappin and Leass 1995) and multimodal expressions (Hul et al. 1995; Eisenstein and Christoudias 2004). Speech Input Gesture Input Speech Recognition Language Understanding Gesture Recognizer\nMultimodal Fusion\nSemantic Representation\nGesture Understanding\nSemantic Representation Semantic Representation\nWhat is the price of this painting Point to a position on the screen\nIntent: Ask Type: Painting Aspect: Price\nType: Painting Id: P23\nIntent: Ask Type: Painting Aspect: Price Id: P23\nType: Wall Id: W1\n(a) (b)\n(c)\nFigure 1: Semantics-based multimodal interpretation\nVisual salience considers an object salient when it attracts a user\u2019s visual attention more than others. The cause of such attention depends on many factors including user intention, familiarity, and physical characteristics of objects. For example, an object may be salient when it has some properties the others do not have, such as it is the only one that is highlighted, or the only one of a certain size, category, or color\n(Landragin et al., 2001). Visual salience can also be useful in input interpretation, for example, for multimodal reference resolution (Kehler 2000) and cross-modal coreference interpretation (Byron et al., 2005).\nWe believe that salience modeling should go beyond reference resolution. Our view is that the salience not only affects the use of referring expressions (and thus is useful for interpreting referring expressions), but also influences the linguistic context of the referring expressions. The spoken utterances that contain these expressions tend to describe information relating to the salient objects (e.g., properties or actions). Therefore, our goal in this paper is to take salience modeling a step further from reference resolution, towards overall language understanding.\n4\n4.1\nA Salience Driven Approach\nThe new salience driven approach is based on the cognitive theory of Conversation Implicature (Grice 1975) and earlier empirical findings of user speech and gesture behavior in multimodal conversation (Oviatt 1999). The theory of Conversation Implicature (Grice 1975) states that speakers tend to make their contribution as informative as is required (for the current purpose of communication) and not make their contribution more informative than is required. In the context of multimodal conversation that involves speech and pen-based gesture, this theory indicates that users most likely will not make any unnecessary deictic gestures unless those gestures help in communicating users\u2019 intention. This is especially true since gestures usually take an extra effort from a user. When a pen-based gesture is intentionally delivered by a user, the information conveyed is often a crucial component in interpretation (Chai et al., 2005).\nSpeech and gesture also tend to complement each other. For example, when a speech utterance is accompanied by a deictic gesture (e.g., point or circle) on a graphical display, the speech input tends to issue commands or inquiries about properties of objects, and the deictic gestures tend to indicate the objects of interest. In addition, as shown in (Oviatt 1999), the deictic gestures often occur before spoken utterances. Our previous work (Chai et al., 2004b) also showed that 85% of time gestures occurred before corresponding speech units. Therefore, gestures can be used as an earlier indicator to anticipate the content of communication in the subsequent spoken utterances.\nOverview The general idea of the salience based approach is shown in Figure 2. For each application domain, there is a physical world representation that captures domain knowledge (details are described later). A deictic gesture can activate several objects on the graphical display. This activation will signal a distribution of objects that are salient. The salient objects are mapped to the physical world representation to indicate a salient part of representation that includes relevant properties or tasks related to the salient objects. This salient part of the physical world is likely to be the potential content of the spoken communication, and thus can be used to tailor language models for spoken language understanding. This process is shown in the middle shaded box of Figure 2. It bridges gesture understanding and language understanding at a stage before multimodal fusion. Note that the use of gesture information can be applied at different stages: during speech recognition to generate hypotheses or post processing of recognized hypotheses before language understanding. In this paper, we focus on the latter.\nThe physical world representation includes the following components: \u2022 Domain Model. This component captures the relevant knowledge about the domain including domain objects, properties of the objects, relations between objects, and task models related to objects. Previous studies have shown that domain knowledge\ncan be used to improve spoken language understanding (Wai et al, 2001). Currently, we apply a frame-based representation where a frame represents an object (or a type of object) in the domain and frame elements represent attributes and tasks related to the objects. Each frame element is associated with a semantic tag which indicates the semantic content of that element. In the future, the domain model might also include knowledge about the interface, for example, visual properties and spatial relations between objects on the interface.\n\u2022 Domain Grammar. This component specifies grammar and vocabularies used to process language inputs. There are two types of representation. The first type is a semantics-based context free grammar where each non-terminal symbol represents a semantic tag (indicating semantic information such as the semantic type of an object, etc). Each word (i.e., the terminal symbol) in the lexicon relates to one or more semantic tags. Some of these semantic tags are directly linked to the frame elements in the domain model since they represent certain properties or tasks. This grammar was manually developed. 4.2\nThe second type of representation is based on annotated user spoken utterances. The data are annotated in terms of relevant semantic information (i.e., using semantic tags) in the utterance and the intended objects of interest (which are directly linked to the domain model). Based on the annotated data, N-grams can be learned to represent the dependency of language in our domain.\nBased on the physical world representation, our approach supports the following operations: Salience modeling. This operation calculates a salience distribution of entities in the physical world. In our current investigation, we limit the scope of entities to a closed set of objects from our physical world representation since the system has knowledge about those objects. These entities could have different salience values depending on whether they are visible on the graphical display, gestured by a user, or mentioned in the prior conversation. In this paper, we focus on the salience modeling using gesture information only. Salience driven language understanding. This operation maps the salience distribution to the physical world representation and uses the salient world to influence spoken language understanding. Note that, in this paper, we are not concerned with acoustic models for speech recognition, but rather we are interested in the use of the salience distribution to prime language models and facilitate language understanding.\nSalience Modeling\nWe use a vector er to represent entities in the physical world representation. For each entity e ek r \u2208 , we use to represent its salience value at time tn. For all the entities, we use P )( kt eP n )(e\nnt v to represent a salience\ndistribution at time tn. Figure 3 shows a sequence of words with corresponding gestures that occur at t1, t2, and t3. As shown in Figure 3, the salience distribution at any given time tn is influenced by a joint effect from this sequence of gestures that happen before tn etc. Depending on its time of occurrence, each gesture may have a different impact on the salience distribution at time tn. Note that although each gesture may have a short duration, here we only consider the beginning time of a gesture. Therefore, for an entity ek, its salience value at time tn is computed as follows:\n1\n1\n( ) ( | ) ( )\n( ) ( |\nn i i\nn\nn i i\nm\nt t k t i\nt k m\nt t t e e i\n)\ng P e g P e\ng P e g\n\u03b1\n\u03b1\n=\n\u2208 =\n= \u2211\n\u2211\u2211 r\n(1)\nIn Equation (1), m (m \u2265 1) is the number of gestures that have occurred before tn. The different impact of a gesture g at time ti that contributes to the salience distribution at time tn is represented as the weight it\n)( in tt g\u03b1 in Equation (1). Currently, we\ncalculate the weight depending on the temporal distance as follows:\n)(] 2000\n)( exp[)( in in tt tt tt g in \u2265 \u2212\u2212 =\u03b1 (2)\nEquation (2) indicates that at a given time tn (measured in milliseconds), the closer a gesture (at ti) is to the time tn, the higher impact this gesture has on the salience distribution (Chai et al., 2004b).\nIt is worth mentioning that a deictic gesture on the graphic display (e.g., pointing and circling) could have ambiguous interpretation by itself. For example,\ngiven an interface, a point or a circle on the screen could result in selection of different entities with different probabilities. Therefore, in Equation (1),\nis the selection probability which indicates the likelihood of selecting an entity e given a gesture at time ti. This selection probability is calculated by a function of the distance between the location of the entity and the focus point of the recognized gesture on the display (Chai et al., 2004a). A normalization factor is incorporated to ensure that the summation of selection probabilities over all possible entities adds up to one. ( | ) it P e g\nWhen no gesture is involved in a given input, the salience distribution at any given time is a uniform distribution. If one or more gestures are involved, then Equation (1) is used to calculate the salience distribution.\n4.3\nP W\nSalience Driven Spoken Language Understanding\nThe salience distribution of entities identified based on the gesture information (as described above) is used to constrain hypotheses for language understanding. More specifically, for each onset of a spoken word at time t (i.e., the beginning time stamp of a spoken word), the salience distribution at t can be calculated based on a sequence of gestures that happen before t by Equation (1). This salience distribution can then be used to prime language models for spoken language processing."
        },
        {
            "heading": "Language Modeling",
            "text": "We first give a brief background of language modeling. Given an observed speech utterance O, the goal of speech recognition is to find a sequence of words W* so that W P , where P(O|W) is the acoustic model and P(W) is the language model. In traditional speech recognition systems, the acoustic model provides the probability of observing the acoustic features given hypothesized word sequences and the language model provides the probability of a sequence of words. The language model is computed as follows: * arg max ( | ) ( )O W=\n)|()...|()|()()( 112131211 \u2212= nn n wwPwwwPwwPwPwP Using the Markov assumption, the language model can be approximated by a bigram model as in:\n\u220f =\n\u2212= n\ni ii n wwPwP 1 11 )|()(\nTo improve the speech understanding results for spoken language interfaces, many systems have\napplied a loosely-integrated approach which decouples the language model from the acoustic model (Zue et al., 1991, Harper et al., 2000). This allows the development of powerful language models independent of the acoustic model, for example, utilizing topics of the utterances (Gildea and Hofmann 1999), syntactic or semantic labels (Heeman 1999), and linguistic structures (Chelba and Jelinek 2000, Wang and Harper 2002). Recently, we have seen work on language understanding based on environment (Schuler 2003) and language modeling using visual context (Roy and Mukherjee 2005). Our salience driven approach is inspired by this earlier work. Here, we do not address the acoustic model of speech recognition, but rather incorporate the salience distribution for language modeling. In particular, our focus is on investigating the effect of incorporating additional information from other modalities (e.g., gesture) with traditional language models."
        },
        {
            "heading": "Primed Language Model",
            "text": "The calculated salience distribution is used to prime the language model. More specifically, we use a class-based bigram model from (Brown et al, 1992):\n)|()|()|( 11 \u2212\u2212 = iiiiii ccPcwPwwP (3) In Equation (3), ci is the class of the word wi, which could be a syntactic class or a semantic class. is the class transition probability, which\nreflects the grammatical formation of utterances. is the word class probability which\nmeasures the probability of seeing a word wi given a class ci. The class-based N-gram model can make better use of limited training data by clustering words into classes. A number of researchers have shown that the class-based N-gram model can successfully improve the performance of speech recognition (Jelinek 1990, Heeman 1999, Kneser and Ney 1993, Samuelsson and Reichl, 1999).\n)|( 1\u2212ii ccP\n)|( ii cwP\nIn our approach, the \u201cclass\u201d used in the classbased bigram model comes from combined semantic and functional classes designed for our domain. For example, \u201cthis\u201d is tagged as Demonstrative, and \u201cprice\u201d is tagged as AttrPrice. As shown in Equation (3), there are two types of parameter estimation. In terms of the class transition probability, as in earlier work, we directly use the annotated data. In terms of the word class distribution, we incorporate the notion of salience. We use the salience distribution to dynamically adjust the world class probability\nas follows: )|( ii cwP\n)( )|(\n)|,( )|( kt\nee ki\nkii ii ePecP\necwP cwP\ni\nk\n\u2211 \u2208 = v\n(4) User index # of Inputs # inputs w/o gesture Baseline WER\n1 21 0 0.287 2 31 0 0.335 3 27 0 0.399 4 10 0 0.680 5 8 1 0.200 6 36 0 0.387 7 18 0 0.250 8 25 1 0.278 9 23 0 0.482 10 11 0 0.117 11 16 3 0.255\nTable 1: Related information about the evaluation data: user type, the number of turns per user, and the baseline word recognition rate.\nIn Equation (4), P is the salience value for an entity at time ti (the onset of the spoken word wi), which can be calculated by Equation (1). Equation (4) indicates that only information associated with the salient entities is used to estimate the word class distribution. In other words, the word class probability favors the salient physical world as indicated by the salience distribution )( kt ei ke\n)(eP it v . More\nspecifically, at time ti, given a semantic class ci, the choice of word \u201cwi\u201d is dependent on the salient physical world at the moment, which is represented as the salience distribution )(eP\nit v at time ti. For all wi,\nthe summation of this word class probability is equal to one. Furthermore, given an entity , and are not dependent on time ti, but rather on the domain and the use of language expressions. Therefore they can be estimated based on the training data that are annotated in terms of semantic information and the intended objects of interest (as discussed in Section 4.1). Since the annotated data is very limited, the sparse data can become a problem for the maximum likelihood estimation. Therefore, a smoothing technique based on the Katz backoff model (Katz, 1987) is applied. For example, to calculate , if a word wi has one or more occurrences in the training data associated with the class ci and the entity , then its count is discounted by a fraction in the maximum likelihood estimation. If wi does not occur, then this approach backs off to the domain grammar and redistributes the remaining probability mass uniformly among words in the lexicon that are linked with class ci and entity e . ke )| ki ec\nk\n,( iwP )| keP\n,( iwP\n( ic\n)| ke\nke\nic\n5 Evaluation\nWe evaluated the salience model during post processing recognized hypotheses. Given possible hypotheses from a speech recognizer, we use the salience-based language model to identify the most likely sequence of words. The salience distribution based on gesture was used to favor words that are consistent with the attention indicated by gestures.\nThe data collected from our previous user studies were used in our evaluation (Chai et al., 2004b). In these studies, users interacted with our multimodal interface using both speech and deictic gestures to find information about real estate properties. In particular, each user was asked to accomplish five\ntasks. Each of these tasks required the user to retrieve different types of information from our interface. For example, one task was to find the least expensive house in the most populated town. The data were recorded from eleven subjects including five nonnative speakers and six native speakers. Each user\u2019s voice was individually trained before the study. Table 1 shows the relevant information about the data such as the total number of inputs (or turns) from each subject, the number of speech alone inputs without any gesture, and the baseline recognition results without using salience-based post processing in terms of the word error rate (WER). In total, we have collected 226 user inputs with an average of eight words per spoken utterance1. As shown in Table 1, the majority of inputs consisted of both speech and gesture. Since currently we only use gesture\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n1 2 3 4 5 6 7 8 9 10 11 User index\nW or\nd E\nrr or\nR at\ne\nBaseline Salience driven model\nFigure 5: Comparison of the baseline and the result from post-processing in terms of WER\n1 The difference between the number of user inputs reported here and that in (Chai et al., 2004b) was caused by the situation where one intended user input (which was the unit for counting in our previous work) was split into a couple turns (which constitute the new counts here).\ninformation in salience modeling, our approach will not affect speech only inputs.\nTo train the salience-based model, we applied the leave-one-out approach. The data from each user was held out as the testing data and the remaining users were used as the training data to acquire relevant probability estimations in Equation (3) and (4).\nFigure 5 shows the comparison results between the baseline and the salience-based model in terms of word error rate (WER). The word error rate as a result of salience-based post processing is significantly better than that from the baseline recognizer (t = 4.75, p < 0.001). The average WER reduction is about 12%.\nWe further evaluated how the salience based model affects the final understanding results. This is because an improvement in WER may not directly lead to an improvement in understanding. We applied our semantic grammar on a sequence of words resulting from both the baseline and the saliencebased post-processing to identify key concepts. In total, there were 686 concepts from the transcribed speech utterances. Table 2 shows the evaluation results. Precision measures the percentage of correctly identified concepts out of the total number of concepts identified based on a sequence of words. Recall measures the percentage of correctly identified concepts out of the total number of intended concepts from user\u2019s utterance. F-measurement combines precision and recall together as follows:\nThe evaluation reported here is only an initial step based on a limited domain. The small scale in the number of objects and the vocabulary size can only demonstrate the potential of the salience-based approach to a limited degree. To further understand the advantages and issues of this approach, we are currently working on a more complex domain with richer concepts and relations, as well as larger vocabularies.\nIt is worth mentioning that the goal of this work is to explore whether salience modeling based on other modalities (e.g., gesture) can be used to prime traditional language models to facilitate spoken\nlanguage processing. The salience driven approach based on additional modalities can be combined with more sophisticated language modeling (e.g., better parameter estimation) in the future.\nExample 1: Transcription: What is the population of this town Baseline recognition: What is the publisher of this time Salience-based processing: what is the population of this town\nUser # Baseline Salience-based\nPrecision 80.3% 84.6%\nRecall 75.7% 83.8%\nF-measure 77.9% 84.2%\nTable2. Overall concept identification comparison between the baseline and the salience driven model."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "This paper presents a new salience driven approach to robust input interpretation in multimodal conversational systems. This approach takes advantage of rich information from multiple modalities. Information from deictic gestures is used to identify a part of the physical world that is salient at a given point of communication. This salient part of the physical world is then used to prime language models for spoken language understanding. Our experimental results have shown the potential of this approach in reducing word error rate and improving concept identification from spoken utterances in our application. Although currently we have only investigated the use of gesture information in salience modeling, the salience driven approach can be extended to include other modalities (e.g., eye gaze) and information (e.g., conversation context). Our future work will specifically investigate how to combine information from multiple sources in salience modeling and how to apply the salience models in different early stages of processing."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work was supported by a CAREER grant IIS-0347548 from the National Science Foundation. The authors would like to thank anonymous reviewers for their helpful comments and suggestions."
        }
    ],
    "title": "A Salience Driven Approach to Robust Input Interpretation in Multimodal Conversational Systems",
    "year": 2005
}