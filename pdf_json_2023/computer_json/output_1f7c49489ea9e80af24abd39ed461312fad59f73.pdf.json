{
    "abstractText": "Drones have become essential tools in a wide range of industries, including agriculture, surveying, and transportation. However, tracking unmanned aerial vehicles (UAVs) in challenging environments, such cluttered or GNSS-denied environments, remains a critical issue. Additionally, UAVs are being deployed as part of multi-robot systems, where tracking their position can be essential for relative state estimation. In this paper, we evaluate the performance of a multi-scan integration method for tracking UAVs in GNSS-denied environments using a solid-state LiDAR and a Kalman Filter (KF). We evaluate the algorithm\u2019s ability to track a UAV in a large open area at various distances and speeds. Our quantitative analysis shows that while \u201dtracking by detection\u201d using a Constant Velocity model is the only method that consistently tracks the target, integrating multiple scan frequencies using a KF achieves lower position errors and represents a viable option for tracking UAVs in similar scenarios.",
    "authors": [
        {
            "affiliations": [],
            "name": "Iacopo Catalano"
        },
        {
            "affiliations": [],
            "name": "Jorge Pe\u00f1a Queralta"
        },
        {
            "affiliations": [],
            "name": "Tomi Westerlund"
        }
    ],
    "id": "SP:98861542c17c3ea81adebca1f9d94390c08d901a",
    "references": [
        {
            "authors": [
                "Dimosthenis C Tsouros",
                "Stamatia Bibi",
                "Panagiotis G Sarigiannidis"
            ],
            "title": "A review on uav-based applications for precision agriculture",
            "year": 2019
        },
        {
            "authors": [
                "Dongliang Wang",
                "Quanqin Shao",
                "Huanyin Yue"
            ],
            "title": "Surveying wild animals from satellites, manned aircraft and unmanned aerial systems (uass): A review",
            "venue": "Remote Sensing,",
            "year": 2019
        },
        {
            "authors": [
                "Jorge Pe\u00f1a Queralta",
                "Jussi Taipalmaa",
                "Bilge Can Pullinen",
                "Victor Kathan Sarker",
                "Tuan Nguyen Gia",
                "Hannu Tenhunen",
                "Moncef Gabbouj",
                "Jenni Raitoharju",
                "Tomi Westerlund"
            ],
            "title": "Collaborative multi-robot search and rescue: Planning, coordination, perception, and active vision",
            "venue": "IEEE Access,",
            "year": 2020
        },
        {
            "authors": [
                "Lucas Prado Osco",
                "Jos\u00e9 Marcato Junior",
                "Ana Paula Marques Ramos",
                "L\u00facio Andr\u00e9 de Castro Jorge",
                "Sarah Narges Fatholahi",
                "Jonathan de Andrade Silva",
                "Edson Takashi Matsubara",
                "Hemerson Pistori",
                "Wesley Nunes Gon\u00e7alves",
                "Jonathan Li"
            ],
            "title": "A review on deep learning in uav remote sensing",
            "venue": "International Journal of Applied Earth Observation and Geoinformation,",
            "year": 2021
        },
        {
            "authors": [
                "Yiming Li",
                "Changhong Fu",
                "Fangqiang Ding",
                "Ziyuan Huang",
                "Geng Lu"
            ],
            "title": "Autotrack: Towards high-performance visual tracking for uav with automatic spatiotemporal regularization",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Nan Jiang",
                "Kuiran Wang",
                "Xiaoke Peng",
                "Xuehui Yu",
                "Qiang Wang",
                "Junliang Xing",
                "Guorong Li",
                "Jian Zhao",
                "Guodong Guo",
                "Zhenjun Han"
            ],
            "title": "Anti-uav: A large multimodal benchmark for uav tracking",
            "venue": "arXiv preprint arXiv:2101.08466,",
            "year": 2021
        },
        {
            "authors": [
                "Ha Sier",
                "Xianjia Yu",
                "Iacopo Catalano",
                "Jorge Pe\u00f1a Queralta",
                "Zhuo Zou",
                "Tomi Westerlund"
            ],
            "title": "UAV tracking with lidar as a camera sensors in GNSS-denied environments",
            "venue": "arXiv preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Jorge Pena Queralta",
                "Qingqing Li",
                "Fabrizio Schiano",
                "Tomi Westerlund"
            ],
            "title": "Viouwb-based collaborative localization and dense scene reconstruction within heterogeneous multi-robot systems",
            "venue": "In 2022 International Conference on Advanced Robotics and Mechatronics (ICARM),",
            "year": 2022
        },
        {
            "authors": [
                "Yang Bai",
                "Koki Asami",
                "Mikhail Svinin",
                "Evgeni Magid"
            ],
            "title": "Cooperative multirobot control for monitoring an expanding flood area",
            "venue": "In 2020 17th International Conference on Ubiquitous Robots (UR),",
            "year": 2020
        },
        {
            "authors": [
                "T. Rou\u010dek"
            ],
            "title": "Darpa subterranean challenge: Multi-robotic exploration of underground environments",
            "venue": "In International Conference on Modelling and Simulation for Autonomous Systesm,",
            "year": 2019
        },
        {
            "authors": [
                "M. Petr\u013a\u0131k"
            ],
            "title": "A robust uav system for operations in a constrained environment",
            "year": 2020
        },
        {
            "authors": [
                "Ismail Guvenc",
                "Farshad Koohifar",
                "Simran Singh",
                "Mihail L Sichitiu",
                "David Matolak"
            ],
            "title": "Detection, tracking, and interdiction for amateur drones",
            "venue": "IEEE Communications Magazine,",
            "year": 2018
        },
        {
            "authors": [
                "S.Hengy"
            ],
            "title": "Multimodal uav detection: study of various intrusion scenarios. In Electro-Optical Remote Sensing XI, volume 10434, page 104340P",
            "venue": "International Society for Optics and Photonics,",
            "year": 2017
        },
        {
            "authors": [
                "J. Pe\u00f1a Queralta"
            ],
            "title": "Autosos: Towards multi-uav systems supporting maritime search and rescue with lightweight ai and edge computing",
            "venue": "arXiv preprint arXiv:2005.03409,",
            "year": 2020
        },
        {
            "authors": [
                "Kailai Li",
                "Meng Li",
                "Uwe D Hanebeck"
            ],
            "title": "Towards high-performance solid-statelidar-inertial odometry and mapping",
            "venue": "arXiv preprint arXiv:2010.13150,",
            "year": 2020
        },
        {
            "authors": [
                "Li Qingqing",
                "Jussi Taipalmaa",
                "Jorge Pe\u00f1a Queralta",
                "Tuan Nguyen Gia",
                "Moncef Gabbouj",
                "Hannu Tenhunen",
                "Jenni Raitoharju",
                "Tomi Westerlund"
            ],
            "title": "Towards active vision with uavs in marine search and rescue: Analyzing human detection at variable altitudes",
            "venue": "IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR),",
            "year": 2020
        },
        {
            "authors": [
                "Dinh Van Nam",
                "Kim Gon-Woo"
            ],
            "title": "Solid-state lidar based-slam: A concise review and application",
            "venue": "IEEE International Conference on Big Data and Smart Computing (BigComp),",
            "year": 2021
        },
        {
            "authors": [
                "Li Qingqing",
                "Yu Xianjia",
                "Jorge Pena Queralta",
                "Tomi Westerlund"
            ],
            "title": "Multimodal lidar dataset for benchmarking general-purpose localization and mapping algorithms",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2022
        },
        {
            "authors": [
                "Li Qingqing",
                "Yu Xianjia",
                "Jorge Pe\u00f1a Queralta",
                "Tomi Westerlund"
            ],
            "title": "Adaptive lidar scan frame integration: Tracking known mavs in 3d point clouds",
            "venue": "arXiv preprint arXiv:2103.04069,",
            "year": 2021
        },
        {
            "authors": [
                "Sedat Dogru",
                "Lino Marques"
            ],
            "title": "Drone detection using sparse lidar measurements",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2022
        },
        {
            "authors": [
                "Jan Razlaw",
                "Jan Quenzel",
                "Sven Behnke"
            ],
            "title": "Detection and tracking of small objects in sparse 3d laser range data",
            "venue": "International Conference on Robotics and Automation (ICRA),",
            "year": 2019
        },
        {
            "authors": [
                "Marcus Hammer",
                "Marcus Hebel",
                "Bj\u00f6rn Borgmann",
                "Martin Laurenzis",
                "Michael Arens"
            ],
            "title": "Potential of lidar sensors for the detection of UAVs",
            "venue": "Laser Radar Technology and Applications XXIII,",
            "year": 2018
        },
        {
            "authors": [
                "Marcus Hammer",
                "Marcus Hebel",
                "Martin Laurenzis",
                "Michael Arens"
            ],
            "title": "Lidar-based detection and tracking of small UAVs",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Keywords: UAV \u00b7 Tracking \u00b7 LiDAR \u00b7 Multi-Scan Integration \u00b7"
        },
        {
            "heading": "1 Introduction",
            "text": "Unmanned Aerial Vehicles (UAVs) have become increasingly popular in various domains, owing to their mobility and versatility as mobile sensing platforms [1,2]. They are commonly used for aerial photography, mapping, surveying, delivery, search and rescue, among others [3, 4]. However, tracking UAVs in challenging environments, such as Global Navigation Satellite System (GNSS)-denied areas or cluttered environments, remains a difficult task [5\u20137]. GNSS signals may not be available in certain areas, such as indoor environments or urban canyons, which limits the accuracy and reliability of UAV tracking. Moreover, they can be susceptible to jamming or spoofing, which can lead to severe consequences such as loss of control or collisions with other objects.\nUAVs are also increasingly part of real-world multi-robot systems, where tracking between robots is often part of relative or global state estimation approaches [8, 9]. Indeed, the deployment of multi-robot systems in GNSS-denied environments has been highlighted as an important and recent development [3].\nar X\niv :2\n30 4.\n00 78\n0v 2\n[ cs\n.R O\n] 1\n0 M\nAn illustrative case in point is the DARPA Subterranean challenge, which has attracted considerable attention [10,11]. According to the reports from the participating teams, the successful localization and collaborative sensing were identified as some of the key challenges faced during the competition. Notably, UAVs were dynamically dispatched from unmanned ground vehicles (UGVs) during the challenge, which further underscores the complex nature of the deployment.\nIn recent years, researchers have shown a growing interest in tracking and detecting UAVs due to two main reasons. The first is the pressing need to identify and detect foreign objects or drones in areas with controlled airspace, including airports [12,13]. The second reason is the potential for optimizing the utilization of UAVs as flexible mobile sensing platforms [14].\nWhile significant progress has been made in UAV tracking using GNSS and other sensors, existing methods are not without limitations. Many methods may rely on expensive hardware or require high levels of computational power, which limits their practicality and scalability.\nSolid-state LiDARs are a promising technology in long-range scanning that produce high-density point clouds, making them ideal for tracking objects in three-dimensional space, such as UAVs [15, 16]. Their main limitation is the restricted field of view (FoV). This is illustrated in Fig. 1. However, solutions to this problem have been proposed, such as utilizing multiple LiDARs or adjusting the position and orientation of the robot base where the LiDAR is installed, in order to compensate for this limitation [17, 18]. The frame or scan frequency concept differs significantly in these LiDARs from the more standard spinning 3D LiDARs. In the latter, a frame can be naturally generated aggregating laser measurements from a single revolution. Solid-state LiDARs, with non-repetitive scan patterns, can output point clouds at adjustable frequencies with varying FoV coverage. This characteristic creates new opportunities for developing Li-\nDAR perception methods that allow for the adaptive adjustment of the frame integration time in order to improve object sensing capabilities.\nIn this article, we present an evaluation of the multi-scan integration method for tracking UAVs in challenging environments using a solid-state LiDAR (see Fig. 1). We build upon our previous work [19] where we presented a qualitative analysis without real tracking, by proposing both a linear and an Extended Kalman Filter for tracking UAVs with a Constant Velocity motion model. Unlike the previous work, where we used \u201dtracking by detection\u201d to estimate the target\u2019s future position, we provide a more sophisticated approach that enables real tracking of UAVs. Additionally, in this work we are able to quantify the tracking performance with ground truth data generated by a motion capture (MOCAP) system during the experiments.\nTo evaluate our method, we integrate different numbers of scans, ranging from 2 scans to 50 scans, to track the UAV\u2019s position accurately. By setting the sensor scan rate to its maximum, 100 Hz, we are able to effectively scan the environment at frequencies ranging from 50 Hz (2 scans) to 2 Hz (50 scans), simply by accumulating scans before processing the data. Our results showcase the limitations of the multi-scan integration method in dynamic environments and multi-UAV systems, and motivate further research to address the limitations and enhance the method\u2019s performance.\nIn the following sections, we will begin by providing a brief review of related work on UAV tracking and LiDAR-based sensing. Then, we will delve into the proposed method in detail and present experimental results that demonstrate the accuracy and robustness of the method in tracking UAVs. Finally, we will conclude by highlighting potential avenues for future research."
        },
        {
            "heading": "2 Related work",
            "text": "LiDAR systems are often employed for detecting and tracking objects, including UAVs. However, tracking UAVs with LiDAR can be challenging due to their small size, varied shapes and materials, high speed, and unpredictable movements.\nIn order to overcome these challenges, researchers have explored various methods to overcome the limitations of 3D LiDAR technology and improve the detection and tracking of UAVs. One approach involves conducting a probabilistic analysis of detections, as described in [20], which allows for achieving detection using fewer LiDAR beams while continuously tracking only a small number of hits. Moreover, increasing the field of view and improving the coverage ratio have been identified as effective means. A different strategy involves combining a segmentation approach and a simple object model while leveraging temporal information, as demonstrated in [21]. This approach has been shown to reduce the parametrization effort and generalize well in different settings. Overall, the use of LiDAR technology offers various methods for improving the detection and tracking of UAVs, and researchers are continually exploring new\ntechniques to overcome the unique challenges posed by these small, fast-moving, and unpredictable objects.\nIn the context of deploying UAVs from a ground robot, one critical factor to consider is the relative localization between different devices. In order to address this, Li et al. [19] proposed a novel approach for tracking UAVs using LiDAR point clouds. This approach takes into account the UAV\u2019s speed and distance from the sensor to dynamically adjust the LiDAR\u2019s frame integration time. This adjustment affects the density and size of the point cloud that needs to be processed.\nAdditionally, Sier et al [7] adopt the LiDAR-as-a-camera concept fusing images and point cloud data generated by a single LiDAR sensor to track UAVs without a priori knowledge. Employing a custom YOLOv5 model trained on panoramic images, they are capable of bringing computer vision capabilities on top of the LiDAR itself.\nAnother technique, departing from the typical sequence of track-after-detect, is to leverage motion information by searching for minor 3D details in the 360\u25e6 LiDAR scans of the scene and analyzing the trajectory of the tracked object to classify UAVs and non-UAV objects by identifying typical movement patterns [22,23]."
        },
        {
            "heading": "3 Methodology",
            "text": "The majority of 3D laser scanners currently available are multi-channel, rotating LiDARs. While high-end devices with 64 or 128 vertical channels can provide excellent angular resolution in both horizontal and vertical dimensions, they are not the most commonly used. Additionally, the repetitive scanning pattern of these devices has been beneficial from a geometric perspective in terms of data processing. However, it limits the FoV coverage and exposure time if the sensor position is fixed. Alternatively, solid state LiDARs with non-repetitive scan patterns can generate dense point clouds with adjustable frequencies and varying FoV coverage as illustrated in Fig. 2, providing opportunities for new perception methods that make use of different multi-scan integration time ranges to better detect objects.\nIn the following formulation we will use discrete steps represented by k owing to the discrete nature of the set of consecutive point clouds. Let Pk(Ikr ) = {pk1 ,pk2 , . . . ,pknk} be the set of nk points in the point cloud generated by the LiDAR sensor at time step k using an integration time Ikr , where r is the range of the interval. Each point pki has a position and velocity vector x k i = [x k i , y k i , z k i ] > and x\u0307ki = [x\u0307 k i , y\u0307 k i , z\u0307 k i ] > , respectively. We also denote by skUAV ={xkUAV ,x\u0307 k UAV } the position and speed of the UAV. The objective of the tracking algorithm is to identify the subset of points in Pk(Ikr ) that corresponds to the UAV, denoted PkUAV , in order to estimate its position and velocity.\nTo initiate the tracking process, we assume that the initial position of the UAV (x0UAV ) is known. The point cloud Pk(Ikr ) is integrated by accumulating the number of scans defined by Ikr . We then employ a nearest-neighbor search\n10 cm\n7 cm\nSparse point cloud\nrepresenting the MAV\nThe UGV is equipped with a limited field-ofview lidar featuring a non-repetitive scan pattern.\nalgorithm to identify the points in the point cloud that are closest to the predicted position of the UAV, based on its initial position. We leverage a priori information about the dimensions of the tracked object to improve the reliability and accuracy of the tracking results: the nearest-neighbor search is constrained to a search radius r around the initial position as represented in Equation 1.\nPkUAV = x \u2208 Pk(Ikr ) : ||x\u2212 xUAV ||2 \u2264 R (1)\nThis allows us to constrain the nearest-neighbor search to a smaller volume around the estimated position, leading to faster and more accurate search results.\nNext, we average the extracted points to estimate a new position xkUAV for the UAV, which serves as the measurement in the Kalman Filter\u2019s measurement update step as expressed in Equation 2.\nx\u0302kUAV = 1 |PkUAV | \u2211\nx\u2208PkUAV\nx (2)\nFor the prediction step of the Kalman Filter, we adopt a Constant Velocity (CV) motion model to obtain a new estimate x\u0302k|k\u22121 of the UAV\u2019s position and velocity at time k as per Eq 3:\nx\u0302k|k\u22121 = xk\u22121 + \u02c6\u0307xk\u22121\u2206t \u02c6\u0307xk|k\u22121 = \u02c6\u0307xk\u22121 (3)\nwhere \u2206t is the time step between consecutive measurements, and x\u0302k|k\u22121 and \u02c6\u0307xk|k\u22121 are the predicted position and velocity, respectively.\nWe then repeat the nearest-neighbor search around the new predicted position to update the measurement in the next iteration of the Kalman Filter. This process is outlined in Algorithm 1.\nAlgorithm 1: UAV tracking with scan integration\nInput: Integration Rate: I\n3D LiDAR point cloud: Pk(I) Last known UAV state: (xk\u22121UAV , x\u0307 k\u22121 UAV ) Output: UAV state: {xkUAV , x\u0307kUAV }\nFunction uav tracking ( P, I, xk\u22121UAV , x\u0307 k\u22121 UAV ) :\nUAV pos estimation: x\u0302kUAV = KFprediction(xk\u22121UAV ); Generate KD Tree: kdtree\u2190 P \u2032 ;\nUAV points: PkUAV = KNN(kdtree, x\u0302kUAV ); UAV measurement: zkUAV = 1\n|Pk UAV |\n\u2211 x\u2208Pk\nUAV x;\nUAV state estimation: xkUAV = KFupdate(zkUAV );\nreturn xkUAV ;\nwhile new Pk (I) do xkUAV = uav tracking ( Pk(I), I,xk\u22121, x\u0307k\u22121 ) ;"
        },
        {
            "heading": "4 Experimental Results",
            "text": "The experimental platform shown in Fig. 3 consists of a Livox Horizon LiDAR with a FoV of 81.7\u00b0\u00d7 25.1\u00b0, which is able to output scanned point clouds up to 100 Hz featuring a non-repetitive pattern. An external position system is used to validate the extracted trajectories.\nWe tested the ability of the algorithm to track a UAV through a trajectory in a large open area, at distances ranging from 2 m to over 20 m from the LiDAR\nscanner, at variable speeds and directions. This testing allows us to evaluate the algorithm\u2019s performance in a realistic scenario. We performed a quantitative analysis of the Absolute Position Error (APE) based on the ground truth. The main results are summarized in Fig. 4, which shows the distribution of APE for different tracking modalities. To enable a fair comparison between trajectories, we transform all trajectories into the reference frame of the ground truth coordinates.\nWe also provide a comprehensive comparison of the APE for different UAV tracking modalities in terms of integration time ranges. Table 1 shows the APE for tracking using both a Linear (KF) and an Extended Kalman Filter (EKF), as well as \u201dtracking by detection\u201d using only a CV model. The KF with a CV model performs better than the more complex EKF.\nTo supplement the quantitative trajectory analysis, we provide a visualization of the trajectories obtained using different integration time ranges and methods. Figs. 5 and 6 show the trajectories for different integration time ranges and tracking modalities.\nThe majority of methods fail to reconstruct the overall trajectory, with only \u201dtracking by detection\u201d using the CV model being able to consistently track the target, although the trajectory itself is noisy. These results suggest that while \u201dtracking by detection\u201d using a CV model is the only method that consistently tracks the target throughout its trajectory, integrating 2 scans using a KF achieves lower position errors, making this a promising alternative for similar scenarios."
        },
        {
            "heading": "5 Conclusion and Future Work",
            "text": "In this study, we investigated the performance of different tracking modalities for UAVs using a solid-state LiDAR with non-repetitive scan patterns. Our results indicate that integrating multiple scans (i.e., scanning at variable frequencies) using a Kalman Filter can effectively reduce the absolute position error with respect to only using a Constant Velocity model, making it a viable option for tracking UAVs in challenging environments. The Constant Velocity model combined with a linear Kalman Filter achieves better results than the Extended Kalman Filter, despite the latter\u2019s added complexity. However, \u201dtracking by detection\u201d using a Constant Velocity model was the only method that consistently tracked the UAV throughout its trajectory.\nIn future research, we plan to investigate the potential of dynamically adjusting and integrating LiDAR scans to enhance the robustness and consistency of the UAV tracking algorithm. Specifically, we aim to explore adaptive scan patterns, optimized frame integration times, and other advanced techniques to improve the accuracy and reliability of the tracking system in challenging environments."
        },
        {
            "heading": "Acknowledgment",
            "text": "This research work is supported by the Academy of Finland\u2019s AeroPolis project (Grant No. 348480)."
        }
    ],
    "title": "Evaluating the Performance of Multi-Scan Integration for UAV LiDAR-based Tracking",
    "year": 2023
}