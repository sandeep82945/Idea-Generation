{
    "abstractText": "Suggesting complementary clothing items to compose an outfit is a process of emerging interest, yet it involves a fine understanding of fashion trends and visual aesthetics. Previous works have mainly focused on recommendation by scoring visual appeal and representing garments as ordered sequences or as collections of pairwise-compatible items. This limits the full usage of relations among clothes. We attempt to bridge the gap between outfit recommendation and generation by leveraging a graph-based representation of items in a collection. The work carried out in this paper, tries to build a bridge between outfit recommendation and generation, by discovering new appealing outfits starting from a collection of pre-existing ones. We propose a transformer-based architecture, named TGNN, which exploits multi-headed self attention to capture relations between clothing items in a graph as a message passing step in Convolutional Graph Neural Networks. Specifically, starting from a seed, i.e. one or more garments, outfit generation is performed by iteratively choosing the garment that is most compatible with the previously chosen ones. Extensive experimentations are conducted with two different datasets, demonstrating the capability of the model to perform seeded outfit generation as well as obtaining state of the art results on compatibility estimation tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Federico Becattini"
        },
        {
            "affiliations": [],
            "name": "Federico Maria Teotini"
        },
        {
            "affiliations": [],
            "name": "Alberto Del Bimbo"
        }
    ],
    "id": "SP:c49346fdf6d0871ef6544e87c4042462e55320c7",
    "references": [
        {
            "authors": [
                "J. McAuley",
                "C. Targett",
                "Q. Shi",
                "A. Van Den Hengel"
            ],
            "title": "Imagebased recommendations on styles and substitutes",
            "venue": "SIGIR 2015 - Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2015, pp. 43\u201352.",
            "year": 2015
        },
        {
            "authors": [
                "M.I. Vasileva",
                "B.A. Plummer",
                "K. Dusad",
                "S. Rajpal",
                "R. Kumar",
                "D. Forsyth"
            ],
            "title": "Learning Type-Aware Embeddings for Fashion Compatibility",
            "venue": "Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), vol. 11220 LNCS, 2018, pp. 405\u2013421.",
            "year": 2018
        },
        {
            "authors": [
                "X. Han",
                "Z. Wu",
                "Y.G. Jiang",
                "L.S. Davis"
            ],
            "title": "Learning fashion compatibility with bidirectional LSTMs",
            "venue": "MM 2017 - Proceedings of the 2017 ACM Multimedia Conference, 2017, pp. 1078\u20131086.",
            "year": 2017
        },
        {
            "authors": [
                "Z. Cui",
                "Z. Li",
                "S. Wu",
                "X. Zhang",
                "L. Wang"
            ],
            "title": "Dressing as a whole: Outfit compatibility learning based on node-wise graph neural networks",
            "venue": "The Web Conference 2019 - Proceedings of the World Wide Web Conference, WWW 2019, 2019, pp. 307\u2013317.",
            "year": 2019
        },
        {
            "authors": [
                "G. Cucurull",
                "P. Taslakian",
                "D. Vazquez"
            ],
            "title": "Context-aware visual compatibility prediction",
            "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, vol. 2019-June, 2019, pp. 12 609\u201312 618.",
            "year": 2019
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is All you Need",
            "venue": "Advances in Neural Information Processing Systems, vol. 30. Curran Associates, Inc., 2017. [Online]. Available: https://proceedings.neurips.cc/paper/2017/ hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html",
            "year": 2017
        },
        {
            "authors": [
                "M. Gori",
                "G. Monfardini",
                "F. Scarselli"
            ],
            "title": "A New Model for Learning in Graph Domains",
            "venue": "Proceedings of the International Joint Conference on Neural Networks, vol. 2, 2005, pp. 729\u2013734. [Online]. Available: https://www.researchgate.net/publication/ 4202380",
            "year": 2005
        },
        {
            "authors": [
                "F. Scarselli",
                "M. Gori",
                "A.C. Tsoi",
                "M. Hagenbuchner",
                "G. Monfardini"
            ],
            "title": "The graph neural network model",
            "venue": "IEEE Transactions on Neural Networks, vol. 20, no. 1, pp. 61\u201380, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "K. Cho",
                "B. Van Merri\u00ebnboer",
                "C. Gulcehre",
                "D. Bahdanau",
                "F. Bougares",
                "H. Schwenk",
                "Y. Bengio"
            ],
            "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
            "venue": "EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference, pp. 1724\u20131734, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "I. Sutskever",
                "O. Vinyals",
                "Q.V. Le"
            ],
            "title": "Sequence to sequence learning with neural networks",
            "venue": "Advances in Neural Information Processing Systems, vol. 4, 2014, pp. 3104\u20133112.",
            "year": 2014
        },
        {
            "authors": [
                "W.L. Hamilton",
                "R. Ying",
                "J. Leskovec"
            ],
            "title": "Inductive representation learning on large graphs",
            "venue": "Advances in Neural Information Processing Systems, vol. 2017-Decem, 2017, pp. 1025\u20131035.",
            "year": 2017
        },
        {
            "authors": [
                "T.N. Kipf",
                "M. Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.",
            "year": 1997
        },
        {
            "authors": [
                "J. Chaitanya"
            ],
            "title": "Transformers are Graph Neural Networks \u2014 NTU Graph Deep Learning Lab",
            "venue": "The Gradient, 2020. [Online]. Available: https://graphdeeplearning.github.io/post/ transformers-are-gnns/",
            "year": 2020
        },
        {
            "authors": [
                "X. Song",
                "F. Feng",
                "J. Liu",
                "Z. Li",
                "L. Nie",
                "J. Ma"
            ],
            "title": "Neurostylist: Neural compatibility modeling for clothing matching",
            "venue": "Proceedings of the 25th ACM international conference on Multimedia, 2017, pp. 753\u2013761.",
            "year": 2017
        },
        {
            "authors": [
                "X. Song",
                "X. Han",
                "Y. Li",
                "J. Chen",
                "X.-S. Xu",
                "L. Nie"
            ],
            "title": "Gp-bpr: Personalized compatibility modeling for clothing matching",
            "venue": "Proceedings of the 27th ACM International Conference on Multimedia, 2019, pp. 320\u2013328.",
            "year": 2019
        },
        {
            "authors": [
                "L.D. Divitiis",
                "F. Becattini",
                "C. Baecchi",
                "A.D. Bimbo"
            ],
            "title": "Garment recommendation with memory augmented neural networks",
            "venue": "International Conference on Pattern Recognition. Springer, 2021, pp. 282\u2013295.",
            "year": 2021
        },
        {
            "authors": [
                "L. De Divitiis",
                "F. Becattini",
                "C. Baecchi",
                "A.D. Bimbo"
            ],
            "title": "Disentangling features for fashion recommendation",
            "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 2022.",
            "year": 2022
        },
        {
            "authors": [
                "F. Becattini",
                "L. De Divitiis",
                "C. Baecchi",
                "A. Del Bimbo"
            ],
            "title": "Fashion recommendation based on style and social events",
            "venue": "arXiv preprint arXiv:2208.00725, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "W. Chen",
                "P. Huang",
                "J. Xu",
                "X. Guo",
                "C. Guo",
                "F. Sun",
                "C. Li",
                "A. Pfadler",
                "H. Zhao",
                "B. Zhao"
            ],
            "title": "POG: Personalized Outfit Generation for Fashion Recommendation at Alibaba iFashion",
            "venue": "arXiv:1905.01866 [cs], May 2019. [Online]. Available: http://arxiv.org/abs/1905.01866",
            "year": 1905
        },
        {
            "authors": [
                "Y. Lin",
                "P. Ren",
                "Z. Chen",
                "Z. Ren",
                "J. Ma",
                "M. De Rijke"
            ],
            "title": "Explainable outfit recommendation with joint outfit matching and comment generation",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 32, no. 8, pp. 1502\u20131516, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "L. De Divitiis",
                "F. Becattini",
                "C. Baecchi",
                "A. Del Bimbo"
            ],
            "title": "Stylebased outfit recommendation",
            "venue": "2021 International Conference on Content-Based Multimedia Indexing (CBMI). IEEE, 2021, pp. 1\u20134.",
            "year": 2021
        },
        {
            "authors": [
                "D. Morelli",
                "M. Fincato",
                "M. Cornia",
                "F. Landi",
                "F. Cesari",
                "R. Cucchiara"
            ],
            "title": "Dress code: High-resolution multi-category virtual tryon",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 2231\u20132235.",
            "year": 2022
        },
        {
            "authors": [
                "N. Zheng",
                "X. Song",
                "Z. Chen",
                "L. Hu",
                "D. Cao",
                "L. Nie"
            ],
            "title": "Virtually trying on new clothing with arbitrary poses",
            "venue": "Proceedings of the 27th ACM international conference on multimedia, 2019, pp. 266\u2013274.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Liu",
                "P. Luo",
                "S. Qiu",
                "X. Wang",
                "X. Tang"
            ],
            "title": "DeepFashion: Powering Robust Clothes Recognition and Retrieval With Rich Annotations",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 1096\u20131104. [Online]. Available: https://openaccess.thecvf.com/content cvpr 2016/html/ Liu DeepFashion Powering Robust CVPR 2016 paper.html",
            "year": 2016
        },
        {
            "authors": [
                "Z. Lu",
                "Y. Hu",
                "Y. Jiang",
                "Y. Chen",
                "B. Zeng"
            ],
            "title": "Learning Binary Code for Personalized Fashion Recommendation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 10 562\u201310 570. [Online]. Available: https://openaccess.thecvf.com/content CVPR 2019/ html/Lu Learning Binary Code for Personalized Fashion Recommendation CVPR 2019 paper.html",
            "year": 2019
        },
        {
            "authors": [
                "A. Veit",
                "B. Kovacs",
                "S. Bell",
                "J. McAuley",
                "K. Bala",
                "S. Belongie"
            ],
            "title": "Learning Visual Clothing Style With Heterogeneous Dyadic Co-Occurrences",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 4642\u20134650. [Online]. Available: JOURNAL OF LTEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11 https://www.cv-foundation.org/openaccess/content iccv 2015/ html/Veit Learning Visual Clothing ICCV 2015 paper.html",
            "year": 2015
        },
        {
            "authors": [
                "X. Yang",
                "X. Du",
                "M. Wang"
            ],
            "title": "Learning to Match on Graph for Fashion Compatibility Modeling",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 01, pp. 287\u2013294, Apr. 2020. [Online]. Available: https://ojs.aaai.org/index.php/AAAI/ article/view/5362",
            "year": 2020
        },
        {
            "authors": [
                "R. Sarkar",
                "N. Bodla",
                "M. Vasileva",
                "Y.-L. Lin",
                "A. Beniwal",
                "A. Lu",
                "G. Medioni"
            ],
            "title": "Outfittransformer: Outfit representations for fashion recommendation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 2263\u20132267.",
            "year": 2022
        },
        {
            "authors": [
                "X. Li",
                "X. Wang",
                "X. He",
                "L. Chen",
                "J. Xiao",
                "T.S. Chua"
            ],
            "title": "Hierarchical Fashion Graph Network for Personalized Outfit Recommendation",
            "venue": "SIGIR 2020 - Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, 2020, pp. 159\u2013168.",
            "year": 2020
        },
        {
            "authors": [
                "F. Becattini",
                "X. Song",
                "C. Baecchi",
                "S.-T. Fang",
                "C. Ferrari",
                "L. Nie",
                "A. Del Bimbo"
            ],
            "title": "Plm-ipe: A pixel-landmark mutual enhanced framework for implicit preference estimation",
            "venue": "ACM Multimedia Asia, 2021, pp. 1\u20135.",
            "year": 2021
        },
        {
            "authors": [
                "A. Revanur",
                "V. Kumar",
                "D. Sharma"
            ],
            "title": "Semi-Supervised Visual Representation Learning for Fashion Compatibility",
            "venue": "Fifteenth ACM Conference on Recommender Systems. Amsterdam Netherlands: ACM, Sep. 2021, pp. 463\u2013472. [Online]. Available: https://dl.acm.org/doi/10.1145/3460231.3474233",
            "year": 2021
        },
        {
            "authors": [
                "J. Johnson",
                "M. Douze",
                "H. J\u00e9gou"
            ],
            "title": "Billion-scale similarity search with GPUs",
            "venue": "IEEE Transactions on Big Data, vol. 7, no. 3, pp. 535\u2013 547, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "P. Veli\u010dkovi\u0107",
                "G. Cucurull",
                "A. Casanova",
                "A. Romero",
                "P. Li\u00f2",
                "Y. Bengio"
            ],
            "title": "Graph Attention Networks",
            "venue": "arXiv:1710.10903 [cs, stat], Feb. 2018. [Online]. Available: http://arxiv.org/abs/1710.10903",
            "year": 2018
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep Residual Learning for Image Recognition",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770\u2013778. [Online]. Available: https://openaccess.thecvf.com/content cvpr 2016/ html/He Deep Residual Learning CVPR 2016 paper.html",
            "year": 2016
        },
        {
            "authors": [
                "W.L. Chiang",
                "Y. Li",
                "X. Liu",
                "S. Bengio",
                "S. Si",
                "C.J. Hsieh"
            ],
            "title": "Cluster-GCN: An efficient algorithm for training deep and large graph convolutional networks",
            "venue": "Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2019, pp. 257\u2013266. [Online]. Available: https://doi.org/10.1145/3292500.3330925",
            "year": 2019
        },
        {
            "authors": [
                "G. Karypis",
                "V. Kumar"
            ],
            "title": "A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs",
            "venue": "SIAM Journal on Scientific Computing, vol. 20, no. 1, pp. 359\u2013392, Jan. 1998. [Online]. Available: https://epubs.siam.org/doi/abs/10.1137/ S1064827595287997",
            "year": 1998
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A Method for Stochastic Optimization",
            "venue": "arXiv:1412.6980 [cs], Jan. 2017. [Online]. Available: http://arxiv.org/abs/1412.6980",
            "year": 2017
        },
        {
            "authors": [
                "A. Veit",
                "S. Belongie",
                "T. Karaletsos"
            ],
            "title": "Conditional Similarity Networks",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 830\u2013838. [Online]. Available: https://openaccess.thecvf.com/content cvpr 2017/html/ Veit Conditional Similarity Networks CVPR 2017 paper.html",
            "year": 2017
        },
        {
            "authors": [
                "R. Tan",
                "M.I. Vasileva",
                "K. Saenko",
                "B.A. Plummer"
            ],
            "title": "Learning Similarity Conditions Without Explicit Supervision",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 10 373\u201310 382. [Online]. Available: https://openaccess.thecvf.com/content ICCV 2019/html/Tan Learning Similarity Conditions Without Explicit Supervision ICCV 2019 paper.html",
            "year": 2019
        },
        {
            "authors": [
                "Y.-L. Lin",
                "S. Tran",
                "L.S. Davis"
            ],
            "title": "Fashion Outfit Complementary Item Retrieval",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 3311\u20133319. [Online]. Available: https://openaccess.thecvf.com/content CVPR 2020/html/Lin Fashion Outfit Complementary Item Retrieval CVPR 2020 paper.html",
            "year": 2020
        },
        {
            "authors": [
                "Y. Hou",
                "E. Vig",
                "M. Donoser",
                "L. Bazzani"
            ],
            "title": "Learning attributedriven disentangled representations for interactive fashion retrieval",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 12 147\u201312 157.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Transformer, Graph Neural Networks, Outfit Generation\nF"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Fashion is one of the most important industries in the world, moving very large amounts of money with thousands of customers and field operators. Choosing garments to create a good looking outfit is a very important task in the fashion field, but it is a difficult one: it involves many complex concepts like style and visual composition expertise, creativity, cultural and social understanding, trends, etc., and they all need to be balanced to make sure that the resulting outfit is indeed aesthetically appealing.\nOutfit creation is an ubiquitous task and, with the rapid growth of online fashion retailers and fashion related social networks (e.g. Instagram), it became a fundamental task, often carried out by entire company departments of fashion experts.\nEvery architecture needs to support two notions in order to put together a good outfit:\n\u2022 The similarity notion, that is when two garments are similar to each other and possibly interchangeable. \u2022 The compatibility notion, meaning that the fashion items composing the same outfit should aesthetically be compatible with each other.\nPrevious studies [1], [2] focused on learning compatibility metrics between pairwise items (Figure 1 a); these kind of architectures cannot model the complex relations between outfit items since each pair is treated independently. Some works [3] attempted to represent an outfit as an ordered sequence of garments (Figure 1 b) and using Recurrent Neural Network (RNN) to model compatibility. This kind of representation, however, is a forcing as it is reasonable to\n\u2022 F. Becattini, F. M. Teotini and A. Del Bimbo are with the Media Integration and Communication Center (MICC) of the University of Florence, Italy. E-mail: name.surname@unifi.it\nFig. 1. Different representations for an outfit composed by four different items: (a) the pairwise one, (b) the ordered sequence one, and (c) the graph one.\nconsider an outfit as a set, thus without a specified order, instead of as a list. To overcome these representation limitations, further architectures [4]\u2013 [5] explored the concept of representing an outfit as a graph (Figure 1 c).\nIn this paper a new architecture is proposed, Transformerbased Graph Neural Network (TGNN), aimed at generating new outfits starting from a garment or a set of garments and thus allowing even inexperienced people to create their own outfit. TGNN is based on two complementary architectures: the Transformer [6] and Graph Neural Network [7], [8]. Transformer is a kind of encoder-decoder architecture [9], [10] born in the Natural Language Processing (NLP) field and it is the base architecture for many state-of-art NLP models. GNN proved successful in modeling complex relations in very large graphs [11], [12] and is one of the most researched fields at the time of writing.\nThe GNN part of TGNN is aimed at learning context\nar X\niv :2\n30 4.\n08 09\n8v 1\n[ cs\n.C V\n] 1\n7 A\npr 2\n02 3\nfeatures from this graph. The Transformer architecture, on the other hand, is employed for its outstanding performance on sequences modeling. Born to overcome the recurrent nature of architectures like LSTM [13] and GRU [9], the Transformer is capable of predicting the next item in a sequence, given the previous ones; this ability is a perfect fit for the outfit generation task starting from a seed: every predicted garment has to be compatible with the seed and the already predicted garments. It has been proved that, under some constraints, GNNs can be seen as Transformer encoders [14]. TGNN uses this concept to integrate these two architectures and to successfully model the compatibility in fashion outfit generation.\nTo summarize, the main contributions of this work are:\n\u2022 The proposal of Transformer-based Graph Neural Network (TGNN), a new architecture aimed at seeded outfit generation, which can better capture the complex relations among multiple items in an outfit. \u2022 The development of an hybrid between Graph Neural Networks and Transformer encoders, treating the nodes\u2019 neighborhoods as interlinked unordered sequences. \u2022 Experimental results on widely used evaluation tasks demonstrate the effectiveness of the developed architecture over other previous state-of-the-art ones. \u2022 The introduction of a new evaluation task, Seeded Items Prediction (SIP), to test how good a model is at iteratively reconstructing a given outfit starting from one outfit item (the seed)."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Fashion recommendation is a hot topic, for which several aspects have been studied. The problem has been declined in many forms, ranging from compatibility estimation [15], [16], [17], [18], [19], outfit generation [20] to plain simple recommendation [21], [22] and try-on [23], [24].\nMany early works [1], [25]\u2013 [26] focus on calculating a pairwise compatibility metric between garments: for example, McAuley et al. [1] extracts visual features to model human visual preference for a pair of items, Veit et al. [27] develops a Siamese based network that estimates pairwise\ncompatibility based on co-occurrence in large-scale user behavior data, while Lu et al [26] aggregates user preferences on each item and integrates them with pairwise compatibility scores. These approaches lack efficiency and accuracy in real-world usage.\nThe first work considering a fashion compatibility between a set of items is Han et al. [3]: here an outfit is considered as an ordered sequence and fed to a bidirectional LSTM network to predict a compatibility score. In this work the Polyvore dataset was also introduced. Vasileva et al. [2] trains pairwise embedding spaces to learn different representations for different pairs of categories, that, however, is not feasible when the number of categories is high. Furthermore, they also extended and enriched the Polyvore dataset introducing more challenging evaluation sets.\nSome works [4], [5], [28] focused on representing an outfit as a graph: in NGNN [4], Cui et al. represented an outfit as a graph to model complex relations among items which has been demonstrated to be more effective than pairwise and sequence representations. In our work we represent outfits as graphs, treating fashion items as nodes, connected to each other if they can be combined to compose an outfit. Differently from prior work, we process such graphs with a model harnessing the effectiveness of Graph Convolutions as well as multi-headed self-attention, typical of transformers [6]. To this end, we integrate the message passing strategy of Graph Convolutions as a form of attention in the encoder structure of a transformer model. Transformers have also been used recently in literature to generate outfit level representations for compatibility prediction tasks [29].\nIn addition to visual features, some take advantage of other kind of information like textual representations [2], [4], [20], categories [2], [30], and user preferences [30], [26], [31]. Lately, there have been studies [32] aimed at operating in semi-supervised settings, as creating fully labeled datasets is expensive and often requires fashion experts\u2019 knowledge."
        },
        {
            "heading": "3 OVERVIEW",
            "text": "Transformer-based Graph Neural Network (TGNN) is a novel architecture whose objective is to discover new aesthetically appealing outfits by picking and combining items from a collection. When referring to an outfit we refer to a composition of several garments or accessories of different categories that can be worn together. For instance, an outfit can be composed by combining a t-shirt, a skirt, a scarf, shoes, a bag and a necklace. Starting from an initial garment seed, TGNN is capable of generating unseen outfits complementing the given item. TGNN will iteratively chose garments from a candidate set that are compatible with the seed and with the other previously chosen garments, until a stop condition is reached. We make the assumption that the collection of garments from which to make a recommendation is fixed.\nThe TGNN architecture is based on a Transformer network: the encoder block operates on a collection of predefined outfits to learn the complex relations standing between garments, while the decoder carries out the outfit generation, conditioning the output with the seed and the contextual information provided by the encoder.\nFollowing prior work, we represent an outfit as a graph clique. Since a single garment can belong to more than one outfit, the collection of pre-defined outfits is naturally modeled as a graph, connecting such cliques through nodes belonging to more outfits. The encoder blocks in TGNN are therefore a special kind of Graph Neural Network that adapts the standard transformer encoder to graph data structures."
        },
        {
            "heading": "3.1 Notation and Problem Definition",
            "text": "Let O be a collection of pre-defined outfits. Each outfit oi \u2208 O is composed of a combination of ni garments gi,j with j = 1, ..., ni belonging to a set G, i.e. oi = {gi,1, \u00b7 \u00b7 \u00b7 , gi,ni}. Each item belongs to a distinct fashion category, which we identify as C(gi,j). Since the same garment can belong to several different outfits, we denote with O(gi) := {oj | gi \u2208 oj} the set of outfits in which the item appears. Every outfit might be composed of a different number of garments, depending on how many fashion categories have been used.\nIn order to build the data structure representing the collection of outfits, we provide the following definitions.\nDefinition 3.1 (Outfit Relation Graph). An Outfit Relation Graph (ORG) is an undirected graph whose nodes are outfits and edges exist between two nodes if the linked outfits share at least one garment. Formally, GO = (V, E) such that\nV \u2286 O\nE = {(oi, oj) | oi \u2229 oj 6= \u2205 , i 6= j}\nDefinition 3.2 (Item Relation Graph). An Item Relation Graph (IRG) is an undirected graph whose nodes are garments and edges exist between two nodes if the linked garments belong to the same outfit. Formally, GG = (V, E) such that\nV \u2286 G\nE = {ei,j := (gi, gj) | O(gi) \u2229O(gj) 6= \u2205}\nWhen dealing with an IRG, the direct neighborhood Ni = {gj | gj \u2208 V, ei,j \u2208 E} of node gi is assumed to contain gi itself, that is, an IRG is an undirected graph with self edges. An example of Item Relation Graph is shown in Fig. 2.\nDefinition 3.3 (ORG to IRG induction). An ORG can induce an IRG, meaning, an IRG can be built starting from an ORG. Formally, given an ORG GO = (V, E), its induction GO = (V , E) is an IRG such that\nV = {gi | O(gi) \u2229 V 6= \u2205}\nE = {ei,j := (gi, gj) | gi, gj \u2208 V ,O(gi) \u2229O(gj) 6= \u2205}\nBased on these concepts, we define the problem of Seeded Item Prediction as the task aimed at generating complementary items to compose an outfit, based on a garment seed and on an IRG GG . A garment seed is a sequence of garments appearing in GG , such that they do not belong to the same outfit, i.e., they are not linked. Formally\n\u03d5 = {gs,1, \u00b7 \u00b7 \u00b7 , gs,n | gi \u2208 V , ei,j /\u2208 E \u2200 i 6= j}"
        },
        {
            "heading": "4 METHODOLOGY",
            "text": "Let GG be an IRG, \u03d5 a garment seed, and C \u2282 G the garment candidate set with \u03d5\u2229 C = \u2205 and g\u03c9 \u2208 C, where g\u03c9 is a fake garment indicating end-of-outfit, to be used as a stop sign during generation.\nAt each time step t, TGNN predicts the next outfit garment g\u0302t, given GG , \u03d5 and all the previous generated garments {g\u0302i}t\u22121i=1 . The element g\u0302t is chosen among the elements of C(t) = C/{g \u2208 O(g\u0302i)}t\u22121i=1 . The outfit generated by TGNN at the end of step t is:\no\u0302(t) = \u03d5 \u222a {g\u0302i}ti=1 g\u0302i \u2208 C , g\u0302i 6= g\u0302j (1)\nThis process continues until the model outputs the stop sign g\u03c9 . The final outfit is therefore o\u0302(T\u22121), where T is the total number of generation steps.\nThe full TGNN architecture is shown in Fig. 3. The node embeddings of GG are first passed to a transition layer whose responsibility mirrors the embedding layer of a standard Transformer. Moreover it adapts the de-dimensional embeddings to the hidden dimension dm of the model. Then, the encoder block operates on this modified graph to generate a new representation E of all the nodes/garments.\nFor the decoder part, at first, o\u0302(t) is passed through another transition layer, then it is fed to the decoder block along with E. The result is a vector h(t) \u2208 Rdm . Then g\u0302t is obtained as:\ng\u0302t = arg max gc\u2208C(t) exp(h(t) \u00b7 \u03c4(gc))\u2211 gi\u2208C(t) exp(h (t) \u00b7 \u03c4(gi)) (2)\nthat is, the garment gc of the candidate set, whose transition representation \u03c4(gc) results in the highest similarity with h(t), among all the candidate set items.\nNote that in principle the number of candidates could become arbitrarily large. However, each element is independently fed to the transition layer and compared to the output of the decoder, as detailed in Eq. 2. This operation can be\ndone easily for large candidate sets, since both operations can be carried out in parallel. If the number of candidate items is so high to make parallel computation unfeasible, an index (e.g. FAISS [33]) could be used to approximate the similarity computation.\nIn addition, in this paper we assume to have a closed collection of garments from which to make recommendations. Adding new garments would require to update the graph, which could be done at any time without requiring any further training."
        },
        {
            "heading": "4.1 Initial Garment Embeddings",
            "text": "Unlike Transformers for NLP [6], which deal with words, it is not possible to build a garment \u201cvocabulary\u201d containing all the existing garments. On the other hand, a garment can be associated with an image and/or a title. This means that instead of learning the initial embeddings as it is done with words, in the case of garments, they can be pre-computed as feature vectors.\nIn this work, each garment image is fed into an ImageNet pretrained ResNet50, and the outputs of the last layer before the classification block are then taken as garment embeddings. Since these embeddings are computed by ResNet50, their dimension is 2048. To compact the features we use PCA and project them to a lower dimension de."
        },
        {
            "heading": "4.2 Transition Layer",
            "text": "As shown in Fig. 3, there are two transition layers, one for the encoder block and one for the decoder block. These are simply feed-forward layers followed by a ReLU activation, that serve the purpose to learn a meaningful and compact representation to be fed to each block of the model. In particular, for the decoder, it also allows to map items in the candidate set into a semantic space suitable for identifying complementary items according to Eq. 2."
        },
        {
            "heading": "4.3 Transformer Encoder as ConvGNN",
            "text": "The encoder block operates on a graph containing a predefined collection of outfits, specifically an Item Relation Graph. The traditional encoder of a transformer is modified only in its self-attention layer: the idea, adapted from GAT [34], is to treat each node along with its direct neighbors as an unordered sequence and to apply to each of them the multi-head scaled dot-product attention mechanism. These sequences are actually outfits and, as outfits can share garments, these sequences are linked together. By stacking multiple of these modified encoder modules, the message passing nature of spatial ConvGNNs can be leveraged, meaning that a single garment can attend not only to the garments with which it shares a common outfit, but also to other garments belonging to linked outfits.\nIn Fig. 4, an example of how attention information is passed between different but linked outfits in an IRG, is shown.\nFormally, referring to the MultiHead attention function adopted in transformers [6], the multi-head scaled dotproduct attention mechanism calculated for a node gi \u2208 Rd is defined as:\nMultiHead(gi,Ni,gi) (3)\nthat is, the query Q and the value V are set to be equal to gi while the key K is a |Ni| \u00d7 d matrix containing the feature vectors of the neighboring nodes of gi."
        },
        {
            "heading": "4.4 Objective Function",
            "text": "Let o = {g1, \u00b7 \u00b7 \u00b7 , gn} be a predefined outfit and let GG = (V, E) be an IRG where g1, \u00b7 \u00b7 \u00b7 , gn \u2208 V but ei,j /\u2208 E \u2200 gi, gj \u2208 o, and C \u2282 G the garments candidate set with o \u2282 C. Thus, for the triplet (o,GG , C) the objective function of TGNN can be written as\nL(o,GG ,C) = \u2212 1\nn n\u2211 t=1 logP (gt+1|g1, \u00b7 \u00b7 \u00b7 , gt, GG , C,\u0398) (4)\nwhere \u0398 denotes the model parameters, gn+1 is assumed to be the end-of-outfit fake garment g\u03c9 , and P (\u00b7) is the probability of choosing the next correct garment conditioned on the previously predicted outfit garments, the IRG and the candidate set.\nIn one training sample, the i-th ground truth \u0393i is the next garment to be produced, that is \u0393i = gi+1, as shown in Fig. 5, and \u0393n = g\u03c9 .\nIn this setting, the conditioned probability of Equation (4) becomes\nP (gt+1|g1, \u00b7 \u00b7 \u00b7 , gt, GG , C,\u0398) = exp(h(t) \u00b7 \u03c4(\u0393t))\u2211\ngi\u2208C exp(h (t) \u00b7 \u03c4(gi))\n(5)"
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In this section we outline the experimental validation of the proposed method and the analysis of the obtained results. In addition, both the network setup and the hyperparameter values used throughout the experiments are explained."
        },
        {
            "heading": "6 DATASET",
            "text": "To perform experimental validation of our approach, we adopt different versions of the Polyvore dataset [2].\nThe dataset has been built from the Polyvore website, a popular social network hub where fashion enthusiasts could upload, tag and title garment images and create custom outfits as compositions of these. The crafted outfits, then, could be shared, commented, liked, but also used as basis for other new outfits.\nAn eary version of the dataset was proposed by Han et al. [3], which is now referred to as the Maryland Polyvore dataset. This dataset was relatively small (around 20k outfits and 160k items), did not contain item categories, and had some inconsistencies in the test set. To resolve these issues, Vasileva et al. [2] produced another outfit dataset based on Polyvore data, referred to as the Polyvore dataset, with both coarse and finer grained item categories (defined respectively as \u201csemantic\u201d and \u201cleaf\u201d categories), titles and descriptions; this dataset is, also, larger than the Maryland dataset as it contains more than 60K outfits and 360K items. Moreover, this dataset provides carefully tailored test-train splits: as some garments appear in many different outfits, the choice of letting these garments appear in unseen test outfits has a significant effect. The Polyvore Dataset comes in two versions:\n\u2022 Polyvore-S (Standard) \u2013 This version is easier as no outfit appearing in the training set or in the test set can appear in the other one, while it is possible that an item belonging to a training outfit is seen in a test one.\nIn Tab. 1, the outfit and item numbers, along with the split sizes, are summarized for both versions. In Fig. 7, the outfit size distribution for the Polyvore-S train split, is visualized."
        },
        {
            "heading": "7 TGNN SETUP",
            "text": "First, each garment image is distilled in a feature vector, as described in Section 4.1, by feeding it to a pretrained ResNet50 [35], and then through a PCA resulting in de = 128 dimensional feature vectors.\nLet GG be the Item Relation Graph of the dataset, as in Definition 3.2. Generally speaking, every dataset of interest will have an IRG too large to handle. In Tab. 2, some graph statistics for the Polyvore IRGs are shown. For example, the Polyvore-S train split IRG contains more than 200k nodes and more than 680k edges.\nThese numbers do not allow operating on the entire IRG at once, as it will saturate the GPU memory. To overcome this issue, we first split the graph into clusters and only retain relevant partitions. Let GO be the dataset ORG, as in Definition 3.1. Based on the idea of Cluster-GCN [36], this ORG is partitioned, using the METIS clustering algorithm [37], in P subgraphs such that each of them contains about \u03c6 nodes (i.e. outfits). We refer to the p-th partition of the ORG as GO,p. In our experiments we use \u03c6 = 50."
        },
        {
            "heading": "7.1 Training Setup",
            "text": "Given an outfit o = {g1, \u00b7 \u00b7 \u00b7 , gn}, a training example is a triplet (o\u0302, GG,o, Co\u0302) where o\u0302 is a random permutation of the original outfit o. This means that the model sees a different shuffled version of the same outfit at every epoch, acting as a form of data augmentation and making the model invariant\nto garment ordering. To build the partitioned IRG GG,o, relative to outfit o, we adopt the following steps:\n1) Find the partition GO,p = (Vp, Ep) such that o \u2208 Vp. 2) From GO,p, build its IRG induction GO,p =\n(Vp, Ep) = GG,p as in Definition 3.3. 3) Remove all the edges ei,j \u2208 Ep such that gi, gj \u2208\no, gi 6= gj , i.e. all edges connecting elements of o.\nFinally, for each garment gi in the outfit that has to be predicted, we build a candidate set Ci containing (nC + nR + 1) garments:\n\u2022 nC random negatives gj \u2208 GG,o such that gj /\u2208 o and C(gj) = C(gi), i.e. belonging to the same category of the ground truth item. \u2022 nR random distractors gj \u2208 GG,o such that gj /\u2208 o. \u2022 1 garment representing the ground truth item \u0393i.\nWe refer to Co\u0302 = {C1, \u00b7 \u00b7 \u00b7 , Cn} as the set of candidate sets for each step. In our experiments we use, unless expressly stated otherwise, nC = 3 and nR = 5.\nThe model is trained to output the sequence of garments in the outfit, starting from a garment seed and optimizing the loss defined in Eq. 4 at each generation step."
        },
        {
            "heading": "7.2 Training Details and Hyperparameters",
            "text": "If not otherwise stated, the following hyperparameter values are used throughout all the experiments. We set the number of modules in the encoder block, i.e. the aggregation\nradius, equal to Kenc = 2. As for the decoder, we use a number of modulesKdec = 4. Garment images are projected using PCA to an embedding of dimension de = 128. The explained variance for the PCA is depicted in Fig. 8. Such embeddings are projected by the transition layer into a hidden space with dm = 256 dimensions. For multi-head attention, we use H = 8 heads.\nThe network is optimized with the Adam [38] optimizer with learning rate 5 \u00b7 10\u22124 and weight decay 5 \u00b7 10\u22125. The learning rate is reduced by a 0.1 factor each time the validation loss reaches a plateau. Dropout is applied to each dense layer and with value 0.35. The model is trained for a maximum of 1000 epochs with an early stopping strategy on the validation loss with a 10 epoch patience.\nThe GPU used during training is a Nvidia Titan RTX with 24GB memory. Half-precision was employed to lower memory requirements and training time."
        },
        {
            "heading": "8 EVALUATION TASKS OVERVIEW",
            "text": "The performance of TGNN is evaluated on three different tasks. Our main focus is on outfit generation, for which we formalize an evaluation protocol named Seeded Item Prediction. However, we also apply our model to standard compatibility tasks, namely Fill-In-The-Blank and Compatibility Prediction, which are widely adopted in literature [2], [3], [32]. In the following we provide a brief overview of such tasks and relative evaluation metrics.\nSeeded Item Prediction (SIP) is a task where the model is presented with a partial outfit and it has to incrementally predict the next complementary item given the previous ones from a collection of possible candidates (Fig. 9). The performance is evaluated using the accuracy in choosing the correct item.\nFill-In-The-Blank (FITB) is a task in which the model is presented with an incomplete outfit missing one item. Along with the outfit, four candidate items (three incorrect and one correct), are provided as possible answers. The task is then to choose the item between the candidates that is the most compatible with the given incomplete outfit (Fig. 10). The results are evaluated as overall accuracy. It must be noted that FITB is a special case of the SIP task: the two tasks are the same when the seed of SIP is the entire outfit minus one item and the possible answer set is composed by\nfour items. In this sense, the commonly adopted FITB task is an oversimplification of SIP.\nCompatibility Prediction (CP) is a task where the model has to predict whether a candidate outfit is compatible or not (Fig. 10). Outfits are scored to assess whether their constituting items are compatible with each other. The task is performed by feeding to the model two different outfits, namely a positive ground truth outfit, which is known to be compatible, and a negative random outfit. The model is requested to score the two outfits and the task is considered successful when the positive one is scored higher than the negative one. The performance is evaluated using the area under the receiver operating characteristic curve (AUROC)."
        },
        {
            "heading": "8.1 Test Setup",
            "text": "Since the TGNN architecture works with triplets containing an outfit, a graph partition and a candidate set in the form of (o,GG,o, Co), like the ones described in Section 7.1, these must be carefully constructed for each evaluation task.\nFor the Fill-In-The-Blank task, given the original complete outfit o = {g1, \u00b7 \u00b7 \u00b7 , gb, \u00b7 \u00b7 \u00b7 , gn}, where gb is the blank garment placeholder corresponding to the item to be predicted, the triplet will be (o\u0302, GG,o, Co\u0302) where: \u2022 o\u0302 is the incomplete outfit, that is, o without the blank\nitem gb. \u2022 GG,o is a graph partition built in the same way as\ndescribed in Section 7.1. However, instead of identifying the partition that exactly contains o, we retrieve the most similar garments in the training set and take the partition containing the maximum number of such garments. This is done because, in general, a test outfit might not be present in the training set. By doing so, we assume that test garments have some sufficiently similar items in the training graph, which we believe to be a reasonable assumption. Other methods from the state of the art not depending on a garment graph may not make this assumption. \u2022 Co\u0302 = Cb containing the four possible answers, among which there is the correct garment. Note that there is a single candidate set, since in the FITB task we only need to perform one prediction step to complete the outfit.\nFor the Compatibility Prediction task, instead, we need to identify a graph partition for both the positive and neg-\native outfits. Both partitions are taken looking for the most similar items in the training set, as in the FITB configuration.\nWe then generate a random partition of the garments and ask the model to recreate the outfit from a single-element seed. We average the likelihood of the correct garment at each generation step to obtain the final compatibility prediction score. In order to do so, we need to define a candidate set Ci for each step. Each one is composed of the ground truth item \u0393i and three other random garments belonging to GG,o.\nFinally, for the Seeded Item Prediction task, the setup is the same as Section 7.1 without the initial random permutation. Also in this case, the graph partition is obtained by taking the one with the maximum number of most similar items to the partial outfit in the training set."
        },
        {
            "heading": "8.2 Baselines",
            "text": "To validate the hypothesis that contextual information, i.e. the Item Relation Graph, and self-attention play an important role for compatibility learning, TGNN is compared with previously reported approaches on the Polyvore dataset:\n\u2022 Siamese Network - The approach described by Veit et al. [27] that estimates pairwise compatibility based on co-occurrence in large-scale user behavior data. \u2022 Bi-LSTM - Han et al. [3] was the first work to consider an outfit as a whole, representing it as an ordered sequence. Taking multi-modal data as input, it can calculate the compatibility scores of outfits by iteratively predicting the next item. \u2022 CSN T1:1 - Learns a pairwise category-dependent transformation using the approach of Veit et al. [39] to project a general embedding to a single type-specific space, measuring compatibility between two item categories. \u2022 Type Aware - This method, described in Vasileva et al. [2], focuses on learning item image embeddings that respect the item type by measuring pairwise item similarity in different semantic subspaces. \u2022 SCE-Net - Introduced in Tan et al. [40], it jointly learns pairwise different representation subspaces for different similarity conditions without explicit supervision. \u2022 CSA-Net - Approach introduced by Lin et al. [41] that models outfit compatibility by encoding category pairs information to each item embedding vector. \u2022 Pseudo-Label - Described in Revanur et al. [32], this method, developed for semi-supervised settings, learns item embeddings by generating pseudo positive and negative pairs on-the-fly during each training epoch. These pairs can be based on different characteristics, like colors, shapes, etc. while discarding the item category notion. \u2022 ADDE-O - The method [42] is based on learning disentangled features for specific attribute classes. This allows to perform targeted manipulations exploiting a memory bank of attribute prototypes. \u2022 MUFIN - Recently proposed in [43], the method addresses the task of Extreme Classification, i.e. a\nWe also report variants of some of the methods, referred to as VSE, which add a visual-semantic embedding, as described in [2], jointly learned with the compatibility embedding."
        },
        {
            "heading": "9 RESULTS",
            "text": "We first evaluate our model on the newly introduced task of Seeded Item Prediction (SIP). The task was created to evaluate the outfit generation capabilities of TGNN. As illustrated in Fig. 9, given an outfit o = {g1, \u00b7 \u00b7 \u00b7 , gn}, the task objective is to correctly predict the next item \u0393i = gi+1 from a candidate set, given the previous items {g1, \u00b7 \u00b7 \u00b7 , gi}. Tab. 3 shows how TGNN performs on this task, along with two baselines, by measuring the accuracy of choosing the correct item. Since SIP is a novel evaluation task proposed in this work, there are no available results from previous works. To provide a solid baseline, the task was evaluated with the publicly released code of the Type Aware architecture1. To generate the results, we trained the model with the same hyperparameter values as described in the original\n1. Original code available at https://github.com/mvasil/ fashion-compatibility\npaper [2]. We provide also a random baseline as a lower bound.\nTGNN performs well on this difficult task, showing that it is capable of correctly generating outfits. There is a gap between the accuracy obtained with the two Polyvore versions. We attribute this to the attention mechanism, whose characteristic is to learn how different items are related among themselves. Therefore, the presence, or absence, in the test set of items seen during the training phase could most likely influence this mechanism, and, in turn, the whole model performance.\nIn Fig. 11 we report qualitative results of correctly generated outfits by TGNN in the Seeded Item Prediction setting. It can be seen that TGNN is able to generate outfits with a variable number of outfits, starting from a variable number of seeds. We also show a few failure cases in Fig. 12. In the first failure case, the model emits the stop sign before completing the outfit, lacking a top garment to complete the outfit. In the second example, the model generates an outfit with a style different from the ground truth one. This behaviour is easily avoidable by adding more seed items to the input.\nWe then evaluate TGNN for the tasks of Fill-In-TheBlank (FITB) and Compatibility Prediction (CP). As shown in Tab. 4, TGNN outperforms previous state of the art methods on both tasks. These results suggest that the graph structure provides valuable information for this kind of task. In particular for FITB it is indeed to be expected that, within an IRG, the missing garment (or one similar enough) will be found close to its complementary items. At the same time, for Compatibility Prediction we believe that the multi-head self-attention mechanisms of TGNN plays an important role: by learning to model compatibilities between an item and all the other ones within the same outfit, the model implicitly learns a global outfit compatibility notion by combining all of these item compatibilities.\nSince several methods adopt a ResNet18 backbone [35] rather than a ResNet50 like ours, we also show results for a\nvariant of TGNN using ResNet18 features to establish a fair comparison. Apart from the backbone the whole training and evaluation procedure is left unchanged. Indeed, using ResNet18 features there is a slight performance drop but TGNN still manages to achieve state of the art results."
        },
        {
            "heading": "9.1 Ablation Studies",
            "text": "To better analyze the TGNN behavior, it is useful to correlate the performance with those hyperparameters peculiar to the proposed architecture, namely \u03c6 \u2013 the one controlling the partition sizes \u2013 and Kenc \u2013 the one defining how far from a node the information gets aggregated into the said node, i.e. the aggregation radius."
        },
        {
            "heading": "9.1.1 Partition Size",
            "text": "In Tab. 5, the TGNN results for different \u03c6 values are shown. It is clear that, when increasing \u03c6, i.e. when the partitions GG,o grow larger, TGNN performance decreases. The main reasons are:\n1) The encoder block is no longer able to learn, and thus to integrate into the nodes useful relational information from the surrounding context. In other words, there is too much information and the encoder block fails to handle it. 2) The Encoder-Decoder Attention of each decoder module relates each item gi \u2208 o with all the nodes of GG,o; with higher \u03c6 values, the number of nodes grows exponentially, thus, making the EncoderDecoder Attention layer increasingly complex.\nOn the contrary, when lowering the partition size too much, the accuracy decreases since not enough context is given to the model."
        },
        {
            "heading": "9.1.2 Aggregation Radius",
            "text": "In Tab. 6, the results obtained by TGNN are shown, at increasing values of the aggregation radius Kenc. The experiments were carried out with all the hyperparameter values fixed to the ones listed in Sec. 7.2. It is clear that increasing Kenc yields better results. In fact, Kenc controls how many encoder modules are used in the encoder block, which, in turn, affects how far from a given node the information gets aggregated, i.e. from the Kenc-neighborhood of the said node. Increasing Kenc does indeed help the encoder block at managing the information in the graph, while learning useful relationships between loosely coupled outfits. Eventually, the benefits of increasing Kenc saturate, bringing negligible or no benefits, yet requiring a much longer training time (Tab. 6)."
        },
        {
            "heading": "10 CONCLUSIONS",
            "text": "This work focused on the problem of outfit generation starting from a garment seed, which consists in proposing a set of garments that, combined with the seed, assembles a compatible outfit. To this aim, Transformer-based Graph Neural Network is proposed as a novel architecture which combines the capabilities of Graph Neural Networks with the powerful Transformer model. The proposed method is capable of learning the complex relations existing between items and outfits by employing graph representations and attention mechanisms. Through message propagation and self-attention, an item compatibility notion is refined by aggregating the interaction information derived from its neighbors. This knowledge is then used in the generation phase to iteratively choose the item that is most compatible with the seed and the previously chosen ones. Extensive\nexperiments on the Polyvore dataset have demonstrated the rationality and the effectiveness of TGNN, which outperforms previous models in outfit compatibility tasks."
        },
        {
            "heading": "11 FURTHER DEVELOPMENTS",
            "text": "Since the iterative generation mechanism is inspired to the one employed in many NLP models, we plan to improve the generation performance with search strategies inspired by the field of NLP, such as Beam Search. Another matter worth investigating is making the architecture work with a dynamic IRG. In other words a graph that can change over time by adding new nodes or edges. Being able to handle such real-world situations could prove fundamental to evolve a research model in a production ready one. Furthermore, to resolve the Encoder-Decoder Attention issue when the \u03c6 value increases, further studies could focus on creating masks in order to intelligently limit the number of partition nodes to which any outfit item is related to. Finally, an assessment from domain experts as an additional term of comparison between methods could help understanding which aspects are taken into account while composing the outfit, such as style, color, etc."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was partially supported by the Italian MIUR within PRIN 2017, Project Grant 20172BH297: I-MALL - improving the customer experience in stores by intelligent computer vision."
        }
    ],
    "title": "Transformer-based Graph Neural Networks for Outfit Generation",
    "year": 2023
}