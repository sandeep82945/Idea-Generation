{
    "abstractText": "In-context learning (ICL) performs tasks by prompting a large language model (LLM) using an instruction and a small set of annotated examples called demonstrations. Recent work has shown that precise details of the inputs used in the ICL prompt significantly impact performance, which has incentivized instruction selection algorithms. The effect of instructionchoice however is severely underexplored, with existing analyses restricted to shallow subsets of models and tasks, limiting the generalizability of their insights. We develop InstructEval, an ICL evaluation suite to conduct a thorough assessment of these techniques. The suite includes 13 open-sourced LLMs of varying scales from four model families, and covers nine tasks across three categories. Using the suite, we evaluate the relative performance of seven popular instruction selection methods over five metrics relevant to ICL. Our experiments reveal that using curated manually-written instructions or simple instructions without any task-specific descriptions often elicits superior ICL performance overall than that of automatic instruction-induction methods, pointing to a lack of generalizability among the latter. We release our evaluation suite for benchmarking instruction selection approaches and enabling more generalizable methods in this space.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Anirudh Ajith"
        },
        {
            "affiliations": [],
            "name": "Chris Pan"
        },
        {
            "affiliations": [],
            "name": "Mengzhou Xia"
        },
        {
            "affiliations": [],
            "name": "Ameet Deshpande"
        },
        {
            "affiliations": [],
            "name": "Karthik Narasimhan"
        }
    ],
    "id": "SP:65388aed2c2eba329ee8f677fd929594c411b42b",
    "references": [
        {
            "authors": [
                "Sweta Agrawal",
                "Chunting Zhou",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Marjan Ghazvininejad"
            ],
            "title": "Incontext examples selection for machine translation",
            "year": 2022
        },
        {
            "authors": [
                "Gunjan Chhablani",
                "Han Wang",
                "Jason Alan Fries",
                "Maged S. Al-shaibani",
                "Shanya Sharma",
                "Urmish Thakker",
                "Khalid Almubarak",
                "Xiangru Tang",
                "Mike Tian-Jian Jiang",
                "Alexander M. Rush"
            ],
            "title": "Promptsource: An integrated",
            "year": 2022
        },
        {
            "authors": [
                "Samuel Weinbach"
            ],
            "title": "Gpt-neox-20b: An opensource autoregressive language model",
            "year": 2022
        },
        {
            "authors": [
                "Sid Black",
                "Gao Leo",
                "Phil Wang",
                "Connor Leahy",
                "Stella Biderman."
            ],
            "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow",
            "venue": "If you use this software, please cite it using these metadata.",
            "year": 2021
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Yanda Chen",
                "Chen Zhao",
                "Zhou Yu",
                "Kathleen McKeown",
                "He He"
            ],
            "title": "On the relation between sensitivity and accuracy in in-context learning",
            "year": 2023
        },
        {
            "authors": [
                "Christopher Clark",
                "Kenton Lee",
                "Ming-Wei Chang",
                "Tom Kwiatkowski",
                "Michael Collins",
                "Kristina Toutanova."
            ],
            "title": "Boolq: Exploring the surprising difficulty of natural yes/no questions",
            "venue": "NAACL.",
            "year": 2019
        },
        {
            "authors": [
                "Mingkai Deng",
                "Jianyu Wang",
                "Cheng-Ping Hsieh",
                "Yihan Wang",
                "Han Guo",
                "Tianmin Shu",
                "Meng Song",
                "Eric Xing",
                "Zhiting Hu."
            ],
            "title": "RLPrompt: Optimizing discrete text prompts with reinforcement learning",
            "venue": "Proceedings of the 2022 Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Hila Gonen",
                "Srini Iyer",
                "Terra Blevins",
                "Noah A. Smith",
                "Luke Zettlemoyer"
            ],
            "title": "Demystifying prompts in language models via perplexity estimation",
            "year": 2022
        },
        {
            "authors": [
                "Tanya Goyal",
                "Junyi Jessy Li",
                "Greg Durrett"
            ],
            "title": "News summarization and evaluation in the era of gpt-3",
            "year": 2023
        },
        {
            "authors": [
                "Or Honovich",
                "Uri Shaham",
                "Samuel R Bowman",
                "Omer Levy."
            ],
            "title": "Instruction induction: From few examples to natural language task descriptions",
            "venue": "arXiv preprint arXiv:2205.10782.",
            "year": 2022
        },
        {
            "authors": [
                "Lifu Huang",
                "Ronan Le Bras",
                "Chandra Bhagavatula",
                "Yejin Choi."
            ],
            "title": "Cosmos QA: Machine reading comprehension with contextual commonsense reasoning",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Mandar Joshi",
                "Eunsol Choi",
                "Daniel Weld",
                "Luke Zettlemoyer."
            ],
            "title": "triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
            "venue": "arXiv e-prints, page arXiv:1705.03551.",
            "year": 2017
        },
        {
            "authors": [
                "Uszkoreit",
                "Quoc Le",
                "Slav Petrov."
            ],
            "title": "Natural questions: A benchmark for question answering research",
            "venue": "Transactions of the Association for Computational Linguistics, 7:453\u2013466.",
            "year": 2019
        },
        {
            "authors": [
                "Yuhui Zhang",
                "Yuta Koreeda"
            ],
            "title": "Holistic evaluation of language models",
            "year": 2022
        },
        {
            "authors": [
                "Jiachang Liu",
                "Dinghan Shen",
                "Yizhe Zhang",
                "Bill Dolan",
                "Lawrence Carin",
                "Weizhu Chen"
            ],
            "title": "2021a. What makes good in-context examples for gpt-3? In Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning",
            "year": 2021
        },
        {
            "authors": [
                "Jiachang Liu",
                "Dinghan Shen",
                "Yizhe Zhang",
                "Bill Dolan",
                "Lawrence Carin",
                "Weizhu Chen"
            ],
            "title": "What makes good in-context examples for gpt-3",
            "year": 2021
        },
        {
            "authors": [
                "Yao Lu",
                "Max Bartolo",
                "Alastair Moore",
                "Sebastian Riedel",
                "Pontus Stenetorp."
            ],
            "title": "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
            "venue": "arXiv preprint arXiv:2104.08786.",
            "year": 2021
        },
        {
            "authors": [
                "Yao Lu",
                "Max Bartolo",
                "Alastair Moore",
                "Sebastian Riedel",
                "Pontus Stenetorp."
            ],
            "title": "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
            "venue": "CoRR, abs/2104.08786.",
            "year": 2021
        },
        {
            "authors": [
                "Andrew L. Maas",
                "Raymond E. Daly",
                "Peter T. Pham",
                "Dan Huang",
                "Andrew Y. Ng",
                "Christopher Potts."
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human",
            "year": 2011
        },
        {
            "authors": [
                "Swaroop Mishra",
                "Daniel Khashabi",
                "Chitta Baral",
                "Hannaneh Hajishirzi."
            ],
            "title": "Cross-task generalization via natural language crowdsourcing instructions",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Saif Mohammad",
                "Felipe Bravo-Marquez",
                "Mohammad Salameh",
                "Svetlana Kiritchenko."
            ],
            "title": "Semeval2018 task 1: Affect in tweets",
            "venue": "Proceedings of the 12th international workshop on semantic evaluation, pages 1\u201317.",
            "year": 2018
        },
        {
            "authors": [
                "Yixin Nie",
                "Adina Williams",
                "Emily Dinan",
                "Mohit Bansal",
                "Jason Weston",
                "Douwe Kiela."
            ],
            "title": "Adversarial nli: A new benchmark for natural language understanding",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for",
            "year": 2020
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Jane Pan",
                "Tianyu Gao",
                "Howard Chen",
                "Danqi Chen"
            ],
            "title": "What in-context learning\"learns\"in-context: Disentangling task recognition and task learning",
            "year": 2023
        },
        {
            "authors": [
                "Ethan Perez",
                "Douwe Kiela",
                "Kyunghyun Cho."
            ],
            "title": "True few-shot learning with language models",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2021
        },
        {
            "authors": [
                "Ohad Rubin",
                "Jonathan Herzig",
                "Jonathan Berant."
            ],
            "title": "Learning to retrieve prompts for in-context learning",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2022
        },
        {
            "authors": [
                "del Moral",
                "Olatunji Ruwase",
                "Rachel Bawden",
                "Stas Bekman"
            ],
            "title": "2022a. Bloom: A 176b-parameter open-access multilingual language model. ArXiv, abs/2211.05100",
            "year": 2022
        },
        {
            "authors": [
                "fan Xu",
                "Yingxin Xu",
                "Yu Xu",
                "Zhe Tan",
                "Zhongli Xie",
                "Zifan Ye",
                "Mathilde Bras",
                "Younes Belkada",
                "Thomas Wolf"
            ],
            "title": "2022b. Bloom: A 176b-parameter open-access multilingual language model",
            "year": 2022
        },
        {
            "authors": [
                "Aarohi Srivastava",
                "Abhinav Rastogi",
                "Abhishek Rao",
                "Abu Awal Md Shoeb",
                "Abubakar Abid",
                "Adam Fisch",
                "Adam R Brown",
                "Adam Santoro",
                "Aditya Gupta",
                "Adri\u00e0 Garriga-Alonso"
            ],
            "title": "Beyond the imitation game: Quantifying and extrapolating",
            "year": 2022
        },
        {
            "authors": [
                "Hongjin Su",
                "Jungo Kasai",
                "Chen Henry Wu",
                "Weijia Shi",
                "Tianlu Wang",
                "Jiayi Xin",
                "Rui Zhang",
                "Mari Ostendorf",
                "Luke Zettlemoyer",
                "Noah A. Smith",
                "Tao Yu."
            ],
            "title": "Selective annotation makes language models better few-shot learners",
            "venue": "ArXiv, abs/2209.01975.",
            "year": 2022
        },
        {
            "authors": [
                "Xinyi Wang",
                "Wanrong Zhu",
                "William Yang Wang."
            ],
            "title": "Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning",
            "venue": "ArXiv, abs/2301.11916.",
            "year": 2023
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Chi",
                "Sharan Narang",
                "Aakanksha Chowdhery",
                "Denny Zhou"
            ],
            "title": "2023b. Self-consistency improves chain of thought reasoning in language models",
            "year": 2023
        },
        {
            "authors": [
                "Albert Webson",
                "Ellie Pavlick"
            ],
            "title": "Do promptbased models really understand the meaning of their prompts",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2022
        },
        {
            "authors": [
                "Jerry W. Wei",
                "Jason Wei",
                "Yi Tay",
                "Dustin Tran",
                "Albert Webson",
                "Yifeng Lu",
                "Xinyun Chen",
                "Hanxiao Liu",
                "Da Huang",
                "Denny Zhou",
                "Tengyu Ma."
            ],
            "title": "Larger language models do in-context learning differently",
            "venue": "ArXiv, abs/2303.03846.",
            "year": 2023
        },
        {
            "authors": [
                "Benfeng Xu",
                "Quan Wang",
                "Zhendong Mao",
                "Yajuan Lyu",
                "Qiaoqiao She",
                "Yongdong Zhang"
            ],
            "title": "2023. knn prompting: Beyond-context learning with calibrationfree nearest neighbor inference",
            "year": 2023
        },
        {
            "authors": [
                "Sohee Yang",
                "Jonghyeon Kim",
                "Joel Jang",
                "Seonghyeon Ye",
                "Hyunji Lee",
                "Minjoon Seo"
            ],
            "title": "Improving probability-based prompt selection through unified evaluation and analysis",
            "year": 2023
        },
        {
            "authors": [
                "Shunyu Yao",
                "Jeffrey Zhao",
                "Dian Yu",
                "Nan Du",
                "Izhak Shafran",
                "Karthik Narasimhan",
                "Yuan Cao"
            ],
            "title": "React: Synergizing reasoning and acting in language models",
            "year": 2023
        },
        {
            "authors": [
                "Rowan Zellers",
                "Ari Holtzman",
                "Yonatan Bisk",
                "Ali Farhadi",
                "Yejin Choi"
            ],
            "title": "Hellaswag: Can a machine really finish your sentence",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
            "year": 2019
        },
        {
            "authors": [
                "Wang",
                "Luke Zettlemoyer."
            ],
            "title": "Opt: Open pre-trained transformer language models",
            "venue": "ArXiv, abs/2205.01068.",
            "year": 2022
        },
        {
            "authors": [
                "Wang",
                "Luke Zettlemoyer"
            ],
            "title": "2022b. Opt: Open pre-trained transformer language models",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Jake Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "NIPS.",
            "year": 2015
        },
        {
            "authors": [
                "Tony Z. Zhao",
                "Eric Wallace",
                "Shi Feng",
                "Dan Klein",
                "Sameer Singh"
            ],
            "title": "Calibrate before use: Improving few-shot performance of language models",
            "year": 2021
        },
        {
            "authors": [
                "Yongchao Zhou",
                "Andrei Ioan Muresanu",
                "Ziwen Han",
                "Keiran Paster",
                "Silviu Pitis",
                "Harris Chan",
                "Jimmy Ba"
            ],
            "title": "Large language models are human-level prompt engineers",
            "year": 2022
        },
        {
            "authors": [
                "Zhou"
            ],
            "title": "2022) to generate instructions for each of the tasks we consider",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "One of the most effective insights in NLP research in recent years has been that large language models trained to perform next-token prediction show emergent in-context learning (ICL) abilities (Brown et al., 2020; Scao et al., 2022a; Zhang et al., 2022a). While the bulk of research interest has shifted away from task-specific models and towards creating \u201cfoundation models\" to perform a variety of tasks using appropriately constructed\n*Equal contribution 1Code: https://github.com/princeton-nlp/\nInstructEval\nprompts, the performance of ICL remains sensitive to the precise details of prompt construction. Prompt engineering remains critical for achieving optimal ICL performance (Perez et al., 2021; Zhao et al., 2021; Webson and Pavlick, 2022).\nIn practice, ICL typically involves prompting a language model using a concatenation of a taskspecific instruction, a short sequence of annotated in-context examples known as demonstrations, and a test example (Figure 2). Much of the research interest surrounding in-context learning has focused on understanding the optimal selection, ordering of demonstrations, and label-space choices (Liu et al., 2021a; Su et al., 2022; Rubin et al., 2022; Wang et al., 2023a; Lu et al., 2021a; Wei et al., 2023; Pan et al., 2023). However, instruction choice remains\nar X\niv :2\n30 7.\n00 25\n9v 2\n[ cs\n.C L\n] 1\n6 Ju\nl 2 02\n3\na relatively underexplored aspect of prompt engineering despite its established significance (Mishra et al., 2022) on downstream performance.\nEven among recent works exploring automatic instruction selection (Honovich et al., 2022; Gonen et al., 2022; Deng et al., 2022; Zhou et al., 2022), the use of different evaluation protocols makes the comparison of their relative performances difficult. Existing studies typically limit their analyses to specific models or tasks; for example, Zhou et al. (2022) focus on a single model, and while Deng et al. (2022) consider multiple model scales, they all belong to a single model family. Moreover, evaluations often span disparate task selections with minimal overlap and are primarily dominated by classification tasks, neglecting other task types like multiple-choice QA or generation. Lastly, most previous works tend to emphasize zero-shot accuracy, overlooking other pertinent ICL metrics such as few-shot accuracy and robustness measures.\nTo address these issues, we build InstructEval, an evaluation suite for the comprehensive evaluation of instruction selection methods. The suite covers a diverse collection of 13 open-sourced autoregressive LLMs from four model families and nine tasks spanning three task types. Additionally, it also incorporates three accuracy metrics and two sensitivity metrics that are of interest to ICL. We perform evaluations of seven popular instruction selection methods including trivial instruction baselines, manually curated instructions, and sophisticated automatic methods using our suite.\nOverall, we find that the relative effectiveness of these approaches varies significantly across different models and task types. We discover that curated manually-written instructions and task-agnostic instructions can elicit better aggregated performance (over models) than automatically induced ones, highlighting the lack of generalizability of the latter. We also find that including instructions in few-shot prompts usually tends to hurt ICL performance at the model scales we consider. Our findings suggest that it may be optimal for ICL practitioners to omit instructions in few-shot settings and use curated manually-written instructions in zero-shot settings, rather than contemporary automatic induction techniques that require substantial computation and hyperparameter tuning to achieve competitive performance. We release the evaluation suite we develop to aid the systematic study of even more questions regarding prompt engineering that we do\nnot explicitly address in our work."
        },
        {
            "heading": "2 Related Work",
            "text": "In-Context Learning and Existing Benchmarks As language models have scaled, in-context learning has emerged as a popular paradigm and remains ubiquitous among several autoregressive LLM families (Brown et al., 2020; Touvron et al., 2023; Scao et al., 2022b; Black et al., 2021; Zhang et al., 2022b). Benchmarks like BigBench (Srivastava et al., 2022) and HELM (Liang et al., 2022) have been created for the holistic evaluation of these models. BigBench focuses on few-shot abilities of state-of-the-art large language models, while HELM extends to consider metrics like robustness and bias. However, these benchmarks focus on evaluating and ranking language models, and do not address the systematic evaluation of prompting methods. Although contemporary work by Yang et al. (2023) also aims to perform a similar systematic analysis of prompting methods, they focus on simple probability-based prompt selection while we evaluate a broader range of methods including trivial instruction baselines, curated manually selected instructions, and sophisticated automated instruction selection.\nAutomated Prompt Engineering Methods There has been interest in performing automated prompt-engineering for target downstream tasks within ICL. This has led to the exploration of various prompting methods, ranging from simple heuristics such as selecting instructions with the lowest perplexity (Gonen et al., 2022), inducing instructions from large language models using a few annotated input-output pairs (Zhou et al., 2022), to utilizing RL objectives to create discrete token sequences as prompts (Deng et al., 2022). However, these works restrict their evaluation to small sets of models and tasks with little intersection, hindering their objective comparison.\nUnderstanding in-context learning There has been much recent work attempting to understand the mechanisms that drive in-context learning. Studies have found that the selection of demonstrations included in prompts significantly impacts fewshot accuracy across most tasks (Liu et al., 2021b; Agrawal et al., 2022; Xu et al., 2023). Works like (Lu et al., 2021b) also show that altering the ordering of a fixed set of demonstrations can affect downstream accuracy. Prompts sensitive to demon-\nstration permutation often exhibit lower accuracies (Chen et al., 2023), making them less reliable, particularly in low-resource domains.\nOur work aims to bridge these gaps by systematically evaluating the efficacy of popular instruction selection approaches over a diverse set of tasks and models, facilitating objective comparison. We evaluate these methods not only on accuracy metrics, but also on sensitivity metrics to glean additional insights. We recognize that other facets of prompting not covered by instruction engineering exist (Wei et al.; Yao et al., 2023; Wang et al., 2023b), and defer these explorations to future work."
        },
        {
            "heading": "3 Evaluation Suite",
            "text": ""
        },
        {
            "heading": "3.1 Prompt format",
            "text": "We define a \u2018prompt\u2019 as the full textual input provided to an LLM. Our evaluation suite supports the use of any number of demonstrations, arbitrary demonstration templates and the inclusion of custom strings anywhere within the prompt. Since the instructions used can be set to any arbitrary strings, users are free to use any external means to select instructions and have them evaluated by our suite.\nFor consistency, we conduct all experiments in this work using prompts that begin with an instruction, continue with a sequence of annotated training\ndemonstrations, and conclude with an unsolved test example2 (Figure 2), and express each example in a minimal, task-specific key-value format (Table 8) that reflects task semantics."
        },
        {
            "heading": "3.2 Metrics",
            "text": "Accuracy metrics Accuracy is typically the primary metric of interest in ICL. While ICL is most commonly performed in few-shot settings where a handful of annotated demonstrations are included in the prompt, models are also prompted zero-shot without the use of such demonstrations. Since realworld scenarios can often contain grammatical errors and misspellings in the test input, it is desirable to find prompts robust to these perturbations. Hence, we measure zero-shot accuracy, few-shot accuracy, and perturbation accuracy3 in our evaluations. Following Liang et al. (2022), we measure perturbation accuracy by introducing random capitalization, spacing, contractions and common misspellings in the test input.\nSensitivity metrics Previous work has shown that the accuracy obtained using a prompt template can fluctuate significantly as a function of the set of demonstrations included in the prompt (Liu et al., 2021a; Su et al., 2022; Rubin et al., 2022; Wang et al., 2023a) and the order they are presented in (Lu et al., 2021b). It may be desirable in practice to identify prompt templates and instructions that offer consistent performance regardless of the choice of demonstrations and their arrangement. Hence, we introduce selectional sensitivity and permutational sensitivity metrics to measure the sensitivity of chosen instructions respectively to selected demonstrations, and the order in which they are arranged. We quantify the sensitivity of an instruction (given a model and task) using the standard deviation of accuracies obtained on varying the selection or permutation of the demonstrations used, each across 16 random choices."
        },
        {
            "heading": "3.3 Aggregating metrics across Models",
            "text": "Each instruction selection method being tested across N models and M datasets yields NM values per metric. Comparing these NM -dimensional vectors directly is complex. It can be challenging\n2Instructions are omitted during \u2018Null instruction\u2019 evaluations. Demonstrations are omitted in zero-shot evaluations.\n3We choose to treat this as an accuracy metric rather than a sensitivity metric since it is not meaningful to measure sensitivity to such perturbations in situations where a prompt only elicits near random-chance task performance from a model.\nto reduce them to a single representative scalar. Simple approaches such as computing the mean of these NM values can prove inadequate since the resulting scores would tend to be heavily influenced by metric values that exhibit a high variance across different inspected methods.\nWe opt against using aggregation techniques used by previous works (Liang et al., 2022; Srivastava et al., 2022) due to their drawbacks (Section C) and instead adopt \u2018mean relative gain\u2019 as a means to aggregate accuracy metrics across multiple models. We rely on simple averaging for sensitivity metrics, partly because we observe that these quantities do not show much variation across methods."
        },
        {
            "heading": "3.3.1 Accuracy metrics",
            "text": "Considering the range of models and datasets in our evaluation suite, we unsurprisingly observe substantial variation in accuracy magnitudes across model scales and tasks. However, we notice that the degree of variation in accuracy due to instruction choice is usually considerably smaller than the degree of variation due to model and task choice.\nTo meaningfully compare and aggregate the relative performance of different instruction selection methods across models, we use a measure called mean relative gain. First, we define the relative gain for a value x from a population P as the percentage by which x exceeds the mean value of P :\nr-gainP (x) = 100\u00d7 x\u2212 \u00b5P \u00b5P\nConsider a collection of models M and instructions I for a task t. Given a model m, we calculate the raw accuracy scores stmi for each instruction i \u2208 I. Taking this set Stm to be the population, we compare the performances of the instructions against each other by computing their correspond-\ning relative gains rtmi = r-gainStm(stmi). Each rtmi represents the degree by which method i outperforms the average performance along the metric on task t for model m.\nWe now define the mean relative gain as\nrti = 1 |M| \u2211 m\u2208M rtmi\nThese rti values, tabulated and analyzed in Section 4, capture not only the ordinal information about each method\u2019s performance on a given task but also provide an intuitive sense of the magnitude by which these methods outperform others. Specifically, if an induction method i has a mean relative gain rti on task t, this means that method i exceeds average performance (across I) on task t by rti percent when averaged across models M."
        },
        {
            "heading": "3.3.2 Sensitivity metrics",
            "text": "To aggregate the sensitivity of an instruction selection/induction method i over all models for a task t, we simply compute the average of the raw sensitivity scores (described in Section 3.2). Specifically, if \u03c3tmi is the raw sensitivity score obtained for model m and task t when using instruction i, then the aggregated sensitivity score \u03c3ti is given by\n\u03c3ti = 1 |M| \u2211 m\u2208M \u03c3tmi\nWe choose to avoid more sophisticated aggregation strategies like relative gain for sensitivity metrics since standard deviations are already secondary metrics making it unintuitive to discuss the relative gain of the standard deviation obtained using a method over the average."
        },
        {
            "heading": "3.4 Tasks",
            "text": "While previous instruction induction (Zhou et al., 2022; Deng et al., 2022) work has tended to focus mostly on classification tasks, we include 9 tasks (Table 1) in our evaluation suite spanning classification (CLS), multiple-choice question-answering (MCQ) and generative question-answering (GQA) to assess the applicability of instruction selection and induction methods to other task-types as well. We concentrate on tasks that are challenging to contemporary language models, and yet are not so demanding that the performance of these models does not exceed random chance. We exclude certain generative tasks, like summarization, which are challenging to assess objectively. 4"
        },
        {
            "heading": "3.5 Models",
            "text": "We include a diverse range of 13 autoregressive LLMs (Table 2) from 4 model families of sizes ranging from 1.1 billion to 20 billion parameters in our evaluation suite. We choose contemporary models that span different architectures and training paradigms which are known to show good ICL performance. This diversity bolsters the generalizability of insights obtained using our evaluation suite while mitigating potential bias towards any specific model family. Moreover, we select opensource models which are large enough to show non-trivial ICL performance while still being small enough to run on reasonable consumer hardware to\n4Standard summarization metrics correlate poorly with human preferences (Liang et al., 2022; Goyal et al., 2023).\nensure the practical significance of our findings."
        },
        {
            "heading": "4 Experimental setup",
            "text": "We perform experiments evaluating 3 families of instruction selection methods (listed in Table 3).\nTask-agnostic instructions In practical ICL settings, it is straightforward to use instructions that contain no task-specific information.\n\u2022 Null instruction: We assess the impact of omitting instructions from the prompt. This amounts to constructing prompts that consist of demonstrations and a test example in few-shot, and only an unanswered test-example in zero-shot settings.\n\u2022 Generic instructions: We assess the impact of using generic task-agnostic instructions such as Complete the following task:. These instructions require minimal effort to write since they do not demand knowledge of the task. We list the set of generic instructions we evaluate in Table 10.\nManual task-specific instructions We evaluate manually-written task-specific instructions that ICL practitioners may use in practice.\n\u2022 PromptSource: PromptSource (Bach et al., 2022) is a public collection of manually-curated prompt templates pertaining to 170+ datasets which are often used off-the-shelf for ICL and are generally considered high-quality.\n\u2022 Ad hoc: ICL practitioners often create taskspecific instructions ad hoc, based on the semantics of the given task. We simulate this mode of instruction selection by asking ChatGPT to generate several paraphrases of task-specific seed instructions we obtain from PromptSource and randomly sampling from the generated set.\nAutomatically synthesized task-specific instructions We evaluate 3 popular automated instruction selection and induction methods that are representative of previous work.\n\u2022 Low Perplexity: (Gonen et al., 2022) find that the perplexity a model associates with an instruction is negatively correlated with its ICL performance when using that instruction. We use the SPELL algorithm proposed by Gonen et al. (2022) to select the least perplexity instructions (for each model) from a large pool of ChatGPT paraphrased instructions.\n\u2022 APE: (Zhou et al., 2022) is an automatic few-shot method for inducing instructions by prompting a language model to describe the given task, and refining the set of generated prompts using accuracy on a small held-out validation set. While Zhou et al. (2022) limit their evaluation to GPT3 (Brown et al., 2020) and InstructGPT (Ouyang et al., 2022), we assess APE\u2019s applicability to a significantly larger set of models and tasks.\n\u2022 RLPrompt (Deng et al., 2022) is a reinforcement-learning-based approach for few-shot prompt induction. While the original authors only evaluate their method using GPT-2 on a few classification tasks, we expand this assessment to many more models and tasks. Notably, we assess the extensibility of RLPrompt to MCQ tasks, but do not test RLPrompt performance on GQA tasks since the algorithm is not directly applicable to generation tasks."
        },
        {
            "heading": "5 Results",
            "text": "We tabulate the mean relative gain values over accuracy metrics in Table 4, and the mean standard deviations corresponding to selectional and permutational sensitivity metrics in Table 5."
        },
        {
            "heading": "5.1 Less sophisticated instruction selection methods tend to show higher accuracy",
            "text": "We find that task-agnostic instructions dominate in few-shot settings with Null instructions and Generic instructions achieving the highest aggregated performance in 5/9 tasks for few-shot accuracy and 6/9 tasks for perturbation accuracy. Although both these methods show above-average performance in few-shot settings, Null instructions tend to perform better among the two.\nAlthough PromptSource instructions only show an average performance in few-shot settings, their manually curated task-specific instructions prove most effective in zero-shot settings, achieving the highest aggregated performance in 6/9 tasks\nand usually achieving markedly higher mean relative gain values than even the runner-up method for the task. This is especially true of GQA tasks where PromptSource instructions outperform the average by >17%.\nAutomatic task-specific instructions are usually outperformed by simple baselines. They fail to achieve the best zero-shot performance on any task we consider. While they do sometimes perform competitively with simpler baselines in the few-shot setting, emerging as the best-performing instructions in 2/9 tasks, this behavior is inconsistent. Although Low Perplexity instructions and APE instructions seldom show above-average performance in either setting, RLPrompt instructions show above-average performance in 5/7 tasks in both settings. They are still usually outperformed by instructions obtained through simpler means such as Null and PromptSource instructions."
        },
        {
            "heading": "5.2 Ranges of variation of aggregated scores",
            "text": "We notice that instructions have a more significant impact in zero-shot settings as compared to few-shot settings. For most tasks, we find that the highest mean relative gain values achieved in the zero-shot setting are markedly greater than those in the few-shot setting. Accordingly, the minimum values for each task are also relatively lower in zero-shot settings. This finding suggests that instructions play a significant role in informing models of semantics in zero-shot settings whereas in few-shot settings, most of a model\u2019s understanding of task-semantics comes from the demonstrations.\nThe degree of variation in accuracy due to instruction choice varies considerably across tasks. AG News and Emotion show the highest variability in few-shot performance while GQA tasks show the most variability in zero-shot settings.\nTable 5 shows that selectional and permutational sensitivities vary dramatically across tasks even though they are roughly consistent across all methods for a given task implying that all the methods we evaluate are comparable in sensitivity, which is unsurprising since none of them explicitly optimize for it. We also find that most methods show comparable, but usually lower permutational sensitivity than selectional sensitivity across all tasks."
        },
        {
            "heading": "5.3 Analysis",
            "text": "We tabulate the mean relative gain values for zeroshot and few-shot accuracies computed separately for \u201csmall\" models with < 6 billion parameters and\n\u201clarge\" models with \u2265 6 billion parameters in Table 6. For ease of comparison, we average the mean relative gain values thus obtained by task-type. Although the observations that PromptSource and task-agnostic instructions tend to perform the best across zero- and few-shot settings persist across model scales, we find that the ranges of variation in the few-shot mean relative gain values for large models are consistently smaller than those for small models for every task-type. This suggests that large models are able to grasp task semantics from demonstrations (when provided) while small models are more sensitive to the instruction used."
        },
        {
            "heading": "5.4 Discussion",
            "text": "Our findings reveal that in practical in-context learning settings, simpler prompting methods, such as task-agnostic or expert manually written instructions, often outperform automatically synthesized ones at the model scales we consider. Task-agnostic methods show strong performance in few-shot settings, whereas expert manual instructions appear crucial for achieving good zero-shot accuracy. The superiority of these straightforward methods over automatically induced instructions, which are often not competitive even with simple baselines, suggests a lack of transferability and generalizability among automatic induction methods. The competitive performance of automatic induction methods like APE and RLPrompt as reported by their au-\nthors implies either a limitation in their generalizability to a broader range of models and tasks, or the need for substantial hyperparameter tuning to get them to work well across models and tasks.\nOur findings suggest that ICL practitioners may often be better off forgoing computationally expensive instruction induction or selection methods in favor of task-agnostic or manually written instructions, which seem to generalize better. Interestingly, we also find that methods that excel for one model and task do not necessarily also perform well for other tasks and models. Consequently, ICL practitioners may be forced to experiment with various instruction selection methods on a model- and task-specific basis in a manner reminiscent of hyperparameter tuning to find the best choice.\nOn the other hand, since few-shot ICL performance remains largely consistent regardless of the choice of instruction, practitioners could perhaps benefit from simply providing a few in-context demonstrations when available. The fact that null instructions tend to outperform all other methods in our study in few-shot settings suggests that it can be challenging to find instructions that reliably inform diverse models about task semantics. When models fail to grasp the semantics signaled by instructions, these may simply serve as a source of noise, hence impairing ICL performance.\nOur findings underscore a broader issue regarding the inconsistent and often insufficient evaluation of instruction selection and induction techniques. We call for more comprehensive evaluations in this space and encourage the use of our evaluation suite to facilitate this process."
        },
        {
            "heading": "6 Conclusion",
            "text": "We conduct the broadest attempt to our knowledge, to systematically study the generalizability of popular instruction selection and induction methods for ICL in LLMs. We find that simpler approaches such as using task-agnostic instructions, expert manual instructions, or even omitting instructions entirely tend to show good performance more consistently when evaluating across a wide variety of tasks and models. Our work indicates the need for more systematic and consistent evaluations in the instruction induction space. To facilitate such analyses, we release the InstructEval suite which provides coverage over 13 diverse autoregressive LLMs and 9 tasks spanning classification, multiplechoice QA, and generative QA."
        },
        {
            "heading": "C Drawbacks of aggregation techniques used in previous work",
            "text": "Some previous works like the HELM (Liang et al., 2022) benchmark also face similar challenges when attempting to compare high-dimensional vectors \u2013 each representing a model evaluated over a variety of tasks \u2013 against each other. HELM resorts to scoring models using head-to-head win rates. The win rate associated with a model indicates the fraction of head-to-head comparisons between the given model and all other models, across all scenarios, where the given model performs better along a specific metric. A notable disadvantage of this scoring technique is that it obscures the magnitude of variation in the metric associated with each test model and only conveys ordinal information about the relative performances of each model. This char-\nGeneric Instructions\nSolve the following task: Find the answer below: Complete the problem.\nFind the best solution to the question below: Complete the question below:\nTable 10: Sample generic instructions\nacteristic of head-to-head win rates makes them unsuitable for spotting broad trends across families of prompting methods.\nIn other works like BIG-bench (Srivastava et al., 2022), raw metric scores representing task performance are normalized to vary from a range of 0-100 such that a normalized score of 0 corresponds to poor performance, while a normalized score of 100 corresponds to excellent performance on the task. This is done in an attempt to be able to compare the performance of a model across a variety of tasks of varying difficulty such that the normalization proves more forgiving on difficult tasks. While this score does capture cardinal information associated with the underlying variable, it relies on the knowledge of human experts to determine raw score thresholds that constitute poor or excellent performance along a given metric. To apply such a normalization scheme in our case, one would need access to a large array of such threshold scores corresponding to each model scale, task, and metric we consider. Obtaining such threshold scores across all our settings is challenging given the number of tests we perform and the variety of metrics we consider. Hence, this type of normalization proves infeasible in our case."
        }
    ],
    "title": "InstructEval: Systematic Evaluation of Instruction Selection Methods",
    "year": 2023
}