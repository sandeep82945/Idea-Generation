{
    "abstractText": "License plates typically have unique color, size, and shape characteristics in each country. This paper presents a general method for character extraction and pattern matching in license plate recognition systems. The proposed method is based on a combination of morphological operations and edge detection techniques, along with the bounding box method for identifying and revealing license plate characters while removing unwanted artifacts such as dust and fog. The mathematical model of foggy images is presented and the sum of gradients of the image, which represents the visibility of the image, is improved. Previous works on license plate recognition have utilized nonintelligent pattern matching techniques. The proposed technique can be applied in a variety of settings, including traffic monitoring, parking management, and law enforcement, among others. The applied algorithm, unlike SOTA-based methods, does not need a huge set of training data and is implemented only by applying standard templates. The main advantages of the proposed algorithm are the lack of a need for a training set, the high speed of the training process, the ability to respond to different standards, the high response speed, and higher accuracy compared to similar tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Saman Rajebi"
        },
        {
            "affiliations": [],
            "name": "Siamak Pedrammehr"
        },
        {
            "affiliations": [],
            "name": "Reza Mohajerpoor"
        }
    ],
    "id": "SP:c6a9c2892cca64d95b49c1322de16b67a46bbd0f",
    "references": [
        {
            "authors": [
                "V. Tsakanikas",
                "T. Dagiuklas"
            ],
            "title": "Video surveillance systems-current status and future trends",
            "venue": "Comput. Electr. Eng",
            "year": 2018
        },
        {
            "authors": [
                "S.K. Sharma",
                "H. Phan",
                "J. Lee"
            ],
            "title": "An application study on road surface monitoring using DTW based image processing and ultrasonic sensors",
            "venue": "Appl. Sci",
            "year": 2020
        },
        {
            "authors": [
                "S.-Z. Wang",
                "H.-J. Lee"
            ],
            "title": "Detection and recognition of license plate characters with different appearances",
            "venue": "IEEE Trans. Intell. Transp. Syst. 2003,",
            "year": 2003
        },
        {
            "authors": [
                "R. Radha",
                "C.P. Sumathi"
            ],
            "title": "A novel approach to extract text from license plate of vehicle",
            "venue": "Signal Process. Image Commun",
            "year": 2012
        },
        {
            "authors": [
                "C.Y. Kuo",
                "Y.R. Lu",
                "S.M. Yang"
            ],
            "title": "On the image sensor processing for lane detection and control in vehicle lane keeping systems",
            "venue": "Sensors 2019,",
            "year": 2019
        },
        {
            "authors": [
                "K. Zheng",
                "M. Wei",
                "G. Sun",
                "B. Anas",
                "Y. Li"
            ],
            "title": "Using vehicle synthesis generative adversarial networks to improve vehicle detection in remote sensing images",
            "venue": "ISPRS Int. J. Geo-Inf. 2019,",
            "year": 2019
        },
        {
            "authors": [
                "H.K. Sulehria",
                "Y. Zhang",
                "D. Irfan",
                "A.K. Sulehria"
            ],
            "title": "Vehicle number plate recognition using mathematical morphology and neural networks",
            "venue": "WSEAS Trans. Comput. 2008,",
            "year": 2008
        },
        {
            "authors": [
                "A. Aggarwal",
                "A. Rani",
                "M. Kumar"
            ],
            "title": "A robust method to authenticate car license plates using segmentation and ROI based approach. Smart Sustain",
            "venue": "Built Environ",
            "year": 2020
        },
        {
            "authors": [
                "T.-G. Kim",
                "B.-J. Yun",
                "T.-H. Kim",
                "J.-Y. Lee",
                "K.-H. Park",
                "Y. Jeong",
                "H.D. Kim"
            ],
            "title": "Recognition of vehicle license plates based on image processing",
            "venue": "Appl. Sci",
            "year": 2021
        },
        {
            "authors": [
                "E. Walia",
                "A. Verma"
            ],
            "title": "Vehicle number plate detection using sobel edge detection technique",
            "venue": "Int. J. Comput. Technol. 2010,",
            "year": 2010
        },
        {
            "authors": [
                "K. Parasuraman",
                "P.V. Kumar"
            ],
            "title": "An efficient method for indian vehicle license plate extraction and character segmentation",
            "venue": "In Proceedings of the IEEE International Conference on Computational Intelligence and Computing Research, Coimbatore,",
            "year": 2010
        },
        {
            "authors": [
                "G.C. Lekhana",
                "R. Srikantaswamy"
            ],
            "title": "Real time license plate recognition system",
            "venue": "Int. J. Eng. Res. Technol. 2012,",
            "year": 2012
        },
        {
            "authors": [
                "A. Kamilaris",
                "F. Prenafeta-Bold\u00fa"
            ],
            "title": "A review of the use of convolutional neural networks in agriculture",
            "venue": "J. Agric. Sci",
            "year": 2018
        },
        {
            "authors": [
                "J.J. Hopfield"
            ],
            "title": "Neural networks and physical systems with emergent collective computational abilities",
            "venue": "Proc. Natl. Acad. Sci. USA",
            "year": 1982
        },
        {
            "authors": [
                "C.-J. Lin",
                "C.-C. Chuang",
                "H.-Y. Lin"
            ],
            "title": "Edge-AI-based real-time automated license plate recognition",
            "venue": "System. Appl. Sci. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "U. Yousaf",
                "A. Khan",
                "H. Ali",
                "F.G. Khan",
                "Z.U. Rehman",
                "S. Shah",
                "F. Ali",
                "S. Pack",
                "S. Ali"
            ],
            "title": "A deep learning based approach for localization and recognition of Pakistani vehicle license plates",
            "year": 2021
        },
        {
            "authors": [
                "S.-H. Park",
                "S.-B. Yu",
                "J.-A. Kim",
                "H. Yoon"
            ],
            "title": "An All-in-one vehicle type and license plate recognition system using YOLOv4",
            "venue": "Sensors 2022,",
            "year": 2022
        },
        {
            "authors": [
                "H. Wang",
                "Y. Li",
                "L.-M. Dang",
                "H. Moon"
            ],
            "title": "Robust korean license plate recognition based on deep neural networks. Sensors",
            "year": 2021
        },
        {
            "authors": [
                "R. Fattal"
            ],
            "title": "Single image dehazing",
            "venue": "ACM Trans. Graph",
            "year": 2008
        },
        {
            "authors": [
                "S.G. Narasimhan",
                "S.K. Nayar"
            ],
            "title": "Chromatic frame work for vision in bad weather",
            "venue": "In Proceedings of the Conference on Computer Vision and Pattern Recognition, Hilton Head, SC, USA,",
            "year": 2000
        },
        {
            "authors": [
                "S.G. Narasimhan",
                "S.K. Nayar"
            ],
            "title": "Vision and the atmosphere",
            "venue": "Int. J. Comput. Vis",
            "year": 2002
        },
        {
            "authors": [
                "R.T. Tan"
            ],
            "title": "Visibility in bad weather from a single image",
            "venue": "In Proceedings of the 2008 IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2008
        },
        {
            "authors": [
                "A. Levin",
                "D. Lischinski",
                "Y. Weiss"
            ],
            "title": "A closed form solution to natural image matting",
            "venue": "In Proceedings of the Conference on Computer Vision and Pattern Recognition,",
            "year": 2006
        },
        {
            "authors": [
                "T. Wei",
                "D. Chen",
                "W. Zhou",
                "J. Liao",
                "H. Zhao",
                "W. Zhang",
                "N. Yu"
            ],
            "title": "Improved image matting via real-time user clicks and uncertainty estimation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Nashville, TN,",
            "year": 2021
        },
        {
            "authors": [
                "S. Cai",
                "X. Zhang",
                "H. Fan",
                "H. Huang",
                "J. Liu",
                "J. Wang",
                "J. Sun"
            ],
            "title": "Disentangled image matting",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), Seoul, Republic of Korea,",
            "year": 2019
        },
        {
            "authors": [
                "Y. Aksoy",
                "T.O. Aydin",
                "M. Pollefeys"
            ],
            "title": "Designing effective inter-pixel information flow for natural image matting",
            "venue": "In Proceedings of the Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Z. Xu",
                "W. Yang",
                "A. Meng",
                "N. Lu",
                "H. Huang",
                "C. Ying",
                "L. Huang"
            ],
            "title": "Towards end-to-end license Plate detection and recognition: A large dataset and baseline",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV), Munich,",
            "year": 2018
        },
        {
            "authors": [
                "A. Menon",
                "B. Omman"
            ],
            "title": "Detection and recognition of multiple license plate from still images",
            "venue": "In Proceedings of the 2018 International Conference on Circuits and Systems in Digital Enterprise Technology (ICCSDET), Kottayam, India,",
            "year": 2018
        },
        {
            "authors": [
                "W. Wang",
                "J. Yang",
                "M. Chen",
                "P. Wang"
            ],
            "title": "A light CNN for end-to-end car license plates detection and recognition",
            "venue": "IEEE Access 2019,",
            "year": 2019
        },
        {
            "authors": [
                "M.N. Murty",
                "V.S. Devi"
            ],
            "title": "Pattern Recognition. Undergraduate Topics in Computer Science",
            "year": 2011
        },
        {
            "authors": [
                "C.H. Fung",
                "M.S. Wong",
                "P.W. Chan"
            ],
            "title": "Spatio-temporal data fusion for satellite images using Hopfield neural network",
            "venue": "Remote Sens. 2019,",
            "year": 2077
        },
        {
            "authors": [
                "X. Xu",
                "S. Chen"
            ],
            "title": "An optical image encryption method using Hopfield neural network",
            "venue": "Entropy 2022,",
            "year": 2022
        },
        {
            "authors": [
                "S.Z. Mohd Jamaludin",
                "M.S. Mohd Kasihmuddin",
                "A.I. Md Ismail",
                "M.A. Mansor",
                "M.F. Md Basir"
            ],
            "title": "Energy based logic mining analysis with Hopfield neural network for recruitment evaluation",
            "venue": "Entropy",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Citation: Rajebi, S.; Pedrammehr, S.;\nMohajerpoor, R. A License Plate\nRecognition System with Robustness\nagainst Adverse Environmental\nConditions Using Hopfield\u2019s Neural\nNetwork. Axioms 2023, 12, 424.\nhttps://doi.org/10.3390/\naxioms12050424\nAcademic Editor: Palle E.T. Jorgensen\nReceived: 28 February 2023\nRevised: 17 April 2023\nAccepted: 24 April 2023\nPublished: 26 April 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: pattern recognition; image processing; independent component analysis; Hopfield\u2019s neural network; license plate; LPR\nMSC: 68T07\n1. Introduction\nNowadays, the use of surveillance-based security systems has become increasingly important in various applications, such as home security and traffic monitoring. Object detection is one of the fundamental building blocks of automated surveillance systems. Among the most used techniques for object recognition in surveillance systems is the recognition of vehicle license plates [1]. Automatic license plate recognition is an imageprocessing-based method that is used for security applications such as controlling access to restricted areas and tracking vehicles. In real-world applications, simple License Plate Recognition (LPR) systems have low detection accuracy [2]. On the one hand, the effects of external factors such as sunlight and car headlights, license plates with inappropriate designs, the wide variety of license plates and, on the other hand, the limited quality of the software and hardware related to the camera, have reduced the accuracy of these systems. However, recent advances in software and hardware have made LPR systems much safer and more widespread [3,4]. A countless number of these systems are working around the world and are growing exponentially and can do more tasks automatically in different market segments. Even if the recognition is not 100%, a results-dependent side program can compensate for the errors and provide an almost flawless system. For example, to calculate the car\u2019s parking\nAxioms 2023, 12, 424. https://doi.org/10.3390/axioms12050424 https://www.mdpi.com/journal/axioms\nAxioms 2023, 12, 424 2 of 12\ntime, from entering to leaving the parking lot, this side program can ignore some ignorable errors in the two recognitions. This intelligent integration can overcome the shortcomings of LPR and produce reliable and fully automated systems [5\u20137]. Figure 1 shows a typical configuration of an LPR system. The license plate reader software is a Windows background program on a PC and an interface between a set of cameras. The program receives the images of the cameras, and by processing them it extracts the license plates of the cars in traffic. The program then displays the results, and can also send them to other parts of the system such as a camera or LED display via serial communication. It then sends this information to the local database or external databases (through the network).\nAxioms 2023, 12, x FOR PEER REVIEW 2 of 13\nthe recognition is not 100%, a results-dependent side program can compensate for the errors and provide an almost flawless system. For example, to calculate the car\u2019s parking time, from entering to leaving the parking lot, this side program can ignore some ignorable errors in the two recognitions. This intelligent integration can overcome the shortcomings of LPR and produce reliable and fully automated systems [5\u20137].\nFigure 1 shows a typical configuration of an LPR system. The license plate reader software is a Windows background program on a PC and an interface between a set of cameras. The program receives the images of the cameras, and by processing them it extracts the license plates of the cars in traffic. The program then displays the results, and can also send them to other parts of the system such as a camera or LED display via serial communication. It then sends this information to the local database or external databases (throug th network).\nFigure 1. Typical configuration of an LPR system.\nThe first step in recognizing a car license plate is to distinguish the car from other objects in the image. For this, the methods presented in previous works can be used [8\u2013 12]. In similar works, the use of a convolutional neural network (CNN) replaces parts of the proposed method in this paper. Despite the ease of use of this new neural network, there are major disadvantages associated with CNNs. The main disadvantage is that CNNs take a much longer time to train. Another important disadvantage is the need for larger training datasets (i.e., hundreds or thousands of images), and for their proper annotation, which is a delicate procedure that must be performed by domain experts. Other disadvantages include problems that might occur when using pretrained models on similar and smaller datasets (i.e., of a few hundreds of images, or smaller), optimization issues due to model complexities, and hardware restrictions [13]. However, the proposed algorithm assumes there are only cars on highways or an absence of objects corresponding to license plates in non-car elements (such as humans, etc.). This contribution can overcome the burdens that were present in previous works.\nIn the literature, the pixel-by-pixel comparison method has been used to match the segments extracted from the image with the defined standard characters. This method, in addition to the low classification accuracy, lack of identification, and removal of noise,\nFigure 1. Typical onfiguration of a LPR system.\nThe first step in recognizing a car license plate is to distinguish the car from other objects in the image. For this, the methods presented in previous works can be used [8\u201312]. In similar works, the use of a convolutional neural network (CNN) replaces parts of the proposed method in this paper. Despite the ease of use of this new neural network, there are major disadvantages associated with CNNs. The main disadvantage is that CNNs take a much longer time to train. A other important disadvantage is the need for larger tr ining datasets (i.e., hundreds or thousands of images), and for their proper annotation, which is a delicate procedure that must be performed by domain experts. Other disadvantages include problems that might occur when using pretrained models on similar and smaller datasets (i.e., of a few hundreds of images, or smaller), optimization issues due to model complexities, and hardware restrictions [13]. However, the proposed algorithm assumes there are only c rs on highways or an ab ence of objects corresponding to license plat in n n-car elements (s ch as humans, etc.). This contribution can overcom the burdens that were present in previous works. In the literature, the pixel-by-pixel comparison method has been used to match the segments extracted from the image with the defined standard characters. This method,\nAxioms 2023, 12, 424 3 of 12\nin addition to the low classification accuracy, lack of identification, and removal of noise, also requires a lot of execution time. We employed the Hopfield neural network [14] to simultaneously speed up the program\u2019s execution and increase the precision, while removing noises on the segments extracted from the image. License plate recognition systems are usually used outdoors. The presence of air pollution, fog, and other factors causes the car license plate images to become blurred. By using frequency domain techniques, it is possible to remove the side- and destructive effects of the environment on the image. In this paper, first, the location of the license plate is identified, and then the disturbing effects of the environment are removed. After extracting the license plate image\u2019s segments, the Hopfield neural network classifies these segments to corresponding defined characters.\n2. Methodology and Simulation Results\nFigure 2 shows the five main steps of license plate recognition. In this structure, the steps for image scheduling, camera settings, and the saving and transferring of results have been ignored. In the following, each of these five steps will be explained in full detail.\nAxioms 2023, 12, x FOR PEER REVIEW 3 of 13\nalso requires a lot of execution time. We employed the Hopfield neural network [14] to simultaneously speed up the program\u2019s execution and increase the precision, while removing noises on the segments extracted from the image. License plate recognition systems are usually used outdoors. The presence f air pollution, f g, and ther factors causes the car license plate i ages to become blurr d. By using f quency domain techniques, it is possible to remove the side- and destructive effects of the environment on the image. In this paper, first, the location of the license plate is identified, and then the disturbing effects of the environment are removed. After extracting the license plate image\u2019s segments, the Hopfield neural network classifies these segments to corresponding defin d characters.\n2. Methodology and Simulation Results Figure 2 shows the five main steps of license plate recognition. In this structure, the\nsteps for image scheduling, camera settings, a d the saving and transferring of results have been ig or d. In the following, each of these fiv steps will be explained in full detail.\nFigure 2. The proposed five main steps of license plate recognition.\n2.1. Preprocessing of the Image In tasks based on image processing, such as car license plate recognition [15\u201318] or\neye tracking, etc., the first step is usually to determine the approximate location of the target object. For this, a large number of different cars with different license plate locations were studied. All the studied cars were photographed at the same distance and angle. In all these photos, the place of installation of the license plates was marked. Figure 3 shows the border of the area where it was possible for a license plate to exist, taking a suitable tolerance.\nFigure 2. The prop sed five main steps of license plate r cognition.\n2.1. Preprocessing of the Image\nIn tasks based on image processing, such as car license plate recognition [15\u201318] or eye tracking, etc., the first step is usually to determine the approximate location of the target obj ct. For this a large number of different cars with ifferent license plate l cations were studied. All the studied cars were photographe at the same distance and angle. In all these photos, the place of installation of the license plates was marked. Figure 3 shows the border of the area where it was possible for a license plate to exist, taking a suitable tolerance.\nAxioms 2023, 12, 424 4 of 12Axioms 2023, 12, x FOR PEER REVIEW 4 of 13\nFigure 3. The border of the area where it is possible for a license plate to exist.\nThe area outside the border of the license plate\u2019s zone (which has non-useful information) will be affected by the Blur filter. Applying this filter reduces the calculations and the possibility of errors in future processing. Figure 4 shows a typical image affected by this filter, where non-useful areas have been blurred.\nFigure 4. A typical image affected by Blur filter; non-useful areas have been blurred.\n2.2. Elimination of Adverse Environmental Effects Before determining the exact location of the license plate, and then its characters\u2019 segments, the adverse effects of the environment, such as the effects of possible fog or smoke in the space between the camera and the license plate, should be corrected as much as possible. Equation (1) shows a blurry image relation [19\u201324]:\n( ) ( ) ( ) (1 ( ))I x J x t x A t x= + \u2212 (1)\nwhere I is the intensity of the light in the image, J is the illumination of the scene, A is the general light of the environment, and t is a parameter that describes the part of the light that was not scattered and reached the camera. The elimination of adverse environmental effects means recovering J , A , and t from I. The term ( ) ( )J x t x in this equation is called direct attenuation, which describes the brightness of the scene and its decay in the environment. The term (1 ( ))A t x\u2212 is called ambient light, which comes from the previously scattered light and leads to a change in the color of the environment. When the space is homogeneous, the transfer coefficient t is described as follows:\nFigure 3. The border of the area where it is possible for a license plate to exist.\nThe area outside the borde of the license plat \u2019s zone (which has non-useful i format on) will be affected by the Blur filter. Applying this filter reduc s the calculations and the possibility of errors in future processing. Figure 4 shows a typical image affected by this filter, wher non-useful areas have been blurr d.\nAxioms 2023, 12, x FOR PEER REVIEW 4 of 13\nFigure 3. The border of the area where it is possible for a license plate to exist.\nThe area outside the border of the license plate\u2019s zone (which has non-useful information) will be affected by the Blur filter. Applying this filter reduces the calculations and the possibility of errors in future processing. Figure 4 shows a typical image affected by this filter, where non-useful areas have been blurred.\nFigure 4. A typical image affected by Blur filter; non-useful areas have been blurred.\n2.2. Elimination of Adverse Environmental Effects Before determining the exact location of the license plate, and then its characters\u2019 segments, the adverse effects of the environment, such as the effects of possible fog or smoke in the space between the camera and the license plate, should be corrected as much as possible. Equation (1) shows a blurry image relation [19\u201324]:\n( ) ( ) ( ) (1 ( ))I x J x t x A t x= + \u2212 (1)\nwhere I is the intensity of the light in the image, J is the illumination of the scene, A is the general light of the environment, and t is a parameter that describes the part of the light that was not scattered and reached the camera. The elimination of adverse environmental effects means recovering J , A , and t from I. The term ( ) ( )J x t x in this equation is called direct attenuation, which describes the brightness of the scene and its decay in the environment. The term (1 ( ))A t x\u2212 is called ambient light, which comes from the previously scattered light and leads to a change in the color of the environment. hen the space is homogeneous, the transfer coefficient t is described as follows:\nFigure 4. A typical image affected by Blur filter; non-useful areas have been blurred.\n2.2. Elimination of Adverse Environmental Effects\nBefore deter ining the exact location of the license plate, and then its characters\u2019 seg ents, the adverse effects of the environ ent, such as the effects of pos ible fog or s oke in the space bet e n the ca era and the license plate, should be cor ected as uch as pos ible. Equation (1) sho s a blur y i age relation [19\u201324]:\nI(x) = J(x)t(x) + A(1\u2212 t(x)) (1)\nwhere I is the intensity of the light in the image, J is the illumination of the scene, A is the general light of the environment, and t is a parameter that describes the part of the light that was not scattered and reached the camera. The elimination of adverse environmental effects means recovering J, A, and t from I. The term J(x)t(x) in this equation is called direct attenuation, which describes the brightness of the scene and its decay in the environment. The term A(1\u2212 t(x)) is called ambient light, which comes from the previously scattered\nAxioms 2023, 12, 424 5 of 12\nlight and leads to a change in the color of the environment. When the space is homogeneous, the transfer coefficient t is described as follows:\nt(x) = e\u2212\u03b2 d(x) (2)\nwhere \u03b2 is the dispersion coefficient. Equation (2) clearly shows that the image brightness decreases exponentially with its depth d. Equation (1) shows that in the RGB color space, vectors A, I(x), and J(x) are coplanar while their endpoints are located on a single line. The transfer coefficient t can be expressed by\nt(x) = ||A\u2212 I(x)|| ||A\u2212 J(x)|| = Ac \u2212 Ic(x) Ac \u2212 Jc(x) (3)\nwhere c represents the index of the color channel. In blurred images, t is less than one. Thus, the resolution of the image, which is the sum of the image gradients, is low. The following illustrates this reduction:\n\u2211 t ||\u2207I(x)|| = t\u2211 t ||\u2207J(x)|| < \u2211 t ||\u2207J(x)|| (4)\nThe transmission coefficient, t, is estimated by maximizing the image resolution, while the intensity J(x) is less than the intensity A. The dark channel for a haze-free outer space image is defined as the following: in a non-sky image, at least one color channel has very low brightness in some pixels. In other words, the image brightness in these pixels is minimum. Equation (5) shows the dark channel definition:\nJdark(x) = min c\u2208{r.g.b} ( min y\u2208\u2126(x) (Jc(y))) (5)\nwhere Jc is a color channel of J and \u2126(x) is a piece of the image centered at x. If the image does not include the sky and does not have fog, the intensity of Jdark will be almost zero. Assuming the value of A for the ambient light and the constant transmission coefficient t(x) in a piece of the image, minimizing the intensity (1) gives:\nmin y\u2208\u2126(x) (Ic(x)) = t(x) min y\u2208\u2126(x) (Jc(y)) + Ac(1\u2212 t(x)) (6)\nDividing the sides of Equation (6) by A and minimizing again, this time among the color channels gives:\nmin c ( min y\u2208\u2126(x)\n( Ic(x)\nAc )) = t(x)min c ( min y\u2208\u2126(x) ( Jc(y) Ac )) + (1\u2212 t(x)) (7)\nBy approximating Equation (7) to zero, the transfer coefficient can be defined as Equation (8). The coefficient w is defined to adjust the blurring in the image.\nt(x) = 1\u2212 w min c ( min y\u2208\u2126(x)\n( Ic(x)\nAc )) (8)\nBy limiting the transmission coefficient on the limit of t0, the brightness of the image is expressed as:\nJ(x) = I(x)\u2212 A\nmax(t(x), t0) + A (9)\nAccording to the above-stated contents and the presented equations, using the algorithm shown in Figure 5 it is possible to reduce the image blurring to an acceptable level.\nAxioms 2023, 12, 424 6 of 12Axioms 2023, 12, x FOR PEER REVIEW 6 of 13\nFigure 5. Implemented algorithm for removing or reducing the image blurring.\nFigure 6 demonstrates the results of the algorithm in modifying an image that was artificially and exaggeratedly fogged.\n(a) (b)\nFigure 6. (a) Artificially and exaggeratedly fogged image; (b) corrected version of fogged image.\n2.3. Determining the Exact Location of the License Plate After removing the adverse environmental effects, according to the following two principles, the exact location of the license plate must be determined: First, due to the difference in the colors around the license plate and its background, by using edge detection on the black\u2013white image the edge of the license plate frame will appear as an edge and a closed path shape. Secondly, according to each country\u2019s standards, the lengthto-width ratio of the license plate will be a fixed value. According to the above content, all the edges on the image are detected. Detected edges become bolder to remove any interruptions in the closed paths. Then, by defining\nFig re 5. I le e te algorit for re ovi g or reducing the i age blurring.\ni e t t t res lt f the algorit in odifying an i t t s rti ci ll exa erate ly fogge .\nAxioms 2023, 12, x FOR PEER REVIEW 6 of 13\nFigure 5. Implemented algorithm for removing or reducing the image blur ing.\nFigure 6 d monstrates the r sults of i hm m i image that was artificial y and gg t dl f d.\n(a) (b)\nFigure 6. (a) Artificial y and exaggeratedly fogged image; (b) cor ected version of fogged image.\n2.3. Determining the Exact Location of the License Plate After removing the adverse environmental effects, ac ording to the fol owing two principles, the exact location of the license plate must be determined: First, due to the difference in the colors around the license plate and its background, by using edge detection on the black\u2013white image the edge of the license plate frame wil appear as an edge and a closed path shape. Secondly, ac ording to each country\u2019s standards, the lengthto-width ratio of the license plate wil be a fixed value. Ac ording to the above content, al the edges on the image are detected. Detected edges become bolder to remove any inter uptions in the closed paths. Then, by defining\nFigure 6. (a) Artificially and exaggeratedly fogged image; (b) corrected version of fogged image.\n. . i i t ti f t e ice se l te\nft r i t r ir ff , t t f ll i t ri i l , t t l ti f li l i : i t, t t iff r in the col rs around the lic nse plate and its background, by sing edge detection on the black\u2013white image the edge of th lic nse plate frame will appear as an edge nd a closed path shape. Secondly, acc r ing to each country\u2019s sta dards, the length-to-width ratio of the license plate will be a fixed value.\nAxioms 2023, 12, 424 7 of 12\nAccording to the above content, all the edges on the image are detected. Detected edges become bolder to remove any interruptions in the closed paths. Then, by defining the closed path edges in the image as objects, the one with the standard license plate\u2019s length-to-width ratio is selected as the main object (the license plate frame). Detecting the main object\u2019s position from the initial image determines the license plate frame. Figure 7 shows all the above steps on a sample image.\nAxioms 2023, 12, x FOR PEER REVIEW 7 of 13\nthe closed path edges in the image as objects, the one with the standard license plate\u2019s length-to-width ratio is selected as the main object (the license plate frame). Detecting the main object\u2019s position from the initial image determines the license plate frame. Figure 7 shows all the above steps on a sample image.\n2.4. Determining the Segments inside the Plate After cutting the image of the license plate from the original image, by applying rotation if needed, removing the unessential edges of the license plate, and turning it into the black and white mode, the segments of recognizable license plate characters are separated [25\u201329].\n2.4. Determining the Segments inside the Plate\nAfter cutting th image of th license plate rom the original image, by applying rotation if nee ed, r moving the unessential d es of the license plate, and turning it into the black and white mode, the segments of recognizable license plate characters are separated [25\u201329].\nAxioms 2023, 12, 424 8 of 12\nIn the image shown in Figure 8, from the left side, the index of the first column has at least one white pixel, labeled as the start index of the first segment. Additionally, the index of the first column without a white pixel is labeled the final index of the first segment. This process is repeated for the whole of the plate to determine all its character segments. Figure 9 shows the cut segments separately.\nAxioms 2023, 12, x FOR PEER REVIEW 8 of 13\nIn the image shown in Figure 8, from the left side, the index of the first column has at least one white pixel, labeled as the start index of the first segment. Additionally, the index of the first column without a white pixel is labeled the final index of the first segment. This process is repeated for the whole of the plate to determine all its character segments. Figure 9 shows the cut segments separately.\nFigure 8. License plate image prepared for extracting its character segments.\nFigure 9. Cut segments of the license plate\u2014black and white mode image.\n2.5. Recognizing the Segments Using Hopfield\u2019s Neural Network Determined character segments in the previous section should be recognized using standard character patterns. Diverse methods of pattern recognition include matching pixel by pixel [26], the k-nearest neighbor, and Bayesian, and various neural networks can be used [30]. The Hopfield neural network is known as the most common method for detecting patterns with binary features. Since the extracted black and white segment images have binary values (zero for black and one for white), they can be recognized using this neural network [31].\nThe main idea of the Hopfield neural network is based on state variables. If the new position of a system depends on its previous one, it can be written in terms of state variables in the form of the following equation [32\u201335]:\n( 1) ( ( ))x t f x t+ = (10)\nThe sequence above continues until its energy is exhausted, and then remains in a balanced state. The system energy should decrease to reach this state. For this purpose, as shown in Figure 10, the Hopfield neural network is designed such that, firstly, the new position of the system is dependent on its previous one, and second, its energy equation decreases.\nFigure 8. License plate image prepared for extracting its character segments.\nAxioms 2023, 12, x FOR PEER REVIEW 8 of 13\nIn the image shown in Figure 8, from the left side, the index of the first column has at least one white pixel, labeled as the start index of the first segment. A ditionally, the index of the first column without a white pixel is labeled the final index of the first seg e t. This process is repeated for the whole of the plate to determine all its character segments. Figure 9 shows the cut segments separately.\nFigure 8. License plate image prepared for extracting its character segments.\nFigure 9. Cut segments of the license plate\u2014black and white mode image.\n2.5. Recognizing the Segments Using Hopfield\u2019s Neural Network Determined character segments in the previous section should be recognized usi g standard character patterns. Diverse metho s of pattern recognition include matching pixel by pixel [26], the k-nearest neighbor, and Bayesian, and various neural networks can be used [30]. The Hopfield neural network is known as the most common method for detecting patterns with binary features. Since the extracted black and white segment images have binary values (zero for black and one for white), they can be recognized using this neural network [31].\nThe main idea of the Hopfield neural network is based on state variables. If the new position of a syste depends on its previous one, it can be written in terms of state variables in the form of the following equation [32\u201335]:\n( 1) ( ( ))x t f x t+ = (10)\nThe sequence above co tinues until its energy is exhausted, and then remains in a balanced state. The system energy should decrease to reach this state. For this purpose, as shown in Figure 10, the Ho field neural network is designed such that, firstly, the new position of the system is dependent on its previous one, and second, its energy equation decreases.\nFigure 9. Cut segments of the license plate\u2014black and white mode image.\n. . ecognizing the Segment Using Hopfield\u2019s Neural Network\netermined ch racter segm nts in the previous s tion should b recognized using standard character patt rns. Diverse methods of pattern recognition include matching pixel by pixel [26], the k-nearest neighbor, and Bayesian, and various neural networks can be used [30]. The Hopfield neural network is known as the most common method for detecting patterns with binary features. Since the extracted black and white segment images have binary values (zero for black and one for white), they can be recognized using this neural network [31]. The main idea of the Hopfield neural network is based on state variables. If the new position of a syste dep nds on its previ us one, it can be written in terms of state variables in the form of the following equation [32\u201335]:\nx(t + 1) = f (x(t)) (10)\nThe sequ nce above conti ues u til its energy is exhausted and then r mains in a balanced state. The system energy should decrease to r ach thi state. For this purpose, as shown in Figure 10, the Hopfield neural network is designed such that, firstly, the new position of the system is dependent on its previous one, and second, its energy equation decreases. Axioms 2023, 12, x FOR PEER REVIEW 9 of 13\nwhere \u03d5 is the function of the neurons in the network and inet is the output of the ith neuron. According to Equation (11), the energy gradient of the system decreases if the derivative of the neuron function is positive. Choosing sign function as a function of the neuron can meet this condition. When Hopfield\u2019s neural network is used as an image classifier, the two-dimensional images should be mapped to one-dimensional mode, and the black pixel values set to \u22121 (while the value of the white pixels are +1).\nThe segments which are in black and white format are resized to a standard size. The matrix of each segment, which is two-dimensional, is transformed into a one-column vector. The black pixels marked with 0 in this matrix are changed to \u22121 and then applied to the Hopfield neural network. On the other hand, in this neural network, the main characters in standard size and 0 values corresponding to black pixels, which are replaced by \u22121, are defined as balanced points. The Hopfield neural network moves the input matrix to the nearest balanced state. In other words, the closest standard character similar\nAxioms 2023, 12, 424 9 of 12\nThe energy function and its gradient for the system shown in Figure 10 are defined as:\nE = \u2212 12 \u2211 i \u2211 j \u03c9ijxixj \u2212\u2211 i bixi dE dt = \u2211\ni\ndE dxi dxi dt\n= \u2211 i\ndE dxi dxi dneti dneti dt\n= \u2211 i\n( \u2212 dnetidt ) dxi dneti dneti dt\n= \u2212\u2211 i\n( dneti\ndt )2 dxi dneti\n= \u2212\u2211 i\n( dneti\ndt\n)2 \u03d5\u2032(neti)\n(11)\nwhere \u03d5 is the function of the neurons in the network and neti is the output of the ith neuron. According to Equation (11), the energy gradient of the system decreases if the derivative of the neuron function is positive. Choosing sign function as a function of the neuron can meet this condition. When Hopfield\u2019s neural network is used as an image classifier, the two-dimensional images should be mapped to one-dimensional mode, and the black pixel values set to \u22121 (while the value of the white pixels are +1). The segments which are in black and white format are resized to a standard size. The matrix of each segment, which is two-dimensional, is transformed into a one-column vector. The black pixels marked with 0 in this matrix are changed to \u22121 and then applied to the Hopfield neural network. On the other hand, in this neural network, the main characters in standard size and 0 values corresponding to black pixels, which are replaced by \u22121, are defined as balanced points. The Hopfield neural network moves the input matrix to the nearest balanced state. In other words, the closest standard character similar to the target segment is recognized. Figure 11 shows the noise (caused by mud) on the license plate. The designed Hopfield\u2019s neural network classified this noised license plate\u2019s character segments without error.\nAxioms 2023, 12, x FOR PEER REVIEW 10 of 13\nto the target segment is recognized. Figure 11 shows the noise (caused by mud) on the license plate. The designed Hopfield\u2019s neural network classified this noised license plate\u2019s character segments without error.\nTo determine the accuracy of Hopfield\u2019s neural twork in determining numbers and letters, according to Figure 12, a set of different car license plates was considered. The graphics on these plates can play the role of noise. Hopfield\u2019s neural network classified the 253 characters on the license plates of this collection (after image processing and s gmenta ion). Among the 253 test characters, only 6 characters w re recognized wrongly, sh wing an accuracy of 97.6%. Using a computer with Core(TM) i7-2640 CPU and 8 GB RAM, the time spent on determining the characters of each license plate is about 0.08 s.\nIn addition to the 253 main characters (numbers or letters), there are also 26 special characters (such as a dash or a combination of numbers and letters). According to the different standards in these plates, all special characters were considered as a unit pattern. In addition to correctly recognizing the main characters, Hopfield\u2019s neural network could also classify special characters in this unit pattern. The updated classification rate considering the special characters was 97.1%.\nThe agile training capability of the Hopfield neural network has made it appropriate for application to plates with different standards, while for other neural networks, such as convolutional neural networks, a huge set of training data must be collected for each standard of the plates. Moreover, the accuracy of the proposed algorithm was higher than a number of similar ones developed on SOTA [15,16]. On the other hand, the time spent on recognizin the characters of each license plate was almost equal to t time spent on recognizing only o e ch racter in methods based on convolutional neural networks [15\u2013 18].\nFigure 11. Detecting procedures of noisy license plate characters using Hopfield\u2019s neural network:\n(a) initial image; (b) detected license plate box; (c) the cut frame of the license plate; (d) black and\nwhite mode of cut frame; (e) cut noisy segments.\nAxioms 2023, 12, 424 10 of 12\nTo determine the accuracy of Hopfield\u2019s neural network in determining numbers and letters, according to Figure 12, a set of different car license plates was considered. The graphics on these plates can play the role of noise. Hopfield\u2019s neural network classified the 253 characters on the license plates of this collection (after image processing and segmentation). Among the 253 test characters, only 6 characters were recognized wrongly, showing an accuracy of 97.6%. Using a computer with Core(TM) i7-2640 CPU and 8 GB RAM, the time spent on determining the characters of each license plate is about 0.08 s. Axioms 2023, 12, x FOR PEER REVIEW 11 of 13\nFigure 12. A set of selected license plates from different standards to determine the CCR of the proposed algorithm.\n3. Conclusions A new license plate pattern recognition system has been presented that is robust against adverse environmental effects such as fog or mud. Unlike previous studies that only considered a certain standard of license plates, this work evaluated all objects irrespective of their types. However, the selection of objects depends on their positions, and if the recognition of the license plate characters is unsuccessful another object enters the recognition process. Additionally, the paper has addressed the challenges posed by the presence of fog and smoke in the image by removing the matter of the image before initiating the license plate recognition process. More importantly, the use of Hopfield\u2019s neural network for license plate recognition, instead of the conventional method of pixelto-pixel comparison of image segments with standard characters, has significantly reduced the execution time and increased the accuracy. The results pinpoint the efficacy of this approach. The neural network has shown capability in removing the noise on the license plate, making it a reliable tool for license plate recognition. The findings of this study contribute to the field of automated surveillance systems by providing an effective and efficient method for license plate recognition.\nAuthor Contributions: All authors contributed equally to writing, editing, and reviewing the manuscript. All authors have read and agreed to the published version of the manuscript.\nFunding: This research received no external funding.\nData Availability Statement: Not applicable.\nConflicts of Interest: The authors declare no conflict of interest.\nFigure 12. A set of selected license plates from different standards to determine the CCR of the proposed algorithm.\nIn addition to the 253 main characters (numbers or letters), there are also 26 special characters (su h as a dash or a combination of numbers and letter ). According t the different standards in these plates, all special characters were considered as a nit patt rn. In addition to co rectly r cog izing the main characters, Hopfield\u2019s neural network could also clas ify special c aracters in this unit pattern. The updated classification rate consideri g the special characters was 97.1%. The agile training capability of the Hopfield neur l network has m d it appr priate for a plication to pl tes with different st ndards, while for other neural n tworks, such as convolutional eural networks, a huge set of training data must be collected for each standard of the plates. Moreover, the accuracy of the proposed algorithm was higher than a number of similar ones developed on SOTA [15,16]. On the other hand, the time spent on recognizing the characters of each license plate was almost equal to the time spent on recognizing only one character in methods based on convolutional neural networks [15\u201318].\nAxioms 2023, 12, 424 11 of 12\n3. Conclusions\nA new license plate pattern recognition system has been presented that is robust against adverse environmental effects such as fog or mud. Unlike previous studies that only considered a certain standard of license plates, this work evaluated all objects irrespective of their types. However, the selection of objects depends on their positions, and if the recognition of the license plate characters is unsuccessful another object enters the recognition process. Additionally, the paper has addressed the challenges posed by the presence of fog and smoke in the image by removing the matter of the image before initiating the license plate recognition process. More importantly, the use of Hopfield\u2019s neural network for license plate recognition, instead of the conventional method of pixelto-pixel comparison of image segments with standard characters, has significantly reduced the execution time and increased the accuracy. The results pinpoint the efficacy of this approach. The neural network has shown capability in removing the noise on the license plate, making it a reliable tool for license plate recognition. The findings of this study contribute to the field of automated surveillance systems by providing an effective and efficient method for license plate recognition.\nAuthor Contributions: All authors contributed equally to writing, editing, and reviewing the manuscript. All authors have read and agreed to the published version of the manuscript.\nFunding: This research received no external funding.\nData Availability Statement: Not applicable.\nConflicts of Interest: The authors declare no conflict of interest.\nReferences 1. Tsakanikas, V.; Dagiuklas, T. Video surveillance systems-current status and future trends. Comput. Electr. Eng. 2018, 70, 736\u2013753. [CrossRef] 2. Sharma, S.K.; Phan, H.; Lee, J. An application study on road surface monitoring using DTW based image processing and ultrasonic sensors. Appl. Sci. 2020, 10, 4490. [CrossRef] 3. Wang, S.-Z.; Lee, H.-J. Detection and recognition of license plate characters with different appearances. IEEE Trans. Intell. Transp. Syst. 2003, 2, 979\u2013984. 4. Radha, R.; Sumathi, C.P. A novel approach to extract text from license plate of vehicle. Signal Process. Image Commun. 2012, 3, 181\u2013192. [CrossRef] 5. Kuo, C.Y.; Lu, Y.R.; Yang, S.M. On the image sensor processing for lane detection and control in vehicle lane keeping systems. Sensors 2019, 19, 1665. [CrossRef] 6. Zheng, K.; Wei, M.; Sun, G.; Anas, B.; Li, Y. Using vehicle synthesis generative adversarial networks to improve vehicle detection in remote sensing images. ISPRS Int. J. Geo-Inf. 2019, 8, 390. [CrossRef] 7. Sulehria, H.K.; Zhang, Y.; Irfan, D.; Sulehria, A.K. Vehicle number plate recognition using mathematical morphology and neural networks. WSEAS Trans. Comput. 2008, 7, 781\u2013790. 8. Aggarwal, A.; Rani, A.; Kumar, M. A robust method to authenticate car license plates using segmentation and ROI based approach. Smart Sustain. Built Environ. 2020, 9, 737\u2013747. [CrossRef] 9. Kim, T.-G.; Yun, B.-J.; Kim, T.-H.; Lee, J.-Y.; Park, K.-H.; Jeong, Y.; Kim, H.D. Recognition of vehicle license plates based on image processing. Appl. Sci. 2021, 11, 6292. [CrossRef] 10. Walia, E.; Verma, A. Vehicle number plate detection using sobel edge detection technique. Int. J. Comput. Technol. 2010, 1, 2229\u20134333. 11. Parasuraman, K.; Kumar, P.V. An efficient method for indian vehicle license plate extraction and character segmentation. In\nProceedings of the IEEE International Conference on Computational Intelligence and Computing Research, Coimbatore, India, 28\u201329 December 2010.\n12. Lekhana, G.C.; Srikantaswamy, R. Real time license plate recognition system. Int. J. Eng. Res. Technol. 2012, 2, 2250\u20133536. 13. Kamilaris, A.; Prenafeta-Bold\u00fa, F. A review of the use of convolutional neural networks in agriculture. J. Agric. Sci. 2018, 156, 312\u2013322. [CrossRef] 14. Hopfield, J.J. Neural networks and physical systems with emergent collective computational abilities. Proc. Natl. Acad. Sci. USA 1982, 79, 2554\u20132558. [CrossRef] 15. Lin, C.-J.; Chuang, C.-C.; Lin, H.-Y. Edge-AI-based real-time automated license plate recognition. System. Appl. Sci. 2022, 12, 1445.\n[CrossRef]\nAxioms 2023, 12, 424 12 of 12\n16. Yousaf, U.; Khan, A.; Ali, H.; Khan, F.G.; Rehman, Z.U.; Shah, S.; Ali, F.; Pack, S.; Ali, S. A deep learning based approach for localization and recognition of Pakistani vehicle license plates. Sensors 2021, 21, 7696. [CrossRef] 17. Park, S.-H.; Yu, S.-B.; Kim, J.-A.; Yoon, H. An All-in-one vehicle type and license plate recognition system using YOLOv4. Sensors 2022, 22, 921. [CrossRef] 18. Wang, H.; Li, Y.; Dang, L.-M.; Moon, H. Robust korean license plate recognition based on deep neural networks. Sensors 2021, 21, 4140. [CrossRef] 19. Fattal, R. Single image dehazing. ACM Trans. Graph. 2008, 27, 1\u20139. [CrossRef] 20. Narasimhan, S.G.; Nayar, S.K. Chromatic frame work for vision in bad weather. In Proceedings of the Conference on Computer Vision and Pattern Recognition, Hilton Head, SC, USA, 13\u201315 June 2000; pp. 598\u2013605. 21. Narasimhan, S.G.; Nayar, S.K. Vision and the atmosphere. Int. J. Comput. Vis. 2002, 48, 233\u2013254. [CrossRef] 22. Tan, R.T. Visibility in bad weather from a single image. In Proceedings of the 2008 IEEE Conference on Computer Vision and Pattern Recognition, Anchorage, AK, USA, 23\u201328 June 2008. 23. Levin, A.; Lischinski, D.; Weiss, Y. A closed form solution to natural image matting. In Proceedings of the Conference on Computer Vision and Pattern Recognition, New York, NY, USA, 17\u201322 June 2006; Volume 1, pp. 61\u201368. 24. Wei, T.; Chen, D.; Zhou, W.; Liao, J.; Zhao, H.; Zhang, W.; Yu, N. Improved image matting via real-time user clicks and uncertainty\nestimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Nashville, TN, USA, 20\u201325 June 2021; pp. 15374\u201315383.\n25. Cai, S.; Zhang, X.; Fan, H.; Huang, H.; Liu, J.; Liu, J.; Liu, J.; Wang, J.; Sun, J. Disentangled image matting. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), Seoul, Republic of Korea, 27 October\u20132 November 2019; pp. 8819\u20138828. 26. Aksoy, Y.; Aydin, T.O.; Pollefeys, M. Designing effective inter-pixel information flow for natural image matting. In Proceedings of the Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21\u201326 July 2017. 27. Xu, Z.; Yang, W.; Meng, A.; Lu, N.; Huang, H.; Ying, C.; Huang, L. Towards end-to-end license Plate detection and recognition: A large dataset and baseline. In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany, 8\u201314 September 2018; pp. 255\u2013271. 28. Menon, A.; Omman, B. Detection and recognition of multiple license plate from still images. In Proceedings of the 2018 International Conference on Circuits and Systems in Digital Enterprise Technology (ICCSDET), Kottayam, India, 21\u201322 December 2018. 29. Wang, W.; Yang, J.; Chen, M.; Wang, P. A light CNN for end-to-end car license plates detection and recognition. IEEE Access 2019, 7, 173875\u2013173883. [CrossRef] 30. Murty, M.N.; Devi, V.S. Pattern Recognition. Undergraduate Topics in Computer Science; Springer: London, UK, 2011. 31. Fung, C.H.; Wong, M.S.; Chan, P.W. Spatio-temporal data fusion for satellite images using Hopfield neural network. Remote Sens. 2019, 11, 2077. [CrossRef] 32. Xu, X.; Chen, S. An optical image encryption method using Hopfield neural network. Entropy 2022, 24, 521. [CrossRef] 33. Yu, F.; Yu, Q.; Chen, H.; Kong, X.; Mokbel, A.A.M.; Cai, S.; Du, S. Dynamic analysis and audio encryption application in IoT of a multi-scroll fractional-order memristive Hopfield neural network. Fractal Fract. 2022, 6, 370. [CrossRef] 34. Mohd Jamaludin, S.Z.; Mohd Kasihmuddin, M.S.; Md Ismail, A.I.; Mansor, M.A.; Md Basir, M.F. Energy based logic mining analysis with Hopfield neural network for recruitment evaluation. Entropy 2021, 23, 40. [CrossRef] 35. Akhmet, M.; Arug\u0306aslan \u00c7in\u00e7in, D.; Tleubergenova, M.; Nugayeva, Z. Unpredictable oscillations for Hopfield-type neural\nnetworks with delayed and advanced arguments. Mathematics 2021, 9, 571. [CrossRef]\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content."
        }
    ],
    "title": "A License Plate Recognition System with Robustness against Adverse Environmental Conditions Using Hopfield\u2019s Neural Network",
    "year": 2023
}