{
    "abstractText": "This paper investigates when one can efficiently recover an approximate Nash Equilibrium (NE) in offline congestion games. The existing dataset coverage assumption in offline general-sum games inevitably incurs a dependency on the number of actions, which can be exponentially large in congestion games. We consider three different types of feedback with decreasing revealed information. Starting from the facility-level (a.k.a., semi-bandit) feedback, we propose a novel one-unit deviation coverage condition and give a pessimism-type algorithm that can recover an approximate NE. For the agent-level (a.k.a., bandit) feedback setting, interestingly, we show the one-unit deviation coverage condition is not sufficient. On the other hand, we convert the game to multi-agent linear bandits and show that with a generalized data coverage assumption in offline linear bandits, we can efficiently recover the approximate NE. Lastly, we consider a novel type of feedback, the game-level feedback where only the total reward from all agents is revealed. Again, we show the coverage assumption for the agent-level feedback setting is insufficient in the game-level feedback setting, and with a stronger version of the data coverage assumption for linear bandits, we can recover an approximate NE. Together, our results constitute the first study of offline congestion games and imply formal separations between different types of feedback.",
    "authors": [
        {
            "affiliations": [],
            "name": "Haozhe Jiang"
        },
        {
            "affiliations": [],
            "name": "Qiwen Cui"
        },
        {
            "affiliations": [],
            "name": "Zhihan Xiong"
        },
        {
            "affiliations": [],
            "name": "Maryam Fazel"
        }
    ],
    "id": "SP:e7f602084368677ff1110fb907b2dfd8ff025ffe",
    "references": [
        {
            "authors": [
                "Eitan Altman",
                "Alexandre Reiffers",
                "Daniel S Menasche",
                "Mandar Datar",
                "Swapnil Dhamal",
                "Corinne Touati"
            ],
            "title": "Mining competition in a multi-cryptocurrency ecosystem at the network edge: a congestion game approach",
            "venue": "ACM SIGMETRICS Performance Evaluation Review,",
            "year": 2019
        },
        {
            "authors": [
                "Ioannis Anagnostides",
                "Ioannis Panageas",
                "Gabriele Farina",
                "Tuomas Sandholm"
            ],
            "title": "On last-iterate convergence beyond zero-sum games",
            "venue": "arXiv preprint arXiv:2203.12056,",
            "year": 2022
        },
        {
            "authors": [
                "Shicong Cen",
                "Chen Cheng",
                "Yuxin Chen",
                "Yuting Wei",
                "Yuejie Chi"
            ],
            "title": "Fast global convergence of natural policy gradient methods with entropy regularization",
            "venue": "Operations Research,",
            "year": 2021
        },
        {
            "authors": [
                "Po-An Chen",
                "Chi-Jen Lu"
            ],
            "title": "Playing congestion games with bandit feedbacks",
            "venue": "In AAMAS, pp",
            "year": 2015
        },
        {
            "authors": [
                "Po-An Chen",
                "Chi-Jen Lu"
            ],
            "title": "Generalized mirror descents in congestion games",
            "venue": "Artificial Intelligence,",
            "year": 2016
        },
        {
            "authors": [
                "Qiwen Cui",
                "Simon S Du"
            ],
            "title": "When is offline two-player zero-sum markov game solvable",
            "venue": "arXiv preprint arXiv:2201.03522,",
            "year": 2022
        },
        {
            "authors": [
                "Qiwen Cui",
                "Simon S Du"
            ],
            "title": "Provably efficient offline multi-agent reinforcement learning via strategy-wise bonus",
            "venue": "arXiv preprint arXiv:2206.00159,",
            "year": 2022
        },
        {
            "authors": [
                "Qiwen Cui",
                "Lin F Yang"
            ],
            "title": "Minimax sample complexity for turn-based stochastic game",
            "venue": "In Uncertainty in Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Qiwen Cui",
                "Zhihan Xiong",
                "Maryam Fazel",
                "Simon S Du"
            ],
            "title": "Learning in congestion games with bandit feedback",
            "venue": "arXiv preprint arXiv:2206.01880,",
            "year": 2022
        },
        {
            "authors": [
                "Benjamin Drighes",
                "Walid Krichene",
                "Alexandre Bayen"
            ],
            "title": "Stability of nash equilibria in the congestion game under replicator dynamics",
            "venue": "In 53rd IEEE Conference on Decision and Control,",
            "year": 2014
        },
        {
            "authors": [
                "St\u00e9phane Durand"
            ],
            "title": "Analysis of Best Response Dynamics in Potential Games",
            "venue": "PhD thesis, Universite\u0301 Grenoble Alpes,",
            "year": 2018
        },
        {
            "authors": [
                "Angelo Fanelli",
                "Michele Flammini",
                "Luca Moscardelli"
            ],
            "title": "The speed of convergence in congestion games under best-response dynamics",
            "venue": "In International Colloquium on Automata, Languages, and Programming,",
            "year": 2008
        },
        {
            "authors": [
                "Am\u00e9lie Heliou",
                "Johanne Cohen",
                "Panayotis Mertikopoulos"
            ],
            "title": "Learning with bandit feedback in potential games",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Christian Ibars",
                "Monica Navarro",
                "Lorenza Giupponi"
            ],
            "title": "Distributed demand management in smart grid with a congestion game",
            "venue": "In 2010 First IEEE International Conference on Smart Grid Communications,",
            "year": 2010
        },
        {
            "authors": [
                "Ying Jin",
                "Zhuoran Yang",
                "Zhaoran Wang"
            ],
            "title": "Is pessimism provably efficient for offline rl",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Ying Jin",
                "Zhuoran Yang",
                "Zhaoran Wang"
            ],
            "title": "Is pessimism provably efficient for offline rl",
            "venue": "Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Ramesh Johari",
                "John N Tsitsiklis"
            ],
            "title": "Network resource allocation and a congestion game",
            "venue": "In Proceedings of the Annual Allerton Conference on Communication Control and Computing,",
            "year": 2003
        },
        {
            "authors": [
                "Robert Kleinberg",
                "Georgios Piliouras",
                "\u00c9va Tardos"
            ],
            "title": "Multiplicative updates outperform generic no-regret learning in congestion games",
            "venue": "In Proceedings of the forty-first annual ACM symposium on Theory of computing,",
            "year": 2009
        },
        {
            "authors": [
                "Elias Koutsoupias",
                "Christos Papadimitriou"
            ],
            "title": "Worst-case equilibria",
            "venue": "In Annual symposium on theoretical aspects of computer science,",
            "year": 1999
        },
        {
            "authors": [
                "Walid Krichene",
                "Benjamin Drigh\u00e8s",
                "Alexandre Bayen"
            ],
            "title": "On the convergence of no-regret learning in selfish routing",
            "venue": "In International Conference on Machine Learning,",
            "year": 2014
        },
        {
            "authors": [
                "Walid Krichene",
                "Benjamin Drigh\u00e8s",
                "Alexandre M Bayen"
            ],
            "title": "Online learning of nash equilibria in congestion games",
            "venue": "SIAM Journal on Control and Optimization,",
            "year": 2015
        },
        {
            "authors": [
                "Tor Lattimore",
                "Csaba Szepesv\u00e1ri"
            ],
            "title": "Confidence Bounds for Least Squares Estimators, pp. 219\u2013230",
            "year": 2020
        },
        {
            "authors": [
                "Sergey Levine",
                "Aviral Kumar",
                "George Tucker",
                "Justin Fu"
            ],
            "title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems",
            "venue": "arXiv preprint arXiv:2005.01643,",
            "year": 2020
        },
        {
            "authors": [
                "John F Nash Jr."
            ],
            "title": "Equilibrium points in n-person games",
            "venue": "Proceedings of the national academy of sciences,",
            "year": 1950
        },
        {
            "authors": [
                "Ioannis Panageas",
                "Georgios Piliouras"
            ],
            "title": "Average case performance of replicator dynamics in potential games via computing regions of attraction",
            "venue": "In Proceedings of the 2016 ACM Conference on Economics and Computation,",
            "year": 2016
        },
        {
            "authors": [
                "Paria Rashidinejad",
                "Banghua Zhu",
                "Cong Ma",
                "Jiantao Jiao",
                "Stuart Russell"
            ],
            "title": "Bridging offline reinforcement learning and imitation learning: A tale of pessimism",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Tongzheng Ren",
                "Jialian Li",
                "Bo Dai",
                "Simon S Du",
                "Sujay Sanghavi"
            ],
            "title": "Nearly horizon-free offline reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Robert W Rosenthal"
            ],
            "title": "A class of games possessing pure-strategy nash equilibria",
            "venue": "International Journal of Game Theory,",
            "year": 1973
        },
        {
            "authors": [
                "Tim Roughgarden",
                "\u00c9va Tardos"
            ],
            "title": "Bounding the inefficiency of equilibria in nonatomic congestion games",
            "venue": "Games and economic behavior,",
            "year": 2004
        },
        {
            "authors": [
                "William H Sandholm",
                "Emin Dokumac\u0131",
                "Ratul Lahkar"
            ],
            "title": "The projection dynamic and the replicator dynamic",
            "venue": "Games and Economic Behavior,",
            "year": 2008
        },
        {
            "authors": [
                "Aaron Sidford",
                "Mengdi Wang",
                "Lin Yang",
                "Yinyu Ye"
            ],
            "title": "Solving discounted stochastic two-player games with near-optimal time and sample complexity",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Jayakumar Subramanian",
                "Amit Sinha",
                "Aditya Mahajan"
            ],
            "title": "Robustness and sample complexity of modelbased marl for general-sum markov games",
            "venue": "arXiv preprint arXiv:2110.02355,",
            "year": 2021
        },
        {
            "authors": [
                "Brian Swenson",
                "Ryan Murray",
                "Soummya Kar"
            ],
            "title": "On best-response dynamics in potential games",
            "venue": "SIAM Journal on Control and Optimization,",
            "year": 2018
        },
        {
            "authors": [
                "Csaba Szepesv\u00e1ri",
                "R\u00e9mi Munos"
            ],
            "title": "Finite time bounds for sampling based fitted value iteration",
            "venue": "In Proceedings of the 22nd international conference on Machine learning,",
            "year": 2005
        },
        {
            "authors": [
                "Tengyang Xie",
                "Ching-An Cheng",
                "Nan Jiang",
                "Paul Mineiro",
                "Alekh Agarwal"
            ],
            "title": "Bellman-consistent pessimism for offline reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Tengyang Xie",
                "Nan Jiang",
                "Huan Wang",
                "Caiming Xiong",
                "Yu Bai"
            ],
            "title": "Policy finetuning: Bridging sampleefficient offline and online reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Wei Xiong",
                "Han Zhong",
                "Chengshuai Shi",
                "Cong Shen",
                "Liwei Wang",
                "Tong Zhang"
            ],
            "title": "Nearly minimax optimal offline reinforcement learning with linear function approximation: Single-agent mdp and markov game",
            "venue": "arXiv preprint arXiv:2205.15512,",
            "year": 2022
        },
        {
            "authors": [
                "Yuling Yan",
                "Gen Li",
                "Yuxin Chen",
                "Jianqing Fan"
            ],
            "title": "Model-based reinforcement learning is minimax-optimal for offline zero-sum markov games",
            "venue": "arXiv preprint arXiv:2206.04044,",
            "year": 2022
        },
        {
            "authors": [
                "Ming Yin",
                "Yu Bai",
                "Yu-Xiang Wang"
            ],
            "title": "Near-optimal provable uniform convergence in offline policy evaluation for reinforcement learning",
            "venue": "arXiv preprint arXiv:2007.03760,",
            "year": 2020
        },
        {
            "authors": [
                "Ming Yin",
                "Yu Bai",
                "Yu-Xiang Wang"
            ],
            "title": "Near-optimal offline reinforcement learning via double variance reduction",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiqing Zhang",
                "Sham Kakade",
                "Tamer Basar",
                "Lin Yang"
            ],
            "title": "Model-based multi-agent rl in zero-sum markov games with near-optimal sample complexity",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Han Zhong",
                "Wei Xiong",
                "Jiyuan Tan",
                "Liwei Wang",
                "Tong Zhang",
                "Zhaoran Wang",
                "Zhuoran Yang"
            ],
            "title": "Pessimistic minimax value iteration: Provably efficient equilibrium learning from offline datasets",
            "venue": "arXiv preprint arXiv:2202.07511,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Congestion game is a special class of general-sum matrix games that models the interaction of players with shared facilities (Rosenthal, 1973). Each player chooses some facilities to utilize and each facility will incur a different reward depending on how congested it is. For instance, in the routing game (Koutsoupias & Papadimitriou, 1999), each player decides a path to travel from the starting point to the destination point in a traffic graph. The facilities are the edges and the joint decision of all the players determines the congestion in the graph. The more players utilizing one edge, the longer the travel time on that edge will be. As one of the most well-known classes of games, congestion game has been successfully deployed in numerous real-world applications such as resource allocation (Johari & Tsitsiklis, 2003), electrical grids (Ibars et al., 2010) and cryptocurrency ecosystem (Altman et al., 2019).\nNash equilibrium (NE), one of the most important concepts in game theory (Nash Jr, 1950), characterizes the emerging behavior in a multi-agent system with selfish players. It is commonly known that solving for the NE is computationally efficient in congestion games as they are isomorphic to potential games (Monderer & Shapley, 1996). Assuming full information access, classic dynamics such as best response dynamics (Fanelli et al., 2008), replicator dynamics (Drighes et al., 2014) and no-regret dynamics (Kleinberg et al., 2009)\n\u2217Equal contribution.\nar X\niv :2\n21 0.\n13 39\n6v 1\n[ cs\n.G T\n] 2\n4 O\nprovably converge to NE in congestion games. Recently Heliou et al. (2017) and Cui et al. (2022) relaxed the full information setting to the online (semi-) bandit feedback setting, achieving asymptotic and nonasymptotic convergence, respectively. It is worth noting that Cui et al. (2022) proposed the first algorithm that has sample complexity independent of the number of actions.\nOffline reinforcement learning has been studied in many real-world applications (Levine et al., 2020). From the theoretical perspective, a line of work provides understandings of offline single-agent decision making, including bandits and Markov Decision Processes (MDPs), where researchers derived favorable sample complexity under the single policy coverage (Rashidinejad et al., 2021; Xie et al., 2021b). However, how to learn in offline multi-agent games with offline data is still far from clear. Recently, the unilateral coverage assumption has been proposed as the minimal assumption for offline zero-sum games and offline general-sum games with corresponding algorithms to learn the NE (Cui & Du, 2022a,b; Zhong et al., 2022). Though their coverage assumption and the algorithms apply to the most general class of normal-form games, when specialized to congestion games, the sample complexity will scale with the number of actions, which can be exponentially large. Since congestion games admit specific structures, one may hope to find specialized data coverage assumptions that permit sample-efficient offline learning.\nIn different applications, the types of feedback, i.e., the revealed reward information, can be different in the offline dataset. For instance, the dataset may include the reward of each facility, the reward of each player, or the total reward of the game. With decreasing information contained in the dataset, different coverage assumptions and algorithms are necessary. In addition, the main challenge in solving congestion games lies in the curse of an exponentially large action set, as the number of actions can be exponential in the number of facilities. In this work, we aim to answer the following question:\nWhen can we find approximate NE in offline congestion games with different types of feedback, without suffering from the curse of large action set?\nWe provide an answer that reveals striking differences between different types of feedback."
        },
        {
            "heading": "1.1 Main Contributions",
            "text": "We provide both positive and negative results for each type of feedback. See Table 1 for a summary."
        },
        {
            "heading": "1. Three types of feedback and corresponding",
            "text": "data coverage assumptions. We consider three types of feedback: facility-level feedback, agent-level feedback, and game-level feedback to model different real-world applications and what dataset coverage assumptions permit finding an approximate NE. In offline general-sum games, Cui & Du (2022b) proposes the unilateral coverage assumption. Although their result can be applied to offline congestion games with agent-level feedback, their unilateral coverage coefficient is at least as large as the number of actions and thus has an exponential dependence on the number of facilities. Therefore, for each type of feedback, we propose a corresponding data coverage assumption to escape the curse of the large action set. Specifically: \u2022 Facility-Level Feedback: For facility-level\nfeedback, the reward incurred in each facility is provided in the offline dataset. This type of feedback has the strongest signal. We propose the One-Unit Deviation coverage assumption (cf. Assumption 2) for this feedback. \u2022 Agent-Level Feedback: For agent-level feedback, only the sum of the facilities\u2019 rewards for each agent is observed. This type of feedback has weaker signals than the facility-level feedback does, and therefore we require a stronger data coverage assumption (cf. Assumption 3).\n\u2022 Game-Level Feedback: For the game-level feedback, only the sum of the agent rewards is obtained. This type of feedback has the weakest signals, and we require the strongest data coverage assumption (Assumption 4). Notably, for the latter two types of feedback, we leverage the connections between congestion games and linear bandits. 2. Sample complexity analyses for different types of feedback. We adopt the surrogate minimization idea in Cui & Du (2022b) and show a unified algorithm (cf. Algorithm 1) with carefully designed bonus terms tailored to different types of feedback can efficiently find an approximate NE, therefore showing our proposed data coverage assumptions are sufficient. For each type of feedback, we give a polynomial upper bound under its corresponding dataset coverage assumption. 3. Separations between different types of feedback. To rigorously quantify the signal strengths in the three types of feedback, we provide concrete hard instances. Specifically, we show there exists a problem instance that satisfies Assumption 2, but with only agent-level feedback, we provably cannot find an approximate NE, yielding a separation between the facility-level feedback and the agent-level feedback. Furthermore, we also show there exists a problem instance that satisfies Assumption 3, but with only gamelevel feedback, we provably cannot find an approximate NE, yielding a separation between the agent-level feedback and game-level feedback."
        },
        {
            "heading": "1.2 Motivating Examples",
            "text": "Here, we provide several concrete scenarios to exemplify and motivate the aforementioned three types of feedback. Formal definitions of the problem can be found in Section 2.\nExample 1 (Facility-level feedback). Suppose Google Maps is trying to improve its route assigning algorithm through historical data based on certain regions. Then, each edge (road) on the traffic graph of this region can be considered as a facility and the action that a user will take is a path that connects a certain origin and destination. In this setting, the cost of each facility is the waiting time on that road, which may increase as the number of users choosing this facility increases. In the historical data, each data point contains the path chosen by each user and his/her waiting time on each road, which is an offline dataset with facility-level feedback.\nExample 2 (Agent-level feedback). Suppose a company is trying to learn a policy to advertise its products from historical data. We can consider a certain set of websites as the facility set, and the products as the players. The action chosen for each product is a subset of websites where the company will place advertisements for that product. The reward for each product is measured by its sales. In the historical data, each data point contains the websites chosen for each product advertisement and the total amount of sales within a certain range of time. This offline dataset inherently has only agent-level feedback since the company cannot measure each website\u2019s contribution to sales.\nExample 3 (Game-level feedback). Under the same setting above, suppose now another company (called B) is also trying to learn such a policy but lacks internal historical data. Therefore, B decides to use the data from the company mentioned in the above example (called A). However, since company B does not have internal access to company A\u2019s database, the precise sales of each product is not visible to company B. As a result, company B can only record the total amount of sales of all concerning products from company A\u2019s public financial reports, making its offline dataset have only the game-level feedback."
        },
        {
            "heading": "1.3 Related Work",
            "text": "Potential Games and Congestion Games. Potential games are a special class of normal-form games with a potential function to quantify the changes in the payoff of each player and deterministic NE is proven to exist (Monderer & Shapley, 1996). Asymptotic convergence to the NE can be achieved by classic game theory dynamics such as best response dynamic (Durand, 2018; Swenson et al., 2018), replicator dynamic (Sandholm et al., 2008; Panageas & Piliouras, 2016) and no-regret dynamic (Heliou et al., 2017). Recently, Cen et al. (2021) proved that natural policy gradient has a convergence rate independent of the number\nof actions in entropy regularized potential games. Anagnostides et al. (2022) provided the non-asymptotic convergence rate for mirror descent and O(1) individual regret for optimistic mirror descent.\nCongestion games are proposed in the seminal work (Rosenthal, 1973) and the equivalence with potential games is proven in (Monderer & Shapley, 1996). Note that congestion games can have exponentially large action sets, so efficient algorithms for potential games are not necessarily efficient for congestion games. Non-atomic congestion games consider separable players, which enjoy a convex potential function if the cost function is non-decreasing (Roughgarden & Tardos, 2004). For atomic congestion games, the potential function is usually non-convex, making the problem more difficult. (Kleinberg et al., 2009; Krichene et al., 2014) show that no-regret algorithms asymptotically converge to NE with full information feedback and (Krichene et al., 2015) provide averaged iterate convergence for bandit feedback. (Chen & Lu, 2015, 2016) provide non-asymptotic convergence by assuming the atomic congestion game is close to a non-atomic one, and thus approximately enjoys the convex potential. Recently, Cui et al. (2022) proposed two Nash-regret minimizing algorithms, an upper-confidence-bound-type algorithm and a Frank-Wolfe-type algorithm for semi-bandit feedback and bandit feedback settings respectively, and showed both algorithms converge at a rate that does not depend on the number of actions. To the best of our knowledge, all of these works either consider the full information setting or the online feedback setting instead of the offline setting in this paper.\nOffline Bandits and Reinforcement Learning. For related works in empirical offline reinforcement learning, interested readers can refer to (Levine et al., 2020). From the theoretical perspective, researchers have been putting effort into understanding what dataset coverage assumptions allow for learning the optimal policy. The most basic assumption is the uniform coverage, i.e., every state-action pair is covered by the dataset (Szepesva\u0301ri & Munos, 2005). Provably efficient algorithms have been proposed for both single-agent and multi-agent reinforcement learning (Yin et al., 2020, 2021; Ren et al., 2021; Sidford et al., 2020; Cui & Yang, 2021; Zhang et al., 2020; Subramanian et al., 2021). In single-agent bandits and reinforcement learning, with the help of pessimism, only single policy coverage is required, i.e., the dataset only needs to cover the optimal policy (Jin et al., 2021a; Rashidinejad et al., 2021; Xie et al., 2021b,a). For offline multi-agent Markov games, Cui & Du (2022a) first show that it is impossible to learn with the single policy coverage and they identify the unilateral coverage assumption as the minimal coverage assumption while Zhong et al. (2022); Xiong et al. (2022) provide similar results with linear function approximation. Recently, Yan et al. (2022); Cui & Du (2022b) give minimax sample complexity for offline zero-sum Markov games. In addition, Cui & Du (2022b) proposes an algorithm for offline multi-player general-sum Markov games that do not suffer from the curse of multiagents."
        },
        {
            "heading": "2 Preliminary",
            "text": ""
        },
        {
            "heading": "2.1 Congestion Game",
            "text": "General-Sum Matrix Game. A general-sum matrix game is defined by a tuple G = ({Ai}mi=1, R), where m is the number of players, Ai is the action space for player i and R(\u00b7|a) is a distribution over [0, rmax]m with mean r(a). When playing the game, all players simultaneously select actions, constituting joint action a and the reward is sampled as r \u223c R(\u00b7|a), where player i gets reward ri.\nLet A = \u220fm i=1Ai. A joint policy is a distribution \u03c0 \u2208 \u2206(A) while a product policy is \u03c0 = \u2297m i=1 \u03c0i with \u03c0i \u2208 \u2206(Ai), where \u2206(X ) denotes the probability simplex over X . If the players follow a policy \u03c0, their actions are sampled from the distribution a \u223c \u03c0. The expected return of player i under some policy \u03c0 is defined as value V \u03c0i = Ea\u223c\u03c0[ri(a)].\nLet \u03c0\u2212i be the joint policy of all the players except for player i. The best response of the player i to policy\n\u03c0\u2212i is defined as \u03c0 \u2020,\u03c0\u2212i i = arg max\u00b5\u2208\u2206(Ai) V \u00b5,\u03c0\u2212i i . Here \u00b5 is the policy for player i and (\u00b5, \u03c0\u2212i) constitutes a joint policy for all players. We can always set the best response to be a pure strategy since the value function is linear in \u00b5. We also denote V \u2020,\u03c0\u2212i i := V \u03c0 \u2020,\u03c0\u2212i i ,\u03c0\u2212i i as the best response value. To evaluate a policy \u03c0, we\nuse the performance gap defined as\nGap(\u03c0) = max i\u2208[m]\n\u00ee V \u2020,\u03c0\u2212i i \u2212 V \u03c0 i \u00f3 .\nA product policy \u03c0 is an \u03b5-approximate NE if Gap(\u03c0) \u2264 \u03b5. A product policy \u03c0 is an NE if Gap(\u03c0) = 0. Note that it is possible to have multiple Nash equilibria in one game. Congestion Game. A congestion game is a general-sum matrix game with special structure. In particular, there is a facility set F such that a \u2286 F for all a \u2208 Ai, meaning that the size of Ai can at most be 2F , where F = |F|. A set of facility reward distributions { Rf (\u00b7|n) \u2223\u2223n \u2208 N} f\u2208F is associated with each facility f .\nLet the number of players choosing facility f in the action be nf (a) = \u2211m i=1 1 {f \u2208 ai}, where a is the joint action. A facility with specific number of players selecting it is said to be a configuration on f . Two joint actions where the same number of players select f are said to have the same configuration on f . The reward associated with facility f is sampled by rf \u223c Rf ( \u00b7 \u2223\u2223nf (a)) and the total reward of player i is ri = \u2211f\u2208ai rf . With slight abuse of notation, let rf (n) be the mean reward that facility f generates when there are n players choosing it. We further assume Rf (\u00b7|n) \u2208 [\u22121, 1] for all n \u2208 [m]. It is well known that every congestion game has pure strategy NE.\nThe information we get from the game each episode is ( ak, rk ) , where ak is the joint action and rk contains the reward signal. In this paper, we will consider three types of reward feedback in congestion games, which essentially make rk different in each data point ( ak, rk ) .\n\u2022 Facility-level feedback (semi-bandit feedback): In each data point ( ak, rk ) , rk contains reward\nreceived from each facility f \u2208 \u22c3m i=1 a k i , meaning that r k = { rf,k } f\u2208 \u22c3m i=1 a k i .\n\u2022 Agent-level feedback (bandit feedback): In each data point ( ak, rk ) , rk contains reward received by\neach player, meaning that rk = { rki }m i=1 , where rki = \u2211 f\u2208aki rf,k.\n\u2022 Game-level feedback: In each data point ( ak, rk ) , rk contains only the total reward received by\nall players, meaning that rk = \u2211m i=1 r k i , which becomes a scalar. This type of feedback is the minimal information we can get and has not been discussed in previous literature."
        },
        {
            "heading": "2.2 Offline Matrix Game",
            "text": "Offline Matrix Game. In the offline setting, the algorithm only has access to an offline dataset D ={( ak, rk )}n k=1\ncollected by some exploration policy \u03c1 in advance. A joint action a is said to be covered if \u03c1(a) > 0. Cui & Du (2022a) has proved that the following assumption is a minimal dataset coverage assumption to learn an NE in a general-sum matrix game. The assumption requires the dataset to cover all unilaterally deviated actions from one NE.\nAssumption 1. There exists an NE \u03c0\u2217 such that for any player i and policy \u03c0i \u2208 \u2206(A), a is covered by \u03c1 as long as (\u03c0i, \u03c0 \u2217 \u2212i)(a) > 0.\nCui & Du (2022b) provides a sample complexity result for matrix games with dependence on C(\u03c0\u2217), where C(\u03c0) quantifies how well \u03c0 is unilaterally covered by the dataset. The definition is as follows.\nDefinition 1. For strategy \u03c0 and \u03c1 satisfying Assumption 1, the unilateral coefficient is defined as\nC(\u03c0) = max i,\u03c0\u2032,\u03c1(a)>0\n(\u03c0\u2032i, \u03c0\u2212i) (a)\n\u03c1(a) . (1)\nSurrogate Minimization. Cui & Du (2022b) proposed an algorithm called Strategy-wise Bonus + Surrogate Minimization (SBSM) to achieve efficient learning under Assumption 1. SBSM motivates a general algorithm framework for learning congestion games in different settings. First we design r\u0302i(a) which estimates the reward player i gets when the joint action is a. Offline bandit (reinforcement learning) algorithm usually leverages the confidence bound (bonus) to create a pessimistic estimate of the reward, inducing conservatism in the output policy and achieving a sample-efficient algorithm. Here we formally define bonus as follows.\nDefinition 2. For any reward estimator r\u0302i : A \u2192 R that estimates reward with expectation ri : A \u2192 R, bi : A \u2192 R is called the bonus term for r\u0302 if for all i \u2208 [m],a \u2208 A, with probability at least 1\u2212 \u03b4, it holds that\n|ri(a)\u2212 r\u0302i(a)| \u2264 bi(a). (2)\nThe formulae for r\u0302i and b vary according to the type of feedback as discussed in later sections. The optimistic and pessimistic values for policy \u03c0 and player i are defined as\nV \u03c0\ni = Ea\u223c\u03c0 [r\u0302i(a) + bi(a)] , V \u03c0 i = Ea\u223c\u03c0 [r\u0302i(a)\u2212 bi(a)] . (3)\nFinally, the algorithm minimizes maxi\u2208[m] [ V \u2020,\u03c0\u2212i i \u2212 V \u03c0 i ] over the policy \u03c0, which serves as a surrogate of\nthe performance gap (see Lemma 1). We summarize it in Algorithm 1. Note that we only take the surrogate gap from SBSM but not the strategy-wise bonus, which is a deliberately designed bonus term depending on the policy. Instead, we design specialized bonus terms by exploiting the unique structure of the congestion game, which will be discussed in detail in later sections.\nAlgorithm 1 Surrogate Minimization for Congestion Games\nRequire: Offline dataset D 1: Compute r\u0302(a), b(a) for all a \u2208 A according to the dataset D. 2: Compute the optimistic value V \u03c0\ni and pessimistic value V \u03c0 i for all policy \u03c0 and player i by (3).\n3: Compute V \u2020,\u03c0\u2212i i = max\u03c0\u2032i\u2208\u2206(Ai) V \u03c0\u2032i,\u03c0\u2212i i .\n4: return arg min\u03c0 maxi\u2208[m] [ V \u2020,\u03c0\u2212i i \u2212 V \u03c0 i ] .\nThe sample complexity of this algorithm is guaranteed by the following theorem.\nTheorem 1. Let \u03a0 be the set of all deterministic policies, and let b be a bonus term for r\u0302. With probability 1\u2212 \u03b4, it holds that\nGap(\u03c0output) \u2264 2 max i\u2208[m] \u00ef max \u03c0\u2032\u2208\u03a0 Ea\u223c(\u03c0\u2032i,\u03c0\u2217\u2212i)bi(a) + Ea\u223c\u03c0\u2217bi(a) \u00f2 .\nwhere \u03c0output is the output of Algorithm 1.\nHere, the expectation of bonus term over some policy reflects the degree of uncertainty of the reward under that policy. Inside the operation max\u03c0\u2208\u03a0[\u00b7], the first term is for unilaterally deviated policy from \u03c0 that maximizes this uncertainty and the second term is the uncertainty for \u03c0. The full proof is deferred to Appendix A. This theorem tells us that if we want to bound the performance gap, we need to precisely estimate rewards induced by unilaterally deviated actions from the NE, which caters to Assumption 1."
        },
        {
            "heading": "3 Offline Congestion Game with Facility-level Feedback",
            "text": "Recall that for Definition 1, if \u03c0 is deterministic, the minimum value of C(\u03c0) is achieved when \u03c1 uniformly covers all actions achievable by unilaterally deviating from \u03c0 (see Proposition 1). Since having \u03c0\u2032i deviate from \u03c0 can at least cover Ai actions, the smallest value of C(\u03c0) scales with maxi\u2208[m]Ai, which is reasonable for general-sum matrix games. However, this is not acceptable for congestion games since the size of action space can be exponential (Ai \u2264 2F ). As a result, covering all possible unilateral deviations becomes intractable.\nCompared to general-sum games, congestion games with facility-level feedback inform us about not only the total reward, but also the individual rewards from all chosen facilities. This allows us to estimate the reward distribution from each facility separately. Instead of covering all unilaterally deviating actions a, we only need to make sure for any such action a and any facility f \u2208 F , we cover some actions that share the same configuration with a on f . This motivates the dataset coverage assumption on facilities rather than actions. In particular, we quantify the facility coverage condition and present the new assumption as follows.\nDefinition 3. For strategy \u03c0, facility f and integer n, the facility cumulative density is defined as d\u03c0f (n) = \u2211\na:nf (a)=n\n\u03c0(a).\nFurthermore, a facility f is said to be covered by \u03c1 at n if d\u03c1f (n) > 0.\nAssumption 2 (One-Unit Deviation). There exists an NE \u03c0\u2217 such that for any player i and policy \u03c0i \u2208 \u2206(A), facility f is covered by \u03c1 at n as long as it is covered by (\u03c0i, \u03c0\u2217\u2212i).\nIn plain text, this assumption requires us to cover all possible facility configurations induced by unilaterally deviated actions. As mentioned in Section 2.1, we can always choose \u03c0\u2217 to be deterministic. In the view of each facility, the unilateral deviation is either a player who did not select it now selects, or a player who selected it now does not select it. Thus for each f \u2208 F , it is sufficient to cover configurations with the number of players selecting f differs from that number of NE by 1. This is why we call it the one-unit deviation assumption. The facility coverage condition is visualized in Figure 1. Meanwhile, Definition 1 is adapted to this assumption as follows.\nDefinition 4. For any strategy, the facility unilateral coefficient is defined as\nCfacility = max i,\u03c0\u2032,f,d\u03c1f (n)>0\nd \u03c0\u2032i,\u03c0\u2212i f (n)\nd\u03c1f (n) .\nThe sample complexity bound depends on Cfacility (see Theorem 3). The minimum value of Cfacility is at most 3, which is acceptable (see Proposition 2). Furthermore, we show that no assumption weaker than the one-unit deviation allows NE learning, as stated in the following theorem.\nTheorem 2. Define a class X of congestion game M and exploration strategy \u03c1 that consists of all M and \u03c1 pairs that Assumption 2 is satisfied except for at most one configuration for one facility. For any algorithm ALG there exists (M,\u03c1) \u2208 X such that the output of ALG is at most a 1/2-NE strategy no matter how much data is collected.\nProof. Consider congestion game with a single facility f and five players. The action space for each player is {\u2205, {f}}. We construct the following two congestion games with deterministic rewards. Since there is only one facility, the reward players receive and whether an joint action is NE only depends on the configuration, i.e. the number of players selecting f . Hence in the remaining part of the proof we will use configuration to describe the action. For the first game, there are two NEs, which are \u201conly one player selecting f\u201d and \u201call players selecting f\u201d. For the second game, the NE is \u201cfour players selecting f\u201d. The exploration policy is set to be\n\u03c1(a) = \u00df 1/20 one, three or four players select f , 0 otherwise.\nFor the first game, we cover the first NE and its unilateral deviation except for two players selecting f . For\nRf(1) = 1 Rf (2) = \u22121 Rf (3) = 1 Rf (4) = 1 Rf(5) = 1\nCongestion Game 1\nRf (1) = 1 Rf (2) = 1 Rf (3) = 1 Rf(4) = 1 Rf (5) = \u22121\nCongestion Game 2\nthe second game, we cover the NE except for five players selecting f . Hence both game with \u03c1 are in X and are not distinguishable for ALG. Let the probability of the output policy selecting four players choosing f be p. Then it is at least p-approximate NE for game 1 and (1\u2212p)-approximate NE for game 2. In conclusion, there exists (M,\u03c1) \u2208 X such that the output of ALG is at most a 1/2-NE strategy no matter how much data is collected.\nIn the facility-level feedback setting, the bonus term is similar to that from Cui et al. (2022). First, we count the number of tuples in datasetD with n players choosing facility f asNf (n) = \u2211 ak\u2208D 1 { nf ( ak ) = n }\n. Then, we define the estimated reward function and bonus term as\nr\u0302fi (a) = \u2211 f\u2208ai\n\u2211 (ak,rk)\u2208D r f,k 1 { nf ( ak ) = nf (a) }\nNf (n) \u2228 1 , bi(a) = \u2211 f\u2208ai\n\u2026 \u03b9\nNf (n) \u2228 1 .\nHere \u03b9 = 2 log(4(m + 1)F/\u03b4). The contribution for each term in bi mimics the bonus terms from the wellknown UCB algorithm. The following theorem provides a sample complexity bound for this algorithm.\nTheorem 3. With probability 1\u2212 \u03b4, if Assumption 2 is satisfied, it holds that\nGap(\u03c0output) \u2264 8 \u221a m+ 1Cfacility\u03b9F/ \u221a n.\nThe proof of this theorem involves bounding the expectation of b by exploiting the special structure of congestion game. The actions can be classified by the configuration on one facility. This helps bound the expectation over actions, which is essentially the sum over Ai actions, by the number of players. Detailed proof is deferred to Section A in the appendix."
        },
        {
            "heading": "4 Offline Congestion Game with Agent-level Feedback",
            "text": ""
        },
        {
            "heading": "4.1 Impossibility Result",
            "text": "In the agent-level feedback setting, we no longer have access to rewards provided by individual facilities, so estimating them separately is no longer feasible. From limited actions covered in the dataset, we may not be able to precisely estimate rewards for all unilaterally deviated actions, and thus unable to learn an approximate NE. This observation is formalized in the following theorem.\nTheorem 4. Define a class X of congestion game M and exploration strategy \u03c1 that consists of all M and \u03c1 pairs such that Assumption 2 is satisfied. For any algorithm ALG there exists (M,\u03c1) \u2208 X such that the output of ALG is at most a 1/8-approximate NE no matter how much data is collected.\nProof. Consider congestion game with two facilities f1, f2 and two players. Action space for both players are unlimited, i.e. A1 = A2 = {\u2205, {f1}, {f2}, {f1, f2}}. We construct the following two congestion games with deterministic rewards. The NE for game 3 is a1 = {f1, f2}, a2 = {f1} or a2 = {f1, f2}, a1 = {f1}. The NE for game 4 is a1 = {f1}, a2 = {f2} or a2 = {f1}, a1 = {f2}. The facility coverage conditions for these NEs are marked by bold symbols in the tables. The exploration policy \u03c1 is set to be\nRf1(2) = 1/2 Rf2(2) = \u22121 Rf1(1) = 1 Rf2(1) = 1\nCongestion Game 3\nRf1(2) = \u22121/4 Rf2(2) = \u22121/4 Rf1(1) = 1 Rf2(1) = 1\nCongestion Game 4\nIt can be easily verified that both f1 and f2 are covered at 1 and 2. However, all information we may extract from the dataset is Rf1(1) + Rf2(1) = 2 and Rf1(2) + Rf2(2) = \u22121/2. It is impossible for the algorithm to distinguish these two games. Suppose the output strategy of ALG selects action such that two players select f1 with probability p. Then \u03c0 is at least a (1 \u2212 p)/4-approximate NE for the first game and at least a p/4-approximate NE for the second game. In conclusion, there exists (M,\u03c1) \u2208 X such that the output of the algorithm ALG is at most a 1/8-approximate NE strategy no matter how much data is collected."
        },
        {
            "heading": "4.2 Solution via Linear Bandit",
            "text": "In the agent-level feedback setting, a congestion game can be viewed as m linear bandits. Let \u03b8 be a d-dimensional vector where d = mF and rf (n) = \u03b8n+mf . Let Ai : A \u2192 {0, 1}d and\n[Ai(a)]j = 1{j = n+mf, f \u2208 ai, n = nf (a)}.\nHere we assign each facility an index in 0, 1, \u00b7 \u00b7 \u00b7 , F \u2212 1. Then the mean reward for player i can be written as ri(a) = \u3008Ai(a), \u03b8\u3009. In the view of bandit problem, i is the index of the bandit and the action taken is a, which is identical for all m bandits. r\u0302i(a) = \u00a8 Ai(a), \u03b8\u0302 \u2202 where \u03b8\u0302 can be estimated through ridge regression together with bonus term as follows.\n\u03b8\u0302 = V \u22121 \u2211\n(ak,rk)\u2208D \u2211 i\u2208[m] Ai(a k)rki , V = I + \u2211 (ak,rk)\u2208D \u2211 i\u2208[m] Ai(a k)Ai(a k)>. (5)\nbi(a) = \u2016Ai(a)\u2016V \u22121 \u221a \u03b2, where \u221a \u03b2 = 2 \u221a d+ \u00a0 d log \u00c5 1 + mnF\nd\n\u00e3 + \u03b9. (6)\nJin et al. (2021b) studied offline linear Markov Decision Process (MDP) and proposed a sufficient coverage assumption for learning optimal policy. A linear bandit is essentially a linear MDP with only one state and horizon equals to 1. Here we adapt the assumption to bandit setting and generalize it to congestion game in Assumption 3.\nAssumption 3 (Weak Covariance Domination). There exists a constant Cagent > 0 and an NE \u03c0 \u2217 such that for all i \u2208 [m] and policy \u03c0i, it holds that\nV I + nCagentEa\u223c(\u03c0i,\u03c0\u2217\u2212i) [ Ai(a)Ai(a) >] . (7) To see why Assumption 3 implies learnability, notice that the right hand side of (7) is equal to the expectation of the covariance matrix V if the data is collected by running policy (\u03c0j , \u03c0 \u2217 \u2212j) for Cagentn episodes. By using such a matrix, we can estimate the rewards of actions sampled from (\u03c0j , \u03c0 \u2217 \u2212j) precisely via linear regression. Here, Assumption 3 states that for all unilaterally deviated policy (\u03c0j , \u03c0 \u2217 \u2212j), we can\nestimate the rewards it generate at least as well as collecting data from (\u03c0j , \u03c0 \u2217 \u2212j) for Cagentn episodes, which implies that we can learn an approximate NE (see Theorem 1). Under Assumption 3, we can obtain the sample complexity bound as follows.\nTheorem 5. If Assumption 3 is satisfied, with probability 1\u2212 \u03b4, it holds that Gap(\u03c0output) \u2264 4 \u00a0 mF\u03b2\nCagentn ,\nwhere \u221a \u03b2 is defined in (6) and \u03c0output is the output of Algorithm 1..\nRemark 1. As an illustrative example, consider a congestion game and full action space, i.e. Ai = 2F for all player i with pure strategy NE. The dataset uniformly covers all actions where only one player deviates and only deviates on one facility. For example, if player 1 chooses {f1, f2}, the dataset should cover player 1 selecting {f1}, {f2}, {f1, f2, f3}, {f1, f2, f4}, \u00b7 \u00b7 \u00b7 with other players unchanged. There are F such actions for each player, so the dataset covers mF actions in total. The change in reward when a single player deviates from \u03c0\u2217 is the sum of change in reward from each deviated facility. With sufficient data, we can precisely estimate the change in reward from each deviated facility and estimate the reward from any unilaterally deviated action afterward. With high probability, Cagent for this example is no smaller than 1/2mF 4 (see Proposition 3 in the appendix). Hence with appropriate dataset coverage, our algorithm can achieve sampleefficient approximate NE learning in agent-level feedback."
        },
        {
            "heading": "5 Offline Congestion Game with Game-Level Feedback",
            "text": "With less information revealed in game-level feedback, a stronger assumption is required to learn an approximate NE, which is formally stated in Theorem 6.\nTheorem 6. Define a class X of congestion game M and exploration strategy \u03c1 that consists of all M and \u03c1 pairs such that Assumption 3 is satisfied. For any algorithm ALG there exists (M,\u03c1) \u2208 X such that the output of ALG is at least 1/4-approximate NE no matter how much data is collected.\nProof. Similar to the proof of Theorem 4, consider a congestion game with two facilities f1, f2 and two players. Action space for both players are unlimited. We construct the following two congestion games with deterministic rewards. The NE for game 5 is a1 = {f1}, a2 = {f1}. The NE for game 6 is a1 = {f1}, a2 = {f2} or a2 = {f1}, a1 = {f2}. The exploration policy is set to be\nRf1(2) = 1/2 Rf2(2) = \u22121 Rf1(1) = 1 Rf2(1) = \u22121\nCongestion Game 5\nRf1(2) = \u22121/2 Rf2(2) = \u22121 Rf1(1) = 1 Rf2(1) = 1\nCongestion Game 6\n\u03c1(a1, a2) =  1/5 a1 = a2 = {f2} 1/5 a1 = {f1}, a2 = \u2205 or a2 = {f1}, a1 = \u2205 1/5 a1 = {f1, f2}, a2 = {f1} or a2 = {f1, f2}, a1 = {f1} 0 otherwise\nThe reward information we can receive from the dataset in the agent-level feedback setting includes: Rf2(2), Rf1(1), Rf1(2) +Rf2(1), Rf2(1). Hence we can compute the NE directly. In the game-level feedback setting, all we can know about Rf1(2) and Rf2(1) is 2Rf1(2) +Rf2(1) = 0. Hence ALG cannot distinguish these two games. Suppose the output of ALG selects action that 2 players select f1 with probability p, then it is at least (1 \u2212 p)/2-approximate NE for game 5 and at least p/2-approximate NE for game 6. In conclusion, ALG is at least 1/4-approximate NE strategy no matter how much data is collected.\nIn the game-level feedback setting, a congestion game can be viewed as a linear bandit. Let A : A \u2192 {0, 1}d and A(a) = \u2211 i\u2208[m]Ai(a). The game-level reward can be written as r(a) = \u3008A(a), \u03b8\u3009. Thus, we can similarly use ridge regression and build bonus terms as follows.\nr\u0302i(a) = \u00a8 Ai(a), \u03b8\u0302 \u2202 , \u03b8\u0302 = V \u22121\n\u2211 (ak,rk)\u2208D A(ak)rk, V = I + \u2211 (ak,r)\u2208D A(ak)A(ak)>, (8)\nbi(a) = max i\u2208[m]\n\u2016Ai(a)\u2016V \u22121 \u221a \u03b2, where \u221a \u03b2 = 2 \u221a d+ \u00bb d log (1 + nm) + \u03b9. (9)\nThe coverage assumption is adapted from Assumption 3 as follows.\nAssumption 4 (Strong Covariance Domination). There exists a constant Cgame > 0 and an NE \u03c0 \u2217 such that for all i \u2208 [m] and policy \u03c0i, it holds that\nV I + nCgameEa\u223c(\u03c0i,\u03c0\u2217\u2212i) [ Ai(a)Ai(a) >] . (10) Note that although the statement of Assumption 4 is identical to that of Assumption 3, the definition of V has changed, so they are actually different. The interpretation of this assumption is similar to that of Assumption 3. It states that for all unilaterally deviated policy (\u03c0i, \u03c0 \u2217 \u2212i), we can estimate the reward at least as well as collecting data from (\u03c0i, \u03c0 \u2217 \u2212i) for coutputn episodes with agent-level feedback. Under this assumption, we get the sample complexity bound as follows.\nTheorem 7. If Assumption 4 is satisfied, with probability 1\u2212 \u03b4, it holds that Gap(\u03c0output) \u2264 4 \u00a0 mF\u03b2\nCgamen ,\nwhere \u03b2 is defined in equation (9) and \u03c0output is the output of Algorithm 1.\nRemark 2. As an illustrative example, consider a congestion game with full action space and pure strategy NE. Let the numbers of players selecting each facility be (n1, n2, \u00b7 \u00b7 \u00b7 , nf ). The dataset uniformly contains the following actions: action where the number of players selecting each facility are (0, n2, \u00b7 \u00b7 \u00b7 , nf ), (n1 \u2212 1, n2, \u00b7 \u00b7 \u00b7 , nf ), (n1 + 1, n2, \u00b7 \u00b7 \u00b7 , nf ) and similar actions for other facilities. Besides, we cover an NE action. From this dataset, we can precisely estimate the reward from each single facility with one-unit deviation configuration from NE and hence estimate the reward of unilaterally deviated actions. With high probability, Cgame for this example is no smaller than 1/24F 3 (see Proposition 4 in the appendix). Hence with appropriate dataset coverage, our algorithm can achieve sample-efficient approximate NE learning in game-level feedback."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we studied NE learning for congestion games in the offline setting. We analyzed the problem under various types of feedback. Hard instances were constructed to show separations between different types of feedback. For each type of feedback, we identified dataset coverage assumptions to ensure NE learning. With tailored reward estimators and bonus terms, we showed the surrogate minimization algorithm is able to find an approximate NE efficiently."
        },
        {
            "heading": "A Omitted Proof in Section 3",
            "text": "Lemma 1. With probability 1\u2212 \u03b4, for any policy \u03c0, we have\nGap(\u03c0) \u2264 max i\u2208[m]\n[ V \u2020,\u03c0\u2212i i \u2212 V \u03c0 i ] .\nIn addition, we have\nGap(\u03c0output) \u2264 min \u03c0 max i\u2208[m]\n[ V \u2020,\u03c0\u2212i i \u2212 V \u03c0 i ] .\nProof. By (3) and (2), with probability 1\u2212 \u03b4\nV \u03c0i \u2264 V \u03c0i \u2264 V \u03c0 i .\nHence\nGap(\u03c0) = max \u03c0\u2032\n[ V \u03c0\u2032i,\u03c0\u2212i i \u2212 V \u03c0 i ] \u2264 max\n\u03c0\u2032 max i\u2208[m]\n[ V \u03c0\u2032i,\u03c0\u2212i i \u2212 V \u03c0 i ] .\nSince both V \u03c0\u2032i,\u03c0\u2212i i and V \u03c0 i are linear in each entry of \u03c0, the first maximizer on the RHS must correspond to a deterministic policy. This proves the first statement. The second statement is by the fact that the algorithm minimizes the RHS of the first statement.\nTheorem 1. Let \u03a0 be the set of all deterministic policies, and let b be a bonus term for r\u0302. With probability 1\u2212 \u03b4, it holds that\nGap(\u03c0output) \u2264 2 max i\u2208[m] \u00ef max \u03c0\u2032\u2208\u03a0 Ea\u223c(\u03c0\u2032i,\u03c0\u2217\u2212i)bi(a) + Ea\u223c\u03c0\u2217bi(a) \u00f2 .\nwhere \u03c0output is the output of Algorithm 1.\nProof.\nV \u03c0i \u2212 V \u03c0 i = Ea\u223c\u03c0[ri(a)\u2212 r\u0302i(a) + bi(a)] \u2264 2Ea\u223c\u03c0bi(a) V \u03c0\ni \u2212 V \u03c0i = Ea\u223c\u03c0[r\u0302i(a)\u2212 ri(a) + bi(a)] \u2264 2Ea\u223c\u03c0bi(a).\nLet \u03c0\u0303 = arg max\u03c0\u2032 maxi\u2208[m] [ V \u03c0\u2032i,\u03c0\u2212i i \u2212 V \u03c0 i ] . By similar argument in Lemma 1 we know that we can always choose \u03c0\u0303 \u2208 \u03a0.\nGap ( \u03c0output ) \u2264min\n\u03c0 max i\u2208[m]\n[ V \u2020,\u03c0\u2212i i \u2212 V \u03c0 i ] = min\n\u03c0 max i\u2208[m]\n[ V \u03c0\u0303i,\u03c0\u2212i i \u2212 V \u03c0 i ] \u2264min\n\u03c0 max i\u2208[m]\n\u00ee V \u03c0\u0303i,\u03c0\u2212i i \u2212 V \u03c0 i + 2Ea\u223c(\u03c0\u0303i,\u03c0\u2212i)bi(a) + 2Ea\u223c\u03c0bi(a) \u00f3 \u2264min\n\u03c0 \u00df max i\u2208[m] \u00ee V \u03c0\u0303i,\u03c0\u2212i i \u2212 V \u03c0 i \u00f3 + max i\u2208[m] [ 2Ea\u223c(\u03c0\u0303i,\u03c0\u2212i)bi(a) + 2Ea\u223c\u03c0bi(a) ]\u2122 = min\n\u03c0\n\u00df Gap(\u03c0) + max\ni\u2208[m] \u00ef 2 max \u03c0\u2032\u2208\u03a0 Ea\u223c(\u03c0\u2032i,\u03c0\u2212i)bi(a) + 2Ea\u223c\u03c0bi(a) \u00f2\u2122\n\u2264Gap(\u03c0\u2217) + max i\u2208[m] \u00ef 2 max \u03c0\u2032\u2208\u03a0 Ea\u223c(\u03c0\u2032i,\u03c0\u2217\u2212i)bi(a) + 2Ea\u223c\u03c0\u2217bi(a) \u00f2\n=2 max i\u2208[m] \u00ef max \u03c0\u2032\u2208\u03a0 Ea\u223c(\u03c0\u2032i,\u03c0\u2217\u2212i)bi(a) + Ea\u223c\u03c0\u2217bi(a) \u00f2\nLemma 2. With probability 1\u2212 \u03b4, we have\n|ri(a)\u2212 r\u0302i(a)| \u2264 bi(a), 1 Nf (n) \u2264 4H\u03b9 nd\u03c1f (n)\nfor all a \u2208 A, i \u2208 [m].\nProof.\nri(a)\u2212 r\u0302i(a) = \u2211 f\u2208ai [ r\u0302f ( nf (a) ) \u2212 rf ( nf (a) )] .\nBy Hoeffding\u2019s bound and union bound we have \u2223\u2223\u2223r\u0302f (nf (a))\u2212 rf (nf (a))\u2223\u2223\u2223 \u2264\u00a0 2 Nf (nf (a)) log 4(m+ 1)F \u03b4\nfor all f \u2208 F , a \u2208 A with probability 1 \u2212 \u03b4/2. Combine the above inequalities we get the first statement hold with probability 1\u2212 \u03b4/2. By lemma A.1 of Xie et al. (2021b), replacing p by d\u03c1f ( nf (a) ) and the union bound we get\n1 Nf (n) \u2264 8 log(2(m+ 1)F/\u03b4) nd\u03c1f (n) \u2264 4\u03b9 nd\u03c1f (n)\nfor all f \u2208 G, a \u2208 A with probability 1\u2212 \u03b4/2. Finally, the proof is complete by using the union bound.\nTheorem 3. With probability 1\u2212 \u03b4, if Assumption 2 is satisfied, it holds that\nGap(\u03c0output) \u2264 8 \u221a m+ 1Cfacility\u03b9F/ \u221a n.\nProof. We have\nEa\u223c(\u03c0\u2032i,\u03c0\u2217\u2212i)bi(a) = \u2211 f\u2208F Ea\u223c(\u03c0\u2032i,\u03c0\u2217\u2212i) \u2026\n\u03b9\nNf (nf (a)) \u2228 1\n\u2264Cfacility \u2211 f\u2208F m\u2211 n\u2032=0 d\u03c1f (n)\n\u221a 4\u03b92\nnd\u03c1f (n \u2032)\n=2Cfacility\u03b9 \u2211 f\u2208F m\u2211 n\u2032=0\n\u00a0 d\u03c1f (n \u2032)\nn\n\u22642Cfacility\u03b9 \u2211 f\u2208F\n\u00c3 m+ 1\nn m\u2211 n\u2032=0 d\u03c1f (n \u2032)\n\u22642 \u221a m+ 1Cfacility\u03b9F/ \u221a n\nThe first inequality is by Definition 4 and Lemma 2. The second inequality is by the fact that d\u03c1f (a) \u2264 1. Combine this with Theorem 1 and Lemma 2 we get the conclusion.\nA.1 Omitted Calculations in Section 3\nProposition 1. Suppose \u03c0 is a deterministic strategy. For a fixed domain of \u03c1, the value of C(\u03c0) is the smallest when \u03c1 is uniform over all actions achievable from unilaterally deviating from \u03c0.\nProof. Assume that \u03c1 covers an action a which is not achievable from unilaterally deviating from pi, then we construct a new \u03c1\u2032 where \u03c1\u2032(a) = 0 and the other entries scales up by factor 1/(1 \u2212 \u03c1(a)). \u03c1\u2032 achieves larger C(\u03c0) than \u03c1. Hence \u03c1 only cover the actions achievable from unilaterally deviating from \u03c0.\nAssume that the distribution is not uniform. Since the best response to a pure strategy can always taken to be a pure strategy, the numerator of 1 can always achieve 1 no matter what a is. Let a\u2217 = arg mina \u03c1(a), then there exists a\u0303 such that \u03c1(a\u0303) > \u03c1(a\u2217). Construct \u03c1\u2032 such that \u03c1\u2032(a\u2217) = \u03c1\u2032(a\u0303) = (\u03c1(a\u2217) + \u03c1(a\u0303))/2, then C(\u03c0) would not increase. By contradiction we get the conclusion.\nProposition 2. The minimum value of Cfacility is no larger than 3.\nProof. Consider the case when \u03c1 is a policy that induces uniform coverage on all facility configurations achievable from \u03c0\u2217. Since at most three configurations are covered for each facility, the minimum value of d\u03c1f (n) is 1/3. Thus the minimum value of C\u0303(\u03c0 \u2217) is no larger than 3"
        },
        {
            "heading": "B Omitted Proof in Section 4",
            "text": "Lemma 3. With probability 1\u2212 \u03b4 we have\n|ri(a)\u2212 r\u0302i(a)| \u2264 bi(a)\nfor all i \u2208 [m],a \u2208 A.\nProof. As a degenerate version of theorem 20.5 of Lattimore & Szepesva\u0301ri (2020), we have with probability 1\u2212 \u03b4 it holds that \u2225\u2225\u2225\u03b8\u0302 \u2212 \u03b8\u2225\u2225\u2225\nV \u2264 \u2016\u03b8\u20162 +\n\u00bb log det(V ) + 2 log(1/\u03b4).\nHence with probability 1\u2212 \u03b4 |ri(a)\u2212 r\u0302i(a)| = \u2223\u2223\u2223\u00a8Ai(a), \u03b8\u0302 \u2212 \u03b8\u2202\u2223\u2223\u2223\n\u2264 \u2016Ai(a)\u2016V \u22121 \u2225\u2225\u2225\u03b8\u0302 \u2212 \u03b8\u2225\u2225\u2225\nV \u2264 \u2016Ai(a)\u2016V \u22121 ( \u2016\u03b8\u20162 + \u00bb log det(V ) + 2 log (1/\u03b4)\n) for all i \u2208 [m] and a \u2208 A. By Lemma 4 of Cui et al. (2022) we have\ndet(V ) \u2264 \u00c5 1 + mnF\nd \u00e3d since by (5) \u2016Ai(a)\u201622 \u2264 F . Besides, \u2016\u03b8\u20162 \u2264 2 \u221a d. The proof is complete by combining all these and taking maxi\u2208[m].\nTheorem 5. If Assumption 3 is satisfied, with probability 1\u2212 \u03b4, it holds that Gap(\u03c0output) \u2264 4 \u00a0 mF\u03b2\nCagentn ,\nwhere \u221a \u03b2 is defined in (6) and \u03c0output is the output of Algorithm 1..\nProof. We have for all i \u2208 [m]\nEa\u223c(\u03c0i,\u03c0\u2217\u2212i)bi(a) =Ea\u223c(\u03c0i,\u03c0\u2217\u2212i) \u00bb A>i (a)V \u22121Ai(a)\n\u2264Ea\u223c(\u03c0i,\u03c0\u2217\u2212i) \u2026 A>j (a) \u00c4 I + CagentnEa\u2032\u223c(\u03c0i,\u03c0\u2217\u2212i) [Ai(a \u2032)Ai(a\u2032)>] \u00e4\u22121 Ai(a)\n=Ea\u223c(\u03c0i,\u03c0\u2217\u2212i) \u2026 tr [\u00c4 I + CagentnEa\u2032\u223c(\u03c0i,\u03c0\u2217\u2212i) [Ai(a \u2032)Ai(a\u2032)>] \u00e4\u22121 Ai(a)A>i (a) ]\n\u2264Ea\u223c(\u03c0i,\u03c0\u2217\u2212i) \u2026 tr [\u00c4 I + CagentnEa\u2032\u223c(\u03c0i,\u03c0\u2217\u2212i) [Ai(a \u2032)Ai(a\u2032)>] \u00e4\u22121 Ai(a)A>i (a) ]\n\u2264 \u2026 tr [\u00c4 I + CagentnEa\u2032\u223c(\u03c0i,\u03c0\u2217\u2212i) [Ai(a \u2032)Ai(a\u2032)>] \u00e4\u22121 Ea\u223c(\u03c0i,\u03c0\u2217\u2212i)Ai(a)A > i (a)\n] =\n1\u221a Cagentn \u2026 tr [ I \u2212 \u00c4 I + CagentnEa\u2032\u223c(\u03c0i,\u03c0\u2217\u2212i) [Ai(a \u2032)Ai(a\u2032)>] \u00e4\u22121]\n\u2264 \u00a0 d\nCagentn =\n\u00a0 mF\nCagentn\nCombine this with Theorem 1 we get the conclusion.\nB.1 Omitted Calculations in Section 4\nLemma 4. If n \u2265 8 log((mF + 1)/\u03b4)(mF + 1), with probability 1\u2212 \u03b4, N(a) \u2265 \u03c1(a)n/2 = n/2(mF + 1) for all a with \u03c1(a) > 0.\nProof. N(a) follows binomial distribution with parameters n and \u03c1(a). By Chernoff bound, for all \u03b5 \u2208 R+, Pr {N(a) \u2264 (1\u2212 \u03b5)n\u03c1(a)} \u2264 exp \u00c5 \u2212\u03b5 2n\u03c1(a)\n2 \u00e3 Hence if \u03c1 \u2265 \u22128 log \u03b4/n, we have for all a covered in the example, we have\nPr {N(a) \u2265 (1\u2212 \u03b5)n\u03c1(a)} \u2265 1\u2212 exp(\u2212\u00b5/8) \u2265 1\u2212 \u03b4.\nBy construction \u03c1(a) = 1/(mF + 1) for covered action a. By the union bound we get the conclusion.\nProposition 3. If n \u2265 8 log((mF + 1)/\u03b4)(mF + 1) and Cagent = 1/2mF 4, then with probability 1 \u2212 \u03b4, Assumption 3 holds for the example described in Remark 1.\nProof. It suffices to show that inequality 7 holds for all pure strategy \u03c0 and i because the right hand side is linear in any entry of \u03c0i. From now on, let us focus on some specific i and pure strategy (\u03c0i, \u03c0 \u2217 \u2212i) choosing a\u0303 deterministically. Without loss of generality, suppose among all elements in a\u0303, facility that deviates from NE are f1, f2, \u00b7 \u00b7 \u00b7 , fs. For convenience, let A0 = Ai(a \u2217) where a\u2217 is the NE. For fj , to estimate its contribution to the reward change, we need an action besides a\u2217, which we denote as afj . That is, afj deviates from a\u2217 only on fj and let Aj = Ai(a\nfj ). Without loss of generality, suppose the contribution corresponds to \u3008Aj \u2212 A0, \u03b8\u3009. Then we can write\nAi(a\u0303) = A0 + \u2211 j\u2208[s] (Aj \u2212A0) = \u2211 j\u2208[s] Aj + (1\u2212 s)A0\nBy Lemma 4, it suffices to show\nI + n\n2(mF + 1) \u2211 j\u2208[s] AjA > j +\nn\n2(mF + 1) A0A\n> 0 I + CagentnAi(a\u0303)Ai(a\u0303)>.\nIn other words, for any x \u2208 RmF , we have n\n2(mF + 1) \u2211 j\u2208[s] x>AjA > j x+\nn\n2(mF + 1) x>A0A > 0 x \u2265 Cagentnx>Ai(a\u0303)Ai(a\u0303)>x.\nFor convenience, let xi = x >Ai, this inequality can be rewritten as\nn\n2(mF + 1) \u2211 j\u2208[s] x2j + n 2(mF + 1) x20 \u2265 Cagentn \u2211 j\u2208[s] xj + (1\u2212 s)x0 2 . By Jensen\u2019s inequality it suffices to show\nn\n2(mF + 1) \u2211 j\u2208[s] x2j + n 2(mF + 1) x20 \u2265 Cagentn(s+ 1) \u2211 j\u2208[s] x2j + (1\u2212 s)2x20  . Hence it suffices to show\nn\n2(mF + 1) \u2265 Cagentn(F + 1)(F \u2212 1)2"
        },
        {
            "heading": "C Omitted Proof in Section 5",
            "text": "Lemma 5. With probability 1\u2212 \u03b4 we have\n|ri(a)\u2212 r\u0302i(a)| \u2264 bi(a)\nfor all i \u2208 [m],a \u2208 A.\nProof. Similar to Lemma 3, we have |ri(a)\u2212 r\u0302i(a)| \u2264 \u2016Ai(a)\u2016V \u22121 ( \u2016\u03b8\u20162 + \u00bb log det(V ) + 2 log(1/\u03b4) ) .\nThe bound of det(V ) is now as follows. The basic idea is the same as that of lemma 4 by Cui et al. (2022) det(V ) \u2264 \u00c5 tr(V )\nd\n\u00e3d = \u00c7 tr(I) + \u2211 (ak,r)\u2208D \u2016A(ak)\u201622\nd\n\u00e5d \u2264 \u00c5 d+ nm2F\nd\n\u00e3d = (1 + nm)mF\nBesides, \u2016\u03b8\u20162 \u2264 2 \u221a d. Combine all these we get the conclusion.\nTheorem 7. If Assumption 4 is satisfied, with probability 1\u2212 \u03b4, it holds that Gap(\u03c0output) \u2264 4 \u00a0 mF\u03b2\nCgamen ,\nwhere \u03b2 is defined in equation (9) and \u03c0output is the output of Algorithm 1.\nProof. The proof is identical to that of Theorem 5 except that Cgame is used instead of Cgame.\nC.1 Omitted Calculations in Section 5\nLemma 6. If n \u2265 8 log((3F + 1)/\u03b4)(3F + 1), with probability 1\u2212 \u03b4, N(a) \u2265 \u03c1(a)n/2 = n/2(3F + 1) for all a with \u03c1(a) > 0.\nThe proof is identical to Lemma 4 except that we have at most 3F +1 actions to cover instead of at most mF + 1 actions.\nProposition 4. If n \u2265 8 log((3F+1)/\u03b4)(3F+1) and Cgame = 1/24F 3, then with probability 1\u2212\u03b4, Assumption 4 holds for the exampled described in Remark 2.\nProof. The procedure is similar to that in the proof of Proposition 3. Let us focus on some specific pure strategy \u03c0 choosing a\u0303 deterministically and player i. To calculate the reward from one facility, we need two actions. Suppose a\u0303i covers f1, f2, \u00b7 \u00b7 \u00b7 , fs with configuration n1, n2, \u00b7 \u00b7 \u00b7 , ns. Let the action vector corresponding to facility fj be Aj,1 and Aj,2. Without loss of generality, suppose the reward from the indivial facility is \u3008Aj,1 \u2212Aj,2, \u03b8\u3009/nj . Then we can write\nAi(a\u0303) = \u2211 j\u2208[s] Aj,1 \u2212Aj,2 nj\nIt suffices to show\nn\n2F (3F + 1) \u2211 j\u2208[s] ( Aj,1A > j,1 +Aj,2A > j,2 ) CgamenAi(a\u0303)Ai(a\u0303)>.\nNote that because {Aj,1, Aj,2} may have repeated elements and repeats at most F times, so we further discount the number of samples on the left hand side by F . Following similar procedure in the proof of Proposition 4 and nj \u2265 1, s \u2264 F we get it suffices to show\nn\n2F (3F + 1) \u2265 Cgamen2F"
        }
    ],
    "title": "Offline congestion games: How feedback type affects data coverage requirement",
    "year": 2022
}