{
    "abstractText": "The success of digital content depends largely on whether viewers empathize with stories and narratives. Researchers have investigated the elements that may elicit empathy from viewers. Empathic response involves affective and cognitive processes and is expressed through multiple verbal and nonverbal modalities. Specifically, eye movements communicate emotions and intentions and may reflect an empathic status. This study explores feature changes in eye movements when a viewer empathizes with the video\u2019s content. Seven feature variables of eye movements (change of pupil diameter, peak pupil dilation, very short, mid, over long fixation duration, saccadic amplitude, and saccadic count) were extracted from 47 participants who viewed eight videos (four empathic videos and four non-empathic videos) distributed in a two-dimensional emotion axis (arousal and valence). The results showed that viewers\u2019 saccadic amplitude and peak pupil dilation in the eigenvalues of eye movements increased in the empathic condition. The fixation time and pupil size change showed limited significance, and whether there were asymmetric pupil responses between the left and right pupils remained inconclusive. Our investigation suggests that saccadic amplitude and peak pupil dilation are reliable measures for recognizing whether viewers empathize with content. The findings provide physiological evidence based on eye movements that both affective and cognitive processes accompany empathy during media consumption.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jing Zhang"
        },
        {
            "affiliations": [],
            "name": "Sung Park"
        },
        {
            "affiliations": [],
            "name": "Ayoung Cho"
        },
        {
            "affiliations": [],
            "name": "Mincheol Whang"
        }
    ],
    "id": "SP:43e7c5cf68fa2ae20a067b819d499a30b8019928",
    "references": [
        {
            "authors": [
                "M.L. Hoffman"
            ],
            "title": "Empathy and Moral Development: Implications for Caring and Justice",
            "year": 2001
        },
        {
            "authors": [
                "M.H. Davis"
            ],
            "title": "Measuring individual differences in empathy: Evidence for a multidimensional approach",
            "venue": "J. Pers. Soc. Psychol",
            "year": 1983
        },
        {
            "authors": [
                "S.D. Preston",
                "F.B.M. De Waal"
            ],
            "title": "Empathy: Its ultimate and proximate bases",
            "venue": "Behav. Brain Sci",
            "year": 2002
        },
        {
            "authors": [
                "B.M.P. Cuff",
                "S.J. Brown",
                "L. Taylor",
                "D.J. Howat"
            ],
            "title": "Empathy: A review of the concept",
            "venue": "Emot. Rev. 2016,",
            "year": 2016
        },
        {
            "authors": [
                "M.L. Hoffman"
            ],
            "title": "Interaction of affect and cognition in empathy. In Emotions, Cognition, and Behavior",
            "year": 1985
        },
        {
            "authors": [
                "N. Eisenberg",
                "R.A. Fabes",
                "P.A. Miller",
                "J. Fultz",
                "R. Shell",
                "R.M. Mathy",
                "R.R. Reno"
            ],
            "title": "Relation of sympathy and personal distress to prosocial behavior: A multimethod study",
            "venue": "J. Pers. Soc. Psychol",
            "year": 1989
        },
        {
            "authors": [
                "F. De Vignemont",
                "T. Singer"
            ],
            "title": "The empathic brain: How, when and why",
            "venue": "Trends Cogn. Sci",
            "year": 2006
        },
        {
            "authors": [
                "G. Rizzolatti",
                "L. Fadiga",
                "V. Gallese",
                "L. Fogassi"
            ],
            "title": "Premotor cortex and the recognition of motor actions",
            "venue": "Cogn. Brain Res",
            "year": 1996
        },
        {
            "authors": [
                "B. Wicker",
                "C. Keysers",
                "J. Plailly",
                "J.-P. Royet",
                "V. Gallese",
                "G. Rizzolatti"
            ],
            "title": "Both of us disgusted in My insula: The common neural basis of seeing and feeling disgust",
            "venue": "Neuron",
            "year": 2003
        },
        {
            "authors": [
                "C. Keysers",
                "B. Wicker",
                "V. Gazzola",
                "J.-L. Anton",
                "L. Fogassi",
                "V. Gallese"
            ],
            "title": "A touching sight: SII/PV activation during the observation and experience of touch",
            "venue": "Neuron",
            "year": 2004
        },
        {
            "authors": [
                "T. Singer",
                "B. Seymour",
                "J. O\u2019doherty",
                "H. Kaube",
                "R.J. Dolan",
                "C.D. Frith"
            ],
            "title": "Empathy for pain involves the affective but not sensory components of pain",
            "venue": "Science",
            "year": 2004
        },
        {
            "authors": [
                "K.G. Thompson",
                "K.L. Biscoe",
                "T.R. Sato"
            ],
            "title": "Neuronal basis of covert spatial attention in the frontal eye",
            "venue": "field. J. Neurosci",
            "year": 2005
        },
        {
            "authors": [
                "C.A. Curcio",
                "K.A. Allen"
            ],
            "title": "Topography of ganglion cells in human retina",
            "venue": "J. Comp. Neurol",
            "year": 1990
        },
        {
            "authors": [
                "B.W. Tatler",
                "J.R. Brockmole",
                "R.H.S. Carpenter"
            ],
            "title": "LATEST: A model of saccadic decisions in space and time",
            "venue": "Psychol. Rev",
            "year": 2017
        },
        {
            "authors": [
                "R.H.S. Carpenter"
            ],
            "title": "The neural control of looking",
            "venue": "Curr. Biol",
            "year": 2000
        },
        {
            "authors": [
                "M. Argyle",
                "M. Cook"
            ],
            "title": "Gaze and Mutual Gaze",
            "year": 1976
        },
        {
            "authors": [
                "S. Baron-Cohen",
                "R. Campbell",
                "A. Karmiloff-Smith",
                "J. Grant",
                "J. Walker"
            ],
            "title": "Are children with autism blind to the mentalistic significance of the eyes? Br",
            "venue": "J. Dev. Psychol",
            "year": 1995
        },
        {
            "authors": [
                "N.J. Emery"
            ],
            "title": "The eyes have it: The neuroethology, function and evolution of social gaze",
            "venue": "Neurosci. Biobehav. Rev",
            "year": 2000
        },
        {
            "authors": [
                "B.M. Hood",
                "J.D. Willen",
                "J. Driver"
            ],
            "title": "Adult\u2019s eyes trigger shifts of visual attention in human infants",
            "venue": "Psychol. Sci",
            "year": 1998
        },
        {
            "authors": [
                "K.A. Pelphrey",
                "N.J. Sasson",
                "J.S. Reznick",
                "G. Paul",
                "B.D. Goldman",
                "J. Piven"
            ],
            "title": "Visual scanning of faces in autism",
            "venue": "J. Autism. Dev. Disord",
            "year": 2002
        },
        {
            "authors": [
                "H. Wang",
                "M. Chignell",
                "M. Ishizuka"
            ],
            "title": "Empathic tutoring software agents using real-time eye tracking",
            "venue": "In Proceedings of the 2006 Symposium on Eye Tracking Research & Applications,",
            "year": 2006
        },
        {
            "authors": [
                "M. Patterson",
                "R. Elliott"
            ],
            "title": "Negotiating masculinities: Advertising and the inversion of the male gaze",
            "venue": "Consum. Mark Cult. 2002,",
            "year": 2002
        },
        {
            "authors": [
                "E.H. Hess"
            ],
            "title": "Attitude and pupil size",
            "venue": "Sci. Am",
            "year": 1965
        },
        {
            "authors": [
                "M.E. Kret"
            ],
            "title": "The role of pupil size in communication. Is there room for learning",
            "venue": "Cogn. Emot",
            "year": 2018
        },
        {
            "authors": [
                "E.H. Hess"
            ],
            "title": "The role of pupil size in communication",
            "venue": "Sci. Am",
            "year": 1975
        },
        {
            "authors": [
                "Y. Wei"
            ],
            "title": "Explore the Blended Teaching Model from the Perspective of Cognitive Load",
            "venue": "In Proceedings of the 5th International Conference on Education, Management, Arts, Economics and Social Science, Sanya, China,",
            "year": 2018
        },
        {
            "authors": [
                "J. Hy\u00f6n\u00e4",
                "J. Tommola",
                "A.-M. Alaja"
            ],
            "title": "Pupil dilation as a measure of processing load in simultaneous interpretation and other language",
            "venue": "tasks. Q. J. Exp. Psychol",
            "year": 1995
        },
        {
            "authors": [
                "O. Kang",
                "T. Wheatley"
            ],
            "title": "Pupil dilation patterns spontaneously synchronize across individuals during shared attention",
            "venue": "J. Exp. Psychol. Gen",
            "year": 2017
        },
        {
            "authors": [
                "M.E. Kret",
                "A.H. Fischer",
                "C.K.W. De Dreu"
            ],
            "title": "Pupil mimicry correlates with trust in in-group partners with dilating pupils",
            "venue": "Psychol. Sci",
            "year": 2015
        },
        {
            "authors": [
                "N.A. Harrison",
                "C.E. Wilson",
                "H.D. Critchley"
            ],
            "title": "Processing of observed pupil size modulates perception of sadness and predicts empathy",
            "venue": "Emotion 2007,",
            "year": 2007
        },
        {
            "authors": [
                "J. Zhang",
                "X. Wen",
                "M. Whang"
            ],
            "title": "Empathy evaluation by the physical elements of the advertising",
            "venue": "Multimed Tools Appl",
            "year": 2021
        },
        {
            "authors": [
                "S. Liang",
                "R. Liu",
                "J. Qian"
            ],
            "title": "Fixation prediction for advertising images: Dataset and benchmark",
            "venue": "J. Vis. Commun. Image Represent. 2021,",
            "year": 2022
        },
        {
            "authors": [
                "W.H. Rizvi"
            ],
            "title": "Brand visual eclipse (BVE): When the brand fixation spent is minimal in relation to the celebrity",
            "venue": "In Information Systems and Neuroscience; Springer: Berlin/Heidelberg, Germany,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Li",
                "G. Wang",
                "Q. Gan"
            ],
            "title": "Research on Cognitive Effects of Narrative Rhetoric in Print Advertisement Based on Eye Movement",
            "venue": "In Proceedings of theE3S Web of Conferences, Odesa, Ukraine,",
            "year": 2021
        },
        {
            "authors": [
                "C.-C. Wang",
                "J.C. Hung"
            ],
            "title": "Comparative analysis of advertising attention to Facebook social network: Evidence from eye-movement data",
            "venue": "Comput. Human. Behav",
            "year": 2019
        },
        {
            "authors": [
                "J.A. Russell"
            ],
            "title": "A circumplex model of affect",
            "venue": "J. Pers Soc. Psychol",
            "year": 1980
        },
        {
            "authors": [
                "H. Soh"
            ],
            "title": "Measuring consumer empathic response to advertising drama",
            "venue": "J. Korea Contents Assoc",
            "year": 2014
        },
        {
            "authors": [
                "M.G.C. Salanga",
                "A.B.I. Bernardo"
            ],
            "title": "Cognitive empathy in intercultural interactions: The roles of lay theories of multiculturalism and polyculturalism",
            "venue": "Curr. Psychol",
            "year": 2019
        },
        {
            "authors": [
                "A. Alcorta-Garza",
                "M. San-Mart\u00edn",
                "R. Delgado-Bolton",
                "J. Soler-Gonz\u00e1lez",
                "H. Roig",
                "L. Vivanco"
            ],
            "title": "Cross-validation of the Spanish HP-version of the jefferson scale of empathy confirmed with some cross-cultural differences",
            "venue": "Front Psychol. 2016,",
            "year": 2016
        },
        {
            "authors": [
                "M. Patterson"
            ],
            "title": "Nonverbal Interpersonal Communication",
            "venue": "In Oxford Research Encyclopedia of Communication;",
            "year": 2018
        },
        {
            "authors": [
                "Z. Hu",
                "M. Gendron",
                "Q. Liu",
                "G. Zhao",
                "H. Li"
            ],
            "title": "Trait anxiety impacts the perceived gaze direction of fearful but not angry faces",
            "venue": "Front Psychol. 2017,",
            "year": 2017
        },
        {
            "authors": [
                "W. Verbeke",
                "R.I. Pozharliev"
            ],
            "title": "Preference Inferences from Eye-Related Cues in Sales-Consumer Settings: ERP Timing and Localization in Relation to Inferring Performance and Oxytocin Receptor (OXTR) Gene Polymorphisms",
            "venue": "Int. J. Mark. Stud. 2016,",
            "year": 2016
        },
        {
            "authors": [
                "P. Sajjacholapunt",
                "L.J. Ball"
            ],
            "title": "The influence of banner advertisements on attention and memory: Human faces with averted gaze can enhance advertising effectiveness",
            "venue": "Front Psychol. 2014,",
            "year": 2014
        },
        {
            "authors": [
                "J.R. Bergstrom",
                "S. Duda",
                "D. Hawkins",
                "M. McGill"
            ],
            "title": "Physiological response measurements",
            "venue": "In Eye Tracking in User Experience Design; Elsevier: Amsterdam, The Netherlands,",
            "year": 2014
        },
        {
            "authors": [
                "S. Ahern",
                "J. Beatty"
            ],
            "title": "Pupillary responses during information processing vary with Scholastic Aptitude",
            "venue": "Test scores. Science",
            "year": 1979
        },
        {
            "authors": [
                "C.H. Chatham",
                "M.J. Frank",
                "Y. Munakata"
            ],
            "title": "Pupillometric and behavioral markers of a developmental shift in the temporal dynamics of cognitive control",
            "venue": "Proc Natl Acad Sci. USA",
            "year": 2009
        },
        {
            "authors": [
                "J. Beatty"
            ],
            "title": "Task-evoked pupillary responses, processing load, and the structure of processing",
            "year": 1982
        },
        {
            "authors": [
                "Q. Wang",
                "M. Wedel",
                "L. Huang",
                "X. Liu"
            ],
            "title": "Effects of model eye gaze direction on consumer visual processing: Evidence from China and America",
            "venue": "Inf. Manag",
            "year": 2018
        },
        {
            "authors": [
                "T. Clausen-May"
            ],
            "title": "Teaching Maths to Pupils with Different Learning Styles; SAGE: Newcastle upon Tyne",
            "year": 2005
        },
        {
            "authors": [
                "A. Allport"
            ],
            "title": "Selection for Action: Some Behavioral and Neurophysiological Considerations of Attention and Action",
            "year": 2016
        },
        {
            "authors": [
                "H. Heuer",
                "A. Sanders"
            ],
            "title": "Perspectives on Perception and Action; Routledge: London, UK, 2016",
            "year": 2016
        },
        {
            "authors": [
                "M. Liotti",
                "D.M. Tucker"
            ],
            "title": "Emotion in Asymmetric Corticolimbic Networks; APA",
            "year": 1995
        },
        {
            "authors": [
                "R. Adolphs",
                "H. Damasio",
                "D. Tranel",
                "G. Cooper",
                "A.R. Damasio"
            ],
            "title": "A role for somatosensory cortices in the visual recognition of emotion as revealed by three-dimensional lesion",
            "venue": "mapping. J. Neurosci",
            "year": 2000
        },
        {
            "authors": [
                "J. Decety",
                "J. Grezes",
                "N. Costes",
                "D. Perani",
                "M. Jeannerod",
                "E. Procyk",
                "F. Grassi",
                "F. Fazio"
            ],
            "title": "Brain activity during observation of actions. Influence of action content and subject\u2019s strategy",
            "venue": "Brain J. Neurol",
            "year": 1997
        },
        {
            "authors": [
                "T. Canli",
                "J.E. Desmond",
                "Z. Zhao",
                "G. Glover",
                "J.D.E. Gabrieli"
            ],
            "title": "Hemispheric asymmetry for emotional stimuli detected with fMRI",
            "venue": "Neuroreport",
            "year": 1998
        },
        {
            "authors": [
                "G.E. Schwartz",
                "R.J. Davidson",
                "F. Maer"
            ],
            "title": "Right hemisphere lateralization for emotion in the human brain: Interactions with cognition",
            "venue": "Science",
            "year": 1975
        },
        {
            "authors": [
                "M. Oliva",
                "A. Anikin"
            ],
            "title": "Pupil dilation reflects the time course of emotion recognition in human vocalizations",
            "venue": "Sci. Rep. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "O. Colizoli",
                "J.W. de Gee",
                "A.E. Urai",
                "T.H. Donner"
            ],
            "title": "Task-evoked pupil responses reflect internal belief states",
            "venue": "Sci. Rep. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "P. Wright",
                "D. Kahneman"
            ],
            "title": "Evidence for alternative strategies of sentence retention",
            "venue": "Q. J. Exp. Psychol",
            "year": 1971
        },
        {
            "authors": [
                "J.M. Findlay",
                "R. Walker"
            ],
            "title": "A model of saccade generation based on parallel processing and competitive inhibition",
            "venue": "Behav. Brain Sci",
            "year": 1999
        },
        {
            "authors": [
                "T.W. Victor",
                "J.L. Harbluk",
                "J.A. Engstr\u00f6m"
            ],
            "title": "Sensitivity of eye-movement measures to in-vehicle task difficulty",
            "venue": "Transp. Res. Part F Traffic Psychol. Behav",
            "year": 2005
        },
        {
            "authors": [
                "T.P. Trappenberg",
                "M.C. Dorris",
                "D.P. Munoz",
                "R.M. Klein"
            ],
            "title": "A model of saccade initiation based on the competitive integration of exogenous and endogenous signals in the superior colliculus",
            "venue": "J. Cogn. Neurosci",
            "year": 2001
        },
        {
            "authors": [
                "J. Hofmeister",
                "D. Heller",
                "R. Radach"
            ],
            "title": "The return sweep in reading",
            "venue": "In Current Oculomotor Research; Springer: Berlin/Heidelberg, Germany,",
            "year": 1999
        },
        {
            "authors": [
                "S. Pannasch",
                "S.M. Dornhoefer",
                "P.J.A. Unema",
                "B.M. Velichkovsky"
            ],
            "title": "The omnipresent prolongation of visual fixations: Saccades are inhibited by changes in situation and in subject\u2019s activity",
            "venue": "Vision Res",
            "year": 2001
        },
        {
            "authors": [
                "P. Unema",
                "S. Dornhoefer",
                "S. Steudel",
                "B. Velichkovsky"
            ],
            "title": "An Attentive Look at Driver\u2019s Fixation Durations (Draft)",
            "venue": "Available online: https://www.researchgate.net/publication/254739710_An_attentive_look_at_driver\\T1\\textquoterights_fixation_du rations_Draft (accessed on",
            "year": 2020
        },
        {
            "authors": [
                "R. Schleicher",
                "N. Galley",
                "S. Briest",
                "L. Galley"
            ],
            "title": "Blinks and saccades as indicators of fatigue in sleepiness warnings: Looking tired? Ergonomics",
            "year": 2008
        },
        {
            "authors": [
                "R.A. Lavine",
                "J.L. Sibert",
                "M. Gokturk",
                "B. Dickens"
            ],
            "title": "Eye-tracking measures and human performance in a vigilance",
            "venue": "task. Aviat. Space Environ. Med",
            "year": 2002
        },
        {
            "authors": [
                "S. Saito"
            ],
            "title": "Does fatigue exist in a quantitative measurement of eye movements",
            "venue": "Ergonomics",
            "year": 1992
        },
        {
            "authors": [
                "N. Galley",
                "G. Andres"
            ],
            "title": "Saccadic eye movements and blinks during long-term driving on the autobahn with minimal alcohol ingestion",
            "venue": "Vis. Veh. 1996,",
            "year": 1996
        },
        {
            "authors": [
                "C. Lamm",
                "C.D. Batson",
                "J. Decety"
            ],
            "title": "The neural substrate of human empathy: Effects of perspective-taking and cognitive appraisal",
            "venue": "J. Cogn. Neurosci",
            "year": 2007
        },
        {
            "authors": [
                "S.D. Goldinger",
                "M.H. Papesh"
            ],
            "title": "Pupil dilation reflects the creation and retrieval of memories",
            "venue": "Curr. Dir. Psychol. Sci",
            "year": 2012
        },
        {
            "authors": [
                "W.D. Poynter"
            ],
            "title": "Pupil-size asymmetry is a physiologic trait related to gender, attentional function, and personality",
            "venue": "Laterality Asymmetries Body Brain Cogn",
            "year": 2017
        },
        {
            "authors": [
                "P.D. Drummond"
            ],
            "title": "Pupil diameter in migraine and tension headache",
            "venue": "J. Neurol. Neurosurg. Psychiatry",
            "year": 1987
        }
    ],
    "sections": [
        {
            "text": "Citation: Zhang, J.; Park, S.; Cho, A.;\nWhang, M. Significant Measures of\nGaze and Pupil Movement for\nEvaluating Empathy between\nViewers and Digital Content. Sensors\n2022, 22, 1700. https://doi.org/\n10.3390/s22051700\nAcademic Editor: Zeljko Zilic\nReceived: 31 December 2021\nAccepted: 18 February 2022\nPublished: 22 February 2022\nPublisher\u2019s Note: MDPI stays neutral\nwith regard to jurisdictional claims in\npublished maps and institutional affil-\niations.\nCopyright: \u00a9 2022 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: empathy evaluation; eye movement; gaze; pupil; fixation; saccade; emotion; cognition; digital content"
        },
        {
            "heading": "1. Introduction",
            "text": "We live in a society with an overflow of media content through various media forms. Digital content consists of a stream of information in digital format that can be stored, streamed, and broadcast. Whereas digital content may include data devoid of any affective characteristics (e.g., weather information and geological information), some content, such as drama and movies, highly depends on its emotional value. Digital content has a spectrum of affective characteristics depending on the purpose of the medium (drama, movie, ads). Most digital content shares a common and permeating goal to produce media that many viewers can relate to, understand, and engage emotionally. For example, the Netflix program most viewed in 2021 was South Korea\u2019s Squid Game. People argue that Squid Game became popular because viewers readily empathize with a character\u2019s emotional state and narrative. The psychology and physiology of empathy have long been studied in the fields of clinical psychology, social development, and neuroscience. While there is no consensus on the definition of empathy, researchers agree that empathy has multiple subcomponents [1\u20133], and some critical elements of empathy (e.g., recognition, process, outcome, and response) are commonly identified (for an extensive review of empathy as a concept, see [4]).\nSensors 2022, 22, 1700. https://doi.org/10.3390/s22051700 https://www.mdpi.com/journal/sensors\nSensors 2022, 22, 1700 2 of 19\nBased on the most prominent empathy theories [1,3,5,6], affective and cognitive processes are the underlying mechanisms that produce empathic outcomes. Affective empathy generally connotes an observer\u2019s visceral reaction to the target\u2019s affective state. Cognitive empathy involves taking the target\u2019s perspective and drawing inferences about their thoughts, feelings, and characteristics. Neuroscientists have identified underlying neurological evidence for empathy [7] by discovering mirror neurons in monkeys [8]. Overlapping brain patterns are observed when an observer perceives the same emotions from a target, suggesting shared affective neural networks [9\u201311]. In this paper, we first discuss related work, summarizing significant gaze and pupil movement measures and comparing eye movement studies on digital content. We then explain our experiment design and protocol, followed by data analysis. We conclude with a discussion on the implications of the findings, the limitations of the study, and call for future research."
        },
        {
            "heading": "2. Related Work",
            "text": "Attention to visual information is a prerequisite for recognition. The cortical area known as the frontal eye field (FEF) plays a vital role in controlling visual attention and eye movements [12]. The fovea on the retina is only a relatively small part, but it contains sufficiently dense cone cells to distinguish the visual world in great detail [13]. Owing to the relatively small fovea, the brain makes significant decisions when controlling eye movements. A saccade is a decision each time we move our eyes, and we have to decide where and when to move them [14,15]. Personalities, desires, goals, beliefs, expectations, predictions, memories, and intentions can influence these decisions. Gaze is a potent social cue in which mutual gaze often signifies threat or evading conveying submission or avoidance [16\u201318]. Processing eye gaze is a foundation for social interactions because explication of the neural substrate for gaze processing is an important step in understanding neuroscience for social cognition [19,20]. Gaze tracking monitors the user\u2019s attention and interests and personalizes the agent\u2019s behaviors [21], which is an essential tool for detecting users\u2019 attention information and focusing on particular content. It is critical to analyze consumers\u2019 attention when an advertisement is shown [22]. Researchers have long confirmed through empirical evidence that eyes can perceive and express emotions. A classic study by Hess [23] demonstrated that pleasant imagery leads to pupil dilation. The relationship between pupil modulation and emotion perception develops with age [24]. Pupil size is generally regarded as a nonverbal communication channel in which social signals are exchanged between individuals at an unconscious level (i.e., non-reportable). Specifically, a person\u2019s feelings or attitudes are embedded in pupil size as a source of information [25]. Involuntary pupil size change is also regulated by the autonomic nervous system. Pupil dilation seems to occur when people feel attracted [25], surprised or uncertain [26], or social interest. Active storage or retrieval of memories also leads to pupil dilation and an increase in cognitive load [27,28]. A pleasant emotion leads to pupil dilation more than an unpleasant one [24]. In the context of empathy, the dilation pattern seems to get synchronized between conversation partners if the dyadic pair shares attention (i.e., \u201ctunes in\u201d) and gets engaged, evident in the shared emotional peak found in a video analysis by Kang [29]. Kang also found that pupil synchronization was the strongest among the high-expressive and highempathic participant groups. Pupil synchronization also interacts with the degree of trust [30] and facial expression of the conversation pair [31]. For example, sad faces elicit more pupil synchronization than happy faces. In short, the analysis of eye movement features is critical for understanding the degree of empathy among individuals. Eye features (i.e., gaze and pupil movement) change when an observer empathizes with an individual. However, research on whether eye features change when empathizing with content is in its infancy. Table 1 compares the most recent studies on eye movement features when viewing media. There is little research analyzing\nSensors 2022, 22, 1700 3 of 19\neye movement features between a person and media suggesting key indicators for use. Furthermore, except for [32], no study has investigated the relationship between gaze and pupil movement for evaluating empathy. In addition, the dependent measures of most studies are limited to a single index (e.g., only they investigated gaze points or time spent of fixation).\nOur study sought to identify significant gaze and pupil movement measures for assessing empathy between viewers and digital content. To the best of our knowledge, this is the first study to investigate the relationship between significant gaze and pupil movements and empathic digital content. Second, the study analyzes a full range of significant measures involving gaze and pupil movements (change of pupil diameter, peak pupil dilation, very short, mid, over long fixation duration, saccadic amplitude, and saccadic count) for use when assessing digital content."
        },
        {
            "heading": "3. Materials and Methods",
            "text": "We adopted Russell\u2019s two-dimensional model [37], where emotional states can be defined at any valence and arousal level. We invited participants to view empathic or non-empathic emotion-eliciting videos with varying valence (i.e., from unpleasant to pleasant) and arousal levels (i.e., from relaxed to aroused). Our research aimed to verify the following nine hypotheses. Based on the aforementioned literature review, we hypothesized a significant difference in eye movement features (pupil size, fixation, and saccade) when a person views digital content:\nSensors 2022, 22, 1700 4 of 19\nHypothesis 1 (H1). . . . between the empathic and non-empathic conditions in all videos (i.e., pleasant-aroused, pleasant-relaxed, unpleasant-aroused, and unpleasant-relaxed).\nHypothesis 2 (H2). . . . between empathic and non-empathic conditions in aroused videos.\nHypothesis 3 (H3). . . . between empathic and non-empathic conditions in relaxed videos.\nHypothesis 4 (H4). . . . between empathic and non-empathic conditions in pleasant videos.\nHypothesis 5 (H5). . . . between empathic and non-empathic conditions in unpleasant videos.\nHypothesis 6 (H6). . . . between empathic and non-empathic conditions in pleasant-aroused videos.\nHypothesis 7 (H7). . . . between empathic and non-empathic conditions in pleasant-relaxed videos.\nHypothesis 8 (H8). . . . between empathic and non-empathic conditions in unpleasant-relaxed videos.\nHypothesis 9 (H9). . . . between empathic and non-empathic conditions in unpleasant-aroused videos."
        },
        {
            "heading": "3.1. Stimuli Selection",
            "text": "In this study, we edited video clips (e.g., dramas or movies) to elicit empathy from the participants. The content to induce empathic conditions was collected in a two-dimensional model. To ensure that the empathic and non-empathic videos were effective, we conducted a stimulus selection experiment before the main experiment. We selected 20 edited dramas or movies containing emotions as candidates. Five video clips were used for each quadrant in a two-dimensional model. Thirty participants viewed emotional videos and responded to a subjective questionnaire. They received $20 for participation in the study. For each condition, among the five candidates, the video with the highest empathic score was selected as the empathic stimulus in the main experiment. Conversely, the video with the lowest empathic score was chosen as the non-empathic stimulus. That is, a pair of empathic and non-empathic videos for each of the four quadrants in the two-dimensional model was selected. In total, eight stimuli were selected for the main experiment. All stimuli are available online (see Supplementary Materials)."
        },
        {
            "heading": "3.2. Experiment Design",
            "text": "When the observer is interested in the target stimulus, the observer\u2019s eye movement characteristics change as a function of the target\u2019s emotional characteristics (empathy, valence, and arousal). To understand the nature of such a change, the main experiment was a factorial design of two (empathy: empathic and non-empathic) \u00d7 two (valence: pleasant and unpleasant) \u00d7 two (arousal: aroused and relaxed) independent variables. A t-test was used to test the difference in eye movement-related dependent measures (pupil size, fixation, and saccade) between the empathic and non-empathic conditions."
        },
        {
            "heading": "3.3. Participants",
            "text": "We conducted an a priori power analysis using the program G*Power with power set at 0.8 and \u03b1 = 0.05, d = 0.6 (independent t-test), two-tailed. The results suggest that an N of approximately 46 is needed to achieve appropriate statistical power. Therefore, 47 university students were recruited for this study. Participants\u2019 ages ranged from 20 to 30 years (mean = 28, STD = 2.9), with 20 (44%) men and 27 (56%) women. We selected participants with a corrective vision of 0.8 or above without any vision deficiency, to ensure reliable recognition of visual stimuli. We recommended that participants sleep sufficiently and prohibited alcohol, caffeine, and smoking the day before the experiment. Because the experiment requires valid recognition of the participant\u2019s facial expression, we limited the use of glasses and cosmetic makeup. All participants were briefed on the purpose and\nSensors 2022, 22, 1700 5 of 19\nprocedure of the experiment and signed a consent form. They were then compensated for their participation with a fee paid to them."
        },
        {
            "heading": "3.4. Experimental Protocol",
            "text": "Figure 1 outlines the experimental process and the environment used in this study. Participants were asked to sit 1 m away from a 27-inch LCD monitor. A webcam was installed on the monitor. Participants\u2019 brainwaves (EEG cap 18 ch), facial expressions (webcam), and eye movements (gaze tracking device) were acquired in addition to subjective responses to a questionnaire. We set the frame rate of the gaze tracking device to 60 frames per second. The participants viewed eight emotion-eliciting (empathy or non-empathy) videos and responded to a questionnaire after each viewing. We excluded the brainwave data from the analysis in this paper.\nSensors 2022, 22, x FOR PEER REVIEW 5 of 20 was a factorial design of two (empathy: empathic and non-empathic) \u00d7 two (valence: pleasant and unpleasant) \u00d7 two (arousal: aroused and relaxed) independent variables. A t-test was used to test the difference in eye movement-related dependent measures (pupil size, fixation, and saccade) between the empathic and non-empathic conditions."
        },
        {
            "heading": "3.3. Participants",
            "text": "We conducted an a priori power analysis using the program G*Power with power set at 0.8 and \u03b1 = 0.05, d = 0.6 (independent t-test), two-tailed. The results suggest that an\nN of approximately 46 is needed to achieve appropriate statistical power. Therefore, 47\nuniversity students were recruited for this study. Participants\u2019 ages ranged from 20 to 30\nyears (mean = 28, STD = 2.9), with 20 (44%) men and 27 (56%) women. We selected\nparticipants with a corrective vision of 0.8 or above without any vision deficiency, to\nensure reliable recognition of visual stimuli. We recommended that participants sleep\nsufficiently and prohibited alcohol, caffeine, and smoking the day before the experiment.\nBecause the experiment requires valid recognition of the participant\u2019s facial expression,\nwe limited the use of glasses and cosmetic makeup. All participants were briefed on the\npurpose and procedure of the experiment and signed a consent form. They were then\ncompensated for their participation with a fee paid to them."
        },
        {
            "heading": "3.4. eri e tal rotocol",
            "text": "Fig re 1 o tli s t i t l t i t i t i t .\nartici t r as t sit 1 away from a 27-inch LCD monitor. ebca as\ninstalled on the monitor. Participants\u2019 brainwaves (EEG cap 18 ch), facial expressions\n(webcam), and eye movements (gaze tracking device) were acquired in addition to\nsubjective responses to a questionnaire. We set the frame rate of the gaze tracking device\nto 60 frames per second. The participants viewed eight emotion-eliciting (empathy or non-\nempathy) videos and responded to a questionnaire after each viewing. We excluded the\nbrainwave data from the analysis in this paper.\nFigure 1. Experimental protocol and configuration.\nWe gathered the participants\u2019 subjective responses using the Consumer Empathic\nResponse to Advertising Scale (CERA), a comprehensive battery of measures involving\naffective and cognitive facets of empathy [38\u201340]. We adopted an empirically validated\nquestionnaire based on the ethnicity of Korean participants, which consisted of nine items\n(see Table 2). The factor loading exceeded 0.4, and the Cronbach\u2019s alpha exceeded 0.8.\nEach construct was measured on a seven-point Likert scale.\nFigure 1. x eri ental protocol and configuration.\ne gathered the participants\u2019 subjective responses using the Consu er E pathic es o se to dvertisi g Scale ( ), a co prehensi e battery of easures involving affective and cognitive facets of e pathy [38\u201340]. e adopted an e pirically validated questionnaire based on the ethnicity of Korean participants, hich consisted of nine ite s (see Table 2). The factor loading exceeded 0.4, and the Cronbach\u2019s alpha exceeded 0.8. Each construct was measured on a seven-point Likert scale."
        },
        {
            "heading": "3.5. Feature Extraction of Eye Movement",
            "text": "Eye movement features play a vital role in face processing and social communication [41,42]. It is one of the most important facial cues for communicating with consumers [43,44]. Eye gaze direction is associated with viewer cognition, such as visual\nSensors 2022, 22, 1700 6 of 19\nattention and emotion. Gaze movements convey emotions and intentions and can reflect empathic conditions. We selected seven feature measures of gaze movement and pupil characteristics for the extraction and analysis, as outlined in Table 3. We did not measure pupil response time and decision time.\n3.5.1. Change in Pupil Diameter\nPupillometry, the measurement of changes in pupil diameter, is a relatively old method for inferring different types of activity in the brain. Pupil dilation is an autonomic sympathetic nervous system response that can provide attention, interest, or emotion indices, and is correlated with mental workload and arousal [45]. Pupil responses may be a useful alternative or an addition to subjective measures. Some cognitive and emotional events occur outside our conscious control and can cause pupils to constrict and expand. UX researchers have recorded data from these events to detect fear, anxiety, mental strain, or task difficulty [46]. In addition, because it is nearly impossible to mask implicit cognitive responses, biases such as social desirability that prevent people from accurately informing researchers of their experiences are of little concern during analysis. Chatham, Frank, and Munakata [47] established the utility of both pupillometry for assessing the temporal dynamics of cognitive control. Changes in central nervous system activity that are systematically related to cognitive processing may be extracted from the raw pupillary record by performing time-locked averaging of critical events in the information-processing task. A task-evoked pupillary response bears the same relationship to the pupillary record from which it is derived, as does an event-related brain potential to spontaneous electroencephalographic (EEG) activity. With averaging, short-latency (i.e., from onset between 100 and 200 ms) phasic task-evoked dilations appear, which terminate rapidly following the completion of processing [48]. In pupillometry, participants were calibrated and then looked at a fixation cross on a blank page for one second to obtain a baseline pupil diameter measurement [49]. We were interested in the relationship between pupil size and the empathic and nonempathic video conditions. Since there is evidence that the left and right pupils may be different [50], we explored the possible differences in the responses of the left and right pupils. The perception-action model is adopted by many fields over time, and perception and action share a common code of representation in the brain [51,52]. The left hemisphere processes detailed information, whereas the right hemisphere is selective for more holistic information [53]. The left prefrontal area is more active in response to semantic cues, whereas the right prefrontal area is more active in generating information from memory. Both are active when the task requires voluntary or imagined actions [54,55]. While the left hemisphere subserves positive emotions, the right hemisphere may subserve fearful or negative emotions [56,57]. Owing to such differential activation as a function of emotion and because pupil sizes reflect brain activity, we speculate that pupil diameter changes may differ between empathic and non-empathic conditions. We calculated the mean baseline pupil diameter for each participant. Specifically, the change in the left pupil diameter (CLPD) and change in the right pupil diameter (CRPD) before and during the stimulus. We calculated the mean values of the CLPD and CRPD across all participant data as dependent measures.\nSensors 2022, 22, 1700 7 of 19\n3.5.2. Peak of Pupil Dilation\nThe decision-making process drives the time course of pupil response. The pupil response reveals the properties of the decisions, such as perceived emotional valence and confidence in the assessment [47,58]. Beatty [48] reviewed all empirical data involving task-evoked pupillary response (TEPR) studies. He concluded that it took six to eight seconds for the participants to recognize and respond during cognitive tasks. The most prominent TEPR research [59,60] has set the pupil dilation experiment\u2019s window size to eight seconds. We also set the window size to be eight seconds because empathic response involves a cognitive process [5]. The phase for extracting the peak value of pupil dilation was divided into three steps.\nStep one: identifying the peak every eight seconds Figure 2 shows a schematic diagram of the peaks found every eight seconds in the raw\ndata. However, peaks every eight seconds may contain false peaks. To counter false peaks, we compared the standard deviation (STD) of the peak positions of all 47 participants.\nSensors 2022, 22, x FOR PEER REVIEW 7 of 20 different [50], we explored the possible differences in the responses of the left and right pupils. The perception-action model is adopted by many fields over time, and perception and action share a common code of representation in the brain [51,52]. The left hemisphere processes detailed information, whereas the right hemisphere is selective for more holistic information [53]. The left prefrontal area is more active in response to semantic cues, whereas the right prefrontal area is more active in generating information from memory. Both are active when the task requires voluntary or imagined actions [54,55]. While the left hemisphere subserves positive emotions, the right hemisphere may subserve fearful or negative emotions [56,57]. Owing to such differential activation as a function of\nemotion and because pupil sizes reflect brain activity, we speculate that pupil diameter\nchanges may differ between empathic and non-empathic conditions. We calculated the\nmean baseline pupil diameter for each participant. Specifically, the change in the left pupil\ndiameter (CLPD) and change in the right pupil diameter (CRPD) before and during the\nstimulus. We calculated the mean values of the CLPD and CRPD across all participant\ndata as dependent measures.\n3.5.2. Peak of Pupil Dilation\nThe decision-making process drives the time course of pupil response. The pupil\nresponse reveals the properties of the decisions, such as perceived emotional valence and\nconfidence in the assessment [47,58]. Beatty [48] reviewed all empirical data involving\ntask-evoked pupillary response (TEPR) studies. He concluded that it took six to eight\nseconds for the participants to recognize and respond during cognitive tasks. The most\nprominent TEPR research [59,60] has set the pupil dilation experiment\u2019s window size to\neight seconds. We also set the window size to be eight seconds because empathic response\ninvolves a cognitive process [5]. The phase for extracting the peak value of pupil dilation\nwas divided into three steps.\nStep one: identifying the peak every eight seconds\nFigure 2 shows a schematic diagram of the peaks found every eight seconds in the\nraw data. However, peaks every eight seconds may contain false peaks. To counter false\npeaks, we compared the standard deviation (STD) of the peak positions of all 47\nparticipants.\nFigure 2. Identifying the peak every eight seconds in a video.\nStep two: find the true peak\nBecause the peak with the smallest dispersion has the highest probability of being a\ntrue peak, we extracted the peak feature measures with the lowest STD for each empathic\nand non-empathic condition. The extracted measures were peak left pupil dilation (PLPD)\nand peak right pupil dilation (PRPD), as shown in Figures 3\u20136. For the eigenvalue, we\nhypothesized that the maximum pupil dilation is greater in the empathic condition than\nin the non-empathic condition.\nFigure 2. Identifying the peak every eight seconds in a video.\nStep two: find the true peak Because the peak with the smallest dispersion has the highest probability of being a\ntrue peak, we extracted the peak feature measures with the lowest STD for each empathic and non-empathic condition. The extracted measures were peak left pupil dilation (PLPD) and peak right pupil dilation (PRPD), as shown in Figures 3\u20136. For the eigenvalue, we hypothesized that the maximum pupil dilation is greater in the empathic condition than in the non-empathic condition. Sensors 2022, 22, x FOR PEER REVIEW 8 of 20\nFigure 3. Comparison of standard deviation (STD) of peak pupil dilation between the empathic and\nnon-empathic conditions in pleasant-aroused content.\nFigure 3. Comparison of standard deviation (STD) of peak pupil dilation between the empathic and non-empathic conditions in pleasant-aroused content.\nSensors 2022, 22, 1700 8 of 19\nSensors 2022, 22, x FOR PEER REVIEW 8 of 20\nFigure 3. Comparison of standard deviation (STD) of peak pupil dilation between the empathic and\nnon-empathic conditions in pleasant-aroused content.\nFigure 4. Comparison of standard deviation (STD) of peak pupil dilation between the empathic and\nnon-empathic conditions in pleasant-relaxed content.\nFigure 5. Comparison of standard deviation (STD) of peak pupil dilation between the empathic and non-empathic conditions in unpleasant-relaxed content.\nFigure 4. ris f st r e iation (S ) of eak il ilation et een t e e athic and non-empathic conditions in pleasant-relaxed content.\nSensors 2022, 22, x FOR PEER REVIEW 8 of 20\nFigure 3. Comparison of standard deviation (STD) of peak pupil dilation between the empathic and\nnon-empathic conditions in pleasant-aroused content.\nFigure 4. Comparison of standard deviation (STD) of peak pupil dilation between the empathic and\nnon-empathic conditions in pleasant-relaxed content.\nFigure 5. Comparison of standard deviation (STD) of peak pupil dilation between the empathic and non-empathic conditions in unpleasant-relaxed content.\ni re . ris f st r e i ti ( ) f e il il ti et ee t e e t ic non-empathic conditions in unpleasant-relaxed content. Sensors 2022, 22, x FOR PEER REVIEW 9 of 20\nSensors 2022, 22, 1700 9 of 19\n3.5.3. Fixation Duration\nThe time between the two saccades is generally called fixation duration. This event is closely related to cognitive processing in alert subjects [61\u201363]. Fixations of different lengths may reflect different neuronal processes, as observed in various studies [64\u201367]. Very short fixations (<150 ms), so-called express fixations, may turn out to be a distinct category caused by low-level visuomotor behavior; they could represent the reflexive unconscious or noncognitive aspects of behavioral control. Media-related fixation involves cognitive saccades (between 150 and 900 ms), positioned between very short (<150 ms) and overlong (>900 ms) saccades [68]. Medium fixation has a reduced fatigue rate compared to short or long fixation [69\u201371]. Galley and Andres [71] reported that visual processing of complex scenes with rapidly changing stimuli (e.g., city rides) typically leads to a fixation of between 200\u2013400 ms, which exceeds the fixation duration of approximately 250 ms during reading. Fixation is associated with content-related identification or cognitive processing; therefore, we focused on fixation duration, ranging from 150 ms to 900 ms, in this study. A short fixation time (150 ms) is insufficient to extract relevant information [65]. In the case of excessively long fixation (>900 ms), a general functional interpretation has not yet been established, except for unconscious driving or low-arousal phase starting during microsleep. Three eigenvalues were extracted: very short fixation, medium fixation duration, and overlong fixation. We calculated the percent dependent measure (%), which represents the fixed time divided by total time. We speculated that empathic videos may elicit more cognitive engagement and increase medium fixation than the non-empathic videos.\n3.5.4. Saccade\nExperimental studies of saccadic eye movements have produced a considerable amount of data. In the case of eye movements elicited by specific visual targets, the significant measures were the metrics of saccadic amplitude and saccadic count. The amplitude is the angle in degrees between two fixation points [61]. Measures were provided based on the calculation of GazePoint equipment, which averages eye positions. We hypothesized that the saccadic amplitude would be greater in the empathic condition than in the non-empathic condition."
        },
        {
            "heading": "4. Results",
            "text": "The results are twofold: the analysis of subjective evaluation and eye movement features."
        },
        {
            "heading": "4.1. Subjective Evaluation",
            "text": "A t-test was used to test the differences between the key features in the empathic and non-empathic conditions.\n4.1.1. The Analysis of Arousal Scores\nWe analyzed the differences in subjective arousal scores between the empathic and nonempathic conditions in four quadrants in the two-dimensional emotion model (i.e., pleasantaroused, pleasant-relaxed, unpleasant-relaxed, and unpleasant-aroused; see Figure 7).\nSensors 2022, 22, 1700 10 of 19\nSensors 2022, 22, x FOR PEER REVIEW 10 of 20"
        },
        {
            "heading": "4. Results",
            "text": "The results are twofold: the analysis of subjective evaluation and eye movement features."
        },
        {
            "heading": "4.1. Subjective Evaluation",
            "text": "A t-test was used to test the differences between the key features in the empathic and non-empathic conditions."
        },
        {
            "heading": "4.1.1. The Analysis of Arousal Scores",
            "text": "We analyzed the differences in subjective arousal scores between the empathic and non-empathic conditions in four quadrants in the two-dimensional emotion model (i.e., pleasant-aroused, pleasant-relaxed, unpleasant-relaxed, and unpleasant-aroused; see\nFigure 7).\nThe results indicated that the arousal scores of the empathic condition in the\npleasant-relaxed content were significantly lower than those in the non-empathic\ncondition. Conversely, the arousal scores of the empathic condition in the unpleasant-\nrelaxed content were significantly higher than those in the non-empathic condition. We\nfound no significant difference between pleasant-aroused and unpleasant-aroused\ncontent.\nFigure 7. Analysis (t-test) of the arousal values between the empathic and non-empathic conditions."
        },
        {
            "heading": "4.1.2. The Analysis of Valence Scores",
            "text": "We analyzed the differences in subjective valence scores between the empathic and\nnon-empathic conditions (see Figure 8). The results indicated that the valence scores of\nthe empathic condition in the pleasant-aroused and pleasant-relaxed content were\nFigure 7. nalysis (t-test) of the arousal values between the empathic and non-empathic conditions.\nThe results indicated that the arousal scores of the empathic condition in the pleasantrelaxed content were signifi antly lower than those in the non-empathic condition. Conversely, the arousal scores of the empathic condition in the unpleasant-relaxed content were significantly higher than those in the non-empathic condition. We found no significant difference between pleasant-aroused and unpleasant-aroused content.\n4.1.2. The Analysis of Valence Scores\nWe analyzed the differences in subjective valence scores between the empathic and non-empathic conditions (see Figure 8). The results indicated that the valence scores of the empathic condition in the pleasant-aroused and pleasant-relaxed content were significantly higher than those in the non-empathic condition. Conversely, the valence scores of the empathic condition in the unpleasant-aroused content were significantly lower than those of the non-empathic condition. We found no significant differences in the unpleasant-relaxed content.\n4.1.3. The Analysis of Cognitive and Affective Empathy Scores\nWe analyzed the differences in subjective cognitive and affective scores between the empathic and non-empathic conditions (Figures 9 and 10). The results indicated that the cognitive empathy scores of the empathic condition in all four contents were significantly higher than those of the non-empathic condition. Similarly, the affective empathy scores of the empathic condition in all content except for pleasant-aroused content were significantly higher than those of the non-empathic condition. In summary, all empathic videos induced target empathy (empathic or non-empathy) in general from the participants as intended.\nSensors 2022, 22, 1700 11 of 19\nSensors 2022, 22, x FOR PEER REVIEW 11 of 20\nsignificantly higher than those in the non-empathic condition. Conversely, the valence\nscores of the empathic condition in the unpleasant-aroused content were significantly\nlower than those of the non-empathic condition. We found no significant differences in\nthe unpleasant-relaxed content.\nFigure 8. Analysis (t-test) of the valence values between the empathic and non-empathic conditions.\n4.1.3. The Analysis of Cognitive and Affective Empathy Scores\nWe analyzed the differences in subjective cognitive and affective scores between the\nempathic and non-empathic conditions (Figures 9 and 10). The results indicated that the\ncognitive empathy scores of the empathic condition in all four contents were significantly\nhigher than those of the non-empathic condition. Similarly, the affective empathy scores\nof the empathic condition in all content except for pleasant-aroused content were\nsignificantly higher than those of the non-empathic condition. In summary, all empathic\nvideos induced target empathy (empathic or non-empathy) in general from the\nparticipants as intended.\nFigure 9. Analysis (t-test) of the cognitive empathy values between the empathic and non-empathic conditions.\nFigure 10. Analysis (t-test) of the affective empathy values between the empathic and non-empathic conditions.\nFigure 9. Analysis (t-test) of the cognitive empathy values between the empathic and nonempathic conditions.\nSensors 2022, 22, 1700 12 of 19\nSensors 2022, 22, x FOR PEER REVIEW 12 of 20\nFigure 9. Analysis (t-test) of the cognitive empathy values between the empathic and non-empathic conditions.\nFigure 10. Analysis (t-test) of the affective empathy values between the empathic and non-empathic conditions.\nFigure 10. Analysis (t-test) of the affective empathy values between the empathic and nonempathic conditions."
        },
        {
            "heading": "4.2. Eye Movement Features",
            "text": "A t-test analysis of the hypotheses was conducted by adjusting alpha levels of 0.05 per test. The results of key features of the nine groups are listed in Tables 4\u20137. Overall, the saccadic amplitude measure (i.e., the mean angle between two fixation points) showed that, except for aroused and relaxed content, it is significantly greater in the empathic condition than in the non-empathic condition. In addition, pupil dilation showed a significant increase in the empathic condition compared to the non-empathic condition with aroused and pleasant content. The peak of pupil dilation ranged from 5.70 mm to 5.88 mm in the empathic condition. The detailed analysis of each content group follows.\nSensors 2022, 22, 1700 13 of 19\n4.2.1. All-Emotions Content\nFor all-emotions content, results indicated the peak of right pupil dilation (Table 5) and saccadic amplitude (Table 7) were significantly different between the empathic and non-empathic conditions (p < 0.001). Interestingly, the saccadic amplitude was greater in the empathic condition (M = 193.74, STD = 2.45; p < 0.001) than in the non-empathic condition (M = 165.86, STD = 3.12; p < 0.001; see Table 7). However, the saccadic count did not show a significant difference. The fixation time in all three ranges (very short, medium, overlong) did not show a significant difference either.\nSensors 2022, 22, 1700 14 of 19\nThe results indicated that the higher the level of empathic condition, the more active the saccadic jump, which may imply that empathic content is more interesting and engaging than non-empathetic content, i.e., viewers engage more in cognitive and attentive processes.\n4.2.2. Pleasant and Unpleasant Content\nFor pleasant content, the results indicated that the peak of left and right pupil dilation, and saccadic amplitude were significantly different between the empathic and non-empathic conditions (p < 0.05). This is consistent with the literature on pupil dilation due to pleasant images [25] and happy facial expressions [24]. For unpleasant content, the mean of medium fixation was significantly smaller in the non-empathic condition (M = 610.11, STD = 12.48; p < 0.05) than in the empathic content (M = 649.50, STD = 12.36; p < 0.05). In addition, the mean of overlong fixation was smaller in the empathic condition (M = 51.38, STD = 2.25; p < 0.05) than in the non-empathic condition (M = 59.58, STD = 3.0; p < 0.05).\n4.2.3. Aroused and Relaxed Content\nFor aroused content, results indicated that the change in left and right pupil diameter, and peak left and right pupil dilation were significantly different between the empathic and non-empathic conditions (p < 0.05). Specifically, the change in left pupil diameter was significantly higher in the non-empathic condition (M = 0.39, STD = 0.06; p < 0.01) than that in the empathic condition (M = 0.22, STD = 0.04; p < 0.01). In addition, the change in right pupil diameter was significantly higher in the non-empathic condition (M = 0.38, STD = 0.06; p < 0.05) than in the empathic condition (M = 0.29, STD = 0.05; p < 0. 05). Overall, the significant pupil dilation is limited to pleasant and aroused conditions. The implications will be discussed in Discussion. For relaxed content, results indicated that the change in left pupil diameter, overlong fixation, and saccadic amplitude were significantly different between the empathic and non-empathic conditions.\n4.2.4. Empathic and Non-Empathic Content\nFigures 11 and 12 depict the relationship between eye-movement feature variables in a two-dimensional emotion map. For pleasant-aroused content, the results indicated that changes in left and right pupil diameter, peak left pupil dilation, medium fixation duration, overlong fixation duration, saccadic amplitude, and saccadic count were significantly different between the empathic and non-empathic conditions (p < 0.05). Sensors 2022, 22, x FOR PEER REVIEW 16 of 20\nFigure 11. Pupil feature variables in a two-dimensional emotion map."
        },
        {
            "heading": "5. Conclusions and Discussion",
            "text": "Figure 11. Pupil feature variables in a two-dimensional emotion map.\nSensors 2022, 22, 1700 15 of 19\nSensors 2022, 22, x FOR PEER REVIEW 16 of 20\nFigure 11. Pupil feature variables in a two-dimensional emotion map.\nFigure 12. Gaze feature variables in a two-dimensional emotion map."
        },
        {
            "heading": "5. Conclusions and Discussion",
            "text": "To the best of our knowledge, this is the first study to suggest significant measures\ninvolving gaze and pupil movements for use when assessing empathic digital content. We\nanalyzed a full range of dependent measures (change in pupil diameter, peak pupil\ndilation, very short, mid, overlong fixation duration, saccadic amplitude, and saccadic\ncount) to understand all aspects of gaze and pupil movements. Our study had more\nindices than other studies (Table 1).\nThe majority (H1, H3, H4, H5, H6) of the hypotheses on peak pupil dilation and\nsaccadic amplitude were supported. In conclusion, we found that saccadic amplitude and\npeak pupil dilation are two significant measures that can be used to assess whether\nviewers empathize with digital content.\nSaccadic amplitude measures showed that, except for aroused and relaxed content,\nthe average angle between two fixation points is significantly greater in the empathic\ncondition than in the non-empathic condition. Because the empathic video was designed\nto induce empathy, as confirmed by the manipulation check, participants may have\nengaged in the story or narrative of the stimuli video (e.g., drama or movie). Participants\nmay be \u201ctuned\u201d into digital content and initiate active information-seeking behavior,\nwhich results in a more dynamic saccadic jump in the region of interest. Participants also\nhad a longer fixation with the empathic video than with non-empathic videos, albeit only\nwith unpleasant and pleasant videos.\nFigure 12. Gaze feature variables in a two-dimensional emotion map.\nFor pleasant-relaxed content, the results indicated that the changes in left and right pupil diameter, medium fixation duration, and saccadic amplitude were significantly different between the empathic and non-empathic conditions (p < 0.05). For unpleasant-relaxed content, the results indicated that peak left pupil dilation, medium fixation duration, saccadic amplitude, and saccadic count were significantly different between the empathic and non-empathic conditions (p < 0.05). For unpleasant-aroused content, the results indicated that PLPD and saccadic amplitude were significantly different between the empathic and non-empathic conditions (p < 0.05)."
        },
        {
            "heading": "5. Conclusions nd Discussion",
            "text": "To the best of our knowledge, this is the first study to suggest significant measures involving gaze and p pil ovem nts f r use when assessing empathic digital content. We analyzed a full range of d pendent measures (change in pupil diamet r, peak pupil dilation, very sh rt, mid, overlong fixation duration, saccadic amplitude, and saccadic count) to understand all spects of gaze an pupil movements. Our study h d more indices than other studies (Table 1). The majority (H1, H3, H4, H5, H6) of the hypo heses on peak pupil dilation and saccadic amplitude were supported. In conclusion, w found that saccadic mplitude nd peak pupil dilation are two significant measures that can be used to assess whether viewers empathize with digital content. Saccadic amplitude measures showed that, except for aroused and relaxed content, the average angle between two fixation points is significantly greater in the empathic condition than in the non-empathic condition. Because the empathic video was designed to induce empathy, as confirmed by the manipulation check, participants may have engaged in the story or narrative of the stimuli video (e.g., drama or movie). Participants may be \u201ctuned\u201d into digital content and initiate active information-seeking behavior, which results in a more dynamic saccadic jump in the region of interest. Participants also had a longer fixation with the empathic video than with non-empathic videos, albeit only with unpleasant and pleasant videos. Second, although not as substantial as with saccadic amplitude, pupil dilation showed a significant increment in the empathic condition compared to the non-empathic condition with aroused and pleasant videos. In general, pupil dilation increases when the pupil is attracted or interested [25]. Empathic videos may certainly have drawn more attention. However, it is paramount to note that a higher form of empathy includes perspective taking [72]. Some stimuli may have induced the participants to refer to their past memories\nSensors 2022, 22, 1700 16 of 19\nto understand the narrative. Memory retrieval is known to elicit pupil dilation [73], leading to cognitive load [27]. It is also interesting that differential pupil dilation between empathic and non-empathic conditions is limited to aroused and pleasant videos. This may be because of the main effect of pleasant images [25] and happy facial expressions [24] on pupil dilation. That is, other videos (unpleasant, relaxed) may have offset the dilation owing to the nature of the video. Further studies may design a more sensitive experiment with substantial statistical power. Third, we did not find conclusive evidence suggesting asymmetric pupil responses when viewing empathic digital content. This is consistent with the current literature suggesting pupil-size asymmetry as a physiological trait (e.g., gender, personality) [74] or limited to cases such as migraine and headache [75]. We acknowledge some limitations of the research. First, we have yet to unravel the physiological mechanisms behind the findings. Future studies may investigate the relationship between brainwaves and gaze movement through EEG data analysis. Second, the videos were not qualitatively analyzed (for example, the identification of emotional peaks, analysis of the actor\u2019s facial expressions) to cross-examine the content and the participants\u2019 responses. Empathy is a co-social behavior between a dyadic pair; looking into the relationship between the content and the change in the participant\u2019s gaze and eye movements in a time series merits further investigation. Third, the current study acquired the gaze data through an eye tracking device, but future research may obtain data through a camera for better usability and ecological validity. For example, Naqvi et al. [76] proposed a fuzzy system-based target selection method for target selection for camera-based gaze trackers. The results suggested better usability and performance than other gaze tracking methods. The fuzzy system uses three features (pupil size, gaze position, texture information of a monitor image at the gaze target) to decide the user\u2019s target selection. Future studies to understand the participant\u2019s empathic gaze movement may adopt such state-of-the-art camera-based gaze tracking methods.\nSupplementary Materials: The following are available online at https://pan.baidu.com/s/1Dms CUAStDvKk_BHHnlwNrg?pwd=d8b2 (accessed on 31 December 2021). All videos used, and all raw data collected in the experiment.\nAuthor Contributions: J.Z.: conceptualization, methodology, software, validation, formal analysis, investigation, resources, data curation, writing, visualization, project administration; S.P.: methodology, validation, formal analysis, investigation, writing, review, editing; A.C.: conceptualization, investigation, review, editing; M.W.: conceptualization, methodology, writing, review, supervision, funding acquisition. All authors have read and agreed to the published version of the manuscript.\nFunding: This work was partly supported by Institute for Information & Communications Technology Promotion (IITP) grant funded by the Korea government (MSIT) (No. 2021-0-00986, Development of Interaction Technology to Maximize Realization of Virtual Reality Contents using Multimodal Sensory Interface) and partly by Electronics and Telecommunications Research Institute (ETRI) grant funded by the Korean government (22ZS1100, Core Technology Research for Self-Improving Integrated Artificial Intelligence System).\nInstitutional Review Board Statement: The study was conducted according to the guidelines of the Declaration of Helsinki, and approved by the Institutional Review Board of Sangmyung University (protocol code C-2021-002, approved 9 July 2021).\nInformed Consent Statement: Informed consent was obtained from all subjects involved in the study. Written informed consent has been obtained from the subjects to publish this paper.\nConflicts of Interest: The authors declare no conflict of interest.\nSensors 2022, 22, 1700 17 of 19"
        }
    ],
    "title": "Significant Measures of Gaze and Pupil Movement for Evaluating Empathy between Viewers and Digital Content",
    "year": 2022
}