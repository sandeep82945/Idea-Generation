{
    "abstractText": "This paper studies the problem of resolving data inconsistency from multiple sources in managing data related to power equipment for China\u2019s state grid corporation. This paper proposes to automatically align inconsistent devices from multiple sources, i.e., the same devices that have multiple entries with different values in each source, by HENGE, a HEtetrogeneous Network GEneration model. HENGE builds multiple data sources into a heterogeneous graph, and captures complex physical and semantic relationships among devices. HENGE combines both feature and relational information and improves alignment accuracy by feature-enhanced residual graph encoder and disentangled representation learning. HENGE is capable to learn from a small amount of labeled data, through a uniformity autoencoder trained on an unsupervised generation task. Experiments on two real-world datasets demonstrate the capability of HENGE in resolving inconsistent device entries in multiple sources. INDEX TERMS Data inconsistency, disentangled representation learning, graph alignment, graph neural network, uniformity autoencoder",
    "authors": [
        {
            "affiliations": [],
            "name": "YUXIANG CAI"
        },
        {
            "affiliations": [],
            "name": "XIN JIANG"
        },
        {
            "affiliations": [],
            "name": "Yang Li"
        },
        {
            "affiliations": [],
            "name": "Xiangyu He"
        }
    ],
    "id": "SP:8ee9be2095a87f90d4409a9938d75c2d474df955",
    "references": [
        {
            "authors": [
                "A. Motro",
                "P. Anokhin",
                "A.C. Acar"
            ],
            "title": "Utility-based resolution of data inconsistencies",
            "venue": "Proceedings of the 2004 international workshop on Information quality in information systems, 2004, pp. 35\u201343.",
            "year": 2004
        },
        {
            "authors": [
                "Y. Jiang",
                "M.A. Jeusfeld",
                "J. Ding"
            ],
            "title": "Evaluating the data inconsistency of open-source vulnerability repositories",
            "venue": "ARES 2021: The 16th International Conference on Availability, Reliability and Security, Vienna, Austria, August 17-20, 2021, D. Reinhardt and T. M\u00fcller, Eds. ACM, 2021, pp. 86:1\u201386:10. [Online]. Available: https://doi.org/10.1145/3465481.3470093",
            "year": 2021
        },
        {
            "authors": [
                "X. Tao",
                "X. Fang"
            ],
            "title": "Detecting data inconsistency based on workflow nets with tables",
            "venue": "IEEE Access, vol. 9, pp. 81 740\u201381 749, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "G. Nikolentzos",
                "P. Meladianos",
                "M. Vazirgiannis"
            ],
            "title": "Matching node embeddings for graph similarity",
            "venue": "Thirty-first AAAI conference on artificial intelligence, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "R. Zafarani",
                "H. Liu"
            ],
            "title": "Connecting users across social media sites: a behavioral-modeling approach",
            "venue": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, 2013, pp. 41\u201349.",
            "year": 2013
        },
        {
            "authors": [
                "J. Zhang",
                "S.Y. Philip"
            ],
            "title": "Multiple anonymized social networks alignment",
            "venue": "2015 IEEE International Conference on Data Mining. IEEE, 2015, pp. 599\u2013608.",
            "year": 2015
        },
        {
            "authors": [
                "R. Singh",
                "J. Xu",
                "B. Berger"
            ],
            "title": "Pairwise global alignment of protein interaction networks by matching neighborhood topology",
            "venue": "Annual international conference on research in computational molecular biology. Springer, 2007, pp. 16\u201331.",
            "year": 2007
        },
        {
            "authors": [
                "C.-S. Liao",
                "K. Lu",
                "M. Baym",
                "R. Singh",
                "B. Berger"
            ],
            "title": "Isorankn: spectral methods for global alignment of multiple protein networks",
            "venue": "Bioinformatics, vol. 25, no. 12, pp. i253\u2013i258, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "B. Perozzi",
                "R. Al-Rfou",
                "S. Skiena"
            ],
            "title": "Deepwalk: Online learning of social representations",
            "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, 2014, pp. 701\u2013 710.",
            "year": 2014
        },
        {
            "authors": [
                "J. Tang",
                "M. Qu",
                "M. Wang",
                "M. Zhang",
                "J. Yan",
                "Q. Mei"
            ],
            "title": "Line: Large-scale information network embedding",
            "venue": "Proceedings of the 24th international conference on world wide web, 2015, pp. 1067\u20131077.",
            "year": 2015
        },
        {
            "authors": [
                "H. Chen",
                "H. Yin",
                "X. Sun",
                "T. Chen",
                "B. Gabrys",
                "K. Musial"
            ],
            "title": "Multi-level graph convolutional networks for cross-platform anchor link prediction",
            "venue": "pp. 1503\u2013 1511, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Wang",
                "Q. Lv",
                "X. Lan",
                "Y. Zhang"
            ],
            "title": "Cross-lingual knowledge graph alignment via graph convolutional networks",
            "venue": "pp. 349\u2013357, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Wu",
                "X. Liu",
                "Y. Feng",
                "Z. Wang",
                "R. Yan",
                "D. Zhao"
            ],
            "title": "Relation-aware entity alignment for heterogeneous knowledge graphs",
            "venue": "arXiv preprint arXiv:1908.08210, 2019.",
            "year": 1908
        },
        {
            "authors": [
                "M. Gori",
                "G. Monfardini",
                "F. Scarselli"
            ],
            "title": "A new model for learning in graph domains",
            "venue": "Proceedings. 2005 IEEE international joint conference on neural networks, vol. 2, no. 2005, 2005, pp. 729\u2013734.",
            "year": 2005
        },
        {
            "authors": [
                "K. Xu",
                "C. Li",
                "Y. Tian",
                "T. Sonobe",
                "K.-i. Kawarabayashi",
                "S. Jegelka"
            ],
            "title": "Representation learning on graphs with jumping knowledge networks",
            "venue": "International conference on machine learning. PMLR, 2018, pp. 5453\u2013 5462.",
            "year": 2018
        },
        {
            "authors": [
                "N. Carlini",
                "F. Tramer",
                "E. Wallace",
                "M. Jagielski",
                "A. Herbert-Voss",
                "K. Lee",
                "A. Roberts",
                "T. Brown",
                "D. Song",
                "U. Erlingsson"
            ],
            "title": "Extracting training data from large language models",
            "venue": "30th USENIX Security Symposium (USENIX Security 21), 2021, pp. 2633\u2013 2650.",
            "year": 2021
        },
        {
            "authors": [
                "H. Huang",
                "J. Zhang",
                "J. Zhang",
                "J. Xu",
                "Q. Wu"
            ],
            "title": "Lowrank pairwise alignment bilinear network for few-shot fine-grained image classification",
            "venue": "IEEE Transactions on Multimedia, vol. 23, pp. 1666\u20131680, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Feizi",
                "G. Quon",
                "M. Recamonde-Mendoza",
                "M. Medard",
                "M. Kellis",
                "A. Jadbabaie"
            ],
            "title": "Spectral alignment of graphs",
            "venue": "IEEE Transactions on Network Science and Engineering, vol. 7, no. 3, pp. 1182\u20131197, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A. Grover",
                "J. Leskovec"
            ],
            "title": "node2vec: Scalable feature learning for networks",
            "venue": "Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, 2016, pp. 855\u2013864.",
            "year": 2016
        },
        {
            "authors": [
                "X. Wang",
                "H. Chen",
                "Y. Zhou",
                "J. Ma",
                "W. Zhu"
            ],
            "title": "Disentangled Representation Learning for Recommendation",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell,",
            "year": 2023
        },
        {
            "authors": [
                "H.-Y. Lee",
                "H.-Y. Tseng",
                "J.-B. Huang",
                "M. Singh",
                "M.-H. Yang"
            ],
            "title": "Diverse image-to-image translation via disentangled representations",
            "venue": "Proceedings of the European conference on computer vision (ECCV), 2018, pp. 35\u201351.",
            "year": 2018
        },
        {
            "authors": [
                "I. Higgins",
                "D. Amos",
                "D. Pfau",
                "S. Racaniere",
                "L. Matthey",
                "D. Rezende",
                "A. Lerchner"
            ],
            "title": "Towards a definition of disentangled representations",
            "venue": "arXiv preprint arXiv:1812.02230, 2018.",
            "year": 1812
        },
        {
            "authors": [
                "C. Qin",
                "B. Shi",
                "R. Liao",
                "T. Mansi",
                "D. Rueckert",
                "A. Kamen"
            ],
            "title": "Unsupervised deformable registration for multi-modal images via disentangled representations",
            "venue": "International Conference on Information Processing in Medical Imaging. Springer, 2019, pp. 249\u2013261.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Hou",
                "X. Liu",
                "Y. Dong",
                "C. Wang",
                "J. Tang"
            ],
            "title": "Graphmae: Self-supervised masked graph autoencoders",
            "venue": "arXiv preprint arXiv:2205.10803, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "K. He",
                "X. Chen",
                "S. Xie",
                "Y. Li",
                "P. Doll\u00e1r",
                "R. Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 16 000\u2013 16 009.",
            "year": 2022
        },
        {
            "authors": [
                "M. Germain",
                "K. Gregor",
                "I. Murray",
                "H. Larochelle"
            ],
            "title": "Made: Masked autoencoder for distribution estimation",
            "venue": "International conference on machine learning. PMLR, 2015, pp. 881\u2013889.",
            "year": 2015
        },
        {
            "authors": [
                "C. Feichtenhofer",
                "H. Fan",
                "Y. Li",
                "K. He"
            ],
            "title": "Masked autoencoders as spatiotemporal learners",
            "venue": "arXiv preprint arXiv:2205.09113, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "P. Velickovic",
                "G. Cucurull",
                "A. Casanova",
                "A. Romero",
                "P. Lio",
                "Y. Bengio"
            ],
            "title": "Graph attention networks",
            "venue": "stat, vol. 1050, p. 20, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J.D.M.-W.C. Kenton",
                "L.K. Toutanova"
            ],
            "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of naacL-HLT, 2019, pp. 4171\u20134186.",
            "year": 2019
        },
        {
            "authors": [
                "A. v. d. Oord",
                "Y. Li",
                "O. Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint, 2018. [Online]. Available: https://arxiv.org/abs/1807.03748",
            "year": 2018
        },
        {
            "authors": [
                "X. Ming"
            ],
            "title": "text2vec: A tool for text to vector",
            "venue": "2022. [Online]. Available: https://github.com/shibing624/text2vec",
            "year": 2022
        },
        {
            "authors": [
                "L.F. Ribeiro",
                "P.H. Saverese",
                "D.R. Figueiredo"
            ],
            "title": "struc2vec: Learning node representations from structural identity",
            "venue": "Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 385\u2013394.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "INDEX TERMS Data inconsistency, disentangled representation learning, graph alignment, graph neural network, uniformity autoencoder\nI. INTRODUCTION China\u2019s state grid corporation is one of the largest electricity companies in the world. It constructs and operates the country\u2019s power infrastructure. In order to maintain this infrastructure, the company needs to process a vast amount of data related to power equipment. The efficiency and reliability of data management is of critical importance.\nData inconsistency has been a haunting problem for power equipment related data management. Copies of equipment data reside in multiple data sources, e.g., each data source is a different database system owned by a specific department, where the department designs and frequently updates the database schema for various business purposes. When the company merges data from different departments (e.g., for statistical analysis), data inconsistency occurs and devices cannot be aligned [1]. Our study shows that more than 40% of devices have different values in each data source, one example of such data inconsistency is shown in Figure 1, .\nTraditionally, manual efforts are required to inspect inconsistent entries and resolve them. Unfortunately, it is labor\ncostly and ineffective, not only because different departments independently update their database schema, but also because key attributes are missing or rewritten. Data inconsistency has been extensively studied in the database community [2, 3, 4]. Nonetheless, previous studies focused on detecting and measuring data inconsistency. The demanding task in power equipment data management is to automatically align inconsistent devices from multiple sources.\nWe consider the task of aligning inconsistent devices as a network alignment problem. In this setting, we can construct a global network or several local networks for multiple data sources, where the devices in the data sources are nodes in the network. By aligning the corresponding nodes (i.e., multiple entries in different data sources that correspond to the same device), we can resolve inconsistencies and integrate the multiple data sources into a unified device management system. Network alignment has been widely used in social networks, biological networks and citation networks[5, 6, 7]. Prior to the advent of deep learning, network alignment was primarily approached using ranking methods[8][9], such as\nVOLUME 10, 2022 1\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nY. Cai et al.: Resolving Power Equipment Data Inconsistency via Heterogeneous Network Alignment\nEquipment ID Usage Name 1302A09MAS Server data processing server 1407B01ASE PC program equipment ... \u2026 \u2026 3301A12AIO Server computing server\nEquipment ID Series Cabinet 1302A09MAS Intel AS-GPS13 1407B01ASE AMD AS-GPS13 \u2026 \u2026 ... 3301A12AIO Intel JK-TAS16\nIdeally aligned devices have the same Equipment ID and the multi-data sources can align by matching the Equipment ID.\nIn reality, since different data sources are managed by different departments, the device Euipment ID may be updated and thus not match.\naligned\npairwise ranking. With the advancement of deep learning, network alignment has shifted towards the use of random walk models[10][11], graph network models[12], and knowledge graphs[13][14].\nHowever, existing network alignment methods are inadequate because of the three unique properties of multi-source power equipment alignment.\nP1: Complex relationships between devices. The relationships between devices can be broadly classified into two categories: physical connections and semantic connections. Physical connections refer to the physical relationships between devices, such as their locations. Semantic connections, on the other hand, refer to the relationships between devices based on their features.\nP2: hybrid contributions of feature information and relational information. In aligning devices, it is equally important to consider the relationship and features of devices. Graph Neural Network (GNN)[15] is a powerful tool for modeling objects with complex relationships. However, its message-passing mechanism may cause the model to be more inclined to capture the relationship information between nodes and ignore the feature information; and its over-smoothing problem[16] may cause falsely similar representations of distinct nodes. These negative effects may be exacerbated in power equipment alignment tasks.\nP3: insufficient training samples. Despite the high volume of testing data, the amount of labeled data that can be used for model training is very limited, because of data privacy [17] and the expensive labeling process. Therefore, how to effectively use a small amount of data to accurately align devices is a challenge.\nIn this paper, we propose HENGE, a novel HEterogeneous Network GEneration model to automatically align devices across multiple data sources. HENGE builds multiple data sources into a heterogeneous graph, and combines physical and semantic relationships. HENGE takes into account both feature and relational information. It improves conventional GNNs by adding a residual channel to each graph convolution layer and repeatedly adding node features, to ensure that the node embedding output by HENGE can retain the original\nfeature information. A disentangled module is proposed to further constrain the model to balance between feature information and relational information. HENGE is capable to learn from a small amount of labeled data, through a novel unsupervised generation task. A Uniformity Autoencoder is presented to recover masked node features through learned neighbor correlations.\nIn summary, our contributions are three-fold. (1) We resolve the data inconsistency problem in power equipment management by network alignment. (2) We propose HENGE that models complex relationships between devices based on a heterogeneous graph, and matches corresponding devices in multiple data sources based on both feature and relational information. (3) We conduct extensive experiments in two real-world datasets and demonstrate HENGE outperforms state-of-the-art competitors with insufficient training data."
        },
        {
            "heading": "II. RELATED WORK",
            "text": ""
        },
        {
            "heading": "A. NETWORK ALIGNMENT",
            "text": "Network alignment aims to uncover the correspondences among nodes in a graph or across several graphs. There are two types of existing studies. Based on pattern matching. In pattern matching, network alignment is regarded as predicting the degree of association between pairs of nodes. [8] aligns protein networks by matching the neighborhood topology. [18] proposes a model LRPABN based on a lowrank pairwise algorithm for few-shot fine-grained image classification. [9] improves on IsoRank by using spectral alignment clusters. [19] is based on graph eigenvalue decomposition, which is used to find the correspondence between two networks in order to compare their similarities.\nBased on graph network. Another type of methods builds a graph network. For example, [10] presents DeepWalk, a novel approach for learning latent representations of vertices in a network. [20] proposes node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. [12] proposes a novel framework that considers multi-level graph convolutions on both local network structure and hypergraph structure in a unified manner. [13] proposes a novel approach for cross-lingual KG alignment via graph convolutional networks (GCNs). [14] proposes a novel Relation-aware Dual-Graph Convolutional Network (RDGCN) to incorporate relation information via attentive interactions between the knowledge graph and its dual relation counterpart, and further capture neighboring structures to learn better entity representations."
        },
        {
            "heading": "B. DISENTANGLED REPRESENTATION LEARNING",
            "text": "Disentangled Representation Learning is an unsupervised learning technique that decomposes each feature into narrowly defined variables and encodes them as separate dimensions. The goal is to mimic the fast intuitive processes of humans using \"high\" and \"low\" dimensional reasoning. Methods for disentangled representation learning include VAE-based, GAN-based, clustering-based methods, and knowledge-guided methods[21]. VAE-based methods"
        },
        {
            "heading": "2 VOLUME 10, 2022",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nY. Cai et al.: Resolving Power Equipment Data Inconsistency via Heterogeneous Network Alignment\nobtain disentangled representation of an object from the perspective of probabilistic generative models. Different from the VAE-based methods, GAN-based methods focus on disentangling the expected factors of the object instead of disentangling each dimension of its representation. Clusteringbased methods utilize the similarities and differences among data points to learn the disentangled concepts behind the observable data. Knowledge-guided methods usually give some supervisions to the model so that the designated parts of the learned representation are able to possess the expected semantics.\nDisentangled representations have been proven to be very effective in graph networks. [22] presents an approach based on disentangled representation for producing diverse outputs without paired training images. [23] proposes that a principled solution to characterising disentangled representations can be found by focusing on the transformation properties of the world. [24] proposes a fully unsupervised multi-modal deformable image registration method (UMDIR), which does not require any ground truth deformation fields or any aligned multi-modal image pairs during training.\nIII. METHODOLOGY In this section, we introduce the HENGE(HEterogeneous Network GEneration) model. As shown as Figure 2, HENGE consists of three stages, graph construction, feature-enhanced residual graph encoder and uniformity autoencoder for generation.\nA. GRAPH CONSTRUCTION We construct a heterogeneous graph for multiple data sources. Without loss of generosity, we illustrate the construction procedure using data from two different databases I,W . The goal is to construct an un-directed labeled graph G = {V , E , Q, A, \u03d5, \u03a8}, where V is a set of nodes, each node v \u2208 RD is a D-dimensional embedding vector, N = |V|. E = {ei,j |i \u2208 V, j \u2208 V} is a set of edges. Q = {I,W} is the set of distinct node types, where I represents the devices from database I , W represents the devices from database W . \u03d5 : V \u2192 Q is the node mapping function. A is the set of distinct edge types. There are three types of edges, A = {SL, SN, SU}, where SL, SN, SU represent the same location, similar name, and similar usage edge relationships of corresponding devices in the two data sources respectively.\nFor two devices in the same database, we can add an edge of the type SL if they appear in the same location. Such an edge type reflects the real spatial relation among power equipment. It is helpful to identify a device by its physical locations. However, it is difficult to match corresponding devices if they have been moved and the location has not been synchronized in one data source. Next, we describe the strategy to link SN and SU edges to capture semantic relationship among devices.\nSpecifically, for each pair of nodes i, j, where node i from database I , node j from database W , we calculated the device\u2019s name similarity sim(vni ,v n j ) between the dif-\nferent databases. We selected the top Kn similar node pairs and built SN edges for them. Similarly, we also calculated the device usage similarity between the different databases, selected the top Ku node pairs, and built SU links for them.\nSNij =\n{ 0, if sim(vni ,v n j ) < 0.9,\n1, else, (1)\nSUij =\n{ 0, if sim(vui ,v u j ) < 0.95,\n1, else, (2)\nwhere SN, SU \u2208 R|I|\u00d7|W|, |I|, |W | represents the number of devices in the two data sources, respectively. sim(\u00b7, \u00b7) denotes cosine similarity between two attribute vectors. vni ,v u i represents the name attribute embedding and the usage attribute embedding of the node vi respectively."
        },
        {
            "heading": "B. FEATURE-ENHANCED RESIDUAL GRAPH ENCODER (FRGE)",
            "text": "Graph Attention Network (GAT) [29] is a neural network model for processing graph data, which can automatically learn important relationships between different nodes in the graph through the attention mechanism. However, since each node embedding is generated based on the information of adjacent nodes, the importance of the information of the node itself is ignored to some extent. In addition, the oversmoothing problem [16] in the graph model further exacerbates the above issue.\nResidual Graph Encoder. To alleviate the oversmoothing problem, we propose a Residual-based Graph Convolution Network (RGCN) with the GAT backbone. By adding shortcut paths[30], the attribute features of nodes are added to each layer of the graph network, thus ensuring that the final learned node representation retains feature information.\nFormally, we define vli = GAT (v l\u22121 i ) as the l-th GAT\nlayer representation of node vli on the graph G, vli = \u03c3( \u2211 j\u2208Ni \u03b1ijWv l\u22121 j ),\n\u03b1ij = exp(LeakyReLU(A[Wvl\u22121i ||Wv l\u22121 j ]))\u2211\nk\u2208Ni exp(LeakyReLU(A[Wv l\u22121 i ||Wv l\u22121 k ]))\n,\n(3) where Ni represents the node vi \u2019s neighbor nodes. A,W are the trainable weight matrix, \u03c3 is the sigmoid activation function, || is the concatenation operation.\nWe added a shortcut path to the massage passing of each layer of GCN, which is shown as follow:\nvi = n\u2211 l=1 SC(GAT (vl\u22121i ),v 0 i , l \u2212 1), (4)\nwhere v0i represents the attribute feature of the node vi and SC is the shortcut function shown as below:\nSC(x, y, l) = { x+ y, l 6= n, x||y, l = n,\n(5)\nVOLUME 10, 2022 3\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nY. Cai et al.: Resolving Power Equipment Data Inconsistency via Heterogeneous Network Alignment\nNote that node embedding will go through the disentangled module to separate finer-grained feature information before the last SC operation.\nDisentangled Representation Learning Module. In order to balance between structural and feature information, we propose a Disentangled Representation Learning (DRL) module. Conventional graph networks blurs the node\u2019s features with its topological information, and may be biased against low-degree nodes which lack of sufficient message passing from neighbors. The DRL encodes the node embedding into feature and structure factors, so that it captures sensitive information for all nodes in identifying corresponding devices. The node embeddings are more general and useful even with limited structure information.\nWe obtain the item representation by decoupling it into three counterparts, namely c1 related to the graph structure and c2 related to the feature itself. We need to align the feature-related embedding with the original node feature information. Formally, assuming vli is the output of the last RGCN layer, we project it onto two feature spaces as follows:\ncki = \u03c3 ( Wkvli + b k )\u2225\u2225\u03c3 (Wkvli + bk)\u2225\u22252 , k = 1, 2 (6)\nwhere W k, bk are the trainable parameters of space k. The node embedding hi after disentangled module can be expressed as follow:\nhi = SC(Wv l i,v 0 i , l) (7)\nwhere W, b are the trainable parameters. To preserve that the two counterparts are disentangled features, we introduce LRD, as shown below:\nLRD = N\u2211 i=1 K\u2211 k=1 \u2212 log exp\n( cki \u00b7 cki /\u03c4 )\u2211K k\u2032=1 exp ( cki \u00b7 ck \u2032 i /\u03c4 ) (8)\nwhere \u03c4 is a temperature parameter. To further emphasize that c2i is a feature-specific embedding, we align c 2 i with the original node feature embedding v0i , shown as following:\nLAL = 1 N N\u2211 i=1 (v0i \u2212 c2i )2 (9)\nThe overall disentangled loss LD is expressed as follow:\nLD = LRD + LAL (10)"
        },
        {
            "heading": "C. UNIFORMITY AUTOENCODER",
            "text": "Inspired by the MAE[26] and BERT[31], We designed an Uniformity Autoencoder model based on the masked mechanism, to learn the internal structure of equipment alignment.\nMasked strategy. We propose two strategies for masking features, RndM (random mask) and RowM (row mask). Specifically, RndM randomly masks some features for each device, aiming to allow the model to learn robust associations between features. RowM randomly masks all features of some devices, and restores the characteristic information of the device by using the neighbor device information through the graph network, aiming at mining the deep feature correlation between devices. In particular, we set p to be the masking ratio. The Uniformity Autoencoder goes through a pretraining phase and a finetuning phase.\nPretraining phase. In the pretraining phase, we mask off the feature information, then use the feature-enhanced residual graph encoder III-B as the encoder of the autoencoder to extract the network structure information and feature information of the power grid equipment, and then use MLP as the decoder to restore the destroyed feature information."
        },
        {
            "heading": "4 VOLUME 10, 2022",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nY. Cai et al.: Resolving Power Equipment Data Inconsistency via Heterogeneous Network Alignment\nFormally, given a equipment node vi\u2019s initial embedding v0i , the pretraining stage can be expressed as follow:\nv\u03020i = mask(v 0 i ) hi = FRGE(v\u0302 0 i ) v\u2032i =MLP (hi)\n(11)\nwhere mask is the masked function in the masked strategy section III-C. hi,v\u2032i represent the hidden state and the reconstruction embedding of node vi respectively. MLP is a twolayer feed-forward neural network with a ReLU nonlinear activation function.\nTo prevent the model from collapsing, we added a uniformity constraint, to map features to a hypersphere space, and increase the expressiveness of the model. We useLcon for the reconstruction loss and Luni for uniformity constraints,\nLcon = 1 N N\u2211 i=1 (v0i \u2212 v\u2032i)2\nLuni = logEvi,vj\u2208V exp(\u22122||v\u2032i \u2212 v\u2032j ||2) (12)\nFinetuning phase. In the finetuning phase, we directly use the full features of the equipment node\u2019s initial embedding v0i to obtain the graph node embedding hi, as shown below:\nhi = FRGE(v 0 i ) (13)\nD. OPTIMIZATION For a pair of equipment hWi and h I i . We treat other equipment hIj that are not aligned with equipment h W i as negative samples and use InfoNCE[32] loss for training:\nLO = \u2211\nhWi \u2208W,hIi\u2208I\n\u2212 log exp(sim(h W i ,h I i )/\u03c4\u2211\nhIj\u2208I exp(sim(hWi ,h I j )/\u03c4)\n(14) Pretraining. The overall pretraining loss LP includes the construction loss Lcon, uniformity loss Luni and disentangled loss LD as follow:\nLP = Lcon + Luni + LD (15)\nFinetuning. Finetuning loss LF includes the objective loss LO and disentangled loss LD as follow:\nLF = LO + LD (16)\nIV. EXPERIMENT In this section, we carry out experiments to answer the following questions.\nRQ1 Did HENGE improve alignment performance on the state power grid equipment alignment task? RQ2 Did the attribute similarity edge strategy work? RQ3 How well did each component in HENGE perform? RQ4 How was HENGE affected by its hyper-parameter set-\nting?"
        },
        {
            "heading": "A. EXPERIMENT SETUP",
            "text": "Dataset. To train the model, we collected two real-world datasets: PC&Server and Router&Switch in the state power grid system. For each dataset, they both have two data sources: I6000 and Web Management(WM). For each device in the data sources, we use the actual aligned devices in two data sources as the ground truth to evaluate model performance. We report key features of devices in both data sources in Table 1, including I6000 equipment and WM equipment, SL, SU, SN, Aligned edges in the training set, and Aligned edges in the test set.\nImplementation. The equipment node embeddings are 4608-dimensional vectors that are composed of 6 equipment feature embeddings. The equipment features contain principal, name, usage, date of manufacture, date of purchase, and brand. Each feature embedding is extracted from a pretrained text2vec[33] model. The GAT embedding size in FRGE is [256,128]. The FC layers size in MLP is [256,4608]. The pretraining epochs and finetuning epochs are both 100. The learning rate is 0.0001 in PC&Server dataset and 0.001 in Router&Switch dataset. HENGE uses Adam as the optimizer.\nBaselines. There are two types of baselines that can be distinguished based on the ranking algorithm or messagepassing approach used. Based on Ranking: (1) EigenAlign [19]proposes a method based on simultaneous alignment of multiple eigenvectors that leads to consistently good performance in different graph models. (2) MLPAlign is a shallow network, trained with a marginal loss. Based on messagepassing: (1) Deepwalk [10] presents a novel approach for learning latent representations of vertices in a network. (2) LINE [11] proposes an edge-sampling algorithm that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. (3) Node2vec [20] proposes an algorithmic framework for learning continuous feature representations for nodes in networks. (4) Struc2Vec [34]presents a novel and flexible framework for learning latent representations for the structural identity of nodes. (5) GAT[29] (6) Sage[35] (7) Align-GCN [13] proposes a novel approach for cross-lingual KG alignment via graph convolutional networks (GCNs). (8) RDGCN [14] proposes a novel Relation-aware Dual-Graph Convolutional Network (RDGCN) to incorporate relation information via attentive interactions between the knowledge graph and its dual relation counterpart, and further capture neighboring structures to learn better entity representations.\nEvaluation Metrics. We use commonly used evaluation metrics, such as Hit Ratio (HR@1, HR@5) and Mean Reciprocal Rank (MRR). The higher the metric value is, the more accurate the results are."
        },
        {
            "heading": "B. COMPARATIVE STUDY",
            "text": "We illustrate the results of different methods under each model in Table 2. Baselines are trained and tested on the same environment. (1) Our method consistently achieves the best results in all measurement metrics, demonstrat-\nVOLUME 10, 2022 5\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nY. Cai et al.: Resolving Power Equipment Data Inconsistency via Heterogeneous Network Alignment\ning the superiority of our method. We italicize the highest score in the baseline in the table. In PC&Server dataset, our method improves HR@5, HR@1, MRR by 100.57%, 32.46%, 63.45% respectively. In Router&Switch dataset, our method improves HR@5, HR@1, MRR by 107.18%, 5.34%, 115.41% respectively. (2) We observe that the messagepassing-based methods are generally better than rankingbased methods. In particular, graph methods[29][35] and knowledge graphs[13][14] are better overall. One of the key advantages of these methods is their ability to incorporate the feature information of the nodes, which has been shown to improve their performance. MLPAlign, in particular, has demonstrated the effectiveness of leveraging node feature information in aligning networks. This emphasis on utilizing node features sets these methods apart from others and has contributed to their success.\nC. ABLATION STUDY To further study the effectiveness of our proposed modules, we conduct ablation studies to remove different modules in our method, namely SimE (Similarity edge establishment\nstrategy in Equation 1), FRGE (Feature-enhanced Residual Graph Encoder in Section III-B) and UAE (Uniformity AutoEncoder in Section III-C). We can observe the effectiveness of each module from the results in Table 3. The base in this table is the result of GAT under the graph that only uses the SL strategy to build edges. For example, after adding SimE, the performance of HR@5, HR@1, and MRR increased by 0.0968, 0.0710, and 0.0698 respectively; adding FRGE and SimE, the performance improved by 0.4581, 0.2352, and 0.3214; adding FRGE, SimE and UAE, the performance increased by 0.4968, 0.2775, and 0.3442, respectively. This justifies the necessity of each module.\nD. IMPACTS OF FEATURE-ENHANCED RESIDUAL GRAPH ENCODER To explore the proposed Feature-enhanced Residual Graph Encoder (FRGE) in detail, we perform model variations in similar scenarios. The base in this table is the result of GAT under the graph that only uses the SL strategy to build edges. From the results in Table 6, we observed a positive effect"
        },
        {
            "heading": "6 VOLUME 10, 2022",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nY. Cai et al.: Resolving Power Equipment Data Inconsistency via Heterogeneous Network Alignment\nof adding align, disen, residual and GAT. This proves the effectiveness of align, disen, Residual and GAT.\nE. IMPACTS OF UNIFORMITY AUTOENCODER To explore UAE in detail, we performed model variations in similar scenarios. The base in the table refers to FRGE. From the results in Table 5, we observed a positive effect of adding UAE-RndM and UAE-RowM. This demonstrates the effectiveness of UAE-RndM and UAE-RowM. In addition, we found that the RowM pre-training strategy is better than the RndM pre-training strategy. The possible reason is that using neighbor information to restore the masked device information can better allow the model to dig out the deep correlation between devices. This is beneficial for the multiple source device alignment task.\nF. IMPACTS OF HYPER-PARAMETERS Finally, we explore the impact of mask ratio p in the Uniformity Autoencoder defined in Section III-C. From the results (Table 6), we find that a mask with probability p = 0.5 produces the best results. Because there are too few features in the mask, it will be difficult for the model to learn the deep association between features and features, and it will be difficult for the model to learn the potential connection between devices after there are too few features in the mask. We believe that retaining 50% of the information in the mask can provide challenging positive samples for the model while retaining 50% of the information can allow the model to learn more effectively."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "This paper shows that the data inconsistency problem in power equipment related data can be alleviated by automatic network alignment. The solution has enormous potential in data management in China\u2019s state grid corporation. The proposed model is shown to be able to capture complex spatial and semantic relationships between devices based on a heterogeneous graph, and matches corresponding devices in multiple data sources based on both feature and relational information. The experimental results also reveal the capability of uniformity autoencoder, which uses unsupervised generation in improving alignment accuracy with insufficient training samples. The proposed framework will be applied to resolve data inconsistency problem in more power equipment data management systems."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "This work is supported by State Grid Fujian Science and Technology Project \u201cResearch on key technologies of full link monitoring of blockchain based on multi-mode and multi-source data fusion\u201d, grant NO. 52130M22000A."
        },
        {
            "heading": "8 VOLUME 10, 2022",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nY. Cai et al.: Resolving Power Equipment Data Inconsistency via Heterogeneous Network Alignment\nlearning for image recognition,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770\u2013778.\n[31] J. D. M.-W. C. Kenton and L. K. Toutanova, \u201cBert: Pretraining of deep bidirectional transformers for language understanding,\u201d in Proceedings of naacL-HLT, 2019, pp. 4171\u20134186. [32] A. v. d. Oord, Y. Li, and O. Vinyals, \u201cRepresentation learning with contrastive predictive coding,\u201d arXiv preprint, 2018. [Online]. Available: https://arxiv.org/abs/1807.03748 [33] X. Ming, \u201ctext2vec: A tool for text to vector,\u201d 2022. [Online]. Available: https://github.com/shibing624/text2vec [34] L. F. Ribeiro, P. H. Saverese, and D. R. Figueiredo, \u201cstruc2vec: Learning node representations from structural identity,\u201d in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 385\u2013394. [35] W. Hamilton, Z. Ying, and J. Leskovec, \u201cInductive representation learning on large graphs,\u201d Advances in neural information processing systems, vol. 30, 2017.\nY uxiang Cai is a PH.D. student at the Shanghai Jiao Tong University. He studied in Fuzhou University and got a master\u2019s degree in software engineering in 2011. Since 2006, Cai Yuxiang has been engaged in electric power informatization related work in State Grid Fujian Electric Power Co., Ltd. As an electric power information expert, he has rich experience in electric power, Internet of things, and data analysis. He is currently studying for a doctorate in electronics and information at\nShanghai Jiaotong University.\nX in Jiang studied in Nanjing University and got a master\u2019s degree in signal and information processing. Since 2012, Xin Jiang has been engaged in power informatization related work in Information and Communication Branch of State Grid Fujian Electric Power Co., Ltd. He has rich experience in information system operation and maintenance, cloud computing platform and grid data security.\nY ang Li is a student in School of informatics from Xiamen University, Xiamen, China. He was born in Foshan, Guangdong. He is currently a graduate student. His research interests include recommender system and graph neural network.\nX iangyu He was born in Hubei Province, China in 2002 and is currently an undergraduate student at the School of Information, Xiamen University. His research interests include recommender systems and graph neural networks.\nCHEN LIN Chen Lin received her PHD degree in software theory from Fudan University, Shanghai, China, in 2010. She is a full professor of computer science at Xiamen University. Her research interest includes big data management and mining.\nVOLUME 10, 2022 9\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
        }
    ],
    "title": "Resolving Power Equipment Data Inconsistency via Heterogeneous Network Alignment",
    "year": 2023
}