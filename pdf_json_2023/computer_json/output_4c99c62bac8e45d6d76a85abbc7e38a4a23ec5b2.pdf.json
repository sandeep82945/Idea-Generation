{
    "abstractText": "In this work, we demonstrate how to reliably estimate epistemic uncertainty while maintaining the flexibility needed to capture complicated aleatoric distributions. To this end, we propose an ensemble of Normalizing Flows (NF), which are state-of-the-art in modeling aleatoric uncertainty. The ensembles are created via sets of fixed dropout masks, making them less expensive than creating separate NF models. We demonstrate how to leverage the unique structure of NFs, base distributions, to estimate aleatoric uncertainty without relying on samples, provide a comprehensive set of baselines, and derive unbiased estimates for differential entropy. The methods were applied to a variety of experiments, commonly used to benchmark aleatoric and epistemic uncertainty estimation: 1D sinusoidal data, 2D windy grid-world (Wet Chicken), Pendulum, and Hopper. In these experiments, we setup an active learning framework and evaluate each model\u2019s capability at measuring aleatoric and epistemic uncertainty. The results show the advantages of using NF ensembles in capturing complicated aleatoric while maintaining accurate epistemic uncertainty estimates.",
    "authors": [
        {
            "affiliations": [],
            "name": "Lucas Berry"
        },
        {
            "affiliations": [],
            "name": "David Meger"
        }
    ],
    "id": "SP:4e68805dd23804783712ebbfe30bdaa87794ba80",
    "references": [
        {
            "authors": [
                "L. Ardizzone",
                "C. L\u00fcth",
                "J. Kruse",
                "C. Rother",
                "U. K\u00f6the"
            ],
            "title": "Guided image generation with conditional invertible neural networks",
            "venue": "arXiv preprint arXiv:1907.02392.",
            "year": 2019
        },
        {
            "authors": [
                "L. Breiman"
            ],
            "title": "Random forests",
            "venue": "Machine learning, 45(1): 5\u201332.",
            "year": 2001
        },
        {
            "authors": [
                "G. Brockman",
                "V. Cheung",
                "L. Pettersson",
                "J. Schneider",
                "J. Schulman",
                "J. Tang",
                "W. Zaremba"
            ],
            "title": "Openai gym",
            "venue": "arXiv preprint arXiv:1606.01540.",
            "year": 2016
        },
        {
            "authors": [
                "H. Choi",
                "E. Jang",
                "A.A. Alemi"
            ],
            "title": "Waic, but why? generative ensembles for robust anomaly detection",
            "venue": "arXiv preprint arXiv:1810.01392.",
            "year": 2018
        },
        {
            "authors": [
                "K. Chua",
                "R. Calandra",
                "R. McAllister",
                "S. Levine"
            ],
            "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models",
            "venue": "Advances in Neural Information Processing Systems, volume 31.",
            "year": 2018
        },
        {
            "authors": [
                "S. Depeweg",
                "J.M. Hern\u00e1ndez-Lobato",
                "F. Doshi-Velez",
                "S. Udluft"
            ],
            "title": "Learning and policy search in stochastic dynamical systems with bayesian neural networks",
            "venue": "arXiv preprint arXiv:1605.07127.",
            "year": 2016
        },
        {
            "authors": [
                "S. Depeweg",
                "J.-M. Hernandez-Lobato",
                "F. Doshi-Velez",
                "S. Udluft"
            ],
            "title": "Decomposition of uncertainty in Bayesian deep learning for efficient and risk-sensitive learning",
            "venue": "International Conference on Machine Learning, 1184\u20131193. PMLR.",
            "year": 2018
        },
        {
            "authors": [
                "A. Der Kiureghian",
                "O. Ditlevsen"
            ],
            "title": "Aleatory or epistemic? Does it matter",
            "venue": "Structural safety,",
            "year": 2009
        },
        {
            "authors": [
                "N. Durasov",
                "T. Bagautdinov",
                "P. Baque",
                "P. Fua"
            ],
            "title": "Masksembles for uncertainty estimation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 13539\u201313548.",
            "year": 2021
        },
        {
            "authors": [
                "C. Durkan",
                "A. Bekasov",
                "I. Murray",
                "G. Papamakarios"
            ],
            "title": "Cubic-Spline Flows",
            "venue": "Workshop on Invertible Neural Networks and Normalizing Flows, International Conference on Machine Learning.",
            "year": 2019
        },
        {
            "authors": [
                "C. Durkan",
                "A. Bekasov",
                "I. Murray",
                "G. Papamakarios"
            ],
            "title": "Neural Spline Flows",
            "venue": "Advances in Neural Information Processing Systems, volume 32.",
            "year": 2019
        },
        {
            "authors": [
                "C. Durkan",
                "A. Bekasov",
                "I. Murray",
                "G. Papamakarios"
            ],
            "title": "nflows: normalizing flows in PyTorch",
            "venue": "https://doi.org/",
            "year": 2020
        },
        {
            "authors": [
                "Y. Freund",
                "R.E. Schapire"
            ],
            "title": "A decision-theoretic generalization of on-line learning and an application to boosting",
            "venue": "Journal of computer and system sciences, 55(1): 119\u2013139.",
            "year": 1997
        },
        {
            "authors": [
                "Y. Gal",
                "Z. Ghahramani"
            ],
            "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
            "venue": "international conference on machine learning, 1050\u2013 1059. PMLR.",
            "year": 2016
        },
        {
            "authors": [
                "Y. Gal",
                "R. Islam",
                "Z. Ghahramani"
            ],
            "title": "Deep bayesian active learning with image data",
            "venue": "International Conference on Machine Learning, 1183\u20131192. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "S.C. Hora"
            ],
            "title": "Aleatory and epistemic uncertainty in probability elicitation with an example from hazardous waste management",
            "venue": "Reliability Engineering & System Safety, 54(2-3): 217\u2013223.",
            "year": 1996
        },
        {
            "authors": [
                "N. Houlsby",
                "F. Husz\u00e1r",
                "Z. Ghahramani",
                "M. Lengyel"
            ],
            "title": "Bayesian active learning for classification and preference learning",
            "venue": "arXiv preprint arXiv:1112.5745.",
            "year": 2011
        },
        {
            "authors": [
                "E. H\u00fcllermeier",
                "W. Waegeman"
            ],
            "title": "Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods",
            "venue": "Machine Learning, 110(3): 457\u2013 506.",
            "year": 2021
        },
        {
            "authors": [
                "A. Kendall",
                "Y. Gal"
            ],
            "title": "What uncertainties do we need in bayesian deep learning for computer vision? Advances in neural information processing",
            "year": 2017
        },
        {
            "authors": [
                "D.P. Kingma",
                "P. Dhariwal"
            ],
            "title": "Glow: Generative flow with invertible 1x1 convolutions",
            "venue": "Advances in neural information processing systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "P. Kirichenko",
                "P. Izmailov",
                "A.G. Wilson"
            ],
            "title": "Why normalizing flows fail to detect out-of-distribution data",
            "venue": "Advances in neural information processing systems, 33: 20578\u201320589.",
            "year": 2020
        },
        {
            "authors": [
                "A. Kirsch",
                "J. Van Amersfoort",
                "Y. Gal"
            ],
            "title": "Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning",
            "venue": "Advances in neural information processing systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "A. Kraskov",
                "H. St\u00f6gbauer",
                "P. Grassberger"
            ],
            "title": "Estimating mutual information",
            "venue": "Physical review E, 69(6): 066138.",
            "year": 2004
        },
        {
            "authors": [
                "T. Kurutach",
                "I. Clavera",
                "Y. Duan",
                "A. Tamar",
                "P. Abbeel"
            ],
            "title": "Model-ensemble trust-region policy optimization",
            "venue": "arXiv preprint arXiv:1802.10592.",
            "year": 2018
        },
        {
            "authors": [
                "B. Lakshminarayanan",
                "A. Pritzel",
                "C. Blundell"
            ],
            "title": "Simple and scalable predictive uncertainty estimation using deep ensembles",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "D.J. MacKay"
            ],
            "title": "Information-based objective functions for active data selection",
            "venue": "Neural computation, 4(4): 590\u2013 604.",
            "year": 1992
        },
        {
            "authors": [
                "G. Papamakarios",
                "E.T. Nalisnick",
                "D.J. Rezende",
                "S. Mohamed",
                "B. Lakshminarayanan"
            ],
            "title": "Normalizing Flows for Probabilistic Modeling and Inference",
            "venue": "J. Mach. Learn. Res., 22(57): 1\u201364.",
            "year": 2021
        },
        {
            "authors": [
                "J. Postels",
                "H. Blum",
                "Y. Str\u00fcmpler",
                "C. Cadena",
                "R. Siegwart",
                "L. Van Gool",
                "F. Tombari"
            ],
            "title": "The hidden uncertainty in a neural networks activations",
            "venue": "arXiv preprint arXiv:2012.03082.",
            "year": 2020
        },
        {
            "authors": [
                "C.E. Rasmussen"
            ],
            "title": "Gaussian processes in machine learning",
            "venue": "Summer school on machine learning, 63\u201371. Springer.",
            "year": 2003
        },
        {
            "authors": [
                "D. Rezende",
                "S. Mohamed"
            ],
            "title": "Variational Inference with Normalizing Flows",
            "venue": "Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, 1530\u20131538. Lille, France.",
            "year": 2015
        },
        {
            "authors": [
                "R.Y. Rubinstein",
                "P.W. Glynn"
            ],
            "title": "How to deal with the curse of dimensionality of likelihood ratios in Monte Carlo simulation",
            "venue": "Stochastic Models, 25(4): 547\u2013568.",
            "year": 2009
        },
        {
            "authors": [
                "B. Settles"
            ],
            "title": "Active learning literature survey",
            "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning.",
            "year": 2009
        },
        {
            "authors": [
                "E.G. Tabak",
                "C.V. Turner"
            ],
            "title": "A family of nonparametric density estimation algorithms",
            "venue": "Communications on Pure and Applied Mathematics, 66(2): 145\u2013164.",
            "year": 2013
        },
        {
            "authors": [
                "E.G. Tabak",
                "E. Vanden-Eijnden"
            ],
            "title": "Density estimation by dual ascent of the log-likelihood",
            "venue": "Communications in Mathematical Sciences, 8(1): 217\u2013233.",
            "year": 2010
        },
        {
            "authors": [
                "E. Todorov",
                "T. Erez",
                "Y. Tassa"
            ],
            "title": "Mujoco: A physics engine for model-based control",
            "venue": "2012 IEEE/RSJ international conference on intelligent robots and systems, 5026\u2013 5033. IEEE.",
            "year": 2012
        },
        {
            "authors": [
                "V. Trep."
            ],
            "title": "The wet game of chicken",
            "venue": "Technical report, Seimens AG.",
            "year": 1994
        },
        {
            "authors": [
                "Q. Wang",
                "S.R. Kulkarni",
                "S. Verdu"
            ],
            "title": "Divergence Estimation for Multidimensional Densities Via k-NearestNeighbor Distances",
            "venue": "IEEE Transactions on Information Theory, 55(5): 2392\u20132405.",
            "year": 2009
        },
        {
            "authors": [
                "C. Winkler",
                "D. Worrall",
                "E. Hoogeboom",
                "M. Welling"
            ],
            "title": "Learning likelihoods with conditional normalizing flows",
            "venue": "arXiv preprint arXiv:1912.00042.",
            "year": 2019
        },
        {
            "authors": [
                "L. Zhang",
                "M. Goldstein",
                "R. Ranganath"
            ],
            "title": "Understanding failures in out-of-distribution detection with deep generative models",
            "venue": "International Conference on Machine Learning, 12427\u201312436. PMLR.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "One common decomposition of uncertainty is aleatoric and epistemic (Hora 1996; Der Kiureghian and Ditlevsen 2009; Hu\u0308llermeier and Waegeman 2021). Aleatoric uncertainty refers to the inherent randomness in the outcome of an experiment, while epistemic uncertainty can be described as ignorance or a lack of knowledge. The important distinction between the two is that epistemic uncertainty can be reduced by the acquisition of more data while aleatoric cannot. Our goal in this paper is to learn aleatoric distributions in high dimensions, with arbitrary distributional form, while also tracking epistemic uncertainty due to non-uniform data sampling.\nNormalizing Flows (NFs) have been shown to be effective at capturing highly expressive aleatoric uncertainty with little prior knowledge (Kingma and Dhariwal 2018; Rezende and Mohamed 2015). This is done by transforming a base distribution via a series of nonlinear bijective mappings, and can model complex heteroscedastic and multi-modal noise. Robotic systems display such noise, as robots interact with\nCopyright \u00a9 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nnonlinear stochastic dynamics often in high dimensions. To further complicate the problem of dynamics modeling, data collection for many robot systems can be prohibitively expensive. Therefore, an active learning framework is usually adopted to iteratively collect data in order to most efficiently improve a model. In order to apply such a framework, ensembles have been employed to capture epistemic uncertainty for deep learning models (Gal, Islam, and Ghahramani 2017).\nIn this paper, we utilize NFs\u2019 ability to capture rich aleatoric uncertainty and extend such models to epistemic uncertainty estimation with ensembles. We then use our NF models to tackle epistemic uncertainty for regression tasks. The contributions of this work are as follows:\n\u2022 We develop two methods for estimating uncertainty for NFs, derive unbiased estimates for said models, and leverage the base distribution to reduce the sampling burden on the estimation of uncertainty.\n\u2022 We leverage memory-efficient ensembling by creating ensembles via fixed dropout masks and apply them to NFs.\n\u2022 We demonstrate the usefulness of uncertainty estimation on an array of previously proposed benchmarks (Depeweg et al. 2018) and novel settings for active learning problems."
        },
        {
            "heading": "2 Problem Statement",
            "text": "Given a dataset D = {xi, yi}Ni=1, where xi \u2208 RK and yi \u2208 RD, we wish to approximate the conditional probability pY |X(y|x). Our models therefore take as input x and output the approximate conditional probability density function, pY |X(y|x) = f\u03b8(y, x), where \u03b8 is a set of parameters to be learned. In the experiments that follow, the ground truth distribution, pY |X(y|x), is not assumed homoscedastic nor do we put restrictions on its shape.\nIn order to capture uncertainty, we utilize the information based criteria proposed in Houlsby et al. (2011) to estimate uncertainty. Let H(\u00b7) denote the differential entropy of a random variable, H(X) = E [\u2212 log(pX(x))], and W \u223c p(w) index different models in a Bayesian framework or an ensemble. Then one can define epistemic uncertainty, I(y\u2217,W ), as the difference of total, H(y\u2217|x\u2217), and aleatoric,\nar X\niv :2\n30 2.\n01 31\n2v 3\n[ cs\n.L G\n] 3\nO ct\n2 02\n3\nEp(w) [H(y \u2217|x\u2217, w)], uncertainty, I(y\u2217,W ) = H(y\u2217|x\u2217)\u2212 Ep(w) [H(y\u2217|x\u2217, w)] , (1) where I(\u00b7) is the mutual information (MI) and y\u2217, x\u2217 denote values not seen during training.\nThese uncertainty estimates can be used for the problem of active learning, where we iteratively add data points to our training dataset in order to improve our model\u2019s performance as much as possible with each data point selected (Settles 2009; MacKay 1992). When doing active learning, one attempts to find the x\u2217 that maximizes Equation (1) and query new points in that region. Intuitively, we are looking for the point x\u2217 which has high total uncertainty, H(y\u2217|x\u2217), but each model has low uncertainty, H(y\u2217|x\u2217, w)."
        },
        {
            "heading": "3 Background",
            "text": "NFs were chosen to capture aleatoric uncertainty as they are capable of representing flexible distributions. Ensembles were chosen as they are a computationally efficient way of estimating epistemic uncertainty for deep learning."
        },
        {
            "heading": "3.1 Normalizing Flows",
            "text": "NFs are non-parametric models that have been shown to be able to fit flexible multi-modal distributions (Tabak and Vanden-Eijnden 2010; Tabak and Turner 2013). They do so by transforming a simple continuous distribution (e.g. Gaussian, Beta, etc.) into a more complex one via the change of variable formula. These transformations make it so one can score and sample from the fitted distribution, thus allowing NFs to be applied to a multitude of problems. Let B be a D-dimensional continuous random vector with pB(b) as its density function and let Y = g(B) where g is invertible, g\u22121 exists, and both g and g\u22121 are differentiable. Using the change of variable formula, we can write the distribution of Y as,\npY (y) = pB(g \u22121(y))|det(J(g\u22121(y)))|, (2)\nwhere J(\u00b7) is the Jacobian and det refers to the determinant. The first term in the product on the RHS of Equation (2) is what changes the shape of the distribution while |det(J(g\u22121(y)))| normalizes it, forcing it to integrate to one.\nNF models can be learned by making g(\u00b7) parameterized by \u03b8, i.e., g\u03b8(\u00b7), and then learned via the log likelihood. In addition, NFs can be made conditional (Winkler et al. 2019; Ardizzone et al. 2019). Following the framework of Winkler et al. (2019), Equation (2) becomes,\npY |X(y|x) = pB|X(g\u22121\u03b8 (y, x))\u00d7 |det(J(g\u22121\u03b8 (y, x)))|, (3)\nlog(pY |X(y|x)) = log(pB|X(g\u22121\u03b8 (y, x)))+ log(|det(J(g\u22121\u03b8 (y, x)))|). (4)\nNote that now g\u22121\u03b8 : Y \u00d7 X 7\u2192 B and NF models commonly chain together multiple bijective mappings to make their models more flexible. When fitting a NF, one typically optimizes the negative log likelihood in Equation (4) over mini batches. For a complete overview of NFs, please refer to Papamakarios et al. (2021)."
        },
        {
            "heading": "3.2 Ensembles",
            "text": "Ensembles use multiple models to obtain better predictive performance and to measure uncertainty. The conditional model can be written as,\nf\u03b8(y, x) = M\u2211 w=1 \u03c0wf\u03b8w(y, x), (5)\nwhere M and \u03c0w are the number of model components and the component weights, respectively.\nEnsembles are typically generated in one of two ways: randomization (Breiman 2001) and boosting (Freund and Schapire 1997). Randomization has been preferred method for deep learning models (Lakshminarayanan, Pritzel, and Blundell 2017). Each model is randomly initialized and then at each training step for a model w, a sample, with replacement, is drawn from a training set D and then a step in the direction of the gradient is taken. This creates diversity as each ensemble component is exposed to a different portion of D at each step in the gradient."
        },
        {
            "heading": "4 Normalizing Flows Ensembles",
            "text": "We propose two approaches for creating NF ensembles, Nflows Out and Nflows Base, both of which rely on neural spline bijective mappings, g\u03b8(\u00b7), as they have been shown to be very flexible distribution approximators (Durkan et al. 2019a,b). Both bagging and random initialization are utilized in training our ensemble components. Each ensemble component is created via fixed dropout masks (Durasov et al. 2021), which reduces the memory and computation cost of our method."
        },
        {
            "heading": "4.1 Nflows Out",
            "text": "Nflows Out creates an ensemble in the nonlinear transformations g\u2019s,\npY |X,W (y|x,w) = f\u03b8w(y, x) = pB|X(g\u22121\u03b8w (y, x))\u00d7 |det(J(g\u22121\u03b8w (y, x)))|. (6)\nThe base distribution is static for each component and the bijective transformation is where the component variability lies. The network g\u03b8w outputs the parameters of cubic spline and thus each ensemble component produces a different cubic spline. By including the complex aleatoric uncertainty prediction of an NF as well as the ability of dropout ensembles to capture uncertainty over learned parameters, our method bridges the state-of-the-art of aleatoric representation and epistemic uncertainty estimation. These new capabilities allows Nflows Out to be applied to new decision making tasks.\nUsing Nflows Out, we can approximate Equation (1). We employ Monte Carlo sampling to estimate the quantities of interest. Thus, total uncertainty is estimated as,\nH(y\u2217|x\u2217) = \u2212E [ log(pY |X(y \u2217|x\u2217)) ]\n(7)\n\u2248 \u2212 1 N N\u2211 n=1 log(pY |X(yn|x\u2217)), (8)\nwhere N is the number of samples drawn. For a given x\u2217 we sample N points from pB|X and then randomly select an ensemble component gw to transform each point. The aleatoric uncertainty, Ep(w) [H(y\u2217|x\u2217, w)], of Equation (1) is calculated in a similar fashion,\nEp(w) [H(y \u2217|x\u2217, w)]\n= \u2212 1 M M\u2211 w=1 E [ log(pY |X,W (y|x\u2217, w)) ] (9)\n\u2248 \u2212 1 M M\u2211 w=1 1 Nw ( Nw\u2211\nnw=1\nlog(pY |X,W (ynw |x\u2217, w)) ) . (10)\nFor a given x\u2217 and w, we sample Nw points from pB|X and then transform the samples according to g\u03b8w . Monte Carlo estimation can suffer from the curse of dimensionality (Rubinstein and Glynn 2009). Experiments and an intuitive proof detailing Monte Carlo\u2019s estimation limitations in high dimensions is contained in the Appendix A.8."
        },
        {
            "heading": "4.2 Nflows Base",
            "text": "In an attempt to alleviate the model\u2019s reliance on sampling, we propose a second ensembling technique for NFs, Nflows Base. The ensembles are created in the base distribution,\npY |X,W (y|x,w) = f\u03b8w(y, x) = pB|X,W (g\u22121\u03b8 (y, x))\u00d7 |det(J(g\u22121\u03b8 (y, x)))|, (11)\nwhere pB|X,W (b|x,w) = N(\u00b5w,\u03a3w), \u00b5w and \u03a3w are estimated via a neural network with fixed dropout masks. In\nNflows Base, the bijective mapping is static for each component while the base distribution varies. Figure 1 depicts Nflows Base on the left and Nflows Out on the right.\nNflows Base has advantages when estimating uncertainty as the component variability is contained in the base distribution and thus analytical formulae can be used to approximate aleatoric uncertainty. Equation (10) becomes,\n\u2248 1 M M\u2211 w=1 1 2 log(det(2\u03c0\u03a3w)). (12)\nThese advantages are seen in memory reduction, computationally efficiency, and estimation error. Let Nx denote the number of x\u2217 to estimate uncertainty for. Then Nflows Out needs to sample TS points where TS = NxNwM and the samples are also used to estimate Equation (8) (Nout = NwM ). On the other hand, for Nflows Base, TS = NxNbase points are needed to estimate uncertainty, where Nbase = NoutM . This reduces the number of samples needed to drawn by a factor M . Please refer to the Appendix A.8 for analysis of time savings. In addition to reducing the sampling required, there is less estimation error as component-wise entropy can be computed directly from Equation (12) instead of through sampling. For nonparametric models, it is uncommon to estimate aleatoric uncertainty without sampling as the output distribution does not hold a parametric form. Note we can use the base distribution to estimate epistemic uncertainty as MI is invariant to bijective mappings (Kraskov, Sto\u0308gbauer, and Grassberger 2004). A proof is contained in Appendix A.10."
        },
        {
            "heading": "5 Baseline Models",
            "text": "We have included several baselines and compared each method\u2019s ability to measure aleatoric and epistemic uncertainty. These baselines are detailed below."
        },
        {
            "heading": "5.1 Probabilistic Network Ensembles",
            "text": "Probabilistic network ensembles (PNEs) have been shown to be a powerful tool to measure uncertainty for neural networks (Chua et al. 2018; Kurutach et al. 2018). We are particularly interested in capturing their capabilities at measuring aleatoric and epistemic uncertainty in supervised learning tasks. PNEs were created with fixed dropout masks with each component modeling a Gaussian,\npY |X(y|x) = 1\nM M\u2211 w=1 pY |X,W (y|x,w). (13)\nThe model is then trained via negative log likelihood, with randomly initialized weights and bootstrapped samples from the training set. We estimate epistemic uncertainty for PNEs via the same method for Nflows Base, Equations (8) and (12)."
        },
        {
            "heading": "5.2 Monte Carlo Dropout",
            "text": "In addition to ensembles, other Bayesian approximations exist in the deep learning community. MC dropout is one of the more prevalent and commonly used Bayesian approximations (Gal and Ghahramani 2016; Gal, Islam, and Ghahramani 2017; Kirsch, Van Amersfoort, and Gal 2019). MC dropout creates ensembles via dropout during training and uses dropout at test time to estimate uncertainty. The output distribution is therefore similar to PNEs in Equation (13).\nHowever, we cannot sample each mask at test time, and thus a random sample of masks needs to be drawn. Therefore, when estimating uncertainty for MC dropout we first sample a set of masks and then sample each Gaussian corresponding to each mask. After which, we apply Equations (8) and (12) to measure uncertainty. Note the each mask has equal probability, as the dropout probability was set to 0.5."
        },
        {
            "heading": "5.3 Gaussian Processes",
            "text": "Gaussian Processes (GPs) are Bayesian models widely used to quantify uncertainty (Rasmussen 2003). A GP model can be fully defined by its mean function m(\u00b7) and a positive semidefinite covariance function/kernel k(\u00b7, \u00b7) of a real process f(x),\nm(x) = E [f(x)] ,\nk(x, x\u2032) = E [(f(x)\u2212m(x))(f(x\u2032)\u2212m(x\u2032))] . (14)\nChoosing the mean and covariance function allows a practitioner to input prior knowledge into the model. From these choices, the predictive posterior becomes,\nEf [x \u2217] = m(x\u2217) = kT\u2217 (K + \u03c3 2 wI) \u22121y (15) varf [x \u2217] = k\u2217\u2217 \u2212 kT\u2217 (K + \u03c32wI)k\u2217, (16)\nwhere k\u2217 = k(X,x\u2217), k\u2217\u2217 = k(x\u2217, x\u2217), K is the kernel matrix with entries Kij = k(xi, xj), and \u03c32w is a noise variance hyperparameter. Note that X and y refer to the complete set of training data. GPs place a probability distribution over functions that are possible fits over the data points. This distribution over functions is used to express uncertainty and is used to quantify epistemic uncertainty."
        },
        {
            "heading": "6 Experiments",
            "text": "We first evaluate each method on two 1D environments, previously proposed (Depeweg et al. 2018), to compare whether each method can capture multi-modal and heteroscedastic noise while measuring epistemic uncertainty. In addition, we provide 5 active learning problems, both 1D and multidimensional. Both openai gym and nflows libraries were utilized with minor changes (Brockman et al. 2016; Durkan et al. 2020). The model from Depeweg et al. (2016, 2018) is Bayesian Neural Network that is limited to mixture Gaussians and is thus not as expressive NFs for aleatoric uncertainty. In addition, there is no source code for it and was not included as a baseline, though we have included MC dropout as a close comparison. For all model hyper-parameters please refer to the Appendix A.1 and the code can be found at https://github.com/nwaftp23/nflows epistemic."
        },
        {
            "heading": "6.1 Data",
            "text": "In order to assess our uncertainty estimation, we evaluate on two 1D benchmarks, Hetero and Bimodal (Depeweg et al. 2018). The Hetero data can be seen in Figure 2 in the top left pane. There are two valleys of low data density displayed in the red bar chart where one would expect high epistemic uncertainty. The Bimodal data can be seen in Figure 3 in the top left pane. Density of data drops off when moving right along the x-axis, thus you would expect epistemic uncertainty to grow as x does. For complete details involved in generating the data, please refer to the Appendix A.2. Both 1D environments were included to provide proof of concept for uncertainty estimation and visualizations.\nIn addition to the 1D environments, we validated our methods across three multi-dimensional environments. Tra-\njectories were gathered from an agent and then the transition function for each environment was estimated, f(st, at) = st+1. The first, Wet Chicken (Trep. 1994), is commonly used to evaluate a model\u2019s capacity to fit multi-modal and heteroscedastic noise (Depeweg et al. 2018, 2016). It simulates a canoe approaching the edge of a waterfall. The paddlers are enticed to the edge of the waterfall as that is the region with the most fish, but as they get closer the probability increases that they fall over the edge and start over. Hence towards the edge of the waterfall the transitions become bimodal. The dynamics are naturally stochastic and are governed by the equations contained in the Appendix A.3. Wet Chicken was included to assess uncertainty estimation on an intrinsically stochastic multi-dimensional environment.\nMoreover, we evaluated all methods on Pendulum (Brockman et al. 2016) and Hopper (Todorov, Erez, and Tassa 2012). These environments are included because they are commonly used in benchmarking and provide us a higher dimensional output space to validate our methods. These environments are inherently nonstochactisic and thus noise was injected into the dynamics in order to produce multi-modal noise. The noise is applied to each action, a\u2032t = at + amax\u03f5, where epsilon is drawn from a multi-modal distribution and amax refers to the maximum value in the action space. Note that the parameters used to create the noise distribution are included in the Appendix A.4 and that at is recorded in the replay buffer, not a\u2032t."
        },
        {
            "heading": "6.2 1D Fits",
            "text": "First, we turn to our 1D environments to give motivation and empirical proof of the epistemic uncertainty estimation. In Figure 2, we can see that each model does a reasonable job\nof capturing the narrow and widening of the ground truth distribution for the Hetero environment. Moreover, Figure 2 displays each model\u2019s capacity to estimate epistemic uncertainty in the red curve and right y-axis. Each model does a good job of capturing higher epistemic uncertainty in the two regions with fewer data points. It is not surprising to see GPs, PNEs, and MC dropout perform well on Hetero, as each method has been shown to accurately fit heteroscedastic Gaussian noise.\nWhile the baseline methods were sufficiently capable of capturing the Hetero data, this not the case for the Bimodal setting. Figure 3 shows that none of the non-NF models can fit both modes. This is to be expected as GPs, PNEs and MC dropout fit a Gaussian at each x. While ensembles such as PNEs and MC dropout can theoretically capture multiple modes, there is no guarantee of this. To guarantee capturing multiple modes, deep learning engineering is required. Once can either manipulate of the loss function or separate the training data according to each mode, both of which may require prior knowledge. On the other hand, NFs are able to fit multiple modes directly via the log likelihood. Note that some fine tuning of the number of transformations and transformation types is required.\nIn addition to the aleatoric uncertainty estimation, Figure 3 displays each model\u2019s ability to capture epistemic uncertainty on the Bimodal data with the red curve on the right y-axis. Nflows Out and Nflows Base are the only models to capture both modes while maintaining accurate epistemic uncertainty estimates. Each model shows the pattern of increasing uncertainty where the data is more scarce with MC dropout having the most trouble displaying this pattern. MC dropout had the most difficulty to estimate epistemic uncertainty on most tasks. This is likely the case because, as opposed to the PNEs or the NFs methods, the number of ensemble components is generally quite larger (2n where n is the number of neurons in your network). Therefore, when estimating Equation (1), a subset of masks needs to be sampled, leading to less stable estimates of uncertainty."
        },
        {
            "heading": "6.3 Active Learning",
            "text": "In order to assess each model\u2019s capability at utilizing epistemic and capturing flexible aleatoric uncertainty, we pro-\nvide an active learning experimental setup. For both 1D and multi-dimensional experiments, each model started with 100 and 200 data points, respectively. At each epoch, the models sampled 1000 new inputs and kept the 10 with the highest epistemic uncertainty. The models are then evaluated by sampling 50 inputs from the test set and averaging the Kullback-Liebler (KL) divergence for the ground truth distribution at those points and model\u2019s distributions,\n1\n50 50\u2211 i=1 DKL(Pi \u2225 Qi) (17)\nNote that the KL divergences reported were estimated via samples using the k-nearest-neighbor method (Wang, Kulkarni, and Verdu 2009). KL divergence was chosen as an evaluation metric as we are most interested in distributional fit. In order to ensure variation, the training and test were gathered via different processes; refer to the Appendix A.5 for more details. All experiments were run across 10 seeds and their mean and standard deviation are reported.\nFigure 4 displays the performance of each method as the training size increases. For each data setting, the NF ensemble models reach lower KLs, thus verifying that they can leverage epistemic uncertainty estimation to learn more expressive aleatoric uncertainty faster. In some cases other models provided better results with small number of data points, this information is conveyed in Table 1, with the best-performing models in bold at different acquisition epochs. Note that in addition to the baselines discussed, we included an NF with no ensembles, using total entropy as the acquisition function."
        },
        {
            "heading": "7 Related Work",
            "text": "Using Bayesian methods, researchers have developed information based criterion for the problem of active learning using Gaussian Processes (GPs) on classification problems (Houlsby et al. 2011). Researchers have leveraged said information based criterion for uncertainty estimation with Bayesian neural networks (Gal, Islam, and Ghahramani 2017; Kendall and Gal 2017; Kirsch, Van Amersfoort, and Gal 2019). These works extended previous epistemic uncertainty estimation by leveraging dropout to estimate uncer-\ntainty on image classification for neural networks. In contrast, our work estimates epistemic uncertainty on a harder output space. The experiments contained in this paper were conducted on regression problems where the output is drawn from continuous distributions in 1-11 dimensions, whereas the previous works applied their methods to classification problems, a 1D categorical output.\nNFs have been shown to be poor estimators of epistemic uncertainty (Kirichenko, Izmailov, and Wilson 2020; Zhang, Goldstein, and Ranganath 2021). Researchers have argued that NFs, inability to differentiate between in and out distribution samples via their likelihood is a methodological shortcoming. Some have found workarounds to this problem, specifically in the sphere of ensembles (Choi, Jang, and Alemi 2018). Ensembles have been shown to be a powerful tool in the belt of a machine learning practitioner by leveraging similar uncertainty quantification benefits to their Bayesian cousins but at a smaller computational footprint (Lakshminarayanan, Pritzel, and Blundell 2017). The work regarding NFs and uncertainty have focused on image generation and unsupervised learning. Our methods differ, as we consider supervised learning problems (Winkler et al. 2019). In addition, the ensembles created in Choi, Jang, and Alemi (2018) contrast with ours as we leverage the base distribution to estimate our uncertainty, use MI instead of WAIC and create our ensembles with less memory.\nIn addition to the examples discussed, work has been done to quantify epistemic uncertainty for regression problems (Depeweg et al. 2018; Postels et al. 2020). Depeweg\net al. (2016) method\u2019s relied on Bayesian approximations to neural networks which modeled mixture of Gaussians and demonstrated their ability to capture uncertainty on three environments. Our work expands on this, by developing more expressive NF models for uncertainty estimation. Postels et al. (2020) develop theory to show how latent representations of a proxy network can be used to estimate uncertainty. They use the proxy network\u2019s latent representation as their conditioner. In contrast, we show to how to do this with one NF model and how to leverage the base distribution to be more sample efficient. This study provided an array of multi-modal problems while Postels et al. (2020) considered a single uni-modal problem. Our work expands on both these papers by providing a comprehensive analysis of different baseline methods, comparing their uncertainty quantification and including higher dimensional data. In addition, we provide a full active learning experimental setup and develop new NF frameworks for measuring uncertainty that are more sample efficient and have lower memory costs."
        },
        {
            "heading": "8 Conclusion",
            "text": "In this paper, we introduced NF ensembles via fixed dropout masks and demonstrated how they can be used efficiently to quantify uncertainty. In doing so, we show how to leverage the base distribution to estimate uncertainty more sample efficiently. Moreover, Nflows Base shows that one can accurately measure uncertainty in the base distribution space. We empirically show that our models outperform the state-ofthe-art in capturing the combination of aleatoric and epis-\ntemic uncertainty on 5 regression tasks. This paper shows that NF ensembles are an expressive model for aleatoric uncertainty while keeping the benefits of previous methods for capturing epistemic uncertainty."
        },
        {
            "heading": "Acknowledgements",
            "text": "Prof. Meger was supported by the National Sciences and Engineering Research Council (NSERC), through the NSERC Canadian Robotics Network (NCRN)."
        },
        {
            "heading": "A Appendix",
            "text": "Please find the supplementary math and figures contained below."
        },
        {
            "heading": "A.1 Model Hyper-Parameters",
            "text": "The PNEs used an architecture of three hidden layers, each with 50 units and ReLU activation functions. MC dropout models had five hidden layers, each with 400 hidden units and ReLU activation functions. This remained constant across all experimental settings. The hyper-parameters for Nflows Out, Nflows Base and Nflows varied across experiments and therefore are described in Table 2. Note that the Nflows hyper-parameters were chosen such that on average they had the same number of parameters as the ensemble components, as the dropout weight was 0.5. The GP used a Radial Basis Function kernel for its covariance function and constant mean for its mean function. We trained all models with 16GB ram on Intel Gold 6148 Skylake @ 2.4 GHz CPUs and NVidia V100SXM2 (16G memory) GPUs. This was accomplished thanks to the Digital Research Alliance of Canada.\nPNEs, Nflows Out and Nflows Base were run with five ensemble components for each experimental setting. To estimate uncertainty at each x conditioned on Nflows Base sampled 1000, Nflows Out 5000, PNEs 5000 and MC drop 2500 points. MC Dropout sampled 20 dropout masks upon which to estimate uncertainty. We found that sampling more became computationally prohibitive and did not increase accuracy of the uncertainty estimates.\nA.2 1D Environments The Hetero data was generated via sampling n points from a categorical distribution with 3 values where pi = 13 and then drawing x from one of three different Gaussians (N(\u22124, 25 ), N(0, 910 ), N(4, 2 5 )) depending on the value of n. Then y was generated as,\ny = 7 sin(x) + 3z \u2223\u2223\u2223cos(x\n2 )\u2223\u2223\u2223 . (18) The Bimodal data was generated by sampling x from an exponential with \u03bb = 2, then n was sampled from a Bernoulli with p = 0.5 and according to the value of n,\ny =\n{ 10 sin(x) + z n = 0\n10 cos(x) + z + 20\u2212 x n = 1 . (19)\nNote that for both Bimodal and Hetero data z \u223c N(0, 1)."
        },
        {
            "heading": "A.3 Wet Chicken",
            "text": "The environment can be viewed as a group of paddlers, the agent, are paddling a canoe on a 2D river. The paddlers position, the state, at time t is represented by (xt, yt). The river has a length l and width w, both of which were set to 5. At the end of the river, in the y-direction, there is a waterfall which forces you to return to the origin (0, 0) when going over. Higher rewards can be found at the edge of the river, rt = \u2212(l \u2212 yt), and thus the paddlers are incentivized to get as close to edge as possible. The agent can take actions\n(at,x, at,y) \u2208 [\u22121, 1]2 which represents the magnitudes of paddling in each direction. In addition to the action, there are turbulences st and drifts vt which behave stochastically.\nThe dynamics are governed by the following system of equations,\nvt = 3\nw xt\nst = 3.5\u2212 vt\nxt+1 =  0 xt + at,x < 0 0 y\u0302t+1 > l w xt + at,x > w\nxt + at,x o/w\nyt+1 =  0 yt + at,y < 0 0 y\u0302t+1 > l\ny\u0302t+1 o/w\nwhere y\u0302t+1 = yt + (at,y \u2212 1) + vt + st\u03c4t and \u03c4t \u223c Unif(\u22121, 1). As the paddlers approach the edge of the waterfall, there transition becomes increasingly bimodal as they might fall over and return to the start. Furthermore the transitions are heteroscedastic, as xt decreases the effect of \u03c4t increases."
        },
        {
            "heading": "A.4 Hopper & Pendulum",
            "text": "Stochasticity was introduced into both environments via a mixture of Gaussians. The mixture weights were sampled from a Dirichlet distribution where \u03b1 was set to a vector of ones, the means of the Gaussians were randomly drawn from a N(0, 1) and the standard deviations of the Gaussians were\nrandomly drawn from a N(0, 0.5) and then squared.\n\u03c0 = [0.062, 0.128, 0.177, 0.001, 0.032, 0.273,\n0.062, 0.033, 0.067, 0.022, 0.142]\n\u00b5 = [0.508,\u22122.059, 1.355,\u22120.675, 0.504, 0.358,\u22120.332,\u22120.647, 2.029,\u22120.294, 0.868]\n\u03c3 = [0.274, 0.276, 0.067, 0.131, 0.028,\n0.008, 0.024, 0.002, 0.008, 0.083, 0.574]\nNote \u03c0 denotes the mixture weights, \u00b5 the means and \u03c3 the standard deviation. There are a total of 11 components and the resulting distribution was mapped through 11+e\u2212x to guarantee the action bounds. Figure 5 shows a sample from the distribution. The distribution was chosen to have more modes than others to offer more diversity in our experiments."
        },
        {
            "heading": "A.5 Train & Test Sets",
            "text": "For Wet Chicken, Pendulum-v0 and Hopper-v2 replay buffers were gathered for both the Train and Test sets. In order to ensure diversity between both sets, the test set was gathered by trained SAC policy while the training set was gathered with a random policy. The SAC policy was trained from the following repository https://github.com/pranz24/pytorch-softactor-critic. For Hetero and Bimodal the training sets were gathered as described whereas the test sets were gathered uniformly at random."
        },
        {
            "heading": "A.6 Additional Results",
            "text": "In addition to the KL metric reported in the text, we evaluated our method on RMSE and Log Likelihood. The RMSE\nis reported in Table 4 and the active learning curves in Figure 6. In all cases Nflows Out and Nflows Base perform favorably to existing methods. Though, WetChicken seems to be a particularly difficult environment for each model to reduce\nits RMSE as they acquire more data. A similar effect was seen for the KL divergence, for PNEs, MC Drop and GPs, and we believe this is because WetChicken has the most interesting stochasticity, both heteroscedastic and multimodal, and thus is difficult to model.\nThe Log Likelihood results are reported in Table 4 and the active learning curves in Figure 7. As before Nflows Out and Nflows Base perform favorably to the other baseline models except for the Hopper-v2 environment. We believe that GPs perform as well in this setting as the noise is homoscedastic. Despite that it is not Gaussian noise, a Gaussian approximation performs well enough.\nWe thought it good practice to include RMSE and Log Likelihood evaluations as they are common. Though, we found KL Divergence the most apt evaluation metric as we are most interested in distributional fit. We want to accurately capture modes to be able to best capture rare events for safety reasons. Nflows Out and Nflows Base perform favorably across all three metrics."
        },
        {
            "heading": "A.7 Acquisition Criteria",
            "text": "In addition to validating our models against baselines, we also evaluated different acquisition criteria. Figure 8 shows the different learning curves across the three metrics on the Bimodal environment for Nflows Out. Using the epistemic uncertainty outperforms all other acquisition criteria. Note that aleatoric uncertainty seems to not improve our model by sampling new data points. This is the case because there is high aleatoric uncertainty in the train set and thus data gets data from the same region.\nFigure 9 depicts the same learning curves for Nflows Base. In this set of graphs, we can see that epistemic uncertainty calculated in the base distribution, Epistemic Unc. Base, and the output distribution, Epistemic Unc. Out, perform very similarly. Thus validating the use of the base distribution to estimate uncertainty. Both figures show that random accumulation of data is superior to that of aleatoric and total uncertainty."
        },
        {
            "heading": "A.8 Monte Carlo High Dimensions",
            "text": "One can think of Monte Carlo estimation as randomly throwing darts at a box with a known area A and multiplying A by the proportion of darts that lie under the function of interest. In higher dimensions, we can think of this as the proportion of darts that land in the hypersphere when randomly throwing at the hypercube. As the number of dimensions d increases the ratio of the volume of the hypersphere to the volume of the hypercube approaches zero.\nProof. We wish to show that limd\u2192\u221e V (S) V (C) = 0, where V (S) and V (C) are the volumes of a hypersphere and hypercube respectively. Let R denote the length of the radius and side length of the hypersphere and hypercube. Note that we are choosing the largest such hypersphere that can fit into\nour hypercube. Thus,\nlim d\u2192\u221e\nV (S) V (C) = lim d\u2192\u221e\nRd\u03c0 d 2\n\u0393( d2+1)\nRd\n= lim d\u2192\u221e\n\u03c0 d 2\n\u0393(d2 + 1)\n= lim d\u2192\u221e\n\u03c0 d 2\nd 2\u0393( d 2 )\n(20)\n= 0 (21)\nOne can get to line (20) via the properties of the gamma function and to line (21) by the fact that limd\u2192\u221e \u0393(d) >> ad, where a is a constant.\nTherefore in higher dimensions one requires more samples to get an accurate estimate as it will be harder to throw a dart into the hypersphere. Figure 11 captures Monte Carlo\u2019s error in higher dimensions. As the the number of dimensions increase so to does the error between the Monte Carlo estimated entropy and the true entropy. In addition, we show the time savings of Nflows Base over Nflows Out. This can be seen in Table 5."
        },
        {
            "heading": "A.9 Aleatoric vs Epistemic Uncertainty",
            "text": "If one were to roll a fair dice, the probability mass function (PMF) is known and thus all uncertainty is known. This example represents aleatoric uncertainty. A dice with an unknown weight, the PMF is unknown but one could be estimated via samples. This is an example of epistemic uncertainty. Figure 10 shows the four scenarios of uncertainty for different weighted dice experiments. Each graph depicts three models represented by different colors:\n(a) Shows an instance of low epistemic and low aleatoric. All three models agree and their PMFs put their mass on one outcome.\n(b) Depicts an instance of high epistemic and low aleatoric. All three models disagree and their PMFs put their mass on one outcome (2, 3, or 4).\n(c) Displays an instance of low epistemic and high aleatoric. All three models agree and their PMFs put their mass equally on each outcome.\n(d) Shows an instance of high epistemic and high aleatoric. All three models disagree and their PMFs put their masses across all possible outcomes.\nThus in order to collect data efficiently, one would most want to gather data for graphs (b) and (d) instead of (a) and (c). Despite the fact that graph (c) has high entropy, it is unlikely that more data could improve the models as they agree."
        },
        {
            "heading": "A.10 MI Invariant to Diffeomorphisms",
            "text": "Theorem A.1. Let y = g(b) where g is a diffeomorphism, i.e. it is a bijection which is differentiable with a differentiable inverse. Then I(Y,W ) = I(B,W ) where W is a random variable representing ensemble components.\nProof. Using the change of variable formula in probability yields:\npY (y) = pB(g \u22121(y))|det(Jg\u22121(y))|\npY,W (y, w) = pB,W (g \u22121(y),W )|det(Jg\u22121(y))|\nApplying these definitions, I(Y,W ) = \u2211 w \u222b y pY,W (y, w) log pY,W (y, w) pY (y)pW (w) dy\n= \u2211 w \u222b b pB,W (b, w) log pB,W (b, w)|det(Jg\u22121(y))| pY (y)pW (w) db\n= \u2211 w \u222b b pB,W (b, w) log pB,W (b, w) pB(b)pW (w) db\n= I(B,W )."
        }
    ],
    "title": "Normalizing Flow Ensembles for Rich Aleatoric and Epistemic Uncertainty Modeling",
    "year": 2023
}