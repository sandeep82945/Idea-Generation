{
    "abstractText": "In recent years, quantum machine learning (QML) has been actively used for various tasks, e.g., classification, reinforcement learning, and adversarial learning. However, these QML studies are unable to carry out complex tasks due to scalability issues on input and output which is currently the biggest hurdle in QML. Therefore, the purpose of this paper is to overcome the problem of scalability. Motivated by this challenge, we focus on projection-valued measurements (PVM) which utilizes the nature of probability amplitude in quantum statistical mechanics. By leveraging PVM, the output dimension is expanded from q, which is the number of qubits, to 2 . We propose a novel QML framework that utilizes PVM for multi-class classification. Our framework is proven to outperform the state-of-the-art (SOTA) methodologies with various datasets, assuming no more than 6 qubits are used. Furthermore, our PVM-based QML shows about 42.2% better performance than the SOTA framework.",
    "authors": [
        {
            "affiliations": [],
            "name": "Won Joon Yun"
        },
        {
            "affiliations": [],
            "name": "Hankyul Baek"
        },
        {
            "affiliations": [],
            "name": "Joongheon Kim"
        }
    ],
    "id": "SP:356c3c7e259a15db7408bf86ea7df8719252904b",
    "references": [
        {
            "authors": [
                "J. Gambetta"
            ],
            "title": "Our new 2022 development roadmap",
            "venue": "IBM Quantum Computing, May 2022.",
            "year": 2022
        },
        {
            "authors": [
                "F. Arute",
                "K. Arya",
                "R. Babbush",
                "D. Bacon",
                "J.C. Bardin",
                "R. Barends",
                "R. Biswas",
                "S. Boixo",
                "F.G. Brandao",
                "D.A. Buell"
            ],
            "title": "Quantum supremacy using a programmable superconducting processor",
            "venue": "Nature, vol. 574, no. 7779, pp. 505\u2013510, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Q. Wu",
                "X. Liu",
                "L. Zhou",
                "Z. Tao",
                "J. Qin"
            ],
            "title": "A quantum framework for modeling interference effects in linguistic distribution multiple criteria group decision making",
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 52, no. 6, pp. 3492\u20133507, June 2022.",
            "year": 2022
        },
        {
            "authors": [
                "V.I. Yukalov",
                "D. Sornette"
            ],
            "title": "Quantitative predictions in quantum decision theory",
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 48, no. 3, pp. 366\u2013381, March 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J. Biamonte",
                "P. Wittek",
                "N. Pancotti",
                "P. Rebentrost",
                "N. Wiebe",
                "S. Lloyd"
            ],
            "title": "Quantum machine learning",
            "venue": "Nature, vol. 549, no. 7671, pp. 195\u2013202, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Xiong",
                "S.X. Ng",
                "G.-L. Long",
                "L. Hanzo"
            ],
            "title": "Dual-frequency quantum phase estimation mitigates the spectral leakage of quantum algorithms",
            "venue": "IEEE Signal Processing Letters, vol. 29, pp. 1222\u20131226, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Du",
                "Y. Yan",
                "Y. Ma"
            ],
            "title": "Quantum-accelerated fractal image compression: An interdisciplinary approach",
            "venue": "IEEE Signal Processing Letters, vol. 22, no. 4, pp. 499\u2013503, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "J. Park",
                "S. Samarakoon",
                "A. Elgabli",
                "J. Kim",
                "M. Bennis",
                "S.-L. Kim",
                "M. Debbah"
            ],
            "title": "Communication-efficient and distributed learning over wireless networks: Principles and applications",
            "venue": "Proceedings of the IEEE, vol. 109, no. 5, pp. 796\u2013819, May 2021.",
            "year": 2021
        },
        {
            "authors": [
                "I. Cong",
                "S. Choi",
                "M.D. Lukin"
            ],
            "title": "Quantum convolutional neural networks",
            "venue": "Nature Physics, vol. 15, no. 12, pp. 1273\u20131278, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H. Baek",
                "W.J. Yun",
                "J. Kim"
            ],
            "title": "Scalable quantum convolutional neural networks",
            "venue": "CoRR, vol. abs/2209.12372, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "I. Nikoloska",
                "O. Simeone"
            ],
            "title": "Training hybrid classical-quantum classifiers via stochastic variational optimization",
            "venue": "IEEE Signal Processing Letters, vol. 29, pp. 977\u2013981, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "W.J. Yun",
                "J. Park",
                "J. Kim"
            ],
            "title": "Quantum multi-agent meta reinforcement learning",
            "venue": "CoRR, vol. abs/2208.11510, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "W.J. Yun",
                "Y. Kwak",
                "J.P. Kim",
                "H. Cho",
                "S. Jung",
                "J. Park",
                "J. Kim"
            ],
            "title": "Quantum multi-agent reinforcement learning via variational quantum circuit design",
            "venue": "Proc. IEEE International Conference on Distributed Computing Systems (ICDCS), Bologna, Italy, July 2022.",
            "year": 2022
        },
        {
            "authors": [
                "W.J. Yun",
                "J.P. Kim",
                "S. Jung",
                "J. Park",
                "M. Bennis",
                "J. Kim"
            ],
            "title": "Slimmable quantum federated learning",
            "venue": "Proc. ICML Workshop on Dynamic Neural Networks (ICML-DyNN), Baltimore, MD, USA, July 2022.",
            "year": 2022
        },
        {
            "authors": [
                "T. Ohyama",
                "Y. Kawamoto",
                "N. Kato"
            ],
            "title": "Intelligent reflecting surface (IRS) allocation scheduling method using combinatorial optimization by quantum computing",
            "venue": "IEEE Transactions on Emerging Topics in Computing, vol. 10, no. 3, pp. 1633\u20131644, July-September 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Kwak",
                "W.J. Yun",
                "S. Jung",
                "J. Kim"
            ],
            "title": "Quantum neural networks: Concepts, applications, and challenges",
            "venue": "Proc. IEEE International Conference on Ubiquitous and Future Networks (ICUFN), Jeju Island, S. Korea, August 2021, pp. 413\u2013416.",
            "year": 2021
        },
        {
            "authors": [
                "M. Cerezo",
                "G. Verdon",
                "H.-Y. Huang",
                "L. Cincio",
                "P.J. Coles"
            ],
            "title": "Challenges and opportunities in quantum machine learning",
            "venue": "Nature Computational Science, vol. 2, no. 9, pp. 567\u2013576, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A. Pesah",
                "M. Cerezo",
                "S. Wang",
                "T. Volkoff",
                "A.T. Sornborger",
                "P.J. Coles"
            ],
            "title": "Absence of barren plateaus in quantum convolutional neural networks",
            "venue": "Physics Review X, vol. 11, p. 041011, October 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S.Y.-C. Chen",
                "S. Yoo"
            ],
            "title": "Federated quantum machine learning",
            "venue": "Entropy, vol. 23, no. 4, p. 460, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Chehimi",
                "W. Saad"
            ],
            "title": "Quantum federated learning with quantum data",
            "venue": "Proc. IEEE International Conference on Acoustics, Speech, & Signal Processing (ICASSP), Singapore and China, May 2022, pp. 8617\u20138621.",
            "year": 2022
        },
        {
            "authors": [
                "T. Hur",
                "L. Kim",
                "D.K. Park"
            ],
            "title": "Quantum convolutional neural network for classical data classification",
            "venue": "Quantum Machine Intelligence, vol. 4, no. 1, pp. 1\u201318, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Kim",
                "J. Huh",
                "D.K. Park"
            ],
            "title": "Classical-to-quantum convolutional neural network transfer learning",
            "venue": "CoRR, vol. abs/2208.14708, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Zheng",
                "Q. Gao",
                "J. L\u00fc",
                "M. Ogorza\u0142ek",
                "Y. Pan",
                "Y. L\u00fc"
            ],
            "title": "Design of a quantum convolutional neural network on quantum circuits",
            "venue": "Journal of the Franklin Institute, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Baek",
                "W.J. Yun",
                "J. Kim"
            ],
            "title": "3D scalable quantum convolutional neural networks for point cloud data processing in classification applications",
            "venue": "CoRR, vol. abs/2210.09728, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Chen"
            ],
            "title": "Quantum dilated convolutional neural networks",
            "venue": "IEEE Access, vol. 10, pp. 20 240\u201320 246, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Choe",
                "M. Perkowski"
            ],
            "title": "Continuous variable quantum mnist classifiers\u2014classical-quantum hybrid quantum neural networks",
            "venue": "Journal of Quantum Information Science, vol. 12, no. 2, pp. 37\u201351, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "L. Banchi",
                "J. Pereira",
                "S. Pirandola"
            ],
            "title": "Generalization in quantum machine learning: A quantum information standpoint",
            "venue": "PRX Quantum, vol. 2, no. 4, p. 040321, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "K. Jacobs"
            ],
            "title": "Quantum measurement theory and its applications",
            "year": 2014
        },
        {
            "authors": [
                "N. Killoran",
                "T.R. Bromley",
                "J.M. Arrazola",
                "M. Schuld",
                "N. Quesada",
                "S. Lloyd"
            ],
            "title": "Continuous-variable quantum neural networks",
            "venue": "Physical Review Research, vol. 1, no. 3, p. 033063, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A. P\u00e9rez-Salinas",
                "A. Cervera-Lierta",
                "E. Gil-Fuster",
                "J.I. Latorre"
            ],
            "title": "Data re-uploading for a universal quantum classifier",
            "venue": "Quantum, vol. 4, p. 226, February 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Jerbi",
                "C. Gyurik",
                "S. Marshall",
                "H.J. Briegel",
                "V. Dunjko"
            ],
            "title": "Variational quantum policies for reinforcement learning",
            "venue": "Proc. Conference on Neural Information Processing Systems (NeurIPS), Virtual, December 2021.",
            "year": 2021
        },
        {
            "authors": [
                "P.-Y. Chen",
                "H. Zhang",
                "Y. Sharma",
                "J. Yi",
                "C.-J. Hsieh"
            ],
            "title": "ZOO: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models",
            "venue": "Proc. ACM Workshop on Artificial Intelligence and Security, November 2017.",
            "year": 2017
        },
        {
            "authors": [
                "G.E. Crooks"
            ],
            "title": "Gradients of parameterized quantum gates using the parameter-shift rule and gate decomposition",
            "venue": "CoRR, vol. abs/1905.13311, 2019.",
            "year": 1905
        },
        {
            "authors": [
                "H. Wang",
                "Y. Ding",
                "J. Gu",
                "Y. Lin",
                "D.Z. Pan",
                "F.T. Chong",
                "S. Han"
            ],
            "title": "QuantumNAS: Noise-adaptive search for robust quantum circuits",
            "venue": "Proc. IEEE International Symposium on High-Performance Computer Architecture (HPCA), Los Alamitos, CA, USA, April 2022, pp. 692\u2013 708.",
            "year": 2022
        },
        {
            "authors": [
                "T. Sleator",
                "H. Weinfurter"
            ],
            "title": "Realizable universal quantum logic gates",
            "venue": "Physical Review Letters, vol. 74, no. 20, p. 4087, 1995.",
            "year": 1995
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Quantum Computing, Quantum Convolutional Neural Network, Quantum Machine Learning.\nI. INTRODUCTION IBM Quantum has publicized the 2022 development roadmap, announcing that 10\u2013100k qubits quantum computers will be developed by 2026 [1]. Spurred by the recent advance of quantum computers, theoretical quantum supremacy has been corroborated through several simulations [2]. Due to its strengths, quantum computing is a strong candidate to replace classical computing in the near future. This has motivated many researchers to attempt combining existing fields with quantum computing [3], [4]. Quantum machine learning (QML) is one of such emerging fields which exploits the strengths of both quantum computing and machine learning [5]\u2013[7]. Basically, QML re-implements the existing machine learning (ML) solutions and neural networks (NN) using quantum neural networks (QNN) [8], [9], e.g., image classification [10], [11], reinforcement learning [12], [13], federated learning (FL) [14], and communication networks [15]. Compared to ML, QML is still in extremely early stages of research, with many terminologies and theories not yet clearly defined [16]. In addition, it also has important challenges to resolve such as scalability, trainability, and difficulty in largescale simulations [17]. Quantum convolution neural network (QCNN) is one of the innovative solutions which has resolved trainability [18] and scalability on input data [10].\nHowever, QML still suffers from scalability on outputs. Thus, QML could only be used for simple tasks (e.g., binary\nThis research was funded by the National Research Foundation of Korea (2022R1A2C2004869). (Corresponding author: Joongheon Kim).\nWon Joon Yun, Hankyul Baek and Joongheon Kim are with the School of Electrical Engineering, Korea University, Seoul 02841, Republic of Korea (e-mails: {ywjoon95,67back, joongheon}@korea.ac.kr).\nclassification [19]\u2013[23]), or classical NNs must be used at the end of QNN in order to obtain good performance (i.e., quantum-classical hybrid computing [24]\u2013[26]). Despite the importance of pure QML in proving quantum supremacy, the multi-class classification with a pure quantum version has not been considered yet.\nMotivated by these trends, we propose the methodology of QML for multi-class classification. To understand how QML can achieve multi-class classification, we investigate quantum measurement theory and its applications. Most quantum computers and QML frameworks follow the positive operatorvalued measure (POVM) to obtain the output [27]. However, the size of the POVM-based output in q-qubit system cannot exceed the number of qubits, i.e., q and this hinders the output scalability of QNN. Next, we focus on the projection-valued measure (PVM), which is a special form of POVM [28]. By utilizing PVM in QNN, 2q outputs (named observables) can be produced by utilizing whole bases of q-qubits. By leveraging this nature of PVM, the quantum machine learning framework for multi-class classification without any classical NN methodologies is proposed. In addition, we propose a probability amplitude regularizer to normalize the observables with respect to true labels such that the performance of QML framework can be significantly improved. Contributions. The contributions of our proposed QML framework are listed as follows. \u2022 We first propose a novel PVM-based QNN for multi-\nclass classification which does not use any classical NN methodologies. Our proposed QML framework provides\nar X\niv :2\n21 0.\n16 73\n1v 2\n[ qu\nan t-\nph ]\n1 4\nN ov\n2 02\n2\n2 Extracted Features \u22f0 Classical Data Intermediate Feature Q C N N Q C N N Q C N N Q C N N Q C N N QNN |0\u27e9 |0\u27e9 |0\u27e9 |0\u27e9 |0\u27e9 |0\u27e9 |0\u27e9 |0\u27e9 |0\u27e9 |0\u27e9\nPrediction \ud835\udc5d(\ud835\udc32|\ud835\udc17; \ud835\udec9)\ud835\udc17\n1 \u27e81| Pr(\ud835\udc66 = 1)\nPr(\ud835\udc66 = 2!) 2! \u27e82!|\n\u22ee\nPOVM POVM PVM\n\u22ef\n\u22ee\n\ud835\udc44-qubits \ud835\udc5e-qubits\nFig. 2. The computing processes on QCNN and QNN. Each color (e.g., red, blue, yellow, magenta, or cyan) indicates the channel. Each 2D grid indicates the patch in each channel. When the quantum state is measured in QCNN, the expected value of projection for each qubit represents the value of the output channel. The extracted features are vectorized classical data, which is given as an input of QNN. QNN makes a probability measure that corresponds to the prediction of classes.\nan insightful bridge between quantum computing and machine learning. \u2022 Moreover, we show the quantum supremacy in QCNN computation operations by comparing the computational complexity of our proposed model to that of classical NN methodologies. \u2022 Finally, we improve the scalability of QML without classical NN methodologies by increasing the input up to 32\u00d7 32\u00d7 3 and output up to 26 classes, respectively. Organization. The rest of this paper is organized as follows. Sec. II introduces the preliminary knowledge of QML. After that, Sec. III presents the proposed novel QML framework for multi-class classification; and then, Sec. IV evaluates the performance of the proposed framework. Lastly, Sec. V concludes this paper and presents future research directions."
        },
        {
            "heading": "II. PRELIMINARIES: QUANTUM MACHINE LEARNING",
            "text": ""
        },
        {
            "heading": "A. Notation and Mathematical Setup",
            "text": "In this paper, we use notations \u0398 = {\u03b8enc;\u03b8PQC} for trainable parameters. We define \u03b6 , {(X,y)} as a sampled mini-batch where X and y stand for the sampled input data and the corresponding labels from mini-batch, respectively. The labels y , {yn}|y|n=1 are one-hot vectors where yn = 1 if the true labels are n \u2208 N[1, |y|], and the other elements are 0. The extracted features are denoted as X\u0302. Dirac-notation is used to represent the quantum state and its operations. In addition, the operators (\u00b7)\u2020 and \u2297 stand for complex conjugate transpose and tensor product, respectively. We use the two terms Q and q separately to indicate a Q-qubit system that encodes classical data and a q-qubit system that performs prediction. The Q qubit quantum state is defined as follows,\n|\u03c8\u3009 = 2Q\u2211 n=1 \u03b1n |n\u3009 , (1)\nwhere \u03b1n and |n\u3009 stand for the probability amplitude and n-th basis in Hilbert space, respectively. By the definition of Hilbert space (i.e., H\u2297Q \u2261 C2Q ), probability amplitude \u2200\u03b1n \u2208 C satisfies the following equation, i.e., \u22112Q n=1 |\u03b1n|2 = 1."
        },
        {
            "heading": "B. Quantum Measurement Theory for Classification",
            "text": "In measurement theory, two main streams of study exist in the literature, i.e., POVM, and PVM [28]. POVM generates\noutputs by projecting quantum states into Pauli-Z measure devices for every qubit, where Pauli-Z is Z = diag(1,\u22121). The n-th projection matrix is defined as Pn , I\u2297n\u22121\u2297Z\u2297I\u2297Q\u2212n, where I stands for 2 \u00d7 2 identity matrix, respectively. PVM calculates the expectation value of a projection Pn as random variable denoted as On \u2208 R[\u22121, 1] where \u2200n \u2208 N[1, Q]. Its expectation value of m-th qubit is \u3008On\u3009 = \u3008\u03c8|Pn |\u03c8\u3009. According to [28], the expectation value of a projection is used with the softmax function and temperature parameter \u03b2 \u2208 R to make a prediction for q-classes as follows,\nPrPOVM (y = n) = exp(\u03b2\u3008On\u3009)\u2211q n=1 exp(\u03b2\u3008On\u3009) . (2)\nNote that the number of qubits q should satisfy q \u2265 |y| in POVM. Additionally, POVM with classical NNs is widely used to make large-scale predictions as it is not limited by the condition q \u2265 |y|. In this paper, PVM is considered to be a special type of POVM and when the probability is measured by projecting the quantum state into projector {|n\u3009\u3008n|}2qn=1, the output is expressed as follows,\nPrPVM (y = n) = \u3008\u03c8|n\u3009\u3008n|\u03c8\u3009 = |\u3008\u03c8|n\u3009|2 = |\u03b1n|2. (3)\nNote that the output of PVM is a probability distribution which can be represented as\nPrPVM (y = n) = 2q\u2211 n=1 |\u03b1n|2 = 1. (4)\nIn this paper, both POVM and PVM are used for QCNNbased image processing and QNN-based classification."
        },
        {
            "heading": "III. QUANTUM MACHINE LEARNING FRAMEWORK FOR MULTI-CLASS CLASSIFICATION",
            "text": ""
        },
        {
            "heading": "A. Architecture",
            "text": "Our QML framework for multi-class classification consists of QCNNs and QNN. The structure of a QCNN/QNN is tripartite: the state encoding, linear transform with parametrized quantum circuits (PQCs), and the measurement [29]. First of all, we consider data-reuploading [30] for state encoder because the classical input size is larger than the number of qubits. To successfully encode the classical input data X, the\n3 Observable Index Pr ob ab ilit y Binary Cross-Entropy Loss Probability Amplitude Regularizer True Label Unused Labels Used Labels Prediction of true label Prediction of other used labels Prediction of unused labels\nFig. 3. The illustration of the objective function (|y| = 10 and q = 4). Two loss functions are used in this paper (i.e., binary cross-entropy loss and probability amplitude regularizer). Leveraging our proposed loss function has increased the prediction frequency of true label while decreasing the prediction frequency of other used labels, and unused labels.\nX is split into [x1; \u00b7 \u00b7 \u00b7 ;xcin ] which are encoded to probability amplitudes and the encoding process is expressed as follows,\n|\u03c8enc\u3009 = U(\u03b8cin)U(xcin) \u00b7 \u00b7 \u00b7U(\u03b81)U(x1)|\u03c80\u3009, (5)\nwhere |\u03c80\u3009 denotes the initial quantum state, e.g., the first standard basis of 2Q-dimensional vector and \u2200\u03b8c \u2282 \u03b8enc, and \u2200c \u2208 N[1, cin]. The encoded state |\u03c8enc\u3009 is processed with PQCs where the result is presented as |\u03c8PQC\u3009 = U(\u03b8PQC)|\u03c8enc\u3009. The processed quantum state |\u03c8PQC\u3009 is measured by POVM in QCNN, and PVM in QNN, respectively."
        },
        {
            "heading": "B. Pipeline",
            "text": "POVM-based QCNN for Image Processing. As presented in Fig. 2, QCNN takes input from classical data and returns the extracted feature. Algorithm 1(lines 2\u201313) presents the progression of image processing with QCNN. In QCNN, the input has W\u00d7H\u00d7cin shape. Note that W\u0302 and H\u0302 depend on the kernel size \u03ba, stride s, and padding d, i.e., W\u0302 = (W + d)/s and H\u0302 = (H + d)/s. Each patch of input is feed-forwarded to QCNN by data-reuploading method (5). Then, the pooling of QCNN is carried out by POVM. The n-th expected value of POVM \u3008On\u3009 corresponds to a scalar value of n-th channel and the output has W\u0302 \u00d7 H\u0302 \u00d7 cout shape. The computational complexity of our QNN per layer is O(W\u0302 \u00b7H\u0302 \u00b7\u03ba2 \u00b7cin), where \u03ba denotes the kernel size, whereas the computational complexity of classical CNN is O(W\u0302 \u00b7 H\u0302 \u00b7 \u03ba2 \u00b7 cin \u00b7 cout). Note that the input and output channels in QCNN are scalable under the constraint \u2200cin, cout \u2208 N[1, Q]. PVM-based QNN for Classification. Generally for quantum circuits, the number of classes |y| is larger than the number of qubits q. However, it is hard to train QCNN/QNN when the number of qubits is increased. Thus, softmax-based POVM such as (2) is limited to simple multi-class classification (e.g., binary classification or 4 classes classification). Thus, in order to extend quantum computing to complex multi-class classification, we consider the PVM-based QNN, where its process is presented in Algorithm 1(lines 14\u201320). Similar to QCNN processing, the extracted features are encoded with (5). To enable multi-class classification, we design the measurement\nAlgorithm 1: QML for Multi-Class Classification\n1 Notation. Processing data X\u0306, Processed data X\u0303, stride s, kernel size \u03ba, and the number of QNN depths D; 2 Image Processing(X): 3 X\u0306\u2190 X; 4 for w \u2208 {0, s, 2s, \u00b7 \u00b7 \u00b7 ,W \u2212 s} do 5 for h \u2208 {0, s, 2s, \u00b7 \u00b7 \u00b7 , H \u2212 s} do 6 Initialize quantum state, |\u03c8\u3009 \u2190 |0\u3009; 7 for c \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , cin} do 8 Prepare data, x\u0306\u2190 X\u0306w:w+\u03ba,h:h+\u03ba,c; 9 Upload data, |\u03c8\u3009 \u2190 U(\u03b8c) \u00b7 U(x\u0306) \u00b7 |\u03c8\u3009;\n10 for c \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , cout} do 11 Measure through POVM and collects\noutputs, X\u0303w s , h s ,c \u2190 \u3008Oc\u3009x\u0306,\u0398;\n12 Get intermediate feature, X\u0306\u2190 X\u0303; 13 Output: Extracted feature X\u0302; 14 Prediction(X\u0302): 15 Initialize p\u2190 \u2205; 16 Prepare the pure density matrices, {|1\u3009\u30081|, \u00b7 \u00b7 \u00b7 , |n\u3009\u3008n|, \u00b7 \u00b7 \u00b7 |2q\u3009\u30082q|}; 17 for m \u2208 {1, \u00b7 \u00b7 \u00b7 , 2q} do 18 Obtain prediction value of class c; 19 p\u2190 p \u222a pc; 20 Output: Prediction p;\nof QNN with PVM. Then, the probability of 2q observables can be obtained when a quantum state |\u03c8\u3009 is projected into pure density matrices, i.e., {|1\u3009\u30081|, \u00b7 \u00b7 \u00b7 , |n\u3009\u3008n|, \u00b7 \u00b7 \u00b7 , |2q\u3009\u30082q|}. The probability of n-th class is expressed as follows,\npn = Pr(y = n|X; \u0398) = |\u03b1n|2. (6)\nNote that our framework does not require softmax function to make predictions with logits since the probability amplitude \u03b1n is mapped to the probability of class n directly. Additionally, the softmax temperature coefficient is not required as well [31]. However, with a na\u0131\u0308ve consideration, PVM suffers from increased error probability due to the presence of unused classes. We will elaborate on how to resolve it on next."
        },
        {
            "heading": "C. Training",
            "text": "In this paper, we design a new regularizer to reduce performance degradation incurred by training errors due to the probability of unused classes. In Fig. 3, we compose the objective function with binary cross-entropy function LBCE and probability amplitude regularizer LPAR for probability regarding unused class indices, which is expressed as follows,\nLBCE(\u0398;X) = \u2212 |y|\u2211 n=1 [yc log pc + (1\u2212 yc) log(1\u2212 pc)], (7)\nLPAR(\u0398;X) = \u2212 2q\u2211\nn\u2032>|y|\nlog(1\u2212 pn\u2032), (8)\n4 0 200 400 600 800\nEpoch\n0\n0.2\n0.4\n0.6\n0.8\nT o p -1\nA c c u ra\nc y\nTrain Test\n0 200 400 600 800\nEpoch\n0\n0.2\n0.4\n0.6\n0.8\nT o p -1\nA c c u ra\nc y\n0 200 400 600 800\nEpoch\n0\n0.1\n0.2\n0.3\n0.4\nT o p -1\nA c c u ra\nc y\n0 200 400 600 800\nEpoch\n0\n0.1\n0.2\n0.3\n0.4\nT o p -1\nA c c u ra\nc y\n(a) MNIST (b) FashionMNIST (c) CIFAR10 (d) EMNIST-letters\nFig. 4. Learning curve with various datasets.\nwhere q \u2265 dlog2(|y|)e. Thus, we finalize the train loss function for a one-step single update as follows,\nL(\u0398; \u03b6) = 1 |\u03b6| \u2211 (X,y)\u2208\u03b6 [LBCE(\u0398;X) + LPAR(\u0398;X)]. (9)\nNext, the gradient of the train loss function is obtained as follows. Because a quantum computer cannot use classical training methods, e.g., back-propagation with chain rule, the 0- th order optimization is utilized to estimate the gradient, called parameter-shift rule [32], [33]. By calculating the symmetric difference quotient of loss L, the loss gradient is obtained as follows,\n\u2202L(\u0398; \u03b6) \u2202\u03b8m = \u2202L(\u0398; \u03b6) \u2202f(\u0398; \u03b6) \u00b7 \u2202f(\u0398; \u03b6) \u2202\u03b8m , (10)\ns.t. \u2202f(\u0398; \u03b6)\n\u2202\u03b8m = f\n( \u0398 + \u03c0\n2 em; \u03b6\n) \u2212 f ( \u0398\u2212 \u03c0\n2 em; \u03b6\n) , (11)\nwhere f(\u0398; \u03b6) denote the output of QNN. In addition, em is an one-hot vector that eliminate all components except \u03b8m, i.e., \u0398 \u00b7 em = \u03b8m and \u2200m \u2208 N[1, |\u0398|], respectively. Finally, the trainable parameters in our QML framework are updated as \u0398\u2190 \u0398\u2212 \u03b7\u2207\u0398L(\u0398), where \u03b7 is a learning rate."
        },
        {
            "heading": "IV. PERFORMANCE EVALUATION",
            "text": ""
        },
        {
            "heading": "A. Experiment Setup",
            "text": "In this section, we mainly investigate the trainability of our QML framework with various datasets and the impact of a probability amplitude regularizer via performance evaluation. For this, we benchmark the training accuracy, test accuracy, comparison to state-of-the-art (SOTA), and the ablation of the regularizer. Because our main scope is to investigate the scalability of QML, we do not interpolate the input data size nor use selected classes. The datasets used in this paper are listed in Table I. We use two QCNN models for image processing where 4\u00d74 kernel is commonly used with stride s = 3 and the\nMethod (Dataset)\nQuantumNAS (MNIST) Ours (MNIST) Ours (FashionMNIST) Ours (CIFAR10) Ours (EMNIST-letters)\nTo p-\n1 Ac\ncu ra cy 0 0.2 0.4 0.6\n0.8\nLoss Function (Dataset)\nw/ \u2112!\"# (CIFAR10) w/o \u2112!\"# (CIFAR10) w/ \u2112!\"# (EMNIST-letters) w/o \u2112!\"# (EMNIST-letters)\nTo p-\n1 Ac\ncu ra\ncy\n0\n0.1\n0.2\n0.3\n0.4\n(a) Comparison with SOTA [34] (b) Impact of Regularizer\nFig. 5. Additional experiments: (a) shows the comparison between our framework and QuantumNAS [34], and (b) shows the ablation study of probability amplitude regularizer.\nkernels have 3 channels. Our framework utilizes 4 qubits and controlled unitary gates for all quantum circuits, e.g., QCNN, and QNN [35]. The hyperparameters used in this paper are as follows, i.e., Adam optimizer, 8 \u00d7 10\u22123 for initial learning rate, and 1,024 for batch size. Note that all experiments for the proposed QML framework are conducted in classical computers with Python v3.8.10 and torchquantum [34]."
        },
        {
            "heading": "B. Numerical Results",
            "text": "Trainability. Fig. 4 shows the learning curves with various datasets. Our framework converges to sub-optimal for all datasets. In Fig. 4(a), the test accuracy increases from 6.25% to 74.2%, and the domain gap between the train set and the test set is about 5.7% between the top-1 accuracies. In Fig. 4(b), the test accuracy increases from 6.25% to 73.8%, where 6.8% gap exists. In Fig. 4(c)/(d), the top-1 accuracy for CIFAR10 and EMNIST-letters show 34.4% and 33.1%, respectively, and the domain gap between the train set and test set is lower than that of MNIST and FashionMNIST. Comparison to SOTA. We compare our framework to QuantumNAS [34]. According to [34], the input is 6 \u00d7 6 sized MNIST images, and QuantumNAS considers quantum noise to exist on the quantum devices being used. However, we benchmark our framework with the original size of the dataset as shown in Table I without considering quantum noise. As shown in Fig. 5(a), our framework outperforms QuantumNAS by 44.2% regarding top-1 accuracy given the MNIST\n5 dataset. Moreover, our framework is benchmarked with various datasets, i.e., FashionMNIST, CIFAR10, EMNIST-letters using less than 7 qubits, whereas QuantumNAS uses 10 qubits. In summary, PVM enables not only multi-class classification but also achieves higher accuracy than SOTA. Ablation Study of Regularizer. In this paper, we propose a probability amplitude regularizer and conduct an ablation study of the regularizer with two datasets (i.e., CIFAR10, EMNIST-letters). As shown in Fig. 5(b), the top-1 accuracy of the training scheme with LPAR shows better performance of 7.5% for CIFAR10 and 17.8% for EMNIST-letters. It is because the prediction frequency of other used labels is decreased via the probability amplitude regularizer. Thus, the significant influence of our proposed regularizer in our QML framework for multi-class classification is demonstrated."
        },
        {
            "heading": "V. CONCLUSIONS AND FUTURE WORK",
            "text": "We aim to expand the limited input and output dimensions of QML and to achieve multi-class classification by leveraging quantum circuits. In order to carry out multiclass classification, we utilize QCNN with POVM as well as QNN with PVM. Additionally, we propose a probability amplitude regularizer which fits the probability distribution of observables to the probability distribution of classification. Via extensive results showing the proposed model outperforming SOTA methodologies, we corroborate the efficacy of PVMbased QML for multi-class classification.\nOur future work directions are as follows, i.e., (1) quantum reinforcement learning with the large action spaces or multiagent setting with less qubits and (2) quantum object detection by leveraging POVM for bounding box prediction and PVM for classification."
        }
    ],
    "title": "Projection Valued Measure-based Quantum Machine Learning for Multi-Class Classification",
    "year": 2022
}