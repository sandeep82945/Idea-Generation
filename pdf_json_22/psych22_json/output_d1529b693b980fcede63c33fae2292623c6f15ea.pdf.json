{
    "abstractText": "27 Perception and categorization of objects in a visual scene are essential to grasp the 28 surrounding situation. However, it is unclear how neural activities in spatially 29 distributed brain regions, especially in terms of temporal dynamics, represent visual 30 objects. To address this issue, we explored the spatial and temporal organization of 31 visual object representations using concurrent functional magnetic resonance imaging 32 (fMRI) and electroencephalography (EEG), combined with neural decoding using deep 33 neural networks (DNNs). Visualization of the fMRI DNN revealed that visual 34 categorization (faces or non-face objects) occurred in brain-wide cortical regions, 35 including the ventral temporal cortex. Interestingly, the EEG DNN valued the earlier 36 phase of neural responses for categorization and the later phase of neural responses for 37 sub-categorization. Combination of the two DNNs improved the classification 38 performance for both categorization and sub-categorization. These deep learning-based 39 results demonstrate a categorization principle in which visual objects are represented in 40 a spatially organized and coarse-to-fine manner. 41 2 . CC-BY-NC-ND 4.0 International license made available under a (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is The copyright holder for this preprint this version posted July 10, 2022. ; https://doi.org/10.1101/2022.04.06.487262 doi: bioRxiv preprint",
    "authors": [
        {
            "affiliations": [],
            "name": "Noriya Watanabe"
        },
        {
            "affiliations": [],
            "name": "Kosuke Miyoshi"
        },
        {
            "affiliations": [],
            "name": "Koji Jimura"
        },
        {
            "affiliations": [],
            "name": "Daisuke Shimane"
        },
        {
            "affiliations": [],
            "name": "Kiyoshi Nakahara"
        },
        {
            "affiliations": [],
            "name": "Masaki Takeda"
        }
    ],
    "id": "SP:6668662bd254fa176e9d9a98d3801fb4dfd32f9b",
    "references": [
        {
            "authors": [
                "P.J. Allen",
                "O. Josephs",
                "R. Turner"
            ],
            "title": "A method for removing imaging artifact",
            "year": 2000
        },
        {
            "authors": [
                "M. Bar"
            ],
            "title": "Visual objects in context",
            "venue": "Nat Rev Neurosci",
            "year": 2004
        },
        {
            "authors": [
                "J.J. Barton"
            ],
            "title": "Disorder of higher visual function",
            "venue": "Curr Opin Neurol",
            "year": 2011
        },
        {
            "authors": [
                "R.M. Cichy",
                "A. Oliva"
            ],
            "title": "A M/EEG-fMRI Fusion Primer: Resolving Human",
            "year": 2020
        },
        {
            "authors": [
                "R.M. Cichy",
                "D. Pantazis",
                "A. Oliva"
            ],
            "title": "Resolving human object recognition",
            "year": 2014
        },
        {
            "authors": [
                "K. Dobs",
                "L. Isik",
                "D. Pantazis",
                "N. Kanwisher"
            ],
            "title": "How face perception",
            "year": 2019
        },
        {
            "authors": [
                "A. Eklund",
                "T.E. Nichols",
                "H. Knutsson"
            ],
            "title": "Cluster failure: Why fMRI",
            "year": 2016
        },
        {
            "authors": [
                "R. Epstein",
                "N. Kanwisher"
            ],
            "title": "A cortical representation of the local visual",
            "year": 1998
        },
        {
            "authors": [
                "M. Frey",
                "M. Nau",
                "C.F. Doeller"
            ],
            "title": "Magnetic resonance-based eye tracking",
            "year": 2021
        },
        {
            "authors": [
                "G. Gainotti",
                "C. Marra"
            ],
            "title": "Differential contribution of right and left temporo",
            "year": 2011
        },
        {
            "authors": [
                "K. Grill-Spector",
                "K.S. Weiner"
            ],
            "title": "The functional architecture of the ventral",
            "year": 2014
        },
        {
            "authors": [
                "H. Hong",
                "D.L. Yamins",
                "N.J. Majaj",
                "J.J. DiCarlo"
            ],
            "title": "Explicit information",
            "year": 2016
        },
        {
            "authors": [
                "T. Horikawa",
                "Y. Kamitani"
            ],
            "title": "Generic decoding of seen and imagined objects",
            "year": 2017
        },
        {
            "authors": [
                "C.P. 61 Hung",
                "G. Kreiman",
                "T. Poggio",
                "J.J. DiCarlo"
            ],
            "title": "Fast readout of object",
            "year": 2005
        },
        {
            "authors": [
                "N. Kanwisher",
                "J. McDermott",
                "M.M. Chun"
            ],
            "title": "The fusiform face area",
            "year": 1997
        },
        {
            "authors": [
                "D. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A Method for Stochastic Optimization",
            "year": 2015
        },
        {
            "authors": [
                "Z. Kourtzi",
                "N. Kanwisher"
            ],
            "title": "Cortical regions involved in perceiving object",
            "year": 2000
        },
        {
            "authors": [
                "G. Kreiman",
                "C. Koch",
                "I. Fried"
            ],
            "title": "Category-specific visual responses",
            "year": 2000
        },
        {
            "authors": [
                "P.A. Bandettini"
            ],
            "title": "Matching categorical object representations in inferior",
            "year": 2008
        },
        {
            "authors": [
                "S.M. Landi",
                "P. Viswanathan",
                "S. Serene",
                "W.A. Freiwald"
            ],
            "title": "A fast link",
            "year": 2021
        },
        {
            "authors": [
                "J. Liu",
                "A. Harris",
                "N. Kanwisher"
            ],
            "title": "Stages of processing in face perception",
            "year": 2002
        },
        {
            "authors": [
                "N.K. Logothetis",
                "D.L. Sheinberg"
            ],
            "title": "Visual object recognition",
            "venue": "Annu Rev",
            "year": 1996
        },
        {
            "authors": [
                "M.L. Mack",
                "T.J. Palmeri"
            ],
            "title": "The timing of visual object categorization",
            "venue": "Front",
            "year": 2011
        },
        {
            "authors": [
                "E. 110 Maris",
                "R. Oostenveld"
            ],
            "title": "Nonparametric statistical testing of EEG",
            "year": 2007
        },
        {
            "authors": [],
            "title": "Executive control by fronto-parietal activity explains counterintuitive",
            "year": 2022
        },
        {
            "authors": [
                "T. Suzuki",
                "Y. Kamitani",
                "I. Hasegawa"
            ],
            "title": "Heterogeneous Redistribution",
            "year": 2018
        },
        {
            "authors": [
                "Y. Miyashita",
                "H.S. Chang"
            ],
            "title": "Neuronal correlate of pictorial short-term",
            "year": 1988
        },
        {
            "authors": [
                "A. 123 Nestor",
                "D.C. Plaut",
                "M. Behrmann"
            ],
            "title": "Unraveling the distributed neural",
            "year": 2011
        },
        {
            "authors": [
                "M.V. Peelen",
                "P.E. Downing"
            ],
            "title": "Category selectivity in human visual cortex",
            "year": 2017
        },
        {
            "authors": [
                "G.A. 133 Rousselet",
                "M. Fabre-Thorpe",
                "S.J. Thorpe"
            ],
            "title": "Parallel processing in high",
            "year": 2002
        },
        {
            "authors": [
                "T. Serre",
                "A. Oliva",
                "T. Poggio"
            ],
            "title": "A feedforward architecture accounts",
            "year": 2007
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very Deep Convolutional Networks",
            "year": 2015
        },
        {
            "authors": [
                "A. Stigliani",
                "K.S. Weiner",
                "K. Grill-Spector"
            ],
            "title": "Temporal Processing Capacity",
            "year": 2015
        },
        {
            "authors": [
                "A. Supratak",
                "H. Dong",
                "C. Wu",
                "Y. Guo"
            ],
            "title": "DeepSleepNet: A Model for",
            "year": 2017
        },
        {
            "authors": [
                "W.A. Suzuki",
                "Y. Naya"
            ],
            "title": "The perirhinal cortex",
            "venue": "Annu Rev Neurosci",
            "year": 2014
        },
        {
            "authors": [
                "K. Tanaka"
            ],
            "title": "Inferotemporal cortex and object vision",
            "venue": "Annu Rev Neurosci 19,",
            "year": 1996
        },
        {
            "authors": [
                "H. Vu",
                "H.C. Kim",
                "M. Jung",
                "J.H. Lee"
            ],
            "title": "fMRI volume classification",
            "year": 2020
        },
        {
            "authors": [
                "Y. Zhu",
                "F Wu"
            ],
            "title": "Decoding and mapping task states of the human brain",
            "year": 2020
        },
        {
            "authors": [
                "K.J. Worsley",
                "K.J. Friston"
            ],
            "title": "Analysis of fMRI time-series revisited--again",
            "year": 1995
        },
        {
            "authors": [
                "D.L. Yamins",
                "J.J. DiCarlo"
            ],
            "title": "Using goal-driven deep learning models",
            "year": 2016
        },
        {
            "authors": [
                "F. Yu",
                "V. Koltun",
                "T. Funkhouser"
            ],
            "title": "Dilated Residual Networks",
            "venue": "CVPR 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Z. Zhang",
                "X. Yu",
                "X. Rong",
                "M. Iwata"
            ],
            "title": "Spatial-Temporal Neural Network",
            "year": 2021
        },
        {
            "authors": [
                "T. Liu"
            ],
            "title": "Automatic Recognition of fMRI-Derived Functional Networks",
            "year": 2018
        },
        {
            "authors": [
                "X. 208 Zheng",
                "C.J. Mondloch",
                "S.J. Segalowitz"
            ],
            "title": "The timing of individual face",
            "year": 2012
        }
    ],
    "sections": [
        {
            "text": "Introduction 42 Visual perception is a rapid cognitive process that allows immediate judgement of 43 safety/risk, enabling to quick planning and acting. The recognition and categorization of 44 a visual scene require a series of processing stages in the ventral visual processing 45 stream (Grill-Spector and Weiner, 2014; Grill-Spector et al., 2017; Hung et al., 2005; 46 Logothetis and Sheinberg, 1996; Peelen and Downing, 2017). The last stage of the 47 ventral stream, where perceptual and mnemonic signals meet, plays a crucial role in this 48 cognitive process by associating the perceived object with the neural representations of 49 memorized objects stored in the long-term memory (Landi et al., 2021; Suzuki and 50 Naya, 2014; Takeda et al., 2018). 51 The functional role of the ventral stream on visual categorization has been 52 investigated in neuropsychological studies. Lesions in the ventral stream can cause 53 various forms of agnosia depending on the location of the core lesion (Barton, 2011; 54 Cavina-Pratesi et al., 2010; Gainotti and Marra, 2011; Konen et al., 2011; Schiltz et al., 55 2006), suggesting module-like spatial organization for visual categories, such as faces 56 and objects. These observations are further supported by accumulating evidence from 57 functional magnetic resonance imaging (fMRI) in humans, such as the identification of 58 category-selective regions for faces (Haxby et al., 2001; Ishai et al., 1999; Kanwisher et 59 al., 1997), objects (Haxby et al., 2001; Ishai et al., 1999; Kourtzi and Kanwisher, 2000; 60 Malach et al., 1995), and places (Epstein and Kanwisher, 1998) in not only the ventral 61 stream, but also other cortical regions. Spatial organization in the categorization was 62 also demonstrated in non-human primates (Kriegeskorte et al., 2008; Tsao et al., 2003; 63 Tsao et al., 2006; Turesson et al., 2012). 64 In contrast, neural representations for categorical subdivision (sub-categorical 65 representation), especially in terms of the temporal aspects, remain unclear. At the finest 66 spatial scale, single neurons show selective responses to identities within a single 67 category in non-human primates (Miyashita and Chang, 1988; Tanaka, 1996). At a 68 larger spatial scale, a high-frequency local field potential carried spike-coupled 69 information, such as the species, view, and identity of the face (Miyakawa et al., 2018). 70 In human studies, extensive evidence has been obtained from magneto-71 /electroencephalography (MEG/EEG) (Carlson et al., 2013; Cichy et al., 2014; Dobs et 72 al., 2019; Liu et al., 2002; Zheng et al., 2012) (see also fMRI studies for sub-categorical 73 representation (Kriegeskorte et al., 2007; Nestor et al., 2011)). However, the temporal 74 trajectories of categorical and sub-categorical representation, that is, whether our brains 75 perform visual object categorization in a fine-to-coarse or coarse-to-fine manner, 76\nremains controversial (Carlson et al., 2013; Cichy et al., 2014; Dobs et al., 2019; Liu et 77 al., 2002; Mack and Palmeri, 2011). 78 Unraveling the complete picture of the spatiotemporal profile of categorical 79 and sub-categorical representations of visual objects is indispensable for understanding 80 the computational model of visual object processing in the brain. To address this issue, 81 we introduced two technical implementations in this study. First, we employed 82 concurrent fMRI and EEG recordings in humans to obtain neural responses to visual 83 stimuli at high spatiotemporal resolution. Second, we utilized a deep neural network 84 (DNN) to resolve the spatiotemporal patterns of the neural activity responsible for the 85 categorical/sub-categorical classification of visual objects. We hypothesized that (1) 86 fMRI data involve spatially resolved patterns of neural activity that contribute to visual 87 object classification between categories (faces or objects) and (2) EEG data involve 88 temporally resolved patterns of neural activity that contribute to visual object 89 classification not only between categories, but also within categories (sub-90 categorization; male faces or female faces, and natural objects or artificial objects), both 91 of which would be processed at different periods (Figure 1A). These hypotheses were 92 tested using newly developed DNN classifiers for multimodal neural data recorded by 93 concurrent fMRI and EEG (Figure 2) as well as univariate analysis for the fMRI and 94 EEG data. 95\n0.5 s 9.5 s\n0.5 s 9.5 s\nB\nC\nFigure 1\nD E\nA Sub-category a1\nSub-category a2\nSub-category b1\nSub-category b2\nCategory A (e.g., Object)\nCategory B (e.g., Face)\nTimeR es\npo ns\ne\nNatural\nFemale\nMale\nArtificial\nFace Object 60\n100\nC or\nre ct\np er\nfo rm\nan ce\n(% )\nMa le Fem ale Nat ura l Arti ficia l\n60\n100\nFace Object 0\n2\nR ea\nct io\nn tim\ne (s\n)\nMa le Fem ale Nat ura l Arti ficia l\n0\n2\nFace Object\n2 identities x\n5 views\nMale face\n2 identities x\n5 views\nFemale face\n2 identities x\n5 views\nNatural object\n2 identities x\n5 views\nArtificial object\nInput (fMRI volume)\n1@79x95x79 8@37x45x37 16@17x21x17\n32@6x7x6\nFC ReLU Dropout\n8,064 128\nConv3D kernel(7x7x7) stride(2,2,2) BatchNorm ReLU\nConv3D kernel(5x5x5) stride(2,2,2) BatchNorm ReLU\nConv3D kernel(3x3x3) stride(3,3,3) BatchNorm ReLU\nDropout Flattenxy z Face / Object Male / Female Natural / Artificial\nB\nC\nFigure 2\nA\nTr ai\nn/ Va\nlid at\nio n\nda ta\nTe st da ta\nNine-Fold cross validation for hyperparameter tuning\nModel 1 Model 2 Model 9\nGeneralization performance in independent data Time (s)0 1 2 3 4 5 6 7 8 9 10\nSt im\nul us\nEE G\nfM R\nI Temporal duration of fMRI/EEG data for DNNs\nD\n63@250\nInput (EEG time series)\nTemporal block 0\n(dilation = 1)\nTemporal block 1\n(dilation = 2)\nTemporal block 2\n(dilation = 4)\nTemporal block n\n(dilation = 2^n)\n63@250 63@25063@25063@250 63\nFace / Object Male / Female Natural / Artificial\nTime point 1 250\nC ha\nnn el 1 63\nE\n63@250 Residual connection\n63@250\nFC\nDilated Conv1D WeightNorm Chomp1D ReLU Dropout Dilated Conv1D WeightNorm Chomp1D ReLU Dropout\nTemporal block\nFC\n63@250\nTime point 1 250\nC ha\nnn el 1 63\nFigure 2. DNN models for multimodal neural data. (A) Temporal relationship across stimulus presentation and fMRI/EEG data windows used for DNN. The fMRI data were taken within 4\u20136 s from stimulus onset to consider the delayed hemodynamic response of the BOLD signal. The EEG data were taken during the period from stimulus onset to 0.5 s after stimulus offset. (B) Schematics of the training/validation/test architecture. In the training/validation dataset (N = 45 participants, one-fold data include data from N = 5 participants), we applied nine-fold cross-validation to tune the hyperparameters and constructed learned DNN models. Then, the generalization performances of these models for the independent test dataset were assessed (N = 5 participants). (C) DNN model for fMRI data. The model includes three convolution layers and two fully connected layers. (D) DNN model for EEG data. The model includes four temporal blocks and one fully connected layer. (E) Temporal block of the EEG DNN. This block includes two dilated convolution layers with a residual connection.\nResults 96 Behavioral results 97 Participants performed a visual object classification task in which they judged whether 98 the presented visual stimuli belonged to the male face, female face, natural object, or 99 artificial object image category (Figure 1B\u2013C). To minimize the effects of low-level 100 visual features of stimuli on differential brain activations and resulting differences in 101 decoding performance, we controlled for the image contrast and luminance (Stigliani et 102 al., 2015) (Figure 1 \u2013 figure supplement 1; see Materials and Methods for details). 103 The participants almost perfectly categorized the presented visual stimuli 104 (Figure 1D; N = 50). The performance was not significantly different between the male 105 and female face trials or between the natural and artificial object trials (Wilcoxon signed 106 rank test, z < 0.4, P > 0.68). Although the reaction times to male faces were faster than 107 those to female faces (z = 3.50, P = 4.63\u00d710-4), there was no significant difference in 108 reaction times within the object sub-category (z = 0.71, P = 0.48) (Figure 1E). 109 110 Spatial organization of category-specific information revealed by fMRI activations 111 We explored brain regions associated with categorical processing based on standard 112 univariate general linear model (GLM) analysis of fMRI data (Figure 3, Figure 3 \u2013 113 figure supplement 1). Robust activations for faces compared with those for objects were 114 observed in various regions, including the temporal occipital fusiform cortex (TOF), 115 which includes the fusiform face area (FFA), and lateral occipital complex (LOC) in the 116 ventral temporal cortex (VTC), as well as the occipital pole, angular gyrus, frontal 117 medial cortex, amygdala, frontal orbital cortex, precuneous (PcC), and middle temporal 118 gyrus (Figure 3A). In contrast, the brain regions activated by objects included the TOF, 119 LOC, middle frontal gyrus, insula, superior frontal gyrus, thalamus, and paracingulate 120 gyrus. These results were consistent with those of previous studies (Haxby et al., 2001; 121 Ishai et al., 1999; Kanwisher et al., 1997; Kourtzi and Kanwisher, 2000; Malach et al., 122 1995). 123 Subsequently, we explored brain regions associated with sub-categorical 124 processing (Figure 3 \u2013 figure supplement 2, 3). The face gender contrast (male face vs. 125 female face) revealed weak activations; there were no male face-dominant clusters and 126 three female face-dominant clusters (Figure 3B). The contrast between natural and 127 artificial objects also showed weak activations; there were no natural object-dominant 128 clusters and two artificial object-dominant clusters (Figure 3C). These results suggest 129 that the differences in the sub-categorical processing of visual objects are hardly 130 detected at the spatiotemporal resolution of fMRI used in this study. 131\nFigure 3\nB CA Face vs Object Male face vs Female face Natural object vs Artificial object\nt = 3.27 t = 9.05\nFace dominant\nt = -3.27 t = -15.40\nObject dominant\nt = 3.27 t = -3.27 t = -5.34 Male face dominant Female face dominant t = 3.27 t = -3.27 t = -5.61 Natural object dominant Artificial object dominant\nD Decoding performance for fMRI test dataset\n1 3 5 7 9 Number of averaged trials\n40\n50\n65 Male face vs Female face\n1 3 5 7 9 Number of averaged trials\n40\n50\n65 Natural object vs Artificial object\n1 3 5 7 9 Number of averaged trials\n65\nAc cu\nra cy\n(% )\nFace vs Object\n* * * * *\nLa te\nra l\nM ed\nia l\nVentral\nE F\nRL\nR L\nFace\nObject\n* * * * * *\n* * * * * * *\nH LH\nRHLH\n* *\n*\n95 \u2020 \u2020 \u2020 \u2020 \u2020 *\n9 99 9 9 0 10 1 0 0 20 1 1\nt = 3.11\nt = -3.11\nt = 48.9\nt = -33.4\nGuided Grad-CAM RHLH\nRH LHLH RH\nFace dominant\nObject dominant\nLa te\nra l\nM ed\nia l\nVentral Dorsal Posterior Anterior\nRH LH LH RH\nL. LOC R. LOC R. TOF R. OcP R. AG L. AG FM C\nR. A myg\ndala R. F OCPcC L. M TG p R. M TG p\nL. T OF/\nLOC\nR. T OF/\nLOC L. MFGi PcG L. M FG s R. S FG\nL. T hala\nmu sR. Ins\nEach model Average\nLearned model Non-learned model\n0\n0.1\nG ui\nde d\nG ra\ndC\nAM v\nal ue\n0\n20 \u00d710^-3\n0\n2.5 \u00d710^-3\n0\n0.06\nG ui\nde d\nG ra\ndC\nAM v\nal ue\n0\n3 \u00d710^-3\n0\n10 \u00d710^-4\nz = -16 z = 4 z = 10\nEnsemble learning\nLearned model Non-learned model\nFigure 3. Spatial representation of categorization/sub-categorization for visual objects revealed by fMRI. (A\u2013C) Activation maps for the contrast between the face and object (A), male face and female face (B) and natural object and artificial object (C) trials. The color bars denote t-values. The significant brain regions were identified using a threshold of P < 0.05 corrected with a cluster-wise family-wise error rate based on a non-parametric permutation test. The identified clusters in the subcortical structures and the insula are shown in the horizontal sections. See also Figure 3 \u2013 figure supplement 1\u20133. (D) Generalization decoding performance for the test dataset. The ensemble accuracy (purple) as well as average accuracy \u00b1 S.D (thick line) across models are shown as a function of the number of averaged trials. The thin lines represent the accuracy in each model. *, binomial test for ensemble accuracy against chance level (50%), FDR corrected q < 0.05. \u2020, t-test for accuracy distribution against chance level, FDR corrected q < 0.05. The number of significant models (binomial test, q < 0.05) is shown above the abscissa. (E) Spatial organization of the guided Grad-CAM values for the contrast of category (between face and object trials). The threshold of significance was set at P < 0.05 corrected with cluster-wise FWE based on a non-parametric permutation test. (F) ROI analysis. Colored and grayed bars denote the guided Grad-CAM values in learned and non-learned models, respectively. For display purposes, the results are plotted in separate panels according to the magnitudes of the values. *, t-test against values obtained from the non-learned model, FDR corrected q < 0.001. Values in non-learned models are nearly zeros. Boxplots show median (black line), 25th/75th percentile (box), nonoutlier minimum/maximum (whisker), and outlier (dot, data points outside 1.5\u00d7 interquartile range). The cluster information is listed in Figure 3 \u2013 figure supplement 1. L, left; R, right; LH, left hemisphere; RH, right hemisphere; AG, angular gyrus; FMC, frontal medial cortex; FOC, frontal orbital cortex; Ins, insular cortex; LOC, lateral occipital cortex; MFG, middle frontal gyrus; MTG, middle temporal gyrus; OcP, occipital pole; PcC, precuneous cortex; PcG, paracingulate gyrus; SFG, superior frontal gyrus; TOF, temporal occipital fusiform cortex; i, inferior; s, superior; p, posterior.\n132 Deep neural decoding of visual object categorization by fMRI DNN 133 To decode information about the visual object category and identify the spatial 134 organization of the responsible brain regions, we used a DNN classifier for the fMRI 135 data, which was based on three-dimensional convolution layers (Figure 2C). We trained 136 the fMRI DNN using an fMRI volume acquired 4 s after the presentation of visual 137 stimuli in each trial (Figure 2A). Training and validation of the DNN were conducted by 138 nine-fold cross-validation, and the generalization performance of the resultant nine 139 models was evaluated using an independent test dataset (Figure 2B; see Materials and 140 Methods for details). 141 The ensemble learning method using a majority voting scheme (Rokach, 2009) 142 revealed that the classification performance in categorization reached statistical 143 significance when the trials were not averaged (72.90%; q = 2.61\u00d710-72, binomial test 144 against chance level 50%, corrected for multiple comparisons by the false discovery rate 145 (FDR) of q < 0.05) (Figure 3D). The performance improved as a function of the number 146 of averaged trials, reaching 85.83% with an average of nine trials (q = 3.89\u00d710-185). In 147 contrast, neither face nor object sub-categorical classification was significant in the 148 averaged trial condition (q > 0.21), except for object sub-categorization when nine trials 149 were averaged. 150 Two additional analyses were performed to examine the robustness of these 151 results. In the first analysis, a t-test showed that for categorical classification, the 152 performance distribution statistically reached above chance when trials were not 153 averaged [68.80 \u00b1 0.39% (mean \u00b1 s.e.m.); t = 48.80, P = 3.44\u00d710-11, corrected by FDR 154 q < 0.05]. The performance improved as a function of the number of averaged trials 155 (linear regression, R2 = 0.82, slope coefficient, t = 13.99, P = 1.32\u00d710-17), reaching 156 84.96 \u00b1 0.71% with an average of nine trials (t = 49.04, P = 3.31\u00d710-11). The highest 157 accuracy reached was 87.60% in the best performing DNN model. In contrast, neither 158 face nor object sub-categorical classification reached statistical significance in either 159 averaged set of trial conditions (q > 0.26). 160 In the second analysis, we counted the number of models showing significant 161 performance against chance level using a binomial test (threshold at P = 0.05, FDR 162 corrected). Although all nine models were significant in categorical classification across 163 all averaged trial conditions, less than two models had significant sub-categorical 164 classification. The results of the two additional analyses support those of the ensemble 165 learning method. 166\nWith respect to important spatial neural activity for visual object classification, 167 guided gradient-weighted class activation mapping (Selvaraju et al., 2019) (guided 168 Grad-CAM) showed that the fMRI DNN model valued brain-wide regions, including 169 the frontal, parietal, temporal, and occipital cortices as well as subcortical structures in 170 both the face and object trials (Figure 3E; Figure 3 \u2013 figure supplement 4). The whole-171 brain activations identified in the univariate analysis and guided Grad-CAM were 172 significantly correlated at the voxel-by-voxel level, although most voxels showed 173 guided Grad-CAM values near zero, leading to a relatively low effect size (Figure 3 \u2013 174 figure supplement 5A; Spearman\u2019s rank correlation, \u03c1 = 0.048, t = 19.73, P < 0.001 for 175 face trial; \u03c1 = -0.21, t = -90.13, P < 0.001 for object trial). The correlation coefficient 176 increased when the target voxels were masked by the meta-analysis maps obtained from 177 Neurosynth (Yarkoni et al., 2011) using the terms \u201cface\u201d and \u201cobject\u201d (Figure 3 \u2013 figure 178 supplement 5B; \u03c1 = 0.27, t = 46.27, P < 0.001 for face trial; \u03c1 = -0.30, t = -56.21, P < 179 0.001 for object trial). This finding suggests that voxels with nearly zero values in the 180 guided Grad-CAM tended to be located outside significant clusters in the univariate 181 analysis. Indeed, the region-of-interest (ROI) analysis revealed that most of the clusters 182 identified in the univariate analysis had significant guided Grad-CAM values in both the 183 face and object trials compared with those obtained in non-learned DNN models in 184 which weight values were assigned randomly (Figure 3F; Wilcoxon signed rank test 185 against guided Grad-CAM values in non-learned models, FDR corrected q < 0.05). 186 Similar results were obtained when ROIs were defined based on the anatomical 187 structure or functional meta-analysis by Neurosynth (Figure 3 \u2013 figure supplement 6); 188 among the three ROI-based analyses, consistent results were obtained in the VTC 189 (LOC, TOF), FMC, and PcC in the face trial, and the VTC (LOC), and thalamus in the 190 object trial. 191 These results indicate that, to categorize faces and objects, our DNN model 192 paid attention to the neural signals in the distributed brain regions identified in the 193 univariate analysis. 194 195 Temporal trajectory of categorical/sub-categorical information by EEG responses 196 To examine the temporal characteristics of categorical and sub-categorical processing of 197 visual objects, we analyzed the event-related potential (ERP) and power spectrogram of 198 the EEG signals (Figure 4; Figure 4 \u2013 figure supplement 1). Regarding the ERPs for 199 categorization, the face stimuli induced an early negative component (< 200 ms from 200 stimulus onset) with a prolonged component (approximately 200\u2013700 ms), wherein the 201 polarity of the potential and its dynamics differed across channels. In contrast, the 202\nFigure 4\nB C\nD\nA\nE F\nG\nDifference (Face minus Object) Difference (Male minus Female) Difference (Natural minus Artificial)\nDecoding performance for EEG test dataset\n1 3 5 7 9 Number of averaged trials\n55\n85\nAc cu\nra cy\n(% )\nFace vs Object\n* * * * *\n1 3 5 7 9 Number of averaged trials\n50\n65 Male face vs Female face\n* * **\n1 3 5 7 9 Number of averaged trials\n50\n65 Natural object vs Artificial object\n* ** *\nTime from stimulus onset (ms) 0 1,000 0 1,000 Time from stimulus onset (ms) 0 1,000 0 1,000 Time from stimulus onset (ms) 0 1,000 0 1,000\nEach model Average Ensemble learning \u2020 \u2020 \u2020 \u2020 \u2020 \u2020 \u2020 \u2020 \u2020 \u2020 \u2020 \u2020 \u2020\n2 96 9 91 92 5 69 99 9 9\n\u2020 \u2020\n*\n1\n-1\n0\n1-1 0\nAP R\nL\n1\n0\n-2 10\n0\n-10\n5\n0\n-5 2\n0\n-2\n0.8\n0\n-1 10\n0\n-10\n5\n0\n-5 2\n0\n-2\n2\n0\n-1 10\n0\n-10\n5\n0\n-5 2\n0\n-2\nERP (\u03bcV) ERP (\u03bcV) ERP (\u03bcV)\nAlpha power (a.u.) Alpha power (a.u.) Alpha power (a.u.)\nBeta power (a.u.) Beta power (a.u.) Beta power (a.u.)\nGamma power (a.u.) Gamma power (a.u.) Gamma power (a.u.)\n0 1,000-0.5\n0\n2.5\n0 1,000-1\n0\n3\nG ui\nde d\nG ra\ndC\nAM (n\nor m\nal iz\ned )\nTime from stimulus onset (ms)\n0 1,000\nFace / Object\nMale / Female\nNatural / Artificial\nTime from stimulus onset (ms)\nG ui\nde d\nG ra\ndC\nAM\nFace Object Male Female Natural Artificial 0\n0.02\n0\n0.4\n0\n2\nFace Object\nMale Female Natural Artificial\n* * * * * Learned model Non-learned model\n* *\nFigure 4. Temporal representation of categorization/sub-categorization for visual objects revealed by EEG. (A) Differential ERP and power spectrogram in the contrast of face minus object trials. The line colors correspond to the channel locations depicted on the left side. A, anterior, P, posterior, R, right, L, left. (B) Differential ERP and power spectrogram in the contrast of male face minus female face trials. (C) Differential ERP and power spectrogram in the contrast of natural object minus artificial object trials. (D) Generalization decoding performance for the test dataset. The configurations are the same as those in Figure 3d. (E) Temporal trajectories of the guided Grad-CAM values for each trial type. The data from all channels were averaged, and values were normalized to the maximum guided Grad-CAM value (=1). The dashed lines represent the threshold (20% of the maximum value). The arrows depict the maximum latency that first exceeded the threshold. (F) Temporal distribution of maximum latency for each trial (dot). The trials for the face, object, male face, female face, natural object, and artificial object are separately colored. *, Tukey\u2013Kramer test, P < 0.05 after Kruskal\u2013Wallis test, \u03c72 = 9.70, P = 0.0078. (G) Comparison of the guided Grad-CAM values at the maximum latency between the learned and non-learned models. *, t-test against values obtained from the non-learned model, FDR corrected q < 0.001. Note that the values in non-learned models are nearly zero. The boxplots show the median (black line), 25th/75th percentile (box), nonoutlier minimum/maximum (whisker), and outlier (dot, data points outside 1.5\u00d7 interquartile range).\nobject stimuli did not evoke a clear early negative component (Figure 4 \u2013 figure 203 supplement 1A). This characteristic led to differential ERPs between the face and object 204 trials (Figure 4A). The ERPs between the faces and objects were significantly different 205 at approximately 200\u2013600 ms from stimulus onset in the brain-wide (50/63) channels 206 (Figure 4 \u2013 figure supplement 1B; cluster-defining threshold of P < 0.05, corrected 207 significance level of P < 0.05). In contrast, the power spectrogram did not show clear 208 differences (Figure 4A; Figure 4 \u2013 figure supplement 1B). 209 In contrast to categorization, the spectral power was more informative in sub-210 categorization. Specifically, in face sub-category processing, the male face stimuli 211 elicited greater beta power than the female face stimuli in a prolonged time window (\u2013212 1,000 ms). However, these two stimuli did not evoke distinct ERP responses (Figure 4B 213 and Figure 4 \u2013 figure supplement 1C). In object sub-category processing, artificial 214 stimuli evoked larger beta power than natural stimuli in a prolonged time window (\u2013215 1,000 ms) in several channels at the posterior part of the brain. Distinctive ERP 216 responses were also observed at 200\u2013300 ms (21/63 channels; Figure 4D and Figure 4 \u2013 217 figure supplement 1D). 218 219 Deep neural decoding of visual object categorization/sub-categorization by EEG 220 DNN 221 To decode information about the visual object category and sub-category as well as to 222 identify its temporal dynamics, we used a DNN classifier for the EEG data, which was 223 based on a temporal convolutional network (TCN) (Bai et al., 2018) (Figure 2D\u2013E). We 224 trained the EEG DNN using a 63-ch EEG time series during 1 s after stimulus onset 225 (Figure 2A). The ensemble learning method revealed that the classification performance 226 in categorization was statistically above the chance when the trials were not averaged 227 (59.33%; binomial test against chance level 50%, q = 5.75\u00d710-13, corrected for multiple 228 comparisons by FDR q < 0.05) (Figure 4D). The performance improved as a function of 229 the number of averaged trials, reaching 75.41% with an average of nine trials (q = 230 4.95\u00d710-89). Regarding face and object sub-categorization, the classification 231 performance improved from 53.50% (no trial average; q = 0.056) to 60.75% (average of 232 nine trials; q = 1.25\u00d710-8) and from 53.48% (no trial average; q = 0.067) to 60.72% 233 (average of nine trials; q = 4.99\u00d710-8), respectively. These results suggest that the EEG 234 DNN succeeded in decoding the sub-categorization of visual objects whereas the fMRI 235 DNN did not. 236 The performance distribution and number of significant models were further 237 analyzed to confirm the robustness of the results. First, the performance distributions of 238\nnine models for categorization statistically reached above chance when the trials were 239 not averaged (57.07 \u00b1 0.15%; t-test against chance level 50%, t = 47.33, P = 4.39\u00d710-11, 240 FDR corrected q < 0.05) (Figure 4D). The performance improved as a function of the 241 number of averaged trials (linear regression, R2 = 0.87, slope coefficient, t = 16.90, P = 242 1.34\u00d710-20), resulting in 73.41 \u00b1 0.74% with an average of nine trials (t = 31.73, P = 243 1.06\u00d710-9). The decoding performance for both face and object sub-categorization also 244 improved as the number of averaged trials increased (face sub-categorization: R2 = 0.44, 245 slope coefficient, t = 5.78, P = 7.51\u00d710-7; object sub-categorization: R2 = 0.59, slope 246 coefficient, t = 8.00, P = 4.78\u00d710-10). Regarding male face/female face classification, 247 the mean accuracy under the condition in which the trials were not averaged was 248 significantly above chance (51.90 \u00b1 0.60%; t = 3.19, P = 0.0129, FDR corrected). When 249 nine trials were averaged, the accuracy reached 57.51 \u00b1 0.64% (t = 11.76, P = 2.49\u00d710-250 6). Notably, the highest accuracy was 59.46% in the best performing model. Regarding 251 natural object/artificial object classification, the accuracy improved from 52.91 \u00b1 0.60% 252 (no trial average; t = 4.86, P = 0.0013, FDR corrected) to 58.71 \u00b1 0.62% (average of 253 nine trials, t = 14.08, P = 6.30\u00d710-7). The highest accuracy was 61.56% in the best 254 performing model. 255 Second, while all nine models were significant in categorical classification 256 across all averaged trial conditions, the number of models with significant sub-257 categorical classification increased as a function of the number of averaged trials 258 (binomial test, threshold at P = 0.05, FDR corrected). In face sub-categorization, the 259 number of significant models increased from one to nine, whereas in object sub-260 categorization, the number increased from two to nine. 261 Visualization by guided Grad-CAM revealed that our DNN model valued 262 differential periods between categorization and sub-categorization (Figure 4E). The 263 model started to value the timepoints at 120 ms and 336 ms in the face and object trials, 264 respectively (\u201cemergence latency\u201d; see Materials and Methods for details). Regarding 265 sub-categorization, the four emergence latencies were 788 ms for male face trials, 216 266 ms for female face trials, 288 ms for natural object trials, and 328 ms for artificial object 267 trials, all of which were slower than that for face trials. Meanwhile, emergence latencies 268 for female face and natural object trials were longer than those for object trials. This 269 \u201ccategorization-then-subcategorization\u201d tendency in the latency of first valued 270 timepoints was also observed in the latency with the maximum guided Grad-CAM 271 value (\u201cmaximum latency\u201d) (Figure 4F). The maximum latency varied among the trials, 272 and their ranges differed between categorization and sub-categorization: the maximum 273 latency in categorization varied between 104 ms and 912 ms (median = 498 ms), 274\nwhereas those in face and object sub-categorization varied between 236 ms and 960 ms 275 (median = 556 ms) and between 228 ms and 932 ms (median = 541 ms), respectively. 276 The maximum latency in categorization was significantly shorter than those in face and 277 object sub-categorization (Kruskal\u2013Wallis test, \u03c72 = 9.70, P = 0.0078; post-hoc Tukey\u2013278 Kramer test, P < 0.05). Notably, the maximum guided Grad-CAM values in each trial 279 type were significantly greater than those in the non-learned models (Figure 4G; 280 Wilcoxon signed rank test against guided Grad-CAM values in non-learned models, 281 FDR corrected q < 0.05). When grouping the guided Grad-CAM values into six brain 282 regions based on electrode channel locations, all regions showed significant guided 283 Grad-CAM values (Figure 4 \u2013 figure supplement 2). Notably, the highest guided Grad-284 CAM value in categorization was for the occipital cortex, whereas the frontal and 285 parietal cortices were more valued in sub-categorization (Friedman\u2019s test with post-hoc 286 Tukey-Kramer method, P < 0.05). 287 Collectively, these results indicate that our EEG DNN model successfully 288 captured not only the categorical representation, but also the sub-categorical 289 representation of visual objects in the temporal domain of neural responses. Our results 290 experimentally support that visual object categorization be performed in a coarse-to-fine 291 manner along the temporal axis. 292 293 Concurrent neural decoding by combined DNN model 294 Simultaneous acquisition of fMRI and EEG data enables concurrent neural decoding 295 using an integrated DNN. Thus, we performed neural decoding for concurrent fMRI and 296 EEG data with the combined fMRI\u2013EEG DNN (Figure 5A). In the DNN, feature 297 extraction layers, three-dimensional (3D) convolution layers for fMRI data and 298 temporal blocks for EEG data were finally concatenated and fully-connected. 299 When the fMRI and EEG activities of nine trials were averaged, the ensemble 300 performance of the fMRI\u2013EEG DNN was 90.05%, 60.70%, and 63.64% for 301 categorization, face sub-categorization, and object sub-categorization, respectively, all 302 of which were significantly above the chance level (Figure 5B; binomial test, P = 303 3.03\u00d710-240, P = 2.51\u00d710-9, and P = 2.44\u00d710-13). The performance distributions in the 304 nine models were also significant against the chance level (categorical, 87.49 \u00b1 1.10%, t 305 = 78.97, P = 7.37\u00d710-13; face sub-categorical, 58.40 \u00b1 1.43%, t = 40.84, P = 1.42\u00d710-10; 306 object sub-categorical, 59.97 \u00b1 2.36%, t = 25.44, P = 6.11\u00d710-9; all q < 0.05 in FDR 307 correction). The best performing models showed performances of 90.46%, 63.81%, and 308 70.04%. A binomial test revealed that nine, seven, and eight models achieved 309\nInput (fMRI volume)\nFC ReLU Dropout\n128\nx y\nz\nB\nFigure 5\nA\nInput (EEG time series)\nFace / Object Male / Female Natural / Artificial Concat\nfMRI 3D convolutions\nEEG temporal blocks\n191\n* * *\n63@250\nTime point 1 250\nC ha\nnn el 1 63 63\nFC ReLU\n256\nFC ReLU\n256\nFC ReLU\n256\nFC ReLU\n256\nDropout FC\n45 50\n100\nAc cu\nra cy\n(% )\nvs fM\nRI vs EE G vs fM RI vs EE G vs fM RI vs EE G\n1\n3\n5\n7\n9\nN um\nbe r o\nf a ve\nra ge\nd tri\nal s\n0\n16\n-16 Face\nvs Object\nMale vs\nFemale\nNatural vs Artificial Face\nvs Object\nMale vs\nFemale\nNatural vs Artificial\n\u0394 Accuracy (% )\nCAverage of nine trials\n* * ** * * * * ** * * * ** * * **\n* \u2022 ** Each model Average Ensemble learning\n\u2020 \u2020 \u2020\n89 7\n1@79x95x79\nFigure 5. Combined fMRI\u2013EEG DNN model. (A) Architecture of the model. Firstly, the fMRI volume data and EEG time series data are separately convolved by the conv3Ds and temporal blocks shown in Figure 2. Next, the resulting outputs are concatenated, are then processed using four fully connected layers. (B) Generalization decoding performance for the test dataset (average of nine trials). The bars denote the mean accuracy across models, and the circles denote the accuracy of each model. *, binomial test for ensemble accuracy against chance level (50%), FDR corrected q < 0.05. \u2020, ttest for accuracy distribution against chance level, FDR corrected q < 0.05. The number of significant models (binomial test, q < 0.05) is shown above the abscissa. (C) Improvement in decoding accuracy of fMRI\u2013EEG DNN against the accuracies of the fMRI and EEG DNN models. The differences in accuracy are color-coded. Note that neither the fMRI DNN nor EEG DNN shows higher accuracy than the fMRI\u2013EEG DNN. *, t-test, FDR corrected q < 0.05. , P < 0.05, uncorrected.\nsignificant decoding performance for categorization, face sub-categorization, and object 310 sub-categorization, respectively (threshold at P = 0.05, FDR corrected). 311 In the comparison of the classification accuracy of the combined fMRI\u2013EEG 312 DNN with the accuracies with fMRI data alone (Figure 3) or EEG data alone (Figure 4), 313 the fMRI\u2013EEG DNN accuracy was higher than those on the fMRI DNN and EEG DNN 314 in all averaged trial conditions (Figure 5C). The categorical classification accuracy of 315 the fMRI\u2013EEG DNN was significantly increased in all averaged trial conditions (t-test, 316 FDR corrected q < 0.05). Regarding sub-categorical classification, the fMRI\u2013EEG DNN 317 showed significantly higher performance compared to the fMRI DNN in all averaged 318 trial conditions (q < 0.05). However, the difference in performance between the fMRI\u2013319 EEG DNN and EEG DNN did not reach statistical significance except for when three 320 trials were averaged. These results suggest that concurrent usage of spatial and temporal 321 neural data contributes to improving the neural decoding of visual objects. 322\nDiscussion 323 This study revealed that the categorical representation of visual objects is supported by 324 spatially organized brain-wide activations in a coarse-to-fine manner. To our 325 knowledge, the present study is the first to examine neural representation of visual 326 objects using multimodal DNNs. This approach enabled highly resolved brain-wide 327 spatiotemporal neural decoding. The present findings will contribute to the construction 328 of a novel paradigm for trial-based neural decoding of internal processes in patients 329 with impaired consciousness in clinical practice. 330 DNNs, which have achieved state-of-the-art results for the large-scale image 331 recognition (Simonyan and Zisserman, 2015), are now being applied to neural data, in 332 what is known as neural decoding (Hong et al., 2016; Horikawa and Kamitani, 2017; 333 Yamins and DiCarlo, 2016). In an fMRI DNN, we managed to use 3D voxel data as 334 inputs, whereas some previous studies used 2D slice data (Sarraf et al., 2017) or 2D 335 flattened cortical data as inputs (Tsumura et al., 2021b) (see also other studies in which 336 3D voxel data were used as input data (Frey et al., 2021; Vu et al., 2020; Wang et al., 337 2020; Zhao et al., 2018)). Such image-like input could be learned by DNNs that have 338 already been trained with a large-scale image set (e.g., ImageNet), such as VGG. The 339 re-training approaches such as fine-tuning and transfer learning help improve the 340 decoding performance. Given that trained models for spatio-temporal data, which are 341 suitable for application to concurrent fMRI and EEG data, are currently unavailable, we 342 could not employ such a strategy in this study. However, there are several clear 343 advantages of our approach, such as (1) inclusion of neural activities in the subcortical 344 structures, which are ignored in the 2D flattened map and (2) coverage of whole brain 345 structures, which is impossible in 2D slice data. Indeed, we could identify the 346 involvement of the amygdala and thalamus in the categorical classification of visual 347 objects. Accumulating open 3D-voxel data available for deep learning is a promising 348 means of improving 3D fMRI decoding using DNNs further. 349 In the EEG DNN, we employed a TCN (Bai et al., 2018) in which time-series 350 data were convoluted in a dilated manner. The TCN had excellent performance for non-351 neural data such as character-level and word-level language modeling. Successful 352 application of the TCN to EEG data in the present study could expand the possibility for 353 the application of TCNs to other biological data such as electromyography data. 354 We obtained high decoding performance for both categorical and sub-355 categorical classification (c.f., in the fMRI\u2013EEG DNN, the ensemble performance 356 reached 90.05%, 60.70%, and 63.64% for the categorical and two sub-categorical 357 classifications, and the best performing model yielded 90.46%, 63.81%, and 70.04%). It 358\nis difficult to directly compare the performance in this study with that in other studies 359 because the experimental paradigms (e.g., decoding technique, stimulus, and sample 360 size) were quite different. Nevertheless, the present finding that the fMRI\u2013EEG DNN 361 outperforms the fMRI DNN and the EEG DNN indicates that a larger number of modal 362 data would improve the precision of identification of the neural representation of visual 363 objects. 364 In machine learning including deep learning, the learning performance is 365 usually evaluated once for the test dataset (Raschka, 2020). In this study, we employed 366 the ensemble learning approach (Rokach, 2009) in which obtained classification labels 367 from nine models were voted and the majority label was treated as the predicted label 368 for the trial. We confirmed the robustness of this ensemble approach using two distinct 369 indices, the performance distribution and number of significant models. Considering the 370 inherent properties of neural data to be noisy and variable between trials, this 371 combinational approach is suitable for the application of deep learning to neural data. 372 For example, when comparing an fMRI volume or an EEG time series with pictures in 373 ImageNet, neural data have some background noise, c.f., it is hardly possible to detect 374 the internal process of the subject by glimpsing an fMRI volume or an EEG time series. 375 In addition, it is difficult to collect adequate amount of neural data that are comparable 376 to the number of pictures in ImageNet (>10,000,000). Noisiness and a smaller sample 377 size would result in higher variance in the decoding performance. The ensemble 378 learning approach would reduce the variance of classification errors, and in addition to a 379 point estimate of the classification performance, our approach can evaluate the 380 performance variance across models. 381 Brain regions in which the fMRI DNN valued for categorization were closely 382 matched with regions identified in the fMRI univariate analysis, including the ventral 383 temporal cortex (FFA for face stimuli and LOC for object stimuli), which is consistent 384 with previous findings (Haxby et al., 2001; Kanwisher et al., 1997; Kourtzi and 385 Kanwisher, 2000; Malach et al., 1995). Notably, we revealed the involvement of the 386 prefrontal cortex and amygdala, which have been also demonstrated in monkey 387 (Freedman et al., 2001) and human (Kreiman et al., 2000) electrophysiology. 388 Considering that the distribution of significant voxels is wider in the guided Grad-CAM 389 than in the univariate analysis, deep learning techniques would have greater sensitivity 390 to responsible brain regions for visual object categorization. 391 Fine and coarse information processing in visual object categorization remains 392 controversial (Carlson et al., 2013; Cichy et al., 2014; Dobs et al., 2019; Liu et al., 393 2002; Mack and Palmeri, 2011). Our deep learning-based approach provided robust 394\nevidence regarding this issue: the temporal trajectory of neural decoding by the EEG 395 DNN demonstrated that the neural activity firstly conveyed categorical information, 396 followed by sub-categorical information. These results are in agreement with the 397 theoretical model of \u201ccoarse-to-fine\u201d categorization (Bar, 2004; Rousselet et al., 2002; 398 Serre et al., 2007). One notable observation in this study is that peak time in the guided 399 Grad-CAM value (maximum latency) varied among trials, which could be caused by the 400 temporal variability of the behavioral states (e.g., attention) among trials. The EEG 401 DNN could detect such variability at any time point owing to the shift invariance in the 402 temporal domain by dilated convolutions. The trial-dependent latencies of the 403 category/sub-category information in the neural responses would be related to the 404 persistent time windows with significant object decoding reported in other studies 405 (Cichy et al., 2014; Dobs et al., 2019). 406 Compared with the situation in which fMRI and EEG recordings are separately 407 conducted, concurrent fMRI/EEG recording has several advantages. First, the variance 408 of behavioral outcomes such as the correct performance and reaction time could be 409 lower than those obtained in separate sessions or on different days. Second, concurrent 410 usage of fMRI and EEG data can result in better decoding performance than the usage 411 of fMRI alone or EEG alone, as shown in this study. It is promising to add other bio-412 signals to detect other aspects of information of visual objects such as valence or 413 arousal. 414 An advanced M/EEG\u2013fMRI fusion approach has recently attracted attention as 415 it links multivariate neural response patterns based on representational similarity (Cichy 416 and Oliva, 2020; Cichy et al., 2014), whereas M/EEG and fMRI data are acquired in 417 separate sessions. The combination of concurrent multimodal recording and 418 representational similarity-based fusion approach would be useful for more accurately 419 elucidating neural representations in the sensory system. 420\nMaterials and Methods 421 422 Participants 423 A total of 53 healthy right-handed subjects (34 males and 19 females; age, 18\u201325 years) 424 with normal or corrected-to-normal vision participated in the study. Of them, data from 425 three participants were excluded due to low behavioral performance: two participants 426 had low correct performance (< 80% at least for one sub-category) and one participant 427 had slow responses (mean response latency > 1.5 s). The participants received 1,000 428 yen per hour for each session. The target number of participants was determined based 429 on the number of trials required in previous deep learning research in neuroscience (e.g., 430 Supratak et al (Supratak et al., 2017)) before starting the collection of neuroimaging 431 data. This study was approved by the institutional review board of Kochi University of 432 Technology, Kami, Japan and complied with the Declaration of Helsinki. Written 433 informed consent was obtained from all participants. 434 435 Experimental design 436 Stimuli. The stimuli consisted of color images (size, 7.4\u00b0 \u00d7 5.8\u00b0) and a white fixation 437 cross (size, 0.3\u00b0 \u00d7 0.3\u00b0). Forty color images from face and inanimate object categories 438 were used as the experimental stimuli. The 20 face and 20 object images were selected 439 from ATR Facial Expression Database DB99 (http://www.atr-p.com/products/face-440 db.html) and Amsterdam Library of Object Images (ALOI; https://aloi.science.uva.nl/) 441 (Geusebroek et al., 2005), respectively. The facial images were obtained from four 442 identities: two male individuals (male face sub-category) and two female individuals 443 (female face sub-category), with neutral facial expressions and five different angles. 444 The object images were obtained from four identities: two natural objects (paprika and 445 broccoli; natural object sub-category) and two artificial objects (hat and whistle; 446 artificial object sub-category), with five different angles. 447 All images were contrast- and luminance-matched using in-house MATLAB 448 (MathWorks, Natick, MA) code (Keerativittayayut et al., 2018) adapted from the 449 SHINE toolbox (lumMatch function) (Willenbockel et al., 2010) for each RGB channel, 450 after figure-ground separation. Based on a prior study (Stigliani et al., 2015), we 451 evaluated (1) contrast, (2) luminance, (3) similarity to other stimuli of the same 452 category, (4) visual field coverage, and (5) spatial frequency power distributions, after 453 converting RGB color images into grayscale images by using the rgb2gray function in 454 MATLAB (Figure 1 \u2013 figure supplement 1). In addition, hue histograms of color images 455 were evaluated for each sub-category. 456\n457 Task procedure. During the recording of brain activities by concurrent fMRI and EEG, 458 the participants performed a visual object classification task, in which sequences of 459 experimental stimuli were presented. The participants judged the corresponding visual 460 sub-categories (male face, female face, natural object, and artificial object) and pressed 461 a corresponding button (Figure 1B\u2013C). At the beginning of each scanning run, a task 462 instruction was presented that indicated the correspondences among four positions of 463 buttons and the four sub-categories. To exclude the possibility that the learning of 464 stimulus-response mapping would affect the neural activity, the correspondence was 465 pseudorandomized across runs. In the scanning run, a fixation cross was firstly 466 presented for 30 s, followed by a sequence of 50 trials. In 40 trials, face or object 467 images were presented, and in 10 trials, blank images were presented to improve 468 dissociation of hemodynamic responses between trials. In each trial, a stimulus was 469 presented for 500 ms, followed by a fixation cross for 9,500 ms. Thus, each scanning 470 run took a total of 530 s. All stimulus images were presented once in each scanning run, 471 and the order of stimulus presentation was pseudorandomized. No visual feedback was 472 presented. Participants practiced the task before starting concurrent fMRI and EEG. 473 The participants conducted 9.96 \u00b1 2.88 runs (mean \u00b1 SD; total 496 runs) while 474 simultaneous fMRI and EEG recordings were performed. Neural data were accordingly 475 obtained from a total of 24,800 trials (9,920 face trials, 9,920 object trials, and 4,960 476 blank trials). Trials wherein the participants did not respond within 2 s (1,005 trials, 5% 477 of total face/object trials) were excluded. The correct performance and reaction time 478 were calculated for each category and sub-category (Figure 1D\u2013E). Only the data from 479 the correct trials were used for the succeeding analyses (18,007 face and object trials). 480 The task was programmed and administered using Psychtoolbox version 3 481 (http://psychtoolbox.org/). The visual stimuli were projected on a screen located behind 482 the scanner using PROPixx DLP LED projector (VPixx Technologies, Canada). The 483 participants viewed the projected visual stimuli through a mirror attached to a head coil. 484 485 fMRI procedure and preprocessing 486 MRI scanning was performed using a 3T MRI scanner (Siemens Prisma, Germany) with 487 a 64-channel head coil. Functional images were acquired using an echo-planar imaging 488 sequence [repetition time (TR): 2.0 s; echo time (TE): 27 ms; flip angle (FA): 90 deg; 489 36 slices; slice thickness: 3 mm; in-plane resolution: 2 \u00d7 2 mm]. Multiband accelerated 490 imaging was unavailable because the guideline for the simultaneous fMRI\u2013EEG 491 recoding by the developer of the MRI-compatible EEG system did not permit it (Brain 492\nProducts, Germany). Each functional run involved 265 volume acquisitions. The first 493 five volumes were discarded for analysis to take into account the equilibrium of 494 longitudinal magnetization. High-resolution anatomical images were acquired using an 495 MP-RAGE T1-weighted sequence (TR: 2,400 ms; TE = 2.32 ms; FA: 8 deg; 192 slices; 496 slice thickness: 0.9 mm; in-plane resolution: 0.93 \u00d7 0.93 mm). 497 All preprocessing was performed using SPM12 software 498 (http://fil.ion.ac.uk/spm/). Functional images were firstly temporally realigned across 499 volumes and runs, and the anatomical image was co-registered to a mean image of the 500 functional images. The functional images were then spatially normalized to a standard 501 Montreal Neurological Institute (MNI) template, with normalization parameters 502 estimated for the anatomical scans. The images were resampled into 2-mm isotropic 503 voxels. For the univariate fMRI analysis, functional volumes were spatially smoothed 504 with a 6-mm full-width at half-maximum Gaussian kernel. For the deep learning 505 analysis, we did not apply spatial smoothing to the fMRI data. 506 507 EEG procedure and preprocessing 508 High-density EEG data were acquired from MRI-compatible 64 sintered Ag/AgCl 509 electrodes with extended scalp coverage, including 63 scalp recording electrodes and an 510 electrode on the upper back for electrocardiogram (ECG) (BrainCap-MR, Brain 511 Products, Germany). Electrodes AFz and FCz served as the ground and online 512 reference, respectively. All signals were recorded with a bandpass of 0.016\u2013250 Hz and 513 digitized at 5,000 Hz (BrainAmp MR Plus, Brain Products). The electrode impedances 514 were lowered to <10 k\u03a9 before recording and monitored throughout the experiment. 515 Offline preprocessing was firstly performed in Brain Vision Analyzer 2 (Brain 516 Products). Initially, we removed the MR gradient artifact using an average template 517 subtraction method (Allen et al., 2000) and down-sampled the resulting EEG data to 518 250 Hz. Secondly, the EEG data were bandpass filtered between 1 Hz and 100 Hz. 519 Thirdly, the cardiovascular artifact was removed from the EEG data. Heartbeat 520 detection using the ECG channel was then performed using a semiautomatic template-521 matching procedure. Fourthly, eye blink correction was performed using independent 522 component analysis (ICA). As specific sensors for vertical/horizontal 523 electrooculography (EOG) were not used, we employed the EEG data obtained from 524 electrode FP1 as a substitute for the vertical activity of the EOG channel. Finally, we 525 applied ICA-based denoising techniques implemented in EEGLAB (runica function). 526 Then, each independent component was labeled using ICLabel plugin 527\n(https://github.com/sccn/ICLabel). Components with <75% probability of being in the 528 brain signal were flagged as artifacts and rejected. 529 The resulting artifact-removed EEG data were further processed in EEGLAB 530 as follows. If the signal was continuously noisy in a certain channel in a scanning run, 531 the EEG data were interpolated at the corresponding channel by replacing it with the 532 average EEG data from the neighboring channels. Accordingly, 16 of the 3,150 533 channels (50 participants \u00d7 63 scalp channels) were interpolated. Thereafter, the EEG 534 data were segmented into 1.5 s epochs (0.5 s pre-stimulus to 1.0 s post-stimulus) time 535 locked to the stimulus onset and were baseline-normalized. The data were then 536 inspected manually to detect any trials containing any excessive noises. After excluding 537 742 trials, 17,265 trials were included in the analyses. The data were then digitally re-538 referenced to the average of all scalp channels. 539 For the ERPs, we firstly trial-averaged the EEG time series for each EEG 540 channel separately. Then, the resulting ERPs were grand-averaged across participants to 541 obtain population ERPs for each EEG channel. The spectral power was also calculated 542 using the mtspecgramc function in the Chronux toolbox for MATLAB 543 (http://chronux.org/). The parameters for the multi-taper time-frequency spectrum were 544 as follows: window = 200 ms, step size = 80 ms, time-bandwidth product = 3, and 545 number of tapers = 5. 546 547 Respiration and eye-tracking recording 548 Respiratory data were recorded by using the BIOPAC MP160 system with 549 AcqKnowledge software (BIOPAC Systems, Inc., Goleta, CA). Raw physiological time 550 series of respiration were sampled at 2,000 Hz and then were preprocessed in the 551 PhysIO toolbox (Kasper et al., 2017) for MATLAB as follows. Firstly, the respiration 552 data were down-sampled to 250 Hz and bandpass-filtered between 0.1 Hz and 5.0 Hz. 553 Then, both the respiratory phase and respiratory volume per time (RVT) for the 554 respiratory data were obtained. From these physiological determinants with noise 555 modeling, (1) 8 RETROICOR (RETROspective Image CORrection) regressors were 556 computed via a fourth-order Fourier expansion of the respiratory phase (Harvey et al., 557 2008), and (2) respiratory response was calculated by convolution of the RVT with the 558 respiration response function (Birn et al., 2006). Accordingly, we obtained a total of 559 nine regressors and treated them as nuisance regressors in the fMRI univariate analysis 560 (see \u201cfMRI univariate analysis\u201d section below). 561 To check whether the participants fixated their eye positions, the eye position 562 during task runs was monitored online at a sampling rate of 1,000 Hz using an MRI-563\ncompatible infrared camera-based eye tracker (EyeLink 1000, SR Research, Ontario, 564 Canada). 565 566 DNN classifier 567 To explore spatiotemporal neural activity representing visual object category and sub-568 category, in-house DNN classifiers (Simonyan and Zisserman, 2015) were constructed 569 (Figure 2). The architecture of the models is described below. The model training, 570 validation, and testing framework for binary classification (face/object, male 571 face/female face, or natural object/artificial object images) were implemented using 572 PyTorch (https://pytorch.org/). The scripts employed for network modeling and 573 analyses are available from GitHub (https://github.com/masaki-takeda/dld). 574 The whole dataset included 17,265 trial-based concurrent fMRI\u2013EEG data 575 from 50 participants. The data from 45 participants were used as a training/validation 576 dataset for optimizing the hyperparameters, and independent data from five participants 577 were used as a hold-out test dataset. In the training/validation dataset, data from five 578 participants were treated as a unit of the fold, resulting in nine-fold datasets (Figure 2B). 579 After preliminarily determining network architectures, we conducted grid searches for 580 hyperparameter tuning by using the train/validation dataset (see \u201cGrid search\u201d section). 581 After completing hyperparameter tuning, we next evaluated the generalization of neural 582 decoding performance for the independent test dataset: nine-fold cross-validation was 583 performed for the training/validation dataset, and then the resulting nine learned models 584 were applied to the hold-out test dataset. For each binary classification for the test 585 dataset, we equalized the number of trials for face/object, male face/female face, and 586 natural object/artificial object by excluding a surplus of trials in the test dataset. This 587 approach resulted in a chance level of 50% (categorization: 734 face trials vs. 734 588 object trials; face sub-categorization: 386 male face trials vs. 386 female face trials; 589 object sub-categorization: 359 natural object trials vs. 359 artificial object trials). The 590 decoding performance for visual object categorization and sub-categorization was then 591 investigated using the following network models based on fMRI data alone (Figure 2C), 592 EEG data alone (Figure 2D\u2013E), and combined fMRI\u2013EEG data (Figure 5A). 593 594 fMRI DNN. The current DNN model for fMRI data (fMRI DNN) consisted of a 3D 595 convolutional neural network (CNN) (Frey et al., 2021) (Figure 2C). The 3D CNN 596 included three convolution layers (conv3d function) for extracting spatial features and 597 two fully connected layers. We also examined other networks with a greater number of 598 convolution and fully connected layers, but the decoding performance did not change 599\ndrastically. Based on the preliminary test for the training/validation dataset, the 600 following hyperparameters and settings were selected: batch size of 100, early stopping, 601 and Adam optimizer (Kingma and Ba, 2015). We then grid-searched for the number of 602 averaged trials, learning rate, and weight decay. 603 The input datasets were trial-based non-smoothed fMRI volumes and were 604 prepared as follows. To consider the delayed hemodynamic response of the BOLD 605 signal, an fMRI volume that was acquired at 4 s after the onset of the visual stimulus 606 presentation was collected (Figure 2A). The input fMRI volume was spatially 607 normalized to the standard MNI space, and the signal intensities outside of the brain 608 were masked to zero. Then, the signal intensity in each voxel was z-scored by the mean 609 and SD of the signal intensity in the volume. 610 611 EEG DNN. Seven DNN models for EEG data (EEG DNNs) were tested in the 612 preliminary inquiry for the training/validation dataset: (1) a custom DNN model 613 including four conv1d layers, (2) a custom DNN model including four conv2d layers for 614 band-passed EEG time series (delta, theta, alpha, beta, and gamma frequency band \u00d7 615 time), (3) a recurrent neural network (RNN) model using long short term memory 616 (LSTM), (4) a convolutional RNN (ConvRNN) model using LSTM after two conv1d 617 layers, (5) a spatial-temporal neural network originally designed for P300 detection 618 (Zhang et al., 2021), (6) a generic temporal convolutional network (TCN) (Bai et al., 619 2018), and (7) a modified version of the TCN. Among them, we adopted the TCN 620 model (6) because of its superior decoding performance and its simplicity (Figure 2D\u2013621 E). The TCN model has several distinguishing characteristics, such as dilated 622 convolutions (dilated conv1d) and the causal architecture (no information leakage from 623 future to past). Based on the preliminary test, a batch size of 100 and early stopping 624 were selected. We then grid-searched for the number of averaged trials, learning rate, 625 and kernel size. The dilation factors for each kernel size were determined to cover all 626 the input time series. 627 The input datasets were trial-based EEG time series data and were prepared as 628 follows. Preprocessed EEG time series data were collected from 63 scalp channels. The 629 segmented data obtained at 1 s from the stimulus onset (250 samples at a sampling rate 630 of 250 Hz) were used (Figure 2A). The resulting segmented data were further z-scored 631 by the mean and SD of the signal amplitude for each channel. 632 633 DNN for combined data. We also tested whether concurrent usage of both fMRI and 634 EEG data improves the decoding performance relative to the usage of fMRI or EEG 635\ndata alone. The DNN architecture for combined data (fMRI\u2013EEG DNN) is shown in 636 Figure 5A. In the model, the feature extraction layers of the two networks for fMRI and 637 EEG data were concatenated and then fully connected. The fMRI and EEG networks 638 were pre-trained separately, and the initial parameters of the convolution layers in the 639 fMRI\u2013EEG DNN were set to the pre-trained parameters. 640 641 Grid search. Before conducting the grid search, we preliminarily searched several 642 hyperparameters such as the number of layers and kernel sizes, and found that several 643 hyperparameters affected the decoding performance: the number of averaged trials, 644 weight decay, and learning rate for the fMRI data, and the number of averaged trials, 645 kernel size for delated convolutional layers, and learning rate for the EEG data. Thus, 646 grid searches were conducted to obtain the optimal combination of these 647 hyperparameters. For the fMRI DNN, the following hyperparameters were tuned: the 648 number of averaged trials (1, 3, 5, 7, or 9), learning rate (0.0001\u20130.01 in five divisions), 649 and weight decay (0\u20130.01 in five divisions). For the EEG DNN, the following 650 hyperparameters were tuned: the number of averaged trials (1, 3, 5, 7, or 9), learning 651 rate (0.0001\u20130.01 in 5 divisions), and kernel size (2\u20139 in five divisions). For the 652 combined fMRI\u2013EEG DNN, the following hyperparameters were tuned: the number of 653 averaged trials (1, 3, 5, 7, or 9), learning rate of feature extraction layers for fMRI data 654 (0.001 or 0.0001), weight decay for fMRI data (0 or 0.001), learning rate of feature 655 extraction layers for EEG data (0.0001 or 0.0005), and kernel size for EEG data (2, 5, 7, 656 or 9). For the combined fMRI\u2013EEG DNN, we fixed the learning rate and weight decay 657 for the fully connected layers as 0.001 and 0, respectively. All grid search analyses were 658 conducted using the training/validation dataset, and the hold-out test dataset was 659 completely free from the hyperparameter settings. 660 661 Data averaging. A single-trial neural signal contains noise derived from the brain and 662 measurement hardware, and one of the methods of increasing the signal to noise (S/N) 663 ratio is to average the data. To estimate how the S/N ratio of brain signals affects the 664 decoding performance, we tested the decoding using both data from a single trial data 665 and averaged data (number of averaged trials \u22649). We averaged the fMRI or EEG data 666 within each subject and then augmented the data to match the total average number of 667 data with the number of all trials (e.g., averaged data from nine trials were augmented 668 by a factor of nine). 669 670\nVisualization of relevant neural activity in DNNs. To highlight important spatiotemporal 671 neural activities for visual object classification, visualization approaches in deep 672 learning were applied. For both the fMRI DNN and EEG DNN, guided Grad-CAM 673 (Selvaraju et al., 2019) was used to create a high-resolution class-discriminative 674 visualization. Guided Grad-CAM was created by multiplying guided backpropagation at 675 the input of the first convolution layer with Grad-CAM at the last convolution layer. 676 The original Grad-CAM uses global average pooling of the gradients (Selvaraju et al., 677 2019), but we did not use it like Layer-CAM (Jiang et al., 2021), because the smoothing 678 effect of pooling was extremely strong, especially for the 3D convolution. In the end, 679 we obtained multiple guided Grad-CAM maps, the number of which corresponded to 680 the number of trials. The obtained guided Grad-CAM values were not normalized. As 681 for EEG guided Grad-CAM, the model without residual connections was used because 682 those connections can cause gridding artifacts with the dilated convolution layers and 683 obfuscate the visualization results (Yu et al., 2017). The resulting guided Grad-CAM 684 values in the EEG DNN were then smoothed using the Gaussian-weighted moving 685 average with a 40-ms window. 686 687 Statistics 688 General. All statistical analyses were performed in MATLAB and Python. All data are 689 shown as mean \u00b1 standard error of the mean unless otherwise stated. The sample sizes 690 were not based on a priori power calculations but are comparable to those employed in 691 other studies in the field using similar techniques (Supratak et al., 2017). 692 693 fMRI univariate analysis. The fMRI univariate analysis was conducted as in previous 694 studies (Matsui et al., 2022; Tanaka et al., 2020; Tsumura et al., 2021a; Tsumura et al., 695 2021b) (Figure 3A\u2013C). In the first-level analysis, a GLM (Worsley and Friston, 1995) 696 approach was used to estimate parameter values for task events. The events of interest 697 were the presentation of male face, female face, natural object, artificial object stimuli 698 (four regressors). Only correct trials were coded as regressors. Those task events were 699 time locked to the onset of stimulus presentation and then convolved with a canonical 700 hemodynamic response function implemented in SPM. Additionally, the onset of blank 701 stimuli, six-axis head movement parameters, white matter signal, CSF signal, and effect 702 of respiration (nine regressors) were included in GLM as nuisance effects, totaling to 18 703 nuisance regressors. Then, parameters were estimated for each voxel across the whole 704 brain. 705\nIn the group level analysis, maps of parameter estimates were first contrasted 706 within individual participants. The contrasted maps were then collected from 707 participants and subjected to group-mean tests. Voxel clusters were first identified using 708 a voxel-wise uncorrected threshold of P < 0.001. The voxel clusters were then tested for 709 significance across the whole brain with a threshold of P < 0.05 corrected by the family-710 wise error (FWE) rate based on permutation methods (5,000 permutations) implemented 711 in the randomise function in the FSL suite (http://fmrib.ox.ac.uk/fsl/), which was 712 empirically validated to appropriately control the false positive rate in a previous study 713 (Eklund et al., 2016). Peaks of significant clusters were then identified and listed in 714 tables. If multiple peaks were identified within 12 mm, the most significant peak was 715 retained. 716 717 EEG univariate analysis. For each participant, a single ERP and power spectrogram 718 were constructed by averaging the data across all trials separately for each visual 719 category or sub-category in each channel. We then averaged the ERPs/spectrograms 720 from all participants, which resulted in a grand average of 63-ch ERPs/spectrograms for 721 the face, object, male face, female face, natural object, and artificial object trials (Figure 722 4A\u2013C; Figure 4 \u2013 figure supplement 1A). The subsequently obtained spectrograms were 723 further bandpass-filtered to divide them into alpha (8\u201315 Hz), beta (15\u201330 Hz), and 724 gamma (30\u201370 Hz) frequency bands. 725 For statistical inference of differential EEG time series of ERP and power 726 spectrograms (e.g., face minus object trials) against zero, permutation-based cluster-size 727 inference (Dobs et al., 2019; Maris and Oostenveld, 2007) (i.e., a cluster refers to a set 728 of contiguous time points) was performed separately for each channel. Time points that 729 exceeded the 97.5th percentile of the permutation distribution (N = 5,000) served as 730 cluster-inducing time points (i.e., equivalent to P < 0.05, two-sided). The significance of 731 clusters was set at the threshold of P < 0.05 (Figure 4 \u2013 figure supplement 1B\u2013D). 732 733 Neural decoding performance. We trained our DNN models using nine-fold cross-734 validation with the training/validation dataset and then evaluated their generalization 735 performance with the test dataset. Thus, a total of nine generalization performances 736 were obtained for the fMRI DNN model, EEG DNN model, and combined fMRI\u2013EEG 737 DNN model. Three different statistical tests were performed to examine the significance 738 of decoding accuracies and their robustness (Figure 3D, 4D, and 5B). Firstly, an 739 ensemble learning method with majority voting (Rokach, 2009) was conducted. In this 740 scheme, each base model (N = 9) made a prediction and voted for each sample (a 741\nstimulus in a trial). Only the sample class with the highest votes was included in the 742 final predictive class. Statistical significance was tested using the binomial test against 743 chance level (50%). Secondly, the t-test was conducted to determine the distribution of 744 accuracies obtained by the nine models against the chance level, with FDR corrections 745 (q < 0.05). Thirdly, the binomial test was performed for each model against chance 746 level, with FDR corrections (q < 0.05), and the number of significant models was 747 counted. 748 749 Visualization of neural decoding. The model that showed the maximum accuracy in the 750 average of nine trials was selected for the subsequent visualization analysis. 751 The statistical significance of the guided Grad-CAM for fMRI data was 752 evaluated by conducting the permutation-based cluster-size inference (Dobs et al., 2019; 753 Maris and Oostenveld, 2007) used in the present univariate analyses (Figure 3E; see 754 \u201cfMRI univariate analysis\u201d section). The spatial distribution of the significant guided 755 Grad-CAM was also evaluated based on the Harvard-Oxford cortical and subcortical 756 structural atlases, which covered 48 cortical and 21 subcortical structural areas, 757 implemented in FSL (Figure 3 \u2013 figure supplement 4; HarvardOxford-cort-maxprob-758 thr50-2mm and HarvardOxford-sub-maxprob-thr50-2mm). We divided the cortical 759 areas into areas in the right and left hemispheres and obtained 95 cortical regions (note 760 that there were no voxels for the left supracalcarine cortex). Regarding the subcortical 761 areas, we bilaterally removed areas labeled as the cerebral white matter, lateral ventricle 762 and cerebral cortex and obtained 15 subcortical regions. We examined whether the 763 guided Grad-CAM values in each brain regions (N = 110) were significantly higher than 764 those obtained when DNNs were not learned. In the non-learned model, the weight 765 values were initialized and fixed as random values. Statistical significance was 766 evaluated by performing the Wilcoxon signed rank test with FDR corrections (q < 0.05). 767 We also conducted two types of correlation analysis (Spearman\u2019s rank 768 correlation; Figure 3 \u2013 figure supplement 5) and three types of ROI analysis to examine 769 the spatial overlap between activations in the univariate analysis and guided Grad-CAM 770 (Figure 3F, Figure 3 \u2013 figure supplement 6). The correlation was calculated either for 771 the voxels masked by the gray matter prior volume of the FSL (avg152T1_gray 772 template, threshold = 0.5) or for the voxels masked by the maps of meta-analysis 773 obtained from Neurosynth (Yarkoni et al., 2011) (https://neurosynth.org) using the 774 terms \u201cface\u201d and \u201cobject\u201d (uniformity test). In the ROI analysis, we examined whether 775 the guided Grad-CAM values in each ROI were significantly higher than those obtained 776 using the non-learned DNNs. The ROIs were selected based on (1) the univariate 777\nanalysis (Figure 3 \u2013 figure supplement 1; Figure 3F), (2) the anatomical structures from 778 the Harvard\u2013Oxford atlas (Figure 3 \u2013 figure supplement 6A), or (3) the maps of meta-779 analysis obtained from Neurosynth (Figure 3 \u2013 figure supplement 6B). In the Harvard-780 Oxford atlas (2), we created ROIs that included the peak voxels of the clusters identified 781 by the univariate analysis, by thresholding the probabilistic map of the region by 50%. 782 Note that we combined MFGi and MFGs for object trials identified in the univariate 783 analysis into one ROI as MFG. In the Neurosynth (3), ROIs were selected with the 784 uniformity test using the terms \u201cface\u201d and \u201cobject.\u201d ROIs with more than 50 voxels 785 were used in the analysis. In all ROI analyses, statistical significance was evaluated 786 using Wilcoxon signed rank test with FDR corrections (q < 0.05). 787 The guided Grad-CAM for EEG data were quantified according to two indices, 788 emergence latency and maximum latency. Firstly, the guided Grad-CAM values for 789 each trial were averaged across 63 channels. Then, for each trial type, we defined the 790 emergence latency as the time point at which the value first exceeded the level at 20% 791 of the maximum of channel-averaged guided Grad-CAM values (Figure 4E). We 792 obtained similar results when the threshold was changed to 10%, 15%, 25%, and 30%. 793 The maximum latency for each trial was also defined as the time point at which the 794 guided Grad-CAM showed maximum value in each trial (Figure 4F), and statistical 795 significance in the maximum latency across categorization/sub-categorization was 796 tested using the Kruskal\u2013Wallis test with the post-hoc Tukey-Kramer test (P < 0.05). 797 We also calculated the maximal latencies and corresponding guided Grad-CAM values 798 when the DNN was not learned. Then, we examined whether the guided Grad-CAM 799 values at the maximum latency were significantly higher in the learned model than in 800 the non-learned model using the Wilcoxon signed rank test with FDR corrections (q < 801 0.05; Figure 4G). Finally, we tested whether the guided Grad-CAM values at the 802 maximum latency differed across brain regions. EEG channels were grouped into six 803 brain regions (frontal cortex, central cortex, parietal cortex, occipital cortex, left 804 temporal cortex, and right temporal cortex; Figure 4 \u2013 figure supplement 2). The 805 electrodes for the six regions are as follows: Fp1, Fp2, Fz, F1, F2, AF3, AF4, AF7, 806 AF8, and Fpz for the frontal region; C3, C4, Cz, FC1, FC2, C1, C2, FC3, and FC4 for 807 the central region; P3, P4, Pz, CP1, CP2, P1, P2, CP3, CP4, P5, P6, and CPz for the 808 parietal region; F3, F7, T7, P7, FC5, CP5, TP9, F5, C5, FT7, TP7, and FT9 for the left 809 temporal region; and F4, F8, T8, P8, FC6, CP6, TP10, F6, C6, FT8, TP8, and FT10 for 810 the right temporal region. The values obtained by each channel in a region were 811 averaged, and regional differences were tested using Friedman\u2019s test with the post-hoc 812 Tukey\u2013Kramer method (P < 0.05). 813\n814 Data and code availability 815 All data supporting the findings of this study are provided within the paper. All custom 816 Python codes used for deep learning and learned model data in this study are available 817 at https://github.com/masaki-takeda/dld. All additional information will be made 818 available upon reasonable request to the authors. 819\nAcknowledgments 820 This study was supported by Kakenhi (Japan Society for the Promotion of Science) 821 grant numbers 20H00521 and 21K18267 to MT; 21H00211 and 21K07262 to NW; 822 21H05060 to KJ; 21K20303 to DS; and 17H00891 to KN. This study was also 823 supported by a grant from Uehara Memorial Foundation to MT, and a grant from 824 Takeda Science Foundation to MT. We thank Dr. Shigeyuki Oba, Dr. Shinichi Yoshida, 825 Mr. Zhen Zhang, and Dr. Makoto Iwata for their technical advice on deep learning. We 826 also thank Mr. Rei Furutani and Ms. Maoko Yamanaka for their technical and 827 administrative assistance, respectively. 828 829 Author contributions 830 N.W., K.J., K.N., and M.T. designed the experiment and study. N.W., R.K., and M.T.831 collected the data. N.W., K.M., and M.T. analyzed the data. N.W., K.M., K.J., D.S., 832 K.N., and M.T. wrote the manuscript.833 834 Competing interests 835 The authors declare no competing interests.836\nFigure 1 \u2013 figure supplement 1\nB C\nE\nA\n-100\n50\n-50\n0\nlo g\n(P ow\ner )\nMale face Female face Natural object Artificial object\nSpatial frequency\n0\n1\nBr ig\nht ne\nss\nPixel intensity\nMale face Female face Natural object Artificial object\nF Hue\nN um\nbe r o\nf p ix\nel s\n0\n10,000\nMale face Female face Natural object Artificial object\nD\nContrast\nMa le Fem ale Nat ura l Arti ficia l\n0\n1 Luminance\nMa le Fem ale Nat ura l Arti ficia l\n0\n1\nMa le Fem ale Nat ura l Arti ficia l\n0.8\n1 Similarity\nFigure 1 \u2013 figure supplement 1. Low-level visual features of stimulus images. (A) Michelson contrast of images as the ratio between the difference and the sum of the maximum and minimum pixel intensities. (B) Luminance as the mean pixel intensity of images. (C) Pixel-based pairwise similarity. To estimate the visual similarity among the images in a category, the mean Euclidean distance between normalized grayscale values (scaled to be between 0 and 1) of each stimulus and every other stimulus of the same sub-category was measured. The pairwise similarity values range from 0 (images with inverted intensity difference at each pixel) to 1 (identical images). Note that the order of the size of the effect [0.0062, standard deviation (S.D.) of the category means in normalized grayscale values] is comparable to the resolution of the display (0.0039, minimum pixel-wise distance resolved by the display in normalized values, corresponding to the change in one gray-level value of a possible 255 increments). Mean \u00b1 S.D. are shown in (A\u2013C). (D) Average spatial frequency across images. (E) Average pixel intensity for each pixel. (F) Average hue histogram across images.\nCluster index Cluster name t value x (mm) y (mm) z (mm) Harvard-Oxford Atlas 1 FMC 8.37 4 48 -10 Frontal Medial Cortex\n7.76 8 58 12 Frontal Pole 6.87 -8 60 20 Frontal Pole 5.69 -12 48 10 Paracingulate Gyrus 4.89 4 32 6 Cingulate Gyrus, anterior division 4.43 -24 42 -4 Frontal Pole 4.17 -10 48 32 Frontal Pole 4.04 -8 32 -6 Cingulate Gyrus, anterior division 3.77 8 48 36 Superior Frontal Gyrus\n2 R. AG 7.25 52 -56 18 Angular Gyrus 6.76 40 -56 24 Angular Gyrus 5.34 60 -38 48 Supramarginal Gyrus, posterior division 5.34 54 -60 36 Lateral Occipital Cortex, superior division 4.64 46 -42 10 Supramarginal Gyrus, posterior division 4.48 64 -44 2 Middle Temporal Gyrus, temporooccipital part 4.23 64 -46 34 Supramarginal Gyrus, posterior division 3.67 46 -62 52 Lateral Occipital Cortex, superior division 3 PcC 6.49 6 -56 20 Precuneous Cortex 6.28 -14 -52 32 Cingulate Gyrus, posterior division 5.51 6 -56 34 Precuneous Cortex 4.51 -12 -48 46 Precuneous Cortex 4 L. AG 5.30 -52 -52 20 Angular Gyrus 5.27 -54 -64 38 Lateral Occipital Cortex, superior division 4.77 -62 -52 34 Supramarginal Gyrus, posterior division 4.59 -38 -60 18 Angular Gyrus 5 R. TOF 9.05 44 -48 -22 Temporal Occipital Fusiform Cortex 6 R. FOC 5.61 40 30 -12 Frontal Orbital Cortex 4.44 26 14 -20 Frontal Orbital Cortex 7 L. MTGp 4.79 -66 -10 -24 Middle Temporal Gyrus, posterior division 8 R. LOCi 5.71 44 -80 -10 Lateral Occipital Cortex, inferior division 9 R. MTGp 5.37 60 -8 -16 Middle Temporal Gyrus, posterior division\n4.47 60 0 -26 Middle Temporal Gyrus, anterior division 10 R. Amygdala 8.46 20 -8 -16 Right Amygdala 11 L. LOCi 5.24 -44 -84 -6 Lateral Occipital Cortex, inferior division 3.47 -50 -80 6 Lateral Occipital Cortex, inferior division 12 R. OcP 5.96 12 -98 12 Occipital Pole\nCluster index Cluster name t value x (mm) y (mm) z (mm) Harvard-Oxford Atlas 1 L. TOF/LOC -15.40 -28 -52 -16 Temporal Occipital Fusiform Cortex\n-11.40 -50 -62 -8 Lateral Occipital Cortex, inferior division -10.60 -26 -70 -12 Occipital Fusiform Gyrus -9.75 -24 -64 46 Lateral Occipital Cortex, superior division -9.66 -30 -84 20 Lateral Occipital Cortex, superior division -9.64 -28 -86 4 Lateral Occipital Cortex, inferior division -9.44 -12 -94 -4 Occipital Pole -8.24 -26 -66 34 Lateral Occipital Cortex, superior division -7.55 -32 -30 -20 Temporal Fusiform Cortex, posterior division -7.31 -34 -50 44 Superior Parietal Lobule -6.69 -46 -30 40 Supramarginal Gyrus, anterior division -5.41 -12 -76 40 Precuneous Cortex -5.39 -24 -62 -28 Cerebellum -4.06 -50 -38 56 Postcentral Gyrus\nIncrease in face trial\nIncrease in object trial\nFigure 3 \u2013 figure supplement 1. Brain regions showing significant signal increases and decreases in the contrast of face vs. object. Coordinates are listed in MNI space.\n-3.86 -40 -64 -30 Cerebellum 2 R. TOF/LOC -15.40 28 -46 -16 Temporal Occipital Fusiform Cortex\n-14.00 38 -86 14 Lateral Occipital Cortex, superior division -12.60 28 -62 -12 Occipital Fusiform Gyrus -10.10 50 -60 -8 Lateral Occipital Cortex, inferior division -9.81 24 -78 -10 Occipital Fusiform Gyrus -9.42 16 -88 0 Occipital Pole -7.47 28 -34 -22 Temporal Fusiform Cortex, posterior division -7.18 28 -68 50 Lateral Occipital Cortex, superior division -6.52 8 -76 -18 Lingual Gyrus -6.44 32 -82 32 Lateral Occipital Cortex, superior division -6.22 34 -72 -44 Cerebellum -5.98 30 -64 36 Lateral Occipital Cortex, superior division -5.81 14 -74 -44 Cerebellum -5.78 32 -68 24 Lateral Occipital Cortex, superior division -5.48 28 -62 -28 Cerebellum -5.44 22 -84 40 Lateral Occipital Cortex, superior division -5.10 10 -70 50 Precuneous Cortex -5.07 10 -84 -28 Occipital Fusiform Gyrus -4.88 32 -56 62 Superior Parietal Lobule -4.76 -8 -70 -42 Cerebellum -4.45 18 -64 60 Lateral Occipital Cortex, superior division -4.44 -26 -64 -48 Cerebellum -4.35 20 -60 -48 Cerebellum -4.14 0 -74 -32 Cerebellum -4.09 50 -44 -10 Inferior Temporal Gyrus, temporooccipital part -3.92 14 -56 -34 Cerebellum -3.84 -38 -56 -48 Cerebellum -3.45 -30 -60 -36 Cerebellum -3.44 26 -50 -34 Cerebellum\n3 L. MFGi -9.85 -48 10 34 Middle Frontal Gyrus -8.34 -46 32 14 Inferior Frontal Gyrus, pars triangularis -7.15 -42 24 24 Middle Frontal Gyrus -7.04 -30 24 4 Insular Cortex -4.85 -52 14 -6 Inferior Frontal Gyrus, pars opercularis -4.84 -30 32 -10 Frontal Orbital Cortex -3.98 -42 8 4 Central Opercular Cortex -3.92 -38 2 42 Precentral Gyrus 4 PcG -6.81 6 10 50 Paracingulate Gyrus -6.63 -6 18 42 Paracingulate Gyrus -6.11 10 20 32 Cingulate Gyrus, anterior division -5.32 -8 4 54 Supplementary Motor Cortex -4.79 -10 -4 70 Superior Frontal Gyrus -4.66 -2 -10 58 Supplementary Motor Cortex -3.84 10 4 70 Superior Frontal Gyrus 5 L. MFGs -5.60 -38 -2 60 Middle Frontal Gyrus -4.76 -22 2 58 Superior Frontal Gyrus 6 R. Ins -5.60 32 24 4 Insular Cortex -4.57 54 16 -2 Inferior Frontal Gyrus, pars opercularis -3.79 52 14 -16 Temporal Pole 7 L. Thalamus -6.39 -10 -18 10 Left Thalamus -4.05 -14 -4 16 Left Caudate -3.60 -10 -20 -8 Left Thalamus 8 R. SFG -5.55 26 4 62 Superior Frontal Gyrus -3.96 36 -2 58 Middle Frontal Gyrus\nAbbreviations. R, right; L, left, AG, angular gyrus; FG, fusiform gyrus; FMC, frontal medial cortex; FOC, frontal orbital cortex; Ins, insular cortex; LOC, lateral occipital cortex; MFG, middle frontal gyrus; MTG, middle temporal gyrus; OcP, occipital pole; OFC, orbitofrontal cortex; OP, occipital pole; PcC, precuneous cortex; PcG, paracingulate gyrus; SFG, superior frontal gyrus; TOF, temporal occipital fusiform cortex; i, inferior; s, superior; p, posterior\nIncrease in male-face trial\nCluster index t value x (mm) y (mm) z (mm) Harvard-Oxford Atlas\nCluster index t value x (mm) y (mm) z (mm) Harvard-Oxford Atlas 1 -5.34 -30 -68 -12 Occipital Fusiform Gyrus\n-4.91 -16 -90 -6 Occipital Pole -4.87 -36 -78 -16 Occipital Fusiform Gyrus -4.84 -28 -72 22 Lateral Occipital Cortex, superior division -4.67 -20 -80 -14 Occipital Fusiform Gyrus -4.42 -42 -80 12 Lateral Occipital Cortex, inferior division -4.40 -54 -68 -8 Lateral Occipital Cortex, inferior division -4.31 -30 -54 -18 Temporal Occipital Fusiform Cortex -4.13 -34 -76 0 Lateral Occipital Cortex, inferior division -4.07 -16 -96 8 Occipital Pole -4.05 -48 -56 -16 Inferior Temporal Gyrus, temporooccipital part -4.02 -38 -88 0 Lateral Occipital Cortex, inferior division -3.61 -46 -78 -4 Lateral Occipital Cortex, inferior division\n2 -4.84 28 -66 -14 Occipital Fusiform Gyrus -4.49 36 -46 -16 Temporal Occipital Fusiform Cortex -3.94 36 -88 -12 Lateral Occipital Cortex, inferior division -3.91 50 -70 -14 Lateral Occipital Cortex, inferior division -3.89 24 -82 -6 Occipital Fusiform Gyrus -3.67 42 -74 -2 Lateral Occipital Cortex, inferior division 3 -4.62 36 -76 4 Lateral Occipital Cortex, inferior division\nIncrease in female-face trial\nFigure 3 \u2013 figure supplement 2. Brain regions showing significant signal increases and decreases in the contrast of male-face vs. female-face. Coordinates are listed in MNI space.\nCluster index t value x (mm) y (mm) z (mm) Harvard-Oxford Atlas\nCluster index t value x (mm) y (mm) z (mm) Harvard-Oxford Atlas 1 -5.61 38 -74 -6 Lateral Occipital Cortex, inferior division\n-5.45 38 -80 6 Lateral Occipital Cortex, inferior division -5.28 28 -78 -12 Occipital Fusiform Gyrus -4.75 34 -82 18 Lateral Occipital Cortex, superior division -3.95 42 -68 -16 Occipital Fusiform Gyrus -3.53 26 -78 0 Occipital Fusiform Gyrus\n2 -4.18 -32 -92 12 Occipital Pole -3.66 -18 -94 22 Occipital Pole\nIncrease in natural-object trial\nIncrease in artificial-object trial\nFigure 3 \u2013 figure supplement 3. Brain regions showing significant signal increases and decreases in the contrast of natural-object vs. artificial-object. Coordinates are listed in MNI space.\nLabel Region Value FDR q Label Region Value FDR q Label Region Value FDR q Label Region Value FDR q 1 L. LOCi Occipital 0.0067 1.10E-113 * 56 Brain-Stem Subcortex 5.33E-06 1.06E-30 * 1 L. TOF Temporal 0.0046 7.44E-92 * 56 R. SmGa Parietal 8.67E-07 0.27 2 R. LOCi Occipital 0.0061 1.16E-114 * 57 R. IFGpo Frontal 4.42E-06 1.72E-05 * 2 L. ITGtp Temporal 0.0035 9.32E-93 * 57 L. FP Frontal 6.47E-07 0.0019 * 3 R. TOF Temporal 0.0023 3.85E-93 * 58 R. PhGa Temporal 4.30E-06 0.0018 * 3 R. TOF Temporal 0.0014 5.46E-84 * 58 L. ScC Frontal 3.93E-07 0.12 4 L. OcP Occipital 0.0013 3.43E-113 * 59 R. SPL Parietal 4.03E-06 2.99E-13 * 4 L. LOCi Occipital 0.0014 6.64E-86 * 59 L. PoG Parietal 3.44E-07 0.00012 * 5 R. OcP Occipital 0.00049 2.02E-112 * 60 R Accumbens Subcortex 3.77E-06 1.53E-05 * 5 L. OcP Occipital 0.00088 8.90E-95 * 60 L Accumbens Subcortex 2.39E-07 0.012 * 6 R. MTGtp Temporal 0.00041 3.33E-89 * 61 L. PT Temporal 3.65E-06 0.016 * 6 R. ITGtp Temporal 0.00084 2.63E-76 * 61 L. PhGa Temporal 2.26E-07 0.68 7 L. TOF Temporal 0.00031 9.49E-58 * 62 L. ITGa Temporal 3.45E-06 0.00014 * 7 R. OFG Occipital 0.00066 6.97E-87 * 62 R. Ins Insula 3.67E-08 1 180-E40.1-xetrocbuSnematuP L36*59-E09.806000.0latipiccOPcO .R8990.060-E10.3latnorFopGFI .L36*04-E22.121000.0latipiccOCcI .L8 9 R. AG Parietal 0.00010 4.93E-33 * 64 L Thalamus Subcortex 2.43E-06 5.62E-07 * 9 L. OFG Occipital 0.00049 2.21E-71 * 64 R Accumbens Subcortex -2.52E-08 0.73\n10 R. SmGp Parietal 0.00010 2.74E-68 * 65 R Pallidum Subcortex 2.23E-06 0.079 10 R. LOCi Occipital 0.00045 1.47E-68 * 65 R. TP Temporal -3.08E-07 0.52 11 L. COpc Frontal 8.85E-05 8.35E-99 * 66 R. MFG Frontal 2.03E-06 0.0056 * 11 R. LOCs Parietal 0.00020 9.14E-84 * 66 R Pallidum Subcortex -4.39E-07 1 12 L. PrG Frontal 7.39E-05 1.11E-108 * 67 L. MTGa Temporal 2.03E-06 0.23 12 L. ITGp Temporal 0.00010 1.70E-16 * 67 L Pallidum Subcortex -4.97E-07 1 13 L. LG Occipital 7.27E-05 7.71E-34 * 68 R Caudate Subcortex 1.93E-06 0.0025 * 13 L. LOCs Parietal 9.80E-05 3.04E-60 * 68 L. TP Temporal -1.10E-06 1 14 R. LG Occipital 7.26E-05 2.95E-31 * 69 L Caudate Subcortex 1.91E-06 4.15E-05 * 14 L. TFCp Temporal 7.93E-05 6.54E-18 * 69 R. COpC Frontal -1.24E-06 1 15 L. SMC Frontal 5.43E-05 6.79E-98 * 70 R. ITGa Temporal 1.62E-06 0.017 * 15 R. FOpC Frontal 5.52E-05 4.75E-55 * 70 L. IFGpo Frontal -1.28E-06 0.30 16 L. PoG Parietal 5.40E-05 2.75E-103 * 71 L. STGa Temporal 1.22E-06 0.022 * 16 R. SPL Parietal 3.93E-05 1.22E-67 * 71 R. MFG Frontal -1.43E-06 1 17 L. H1/H2 Parietal 4.98E-05 3.52E-39 * 72 R. SFG Frontal 1.21E-06 0.0041 * 17 L. IFGpt Frontal 3.74E-05 1.15E-18 * 72 L. PcG Frontal -1.70E-06 1 18 L. LOCs Parietal 4.92E-05 6.98E-51 * 73 R. PcC Parietal 6.89E-07 0.0017 * 18 R. IcC Occipital 3.49E-05 7.43E-12 * 73 L. FOpc Frontal -2.24E-06 0.62 19 R. LOCs Parietal 4.40E-05 7.36E-47 * 74 R. TP Temporal 6.14E-07 0.54 19 L. SPL Parietal 2.82E-05 7.56E-47 * 74 R. SFG Frontal -2.71E-06 1 20 L. POpc Parietal 3.88E-05 1.12E-37 * 75 R. STGp Temporal 5.48E-07 0.54 20 L. PrG Frontal 2.71E-05 9.67E-62 * 75 R. H1/H2 Parietal -2.91E-06 1 21 L. PcC Parietal 3.84E-05 2.00E-67 * 76 L. FMC Frontal 1.49E-07 0.28 21 L. LG Occipital 2.66E-05 5.88E-06 * 76 L. STGa Temporal -2.97E-06 1 22 L. SmGa Parietal 3.82E-05 1.66E-36 * 77 R Thalamus Subcortex 1.25E-07 0.31 22 R. TFCp Temporal 2.62E-05 9.81E-11 * 77 R. PP Temporal -3.36E-06 1 23 R. CGa Frontal 3.65E-05 2.41E-60 * 78 R. SmGa Parietal -8.87E-07 0.59 23 R Caudate Subcortex 1.83E-05 2.79E-24 * 78 R Amygdala Subcortex -3.41E-06 1 24 L. SPL Parietal 2.94E-05 5.83E-85 * 79 L. PhGa Temporal -1.01E-06 0.54 24 L. SMC Frontal 1.76E-05 3.32E-09 * 79 L Amygdala Subcortex -3.51E-06 1 25 L. CGa Frontal 2.90E-05 7.12E-57 * 80 L Accumbens Subcortex -1.27E-06 1 25 R. PcC Parietal 1.73E-05 1.89E-26 * 80 R. ScC Frontal -3.55E-06 1 26 L. Ins Insula 2.84E-05 1.07E-28 * 81 R. Ins Insula -1.96E-06 0.91 26 L. POpc Parietal 1.50E-05 1.16E-29 * 81 L. Ins Insula -3.58E-06 1 27 L. SmGp Parietal 2.27E-05 5.32E-13 * 82 R. MTGa Temporal -2.18E-06 1 27 R. CC Occipital 1.30E-05 2.72E-05 * 82 L. FMC Frontal -3.67E-06 1 28 R. ScC Frontal 2.24E-05 1.56E-53 * 83 L. MFG Frontal -2.39E-06 1 28 L. H1/H2 Parietal 1.30E-05 3.78E-14 * 83 L. SmGa Parietal -3.78E-06 1 29 L. PP Temporal 2.21E-05 3.47E-19 * 84 L. ScC Frontal -2.39E-06 1 29 R. ITGa Temporal 1.05E-05 2.85E-33 * 84 L. SFG Frontal -3.92E-06 1 30 L Putamen Subcortex 2.21E-05 1.64E-41 * 85 R. STGa Temporal -2.79E-06 1 30 R Thalamus Subcortex 9.74E-06 1.80E-34 * 85 L. COpc Frontal -4.58E-06 1 31 R. SccC Occipital 2.00E-05 0.00018 * 86 L. SFG Frontal -3.02E-06 1 31 R. CGa Frontal 8.03E-06 1.18E-17 * 86 R. FP Frontal -5.28E-06 1 32 R. FMC Frontal 1.95E-05 2.15E-37 * 87 R. PP Temporal -4.09E-06 1 32 L. ITGa Temporal 7.80E-06 1.20E-29 * 87 R. FMC Frontal -5.34E-06 1 33 L. OFG Occipital 1.85E-05 1 88 L. STGp Temporal -6.18E-06 1 33 R. CGp Parietal 6.71E-06 0.00098 * 88 R. SccC Occipital -5.57E-06 1 34 R Amygdala Subcortex 1.78E-05 1.78E-28 * 89 L Pallidum Subcortex -7.76E-06 1 34 R. PrG Frontal 6.39E-06 2.71E-21 * 89 R. TFCa Temporal -7.03E-06 1 35 R. SMC Frontal 1.76E-05 1.82E-47 * 90 R. FP Frontal -9.19E-06 1 35 R. PoG Parietal 5.97E-06 1.22E-38 * 90 R. AG Parietal -8.02E-06 1 36 R. CGp Parietal 1.75E-05 9.07E-36 * 91 R. MTGp Temporal -9.29E-06 1 36 R. SMC Frontal 5.66E-06 2.33E-10 * 91 R. STGp Temporal -8.02E-06 1 37 L. TFCp Temporal 1.71E-05 6.56E-11 * 92 L. CGp Parietal -9.78E-06 1 37 R. PcG Frontal 5.60E-06 3.42E-09 * 92 L. SmGp Parietal -1.04E-05 1 38 R. TFCa Temporal 1.66E-05 1.10E-40 * 93 L. FP Frontal -1.02E-05 1 38 R. LG Occipital 5.42E-06 1 93 L. CC Occipital -1.05E-05 1 39 L. FOpc Frontal 1.63E-05 8.63E-29 * 94 L. IFGpt Frontal -1.02E-05 1 39 L. FOC Frontal 4.93E-06 1.92E-17 * 94 R. SmGp Parietal -1.19E-05 1 40 R. IcC Occipital 1.60E-05 0.0034 * 95 L. PhGp Temporal -1.16E-05 1 40 L. TFCa Temporal 4.65E-06 5.25E-11 * 95 R. ITGp Temporal -1.26E-05 1 41 L. TP Temporal 1.60E-05 2.96E-46 * 96 L. MTGp Temporal -1.30E-05 1 41 L Caudate Subcortex 4.58E-06 1.58E-13 * 96 L. STGp Temporal -1.37E-05 1 42 R Putamen Subcortex 1.53E-05 3.82E-27 * 97 L. ITGp Temporal -1.54E-05 1 42 R. IFGpo Frontal 4.52E-06 4.07E-10 * 97 R. MTGtp Temporal -1.55E-05 1 43 R. FOC Frontal 1.49E-05 9.81E-49 * 98 R. H1/H2 Parietal -1.65E-05 1 43 Brain-Stem Subcortex 3.49E-06 2.52E-15 * 98 L. PP Temporal -1.59E-05 1 44 L. PcG Frontal 1.46E-05 7.25E-21 * 99 L Hippocampus Subcortex -2.84E-05 1 44 R. STGa Temporal 3.12E-06 3.21E-08 * 99 L. IcC Occipital -1.80E-05 1 45 R. PcG Frontal 1.46E-05 8.66E-14 * 100 R. POpC Parietal -6.59E-05 1 45 L. CGp Parietal 3.01E-06 0.0040 * 100 R. IFGpt Frontal -1.88E-05 1 46 R. COpC Frontal 1.39E-05 3.19E-13 * 101 R. ITGp Temporal -7.27E-05 1 46 L Thalamus Subcortex 2.81E-06 5.80E-08 * 101 R. MTGp Temporal -1.91E-05 1 47 L. AG Parietal 1.16E-05 0.0043 * 102 R Hippocampus Subcortex -7.60E-05 1 47 L. CGa Frontal 2.70E-06 0.0051 * 102 L Hippocampus Subcortex -1.91E-05 1 48 R. TFCp Temporal 1.11E-05 0.042 * 103 L. CC Occipital -8.47E-05 1 48 R. POpC Parietal 2.68E-06 3.36E-05 * 103 L. PcC Parietal -2.22E-05 1 49 R. PoG Parietal 1.01E-05 1.73E-41 * 104 R. CC Occipital -9.99E-05 1 49 L. MTGa Temporal 2.68E-06 0.0031 * 104 L. PT Temporal -3.06E-05 1 50 R. PrG Frontal 9.93E-06 2.87E-39 * 105 R. PhGp Temporal -0.00012 1 50 R. MTGa Temporal 2.63E-06 3.37E-07 * 105 R Hippocampus Subcortex -4.91E-05 1 51 R. IFGpt Frontal 9.56E-06 4.09E-20 * 106 L. MTGtp Temporal -0.00014 1 51 R. FOC Frontal 2.07E-06 1.96E-06 * 106 L. AG Parietal -5.36E-05 1 52 L. TFCa Temporal 6.93E-06 9.98E-07 * 107 R. PT Temporal -0.00016 1 52 R. PhGa Temporal 1.69E-06 0.0082 * 107 R. PhGp Temporal -5.48E-05 1 53 L. FOC Frontal 5.89E-06 2.01E-19 * 108 R. ITGtp Temporal -0.00033 1 53 L. MFG Frontal 1.62E-06 0.0067 * 108 L. MTGp Temporal -6.69E-05 1 54 R. FOpC Frontal 5.63E-06 9.07E-09 * 109 L. ITGtp Temporal -0.00039 1 54 R Putamen Subcortex 1.30E-06 0.042 * 109 L. PhGp Temporal -0.00011 1 55 L Amygdala Subcortex 5.61E-06 1.46E-06 * 110 R. OFG Occipital -0.00040 1 55 R. PT Temporal 9.14E-07 0.17 110 L. MTGtp Temporal -0.00030 1\nFace trial Object trial\nFigure 3 \u2013 figure supplement 4. Guided Grad-CAM values for face and object trials in anatomically-defined brain regions. The brain regions are sorted by the magnitudes of the values. An asterisk denotes FDR q < 0.05.\n. C C -B Y -N C -N D 4.0 International license\nm ade available under a\n(w hich w as not certified by peer review ) is the author/funder, w ho has granted bioR xiv a license to display the preprint in perpetuity. It is T he copyright holder for this preprint this version posted July 10, 2022. ; https://doi.org/10.1101/2022.04.06.487262 doi: bioR xiv preprint\nFigure 3 \u2013 figure supplement 5\nG ui\nde d\nG ra\ndC\nAM v\nal ue\n(a .u\n.)\nt values in univariate analysis\nFace dominant Object dominant\nFace dominant Object dominant\n-15 -10 -5 0 5 10\n-0.02\n0\n0.02\n0.04\n0.06\n0.08\nFace\n-15 -10 -5 0 5 10\n-0.02\n0\n0.02\n0.04\n0.06 Object\nB\nA\nG ui\nde d\nG ra\ndC\nAM v\nal ue\n(a .u\n.)\nt values in univariate analysis\nFace dominant Object dominant\nFace dominant Object dominant\n-15 -10 -5 0 5 10\n-0.02\n0\n0.02\n0.04\n0.06\n0.08\nFace\n-15 -10 -5 0 5 10\n-0.02\n0\n0.02\n0.04\n0.06 Object\nAll voxels (masked by avg152T1_gray.img)\nSelected voxels (masked by Neurosynth-defined ROIs)\n\u03c1 = 0.048* \u03c1 = -0.21*\n\u03c1 = 0.27* \u03c1 = -0.30*\nFigure 3 \u2013 figure supplement 5. Voxel-based relationship between t-values in the univariate analysis and the guided Grad-CAM values in the face and object trial. (A) Relationship between the two values when voxels are masked by the gray matter prior volume of the FSL. (B) Relationship between the two values when the voxels are masked by functional ROIs obtained from Neurosynth using the terms \u201cface\u201d and \u201cobject.\u201d *, P < 0.001.\nFigure 3 \u2013 figure supplement 6\nL. L OC R. L OC R. T OF\n0\n0.04\nG ui\nde d\nG ra\ndC\nAM v\nal ue\nR. O cP R. A G L. A G\n0\n5 \u00d710^-3\nFM C\nR. am\nyg da\nla R. FO C Pc C L. MT Gp R. M TG p\n0\n1 \u00d710^-3\n0\n0.03\nG ui\nde d\nG ra\ndC\nAM v\nal ue\n0\n15 \u00d710^-4\nR. S FG\nL. t hala\nmu s PcG\n0\n2 \u00d710^-4\n\u00d710^-3 \u00d710^-3\n\u00d710^-3 \u00d710^-3\nFace\nObject\n* * * *\n* * * * * * * * *\n*\nL. T OF\n/LO C\nR. T OF\n/LO C L. MFG R. Ins\nLearned model Non-learned model\nLearned model Non-learned model\n*\nAnatomically defined ROIsA\nL. TOF R. TOF 0\n10\nL. S PL L. M FG R . F O C R . M FG Pc G Pc C\nR . c\nau da te L. M TG p FM C R . t ha la m us L. p ut am en L. F O C L. th al am us FP\n0\n1.5\nL. OFG/TOF/ LOC/SPL\n0\n2\nR. ca\nud ate\nR. th\nala mu\ns Pc G\nR. In\ns/I FG\n/Pc G/\nMF G L. tha lam us R. pu tam en\nL. am\nyg da\nla/ hip\npo ca\nmp us\nR. am\nyg da\nla/ hip\npo ca\nmp us Pc C\nR. LO\nCs\n0\n1\nG ui\nde d\nG ra\ndC\nAM v\nal ue\nG ui\nde d\nG ra\ndC\nAM v\nal ue\nFace\nObject\nLearned model Non-learned model\nLearned model Non-learned model\nNeurosynth-defined ROIsB\n* * * ** * * * * * * * * * *\n* * * * * *\nFigure 3 \u2014 figure supplement 6. ROI analysis. Guided Grad-CAM values for each ROI when ROIs are defined (A) anatomically (Harvard\u2013Oxford atlas) and (B) functionally (Neurosynth). The figure configurations are the same as those in Figure 3f. L, left; R, right; AG, angular gyrus; FMC, frontal medial cortex; FOC, frontal orbital cortex; FP, frontal pole; IFG, inferior frontal gyrus; Ins, insular cortex; LOC, lateral occipital cortex; MFG, middle frontal gyrus; MTG, middle temporal gyrus; OcP, occipital pole; OFG, occipital fusiform gyrus; PcC, precuneous cortex; PcG, paracingulate gyrus; SFG, superior frontal gyrus; SPL, superior parietal lobule; TOF, temporal occipital fusiform cortex; i, inferior; s, superior; p, posterior.\nFigure 4 \u2013 figure supplement 1\nER P\n(\u03bc V)\n0 1,000\nTime from stimulus onset (ms)\n0 1,000 0 1,000\n6\n0\n-3 6\n0\n-3\n6\n0\n-3 6\n0\n-3\n6\n0\n-3 6\n0\n-3\nFace\nObject\nMale face\nFemale face\nNatural object\nArtificial object\n1\n-1\n0\n1-1 0\nAP R\nL\nA\nC ha\nnn el\n0 1,000 0 1,000 Time from stimulus onset (ms) 0 1,000 0 1,000 0 1,000 0 1,000\n1\n63\n1\n63\nC DB Difference (Face minus Object) Difference (Male minus Female) Difference (Natural minus Artificial) ERP ERP ERP\nAlpha power Alpha power Alpha power\nBeta power Beta power Beta power\nGamma power Gamma power Gamma power\nFigure 4 \u2014 figure supplement 1. EEG responses. (A) ERPs for face trials, object trials, male face trials, female face trials, natural object trials, and artificial object trials. The line colors correspond to the channel locations depicted on the left side. A, anterior, P, posterior, R, right, L, left. (B\u2013D) Significance of temporal trajectories in ERPs and power spectrograms. The colored lines depict the time points showing significant differential ERP amplitudes or power spectrograms against zero in the respective channels. The color of each line depicts the location of the corresponding channel (left). Time points that exceeded the 97.5th percentile of the permutation distribution (N = 5,000) served as clusterinducing time points, and the significance of clusters was set at the threshold of P < 0.05.\nFigure 4 \u2013 figure supplement 2\nB CA\nRegion\nG ui\nde d\nG ra\ndC\nAM\nFro nta\nl\nCe ntr\nal\nPa rie\ntal\nOc cip\nita l L. tem po ral R. te mp ora\nl0\n0.05 Face\nFro nta\nl\nCe ntr\nal\nPa rie\ntal\nOc cip\nita l L. tem po ral R. te mp ora\nl0\n1 Male face\nFro nta\nl\nCe ntr\nal\nPa rie\ntal\nOc cip\nita l L. tem po ral R. te mp ora\nl0\n4 Natural object\nFro nta\nl\nCe ntr\nal\nPa rie\ntal\nOc cip\nita l L. tem po ral R. te mp ora\nl0\n0.1 Object\nFro nta\nl\nCe ntr\nal\nPa rie\ntal\nOc cip\nita l L. tem po ral R. te mp ora\nl0\n0.25 Female face\nFro nta\nl\nCe ntr\nal\nPa rie\ntal\nOc cip\nita l L. tem po ral R. te mp ora\nl0\n2.5 Artificial object\nFace\nFro nta\nl Ce ntr al Pa rie tal Oc cip ita l\nL.t em\npo ral R. tem po ral\nFrontal Central Parietal\nOccipital L.temporal R.temporal\nMale face\nFro nta\nl Ce ntr al Pa rie tal Oc cip ita l\nL.t em\npo ral R. tem po ral\nFrontal Central Parietal\nOccipital L.temporal R.temporal\nNatural object\nFro nta\nl Ce ntr al Pa rie tal Oc cip ita l\nL.t em\npo ral R. tem po ral\nFrontal Central Parietal\nOccipital L.temporal R.temporal\nObject\nFro nta\nl Ce ntr al Pa rie tal Oc cip ita l\nL.t em\npo ral\nR. tem\npo ral\nFrontal Central Parietal\nOccipital L.temporal R.temporal\nFemale face\nFro nta\nl Ce ntr al Pa rie tal Oc cip ita l\nL.t em\npo ral\nR. tem\npo ral\nFrontal Central Parietal\nOccipital L.temporal R.temporal\nArtificial object\nFro nta\nl Ce ntr al Pa rie tal Oc cip ita l\nL.t em\npo ral\nR. tem\npo ral\nFrontal Central Parietal\nOccipital L.temporal R.temporal\nE FD Significant\nNon-significant\nFigure 4 \u2014 figure supplement 2. Regional analysis of guided Grad-CAM. (A\u2013C) Comparison of the guided Grad-CAM values at the maximum latency in face and object trials (A), male face and female face trials (B), and natural object and artificial object trials (C) across six regions. All 63 channels are categorized as either frontal, central, parietal, occipital, left temporal, and right temporal regions based on their locations. The trial-based median value distributions across channels in a region are shown. The regional differences in the maximum guided Grad-CAM values are statistically significant in all trial types (Friedman\u2019s test, P < 0.001). (D\u2013F) Results of multiple comparison test (Tukey\u2013Kramer method, P < 0.05). Significant differences in the guided GradCAM values between regions are color-coded.\nReferences 1 Allen, P.J., Josephs, O., and Turner, R. (2000). A method for removing imaging artifact 2 from continuous EEG recorded during functional MRI. Neuroimage 12, 230-239. 3 10.1006/nimg.2000.0599. 4 Bai, S., Kolter, J.Z., and Koltun, V. (2018). An Empirical Evaluation of Generic 5 Convolutional and Recurrent Networksfor Sequence Modeling. arXiv:1803.01271v2. 6 10.48550/arXiv.1803.01271. 7 Bar, M. (2004). Visual objects in context. Nat Rev Neurosci 5, 617-629. 8 10.1038/nrn1476. 9\nBarton, J.J. (2011). Disorder of higher visual function. Curr Opin Neurol 24, 1-5. 10 10.1097/WCO.0b013e328341a5c2. 11 Birn, R.M., Diamond, J.B., Smith, M.A., and Bandettini, P.A. (2006). Separating 12 respiratory-variation-related fluctuations from neuronal-activity-related fluctuations 13 in fMRI. Neuroimage 31, 1536-1548. 10.1016/j.neuroimage.2006.02.048. 14 Carlson, T., Tovar, D.A., Alink, A., and Kriegeskorte, N. (2013). Representational 15 dynamics of object vision: the first 1000 ms. J Vis 13, 1. 10.1167/13.10.1. 16 Cavina-Pratesi, C., Kentridge, R.W., Heywood, C.A., and Milner, A.D. (2010). Separate 17 processing of texture and form in the ventral stream: evidence from FMRI and visual 18 agnosia. Cereb Cortex 20, 433-446. 10.1093/cercor/bhp111. 19 Cichy, R.M., and Oliva, A. (2020). A M/EEG-fMRI Fusion Primer: Resolving Human 20 Brain Responses in Space and Time. Neuron 107, 772-781. 21 10.1016/j.neuron.2020.07.001. 22 Cichy, R.M., Pantazis, D., and Oliva, A. (2014). Resolving human object recognition in 23 space and time. Nat Neurosci 17, 455-462. 10.1038/nn.3635. 24 Dobs, K., Isik, L., Pantazis, D., and Kanwisher, N. (2019). How face perception unfolds 25 over time. Nat Commun 10, 1258. 10.1038/s41467-019-09239-1. 26 Eklund, A., Nichols, T.E., and Knutsson, H. (2016). Cluster failure: Why fMRI 27 inferences for spatial extent have inflated false-positive rates. Proc Natl Acad Sci U 28 S A 113, 7900-7905. 10.1073/pnas.1602413113. 29 Epstein, R., and Kanwisher, N. (1998). A cortical representation of the local visual 30 environment. Nature 392, 598-601. 10.1038/33402. 31 Freedman, D.J., Riesenhuber, M., Poggio, T., and Miller, E.K. (2001). Categorical 32 representation of visual stimuli in the primate prefrontal cortex. Science 291, 312-33 316. 10.1126/science.291.5502.312.34\nFrey, M., Nau, M., and Doeller, C.F. (2021). Magnetic resonance-based eye tracking 35 using deep neural networks. Nat Neurosci 24, 1772-1779. 10.1038/s41593-021-36 00947-w. 37 Gainotti, G., and Marra, C. (2011). Differential contribution of right and left temporo-38 occipital and anterior temporal lesions to face recognition disorders. Front Hum 39 Neurosci 5, 55. 10.3389/fnhum.2011.00055. 40 Geusebroek, J., Burghouts, G.J., and Smeulders, A.W.M. (2005). The Amsterdam 41 Library of Object Images. Int. J. Comput. Vis. 61, 103-112. 42 10.1023/B:VISI.0000042993.50813.60. 43 Grill-Spector, K., and Weiner, K.S. (2014). The functional architecture of the ventral 44 temporal cortex and its role in categorization. Nat Rev Neurosci 15, 536-548. 45 10.1038/nrn3747. 46 Grill-Spector, K., Weiner, K.S., Kay, K., and Gomez, J. (2017). The Functional 47 Neuroanatomy of Human Face Perception. Annu Rev Vis Sci 3, 167-196. 48 10.1146/annurev-vision-102016-061214. 49 Harvey, A.K., Pattinson, K.T., Brooks, J.C., Mayhew, S.D., Jenkinson, M., and Wise, 50 R.G. (2008). Brainstem functional magnetic resonance imaging: disentangling signal 51 from physiological noise. J Magn Reson Imaging 28, 1337-1344. 52 10.1002/jmri.21623. 53 Haxby, J.V., Gobbini, M.I., Furey, M.L., Ishai, A., Schouten, J.L., and Pietrini, P. 54 (2001). Distributed and overlapping representations of faces and objects in ventral 55 temporal cortex. Science 293, 2425-2430. 10.1126/science.1063736. 56 Hong, H., Yamins, D.L., Majaj, N.J., and DiCarlo, J.J. (2016). Explicit information for 57 category-orthogonal object properties increases along the ventral stream. Nat 58 Neurosci 19, 613-622. 10.1038/nn.4247. 59 Horikawa, T., and Kamitani, Y. (2017). Generic decoding of seen and imagined objects 60 using hierarchical visual features. Nat Commun 8, 15037. 10.1038/ncomms15037. 61 Hung, C.P., Kreiman, G., Poggio, T., and DiCarlo, J.J. (2005). Fast readout of object 62 identity from macaque inferior temporal cortex. Science 310, 863-866. 63 10.1126/science.1117593. 64 Ishai, A., Ungerleider, L.G., Martin, A., Schouten, J.L., and Haxby, J.V. (1999). 65 Distributed representation of objects in the human ventral visual pathway. Proc Natl 66 Acad Sci U S A 96, 9379-9384. 10.1073/pnas.96.16.9379. 67 Jiang, P.T., Zhang, C.B., Hou, Q., Cheng, M.M., and Wei, Y. (2021). LayerCAM: 68 Exploring Hierarchical Class Activation Maps for Localization. IEEE Trans Image 69 Process 30, 5875-5888. 10.1109/TIP.2021.3089943. 70\nKanwisher, N., McDermott, J., and Chun, M.M. (1997). The fusiform face area: a 71 module in human extrastriate cortex specialized for face perception. J Neurosci 17, 72 4302-4311. 10.1523/JNEUROSCI.17-11-04302.1997. 73 Kasper, L., Bollmann, S., Diaconescu, A.O., Hutton, C., Heinzle, J., Iglesias, S., 74 Hauser, T.U., Sebold, M., Manjaly, Z.M., Pruessmann, K.P., and Stephan, K.E. 75 (2017). The PhysIO Toolbox for Modeling Physiological Noise in fMRI Data. J 76 Neurosci Methods 276, 56-72. 10.1016/j.jneumeth.2016.10.019. 77 Keerativittayayut, R., Aoki, R., Sarabi, M.T., Jimura, K., and Nakahara, K. (2018). 78 Large-scale network integration in the human brain tracks temporal fluctuations in 79 memory encoding performance. Elife 7, e32696. 10.7554/eLife.32696. 80 Kingma, D., and Ba, J. (2015). Adam: A Method for Stochastic Optimization. ICLR 81 2015. 10.48550/arXiv.1412.6980. 82 Konen, C.S., Behrmann, M., Nishimura, M., and Kastner, S. (2011). The functional 83 neuroanatomy of object agnosia: a case study. Neuron 71, 49-60. 84 10.1016/j.neuron.2011.05.030. 85 Kourtzi, Z., and Kanwisher, N. (2000). Cortical regions involved in perceiving object 86 shape. J Neurosci 20, 3310-3318. 10.1523/JNEUROSCI.20-09-03310.2000. 87 Kreiman, G., Koch, C., and Fried, I. (2000). Category-specific visual responses of 88 single neurons in the human medial temporal lobe. Nat Neurosci 3, 946-953. 89 10.1038/78868. 90 Kriegeskorte, N., Formisano, E., Sorger, B., and Goebel, R. (2007). Individual faces 91 elicit distinct response patterns in human anterior temporal cortex. Proc Natl Acad 92 Sci U S A 104, 20600-20605. 10.1073/pnas.0705654104. 93 Kriegeskorte, N., Mur, M., Ruff, D.A., Kiani, R., Bodurka, J., Esteky, H., Tanaka, K., 94 and Bandettini, P.A. (2008). Matching categorical object representations in inferior 95 temporal cortex of man and monkey. Neuron 60, 1126-1141. 96 10.1016/j.neuron.2008.10.043. 97\nLandi, S.M., Viswanathan, P., Serene, S., and Freiwald, W.A. (2021). A fast link 98 between face perception and memory in the temporal pole. Science 373, 581-585. 99 10.1126/science.abi6671. 100 Liu, J., Harris, A., and Kanwisher, N. (2002). Stages of processing in face perception: 101 an MEG study. Nat Neurosci 5, 910-916. 10.1038/nn909. 102 Logothetis, N.K., and Sheinberg, D.L. (1996). Visual object recognition. Annu Rev 103 Neurosci 19, 577-621. 10.1146/annurev.ne.19.030196.003045. 104 Mack, M.L., and Palmeri, T.J. (2011). The timing of visual object categorization. Front 105 Psychol 2, 165. 10.3389/fpsyg.2011.00165. 106\nMalach, R., Reppas, J.B., Benson, R.R., Kwong, K.K., Jiang, H., Kennedy, W.A., 107 Ledden, P.J., Brady, T.J., Rosen, B.R., and Tootell, R.B. (1995). Object-related 108 activity revealed by functional magnetic resonance imaging in human occipital 109 cortex. Proc Natl Acad Sci U S A 92, 8135-8139. 10.1073/pnas.92.18.8135. 110 Maris, E., and Oostenveld, R. (2007). Nonparametric statistical testing of EEG- and 111 MEG-data. J Neurosci Methods 164, 177-190. 10.1016/j.jneumeth.2007.03.024. 112 Matsui, T., Hattori, Y., Tsumura, K., Aoki, R., Takeda, M., Nakahara, K., and Jimura, 113 K. (2022). Executive control by fronto-parietal activity explains counterintuitive114 decision behavior in complex value-based decision-making. Neuroimage 249,115 118892. 10.1016/j.neuroimage.2022.118892.116 Miyakawa, N., Majima, K., Sawahata, H., Kawasaki, K., Matsuo, T., Kotake, N., 117 Suzuki, T., Kamitani, Y., and Hasegawa, I. (2018). Heterogeneous Redistribution of 118 Facial Subcategory Information Within and Outside the Face-Selective Domain in 119 Primate Inferior Temporal Cortex. Cereb Cortex 28, 1416-1431. 120 10.1093/cercor/bhx342. 121 Miyashita, Y., and Chang, H.S. (1988). Neuronal correlate of pictorial short-term 122 memory in the primate temporal cortex. Nature 331, 68-70. 10.1038/331068a0. 123 Nestor, A., Plaut, D.C., and Behrmann, M. (2011). Unraveling the distributed neural 124 code of facial identity through spatiotemporal pattern analysis. Proc Natl Acad Sci U 125 S A 108, 9998-10003. 10.1073/pnas.1102433108. 126 Peelen, M.V., and Downing, P.E. (2017). Category selectivity in human visual cortex: 127 Beyond visual object recognition. Neuropsychologia 105, 177-183. 128 10.1016/j.neuropsychologia.2017.03.033. 129 Raschka, S. (2020). Model Evaluation, Model Selection, and Algorithm Selection in 130 Machine Learning. arXiv:1811.12808v3. 10.48550/arXiv.1811.12808. 131 Rokach, L. (2009). Ensemble-based classifiers. Artificial Intelligence Review 33, 1-39. 132 10.1007/s10462-009-9124-7. 133 Rousselet, G.A., Fabre-Thorpe, M., and Thorpe, S.J. (2002). Parallel processing in high-134 level categorization of natural images. Nat Neurosci 5, 629-630. 10.1038/nn866. 135 Sarraf, S., DeSouza, D.D., Anderson, J., and Tofighi, G. (2017). DeepAD: Alzheimer\u2019s 136 Disease Classification via Deep Convolutional Neural Networks using MRI and 137 fMRI. bioRxiv. 10.1101/070441. 138 Schiltz, C., Sorger, B., Caldara, R., Ahmed, F., Mayer, E., Goebel, R., and Rossion, B. 139 (2006). Impaired face discrimination in acquired prosopagnosia is associated with 140 abnormal response to individual faces in the right middle fusiform gyrus. Cereb 141 Cortex 16, 574-586. 10.1093/cercor/bhj005. 142\nSelvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., and Batra, D. (2019). 143 Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based 144 Localization. International Journal of Computer Vision 128, 336-359. 145 10.1007/s11263-019-01228-7. 146 Serre, T., Oliva, A., and Poggio, T. (2007). A feedforward architecture accounts for 147 rapid categorization. Proc Natl Acad Sci U S A 104, 6424-6429. 148 10.1073/pnas.0700622104. 149 Simonyan, K., and Zisserman, A. (2015). Very Deep Convolutional Networks for 150 Large-Scale Image Recognition. ICLR2015. 10.48550/arXiv.1409.1556. 151 Stigliani, A., Weiner, K.S., and Grill-Spector, K. (2015). Temporal Processing Capacity 152 in High-Level Visual Cortex Is Domain Specific. J Neurosci 35, 12412-12424. 153 10.1523/JNEUROSCI.4822-14.2015. 154 Supratak, A., Dong, H., Wu, C., and Guo, Y. (2017). DeepSleepNet: A Model for 155 Automatic Sleep Stage Scoring Based on Raw Single-Channel EEG. IEEE Trans 156 Neural Syst Rehabil Eng 25, 1998-2008. 10.1109/TNSRE.2017.2721116. 157 Suzuki, W.A., and Naya, Y. (2014). The perirhinal cortex. Annu Rev Neurosci 37, 39-158 53. 10.1146/annurev-neuro-071013-014207. 159 Takeda, M., Hirabayashi, T., Adachi, Y., and Miyashita, Y. (2018). Dynamic laminar 160 rerouting of inter-areal mnemonic signal by cognitive operations in primate temporal 161 cortex. Nat Commun 9, 4629. 10.1038/s41467-018-07007-1. 162 Tanaka, D., Aoki, R., Suzuki, S., Takeda, M., Nakahara, K., and Jimura, K. (2020). 163 Self-Controlled Choice Arises from Dynamic Prefrontal Signals That Enable Future 164 Anticipation. J Neurosci 40, 9736-9750. 10.1523/JNEUROSCI.1702-20.2020. 165 Tanaka, K. (1996). Inferotemporal cortex and object vision. Annu Rev Neurosci 19, 166 109-139. 10.1146/annurev.ne.19.030196.000545. 167 Tsao, D.Y., Freiwald, W.A., Knutsen, T.A., Mandeville, J.B., and Tootell, R.B. (2003). 168 Faces and objects in macaque cerebral cortex. Nat Neurosci 6, 989-995. 169 10.1038/nn1111. 170 Tsao, D.Y., Freiwald, W.A., Tootell, R.B., and Livingstone, M.S. (2006). A cortical 171 region consisting entirely of face-selective cells. Science 311, 670-674. 172 10.1126/science.1119983. 173 Tsumura, K., Aoki, R., Takeda, M., Nakahara, K., and Jimura, K. (2021a). Cross-174 Hemispheric Complementary Prefrontal Mechanisms during Task Switching under 175 Perceptual Uncertainty. J Neurosci 41, 2197-2213. 10.1523/JNEUROSCI.2096-176 20.2021. 177\nTsumura, K., Kosugi, K., Hattori, Y., Aoki, R., Takeda, M., Chikazoe, J., Nakahara, K., 178 and Jimura, K. (2021b). Reversible Fronto-occipitotemporal Signaling Complements 179 Task Encoding and Switching under Ambiguous Cues. Cereb Cortex, bhab324. 180 10.1093/cercor/bhab324. 181 Turesson, H.K., Logothetis, N.K., and Hoffman, K.L. (2012). Category-selective phase 182 coding in the superior temporal sulcus. Proc Natl Acad Sci U S A 109, 19438-19443. 183 10.1073/pnas.1217012109. 184 Vu, H., Kim, H.C., Jung, M., and Lee, J.H. (2020). fMRI volume classification using a 185 3D convolutional neural network robust to shifted and scaled neuronal activations. 186 Neuroimage 223, 117328. 10.1016/j.neuroimage.2020.117328. 187 Wang, X., Liang, X., Jiang, Z., Nguchu, B.A., Zhou, Y., Wang, Y., Wang, H., Li, Y., 188 Zhu, Y., Wu, F., et al. (2020). Decoding and mapping task states of the human brain 189 via deep learning. Hum Brain Mapp 41, 1505-1519. 10.1002/hbm.24891. 190 Willenbockel, V., Sadr, J., Fiset, D., Horne, G.O., Gosselin, F., and Tanaka, J.W. 191 (2010). Controlling low-level image properties: the SHINE toolbox. Behav Res 192 Methods 42, 671-684. 10.3758/BRM.42.3.671. 193 Worsley, K.J., and Friston, K.J. (1995). Analysis of fMRI time-series revisited--again. 194 Neuroimage 2, 173-181. 10.1006/nimg.1995.1023. 195 Yamins, D.L., and DiCarlo, J.J. (2016). Using goal-driven deep learning models to 196 understand sensory cortex. Nat Neurosci 19, 356-365. 10.1038/nn.4244. 197 Yarkoni, T., Poldrack, R.A., Nichols, T.E., Van Essen, D.C., and Wager, T.D. (2011). 198 Large-scale automated synthesis of human functional neuroimaging data. Nat 199 Methods 8, 665-670. 10.1038/nmeth.1635. 200 Yu, F., Koltun, V., and Funkhouser, T. (2017). Dilated Residual Networks. CVPR 2017, 201 636-644. 10.1109/cvpr.2017.75.202 Zhang, Z., Yu, X., Rong, X., and Iwata, M. (2021). Spatial-Temporal Neural Network 203 for P300 Detection. IEEE Access 9, 163441-163455. 10.1109/access.2021.3132024. 204 Zhao, Y., Dong, Q., Zhang, S., Zhang, W., Chen, H., Jiang, X., Guo, L., Hu, X., Han, J., 205 and Liu, T. (2018). Automatic Recognition of fMRI-Derived Functional Networks 206 Using 3-D Convolutional Neural Networks. IEEE Trans Biomed Eng 65, 1975-1984. 207 10.1109/TBME.2017.2715281. 208 Zheng, X., Mondloch, C.J., and Segalowitz, S.J. (2012). The timing of individual face 209 recognition in the brain. Neuropsychologia 50, 1451-1461. 210 10.1016/j.neuropsychologia.2012.02.030. 211 212"
        }
    ],
    "title": "Multimodal deep neural decoding of visual object representation in humans",
    "year": 2022
}