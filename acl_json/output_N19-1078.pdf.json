{
    "abstractText": "Contextual string embeddings are a recent type of contextualized word embedding that were shown to yield state-of-the-art results when utilized in a range of sequence labeling tasks. They are based on character-level language models which treat text as distributions over characters and are capable of generating embeddings for any string of characters within any textual context. However, such purely character-based approaches struggle to produce meaningful embeddings if a rare string is used in a underspecified context. To address this drawback, we propose a method in which we dynamically aggregate contextualized embeddings of each unique string that we encounter. We then use a pooling operation to distill a global word representation from all contextualized instances. We evaluate these pooled contextualized embeddings on common named entity recognition (NER) tasks such as CoNLL-03 and WNUT and show that our approach significantly improves the state-of-the-art for NER. We make all code and pre-trained models available to the research community for use and reproduction.",
    "authors": [
        {
            "affiliations": [],
            "name": "Alan Akbik"
        },
        {
            "affiliations": [],
            "name": "Tanja Bergmann"
        },
        {
            "affiliations": [],
            "name": "Roland Vollgraf"
        }
    ],
    "id": "SP:6fa04b5110754dfb3b7e8a99ede56ffdfba64e8d",
    "references": [
        {
            "authors": [
                "Gustavo Aguilar",
                "Adrian Pastor Lopez Monroy",
                "Fabio Gonz\u00e1lez",
                "Thamar Solorio."
            ],
            "title": "Modeling noisiness to recognize named entities using multitask neural networks on social media",
            "venue": "Proceedings of the 2018 Conference of the North American",
            "year": 2018
        },
        {
            "authors": [
                "Alan Akbik",
                "Tanja Bergmann",
                "Duncan Blythe",
                "Kashif Rasul",
                "Stefan Schweter",
                "Roland Vollgraf."
            ],
            "title": "Flair: An easy-to-use framework for state-of-theart nlp",
            "venue": "NAACL, 2019 Annual Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Alan Akbik",
                "Duncan Blythe",
                "Roland Vollgraf."
            ],
            "title": "Contextual string embeddings for sequence labeling",
            "venue": "COLING 2018, 27th International Conference on Computational Linguistics, pages 1638\u2013 1649.",
            "year": 2018
        },
        {
            "authors": [
                "Piotr Bojanowski",
                "Edouard Grave",
                "Armand Joulin",
                "Tomas Mikolov."
            ],
            "title": "Enriching word vectors with subword information",
            "venue": "Transactions of the Association for Computational Linguistics, 5:135\u2013146.",
            "year": 2017
        },
        {
            "authors": [
                "Kevin Clark",
                "Minh-Thang Luong",
                "Christopher D. Manning",
                "Quoc V. Le."
            ],
            "title": "Semi-supervised sequence modeling with cross-view training",
            "venue": "EMNLP.",
            "year": 2018
        },
        {
            "authors": [
                "Leon Derczynski",
                "Eric Nichols",
                "Marieke van Erp",
                "Nut Limsopatham."
            ],
            "title": "Results of the wnut2017 shared task on novel and emerging entity recognition",
            "venue": "Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 140\u2013147.",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation, 9(8):1735\u20131780.",
            "year": 1997
        },
        {
            "authors": [
                "Zhiheng Huang",
                "Wei Xu",
                "Kai Yu."
            ],
            "title": "Bidirectional lstm-crf models for sequence tagging",
            "venue": "arXiv preprint arXiv:1508.01991.",
            "year": 2015
        },
        {
            "authors": [
                "Guillaume Lample",
                "Miguel Ballesteros",
                "Sandeep Subramanian",
                "Kazuya Kawakami",
                "Chris Dyer."
            ],
            "title": "Neural architectures for named entity recognition",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
            "year": 2016
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Ilya Sutskever",
                "Kai Chen",
                "Greg S Corrado",
                "Jeff Dean."
            ],
            "title": "Distributed representations of words and phrases and their compositionality",
            "venue": "Advances in neural information processing systems, pages 3111\u20133119.",
            "year": 2013
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher Manning."
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543.",
            "year": 2014
        },
        {
            "authors": [
                "Matthew Peters",
                "Waleed Ammar",
                "Chandra Bhagavatula",
                "Russell Power."
            ],
            "title": "Semi-supervised sequence tagging with bidirectional language models",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2017
        },
        {
            "authors": [
                "Matthew E. Peters",
                "Mark Neumann",
                "Mohit Iyyer",
                "Matt Gardner",
                "Christopher Clark",
                "Kenton Lee",
                "Luke Zettlemoyer."
            ],
            "title": "Deep contextualized word representations",
            "venue": "Proc. of NAACL.",
            "year": 2018
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
            "year": 2017
        },
        {
            "authors": [
                "Erik F Tjong Kim Sang",
                "Fien De Meulder."
            ],
            "title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
            "venue": "Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4,",
            "year": 2003
        }
    ],
    "sections": [
        {
            "text": "Proceedings of NAACL-HLT 2019, pages 724\u2013728 Minneapolis, Minnesota, June 2 - June 7, 2019. c\u00a92019 Association for Computational Linguistics\n724"
        },
        {
            "heading": "1 Introduction",
            "text": "Word embeddings are a crucial component in many NLP approaches (Mikolov et al., 2013; Pennington et al., 2014) since they capture latent semantics of words and thus allow models to better train and generalize. Recent work has moved away from the original \u201cone word, one embedding\u201d paradigm to investigate contextualized embedding models (Peters et al., 2017, 2018; Akbik et al., 2018). Such approaches produce different embeddings for the same word depending on its context and are thus capable of capturing latent contextualized semantics of ambiguous words.\nRecently, Akbik et al. (2018) proposed a character-level contextualized embeddings ap-\nproach they refer to as contextual string embeddings. They leverage pre-trained character-level language models from which they extract hidden states at the beginning and end character positions of each word to produce embeddings for any string of characters in a sentential context. They showed these embeddings to yield state-of-the-art results when utilized in sequence labeling tasks such as named entity recognition (NER) or part-of-speech (PoS) tagging. Underspecified contexts. However, such contextualized character-level models suffer from an inherent weakness when encountering rare words in an underspecified context. Consider the example text segment shown in Figure 1: \u201cFung Permadi (Taiwan) v Indra\u201d, from the English CONLL-03 test data split (Tjong Kim Sang and De Meulder, 2003). If we consider the word \u201cIndra\u201d to be rare (meaning no prior occurrence in the corpus used to generate word embeddings), the underspecified context allows this word to be interpreted as either a person or an organization. This leads to an underspecified embedding that ultimately causes an incorrect classification of \u201cIndra\u201d as an organization in a downstream NER task. Pooled Contextual Embeddings. In this paper, we present a simple but effective approach to address this issue. We intuit that entities are normally only used in underspecified contexts if they are expected to be known to the reader. That is, they are either more clearly introduced in an earlier sentence, or part of general in-domain knowl-\n2\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nNAACL-HLT 2019 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.\nfigure2-crop.pdf\nFigure 2: PLACEHOLDER Illustration of character-level RNN language model.\nedge a reader is expected to have. Indeed, the\nstring \u201cIndra\u201d in the CONLL-03 data also occurs\nin the earlier sentence \u201cIndra Wijaya (Indonesia)\nbeat Ong Ewe Hock\u201d. Based on this, we propose\nan approach in which we dynamically aggregate\ncontextualized embeddings of each unique string that we encounter as we process a dataset. We then use a pooling operation to distill a global word representation from all contextualized instances that we use in combination with the current contextualized representation as new word embedding.\nWe evaluate our proposed embedding approach on the task of named entity recognition on the CONLL-03 (English, German and Dutch) and WNUT datasets. In all cases, we find that our approach outperforms previous approaches and yields new state-of-the-art scores. We contribute our approach and all pre-trained models to the open source FLAIR1 framework, to ensure reproducibility of these results.\n2 Method\nOur proposed approach dynamically builds up a \u201cmemory\u201d of contextualized embeddings and applies a pooling operation to distill a global contextualized embedding for each word. It requires an embed() function that produces a contextualized embedding for a given word in a sentence context (see Akbik et al. (2018)). It also requires a memory that records for each unique word all previous contextual embeddings, and a pool() operation to pool embedding vectors.\nThis is illustrated in Algorithm 1: to embed a word (in a sentential context), we first call the embed() function (line 2) and add the resulting\n1https://github.com/zalandoresearch/flair\nembedding to the memory for this word (line 3).\nWe then call the pooling operation over all contex-\ntualized embeddings for this word in the memory\n(line 4) to compute the pooled contextualized em-\nbedding. Finally, we concatenate the original con-\ntextual embedding together with the pooled representation, to ensure that both local and global interpretations are represented (line 5). This means that the resulting pooled contextualized embedding has twice the dimensionality of the original embedding.\nAlgorithm 1 Compute pooled embedding Input: sentence, memory\n1: for word in sentence do 2: embcontext \u2190 embed(word) within sentence 3: add embcontext to memory[word] 4: embpooled \u2190 pool(memory[word]) 5: word.embedding \u2190 concat(embpooled, embcontext) 6: end for\nCrucially, our approach expands the memory each time we embed a word. Therefore, the same word in the same context may have different embeddings over time as the memory is built up. Pooling operations. Per default, we use mean pooling to average a word\u2019s contextualized embedding vectors. We also experiment with min and max pooling to compute a vector consisting of all element-wise minimum or maximum values. Training downstream models. When training downstream task models (such as for NER), we typically make many passes over the training data. As Algorithm 2 shows, we reset the memory at the beginning of each pass over the training data (line\nembcontext Indra2\nI n d r a W i j a y a b e a t O n g E w e\nembcontext Indra3\nA n d I n d r a s a i d t h a t . . .\nmemory\nIndraembproposed\nF u n g P e r m a d i v I n d r a\nCharacter Language Model\nembcontext Indra1\npooling concatenation\ncurrent sentence\nFigure 2: Example of how we generate our proposed embedding (embproposed) for the word \u201cIndra\u201d in the example text segment \u201cFung Permadi v I dra\u201d. We extr ct a contextu l string embedding (embcontext) for this word and retrieve from the memory all emb ddings that were produced for this string o previous sentences. We pool and concatenate all local contextualized embeddings to produce the final embedding.\nedge a read r is expected to have. Indeed, the string \u201cIndra\u201d in the CONLL-03 data also ccurs in the earlier sentence \u201cIndra Wijaya (Indonesia) beat Ong Ewe Hock\u201d.\nBased on this, we propose an approach in which we dynamically aggregate contextualized embeddings of each unique string that we enco nter as we process a dataset. We then u a pooling operation to distill a global word representation from all contextualized instances that we use in combination with the current contextualized representation as new word embedding. Our approach thus produces evolving word representations that change over time as more instances of the same word are observed in the data.\nWe evaluate our proposed embedding approach on the task of named entity recognition on the CONLL-03 (English, German and Dutch) and WNUT datasets. In all cases, we find that our approach outperforms previous approaches and yields new state-of-the-art scores. We contribute our approach and all pre-trained models to the open source FLAIR1 framework (Akbik et al., 2019), to en ure reproducibility of these results."
        },
        {
            "heading": "2 Method",
            "text": "Our proposed approach (see Figure 2) dynamically builds up a \u201cmemory\u201d of contextualized embeddings and applies a pooling operation to distill a global contextualized embedding for each word. It requires an embed() function that produces a contextualized embedding for a given word in a\n1https://github.com/zalandoresearch/flair\nen e ce context (see Akbik et al. (2018)). It also requires a m mory that records for each unique word all previous contextual embeddings, and a pool() operation to pool embedding vectors.\nThis is illustrated in Algorithm 1: to embed a word (in a sentential context), we first call the embed() function (line 2) and add the resulting embedding to the memory for this word (line 3). We th n call the pooling operation over all contextualized embeddings for this word in the memory (line 4) to compute the pooled contextualized embedding. Finally, we concatenate the original contextual embedding together with the pooled representation, to ensure that both local and global interpretations are represented (line 5). This means that the res lting pooled contextualized embedding has twice the dimensionality of the original embedding.\nAlgorithm 1 Compute pooled embedding Input: sentence, memory\n1: for word in sentence do 2: embcontext \u2190 embed(word) within sentence 3: add embcontext to memory[word] 4: embpooled\u2190 pool(memory[word]) 5: w rd.embedding\u2190 concat(embpooled, embcontext) 6: end for\nPooling operations. We experiment with different pooling operations: mean pooling to average a word\u2019s contextualized embedding vectors, and min and max pooling to compute a vector of all\nelement-wise minimum or maximum values. Training downstream models. When training downstream task models (such as for NER), we typically make many passes over the training data. As Algorithm 2 shows, we reset the memory at the beginning of each pass over the training data (line 2), so that it is build up from scratch at each epoch.\nAlgorithm 2 Training 1: for epoch in epochs do 2: memory\u2190 map of word to list 3: train and evaluate as usual 4: end for\nThis approach ensures that the downstream task model learns to leverage pooled embeddings that are built up (e.g. evolve) over time. It also ensures that pooled embeddings during training are only computed over training data. After training, (i.e. during NER prediction), we do not reset embeddings and instead allow our approach to keep expanding the memory and evolve the embeddings."
        },
        {
            "heading": "3 Experiments",
            "text": "We verify our proposed approach in four named entity recognition (NER) tasks: We use the English, German and Dutch evaluation setups of the CONLL-03 shared task (Tjong Kim Sang and De Meulder, 2003) to evaluate our approach on classic newswire data, and the WNUT-17 task on emerging entity detection (Derczynski et al., 2017) to evaluate our approach in a noisy user-generated data setting with few repeated entity mentions."
        },
        {
            "heading": "3.1 Experimental Setup",
            "text": "We use the open source FLAIR framework in all our experiments. It implements the standard BiLSTM-CRF sequence labeling architecture (Huang et al., 2015) and includes pre-trained\ncontextual string embeddings for many languages. To FLAIR, we add an implementation of our proposed pooled contextualized embeddings. Hyperparameters. For our experiments, we follow the training and evaluation procedure outlined in Akbik et al. (2018) and follow most hyperparameter suggestions as given by the in-depth study presented in Reimers and Gurevych (2017). That is, we use an LSTM with 256 hidden states and one layer (Hochreiter and Schmidhuber, 1997), a locked dropout value of 0.5, a word dropout of 0.05, and train using SGD with an annealing rate of 0.5 and a patience of 3. We perform model selection over the learning rate \u2208 {0.01, 0.05, 0.1} and mini-batch size \u2208 {8, 16, 32}, choosing the model with the best F-measure on the validation set. Following Peters et al. (2017), we then repeat the experiment 5 times with different random seeds, and train using both train and development set, reporting both average performance and standard deviation over these runs on the test set as final performance. Standard word embeddings. The default setup of Akbik et al. (2018) recommends contextual string embeddings to be used in combination with standard word embeddings. We use GLOVE embeddings (Pennington et al., 2014) for the English tasks and FASTTEXT embeddings (Bojanowski et al., 2017) for all newswire tasks. Baselines. Our baseline are contextual string embeddings without pooling, i.e. the original setup proposed in Akbik et al. (2018)2. By comparing against this baseline, we isolate the impact of our proposed pooled contextualized embeddings.\n2Our reproduced numbers are slightly lower than we reported in Akbik et al. (2018) where we used the official CONLL-03 evaluation script over BILOES tagged entities. This introduced errors since this script was not designed for S-tagged entities.\nIn addition, we list the best reported numbers for the four tasks. This includes the recent BERT approach using bidirectional transformers by Devlin et al. (2018), the semi-supervised multitask learning approach by Clark et al. (2018), the ELMo word-level language modeling approach by Peters et al. (2018), and the best published numbers for WNUT-17 (Aguilar et al., 2018) and German and Dutch CONLL-03 (Lample et al., 2016)."
        },
        {
            "heading": "3.2 Results",
            "text": "Our experimental results are summarized in Table 1 for each of the four tasks. New state-of-the-art scores. We find that our approach outperforms all previously published results, raising the state-of-the-art for CONLL-03 on English to 93.18 F1-score (\u21910.32 pp vs. previous best), German to 88.27 (\u21910.86 pp) and Dutch to 90.44 (\u21910.28 pp). The consistent improvements against the contextual string embeddings baseline indicate that our approach is generally a viable option for embedding entities in sequence labeling. Less pronounced impact on WNUT-17. However, we also find no significant improvements on the WNUT-17 task on emerging entities. Depending on the pooling operation, we find comparable results to the baseline. This result is expected since most entities appear only few times in this dataset, giving our approach little evidence to aggregate and pool. Nevertheless, since recent work has not yet experimented with contextual embeddings on WNUT, as side result we report a new state-of-the-art of 49.59 F1 vs. the previous best reported number of 45.55 (Aguilar et al., 2018). Pooling operations. Comparing the pooling operations discussed in Section 2, we generally find similar results. As Table 1 shows, min pooling performs best for English and German CoNLL, while mean pooling is best for Dutch and WNUT."
        },
        {
            "heading": "3.3 Ablation: Character Embeddings Only",
            "text": "To better isolate the impact of our proposed approach, we run experiments in which we do not use any classic word embeddings, but rather rely solely on contextual string embeddings. As Table 2 shows, we observe more pronounced im-\nprovements of pooling vis-a-vis the baseline approach in this setup. This indicates that pooled contextualized embeddings capture global semantics words similar in nature to classical word embeddings."
        },
        {
            "heading": "4 Discussion and Conclusion",
            "text": "We presented a simple but effective approach that addresses the problem of embedding rare strings in underspecified contexts. Our experimental evaluation shows that this approach improves the stateof-the-art across named entity recognition tasks, enabling us to report new state-of-the-art scores for CONLL-03 NER and WNUT emerging entity detection. These results indicate that our embedding approach is well suited for NER. Evolving embeddings. Our dynamic aggregation approach means that embeddings for the same words will change over time, even when used in exactly the same contexts. Assuming that entity names are more often used in well-specified contexts, their pooled embeddings will improve as more data is processed. The embedding model thus continues to \u201clearn\u201d from data even after the training of the downstream NER model is complete and it is used in prediction mode. We consider this idea of constantly evolving representations a very promising research direction. Future work. Our pooling operation makes the conceptual simplification that all previous instances of a word are equally important. However, we may find more recent mentions of a word - such as words within the same document or news cycle - to be more important for creating embeddings than mentions that belong to other documents or news cycles. Future work will therefore examine methods to learn weighted poolings of previous mentions. We will also investigate applicability of our proposed embeddings to tasks beside NER. Public release. We contribute our code to the FLAIR framework3. This allows full reproduction of all experiments presented in this paper, and al-\n3The proposed embedding is added to FLAIR in release 0.4.1. as the PooledFlairEmbeddings class (see Akbik et al. (2019) for more details).\nlows the research community to use our embeddings for training downstream task models."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank the anonymous reviewers for their\nhelpful comments. This project has received funding from the\nEuropean Union\u2019s Horizon 2020 research and innovation pro-\ngramme under grant agreement no 732328 (\u201cFashionBrain\u201d)."
        }
    ],
    "title": "Pooled Contextualized Embeddings for Named Entity Recognition",
    "year": 2019
}