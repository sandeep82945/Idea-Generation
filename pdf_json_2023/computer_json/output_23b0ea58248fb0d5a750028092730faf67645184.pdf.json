{
    "abstractText": "The future of automated driving (AD) is rooted in the development of robust, fair and explainable artificial intelligence methods. Upon request, automated vehicles must be able to explain their decisions to the driver and the car passengers, to the pedestrians and other vulnerable road users and potentially to external auditors in case of accidents. However, nowadays, most explainable methods still rely on quantitative analysis of the AD scene representations captured by multiple sensors. This paper proposes a novel representation of AD scenes, called Qualitative eXplainable Graph (QXG), dedicated to qualitative spatiotemporal reasoning of long-term scenes. The construction of this graph exploits the recent Qualitative Constraint Acquisition paradigm. Our experimental results on NuScenes, an open real-world multi-modal dataset, show that the qualitative explainable graph of an AD scene composed of 40 frames can be computed in real-time and light in space storage which makes it a potentially interesting tool for improved and more trustworthy perception and control processes in AD.",
    "authors": [
        {
            "affiliations": [],
            "name": "Nassim Belmecheri"
        },
        {
            "affiliations": [],
            "name": "Nadjib Lazaar"
        },
        {
            "affiliations": [],
            "name": "Helge Spieker"
        }
    ],
    "id": "SP:1f6485ec48d2672392ffcab44a672158f8814dfe",
    "references": [
        {
            "authors": [
                "James F. Allen"
            ],
            "title": "Maintaining knowledge about temporal intervals",
            "venue": "Communications of the ACM,",
            "year": 1983
        },
        {
            "authors": [
                "Shahin Atakishiyev",
                "Mohammad Salameh",
                "Hengshuai Yao",
                "Randy Goebel"
            ],
            "title": "Explainable artificial intelligence for autonomous driving: A comprehensive overview and field guide for future research",
            "year": 2023
        },
        {
            "authors": [
                "P. Balbiani",
                "J.-F Condotta",
                "L. Cerro"
            ],
            "title": "A new tractable subclass of the rectangle algebra",
            "venue": "IJCAI International Joint Conference on Artificial Intelligence,",
            "year": 2000
        },
        {
            "authors": [
                "Alejandro Barrera",
                "Carlos Guindel",
                "Jorge Beltr\u00e1n",
                "Fernando Garc\u0131\u0301a"
            ],
            "title": "Birdnet+: End-to-end 3d object detection in lidar bird\u2019s eye view",
            "venue": "IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC),",
            "year": 2020
        },
        {
            "authors": [
                "Mohamed-Bachir Belaid",
                "Nassim Belmecheri",
                "Arnaud Gotlieb",
                "Nadjib Lazaar",
                "Helge Spieker"
            ],
            "title": "GEQCA: generic qualitative constraint acquisition",
            "venue": "In Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI),",
            "year": 2022
        },
        {
            "authors": [
                "Holger Caesar",
                "Varun Bankiti",
                "Alex H. Lang",
                "Sourabh Vora",
                "Venice Erin Liong",
                "Qiang Xu",
                "Anush Krishnan",
                "Yu Pan",
                "Giancarlo Baldan",
                "Oscar Beijbom"
            ],
            "title": "nuScenes: A Multimodal Dataset for Autonomous Driving",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Xinli Geng",
                "Huawei Liang",
                "Biao Yu",
                "Pan Zhao",
                "Liuwei He",
                "Rulin Huang"
            ],
            "title": "A scenario-adaptive driving behavior prediction approach to urban autonomous driving",
            "venue": "Applied Sciences,",
            "year": 2017
        },
        {
            "authors": [
                "Ross Girshick",
                "Jeff Donahue",
                "Trevor Darrell",
                "Jitendra Malik"
            ],
            "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2014
        },
        {
            "authors": [
                "Hua Hua",
                "Dongxu Li",
                "Ruiqi Li",
                "Peng Zhang",
                "Jochen Renz",
                "Anthony Cohn"
            ],
            "title": "Towards explainable action recognition by salient qualitative spatial object relation chains",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 36:5710\u20135718,",
            "year": 2022
        },
        {
            "authors": [
                "Jinkyu Kim",
                "Anna Rohrbach",
                "Zeynep Akata",
                "Suhong Moon",
                "Teruhisa Misu",
                "Yi-Ting Chen",
                "Trevor Darrell",
                "John Canny"
            ],
            "title": "Towards Explainable and Advisable Model for Self-driving Cars",
            "venue": "Applied AI Letters,",
            "year": 2021
        },
        {
            "authors": [
                "Jinkyu Kim",
                "Anna Rohrbach",
                "Trevor Darrell",
                "John Canny",
                "Zeynep Akata"
            ],
            "title": "Textual explanations for self-driving vehicles",
            "venue": "In European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "B Ravi Kiran",
                "Ibrahim Sobh",
                "Victor Talpaert",
                "Patrick Mannion",
                "Ahmad A. Al Sallab",
                "Senthil Yogamani",
                "Patrick P\u00e9rez"
            ],
            "title": "Deep reinforcement learning for autonomous driving: A survey",
            "venue": "IEEE Transactions on Intelligent Transportation Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yin Li",
                "Chun Huang",
                "Ram Nevatia"
            ],
            "title": "Learning to associate: Hybridboosted multi-target tracker for crowded scene",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2009
        },
        {
            "authors": [
                "Wei Liu",
                "Dragomir Anguelov",
                "Dumitru Erhan",
                "Christian Szegedy",
                "Scott Reed"
            ],
            "title": "Ssd: Single shot multibox detector",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2016
        },
        {
            "authors": [
                "Xuyang Lu",
                "Yang Gao"
            ],
            "title": "Guide and interact: scene-graph based generation and control of video captions",
            "venue": "Multimedia Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Roger D. Maddux"
            ],
            "title": "Relation algebras for reasoning about time and space",
            "venue": "In Algebraic Methodology and Software Technology",
            "year": 1994
        },
        {
            "authors": [
                "Anton Milan",
                "Laura Leal-Taix\u2019e",
                "Ian Reid",
                "Stefan Roth",
                "Konrad Schindler"
            ],
            "title": "Mot16: A benchmark for multi-object tracking",
            "venue": "In Computer Vision and Pattern Recognition",
            "year": 2016
        },
        {
            "authors": [
                "Khan Muhammad",
                "Amin Ullah",
                "Jaime Lloret",
                "Javier Del Ser",
                "Victor Hugo C. de Albuquerque"
            ],
            "title": "Deep learning for safe autonomous driving: Current challenges and future directions",
            "venue": "IEEE Transactions on Intelligent Transportation Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Ilja Nastjuk",
                "Bernd Herrenkind",
                "Mauricio Marrone",
                "Alfred Brendel",
                "Lutz Kolbe"
            ],
            "title": "What drives the acceptance of autonomous driving? An investigation of acceptance factors from an end-user\u2019s perspective",
            "venue": "Technological Forecasting and Social Change,",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Omeiza",
                "Helena Webb",
                "Marina Jirotka",
                "Lars Kunze"
            ],
            "title": "Explanations in Autonomous Driving: A Survey",
            "venue": "IEEE Transactions on Intelligent Transportation Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Joseph Redmon",
                "Santosh Divvala",
                "Ross Girshick",
                "Ali Farhadi"
            ],
            "title": "You only look once: Unified, real-time object detection",
            "venue": "In Computer Vision and Pattern Recognition",
            "year": 2016
        },
        {
            "authors": [
                "J. Renz",
                "B. Nebel"
            ],
            "title": "Efficient Methods for Qualitative Spatial Reasoning",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2001
        },
        {
            "authors": [
                "Jakob Suchan",
                "Mehul Bhatt",
                "Srikrishna Varadarajan"
            ],
            "title": "Commonsense visual sensemaking for autonomous driving \u2013 On generalised neurosymbolic online abduction integrating vision and semantics",
            "venue": "Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Lukas Westhofen",
                "Christian Neurohr",
                "Martin Butz",
                "Maike Scholtes",
                "Michael Schuldes"
            ],
            "title": "Using ontologies for the formalization and recognition of criticality for automated driving, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Yiran Xu",
                "Xiaoyin Yang",
                "Lihang Gong",
                "Hsuan-Chu Lin",
                "Tz-Ying Wu",
                "Yunsheng Li",
                "Nuno Vasconcelos"
            ],
            "title": "Explainable Object-Induced Action Decision for Autonomous Vehicles",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Dan Zhang",
                "Huchuan Lu",
                "Ming-Hsuan Yang"
            ],
            "title": "Robust visual tracking via structured multi-task sparse learning",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2012
        },
        {
            "authors": [
                "Xizhe Zhang",
                "Siddartha Khastgir",
                "Paul Jennings"
            ],
            "title": "Scenario Description Language for Automated Driving Systems: A Two Level Abstraction Approach",
            "venue": "IEEE International Conference on Systems, Man, and Cybernetics (SMC),",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION\nThe development of Automated Vehicles (AVs) has considerably accelerated these last years with the uptake of Artificial Intelligence (AI) methods and Deep Learning (DL) models. For instance, the first level-3 automated system allowing driver\u2019s hands-off has recently been released in Europe1. With the wide adoption of AI, it has become crucial to explain AV automated perception and control mechanisms [20], especially those which are constructed with opaque DL models such as CNNs, RNNs or transformers [12], [18], [25]. In fact, societal acceptance of AVs significantly depends on these AI models\u2019 trustworthiness, transparency and reliability [19].\nRecent years have witnessed flourishing advances in Explainable AI methods dedicated to AVs. According to [2], they can be classified in three main categories: 1) Vision-based explanations are concerned with determin-\ning which portions of an image influence an AV controller to take an action [20]; 2) Feature importance scores indicate how much each input contributes to the prediction of the model quantitatively; 3) Textual-based explanations provide intelligible arguments to document AV decisions, using a natural language [10];\nThis work has received support from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 101076911 (AI4CCAM Project)\n1Simula Research Laboratory, Oslo, Norway {nassim,arnaud,helge}@simula.no\n2LIRMM, U. of Montpellier, CNRS, Montpellier, France lazaar@lirmm.fr\n1etsc.eu/europes-first-cars-with-level-3-auto... -go-on-sale-in-germany/\nRegarding the latter category, even though advanced explanations of driving scenes have been proposed in the literature, there is a lack of automated methods capable of interpreting long-term sequences without the support of human mentors or annotated datasets [25]. Actually, automated support for scene explanation based on multi-sensors and videos is still restricted to quantitative analysis (e.g., saliency heatmaps) [20].\nThis paper proposes a novel representation of AD scenes called Qualitative eXplainable Graph (QXG) dedicated to qualitative spatiotemporal reasoning for scene description and interpretation. By using a recent framework named Generic Qualitative Constraint Acquisition (GEQCA) introduced in [5], which can extract qualitative constraints among identified entities in scenes, we propose a light representation of the spatial and temporal relations for an entire scene. Using the multimodal real-world NuScenes dataset [6], we show that QXGs can be computed in realtime for each available LiDAR and camera sensor of an AV ; Thus, they can be used to support its perception and control mechanism. The possible applications of this graph include scene description and interpretation to support perception and control systems, the generation of driving scenarios in description languages and goal recognition (to infer the goal of other vehicles). However, this paper\u2019s scope is restricted to the QXG construction and, even though its exploitation is illustrated in an example (in Sec. II), its usefulness is not fully demonstrated here.\nTo sum up, the contribution of this paper is three-fold:\n1) We introduce a novel symbolic representation of spatiotemporal relations of driving scenes, dedicated to the perception and control mechanisms of AVs. That representation is sufficiently light to efficiently encode scenes containing 40 frames and up to 300 entities per scene; 2) We present a time- and space-polynomial complexity algorithm which constructs QXGs (cf. Algorithm 2 - QXG-BUILDER). That algorithm uses Generic Qualitative Constraint Acquisition [5] which is parametrized by Rectangle Algebra [3]. This algorithm is instrumental to ensure the exploitation of QXGs in realtime; 3) By using the multimodal NuScenes dataset [6], we show that QXGs can indeed be constructed or updated in less than 0.5 sec per frame over 850 video sequences representing a total of more 4hours and 40 GB of camera and LiDAR data. We also show that the resulting graphs are light and efficient in space storage. ar X\niv :2\n30 8.\n12 75\n5v 1\n[ cs\n.A I]\n2 4\nA ug\n2 02\n3"
        },
        {
            "heading": "II. MOTIVATIONS",
            "text": "This section introduces QXG and illustrates its usage on a simple scene shown in Fig. 1. For the sake of simplicity, that scene, which is extracted from the NuScenes dataset, is composed of only four frames. The QXG captures the exact spatiotemporal relationships between objects of the scene, but there is no quantitative information in the QXG such as distances between objects or speed of objects. In the scene, there is an ego car (o1) and three other vehicles (o2, o3, and o4). Some objects, like o1 and o2, persist throughout the entire scene, while others exhibit temporal variations. For instance, o3 disappears before the end of the scene, and o4 only appears after the scene\u2019s beginning. All this information is captured by the QXG, along with the different relationships that objects can have with each other frame by frame. For example, between o3 and o4, there is a relationship observed only in frames 2 and 3, and this relationship is the same on both frames.\nWe have identified several motivations for computing QXG of AD scenes. We discuss here four distinct motivations and include an experimental validation for the first one. The others will be part of our future work.\n1) Efficient Processing and Storage: QXGs offer benefits in terms of data processing and storage efficiency. By abstracting a long-term scene into high-level qualitative relationships, the volume of information to be processed and stored is significantly reduced as compared to raw sensor data or pixel-based representations. For example, instead of storing and processing detailed pixel information for every frame in a dataset like in NuScenes (which could occupy up to 40 GB of storage), QXGs capture essential spatiotemporal relationship information, resulting in a more compact data representation (in less than 4 GB). This reduction in data size enables faster processing and optimization of computational resources, allowing AVs to perform real-time processing and alleviating the computational burden associated with raw sensor data. 2) Interpretability and Explainability of Long Scenes: QXGs enable explanations of AV actions because they capture the qualitative relationships of entities within long-term scenes. This human-interpretable form allows for clear insights into the relationships between objects, such as the pedestrian\u2019s position, velocity, and intention. By showcasing factors like proximity, traffic rules, and the pedestrian\u2019s intention to cross over long-term scene (typically more than 4sec), QXGs provide a basis for understanding and justifying the vehicle\u2019s decision to stop for a pedestrian. Ultimately, this fosters increased trust and accountability in AVs. 3) Learning and Mining from QXGs: QXGs provide valuable opportunities for learning and mining patterns from AD scenes. By analyzing the qualitative relationships in the graph, data mining techniques can uncover hidden spatial and temporal patterns, correlations, and higher-level knowledge about the\nO4O4\nO4 O4\nFrame 1 Frame 2\nFrame 3 Frame 4\nobjects in the scene. This opens the door for the development of advanced learning and mining techniques that leverage the structured and interpretable nature of QXGs to enhance various aspects of AD, including road user behavior prediction, anomaly detection, scene understanding for connected road infrastructures. 4) Enhanced Scene Description & Scenario Generation: QXGs, as symbolic representations, have the potential to enhance scene description by providing contextual information to generic large language models (LLMs). By incorporating qualitative relationships that unfold over time, these models can generate more accurate and contextually informed descriptions of scenes. For example, ChatGPT fed with the QXG of Fig. 1 produces a description like: \u201dIn this scene, there are four objects: o1 (the ego car), o2, o3, and o4. Throughout all frames, o1 closely follows o2 on the same lane...\u201d This enriched description captures the dynamics and relationships among objects, resulting in a more detailed and comprehensive understanding of the scene. Furthermore, from QXGs, it becomes possible to generate automatically driving scenarios by using Scenario Description Language (SDL) [27]."
        },
        {
            "heading": "III. BACKGROUND",
            "text": "This section introduces the necessary background on object detection and tracking, qualitative calculus and qualitative constraint acquisition, required to understand the paper."
        },
        {
            "heading": "A. Object Detection and Tracking",
            "text": "Object detection involves identifying the presence, position, and type of objects or entities within an image or video.\nOne common approach to object detection is to use convolutional neural networks (CNNs), that can automatically learn to recognize and localize objects using bounding boxes [8], [14], [21]. Bounding boxes tightly enclose an object by its top-left corner coordinates (x, y) and its width and height (w, h). Bounding boxes represent an over-approximation of the spatial information of an object, which can be useful for object tracking and classification. Object tracking is the process of following a particular object in a video sequence over time [13], [17], [26]. Note that object tracking can be challenging due to issues such as occlusion, changes in lighting conditions, or object deformation.\nIn the context of AVs, a scene is a sequence of n frames fi over time, denoted by S = \u27e8f1, . . . , fn\u27e9. We define the process of object detection-tracking as follows: Given a frame fk in S, the objectDT(S, fk) function detects the presence of objects in fk by computing their corresponding bounding boxes and tracks them w.r.t. the previously detected objects in the time-previous frames in S. We assume the existence of a set of m objects O = {o1, . . . om} present in S, where oi appears in at least one frame fj \u2208 S."
        },
        {
            "heading": "B. Qualitative Calculus and Rectangle Algebra",
            "text": "Qualitative Calculus (QC) is a computational method which reasons over the qualitative relationships between physical properties, such as position, velocity, and acceleration, without relying on precise quantitative information. QC is parametrized by an algebra which can be dedicated to only temporal [1] or spatial relations [22], or both [16]. In the context of AVs, qualitative reasoning can be used under the form of ontologies [7], [24] or neurosymbolic online abduction [23] to represent driving scenarios and traffic.\nIn this paper, we are using Rectangle Algebra (RA) [3] where each object is represented as a rectangle, and the relationship between two objects is captured by a constraint on the positions and sizes of those rectangles. There are 169 permitted relations between two rectangles whose sides are parallel to the axes of some orthogonal basis. RA extends Allen\u2019s interval algebra [1] to spatial reasoning by introducing new relations and combining them to describe spatial arrangements.\nThe operation of composition in RA preserves the concept of weak preconvexity, which means that all regions connected by a path of relations must be considered part of the same connected component. The operation of intersection\npreserves the concept of strong preconvexity, which means that if two regions overlap, they must be considered part of the same connected component. The language of Allen\u2019s relations consists of 13 relations denoted by \u0393 = {p, m, o, d, s, f, eq, pi, mi, oi, di, si, fi},2 which can be used to generate the language of RA\nO1\nO2 y\nx\nFig. 3. RA example: O1 (o, p) O2.\ndenoted by \u03a6 = \u03932. Overall, RA provides a powerful and flexible tool for reasoning about spatial relations between bounding boxes, as illustrated in\nFig. 3. In this case, vehicle O1 overlaps O2 on the x axis and precedes O2 on the y axis: O1 (o, p) O2."
        },
        {
            "heading": "C. Qualitative Constraint Acquisition",
            "text": "GEQCA (Generic Qualitative Constraint Acquisition) is a generic and active learning approach for acquiring constraints between pairs of entities using qualitative reasoning [5]. In this paper, we present a simplified version of GEQCA that illustrates its core functionality (cf. Algorithm 1). GEQCA takes as inputs a set of objects and a language of constraints and outputs a graph of qualitative constraints which represents an equivalence class of learned concepts. The algorithm starts by initializing a complete graph G with all possible relations between pairs of objects (line 1). Then, for each pair of objects (Xi, Xj) and for each possible relation r between the selected pair, GEQCA presents the relation r to the user through a qualitative query (line 4). By responding \u201dno\u201d, the user indicates that the relation does not hold, and then GEQCA removes the relation r from the corresponding edge Eij (line 5). Subsequently, GEQCA applies path consistency (PC) (line 5) to eliminate non-feasible relations from the graph G. This step indirectly reduces the number of iterations required and minimizes the user\u2019s effort in handling queries."
        },
        {
            "heading": "IV. QUALITATIVE EXPLAINABLE GRAPH BUILDER",
            "text": "We present our approach, called QXG Builder, which constructs a spatio-temporal explainable graph to represent a scene. The qualitative explainable graph (QXG) of a scene\n2[p: precedes; m: meets; o :overlaps; d :during; s :starts; f :finishes; eq :equals; pi :preceded by; mi :met by; oi :overlapped by; di :contains; si :started by; fi :finished by].\nAlgorithm 1 GEQCA In : set of objects X; \u0393 language; Out : a qualitative graph G;\n1 G\u2190 (X, {Eij = \u0393 : i < j}); 2 foreach (Xi, Xj) \u2208 X2 : i < j do 3 foreach r \u2208 Eij do 4 if (ask(Xi, r,Xj) = \u201dno\u201d) then 5 Eij \u2190 Eij \\ {r}; PC(G); 6 return G;\nS is defined as a pair (O, E), where O represents the set of objects and E represents the set of labeled edges. The QXG is a graph with labeled vertices, where the function box(oi, fj) takes an object oi \u2208 O and a frame fj \u2208 S as input and computes the corresponding bounding box of oi in fj . This bounding box represents a rectangle that can be projected onto a 2D space. Additionally, the QXG is a graph with labeled edges. Each pair of objects (oi, oj) (where i < j) is labeled with a vector Vij of relations from the RA. More specifically, Vij [k] denotes the atomic relation between the bounding boxes of oi and oj at frame fk.\nAlgorithm 2 QXG-BUILDER In : Sequence of frames S = \u27e8f1, . . . , fn\u27e9; // Scene RA language \u03a6; Out : Qualitative eXplainable Graph QXG = (O, E);\n1 QXG \u2190 (\u2205,\u2205); 2 for k \u2208 1..n do 3 \u2126k \u2190 objectDT(S, fk); 4 foreach (oi, oj) \u2208 \u21262k , s.t., i < j do 5 Gk \u2190GEQCA({oi, oj},\u03a6); 6 update(QXG, Gk); 7 return QXG;\nAlgorithm 2 takes a scene S as input, which is a sequence of n frames, along with \u03a6 the set of 169 possible RA relations. The algorithm then constructs and generates the corresponding QXG. The algorithm begins with an empty graph (line 1) and iterates over the m frames of S (line 2). During each iteration, for a given frame fk, QXG-BUILDER invokes the objectDT function for object detection and tracking (line 3). For every pair of objects (oi, oj) in frame fk, QXG-BUILDER employs GEQCA to acquire the appropriate RA relation between (oi, oj), and updates the graph accordingly. The graph is then updated accordingly. It is worth noting that since the GEQCA call operates on pairs of objects rather than the entire graph, no PC is maintained during the call, as at least three nodes are required in the graph to apply PC. If the objects are new, they are added as vertices, and the corresponding edge is added or updated with the relation at frame k Vij [k] (lines 4-6).\nIn the GEQCA procedure, the role of the human user is replaced by an automated oracle capable of classifying qualitative queries based on the presence or absence of a given relation between object pairs in a specific frame. In\nour scenario, the high volume of queries is not a concern since a program assumes the user\u2019s role in the iterative process. We will experimentally demonstrate the considerable efficiency of constraint acquisition using GEQCA compared to a brute force approach that requires iterating over all 169 RA relations for each selected object pair.\nTheorem 1: Consider a scene S consisting of n frames depicting m objects. Let DT be the time complexity of the detection and tracking function. Algorithm QXG-BUILDER constructs a QXG with a time complexity of O(n\u00d7 (DT + m2)) and a space complexity of O(n\u00d7m2).\nProof: Let n = |S| denote the number of frames in the scene S, and let m = | \u22c3 \u2126i| represent the total number of objects across all frames, where \u2126i = objectDT(S, fi). In Algorithm QXG-BUILDER, the main loop iterates over n frames (line 2). Within each iteration, the objectDT function is called, which has a time complexity of DT . The inner loop iterates over m(m\u22121)/2 object pairs, representing all possible combinations of objects in a frame (line 4). For each object pair, the acquisition step through GEQCA is bounded by a maximum of 169 RA relations. Therefore, the worst-case time complexity of the inner loop is O(m2). Combining the time complexities of the main loop and the inner loop, the overall time complexity of Algorithm QXGBUILDER is O(n\u00d7 (DT +m2)).\nNow let\u2019s consider the space complexity. The space required by the algorithm is determined by the size of the QXG, which consists of vertices and edges. The number of vertices is bounded by the total number of objects across all frames, i.e., m. The number of edges is determined by the number of object pairs, which is on the order of m2. Each edge in the QXG represents the relation between two objects in a specific frame, resulting in a vector of n possible relations for each object pair. This means that for every object pair, there are n potential relations. Therefore, considering the number of vertices, edges, and the vector of possible relations, the space complexity of the QXG is O(n\u00d7m2).\nIt is important to emphasize that in practice, the objectDT function does not exceed 100 milliseconds [4]. This makes QXG-BUILDER run in cubic time complexity O(n\u00d7m2)."
        },
        {
            "heading": "V. RUNNING EXAMPLE",
            "text": "In this section, we provide a practical example to demonstrate the functionality of our QXG-BUILDER algorithm on a specific scene. The scene we consider is depicted in Figure 1 and consists of four frames. Figure 2 illustrates the stepby-step construction of the QXG, frame by frame.\nIn the scene, the largest car, shown in purple and maintaining its position, represents a self-driving car equipped with a Top LiDAR sensor. The QXG-BUILDER algorithm starts with an empty graph and processes the first frame. In this frame, three objects, namely \u21261 = {o1, o2, o3}, are detected.\nNext, the algorithm iterates over each pair of objects to acquire the corresponding RA relation at frame 1. Taking the pair (o2, o3) as an example, the GEQCA component\nlearns that o2 precedes (resp., is preceded by) o3 on the xaxis (resp., y-axis), resulting in an edge V23 = \u27e8(p, pi)1\u27e9 with the exponent 1 representing frame 1. The QXG is then updated with the objects O = {o1, o2, o3} and the edges E = {(V12, V13, V23)}.\nMoving to frame 2, a new object o4 is detected, and all edges between object pairs are updated with the RA relations holding at frame 2. For example, the edge between (o1, o3) is updated with (p, pi)2, and a new edge is added between (o3, o4) in the graph, represented by V34 = \u27e8(p, oi)2\u27e9. Similar updates are performed for the pairs (o1, o4) and (o2, o4), resulting in new edges added to the QXG due to the presence of the newly detected object o4.\nIn frame 3, all edges are updated with the corresponding relations that hold between objects at frame 3. For instance, the edge V13 between (o1, o3) is updated with (di, pi)3.\nMoving to frame 4, all edges are updated except for the ones involving o3, which is no longer present in frame 4.\nFinally, the QXG-BUILDER algorithm returns a graph containing the four objects and the edges representing the RA relations holding between objects at each frame."
        },
        {
            "heading": "VI. EXPERIMENTS",
            "text": "This section presents an experimental evaluation of QXGBUILDER using the NuScenes open real-world dataset. QXG-BUILDER is implemented in Python; it complete code, along with comprehensive description is available online3. All tests were performed on a standard machine, an Intel Core i7 processor running at 2.8GHz with 64GB RAM."
        },
        {
            "heading": "A. NuScenes: A Real-world Multimodal AD Dataset",
            "text": "The NuScenes dataset is a comprehensive dataset designed for AD research [6]. It contains real-world sensor data from AVs in diverse urban driving scenarios, along with annotations for object detection, tracking, and other relevant attributes. The dataset includes high-definition maps aligned with the sensor data. It is widely utilized in the development and evaluation of AD algorithms. The dataset consists of multi-sensor data from various sources mounted on a selfdriving car, including six cameras providing 360-degree visual coverage, one LiDAR generating detailed 3D point clouds, five RADARs for detecting object velocities, an IMU for tracking vehicle movement, and a GNSS receiver for global positioning data. The dataset contains 1, 000 realworld scenarios from Singapore and Boston, each lasting 20 seconds. It provides annotations for 23 different object types such as cars, pedestrians, trucks, and bicycles, which were crucial for constructing the graph. These annotations can also be predicted using detection algorithms like BirdNet+, which utilize LiDAR data as input [4]. To work with the dataset, a Development Kit (devkit) is provided, offering scripts and functions for parsing, retrieving, and manipulating the necessary elements4.\nOur experiments are conducted on a subset of the dataset, specifically the 850 annotated scenarios. The remaining 150\n3https://github.com/simula-vias/qxg-builder 4https://github.com/nutonomy/nuscenes-devkit\nscenarios are reserved for testing purposes. Each scenario consists of a LiDAR scene, six camera scenes, and radar data. Each scene comprises 40 frames, with a capture rate of 2 frames per second. We execute the QXG-BUILDER algorithm on both LiDAR and camera scenes, resulting in the construction of 5, 950 QXGs. The number of objects in each scene varies, ranging from a minimum of 2 objects to a maximum of 285 objects. On average, there are approximately 55 objects per scene.\nThe LiDAR sensor offers us a convenient bird\u2019s eye view perception, making it suitable for a straightforward application of our approach. However, when working with camera sensor data, we initially obtain a frontal view in a three-dimensional coordinate system. To accommodate this data for our method, we perform a simple projection onto a 2D plane. This enables us to effectively incorporate camera sensor information into our study."
        },
        {
            "heading": "B. Results",
            "text": "Table I summarizes our results on constructing QXGs from the NuScenes dataset for each sensor (LiDAR and the six cameras). The table includes the average number of objects and the density of the QXGs. We also provide the memory occupied by the raw data and the memory occupied by the constructed QXGs, measured in MB. In terms of CPU time, we report the average and maximum execution times in milliseconds for QXG-BUILDER, comparing its performance to the Brute Force baseline (BF). The reported time represents the processing time per frame, as QXG-BUILDER iteratively constructs the QXG throughout the frames of the scene. The reported time does not include the detection and tracking time as NuScenes provides precomputed results for these tasks. It is worth noting that the detection and tracking process does not exceed 100 milliseconds per frame on the NuScenes dataset, using methods such as BirdNet+ [4]. It is worth noting that the baseline approach does not utilize qualitative acquisition through GEQCA and involves checking the 169 RA relations for each pair of objects.\n1) Constructed QXGs: The first observation that can be drawn from Table I is that the constructed QXGs exhibit high density. This is due to the captured relationships between objects throughout the 40 frames that constitute a scene. Furthermore, it is observed that the LiDAR-based QXGs are twice as dense as the QXGs generated from the camera data. This observation can also be extended to the number of objects, where the LiDAR sensor\u2019s 360-degree view allows it to capture all objects around the vehicle simultaneously. Consequently, the LiDAR-based QXGs tend to be richer in information compared to the camera-based QXGs. However, it is important to note that the different QXGs derived from a given scenario can complement each other, resulting in a more comprehensive scene description.\n2) Storage: The 850 \u00d7 7 scenes in the dataset initially occupy a memory space of 43.3GB. However, by utilizing QXG-BUILDER, we are able to obtain a concise representation of all the scenes through QXGs, resulting in a reduced memory footprint of only 3GB. Table I presents\nTABLE I GRAPH CONSTRUCTION TIME AND FINAL GRAPH SIZES FOR DIFFERENT SENSORS. TIMES FOR BRUTE FORCE AND QXG-BUILDER ARE GIVEN IN MILLISECONDS.\nSensor QXG Memory (MB) BF (ms) QXG-BUILDER(ms) #Objects Density Scenes QXG Avg. Max Avg. Max\nLIDAR 76 85% 20,800 1,400 253.4 3,985 1.7 40 Camera (Front) 38 53% 4,271 377 24.4 2,478 0.2 15 Camera (Front Left) 28 47% 3,639 225 11.2 442 0.1 20 Camera (Front Right) 28 47% 4,000 234 10.0 318 0.1 8 Camera (Back) 41 57% 3,800 468 31.6 2,452 0.2 23 Camera (Back Left) 27 44% 3,300 225 9.1 2,531 0.1 19 Camera (Back Right) 27 44% 3,800 230 7.2 468 0.0 20\n50 10 0\n15 0\n23 0\nScenes\n0\n5\n10\n15\n20\n25\nM em\nor y\nSp ac\ne (M\nB)\nLIDAR_TOP LIDAR_TOP QXG-BUILDER\nFig. 4. Memory storage for LiDAR data vs QXG.\nthe memory occupation in MB per sensor, showcasing a reduction in storage requirements ranging between 88% and 94%. Particularly, for the LiDAR sensor, which typically has higher data storage demands, we observe a significant reduction of 93% using our QXG representation.\nFor a detailed analysis, Fig. 4 illustrates the memory usage of LiDAR data and the corresponding QXGs for four selected scenes. These scenes are chosen based on the total number of objects present, ranging from 50 to 230 objects. It is important to note that LiDAR data occupy a consistent amount of memory space across all scenes (approximately 25MB per scene). However, QXG-BUILDER generates QXGs with varying memory occupancy depending on the number of objects present in the scene. The reduction in memory occupation is significant, with reductions of 98%, 92%, 81%, and 55% observed for scenes with 50, 100, 150, and 230 objects, respectively.\nThe memory data from the 850 QXGs extracted from LiDAR data allowed us to analyze the memory occupancy trend as a function of the number of objects. The trend can be approximated by the function f(x) = a + xb, where a = 0.000194425 and b = 2.008018. This function enables us to predict the number of objects required to achieve a QXG size of 25MB (which is the current size of the raw\n0 20 40 60 80 100 120 140 160 Number of Objects per Frame\n10\u22125\n10\u22123\n10\u22121\nGr ap\nh Co\nns tru\nct io\nn Ti\nm e\n[s ec ] 0 200 400 600 Nu m be r o f f ra m es QXG-BUILDER BF Number of frames\nFig. 5. Time per frame with respect to the number of objects (in seconds)\nLiDAR scene). To reach a QXG size of 25MB, a scene would need to have more than 358 objects.\nFurthermore, it is worth mentioning that our approach has a space complexity of O(n \u2217 m2), where n is the number of frames and m is the number of objects. Given that our dataset consists of approximately 40 frames, this results in a quadratic complexity. This substantial decrease in storage requirements is promising for QXG processing in autonomous vehicles as a viable alternative to storing raw sensor data.\n3) CPU Time Processing: In the CPU time results section of Table I, a significant observation can be made regarding the reduction in time required to construct QXGs by utilizing qualitative acquisition through GEQCA. For LiDAR data, where the volume of data is substantial, the average processing time per frame without qualitative acquisition is approximately 0.2 seconds, with a maximum observed time of around 4 seconds. However, by employing GEQCA, the maximum observed processing time per frame is reduced to 0.04 seconds.\nThese findings highlight the substantial time savings achieved through the use of qualitative acquisition, particularly for LiDAR data, where the reduction in processing time is more pronounced due to the larger data volume. To support our findings, Fig. 5 illustrates the time per frame in seconds as a function of the number of objects (log scale on the y1-axis) and the distribution of objects across the\ntotal number of frames (y2-axis) for LiDAR data. It is worth noting that the complexity of QXG-BUILDER is O(n\u00d7m2), where n represents the number of frames and m represents the number of objects. The reported time is per frame, and the variation in the number of objects clearly demonstrates the quadratic complexity in m. The distance between the two curves represents a constant factor K, which is explained by the number of RA relations (K = 169). Using GEQCA, our QXG-BUILDER is able to process a frame in a time bounded above by 40 milliseconds, meaning that QXG-BUILDER can handle at least 25 frames with over 280 objects per second. In contrast, the baseline method, which does not use qualitative acquisition, exceeds one second per frame starting from 100 objects and reaches up to four seconds for frames with more than 280 objects.\nNote that in Fig. 5 the object distribution in the dataset reveals that the majority of frames contain between 10 and 40 objects. For frames within this range, QXG-BUILDER requires less than 5 milliseconds, while the brute force approach takes between 100 and 200 milliseconds.\nLet us examine a specific LiDAR scene, namely (scene\u2212 0247). This scene contains 286 objects across 40 frames, and the resulting QXG has a density of 80%. Fig. 6 illustrates the processing time per frame for the BF approach and QXG-BUILDER. We can observe a consistent pattern in QXG construction, where the processing time gradually increases for each frame until reaching a point (frame 20) where the processing time becomes less significant. This can be attributed to the fact that initially, the graph starts with zero objects and gradually increases in terms of nodes. It is worth noting that QXG-BUILDER consistently maintains a processing time below the 40 milliseconds threshold for all frames. In contrast, the brute force approach exceeds the threshold, reaching the 4-second mark (a factor of 100 difference) at frame 19.\nIn summary, QXG-BUILDER offers fast CPU time, enabling it to process multiple frames per second. This makes it ideal for real-time or on-the-fly applications in autonomous vehicles (AVs), allowing for timely analysis and decisionmaking. Its efficiency in handling computational demands makes QXG-BUILDER a valuable tool for enhancing AV performance and safety."
        },
        {
            "heading": "VII. RELATED WORKS",
            "text": "Our approach uses qualitative reasoning in the context of AVs as done in [7], [23] and [9]. Though it completely differs from these works by its unique usage of constraint acquisition which was introduced in GEQCA [5], in querying each frame for object detection and sampling, the overall goal of building QXG remains associated with scene description and interpretation. Another difference is the usage of Rectangle Algebra, which, according to our knowledge, has not been used before for AVs and which seems particularly well fitted.\nThe QXG representation of the driving scene is fundamentally different from visual-based representations such as object-centric attention maps, as proposed in [10], [25]. Indeed, QXGs capture qualitative relationships between entities\nwhile saliency maps or CAMs are only based on visionbased detection of importance. Even though, vision-based approaches are capable of deriving textual explanations [11], they can hardly be used to interpret long-term sequences (more than 4 sec).\nRegarding approaches which are not dedicated to AVs, video captioning is a wide area which has been explored in depth with RNNs and attention models. The closest approach is from [15] where scene-graph guidance and interaction (SGI) is used to guide the textual description of a video. Our approach shares the graph extraction capability but differs in its usage of qualitative calculus which enables the formalization of the generated explanations."
        },
        {
            "heading": "VIII. CONCLUSION",
            "text": "In this paper, we have introduced QXG-BUILDER, an algorithm specifically designed to construct Qualitative eXplanation Graphs (QXGs) from sensor data in the context of AVs. Our algorithm leverages the power of qualitative constraint acquisition through GEQCA to enhance the interpretability and efficiency of long-term driving scenes. The computational efficiency of QXG-BUILDER is a key advantage, as it enables rapid processing of multiple frames per second. This real-time capability makes it highly suitable for on-the-fly and real-time applications in AVs. By efficiently processing sensor data, QXG-BUILDER enables AVs to react promptly to their environment, leading to enhanced performance and safety. Another significant advantage of QXG-BUILDER is its ability to reduce memory storage requirements compared to storing raw sensor data. By constructing QXGs, we represent the scene in a more compact and structured manner, resulting in a substantial decrease in memory occupancy. This reduction in memory usage allows for more efficient storage and retrieval of scene information, which is crucial for AVs with limited computational resources.\nThe use of QXGs offers numerous benefits for AVs. Firstly, it provides a structured and graph-based representation of the scene, enabling a more intuitive understanding\nof the decision-making processes involved in AV behaviors. This enhanced interpretability can greatly benefit both developers and end-users, as it allows for more transparent and explainable autonomous systems. Additionally, QXGs facilitate the extraction of valuable insights from the scene, enabling further analysis and learning.\nFor future work, there are several avenues for exploration. One potential direction is to investigate the scalability of QXG-BUILDER for larger and more complex scenes, considering an increasing number of objects and frames. Additionally, the integration of QXG-BUILDER with other perception and decision-making modules in AVs can be explored to enhance the overall autonomy and performance of the system. Furthermore, exploring the potential of QXGs for anomaly detection and predictive analysis in AVs could improve safety and robustness."
        }
    ],
    "title": "Acquiring Qualitative Explainable Graphs for Automated Driving Scene Interpretation",
    "year": 2023
}