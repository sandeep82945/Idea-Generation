{
    "abstractText": "Deep learning in general domains has constantly been extended to domain-specific tasks requiring the recognition of fine-grained characteristics. However, real-world applications for fine-grained tasks suffer from two challenges: a high reliance on expert knowledge for annotation and necessity of a versatile model for various downstream tasks in a specific domain (e.g., prediction of categories, bounding boxes, or pixel-wise annotations). Fortunately, the recent self-supervised learning (SSL) is a promising approach to pretrain a model without annotations, serving as an effective initialization for any downstream tasks. Since SSL does not rely on the presence of annotation, in general, it utilizes the large-scale unlabeled dataset, referred to as an open-set. In this sense, we introduce a novel Open-Set Self-Supervised Learning problem under the assumption that a large-scale unlabeled open-set is available, as well as the fine-grained target dataset, during a pretraining phase. In our problem setup, it is crucial to consider the distribution mismatch between the open-set and target dataset. Hence, we propose SimCore algorithm to sample a coreset, the subset of an open-set that has a minimum distance to the target dataset in the latent space. We demonstrate that SimCore significantly improves representation learning performance through extensive experimental settings, including eleven fine-grained datasets and seven open-sets in various downstream tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sungnyun Kim"
        },
        {
            "affiliations": [],
            "name": "Sangmin Bae"
        },
        {
            "affiliations": [],
            "name": "Se-Young Yun"
        }
    ],
    "id": "SP:cf03176438a360aca99ac4ee0b79d78591e37460",
    "references": [
        {
            "authors": [
                "Rahaf Aljundi",
                "Min Lin",
                "Baptiste Goujaud",
                "Yoshua Bengio"
            ],
            "title": "Gradient based sample selection for online continual learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "David Arthur",
                "Sergei Vassilvitskii"
            ],
            "title": "k-means++: The advantages of careful seeding",
            "venue": "Technical report,",
            "year": 2006
        },
        {
            "authors": [
                "Jordan T Ash",
                "Chicheng Zhang",
                "Akshay Krishnamurthy",
                "John Langford",
                "Alekh Agarwal"
            ],
            "title": "Deep batch active learning by diverse, uncertain gradient lower bounds",
            "venue": "arXiv preprint arXiv:1906.03671,",
            "year": 1906
        },
        {
            "authors": [
                "Abhijit Bendale",
                "Terrance Boult"
            ],
            "title": "Towards open world recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Abhijit Bendale",
                "Terrance E Boult"
            ],
            "title": "Towards open set deep networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "David Berthelot",
                "Nicholas Carlini",
                "Ekin D Cubuk",
                "Alex Kurakin",
                "Kihyuk Sohn",
                "Han Zhang",
                "Colin Raffel"
            ],
            "title": "Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "David Berthelot",
                "Nicholas Carlini",
                "Ian Goodfellow",
                "Nicolas Papernot",
                "Avital Oliver",
                "Colin A Raffel"
            ],
            "title": "Mixmatch: A holistic approach to semi-supervised learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Terrance E Boult",
                "Steve Cruz",
                "Akshay Raj Dhamija",
                "Manuel Gunther",
                "James Henrydoss",
                "Walter J Scheirer"
            ],
            "title": "Learning and the unknown: Surveying steps toward open world recognition",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Kaidi Cao",
                "Maria Brbic",
                "Jure Leskovec"
            ],
            "title": "Open-world semi-supervised learning",
            "venue": "arXiv preprint arXiv:2102.03526,",
            "year": 2021
        },
        {
            "authors": [
                "Mathilde Caron",
                "Ishan Misra",
                "Julien Mairal",
                "Priya Goyal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Unsupervised learning of visual features by contrasting cluster assignments",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Guangyao Chen",
                "Peixi Peng",
                "Xiangqian Wang",
                "Yonghong Tian"
            ],
            "title": "Adversarial reciprocal points learning for open set recognition",
            "venue": "arXiv preprint arXiv:2103.00953,",
            "year": 2021
        },
        {
            "authors": [
                "Liang-Chieh Chen",
                "Yukun Zhu",
                "George Papandreou",
                "Florian Schroff",
                "Hartwig Adam"
            ],
            "title": "Encoder-decoder with atrous separable convolution for semantic image segmentation",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Abhinav Gupta"
            ],
            "title": "Webly supervised learning of convolutional networks",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Xinlei Chen",
                "Kaiming He"
            ],
            "title": "Exploring simple siamese representation learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Yanbei Chen",
                "Xiatian Zhu",
                "Wei Li",
                "Shaogang Gong"
            ],
            "title": "Semisupervised learning under class distribution mismatch",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Mircea Cimpoi",
                "Subhransu Maji",
                "Iasonas Kokkinos",
                "Sammy Mohamed",
                "Andrea Vedaldi"
            ],
            "title": "Describing textures in the wild",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2014
        },
        {
            "authors": [
                "Elijah Cole",
                "Xuan Yang",
                "Kimberly Wilber",
                "Oisin Mac Aodha",
                "Serge Belongie"
            ],
            "title": "When does contrastive visual representation learning work",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In 2009 IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Alaaeldin El-Nouby",
                "Gautier Izacard",
                "Hugo Touvron",
                "Ivan Laptev",
                "Herv\u00e9 Jegou",
                "Edouard Grave"
            ],
            "title": "Are large-scale datasets necessary for self-supervised pre-training",
            "venue": "arXiv preprint arXiv:2112.10740,",
            "year": 2021
        },
        {
            "authors": [
                "Linus Ericsson",
                "Henry Gouk",
                "Timothy M Hospedales"
            ],
            "title": "How well do self-supervised models transfer",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Priya Goyal",
                "Mathilde Caron",
                "Benjamin Lefaudeux",
                "Min Xu",
                "Pengchao Wang",
                "Vivek Pai",
                "Mannat Singh",
                "Vitaliy Liptchinsky",
                "Ishan Misra",
                "Armand Joulin"
            ],
            "title": "Self-supervised pretraining of visual features in the wild",
            "venue": "arXiv preprint arXiv:2103.01988,",
            "year": 2021
        },
        {
            "authors": [
                "Jean-Bastien Grill",
                "Florian Strub",
                "Florent Altch\u00e9",
                "Corentin Tallec",
                "Pierre Richemond",
                "Elena Buchatskaya",
                "Carl Doersch",
                "Bernardo Avila Pires",
                "Zhaohan Guo",
                "Mohammad Gheshlaghi Azar"
            ],
            "title": "Bootstrap your own latent-a new approach to self-supervised learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Bo Han",
                "Quanming Yao",
                "Xingrui Yu",
                "Gang Niu",
                "Miao Xu",
                "Weihua Hu",
                "Ivor Tsang",
                "Masashi Sugiyama"
            ],
            "title": "Co-teaching: Robust training of deep neural networks with extremely noisy labels",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Kai Han",
                "Sylvestre-Alvise Rebuffi",
                "Sebastien Ehrhardt",
                "Andrea Vedaldi",
                "Andrew Zisserman"
            ],
            "title": "Automatically discovering and learning new visual categories with ranking statistics",
            "venue": "arXiv preprint arXiv:2002.05714,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Yen-Chang Hsu",
                "Zhaoyang Lv",
                "Zsolt Kira"
            ],
            "title": "Learning to cluster in order to transfer across domains and tasks",
            "venue": "arXiv preprint arXiv:1711.10125,",
            "year": 2017
        },
        {
            "authors": [
                "Ashish Jaiswal",
                "Ashwin Ramesh Babu",
                "Mohammad Zaki Zadeh",
                "Debapriya Banerjee",
                "Fillia Makedon"
            ],
            "title": "A survey on contrastive self-supervised learning",
            "year": 2020
        },
        {
            "authors": [
                "Aditya Khosla",
                "Nityananda Jayadevaprakash",
                "Bangpeng Yao",
                "Fei-Fei Li"
            ],
            "title": "Novel dataset for fine-grained image categorization: Stanford dogs",
            "venue": "In Proc. CVPR workshop on fine-grained visual categorization (FGVC),",
            "year": 2011
        },
        {
            "authors": [
                "Krishnateja Killamsetty",
                "Xujiang Zhao",
                "Feng Chen",
                "Rishabh Iyer"
            ],
            "title": "Retrieve: Coreset selection for efficient and robust semi-supervised learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Jonathan Krause",
                "Michael Stark",
                "Jia Deng",
                "Li Fei-Fei"
            ],
            "title": "3d object representations for fine-grained categorization",
            "venue": "In Proceedings of the IEEE international conference on computer vision workshops,",
            "year": 2013
        },
        {
            "authors": [
                "Cheng-Han Lee",
                "Ziwei Liu",
                "Lingyun Wu",
                "Ping Luo"
            ],
            "title": "Maskgan: Towards diverse and interactive facial image manipulation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Chunyuan Li",
                "Jianwei Yang",
                "Pengchuan Zhang",
                "Mei Gao",
                "Bin Xiao",
                "Xiyang Dai",
                "Lu Yuan",
                "Jianfeng Gao"
            ],
            "title": "Efficient selfsupervised vision transformers for representation learning",
            "venue": "arXiv preprint arXiv:2106.09785,",
            "year": 2021
        },
        {
            "authors": [
                "Junnan Li",
                "Richard Socher",
                "Steven CH Hoi"
            ],
            "title": "Dividemix: Learning with noisy labels as semi-supervised learning",
            "venue": "arXiv preprint arXiv:2002.07394,",
            "year": 2002
        },
        {
            "authors": [
                "Junnan Li",
                "Caiming Xiong",
                "Steven CH Hoi"
            ],
            "title": "Mopro: Webly supervised learning with momentum prototypes",
            "venue": "arXiv preprint arXiv:2009.07995,",
            "year": 2020
        },
        {
            "authors": [
                "Wen Li",
                "Limin Wang",
                "Wei Li",
                "Eirikur Agustsson",
                "Luc Van Gool"
            ],
            "title": "Webvision database: Visual learning and understanding from web data",
            "venue": "arXiv preprint arXiv:1708.02862,",
            "year": 2017
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Focal loss for dense object detection",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In European conference on computer vision,",
            "year": 2014
        },
        {
            "authors": [
                "Ziwei Liu",
                "Ping Luo",
                "Xiaogang Wang",
                "Xiaoou Tang"
            ],
            "title": "Deep learning face attributes in the wild",
            "venue": "In Proceedings of International Conference on Computer Vision (ICCV),",
            "year": 2015
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Sgdr: Stochastic gradient descent with warm restarts",
            "venue": "arXiv preprint arXiv:1608.03983,",
            "year": 2016
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101,",
            "year": 2017
        },
        {
            "authors": [
                "Wei Luo",
                "Xitong Yang",
                "Xianjie Mo",
                "Yuheng Lu",
                "Larry S Davis",
                "Jun Li",
                "Jian Yang",
                "Ser-Nam Lim"
            ],
            "title": "Cross-x learning for fine-grained visual categorization",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Subhransu Maji",
                "Esa Rahtu",
                "Juho Kannala",
                "Matthew Blaschko",
                "Andrea Vedaldi"
            ],
            "title": "Fine-grained visual classification of aircraft",
            "venue": "arXiv preprint arXiv:1306.5151,",
            "year": 2013
        },
        {
            "authors": [
                "Maria-Elena Nilsback",
                "Andrew Zisserman"
            ],
            "title": "Automated flower classification over a large number of classes",
            "venue": "Sixth Indian Conference on Computer Vision, Graphics & Image Processing,",
            "year": 2008
        },
        {
            "authors": [
                "Kun-Peng Ning",
                "Xun Zhao",
                "Yu Li",
                "Sheng-Jun Huang"
            ],
            "title": "Active learning for open-set annotation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Avital Oliver",
                "Augustus Odena",
                "Colin A Raffel",
                "Ekin Dogus Cubuk",
                "Ian Goodfellow"
            ],
            "title": "Realistic evaluation of deep semi-supervised learning algorithms",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748,",
            "year": 2018
        },
        {
            "authors": [
                "Omkar M Parkhi",
                "Andrea Vedaldi",
                "Andrew Zisserman",
                "CV Jawahar"
            ],
            "title": "Cats and dogs",
            "year": 2012
        },
        {
            "authors": [
                "Ariadna Quattoni",
                "Antonio Torralba"
            ],
            "title": "Recognizing indoor scenes",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Joshua Robinson",
                "Ching-Yao Chuang",
                "Suvrit Sra",
                "Stefanie Jegelka"
            ],
            "title": "Contrastive learning with hard negative samples",
            "venue": "arXiv preprint arXiv:2010.04592,",
            "year": 2010
        },
        {
            "authors": [
                "Kuniaki Saito",
                "Donghyun Kim",
                "Kate Saenko"
            ],
            "title": "Openmatch: Open-set consistency regularization for semi-supervised learning with outliers",
            "venue": "arXiv preprint arXiv:2105.14148,",
            "year": 2021
        },
        {
            "authors": [
                "Walter J Scheirer",
                "Anderson de Rezende Rocha",
                "Archana Sapkota",
                "Terrance E Boult"
            ],
            "title": "Toward open set recognition",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2012
        },
        {
            "authors": [
                "Ozan Sener",
                "Silvio Savarese"
            ],
            "title": "Active learning for convolutional neural networks: A core-set approach",
            "venue": "arXiv preprint arXiv:1708.00489,",
            "year": 2017
        },
        {
            "authors": [
                "Ashutosh Singla",
                "Lin Yuan",
                "Touradj Ebrahimi"
            ],
            "title": "Food/nonfood image classification and food categorization using pretrained googlenet model",
            "venue": "In Proceedings of the 2nd International Workshop on Multimedia Assisted Dietary Management,",
            "year": 2016
        },
        {
            "authors": [
                "Kihyuk Sohn",
                "David Berthelot",
                "Nicholas Carlini",
                "Zizhao Zhang",
                "Han Zhang",
                "Colin A Raffel",
                "Ekin Dogus Cubuk",
                "Alexey Kurakin",
                "Chun-Liang Li"
            ],
            "title": "Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Jong-Chyi Su",
                "Zezhou Cheng",
                "Subhransu Maji"
            ],
            "title": "A realistic evaluation of semi-supervised learning for fine-grained classification",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Zeren Sun",
                "Yazhou Yao",
                "Xiu-Shen Wei",
                "Yongshun Zhang",
                "Fumin Shen",
                "Jianxin Wu",
                "Jian Zhang",
                "Heng Tao Shen"
            ],
            "title": "Webly supervised fine-grained recognition: Benchmark datasets and an approach",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Mingxing Tan",
                "Quoc Le"
            ],
            "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Yonglong Tian",
                "Olivier J Henaff",
                "A\u00e4ron van den Oord"
            ],
            "title": "Divide and contrast: Self-supervised learning from uncurated data",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Rishabh Tiwari",
                "Krishnateja Killamsetty",
                "Rishabh Iyer",
                "Pradeep Shenoy"
            ],
            "title": "Gcr: Gradient coreset based replay buffer selection for continual learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Laurens Van der Maaten",
                "Geoffrey Hinton"
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of machine learning research,",
            "year": 2008
        },
        {
            "authors": [
                "Grant Van Horn",
                "Oisin Mac Aodha",
                "Yang Song",
                "Yin Cui",
                "Chen Sun",
                "Alex Shepard",
                "Hartwig Adam",
                "Pietro Perona",
                "Serge Belongie"
            ],
            "title": "The inaturalist species classification and detection dataset",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Sagar Vaze",
                "Kai Han",
                "Andrea Vedaldi",
                "Andrew Zisserman"
            ],
            "title": "Open-set recognition: A good closed-set classifier is all you need",
            "venue": "arXiv preprint arXiv:2110.06207,",
            "year": 2021
        },
        {
            "authors": [
                "Dan Wang",
                "Yi Shang"
            ],
            "title": "A new active labeling method for deep learning",
            "venue": "In 2014 International Joint Conference on Neural Networks),",
            "year": 2014
        },
        {
            "authors": [
                "Feng Wang",
                "Huaping Liu"
            ],
            "title": "Understanding the behaviour of contrastive loss",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Tongzhou Wang",
                "Phillip Isola"
            ],
            "title": "Understanding contrastive representation learning through alignment and uniformity on the hypersphere",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Kai Wei",
                "Rishabh Iyer",
                "Jeff Bilmes"
            ],
            "title": "Submodularity in data subset selection and active learning",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "Xiu-Shen Wei",
                "Yi-Zhe Song",
                "Oisin Mac Aodha",
                "Jianxin Wu",
                "Yuxin Peng",
                "Jinhui Tang",
                "Jian Yang",
                "Serge Belongie"
            ],
            "title": "Finegrained image analysis with deep learning: A survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Zhirong Wu",
                "Yuanjun Xiong",
                "Stella X Yu",
                "Dahua Lin"
            ],
            "title": "Unsupervised feature learning via non-parametric instance discrimination",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Saining Xie",
                "Ross Girshick",
                "Piotr Doll\u00e1r",
                "Zhuowen Tu",
                "Kaiming He"
            ],
            "title": "Aggregated residual transformations for deep neural networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Bangpeng Yao",
                "Xiaoye Jiang",
                "Aditya Khosla",
                "Andy Lai Lin",
                "Leonidas Guibas",
                "Li Fei-Fei"
            ],
            "title": "Human action recognition by learning bases of action attributes and parts",
            "venue": "In 2011 International conference on computer vision,",
            "year": 2011
        },
        {
            "authors": [
                "Jaehong Yoon",
                "Divyam Madaan",
                "Eunho Yang",
                "Sung Ju Hwang"
            ],
            "title": "Online coreset selection for rehearsal-based continual learning",
            "venue": "arXiv preprint arXiv:2106.01085,",
            "year": 2021
        },
        {
            "authors": [
                "Jure Zbontar",
                "Li Jing",
                "Ishan Misra",
                "Yann LeCun",
                "St\u00e9phane Deny. Barlow"
            ],
            "title": "twins: Self-supervised learning via redundancy reduction",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Bowen Zhang",
                "Yidong Wang",
                "Wenxin Hou",
                "Hao Wu",
                "Jindong Wang",
                "Manabu Okumura",
                "Takahiro Shinozaki"
            ],
            "title": "Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Zhun Zhong",
                "Linchao Zhu",
                "Zhiming Luo",
                "Shaozi Li",
                "Yi Yang",
                "Nicu Sebe"
            ],
            "title": "Openmix: Reviving known knowledge for discovering novel visual categories in an open world",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "The success of deep learning in general computer vision tasks has encouraged its widespread applications to specific domains of industry and research [21, 46, 73], such as facial recognition or vehicle identification. We particularly focus on the visual recognition of fine-grained datasets, where the goal is to differentiate between hard-to-distinguish images. However, real-world application for fine-grained tasks poses two challenges for practitioners and researchers developing algorithms. First, it requires a number of experts for anno-\n*equal contribution https://github.com/sungnyun/openssl-simcore\ntation, which incurs a large cost [3, 14, 60]. For example, ordinary people do not have professional knowledge about aircraft types or fine-grained categories of birds. Therefore, a realistic presumption for a domain-specific fine-grained dataset is that there may be no or very few labeled samples. Second, fine-grained datasets are often re-purposed or used for various tasks according to the user\u2019s demand, which motivates development of a versatile pretrained model. One might ask, as a target task, that bird images be classified by species or even segmented into foreground and background. A good initialization model can handle a variety of annotations for fine-grained datasets, such as multiple attributes [43, 47, 74], pixel-level annotations [53, 74], or bounding boxes [35, 47, 53].\nRecently, self-supervised learning (SSL) [11, 14, 24, 28] has enabled learning how to represent the data even without annotations, such that the representations serve as an effective initialization for any future downstream tasks. Since labeling is not necessary, SSL generally utilizes an openset, or large-scale unlabeled dataset, which can be easily obtained by web crawling [23, 64], for the pretraining. In this paper, we introduce a novel Open-Set Self-Supervised Learning (OpenSSL) problem, where we can leverage the open-set as well as the training set of fine-grained target dataset. Refer to Figure 1 for the overview of OpenSSL.\nIn the OpenSSL setup, since the open-set may contain instances irrelevant to the fine-grained dataset, we should consider the distribution mismatch. A large distribution mismatch might inhibit representation learning for the target task. For instance, in Figure 2, SSL on the open-set (OS) does not always outperform SSL on the fine-grained dataset (X) because it depends on the semantic similarity betweenX and OS. This is in line with the previous observations [21,22,64] that the performance of self-supervised representation on downstream tasks is correlated with similarity of pretraining and fine-tuning datasets.\nTo alleviate this mismatch issue, we could exploit a coreset, a subset of an open-set, which shares similar semantics with the target dataset. As a motivating experiment, we manually selected the relevant classes from ImageNet (OSoracle) that are supposed to be helpful according to each target\nar X\niv :2\n30 3.\n11 10\n1v 2\n[ cs\n.C V\n] 2\n4 M\nar 2\n02 3\ndataset. Interestingly, in Figure 2, merging OSoracle to X shows a significant performance gain, and its superiority over merging the entire open-set (X+OS) or the randomly sampled subset (X+OSrand) implies the necessity of a sampling algorithm for the coreset in the OpenSSL problem.\nTherefore, we propose SimCore, a simple yet effective coreset sampling algorithm from an unlabeled open-set. Our main goal is to find a subset semantically similar to the target dataset. We formulate the data subset selection problem to obtain a coreset that has a minimum distance to the target dataset in the latent space. SimCore significantly improves performance in extensive experimental settings (eleven finegrained datasets and seven open-sets), and shows consistent gains with different architectures, SSL losses, and downstream tasks. Our contributions are outlined as follows: \u2022 We first propose a realistic OpenSSL task, assuming an\nunlabeled open-set available during the pretraining phase on the fine-grained dataset.\n\u2022 We propose a coreset selection algorithm, SimCore, to leverage a subset semantically similar to the target dataset.\n\u2022 Our extensive experiments with eleven fine-grained datasets and seven open-sets substantiate the significance of data selection in our OpenSSL problem."
        },
        {
            "heading": "2. Related Works",
            "text": ""
        },
        {
            "heading": "2.1. Self-Supervised Learning",
            "text": "After Oord et al. [52] proposed an InfoNCE loss, contrastive learning algorithms began to show remarkable improvements in representation learning [10, 11, 14, 16, 24, 28, 37]. While a large-scale open-set enhances the generalization of learned representation [19, 31, 64], recent literature has pointed out the distribution mismatch between pretraining and fine-tuning datasets [21, 23, 64]. Particularly, El et al. [21] claimed that pretraining on ImageNet may not always be effective on the target task from different domains. Tian et al. [64] found that pretraining with uncurated data, a more realistic scenario, deteriorates the target task performance. Although the motivations coincide with ours, their proposed methods are focused on a single data scheme: a denoising autoencoder [21] only for fine-grained dataset, and distillation from expert models [64] or a novel architecture [23] for uncurated open-sets. In contrast, we propose an explicit sampling strategy from an open-set, which becomes more effective by augmenting fine-grained dataset with well-matched samples, as well as achieving robustness to the distribution discrepancy or curation of the open-set."
        },
        {
            "heading": "2.2. Coreset Selection from Open-Set",
            "text": "In an OpenSSL problem, we denote an open-set as the additional unlabeled pretraining set that includes instances either from relevant or irrelevant domain to target dataset. The assumption of available open-set is also common in other research fields, such as open-set recognition [5,12,57,68], webly supervised learning [15, 39, 62], open-set [17, 33, 51, 56] or open-world [4, 8, 9] semi-supervised learning, and openset annotation [50], although its detailed meaning varies in each field. We summarize the details in Table 1. Especially, our OpenSSL task is to recognize coreset from a large-scale open-set, without exploiting any label information. From this perspective, recent coreset selection approaches give us a good intuition. Existing studies find the representative subset of the unlabeled set for active selection [58, 72], or find the subset of current task data to avoid catastrophic forgetting for continual learning [1, 65, 78]. In the meantime, several works on self-supervised learning have developed a novel loss function that leverages hard negative samples, i.e., hard negative mining [55, 70], which shares similar concepts to our coreset. Our problem setup further requires an effective algorithm that takes into consideration of the distribution discrepancy in the open-set."
        },
        {
            "heading": "3. Method",
            "text": ""
        },
        {
            "heading": "3.1. OpenSSL Problem Formulation",
            "text": "SSL is a groundbreaking paradigm for learning inherent properties of data, while discarding irrelevant signals by discriminating perturbed samples. Recent literature has proposed a contrastive loss function to encourage making representations from the same image similar and representations from different images dissimilar [14, 28, 79].\nGiven an input data X = {xi}Ni=1, we generate two copies of random augmented image, A(X) = {x\u0303i}2Ni=1, where x\u0303i and x\u0303N+i are an augmented pair of each other. Let E\u03b8 be an encoder network. Then, with the augmented pairs, we generally formulate a contrastive loss as follows:\nL(X;E\u03b8) = 1 2|X| \u2211\nx\u0303i\u2208A(X)\n`ssl(zi, z + i ; {z \u2212 i }) (1)\nwhere zi = E\u03b8(x\u0303i), z+i denotes the representation from the augmented pair of x\u0303i, and {z\u2212i } is the set of features from all the other samples. Eq. 1 forces zi to be closer with z+i and farther from {z \u2212 i }. Projection heads [14] or predictors [24] are often used in measuring the similarities of representations. In addition, non-contrastive methods\n[10, 11, 16, 24] have also been proposed by not utilizing any negative pair set, i.e., {z\u2212i } = \u2205.\nIn the OpenSSL problem, X corresponds to the training set of the fine-grained target dataset. Using Eq. 1, we can pretrain an encoder without any annotation of X . Furthermore, we have an unlabeled open-set U , which can be jointly used with X . Rather than pretraining on a simple fusion of X and U , we are motivated to sample a relevant subset S from the open-set. We then pretrain the encoder with L(X \u222a S;E\u03b8)."
        },
        {
            "heading": "3.2. Simple Coreset Sampling from Open-Set",
            "text": "We introduce a simple coreset sampling algorithm, coined as SimCore. Motivated by Figure 2, selecting an appropriate coreset is a key for the OpenSSL problem. To this end, we build a set with the open-set samples that are the nearest neighbors of the target samples.\nThis is formulated by finding a subset S that maximizes the following objective function:\nf(S) = \u2211 x\u2208X max u\u2208S w(x, u), where S \u2286 U , U\u2229X = \u2205 (2)\nwhile w(x, u) = z>x zu estimates similarity of two representations, and z is the normalized feature from the encoder E\u03b8 pretrained on X with small epochs. From Eq. 2, SimCore finds a subset that shares the most similar semantics with the target set. This is reminiscent of the facility location function [48, 72], ffac(S) = \u2211 x\u2208U maxu\u2208S w(x, u),S \u2286 U . However, f(S) is different from ffac(S), since target samples are the elements of X , which is disjoint with S.\nMeanwhile, the direct calculation of pairwise similarities requires the complexity of O(|X|| U|), which might be extremely large. To reduce the computational overhead and make the algorithm scalable, we adapt k-means clustering [2] for the target dataset X , and replace it with the centroid set X\u0302 . Its associated objective function is f\u0302(S). Even when exploiting only the centroids (k = 100 in practice), we show significant performance gains on various benchmarks.\nIterative coreset sampling: f\u0302(S) is a monotonically increasing submodular function. One remark is that if there is no constrained budget on S, a subset achieving the maximum value of f\u0302(S) is not unique. If we denote S\u2217 as a minimal set with the maximum value, S\u2217 is obtained when including only the instances closest to each instance of X\u0302 (i.e., |S\u2217| \u2264 |X\u0302|). Since we want to sample sufficiently large subset, we re-define Eq. 2 with the selection round t: f\u0302(St) where the candidate set is Ut. Thus, we iterate the rounds to repeat sampling S\u2217t and excluding them from the candidate set, until we reach the proper budget size (see Algorithm 1). We collect all the coreset samples into a set I = S\u22171 \u222a S\u22172 \u222a \u00b7 \u00b7 \u00b7 \u222a S\u2217T to merge with X . Stopping criterion: However, we have no knowledge in practice if every sampled instance within the budget is sufficiently close to the target set. This necessitates stopping\nAlgorithm 1: Simple coreset sampling from open-set 1 Require: E\u03b8: encoder pretrained on X; 2 Require: U0: initial candidate set (open-set); 3 Require: B, \u03c4 : coreset budget, threshold; 4 initialize I \u2190 \u2205, t\u2190 0; 5 replace X\u0302 \u2190 cluster centroids of X; 6 calculate zx, zu \u2190 E\u03b8(x), E\u03b8(u) for \u2200x, u\u2208X\u0302\u00d7U0; 7 while |I| < B do 8 set S\u2217t as the elements in Ut that are closest to each element in X\u0302 (Eq. 2); 9 I \u2190 I \u222a S\u2217t , Ut+1 \u2190 Ut \\ S\u2217t\n10 t\u2190 t+ 1 11 // stopping criterion 12 if f\u0302(S\u2217t ) < \u03c4 \u00b7 f\u0302(S\u22171 ) then 13 stop sampling; 14 end"
        },
        {
            "heading": "15 end",
            "text": "16 re-initialize \u03b8 and pretrain E\u03b8 with X \u222a I;\ncriterion that blocks the sampling process from continuing when the samples are no longer close to the target set. To this end, we calculate the ratio f\u0302(S\u2217t )/f\u0302(S\u22171 ) at each iteration and stop the sampling process if its value is less than the threshold. This implies that we stop at iteration t if the sampled subset is not as similar as the first subset is to the target set. We filtered-out by using the threshold \u03c4 = 0.95 throughout experiments."
        },
        {
            "heading": "4. Experiments",
            "text": "We evaluate the learned representation quality according to each pretraining dataset. We thus demonstrate, through various fine-grained target tasks, that SimCore samples an effective subset that enhances pretraining. Here, we used eleven target datasets: Aircraft [47], Cars [35], Pet [53], Birds [74], Dogs [32], Flowers [49], Action [77], Indoor [54], Textures [18], Faces [36], and Food [59]. For the open-set, we mainly used ImageNet-1k [20], while we extended to other open-sets in Section 4.2. By default, we used SimCLR [14] with ResNet50 architecture as the encoder. The detailed experimental setups and dataset configurations are in Appendix B."
        },
        {
            "heading": "4.1. Performance Evaluation on Target Tasks",
            "text": "Linear evaluation: Conventional SSL literature evaluates linear probing performance with frozen pretrained encoder to measure the representation quality [14, 24]. Table 2 summarizes the linear evaluation results on eleven fine-grained datasets. We note two observations from this experiment. First, we evaluated if the coreset sampled by SimCore is qualitative as pretraining data. To this end, we set the budget size to p = 1% or p = 5% (i.e., sampling p-ratio of the open-\nset) and compared them with p% random sampling strategy. In every case, SimCore outperformed the random sampling. This demonstrates that exploiting the coreset is actually crucial compared to na\u00efvely using random samples. Second, using a number of cluster centroids of the target dataset is more advantageous than a single cluster, although SimCore with k = 1 also outperformed the random sampling.\nInterestingly, the different trend across target datasets gives us a hint about the optimal coreset size based on the level of distribution mismatch to the open-set. For example, in datasets like Pet and Birds, OS pretraining was pretty effective, and in those datasets, SimCore took advantage of the large budget size. This implies that several target datasets require more coreset samples than do others. However, in practice, we cannot pre-define the optimal budget size, since we do not have much knowledge about an uncurated openset. Therefore, we should handle SimCore with a stopping criterion, as we already have proposed in Section 3.2.\nSurprisingly, SimCore with a stopping criterion highly improves the accuracy by +10.5% (averaged over 11 datasets), compared to the X pretraining. This is much larger gain compared to the large-scale OS pretraining (+2.7%) and 1% random sampling (+1.3%). This is because SimCore adaptively samples a proper amount of coreset, and this amount differs by each target dataset. For Aircraft and Cars, SimCore sampled around 1% of ImageNet. This is a reasonable number, because in the ImageNet dataset [20], there are actually 4 aircraft-related and 10 cars-related classes (refer to Appendix A), out of 1,000 classes in total. Different encoder architectures and SSL methods: In Table 3, we have applied SimCore to different architectures: EfficientNet-B0 [63], ResNet18 [29], ResNeXt50 [76], and ResNet101 [29]. Regardless of whether the encoder architecture is much smaller or larger, SimCore greatly improves pretraining on the target dataset. Moreover, we have experimented with various SSL methods, such as BYOL [24],\nFigure 4. Selected samples of Places365 coreset (top) for Pet (left) and Birds (right) and iNaturalist coreset (bottom) for Action (left) and Indoor (right). Captions are the actual labels in each open-set.\nSwAV [10], DINO [11], and MAE [27], in Table 4. SimCore consistently demonstrates the effect of merging the coreset samples, even with the recent autoencoder-based SSL."
        },
        {
            "heading": "4.2. SimCore on Various Open-Sets",
            "text": "Thus far, we have used ImageNet-1k benchmark [20] as the open-set, which is a well-curated dataset covering general domains [23, 64]. Drawing a coreset from ImageNet has shown large performance gains in the target tasks. However, in practice, an open-set is far from what we know about; it is rather a grouping of data arbitrarily drawn from the web or database. Here, we show that our SimCore with any open-set robustly improves pretraining because it finds a well-matched coreset. We experimented with three other open-sets: MS COCO [42], iNaturalist 2021-mini [67], and Places365 [82].\nFigure 3 shows that SimCore consistently outperforms the pretraining without OS, while the performance of SimCore depends on the open-set. In unnatural fine-grained datasets\nOS Aircraft Cars Birds Pet Action Indoor\n7 46.6 55.4 29.3 59.2 43.8 54.1 ImageNet-1k 48.3 60.3 37.7 79.7 67.5 72.0\nALL 58.8 64.8 38.0 79.5 67.2 75.1 WebVision 48.2 59.1 36.7 80.3 67.1 72.6 WebFG-496 55.4 63.1 37.7 - - -\n(a) Aircraft (b) Cars\n(c) Pet (d) Birds\nFigure 5. Feature distribution map of OS (left), X (middle), and the coreset sampled by SimCore (right).\nlike Action and Indoor, using iNaturalist is not as effective as ImageNet, but it offers a good coreset for Birds. As expected, for Indoor, Places365 offers the best coreset among every open-sets because it contains a lot of scenery semantics.\nNevertheless, we have observed the unexpected gains, such as Places365 open-set for Pet target dataset. Figure 4 illustrates a few selected samples of those coresets. It is interesting to see that SimCore has found the animal images, although the actual labels correspond to the locations. Also, the iNaturalist coreset contains natural creatures that are held by humans or located in indoor. This might help in Action target, part of which are humans taking photos, fishing, or gardening, as well as in Indoor scenery target.\nUncurated open-sets: To demonstrate the effect of SimCore in the more realistic scenario, we have tested SimCore with uncurated open-sets. First, we used a combined dataset of all four pre-mentioned open-sets (ALL), to simulate the more large-scaled and heterogeneous open-set case. In addition, we further used web-crawled image dataset queried by ImageNet classes (WebVision [40]), and the images queried by Aircraft+Cars+Birds classes (WebFG-496 [62]). Interestingly, in the case of WebFG-496 that includes noisy instances, SimCore sampled slightly less of the actual crawled set for each target. For example, while WebFG-496 contains 13,508 number of queried instances for Aircraft, SimCore sampled a coreset of 8,089 instances. Table 5 demonstrates that these uncurated open-sets are comparable to the curated ones and significantly outperform the pretraining without open-sets. Indeed, SimCore could sample a useful coreset regardless of how uncurated an open-set is."
        },
        {
            "heading": "4.3. Qualitative Evaluation",
            "text": "Feature distribution analysis: To analyze how the SimCore algorithm samples the coreset in the latent space, we\nvisualized the feature distribution on a unit ring by Gaussian kernel density estimation in R2 [71] (implementation details in Appendix D). The results in Figure 5 are interesting, as SimCore actually samples the instances that are closely embedded to the target data. In addition, through a comparison of the occupied areas of OS and X , we confirmed the distribution similarity of the open-set to each target dataset, indicating the sampling ratios by SimCore in Table 2 are reasonable.\nCoreset visualization: We visualized which instances from the open-set are actually sampled by our SimCore algorithm. To this end, we displayed in Figure 6 the ground-truth labels of the coreset samples when the target dataset is Pet or Birds. For comparison, we also displayed the coreset by SimCore with k = 1, using a single centroid. We used the open-set as ImageNet, so the ground-truth labels of coreset samples correspond to the ImageNet classes. Note that Pet contains 12 cat breeds and 25 dog breeds [53], and Birds contains 200 bird species [74].\nFor the Pet dataset (Figure 6a), SimCore with k = 1 sampled mostly animal images, but included data somewhat irrelevant to cats and dogs. The second most class is giant panda, the third is koala, and the eighth is guenon, a kind of monkey. On the contrary, SimCore with k = 100 sampled mostly cat or dog images, with up to top-20 classes, each being a breed of either cat or dog. Interestingly, eight out of the top-10 classes were those that overlap with the Pet class labels, such as Persian cat, Saint Bernard, etc.\nFor the Birds dataset (Figure 6b), SimCore with k = 1\nsampled a lot of irrelevant images, such as French horn, trombone, bullet train, admiral, hard disc, maillot, etc. On the contrary, SimCore with k = 100 sampled only the bird species up to the top-20 classes, including bulbul, chickadee, brambling, bee eater, house finch, goldfinch, etc."
        },
        {
            "heading": "4.4. Comparisons with Open-Set Semi-Supervised",
            "text": "and Webly Supervised Learning\nPrior literature has similarly utilized unlabeled or noisylabeled open-sets, such as open-set semi-supervised learning (OpenSemi) [56, 61] and webly supervised learning (WeblySup) [15, 62]. OpenSemi and WeblySup work with predefined labels and co-train with the entire huge-scale open-set. Our OpenSSL, on the other hand, is a valuable problem itself in that a model can be pretrained without any label information and with efficient subset sampling. We thus design an experiment under each framework, setting (X / OS) as the following example: (Birds50% / Birds50%+WebFG) and (Birds100% / WebFGBirds), respectively.\nTable 6 summarizes the comparisons with the representative methods of each learning framework. We observed two findings from Table 6. (1) When the SimCore model, using WebFG-496 as an open-set, is simply fine-tuned on each target, it outperforms OpenSemi and WeblySup methods. (2) SimCore can synergize with both frameworks, serving as an effective initialization. These results are in line with Su et al. [61], where Self-Training with SSL pretrained model was most preferred."
        },
        {
            "heading": "4.5. Different Downstream Tasks",
            "text": "kNN classifier: We have shown the linear evaluation performance on each fine-grained dataset to evaluate the quality of learned representation. Here, we evaluate on another downstream task, nearest neighbor (NN) classification. The NN classification is a nonparametric way to classify test images, making the prediction via weighted voting of nearest neighbors [10, 75]. Table 7a summarizes the 20 NN and 200 NN classifier results, presenting that SimCore shows the best accuracy, except for Birds where every accuracy was low. Semi-supervised learning: In addition, when part of the datasets becomes labeled by expert annotators, we can use those labels to further fine-tune the entire network. Here, we followed the semi-supervised learning protocol in [14,24], where the randomly selected samples are annotated and used in the fine-tuning. In Table 7b, we show the results with three label ratios, each trained with 100 epochs. SimCore again showed the best results overall, by particularly large margins in Aircraft and Pet datasets. For other semi-supervised learning algorithms, refer to Appendix F.3. Object detection and pixel-wise segmentation: For object detection, we used RetinaNet detector [41] with ResNet50 encoder and trained for 100 epochs. The metric is mAP evaluated as in [42], averaging over 10 IoU thresholds from 0.5 to 0.95 with a step size 0.05. mAP50 is the\nmAP when IoU threshold is 0.5. For pixel-wise segmentation, we used DeepLabV3+ segmentation model [13] and trained for 30 epochs. In Table 7c, our SimCore-pretrained encoder could more effectively identify the vehicle objects and segregate the foreground part of pet and bird images.\nMulti-attribute classification: Fine-grained images are often distinguished by multiple attributes. For example, one might ask if the aircraft images could be classified by manufacturers (e.g., Boeing), families (e.g., Boeing 737), or variants (e.g., Boeing 737-700). The Cars dataset can also be identified by brands (e.g., Audi) or types (e.g., Coupe), in addition to the models (e.g., Audi TTS Coupe 2012) we used in linear evaluation. To this end, we evaluated our pretrained models on multi-attribute classification tasks of three finegrained datasets, and SimCore excelled the baselines in every task (see Table 7d)."
        },
        {
            "heading": "5. Conclusion",
            "text": "In this work, we present a novel OpenSSL problem, where a large-scale open-set can be utilized in self-supervised learning on a fine-grained target dataset. Thereafter, we propose SimCore algorithm, which samples the effective coreset from an open-set with semantics similar to the target dataset. We demonstrate that the coreset samples enhance representation learning for any fine-grained downstream tasks. We believe that our approach will stimulate more investigation into fine-grained SSL with the open-set in the future.\nAcknowledgement. This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2019-0-00075, Artificial Intelligence Graduate School Program (KAIST) and No. 2022-0-00871, Development of AI Autonomy and Knowledge Enhancement for AI Agent Collaboration)."
        },
        {
            "heading": "A. Details of Motivating Experiment in Figure 2",
            "text": "In Table 8, we described all the selected classes from ImageNet to construct OSoracle, for each target dataset. For clarity, we also visualized examples for each selected category in Figures 7\u201310. Also, Table 9 summarizes the exact numbers of the motivating experiment results."
        },
        {
            "heading": "B. Experimental Settings",
            "text": "We used eleven fine-grained datasets and four open-sets in the main experiments. We summarized the dataset configurations in the following Table 10. We followed the linear evaluation protocol used in recent SSL literature [14, 24]: pretraining the encoder and fine-tuning only the classifier with the frozen encoder part."
        },
        {
            "heading": "B.1. Datasets",
            "text": "Since the SSL pretraining does not rely on any label information, we incorporate the train and validation set of the original benchmarks, such as FGVC-Aircraft, Oxford 102 Flower, Describable Textures, and Food 11, to enlarge the number of training samples. Besides, in the case of CelebAMask-HQ, we exclude identities that contain less than 15 images. The original number of 30,000 images thus have reduced to 4,263, and the number of identities have reduced from 6,217 to 307."
        },
        {
            "heading": "B.2. Encoder Pretraining",
            "text": "Basic settings: We set the cost of pretraining to 8 GPU days (4 GPUs \u00d7 2 days) since the train set sizes vary depending on the pretraining dataset. For example, we pretrained the model for longer epochs of 5K on small-scale fine-grained datasets, because one epoch is a few hundred times shorter in iteration than an epoch of OS. Meanwhile, although it took more than 8 GPU days, we trained 200 epochs for ImageNet open-set (OS) and X+OS experiments for full pretraining.\nFor SimCore pretraining, every 1% sampling last 5K epochs, but every 5% sampling last 2K epochs due to the excess of 8 GPU days. Since SimCore with a stopping criterion had different sampling ratios according to the target dataset, in this case, we trained the model for min(5K, 10K/p) epochs. If the open-set changes, we increased or decreased the epochs by the ratio between the open-set size and ImageNet size, i.e., X+Places365SimCore uses \u00d76.3 shorter epochs than X+ImageNetSimCore. Also, when using SimCore with a stopping criterion, a budget limit is not necessarily needed. Nonetheless, in case there are sufficiently large number of core-samples in the open-set, we set the budget to \u00d750 of the target dataset size to see the effect of a small coreset. Note that we used the stopping threshold \u03c4 as 0.95 in default.\nSimCLR method: When pretraining the encoder with the SimCLR method, SGD optimizer is used with an initial learning rate of 0.1 and `2 regularization parameter 1e-4, with 512 batch size. The learning rate was scheduled by cosine annealing [44], decayed to zero on the last epoch. We followed the same hyperparameters, the temperature of 0.07 and the projection dimension of 128, as in the original paper [14]. For data augmentation, we used a random resized crop, horizontal flip, color jitter, and random gray-scale.\nBYOL method: We used an Adam optimizer [34] with a learning rate of 1e-3 and `2 regularization parameter 1e-6. BYOL method is highly sensitive to the exponential moving average (EMA) value of a momentum encoder [28]. Therefore, we followed the same EMA scheduler as in the original implementation [24], of which the momentum starts from 0.996 to 1. We should also note that we did not use the EMA scheduler for the experiments with short epochs.\nSwAV method: We mostly followed the same settings as the SimCLR method, such as an SGD optimizer with a learning rate of 0.1 and weight decay of 1e-4. In the case of pretraining on ImageNet, we used the temperature value of 0.1 and an epsilon value of 0.02, while we froze the 3,000-dimensional prototypes for one epoch. For every other cases, we froze the 100-dimensional prototypes for ten epochs. DINO method: We mostly followed the pretraining setups as in the original paper [11]. We used an AdamW optimizer [45] with a learning rate of 1e-3. Cosine annealing and warmup were used during the 2% of total epochs. We scheduled an `2 regularization parameter from 0.04 to 0.4, and the EMA momentum was increased from 0.996 to 1.0. Note that the `2 regularization had not been applied to any biases and normalization parameters. Besides, we scheduled the teacher temperature from 0.04 to 0.07 for the 40% of total epochs and set the student temperature as 0.1. The center momentum was 0.9, and we set the dimension of projector as 65,536. As a data augmentation technique, we used additional four local croppings [10] (96\u00d796). The encoder\u2019s last layer was frozen during the first 10 epochs. These numerous hyperparameters were highly fit to the ImageNet pretraining, whereas they did not seem effective in several fine-grained datasets. In practice, the stopping threshold \u03c4 of 0.6 was optimal for the DINO method, which can be attributed to the large dimension of the projector. MAE method: We used an AdamW optimizer with a learning rate of 3e-4, cosine annealing, and warmup epochs of 100. Furthermore, we used the constant `2 regularization parameter of 0.05. As in the original paper [27], we set the mask ratio as 75%, and we normalized the pixel values when calculating the reconstruction loss."
        },
        {
            "heading": "B.3. Fine-Tuning for Linear Evaluation",
            "text": "We fine-tuned a linear classifier for 100 epochs and searched the optimal learning rate among five logarithmically spaced values from 1 to 102. We decayed the learning rate by 0.1 at 60 and 80 epochs, and any regularization techniques, such as weight decay, were not used."
        },
        {
            "heading": "B.4. Open-Set Semi-Supervised and Webly Supervised Learning",
            "text": "SimCore fine-tuning: For SimCore fine-tuning baselines, we fine-tuned the SimCore pretrained models for 200 epochs with the batch size of 256. We used an SGD optimizer with `2 regularization parameter of 1e-4. The learning rate started from the value of 3e-2 and was decayed by a cosine annealing scheduler. OpenSemi framework: Under the open-set semi-supervised learning (OpenSemi) framework, we used an SGD optimizer with a learning rate of 3e-2, `2 regularization parameter of 1e-4, and cosine annealing. In default, we trained the random initialized model for 128 epochs with iteration steps of 512 and the batch size of 64. For the models pretrained by SimCore, we used the shorter epochs of 32.\nFor the Self-Training algorithm [61], we used the temperature of 1.0 and set the coefficient of the unlabeled loss as 0.5. We trained a teacher network from scratch for 500 epochs and fine-tuned the SimCore initialized model for 100 epochs.\nFor the OpenMatch algorithm [56], we set the coefficient of open-set entropy minimization (OEM) loss and soft open-set consistency regularization (SOCR) loss as 0.1 and 0.5, respectively. Besides, we set the epochs to start FixMatch training as 20 for random initialized models and 5 for SimCore pretrained models. WeblySup framework: We used an SGD optimizer with a learning rate of 3e-2, `2 regularization parameter of 1e-4, and cosine annealing. For the Co-Teaching algorithm [25], we used the batch size of 256 and the forget ratio of 0.2. In addition, we followed the values of other hyperparameters as in Han et al. We set the epochs as 1,000 for training from scratch, and 200 for the experiments of SimCore initialization. Note that we modified the learning rate and forget ratio to 0.1 for Cars dataset.\nFor the DivideMix method [38], we trained the model with 400 epochs and the batch size of 128. We set the probability threshold as 0.2 and warmup epochs as 30 for every dataset. Since the DivideMix algorithm includes the training schemes of MixMatch [7], we used the default hyperparameters of the MixMatch algorithm, such as the unlabeled loss coefficient of 75 and alpha value of 0.75."
        },
        {
            "heading": "B.5. Downstream Tasks",
            "text": "For the object detection task, we used an Adam optimizer with a learning rate of 1e-4 and the batch size of 16. Besides, we set the total epochs and IoU threshold of non-maximum suppression as 30 and 0.2, respectively. We should note that the backbone network of DeepLabV3+ is frozen during fine-tuning, and we trained an FPN network from scratch.\nFor the semantic segmentation task, we have also frozen the SSL pretrained ResNet-50 encoder of RetinaNet. We used an SGD optimizer with `2 regularization parameter of 1e-4 and the Nesterov momentum. The optimal learning rate is chosen among five logarithmically spaced values from 1e-1 to 1e-3."
        },
        {
            "heading": "C. Sampling Ratios of SimCore Experiments",
            "text": "The coreset denotes a subset of the open-set that is semantically similar to the target dataset. Since we measure this similarity on the feature space generated by a retrieval model, the sampling ratio with a stopping criterion could vary depending on several factors, such as model architectures, SSL losses, open-set, or target dataset. That is, an image pair can be recognized as similar or dissimilar according to how they are represented.\nThe exact values of sampling ratios in our experiments are summarized in Tables 11\u201312. In effect, SimCLR with any architecture samples less than 1% as the coreset of Cars, but BYOL with ResNet50 samples over 30% of the open-set. Although the proper sampling ratios were different across each method, SimCore has consistently improved performances with the selected samples.\nFurthermore, SimCore samples the reasonable amount with any open-set samples, e.g., sampling number of iNaturalist (32K) vs. Places365 (268K) in Indoor. Along with Figure 3 and Table 5, the performance gain of SimCore is correlated with the semantic similarity between X and OS, suggesting the benefit of suitable open-sets.\nD. Implementation Details of Feature Distribution Analysis Figure 5 in Section 4.3 visualizes feature distribution of each pretraining dataset. We extracted the feature embeddings with the retrieval model (i.e., pretrained encoder on the target dataset for short epochs). Then, we reduced all representations to two-dimensional vectors via t-SNE [66]. These representation vectors are distributed on a unit ring by Gaussian kernel density estimation, as introduced in [71].\nIn detail, we visualized the feature distributions of OS, X , and the coreset sampled from OS. For the visualization of OS, ImageNet in this case, we randomly selected 10% of OS for faster convergence of t-SNE. For the coreset, SimCore samples 1% of OS. Unlike in Aircraft and Cars, in Pet and Birds, the distribution of X features are more overlapped with that of OS features. This implies high semantic similarity between the open-set (ImageNet) and the target dataset (Pet or Birds)."
        },
        {
            "heading": "E. Sensitivity Study",
            "text": ""
        },
        {
            "heading": "E.1. Stopping Threshold",
            "text": "The stopping criterion in SimCore algorithm requires a threshold value \u03c4 . In default, we used \u03c4 = 0.95 throughout the experiments. Here, we investigate the sensitivity of SimCore performance to its stopping threshold value. In Table 13, we summarized both sampling ratios and classification accuracies by varying the stopping threshold. We could observe that the \u03c4 value of 0.95 generally performs well in both SimCLR and MAE."
        },
        {
            "heading": "E.2. Retrieval Model",
            "text": "Prior to the coreset sampling, we pretrain the encoder on a target dataset for short epochs, 1K in our experiments. We refer this model, used in the coreset selection, to a retrieval model. This is necessary because we should measure the similarity between the features from the target dataset and from the open-set. Figure 11 presents SimCore performances, given different pretraining epochs of retrieval models. For comparison, the sampling ratio is set to p = 1%, and the random sampling strategy is also included.\nAs a result, 0.5K ep. model performed marginally worse than other SimCore models, whereas 1K ep. was comparable to 5K ep. model. Still, every SimCore models outperformed random sampling or without OS. We should note that 1K ep. pretraining takes up a small portion of the entire training process. On the basis of Pet, 1K pretraining of the retrieval model only costs 4.3% of the total iterations.\nHowever, one can still have a concern on the training cost for the retrieval model. To address this, we conducted an additional experiment, where the models are pretrained on only target datasets for the same number of iterations as our SimCore with the sampling ratio of 1%. For example, we trained the models on Pet for 18,925 epochs to exactly match the iterations to SimCore 1% experiment. Interestingly, we obtained the results as follows, compared to SimCore: 52.26% in Aircraft (+3.81%), 52.97% in Cars (\u22126.03%), 60.80% in Pet (\u221216.33%), and 30.59% in Birds (\u22125.97%). Based on these results, we have confirmed the efficacy of the sampled coreset, as simply increasing the pretraining epochs does not result in the performance improvement."
        },
        {
            "heading": "E.3. Number of Clusters",
            "text": "To reduce the complexity, we have used 100 centroids to calculate the f\u0302(S) value after k-means clustering. Here, we show that SimCore is robust to the number of k. Figure 12 shows SimCore performance results according to different k values, {1, 10, 102, 103, |X|}, with 1% coreset sampling from ImageNet. Except for the single centroid case, SimCore exhibited overall high accuracies."
        },
        {
            "heading": "F. Additional Experiments",
            "text": ""
        },
        {
            "heading": "F.1. Two-Stage SSL Pretraining",
            "text": "If a good initialization pretrained on OS is available, we could further pretrain this model on either X (OS\u2192 X) or the union of X and the coreset (OS\u2192 X+OSSimCore). Therefore, we additionally experimented with the checkpoint of the official code of SimCLR [14], which is already pretrained on the ImageNet dataset. Here, we used an Adam optimizer with a learning rate of 1e-3 and reduced the epochs to 20% compared to training from scratch. Table 14 summarizes the evaluation results of linear probing for three pretraining schemes. In practice, our SimCore outperforms baselines in 10 out of 11 fine-grained datasets. This demonstrates that a well-pretrained model can also be effectively exploited through two-stage pretraining schemes with the SimCore algorithm."
        },
        {
            "heading": "F.2. Sampling Strategy",
            "text": "In our default SimCore sampling, we sampled the coreset only once after the training of the retrieval model. In practice, the sampling procedure only takes \u223c20 minutes, mostly for computing the pairwise similarity (note that SSL pretraining takes \u223c2 days). There are several available variations on this sampling strategy. One na\u00efve way is to sample a coreset several times during the pretraining, since the model evolves and thus retrieves different coresets throughout the training.\nIn this sense, we resampled the coreset (p = 1%) three times during 5K epochs training, while maintaining the entire cost for pretraining. As a result, this sampling strategy yielded the performances of 49.99% in Aircraft (+1.54%), 57.63% in Cars (\u22121.37%), 79.75% in Pet (+2.62%), and 38.08% in Birds (+1.52%). Thus, exploring the coreset sampling strategies may be a future work that potentially improves the performance of SimCore."
        },
        {
            "heading": "F.3. Additional Fine-Tuning Schemes",
            "text": "Semi-supervised learning: Table 15 presents the experimental results on fine-tuning using various semi-supervised learning methods, including MixMatch [7], ReMixMatch [6], FixMatch [60], and FlexMatch [80]. We would note that the semisupervised learning approach in Table 7b has simply fine-tuned models with a small portion of labeled data, following the learning protocols described in [14, 24]. In terms of implementation details, we used the default hyperparameter settings for each method and fine-tuned models for 32 epochs, with iteration steps of 512. In most cases, we found that our SimCore pretraining approach showed the significant performance improvements, regardless of semi-supervised learning methods. This suggests that a self-supervised pretraining with coreset selection, followed by the use of semi-supervised algorithms, can be an effective approach when only a small amount of labeled data is available.\nActive learning: We conducted additional experiments on various active learning methods for fine-tuning the SSL-pretrained models. Table 16 presents the experimental results with four standard active selection methods, such as Random, Entropy [69], Coreset [58], and BADGE [3]. We randomly selected the first 10% of the data and sequentially queried 10% ratio using each active learning algorithm. After annotating the queried samples, we fine-tuned models for 100 epochs, starting from the checkpoint of the previous active learning round. As an effective initialization, our SimCore pretraining approach outperformed both pretraining baselines using either X or OS, in the active learning fine-tuning schemes as well."
        },
        {
            "heading": "G. Comparisons with Hard Negative Mining",
            "text": "SimCore can be thought of as hard sample mining from the open-set. In SSL literature, hard negative mining (HNM) is a well-known technique for leveraging the informative sample features. Robinson et al. [55] proposed an implicit method for mining the negatives that are similar to an anchor (zi in Eq. 1), while Wang et al. [70] suggested explicit sampling to inflict large gradient penalties.\nTable 17 compares SimCore with the existing HNM approaches. Hard negative sampling (HNS) and explicit HNS (EHNS) slightly improve the performance; however, both still use all open-set samples, some of which may not be relevant to the target. In other words, they employ hard negatives for every open-set anchors, which may reduce the performance due to the distribution mismatch to the target dataset.\nSimCore, on the other hand, retrieves only the coreset, reducing the need for an additional negative mining in the loss function. Nonetheless, SimCore can be applied to any SSL method including HNM-based losses. Thus, we have tried using HNS loss after the explicit coreset sampling, achieving a small gain in the Birds dataset.\nExplicit HNS with memory bank: We have compared SimCore with previous hard negative mining techniques: HNS [55] and EHNS [70]. However, they only slightly improved the performance because they still use all open-set samples as an anchor. To this end, we modified the sampling strategy of EHNS to better align with our conception.\nSpecifically, because EHNS is an explicit sampling method based on the similarities with an anchor, we can only use target data samples as the anchor and utilize hard negatives in the open-set. There are two ways to make it available:\n1. We can sample a minibatch from X+OS as before, but only calculates the EHNS loss with the target anchors.\n2. We can sample a minibatch only from X . The negative pairs are from other target sample\u2019s features and open-set features, where the open-set features are saved in a memory bank [28].\nFor the first strategy, the size difference between OS and X is so large that there are not enough target samples in a minibatch, which prevented the model from being converged. The second strategy is more reasonable in that a minibatch is comprised of only X samples, while the explicit hard negative sampling is done with other target features and open-set features.\nTable 17 presents the result of EHNS with memory bank (EHNS-m), showing that EHNS-m is not much effective. This is because negative pairs from the memory bank do not impose any gradients\u2013only the target samples are directly involved in learning. To make the pretraining effective, therefore, it is crucial to sample a coreset and employ the entire coreset samples into the training."
        }
    ],
    "title": "Coreset Sampling from Open-Set for Fine-Grained Self-Supervised Learning",
    "year": 2023
}