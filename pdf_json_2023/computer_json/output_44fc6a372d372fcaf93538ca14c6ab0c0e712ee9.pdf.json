{
    "abstractText": "Numerical modeling of localizations is a challenging task due to the evolving rough solution in which the localization paths are not predefined. Despite decades of efforts, there is a need for innovative discretization-independent computational methods to predict the evolution of localizations. In this work, an improved version of the neural network-enhanced Reproducing Kernel Particle Method (NN-RKPM) is proposed for modeling brittle fracture. In the proposed method, a background reproducing kernel (RK) approximation defined on a coarse and uniform discretization is enriched by a neural network (NN) approximation under a Partition of Unity framework. In the NN approximation, the deep neural network automatically locates and inserts regularized discontinuities in the function space. The NN-based enrichment functions are then patched together with RK approximation functions using RK as a Partition of Unity patching function. The optimum NN parameters defining the location, orientation, and displacement distribution across location together with RK approximation coefficients are obtained via the energy-based loss function minimization. To regularize the NN-RK approximation, a constraint on the spatial gradient of the parametric coordinates is imposed in the loss function. Analysis of the convergence properties shows that the solution convergence of the proposed method is guaranteed. The effectiveness of the proposed method is demonstrated by a series of numerical examples involving damage propagation and branching.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jonghyuk Baek"
        },
        {
            "affiliations": [],
            "name": "J. S. Chen"
        }
    ],
    "id": "SP:49e2b2babc1210fb4984aba6a844240309b6635f",
    "references": [
        {
            "authors": [
                "A.R. Barron"
            ],
            "title": "Universal approximation bounds for superpositions of a sigmoidal function",
            "venue": "IEEE Trans. Inf. Theory. 39 ",
            "year": 1993
        },
        {
            "authors": [
                "O. Calin"
            ],
            "title": "Deep learning architectures",
            "venue": "Springer",
            "year": 2020
        },
        {
            "authors": [
                "M. Mozaffar",
                "R. Bostanabad",
                "W. Chen",
                "K. Ehmann",
                "J. Cao",
                "M.A. Bessa"
            ],
            "title": "Deep learning predicts path-dependent plasticity",
            "venue": "Proc. Natl. Acad. Sci. 116 ",
            "year": 2019
        },
        {
            "authors": [
                "Q. He",
                "J.-S. Chen"
            ],
            "title": "A physics-constrained data-driven approach based on locally convex reconstruction for noisy database",
            "venue": "Comput. Methods Appl. Mech. Eng. 363 ",
            "year": 2020
        },
        {
            "authors": [
                "L. Wu",
                "V.D. Nguyen",
                "N.G. Kilingar",
                "L. Noels"
            ],
            "title": "A recurrent neural network-accelerated multi-scale model for elasto-plastic heterogeneous materials subjected to random cyclic and non-proportional loading paths",
            "venue": "Comput. Methods Appl. Mech. Eng. 369 ",
            "year": 2020
        },
        {
            "authors": [
                "X. He",
                "Q. He",
                "J.-S. Chen"
            ],
            "title": "Deep autoencoders for physics-constrained data-driven nonlinear materials modeling",
            "venue": "Comput. Methods Appl. Mech. Eng. 385 ",
            "year": 2021
        },
        {
            "authors": [
                "D.W. Abueidda",
                "S. Koric",
                "N.A. Sobh",
                "H. Sehitoglu"
            ],
            "title": "Deep learning for plasticity and thermo-viscoplasticity",
            "venue": "Int. J. Plast. 136 ",
            "year": 2021
        },
        {
            "authors": [
                "X. He",
                "J.-S. Chen"
            ],
            "title": "Thermodynamically consistent machine-learned internal state variable approach for data-driven modeling of path-dependent materials",
            "venue": "Comput. Methods Appl. Mech. Eng. 402 ",
            "year": 2022
        },
        {
            "authors": [
                "K. Lee",
                "K.T. Carlberg"
            ],
            "title": "Model reduction of dynamical systems on nonlinear manifolds using deep convolutional autoencoders",
            "venue": "J. Comput. Phys. 404 ",
            "year": 2020
        },
        {
            "authors": [
                "Y. Kim",
                "Y. Choi",
                "D. Widemann",
                "T. Zohdi"
            ],
            "title": "A fast and accurate physics-informed neural network reduced order model with shallow masked autoencoder",
            "venue": "J. Comput. Phys. 451 ",
            "year": 2022
        },
        {
            "authors": [
                "M. Raissi",
                "P. Perdikaris",
                "G.E. Karniadakis"
            ],
            "title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "venue": "J. Comput. Phys. 378 ",
            "year": 2019
        },
        {
            "authors": [
                "E. Haghighat",
                "M. Raissi",
                "A. Moure",
                "H. Gomez",
                "R. Juanes"
            ],
            "title": "A physics-informed deep 40 learning framework for inversion and surrogate modeling in solid mechanics",
            "venue": "Comput. Methods Appl. Mech. Eng. 379 ",
            "year": 2021
        },
        {
            "authors": [
                "K. Taneja",
                "X. He",
                "Q. He",
                "X. Zhao",
                "Y.-A. Lin",
                "K.J. Loh",
                "J.-S. Chen"
            ],
            "title": "A Feature-Encoded Physics-Informed Parameter Identification Neural Network for Musculoskeletal Systems",
            "venue": "J. Biomech. Eng. 144 ",
            "year": 2022
        },
        {
            "authors": [
                "C. Daux",
                "N. Mo\u00ebs",
                "J. Dolbow",
                "N. Sukumar",
                "T. Belytschko"
            ],
            "title": "Arbitrary branched and intersecting cracks with the extended finite element method",
            "venue": "Int. J. Numer. Methods Eng. 48 (2000) 1741\u20131760. https://doi.org/https://doi.org/10.1002/1097- 0207",
            "year": 2000
        },
        {
            "authors": [
                "N. Sukumar",
                "N. Mo\u00ebs",
                "B. Moran",
                "T. Belytschko"
            ],
            "title": "Extended finite element method for three-dimensional crack modelling",
            "venue": "Int. J. Numer. Methods Eng. 48 (2000) 1549\u20131570. https://doi.org/https://doi.org/10.1002/1097-0207",
            "year": 2000
        },
        {
            "authors": [
                "C.A. Duarte",
                "O.N. Hamzeh",
                "T.J. Liszka",
                "W.W. Tworzydlo"
            ],
            "title": "A generalized finite element method for the simulation of three-dimensional dynamic crack propagation",
            "venue": "Comput. Methods Appl. Mech. Eng. 190 ",
            "year": 1016
        },
        {
            "authors": [
                "T. Belytschko",
                "T. Black"
            ],
            "title": "Elastic crack growth in finite elements with minimal remeshing",
            "venue": "Int. J. Numer. Methods Eng. 45 (1999) 601\u2013620. https://doi.org/https://doi.org/10.1002/(SICI)1097-0207",
            "year": 1999
        },
        {
            "authors": [
                "J. Dolbow",
                "N. Mo\u00ebs",
                "T. Belytschko"
            ],
            "title": "Discontinuous enrichment in finite elements with a partition of unity method",
            "venue": "Finite Elem. Anal. Des. 36 ",
            "year": 1016
        },
        {
            "authors": [
                "D. Organ",
                "M. Fleming",
                "T. Terry",
                "T. Belytschko"
            ],
            "title": "Continuous meshless approximations for nonconvex bodies by diffraction and transparency",
            "venue": "Comput. Mech. 18 ",
            "year": 1996
        },
        {
            "authors": [
                "N. Sukumar",
                "B. Moran",
                "T. Black",
                "T. Belytschko"
            ],
            "title": "An element-free Galerkin method for three-dimensional fracture mechanics",
            "venue": "Comput. Mech. 20 ",
            "year": 1997
        },
        {
            "authors": [
                "B.Z. P",
                "J. Milan"
            ],
            "title": "Nonlocal Integral Formulations of Plasticity and Damage: Survey of Progress",
            "venue": "J. Eng. Mech",
            "year": 2002
        },
        {
            "authors": [
                "R.D. Mindlin"
            ],
            "title": "Second gradient of strain and surface-tension in linear elasticity",
            "venue": "Int. J. Solids Struct. 1 ",
            "year": 1016
        },
        {
            "authors": [
                "E.C. Aifantis"
            ],
            "title": "On the Microstructural Origin of Certain Inelastic Models",
            "venue": "J. Eng. Mater. Technol. 106 ",
            "year": 1984
        },
        {
            "authors": [
                "R. De Borst",
                "H.-B. M\u00fchlhaus"
            ],
            "title": "Gradient-dependent plasticity: Formulation and algorithmic aspects",
            "venue": "Int. J. Numer. Methods Eng. 35 ",
            "year": 1992
        },
        {
            "authors": [
                "C. Miehe",
                "M. Hofacker",
                "F. Welschinger"
            ],
            "title": "A phase field model for rate-independent crack propagation: Robust algorithmic implementation based on operator splits",
            "venue": "Comput. Methods Appl. Mech. Eng. 199 ",
            "year": 2010
        },
        {
            "authors": [
                "C. Miehe",
                "F. Welschinger",
                "M. Hofacker"
            ],
            "title": "Thermodynamically consistent phase-field models of fracture: Variational principles and multi-field FE implementations",
            "venue": "Int. J. Numer. Methods Eng. 83 ",
            "year": 2010
        },
        {
            "authors": [
                "M.J. Borden",
                "C. V Verhoosel",
                "M.A. Scott",
                "T.J.R. Hughes",
                "C.M. Landis"
            ],
            "title": "A phase-field description of dynamic brittle fracture",
            "venue": "Comput. Methods Appl. Mech. Eng. 217\u2013220 ",
            "year": 2012
        },
        {
            "authors": [
                "R.J.M. Geelen",
                "Y. Liu",
                "T. Hu",
                "M.R. Tupek",
                "J.E. Dolbow"
            ],
            "title": "A phase-field formulation for dynamic cohesive fracture",
            "venue": "Comput. Methods Appl. Mech. Eng. 348 ",
            "year": 2019
        },
        {
            "authors": [
                "E. Samaniego",
                "C. Anitescu",
                "S. Goswami",
                "V.M. Nguyen-Thanh",
                "H. Guo",
                "K. Hamdia",
                "X. Zhuang",
                "T. Rabczuk"
            ],
            "title": "An energy approach to the solution of partial differential equations in computational mechanics via machine learning: Concepts",
            "venue": "implementation and applications, Comput. Methods Appl. Mech. Eng. 362 ",
            "year": 2020
        },
        {
            "authors": [
                "L. Zhang",
                "L. Cheng",
                "H. Li",
                "J. Gao",
                "C. Yu",
                "R. Domel",
                "Y. Yang",
                "S. Tang",
                "W.K. Liu"
            ],
            "title": "Hierarchical deep-learning neural networks: finite elements and beyond",
            "venue": "Comput. Mech. 67 ",
            "year": 2021
        },
        {
            "authors": [
                "L. Lu",
                "P. Jin",
                "G. Pang",
                "Z. Zhang",
                "G.E. Karniadakis"
            ],
            "title": "Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators",
            "venue": "Nat. Mach. Intell. 3 ",
            "year": 2021
        },
        {
            "authors": [
                "L. Lu",
                "X. Meng",
                "Z. Mao",
                "G.E. Karniadakis"
            ],
            "title": "DeepXDE: A Deep Learning Library for Solving Differential Equations",
            "venue": "SIAM Rev. 63 ",
            "year": 2021
        },
        {
            "authors": [
                "J. Baek",
                "J.-S. Chen",
                "K. Susuki"
            ],
            "title": "A neural network-enhanced reproducing kernel particle method for modeling strain localization",
            "venue": "Int. J. Numer. Methods Eng. 123 ",
            "year": 2022
        },
        {
            "authors": [
                "E. Haghighat",
                "R. Juanes"
            ],
            "title": "SciANN: A Keras/TensorFlow wrapper for scientific computations and physics-informed deep learning using artificial neural networks",
            "venue": "Comput. Methods Appl. Mech. Eng. 373 ",
            "year": 2021
        },
        {
            "authors": [
                "M.M. Almajid",
                "M.O. Abu-Al-Saud"
            ],
            "title": "Prediction of porous media fluid flow using physics informed neural networks",
            "venue": "J. Pet. Sci. Eng. 208 ",
            "year": 2022
        },
        {
            "authors": [
                "E. Haghighat",
                "D. Amini",
                "R. Juanes"
            ],
            "title": "Physics-informed neural network simulation of multiphase poroelasticity using stress-split sequential training",
            "venue": "Comput. Methods Appl. Mech. Eng. 397 ",
            "year": 2022
        },
        {
            "authors": [
                "T. Chen",
                "H. Chen"
            ],
            "title": "Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems",
            "venue": "IEEE Trans. Neural Networks. 6 ",
            "year": 1995
        },
        {
            "authors": [
                "B. Shen",
                "O. Stephansson"
            ],
            "title": "Modification of the G-criterion for crack propagation subjected to compression",
            "venue": "Eng. Fract. Mech. 47 ",
            "year": 1016
        },
        {
            "authors": [
                "J.-S. Chen",
                "C.-T. Wu",
                "S. Yoon",
                "Y. You"
            ],
            "title": "A stabilized conforming nodal integration for Galerkin mesh-free methods",
            "venue": "Int. J. Numer. Methods Eng. 50 (2001) 435\u2013466. https://doi.org/https://doi.org/10.1002/1097-0207",
            "year": 1012
        },
        {
            "authors": [
                "H. Wei",
                "J.S. Chen"
            ],
            "title": "A damage particle method for smeared modeling of brittle fracture",
            "venue": "Int. J. Multiscale Comput. Eng. 16 ",
            "year": 2018
        },
        {
            "authors": [
                "H.-Y. Hu",
                "J.-S. Chen",
                "W. Hu"
            ],
            "title": "Error analysis of collocation method based on reproducing kernel approximation",
            "venue": "Numer. Methods Partial Differ. Equ. 27 ",
            "year": 2011
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A Method for Stochastic Optimization",
            "venue": "BT - 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, ",
            "year": 2015
        },
        {
            "authors": [
                "J. Nocedal",
                "S.J. Wright"
            ],
            "title": "Large-Scale Unconstrained Optimization, in: Numer. Optim",
            "year": 2006
        },
        {
            "authors": [
                "J.-S. Chen",
                "X. Zhang",
                "T. Belytschko"
            ],
            "title": "An implicit gradient model by a reproducing kernel strain regularization in strain localization problems",
            "venue": "Comput. Methods Appl. Mech. Eng. 193 ",
            "year": 2004
        },
        {
            "authors": [
                "A. Muix\u00ed",
                "A. Rodr\u00edguez-Ferran",
                "S. Fern\u00e1ndez-M\u00e9ndez"
            ],
            "title": "A hybridizable discontinuous Galerkin phase-field model for brittle fracture with adaptive refinement",
            "venue": "Int. J. Numer. Methods Eng. 121 ",
            "year": 2020
        },
        {
            "authors": [
                "A. Muix\u00ed",
                "O. Marco",
                "A. Rodr\u00edguez-Ferran",
                "S. Fern\u00e1ndez-M\u00e9ndez"
            ],
            "title": "A combined XFEM phase-field computational model for crack growth without remeshing",
            "venue": "Comput. Mech. 67 ",
            "year": 2021
        },
        {
            "authors": [
                "A. Bobet",
                "H.H. Einstein"
            ],
            "title": "Numerical modeling of fracture coalescence in a model rock material",
            "venue": "Int. J. Fract. 92 ",
            "year": 1998
        }
    ],
    "sections": [
        {
            "text": "* Corresponding author.\nE-mail address: jsc137@ucsd.edu\nNumerical modeling of localizations is a challenging task due to the evolving rough solution in which the localization paths are not predefined. Despite decades of efforts, there is a need for innovative discretization-independent computational methods to predict the evolution of localizations. In this work, an improved version of the neural network-enhanced Reproducing Kernel Particle Method (NN-RKPM) is proposed for modeling brittle fracture. In the proposed method, a background reproducing kernel (RK) approximation defined on a coarse and uniform discretization is enriched by a neural network (NN) approximation under a Partition of Unity framework. In the NN approximation, the deep neural network automatically locates and inserts regularized discontinuities in the function space. The NN-based enrichment functions are then patched together with RK approximation functions using RK as a Partition of Unity patching function. The optimum NN parameters defining the location, orientation, and displacement distribution across location together with RK approximation coefficients are obtained via the energy-based loss function minimization. To regularize the NN-RK approximation, a constraint on the spatial gradient of the parametric coordinates is imposed in the loss function. Analysis of the convergence properties shows that the solution convergence of the proposed method is guaranteed. The effectiveness of the proposed method is demonstrated by a series of numerical examples involving damage propagation and branching.\nKeywords: neural network, enrichment, reproducing kernel, fracture, damage"
        },
        {
            "heading": "1. Introduction",
            "text": "Neural networks (NNs) have been shown to have powerful approximation ability [1,2]. The strong adaptivity and hidden information extraction capability have made deep neural networks a core element of machine learning in various applications. This feature also makes NNs appealing for solving challenging problems in computational mechanics. For example, datadriven computations for path-dependent material modeling [3\u20138], reduced order modeling [9,10], and parameter identification [11\u201313]. Additionally, the flexible adaptivity in NN allows an approximation space to be goal-specifically optimized. Utilizing this flexibility in the approximation space, NNs can be considered an alternative to traditional mesh-based methods in solving challenging problems involving localizations, such as fracture, for which special treatment is needed near the localizations.\nTraditional approaches for fracture modeling can be divided into two broad categories: discrete crack approaches and diffuse crack approaches. The former category includes extended or generalized FEMs [14\u201316], partition of unity-based enrichment [17,18], and meshfree method with near-tip enrichment [19,20]. In these methods, strong discontinuities are directly inserted into the approximation, necessitating the detection and tracking of crack surfaces, significantly increasing the complexity of the computation for multidimensional problems. Nonlocal averaging [21], high order gradient models [22\u201324], and phase field methods [25\u201328] have been employed in the diffuse crack approaches. In this family of methods, nonlocal effects are typically introduced in the approximation or in the energy function, yielding diffused, regularized representation of cracks. This property enables traditional mesh-based or meshfree methods to approximate localizations without enrichment and the need for localization tracking. However, for sufficient accuracy, intense mesh refinement is required in the regions of localizations. For example, Geelen et al. (2019) [28] used an element size as small as one-tenth the width of the diffuse crack.\nWith their adaptive nature as an approximation, NNs provide a new paradigm in searching for solutions of mathematical models. Recently, NNs have been successfully applied as a solver of partial differential equations [11,12,29\u201333]. In physics-informed neural network (PINN)by Raissi et al. ) [11,12], the solution of a PDE is approximated by densely-connected deep neural networks with the residual-based loss function minimization. Haghighat and Juanes (2021) [34] developed the Python package SciANN for scientific computing using PINN and demonstrated its ability to capture strain and stress localization in a perfectly plastic material. More recently, PINNs have been extended to multi-physics problems [35,36]. However, one drawback of utilizing a deep neural network combined with a residual-based and collocated loss function is its computational cost, e.g., in [34], where 100 million unknown weights and biases were used. Samaniego et al. (2020) [29] demonstrated that potential-based loss functions produced superior results with significantly fewer unknowns than the residual-based loss function commonly used in PINN. Zhang et al. (2021) [30] proposed a deep neural network that reproduces standard approximations along with automatic refinement enabled by treating nodal positions as unknown network parameters, which, however, introduces sparsity into the neural network. Lu et al. (2021) [31], based on the universal approximation theorem [37], designed a new deep neural network architecture, in which the output of one deep neural network is multiplied by the output of another deep neural network, resulting effective approximations of nonlinear operators in partial differential equations.\nDespite the growing interest in PINNs, there has been limited research on developing effective and computationally efficient NN-based approximation for modeling localizations. Baek et al. (2022) [33] proposed a neural network-enhanced reproducing kernel particle method (NNRKPM) for modeling localizations. In this work, the approximation is constructed as the superposition of the NN approximation and the reproducing kernel (RK) approximation. For computational efficiency, NNs are limited to approximating localizations, while the RK approximation on a coarse and uniform discretization is employed to approximate the smooth solutions. In this approach, the NN approximation control parameters play the role in automatically capturing the location, orientation, and the localization profile at the localizations. These NN parameters are determined by the optimization of an energy-based loss function. In this work, we propose an improved version of NN-RKPM in which the NN approximation and\nthe background RK approximation are patched together with Partition of Unity for ensured convergence. This approach is derived through an NN-based correction of standard RK shape functions. In the modified NN-RK approximation, the deep neural network automatically locates and inserts regularized discontinuities in the function space, and the NN enriched RK coefficient function provides varying magnitude of the discontinuity along the localization path. Additionally, convergence properties of the proposed method are analyzed.\nThe paper is organized as follows. In Section 2, the basic equations are provided, including the minimization problem for brittle fracture and the reproducing kernel particle method. In Section 3, a neural network-enriched Partition of Unity reproducing kernel approximation is proposed, along with convergence analysis and regularization technique. In Section 4, the implementation details including the neural network architecture and solution procedure are provided. This is followed by numerical examples in Section 5 and concluding remarks in Section 6."
        },
        {
            "heading": "2. Background",
            "text": ""
        },
        {
            "heading": "2.1. Minimization Problem for Fracture",
            "text": "For a domain \u03a9 \u2208 \u211d\ud835\udc51 with the space dimension \ud835\udc51 and its boundary \ud835\udf15\u03a9 = \u2202\u03a9\ud835\udc54 \u222a \u2202\u03a9\u210e that consists of the Dirichlet boundary \ud835\udf15\u03a9\ud835\udc54 and the Neumann boundary \ud835\udf15\u03a9\u210e, let us consider the following minimization problem: for \ud835\udc2e \u2208 \ud835\udc3b1, \ud835\udc2e = \ud835\udc20 on \ud835\udf15\u03a9\ud835\udc54,\nmin \ud835\udc2e \u03a0(\ud835\udc2e) = \u222b \ud835\udf13(\ud835\udc2e) \ud835\udc51\u03a9 \u03a9 \u2212 \u222b \ud835\udc2e \u22c5 \ud835\udc1b \ud835\udc51\u03a9 \u03a9 \u2212 \u222b \ud835\udc2e \u22c5 \ud835\udc21 \ud835\udc51\u0393 \u2202\u03a9h , (1)\nwhere \ud835\udc2e, \ud835\udf13(\ud835\udc2e), \ud835\udc1b, and \ud835\udc21 are the displacement, energy density functional, body force, and traction, respectively. The energy density functional \ud835\udf13(\ud835\udc2e) has the following form:\n\ud835\udf13(\ud835\udc2e) = \ud835\udc54 (\ud835\udf02(\ud835\udec6(\ud835\udc2e))) \ud835\udf130 +(\ud835\udc2e) + \ud835\udf130 \u2212(\ud835\udc2e) + ?\u0305? (\ud835\udf02(\ud835\udec6(\ud835\udc2e))). (2)\nHerein, \ud835\udec6 = 1\n2 (\u2207\ud835\udc2e + (\u2207\ud835\udc2e)\ud835\udc47) , \ud835\udf02 , and \ud835\udc54 are the strain tensor, the (strain dependent) damage\nvariable, and the degradation function, respectively. Three energy density components \ud835\udf130 +, \ud835\udf130 \u2212, and ?\u0305? denote non-degraded tensile strain energy, compressive strain energy, and dissipation functional, respectively. The tensile and compressive strain energies are defined as\n\ud835\udf130 = \ud835\udf07\ud835\udf00?\u0305?\ud835\udf00?\u0305? + \ud835\udf06\n2 \ud835\udc61\ud835\udc5f(?\u0305?)2,\n\ud835\udf130 + = \ud835\udf07\u2329\ud835\udf00?\u0305?\u232a+\u2329\ud835\udf00?\u0305?\u232a+ +\n\ud835\udf06 2 \u2329\ud835\udc61\ud835\udc5f(?\u0305?)\u27e9+ 2 ,\n\ud835\udf130 \u2212 = \ud835\udf130 \u2212 \ud835\udf130 +,\n(3)\nwhere the summation notation is adopted. In (3), ?\u0305?, \ud835\udf06, and \ud835\udf07 are principal strain, Lam\u00e9\u2019s first and second parameters, respectively. \u2329\u22c5\u27e9+ = max(\u22c5 ,0) and \u2329\u22c5\u27e9\u2212 = min(\u22c5 ,0) are additionally used. The stress is defined as\n\ud835\uded4 = \ud835\udc54(\ud835\udf02(\ud835\udec6)) \ud835\udf15\ud835\udf130\n+\n\ud835\udf15\ud835\udec6 +\n\ud835\udf15\ud835\udf130 \u2212\n\ud835\udf15\ud835\udec6 . (4)\nIn this work, the damage variable, dissipation functional, and degradation function are defined as follows:\n\ud835\udf02 = \ud835\udf130\n+\n\ud835\udf130 + + \ud835\udc5d\n(5)\n?\u0305? = \ud835\udc5d\ud835\udf022, (6)\n\ud835\udc54 = (1 \u2212 \ud835\udf02)2, (7)\nwhere \ud835\udc5d is a fracture energy-dependent material property. The adopted dissipation functional and degradation function in Eqs. (6) and (7) are the same as what is used in Miehe et al. (2010)[25] except the absence of the higher order term \ud835\udcaa(\u2207\ud835\udf022) in the dissipation functional in (6). Therefore, it is straightforward to show that the damage model in Eqs. (5)-(7) is variationally consistent, i.e., for \ud835\udc2e \u2208 \ud835\udc3b1, \ud835\udc2e = \ud835\udc20 on \ud835\udf15\u03a9\ud835\udc54, for all \ud835\udeff\ud835\udc2e \u2208 \ud835\udc3b 1, \ud835\udeff\ud835\udc2e = \ud835\udfce on \ud835\udf15\u03a9\ud835\udc54,\n\ud835\udeff\u03a0 = \u222b \ud835\udeff\ud835\udec6(\ud835\udc2e): \ud835\uded4(\ud835\udec6) \ud835\udc51\u03a9 \u03a9 = \u222b \ud835\udeff\ud835\udc2e \u22c5 \ud835\udc1b \ud835\udc51\u03a9 \u03a9 + \u222b \ud835\udeff\ud835\udc2e \u22c5 \ud835\udc21 \ud835\udc51\u0393 \u2202\u03a9\u210e , (8)\nwhich leads to the following balance equation:\n\u2207 \u22c5 \ud835\uded4 + \ud835\udc1b = 0 in \u03a9, (9)\nwith the boundary conditions\n\ud835\udc2e = \ud835\udc20 on \ud835\udf15\u03a9\ud835\udc54, (10)\n\u2207\ud835\udc2e \u22c5 \ud835\udc27 = \ud835\udc21 on \ud835\udf15\u03a9\u210e, (11)\nwhere \ud835\udc27 denotes the surface normal vector.\nTo achieve the irreversibility of the damage, a history variable\n\u210b = max ( max \ud835\udc61\u2208[0,\ud835\udc47]\n{\ud835\udf130 +(\ud835\udec6) \u2212 \ud835\udf13\ud835\udc50} , 0) (12)\nis employed to describe the damage variable:\n\ud835\udf02 = \u210b\n\u210b + \ud835\udc5d . (13)\nFor Eq. (12), the critical fracture energy \ud835\udf13\ud835\udc50 is defined as\n\ud835\udf13\ud835\udc50 = \ud835\udc53\ud835\udc61\n2\ud835\udc38 (14)\nwith the tensile strength of material \ud835\udc53\ud835\udc61 and Young\u2019s modulus \ud835\udc38. The model parameter \ud835\udc5d takes the following form\n\ud835\udc5d = \ud835\udca2\ud835\udc50 \u2113 , (15)\nwith critical energy release rate \ud835\udca2\ud835\udc50 and length scale parameter \u2113. To take mixed mode fracture into account, we adopt the \u2131-criterion[38], with the mode I critical energy release rate \ud835\udca2\ud835\udc50\ud835\udc3c and the mode II critical energy release rate \ud835\udca2\ud835\udc50\ud835\udc3c\ud835\udc3c:\n\u2131 \u2261 \ud835\udf130\n+\n\ud835\udca2\ud835\udc50 \u2248\n\ud835\udf13\ud835\udc3c + \ud835\udca2\ud835\udc50\ud835\udc3c + \ud835\udf13\ud835\udc3c\ud835\udc3c + \ud835\udca2\ud835\udc50\ud835\udc3c\ud835\udc3c , (16)\nwith\n\ud835\udf13\ud835\udc3c + =\n\ud835\udf06 2 \u2329\u2211\ud835\udf00?\u0305?\u27e9+ 2 , (17)\n\ud835\udf13\ud835\udc3c\ud835\udc3c + = \ud835\udf07\u2329\ud835\udf00?\u0305?\u232a+\u2329\ud835\udf00?\u0305?\u232a+. (18)\nEq. (16) leads to the following critical energy release rate:\n\ud835\udca2\ud835\udc50 = \ud835\udf130\n+\n\ud835\udf13\ud835\udc3c +/\ud835\udca2\ud835\udc50\ud835\udc3c + \ud835\udf13\ud835\udc3c\ud835\udc3c +/\ud835\udca2\ud835\udc50\ud835\udc3c\ud835\udc3c . (19)\nNote that Eq. (19) implies \ud835\udca2\ud835\udc50 = \ud835\udca2\ud835\udc50\ud835\udc3c for pure mode I fracture when \ud835\udf130 + = \ud835\udf13\ud835\udc3c + and \ud835\udca2\ud835\udc50 = \ud835\udca2\ud835\udc50\ud835\udc3c\ud835\udc3c for pure mode II fracture when \ud835\udf130 + = \ud835\udf13\ud835\udc3c\ud835\udc3c +.\nRemark 1.1. With \ud835\udca2\ud835\udc50 defined in (19) which is a function of strain, the functional \u03a0 defined in (1) is not a minimization functional for the Euler-Lagrange equation (9). Therefore, in this work, we solve the minimization problem in (1) and the \ud835\udca2\ud835\udc50 calculation in (19) in a staggered manner.\nRemark 1.2. Different from the phase field fracture methods, the damage model described in this section is a local model in the absence of the higher order term in the dissipation functional. Therefore, there is possibility of the loss of ellipticity and the discretization-dependence of the numerical solution. This issue will be addressed in Section 3.3."
        },
        {
            "heading": "2.2. Reproducing kernel particle method for background approximation",
            "text": "Here we review the standard reproducing kernel particle method (RKPM) that is used to approximate smooth part of the solution in the proposed approach (see Section 3)."
        },
        {
            "heading": "2.2.1. Reproducing kernel approximation",
            "text": "Let \u03a9 be a domain discretized by \ud835\udc41\ud835\udc43 nodes with nodal coordinate {\ud835\udc31\ud835\udc3c}\ud835\udc3c\u2208\ud835\udcae with a node set \ud835\udcae = {1, \u22ef , \ud835\udc41\ud835\udc43}. The reproducing kernel (RK) approximation, \ud835\udc62\ud835\udc45\ud835\udc3e(\ud835\udc31), of a function \ud835\udc62(\ud835\udc31) is\n\ud835\udc62 \ud835\udc45\ud835\udc3e(\ud835\udc31) = \u2211 \u03a8\ud835\udc3c(\ud835\udc31)\ud835\udc51\ud835\udc3c\n\ud835\udc3c\u2208\ud835\udcae\n, (20)\nwith an RK shape function \u03a8\ud835\udc3c(\ud835\udc31) and a generalized nodal coefficient \ud835\udc51\ud835\udc3c. The RK shape function is a correction of a kernel function, \u03a6\ud835\udc4e(\ud835\udc31 \u2212 \ud835\udc31\ud835\udc3c), defined on the compact support of node \ud835\udc3c with a support size of \ud835\udc4e:\n\u03a8\ud835\udc3c(\ud835\udc31) = \ud835\udc36\ud835\udc3c(\ud835\udc31)\u03a6\ud835\udc4e(\ud835\udc31 \u2212 \ud835\udc31\ud835\udc3c), (21)\nwhere the kernel correction function \ud835\udc36\ud835\udc3c(\ud835\udc31) is defined as\n\ud835\udc36\ud835\udc3c(\ud835\udc31) \u2261 { \u2211 (\ud835\udc31 \u2212 \ud835\udc31\ud835\udc3c) \ud835\udec2\ud835\udc4f\ud835\udec2(\ud835\udc31)\n|\ud835\udec2|\u2264\ud835\udc5b\n}, (22)\nwhere (\ud835\udc31 \u2212 \ud835\udc31\ud835\udc3c) \ud835\udec2 is a basis function, \ud835\udec2 = (\ud835\udefc1, \ud835\udefc2, \u2026 , \ud835\udefc\ud835\udc51) is a multi-dimensional index, and |\ud835\udec2| \u2261 \u2211 \ud835\udefc\ud835\udc56 \ud835\udc51 \ud835\udc56=1 . \ud835\udc31 \ud835\udefc is defined as\n\ud835\udc31\ud835\udec2 \u2261 \ud835\udc651 \ud835\udefc1 \u22c5 \ud835\udc652 \ud835\udefc2 \u22c5 \u2026 \u22c5 \ud835\udc65\ud835\udc51 \ud835\udefc\ud835\udc51 . (23)\nThe coefficients, \ud835\udc4f\ud835\udec2(\ud835\udc31), are obtained by solving the following set of reproducing conditions:\n\u2211 \u03a8\ud835\udc3c(\ud835\udc31)\ud835\udc31\ud835\udc3c \ud835\udec2 \ud835\udc3c\u2208\ud835\udcae = \ud835\udc31\ud835\udec2, |\ud835\udec2| \u2264 \ud835\udc5b. (24)\nThe results RK shape function takes the following explicit form:\n\u03a8\ud835\udc3c(\ud835\udc31) = \ud835\udc07 \ud835\udc47(\ud835\udfce)\ud835\udc0c\u22121(\ud835\udc31)\ud835\udc07(\ud835\udc31 \u2212 \ud835\udc31\ud835\udc3c)\u03a6\ud835\udc4e(\ud835\udc31 \u2212 \ud835\udc31\ud835\udc3c), (25)\nwhere the moment matrix \ud835\udc0c(\ud835\udc31) and the basis vector \ud835\udc07(\ud835\udc31 \u2212 \ud835\udc31\ud835\udc3c) are defined as\n\ud835\udc0c(\ud835\udc31) = \u2211 \ud835\udc07(\ud835\udc31 \u2212 \ud835\udc31\ud835\udc3c)\ud835\udc07 \ud835\udc47(\ud835\udc31 \u2212 \ud835\udc31\ud835\udc3c)\u03a6\ud835\udc4e(\ud835\udc31 \u2212 \ud835\udc31\ud835\udc3c)\n\ud835\udc3c\u2208\ud835\udcae\n, (26)\n\ud835\udc07(\ud835\udc31 \u2212 \ud835\udc31\ud835\udc3c) = [1, (\ud835\udc651 \u2212 \ud835\udc651\ud835\udc3c), (\ud835\udc652 \u2212 \ud835\udc652\ud835\udc3c), (\ud835\udc653 \u2212 \ud835\udc653\ud835\udc3c), \u22ef , (\ud835\udc653 \u2212 \ud835\udc653\ud835\udc3c) \ud835\udc5b]\ud835\udc47 . (27)\nThe kernel function \u03a6\ud835\udc4e(\ud835\udc31 \u2212 \ud835\udc31\ud835\udc3c) determines the order of continuity, while the basis vector \ud835\udc07(\ud835\udc31 \u2212 \ud835\udc31\ud835\udc3c) determines the polynomial completeness. Thus, it is straightforward to introduce high order continuity into the approximation space, independent of the basis order, which makes the RK approximation more appealing for approximating the smooth part of solution than the C0 interpolation-type approximations used in finite element methods. Figure 1 shows a smooth RK shape function constructed on the linear basis.\nFor a quasi-uniform RK points distribution, the following global error estimation of standard RK approximation \ud835\udc62\ud835\udc45\ud835\udc3e holds, for \ud835\udc62 \u2208 \ud835\udc3b\ud835\udc5f, [41]\n\u2016\ud835\udc62\ud835\udc45\ud835\udc3e \u2212 \ud835\udc62\u2016\ud835\udc59,\u03a9 \u2264 \ud835\udc36\ud835\udc58\ud835\udc4e \ud835\udefe|\ud835\udc62|\ud835\udc5d+1,\u03a9, (28)\nwhere \ud835\udc4e, \ud835\udc36, \ud835\udc58, \ud835\udc5d, and \ud835\udefe = min(\ud835\udc5d + 1 \u2212 \ud835\udc59, \ud835\udc5f \u2212 \ud835\udc59) are the support size, a generic constant, the number of overlapping points, the order of RK basis, and the convergence rate, respectively."
        },
        {
            "heading": "2.2.2. Stabilized conforming nodal integration",
            "text": "When Gauss integration (GI) is used for RKPM, a significantly high-order rule is required to yield optimal solution convergence, due to the rational shape function given in Eq. (26). This, in turn, leads to a significant increase in computational cost. To address this issue, the stabilized conforming nodal integration (SCNI) was proposed in [39]. SCNI enables optimal solution convergence for RKPM with a linear basis by satisfying the linear integration constraint. Compared to high-order GI, SCNI is computationally much more efficient as it eliminates the need to evaluate direct derivatives of RK shape functions at a large number of integration points. Additionally, Wei and Chen (2018) [40] show that the strain smoothing employed in SCNI helps to suppress spurious stress oscillation that can arise in localization problems. For this reason, SCNI is utilized to perform the domain integration required in Eq. (1).\nIn SCNI, the domain is partitioned into \ud835\udc41\ud835\udc3c\ud835\udc36 conforming smoothing cells, such as Voronoi cells, as illustrated in Figure 2 where \ud835\udc41\ud835\udc3c\ud835\udc36 denotes the number of smoothing cells. Note that, while \ud835\udc41\ud835\udc3c\ud835\udc36 coincides with the number of particles for standard meshfree methods, the smoothing cells can be further refined to improve accuracy.\nThe integration of the loss function by SCNI is performed as follows:\n\u222b \ud835\udf13(\ud835\udc2e\u210e, \u2207\ud835\udc2e\u210e) \ud835\udc51\u03a9 \u03a9 \u2248 \u2211 \ud835\udf13 (\ud835\udc2e\u210e(\ud835\udc31\ud835\udc3f), \u2207\u0303\ud835\udc2e \u210e(\ud835\udc31\ud835\udc3f)) \ud835\udc49\ud835\udc3f\n\ud835\udc41\ud835\udc3c\ud835\udc36\n\ud835\udc3f\n, (29)\nwhere \u2207\u0303\ud835\udc2e\u210e is the smoothed gradient of \ud835\udc2e defined as\n\u2207\u0303\ud835\udc2e\u210e(\ud835\udc31\ud835\udc3f) \u2261 1\n\ud835\udc49\ud835\udc3f \u222b \ud835\udc2e\u210e(\ud835\udc31) \u2297 \ud835\udc27(\ud835\udc31) \ud835\udc51\u0393\n\u0393\ud835\udc3f\n\u2248 1\n\ud835\udc49\ud835\udc3f \u2211 \ud835\udc2e\u210e(\ud835\udc31\ud835\udc3f \ud835\udc58) \u2297 \ud835\udc5b\ud835\udc3f \ud835\udc58\n\ud835\udc41\ud835\udc60\ud835\udc52\ud835\udc54 \ud835\udc3f\n\ud835\udc58=1\n, (30)\nwhere \ud835\udc49\ud835\udc3f, \ud835\udc31\ud835\udc3f \ud835\udc58 , \ud835\udc5b\ud835\udc3f \ud835\udc58 , and \ud835\udc41\ud835\udc60\ud835\udc52\ud835\udc54 \ud835\udc3f are the cell volume, the centroid of \ud835\udc58-th boundary segment, the surface normal of \ud835\udc58 -th boundary segment, and the number of boundary segments of the integration cell \ud835\udc3f, respectively."
        },
        {
            "heading": "3. Neural Network-enhanced Reproducing Kernel Approximation",
            "text": "Figure 3 schematically illustrates a domain discretization by quasi-uniformly distributed background RK nodes, along with the evolving localizations in the domain. It is expected that\nthe true solution would be rough near localizations and smooth in the remaining part of the domain. As discussed in Section 2.2.1, the RK approximation is intended to capture the smooth part of the solution. With the enrichment function (to be constructed) near the evolving localizations, the total solution is constructed by superposing a background RK approximation \ud835\udc62\ud835\udc45\ud835\udc3e(\ud835\udc31) and a neural network (NN) approximation \ud835\udc62\ud835\udc41\ud835\udc41(\ud835\udc31) as follows: for \ud835\udc31 \u2208 \u03a9,\n\ud835\udc62\u210e(\ud835\udc31) = \ud835\udc62\ud835\udc45\ud835\udc3e(\ud835\udc31) + \ud835\udc62\ud835\udc41\ud835\udc41(\ud835\udc31), (31)\nwhere \ud835\udc62\u210e(\ud835\udc31) is an NN-enhanced RK (NN-RK) approximation. With this construction, uniform RK discretization is considered as a background discretization, and the localized solution will be represented by the NN approximation. The NN-RK approximation utilizes the RK approximation\u2019s flexibility in selecting the order of continuity and the order of monomial bases."
        },
        {
            "heading": "3.1.1. A neural network-based correction of RK approximation",
            "text": "In this section, we derive the NN-RK approximation through a neural network-based correction (NN-correction) of an RK approximation. Let \u03a9 be a domain discretized by \ud835\udc41\ud835\udc43 background RK nodes with nodal coordinate {\ud835\udc31\ud835\udc3c}\ud835\udc3c\u2208\ud835\udcae in a node set \ud835\udcae = {1, \u22ef , \ud835\udc41\ud835\udc43}. In addition, define a node subset \ud835\udcae\u0305 that contains the nodes with the associated RK shape functions to be corrected near localization. In this work, \ud835\udcae\u0305 = {\ud835\udc3d | \u2203\ud835\udc31 \u2208 \ud835\udc60\ud835\udc62\ud835\udc5d\ud835\udc5d(\u03a8\ud835\udc3d), \ud835\udf130 +(\ud835\udc31) \u2265 \ud835\udf05\ud835\udf13\ud835\udc50} with \ud835\udf05 = 0.5 is applied. We start with the following NN-corrected RK approximation:\n\ud835\udc62\u210e(\ud835\udc31) = \u2211 \u03a8\u0305\ud835\udc3c(\ud835\udc31)?\u0305?\ud835\udc3c \ud835\udc3c\u2208\ud835\udcae , (32)\nwhere the NN-corrected RK shape function \u03a8\u0305\ud835\udc3c(\ud835\udc31) is defined as follows:\n\u03a8\u0305\ud835\udc3c(\ud835\udc31) = {\n\ud835\udc36?\u0305?(\ud835\udc31)\u03a8\ud835\udc3c(\ud835\udc31), \ud835\udc3c \u2208 \ud835\udcae\u0305\n\u03a8\ud835\udc3c(\ud835\udc31), \ud835\udc3c \u2208 \ud835\udcae\\\ud835\udcae\u0305, (33)\nwhere \u03a8\ud835\udc3c(\ud835\udc31) and \ud835\udc36?\u0305?(\ud835\udc31) denote the original RK shape function defined in Section 2.2.1 and an NN-correction function, respectively. The NN-correction function takes the following form of a neural network with \ud835\udc5b neurons possessed by the last hidden layer:\n\ud835\udc36?\u0305?(\ud835\udc31) \u2261 ?\u0305?\ud835\udc3c + \u2211 ?\u0305?\ud835\udc3c\ud835\udc3e\ud835\udf01\ud835\udc3c\ud835\udc3e(\ud835\udc31)\n\ud835\udc5b\n\ud835\udc3e=1\n, (34)\nwhere ?\u0305?\ud835\udc3c, ?\u0305?\ud835\udc3c\ud835\udc3e, and \ud835\udf01\ud835\udc3c\ud835\udc3e(\ud835\udc31) denote bias, weight, and last hidden layer\u2019s output. By substituting (33) and (34) into (32) and defining \ud835\udc51\ud835\udc3c = ?\u0305?\ud835\udc3c?\u0305?\ud835\udc3c and \ud835\udc64\ud835\udc3c\ud835\udc3e\n\ud835\udc36 = ?\u0305?\ud835\udc3c\ud835\udc3e?\u0305?\ud835\udc3c, we have a general expression of NN-RK approximation as follows:\n\ud835\udc62\u210e(\ud835\udc31) = \ud835\udc62\ud835\udc45\ud835\udc3e(\ud835\udc31) + \ud835\udc62\ud835\udc41\ud835\udc41(\ud835\udc31), (35)\n\ud835\udc62\ud835\udc45\ud835\udc3e = \u2211 \u03a8\ud835\udc3c(\ud835\udc31)\ud835\udc51\ud835\udc3c \ud835\udc3c\u2208\ud835\udcae , (36)\n\ud835\udc62\ud835\udc41\ud835\udc41 = \u2211 \u2211 \u03a8\ud835\udc3c(\ud835\udc31)\ud835\udf01\ud835\udc3c\ud835\udc3e(\ud835\udc31)\ud835\udc64\ud835\udc3c\ud835\udc3e \ud835\udc36\n\ud835\udc5b\n\ud835\udc3e=1\ud835\udc3c\u2208?\u0305?\n. (37)\nRemark 3.1. The background RK approximation \ud835\udc62\ud835\udc45\ud835\udc3e(\ud835\udc31) in (36) is a standard RK approximation based on a polynomial RK basis. Meanwhile, the NN approximation \ud835\udc62\ud835\udc41\ud835\udc41(\ud835\udc31) in (37) contains nonstandard adaptive basis functions, which enables it to capture localized material responses with a coarse background RK discretization.\nRemark 3.2. As the RK shape functions possess the property of partition of unity, the NN-RK approximation\n\ud835\udc62\u210e(\ud835\udc31) = \ud835\udc62\ud835\udc45\ud835\udc3e(\ud835\udc31) + \ud835\udc62\ud835\udc41\ud835\udc41(\ud835\udc31) = \u2211 \u03a8\ud835\udc3c(\ud835\udc31) (\ud835\udc51\ud835\udc3c + \u2211 \ud835\udf01\ud835\udc3c\ud835\udc3e(\ud835\udc31)\ud835\udc64\ud835\udc3c\ud835\udc3e \ud835\udc36\n\ud835\udc5b\n\ud835\udc3e=1\n)\n\ud835\udc3c\u2208\ud835\udcae\n,\n\ud835\udc64\ud835\udc3c\ud835\udc3e \ud835\udc36 = 0, \u2200\ud835\udc3c \u2208 \ud835\udcae\\\ud835\udcae\u0305\n(38)\ncan be viewed as patching the RK and NN approximations under the Partition of Unity framework.\nRemark 3.3. In (37), \ud835\udf01\ud835\udc3c\ud835\udc3e(\ud835\udc31) is the activated output of \ud835\udc3e-th neuron in the last hidden layer of a neural network associated with node \ud835\udc3c . By having \ud835\udf01\ud835\udc3c\ud835\udc3e(\ud835\udc31) \u2261 \ud835\udf01\ud835\udc3e(\ud835\udc31) for all \ud835\udc3c \u2208 \ud835\udcae\u0305 , \ud835\udf01\ud835\udc3e(\ud835\udc31) is detached from a specific background node and becomes a flexible foreground quantity. Then, the NN approximation in (37) can be rewritten as follows:\n\ud835\udc62\ud835\udc41\ud835\udc41 = \u2211 \ud835\udf01\ud835\udc3e(\ud835\udc31)\ud835\udc63\ud835\udc3e(\ud835\udc31)\n\ud835\udc5b\n\ud835\udc3e=1\n, (39)\n\ud835\udc63\ud835\udc3e(\ud835\udc31) \u2261 \u2211 \u03a8\ud835\udc3c(\ud835\udc31)\ud835\udc64\ud835\udc3c\ud835\udc3e \ud835\udc36\n\ud835\udc3c\u2208?\u0305?\n. (40)\nRemark 3.4. The neural network to generate \ud835\udf01\ud835\udc3c\ud835\udc3e(\ud835\udc31) can be either a traditional or a nonstandard neural network. In section 3.1.2, we present a modified deep neural network designed to effectively capture localizations."
        },
        {
            "heading": "3.1.2. Block-level neural network approximation",
            "text": "In this work, we introduce a modified deep neural network to increase the sparsity of the network architecture, improve the interpretability, and capture localizations effectively. In this regard, the following block-level NN approximation is introduced.\n\ud835\udc62\ud835\udc41\ud835\udc41 = \u2211 \ud835\udc62\ud835\udc3d \ud835\udc35(\ud835\udc31)\n\ud835\udc5b\ud835\udc35\n\ud835\udc3d=1\n, (41)\nwhere \ud835\udc5b\ud835\udc35 is the number of NN blocks, and the block-level NN approximation \ud835\udc62\ud835\udc3d \ud835\udc35(\ud835\udc31) is defined as follows:\n\ud835\udc62\ud835\udc3d \ud835\udc35(\ud835\udc31) = \u2211 ?\u0302?\ud835\udc3d\ud835\udc3e(\ud835\udc31)\ud835\udc63\ud835\udc3d\ud835\udc3e(\ud835\udc31)\n\ud835\udc5b\ud835\udc41\ud835\udc3e\n\ud835\udc3e=1\n, (42)\n\ud835\udc63\ud835\udc3d\ud835\udc3e(\ud835\udc31) = \u2211 \u03a8\ud835\udc3c(\ud835\udc31)?\u0302?\ud835\udc3c\ud835\udc3d\ud835\udc3e \ud835\udc36\n\ud835\udc3c\u2208?\u0305?\n, (43)\nwhere ?\u0302?\ud835\udc3d\ud835\udc3e(\ud835\udc31) and \ud835\udc5b\ud835\udc41\ud835\udc3e are \ud835\udc3e-th NN kernel function in \ud835\udc3d-th NN block and the number of NN kernel functions per NN block, respectively. Note that (41)-(43) are shown to be equivalent to (39) and (40) by flattening the indices \ud835\udc3d\ud835\udc3e in (42) and (43) into \ud835\udc3e.\nFigure 4 illustrates the modified network architecture of \ud835\udc3d -th NN block, for which the construction is made so that the neural network approximation can capture complicated localization topologies effectively. Also, the construction of the neural network at the block level significantly increases the sparsity of the weight matrices, compared to the densely connected standard deep neural networks utilized in many previous studies in literature [29,34]. As shown in Figure 4, three sets of unknown parameters are involved in the NN approximation: the location-control weight set \ud835\udc16\ud835\udc3d \ud835\udc3f , the shape-control weight set \ud835\udc16\ud835\udc3d \ud835\udc46 as well as the NN-\ncorrection weight set \ud835\udc16\ud835\udc3d \ud835\udc36 = {{?\u0302?\ud835\udc3c\ud835\udc3d\ud835\udc3e \ud835\udc36 } \ud835\udc3c\u2208?\u0305? } \ud835\udc3e=1\n\ud835\udc5b\ud835\udc41\ud835\udc3e in (43). These parameters are to be automatically\ndetermined by solving the minimization problem (1). Details on the sub-blocks described in\nFigure 4 and their associated unknown parameters are explained in the following subsections."
        },
        {
            "heading": "3.1.3. Parametrization sub-block",
            "text": "As shown in Figure 4, the parametric coordinate \ud835\udc32\ud835\udc3d in Layer PC is the output of the parametrization sub-block, which is an intermediate variable of a densely connected deep neural network \ud835\udca9: \ud835\udc31 \u2192 \ud835\udc32\ud835\udc3d that takes \ud835\udc31 \u2208 \u211d \ud835\udc51 and \ud835\udc32\ud835\udc3d \u2261 \ud835\udc32(\ud835\udc31; \ud835\udc16\ud835\udc3d \ud835\udc3f) \u2208 \u211d\ud835\udc51 as its input and output, respectively. The parametrization projects complicated localization patterns onto a parametric space, so that complicated localizations can be captured with NN kernel functions in a simple mathematical form. With \ud835\udc5b\ud835\udc3b\ud835\udc3f hidden layers, the function \ud835\udc32(\ud835\udc31; \ud835\udc16\ud835\udc3d \ud835\udc3f) is defined as\n\ud835\udc32(\ud835\udc31; \ud835\udc16\ud835\udc3d \ud835\udc3f) = \ud835\udc1f(\u22c5; {\ud835\udc30\ud835\udc3d(\ud835\udc5b\ud835\udc3b\ud835\udc3f+1) \ud835\udc3f , \ud835\udc4f\ud835\udc3d(\ud835\udc5b\ud835\udc3b\ud835\udc3f+1) \ud835\udc3f }) \u2218 \ud835\udc21(\u22c5; {\ud835\udc30\ud835\udc3d\ud835\udc5b\ud835\udc3b\ud835\udc3f \ud835\udc3f , \ud835\udc4f\ud835\udc3d\ud835\udc5b\ud835\udc3b\ud835\udc3f \ud835\udc3f }) \u2218 \u22ef \u2218 \ud835\udc21(\ud835\udc31; {\ud835\udc30\ud835\udc3d1 \ud835\udc3f , \ud835\udc4f\ud835\udc3d1 \ud835\udc3f }) (44)\nwith\n\ud835\udc21(\ud835\udecf; {\ud835\udc30\ud835\udc3d\ud835\udc59 \ud835\udc3f , \ud835\udc4f\ud835\udc3d\ud835\udc59 \ud835\udc3f }) = \ud835\udcb6 (\ud835\udc1f(\ud835\udecf; {\ud835\udc30\ud835\udc3d\ud835\udc59 \ud835\udc3f , \ud835\udc4f\ud835\udc3d\ud835\udc59 \ud835\udc3f })), (45)\n\ud835\udc1f(\ud835\udecf; {\ud835\udc30\ud835\udc3d\ud835\udc59 \ud835\udc3f , \ud835\udc4f\ud835\udc3d\ud835\udc59 \ud835\udc3f }) = \ud835\udc30\ud835\udc3d\ud835\udc59 \ud835\udc3f \ud835\udecf + \ud835\udc4f\ud835\udc3d\ud835\udc59 \ud835\udc3f . (46)\nIn (44), \ud835\udc30\ud835\udc3d\ud835\udc59 \ud835\udc3f and \ud835\udc4f\ud835\udc3d\ud835\udc59 \ud835\udc3f denote weight and bias of layer \ud835\udc59, respectively, and the location-control parameter set \ud835\udc16\ud835\udc3d \ud835\udc3f in Figure 4 is defined as \ud835\udc16\ud835\udc3d \ud835\udc3f = {\ud835\udc30\ud835\udc3d\ud835\udc59 \ud835\udc3f , \ud835\udc4f\ud835\udc3d\ud835\udc59\n\ud835\udc3f } \ud835\udc59=1\n\ud835\udc5b\ud835\udc3b\ud835\udc3f+1 . In (45), \ud835\udcb6(\u22c5) denote an\nactivation function. In this work, the hyperbolic tangent activation function is used."
        },
        {
            "heading": "3.1.4. NN kernel function",
            "text": "As shown in Figure 4, the NN kernel functions ?\u0302?\ud835\udc3d\ud835\udc3e(\ud835\udc31) in Layer NNK is the outcome of the normalization of unnormalized NN kernel functions \ud835\udf19\ud835\udc3d\ud835\udc3e(\ud835\udc31). The normalization is defined as\n?\u0302?\ud835\udc3d\ud835\udc3e(\ud835\udc31) = \ud835\udf19\ud835\udc3d\ud835\udc3e(\ud835\udc31)\n\u2211 \u2211 \ud835\udf19\ud835\udc3c\ud835\udc3f(\ud835\udc31) \ud835\udc5b\ud835\udc41\ud835\udc3e \ud835\udc3f=1 \ud835\udc5b\ud835\udc35 \ud835\udc3c=1\n, (47)\nand the NN kernel function \ud835\udf19\ud835\udc3d\ud835\udc3e(\ud835\udc31) is defined as\n\ud835\udf19\ud835\udc3d\ud835\udc3e(\ud835\udc31) = \u220f \u220f ?\u0305?\ud835\udc56(\ud835\udc66\ud835\udc3d\ud835\udefc; {?\u0305?\ud835\udefc\ud835\udc56 \ud835\udc3d\ud835\udc3e, \ud835\udc50\ud835\udefc\ud835\udc56 \ud835\udc3d\ud835\udc3e, \ud835\udefd\ud835\udefc\ud835\udc56 \ud835\udc3d\ud835\udc3e})\n2\n\ud835\udc56=1\n\ud835\udc51\n\ud835\udefc=1\n, (48)\nwhere ?\u0305?\ud835\udc56 and {?\u0305?\ud835\udefc\ud835\udc56 \ud835\udc3d\ud835\udc3e, \ud835\udc50\ud835\udefc\ud835\udc56 \ud835\udc3d\ud835\udc3e, \ud835\udefd\ud835\udefc\ud835\udc56 \ud835\udc3d\ud835\udc3e} denote a regularized step function and shape-control parameters, respectively. The shape-control weight set \ud835\udc16\ud835\udc3d \ud835\udc46 in Figure 4 is defined as \ud835\udc16\ud835\udc3d \ud835\udc46 =\n{{{?\u0305?\ud835\udefc\ud835\udc56 \ud835\udc3d\ud835\udc3e, \ud835\udc50\ud835\udefc\ud835\udc56 \ud835\udc3d\ud835\udc3e, \ud835\udefd\ud835\udefc\ud835\udc56 \ud835\udc3d\ud835\udc3e}\n\ud835\udefc=1\n\ud835\udc51 }\n\ud835\udc56=1\n2\n} \ud835\udc3e=1\n\ud835\udc5b\ud835\udc41\ud835\udc3e\n. In this work, the regularized step function is constructed based\non the parametric softplus activation function \ud835\udc46 defined as follows:\n?\u0305?\ud835\udc56(\ud835\udc66; {?\u0305?\ud835\udc56, \ud835\udc50\ud835\udc56, \ud835\udefd\ud835\udc56}) = \ud835\udc46 (\ud835\udc67\ud835\udc56(\ud835\udc66) + 1\n2 ; \ud835\udefd\ud835\udc56) \u2212 \ud835\udc46 (\ud835\udc67\ud835\udc56(\ud835\udc66) \u2212\n1 2 ; \ud835\udefd\ud835\udc56), (49)\n\ud835\udc67\ud835\udc56(\ud835\udc66) = (\u22121) \ud835\udc56(\ud835\udc66 \u2212 ?\u0305?)/\ud835\udc50 , \ud835\udc56 = 1, 2, (50)\n\ud835\udc46(\ud835\udc67; \ud835\udefd) = 1\n\ud835\udefd log(1 + \ud835\udc52\ud835\udefd\ud835\udc67). (51)\nIn (49)-(51), \ud835\udefd\ud835\udc56 controls the sharpness in the transition of derivative as shown in Figure 5 (a-b), and \ud835\udc50\ud835\udc56 controls the sharpness of the solution transition as shown in Figure 5 (c). In addition, ?\u0305?\ud835\udc56 influences the support of ?\u0305?\ud835\udc56. Note that ?\u0305?\ud835\udc56 is the output of Layer RSF in Figure 4, and (1/\ud835\udc50\ud835\udc56) and (\u2212?\u0305?\ud835\udc56/\ud835\udc50\ud835\udc56) are respectively the weight and the bias of Layer RSF. Figure 6 shows a schematic illustration of a two-dimensional NN kernel which possesses a sharp transition in direction \ud835\udc66. Interested readers refer to [33] for more details on the NN kernel functions."
        },
        {
            "heading": "3.2. Convergence Properties",
            "text": "An error bound of the proposed NN-RK approximation is estimated. Let \u03a9\u0302 be the transition zone near the localization domain. Then, we have\n\u2016\ud835\udc62\u210e \u2212 \ud835\udc62\u20160,\u03a9 \u2264 \u2016\ud835\udc62 \u210e \u2212 \ud835\udc62\u20160,\u03a9\\\u03a9\u0302 + \u2016\ud835\udc62 \u210e \u2212 \ud835\udc62\u20160,\u03a9\u0302. (52)\nAs shown in Figure 7, we consider an arbitrary \ud835\udc62 with a sharp transition occurring in \u03a9\u0302 = [\u2212\u2113/2, +\u2113/2] and its approximation \ud835\udc62\u210e with a transition occurring in \u03a9\u03022. For both \ud835\udc62 and \ud835\udc62 \u210e, it is assumed that there are weak discontinuities on the boundaries of the transition zones. For brevity, let us introduce the following function \ud835\udc64:\n\ud835\udc64(\ud835\udf12; \ud835\udf09) \u2261 \u27e6\ud835\udf09\u27e7\n\u2113 \ud835\udf12 + \u27ea\ud835\udf09\u27eb, (53)\nwhere \u27e6\ud835\udf09\u27e7 \u2261 \ud835\udf09+ \u2212 \ud835\udf09\u2212 and \u27ea\ud835\udf09\u27eb \u2261 (\ud835\udf09+ + \ud835\udf09\u2212)/2 are a difference operator and an average operator, respectively, with \ud835\udf09+ \u2261 \ud835\udf09(\ud835\udc65 = +\u2113/2) and \ud835\udf09\u2212 \u2261 \ud835\udf09(\ud835\udc65 = \u2212\u2113/2). Using (53), the true solution \ud835\udc62 in the transition domain \u03a9\u0302 can be written in a parametric coordinate \ud835\udc66\ud835\udc62as\n\ud835\udc62(\ud835\udc65) = \ud835\udc64(\ud835\udc66\ud835\udc62(\ud835\udc65); \ud835\udc62\u0393), (54)\nwhere \ud835\udc62\u0393 is the value of \ud835\udc62 on the boundary of \u03a9\u0302, and, from (53) and (54), \ud835\udc66\ud835\udc62(\ud835\udc65) is obtained as\n\ud835\udc66\ud835\udc62(\ud835\udc65) =\n\u2113\n\u27e6\ud835\udc62\u0393\u27e7 (\ud835\udc62(\ud835\udc65) \u2212 \u27ea\ud835\udc62\u0393\u27eb). (55)\nSimilarly, the approximated solution \ud835\udc62\u210e(\ud835\udc65) in the transition domain \u03a9\u0302 is written in an approximated parametric coordinate \ud835\udc4cas\n\ud835\udc62\u210e(\ud835\udc65) = \ud835\udc64 (\ud835\udc4c(\ud835\udc65); \ud835\udc62\u210e \u0393 ), (56)\nwith\n\ud835\udc4c(\ud835\udc65) = { \u2212\u2113/2, \ud835\udc65 \u2208 \u03a9\u03021 \ud835\udc66(\ud835\udc65), \ud835\udc65 \u2208 \u03a9\u03022 \u2113/2, \ud835\udc65 \u2208 \u03a9\u03023 , (57)\nwhere \ud835\udc66(\ud835\udc65) is the neural network-based parametrization defined in (44), and \ud835\udc62\u210e \u0393 is the value of \ud835\udc62\u210e on the boundary of \u03a9\u0302. In (57), the subdomains are defined as \u03a9\u03021 = {\ud835\udc65 | \ud835\udc66(\u2212\u2113/2) \u2264 \ud835\udc66(\ud835\udc65) \u2264 \u2212\u2113/2} , \u03a9\u03022 = {\ud835\udc65 | \u2212 \u2113/2 < \ud835\udc66(\ud835\udc65) \u2264 \u2113/2} , and \u03a9\u03023 = {\ud835\udc65 | \u2113/2 < \ud835\udc66(\ud835\udc65) \u2264 \ud835\udc66(\u2113/2)} . Note that, with \ud835\udefd \u2192 \u221e, the NN kernel function defined in (48)-(50) introduces weak discontinuities on \ud835\udc66(\ud835\udc65) = \u00b1\u2113/2.\nWith (54) and (56), the last term in (52) becomes\n\u2016\ud835\udc62\u210e \u2212 \ud835\udc62\u20160,\u03a9\u0302 = \u2016\ud835\udc64 (\ud835\udc4c(\ud835\udc65); \ud835\udc62 \u210e\u0393) \u2212 \ud835\udc64(\ud835\udc66\ud835\udc62(\ud835\udc65); \ud835\udc62\u0393)\u2016\n0,\u03a9\u0302\n= \u2016\ud835\udc64 (\ud835\udc4c(\ud835\udc65); \ud835\udc62\u210e \u0393\n) \u2212 \ud835\udc64(\ud835\udc4c(\ud835\udc65); \ud835\udc62\u0393) + \ud835\udc64(\ud835\udc4c(\ud835\udc65); \ud835\udc62\u0393) \u2212 \ud835\udc64(\ud835\udc66\ud835\udc62(\ud835\udc65); \ud835\udc62\u0393)\u2016 0,\u03a9\u0302\n= \u2016\ud835\udc64 (\ud835\udc4c(\ud835\udc65); \ud835\udc62\u210e \u0393\n\u2212 \ud835\udc62\u0393) + \ud835\udc64(\ud835\udc4c(\ud835\udc65); \ud835\udc62\u0393) \u2212 \ud835\udc64(\ud835\udc66\ud835\udc62(\ud835\udc65); \ud835\udc62\u0393)\u2016 0,\u03a9\u0302\n\u2264 \u2016\ud835\udc64 (\ud835\udc4c(\ud835\udc65); \ud835\udc62\u210e \u0393\n\u2212 \ud835\udc62\u0393)\u2016 0,\u03a9\u0302 + \u2016\ud835\udc64(\ud835\udc4c(\ud835\udc65); \ud835\udc62\u0393) \u2212 \ud835\udc64(\ud835\udc66\ud835\udc62(\ud835\udc65); \ud835\udc62\u0393)\u2016 0,\u03a9\u0302 .\n(58)\nThe first term on the right-hand side of (58) is bounded as follows:\n\u2016\ud835\udc64 (\ud835\udc4c(\ud835\udc65); \ud835\udc62\u210e \u0393\n\u2212 \ud835\udc62\u0393)\u2016 0,\u03a9\u0302\n= \u2016(\u27e6\ud835\udc62\u210e \u0393 \u2212 \ud835\udc62\u0393\u27e7 /\u2113) \ud835\udc4c(\ud835\udc65) + \u27ea\ud835\udc62\u210e \u0393\n\u2212 \ud835\udc62\u0393\u27eb\u2016 0,\u03a9\u0302\n\u2264 \u2016|\ud835\udc62\u210e \u0393\u2212 \u2212 \ud835\udc62\u0393\u2212| + |\ud835\udc62\u210e \u0393+\n\u2212 \ud835\udc62\u0393+| \u2016 0,\u03a9\u0302\n\u2264 \u2016\ud835\udc62\u210e \u0393\u2212\n\u2212 \ud835\udc62\u0393\u2212\u2016 0,\u03a9\u0302\n+ \u2016\ud835\udc62\u210e \u0393+\n\u2212 \ud835\udc62\u0393+\u2016 0,\u03a9\u0302\n= \u21131/2 (|\ud835\udc62\u210e \u0393\u2212 \u2212 \ud835\udc62\u0393\u2212| + |\ud835\udc62\u210e \u0393+ \u2212 \ud835\udc62\u0393+|)\n(59)\nThe second term on the right-hand side of (58) is bounded as follows:\n\u2016\ud835\udc64(\ud835\udc4c(\ud835\udc65); \ud835\udc62\u0393) \u2212 \ud835\udc64(\ud835\udc66(\ud835\udc65); \ud835\udc62\u0393)\u2016 0,\u03a9\u0302\n= \u2016 \u27e6\ud835\udc62\u0393\u27e7\n\u2113 (\ud835\udc4c(\ud835\udc65) \u2212 \ud835\udc66\ud835\udc62(\ud835\udc65))\u2016\n0,\u03a9\u0302\n= |\u27e6\ud835\udc62\u0393\u27e7|\n\u2113 \u2016\ud835\udc4c(\ud835\udc65) \u2212 \ud835\udc66\ud835\udc62(\ud835\udc65)\u20160,\u03a9\u0302\n\u2264 |\u27e6\ud835\udc62\u0393\u27e7|\n\u2113 \u2016\ud835\udc66(\ud835\udc65) \u2212 \ud835\udc66\ud835\udc62(\ud835\udc65)\u20160,\u03a9\u0302.\n(60)\nTherefore, for \u03a9\u0302, the following error bound is obtained.\n\u2016\ud835\udc62\u210e \u2212 \ud835\udc62\u20160,\u03a9\u0302 \u2264 \u2113 1/2 (|\ud835\udc62\u210e\n\u0393\u2212 \u2212 \ud835\udc62\u0393\u2212| + |\ud835\udc62\u210e \u0393+ \u2212 \ud835\udc62\u0393+|) +\n|\u27e6\ud835\udc62\u0393\u27e7|\n\u2113 \u2016\ud835\udc66(\ud835\udc65) \u2212 \ud835\udc66\ud835\udc62(\ud835\udc65)\u20160,\u03a9\u0302. (61)\nFor multi-dimensions, we have\n\u2016\ud835\udc62\u210e \u2212 \ud835\udc62\u20160,\u03a9\u0302 \u2264 \u2113 1/2\u2016\ud835\udc62\u210e \u2212 \ud835\udc62\u20160,\u0393\u0302 +\n|\u27e6\ud835\udc62\u0393\u27e7|\n\u2113 \u2016\ud835\udc66(\ud835\udc65) \u2212 \ud835\udc66\ud835\udc62(\ud835\udc65)\u20160,\u03a9\u0302, (62)\nwhere \u0393\u0302 \u2261 \ud835\udf15\u03a9\u0302\\\ud835\udf15\u03a9 denotes the interface of weak discontinuity. Using the Sobolev trace inequality and (28), the first term on the right-hand side of (62) is bounded as follows: with a\ngeneric constant ?\u0302? and ?\u0302\u0302?,\n\u2016\ud835\udc62\u210e \u2212 \ud835\udc62\u20160,\u0393\u0302 \u2264 \u2016\ud835\udc62 \u210e \u2212 \ud835\udc62\u20160,\u2202(\u03a9\\\u03a9\u0302) \u2264 ?\u0302?\u2016\ud835\udc62 \u210e \u2212 \ud835\udc62\u2016\n0,\u03a9\\\u03a9\u0302 1/2 \u2016\ud835\udc62\u210e \u2212 \ud835\udc62\u2016 1,\u03a9\\\u03a9\u0302 1/2\n\u2264 ?\u0302\u0302?\ud835\udc58\ud835\udc4e?\u0302?|\ud835\udc62|\ud835\udc5d+1,\u03a9\\\u03a9\u0302, (63)\nwhere \ud835\udefe = max(\ud835\udc5d + 0.5, ?\u0303?) where ?\u0303? and \ud835\udc5d denotes the regularity of \ud835\udc62 in \u03a9\\\u03a9\u0302 and the order of basis of the background RK discretization, respectively. With (28), (62), and (63), the global error (52) has the following error bound:\n\u2016\ud835\udc62\u210e \u2212 \ud835\udc62\u20160,\u03a9 \u2264 (\ud835\udc36\ud835\udc4e \ud835\udefe + ?\u0302\u0302?\ud835\udc4e?\u0302?) \ud835\udc58|\ud835\udc62|\ud835\udc5d+1,\u03a9\\\u03a9\u0302 +\n|\u27e6\ud835\udc62\u0393\u27e7|\n\u2113 \u2016\ud835\udc66(\ud835\udc65) \u2212 \ud835\udc66\ud835\udc62(\ud835\udc65)\u20160,\u03a9\u0302, (64)\nwhere \ud835\udefe = max(\ud835\udc5d + 1, ?\u0303?) . For smooth \ud835\udc62 in \u03a9\\\u03a9\u0302 , \ud835\udefe = \ud835\udefe \u2212 0.5 holds, which means that \ud835\udefe\ndominates the first term on the right-hand side of (64), leading to\n\u2016\ud835\udc62\u210e \u2212 \ud835\udc62\u20160,\u03a9 \u2264 (\ud835\udc36 + ?\u0302\u0302?) \ud835\udc4e ?\u0302?\ud835\udc58|\ud835\udc62|\ud835\udc5d+1,\u03a9\\\u03a9\u0302 +\n|\u27e6\ud835\udc62\u0393\u27e7|\n\u2113 \u2016\ud835\udc66(\ud835\udc65) \u2212 \ud835\udc66\ud835\udc62(\ud835\udc65)\u20160,\u03a9\u0302, (65)\nIn the last term of (65), \u2016\ud835\udc66(\ud835\udc65) \u2212 \ud835\udc66\ud835\udc62(\ud835\udc65)\u20160,\u03a9\u0302 denotes the parametrization error. (65) implies that, when the parametrization error is relatively large, the solution convergence will be governed by the convergence of the parametrization. Conversely, for \u2016\ud835\udc66(\ud835\udc65) \u2212 \ud835\udc66\ud835\udc62(\ud835\udc65)\u20160,\u0393 \u2192 0 , the convergence will be governed by the background RK discretization with a rate of \ud835\udefe, e.g., 1.5 when a linear RK basis is used. The error bound of \u2016\ud835\udc66(\ud835\udc65) \u2212 \ud835\udc66\ud835\udc62(\ud835\udc65)\u20160,\u0393 follows the universal approximation theorem [1,37] when a neural network is used for parametrization. For example, for a neural network with a single hidden layer, the error bound is estimated as follows [1]: with a generic constant \ud835\udc36\ud835\udc66 < \u221e,\n\u2016\ud835\udc66(\ud835\udc65) \u2212 \ud835\udc66\ud835\udc62(\ud835\udc65)\u20160,\u0393 \u2264 \ud835\udc36\ud835\udc66\ud835\udc5b\ud835\udc41\ud835\udc45 \u22121/2 , (66)\nwhich leads to the following error estimation of NN-RK approximation\n\u2016\ud835\udc62\u210e \u2212 \ud835\udc62\u20160,\u03a9 \u2264 (\ud835\udc36 + ?\u0302\u0302?) \ud835\udc4e ?\u0302?\ud835\udc58|\ud835\udc62|\ud835\udc5d+1,\u03a9\\\u03a9\u0302 + \ud835\udc36\ud835\udc66\n|\u27e6\ud835\udc62\u0393\u27e7|\n\u2113 \ud835\udc5b\ud835\udc41\ud835\udc45\n\u22121/2 . (67)"
        },
        {
            "heading": "3.3. Regularization",
            "text": "To avoid the potential loss of ellipticity of the problem and the resulting discretization sensitivity in the numerical solution of the local problem defined in Section 2, a regularization treatment is needed. A straightforward remedy is to impose a proper constraint such that the physical bandwidth of the damage does not become narrower than a certain limit. To analyze a localization width possessed by the NN-RK approximation, we start with a Taylor expansion of the parametric coordinate as follows:\n\ud835\udc66(\ud835\udc31) \u2248 ?\u0305? + (\ud835\udc31 \u2212 ?\u0305?) \u22c5 \ud835\udec1\ud835\udc31\ud835\udc66(?\u0305?), (68)\nwhere ?\u0305? = \ud835\udc66(?\u0305?), and ?\u0305? is defined in Section 3.1.4, for which the superscripts and subscripts are omitted for brevity. With (68), \ud835\udc67 defined in (50) is written as\n\ud835\udc67(\ud835\udc66(\ud835\udc31); {\ud835\udc66\u0305, \ud835\udc50}) = (\ud835\udc66(\ud835\udc31) \u2212 ?\u0305?)\n\ud835\udc50 \u2248\n(\ud835\udc31 \u2212 ?\u0305?) \u22c5 \ud835\udec1\ud835\udc31\ud835\udc66(?\u0305?)\n\ud835\udc50 \u2261\n\ud835\udf09\u0305(\ud835\udc31; ?\u0305?)\n\ud835\udc50 , (69)\nwith \ud835\udf09\u0305(\ud835\udc31; ?\u0305?) \u2261 (\ud835\udc31 \u2212 ?\u0305?) \u22c5 \ud835\udec1\ud835\udc31\ud835\udc66(?\u0305?). When \u2016 \ud835\udec1\ud835\udc31\ud835\udc66(?\u0305?)\u2016 = 1, \ud835\udf09\u0305(\ud835\udc31; ?\u0305?) in (69) is a projection of the physical coordinate onto the direction normal to the localization. Therefore, by satisfying conditions\n\u2016 \ud835\udec1\ud835\udc31\ud835\udc66(?\u0305?)\u2016 \u2264 1,\n\ud835\udc50 \u2265 \u2113, (70)\nthe transition width of ?\u0305? in (49) has a lower bound of \u2113, and thus the localization width in the NN-RK approximation has the same lower bound. In this work, a constraint \u2016\ud835\udec1\ud835\udc31\ud835\udc66\u2016 \u2264 1 is imposed in the loss function (1), and the lower bound of the sharpness control parameter \ud835\udc50 in (50) is set to an NN length scale parameter \u2113. The modified loss function with regularization reads:\nmin \ud835\udc2e\n\u03a0\u0305(\ud835\udc2e, \ud835\udc32) = \u03a0(\ud835\udc2e) + \u03a0Reg(\ud835\udc32),\n\u03a0Reg(\ud835\udc32) = \ud835\udf05\ud835\udf07\n2 \u2211 \u222b \u2329\u2016\ud835\udec1\ud835\udc31\ud835\udc66\ud835\udc3d\ud835\udefc(\ud835\udc31)\u2016 \u2212 1\u27e9+\n2 \ud835\udc51\u03a9\n\u03a9\ud835\udefc,\ud835\udc3d\n, (71)\nwhere \u03a0 is the potential function defined in (1), and \ud835\udf05 is the normalized penalty parameter. In this work, \ud835\udf05 = 104 is used. Note that this approach is different from the ?\u0302? -regularization introduced by Baek et al. (2022) [33] in which the parametric coordinates are directly scaled by ?\u0302? as follows:\n\ud835\udc67 = (\ud835\udc66 \u2212 ?\u0305?)?\u0302?\n\ud835\udc50 , where ?\u0302? \u2261 1/ max(\u2016\ud835\udec1\ud835\udc31\ud835\udc66\u2016, 1). (72)\nAn advantage of the regularization designed in this work over the ?\u0302?-regularization is that the necessity to compute the second order gradient of \ud835\udc66 for the evaluation of the strain energy in the loss function is avoided."
        },
        {
            "heading": "4. Numerical implementation",
            "text": "The minimization problem is rewritten as follows:\nmin \ud835\udc1d,\ud835\udc16\n[\u03a0(\ud835\udc2e\u210e(\ud835\udc1d, \ud835\udc16)) + \u03a0Reg(\ud835\udc32(\ud835\udc31; \ud835\udc16\ud835\udc3f))], (73)\nwhere \ud835\udc2e\u210e(\ud835\udc1d, \ud835\udc16) = \ud835\udc2e\ud835\udc45\ud835\udc3e(\ud835\udc1d) + \ud835\udc2e\ud835\udc41\ud835\udc41(\ud835\udc16) is the NN-RK approximation with the RK coefficient set, \ud835\udc1d, and the neural network weight set, \ud835\udc16 = {\ud835\udc16\ud835\udc3f , \ud835\udc16\ud835\udc46, \ud835\udc16\ud835\udc36} with \ud835\udc16\ud835\udc3f = {\ud835\udc16\ud835\udc3d \ud835\udc3f}\n\ud835\udc3d=1\n\ud835\udc5b\ud835\udc35 , \ud835\udc16\ud835\udc46 =\n{\ud835\udc16\ud835\udc3d \ud835\udc46}\n\ud835\udc3d=1\n\ud835\udc5b\ud835\udc35 , and \ud835\udc16\ud835\udc36 = {\ud835\udc16\ud835\udc3d\n\ud835\udc36} \ud835\udc3d=1\n\ud835\udc5b\ud835\udc35 . In (73), \ud835\udf13 and \ud835\udc39 denote the energy density and the external\nwork defined in (1), respectively.\nFigure 8 shows the flowchart of the solution procedure. In the flowchart \ud835\udc5b and \ud835\udc5b\ud835\udc40\ud835\udc4e\ud835\udc65 denotes the loading step and the maximum loading step, respectively. At loading step \ud835\udc5b + 1, the solution procedure mainly consists of two parts: RK precomputation stage and NN-RKPM optimization stage.\nA. RK precomputation stage\nTo obtain the initial guesses \ud835\udc1d(\u0305\ud835\udc5b+1) and ?\u0305?\ud835\udc36 (\ud835\udc5b+1) to be used in the NN-RKPM optimization stage, the minimization problem (73) is first solved only for \ud835\udc1d(\ud835\udc5b+1) and \ud835\udc16\ud835\udc36 (\ud835\udc5b+1) :\n?\u0305?(\ud835\udc5b+1), \ud835\udc16\ud835\udc36 (\ud835\udc5b+1)\n= argmin \ud835\udc1d,\ud835\udc16\ud835\udc36\n[\u03a0 (\ud835\udc2e\u210e (\ud835\udc1d, {\ud835\udc16\ud835\udc3f (\ud835\udc5b) , \ud835\udc16\ud835\udc46 (\ud835\udc5b) , \ud835\udc16\ud835\udc36})) + \u03a0Reg (\ud835\udc32 (\ud835\udc31; \ud835\udc16\ud835\udc3f (\ud835\udc5b) ))]\nsubjected to \ud835\udc2e(\ud835\udc31) = \ud835\udc20(\ud835\udc5b+1) on \ud835\udf15\u03a9\ud835\udc54 .\n(74)\nIn this stage, the weight sets {\ud835\udc16\ud835\udc3f , \ud835\udc16\ud835\udc46} and the damage \ud835\udf02 from the previous loading step are used. Also, the damage is not updated. This is equivalent to the standard Galerkin-based RKPM problem and can be solved by a standard matrix solver.\nB. NN-RKPM optimization stage\nIn the second stage, the minimization problem (73) is solved for the entire unknown parameters \ud835\udc1d and \ud835\udc16.\n\ud835\udc1d(\u0305\ud835\udc5b+1), \ud835\udc16(\ud835\udc5b+1) = argmin \ud835\udc1d,\ud835\udc16 [\u03a0(\ud835\udc2e\u210e(\ud835\udc1d, \ud835\udc16)) + \u03a0Reg(\ud835\udc32(\ud835\udc31; \ud835\udc16\ud835\udc3f))]\nsubjected to \ud835\udc2e(\ud835\udc31) = \ud835\udc20(\ud835\udc5b+1) on \ud835\udf15\u03a9\ud835\udc54.\n(75)\nIn this stage, the damage is updated as well. The minimization problem can be solved iteratively by a suitable optimizer. In this work, Adam [42], a first-order optimizer with adaptive learning rate, is used for the first several epochs. Then, the optimizer is switched to limited-memory Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm (L-BFGS) [43], a second-order optimizer, for the remaining optimization.\nFor domain integration involved in (73), SCNI introduced in Section 2.2.2 is used with refined smoothing cells near localization. As discussed in Section 2.2.2, the advantage of using SCNI for the proposed method is twofold: 1) it eliminates the requirement of computing the computationally expensive direct derivative of \ud835\udc2e\ud835\udc41\ud835\udc41 with the automatic differentiation to evaluate strain and stress, and 2) it suppresses stress oscillations.\nFor computationally efficient implementation of the strain smoothing operation in SCNI, precomputed sparse smoothing matrices \ud835\udc0f\ud835\udefc with \ud835\udefc = 1 \u22ef \ud835\udc51 can be considered to perform the following global smoothing:\n\ud835\udc14\ud835\udefc \u2207\u0303 = \ud835\udc0f\ud835\udefc\ud835\udc14 \ud835\udc60\ud835\udc62\ud835\udc5f\ud835\udc53 , (76)\nby which the strain smoothing in all the smoothing cells as discussed in section 2 are conducted\nsimultaneously. In (76), \ud835\udc14\ud835\udefc \u2207\u0303 = [\ud835\udc62,\ud835\udefc \u210e\u0303 (\ud835\udc311), \u22ef , \ud835\udc62,\ud835\udefc \u210e\u0303 (\ud835\udc31\ud835\udc3f), \u22ef , \ud835\udc62,\ud835\udefc \u210e\u0303 (\ud835\udc31\ud835\udc41\ud835\udc3c\ud835\udc36)] \ud835\udc47 is a column vector containing the smoothed gradients of \ud835\udc62\u210e with respect to \ud835\udc65\ud835\udefc for all the smoothing cells in the\ndomain, i.e., \ud835\udc3f = 1 \u22ef \ud835\udc41\ud835\udc3c\ud835\udc36 . \ud835\udc14 \ud835\udc60\ud835\udc62\ud835\udc5f\ud835\udc53 = [\ud835\udc62\u210e(\ud835\udc311 \ud835\udc60\ud835\udc62\ud835\udc5f\ud835\udc53 ), \u22ef , \ud835\udc62\u210e(\ud835\udc31\ud835\udc52 \ud835\udc60\ud835\udc62\ud835\udc5f\ud835\udc53 ), \u22ef , \ud835\udc62\u210e (\ud835\udc31\ud835\udc41\ud835\udc60\ud835\udc52\ud835\udc54 \ud835\udc60\ud835\udc62\ud835\udc5f\ud835\udc53 )]\n\ud835\udc47\nis a column\nvector containing \ud835\udc62\u210e evaluated at a smoothing cell surface evaluation point \ud835\udc31\ud835\udc52 \ud835\udc60\ud835\udc62\ud835\udc5f\ud835\udc53\nfor \ud835\udc52 = 1 \u22ef \ud835\udc41\ud835\udc60\ud835\udc62\ud835\udc5f\ud835\udc53, where \ud835\udc41\ud835\udc60\ud835\udc62\ud835\udc5f\ud835\udc53 denotes the total number of smoothing cell surface evaluation points in the domain. The (\ud835\udc3f, \ud835\udc52) component of the smoothing operator \ud835\udc0f\ud835\udefc is\n\ud835\udc43\ud835\udefc\ud835\udc3f\ud835\udc52 = {\n1\n\ud835\udc49\ud835\udc3f \ud835\udc34\ud835\udc52\ud835\udc5b\ud835\udefc\n\ud835\udc3e, if \u0393\ud835\udc52 \u2282 \u03a9\ud835\udc3f\n0, otherwise\n, (77)\nwhere \u0393\ud835\udc52 , \u03a9\ud835\udc3f , \ud835\udc5b\ud835\udefc \ud835\udc3e , and \ud835\udc34\ud835\udc52 denote \ud835\udc52-th smoothing cell surface segment, \ud835\udc3f-th smoothing cell domain, \ud835\udefc-th component of the surface normal, and the area of \ud835\udc52-th smoothing cell surface segment, respectively. The same procedure can be used to compute \u2207\u0303\ud835\udc66\ud835\udc56 for Eq. (71)."
        },
        {
            "heading": "5. Numerical Examples",
            "text": "Several numerical examples are presented to demonstrate the proposed method\u2019s accuracy, regularization ability, and capability to capture complicated localization patterns. Unless otherwise specified, for the RK approximation, the linear basis with cubic B-spline kernel function of normalized support size 2.0 is used, and, for the NN approximation, a single 4-kernel NN block is used along with a densely connected neural network with the hyperbolic tangent activation function for the parametrization sub-block. For the domain integration, SCNI is used with refined smoothing cells in the zone along the expected damage path."
        },
        {
            "heading": "5.1. Elasticity with pre-existing damaged zone",
            "text": "Consider a domain [\u2212\ud835\udc3f/2, \ud835\udc3f/2] \u00d7 [\u2212\ud835\udc3b/2, \ud835\udc3b/2] with a degraded zone with width \ud835\udc64 . We consider two different cases of pre-existing damaged zone geometry, as show in Figure 9(a) and (b). For both cases, \ud835\udc3f = 2 mm and \ud835\udc3b = 0.5 mm are used. For Case I, the degraded zone is vertically aligned at the center of the domain. For Case II, the anti-symmetric degraded zone is centered at the origin with \ud835\udc31\ud835\udc501 = (\u22120.1, \u22120.5), \ud835\udc451 = 0.35, \ud835\udc31\ud835\udc502 = (\u22120.1, 0), and \ud835\udc452 = 0.1 in unit of mm. For both cases, Dirichlet boundary conditions are applied to the left and right surfaces with \ud835\udc54 = 1 \u00d7 10\u22122 mm, and zero traction boundary conditions are applied to the top and bottom surfaces. For Case I, \ud835\udc64 = \ud835\udc3b/100, \ud835\udc38 = 210 GPa, and \ud835\udf08 = 0 are used, and for Case II, \ud835\udc64 = \ud835\udc3b/1000 , \ud835\udc38 = 210 GPa, and \ud835\udf08 = 0.3 are used. The Young\u2019s modulus within the degraded zones is \ud835\udc58\ud835\udc38 with \ud835\udc58 = 10\u22122 for Case I and \ud835\udc58 = 10\u22123 for Case II.\nFor Case I, the exact solution is as follows:\n\ud835\udc621(\ud835\udc31) = { \ud835\udc4f(\ud835\udc651 + \ud835\udc3f) \u2212 \ud835\udc54, \ud835\udc651 \u2264 \u2212\ud835\udc64/2 (\ud835\udc4f/\ud835\udc58)\ud835\udc651, \u2212\ud835\udc64/2 < \ud835\udc651 \u2264 \ud835\udc64/2\n\ud835\udc4f(\ud835\udc651 \u2212 \ud835\udc3f) + \ud835\udc54, \ud835\udc651 > \ud835\udc64/2\n\ud835\udc622(\ud835\udc31) = 0\n(78)\nwhere \ud835\udc4f = 2\ud835\udc54/((1/\ud835\udc58 \u2212 1)\ud835\udc64 + 2\ud835\udc3f) . For the numerical solution, the domain is uniformly\ndiscretized by 21 \u00d7 6 RK nodes (see Figure 10 (a)), and a single 10-neuron hidden layer is used for the parametrization sub-block. Figure 11 shows the displacement predicted by the proposed method. The numerical solution captures the sharp transition in the horizontal displacement very well along with the zero vertical displacement due to zero Poisson\u2019s ratio. As shown in the figures in the 2nd row in Figure 11, the NN approximation appears near the localization capturing the sharp transition of \ud835\udc621, and the RK approximation captures the solution in the other area, with smooth transition between two approximations. Figure 12 shows the horizontal displacement and normal strain along \ud835\udc66 = 0 in which the numerical solution is shown to be highly accurate compared to the exact solution. The computed \ud835\udc3f2 norm and \ud835\udc3b 1 semi-norm of\nthe solution error are 2.921 \u00d7 10\u22124 and 2.437 \u00d7 10\u22126, respectively.\nFor Case II, the background RK discretizations employed in this section are plotted in Figure 10 (a-c), and a 1,070,298-node, body-fitted Q8-FEM solution with a minimum nodal spacing of \ud835\udc3b/2000 near the localization (see Figure 13 for discretization) is used as a reference solution. Figure 14 shows the numerical solution for Case II, using 41 \u00d7 11 uniformly distributed background RK nodes (Figure 10 (b)) and a single 40-neuron hidden layer. Although the background RK discretizations shown in Figure 8 are relatively coarse compared to the width of degraded zone, the displacements predicted by the proposed method match the reference solution very well. The convergence curve for varying background RK nodal spacing (\u210e) and the convergence curve for the varying number of neurons (\ud835\udc5b\ud835\udc41\ud835\udc45) are plotted in Figure 15 (a) and\n(b), respectively. For the convergence study shown in Figure 15 (a), a fixed value of \ud835\udc5b\ud835\udc41\ud835\udc45 = 160 is used, and for the study shown in Figure 15(b), a fixed value of \u210e = \ud835\udc3b/40 is used. Both results show convergence behaviors consistent with the error analysis result presented in Section 3.2."
        },
        {
            "heading": "5.2. Pre-notched specimen subjected to simple shear",
            "text": "A benchmark problem of pre-notched specimen under simple shear is considered. As shown in Figure 16, a specimen with domain \u03a9 = [\u2212\ud835\udc3f, \ud835\udc3f] \u00d7 [\u2212\ud835\udc3f, \ud835\udc3f] with a pre-existing crack of length \ud835\udc3f is subjected to Dirichlet boundary conditions on the top and bottom surfaces. Specimen dimension \ud835\udc3f = 0.5 mm is used in this problem. The horizontal boundary value \ud835\udc54 applied to the top surface is increased up to 15 \u00d7 10\u22123 mm with an increment of 1 \u00d7 10\u22124 mm. The material properties of \ud835\udc38 = 210 GPa, \ud835\udf08 = 0.3, \ud835\udca2\ud835\udc50 = 2.7 N/mm are used. As shown in Figure 17, three levels of RK discretizations are used to study the regularization capability of the proposed method. For verification, a reference solution based on the reproducing kernel strain regularization [44] method is employed using 160,801 uniformly distributed RK nodes with nodal spacing of \u210e = \ud835\udc3f/200.\nFigure 18 (a-c) shows the damage propagation predicted by the proposed method. The damage is initiated with an orientation of approximately 65\u00b0 and gradually changes the direction to the lower right corner during the propagation. The predicted damage paths plotted in Figure 18 (d) are not sensitive to the background RK discretization and agree very well with the reference solution. In addition, as shown in Figure 19, the load-displacement curves also demonstrate the good regularization capability of the proposed method and present reasonable agreement with the reference solution."
        },
        {
            "heading": "5.3. Quasi-static crack branching problem",
            "text": "In this section, the proposed method\u2019s ability to capture branching is demonstrated through a numerical example inspired by the problem proposed by Muixi et al [45,46]. Consider a square domain \u03a9 = [\u2212\ud835\udc3f, \ud835\udc3f] \u00d7 [\u2212\ud835\udc3f, \ud835\udc3f] with a pre-existing notch with a length of \ud835\udc3f, as shown in Figure 20. The specimen is subjected to vertical displacement boundary conditions \ud835\udc54(\ud835\udc65) = \ud835\udc54\ud835\udc37(1 \u2212 \ud835\udc65\n2)/8 on the top and bottom surfaces while the right surface is fixed in both directions. Herein, \ud835\udc3f = 1 mm is considered, and \ud835\udc54\ud835\udc37 is applied up to 0.08 mm with \u2206\ud835\udc54\ud835\udc37 = 4 \u00d7 10 \u22123 mm. The material properties \ud835\udc38 = 20 GPa, \ud835\udf08 = 0.3, and \ud835\udca2\ud835\udc50 = 8.9 \u00d7 10 \u22125 kN/mm are used.\nIn Figure 21, a progressive damage field is plotted in which the fracture initially propagates horizontally and branched near the fixed boundary as the accumulated strain energy associated with the vertical strain decreases due to the displacement constraint, which prevents further propagation of the fracture toward the fixed boundary. The branching is predicted to occur abruptly, then the propagation rate slows down. At the late stage of simulation, two branches switch the direction to the left. The overall trend of the damage propagation agrees with the reference PF-XFEM solution [46] superimposed in Figure 21 (d)."
        },
        {
            "heading": "5.4. Mixed-mode fracture of a doubly notched rock-like specimen subjected to uniaxial compression",
            "text": "A uniaxial compression of a rock-like specimen with double pre-existing cracks [47] is simulated. As shown in Figure 22, a rectangular specimen with \ud835\udc3b = 152.4 mm consists of two 1-mm thick pre-existing cracks with \ud835\udc3f = \ud835\udc50 = \ud835\udc64 = 12.7 mm and \ud835\udefc = 45\u00b0 . The Dirichlet boundary condition on the top surface is prescribed up to \ud835\udc54 = \u22120.65 mm with the increment \u2206\ud835\udc54 = \u22121 \u00d7 10\u22122 mm. Material parameters are Young\u2019s modulus of \ud835\udc38 = 5.96 GPa, Poisson\u2019s ratio of \ud835\udf08 = 0.24, the mode-I fracture energy of \ud835\udca2\ud835\udc3c = 5 N/m, and the mode-II fracture energy of \ud835\udca2\ud835\udc3c\ud835\udc3c = 20\ud835\udca2\ud835\udc3c . The domain is uniformly discretized by 16 \u00d7 31 RK particles. For NN approximation, the parametrization subblock consists of a neural network with two 40-neuron hidden layers along with the hyperbolic tangent activation function, which involves 1,842 unknown weights and biases. The NN length scale of 1 mm is employed. Figure 23 shows the predicted damage propagation in the rock specimen. At the initial stage, four wing cracks are initiated from the four corners of the pre-existing notches and propagates with curved paths. Then, secondary shear cracks start to develop approximately at \ud835\udc54 = \u22120.65 mms the experimental observation [47]."
        },
        {
            "heading": "6. Conclusion",
            "text": "An improved neural network-enhanced reproducing kernel particle method has been proposed for modeling brittle fracture. Derived through an NN-based correction of standard RK shape functions, the proposed method enriches a background reproducing kernel (RK) approximation with a coarse and uniform discretization by a neural network (NN) approximation equipped with a Partition of Unity property. The NN approximation is constructed by a deep neural network designed to capture localization, and the NN based enrichment functions are then patched together with RK approximation functions using RK as a Partition of Unity patching function. In the NN approximation, the deep neural network locates and inserts regularized discontinuities in the approximation function automatically, and the resulting NN enriched RK coefficient function provides varying magnitude of the discontinuities along the localization path.\nTo automatically capture the location, orientation, and solution transition across and along the localization, the optimum values of the control parameters contained in the deep neural network as well as the RK coefficients are obtained via minimization of the energy-based loss function. A regularization by introducing a constraint on the spatial gradient of the parametric coordinates to the loss function is employed to ensure a discretization-independent solution. Error analysis of the proposed NN-RK approximation is performed, and its verification with the numerical results show good agreement on the convergence rates. The numerical examples demonstrate the effectiveness of the proposed method in modeling damage evolution and branching with a fixed background discretization without conventional adaptive refinement."
        },
        {
            "heading": "Acknowledgments",
            "text": "The support from the National Science Foundation under award #1826221 to University of California, San Diego, is greatly acknowledged."
        }
    ],
    "title": "A Neural Network-Based Enrichment of Reproducing Kernel Approximation for Modeling Brittle Fracture",
    "year": 2023
}