{
    "abstractText": "The use of Deep Neural Network (DNN) models in risk-based decision-making has attracted extensive attention with broad applications in medical, finance, manufacturing, and quality control. To mitigate prediction-related risks in decision making, prediction confidence or uncertainty should be assessed alongside the overall performance of algorithms. Recent studies on Bayesian deep learning helps quantify prediction uncertainty arises from input noises and model parameters. However, the normality assumption of input noise in these models limits their applicability to problems involving categorical and discrete feature variables in tabular datasets. In this paper, we propose a mathematical framework to quantify prediction uncertainty for DNN models. The prediction uncertainty arises from errors in predictors that follow some known finite discrete distribution. We then conducted a case study using the framework to predict treatment outcome for tuberculosis patients during their course of treatment. The results demonstrate under a certain level of risk, we can identify risk-sensitive cases, which are prone to be misclassified due to error in predictors. Comparing to the Monte Carlo dropout method, our proposed framework is more aware of misclassification cases. Our proposed framework for uncertainty quantification in deep learning can support risk-based decision making in applications when discrete errors in predictors are present.",
    "authors": [
        {
            "affiliations": [],
            "name": "Maryam Kheirandish"
        },
        {
            "affiliations": [],
            "name": "Shengfan Zhang"
        },
        {
            "affiliations": [],
            "name": "Donald G. Catanzaro"
        },
        {
            "affiliations": [],
            "name": "Valeriu Crudu"
        }
    ],
    "id": "SP:20fd9c6d12331bab9b88b3871bba834597e2d655",
    "references": [
        {
            "authors": [
                "Abdar",
                "Moloud",
                "Farhad Pourpanah",
                "Sadiq Hussain",
                "Dana Rezazadegan",
                "Li Liu",
                "Mohammad Ghavamzadeh",
                "Paul Fieguth",
                "Xiaochun Cao",
                "Abbas Khosravi",
                "U Rajendra Acharya"
            ],
            "title": "A review of uncertainty quantification in deep learning: Techniques, applications and challenges.\" Information fusion 76:243-297",
            "year": 2021
        },
        {
            "authors": [
                "Abell\u00e1n",
                "Joaqu\u00edn",
                "George J Klir",
                "Seraf\u00edn Moral"
            ],
            "title": "Disaggregated total uncertainty measure for credal sets.",
            "venue": "International Journal of General Systems",
            "year": 2006
        },
        {
            "authors": [
                "Blundell",
                "Charles",
                "Julien Cornebise",
                "Koray Kavukcuoglu",
                "Daan Wierstra"
            ],
            "title": "Weight uncertainty in neural network.\" International conference on machine learning",
            "year": 2015
        },
        {
            "authors": [
                "Caulfield",
                "Adam J",
                "Nancy L Wengenack"
            ],
            "title": "Diagnosis of active tuberculosis disease: From microscopy to molecular techniques.\" Journal of Clinical Tuberculosis and Other Mycobacterial Diseases 4:33-43",
            "year": 2016
        },
        {
            "authors": [
                "Chang",
                "An-Hsing",
                "Li-Kai Yang",
                "Rua-Huan Tsaih",
                "Shih-Kuei Lin"
            ],
            "title": "Machine learning and artificial neural networks to construct P2P lending credit-scoring model: A case using Lending Club data.",
            "venue": "Quantitative Finance and Economics",
            "year": 2022
        },
        {
            "authors": [
                "Davis",
                "J Lucian",
                "Adithya Cattamanchi",
                "Luis E Cuevas",
                "Philip C Hopewell",
                "Karen R Steingart"
            ],
            "title": "Diagnostic accuracy of same-day microscopy versus standard microscopy for pulmonary tuberculosis: a systematic review and meta-analysis.\" The Lancet infectious diseases",
            "year": 2013
        },
        {
            "authors": [
                "De Angeli",
                "Kevin",
                "Shang Gao",
                "Andrew Blanchard",
                "Eric B Durbin",
                "Xiao-Cheng Wu",
                "Antoinette Stroup",
                "Jennifer Doherty",
                "Stephen M Schwartz",
                "Charles Wiggins",
                "Linda Coyle"
            ],
            "title": "2022. \"Using ensembles and distillation to optimize the deployment of deep learning models for the classification of electronic cancer pathology reports.\" JAMIA open 5 (3):ooac075",
            "year": 2022
        },
        {
            "authors": [
                "Denker",
                "John",
                "Yann LeCun"
            ],
            "title": "Transforming neural-net output levels to probability distributions.\" Advances in neural information processing systems",
            "year": 1990
        },
        {
            "authors": [
                "Depeweg",
                "Stefan",
                "Jose-Miguel Hernandez-Lobato",
                "Finale Doshi-Velez",
                "Steffen Udluft"
            ],
            "title": "Decomposition of uncertainty in Bayesian deep learning for efficient and risk-sensitive learning.",
            "venue": "International Conference on Machine Learning",
            "year": 2018
        },
        {
            "authors": [
                "Depeweg",
                "Stefan",
                "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato",
                "Finale Doshi-Velez",
                "Steffen Udluft"
            ],
            "title": "Learning and policy search in stochastic dynamical systems with bayesian neural networks.\" arXiv preprint arXiv:1605.07127",
            "year": 2016
        },
        {
            "authors": [
                "Der Kiureghian",
                "Armen",
                "Ove Ditlevsen"
            ],
            "title": "Aleatory or epistemic? Does it matter?\" Structural safety",
            "year": 2009
        },
        {
            "authors": [
                "Gal",
                "Yarin."
            ],
            "title": "Uncertainty in deep learning",
            "venue": "PhD thesis, University of Cambridge.",
            "year": 2016
        },
        {
            "authors": [
                "Gal",
                "Yarin",
                "Zoubin Ghahramani"
            ],
            "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning.\" international conference on machine learning",
            "year": 2016
        },
        {
            "authors": [
                "Graves",
                "Alex"
            ],
            "title": "Practical variational inference for neural networks.\" Advances in neural information processing systems",
            "year": 2011
        },
        {
            "authors": [
                "Heirung",
                "Tor Aksel N",
                "Joel A Paulson",
                "Shinje Lee",
                "Ali Mesbah"
            ],
            "title": "Model predictive control with active learning under model uncertainty: Why, when, and how.",
            "venue": "AIChE Journal",
            "year": 2018
        },
        {
            "authors": [
                "Hora",
                "Stephen C"
            ],
            "title": "Aleatory and epistemic uncertainty in probability elicitation",
            "year": 1996
        },
        {
            "authors": [
                "H\u00fcllermeier",
                "Eyke",
                "Willem Waegeman"
            ],
            "title": "Aleatoric and epistemic uncertainty",
            "year": 2021
        },
        {
            "authors": [
                "Lakshminarayanan",
                "Balaji",
                "Alexander Pritzel",
                "Charles Blundell"
            ],
            "title": "Simple and scalable",
            "year": 2017
        },
        {
            "authors": [
                "Louizos",
                "Christos",
                "Max Welling"
            ],
            "title": "Structured and efficient variational deep learning",
            "year": 2016
        },
        {
            "authors": [
                "J Technology. Martin",
                "C Elster"
            ],
            "title": "Aleatoric Uncertainty for Errors-in-Variables Models in Deep",
            "year": 2022
        },
        {
            "authors": [
                "Nguyen"
            ],
            "title": "Dropconnect is effective in modeling uncertainty of bayesian deep",
            "year": 2021
        },
        {
            "authors": [
                "Neal",
                "Radford"
            ],
            "title": "Bayesian learning via stochastic dynamics.\" Advances in neural",
            "year": 1992
        },
        {
            "authors": [
                "World Health Organization. Rossi",
                "Simone",
                "Pietro Michiardi",
                "Maurizio Filippone"
            ],
            "title": "Good initializations",
            "year": 2019
        },
        {
            "authors": [
                "Seghouane",
                "A-K",
                "Gilles Fleury"
            ],
            "title": "A cost function for learning feedforward neural",
            "year": 2001
        },
        {
            "authors": [
                "Seo",
                "Jigu",
                "Sungwook Park"
            ],
            "title": "signal processing and its applications (Cat",
            "year": 2023
        },
        {
            "authors": [
                "Theodoridis",
                "Sergios"
            ],
            "title": "Machine learning: a Bayesian and optimization perspective: Academic press",
            "year": 2015
        },
        {
            "authors": [
                "Van Gorp",
                "J\u00fcrgen",
                "Johan Schoukens",
                "Rik Pintelon"
            ],
            "title": "The errors-in-variables cost function for learning neural networks with noisy inputs.\" Intelligent Engineering through Artificial Neural Networks 8:141-146",
            "year": 1998
        },
        {
            "authors": [
                "Van Gorp",
                "J\u00fcrgen",
                "Johan Schoukens",
                "Rik Pintelon"
            ],
            "title": "Learning neural networks with noisy inputs using the errors-in-variables approach.",
            "venue": "IEEE Transactions on Neural Networks",
            "year": 2000
        },
        {
            "authors": [
                "Walker",
                "Warren E",
                "Poul Harremo\u00ebs",
                "Jan Rotmans",
                "Jeroen P Van Der Sluijs",
                "Marjolein BA Van Asselt",
                "Peter Janssen",
                "Martin P Krayer von Krauss"
            ],
            "title": "Defining uncertainty: a conceptual basis for uncertainty management in model-based decision support.\" Integrated assessment",
            "year": 2003
        },
        {
            "authors": [
                "Zhang",
                "Xu-Yao",
                "Guo-Sen Xie",
                "Xiuli Li",
                "Tao Mei",
                "Cheng-Lin Liu"
            ],
            "title": "A survey on learning to reject.",
            "venue": "Proceedings of the IEEE",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "The use of Deep Neural Network (DNN) models in risk-based decision-making has attracted extensive attention with broad applications in medical, finance, manufacturing, and quality control. To mitigate prediction-related risks in decision making, prediction confidence or uncertainty should be assessed alongside the overall performance of algorithms. Recent studies on Bayesian deep learning helps quantify prediction uncertainty arises from input noises and model parameters. However, the normality assumption of input noise in these models limits their applicability to problems involving categorical and discrete feature variables in tabular datasets. In this paper, we propose a mathematical framework to quantify prediction uncertainty for DNN models. The prediction uncertainty arises from errors in predictors that follow some known finite discrete distribution. We then conducted a case study using the framework to predict treatment outcome for tuberculosis patients during their course of treatment. The results demonstrate under a certain level of risk, we can identify risk-sensitive cases, which are prone to be misclassified due to error in predictors. Comparing to the Monte Carlo dropout method, our proposed framework is more aware of misclassification cases. Our proposed framework for uncertainty quantification in deep learning can support risk-based decision making in applications when discrete errors in predictors are present."
        },
        {
            "heading": "1. Introduction",
            "text": "Decision-making based on predictions, especially in classification problems, has been a\nsubject of interest since machine-learning (ML) models were invented. To understand and manage the risk of using predictions in decision making, it is essential to quantify the uncertainty of each prediction in addition to assessing the overall performance of these learning algorithms. Quantifying the uncertainty associated with such algorithms is not trivial, and the complexity of calculation increases as the nonlinearity of such algorithms increases. In this essence, uncertainty quantification (UQ) of Deep Neural Network (DNN) classification models, which have a nonlinear and complex mathematical structure, is especially challenging. Despite this challenge, DNN has become a powerful prediction tool in many fields from language processing, finance, investment, engineering to medicine. Although the application of these models has been tied to image and video processing for years, they are becoming more popular for tabular data with various applications such as to predict disease outcomes (De Angeli et al. 2022), credit risk (Chang et al. 2022), exchange rate (Mourtas et al. 2023), and vehicle emissions (Seo and Park 2023). The growing application of DNN classification models in life-threatening problems, such as medical decision making, increases the importance of assessing the prediction uncertainty of these models."
        },
        {
            "heading": "1.1 Uncertainty Definition",
            "text": "Adopting the general definition of uncertainty as any deviation from complete\ndeterminism (Walker et al. 2003), several uncertainty typologies have been created for various purposes. Suppose \ud835\udc37 is the dataset consisting of \ud835\udc41 training data points, {(\ud835\udc311, \ud835\udc661), (\ud835\udc312, \ud835\udc662), \u2026 , (\ud835\udc31\ud835\udc41, \ud835\udc66\ud835\udc41)} where \ud835\udc66\ud835\udc56 = {1,2, \u2026 , \ud835\udc36} for \ud835\udc56 = 1, \u2026 , \ud835\udc41. For each new query \ud835\udc65\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc51, the classification model outputs predictive distribution \ud835\udc5d\ud835\udc50(\ud835\udc65 \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc51) =\n\ud835\udc5d(\ud835\udc66\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc51 = \ud835\udc50|\ud835\udc65\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc51 , \ud835\udc37). The predicted class is then \ud835\udc50\u2217 = argmax 1,2,\u2026,\ud835\udc36 \ud835\udc5d\ud835\udc50(\ud835\udc65 \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc51). There are two common metrics in the literature for quantifying the uncertainty of a prediction, variation ratio and predictive entropy (Gal 2016). The variation ratio approximates the quantity 1 \u2212 \ud835\udc5d\ud835\udc50\u2217(\ud835\udc65 \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc51), whereas predictive entropy quantifies the degree of uncertainty for the predictive distribution, \ud835\udc5d\ud835\udc50(\ud835\udc65 \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc51)."
        },
        {
            "heading": "1.2. Types of Uncertainty",
            "text": "Our research is focused on the uncertainty that should be considered in model-based\ndecision-support. Uncertainty can be explored in three dimensions, location, level, and nature according to the literature on uncertainty management (Walker et al. 2003). The location of uncertainty identifies where it is demonstrated within the whole model, which can be within the context, model structure, inputs, parameters, or outcomes. There are four levels of uncertainty to recognize when developing strategies to cope with uncertainty in policy making, i.e., statistical uncertainty, scenario uncertainty, recognized ignorance, and total ignorance or deep uncertainty (Walker et al. 2003). Based on this definition, recognized ignorance can further be divided into reducible and irreducible ignorance. The nature of uncertainty, which can be epistemic or aleatoric, is the dimension of uncertainty that has been welcomed by the machine learning community in recent years. Epistemic uncertainty is due to the lack of knowledge about the behavior of system, and aleatoric uncertainty refers to the notion of randomness (H\u00fcllermeier and Waegeman 2021, Hora 1996). According to this definition, aleatoric uncertainty is irreducible, as it occurs even if the best model is known. In contrast, epistemic uncertainty can be reduced by employing a more proper model. However, aleatoric and epistemic uncertainty should not be considered as absolute notions, since they may turn into each other during the\nstudy, based on the context and application of the model (H\u00fcllermeier and Waegeman 2021, Der Kiureghian and Ditlevsen 2009).\nBoth aleatoric and epistemic uncertainty can occur in any location of the model, from\ninput data to model structure (Walker et al. 2003). To illustrate, consider the binary classification problem represented in Figure 1, in which a model is trained on trainset \ud835\udc37 using \ud835\udc651 and \ud835\udc652 as predictors with positive and negative labels. Suppose two support vector machines with radial basis function kernel but different hyperparameter initialization are trained to predict labels. Three new queries with true negative label \ud835\udc4b1, \ud835\udc4b2 and \ud835\udc4b3 are given to the model to be classified. Then the prediction has high epistemic uncertainty for \ud835\udc4b1 due to lack of knowledge, high aleatoric uncertainty for \ud835\udc4b2 due to stochasticity of data, and high epistemic uncertainty for \ud835\udc4b3 due to selection of model hyperparameters.\nprediction uncertainty is high for \ud835\udc4b3 since different hypotheses about model hyperparameters will change predictions for this query. Obtaining more knowledge about the behavior of data reduces prediction uncertainty in favor of this point.\nIn a problem setting, aleatoric uncertainty is often irreducible, meaning that even\ncollecting more training data or information about the behavior of the system may not decrease the inherent randomness of data (H\u00fcllermeier and Waegeman 2021). Although aleatoric uncertainty can be reduced by adding more variables to the model as illustrated in Figure 2, it also changes the problem setting since it changes the set of predictors. Conversely, epistemic uncertainty is known as reducible uncertainty. It implies that collecting more information about data or behavior of the system may decrease this type of uncertainty without any modification to the problem setting.\nFigure 3(b) illustrates the regions characterized by elevated levels of aleatoric and epistemic uncertainty during prediction when the uncertainty in \ud835\udc4b1 is not considered. In this context,\ninstances where the new query falls outside the established classification regions result in increased epistemic uncertainty, attributed to the lack of available information. When the new query falls within the intersection of classification boundaries, the aleatoric uncertainty is amplified, stemming from the inherent randomness within the data. Suppose the errors in variable \ud835\udc651 follow a discrete distribution. When observing \ud835\udc651 as \ud835\udc65 \u2217, the true value can be ?\u030c? < \ud835\udc65\u2217 with a probability of \ud835\udc5d1, or ?\u0302? > \ud835\udc65 \u2217 with a probability of \ud835\udc5d2.\nFigure 3(c) demonstrates the alterations in predictions (referred to as transition points in\nthe following) and associated uncertainties when the true value of \ud835\udc651 deviates from \ud835\udc65 \u2217. More specifically, transition 2 highlights the genuine significance of \ud835\udc651 as ?\u0302? for the observed value \ud835\udc65 \u2217, leading to prediction being altered with a probability of \ud835\udc5d2 . In the same manner, transition 6 alters the degree of aleatoric uncertainty with a probability of \ud835\udc5d1, and transition 4 alters the degree of epistemic uncertainty with a probability of \ud835\udc5d2. In contrast, Figure 3(d) exemplifies instances where deviations from the observed values of \ud835\udc651 do not impact either prediction or the associated degrees of uncertainty.\nIgnoring the errors in variables, where the new query falls outside the classification regions, results in increased epistemic uncertainty. When the new query falls within the intersection of classification boundaries, the aleatoric uncertainty is amplified, stemming from the inherent randomness within the data. c) Considering errors in variable \ud835\udc651, transition 2 affects the final prediction, transition 6 affects degree of aleatoric uncertainty, and transition 4 increases epistemic uncertainty. d) Considering errors in variable \ud835\udc651, for some instances, and some small deviations from observed values, neither prediction nor corresponding uncertainties are affected."
        },
        {
            "heading": "1.3 An illustrative Example in Health Care",
            "text": "Tuberculosis (TB) is a global health challenge and the number one cause of death due to\ninfectious diseases. Clinical tools aiding physicians in selecting treatment options for drugsusceptible TB are typically limited to one standard anti-TB regime with any changes to regimes\ndriven by reacting to patient side-effects. Effective patient outcome prediction has the potential to assist physicians in developing individualized patient medical treatment course based on patient-based parameters that indicate their progression through therapy. This example addresses the problem of predicting TB treatment binary outcome (i.e., a patient is cured or not) using follow-up information such as laboratory test results in addition to baseline data such as age, gender, and TB type (Kheirandish et al. 2022). Typically, four standard laboratory tests are used to tract patient-progress follow-up are the Acid-Fast Bacillus (AFB) smear, culture, GeneXpert, and Drug Susceptibility Test (DST). AFB smear, the cheapest and fastest method for detecting mycobacterium tuberculosis (Mtb), has a significant false negative rate, so that negative results do not exclude TB disease. While culture (both solid and liquid) is known as the gold standard for lab confirmation of TB disease, is much slower and because it requires sophisticated biosafety equipment to protect laboratory workers, is more expensive than AFB smear. We address the question of whether ordering and waiting on culture results is always necessary for treatment monitoring.\nDuring treatment, a physician typically orders multiple AFB smears and at least one\nculture (Organization and Initiative 2010). If a physician feels patient progress through therapy is not satisfactorily, they may order additional tests (e.g., DST)\nTo illustrate, we selected the records of five tuberculosis patients who received treatment\nand their associate outcome from a longitudinal dataset containing 19,252 patients from the Republic of Moldova (SIMETB 2016). The same list of predictors as found in Kheirandish et al. (2022) is utilized to train an DNN, specifically Long-Short-Term-Memory (LSTM), classification model at a number of predetermined time points from the start of treatment. Because Mtb grow so slowly, the results of a culture test arrive in about eight weeks after the\nsample is taken. Although the results may be known during model training phase, they are typically not available at the actual prediction time. To assess the effect of low sensitivity of smear test on model predictions, we consider and compare two scenarios in five patients based on whether culture information is utilized or not, at prediction (Figure 4). In the first scenario, the last culture results are not utilized when predicting the treatment outcome which reflects the fact that the previous culture result has not arrived. In contrast, the second scenario involves waiting to receive culture results and utilize them to verify the smear results received earlier.\nAs shown in Figure 4, for Patient 1, ordering and waiting for culture results do not impact\nthe prediction performance, as the prediction model consistently predicts the true label regardless of this additional step. Similarly, for Patient 5, the process of ordering and waiting on culture results does not help correct the wrong prediction. Consequently, ordering culture tests for both Patients 1 and 5 would merely incur additional treatment costs, and extend decision time on treatment without contributing to treatment monitoring. This holds true for Patient 2 at the 2- month follow-up, Patient 3 at the 2-month and 6-month follow-ups, and Patient 4 at 2-month follow-up.\nScenario two. Each box represents a patient\u2019s case, and the black solid horizontal line is the corresponding true treatment outcome. Gray and black points are predicted outcomes and the arcs link two corresponding predictions of the two scenarios. Not waiting for culture results in any of the follow-up sessions is the more advantageous decision for Patients 1 and 5.\nDiscovering the situations in which ordering smear tests is sufficient, despite its low\nsensitivity, has the potential to improve the efficacy of treatment monitoring while saving time and medical resources. Examining whether the impact of errors in smear results on prediction uncertainty is concerning can reveal these instances. This paper addresses the problem of quantifying prediction uncertainty that originates from errors in variables for Deep Neural Networks (DNN) binary classification models. Since uncertainty in smear results comes from low sensitivity of the test, in our method we assume that errors in variables follow some known discrete distribution."
        },
        {
            "heading": "1.4 Assessing Prediction Uncertainty",
            "text": "The uncertainty that comes from errors in input can be assumed as epistemic uncertainty\nif repeated measurements aid in reducing the error. Otherwise, it should be considered as aleatoric uncertainty since collecting more information does not help decrease the related uncertainty. However, in the computer vision literature, aleatoric is known as data uncertainty, and epistemic as model parameters uncertainty (Abdar et al. 2021). The necessity of revisiting the definitions and concepts of aleatoric and epistemic uncertainty seems imperative since deep learning models are being applied to more applications, such as modeling tabular data in addition to image modeling. An explicit distinction between these two types of uncertainty is necessary for risk-based decision making. It benefits the decision making problems by refusing or delaying\nthe ultimate decision, such as in the learning-to-reject methods (Zhang et al. 2023), or by offering to take proper actions to reduce the uncertainty, such as in active learning (Heirung et al. 2018).\nIn this paper, we focus on quantifying the deep learning classification uncertainty, which\nmanifests from the errors in variables following some discrete distribution. These errors are assumed to be aleatoric that should be represented rather than being controlled, and they originated from any sources ranging from the measurement errors to random disturbances. In Section 2, we review the existing methods of uncertainty quantification in the machine/deep learning literature. In Section 3 we introduce the mathematical formulation and solution. Finally, the results regarding applying the mathematical model on the real problem introduced in Section 1.3 will be represented and discussed in Section 5."
        },
        {
            "heading": "2. Literature Review",
            "text": "This section presents machine learning methods on representing uncertainty in\nclassification models with a focus on the methods that distinguish aleatoric and epistemic uncertainties. The main goal of these methods is to predict the label for a set of observed predictors and the degree of uncertainty which is attached to this prediction. Bayesian inferencebased methods (Theodoridis 2015) are the most common machine learning methods to quantify aleatoric and epistemic uncertainty. In these methods, a prior distribution is assumed for any desired source of epistemic uncertainty, which can be the set of parameters or any other hypothesis space. Then the knowledge about this distribution such as in Gaussian processes models (Seeger 2004) and Bayesian learning, or the set of distributions such as credal sets models (Abell\u00e1n, Klir, and Moral 2006) are updated in view of additional observations. Ensemble learning, exemplified by bagging or boosting, represents another significant class of\nmethods to quantify uncertainty. Such methods produce a set of predictors that is tempting to produce probability estimates following basic frequentist principles.\nDNN models also benefit from common uncertainty quantification methods in machine\nlearning. What makes the DNNs different from other machine learning models in essence of uncertainty quantification is that their modelling flexibility makes the epistemic uncertainty related to model hypothesis space negligible. Therefore, the main source of epistemic uncertainty is related to parameter estimation. Deep learning classification models capture aleatoric uncertainty through some transformation functions such as softmax or sigmoid layers. However, to capture epistemic uncertainty related to model parameters, Bayesian deep learning frameworks (Denker and LeCun 1990, Graves 2011, Neal 2012) were invented, in which each network weight is represented by a probability distribution. This probability distribution is updated through Bayesian deep learning. Finally, predictive distribution of labels is computed by the factorization of conditional probabilities of response given some weights and the posterior distribution of weights. Since the posterior distribution of weights cannot be obtained analytically, some approximation techniques are needed. That is how different uncertainty quantification techniques on DNN models vary from a Bayesian perspective. Laplace approximation (Mackay 1992), Markov Chain Monte Carlo (MCMC) (Neal 1992), ensemble learning (Lakshminarayanan, Pritzel, and Blundell 2017), and variational inference methods (Rossi, Michiardi, and Filippone 2019, Graves 2011, Blundell et al. 2015, Louizos and Welling 2016) are among the approximation methods to derive the posterior distribution of weights in neural networks models.\nSome uncertainty quantification methods in the deep learning literature only focus on\nlearning the total uncertainty in prediction, considering uncertainty from model parameters.\nWhile some methods advance to distinguish between aleatoric and epistemic uncertainty of predictions. For instance, using Monte Carlo dropout (MC-dropout) as a learning technique aids in training a DNN considering uncertainty in parameters. Then, given a new query, this trained model derives the predictive distribution of labels through performing some stochastic forward passes and averaging the results (Gal and Ghahramani 2016). While this method can be used to quantify the total uncertainty of prediction, requires additional steps to distinguish between the aleatoric and epistemic components of this uncertainty.\nOne approach for uncertainty decomposition involves measuring both total and aleatoric\nuncertainty, and then calculating the epistemic uncertainty as the resulting difference. This difference equals the mutual information between the prediction and the posterior over the model parameters, which was first introduced for Gaussian Process Classifier (Houlsby et al. 2011) and was adopted as another measure of uncertainty in MC-dropout technique (Gal 2016). Depeweg et al. (2018) undertook a deliberate endeavour to measure and distinguish aleatoric and epistemic uncertainty by adopting this mutual information as the epistemic uncertainty in Bayesian deep learning with latent variables. A similar approach was recently employed to decompose aleatoric and epistemic uncertainty in DNNs with DropConnect (Mobiny et al. 2021).\nErrors in variables which are also known as measurement errors, noise in data, and input\nuncertainty, are one of the common sources of aleatoric uncertainty in prediction models. The literature on training DNNs, whether deterministic or Bayesian, with consideration for errors in variables, is extensive when concerning regression problems (Seghouane and Fleury 2001, Van Gorp, Schoukens, and Pintelon 1998, 2000, Martin and Elster 2022). In the realm of training regression DNNs while accounting for uncertain inputs, two prevailing approaches are prominent: the utilization of DNNs with errors in variables (Martin and Elster 2022) and\nBayesian Neural Networks with latent variables (Depeweg et al. 2016). These methodologies represent widely adopted strategies for handling the challenge of uncertainty in input data during the training process. However, to the best of our knowledge, comprehensive models addressing classification DNNs with uncertain inputs remain relatively unexplored. Moreover, existing DNNs that account for input uncertainty predominantly rely on an underlying assumption of normality for noise in inputs. This assumption restricts the applicability of such models primarily to prediction scenarios featuring continuous predictors.\nIn this paper, we propose a mathematical framework to quantify prediction uncertainty\narises from errors in inputs in classification deep learning models when errors follow some discrete distribution, which complements the literature."
        },
        {
            "heading": "3. Mathematical Framework",
            "text": "In this section, a mathematical framework to quantify the uncertainty of binary\nclassification DNN model is presented. The uncertainty is divided into the part which arises from errors in at least one of the predictors if the error follows a discrete distribution with known probability mass function. Since the notion of error is assumed to be discrete, the model is mostly practical when uncertain predictors are discrete variables. However, no specific assumptions are made regarding the notion of predictors, leaving them open to being discrete or continuous. In this methodology, during the training phase, we have access to the true values of predictors in the dataset. However, at the prediction, the true value of the new query may differ from the observed value. The primary objective of this approach is to evaluate the impact of any divergence between the true values and the observed values on the model predictions uncertainty. The model is trained based on true values, and we aim to investigate how uncertainty arises in predictions when true values deviate from what was originally observed.\nIn our methodology, we extend the original concept by performing calculations based on\nthe inputs of the sigmoid layer, rather than relying on the outputs of the sigmoid layer, as initially elucidated in(Gal and Ghahramani 2016). This modification allows for a more comprehensive and refined analysis of the model uncertainty in binary classification. By considering the sigmoid layer inputs, we can obtain additional insights into the underlying probabilities and decisionmaking processes, leading to a more accurate assessment of the model prediction uncertainty.\nGiven a dataset \ud835\udc37 = {(\ud835\udc31\ud835\udc5b, \ud835\udc66\ud835\udc5b), \ud835\udc5b = 0,1, \u2026 , \ud835\udc41} where \ud835\udc41 is the dataset size, each entry\nconsists of \ud835\udc43 features \ud835\udc31\ud835\udc5b \u2208 \u211d \ud835\udc43, and the corresponding class label \ud835\udc66\ud835\udc5b \u2208 {0,1}, we consider a classification DNN model with a set of parameters \ud835\udec9. The neural network can have any structure but employs the sigmoid activation function and two nodes in the last layer.\nWe assume that \ud835\udc5d\ud835\udf43(\ud835\udc31) which represents the probability distribution of true values of\nfeatures, \ud835\udc31, when features are observed as \ud835\udf43, is known. The predictive distribution for the target variable \ud835\udc66\ud835\udc5b\ud835\udc52\ud835\udc64 associated with the observed features \ud835\udf43 is as Equation (1).\n\ud835\udc5d\ud835\udf43(\ud835\udc66 \ud835\udc5b\ud835\udc52\ud835\udc64|\ud835\udc37) = \u2211 \u222b \ud835\udc5d\ud835\udf43(\ud835\udc66 \ud835\udc5b\ud835\udc52\ud835\udc64|\ud835\udc31, \ud835\udec9 )\ud835\udc5d\ud835\udf43(\ud835\udc31)\ud835\udc5d(\ud835\udec9|\ud835\udc03) \ud835\udc51\ud835\udec9\n\ud835\udc99\n. Eq. (1)\nAccepting prediction uncertainty as how spread the predictive distribution is around the mode\n(Gal 2016), Equation (2) derives prediction uncertainty, \ud835\udc48, when \ud835\udc50\u2217 = argmax 0,1 (\ud835\udc43\ud835\udf43(\ud835\udc66 \ud835\udc5b\ud835\udc52\ud835\udc64 =\n\ud835\udc50|\ud835\udc37)). In the binary case, prediction uncertainty attains its maximum at 0.5, when\n\ud835\udc43\ud835\udf43(\ud835\udc66 \ud835\udc5b\ud835\udc52\ud835\udc64 = 0|\ud835\udc37) = \ud835\udc43\ud835\udf43(\ud835\udc66 \ud835\udc5b\ud835\udc52\ud835\udc64 = 1|\ud835\udc37) = 0.5. It attains its minimum at 0, when \ud835\udc43\ud835\udf43(\ud835\udc66 \ud835\udc5b\ud835\udc52\ud835\udc64 = \ud835\udc50|\ud835\udc37)=1 for one class and \ud835\udc43\ud835\udf43(\ud835\udc66 \ud835\udc5b\ud835\udc52\ud835\udc64 = \ud835\udc50|\ud835\udc37) = 0 for the other class.\n\ud835\udc48 = 1 \u2212 \ud835\udc43\ud835\udf43(\ud835\udc66 \ud835\udc5b\ud835\udc52\ud835\udc64 = \ud835\udc50\u2217|\ud835\udc37) = 1 \u2212 \u2211 \u222b \ud835\udc43\ud835\udf43(\ud835\udc66 \ud835\udc5b\ud835\udc52\ud835\udc64 = \ud835\udc50\u2217|\ud835\udc31, \ud835\udec9 )\ud835\udc5d\ud835\udf43(\ud835\udc31)\ud835\udc5d(\ud835\udec9|\ud835\udc37) \ud835\udc51\ud835\udec9\n\ud835\udc99\n. Eq. (2)\nMoving a layer back from the sigmoid layer, \ud835\udc4d0 and \ud835\udc4d1 are the outputs of the trained DNN model which are in fact sigmoid layer inputs. The \ud835\udc4d0 and \ud835\udc4d1 are a function of \ud835\udc31 and \ud835\udec9 \u2217, \ud835\udc51\ud835\udc5b\ud835\udc5b(\ud835\udc31, \ud835\udec9\u2217), in which the function \ud835\udc51\ud835\udc5b\ud835\udc5b(\u2219) is defined by the network structure of DNN model. Since \ud835\udc31, and \ud835\udec9\u2217 are all random variables, it makes the \ud835\udc4d0 and \ud835\udc4d1 to be a random variable as well. Finding the probability distribution of \ud835\udc4d0 and \ud835\udc4d1 is not trivial due to the nonlinearity of the function, \ud835\udc51\ud835\udc5b\ud835\udc5b(\u2219). However, the mean and variance of the \ud835\udc4d0 and \ud835\udc4d1 can be estimated.\nWhen \ud835\udc670 and \ud835\udc671 are predicted by the DNN model, \ud835\udc5d\ud835\udf43(\ud835\udc66 \ud835\udc5b\ud835\udc52\ud835\udc64) is estimated by ?\u0302?\ud835\udf43(\ud835\udc66 \ud835\udc5b\ud835\udc52\ud835\udc64 = 0) = \ud835\udf0e0(\ud835\udc670, \ud835\udc671) = \ud835\udc52\u2212\ud835\udc670\n\ud835\udc52\u2212\ud835\udc670+\ud835\udc52\u2212\ud835\udc671 and ?\u0302?\ud835\udf43(\ud835\udc66\n\ud835\udc5b\ud835\udc52\ud835\udc64 = 1) = \ud835\udf0e1(\ud835\udc670, \ud835\udc671) = \ud835\udc52\u2212\ud835\udc671\n\ud835\udc52\u2212\ud835\udc670+\ud835\udc52\u2212\ud835\udc671 . Without compromising the\ngenerality of method, we derive expected value of ?\u0302?\ud835\udf43(\ud835\udc66 \ud835\udc5b\ud835\udc52\ud835\udc64 = 1) assuming that \ud835\udc50\u2217 =\nargmax 0,1\n(\ud835\udc43\ud835\udf43(\ud835\udc66 \ud835\udc5b\ud835\udc52\ud835\udc64 = \ud835\udc50|\ud835\udc37)) = 1We start with quadratic Taylor approximation of the sigmoid\nfunction \ud835\udf0e1(\ud835\udc4d0, \ud835\udc4d1) around \ud835\udf070 = E[\ud835\udc4d0] and \ud835\udf071 = \ud835\udc38[\ud835\udc4d1] as follows:\n\ud835\udf0e1(\ud835\udc4d0, \ud835\udc4d1) \u2248 \ud835\udc44(\ud835\udc4d0, \ud835\udc4d1)\n= \ud835\udf0e1(\ud835\udf070, \ud835\udf071) + \ud835\udf0e1 \u2032(\ud835\udf070, \ud835\udf071)(\ud835\udc4d0 \u2212 \ud835\udf070) + \ud835\udf0e1 \u2032(\ud835\udf070, \ud835\udf071)(\ud835\udc4d1 \u2212 \ud835\udf071) + 1\n2 \ud835\udf0e1\u2033(\ud835\udf070, \ud835\udf071)(\ud835\udc4d0 \u2212 \ud835\udf070)\n2 + 1\n2 \ud835\udf0e1\u2033(\ud835\udf070, \ud835\udf071)(\ud835\udc4d1 \u2212 \ud835\udf071)\n2\n+ \ud835\udf0e1\u2033(\ud835\udf070, \ud835\udf071)(\ud835\udc4d0 \u2212 \ud835\udf070)(\ud835\udc4d1 \u2212 \ud835\udf071).\nEq. (3)\nTaking derivatives of the sigmoid functions \ud835\udf0e0 and \ud835\udf0e1, i.e.,\n\ud835\udf15\ud835\udf0e\ud835\udc56(\ud835\udc670, \ud835\udc671)\n\ud835\udf15\ud835\udc67\ud835\udc56 = \ud835\udf0e\ud835\udc56(\ud835\udc670, \ud835\udc671)[1 \u2212 \ud835\udf0e\ud835\udc56(\ud835\udc670, \ud835\udc671)], \ud835\udc56 = 0,1;\n\ud835\udf15\ud835\udf0e\ud835\udc56(\ud835\udc670, \ud835\udc671)\n\ud835\udf15\ud835\udc67\ud835\udc57 = \u2212\ud835\udf0e\ud835\udc56(\ud835\udc670, \ud835\udc671)\ud835\udf0e\ud835\udc57(\ud835\udc670, \ud835\udc671), \ud835\udc56 = 0, 1; \ud835\udc57 = 0, 1, \ud835\udc57 \u2260 \ud835\udc56,\nEq. (4)\n\ud835\udc44(\ud835\udc4d0, \ud835\udc4d1) can be rewritten as Equation (5).\n\ud835\udc44(\ud835\udc4d0, \ud835\udc4d1) = \ud835\udf0e1(\ud835\udf070, \ud835\udf071) + \ud835\udf0e1(\ud835\udf070, \ud835\udf071)\ud835\udf0e0(\ud835\udf070, \ud835\udf071)[\ud835\udc4d1 \u2212 \ud835\udc4d0 \u2212 (\ud835\udf071 \u2212 \ud835\udf070)]\n+ 1\n2 \ud835\udf0e0(\ud835\udf070, \ud835\udf071)\ud835\udf0e1(\ud835\udf070, \ud835\udf071)[\ud835\udf0e0(\ud835\udf070, \ud835\udf071) \u2212 \ud835\udf0e1(\ud835\udf070, \ud835\udf071)][(\ud835\udc4d1 \u2212 \ud835\udf071)\n2\n+ (\ud835\udc4d0 \u2212 \ud835\udf070) 2 \u2212 2(\ud835\udc4d1 \u2212 \ud835\udf071)(\ud835\udc4d0 \u2212 \ud835\udf070)]. Eq. (5)\nDefining \u2206 = \ud835\udc4d1 \u2212 \ud835\udc4d0 and \ud835\udf07\u2206 = \u03bc1 \u2212 \u03bc0, Equation (5) is rewritten as\n\ud835\udc44(\ud835\udc4d0, \ud835\udc4d1) = \ud835\udf0e1(\ud835\udf070, \ud835\udf071)\n+ \ud835\udf0e1(\ud835\udf070, \ud835\udf071)\ud835\udf0e0(\ud835\udf070, \ud835\udf071)[\u2206 \u2212 \ud835\udf07\u2206] + 1\n2 \ud835\udf0e0(\ud835\udf070, \ud835\udf071)\ud835\udf0e1(\ud835\udf070, \ud835\udf071)[\ud835\udf0e0(\ud835\udf070, \ud835\udf071) \u2212 \ud835\udf0e1(\ud835\udf070, \ud835\udf071)](\u2206 \u2212 \ud835\udf07\u2206) 2. Eq. (6)\nBy taking the expected value from both sides of Equation (6) and using the fact that \ud835\udc38[\u2206 \u2212 \ud835\udf07\u2206] = 0 and \ud835\udf0e0(\ud835\udf070, \ud835\udf071) = 1 \u2212 \ud835\udf0e1(\ud835\udf070, \ud835\udf071), we derive Equation (7).\n\ud835\udc38[\ud835\udc44(\ud835\udc4d0, \ud835\udc4d1)] = \ud835\udf0e1(\ud835\udf070, \ud835\udf071)\n+ 1\n2 \ud835\udf0e1(\ud835\udf070, \ud835\udf071)[1 \u2212 \ud835\udf0e1(\ud835\udf070, \ud835\udf071)][1 \u2212 2\ud835\udf0e1(\ud835\udf070, \ud835\udf071)]\ud835\udc49\ud835\udc4e\ud835\udc5f[\u2206]. Eq. (7)\nTo derive \ud835\udf070, \ud835\udf071, and \ud835\udc49\ud835\udc4e\ud835\udc5f[\u2206], it is important to note that the randomness of \ud835\udc4d\ud835\udc56 in equation (6) arises from both parameters (\ud835\udf3d) and error in predictors (\ud835\udc31). Let \ud835\udc67\ud835\udc56 denote the conditional expectation E[\ud835\udc4d\ud835\udc56|\ud835\udc31, \ud835\udf3d], then,\n\ud835\udf07\ud835\udc56 = E[\ud835\udc4d\ud835\udc56] = \u2211 \u222b \ud835\udc67\ud835\udc56\ud835\udc5d\ud835\udf43(\ud835\udc31)\ud835\udc5d(\ud835\udf3d|\ud835\udc37) \ud835\udc51\ud835\udf3d\n\ud835\udc31\n, \ud835\udc56 = 0,1. Eq. (8)\n\ud835\udc49\ud835\udc4e\ud835\udc5f[\u2206] = \ud835\udc38[\u22062] \u2212 (E[\u2206])2\n= \u2211 \u222b(\ud835\udc671 \u2212 \ud835\udc670) 2\ud835\udc5d\ud835\udf43(\ud835\udc31)\ud835\udc5d(\ud835\udf3d|\ud835\udc37) \ud835\udc51\ud835\udf3d\n\ud835\udc31\n\u2212 (\u2211 \u222b(\ud835\udc671 \u2212 \ud835\udc670)\ud835\udc5d\ud835\udf43(\ud835\udc31)\ud835\udc5d(\ud835\udf3d|\ud835\udc31) \ud835\udc51\ud835\udf3d\n\ud835\udc31\n)\n2\n. Eq. (9)\nThe posterior distribution of parameters which is denoted as \ud835\udc5d(\ud835\udf3d|\ud835\udc03) in equations (1), (8),\nand (9) is intractable (H\u00fcllermeier and Waegeman 2021). Therefore, there is a need for an approximation technique to calculate the integral in these equations. Our method is flexible to use various uncertainty quantification techniques for DNN models which are available in the literature, such as Bayesian neural networks (Denker and LeCun 1990, Graves 2011, Neal 2012), MC-dropout (Gal and Ghahramani 2016), variational inference (Rossi, Michiardi, and Filippone 2019, Graves 2011, Blundell et al. 2015, Louizos and Welling 2016), and ensemble learning (Lakshminarayanan, Pritzel, and Blundell 2017). To illustrate, we adopt an ensemble learning model in the training phase. The model obtains \ud835\udc47 bootstrap samples from the trainset, each generating a set of trained parameters. The \ud835\udf07\ud835\udc56 and \ud835\udc49\ud835\udc4e\ud835\udc5f[\u0394] in Equations (8) and (9) are approximated by ?\u0302?\ud835\udc56 and \ud835\udc49?\u0302?\ud835\udc5f[\u2206] in Equations (10) and (11), respectively.\n?\u0302?\ud835\udc56 = 1\n\ud835\udc47 \u2211 \u2211 \ud835\udc67\ud835\udc56\n\ud835\udc61\ud835\udc5d\ud835\udf43(\ud835\udc31)\n\ud835\udc47\n\ud835\udc61=1\ud835\udc31\n, \ud835\udc56 = 0,1. Eq. (10)\n\ud835\udc49?\u0302?\ud835\udc5f[\u2206] = 1\n\ud835\udc47 \u2211 \u2211(\ud835\udc671\n\ud835\udc61 \u2212 \ud835\udc670 \ud835\udc61)2\ud835\udc5d\ud835\udf43(\ud835\udc31)\n\ud835\udc47\n\ud835\udc61=1\ud835\udc31\n\u2212 ( 1\n\ud835\udc47 \u2211 \u2211(\ud835\udc671\n\ud835\udc61 \u2212 \ud835\udc670 \ud835\udc61)\ud835\udc5d\ud835\udf43(\ud835\udc31)\n\ud835\udc47\n\ud835\udc61=1\ud835\udc31\n)\n2\nEq. (11)\nLastly, Prediction uncertainty in Equation (2) can be estimated by Equation (11). Since\nthe prediction uncertainty is estimated considering the uncertainty in parameters and errors in predictors, it is specifically referred to as Errors in Variables (EIV) uncertainty, ?\u0302?\ud835\udc38\ud835\udc3c\ud835\udc49\n?\u0302?\ud835\udc38\ud835\udc3c\ud835\udc49 = 1 \u2212 \ud835\udf0e1(?\u0302?0, ?\u0302?1) \u2212 1\n2 \ud835\udf0e1(?\u0302?0, ?\u0302?1)[1 \u2212 \ud835\udf0e1(?\u0302?0, ?\u0302?1)][1 \u2212 2\ud835\udf0e1(?\u0302?0, ?\u0302?1)]\ud835\udc49?\u0302?\ud835\udc5f[\u2206]. Eq. (12)\nThe prediction uncertainty that does not originate from errors in variables can be derived\nwhen \ud835\udc5d\ud835\udf43(\ud835\udc31) = { 1, \ud835\udc31 = \ud835\udf43 0, otherwise . Proposition 1 derived in the following indicates when the observed and true feature values for a new data point are identical, 1 \u2212 \ud835\udc38[\ud835\udc44(\ud835\udc4d0, \ud835\udc4d1)] yields the prediction uncertainty that does not result from errors in variables but parameters. This prediction uncertainty is referred to as non-EIV uncertainty, \ud835\udc48\ud835\udc5b\ud835\udc5c\ud835\udc5b\ud835\udc38\ud835\udc3c\ud835\udc49in this research and can be derived through Equations (13-15).\n?\u0302?\ud835\udc5b\ud835\udc5c\ud835\udc5b\ud835\udc38\ud835\udc3c\ud835\udc49 = 1 \u2212 \ud835\udc38[\ud835\udc44(\ud835\udc4d0, \ud835\udc4d1)]\n= 1 \u2212 \ud835\udf0e1(?\u0302?0, ?\u0302?1) \u2212 1\n2 \ud835\udf0e1(?\u0302?0, ?\u0302?1)[1 \u2212 \ud835\udf0e1(?\u0302?0, ?\u0302?1)][1 \u2212 2\ud835\udf0e1(?\u0302?0, ?\u0302?1)]\ud835\udc49?\u0302?\ud835\udc5f[\u2206]. Eq. (13)\n?\u0302?\ud835\udc56 = 1\n\ud835\udc47 \u2211 \ud835\udc67\ud835\udc56\n\ud835\udc61\n\ud835\udc47\n\ud835\udc61=1\n, \ud835\udc56 = 0,1. Eq. (14)\n\ud835\udc49?\u0302?\ud835\udc5f[\u2206] = 1\n\ud835\udc47 \u2211(\ud835\udc671\n\ud835\udc61 \u2212 \ud835\udc670 \ud835\udc61)2\n\ud835\udc47\n\ud835\udc61=1\n\u2212 ( 1\n\ud835\udc47 \u2211(\ud835\udc671\n\ud835\udc61 \u2212 \ud835\udc670 \ud835\udc61)\n\ud835\udc47\n\ud835\udc61=1\n)\n2\n. Eq. (15)\nProposition 1: When \ud835\udc5d\ud835\udf43(\ud835\udc31) = { 1, \ud835\udc31 = \ud835\udf43 0, otherwise , then 1 \u2212 \ud835\udc38[\ud835\udc44(\ud835\udc4d0, \ud835\udc4d1)] derives prediction uncertainty considering the uncertainty in model parameters.\nProof: \ud835\udc38[\ud835\udc44(\ud835\udc4d0, \ud835\udc4d1)] in Equation (7) estimates predictive uncertainty in Equation (1) without any assumption about the shape of \ud835\udc5d\ud835\udf43(\ud835\udc31). Substituting \ud835\udc5d\ud835\udf43(\ud835\udc31) with \ud835\udc5d\ud835\udf43(\ud835\udc31) = { 1, \ud835\udc31 = \ud835\udf43 0, otherwise , modifies the Equation (1) into Equation (16),\n\ud835\udc5d\ud835\udf43(\ud835\udc66 \ud835\udc5b\ud835\udc52\ud835\udc64|\ud835\udc37) = \u2211 \u222b \ud835\udc5d\ud835\udf43(\ud835\udc66 \ud835\udc5b\ud835\udc52\ud835\udc64|\ud835\udc31, \ud835\udec9 )\ud835\udc5d\ud835\udf43(\ud835\udc31)\ud835\udc5d(\ud835\udec9|\ud835\udc03) \ud835\udc51\ud835\udec9\n\ud835\udc99\n= \u222b p(\ud835\udc66\ud835\udc5b\ud835\udc52\ud835\udc64|\ud835\udc31, \ud835\udec9 )\ud835\udc5d(\ud835\udec9|\ud835\udc03) \ud835\udc51\ud835\udec9 Eq. (16)\nThe term \u222b p(\ud835\udc66\ud835\udc5b\ud835\udc52\ud835\udc64|\ud835\udc31, \ud835\udec9 )\ud835\udc5d(\ud835\udec9|\ud835\udc03) \ud835\udc51\ud835\udec9 in Equation (16) demonstrates predictive distribution considering uncertainty in model parameters (Gal 2016, H\u00fcllermeier and Waegeman 2021). Therefore, 1 \u2212 \ud835\udc38[\ud835\udc44(\ud835\udc4d0, \ud835\udc4d1)] in Equation (12) estimates prediction uncertainty considering uncertainty in model parameters when \ud835\udc5d\ud835\udf43(\ud835\udc31) = { 1, \ud835\udc31 = \ud835\udf43 0, otherwise . \u220e\nIn classification Bayesian DNNs, the predictive distribution derived by applying an\nelementwise sigmoid function to the estimated expected values, \ud835\udf07\ud835\udc56 's. Consequently, the quantified prediction uncertainty in these models corresponds to the first term of Equation (7), \ud835\udc38[\ud835\udf0e1(\ud835\udc670, \ud835\udc671)] = \ud835\udf0e1(\ud835\udf070, \ud835\udf071) when \ud835\udc5d\ud835\udf43(\ud835\udc31) = { 1, \ud835\udc31 = \ud835\udf43 0, otherwise . This indicates that Bayesian DNNs quantify prediction uncertainty resulting from model parameters as a linear Taylor approximation of the sigmoid function, while our model quantifies the uncertainty through a quadratic Taylor approximation of sigmoid function.\nFigures (5) and (6) respectively, provide an illustration of the proposed training and\nprediction algorithms. These visual representations provide a comprehensive understanding of\nthe implemented methodology.\n0, otherwise\n, there is no error in variables and the quantified\nuncertainty does not arises from errors in variables but uncertainty in parameters, denoted as non-EIV uncertainty, \ud835\udc48\ud835\udc5b\ud835\udc5c\ud835\udc5b\ud835\udc38\ud835\udc3c\ud835\udc49. The T bootstrap samples are taken from the train set \ud835\udc37, each generating a set of trained parameters, (\ud835\udf031 \u2217, \ud835\udf032 \u2217, \u2026 , \ud835\udf03\ud835\udc47 \u2217 ). Each trained model \ud835\udc61 = 1,2, \u2026 , \ud835\udc47 outputs \ud835\udc670 \ud835\udc61 and \ud835\udc671 \ud835\udc61 corresponding the labels 0 and 1. A sigmoid function \ud835\udf0e(\u2219) is used to estimate predictive distribution."
        },
        {
            "heading": "4. Experimental Results and Discussion",
            "text": "In this section, we apply the uncertainty quantification method to the tuberculosis dataset\nintroduced in Section 1.3. The LSTM model predicts tuberculosis treatment outcome at 3, 4, 6,\nand 9-month follow-ups. The sensitivity and specificity of the smear test is derived from the literature (Davis et al. 2013) as 64% and 98%, respectively. The culture test is assumed to be perfect due to its high sensitivity and specificity (Caulfield and Wengenack 2016). Therefore, the results of the smear test are confirmed by culture when available for both the train and test sets. However, it is assumed that the culture results from the final time point for the test set remain pending. At each follow-up time point, the longitudinal dataset is partitioned into train and test sets (0.8/0.2) to train a LSTM model. The models are trained with 100, 200, and 500 ensembles. Within each ensemble iteration, the training set is further randomly divided into training and validation subsets. The uncertainty of a prediction is derived using the algorithm depicted in Figure 6. Greater non-EIV uncertainty indicates less model confidence on the predicted labels, which corresponds to an increased probability of encountering misclassifications.\nAccording to Proposition 1, when all predictors are accurate, 1 \u2212 \ud835\udc38[\ud835\udf0e1(\ud835\udc4d0, \ud835\udc4d1)] yields the\nprediction uncertainty that accounts for model parameters uncertainty and referred to as non-EIV uncertainty in this paper. Figure 7 represents a comparative analysis between the non-EIV uncertainty derived from our proposed methodology and the MC-dropout method (Gal 2016). The MC-dropout method serves as an approximation of uncertainty within Bayesian deep Gaussian models, when the epistemic uncertainty arises from model parameters. An ideal uncertainty quantification model should exhibit reduced confidence scores when misclassifications occur. This is equivalent to having a greater degree of prediction uncertainty for misclassification cases. In this essence, the ideal uncertainty quantification approach should exhibit the prediction uncertainty as 0.5 for all misclassification instances. To assess our suggested quantification method, we determine the proportion of misclassification cases in the test set with prediction uncertainty greater than several thresholds. This proportion should be\nequal to one for all possible thresholds for the ideal model, represented as the horizontal dashed line in Figure 7. The closer the corresponding line of a quantification method is to the horizontal dashed line, the better the model performs. Therefore, our model outperforms the MC-dropout method as it is shown in Figure 7.\nProposition 1 states if uncertainty in predictors reaches zero, then EIV and non-EIV\nuncertainties become equal. While the converse of this statement may not hold true, the\ncondition of equal EIV and non-EIV uncertainties can be used to identify potential instances affected by errors in predictors. A graphical representation featuring EIV and non-EIV uncertainties across all data points within the test set facilitates the comprehensive evaluation of model sensitivity to errors in variables. Greater proximity of data points to the identity line signifies the sensitivity of prediction model to errors in variables. Figure 8 demonstrates that predictive models designed to predict TB treatment outcome, using the baseline and follow-up data spanning a duration of 9 months from treatment initiation are more sensitive to errors in smear results than models using information limited to 3, 4 or 6 months.\nFigure 8. Quantified EIV and non-EIV uncertainties of prediction models to predict TB treatment outcome, utilizing baseline and follow-up information up to 3, 4, 6, and 9 months from start of treatment. Each point is a query in the test set. EIV and non-EIV uncertainties are referred to the predicted class for each query, which can be either cured (coded as 1) or not cured (coded as 0). The points that are close to the identity line are more probable to be insensitive to errors in smear test.\nIn Section 1.3, we introduced two scenarios based on whether the most recent smear\nresults were confirmed by culture or not. For each data point within the test set, the predictive label may either change or remain consistent when culture results are considered as a confirmation of smear. A change in the predicted label signifies that the prediction uncertainty associated with the initial label has transitioned from a level below 0.5 to a level above 0.5 when the actual smear result deviates from the observed one. This indicates that errors in smear results affect model predictions for that specific data point.\nFigure 9 highlights that EIV and non-EIV uncertainties when data points in the test set\nare categorized based on whether their predicted label changes or not, when the observed and true smear values differ. It demonstrates that when predicted labels are not prone to alteration in the presence of smear errors, EIV and non-EIV uncertainties tend to closely align. Additionally, it underscores the need to accept a certain degree of risk when determining the sensitivity of predictions to smear errors based on the proximity of EIV and non-EIV uncertainties. It should also be noted that although substantial differences between EIV and non-EIV uncertainties can pinpoint cases in which predictions are susceptible to change, they do not reveal the specific direction of these changes. Alterations in predicted labels can either transform misclassified labels into accurate ones or vice versa. The risk of encountering misclassifications should be placed solely based on non-EIV uncertainty."
        },
        {
            "heading": "5. Conclusion",
            "text": "In this article, we present a mathematical framework designed for quantifying prediction\nuncertainty in deep neural network models when input variables contain error. Under the\nassumption that these errors follow a known discrete distribution, our model can assess the effect of these errors on predictions.\nThe model outputs prediction uncertainty considering errors in both variables and model\nparameters, referred to EIV uncertainty in this article. Additionally, the model provides a counterpart measure referred as non-EIV uncertainty, which disregards errors in variables. It is proved that when all predictors are accurate, EIV and non-EIV uncertainties are equal. This is used to identify a way to detect the instances whose predictions are sensitive to errors in variables. We apply our proposed framework to a dataset containing longitudinal information of tuberculosis patients. Four distinct deep neural network models are trained to predict binary tuberculosis treatment outcome at 3, 4, 6, and 9 months from treatment initiation, accounting for errors associated with smear results.\nThe results demonstrate that the non-EIV uncertainty derived from our model\noutperforms the MC-dropout method, which primarily accounts for uncertainty in model parameters. It is worth noting that MC-dropout is a widely utilized technique for estimating Bayesian deep Gaussian models but cannot be use to assess EIV with discrete predictors. Our results show that proximity between EIV and non-EIV uncertainties can help identify instances robust to prediction alterations in the presence of errors in predictors. This involves accepting some level of risk dependent on the selected proximity threshold. Consequently, one promising future work is to further investigate the behavior of EIV and non-EIV uncertainty. This helps establish effective metrics and associated thresholds for determining when errors in predictors pose substantial challenges."
        }
    ],
    "title": "Quantifying Uncertainty in Deep Learning Classification with Noise in Discrete Inputs for Risk-Based Decision Making",
    "year": 2023
}