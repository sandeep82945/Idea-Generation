{
    "abstractText": "Integrating data across institutions can improve learning efficiency. To integrate data efficiently while protecting privacy, we propose A one\u2010shot, summary\u2010statistics\u2010based, Distributed Algorithm for fitting Penalized (ADAP) regression models across multiple datasets. ADAP utilizes patient\u2010level data from a lead site and incorporates the first\u2010order (ADAP1) and second\u2010order gradients (ADAP2) of the objective function from collaborating sites to construct a surrogate objective function at the lead site, where model fitting is then completed with proper regularizations applied. We evaluate the performance of the proposed method using both simulation and a real\u2010world application to study risk factors for opioid use disorder (OUD) using 15,000 patient data from the OneFlorida Clinical Research Consortium. Our results show that ADAP performs nearly the same as the pooled estimator but achieves higher estimation accuracy and better variable selection than the local and average estimators. Moreover, ADAP2 successfully handles heterogeneity in covariate distributions.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiaokang Liu"
        },
        {
            "affiliations": [],
            "name": "Rui Duan"
        },
        {
            "affiliations": [],
            "name": "Chongliang Luo"
        },
        {
            "affiliations": [],
            "name": "Alexis Ogdie"
        },
        {
            "affiliations": [],
            "name": "Jason H. Moore"
        },
        {
            "affiliations": [],
            "name": "Henry R. Kranzler"
        },
        {
            "affiliations": [],
            "name": "Jiang Bian"
        },
        {
            "affiliations": [],
            "name": "Yong Chen"
        }
    ],
    "id": "SP:d090dd699481505a966c9d58bcc647489c6ffba8",
    "references": [
        {
            "authors": [
                "https:// www"
            ],
            "title": "fda. gov/ regul atory- infor mation/ search- fda- guida nce- docum ents/ use- elect ronic- health- record- data- clini cal- inves tigat ions- guida nce- indus try (Accessed May 2021)",
            "year": 2021
        },
        {
            "authors": [
                "P.B. Jensen",
                "L.J. Jensen",
                "S. Brunak"
            ],
            "title": "Mining electronic health records: Towards better research applications and clinical care",
            "venue": "Nat. Rev. Genet",
            "year": 2012
        },
        {
            "authors": [
                "A. Atreja",
                "J.P. Achkar",
                "A.K. Jain",
                "C.M. Harris",
                "B.A. Lashner"
            ],
            "title": "Using technology to promote gastrointestinal outcomes research: A case for electronic health records",
            "venue": "Am. J. Gastroenterol",
            "year": 2008
        },
        {
            "authors": [
                "J.W. Smoller"
            ],
            "title": "The use of electronic health records for psychiatric phenotyping and genomics",
            "venue": "Am. J. Med. Genet. B Neuropsychiatr. Genet",
            "year": 2018
        },
        {
            "authors": [
                "W. Du",
                "Y.S. Han",
                "S. Chen"
            ],
            "title": "Privacy-preserving multivariate statistical analysis: Linear regression and classification",
            "venue": "In Proceedings of the 2004 SIAM International Conference on Data Mining",
            "year": 2004
        },
        {
            "authors": [
                "G. Hripcsak"
            ],
            "title": "Observational health data sciences and informatics (OHDSI): Opportunities for observational researchers",
            "venue": "Stud. Health Technol. Inform. 216,",
            "year": 2015
        },
        {
            "authors": [
                "J.M. Overhage",
                "P.B. Ryan",
                "C.G. Reich",
                "A.G. Hartzema",
                "P.E. Stang"
            ],
            "title": "Validation of a common data model for active safety surveillance research",
            "venue": "J. Am. Med. Inform. Assoc",
            "year": 2011
        },
        {
            "authors": [
                "J Liu"
            ],
            "title": "From distributed machine learning to federated learning: A survey",
            "venue": "Knowl. Inf. Syst",
            "year": 2022
        },
        {
            "authors": [
                "Y Chen"
            ],
            "title": "Regression cubes with lossless compression and aggregation",
            "venue": "IEEE Trans. Knowl. Data Eng",
            "year": 2006
        },
        {
            "authors": [
                "C Luo"
            ],
            "title": "DLMM as a lossless one-shot algorithm for collaborative multi-site distributed linear mixed models",
            "venue": "Nat. Commun",
            "year": 2022
        },
        {
            "authors": [
                "Y. Wu",
                "X. Jiang",
                "J. Kim",
                "L. Ohno-Machado"
            ],
            "title": "Grid Binary LOgistic REgression (GLORE): Building shared models without sharing data",
            "venue": "J. Am. Med. Inform. Assoc",
            "year": 2012
        },
        {
            "authors": [
                "Lu",
                "C. L"
            ],
            "title": "WebDISCO: A web service for distributed cox model learning without patient-level data sharing",
            "venue": "J. Am. Med. Inform. Assoc",
            "year": 2015
        },
        {
            "authors": [
                "Y. Zhang",
                "J.C. Duchi",
                "M.J. Wainwright"
            ],
            "title": "Communication-efficient algorithms for statistical optimization",
            "venue": "J. Mach. Learn. Res",
            "year": 2013
        },
        {
            "authors": [
                "J.D. Lee",
                "Q. Liu",
                "Y. Sun",
                "J.E. Taylor"
            ],
            "title": "Communication-efficient sparse regression",
            "venue": "J. Mach. Learn. Res",
            "year": 2017
        },
        {
            "authors": [
                "H. Battey",
                "J. Fan",
                "H. Liu",
                "J. Lu",
                "Z. Zhu"
            ],
            "title": "Distributed testing and estimation under sparse high dimensional models",
            "venue": "Ann. Stat. 46,",
            "year": 2018
        },
        {
            "authors": [
                "E. Dobriban",
                "Y. Sheng"
            ],
            "title": "Distributed linear regression by averaging",
            "venue": "Ann. Stat",
            "year": 2021
        },
        {
            "authors": [
                "E. Dobriban",
                "Y. Sheng"
            ],
            "title": "WONDER: Weighted one-shot distributed ridge regression in high dimensions",
            "venue": "J. Mach. Learn. Res",
            "year": 2020
        },
        {
            "authors": [
                "R Duan"
            ],
            "title": "Learning from local to global: An efficient distributed algorithm for modeling time-to-event data",
            "venue": "J. Am. Med. Inform. Assoc",
            "year": 2020
        },
        {
            "authors": [
                "M.I. Jordan",
                "J.D. Lee",
                "Y. Yang"
            ],
            "title": "Communication-efficient distributed statistical inference",
            "venue": "J. Am. Stat. Assoc. 114,",
            "year": 2018
        },
        {
            "authors": [
                "J. Wang",
                "M. Kolar",
                "N. Srebro",
                "T. Zhang"
            ],
            "title": "Efficient distributed learning with sparsity",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "R. Duan",
                "M.R. Boland",
                "J.H. Moore",
                "Y. Chen"
            ],
            "title": "ODAL: A one-shot distributed algorithm to perform logistic regressions on electronic health records data from multiple clinical sites",
            "venue": "BIOCOMPUTING",
            "year": 2019
        },
        {
            "authors": [
                "R Duan"
            ],
            "title": "Learning from electronic health records across multiple sites: A communication-efficient and privacy-preserving distributed algorithm",
            "venue": "J. . Am. Med. Inform. Assoc",
            "year": 2020
        },
        {
            "authors": [
                "Edmondson",
                "M. J"
            ],
            "title": "An efficient and accurate distributed learning algorithm for modeling multi-site zero-inflated count outcomes",
            "venue": "Sci. Rep. 11,",
            "year": 2021
        },
        {
            "authors": [
                "R. Tibshirani"
            ],
            "title": "Regression shrinkage and selection via the lasso",
            "venue": "J. R. Stat. Soc. Ser. B Stat. Methodol",
            "year": 1996
        },
        {
            "authors": [
                "A.E. Hoerl",
                "R.W. Kennard"
            ],
            "title": "Ridge regression: Biased estimation for nonorthogonal problems",
            "venue": "Technometrics 12,",
            "year": 1970
        },
        {
            "authors": [
                "H. Zou",
                "T. Hastie"
            ],
            "title": "Regularization and variable selection via the elastic net",
            "venue": "J. R. S. Soc. Ser. B Stat. Methodol",
            "year": 2005
        },
        {
            "authors": [
                "J. Fan",
                "Y. Guo",
                "K. Wang"
            ],
            "title": "Communication-efficient accurate statistical estimation",
            "venue": "J. Am. Stat. Assoc",
            "year": 2021
        },
        {
            "authors": [
                "J Tong"
            ],
            "title": "Robust-ODAL: Learning from heterogeneous health systems without sharing patient-level data",
            "venue": "Pac. Symp. Biocomput",
            "year": 2020
        },
        {
            "authors": [
                "R. Duan",
                "Y. Ning",
                "Y. Chen"
            ],
            "title": "Heterogeneity-aware and communication efficient distributed statistical inference",
            "venue": "Biometrika 109,",
            "year": 2022
        },
        {
            "authors": [
                "E Shenkman"
            ],
            "title": "OneFlorida Clinical Research Consortium: Linking a clinical and translational science institute with a community-based distributive medical education model",
            "venue": "Acad. Med",
            "year": 2018
        },
        {
            "authors": [
                "Fleurence",
                "R. L"
            ],
            "title": "Launching PCORnet, a national patient-centered clinical research network",
            "venue": "J. Am. Med. Inform. Assoc",
            "year": 2014
        },
        {
            "authors": [
                "S. Okie"
            ],
            "title": "A flood of opioids, a rising tide of deaths",
            "venue": "N. Engl. J. Med",
            "year": 2010
        },
        {
            "authors": [
                "Paulozzi",
                "L. J"
            ],
            "title": "Vital signs: Overdoses of prescription opioid pain relievers-United States, 1999\u20132008",
            "venue": "Morb. Mortal. Wkly. Rep. 60,",
            "year": 2011
        },
        {
            "authors": [
                "Vowles",
                "K. E"
            ],
            "title": "Rates of opioid misuse, abuse, and addiction in chronic pain: A systematic review and data synthesis",
            "venue": "Pain 156,",
            "year": 2015
        },
        {
            "authors": [
                "Saha",
                "T. D"
            ],
            "title": "Nonmedical prescription opioid use and DSM-5 nonmedical prescription opioid use disorder in the United States",
            "venue": "J. Clin. Psychiatry",
            "year": 2016
        },
        {
            "authors": [
                "Soares",
                "W.E. 3rd"
            ],
            "title": "Emergency department visits for nonfatal opioid overdose during the COVID-19 pandemic across six US health care systems",
            "venue": "Ann. Emerg. Med",
            "year": 2022
        },
        {
            "authors": [
                "Q Li"
            ],
            "title": "Assessing the validity of a priori patient-trial generalizability score using real-world data from a large clinical data research network: A colorectal cancer clinical trial case study",
            "venue": "AMIA Annu. Symp. Proc. 2019,",
            "year": 2019
        },
        {
            "authors": [
                "McDonough",
                "C. W"
            ],
            "title": "Optimizing identification of resistant hypertension: Computable phenotype development and validation",
            "venue": "Pharmacoepidemiol. Drug Saf",
            "year": 2020
        },
        {
            "authors": [
                "J Tong"
            ],
            "title": "Identifying clinical risk factors for opioid use disorder using a distributed algorithm to combine real-world data from a large clinical data research network",
            "venue": "AMIA Annu Symp Proc",
            "year": 2020
        },
        {
            "authors": [
                "J. Friedman",
                "T. Hastie",
                "R. Tibshirani"
            ],
            "title": "Regularization paths for generalized linear models via coordinate descent",
            "venue": "J. Stat. Softw. 33,",
            "year": 2010
        },
        {
            "authors": [
                "T. Cai",
                "M. Liu",
                "Y. Xia"
            ],
            "title": "Individual data protected integrative regression analysis of high-dimensional heterogeneous data",
            "venue": "J. Am. Stat. Assoc",
            "year": 2021
        },
        {
            "authors": [
                "C Luo"
            ],
            "title": "dPQL: A lossless distributed algorithm for generalized linear mixed model with application to privacy-preserving hospital profiling",
            "venue": "J Am Med Inform Assoc. https:// doi. org/",
            "year": 2022
        },
        {
            "authors": [
                "J Tong"
            ],
            "title": "Distributed learning for heterogeneous clinical data with application to integrating COVID-19 data across 230 sites",
            "venue": "NPJ Digit. Med",
            "year": 2022
        },
        {
            "authors": [
                "C. Luo",
                "R. Duan",
                "A.C. Naj",
                "H.R. Kranzler",
                "J. Bian",
                "Y. Chen"
            ],
            "title": "ODACH: A one-shot distributed algorithm for Cox model with heterogeneous multi-center data",
            "venue": "Sci. Rep",
            "year": 2022
        },
        {
            "authors": [
                "Edmondson",
                "M. J"
            ],
            "title": "Distributed Quasi-Poisson regression algorithm for modeling multi-site count outcomes in distributed data networks",
            "venue": "J Biomed Inform",
            "year": 2022
        },
        {
            "authors": [
                "L. Sweeney"
            ],
            "title": "k-anonymity: A model for protecting privacy",
            "venue": "Int. J. Uncertain. Fuzziness Knowl. Based Syst. 10,",
            "year": 2002
        },
        {
            "authors": [
                "C. Dwork",
                "F. McSherry",
                "K. Nissim",
                "A. Smith"
            ],
            "title": "Calibrating noise to sensitivity in private data analysis",
            "venue": "J. Priv. Confid",
            "year": 2017
        },
        {
            "authors": [
                "L. Wasserman",
                "S. Zhou"
            ],
            "title": "A statistical framework for differential privacy",
            "venue": "J. Am. Stat. Assoc. 105,",
            "year": 2010
        },
        {
            "authors": [
                "D Froelicher"
            ],
            "title": "Truly privacy-preserving federated analytics for precision medicine with multiparty homomorphic encryption",
            "venue": "Nat. Commun",
            "year": 2021
        },
        {
            "authors": [
                "S. Van Buuren",
                "K. Groothuis-Oudshoorn"
            ],
            "title": "mice: Multivariate imputation by chained equations in R",
            "venue": "J. Stat. Softw. 45,",
            "year": 2011
        }
    ],
    "sections": [
        {
            "text": "1 Vol.:(0123456789) Scientific Reports | (2022) 12:11073 | https://doi.org/10.1038/s41598-022-14029-9\nwww.nature.com/scientificreports"
        },
        {
            "heading": "Multisite learning",
            "text": "of high\u2011dimensional heterogeneous data with applications to opioid use disorder study of 15,000 patients across 5 clinical sites"
        },
        {
            "heading": "Xiaokang Liu1, Rui Duan2, Chongliang Luo1,3, Alexis Ogdie4, Jason H. Moore5,",
            "text": "Henry R. Kranzler6, Jiang Bian7 & Yong Chen1*\nIntegrating data across institutions can improve learning efficiency. To integrate data efficiently while protecting privacy, we propose A one\u2011shot, summary\u2011statistics\u2011based, Distributed Algorithm for fitting Penalized (ADAP) regression models across multiple datasets. ADAP utilizes patient\u2011level data from a lead site and incorporates the first\u2011order (ADAP1) and second\u2011order gradients (ADAP2) of the objective function from collaborating sites to construct a surrogate objective function at the lead site, where model fitting is then completed with proper regularizations applied. We evaluate the performance of the proposed method using both simulation and a real\u2011world application to study risk factors for opioid use disorder (OUD) using 15,000 patient data from the OneFlorida Clinical Research Consortium. Our results show that ADAP performs nearly the same as the pooled estimator but achieves higher estimation accuracy and better variable selection than the local and average estimators. Moreover, ADAP2 successfully handles heterogeneity in covariate distributions.\nElectronic health records (EHR), which routinely incorporate information from health care providers and medical devices1, contain information about patients\u2019 diagnoses, laboratory test results and medication use and are important resources for biomedical and clinical research2\u20134. With the wide adoption of EHR systems throughout the United States and other countries, there is a growing need to integrate data horizontally from different institutions, i.e., combining data with the same set of features but different patient populations5. Such integration can greatly enrich the study population, increase statistical power, reduce the potential for regional bias, and provide opportunities to study rare medical conditions.\nHowever, data integration across institutions has many practical challenges. First, collaborating institutions must adopt data harmonization procedures to facilitate a commonly applicable data analysis approach. There have been many efforts in large research networks to develop common data models (CDM) that create a unified data structure and variable definitions for all the collaborating institutions. For example, the Observational Medical Outcomes Partnership (OMOP) CDM6,7 was developed by the Observational Health Data Sciences and Informatics (OHDSI) network to standardize patient records in a consistent format.\nDue to privacy protections, it is often not feasible to share individual-level patient data across multiple sites. Distributed algorithms (also known as federated learning8 algorithms) that coordinately execute a computing task at each site can bypass the need of sharing individual-level data to achieve data integration by sharing only summary-level statistics. Some recent, exciting developments in the related areas of statistics and machine\nOPEN\n1Department of Biostatistics, Epidemiology and Informatics, University of Pennsylvania Perelman School of Medicine, 423 Guardian Drive, Philadelphia, PA 19104, USA. 2Department of Biostatistics, Harvard T.H. Chan School of Public Health, Harvard University, Boston, MA, USA. 3Division of Public Health Sciences, Washington University School of Medicine in St. Louis, St. Louis, MO, USA. 4Department of Medicine, Department of Biostatistics, Epidemiology and Informatics, University of Pennsylvania Perelman School of Medicine, Philadelphia, PA, USA. 5Department of Computational Biomedicine, Cedars-Sinai Medical Center, Los Angeles, CA 90096, USA. 6Department of Psychiatry, University of Pennsylvania Perelman School of Medicine and the VISN 4 MIRECC, Crescenz VAMC, Philadelphia, PA, USA. 7Department of Health Outcomes and Biomedical Informatics, University of Florida Health Cancer Center, Gainesville, FL, USA. *email: ychen123@upenn.edu\n2 Vol:.(1234567890) Scientific Reports | (2022) 12:11073 | https://doi.org/10.1038/s41598-022-14029-9\nlearning provide the potential for models that incorporate data across multiple datasets in a distributed manner. For example, Chen et\u00a0al.9 developed a distributed algorithm for general linear regression and Luo et\u00a0al.10 proposed a distributed linear mixed model. Some of the distributed algorithms require iterative communication across datasets until they converge, e.g., GLORE11 for logistic regression and WebDISCO12 for a Cox proportional hazards model. However, multiple rounds of communication can impose a high communication cost, which is measured by the total number of digits transferred and the time and labor cost spent in communications, to complete the task and requires cooperation among institutions, which may not be feasible for some applications. Thus, one-shot methods that require only a single round of communication are preferred. Among a variety of recently developed one-shot algorithms, a commonly used approach is to average all of the local estimates (e.g., Zhang et\u00a0al.13, Lee et\u00a0al.14, Battey et\u00a0al.15, Dobriban and Sheng16,17). The potential limitation of the averaging-type of methods is their lack of accuracy when studying rare conditions. For example, in18 the authors demonstrated that when the event rate is low the averaging-type estimator can lead to non-negligible bias and large variance. Another approach constructs a surrogate of the global likelihood function using the individual-level data in a lead site and summary-level statistics from the collaborating sites (e.g., Jordan et\u00a0al.19, Wang et\u00a0al.20, Duan et\u00a0al.21, and Duan et\u00a0al.22). In addition, efficient one-shot distributed algorithms have been proposed to deal with zeroinflated count data23 and time-to-event data18.\nMost of the aforementioned work involves low-dimensional regression models where the number of predictors is smaller than the sample size. A unique challenge in the era of big data is high dimensionality, where the number of parameters can be extremely large and much bigger than the sample size. For example, in genomic studies thousands of genetic variants are observed for each subject, and methods are needed to select the truly influential variables. Penalized regression is one of the most commonly used techniques for variable selection. It maximizes the goodness of fit of the model while controlling the model complexity by restricting the regression coefficients. For example, lasso regression24 allows simultaneous model estimation and variable selection by adding an l1 penalty of the regression coefficients. Ridge regression25 exploits an l2 penalty to handle the high collinearity among covariates and elastic-net26 flexibly combines the l1 and l2 penalties so that strongly correlated covariates are included in or excluded from the model together. Some distributed algorithms are proposed for penalized regressions (e.g., Lee et\u00a0al.14, Battey et\u00a0al.15, Dobriban and Sheng17, and Fan et\u00a0al.27). However, these methods either assume a homogeneous application scenario that can introduce bias into the estimation in the presence of heterogeneity across sites28,29 or cannot accommodate rare outcomes, underscoring the need for a framework that deals with the two issues simultaneously.\nIn this article, we propose a distributed algorithm for penalized regression. Different from the existing methods19,20,27, our method accounts for heterogeneity in covariate distributions across multiple sites by incorporating the second-order gradient information when creating the surrogate objective function. To be compatible with the structure of the surrogate function, a modified cross-validation strategy is used to tune the level of regularization. We evaluate the performance of the proposed method using both simulation and a real-world application to study risk factors for opioid use disorder (OUD) with data from five participating sites of the OneFlorida Clinical Research Consortium30, a clinical data research network contributing to the national Patient-Centered Clinical Research Network (PCORnet)31. We chose to focus on OUD because of the substantial implications the disorder has for public health. Between 1990 and 2010, U.S. opioid analgesic prescriptions increased by a factor of 1032, contributing to an epidemic of opioid misuse, abuse, and overdose deaths32\u201334. By 2018, 3.7% of U.S. adults reported past-year misuse of a prescription opioid pain reliever35. With the increase in misuse of opioids, the prevalence of prescription OUD among U.S. adults reached 2.1 million (or 0.9%)36. During the COVID-19 pandemic, despite decreases in emergency department visits for other medical emergencies, during 2020 the rates of opioid overdose-related visits in six healthcare systems increased37. These findings are consistent with a widespread increase in opioid-related complications during the pandemic.\nThe OneFlorida data repository integrates multiple data sources from its participating healthcare organizations and provides real-world data to support biomedical and clinical research38\u201340. For this study, we extracted EHRs (covering patient records from 01/01/2012 up to 07/31/2020) from the 5 participating sites for 15,000 patients who had chronic pain and an opioid prescription (including buprenorphine, codeine, fentanyl, hydromorphone, meperidine, methadone, morphine, oxycodone, tramadol, and hydrocodone) and no cancer or OUD diagnosis before their first opioid prescription. Among these patients who were exposed to an opioid, we define a case of OUD as having a first diagnosis of OUD after their first prescription and define a control as having no diagnosis of OUD during the entire time window. A list of risk factors was compiled from the literature and extracted from the database, including basic demographic features such as age, gender and race, and co-occurring diagnoses, e.g., depression and sleep disorder (see Supplementary Table\u00a02 for all 42 covariates). A logistic lasso regression is then applied to locate truly influential risk factors. These numerical study results show that, by adding the second-order gradient information, estimation, prediction, and variable selection of ADAP2 are improved compared to some alternative methods and ADAP2 is robust to the heterogeneity in covariate distributions."
        },
        {
            "heading": "Results",
            "text": "Overview of the ADAP method. The ADAP method aims to efficiently learn a global parsimonious association relationship between an outcome of interest and a large amount of risk factors through integrate data across multiple sites while protecting privacy. Briefly, we model the association using a regression model with coefficient \u03b2 and a loss function (objective function) is used to measure the goodness of fit of the model. ADAP utilizes patient-level data from a lead site and incorporates the first-order (ADAP1) and second-order gradients (ADAP2) of the objective function from collaborating sites to construct a surrogate objective function19,20 at the lead site. Then, after applying proper regularizations to the surrogate objective function to select truly influen-\n3 Vol.:(0123456789) Scientific Reports | (2022) 12:11073 | https://doi.org/10.1038/s41598-022-14029-9\ntial risk factors, we obtain the estimates of the association coefficients \u03b2 by optimizing the penalized surrogate objective function.\nTo demonstrate the property of ADAP, we compared ADAP1 \u03b2\u0302(1) and ADAP2 \u03b2\u0302(2) to several benchmark methods, including the local estimator \u03b2\u03021 obtained from a single dataset (i.e., the lead site dataset), the pooled estimator \u03b2\u0302N obtained from the combined patient-level data across all sites, and the average estimator \u03b2\u0302ave obtained by averaging all local estimators. The pooled estimator is considered as a gold standard as it directly uses all the patient-data without constrains on data-sharing, but it is not available in practice. The details of ADAP, as well as the benchmark methods can be found in \u201cMethods\u201d. For illustration, in the following we mainly consider the lasso logistic regression.\nEvaluation of estimation accuracy and variable selection through simulation studies. We evaluate the performance of ADAP under various settings to see the effects of number of sites K (Setting 1), heterogeneous covariate distribution (Setting 2), lead site\u2019s sample size n1 (Setting 3), and a shared sample size n by each site (Setting 4) on the performance of the method. Except for the estimation performance, which is measured by the Euclidean distance of the estimate to its true value, we also tested the variable selection ability in Setting 5 with multiple levels of association magnitude by calculating the true positive rate and false positive rate. In all five settings, we let site 1 be the lead site.\nFigure\u00a01 displays the results of Setting 1 and Setting 2. As expected, ADAP2 leverages more information from each site\u2019s loss function than ADAP1 and therefore outperforms ADAP1 in terms of estimation error, with an estimation error closest to the pooled estimator among all of the other methods. In particular, we can see in panel (b) of Fig.\u00a01 that heterogeneity in covariates greatly inflates the estimation error of the local estimator and ADAP1 [compared to the homogeneous case displayed in panel (a)], while ADAP2 still maintains a high estimation accuracy, which demonstrates the ability of ADAP2 to handle heterogeneity. In addition, when we increase the number of sites K, a larger total sample size N(=\n\u2211K k=1 nk) provides both the pooled estimator and\nthe ADAP estimators with more information and improves the estimation accuracy. The average estimator is not as good as the ADAP estimators. Panel (a) of Fig.\u00a02 displays the results of Setting 3, from which we observe that the ADAP methods perform much better than the average estimator and the local estimator which only uses the information from a single site. In terms of the estimation error, ADAP2 is better than ADAP1, especially when the local sample size is small (i.e., n1N < 0.3 ). When n1 N gets larger, the ADAP methods perform more similar to the pooled estimator. Thus, in applications, it is preferable to select a lead site with a larger sample size. The results of Setting 4 are displayed in panel (b) of Fig.\u00a02, where N increases along with an increasing n and a fixed K, and we can see that ADAP methods show a larger improvement over the average and local estimators when n is small, and when n gets larger the difference between methods becomes smaller. This suggests that using ADAP is the most efficient in a small sample setting.\nTables\u00a01 and 2 display the results of variable selection in Setting 5, and ADAP2 performs better in identifying true positives, but at the expense of having more false positives. The false positive rate of ADAP2 is similar to the pooled estimator and being much lower than the average estimator. Thus, applying ADAP2 ensures a higher probability to recover more true positives than ADAP1 and has a moderate level of false positive rate. The local estimator has a low false positive rate, but its true positive rate is also the lowest among all methods. The average estimator does not perform as well as the ADAP methods since the averaging operation destroys the sparsity of each site\u2019s local estimate.\nFigure\u00a01. Simulation results under Setting 1 and Setting 2. Both plots display the change of estimation error averaged over 200 replications along with an increasing number of sites K. The panel (a) is for Setting 1, where covariates are generated from one shared multivariate normal distribution and the panel (b) is for Setting 2 where the heterogeneous covariates are considered.\n4 Vol:.(1234567890) Scientific Reports | (2022) 12:11073 | https://doi.org/10.1038/s41598-022-14029-9\nTo account for the uncertainties in the comparison, we have also conducted the one-sided paired t-test for each pair of methods to check the significance of the above-claimed benefits. With a significance level of 0.05, the ADAP methods outperform the average and the local estimators in terms of estimation and variable selection for most of the considered settings. The empirical measurements of the improvement and the corresponding test results are displayed in Supplementary Tables\u00a04\u20139.\nResults from the analysis of the OUD dataset. The logistic lasso regression is applied to the OUD data and fitted by all the above-described methods, and we use site 4 as the lead site without a loss of generality. As the OUD data come from a real distributed research network, they exhibit heterogeneity across sites and provide an ideal environment to compare methods. Therefore, in addition to comparing the estimation and variable selection performance, we also conduct a random-splitting procedure to measure the prediction performance with a set of increasing training set sizes and use AUC as a prediction performance metric for each method.\nIt takes around 6\u00a0s to fit ADAP1 and takes around 45\u00a0s to fit ADAP2 on an iMac with 3.8\u00a0GHz 8-Core Intel Core i7 processor, and the estimation results are reported in Supplementary Table\u00a03. As the true coefficient vector is unavailable, we use the pooled estimator \u03b2\u0302N as a gold standard and measure the approximation error of other\nFigure\u00a02. Simulation results under Setting 3 and Setting 4. The panel (a) is for Setting 3, which shows the change of estimation error averaged over 200 replications along with an increasing local sample size n1. The total sample size N is fixed at 10,000. The panel (b) is for Setting 4 where all sites share the same sample size n and the number of sites K is fixed at 10. The plot shows the change of estimation error averaged over 200 replications along with an increasing n (i.e., an increasing N).\n5 Vol.:(0123456789) Scientific Reports | (2022) 12:11073 | https://doi.org/10.1038/s41598-022-14029-9\nestimators to the gold standard by computing the relative estimation bias with respect to the pooled estimator for each coefficient that has a nonzero pooled estimate (see Fig.\u00a03). Among the methods under comparison, ADAP2 has relative bias < 20% for 73% of the covariates, and it provides the best approximation to the pooled estimator ( \ufffd\u03b2\u0302(2) \u2212 \u03b2\u0302N\ufffd22 = 0.24 ). The local estimator has the largest deviance to the pooled estimator ( \ufffd\u03b2\u03021 \u2212 \u03b2\u0302N\ufffd 2 2 = 2.72 ), and only 25% of the covariates have relative bias < 20%. The average estimator does not perform as well as the ADAP methods ( \ufffd\u03b2\u0302ave \u2212 \u03b2\u0302N\ufffd22 = 0.88 ) and has 40% of the covariates having relative bias < 20%. ADAP1 has 60% of the covariates whose relative bias < 20% and \ufffd\u03b2\u0302(1) \u2212 \u03b2\u0302N\ufffd22 = 0.64. Thus, among the four methods in this analysis, ADAP2 is the most consistent with the pooled estimator. ADAP1\u2019s performance is better than the local and the average estimators, which can be explained by the relatively homogeneous covariate distributions among the five sites (see Supplementary Table\u00a02).\nAs for variable selection, by treating the pooled estimator as a gold standard, the local estimator and average estimator each have three estimates whose signs are opposite to the corresponding pooled estimates. The local estimator has one false positive while the average estimator and ADAP2 estimator have three and two false\nFigure\u00a03. Coefficient estimation results for the OUD analysis. The panel (a) displays the coefficient estimates (log odds ratio) for all variables (sorted by the pooled estimates in decreasing order) with the pooled estimator as a gold standard, and the panel (b) shows the estimation bias relative to the pooled estimator for all nonzero pooled estimates (without sorting). As the relative estimation bias of the local estimator contains extremely large values, for ease of display we exclude it from panel (b). From the two plots, we see that ADAP2 yields estimates that are most consistent with the pooled estimator while the local estimator shows large deviations from the pooled estimator.\n6 Vol:.(1234567890) Scientific Reports | (2022) 12:11073 | https://doi.org/10.1038/s41598-022-14029-9\npositives, respectively. The local estimator missed nine predictive covariates and the ADAP1 estimator missed three predictive covariates. In view of these considerations, ADAP2 performs better than the other methods.\nWe present the prediction results in Fig.\u00a04. In general, as the training set becomes larger, the AUC increases for all methods. Among the five methods, the pooled estimator has the best prediction performance and ADAP2 performs very close to the pooled estimator. ADAP1 and the average estimator have lower AUCs than ADAP2, and the local estimator has the worst prediction performance. We have constructed the empirical 95% confidence intervals defined by the 2.5th percentile and the 97.5th percentile of the pairwise AUC difference between ADAP2 and the local, the average, and ADAP1 estimator to check the significance of the prediction performance improvement for ADAP2. Once the confidence interval is on the positive side of zero, there is a significant improvement. The results are in Supplementary Table\u00a010 and show a significant improvement for most cases, and for the remaining cases, ADAP2 performs as well as other methods. Thus, by collecting the second-order gradient information from all sites to form the surrogate function, ADAP2 has a comparable or higher prediction accuracy than other methods and performs as well as the pooled estimator.\nBased on the estimated effects, for the patients with chronic pain and an opioid prescription, some co-occurring psychiatric and substance use disorder diagnoses (e.g., anxiety and cocaine-related disorder) contribute to the identification of OUD-positive cases. Interestingly, patients with sleep disorders are at decreased risk of OUD. Compared to patients with a normal BMI, patients who are overweight or obese are less likely to be OUD positive. Smoking is a risk factor for OUD, and non-Hispanic white (NHW) patients have a greater risk to develop OUD. People receiving Medicaid also have a higher risk of being OUD positive. Age and sex also have effects on the risk of OUD, with adults younger than 64 and men at greater risk of the disorder."
        },
        {
            "heading": "Discussion",
            "text": "In this study, we introduce a one-shot privacy-preserving algorithm to fit penalized regression in a distributed manner. A properly selected penalty on the coefficient vector is added to the surrogate loss function to balance the trade-off between goodness-of-fit and model complexity. The first-order and second-order gradients are collected from collaborating sites and contribute to construction of the surrogate loss function. The simulation study and the application to the analysis of OUD data both demonstrate the superiority of the ADAP methods over the local and average estimators. Moreover, the improvement of ADAP2 over ADAP1 is achieved at the expense of transferring more digits to the lead site, i.e., transferring the second-order gradient information that contains O(Kp2) numbers. With a moderate dimension p , transferring O(Kp2 + Kp) numbers will not result in severe technical problems in practical applications. However, this becomes a limitation of ADAP2 when the dimension p goes large and new techniques are then needed to reduce the communication burden. Otherwise, using ADAP1 is preferable in this situation.\nFigure\u00a04. Prediction performance measured by averaging AUCs obtained through 200 random-splitting procedures on the OUD data. The training size index t takes values from 1 to 9, and each t means that we randomly select t \u00d7 100 cases and t \u00d7 200 controls from each site to form a training set, with the remaining data used to test the fitted model. Thus, the plot shows the pattern of variation in AUC as a function of the size of the training set. In general, as the training set size becomes larger, the AUC is increased for all methods. Among the five methods, the pooled estimator performs the best, with the performance of ADAP2 very close to that of the pooled estimator. ADAP1 and the average estimator have lower AUCs than ADAP2, and the local estimator has the worst prediction performance.\n7 Vol.:(0123456789) Scientific Reports | (2022) 12:11073 | https://doi.org/10.1038/s41598-022-14029-9\nIterative algorithms are promising to further improve the estimation accuracy of ADAP methods19. However, the number of digits transferred in the iterative communications could be significantly larger than one-shot algorithms, potentially increasing the risk of re-identification of subjects. Moreover, in practice, the higher communication cost, specifically more rounds of communications involving several different requests made by researchers to each individual site participating in a study, also leads to higher operational efforts across participating sites, which requires timely cooperation and coordination among the researchers and could potentially delay the completion of the learning projects. In contrast, one-shot methods are more appealing as they do not require iterative communication. Specifically, in ADAP, after the initial estimator being broadcasted from the lead site to the collaborating sites, the collaborating sites only need to take one round of action, i.e., receive the broadcasted estimates, calculate and transfer the gradient information to the lead site, which saves both time and labor cost.\nFollowing Friedman et\u00a0al.41, we adopted the coordinate descent algorithm within a nested loop together with 5-fold cross-validation to fit a solution path and select the tuning parameter. We also experimented with using the lead site\u2019s tuning parameter to fit the surrogate model, but the resultant estimator approximated the pooled estimator poorly. The l1 penalty introduces bias into the estimates when it filters off random noise, i.e., it pushes all the coefficients toward zero, leaving only those with strong signals, but shrunken in magnitude. When the sample is large enough and the signal is strong, a small tuning parameter is preferred; otherwise, a larger tuning parameter is needed to recover the true signals from the random noise. Therefore, with a large difference in the amount of available information, the magnitude of regularization needed by a local model is too large for the global model. Another concern in tuning is the transferability of the algorithm, as the tuning is done at the lead site. By changing to a different lead site, the final model could be different in the selected covariates and its estimates, and this is also a limitation of ADAP and requires further investigation. Some of distributed algorithms that employ the same surrogate loss function strategy select each site as a lead site and then average all of the resultant estimators as a final estimator. However, this is not applicable in our case, as taking an average can degrade the variable selection. In practice, to avoid this effect and achieve a better approximation to the pooled model, we recommend using a lead site with a large sample.\nA practical concern in distributed learning is the potential heterogeneity across multiple sites, including the heterogeneity underlying the association pattern (as each site has model parameters that are not exactly the same) and the covariate distribution. There are existing efforts devoted to solving the heterogeneity problem in distributed learning. For example, Tong et\u00a0al.28 used the robustness of the median compared to the mean to relieve the effects of \u2018outlying studies\u2019, Duan et\u00a0al.29 proposed a density ratio tilting method to accommodate the heterogeneous nuisance parameters across sites, and Cai et\u00a0al.42 employed effect decomposition to allow site-specific effects of covariates on the outcome. Another limitation of ADAP is that ADAP2 only handles the heterogeneity in covariate distributions by incorporating the second-order gradients of all sites when creating the surrogate objective function. Therefore, further development of ADAP methods to deal with heterogeneity in the model parameters is needed.\nSome important work remains for future investigation. First, as the number of high-dimensional association studies increases (e.g., in genome-wide association studies that connect a phenotype with millions of genotypes to identify risk variants), methods are needed to reduce the communication cost of sharing second-order gradients. Some numerical methods that approximate the second-order gradient provide a possible solution that warrants further exploration. Another pressing issue that requires resolution is the heterogeneity in model parameters. Whereas a mixed-effects model could be used in this situation10,43, we plan to explore this and other modeling approaches44,45 in the high-dimensional scenario\u00a0to develop distributed algorithms that integrate information across multiple datasets while accounting for heterogeneity.\u00a0Generalization of our work\u00a0to other types of outcomes\u00a0such as count data46 is of interest. Third, we regard ADAP as a privacy-preserving approach since it is a one-shot distributed algorithm where only summary statistics are needed to communicate across sites participating in a collaborative study. However, we have not rigorously checked if ADAP meets some privacy-preserving criteria such as k-anonymity47 or differential privacy48,49. In the future, we will measure the privacy leaking risk of applying ADAP and enhance it by using some techniques such as differential privacy and multiparty homomorphic encryption50. Finally, to evaluate the portability of predictive models constructed via the ADAP algorithm using data from OneFlorida data, we plan to evaluate them externally using the data from the STAR (Stakeholders, Technology, and Research) Network, which contains centralized data from 8 healthcare organizations.\nTo conclude, in this study we considered a communication-efficient distributed learning framework for penalized regression. There are multiple penalty functions and regression models that can be embedded into this framework to satisfy different analysis demands. Simulation studies and an application to a clinical research network studying OUD demonstrated the validity and feasibility of the ADAP methods. The novelty of the proposed method mainly manifests in the following aspects. First, by exploiting the surrogate likelihood idea, ADAP provides a flexible framework to apply penalized regression distributedly. The algorithm protects the patients\u2019 privacy and only requires one round of communication from the collaborating site. Second, by constructing a surrogate of the global likelihood function, ADAP outperforms the average-type estimators in that it accommodates rare outcomes and small sample sizes, and it also avoids severe over-selection brought by the averaging operation. Third, compared with some methods that are not robust to the violation of the homogeneity assumption, incorporation of the second-order gradient information in ADAP2 successfully accommodates heterogeneity in covariate distributions while boosting the true positive rate and improving estimation and prediction accuracy. We conclude that the ADAP2 estimator is a good approximation of the pooled estimator that is robust to heterogeneity across sites. The application of these findings to OUD underscores the potential contribution of this approach for addressing important public health problems.\n8 Vol:.(1234567890) Scientific Reports | (2022) 12:11073 | https://doi.org/10.1038/s41598-022-14029-9"
        },
        {
            "heading": "Methods",
            "text": "Data and problem formulation. We consider the relationship between an outcome y \u2208 R and a set of covariates x = (1, x1, . . . , xp\u22121)T \u2208 Rp . Suppose in a multisite study with K sites, we observe (xki , yki), i = 1, . . . , nk for the i-th individual in the k-th site k = 1, . . . ,K , where\n\u2211K k=1 nk = N is the total sample size. We model\nP(y|x) using a regression model with coefficient \u03b2 , and a loss function in the k-th site is defined as\nwhere f (\u03b2; xki , yki) is a prespecified loss function. For example, f (\u03b2; xki , yki) = \u2212ykixTki\u03b2 + log{1+ exp(x T ki\u03b2)} if we use a logistic regression. We first define several benchmark estimators.\nBenchmark methods. Without loss of generality, we let site 1 be the lead site and obtain the local estimator as\nwhere P (\u03b2) is a penalty function with > 0 being a tuning parameter controlling the level of penalization. Similarly, for other sites we can obtain \u03b2\u0302k = argmin\n\u03b2 Lk(\u03b2)+ P (\u03b2) based on locally stored data, and the average\nestimator can then be defined as the weighted average of each site\u2019s estimate\nBy assuming that individual-level data-sharing is allowed, we obtain the global loss function\nand get the pooled estimator from\nThe local estimator is not efficient as it does not utilize information from other sites. The pooled estimator is considered as a gold standard as it directly uses all the patient-data without constrains on data-sharing, but it is not available in practice. We then introduce ADAP estimators.\nProposed method: ADAP. The main idea of ADAP is to build a surrogate loss function19,20 to approximate L(\u03b2) as\nwhere \u2207Lk(\u03b2) = 1nk \u2211nk\ni=1 \u2207f (\u03b2; xki , yki) denotes the first-order gradient of Lk(\u03b2) at an initial estimator \u03b2 , and \u2207L(\u03b2) = 1N \u2211K k=1 nk\u2207Lk(\u03b2) . In this way, each collaborating site only needs to send its first-order gradient (a p-dimensional vector) to the lead site to construct L\u03031(\u03b2) . The ADAP1 estimator19,20,27 is obtained from\nThis approximation can be improved by further requiring the second-order gradient of Lk(\u03b2) , which gives us the second-order surrogate function22\nwhere \u22072Lk(\u03b2) = 1nk \u2211nk i=1 \u2207 2f (\u03b2; xki , yki) is the second-order gradient (a p\u00d7 p-dimensional matrix) and\n\u22072L(\u03b2) = 1N \u2211K k=1 nk\u2207 2Lk(\u03b2) . It is worth mentioning that, by collecting the second-order gradients from all sites, the resulting estimator is robust to the potential heterogeneity in covariate distributions. Therefore, at the expense of transferring additional O(Kp2) numbers, we can construct L\u03032(\u03b2) and get the ADAP2 estimator.\nTo satisfy various demands, different penalties can be applied, e.g., P (\u03b2) = \ufffd\u03b2\ufffd1 for the lasso regression and P = (\u03b1\ufffd\u03b2\ufffd1 + (1\u2212 \u03b1)\ufffd\u03b2\ufffd22) with \u03b1 \u2208 (0, 1) for the elastic-net method. For illustration, in the following we mainly consider the lasso logistic regression. As for selecting the initial estimator \u03b2 , both the local estimator \u03b2\u03021 and the average estimator \u03b2\u0302ave are good choices. The algorithm for both ADAP1 and ADAP2 is summarized below.\nLk(\u03b2) = 1\nnk\nnk\u2211\ni=1\nf (\u03b2; xki , yki),\n\u03b2\u03021 = argmin \u03b2 L1(\u03b2)+ P (\u03b2) (local),\n\u03b2\u0302ave =\nK\u2211\nk=1\nnk N \u03b2\u0302k (average).\nL(\u03b2) = 1\nN\nK\u2211\nk=1\nnkLk(\u03b2),\n\u03b2\u0302N = argmin \u03b2 L(\u03b2)+ P (\u03b2) (pooled).\nL\u03031(\u03b2) = L1(\u03b2)+ {\u2207L(\u03b2)\u2212\u2207L1(\u03b2)} T\u03b2\n\u03b2\u0302(1) = argmin \u03b2 L\u03031(\u03b2)+ P (\u03b2) (ADAP1).\nL\u03032(\u03b2) = L1(\u03b2)+ {\u2207L(\u03b2)\u2212\u2207L1(\u03b2)} T\u03b2 +\n1 2 (\u03b2 \u2212 \u03b2)T {\u22072L(\u03b2)\u2212\u22072L1(\u03b2)}(\u03b2 \u2212 \u03b2)\n\u03b2\u0302(2) = argmin \u03b2 L\u03032(\u03b2)+ P (\u03b2) (ADAP2).\n9 Vol.:(0123456789) Scientific Reports | (2022) 12:11073 | https://doi.org/10.1038/s41598-022-14029-9\nWe note that in Step 6, a solution path of ADAP is obtained by embedding the coordinate descent algorithm into a nested loop41, and we use a modified 5-fold cross-validation to select . Specifically, each time we leave out a fold of local data and use the remaining four folds together with the aggregated gradient information to form a surrogate loss function and obtain a solution path, and then we use the left-out data to compute the deviance, i.e., the negative log-likelihood function evaluated at the current estimates. The selected is the one that minimizes the averaged deviance among 5 folds.\nAlgorithm\u00a01 requires only one round of communication from the collaborating sites to the lead site to transfer the gradient information. In total O(Kp) and O(Kp+ Kp2) numbers are transferred in ADAP1 and ADAP2, respectively. For comparison among methods, the computation based on the global loss function requires sharing individual-level data (i.e., O(KNp) numbers) and is prohibited in practice due to privacy concerns; the local loss function does not need data-sharing; the average estimator requires collaborating sites to share their estimates (in total O(Kp) numbers) to the lead site in one round of communication. As shown in our simulation and real-world data application, although the communication cost is comparable to that of the average estimator (for ADAP1) or greater than the average estimator (for ADAP2), ADAP algorithms perform better than the average estimator.\nSimulation study. We evaluate the performance of ADAP under five settings, and the methods under comparison are: the local estimator \u03b2\u03021 , the average estimator \u03b2\u0302ave , the pooled estimator \u03b2\u0302N , ADAP1 estimator \u03b2\u0302(1) , and ADAP2 estimator \u03b2\u0302(2) . Without a loss of generality, site 1 is treated as the lead site. The considered settings are:\nSetting 1: We fix the local sample size n1 = 1, 000 and increase the number of sites K \u2208 (5, 10, 20, 30, 40, 50) . For other sites, we let nk = 1, 000\u00d7 10uk with uk \u223c U(\u22121, 1) . The dimension is p = 200 and the covariates are generated from a multivariate normal distribution zki \u223c Np\u22121(0, \ufffd) with \ufffd = (0.1I(i \ufffd=j)) \u2208 R(p\u22121)\u00d7(p\u22121) and xki = (1, zTki) T . The true coefficient vector \u03b2\u2217 = (\u22122.5, 0.5, . . . , 0.5 \ufe38 \ufe37\ufe37 \ufe38\n10\n, 0, . . . , 0 \ufe38 \ufe37\ufe37 \ufe38\n189\n).\nSetting 2: The covariates are generated from heterogeneous multivariate normal distributions, i.e., zki \u223c Np\u22121(\u00b5k , \ufffdk) where \u00b5k = (\u00b5k1, 0, . . . 0\n\ufe38 \ufe37\ufe37 \ufe38\np\u22122\n) \u2208 Rp\u22121 with \u00b5k1 \u223c U(\u22121, 1) , \ufffdk = (\u03c1 I(i \ufffd=j) k ) \u2208 R (p\u22121)\u00d7(p\u22121)\nwith \u03c1k \u223c U(0.1, 0.5) and xki = (1, zTki) T . We let \u00b511 = min k \u00b5k1 , and if z1 is the age, then this setting is to\n10\nVol:.(1234567890)\nScientific Reports | (2022) 12:11073 | https://doi.org/10.1038/s41598-022-14029-9\nmimic the scenario where the lead site\u2019s patients are the youngest among all the sites. All other details are the same as in setting 1. Setting 3: We fix the total sample size N = 10,000, and let K = 11 and increase the local sample size n1 \u2208 (500, 1, 000, 2, 000, 3, 000, 4, 000, 5, 000, 6, 000, 7, 000, 8, 000). The remaining N \u2212 n1 samples are evenly assigned to other sites. The dimension is p = 200, and the covariates are generated as in setting 1 except \ufffd = (0.5I(i \ufffd=j)) \u2208 R(p\u22121)\u00d7(p\u22121) . The true coefficient vector is \u03b2\u2217 = (\u22122, 1.5, 1, 1, 1, 1, 0, . . . 0\n\ufe38 \ufe37\ufe37 \ufe38\n194\n).\nSetting 4: We fix the number of sites K = 10 , and let nk = n for k = 1, 2, . . .K and increase the sample size n \u2208 (300, 400, 500, . . . , 1, 300) . All other details are the same as in setting 1 except \ufffd = (0.5I(i \ufffd=j)) \u2208 R(p\u22121)\u00d7(p\u22121). Setting 5: This setting is to see the variable selection performance of the ADAP methods. In total we have 10 sites and each site has 1000 samples. The dimension is p = 200 , and the covariates are generated as in setting 1 except \ufffd = (0.5I(i \ufffd=j)) \u2208 R(p\u22121)\u00d7(p\u22121) . We let the true intercept to be -2 and let 10 covariates have the same non-zero coefficients with magnitude selected from (0.1, 0.2, 0.3, 0.4, 0.5). All the other coefficients are zero.\nFor the first four settings, the simulation is repeated 200 times and the number of replications under Setting 5 is 400. We calculate the Euclidean distance of the estimate to its true value to see the parameter estimation performance, e.g., the estimation error for the local estimator is calculated as 1200 \u2211200 r=1 \ufffd\u03b2\u0302 (r) 1 \u2212 \u03b2\n\u2217\ufffd2 , and we use the true positive rate and false positive rate to see the variable selection performance for Setting 5.\nApplication to opioid use disorder (OUD) data. The logistic lasso regression is applied to data from five participating sites of the OneFlorida Clinical Research Consortium30. Combining all five sites, we obtained a total of 5000 cases and 10,000 controls, with each site contributing a small subset of all of their EHRs, i.e., 1000 cases and 2000 controls, or a case\u2013control ratio of 1:2. A list of risk factors was compiled from the literature and extracted from the database, including basic demographic features such as age, gender and race, and co-occurring diagnoses, e.g., depression and sleep disorder (see Supplementary Table\u00a02 for all covariates).\nFor smoking status, race, and insurance type, we treat missing values as a separate group. BMI (body mass index, taking the average value during the 12\u00a0months before the first prescription) and age are categorized into discrete variables to model the possible nonlinear relationship between these factors and OUD. As there is a large proportion of missing values (52.3%) for BMI, we imputed them using an R package MICE51 by regressing BMI on all the other variables in the data and then predicting the missing values based on the fitted model. We then categorized them based on a pre-specified range stated in Supplementary Table\u00a01, where the details of all the dummy variable creation can be found. After excluding nine extremely rare (with a prevalence in the overall sample < 0.2%) covariates from the analysis, we have 42 covariates in the model. See Supplementary Table\u00a02 for a summary of characteristics of each variable across the five sites.\nWithout a loss of generality, we use site 4 as the lead site and apply all the above-described methods to the OUD data. In addition to comparing the estimation and variable selection performance, we conduct a random-splitting procedure to measure the prediction performance. Specifically, we randomly decompose the whole dataset into a training set to fit the model and a testing set to calculate AUC. Using a training size index t ( t = 1, . . . , 9 ) to denote the training set size, each time we randomly select t \u00d7 100 cases and t \u00d7 200 controls from each site to form a training set and use the remaining samples as the testing set. To account for the randomness of decomposition, we repeat the random-splitting procedure 200 times and compute the average AUC for each method as a prediction performance metric. Note that when t is small, there are several fittings that failed due to the existence of some rare covariates whose prevalence becomes even lower or achieves zero in the training set after data splitting. To ensure more robust results, the three least prevalent covariates are removed, and the average AUC is calculated based on the successfully fitted results.\nEthics. The experimental protocol was approved by the University of Florida (UF) Institute Review Board (IRB) as the ethics committee under the protocol number IRB202001100. As part of the UF IRB process, the protocol has been reviewed in accordance with the institutional guidelines and consent waivers were approved as part of the IRB protocol.\nData availability The OUD dataset is available upon application to the OneFlorida+ network through the link: https:// onefl orida conso rtium. org/ front- door/ resea rch- infra struc ture- utili zation- appli cation/. For the reader\u2019s convenience, a synthetic OUD dataset and the related R codes to conduct analysis can be found at https:// github. com/ Pennc il/ ADAP.\nCode availability For dissemination, the R codes are available at https:// github. com/ Pennc il/ pda and through our R package: \u2018pda\u201952 in version 1.0-2.\nReceived: 12 January 2022; Accepted: 31 May 2022\n11\nVol.:(0123456789)\nScientific Reports | (2022) 12:11073 | https://doi.org/10.1038/s41598-022-14029-9"
        },
        {
            "heading": "Acknowledgements",
            "text": "Although unrelated to the work here, Dr. Kranzler is a member of a scientific advisory board for Dicerna Pharmaceuticals, Sophrosyne Pharmaceuticals, and Enthion Pharmaceuticals; a consultant for Sobrera Pharmaceuticals; a member of the American Society of Clinical Psychopharmacology\u2019s Alcohol Clinical Trials Initiative, which was supported in the last three years by Alkermes, Dicerna, Ethypharm, Lundbeck, Mitsubishi, Otsuka, and Pear Therapeutics; holds U.S. Patent 10,900,082. Genotype-Guided Dosing of Opioid Receptor Agonists. 26 Jan. 2021; and has received research support and medication supplies from Alkermes for an investigator-initiated clinical trial of alcohol use disorder. All the figures in this article are drawn by Dr. Xiaokang Liu using R in version 4.0.2 (https:// www.R- proje ct. org/)."
        },
        {
            "heading": "Author contributions",
            "text": "X.L., R.D., C.L., and Y.C. designed methods and experiments; X.L. conducted simulation experiments and data analysis; J.B. provided the dataset from the OneFlorida Clinical Research Consortium; X.L. drafted the main article; all authors made substantial contributions to revise the manuscript; all authors interpreted the results and provided instructive comments; all authors have approved the article."
        },
        {
            "heading": "Funding",
            "text": "Research reported in this publication was supported in part by National Institutes of Health (NIH) grants R01LM012607\u00a01R01LM013519, 1R56AG074604,\u00a0R01AI130460\u00a0and\u00a01R01AG073435 (YC, XL, CL), R01CA246518 (JB), R56AG069880 (JB, YC), and LM010098 (JM), a Centers for Disease Control and Prevention (CDC) Award U18DP006512 (JB), a Patient-Centered Outcomes Research Institute (PCORI) Project Program Award ME2019C3-18315\u00a0and ME-2018C3-14899 (YC, XL, CL) and the VISN4 Mental Illness Research, Education and Clinical Center of the Crescenz VAMC in Philadelphia (HK). All statements in this report, including its findings and conclusions, are solely those of the authors and do not necessarily represent the views of PCORI and its Board of Governors or Methodology Committee, CDC, or NIH."
        },
        {
            "heading": "Competing interests",
            "text": "The authors declare no competing interests."
        },
        {
            "heading": "Additional information",
            "text": "Supplementary Information The online version contains supplementary material available at https:// doi. org/ 10. 1038/ s41598- 022- 14029-9.\nCorrespondence and requests for materials should be addressed to Y.C.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or\nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n\u00a9 The Author(s) 2022"
        }
    ],
    "title": "Multisite learning of high-dimensional heterogeneous data with applications to opioid use disorder study of 15,000 patients across 5 clinical sites",
    "year": 2022
}